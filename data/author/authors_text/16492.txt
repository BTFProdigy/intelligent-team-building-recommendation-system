NAACL-HLT Workshop on the Induction of Linguistic Structure, pages 23?30,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Toward Tree Substitution Grammars with Latent Annotations
Francis Ferraro and Benjamin Van Durme and Matt Post
Center for Language and Speech Processing, and
Human Language Technology Center of Excellence
Johns Hopkins University
Abstract
We provide a model that extends the split-
merge framework of Petrov et al (2006) to
jointly learn latent annotations and Tree Sub-
stitution Grammars (TSGs). We then conduct
a variety of experiments with this model, first
inducing grammars on a portion of the Penn
Treebank and the Korean Treebank 2.0, and
next experimenting with grammar refinement
from a single nonterminal and from the Uni-
versal Part of Speech tagset. We present quali-
tative analysis showing promising signs across
all experiments that our combined approach
successfully provides for greater flexibility
in grammar induction within the structured
guidance provided by the treebank, leveraging
the complementary natures of these two ap-
proaches.
1 Introduction
Context-free grammars (CFGs) are a useful tool for
describing the structure of language, modeling a va-
riety of linguistic phenomena while still permitting
efficient inference. However, it is widely acknowl-
edged that CFGs employed in practice make unre-
alistic independence and structural assumptions, re-
sulting in grammars that are overly permissive. One
successful approach has been to refine the nonter-
minals of grammars, first manually (Johnson, 1998;
Klein and Manning, 2003) and later automatically
(Matsuzaki et al, 2005; Dreyer and Eisner, 2006;
Petrov et al, 2006). In addition to improving pars-
ing accuracy, the automatically learned latent anno-
tations of these latter approaches yield results that
accord well with human intuitions, especially at the
lexical or preterminal level (for example, separating
demonstrative adjectives from definite articles under
the DT tag). It is more difficult, though, to extend
this analysis to higher-level nonterminals, where the
long-distance interactions among latent annotations
of internal nodes are subtle and difficult to trace.
In another line of work, many researchers have ex-
amined the use of formalisms with an extended do-
main of locality (Joshi and Schabes, 1997), where
the basic grammatical units are arbitrary tree frag-
ments instead of traditional depth-one context-free
grammar productions. In particular, Tree Substitu-
tion Grammars (TSGs) retain the context-free prop-
erties of CFGs (and thus the cubic-time inference)
while at the same time allowing for the modeling of
long distance dependencies. Fragments from such
grammars are intuitive, capturing exactly the sorts of
phrasal-level properties (such as predicate-argument
structure) that are not present in Treebank CFGs and
which are difficult to model with latent annotations.
This paper is motivated by the complementarity
of these approaches. We present our progress in
learning latent-variable TSGs in a joint approach that
extends the split-merge framework of Petrov et al
(2006). We present our current results on the Penn
and Korean treebanks (Marcus et al, 1993; Han et
al., 2001), demonstrating that we are able to learn
fragments that draw on the strengths of both ap-
proaches. Table 1 situates this work among other
contributions.
In addition to experimenting directly with the
Penn and Korean Treebanks, we also conducted two
experiments in this framework with the Universal
23
CFG TSG
none Charniak ?97 Cohn et al ?09
manual Klein & Manning ?03 Bansal & Klein ?10
automatic Matsuzaki et al ?05 This paper
Petrov et al ?06
Dreyer & Eisner ?06
Table 1: Representative prior work in learning refine-
ments for context-free and tree substitution grammars,
with zero, manual, or automatically induced latent anno-
tations.
POS tagset (Petrov et al, 2011). First, we investigate
whether the tagset can be automatically derived af-
ter mapping all nonterminals to a single, coarse non-
terminal. Second, we begin with the mapping de-
fined by the tagset, and investigate how closely the
learned annotations resemble the original treebank.
Together with our TSG efforts, this work is aimed at
increased flexibility in the grammar induction pro-
cess, while retaining the use of Treebanks for struc-
tural guidance.
2 Background
2.1 Latent variable grammars
Latent annotation learning is motivated by the ob-
served coarseness of the nonterminals in treebank
grammars, which often group together nodes with
different grammatical roles and distributions (such
as the role of NPs in subject and object position).
Johnson (1998) presented a simple parent-annotation
scheme that resulted in significant parsing improve-
ment. Klein and Manning (2003) built on these ob-
servations, introducing a series of manual refine-
ments that captured multiple linguistic phenomena,
leading to accurate and fast unlexicalized parsing.
Later, automated methods for nonterminal refine-
ment were introduced, first splitting all categories
equally (Matsuzaki et al, 2005), and later refin-
ing nonterminals to different degrees (Petrov et al,
2006) in a split-merge EM framework. This lat-
ter approach was able to recover many of the splits
manually determined by Klein and Manning (2003),
while also discovering interesting, novel clusterings,
especially at the lexical level. However, phrasal-
level analysis of latent-variable grammars is more
difficult. (2006) observed that these grammars could
learn long-distance dependencies through sequences
of substates that place all or most of their weight on
(a) A TSG fragment.
SBAR
IN
for
S
NP VP
TO
to
VP
(b) Equivalent CFG rules.
SBAR ? IN S
IN ? for
S ? NP VP
VP ? TO VP
TO ? to
Figure 1: Simple example of a TSG fragment and an
equivalent representation with a CFG.
particular productions, but such patterns must be dis-
covered manually via extensive analysis.
2.2 Tree substitution grammars
Tree substitution grammars (TSGs) allow for com-
plementary analysis. These grammars employ an ex-
tended domain of locality over traditional context-
free grammars by generalizing the atomic units of the
grammar from depth-one productions to fragments
of arbitrary size. An example TSG fragment along
with equivalent CFG rules are depicted in Figure 1.
The two formalisms areweakly equivalent, and com-
puting the most probable derivation of a sentence
with a TSG can be done in cubic time.
Unfortunately, learning TSGs is not straight-
forward, in large part because TSG-specific re-
sources (e.g., large scale TSG-annotated treebanks)
do not exist. One class of existing approaches,
known as Data-Oriented Parsing, simply uses all the
fragments (Bod, 1993, DOP). This does not scale
well to large treebanks, forcing the use of implicit
representations (Goodman, 1996) or heuristic sub-
sets (Bod, 2001). It has also been generally ob-
served that the use of all fragments results in poor,
overfit grammars, though this can be addressed with
held-out data (Zollmann and Sima?an, 2005) or sta-
tistical estimators to rule out fragments that are un-
likely to generalize (Zuidema, 2007). More recently,
a number of groups have found success employing
Bayesian non-parametric priors (Post and Gildea,
2009; Cohn et al, 2010), which put a downward
pressure on fragment size except where the data
warrant the inclusion of larger fragments. Unfortu-
nately, proper inference under these models is in-
tractable, and though Monte Carlo techniques can
24
provide an approximation, the samplers can be com-
plex, difficult to code, and slow to converge.
This history suggests two approaches to state-split
TSGs: (1) a Bayesian non-parametric sampling ap-
proach (incorporate state-splitting into existing TSG
work), or (2) EM (incorporate TSG induction into
existing state-splitting work). We choose the latter
path, and in the next section will describe our ap-
proach which combines the simplicity of DOP, the
intuitions motivating the Bayesian approach, and the
efficiency of EM-based state-splitting.
In related work, Bansal and Klein (2010) combine
(1996)?s implicit DOP representation with a num-
ber of the manual refinements described in Klein and
Manning (2003). They achieve some of the best re-
ported parsing scores for TSGwork and demonstrate
the complementarity of the tasks, but their approach
is not able to learn arbitrary distributions over frag-
ments, and the state splits are determined in a fixed
pre-processing step. Our approach addresses both of
these limitations.
3 State-Split TSG Induction
In this sectionwe describe howwe combine the ideas
of dop, Bayesian-induced TSGs and Petrov et al
(2006)?s state-splitting framework.1 We are able to
do so by adding a coupling step to each iteration.
That is, each iteration is of the form:
(1) split all symbols in two,
(2) merge 50% of the splits, and
(3) couple existing fragments.
Because every step results in a new grammar, pro-
duction probabilities are fit to observed data by run-
ning at most 50 rounds of EM after every step listed
above.2 We focus on our contribution ? the cou-
pling step? and direct those interested in details re-
garding splitting/merging to (Petrov et al, 2006).
Let T be a treebank and let F be the set of all
possible fragments in T . Define a tree T ? T
as a composition of fragments {Fi}ni=1 ? F , with
T = F1 ? ? ? ? ? Fn. We use X to refer to an arbi-
trary fragment, with rX being the root of X . Two
1Code available at cs.jhu.edu/~ferraro.
2We additionally apply Petrov et al (2006)?s smoothing step
between split and merge.
fragments X and Y may compose (couple), which
we denote byX ?Y .3 We assume thatX and Y may
couple only if X ? Y is an observed subtree.
3.1 Coupling Procedure
While Petrov et al (2006) posit all refinements sim-
ulatenously and then retract half, applying this strat-
egy to the coupling step would result in a combina-
torial explosion. We control this combinatorial in-
crease in three ways. First, we assume binary trees.
Second, we introduce a constraint set C ? F that dic-
tates what fragments are permitted to compose into
larger fragments. Third, we adopt the iterative ap-
proach of split-merge and incrementally make our
grammar more complex by forbidding a fragment
from participating in ?chained couplings:? X ?Y ?Z
is not allowed unless eitherX ?Y or Y ?Z is a valid
fragment in the previous grammar (and the chained
coupling is allowed by C). Note that setting C = ?
results in standard split/merge, while C = F results
in a latently-refined dop-1 model.
We say that ?XY? represents a valid coupling ofX
and Y only if X ? Y is allowed by C, whereas ?XY?
represents an invalid coupling ifX?Y is not allowed
by C. Valid couplings result in new fragments. (We
describe how to obtain C in ?3.3.)
Given a constraint set C and a current grammar G,
we construct a new grammar G?. For every fragment
F ? G, hypothesize a fragment F ? = F ? C, pro-
vided F ? C is allowed byC. In order to add F and
F ? to G?, we assign an initial probability to both frag-
ments (?3.2), and then use EM to determine appro-
priate weights. We do not explicitly remove smaller
fragments from the grammar, though it is possible
for weights to vanish throughout iterations of EM.
Note that a probabilistic TSG fragment may be
uniquely represented as its constituent CFG rules:
make the root of every internal depth-one subtree
unique (have unit probability) and place the entirety
of the TSG weight on the root depth-one rule. This
representation has multiple benefits: it not only al-
lows TSG induction within the split/merge frame-
work, but it also provides a straight-forward way to
use the inside-outside algorithm.
3Technically, the composition operator (?) is ambiguous if
there is more than one occurrence of rY in the frontier of X .
Although notation augmentations could resolve this, we rely on
context for disambiguation.
25
3.2 Fragment Probability Estimation
First, we define a count function c over fragments by
c(X) =
?
T?P(T )
?
??T
?X,? , (1)
where P(T ) is a parsed version of T , ? is a subtree
of T and ?X,? is 1 iff X matches ? .4 We may then
count fragment co-occurrence by
?
Y
c(X ? Y ) =
?
Y :?XY?
c(X ? Y ) +
?
Y :?XY?
c(X ? Y ).
Prior to running inside-outside, we must re-
allocate the probability mass from the previous frag-
ments to the hypothesized ones. As this is just
a temporary initialization, can we allocate mass
as done when splitting, where each rule?s mass is
uniformly distributed, modulo tie-breaking random-
ness, among its refinement offspring? Split/merge
only hypothesizes that a node should have a particu-
lar refinement, but by learning subtrees our coupling
method hypothesizes that deeper structure may bet-
ter explain data. This leads to the realization that a
symbol may both subsume, and be subsumed by, an-
other symbol in the same coupling step; it is not clear
how to apply the above redistribution technique to
our situation.
However, even if uniform-redistribution could
easily be applied, we would like to be able to indi-
cate how much we ?trust? newly hypothesized frag-
ments. We achieve this via a parameter ? ? [0, 1]:
as ? ? 1, we wish to move more of P [X | rX ]
to P [?XY? | rX ]. Note that we need to know which
fragmentsL couple below withX (?XL?), and which
fragments U couple above (?UX?).
For reallocation, we remove a fraction of the num-
ber of occurrences of top-couplings of X:
c? (X) = 1 ? ?
?
Y :?XY? c(X ? Y )
?
Y c(X ? Y )
, (2)
and some proportion of the number of occurrences
of bottom-couplings of X:
sub(X) =
?
U :?UX? c(U ?X)
?
U,L:?UL?
rX=rL
c(U ? L)
. (3)
4We use a parsed version because there are no labeled inter-
nal nodes in the original treebank.
To prevent division-by-zero (e.g., for pre-terminals),
(2) returns 1 and (3) returns 0 as necessary.
Given any fragmentX in an original grammar, let
? be its conditional probability: ? = P [X | rX ] .
For a new grammar, define the new conditional prob-
ability for X to be
P [X | rX ] ? ? ? |c?(X) ? sub(X)|, (4)
and
P [?XY? | rX ] ? ??
c(X ? Y )
?
Y c(X ? Y )
(5)
for applicable Y .
Taken together, equations (4) and (5) simply say
that X must yield some percentage of its current
mass to its hypothesized relatives ?XY?, the amount
of which is proportionately determined by c?. But we
may also hypothesize ?ZX?, which has the effect of
removing (partial) occurrences of X .5
Though we would prefer posterior counts of frag-
ments, it is not obvious how to efficiently obtain pos-
terior ?bigram? counts of arbitrarily large latent TSG
fragments (i.e., c(X ? Y )). We therefore obtain, in
linear time, Viterbi counts using the previous best
grammar. Although this could lead to count sparsity,
in practice our previous grammar provides sufficient
counts across fragments.
3.3 Coupling from Common Subtrees
We now turn to the question of how to acquire the
constraint set C. Drawing on the discussion in ?2.2,
the constraint set should, with little effort, enforce
sparsity. Similarly to our experiments in classifi-
cation with TSGs (Ferraro et al, 2012), we extract
a list of the K most common subtrees of size at
most R, which we refer to as F?R,K?. Note that if
F ? F?R,K?, then all subtreesF ? ofF must also be in
F?R,K?.6 Thus, we may incrementally build F?R,K?
in the following manner: given r, for 1 ? r ? R,
maintain a ranking S, by frequency, of all fragments
of size r; the key point is that S may be built from
F?r?1,K?. Once all fragments of size r have been
considered, retain only the top K fragments of the
ranked set F?r,K? = F?r?1,K? ? S.
5If c?(X) = sub(X), then define Eqn. (4) to be ?.
6Analogously, if an n-gram appears K times, then all con-
stituentm-grams,m < n, must also appear at leastK times.
26
This incremental approach is appealing for two
reasons: (1) practically, it helps temper the growth
of intermediate rankings F?r,K?; and (2) it provides
two tunable parametersR andK, which relate to the
base measure and concentration parameter of previ-
ous work (Post and Gildea, 2009; Cohn et al, 2010).
We enforce sparsity by thresholding at every itera-
tion.
4 Datasets
We perform a qualitative analysis of fragments
learned on datasets for two languages: the Ko-
rean Treebank v2.0 (Han and Ryu, 2005) and a
comparably-sized portion of the WSJ portion of the
Penn Treebank (Marcus et al, 1993). The Korean
Treebank (KTB) has predefined splits; to be compa-
rable for our analysis, from the PTB we used ?2-3
for training and ?22 for validation (we refer to this
as wsj2-3). As described in Chung et al (2010), al-
though Korean presents its own challenges to gram-
mar induction, the KTB yields additional difficulties
by including a high occurrence of very flat rules (in
5K sentences, there are 13 NP rules with at least four
righthand side NPs) and a coarser nonterminal set
than that of the Penn Treebank. On both sets, we
run for two iterations.
Recall that our algorithm is designed to induce a
state-split TSG on a binarized tree; as neither dataset
is binarized in native form we apply a left-branching
binarization across all trees in both collections as a
preprocessing step. Petrov et al (2006) found differ-
ent binarization methods to be inconsequential, and
we have yet to observe significant impact of this bi-
narization decision (this will be considered in more
detail in future work).
Recently Petrov et al (2011) provided a set of
coarse, ?universal? (as measured across 22 lan-
guages), part-of-speech tags. We explore here the
interaction of this tagset in our model on wsj2-3: call
thismodified version uwsj2-3, onwhichwe run three
iterations. By further coarsening the PTB tags, we
can ask questions such as: what is the refinement
pattern? Can we identify linguistic phenomena in a
different manner than we might without the univer-
sal tag set? Then, as an extreme, we replace all POS
tags with the same symbol ?X,? to investigate what
predicate/argument relationships can be derived: we
(a) Modal construction.
S2
S
NP0 VP0
VP
MD
will
VP0
(b) Modifiable NP.
NP2
NP
NN
president
PP0
(c) Nominal-modification.
NP0
NP
NP
NNP3 NNP1
NNP0
NNP0
(d) PP construction.
PP0
IN
at
NP
NP0 NNP0
(e) Initial Quotation.
SINV1
SINV
SINV
SINV0 ,0
?0
VP
VBZ0
Figure 2: Example fragments learned on wsj2-3.
call this set xwsj2-3 and run four times on it.7
5 Fragment Analysis
In this section we analyze hand-selected preliminary
fragments and lexical clusterings our system learns.
WSJ, ?2-3 As Figure 2 illustrates, after two iter-
ations we learn various types of descriptive lexical-
ized and unlexicalized fragments. For example, Fig-
ure 2a concisely creates a four-step modal construc-
tion (will), while 2b demonstrates how a potentially
useful nominal can be formed. Further, learned frag-
ments may generate phrases with multiple nominal
modifiers (2c), and lexicalized PPs (2d).
Note that phrases such as NP0 and VP0 are of-
ten lexicalized themselves (with determiners, com-
mon verbs and other constructions), though omitted
due to space constraints; these lexicalized phrases
could be very useful for 2a (given the incremental
7While the universal tag set has a Korean mapping, the sym-
bols do not coincide with the KTB symbols.
27
(a) Common noun refinements.
NNC
0 ?? ?? ??case this day at the moment
1 ?? ?? ??international economy world
2 ?? ?? ??related announcement report
(b) Verbal inflection.
VV0
NNC2 XSV
?
(c) Adjectival inflection.
VJ0
NNC1 XSJ
?
Figure 3: Clusters and fragments for the KTB.
coupling employed, 2a could not have been further
expanded in two iterations). Figure 2c demonstrates
how TSGs and latent annotations are naturally com-
plementary: the former provides structure while the
latter describes lexical distributions of nominals.
Figure 2e illustrates a final example of syntactic
structure, as we begin to learn how to properly an-
alyze a complex quotation. A full analysis requires
only five TSG rules while an equivalent CFG-only
construction requires eight.
KTB2 To illustrate emergent semantic and syntac-
tic patterns, we focus on common noun (NNC) re-
finements. As seen in Table 3a, top words from
NNC0 represent time expressions and planning-
related. As a comparison, two other refinements,
NNC1 and NNC2, are not temporally representative.
This distinction is important as NNC0 easily yields
adverbial phrases, while the resultant adverbial yield
for either NNC1 or NNC2 is much smaller.
Comparing NNC1 and NNC2, we see that the
highest-ranked members of the latter, which include
report and announcement, can be verbalized by ap-
pending an appropriate suffix. Nouns under NNC1,
such as economy and world, generally are subject
to adjectival, rather than verbal, inflection. Figures
3b and 3c capture these verbal and adjectival inflec-
tions, respectively, as lexicalized TSG fragments.
WSJ, ?2-3, Universal Tag Set In the preliminary
work done here, we find that after a small number of
iterations we can identify various cluster classifica-
tions for different POS tags. Figures 4a, 4b and 4c
provide examples for NOUN, VERB and PRON, re-
spectively. For NOUNs we found that refinements
correspond to agentive entities (refinements 0, 1,
e.g., corporations or governments), market or stock
concepts (2), and numerically-modifiable nouns (7).
Some refinements overlapped, or contained common
nouns usable in many different contexts (3).
Similarly for VERBs (4b), we find suggested dis-
tinctions among action (1) and belief/cognition (2)
verbs.8 Further, some verb clusters are formed of
eventive verbs, both general (3) and domain-specific
(0). Another cluster is primarily of copula/auxiliary
verbs (7). The remaining omitted categories appear
to overlap, and only once we examine the contexts
in which they occur do we see they are particularly
useful for parsing FRAGs.
Though NOUN and VERB clusters can be dis-
cerned, there tends to be overlap among refinements
that makes the analysis more difficult. On the other
hand, refinements for PRON (4c) tend to be fairly
clean and it is generally simple to describe each: pos-
sessives (1), personified wh-words (2) and general
wh-words (3). Moreover, both subject (5) and ob-
ject (6) are separately described.
Promisingly, we learn interactions among various
refinements in the form of TSG rules, as illustrated
by Figures 4d-4g. While all four examples involve
VERBs it is enlightening to analyze a VERB?s re-
finement and arguments. For example, the refine-
ments in 4d may lend a simple analysis of financial
actions, while 4e may describe different NP interac-
tions (note the different refinement symbols). Dif-
ferent VERB refinements may also coordinate, as in
4f, where participle or gerund may help modify a
main verb. Finally, note how in 4g, an object pro-
noun correctly occurs in object position. These ex-
amples suggest that even on coarsened POS tags, our
method is able to learn preliminary joint syntactic
and lexical relationships.
WSJ, ?2-3, Preterminals as X In this experiment,
we investigate whether the manual annotations of
Petrov et al (2011) can be re-derived through first
reducing one?s non-terminal tagset to the symbol
X and splitting until finding first the coarse grain
8The next highest-ranked verbs for refinement 1 include re-
ceived, doing and announced.
28
(a) Noun refinements.
NOUN
0 Corp Big Co.
1 Mr. U.S. New
2 Bush prices trading
3 Japan September Nissan
7 year % months
(b) Verb refinements.
VERB
0 says said sell buy rose
1 have had has been made
2 said says say added believe
3 sold based go trading filed
7 is are be was will
(c) Pronoun refinements.
PRON
1 its his your
2 who whom ?
3 what whose What
5 it he they
6 it them him
(d) VP structure.
VP0
VERB0 NP
ADJ3 NOUN3
(e) Declarative sentence.
S0
NP4 VP
VERB1 NP1
(f) Multiple VP interactions.
VP0
VP
VERB7 ADVP0
VP
VERB0 NP0
(g) Accusative use.
VP0
VERB0 NP
PRON6
Figure 4: Highest weighted representatives for lexical categories (4a-4c) and learned fragments (4d-4g), for uwsj2-3.
X Universal Tag
0 two market brain NOUN
1 ?s said says VERB
2 % company year NOUN
3 it he they PRON
5 also now even ADV
6 the a The DET
7 10 1 all NUM
9 . ? ... .
10 and or but CONJ
12 which that who PRON
13 is was are VERB
14 as of in ADP
15 up But billion ADP
Table 2: Top-three representatives for various refine-
ments of X, with reasonable analogues to Petrov et al
(2011)?s tags. Universal tag recovery is promising.
tags of the universal set, followed by finer-grain tags
from the original treebank. Due to the loss of lexi-
cal information, we run our system for four iterations
rather than three.
As observed in Table 2, there is strong overlap
observed between the induced refinements and the
original universal tags. Though there are 16 refine-
ments of X , due to lack of cluster coherence not all
are listed. Those tags and unlisted refinements seem
to be interwoven in a non-trivial way. We also see
complex refinements of both open- and closed-class
words occurring: refinements 0 and 2 correspond
with the open-class NOUN, while refinements 3 and
12, and 14 and 15 both correspond with the closed
classes PRON and ADP, respectively. Note that 1
and 13 are beginning to split verbs by auxiliaries.
6 Conclusion
We have shown that TSGs may be encoded and in-
duced within a framework of syntactic latent an-
notations. Results were provided for induction us-
ing the English Penn, and Korean Treebanks, with
further experiments based on the Universal Part of
Speech tagset. Examples shown suggest the promise
of our approach, with future work aimed at exploring
larger datasets using more extensive computational
resources.
Acknowledgements Thank you to the reviewers
for helpful feedback, and to JohnsHopkinsHLTCOE
for providing support. We would also like to thank
Byung Gyu Ahn for graciously helping us analyze
the Korean results. Any opinions expressed in this
work are those of the authors.
References
Mohit Bansal and Dan Klein. 2010. Simple, accurate
parsing with an all-fragments grammar. In Proceed-
ings of ACL, pages 1098?1107. Association for Com-
putational Linguistics.
Rens Bod. 1993. Using an annotated corpus as a stochas-
29
tic grammar. In Proceedings of EACL, pages 37?44.
Association for Computational Linguistics.
Rens Bod. 2001. What is the minimal set of fragments
that achievesmaximal parse accuracy? InProceedings
of ACL, pages 66?73. Association for Computational
Linguistics.
Eugene Charniak. 1997. Statistical parsing with a
context-free grammar and word statistics. In Proceed-
ings of AAAI, pages 598?603.
Tagyoung Chung, Matt Post, and Daniel Gildea. 2010.
Factors affecting the accuracy of korean parsing. In
Proceedings of the NAACL HLT Workshop on Sta-
tistical Parsing of Morphologically-Rich Languages
(SPMRL), pages 49?57, Los Angeles, California,
USA, June.
Trevor Cohn, Sharon Goldwater, and Phil Blunsom.
2009. Inducing compact but accurate tree-substitution
grammars. In Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 548?556, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Trevor Cohn, Phil Blunsom, and Sharon Goldwater.
2010. Inducing tree-substitution grammars. Journal of
Machine Learning Research, 11:3053?3096, Decem-
ber.
Markus Dreyer and Jason Eisner. 2006. Better informed
training of latent syntactic features. In Proceedings of
EMNLP, pages 317?326, Sydney, Australia, July. As-
sociation for Computational Linguistics.
Francis Ferraro, Matt Post, and Benjamin Van Durme.
2012. Judging Grammaticality with Count-Induced
Tree Substitution Grammars. In Proceedings of the
Seventh Workshop in Innovated Use of NLP for Build-
ing Educational Applications.
Joshua Goodman. 1996. Efficient algorithms for pars-
ing the dop model. In Proceedings of EMNLP, pages
143?152.
Na-Rae Han and Shijong Ryu. 2005. Guidelines for
Penn Korean Treebank. Technical report, University
of Pennsylvania.
Chung-hye Han, Na-Rae Han, and Eon-Suk Ko. 2001.
Bracketing guidelines for penn korean treebank. Tech-
nical report, IRCS, University of Pennsylvania.
Mark Johnson. 1998. PCFG models of linguis-
tic tree representations. Computational Linguistics,
24(4):613?632.
Aravind K. Joshi and Yves Schabes. 1997. Tree-
adjoining grammars. In G. Rozenberg and A. Salo-
maa, editors, Handbook of Formal Languages: Be-
yond Words, volume 3, pages 71?122. Springer.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of the 41st An-
nual Meeting on Association for Computational Lin-
guistics - Volume 1, pages 423?430, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a large annotated cor-
pus of english: The Penn Treebank. Computational
linguistics, 19(2):313?330.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic cfg with latent annotations. InPro-
ceedings of ACL, pages 75?82, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. InProceedings of ACL-ICCL,
pages 433?440, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2011.
A universal part-of-speech tagset. In ArXiv, April.
Matt Post and Daniel Gildea. 2009. Bayesian learning of
a tree substitution grammar. In Proceedings of ACL-
IJCNLP (short papers), pages 45?48, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Andreas Zollmann and Khalil Sima?an. 2005. A consis-
tent and efficient estimator for data-oriented parsing.
Journal of Automata Languages and Combinatorics,
10(2/3):367.
Willem Zuidema. 2007. Parsimonious data-oriented
parsing. In Proceedings of EMNLP-CoNLL, pages
551?560.
30
The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 116?121,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Judging Grammaticality with Count-Induced Tree Substitution Grammars
Francis Ferraro, Matt Post and Benjamin Van Durme
Department of Computer Science, and HLTCOE
Johns Hopkins University
{ferraro,post,vandurme}@cs.jhu.edu
Abstract
Prior work has shown the utility of syntactic
tree fragments as features in judging the gram-
maticality of text. To date such fragments have
been extracted from derivations of Bayesian-
induced Tree Substitution Grammars (TSGs).
Evaluating on discriminative coarse and fine
grammaticality classification tasks, we show
that a simple, deterministic, count-based ap-
proach to fragment identification performs on
par with the more complicated grammars of
Post (2011). This represents a significant re-
duction in complexity for those interested in
the use of such fragments in the development
of systems for the educational domain.
1 Introduction
Automatically judging grammaticality is an im-
portant component in computer-assisted education,
with potential applications including large-scale es-
say grading and helping to interactively improve the
writing of both native and L2 speakers. While n-
gram models have been productive throughout nat-
ural language processing (NLP), they are obviously
insufficient as models of languages, since they do
not model language structure or correspondences
beyond the narrow Markov context.
Context-free grammars (CFGs) address many of
the problems inherent in n-grams, and are there-
fore intuitively much better suited for grammatical-
ity judgments. Unfortunately, CFGs used in practice
are permissive (Och et al, 2004) and make unreal-
istic independence and structural assumptions, re-
sulting in ?leaky? grammars that overgenerate and
thus serve poorly as models of language. How-
ever, approaches that make use of the CFG produc-
tions as discriminative features have performed bet-
ter. Cherry and Quirk (2008) improved upon an n-
gram baseline in grammatical classification by ad-
justing CFG production weights with a latent SVM,
while others have found it useful to use comparisons
between scores of different parsers (Wagner et al,
2009) or the use of CFG productions in linear clas-
sification settings (Wong and Dras, 2010) in classi-
fying sentences in different grammaticality settings.
Another successful approach in grammaticality
tasks has been the use of grammars with an extended
domain of locality. Post (2011) demonstrated that
larger syntactic patterns obtained from Tree Sub-
stitution Grammars (Joshi, 1985) outperformed the
Cherry and Quirk models. The intuitions underlying
their approach were that larger fragments are more
natural atomic units in modeling grammatical text,
and that larger fragments reduce the independence
assumptions of context-free generative models since
there are fewer substitution points in a derivation.
Their grammars were learned in a Bayesian setting
with Dirichlet Process priors, which have simple for-
mal specifications (c.f., Goldwater et al (2009, Ap-
pendix A)), but which can become quite complicated
in implementation.
In this paper, we observe that fragments used for
classification do not require an underlying proba-
bilistic model. Here, we present a simple extraction
method that elicits a classic formal non-probabilistic
grammar from training data by deterministically
counting fragments. Whereas Post parses with his
TSG and extracts the Viterbi derivation, we use an
116
SBAR
IN
for
S
NP VP
TO
to
VP
(a) A TSG fragment.
SBAR? IN S
IN? for
S? NP VP
VP? TO VP
TO? to
(b) Equivalent CFG rules.
Figure 1: Equivalent TSG fragment and CFG rules.
off-the-shelf parser and pattern match the fragments
in our grammar against the tree. With enough pos-
itive and negative training data (in the form of au-
tomatic parses of good and bad sentences), we can
construct classifiers that learn which fragments cor-
relate with grammaticality. The resulting model re-
sults in similar classification accuracy while doing
away with the complexity of Bayesian techniques.
2 Tree Substitution Grammars (TSGs)
Though CFGs and TSGs are weakly equivalent,
TSGs permit nonterminals to rewrite as tree frag-
ments of arbitrary size, whereas CFG rewrites are
limited to depth-one productions. Figure 1 de-
picts an example TSG fragment and equivalent CFG
rules; note that the entire internal structure of 1a is
described within a single rewrite.
Unfortunately, learning probabilistic TSGs is not
straight-forward, in large part because TSG-specific
resources (e.g., large scale TSG-annotated tree-
banks) do not exist. Approaches to this problem be-
gan by taking all fragments Fall in a treebank (Bod,
1993; Goodman, 1996), which resulted in very large
grammars composed mostly of fragments very un-
likely to generalize.1 A range of heuristic solutions
reduced these grammar sizes to a much smaller,
more compact subset of all fragments (Zollmann
and Sima?an, 2005; Zuidema, 2007). More recently,
more principled models have been proposed, taking
the form of inference in Bayesian non-parametric
models (Post and Gildea, 2009; Cohn et al, 2009).
In addition to providing a formal model for TSGs,
these techniques address the overfitting problem of
1The n-gram analog would be something like storing all 30-
grams seen in a corpus.
all fragments grammars with priors that discourage
large fragments unless there is enough evidence to
warrant their inclusion in the grammar. The problem
with such approaches, however, is that the sampling
procedures used to infer them can be complex, dif-
ficult to code, and slow to converge. Although more
general techniques have been proposed to better ex-
plore the search space (Cohn and Blunsom, 2010;
Cohn et al, 2010; Liang et al, 2010), the complex-
ity and non-determinism of these samplers remain,
and there are no publicly available implementations.
The underlying premise behind these grammar
learning approaches was the need for a probabilis-
tic grammar for parsing. Post (2011) showed that
the fragments extracted from derivations obtained
by parsing with probabilistic TSGs were useful as
features in two coarse-grained grammaticality tasks.
In such a setting, fragments are needed for classifica-
tion, but it is not clear that they need to be obtained
from derivations produced by parsing with proba-
bilistic TSGs. In the next section, we describe a sim-
ple, deterministic, count-based approach to learn-
ing an unweighted TSG. We will then demonstrate
(?4) the effectiveness of these grammars for gram-
maticality classification when fragments are pattern-
matched against parse trees obtained from a state-of-
the-art parser.
3 Counting Common Subtrees
Rather than derive probabilistic TSGs, we employ
a simple, iterative and deterministic (up to tie-
breaking) alternative to TSG extraction. Our method
extracts F?R,K?, the K most common subtrees of
size at most R. Though selecting the top K-most-
frequent fragments from all fragments is computa-
tionally challenging through brute force methods,
note that if F ? F?R,K?, then all subtrees F
? of F
must also be in F?R,K?.
2 Thus, we may incremen-
tally build F?R,K? in the following manner: given r,
for 1 ? r ? R, maintain a ranking S, by frequency,
of all fragments of size r; the key point is that S may
be built from F?r?1,K?. Once all fragments of size
r have been considered, retain only the top K frag-
ments of the ranked set F?r,K? = F?r?1,K? ? S.
3
2Analogously, if an n-gram appears K times, then all con-
stituent m-grams, m < n, must also appear at least K times.
3We found that, at the thresholding stage, ties may be arbi-
trarily broken with neglible-to-no effect on results.
117
Algorithm 1 EXTRACTFRAGMENTS (R,K)
Assume: Access to a treebank
1: S ? ?
2: F?1,K? ? top K CFG rules used
3: for r = 2 to R do
4: S ? S ? {observed 1-rule extensions of F ?
F?r?1,K?}
5: F?r,K? ? top K elements of F?r?1,K? ? S
6: end for
Pseudo-code is provided in Algorithm 1.4
This incremental approach is appealing for two
reasons. Firstly, our approach tempers the growth
of intermediate rankings F?r,K?. Secondly, we
have two tunable parameters R and K, which can
be thought of as weakly being related to the base
measure and concentration parameter of (Post and
Gildea, 2009; Cohn et al, 2010). Note that by
thresholding at every iteration, we enforce sparsity.
4 Experiments
We view grammaticality judgment as a binary clas-
sification task: is a sequence of words grammatical
or not? We evaluate on two tasks of differing granu-
larity: the first, a coarse-grain classification, follows
Cherry and Quirk (2008); the other, a fine-grain ana-
logue, is built upon Foster and Andersen (2009).
4.1 Datasets
For the coarse-grained task, we use the BLLIP5-
inspired dataset, as in Post (2011), which dis-
criminates between BLLIP sentences and Kneyser-
Ney trigram generated sentences (of equal length).
Grammatical and ungrammatical examples are given
in 1 and 2 below, respectively:
(1) The most troublesome report may be the
August merchandise trade deficit due out
tomorrow .
(2) To and , would come Hughey Co. may be
crash victims , three billion .
For the fine-grained task we use a version of the
BNC that has been automatically modified to be
4Code is available at: cs.jhu.edu/?ferraro.
5LDC2000T43
ungrammatical, via insertions, deletions or substi-
tutions of grammatically important words. As has
been argued in previous work, these automatically
generated errors, simulate more realistic errors (Fos-
ter and Andersen, 2009). Example 3 gives an origi-
nal sentence, with an italicized substitution error:
(3) The league ?s promoters hope retirees and
tourists will join die-hard fans like Mr. de
Castro and pack then stands to see the seniors .
Both sets contain train/dev/test splits with an
equal number of positive and negative examples, and
all instances have an available gold-standard parse6.
4.2 Models and Features
Algorithm 1 extracts common constructions, in the
form of count-extracted fragments. To test the ef-
ficacy of these fragments, we construct and experi-
ment with various discriminative models.
Given count-extracted fragments obtained from
EXTRACTFRAGMENTS(R,K), it is easy to define a
feature vector: for each query, there is a binary fea-
ture indicating whether a particular extracted frag-
ment occurs in its gold-standard parse. These count-
extracted features, along with the sentence length,
define the first model, called COUNT.
Although our extracted fragments may help
identify grammatical constructions, capturing un-
grammatical constructions may be difficult, since
we do not parse with our fragments. Thus,
we created two augmented models, COUNT+LEX
and COUNT+CFG, which built upon and extended
COUNT. COUNT+LEX included all preterminal and
lexical items. For COUNT+CFG, we included a bi-
nary feature for every rule that was used in the most
likely parse of a query sentence, according to a
PCFG7.
Following Post (2011), we train an `-2 regular-
ized SVM using liblinear8 (Fan et al, 2008)
per model. We optimized the models on dev data,
letting the smoothing parameter be 10m, for integral
m ? [?4, 2]: 0.1 was optimal for all models.
6We parsed all sentences with the Berkeley parser (Petrov et
al., 2006).
7We used the Berkeley grammar/parser (Petrov et al, 2006)
in accurate mode; all other options were their default values.
8csie.ntu.edu.tw/?cjlin/liblinear/
118
Task COUNT COUNT+LEX COUNT+CFG
coarse 86.3 86.8 88.3
fine 62.9 64.3 67.0
(a) Our count-based models, with R = 15, K = 50k.
Task 3 5 10 15
coarse 89.2 89.1 88.6 88.3
fine 67.9 67.2 67.2 67.0
(b) Performance of COUNT+CFG, with K =
50k and varying R.
Table 1: Development accuracy results.
Our three models all have the same two tunable
parameters, R and K. While we initially experi-
mented with R = 31,K ? {50k, 100k} ? in or-
der to be comparable to the size of Post (2011)?s ex-
tracted TSGs ? we noticed that very few, if any,
fragments of size greater than 15 are able to sur-
vive thresholding. Dev experimentation revealed
that K = 50k and 100k yielded nearly the same
results; for brevity, we report in Table 1a dev re-
sults for all three models, with R = 15,K =
50k. The differences across models was stark, with
COUNT+CFG yielding a two point improvement over
COUNT on coarse, but a four point improvement
on fine. While COUNT+LEX does improve upon
COUNT, on both tasks it falls short of COUNT+CFG.
These differences are not completely surprising:
one possible explanation is that the PCFG features
in COUNT+CFG yield useful negatively-biased fea-
tures, by providing a generative explanation. Due
to the supremacy of COUNT+CFG, we solely report
results on COUNT+CFG.
In Table 1b, we also examine the effect of ex-
tracted rule depth on dev classification accuracy,
where we fix K = 50k and vary R ? {3, 5, 10, 15},
where the best results are achieved with R = 3.
We evaluate two versions of COUNT+CFG: one with
R = 3 and the other with R = 15 (K = 50k for
both).
5 Results and Fragment Analysis
We build on Post (2011)?s results and compare
against bigram, CFG and TSG baselines. Each base-
line model is built from the same `-2 regularized
Method coarse fine
COUNT+CFG, R = 3 89.1 67.2
COUNT+CFG, R = 15 88.2 66.6
bigram 68.4 61.4
CFG 86.3 64.5
TSG 89.1 67.0
Table 2: Classification accuracy on test portions for
both coarse and fine, with K = 50k. Chance is 50%
for each task.
SVM as above, and each is optimized on dev data.
For the bigram baseline, the binary features corre-
spond with whether a particular bigram appears in
an instance, while the CFG baseline is simply the
augmentation feature set used for COUNT+CFG. For
the TSG baseline, the binary features correspond
with whether a particular fragment is used in the
most probable derivation of each input sentence (us-
ing Post?s Bayesian TSGs). All baselines use the
sentence length as a feature as well.
The results on the test portions of each dataset are
given in Table 2. When coupled with the best parse
output, our counting method was able to perform on
par with, and even surpass, Post?s TSGs. The sim-
pler model (R = 3) ties TSG performance on coarse
and exceeds it by two-tenths on fine; the more com-
plex model (R = 15) gets within a point on coarse
and four-tenths on fine. Note that both versions of
COUNT+CFG surpass the CFG baseline on both sets,
indicating that (1) encoding deeper structure, even
without an underlying probabilistic model, is use-
ful for grammaticality classifications, and (2) this
deeper structure can be achieved by a simple count-
ing scheme.
As PCFG output comprises a portion of our fea-
ture set, it is not surprising that a number of the
most discriminative positive and negative features,
such as flat NP and VP rules not frequent enough
to survive thresholding, were provided by the CFG
parse. While this points out a limitation of our
non-adaptive thresholding, note that even among
the highest weighted features, PCFG and count-
extracted features were interspersed. Further, con-
sidering that both versions of COUNT+CFG outper-
formed CFGs, it seems our method adds discrimina-
tive power to the CFG rules.
119
(a) Coarse (b) Fine
Grammatical Ungrammatical Grammatical Ungrammatical
1 (S NP VP (. .)) (S NP (VP (VBP are)
PP))
10 (SBAR (IN if) S) (SBAR (S VP))
2 (S (S (VP VBG NP))
VP)
(VP VBZ (S VP)) 11 (NP (DT these) NNS) (SBAR DT (S NP
VP))
3 (SBAR (IN while) S) (SBAR (S VP) ) 12 (VP (VBG being) VP) (S (VP VB NP))
4 (VP (VBD called) S) (VP VBN (S VP)) 13 (PP IN (S NP (VP
VBG NP)))
(S (VP VBZ NP))
5 (VP (VB give) NP NP) (NP (NP JJ NN)
SBAR)
14 (S (VP VBG VP)) (VP VB (S VP))
6 (NP NNP NNP NNP
(NNP Inc.))
(VP NN (PP IN NP)) 15 (PP IN (SBAR (IN
whether) S))
(S (VP VBP VP))
7 (PP (IN with) (S NP
VP))
(S (VP MD VP)) 16 (VP (VBD had) (VP
VBN S))
(S NP (VP (VBD
said)))
8 (SBAR (IN for) (S NP
(VP (TO to) VP)))
(SBAR (S (NP NNS)
VP))
17 (VP MD (VP VB NP
(PP IN NP) PP))*
(PP (PP IN NP) (CC
and) PP)*
9 (PRN (-LRB- -LRB-)
NP (-RRB- -RRB-))*
(S (ADJP JJ))* 18 (NP (DT no) NNS)* (PP (IN As) NP)*
Table 3: Most discriminative count-based features for COUNT+CFG on both tasks. For comparability to Post
(2011), R = 15,K = 50k, are shown. Asterisks (*) denote fragments hand-selected from the top 30.
Table 5 presents top weighted fragments from
COUNT+CFG on both coarse and fine, respectively.
Examining useful grammatical features across tasks,
we see a variety of fragments: though our fragments
heavily weight simple structure such as proper punc-
tuation (ex. 1) and parentheticals (ex. 9), they also
capture more complex phenomena such as lexical
argument descriptions (e.g., give, ex. 5). Our ex-
tracted fragments also describe common construc-
tions and transitions (e.g., 3, 8 and 15) and involved
verb phrases (e.g., gerunds in 2 and 14, passives in
16, and modals in 17).
Though for both tasks some ungrammatical frag-
ments easily indicate errors, such as sentence frag-
ments (e.g., example 6) or repeated words (ex. 11),
in general the analysis is more difficult. In part, this
is because, when isolated from errors, one may con-
struct grammatical sentences that use some of the
highest-weighted ungrammatical fragments. How-
ever, certain errors may force particular rules to be
inappropriately applied when acquiring the gold-
standard parse. For instance, example 10 typically
coordinates with larger VPs, via auxiliary verbs or
expletives (e.g., it). Affecting those crucial words
can significantly change the overall parse structure:
consider that in ?said it is too early. . . ,? it provides a
crucial sentential link; without it, ?is too early? may
be parsed as a sentence, and then glued on to the
former part.
6 Conclusion
In this work, we further examined TSGs as useful
judges of grammaticality for written English. Us-
ing an iterative, count-based approach, along with
the most likely PCFG parse, we were able to train a
discriminative classifier model ? COUNT+CFG ?
that surpassed the PCFG?s ability to judge gram-
maticality, and performed on par with Bayesian-
TSGs. Examining the highest weighted features, we
saw that complex structures and patterns encoded by
the count-based TSGs proved discriminatively use-
ful. This suggests new, simpler avenues for frag-
ment learning, especially for grammaticality judg-
ments and other downstream tasks.
Acknowledgements Thank you to the reviewers
for helpful feedback, and thanks to Johns Hopkins
HLTCOE for providing support. Any opinions ex-
pressed in this work are those of the authors.
120
References
R. Bod. 1993. Using an annotated corpus as a stochas-
tic grammar. In Proceedings of the sixth conference
on European chapter of the Association for Computa-
tional Linguistics, pages 37?44. Association for Com-
putational Linguistics.
Colin Cherry and Chris Quirk. 2008. Discrimina-
tive, syntactic language modeling through latent svms.
Proceeding of Association for Machine Translation in
the America (AMTA-2008).
Trevor Cohn and Phil Blunsom. 2010. Blocked inference
in bayesian tree substitution grammars. In Proceed-
ings of ACL (short papers), pages 225?230, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Trevor Cohn, Sharon Goldwater, and Phil Blunsom.
2009. Inducing compact but accurate tree-substitution
grammars. In Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 548?556, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Trevor Cohn, Phil Blunsom, and Sharon Goldwater.
2010. Inducing tree-substitution grammars. Journal
of Machine Learning Research, 11:3053?3096, De-
cember.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A
library for large linear classification. Journal of Ma-
chine Learning Research, 9:1871?1874, June.
Jennifer Foster and Oistein E. Andersen. 2009. Gen-
ERRate: generating errors for use in grammatical error
detection. In Proceedings of the Fourth Workshop on
Innovative Use of NLP for Building Educational Ap-
plications, pages 82?90.
Sharon Goldwater, Thomas L. Griffiths, and Mark John-
son. 2009. A Bayesian framework for word segmen-
tation: Exploring the effects of context. Cognition,
112(1):21 ? 54.
Joshua Goodman. 1996. Efficient algorithms for parsing
the dop model. In Proceedings of EMNLP, pages 143?
152.
A.K. Joshi. 1985. Tree adjoining grammars: How much
context-sensitivity is required to provide reasonable
structural descriptions? Natural language parsing,
pages 206?250.
Percy Liang, Michael .I. Jordan, and Dan Klein. 2010.
Type-based MCMC. In Human Language Technolo-
gies: The 2010 Annual Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics. Association for Computational Linguistics.
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,
Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar
Kumar, Libin Shen, David Smith, Katherine Eng, et al
2004. A smorgasbord of features for statistical ma-
chine translation. In Proceedings of NAACL.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and in-
terpretable tree annotation. In Proceedings of ACL-
ICCL, pages 433?440, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
Matt Post and Daniel Gildea. 2009. Bayesian learn-
ing of a tree substitution grammar. In Proceedings
of ACL-IJCNLP (short papers), pages 45?48, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Matt Post. 2011. Judging grammaticality with tree
substitution grammar derivations. In Proceedings of
ACL (short papers), pages 217?222, Stroudsburg, PA,
USA. Association for Computational Linguistics.
J. Wagner, J. Foster, and J. van Genabith. 2009. Judg-
ing grammaticality: Experiments in sentence classifi-
cation. CALICO Journal, 26(3):474?490.
Sze-Meng Jojo Wong and Mark Dras. 2010. Parser
features for sentence grammaticality classification. In
Proceedings of the Australasian Language Technology
Association Workshop.
Andreas Zollmann and Khalil Sima?an. 2005. A consis-
tent and efficient estimator for Data-Oriented Parsing.
Journal of Automata, Languages and Combinatorics,
10(2/3):367?388.
Willem Zuidema. 2007. Parsimonious data-oriented
parsing. In Proceedings of EMNLP-CoNLL, pages
551?560.
121
Proceedings of the Fourth Workshop on Teaching Natural Language Processing, pages 66?76,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
A Virtual Manipulative for Learning Log-Linear Models
Francis Ferraro and Jason Eisner
Department of Computer Science
Johns Hopkins University
Baltimore, MD, USA
{ferraro, jason}@cs.jhu.edu
Abstract
We present an open-source virtual ma-
nipulative for conditional log-linear mod-
els. This web-based interactive visual-
ization lets the user tune the probabili-
ties of various shapes?which grow and
shrink accordingly?by dragging sliders
that correspond to feature weights. The
visualization displays a regularized train-
ing objective; it supports gradient as-
cent by optionally displaying gradients
on the sliders and providing ?Step? and
?Solve? buttons. The user can sam-
ple parameters and datasets of differ-
ent sizes and compare their own pa-
rameters to the truth. Our web-
site, http://cs.jhu.edu/?jason/
tutorials/loglin/, guides the user
through a series of interactive lessons and
provides auxiliary readings, explanations,
practice problems and resources.
1 Introduction
We argue that if one is going to teach only a sin-
gle machine learning technique in a computational
linguistics course, it should be conditional log-
linear modeling. Such models are pervasive in nat-
ural language processing. They have the form
p~?(y | x) ? exp
(
~? ? ~f (x, y)
)
, (1)
where ~f extracts a feature vector from context x
and outcome y ? Y(x). The set of possible out-
comes Y(x) might depend on the context x.1
1The model is equivalent to logistic regression when y is
a binary variable, that is, when Y(x) = {0, 1}.
We then present an interactive web visualiza-
tion that guides students through playing with log-
linear models and their estimation. This open-
source tool, available at http://cs.jhu.
edu/?jason/tutorials/loglin/, is in-
tended to develop intuitions, so that basic log-
linear models can be then taken for granted in fu-
ture lectures. It can be used near the start of a
course, perhaps after introducing probability no-
tation and n-gram models.
We used the tool in our Natural Language Pro-
cessing (NLP) class and received very positive
feedback. Students were excited by it, with some
saying the tool helped develop their ?physical in-
tuition? for log-linear models. Other test users
with no technical background also enjoyed work-
ing through the introductory lessons and found
that they began to understand the model.
The app includes 18 ready-to-use lessons for in-
dividual or small-group study or classroom use.
Each lesson, e.g. Figure 1, guides the student to
fit a probability model p~?(y | x) over some collec-
tion Y of shapes, words, or other images such as
parse trees. Each lesson is peppered with ques-
tions; students can be asked to answer some of
these questions in writing.2 Ambitious instruc-
tors can add new lessons or edit existing ones by
writing configuration files (see section 5.3). This
is useful for emphasizing specific concepts or ap-
plications. Section 8 provides some history and
applications of log-linear modeling, as well as as-
signment ideas.
2There are approximately 6 questions per lesson. We
found that answering all the questions took our students about
2300 words, or just under 23 words per question, which was
probably both unreasonable and unnecessary.
66
Figure 1: The first lesson; the lower half is larger on the actual application.
2 Why Teach With Log-Linear Models?
Log-linear models are very handy in NLP. They
can be used throughout a course, when one needs
? a global classifier for an applied task, such as
detecting sentiment, topic, spam, or gender;
? a local classifier for structure annotation,
such as tags or segment boundaries;
? a local classifier to be applied repeatedly in
sequential decision-making;
? a local conditional probability within some
generative process, such as an n-gram model,
HMM, PCFG, probabilistic FSA or FST,
noisy-channel MT model, or Bayes net;
? a global structured prediction method. Here
y is a complete structured object such as a
tagging, segmentation, parse, alignment, or
translation. Then p(y | x) is a Markov ran-
dom field or a conditional random field, de-
pending on whether x is empty or not.
Log-linear models over discrete variables are
also sufficiently expressive for an NLP course.
Students may experiment freely with adding their
own creative model features that refer to salient at-
tributes or properties of the data, since the proba-
bility (1) may consider any number of informative
features of the (x, y) pair.
How about training? Estimation of the pa-
rameter weights ~? from a set of fully observed
(x, y) pairs is simply a convex optimization prob-
lem. Maximizing the regularized conditional log-
likelihood
F (~?) =
(
N?
i=1
log p~? (yi | xi)
)
? C ?R(~?) (2)
is a simple, uniform training principle that can be
used throughout the course. The scaled regular-
izer C ? R(~?) prevents overfitting on sparse fea-
tures. This is arguably more straightforward than
the traditional NLP smoothing methods for esti-
mating probabilities from sparse data (Chen and
Goodman, 1996), which require applying various
ad hoc formulas to counts, and which do not gen-
eralize well to settings where there is not a natural
sequence of backoff models. There exist fast and
usable tools that students can use to train their log-
67
linear models, including, among others, MegaM
(Daume? III, 2004), and NLTK (Bird et al, 2009).3
Formally, log-linear models are a good gate-
way to a more general understanding of undirected
graphical models and the exponential family, in-
cluding globally normalized joint or conditional
distributions over trees and sequences.
One reason that log-linear models are both ver-
satile and pedagogically useful is that they do not
just make predictions, but explicitly model proba-
bilities. These can be
? combined with other probabilities using the
usual rules of probability;
? marginalized at test time to obtain the prob-
ability that the outcome y has a particular
property (e.g., one can sum over alignments);
? marginalized at training time in the case of
incomplete data y (e.g., the training data may
not include alignments);
? used to choose among possible decisions by
computing their expected loss (risk).
The training procedure also takes a probabilis-
tic view. Equation (2) helps illustrate impor-
tant statistical principles such as maximum likeli-
hood,4 regularization (the bias-variance tradeoff),
and cross-validation, as well as optimization prin-
ciples such as gradient ascent.
Log-linear models also provide natural exten-
sions of commonly taught NLP methods. For ex-
ample, under a probabilistic context-free gram-
mar (PCFG),5 p(parse tree | sentence) is propor-
tional to a product of rule probabilities. Simply
replacing each rule probability with an arbitrary
non-negative potential?an exponentiated weight,
or sum of weights of features of that rule?gives
an instance of (1). The same parsing algorithms
still apply without modification, as does the same
inside-outside approach to computing the poste-
rior expectation of rule counts and feature counts.
Immediate variants include CRF CFGs (Finkel
3A caveat is that generic log-linear training tools will iter-
ate over the setY(x) in order to maximize (1) and to compute
the constant of proportionality in (1) and the gradient of (2).
This is impractical when Y(x) is large, as in language mod-
eling or structured prediction. See Section 8.
4Historically, this objective has been regarded as the opti-
mization dual of a maximum entropy problem (Berger et al,
1996), motivating the log-linear form of (2). We have consid-
ered adding a maximum entropy view to our manipulative.
5Likewise for Markov or hidden Markov models.
et al, 2008), in which the rule features become
position-dependent and sentence-dependent, and
log-linear PCFGs (Berg-Kirkpatrick et al, 2010),
in which the feature-rich rule potentials are locally
renormalized into rule probabilities via (1).
For all these reasons, we recommend log-linear
models as one?s ?go-to? machine learning tech-
nique when teaching. Other linear classifiers,
such as perceptrons and SVMs, similarly choose
y given x based on a linear score ~f ? ~?(x, y)?but
these scores have no probabilistic interpretation,
and the procedures for training ~? are harder to un-
derstand or to justify. Thus, they can be taught
as variants later on or in another course. Further
reading includes (Smith, 2011).
3 The Teaching Challenge
Unfortunately, there is a difficulty with introduc-
ing log-linear models early in a course. Once
grasped, they seem very simple. But they are not
so easy to grasp for a student who has not had
any experience with high-dimensional parametric
functions, feature design, or statistical estimation.
The interaction among the parameters can be be-
wildering. Log-likelihood, gradient ascent, and
overfitting may also be new ideas.
Students who lack intuitions about these mod-
els will fail to follow subsequent lectures. They
will also have trouble with homework projects?
interpreting the weights learned by their model,
and diagnosing problems with their features or
their implementation. A student cannot even de-
sign appropriate feature sets without understand-
ing how the weights of these features interact to
define a distribution. We will discuss some of the
necessary intuitions in sections 6 and 7.
We would like equations (1), (2), and the gradi-
ent formula to be more than just recipes. The stu-
dent should regard them as familiar objects with
predictable behavior. Like computer science, ped-
agogy proceeds by layering new ideas on top of
already-familiar abstractions. A solid understand-
ing of basic log-linear models is prerequisite to
? using them in NLP applications that have
their own complexities,
? using them as component distributions within
larger probability models or decision rules,
? generalizing the algorithms for working with
(1) and (2) to settings where one cannot eas-
ily enumerate Y .
68
4 (Virtual) Manipulatives
Familiar concrete concepts have often been in-
voked to help develop intuitions about abstract
mathematical concepts. Specifically within early
math education, manipulatives?tactile objects?
have been shown to be effective hands-on teaching
tools. Examples include Cuisenaire rods for ex-
ploring arithmetic concepts like sums, ratios, and
place value, or geoboards for exploring geometric
concepts like area and perimeter.6 The key idea is
to ground and link the mathematical language to a
well-known physical object that can be inspected
and manipulated. For more, see the classic and re-
cent analyses from Sowell (1989) and Carbonneau
et al (2013).
Research has shown concrete manipulatives to
be effective, but practical widespread use of them
presents certain problems, including procurement
of necessary materials, replicability, and applica-
bility to certain groups of students and to con-
cepts that have no simple physical realization.
These issues have spurred interest over the past
two decades in virtual manipulatives implemented
in software, including the creation of the National
Library of Virtual Manipulatives.7 Both Clements
and McMillen (1996) and Moyer et al (2002) pro-
vide accessible overviews of virtual manipulatives
in early math education. Virtual manipulatives
give students the ability to effect changes on a
complex system and so learn its underlying prop-
erties (Moyer et al, 2002). This last point is par-
ticularly relevant to log-linear models.
Members of the NLP and speech communities
have previously explored manipulatives and the
idea of ?learning by doing.? Eisner (2002) im-
plemented HMM posterior inference and forward-
backward training on a spreadsheet, so that editing
the data or initial parameters changed the numeri-
cal computations and the resulting graphs. VIS-
PER, an applied educational tool that wrapped
various speech technologies, was targeted toward
understanding the acoustics and overall recog-
nition pipeline (Nouza et al, 1997). Light et
al. (2005) developed web interfaces for a num-
ber of core NLP technologies and systems, such
as parsers, part-of-speech taggers, and finite-state
6Cuisenaire rods are color-coded blocks with lengths from
1 to 10. A geoboard is a board representing the plane, with
pegs at the integral points. A rubber band can be stretched
around selected pegs to define a polygon.
7nlvm.usu.edu/en/nav/vlibrary.html and
enlvm.usu.edu/ma/nav/doc/intro.jsp
transducers. Matt Post created a Model 1 stack
decoder visualization for a recent machine trans-
lation class (Lopez et al, 2013).8 Most manipula-
tives/interfaces targeted at NLP have been virtual,
but a notable exception is van Halteren (2002),
who created a (physical) board game for parsing.
In machine learning, there is a plethora of vir-
tual manipulatives demonstrating central concepts
such as decision boundaries and kernel methods.9
There are also several systems for teaching artifi-
cial intelligence: these tend to to involve control-
ling virtual robots10 or physical ones (Tokic and
Bou Ammar, 2012). Overall, manipulatives for
NLP and ML seem to be a successful pedagogi-
cal direction that we hope will continue.
Next, we present our main contribution, a vir-
tual manipulative that teaches log-linear models.
We ground the models in simple objects such as
circles and regular polygons, in order to appeal to
the students? physical intuitions. Later lessons can
move on from shapes, instead using words or im-
ages from a particular application of interest.
5 Our Log-Linear Virtual Manipulative
Figure 1 shows a screenshot of the tool,
available at http://cs.jhu.edu/?jason/
tutorials/loglin/. We encourage you to
play with it as you read.
5.1 Student Interface
Successive lessons introduce various challenges or
subleties. In each lesson, the user experiments
with modeling some given dataset D using some
given set of K features. Dataset: For each context
x, the outcomes y ? Y(x) are displayed as shapes,
images or words. Features: For each feature fi,
there is a slider to manipulate ?i.
Each shape y is sized proportionately to its
model probability p~?(y | x) (equation (1)), so it
grows or shrinks as the user changes ~?. In con-
trast, the empirical probability
p? (y | x) =
c(x, y)
c(x)
(= ratio of counts) (3)
is constant and is shown by a gray outline.
8github.com/mjpost/stack-decoder
9E.g., http://cs.cmu.edu/?ggordon/SVMs/
svm-applet.html.
10E.g., http://www-inst.eecs.berkeley.edu/
?cs188/pacman/pacman.html and http://www.
cs.rochester.edu/trac/quagents.
69
The size and color of y indicate how p~?(y | x)
compares to this empirical probability (Figure 2).
Reinforcing this, the observed count c(x, y) is
shown at the upper left of y, while the expected
count c(x) ? p~?(y | x) is shown at the upper right,
following the same color scheme (Figure 1).
We begin with globally normalized models
(only one context x). For example, the data in
Figure 1?30 solid circles, 15 striped circles, 10
solid triangles, and 5 striped triangles?are to be
modeled with the two indicator features fcircle and
fsolid. With ~? = 0 we have the uniform dis-
tribution, so the solid circle is contained in its
gray outline (p?(solid circle) > p~?(solid circle)),
the striped triangle contains its gray outline
(p?(striped triangle) < p~?(striped triangle)), and
the striped circle and gray outline are coincident
(p?(striped circle) = p~?(striped circle)).
A student can try various activities:
In the outcome matching activity, the goal is to
match the model p~? to p?. The game is to make all
of the outcomes match their corresponding gray
outlines in size (and color). The student ?wins?
once the maximum number of objects turn gray.
In the feature matching activity, the goal is to
match the expected feature vector Ep~? [
~f ] to the
observed feature vector Ep?[~f ]. In Figure 1, the
student would seek a model that correctly predicts
the total number of circles and the total number
of solid objects?even if the specific number of
solid circles is predicted wrong. (The predicted
and observed counts for a feature can easily be
found by adding up the displayed counts of indi-
vidual outcomes having that feature. For conve-
nience, they are also displayed in a tooltip on the
feature?s slider.) This game can always be won,
even if the given features are not adequately ex-
pressive to succeed at outcome matching on the
given dataset.
In the log-likelihood activity, the goal is to max-
imize the log-likelihood. The log-likelihood bar
(Figure 1) adapts to changes in ~?, just like the
shapes. The game is to make the bar as long as
possible.11 In later lessons, the student instead
tries to maximize a regularized version of the log-
likelihood bar, which is visibly shortened by a
penalty for large weights (to prevent overfitting).
Winning any of these games with more complex
models becomes difficult or at least tedious, so au-
11Once the gradient is introduced in a later lesson, knowing
when you have ?won? becomes clearer.
Quantity
of Interest
> 0 = 0 < 0
red gray blue
p?? p~?
Ep? [?]?
Ep~? [?]
15 30 60
?~?F
Figure 2: Color and area indicate differences be-
twen the empirical distribution (gray outline) and
model distribution. Red (or blue) indicates a
model probability or parameter that should be in-
creased (or decreased) to fit the data.
Figure 3: Gradient components use the same color
coding as given in Figure 2. The length of each
component indicates its potential effect on the ob-
jective. Note that the sliders use a nonlinear scale
from ?? to +?.
tomatic methods come as a relief. The student may
view hints on the sliders, showing which way each
slider should be nudged (Figure 3). These hints
correspond to components of the log-likelihood
gradient. Further automation is offered by the
?Step? button, which automatically nudges all pa-
rameters by taking a step of gradient ascent,12 and
even more by the ?Solve? button, which steps all
the way to the maximum.13
Our lessons guide the student to appreciate the
relationship among the three activities. First, fea-
ture matching is a weaker, attainable version of
outcome matching (when outcome matching is
12When `1 regularization is used, the optimal ~? often con-
tains many 0 weights, and a step is not permitted to jump
over a (possibly optimal) weight of 0. It stops at 0, though if
warranted, it can continue past 0 on the next step.
13The ?Solve? button adapts the stepsize at each step, using
a backtracking line search with the Armijo condition. This
ensures convergence.
70
possible it certainly achieves feature matching as
well). Second, feature matching is equivalent
to maximizing the (unregularized) log-likelihood.
Thus the mismatch is 0 iff the gradient of log-
likelihood is 0. In fact, the mismatch equals the
gradient even when they are not 0! Thus, drag-
ging the sliders in the direction of the gradient
hints can be viewed as a correct strategy for either
the feature matching game or the log-likelihood
game. This connection shows that the current gra-
dient of log-likelihood can easily be computed by
summing up the observed and currently predicted
counts of each feature. After understanding this
and playing with the ?Step? and ?Solve? buttons,
the student should be able to imagine writing code
to train log-linear models.
5.2 Guided Exploration
We expect students to ?learn by playing.? The user
can experiment at any time with the sliders, with
gradient ascent and its stepsize, with the type and
strength of regularization, and with the size of the
dataset. The user can also sample new data or new
parameters, and can peek at the true parameters.
These options are described further in Section 7.
We encourage experimentation by providing
tooltips that appear whenever a student hovers the
mouse pointer over a element of the GUI. Tooltips
provide guidance about whatever the student is
looking at right then. Some are static explanations
(e.g., what does this gray bar represent?). Others
dynamically update with changes to the parame-
ters (e.g., the tooltips on the feature sliders show
the observed and expected counts of that feature).
Students see the tooltips repeatedly, which can
help them absorb and reinforce concepts over an
extended period of time. Students who like to
learn by browsing and experimenting can point to
various tooltips and get a sense of how the differ-
ent concepts fit together. Some tooltips explicitly
refer to one another, linking GUI elements such as
the training objective, the regularization choices,
and the gradient.
Though the user is welcome to play, we also
provide some guidance. Each lesson displays in-
structions that explain the current dataset, jus-
tify modeling choices, introduce new functional-
ity, lead the user through a few activities, and ask
lesson-specific questions. The first lesson also
links to a handout with a more formal textbook-
style treatment. The last lesson links to further
Figure 4: Inventory of available shapes
(circle/triangle/square/pentagon) and fills
(solid/striped/hollow). Text and arbitrary im-
ages may be used instead of shapes. Color and
size are reserved to indicate how the current
model?s predictions of outcome counts or feature
counts compare to the empirical values?see
Figure 2.
reading and exercises.
5.3 Instructor Interface: Creating and
Tailoring Lessons
An instructor may optionally wish to tailor lessons
to his or her students? needs, interests, and abil-
ities. Shapes provide a nice introduction to log-
linear models, but eventually NLP students will
want to think about NLP problems, whereas vi-
sion students will want to think about vision prob-
lems. Thus, we have designed the manipulative to
handle text and arbitrary images, as well as the 12
shape-fill combinations shown in Figure 4.
Tailoring lessons to the students? needs is as
simple as editing a couple of text files. These must
specify (1) a set of features, (2) a set of contexts,14
and (3) for each context, a set of featurized events,
including counts and visual positions. This simple
format allows one to describe some rather involved
models. Some of the features may be ?hidden?
from the student, thereby allowing the student to
experience model mismatch. Note that the visual
positioning information is pedagogically impor-
tant: aligning objects by orthogonal descriptions
can make feature contrasts stand out more, e.g.,
circles vs. triangles or solid vs. striped.
The configuration files can turn off certain fea-
tures on a per-lesson basis (without program-
14The set of contexts may be omitted when there is only
one context (i.e., an unconditioned model).
71
ming). This is useful for, e.g., hiding the ?Solve?
button in early lessons, adding new tooltips, or
specializing the existing tooltips on a per-lesson
basis.
However, being a manipulative rather than a
tutoring system, our software does not monitor
the user?s progress through a lesson and provide
guidance via lesson-specific hints, warnings, ques-
tions, or feedback. (The software is open-source,
so others are free to extend it in this way.)
5.4 Back-End Implementation
Anyone can use our virtual manipulative sim-
ply by visiting its website. There is no start-up
cost. Aside from reading the data, model and in-
structions from the web server, it is fully client-
side. The Javascript back-end uses common and
well-supported open-source libraries that provide
a consistent experience across browsers.15 The
manipulative relies on certain capabilities from the
HTML5 standard. Not all browsers in current
use support these capabilities, notably Internet Ex-
plorer 9 and under. The tool works with recent
versions of Firefox, Chrome and Safari.
6 Pedagogical Aims
6.1 Modeling and Estimation
When faced with a dataset D of (x, y) pairs, one
often hopes to choose an appropriate model.
When are log-linear models appropriate? Why
does their hypothesis space include the uniform
distribution? For what feature sets does it include
every distribution?
One should also understand statistical estima-
tion. How do the features interact? When esti-
mating their weights, can raising one weight alter
or reverse the desired changes to other weights?
How can parameter estimation go wrong statis-
tically (overfitting, perhaps driving parameters to
??)? What might happen if we have a very
large feature set? Can we design regularized es-
timators that prevent overfitting (the bias-variance
tradeoff)? What is the effect of the regularization
constant on small and large datasets? On rare and
frequent contexts? On rare and frequent features?
On useful features (including features that always
or never fire) and useless ones?
15Specifically and in order, d3 (d3js.org/), jQuery
(jquery.com/), jQuery UI (jqueryui.com),
jQuery Tools (jquerytools.org/), and qTip
(craigsworks.com/projects/qtip/).
Finally, one is responsible for feature design.
Which features usefully distinguish among the
events? How do non-binary features work and
when are they appropriate? When can a feature
safely be omitted because it provides no additional
modeling power? How does the choice of features
affect generalization, particularly if the objective
is regularized? In particular, how do shared fea-
tures and backoff features allow a model to gen-
eralize to novel contexts and outcomes (or rare
ones)? How do the resulting patterns of general-
ization relate qualitatively to traditional smoothing
techniques in NLP (Chen and Goodman, 1996)?
6.2 Training Algorithm
We also aim to convey intuitions about a specific
training algorithm. We use the regularized condi-
tional log-likelihood (2) to define the goodness of
a parameter vector ~?. The best choice is then the ~?
that solves equation (4):
0 = ?~?F = Ep?
[
~f(X,Y )
]
? Ep~?
[
~f(X,Y )
]
? C?~?R(
~?)
(4)
where because our model is conditional, p~?(x, y)
denotes the hybrid distribution p?(x) ? p~?(y | x).
Many important concepts are visible in (2) and
(4). As discussed earlier, (4) includes the dif-
ference between observed and expected feature
counts,
Ep?
[
~f(X,Y )
]
? Ep~?
[
~f(X,Y )
]
. (5)
Students must internalize this concept and the
meaning of the two counts above. This prepares
them to understand the extension to structured pre-
diction, where these counts can be more diffi-
cult to compute (see Section 8). It also prepares
them to generalize to training latent-variable mod-
els (Petrov and Klein, 2008). In that setting, the
observed count can no longer be observed but is
replaced by another expectation under the model,
conditioned on the partial training data.
(4) also includes a weight decay term for regu-
larization. We allow both `1 and `2 regularization:
R(~?) = ?~??1 versus R(~?) = ?~??22. One can see
experimentally that strong `1 regularization tries
to use a few larger weights and leave the rest at
0, while strong `2 regularization tries to share the
work among many smaller weights. One can ob-
serve how for a given C, the regularization term is
72
more important for small datasets, since for larger
datasets it is dominated by the log-likelihood.
Once one can compute the gradient, one can
?follow? it along the surface, in a way that is guar-
anteed to increase the convex objective function up
to its global maximum. The ?Solve? button does
this and indeed one can watch the log-likelihood
bar continually increase. Yet one should observe
what might go wrong here as well. Gradient ascent
can oscillate if a fixed stepsize is used (by click-
ing ?Step? repeatedly). One may also notice that
?Solve? is somewhat slow to converge on some
problems, which motivates considering alternative
optimization algorithms (Malouf, 2002).
We should note that we are not concerned
with efficiency issues, e.g., tractably computing
the normalizers Z(x). Efficient normalization is
a crucial practical ingredient in using log-linear
models, but our primary concern is to impart a
near-physical intuitive understanding of the mod-
els themselves. See Section 8 or Smith (2011) for
strategies on computing the normalizer.
7 Provided Lessons
In this section we provide an overview of the 18
currently available lessons. (Of course, you can
work through the lessons yourself for further de-
tails.) ?Core? lessons that build intuition precede
the ?applied? lessons focused on NLP tasks or
problems. Instructors should feel especially free
to replace or reorder the ?applied? lessons.
Core lessons 1?5 provide a basic introduction
to log-linear modeling, using unconditioned distri-
butions over only four shapes as shown in Figure
1. We begin by matching outcomes using just ?cir-
cle? and ?solid? features. We discover in lesson 2
that it is redundant to add ?triangle? and ?striped?
features. In lesson 3 we encounter a dataset which
these features cannot fit, because the shape and
fill attributes are not statistically independent. We
remedy this in lesson 4 with a conjunctive ?striped
triangle? feature.
Because outcome matching fails in lesson 3,
lessons 3?4 introduce feature matching and log-
likelihood as suitable alternatives. Lesson 5 briefly
illustrates a non-binary feature function, ?number
of sides? (taking values 3, 4, and 5 on triangles,
squares, and pentagons). This clarifies the match-
ing of feature counts: here we are trying to predict
the total number of sides in the dataset.
Lessons 6?8 focus on optimization. They move
up to the harder setting of 9 shapes with 6 fea-
tures, so we tell students how to turn on the gra-
dient ?hints? on the sliders. We explain how these
hints relate to feature matching and log-likelihood.
We invite the students to try using the hints on
earlier lessons?and on new random datasets that
they can generate by clicking. In Lesson 7, we
introduce the ?Step? and ?Solve? buttons to help
even more with a difficult dataset. Students use all
these GUI elements to climb the convex objective
and increase the log-likelihood bar.
At this point we introduce regularization. Les-
son 6 invited students to generate small random
datasets and observe their high variance and the
tendency to overfit them. Lesson 8 gives a more
dramatic illustration of overfitting: with no ob-
served pentagons, the solver sends ?pentagon ?
?? to make p~?(pentagon) ? 0. We prevent this
by adding a regularization penalty, which reserves
some probability for pentagons. Striped pentagons
turn out to be the least likely pentagons, because
stripes were observed to be uncommon on other
shapes (so ?striped < 0). Thus we see that our
choice of features allows this ?smoothing method?
to make useful generalizations about novel out-
comes.
Lessons 9?10 consider the effect of `1 versus
`2 regularization, and the competition between the
regularizer (scaled by the constant C) and the log-
likelihood (scaled by the dataset size N ).16
Lessons 11?13 introduce conditional models,
showing how features are shared among three con-
texts. The third context is unobserved, yet our
trained model makes plausible predictions about
it. The conditional probabilities of unobserved
shapes are positive even without regularization, in
contrast to the joint probabilities in lesson 9.
We see that a frequent context x generally has
more influence on the parameters. But this need
not be true if the parameters do not help to distin-
guish among the particular outcomes Y(x).
Lessons 14?15 explore feature design in condi-
tional models. We model conditional probabilities
of the form p(fill | shape). ?Unigram? features
can favor certain fills y regardless of the shape.
?Bigram? features that look at y and x together
can favor different fills for each shape type. We
see that features that depend only on the shape x
cannot distinguish among fills y, and so have no
16Clever students may think to try setting C < 0, which
breaks convexity of the objective function.
73
effect on the conditional probabilities p(y | x).
Lesson 15 illustrates how regularization pro-
motes generalization and feature selection. Once
we have a full set of bigram features, the uni-
gram features are redundant. We never have to
put a high weight on ?solid?: we can accomplish
the same thing by putting high weights on ?solid
triangle? and ?solid circle? separately. Yet this
misses a generalization because it does not pre-
dict that ?solid? is also likely for pentagons. For-
tunately, regularization encourages us to avoid too
many high weights. So we prefer to put a single
high weight on ?solid,? and use the ?solid triangle?
and ?solid circle? features only to model smaller
shape-specific deviations from that generalization.
As a result, we will indeed extrapolate that pen-
tagons tend to be solid as well.
Lesson 16 begins the application-driven
lessons:
One lesson builds on the ?unigram? and ?bi-
gram? concepts to create a ?bigram language
model??a model of shape sequences over a vo-
cabulary of 9 shapes. A shape?s probability de-
pends not only on its attributes but also on the
attributes that it shares with the previous shape.
What is the probability of a striped square given
that the previous shape was also striped, or a
square, or a striped square?
We also apply log-linear modeling to the task
of text categorization (spam detection). We chal-
lenge the students to puzzle out how this model
is set up and how to generalize it to three-way
categorization. Our contexts in this case are
documents?actually very short phrases. Most
contexts are seen only once, with an outcome of ei-
ther ?mail? or ?spam.? Our feature set implements
logistic regression (footnote 1): each feature con-
joins y = spam with some property of the text
x, such as ?contains ?parents?,? ?has boldface,? or
?mentions money.?
Additional linguistic application lessons may be
added in the near future?e.g., modeling the rela-
tive probability of grammar rules or parse trees.
The final lesson summarizes what has been
learned, mentions connections to other ideas in
machine learning, and points the student to further
resources.
8 Graduating to Real Applications
At the time of writing, 3266 papers in the ACL
Anthology mention log-linear models, with 137
using ?log-linear,? ?maximum entropy? or ?max-
ent? in the paper title. These cover a wide range of
applications that can be considered in lectures or
homework projects.
Early papers may cover the most fundamen-
tal applications and the clearest motivation. Con-
ditional log-linear models were first popularized
in computational linguistics by a group of re-
searchers associated with the IBM speech and lan-
guage group, who called them ?maximum entropy
models,? after a principle that can be used to mo-
tivate their form (Jaynes, 1957). They applied the
method to various binary or multiclass classifica-
tion problems in NLP, such as prepositional phrase
attachment (Ratnaparkhi et al, 1994), text catego-
rization (Nigam et al, 1999), and boundary pre-
diction (Beeferman et al, 1999).
Log-linear models can be also used for struc-
tured prediction problems in NLP such as tagging,
parsing, chunking, segmentation, and language
modeling. A simple strategy is to reduce struc-
tured prediction to a sequence of multiclass pre-
dictions, which can be individually made with a
conditional log-linear model (Ratnaparkhi, 1998).
A more fully probabilistic approach?used in the
original ?maximum entropy? papers?is to use (1)
to define the conditional probabilities of the steps
in a generative process that gradually produces the
structure (Rosenfeld, 1994; Berger et al, 1996).17
This idea remains popular today and can be used
to embed rich distributions into a variety of gener-
ative models (Berg-Kirkpatrick et al, 2010). For
example, a PCFG that uses richly annotated non-
terminals involves a large number of context-free
rules. Rather than estimating their probabilities
separately, or with traditional backoff smoothing,
a better approach is to use (1) to model the proba-
bility of all rules given their left-hand sides, based
on features that consider attributes of the nonter-
minals.18
The most direct approach to structured predic-
tion is to simply predict the structured output all
at once, so that y is a large structured object with
many features. This is conceptually natural but
means that the normalizer Z(x) involves sum-
ming over a large space Y(x) (footnote 3). One
17Even predicting the single next word in a sentence can be
broken down into a sequence of binary decisions in this way.
This avoids normalizing over the large vocabulary (Mnih and
Hinton, 2008).
18E.g., case, number, gender, tense, aspect, mood, lexical
head. In the case of a terminal rule, the spelling or morphol-
ogy of the terminal symbol can be considered.
74
can restrict Y(x) before training (Johnson et al,
1999). More common is to sum efficiently by
dynamic programming or sampling, as is typical
in linear-chain conditional random fields (Lafferty
et al, 2001), whole-sentence language modeling
(Rosenfeld et al, 2001), and CRF CFGs (Finkel
et al, 2008). This topic is properly deferred until
such algorithmic techniques are introduced later in
an NLP class, for example in a unit on parsing (see
discussion in section 2). We prepare students for
it by mentioning this point in our final lesson.19
Our final lesson also leads to a web page where
we link to log-linear software and to various
pencil-and-paper problems, homework projects,
and readings that an instructor may consider as-
signing. We welcome suggested additions to this
page.
9 Conclusion
We have introduced an open-source, web-based
virtual manipulative for log-linear models. In-
cluded with the code are 18 lessons peppered with
questions, a handout that gives a formal treatment
of the necessary derivations, and auxiliary infor-
mation including further reading, practice prob-
lems, and recommended software. A version is
available at http://cs.jhu.edu/?jason/
tutorials/loglin/.
Acknowledgements We would like to thank the
anonymous reviewers for their helpful feedback
and suggestions and the entire Fall 2012 Natural
Language Processing course at Johns Hopkins.
References
Doug Beeferman, Adam Berger, and John Lafferty.
1999. Statistical models for text segmentation. Ma-
chine Learning, 34(1?3):177?210.
Taylor Berg-Kirkpatrick, Alexandre Bouchard-Co?te?,
DeNero, John DeNero, and Dan Klein. 2010. Pain-
less unsupervised learning with features. In Pro-
ceedings of NAACL, June.
Adam L. Berger, Stephen A. Della Pietra, and Vin-
cent J. Della Pietra. 1996. A maximum-entropy
approach to natural language processing. Compu-
tational Linguistics, 22(1):39?71.
19This material can also be connected to other topics in
machine learning. Dynamic programming and sampling are
also used for exact or approximate computation of normal-
izers in undirected graphical models (Markov random fields
or conditional random fields), which are really just log-linear
models for structured prediction of tuples.
Steven Bird, Ewan Klein, and Edward Loper.
2009. Natural Language Processing with Python.
O?Reilly Media.
Kira J. Carbonneau, Scott C. Marley, and James P.
Selig. 2013. A meta-analysis of the efficacy of
teaching mathematics with concrete manipulatives.
Journal of Educational Psychology, 105(2):380 ?
400.
Stanley Chen and Joshua Goodman. 1996. An empir-
ical study of smoothing techniques. In Proceedings
of ACL.
Douglas H. Clements and Sue McMillen. 1996. Re-
thinking ?concrete? manipulatives. Teaching Chil-
dren Mathematics, 2(5):pp. 270?279.
Hal Daume? III. 2004. Notes on CG and
LM-BFGS optimization of logistic regression.
Paper available at http://pub.hal3.name#
daume04cg-bfgs, implementation available at
http://hal3.name/megam/, August.
Jason Eisner. 2002. An interactive spreadsheet
for teaching the forward-backward algorithm. In
Dragomir Radev and Chris Brew, editors, Proceed-
ings of the ACL Workshop on Effective Tools and
Methodologies for Teaching NLP and CL, pages 10?
18, Philadelphia, July.
Jenny Rose Finkel, Alex Kleeman, and Christopher D
Manning. 2008. Efficient, feature-based, condi-
tional random field parsing. Proceedings of ACL-
08: HLT, pages 959?967.
E. T. Jaynes. 1957. Information theory and statistical
mechanics. Physics Reviews, 106:620?630.
Mark Johnson, Stuart Geman, Stephen Canon, Zhiyi
Chi, and Stefan Riezler. 1999. Estimators for
stochastic ?unification-based? grammars. In Pro-
ceedings of ACL.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of ICML.
Marc Light, Robert Arens, and Xin Lu. 2005. Web-
based interfaces for natural language processing
tools. In Proceedings of the Second ACL Workshop
on Effective Tools and Methodologies for Teaching
NLP and CL, pages 28?31, Ann Arbor, Michigan,
June. Association for Computational Linguistics.
Adam Lopez, Matt Post, Chris Callison-Burch,
Jonathan Weese, Juri Ganitkevitch, Narges Ah-
midi, Olivia Buzek, Leah Hanson, Beenish Jamil,
Matthias Lee, et al 2013. Learning to translate with
products of novices: Teaching MT with competi-
tive challenge problems. Transactions of the ACL
(TACL), 1.
Robert Malouf. 2002. A comparison of algorithms
for maximum entropy parameter estimation. In Pro-
ceedings of CoNLL, pages 49?55.
75
Andriy Mnih and Geoffrey Hinton. 2008. A scalable
hierarchical distributed language model. In Pro-
ceedings of NIPS.
Patricia S. Moyer, Johnna J. Bolyard, and Mark A.
Spikell. 2002. What are virtual manipulatives?
Teaching Children Mathematics, 8(6):372?377.
Kamal Nigam, John Lafferty, and Andrew McCallum.
1999. Using maximum entropy for text classifica-
tion. In IJCAI-99 Workshop on Machine Learning
for Information Filtering, pages 61?67.
Jan Nouza, Miroslav Holada, and Daniel Hajek. 1997.
An educational and experimental workbench for vi-
sual processing of speech data. In Fifth European
Conference on Speech Communication and Technol-
ogy.
Slav Petrov and Dan Klein. 2008. Sparse multi-scale
grammars for discriminative latent variable parsing.
In Proceedings of EMNLP, pages 867?876, October.
Adwait Ratnaparkhi, Jeff Reynar, and Salim Roukos.
1994. A maximum entropy model for prepositional
phrase attachment. In Proceedings of the ARPA Hu-
man Language Technology Workshop, pages 250?
255.
Adwait Ratnaparkhi. 1998. Maximum Entropy Models
for Natural Language Ambiguity Resolution. Ph.D.
thesis, University of Pennsylvania, July.
Ronald Rosenfeld, Stanley F. Chen, and Xiaojin Zhu.
2001. Whole-sentence exponential language mod-
els: A vehicle for linguistic-statistical integration.
Computer Speech and Language, 15(1).
Ronald Rosenfeld. 1994. Adaptive Statistical Lan-
guage Modeling: A Maximum Entropy Approach.
Ph.D. thesis, Carnegie Mellon University.
Noah A. Smith. 2011. Linguistic Structure Prediction.
Synthesis Lectures on Human Language Technolo-
gies. Morgan and Claypool, May.
Evelyn J Sowell. 1989. Effects of manipulative materi-
als in mathematics instruction. Journal for Research
in Mathematics Education, pages 498?505.
Michel Tokic and Haitham Bou Ammar. 2012. Teach-
ing reinforcement learning using a physical robot.
In Proceedings of the ICML Workshop on Teaching
Machine Learning.
Hans van Halteren. 2002. Teaching nlp/cl through
games: The case of parsing. In Proceedings of
the ACL Workshop on Effective Tools and Method-
ologies for Teaching NLP and CL, pages 1?9,
Philadelphia, Pennsylvania, USA, July. Association
for Computational Linguistics.
76
