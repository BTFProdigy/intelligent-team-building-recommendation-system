Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 882?889,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Combining Statistical and Knowledge-based Spoken Language 
Understanding in Conditional Models 
Ye-Yi Wang, Alex Acero, Milind Mahajan 
Microsoft Research 
One Microsoft Way 
Redmond, WA 98052, USA 
{yeyiwang,alexac,milindm}@microsoft.com
John Lee 
Spoken Language Systems 
MIT CSAIL  
Cambridge, MA 02139, USA 
jsylee@csail.mit.edu 
 
Abstract 
Spoken Language Understanding (SLU) 
addresses the problem of extracting semantic 
meaning conveyed in an utterance. The 
traditional knowledge-based approach to this 
problem is very expensive -- it requires joint 
expertise in natural language processing and 
speech recognition, and best practices in 
language engineering for every new domain. 
On the other hand, a statistical learning 
approach needs a large amount of annotated 
data for model training, which is seldom 
available in practical applications outside of 
large research labs. A generative HMM/CFG 
composite model, which integrates easy-to-
obtain domain knowledge into a data-driven 
statistical learning framework, has previously 
been introduced to reduce data requirement. 
The major contribution of this paper is the 
investigation of integrating prior knowledge 
and statistical learning in a conditional model 
framework. We also study and compare  
conditional random fields (CRFs) with 
perceptron learning for SLU. Experimental 
results show that the conditional models 
achieve more than 20% relative reduction in 
slot error rate over the HMM/CFG model, 
which had already achieved an SLU accuracy 
at the same level as the best results reported 
on the ATIS data. 
1 Introduction 
Spoken Language Understanding (SLU) 
addresses the problem of extracting meaning 
conveyed in an utterance. Traditionally, the 
problem is solved with a knowledge-based 
approach, which requires joint expertise in 
natural language processing and speech 
recognition, and best practices in language 
engineering for every new domain. In the past 
decade many statistical learning approaches have 
been proposed, most of which exploit generative 
models, as surveyed in (Wang, Deng et al, 
2005). While the data-driven approach addresses 
the difficulties in knowledge engineering, it 
requires a large amount of labeled data for model 
training, which is seldom available in practical 
applications outside of large research labs. To 
alleviate the problem, a generative HMM/CFG 
composite model has previously been introduced 
(Wang, Deng et al, 2005). It integrates a 
knowledge-based approach into a statistical 
learning framework, utilizing prior knowledge to 
compensate for the dearth of training data. In the 
ATIS evaluation (Price, 1990), this model 
achieves the same level of understanding 
accuracy (5.3% error rate on standard ATIS 
evaluation) as the best system (5.5% error rate), 
which is a semantic parsing system based on a 
manually developed grammar. 
Discriminative training has been widely used 
for acoustic modeling in speech recognition 
(Bahl, Brown et al, 1986; Juang, Chou et al, 
1997; Povey and Woodland, 2002). Most of the 
methods use the same generative model 
framework, exploit the same features, and apply 
discriminative training for parameter 
optimization. Along the same lines, we have 
recently exploited conditional models by directly 
porting the HMM/CFG model to Hidden 
Conditional Random Fields (HCRFs) 
(Gunawardana, Mahajan et al, 2005), but failed 
to obtain any improvement. This is mainly due to 
the vast parameter space, with the parameters 
settling at local optima. We then simplified the 
original model structure by removing the hidden 
variables, and introduced a number of important 
overlapping and non-homogeneous features. The 
resulting Conditional Random Fields (CRFs) 
(Lafferty, McCallum et al, 2001) yielded a 21% 
relative improvement in SLU accuracy. We also 
applied a much simpler perceptron learning 
algorithm on the conditional model and observed 
improved SLU accuracy as well.  
In this paper, we will first introduce the 
generative HMM/CFG composite model, then 
discuss the problem of directly porting the model 
to HCRFs, and finally introduce the CRFs and 
882
the features that obtain the best SLU result on 
ATIS test data. We compare the CRF and 
perceptron training performances on the task. 
2 Generative Models 
The HMM/CFG composite model (Wang, Deng 
et al, 2005) adopts a pattern recognition 
approach to SLU. Given a word sequence W , an 
SLU component needs to find the semantic 
representation of the meaning M  that has the 
maximum a posteriori probability ( )Pr |M W :   
 
( )
( ) ( )
? arg max Pr |
arg max Pr | Pr
M
M
M M W
W M M
=
= ?  
The composite model integrates domain 
knowledge by setting the topology of the prior 
model, ( )Pr ,M according to the domain 
semantics; and by using PCFG rules as part of 
the lexicalization model ( )Pr |W M . 
The domain semantics define an application?s 
semantic structure with semantic frames. 
Figure 1 shows a simplified example of three 
semantic frames in the ATIS domain. The two 
frames with the ?toplevel? attribute are also 
known as commands. The ?filler? attribute of a 
slot specifies the semantic object that can fill it. 
Each slot may be associated with a CFG rule, 
and the filler semantic object must be 
instantiated by a word string that is covered by 
that rule. For example, the string ?Seattle? is 
covered by the ?City? rule in a CFG. It can 
therefore fill the ACity (ArrivalCity) or the 
DCity (DepartureCity) slot, and instantiate a 
Flight frame.  This frame can then fill the Flight 
slot of a ShowFlight frame. Figure 2 shows a 
semantic representation according to these 
frames.   
 
< frame name=?ShowFlight? toplevel=?1?>   
        <slot name=?Flight? filler=?Flight?/>   
< /frame>   
< frame name=?GroundTrans? toplevel=?1?>   
       < slot name=?City? filler=?City?/>   
< /frame>   
< frame name=?Flight?>   
        <slot name=?DCity? filler=?City?/>   
       < slot name=?ACity? filler=?City?/>   
< /frame>   
Figure 1. Simplified domain semantics for the ATIS 
domain.  
The semantic prior model comprises the 
HMM topology and state transition probabilities. 
The topology is determined by the domain 
semantics, and the transition probabilities can be 
estimated from training data. Figure 3 shows the 
topology of the underlying states in the statistical 
model for the semantic frames in Figure 1. On 
top is the transition network for the two top-level 
commands. At the bottom is a zoomed-in view 
for the ?Flight? sub-network. State 1 and state 4 
are called precommands. State 3 and state 6 are 
called postcommands. States 2, 5, 8 and 9 
represent slots. A slot is actually a three-state 
sequence ? the slot state is preceded by a 
preamble state and followed by a postamble 
state, both represented by black circles. They 
provide contextual clues for the slot?s identity. 
<ShowFlight>   
      < Flight>   
          < DCity filler=?City?>Seattle< /DCity>   
          <ACity filler=?City?>Boston< /ACity>   
      < /Flight>   
< /ShowFlight>   
Figure 2. The semantic representation for ?Show me 
the flights departing from Seattle arriving at Boston? 
is an instantiation of the semantic frames in Figure 1. 
 
 
Figure 3. The HMM/CFG model?s state topology, as 
determined by the semantic frames in Figure 1.  
The lexicalization model, ( )Pr |W M , depicts 
the process of sentence generation from the 
topology by estimating the distribution of words 
emitted by a state. It uses state-dependent n-
grams to model the precommands, 
postcommands, preambles and postambles, and 
uses knowledge-based CFG rules to model the 
slot fillers. These rules help compensate for the 
dearth of domain-specific data.  In the remainder 
of this paper we will say a string is ?covered by a 
CFG non-terminal (NT)?, or equivalently, is 
?CFG-covered for s? if the string can be parsed 
by the CFG rule corresponding to the slot s.  
 
Given the semantic representation in Figure 2, 
the state sequence through the model topology in 
883
Figure 3 is deterministic, as shown in Figure 4. 
However, the words are not aligned to the states 
in the shaded boxes. The parameters in their 
corresponding n-gram models can be estimated 
with an EM algorithm that treats the alignments 
as hidden variables. 
 
 
Figure 4. Word/state alignments. The segmentation 
of the word sequences in the shaded region is hidden. 
The HMM/CFG composite model was 
evaluated in the ATIS domain (Price, 1990). The 
model was trained with ATIS3 category A 
training data (~1700 annotated sentences) and 
tested with the 1993 ATIS3 category A test 
sentences (470 sentences with 1702 reference 
slots).  The slot insertion-deletion-substitution 
error rate (SER) of the test set is 5.0%, leading to 
a 5.3% semantic error rate in the standard end-to-
end ATIS evaluation, which is slightly better 
than the best manually developed system (5.5%). 
Moreover, a steep drop in the error rate is 
observed after training with only the first two 
hundred sentences.  This demonstrates that the 
inclusion of prior knowledge in the statistical 
model helps alleviate the data sparseness 
problem. 
3 Conditional Models 
We investigated the application of conditional 
models to SLU. The problem is formulated as 
assigning a label l  to each element in an 
observation .o  Here, o  consists of a word 
sequence 1o
?  and a list of CFG non-terminals 
(NT) that cover its subsequences, as illustrated in  
Figure 5. The task is to label ?two? as the ?Num-
of-tickets? slot of the ?ShowFlight? command, 
and ?Washington D.C.? as the ArrivalCity slot 
for the same command. To do so, the model must 
be able to resolve several kinds of ambiguities: 
 
1. Filler/non-filler ambiguity, e.g., ?two? can 
either fill a Num-of-tickets slot, or its 
homonym ?to? can form part of the preamble 
of an ArrivalCity slot. 
2. CFG ambiguity, e.g., ?Washington? can be 
CFG-covered as either City or State. 
3. Segmentation ambiguity, e.g., [Washington] 
[D.C.] vs. [Washington D.C.]. 
4. Semantic label ambiguity, e.g., ?Washington 
D.C.? can fill either an ArrivalCity or 
DepartureCity slot. 
 
Figure 5. The observation includes a word sequence 
and the subsequences covered by CFG non-terminals.  
3.1 CRFs and HCRFs 
Conditional Random Fields (CRFs) (Lafferty, 
McCallum et al, 2001) are undirected 
conditional graphical models that assign the 
conditional probability of a state (label) sequence 
1s
?  with respect to a vector of features 1 1( )f os
? ?, . 
They are of the following form: 
( )1 11( ) exp ( )( )o f oop s sz? ?? ??| ; = ? , .;  (1) 
Here ( )
1
1( ) exp ( )
s
z s
?
?? ?; = ? ,?o f o  normalizes 
the distribution over all possible state sequences. 
The parameter vector ?  is trained conditionally 
(discriminatively). If we assume that 1s
?  is a 
Markov chain given o  and the feature functions 
only depend on two adjacent states, then  
1
( 1) ( )
1
( )
1   = exp ( )
( )
t t
k k
k t
p s
f s s t
z
?
?
?
??
?
=
| ;
? ?, , ,? ?; ? ?? ?
o
o
o
 (2) 
In some cases, it may be natural to exploit 
features on variables that are not directly 
observed. For example, a feature for the Flight 
preamble may be defined in terms of an observed 
word and an unobserved state in the shaded 
region in Figure 4: 
( 1) ( )
FlightInit,flights
( )
( )
1 if =FlightInit  = flights;
    =
0 otherwise                                 
o
o
t t
t t
f s s t
s
? , , ,
? ???
 (3) 
In this case, the state sequence 1s
?  is only 
partially observed in the meaning representation 
5 8: ( ) "DCity" ( ) "ACity"M M s M s= ? = for the 
words ?Seattle? and ?Boston?. The states for the 
remaining words are hidden. Let ( )M?  represent 
the set of all state sequences that satisfy the 
constraints imposed by .M  To obtain the 
conditional probability of ,M we need to sum 
over all possible labels for the hidden states: 
884
 1
( 1) ( )
1( )
( )
1   exp ( )
( )
t t
k k
k ts M
p M
f s s t
z ?
?
?
??
?
=??
| ; =
? ?, , ,? ?; ? ?? ? ?
o
o
o
 
CRFs with features dependent on hidden state 
variables are called Hidden Conditional Random 
Fields (HCRFs). They have been applied to tasks 
such as phonetic classification (Gunawardana, 
Mahajan et al, 2005) and object recognition 
(Quattoni, Collins et al, 2004). 
3.2 Conditional Model Training 
We train CRFs and HCRFs with gradient-based 
optimization algorithms that maximize the log 
posterior. The gradient of the objective function 
is  
( ) ( )
( ) ( )
1
1
1
1
( ) ( )
( )
P P s
P P s
L s
s
?
?
?
?
?
? ?
?
, | ,
|
? ?? = , ;? ?
? ?? , ;? ?
l o l o
o o
E f o
             E f o


 
which is the difference between the conditional 
expectation of the feature vector given the 
observation sequence and label sequence, and the 
conditional expectation given the observation 
sequence alone. With the Markov assumption in 
Eq. (2), these expectations can be computed 
using a forward-backward-like dynamic 
programming algorithm. For CRFs, whose 
features do not depend on hidden state 
sequences, the first expectation is simply the 
feature counts given the observation and label 
sequences. In this work, we applied stochastic 
gradient descent (SGD) (Kushner and Yin, 1997) 
for parameter optimization. In our experiments 
on several different tasks, it is faster than L-
BFGS (Nocedal and Wright, 1999), a quasi-
Newton optimization algorithm. 
3.3 CRFs and Perceptron Learning 
Perceptron training for conditional models 
(Collins, 2002) is an approximation to the SGD 
algorithm, using feature counts from the Viterbi 
label sequence in lieu of expected feature counts. 
It eliminates the need of a forward-backward 
algorithm to collect the expected counts, hence 
greatly speeds up model training.  This algorithm 
can be viewed as using the minimum margin of a 
training example (i.e., the difference in the log 
conditional probability of the reference label 
sequence and the Viterbi label sequence) as the 
objective function instead of the conditional 
probability: 
( ) ( ) ( )
l
l o l o
'
' log | ; max log ' | ;L P P? ? ?= ?  
Here again, o  is the observation and l  is its 
reference label sequence. In perceptron training, 
the parameter updating stops when the Viterbi 
label sequence is the same as the reference label 
sequence. In contrast, the optimization based on 
the log posterior probability objective function 
keeps pulling probability mass from all incorrect 
label sequences to the reference label sequence 
until convergence. 
In both perceptron and CRF training, we 
average the parameters over training iterations 
(Collins, 2002). 
4 Porting HMM/CFG Model to HCRFs 
In our first experiment, we would like to exploit 
the discriminative training capability of a 
conditional model without changing  the 
HMM/CFG model?s topology and feature set.  
Since the state sequence is only partially labeled, 
an HCRF is used to model the conditional 
distribution of the labels. 
4.1 Features 
We used the same state topology and features as 
those in the HMM/CFG composite model.  The 
following indicator features are included: 
Command prior features capture the a priori 
likelihood of different top-level commands:  
 
( 1) ( )
( )
( )
1 if =0 C( )
    = , CommandSet
0 otherwise              
oPR t t
t
cf s s t
t s c
c
? , , ,
? ? = ? ???
 
Here C(s) stands for the name of the command 
that corresponds to the transition network 
containing state s. 
State Transition features capture the likelihood 
of transition from one state to another: 
( 1) ( )
( 1) ( ) 1 2
1 2
,1 2
1 if 
( ) ,   
0 otherwise              
where  is a legal transition according to the 
state topology.
o
t t
TR t t
s s
s s s s
f s s t
s s
?
? ? = , =, , , = ??
?
 
Unigram and Bigram features capture the 
likelihoods of words emitted by a state: 
885
( )
( 1) ( )
1
( 1) ( )
1
( 1) ( ) 1
1 2
,
, ,1 2
1 if 
( ) ,
0 otherwise              
( )
1 if 
     = ,
0 otherwise                                                
 
o
o
o
o o
t t
UG t t
BG t t
t t t t
s w
s w w
s s w
f s s t
f s s t
s s s s w w
?
?
?
?
? ?
? = ? =, , , = ??
, , ,
? = ? = ? = ? =??
( ) 1 2    | isFiller ; , TrainingDatas s w w w? ? ? ?
 
The condition 1isFiller( )s  restricts 1s  to be a slot 
state and not a pre- or postamble state. 
4.2 Experiments 
The model is trained with SGD with the 
parameters initialized in two ways. The flat start 
initialization sets all parameters to 0. The 
generative model initialization uses the 
parameters trained by the HMM/CFG model. 
Figure 6 shows the test set slot error rates 
(SER) at different training iterations. With the 
flat start initialization (top curve), the error rate 
never comes close to the 5% baseline error rate 
of the HMM/CFG model. With the generative 
model initialization, the error rate is reduced to 
4.8% at the second iteration, but the model 
quickly gets over-trained afterwards. 
0
5
10
15
20
25
30
35
0 20 40 60 80 100 120
Figure 6. Test set slot error rates (in %) at different 
training iterations. The top curve is for the flat start 
initialization, the bottom for the generative model  
initialization. 
The failure of the direct porting of the 
generative model to the conditional model can be 
attributed to the following reasons: 
? The conditional log-likelihood function is 
no longer a convex function due to the 
summation over hidden variables. This 
makes the model highly likely to settle on 
a local optimum. The fact that the flat start 
initialization failed to achieve the accuracy 
of the generative model initialization is a 
clear indication of the problem. 
? In order to account for words in the test 
data, the n-grams in the generative model 
are properly smoothed with back-offs to 
the uniform distribution over the 
vocabulary. This results in a huge number 
of parameters, many of which cannot be 
estimated reliably in the conditional 
model, given that model regularization is 
not as well studied as in n-grams.  
? The hidden variables make parameter 
estimation less reliable, given only a small 
amount of training data. 
5 CRFs for SLU 
An important lesson we have learned from the 
previous experiment is that we should not think 
generatively when applying conditional models. 
While it is important to find cues that help 
identify the slots, there is no need to exhaustively 
model the generation of every word in a 
sentence. Hence, the distinctions between pre- 
and postcommands, and pre- and postambles are 
no longer necessary. Every word that appears 
between two slots is labeled as the preamble state 
of the second slot, as illustrated in Figure 7. This 
labeling scheme effectively removes the hidden 
variables and simplifies the model to a CRF. It 
not only expedites model training, but also 
prevents parameters from settling at a local 
optimum, because the log conditional probability 
is now a convex function. 
 
Figure 7.  Once the slots are marked in the 
simplified model topology, the state sequence is fully 
marked, leaving no hidden variables and resulting in a 
CRF. Here, PAC stands for ?preamble for arrival 
city,? and PDC for ?preamble for departure city.?  
The command prior and state transition 
features (with fewer states) are the same as in the 
HCRF model. For unigrams and bigrams, only 
those that occur in front of a CFG-covered string 
are considered.  If the string is CFG-covered for 
slot s, then the unigram and bigram features for 
the preamble state of s are included. Suppose the 
words ?that departs? occur at positions 
1 and t t?  in front of the word ?Seattle,? which 
is CFG-covered by the non-terminal City.  Since 
City can fill a DepartureCity or ArrivalCity slot, 
the four following features are introduced:  
886
( 1) ( ) ( 1) ( )
1 1PDC,that PAC,that
( ) ( ) 1o oUG t t UG t tf s s t f s s t? ?? ?, , , = , , , =
And  
 
( 1) ( )
1
( 1) ( )
1
PDC,that,departs
PAC,that,departs
( )
( ) 1
o
o
BG t t
BG t t
f s s t
f s s t
?
?
?
?
, , , =
, , , =  
Formally, 
( )
( 1) ( )
1
( 1) ( )
1
( 1) ( ) 1
1 2
,
, ,1 2
1 if 
( ) ,
0 otherwise              
( )
1 if 
     = ,
0 otherwise                                           
o
o
o
o o
t t
UG t t
BG t t
t t t t
s w
s w w
s s w
f s s t
f s s t
s s s w w
?
?
?
?
? ?
? = ? =, , , = ??
, , ,
? = = ? = ? =??
 
 
( )
1 2 1 2
     | isFiller ;
     , | in  the training data,   and   
appears in front of sequence that is CFG-covered
for .
s s
w w w w w w
s
? ?
?  
5.1 Additional Features 
One advantage of CRFs over generative models 
is the ease with which overlapping features can 
be incorporated. In this section, we describe 
three additional feature sets. 
 
The first set addresses a side effect of not 
modeling the generation of every word in a 
sentence. Suppose a preamble state has never 
occurred in a position that is confusable with a 
slot state s, and a word that is CFG-covered for s 
has never occurred as part of the preamble state 
in the training data. Then, the unigram feature of 
the word for that preamble state has weight 0, 
and there is thus no penalty for mislabeling the 
word as the preamble. This is one of the most 
common errors observed in the development set. 
The chunk coverage for preamble words feature 
introduced to model the likelihood of a CFG-
covered word being labeled as a preamble: 
( 1) ( )
( ) ( )
,
( )
1 if  C( ) covers( , )  isPre( )    
0 otherwise                                                   
t tCC
t tt
c NT
f s s t
s c NT s
?
?????
, , ,
= ? ?=
o
o
 
where isPre( )s  indicates that s is a preamble 
state.  
Often, the identity of a slot depends on the 
preambles of the previous slot. For example, ?at 
two PM? is a DepartureTime in ?flight from 
Seattle to Boston at two PM?, but it is an 
ArrivalTime in ?flight departing from Seattle 
arriving in Boston at two PM.? In both cases, the 
previous slot is ArrivalCity, so the state 
transition features are not helpful for 
disambiguation.  The identity of the time slot 
depends not on the ArrivalCity slot, but on its 
preamble. Our second feature set, previous-slot 
context, introduces this dependency to the model: 
( 1) ( )
( 1) ( )
1 2 1
1 1 2
, ,1 2
( )
1 if ( , , 1)
     =  isFiller( )  Slot( ) Slot( )
0 otherwise                                                
PC t t
t t
ws sf s s t
s s s s w s t
s s s
?
?
, , ,
? = ? = ? ?? ?? ? ? ????
o
o  
Here Slot( )s  stands for the slot associated with 
the state ,s  which can be a filler state or a 
preamble state, as shown in Figure 7. 
1( , , 1)os t? ?  is the set of k words (where k is an 
adjustable window size) in front of the longest 
sequence that ends at position 1t ? and that is 
CFG-covered by 1Slot( )s . 
The third feature set is intended to penalize 
erroneous segmentation, such as segmenting 
?Washington D.C.? into two separate City slots. 
The chunk coverage for slot boundary feature is 
activated when a slot boundary is covered by a 
CFG non-terminal NT, i.e., when words in two 
consecutive slots (?Washington? and ?D.C.?) can 
also be covered by one single slot: 
( 1) ( )
( )
1
( 1) ( )
( 1) ( )
,
( )
          if  C( ) covers( , )1
          isFiller( )  isFiller( )    
          
0 otherwise                        
t tSB
t t
t
t t
t t
c NT
f s s t
s c NT
s s
s s
?
?
?
?
?????????
, , ,
= ?
? ?=
? ?
o
o
 
This feature set shares its weights with the 
chunk coverage features for preamble words, 
and does not introduce any new parameters. 
 
Features # of Param. SER 
Command Prior 6   
+State Transition +1377 18.68%
+Unigrams +14433 7.29% 
+Bigrams +58191 7.23% 
+Chunk Cov Preamble Word +156 6.87% 
+Previous-Slot Context +290 5.46% 
+Chunk Cov Slot Boundaries +0 3.94% 
Table 1. Number of additional parameters and the 
slot error rate after each new feature set is introduced. 
5.2 Experiments 
Since the objective function is convex, the 
optimization algorithm does not make any 
significant difference on SLU accuracy. We 
887
trained the model with SGD.  Other optimization 
algorithm like Stochastic Meta-Decent 
(Vishwanathan, Schraudolph et al, 2006) can be 
used to speed up the convergence. The training 
stopping criterion is cross-validated with the 
development set. 
Table 1 shows the number of new parameters 
and the slot error rate (SER) on the test data, 
after each new feature set is introduced. The new 
features improve the prediction of slot identities 
and reduce the SER by 21%, relative to the 
generative HMM/CFG composite model. 
The figures below show in detail the impact of 
the n-gram, previous-slot context and chunk 
coverage features.  The chunk coverage feature 
has three settings: 0 stands for no chunk 
coverage features; 1 for chunk coverage features 
for preamble words only; and 2 for both words 
and slot boundaries.  
Figure 8 shows the impact of the order of n-
gram features. Zero-order means no lexical 
features for preamble states are included. As the 
figure illustrates, the inclusion of CFG rules for 
slot filler states and domain-specific knowledge 
about command priors and slot transitions have 
already produced a reasonable SER under 15%. 
Unigram features for preamble states cut the 
error by more than 50%, while the impact of 
bigram features is not consistent -- it yields a 
small positive or negative difference depending 
on other experimental parameter settings. 
0%
2%
4%
6%
8%
10%
12%
14%
16%
0 1 2Ngram Order 
Slo
t E
rro
r R
ate
ChunkCoverage=0
ChunkCoverage=1
ChunkCoverage=2
 
Figure 8.  Effects of the order of n-grams on SER. 
The window size for the previous-slot context features 
is 2.  
Figure 9 shows the impact of the CFG chunk 
coverage feature.  Coverage for both preamble 
words and slot boundaries help improve the SLU 
accuracy. 
Figure 10 shows the impact of the window 
size for the previous-slot context feature. Here, 0 
means that the previous-slot context feature is 
not used. When the window size is k, the k words 
in front of the longest previous CFG-covered 
word sequence are included as the previous-slot 
unigram context features. As the figure 
illustrates, this feature significantly reduces SER, 
while the window size does not make any 
significant difference.  
0%
2%
4%
6%
8%
10%
12%
14%
16%
0 1 2
Chunk Coverage 
Slo
t E
rro
r R
ate
n=0
n=1
n=2
 
Figure 9. Effects of the chunk coverage feature. The 
window size for the previous-slot context feature is 2. 
The three lines correspond to different n-gram orders, 
where 0-gram indicates that no preamble lexical 
features are used.  
It is important to note that overlapping 
features like ,  and CC SB PCf f f  could not be easily 
incorporated into a generative model. 
0%
2%
4%
6%
8%
10%
12%
0 1 2
Window Size 
Slo
t E
rro
r R
ate n=0
n=1
n=2
 
Figure 10. Effects of the window size of the 
previous-slot context feature. The three lines represent 
different orders of n-grams (0, 1, and 2). Chunk 
coverage features for both preamble words and slot 
boundaries are used. 
5.3 CRFs vs. Perceptrons 
Table 2 compares the perceptron and CRF 
training algorithms, using chunk coverage 
features for both preamble words and slot 
boundaries, with which the best accuracy results 
888
are achieved. Both improve upon the 5% 
baseline SER from the generative HMM/CFG 
model. CRF training outperforms the perceptron 
in most settings, except for the one with unigram 
features for preamble states and with window 
size 1 -- the model with the fewest parameters. 
One possible explanation is as follows.  The 
objective function in CRFs is a convex function, 
and so SGD can find the single global optimum 
for it.  In contrast, the objective function for the 
perceptron, which is the difference between two 
convex functions, is not convex.  The gradient 
ascent approach in perceptron training is hence 
more likely to settle on a local optimum as the 
model becomes more complicated. 
 
  PSWSize=1 PSWSize=2 
  Perceptron CRFs Perceptron CRFs
n=1 3.76% 4.11% 4.23% 3.94%
n=2 4.76% 4.41% 4.58% 3.94%
Table 2. Perceptron vs. CRF training.  Chunk 
coverage features are used for both preamble words 
and slot boundaries. PSWSize stands for the window 
size of the previous-slot context feature. N is the order 
of the n-gram features. 
The biggest advantage of perceptron learning 
is its speed.  It directly counts the occurrence of 
features given an observation and its reference 
label sequence and Viterbi label sequence, with 
no need to collect expected feature counts with a 
forward-backward-like algorithm.  Not only is 
each iteration faster, but fewer iterations are 
required, when using SLU accuracy on a cross-
validation set as the stopping criterion. Overall, 
perceptron training is 5 to 8 times faster than 
CRF training. 
6 Conclusions 
This paper has introduced a conditional model 
framework that integrates statistical learning 
with a knowledge-based approach to SLU. We 
have shown that a conditional model reduces 
SLU slot error rate by more than 20% over the 
generative HMM/CFG composite model. The 
improvement was mostly due to the introduction 
of new overlapping features into the model. We 
have also discussed our experience in directly 
porting a generative model to a conditional 
model, and demonstrated that it may not be 
beneficial at all if we still think generatively in 
conditional modeling; more specifically, 
replicating the feature set of a generative model 
in a conditional model may not help much. The 
key benefit of conditional models is the ease with 
which they can incorporate overlapping and non-
homogeneous features. This is consistent with 
the finding in the application of conditional 
models for POS tagging (Lafferty, McCallum et 
al., 2001). The paper also compares different 
training algorithms for conditional models.  In 
most cases, CRF training is more accurate, 
however, perceptron training is much faster. 
References 
Bahl, L., P. Brown, et al 1986. Maximum mutual 
information estimation of hidden Markov model 
parameters for speech recognition. IEEE 
International Conference on Acoustics, Speech, 
and Signal Processing. 
Collins, M. 2002. Discriminative Training Methods 
for Hidden Markov Models: Theory and 
Experiments with Perceptron Algorithms. EMNLP, 
Philadelphia, PA. 
Gunawardana, A., M. Mahajan, et al 2005. Hidden 
conditional random fields for phone classification. 
Eurospeech, Lisbon, Portugal. 
Juang, B.-H., W. Chou, et al 1997. "Minimum 
classification error rate methods for speech 
recognition." IEEE Transactions on Speech and 
Audio Processing 5(3): 257-265. 
Kushner, H. J. and G. G. Yin. 1997. Stochastic 
approximation algorithms and applications, 
Springer-Verlag. 
Lafferty, J., A. McCallum, et al 2001. Conditional 
random fields: probabilistic models for segmenting 
and labeling sequence data. ICML. 
Nocedal, J. and S. J. Wright. 1999. Numerical 
optimization, Springer-Verlag. 
Povey, D. and P. C. Woodland. 2002. Minimum 
phone error and I-smoothing for improved 
discriminative training. IEEE International 
Conference on Acoustics, Speech, and Signal 
Processing. 
Price, P. 1990. Evaluation of spoken language system: 
the ATIS domain. DARPA Speech and Natural 
Language Workshop, Hidden Valley, PA. 
Quattoni, A., M. Collins and T. Darrell.  2004. 
Conditional Random Fields for Object 
Recognition.  NIPS. 
Vishwanathan, S. V. N., N. N. Schraudolph, et al 
2006. Accelerated Training of conditional random 
fields with stochastic meta-descent. The Learning 
Workshop, Snowbird, Utah. 
Wang, Y.-Y., L. Deng, et al 2005. "Spoken language 
understanding --- an introduction to the statistical 
framework." IEEE Signal Processing Magazine 
22(5): 16-31. 
 
889
Information Extraction Using the Structured Language Model
Ciprian Chelba and Milind Mahajan
Microsoft Research
Microsoft Corporation
One Microsoft Way, Redmond, WA 98052
fchelba,milindmg@microsoft.com
Abstract
The paper presents a data-driven approach to infor-
mation extraction (viewed as template lling) using
the structured language model (SLM) as a statistical
parser. The task of template lling is cast as con-
strained parsing using the SLM. The model is auto-
matically trained from a set of sentences annotated
with frame/slot labels and spans. Training pro-
ceeds in stages: rst a constrained syntactic parser
is trained such that the parses on training data meet
the specied semantic spans, then the non-terminal
labels are enriched to contain semantic information
and nally a constrained syntactic+semantic parser
is trained on the parse trees resulting from the pre-
vious stage. Despite the small amount of training
data used, the model is shown to outperform the
slot level accuracy of a simple semantic grammar
authored manually for the MiPad | personal infor-
mation management | task.
1 Introduction
Information extraction from text can be character-
ized as template lling (Jurafsky and Martin, 2000):
a given template or frame contains a certain num-
ber of slots that need to be lled in with segments
of text. Typically not all the words in text are rele-
vant to a particular frame. Assuming that the seg-
ments of text relevant to lling in the slots are non-
overlapping contiguous strings of words, one can rep-
resent the semantic frame as a simple semantic parse
tree for the sentence to be processed. The tree has
two levels: the root node is tagged with the frame
label and spans the entire sentence; the leaf nodes
are tagged with the slot labels and span the strings
of words relevant to the corresponding slot.
Consider the semantic parse S for a sentence W
presented in Fig. 1. CalendarTask is the frame tag,
(CalendarTask schedule meeting with
(ByFullName*Person megan hokins) about
(SubjectByWildCard*Subject internal lecture)
at (PreciseTime*Time two thirty p.m.))
Figure 1: Sample sentence and semantic parse
spanning the entire sentence; the remaining ones are
slot tags with their corresponding spans.
In the MiPad scenario (Huang et al, 2000) | es-
sentially a personal information management (PIM)
task | there is a module that is able to convert
the information extracted according to the semantic
parse into specic actions. In this case the action is
to schedule a calendar appointment.
We view the problem of information extraction as
the recovery of the two-level semantic parse S for a
given word sequence W .
We propose a data driven approach to information
extraction that uses the structured language model
(SLM) (Chelba and Jelinek, 2000) as an automatic
parser. The parser is constrained to explore only
parses that contain pre-set constituents | spanning
a given word string and bearing a tag in a given
set of semantic tags. The constraints available dur-
ing training and test are dierent, the test case con-
straints being more relaxed as explained in Section 4.
The main advantage of the approach is that it
doesn't require any grammar authoring expertise.
The approach is fully automatic once the annotated
training data is provided; it does assume that an
application schema | i.e. frame and slot structure
| has been dened but does not require seman-
tic grammars that identify word-sequence to slot or
frame mapping. However, the process of convert-
ing the word sequence coresponding to a slot into
actionable canonical forms | i.e. convert half past
two in the afternoon into 2:30 p.m. | may require
grammars. The design of the frames | what infor-
mation is relevant for taking a certain action, what
slot/frame tags are to be used, see (Wang, 1999) |
is a delicate task that we will not be concerned with
for the purposes of this paper.
The remainder of the paper is organized as fol-
lows: Section 2 reviews the structured language
model (SLM) followed by Section 3 which describes
in detail the training procedure and Section 4 which
denes the operation of the SLM as a constrained
parser and presents the necessary modications to
the model. Section 5 compares our approach to oth-
ers in the literature, in particular that of (Miller et
al., 2000). Section 6 presents the experiments we
have carried out. We conclude with Section 7.
2 Structured Language Model
We proceed with a brief review of the structured
language model (SLM); an extensive presentation
of the SLM can be found in (Chelba and Jelinek,
2000). The model assigns a probability P (W;T )
to every sentence W and its every possible binary
parse T . The terminals of T are the words of W
with POStags, and the nodes of T are annotated
with phrase headwords and non-terminal labels. Let
(<s>, SB)   .......   (w_p, t_p) (w_{p+1}, t_{p+1}) ........ (w_k, t_k) w_{k+1}.... </s>
h_0 = (h_0.word, h_0.tag)h_{-1}h_{-m} = (<s>, SB)
Figure 2: A word-parse k-prex
W be a sentence of length n words to which we
have prepended the sentence beginning marker <s>
and appended the sentence end marker </s> so that
w
0
=<s> and w
n+1
=</s>. Let W
k
= w
0
: : : w
k
be
the word k-prex of the sentence | the words from
the begining of the sentence up to the current po-
sition k | and W
k
T
k
the word-parse k-prex. Fig-
ure 2 shows a word-parse k-prex; h_0 .. h_{-m}
are the exposed heads, each head being a pair (head-
word, non-terminal label), or (word, POStag) in the
case of a root-only tree. The exposed heads at a
given position k in the input sentence are a function
of the word-parse k-prex.
2.1 Probabilistic Model
The joint probability P (W;T ) of a word sequence W
and a complete parse T can be broken into:
P (W;T ) =
Q
n+1
k=1
[P (w
k
=W
k 1
T
k 1
)  P (t
k
=W
k 1
T
k 1
; w
k
) 
Q
N
k
i=1
P (p
k
i
=W
k 1
T
k 1
; w
k
; t
k
; p
k
1
: : : p
k
i 1
)]
where:
 W
k 1
T
k 1
is the word-parse (k   1)-prex
 w
k
is the word predicted by WORD-
PREDICTOR
 t
k
is the tag assigned to w
k
by the TAGGER
 N
k
 1 is the number of operations the PARSER
executes at sentence position k before passing
control to the WORD-PREDICTOR (theN
k
-th
operation at position k is the null transition);
N
k
is a function of T
 p
k
i
denotes the i-th PARSER operation carried
out at position k in the word string; the opera-
tions performed by the PARSER are illustrated
in Figures 3-4 and they ensure that all possible
binary branching parses with all possible head-
word and non-terminal label assignments for the
w
1
: : : w
k
word sequence can be generated. The
p
k
1
: : : p
k
N
k
sequence of PARSER operations at
position k grows the word-parse (k   1)-prex
into a word-parse k-prex.
...............
T?_0
T_{-1} T_0<s> T?_{-1}<-T_{-2}
h_{-1} h_0
h?_{-1} = h_{-2}
T?_{-m+1}<-<s>
h?_0 = (h_{-1}.word, NTlabel)
Figure 3: Result of adjoin-left under NTlabel
............... T?_{-1}<-T_{-2} T_0
h_0h_{-1}
<s>
T?_{-m+1}<-<s>
h?_{-1}=h_{-2}
T_{-1}
h?_0 = (h_0.word, NTlabel)
Figure 4: Result of adjoin-right under NTlabel
Our model is based on three probabilities, each
estimated using deleted interpolation and parame-
terized (approximated) as follows:
P (w
k
=W
k 1
T
k 1
)
:
= P (w
k
=h
0
; h
 1
)
P (t
k
=w
k
;W
k 1
T
k 1
)
:
= P (t
k
=w
k
; h
0
; h
 1
)
P (p
k
i
=W
k
T
k
)
:
= P (p
k
i
=h
0
; h
 1
)
It is worth noting that if the binary branching struc-
ture developed by the parser were always right-
branching and we mapped the POStag and non-
terminal label vocabularies to a single type then
our model would be equivalent to a trigram lan-
guage model. Since the number of parses for a
given word prex W
k
grows exponentially with k,
jfT
k
gj  O(2
k
), the state space of our model is huge
even for relatively short sentences, so we had to use
a search strategy that prunes it. Our choice was a
synchronous multi-stack search algorithm which is
very similar to a beam search.
The language model probability assignment for
the word at position k + 1 in the input sentence is
made using:
P (w
k+1
=W
k
) =
X
T
k
2S
k
P (w
k+1
=W
k
T
k
)  Proceedings of the Second Workshop on Statistical Machine Translation, pages 72?79,
Prague, June 2007. c?2007 Association for Computational Linguistics
Training Non-Parametric Features for Statistical Machine Translation
Patrick Nguyen, Milind Mahajan and Xiaodong He
Microsoft Corporation
1 Microsoft Way,
Redmond, WA 98052
{panguyen,milindm,xiaohe}@microsoft.com
Abstract
Modern statistical machine translation sys-
tems may be seen as using two components:
feature extraction, that summarizes informa-
tion about the translation, and a log-linear
framework to combine features. In this pa-
per, we propose to relax the linearity con-
straints on the combination, and hence relax-
ing constraints of monotonicity and indepen-
dence of feature functions. We expand fea-
tures into a non-parametric, non-linear, and
high-dimensional space. We extend empir-
ical Bayes reward training of model param-
eters to meta parameters of feature genera-
tion. In effect, this allows us to trade away
some human expert feature design for data.
Preliminary results on a standard task show
an encouraging improvement.
1 Introduction
In recent years, statistical machine translation have
experienced a quantum leap in quality thanks to au-
tomatic evaluation (Papineni et al, 2002) and error-
based optimization (Och, 2003). The conditional
log-linear feature combination framework (Berger,
Della Pietra and Della Pietra, 1996) is remarkably
simple and effective in practice. Therefore, re-
cent efforts (Och et al, 2004) have concentrated on
feature design ? wherein more intelligent features
may be added. Because of their simplicity, how-
ever, log-linear models impose some constraints on
how new information may be inserted into the sys-
tem to achieve the best results. In other words,
new information needs to be parameterized care-
fully into one or more real valued feature functions.
Therefore, that requires some human knowledge and
understanding. When not readily available, this
is typically replaced with painstaking experimenta-
tion. We propose to replace that step with automatic
training of non-parametric agnostic features instead,
hopefully relieving the burden of finding the optimal
parameterization.
First, we define the model and the objective func-
tion training framework, then we describe our new
non-parametric features.
2 Model
In this section, we describe the general log-linear
model used for statistical machine translation, as
well as a training objective function and algorithm.
The goal is to translate a French (source) sentence
indexed by t, with surface string ft. Among a set of
Kt outcomes, we denote an English (target) a hy-
pothesis with surface string e(t)k indexed by k.
2.1 Log-linear Model
The prevalent translation model in modern systems
is a conditional log-linear model (Och and Ney,
2002). From a hypothesis e(t)k , we extract features
h(t)k , abbreviated hk, as a function of e
(t)
k and ft. The
conditional probability of a hypothesis e(t)k given a
source sentence ft is:
pk , p(e(t)k |ft) ,
exp[? ? hk]
Zft;?
,
72
where the partition function Zft;? is given by:
Zft;? =
?
j
exp[? ? hj ].
The vector of parameters of the model ?, gives a
relative importance to each feature function compo-
nent.
2.2 Training Criteria
In this section, we quickly review how to adjust ?
to get better translation results. First, let us define
the figure of merit used for evaluation of translation
quality.
2.2.1 BLEU Evaluation
The BLEU score (Papineni et al, 2002) was de-
fined to measure overlap between a hypothesized
translation and a set of human references. n-gram
overlap counts {cn}4n=1 are computed over the test
set sentences, and compared to the total counts of
n-grams in the hypothesis:
cn,(t)k , max. # of matching n-grams for hyp. e(t)k ,
an,(t)k , # of n-grams in hypothesis e(t)k .
Those quantities are abbreviated ck and ak to sim-
plify the notation. The precision ratio Pn for an n-
gram order n is:
Pn ,
?
t c
n,(t)
k
?
t a
n,(t)
k
.
A brevity penalty BP is also taken into account, to
avoid favoring overly short sentences:
BP , min{1; exp(1 ? ra)},
where r is the average length of the shortest sen-
tence1, and a is the average length of hypotheses.
The BLEU score the set of hypotheses {e(t)k } is:
B({e(t)k }) , BP ? exp
( 4
?
n=1
1
4 logPn
)
.
1As implemented by NIST mteval-v11b.pl.
Oracle BLEU hypothesis: There is no easy way
to pick the set hypotheses from an n-best list that
will maximize the overall BLEU score. Instead, to
compute oracle BLEU hypotheses, we chose, for
each sentence independently, the hypothesis with the
highest BLEU score computed for a sentence itself.
We believe that it is a relatively tight lower bound
and equal for practical purposes to the true oracle
BLEU.
2.2.2 Maximum Likelihood
Used in earlier models (Och and Ney, 2002), the
likelihood criterion is defined as the likelihood of an
oracle hypothesis e(t)k? , typically a single reference
translation, or alternatively the closest match which
was decoded. When the model is correct and infi-
nite amounts of data are available, this method will
converge to the Bayes error (minimum achievable
error), where we define a classification task of se-
lecting k? against all others.
2.2.3 Regularization Schemes
One can convert a maximum likelihood problem
into maximum a posteriori using Bayes? rule:
argmax
?
?
t
p(?|{e(t)k , ft}) = argmax?
?
t
pkp0(?),
where p0(?) is the prior distribution of ?. The
most frequently used prior in practice is the normal
prior (Chen and Rosenfeld, 2000):
log p0(?) , ?||?||
2
2?2 ? log |?|,
where ?2 > 0 is the variance. It can be thought of
as the inverse of a Lagrange multiplier when work-
ing with constrained optimization on the Euclidean
norm of ?. When not interpolated with the likeli-
hood, the prior can be thought of as a penalty term.
The entropy penalty may also be used:
H , ? 1T
T
?
t=1
Kt
?
k=1
pk log pk.
Unlike the Gaussian prior, the entropy is indepen-
dent of parameterization (i.e., it does not depend on
how features are expressed).
73
2.2.4 Minimum Error Rate Training
A good way of training ? is to minimize empirical
top-1 error on training data (Och, 2003). Compared
to maximum-likelihood, we now give partial credit
for sentences which are only partially correct. The
criterion is:
argmax
?
?
t
B({e(t)
k?
}) : e(t)
k?
= argmax
e(t)j
pj.
We optimize the ? so that the BLEU score of the
most likely hypotheses is improved. For that reason,
we call this criterion BLEU max. This function is
not convex and there is no known exact efficient op-
timization for it. However, there exists a linear-time
algorithm for exact line search against that objec-
tive. The method is often used in conjunction with
coordinate projection to great success.
2.2.5 Maximum Empirical Bayes Reward
The algorithm may be improved by giving partial
credit for confidence pk of the model to partially cor-
rect hypotheses outside of the most likely hypothe-
sis (Smith and Eisner, 2006):
1
T
T
?
t=1
Kt
?
k=1
pk logB({ek(t)}).
Instead of the BLEU score, we use its logrithm, be-
cause we think it is exponentially hard to improve
BLEU. This model is equivalent to the previous
model when pk give all the probability mass to the
top-1. That can be reached, for instance, when ?
has a very large norm. There is no known method
to train against this objective directly, however, ef-
ficient approximations have been developed. Again,
it is not convex.
It is hoped that this criterion is better suited for
high-dimensional feature spaces. That is our main
motivation for using this objective function through-
out this paper. With baseline features and on our
data set, this criterion also seemed to lead to results
similar to Minimum Error Rate Training.
We can normalize B to a probability measure
b({e(t)k }). The empirical Bayes reward also coin-
cides with a divergence D(p||b).
2.3 Training Algorithm
We train our model using a gradient ascent method
over an approximation of the empirical Bayes re-
ward function.
2.3.1 Approximation
Because the empirical Bayes reward is defined
over a set of sentences, it may not be decomposed
sentence by sentence. This is computationally bur-
densome. Its sufficient statistics are r, ?t ck and
?
t ak. The function may be reconstructed in a first-
order approximation with respect to each of these
statistics. In practice this has the effect of commut-
ing the expectation inside of the functional, and for
that reason we call this criterion BLEU soft. This ap-
proximation is called linearization (Smith and Eis-
ner, 2006). We used a first-order approximation for
speed, and ease of interpretation of the derivations.
The new objective function is:
J , log B?P +
4
?
n=1
1
4 log
?
t Ec
n,(t)
k
?
t Ea
n,(t)
k
,
where the average bleu penalty is:
log B?P , min{0; 1 ? r
Ek,ta1,(t)k
}.
The expectation is understood to be under the cur-
rent estimate of our log-linear model. Because B?P is
not differentiable, we replace the hard min function
with a sigmoid, yielding:
log B?P ? u(r ? Ek,ta1,(t)k )
(
1? r
Ek,ta1,(t)k
)
,
with the sigmoid function u(x) defines a soft step
function:
u(x) , 11 + e??x ,
with a parameter ? ? 1.
2.3.2 Gradients and Sufficient Statistics
We can obtain the gradients of the objective func-
tion using the chain rule by first differentiating with
respect to the probability. First, let us decompose
the log-precision of the expected counts:
log P?n = log Ecn,(t)k ? log Ea
n,(t)
k .
74
Each n-gram precision may be treated separately.
For each n-gram order, let us define sufficient statis-
tics ? for the precision:
?c? ,
?
t,k
(??pk)ck; ?a? ,
?
t,k
(??pk)ak,
where the gradient of the probabilities is given by:
??pk = pk(hk ? h?),
with:
h? ,
Kt
?
j=1
pjhj .
The derivative of the precision P?n is:
??log P?n =
1
T
[ ?c?
Eck
? ?
a
?
Eak
]
For the length, the derivative of log B?P is:
u(r?Ea)
[
(ra ? 1)[1 ? u(r ? Ea)]? +
r
(Ea)2
]
?a1? ,
where ?a1? is the 1-gram component of ?a?. Finally,
the derivative of the entropy is:
??H =
?
k,t
(1 + log pk)??pk.
2.3.3 RProp
For all our experiments, we chose RProp (Ried-
miller and Braun, 1992) as the gradient ascent al-
gorithm. Unlike other gradient algorithms, it is only
based on the sign of the gradient components at each
iteration. It is relatively robust to the objective func-
tion, requires little memory, does not require meta
parameters to be tuned, and is simple to implement.
On the other hand, it typically requires more iter-
ations than stochastic gradient (Kushner and Yin,
1997) or L-BFGS (Nocedal and Wright, 1999).
Using fairly conservative stopping criteria, we ob-
served that RProp was about 6 times faster than Min-
imum Error Rate Training.
3 Adding Features
The log-linear model is relatively simple, and is usu-
ally found to yield good performance in practice.
With these considerations in mind, feature engineer-
ing is an active area of research (Och et al, 2004).
Because the model is fairly simple, some of the in-
telligence must be shifted to feature design. After
having decided what new information should go in
the overall score, there is an extra effort involved
in expressing or parameterizing features in a way
which will be easiest for the model learn. Experi-
mentation is usually required to find the best config-
uration.
By adding non-parametric features, we propose
to mitigate the parameterization problem. We will
not add new information, but rather, propose a way
to insulate research from the parameterization. The
system should perform equivalently invariant of any
continuous invertible transformation of the original
input.
3.1 Existing Features
The baseline system is a syntax based machine
translation system as described in (Quirk, Menezes
and Cherry, 2005). Our existing feature set includes
11 features, among which the following:
? Target hypothesis word count.
? Treelet count used to construct the candidate.
? Target language models, based on the Giga-
word corpus (5-gram) and target side of parallel
training data (3-gram).
? Order models, which assign a probability to the
position of each target node relative to its head.
? Treelet translation model.
? Dependency-based bigram language models.
3.2 Re-ranking Framework
Our algorithm works in a re-ranking framework.
In particular, we are adding features which are not
causal or additive. Features for a hypothesis may
not be accumulating by looking at the English (tar-
get) surface string words from the left to the right
and adding a contribution per word. Word count,
for instance, is causal and additive. This property
is typically required for efficient first-pass decod-
ing. Instead, we look at a hypothesis sentence as a
whole. Furthermore, we assume that the Kt-best list
provided to us contains the entire probability space.
75
In particular, the computation of the partition func-
tion is performed over all Kt-best hypotheses. This
is clearly not correct, and is the subject of further
study. We use the n-best generation scheme inter-
leaved with ? optimization as described in (Och,
2003).
3.3 Issues with Parameterization
As alluded to earlier, when designing a new feature
in the log-linear model, one has to be careful to find
the best embodiment. In general, a set of features
must satisfy the following properties, ranked from
strict to lax:
? Linearity (warping)
? Monotonicity
? Independence (conjunction)
Firstly, a feature should be linearly correlated with
performance. There should be no region were it
matters less than other regions. For instance, in-
stead of a word count, one might consider adding
its logarithm instead. Secondly, the ?goodness? of a
hypothesis associated with a feature must be mono-
tonic. For instance, using the signed difference be-
tween word count in the French (source) and En-
glish (target) does not satisfy this. (In that case, one
would use the absolute value instead.) Lastly, there
should be no inter-dependence between features. As
an example, we can consider adding multiple lan-
guage model scores. Whether we should consider
ratios those of, globally linearly or log-linearly in-
terpolating them, is open to debate. When features
interact across dimensions, it becomes unclear what
the best embodiment should be.
3.4 Non-parametric Features
A generic solution may be sought in non-parametric
processing. Our method can be derived from a quan-
tized Parzen estimate of the feature density function.
3.4.1 Parzen Window
The Parzen window is an early empirical kernel
method (Duda and Hart, 1973). For an observation
hm, we extrapolate probability mass around it with
a smoothing window ?(?). The density function is:
p(h) = 1M
K
?
m=1
?(h? hm),
assuming ?(?) is a density function. Parzen win-
dows converge to the true density estimate, albeit
slowly, under weak assumptions.
3.4.2 Bin Features
One popular way of using continuous features in
log-linear models is to convert a single continuous
feature into multiple ?bin? features. Each bin feature
is defined as the indicator function of whether the
original continuous feature was in a certain range.
The bins were selected so that each bin collects an
equal share of the probability mass. This is equiva-
lent to the maximum likelihood estimate of the den-
sity function subject to a fixed number of rectangular
density kernels. Since that mapping is not differen-
tiable with respect to the original features, one may
use sigmoids to soften the boundaries.
Bin features are useful to relax the requirements
of linearity and monotonicity. However, because
they work on each feature individually, they do not
address the problem of inter-dependence between
features.
3.4.3 Gaussian Mixture Model Features
Bin features may be generalized to multi-
dimensional kernels by using a Gaussian smoothing
window instead of a rectangular window. The direct
analogy is vector quantization. The idea is to weight
specific regions of the feature space differently. As-
suming that we have M Gaussians each with mean
vector ?m and diagonal covariance matrix Cm, and
prior weight wm. We will add m new features, each
defined as the posterior in the mixture model:
hm , wmN (h;?m, Cm)?
r wrN (h;?r, Cr)
.
It is believed that any reasonable choice of kernels
will yield roughly equivalent results (Povey et al,
2004), if the amount of training data and the number
of kernels are both sufficiently large. We show two
methods for obtaining clusters. In contrast with bins,
lossless representation becomes rapidly impossible.
ML kernels: The canonical way of obtaining clus-
ter is to use the standard Gaussian mixture training.
First, a single Gaussian is trained on the whole data
set. Then, the Gaussian is split into two Gaussians,
with each mean vector perturbed, and the Gaus-
sians are retrained using maximum-likelihood in an
76
expectation-maximization framework (Rabiner and
Huang, 1993). The number of Gaussians is typically
increased exponentially.
Perceptron kernels: We also experimented with
another quicker way of obtaining kernels. We
chose an equal prior and a global covariance matrix.
Means were obtained as follows: for each sentence
in the training set, if the top-1 candidate was differ-
ent from the approximate maximum oracle BLEU
hypothesis, both were inserted. It is a quick way
to bootstrap and may reach the oracle BLEU score
quickly.
In the limit, GMMs will converge to the oracle
BLEU. In the next section, we show how to re-
estimate these kernels if needed.
3.5 Re-estimation Formul?
Features may also be trained using the same empir-
ical maximum Bayes reward. Let ? be the hyper-
parameter vector used to generate features. In the
case of language models, for instance, this could be
backoff weights. Let us further assume that the fea-
ture values are differentiable with respect to ?. Gra-
dient ascent may be applied again but this time with
respect to ?. Using the chain rule:
??J = (??h)(?hpk)(?pkJ),
with ?hpk = pk(1 ? pk)?. Let us take the example
of re-estimating the mean of a Gaussian kernel ?m:
??mhm = ?wmhm(1 ? hm)C?1m (?m ? h),
for its own feature, and for other posteriors r 6= m:
??mhr = ?wrhrhmC?1m (?m ? h),
which is typically close to zero if no two Gaussians
fire simultaneously.
4 Experimental Results
For our experiments, we used the standard NIST
MT-02 data set to evaluate our system.
4.1 NIST System
A relatively simple baseline was used for our exper-
iments. The system is syntactically-driven (Quirk,
Menezes and Cherry, 2005). The system was trained
on 175k sentences which were selected from the
NIST training data (NIST, 2006) to cover words in
source language sentences of the MT02 develop-
ment and evaluation sets. The 5-gram target lan-
guage model was trained on the Gigaword mono-
lingual data using absolute discounting smoothing.
In a single decoding, the system generated 1000 hy-
potheses per sentence whenever possible.
4.2 Leave-one-out Training
In order to have enough data for training, we gen-
erated our n-best lists using 10-fold leave-one-out
training: base feature extraction models were trained
on 9/10th of the data, then used for decoding the
held-out set. The process was repeated for all 10
parts. A single ? was then optimized on the com-
bined lists of all systems. That ? was used for an-
other round of 10 decodings. The process was re-
peated until it reached convergence after 7 iterations.
Each decoding generated about 100 hypotheses, and
there was relatively little overlap across decodings.
Therefore, there were about 1M hypotheses in total.
The combined list of all iterations was used for all
subsequent experiments of feature expansion.
4.3 BLEU Training Results
We tried training systems under the empirical Bayes
reward criterion, and appending either bin or GMM
features. We will find that bin features are es-
sentially ineffective while GMM features show a
modest improvement. We did not retrain hyper-
parameters.
4.3.1 Convexity of the Empirical Bayes Reward
The first question to ask is how many local op-
tima does the cost surface have using the standard
features. A complex cost surface indicates that some
gain may be had with non-linear features, but it also
shows that special care should be taken during op-
timization. Non-convexity is revealed by sensitivity
to initialization points. Thus, we decided to initial-
ize from all vertices of the unit hypercube, and since
we had 11 features, we ran 211 experiments. The
histogram of BLEU scores on dev data after conver-
gence is shown on Figure 1. We also plotted the his-
togram of an example dimension in Figure 2. The
range of BLEU scores and lambdas is reasonably
narrow. Even though ? seems to be bimodal, we see
77
that this does not seriously affect the BLEU score.
This is not definitive evidence but we provisionally
pretend that the cost surface is almost convex for
practical purposes.
24.8 24.9 25 25.1 25.2 25.3 25.4
0
200
400
600
800
BLEU score
n
u
m
be
r o
f t
ra
in
ed
 m
od
el
s
Figure 1: Histogram of BLEU scores after training
from 211 initializations.
?60 ?40 ?20 0
0
100
200
300
400
500
600
700
? value
n
u
m
be
r o
f t
ra
in
ed
 m
od
el
s
Figure 2: Histogram of one ? parameter after train-
ing from 211 initializations.
4.3.2 Bin Features
A log-linear model can be converted into a bin
feature model nearly exactly by setting ? values
in such a way that scores will be equal. Equiva-
lent weights (marked as ?original? in Figure 3) have
the shape of an error function (erf): this is because
the input feature is a cummulative random variable,
which quickly converges to a Gaussian (by the cen-
tral limit theorem). After training the ? weights for
the log-linear model, weights may be converted into
bins and re-trained. On Figure 3, we show that relax-
ing the monotonicity constraint leads to rough val-
ues for ?. Surprisingly, the BLEU score and ob-
jective on the training set only increases marginally.
Starting from ? = 0, we obtained nearly exactly the
same training objective value. By varying the num-
ber of bins (20-50), we observed similar behavior as
well.
0 10 20 30 40 50
?1.5
?1
?0.5
0
0.5
1
bin id
va
lu
e
 
 
original weights
trained weights
Figure 3: Values before and after training bin fea-
tures. Monotonicity constraint has been relaxed.
BLEU score is virtually unchanged.
4.3.3 GMM Features
Experiments were carried out with GMM fea-
tures. The summary is shown on Table 1. The
baseline was the log-linear model trained with the
baseline features. The baseline features are included
in all systems. We trained GMM models using the
iterative mixture splitting interleaved with EM re-
estimation, split up to 1024 and 16384 Gaussians,
which we call GMM-ML-1k and GMM-ML-16k re-
spectively. We also used the ?perceptron? selec-
tion features on the training set to bootstrap quickly
to 300k Gaussians (GMM-PCP-300k), and ran the
same algorithm on the development set (GMM-
PCP-2k). Therefore, GMM-PCP-300k had 300k
features, and was trained on 175k sentences (each
with about 700 hypotheses). For all experiments but
?unreg? (unregularized), we chose a prior Gaussian
prior with variance empirically by looking at the de-
velopment set. For all but GMM-PCP-300k, regu-
larization did not seem to have a noticeably positive
effect on development BLEU scores. All systems
were seeded with the baseline log-linear model, and
78
all additional weights set to zero, and then trained
with about 50 iterations, but convergence in BLEU
score, empirical reward, and development BLEU
score occurred after about 30 iterations. In that set-
ting, we found that regularized empirical Bayes re-
ward, BLEU score on training data, and BLEU score
on development and evaluation to be well corre-
lated. Cursory experiments revealed that using mul-
tiple initializations did not significantly alter the fi-
nal BLEU score.
System Train Dev Eval
Oracle 14.10 N/A N/A
Baseline 10.95 35.15 25.95
GMM-ML-1k 10.95 35.15 25.95
GMM-ML-16k 11.09 35.25 25.89
GMM-PCP-2k 10.95 35.15 25.95
GMM-PCP-300k-unreg 13.00 N/A N/A
GMM-PCP-300k 12.11 35.74 26.42
Table 1: BLEU scores for GMM features vs the lin-
ear baseline, using different selection methods and
number of kernels.
Perceptron kernels based on the training set im-
proved the baseline by 0.5 BLEU points. We mea-
sured significance with the Wilcoxon signed rank
test, by batching 10 sentences at a time to produce
an observation. The difference was found to be sig-
nificant at a 0.9-confidence level. The improvement
may be limited due to local optima or the fact that
original feature are well-suited for log-linear mod-
els.
5 Conclusion
In this paper, we have introduced a non-parametric
feature expansion, which guarantees invariance to
the specific embodiment of the original features.
Feature generation models, including feature ex-
pansion, may be trained using maximum regular-
ized empirical Bayes reward. This may be used as
an end-to-end framework to train all parameters of
the machine translation system. Experimentally, we
found that Gaussian mixture model (GMM) features
yielded a 0.5 BLEU improvement.
Although this is an encouraging result, further
study is required on hyper-parameter re-estimation,
presence of local optima, use of complex original
features to test the effectiveness of the parameteri-
zation invariance, and evaluation on a more compet-
itive baseline.
References
K. Papineni, S. Roukos, T. Ward, W.-J. Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. ACL?02.
A. Berger, S. Della Pietra, and V. Della Pietra. 1996.
A Maximum Entropy Approach to Natural Language
Processing. Computational Linguistics, vol 22:1, pp.
39?71.
S. Chen and R. Rosenfeld. 2000. A survey of smoothing
techniques for ME models. IEEE Trans. on Speech and
Audio Processing, vol 8:2, pp. 37?50.
R. O. Duda and P. E. Hart. 1973. Pattern Classification
and Scene Analysis. Wiley & Sons, 1973.
H. J. Kushner and G. G. Yin. 1997. Stochastic Approxi-
mation Algorithms and Applications. Springer-Verlag,
1997.
National Institute of Standards and Technology. 2006.
The 2006 Machine Translation Evaluation Plan.
J. Nocedal and S. J. Wright. 1999. Numerical Optimiza-
tion. Springer-Verlag, 1999.
F. J. Och. 2003. Minimum Error Rate Training in Statis-
tical Machine Translation. ACL?03.
F. J. Och, D. Gildea, S. Khudanpur, A. Sarkar, K. Ya-
mada, A. Fraser, S. Kumar, L. Shen, D. Smith, K. Eng,
V. Jain, Z. Jin, and D. Radev. 2004. A Smorgas-
bord of Features for Statistical Machine Translation.
HLT/NAACL?04.
F. J. Och and H. Ney. 2002. Discriminative Training
and Maximum Entropy Models for Statistical Machine
Translation. ACL?02.
D. Povey, B. Kingsbury, L. Mangu, G. Saon, H. Soltau
and G. Zweig. 2004. fMPE: Discriminatively trained
features for speech recognition. RT?04 Meeting.
C. Quirk, A. Menezes and C. Cherry. 2005. De-
pendency Tree Translation: Syntactically Informed
Phrasal SMT. ACL?05.
L. R. Rabiner and B.-H. Huang. 1993. Fundamentals of
Speech Recognition. Prentice Hall.
M. Riedmiller and H. Braun. 1992. RPROP: A Fast
Adaptive Learning Algorithm. Proc. of ISCIS VII.
D. A. Smith and J. Eisner. 2006. Minimum-Risk
Annealing for Training Log-Linear Models. ACL-
COLING?06.
79
