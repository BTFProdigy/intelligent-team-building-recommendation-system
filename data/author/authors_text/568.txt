Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 819?826, Prague, June 2007. c?2007 Association for Computational Linguistics
Learning Noun Phrase Query Segmentation
Shane Bergsma and Qin Iris Wang
Department of Computing Science
University of Alberta
Edmonton, Alberta, Canada, T6G 2E8
{bergsma,wqin}@cs.ualberta.ca
Abstract
Query segmentation is the process of tak-
ing a user?s search-engine query and di-
viding the tokens into individual phrases
or semantic units. Identification of these
query segments can potentially improve
both document-retrieval precision, by first
returning pages which contain the exact
query segments, and document-retrieval re-
call, by allowing query expansion or substi-
tution via the segmented units. We train and
evaluate a machine-learned query segmenta-
tion system that achieves 86% segmentation-
decision accuracy on a gold standard set of
segmented noun phrase queries, well above
recently published approaches. Key en-
ablers of this high performance are features
derived from previous natural language pro-
cessing work in noun compound bracketing.
For example, token association features be-
yond simple N-gram counts provide power-
ful indicators of segmentation.
1 Introduction
Billions of times every day, people around the world
communicate with Internet search engines via a
small text box on a web page. The user provides
a sequence of words to the search engine, and the
search engine interprets the query and tries to return
web pages that not only contain the query tokens,
but that are also somehow about the topic or idea
that the query terms describe.
Recent years have seen a widespread recognition
that the user is indeed providing natural language
text to the search engine; query tokens are not inde-
pendent, unordered symbols to be matched on a web
document but rather ordered words and phrases with
syntactic relationships. For example, Zhai (1997)
pointed out that indexing on single-word symbols is
not able to distinguish a search for ?bank terminol-
ogy? from one for ?terminology bank.? The reader
can submit these queries to a current search engine
to confirm that modern indexing does recognize the
effect of token order on query meaning in some way.
Accurately interpreting query semantics also de-
pends on establishing relationships between the
query tokens. For example, consider the query ?two
man power saw.? There are a number of possible
interpretations of this query, and these can be ex-
pressed through a number of different segmentations
or bracketings of the query terms:
1. [two man power saw]
2. [two man] [power saw]
3. [two] [man] [power saw]
4. [two] [man power] [saw], etc.
One simple way to make use of these interpretations
in search would be to put quotation marks around the
phrasal segments to require the search engine to only
find pages with exact phrase matches. If, as seems
likely, the searcher is seeking pages about the large,
mechanically-powered two-man saws used by lum-
berjacks and sawyers to cut big trees, then the first
segmentation is correct. Indeed, a phrasal search
for ?two man power saw? on Google does find the
device of interest. So does the second interpreta-
tion, but along with other, less-relevant pages dis-
cussing competitions involving ?two-man handsaw,
819
two-woman handsaw, power saw log bucking, etc.?
The top document returned for the third interpreta-
tion, meanwhile, describes a man on a rampage at a
subway station with two cordless power saws, while
the fourth interpretation finds pages about topics
ranging from hockey?s thrilling two-man power play
advantage to the man power situation during the
Second World War. Clearly, choosing the right seg-
mentation means finding the right documents faster.
Query segmentation can also help if insufficient
pages are returned for the original query. A tech-
nique such as query substitution or expansion (Jones
et al, 2006) can be employed using the segmented
units. For example, we could replace the sexist ?two
man? modifier with the politically-correct ?two per-
son? phrase in order to find additional relevant doc-
uments. Without segmentation, expanding via the
individual words ?two,? ?man,? ?power,? or ?saw?
could produce less sensible results.
In this paper, we propose a data-driven, machine-
learned approach to query segmentation. Similar to
previous segmentation approaches described in Sec-
tion 2, we make a decision to segment or not to seg-
ment between each pair of tokens in the query. Un-
like previous work, we view this as a classification
task where the decision parameters are learned dis-
criminatively from gold standard data. In Section 3,
we describe our approach and the features we use.
Section 4 describes our labelled data, as well as the
specific tools used for our experiments. Section 5
provides the results of our evaluation, and shows the
strong gains in performance possible using a wide
set of features within a discriminative framework.
2 Related Work
Query segmentation has previously been ap-
proached in an unsupervised manner. Risvik et
al. (2003) combine the frequency count of a seg-
ment and the mutual information (MI) between pairs
of words in the segment in a heuristic scoring func-
tion. The system chooses the segmentation with the
highest score as the output segmentation. Jones et
al. (2006) use MI between pairs of tokens as the sole
factor in deciding on segmentation breaks. If the MI
is above a threshold (optimized on a small training
set), the pair of tokens is joined in a segment. Oth-
erwise, a segmentation break is made.
Query segmentation is related to the task of noun
compound (NC) bracketing. NC bracketing deter-
mines the syntactic structure of an NC as expressed
by a binary tree, or, equivalently, a binary bracket-
ing (Nakov and Hearst, 2005a). Zhai (1997) first
identified the importance of syntactic query/corpus
parsing for information retrieval, but did not con-
sider query segmentation itself. In principle, as
N increases, the number of binary trees for an N -
token compound is much greater than the 2N?1 pos-
sible segmentations. In practice, empirical NC re-
search has focused on three-word compounds. The
computational problem is thus deciding whether the
three-word NC has a left or right-bracketing struc-
ture (Lauer, 1995). For the segmentation task,
analysing a three-word NC requires deciding be-
tween four different segmentations. For example,
there are two bracketings for ?used car parts,? the
left-bracketing ?[[used car] parts]? and the right-
bracketing ?[used [car parts]],? while there are four
segmentations, including the case where there is
only one segment, ?[used car parts]? and the base
case where each token forms its own segment,
?[used] [car] [parts].? Query segmentation thus nat-
urally handles the case where the query consists of
multiple, separate noun phrases that should not be
analysed with a single binary tree.
Despite the differences between the tasks, it is
worth investigating whether the information that
helps disambiguate left and right-bracketings can
also be useful for segmentation. In particular, we
explored many of the sources of information used
by Nakov and Hearst (2005a), as well as several
novel features that aid segmentation performance
and should also prove useful for NC analysis re-
searchers. Unlike all previous approaches that we
are aware of, we apply our features in a flexible
discriminative framework rather than a classification
based on a vote or average of features.
NC analysis has benefited from the recent trend
of using web-derived features rather than corpus-
based counts (Keller and Lapata, 2003). Lapata and
Keller (2004) first used web-based co-occurrence
counts for the bracketing of NCs. Recent inno-
vations have been to use statistics ?beyond the N-
gram,? such as counting the number of web pages
where a pair of words w, x participate in a genitive
relationship (?w?s x?), occur collapsed as a single
820
phrase (?wx?) (Nakov and Hearst, 2005a) or have
a definite article as a left-boundary marker (?the
w x?) (Nicholson and Baldwin, 2006). We show
strong performance gains when such features are
employed for query segmentation.
NC bracketing is part of a larger field of research
on multiword expressions including general NC in-
terpretation. NC interpretation explores not just
the syntactic dependencies among compound con-
stituents, but the semantics of the nominal relation-
ships (Girju et al, 2005). Web-based statistics have
also had an impact on these wider analysis tasks, in-
cluding work on interpretation of verb nominalisa-
tions (Nicholson and Baldwin, 2006) and NC coor-
dination (Nakov and Hearst, 2005b).
3 Methodology
3.1 Segmentation Classification
Consider a query x = {x1, x2, ..., xN} consistingof N query tokens. Segmentation is a mapping S :
x ? y ? YN , where y is a segmentation from theset YN . Since we can either have or not have a seg-mentation break at each of the N?1 spaces between
the N tokens, |YN | = 2N?1. Supervised machinelearning can be applied to derive the mapping S au-
tomatically, given a set of training examples con-
sisting of pairs of queries and their segmentations
T = {(xi,yi)}. Typically this would be done via aset of features ?(x,y) for the structured examples.
A set of weights w can be learned discriminatively
such that each training example (xi,yi) has a higherscore, Scorew(x,y) = w ? ?(x,y), than alterna-tive query-segmentation pairs, (xi, zi), zi 6= yi.1 Attest time, the classifier chooses the segmentation for
x that has the highest score according to the learned
parameterization: y? = argmaxy Scorew(x,y).Unlike many problems in NLP such as parsing or
part-of-speech tagging, the small cardinality of YNmakes enumerating all the alternative query segmen-
tations computationally feasible.
In our preliminary experiments, we used a Sup-
port Vector Machine (SVM) ranker (Joachims,
2002) to learn the structured classifier.2 We also in-
1See e.g. Collins (2002) for a popular training algorithm.
2A ranking approach was also used previously by Daume? III
and Marcu (2004) for the CoNLL-99 nested noun phrase iden-
tification task.
vestigated a Hidden Markov Model SVM (Altun et
al., 2003) to label the segmentation breaks using in-
formation from past segmentation decisions. Ulti-
mately, the mappings produced by these approaches
were not as accurate as a simple formulation that
creates a full query segmentation y as the combi-
nation of independent classification decisions made
between each pair of tokens in the query.3
In the classification framework, the input is a
query, x, a position in the query, i, where 0<i<N ,
and the output is a segmentation decision yes/no.
The training set of segmented queries is converted
into examples of decisions between tokens and
learning is performed on this set. At test time, N ?1
segmentation decisions are made for the N -length
query and an output segmentation y is produced.
Here, features depend only on the input query x and
the position in the query i. For a decision at position
i, we use features from tokens up to three positions
to the left and to the right of the decision location.
That is, for a decision between xL0 and xR0, we ex-tract features from a window of six tokens in the
query: {..., xL2, xL1, xL0, xR0, xR1, xR2, ...}. Wenow detail the features derived from this window.
3.2 Features
There are a number of possible indicators of whether
a segmentation break occurs between a pair of to-
kens. Some of these features fire separately for each
token x in our feature window, while others are de-
fined over pairs or sets of tokens in the window. We
first describe the features that are defined for the to-
kens around the decision boundary, xL0 and xR0,before describing how these same features are ex-
tended to longer phrases and other token pairs.
3.2.1 Decision-boundary features
Table 1 lists the binary features that fire if partic-
ular aspects of a token or pair of tokens are present.
For example, one of the POS-tags features will fire
if the pair?s part-of-speech tags are DT JJ , another
feature will fire if the position of the pair in the to-
3The structured learners did show large gains over the clas-
sification framework on the dev-set when using only the basic
features for the decision-boundary tokens (see Section 3.2.1),
but not when the full feature set was deployed. Also, features
only available to structured learners, e.g. number of segments
in query, etc., did improve the performance of the structured
approaches, but not above that of the simpler classifier.
821
Table 1: Indicator features.
Name Description
is-the token x = ?the?
is-free token x = ?free?
POS-tags Part-of-speech tags of pair xL0 xR0fwd-pos position from beginning, i
rev-pos position from end N ? i
ken is 2, etc. The two lexical features (for when the
token is ?the? and when the token is ?free?) fire sep-
arately for the left and right tokens around the deci-
sion boundary. They are designed to add discrimi-
nation for these common query words, motivated by
examples in our training set. For example, in the
training set, ?free? often occurs in its own segment
when it?s on the left-hand-side of a decision bound-
ary (e.g. ?free? ?online? ...), but may join into a
larger segment when it?s on the right-hand-side of a
collocation (e.g. ?sulfite free? or ?sugar free?). The
classifier can use the feature weights to encourage or
discourage segmentation in these specific situations.
For statistical features, previous work (Section 2)
suggests that the mutual information between the de-
cision tokens xL0 and xR0 may be appropriate. Thelog of the pointwise mutual information (Church and
Hanks, 1989) between the decision-boundary tokens
xL0, xR0 is:
MI(xL0, xR0) = log Pr(xL0xR0)Pr(xL0)Pr(xR0)
This is equivalent to the sum: log C(xL0xR0) +
log K ? log C(xL0) ? log C(xR0). For web-basedfeatures, the counts C(.) can be taken as a search en-
gine?s count of the number of pages containing the
term. The normalizer K is thus the total number of
pages on the Internet.
Represented as a summation, we can see that pro-
viding MI as the feature effectively ties the weights
on the logarithmic counts C(xL0xR0), C(xL0), and
C(xR0). Another approach would be to providethese logarithmic counts as separate features to our
learning algorithm, which can then set the weights
optimally for segmentation. We call this set of
counts the ?Basic? features. In Section 5, we con-
firm results on our development set that showed us-
ing the basic features untied increased segmentation
Table 2: Statistical features.
Name Description
web-count count of ?x? on the web
pair-count web count ?w x?
definite web count ?the w x?
collapsed web count ?wx? (one word)
and-count web count ?w and x?
genitive web count ?w?s x?
Qcount-1 Counts of ?x? in query database
Qcounts-2 Counts of ?w x? in database
performance by up to 4% over using MI ? an impor-
tant observation for all researchers using association
models as features in their discriminative classifiers.
Furthermore, with this technique, we do not need
to normalize the counts for the other pairwise statis-
tical features given in Table 2. We can simply rely
on our learning algorithm to increase or decrease the
weights on the logarithm of the counts as needed.
To illustrate how the statistical features work,
consider a query from our development set: ?star
wars weapons guns.? The phrase ?star wars? can
easily be interpreted as a phrase; there is a high
co-occurrence count (pair-count), and many pages
where they occur as a single phrase (collapsed),
e.g. ?starwars.com.? ?Weapons? and ?guns,? on the
other hand, should not be joined together. Although
they may have a high co-occurrence count, the coor-
dination feature (and-count) is high (?weapons and
guns?) showing these to be related concepts but not
phrasal constituents. Including this novel feature re-
sulted in noticeable gains on the development set.
Since this is a query-based segmentation, features
that consider whether sets of tokens occurred else-
where in the query database may provide domain-
specific discrimination. For each of the Qcount fea-
tures, we look for two quantities: the number of
times the phrase occurs as a query on its own and the
number of times the phrase occurs within another
query.4 Including both of these counts also resulted
in performance gains on the development set.
We also extensively investigated other corpus-
based features, such as the number of times the
phrase occurred hyphenated or capitalized, and the
4We exclude counts from the training, development, and
testing queries discussed in Section 4.1.
822
corpus-based distributional similarity (Lin, 1998)
between a pair of tokens. These features are
not available from search-engine statistics because
search engines disregard punctuation and capitaliza-
tion, and collecting page-count-based distributional
similarity statistics is computationally infeasible.
Unfortunately, none of the corpus-based features
improved performance on the development set and
are thus excluded from further consideration. This
is perhaps not surprising. For such a task that in-
volves real user queries, with arbitrary spellings and
sometimes exotic vocabulary, gathering counts from
web search engines is the only way to procure reli-
able and broad-coverage statistics.
3.2.2 Context Features
Although the tokens at the decision boundary
are of paramount importance, information from the
neighbouring tokens is also critical for segmentation
decision discrimination. We thus include features
that take into consideration the preceding and fol-
lowing tokens, xL1 and xR1, as context information.We gather all the token indicator features for each of
these tokens, as well as all pairwise features between
xL1 and xL0, and then xR0 and xR1. If context to-kens are not available at this position in the query,
a feature fires to indicate this. Also, if the context
features are available, we include trigram web and
query-database counts of ?xL1 xL0 xR0? and ?xL0
xR0 xR1?, and a fourgram spanning both contexts.Furthermore, if tokens xL2 and xR2 are available, wecollect relevant token-level, pairwise, trigram, and
fourgram counts including these tokens as well.
In Section 5, we show that context features are
very important. They allow our system to implic-
itly leverage surrounding segmentation decisions,
which cannot be accessed directly in an independent
segmentation-decision classifier. For example, con-
sider the query ?bank loan amoritization schedule.?
Although ?loan amoritization? has a strong connec-
tion, we may nevertheless insert a break between
them because ?bank loan? and ?amoritization sched-
ule? each have even stronger association.
3.2.3 Dependency Features
Motivated by work in noun phrase parsing, it
might be beneficial to check if, for example, token
xL0 is more likely to modify a later token, such as
xR1. For example, in ?female bus driver?, we mightnot wish to segment ?female bus? because ?female?
has a much stronger association with ?driver? than
with ?bus?. Thus, as features, we include the pair-
wise counts between xL0 and xR1, and then xL1 and
xR0. Features from longer range dependencies didnot improve performance on the development set.
4 Experimental Setup
4.1 Data
Our dataset was taken from the AOL search query
database (Pass et al, 2006), a collection of 35
million queries submitted to the AOL search en-
gine. Most punctuation has been removed from the
queries.5 Along with the query, each entry in the
database contains an anonymous user ID and the do-
main of the URL the user clicked on, if they selected
one of the returned pages. For our data, we used only
those queries with a click-URL. This subset has a
higher proportion of correctly-spelled queries, and
facilitates annotation (described below).
We then tagged the search queries using a max-
imum entropy part-of-speech tagger (Ratnaparkhi,
1996). As our approach was designed particularly
for noun phrase queries, we selected for our final ex-
periments those AOL queries containing only deter-
miners, adjectives, and nouns. We also only consid-
ered phrases of length four or greater, since queries
of these lengths are most likely to benefit from a seg-
mentation, but our approach works for queries of any
length. Future experiments will investigate applying
the current approach to phrasal verbs, prepositional
idioms and segments with other parts of speech.
We randomly selected 500 queries for training,
500 for development, and 500 for final testing.
These were all manually segmented by our annota-
tors. Manual segmentation was done with improv-
ing search precision in mind. Annotators were asked
to analyze each query and form an idea of what the
user was searching for, taking into consideration the
click-URL or performing their own online searches,
if needed. The annotators were then asked to seg-
ment the query to improve search retrieval, by forc-
ing a search engine to find pages with the segments
5Including, unfortunately, all quotation marks, precluding
our use of users? own segmentations as additional labelled ex-
amples or feature data for our system
823
occurring as unbroken units.
One annotator segmented all three data sets, and
these were used for all the experiments. Two ad-
ditional annotators also segmented the final test set
to allow inter-annotator agreement calculation. The
pairwise agreement on segmentation decisions (be-
tween each pair of tokens) was between 84.0% and
84.6%. The agreement on entire queries was be-
tween 57.6% and 60.8%. All three agreed com-
pletely on 219 of the 500 queries, and we use this
?intersected? set for a separate evaluation in our ex-
periments.6 If we take the proportion of segmenta-
tion decisions the annotators would be expected to
agree on by chance to be 50%, the Kappa statis-
tic (Jurafsky and Martin, 2000, page 315) is around
.69, below the .8 considered to be good reliability.
This observed agreement was lower than we an-
ticipated, and reflects both differences in query in-
terpretation and in the perceived value of differ-
ent segmentations for retrieval performance. An-
notators agreed that terms like ?real estate,? ?work
force,? ?west palm beach,? and ?private investiga-
tor? should be separate segments. These are colloca-
tions in the linguistics sense (Manning and Schu?tze,
1999, pages 183-187); we cannot substitute related
words for terms in these expressions nor apply syn-
tactic transformations or paraphrases (e.g. we don?t
say ?investigator of privates?). However, for a query
such as ?bank manager,? should we exclude web
pages that discuss ?manager of the bank? or ?branch
manager for XYZ bank?? If a user is searching for a
particular webpage, excluding such results could be
harmful. However, for query substitution or expan-
sion, identifying that ?bank manager? is a single unit
may be useful. We can resolve the conflicting objec-
tives of our two motivating applications by moving
to a multi-layer query bracketing scheme, first seg-
menting unbreakable collocations and then building
them into semantic units with a query segmentation
grammar. This will be the subject of future research.
4.2 Experiments
All of our statistical feature information was col-
lected using the Google SOAP Search API.7 For
training and classifying our data, we use the popular
6All queries and statistical feature information is available
at http://www.cs.ualberta.ca/?bergsma/QuerySegmentation/
7http://code.google.com/apis/soapsearch/
Support Vector Machine (SVM) learning package
SVMlight (Joachims, 1999). SVMs are maximum-
margin classifiers that achieve good performance on
a range of tasks. In each case, we learn a linear ker-
nel on the training set segmentation decisions and
tune the parameter that trades-off training error and
margin on the development set.
We use the following two evaluation criteria:
1. Seg-Acc: Segmentation decision accuracy: the
proportion of times our classifier?s decision to
insert a segment break or not between a pair of
tokens agrees with the gold standard decision.
2. Qry-Acc: Query segmentation accuracy: the
proportion of queries for which the complete
segmentation derived from our classifications
agrees with the gold standard segmentation.
5 Results
Table 3 provides our results for various configu-
rations of features and token-combinations as de-
scribed in Section 3.8 For comparison, a baseline
that always chooses a segmentation break achieves
44.8% Seg-Acc and 4.2% Qry-Acc, while a system
that inserts no breaks achieves 55.2% Seg-Acc and
4.0% Qry-Acc. Our comparison system is the MI
approach used by Jones et al (2006), which achieves
68% Seg-Acc and 26.6% Qry-Acc (Table 3). We let
the SVM set the threshold for MI on the training set.
Note that the Basic, Decision-Boundary system
(Section 3.2.1), which uses exactly the same co-
occurrence information as the MI system (in the
form of the Basic features) but allows the SVM to
discriminatively weight the logarithmic counts, im-
mediately increases Seg-Acc performance by 3.7%.
Even more strikingly, adding the Basic count infor-
mation for the Context tokens (Section 3.2.2) boosts
performance by another 8.5%, increasing Qry-Acc
by over 22%. Smaller, further gains arise by adding
Dependency token information (Section 3.2.3).
Also, notice that moving from Basic features for
the Decision-Boundary tokens to all of our indica-
tor (Table 1) and statistical (Table 2) features (re-
ferred to as All features) increases performance from
71.7% to 84.3%. These gains convincingly justify
8Statistically significant intra-row differences in Qry-Acc
are marked with an asterix (McNemar?s test, p<0.05)
824
Table 3: Segmentation Performance (%)
Feature Type Feature Span Test Set Intersection SetSeg-Acc Qry-Acc Seg-Acc Qry-Acc
MI Decision-Boundary 68.0 26.6 73.8 34.7
Basic Decision-Boundary 71.7 29.2 77.6 39.7
Basic Decision-Boundary, Context 80.2 52.0* 85.6 62.1*
Basic Decision-Boundary, Context, Dependency 81.1 53.2 86.2 64.8
All Decision-Boundary 84.3 57.8* 86.6 63.5
All Decision-Boundary, Context 86.3 63.8* 89.2 71.7*
All Decision-Boundary, Context, Dependency 85.8 61.0 88.7 69.4
our use of an expanded feature set for this task.
Including Context with the expanded features adds
another 2%, while adding Dependency information
actually seems to hinder performance slightly, al-
though gains were seen when adding Dependency
information on the development set.
Note, however, that these results must also be
considered in light of the low inter-annotator agree-
ment (Section 4.1). Indeed, results are lower if we
evaluate using the test-set labels from another an-
notator (necessarily training on the original anno-
tator?s labels). On the intersected set of the three
annotators, however, results are better still: 88.7%
Seg-Acc and 69.4% Qry-Acc on the intersected
queries for the full-featured system (Table 3). Since
high performance is dependent on consistent train-
ing and test labellings, it seems likely that develop-
ing more-explicit annotation instructions may allow
further improvements in performance as within-set
and between-set annotation agreement increases.
It would also be theoretically interesting, and of
significant practical importance, to develop a learn-
ing approach that embraces the agreement of the
annotations as part of the learning algorithm. Our
initial ranking formulation (Section 3.1), for exam-
ple, could learn a model that prefers segmentations
with higher agreement, but still prefers any anno-
tated segmentation to alternative, unobserved struc-
tures. As there is growing interest in making max-
imal use of annotation resources within discrimina-
tive learning techniques (Zaidan et al, 2007), devel-
oping a general empirical approach to learning from
ambiguously-labelled examples would be both an
important contribution to this trend and a potentially
helpful technique in a number of NLP domains.
6 Conclusion
We have developed a novel approach to search query
segmentation and evaluated this approach on actual
user queries, reducing error by 56% over a recent
comparison approach. Gains in performance were
made possible by both leveraging recent progress in
feature engineering for noun compound bracketing,
as well as using a flexible, discriminative incorpora-
tion of association information, beyond the decision-
boundary tokens. We have created and made avail-
able a set of manually-segmented user queries, and
thus provided a new testing platform for other re-
searchers in this area. Our initial formulation of
query segmentation as a structured learning prob-
lem, and our leveraging of association statistics be-
yond the decision boundary, also provides power-
ful tools for noun compound bracketing researchers
to both move beyond three-word compounds and to
adopt discriminative feature weighting techniques.
The positive results achieved on this important ap-
plication should encourage further inter-disciplinary
collaboration between noun compound interpreta-
tion and information retrieval researchers. For ex-
ample, analysing the semantics of multiword expres-
sions may allow for more-focused query expansion;
knowing to expand ?bank manager? to include pages
describing a ?manager of the bank,? but not doing
the same for non-compositional phrases like ?real
estate? or ?private investigator,? requires exactly the
kind of techniques being developed in the noun com-
pound interpretation community. Thus for query ex-
pansion, as for query segmentation, work in natural
language processing has the potential to make a real
and immediate impact on search-engine technology.
825
The next step in this research is to directly inves-
tigate how query segmentation affects search perfor-
mance. For such an evaluation, we would need to
know, for each possible segmentation (including no
segmentation), the document retrieval performance.
This could be the proportion of returned documents
that are deemed to be relevant to the original query.
Exactly such an evaluation was recently used by Ku-
maran and Allan (2007) for the related task of query
contraction. Of course, a dataset with queries and
retrieval scores may serve for more than evaluation;
it may provide the examples used by the learning
module. That is, the parameters of the contraction
or segmentation scoring function could be discrim-
inatively set to optimize the retrieval of the training
set queries. A unified framework for query contrac-
tion, segmentation, and expansion, all based on dis-
criminatively optimizing retrieval performance, is
a very appealing future research direction. In this
framework, the size of the training sets would not
be limited by human annotation resources, but by
the number of queries for which retrieved-document
relevance judgments are available. Generating more
training examples would allow the use of more pow-
erful, finer-grained lexical features for classification.
Acknowledgments
We gratefully acknowledge support from the Natu-
ral Sciences and Engineering Research Council of
Canada, the Alberta Ingenuity Fund, the Alberta In-
genuity Center for Machine Learning, and the Al-
berta Informatics Circle of Research Excellence.
References
Yasemin Altun, Ioannis Tsochantaridis, and Thomas Hofmann.
2003. Hidden markov support vector machines. In ICML.
Kenneth Ward Church and Patrick Hanks. 1989. Word associ-
ation norms, mutual information, and lexicography. In ACL,
pages 76?83.
Michael Collins. 2002. Discriminative training methods for
hidden markov models: Theory and experiments with per-
ceptron algorithms. In EMNLP, pages 1?8.
Roxana Girju, Dan Moldovan, Marta Tatu, and Daniel Antohe.
2005. On the semantics of noun compounds. Computer
Speech and Language, 19(4):479?496.
Hal Daume? III and Daniel Marcu. 2004. NP bracketing by
maximum entropy tagging and SVM reranking. In EMNLP,
pages 254?261.
Thorsten Joachims. 1999. Making large-scale Support Vector
Machine learning practical. In B. Scho?lkopf and C. Burges,
editors, Advances in Kernel Methods: Support Vector Ma-
chines, pages 169?184. MIT-Press.
Thorsten Joachims. 2002. Optimizing search engines using
clickthrough data. In ACM Conference on Knowledge Dis-
covery and Data Mining, pages 133?142.
Rosie Jones, Benjamin Rey, Omid Madani, and Wiley Greiner.
2006. Generating query substitutions. In WWW, pages 387?
396.
Daniel Jurafsky and James H. Martin. 2000. Speech and lan-
guage processing. Prentice Hall.
Frank Keller and Mirella Lapata. 2003. Using the web to obtain
frequencies for unseen bigrams. Computational Linguistics,
29(3):459?484.
Giridhar Kumaran and James Allan. 2007. A case for shorter
queries, and helping users create them. In NAACL-HLT,
pages 220?227.
Mirella Lapata and Frank Keller. 2004. The web as a base-
line: Evaluating the performance of unsupervised web-based
models for a range of NLP tasks. In HLT-NAACL, pages
121?128.
Mark Lauer. 1995. Corpus statistics meet the noun compound:
Some empirical results. In ACL, pages 47?54.
Dekang Lin. 1998. Automatic retrieval and clustering of simi-
lar words. In COLING/ACL, pages 768?773.
Christopher D. Manning and Hinrich Schu?tze. 1999. Foun-
dations of Statistical Natural Language Processing. MIT
Press.
Preslav Nakov and Marti Hearst. 2005a. Search engine statis-
tics beyond the n-gram: Application to noun compound
bracketing. In CoNLL, pages 17?24.
Preslav Nakov and Marti Hearst. 2005b. Using the web as
an implicit training set: application to structural ambiguity
resolution. In HLT/EMNLP, pages 835?842.
Jeremy Nicholson and Timothy Baldwin. 2006. Interpretation
of compound nominalisations using corpus and web statis-
tics. In ACL Workshop on Multiword Expressions, pages
54?61.
Greg Pass, Abdur Chowdhury, and Cayley Torgeson. 2006. A
picture of search. In The First International Conference on
Scalable Information Systems.
Adwait Ratnaparkhi. 1996. A maximum entropy model for
part-of-speech tagging. In EMNLP, pages 133?142.
Knut Magne Risvik, Tomasz Mikolajewski, and Peter Boros.
2003. Query segmentation for web search. In WWW (Poster
Session).
Omar Zaidan, Jason Eisner, and Christine Piatko. 2007. Using
?annotator rationales? to improve machine learning for text
categorization. In NAACL-HLT, pages 260?267.
Chengxiang Zhai. 1997. Fast statistical parsing of noun phrases
for document indexing. In ANLP, pages 312?319.
826
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 59?68,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Discriminative Learning of Selectional Preference from Unlabeled Text
Shane Bergsma
Department of Computing Science
University of Alberta
Edmonton, Alberta
Canada, T6G 2E8
bergsma@cs.ualberta.ca
Dekang Lin
Google, Inc.
1600 Amphitheatre Parkway
Mountain View
California, 94301
lindek@google.com
Randy Goebel
Department of Computing Science
University of Alberta
Edmonton, Alberta
Canada, T6G 2E8
goebel@cs.ualberta.ca
Abstract
We present a discriminative method for learn-
ing selectional preferences from unlabeled
text. Positive examples are taken from ob-
served predicate-argument pairs, while nega-
tives are constructed from unobserved combi-
nations. We train a Support Vector Machine
classifier to distinguish the positive from the
negative instances. We show how to parti-
tion the examples for efficient training with
57 thousand features and 6.5 million training
instances. The model outperforms other re-
cent approaches, achieving excellent correla-
tion with human plausibility judgments. Com-
pared to Mutual Information, it identifies 66%
more verb-object pairs in unseen text, and re-
solves 37% more pronouns correctly in a pro-
noun resolution experiment.
1 Introduction
Selectional preferences (SPs) tell us which argu-
ments are plausible for a particular predicate. For
example, Table 2 (Section 4.4) lists plausible and
implausible direct objects (arguments) for particu-
lar verbs (predicates). SPs can help resolve syntac-
tic, word sense, and reference ambiguity (Clark and
Weir, 2002), and so gathering them has received a
lot of attention in the NLP community.
One way to determine SPs is from co-occurrences
of predicates and arguments in text. Unfortunately,
no matter how much text we use, many acceptable
pairs will be missing. Bikel (2004) found that only
1.49% of the bilexical dependencies considered by
Collins? parser during decoding were observed dur-
ing training. In our parsed corpus (Section 4.1),
for example, we find eat with nachos, burritos, and
tacos, but not with the equally tasty quesadillas,
chimichangas, or tostadas. Rather than solely re-
lying on co-occurrence counts, we would like to use
them to generalize to unseen pairs.
In particular, we would like to exploit a number
of arbitrary and potentially overlapping properties
of predicates and arguments when we assign SPs.
We do this by representing these properties as fea-
tures in a linear classifier, and training the weights
using discriminative learning. Positive examples
are taken from observed predicate-argument pairs,
while pseudo-negatives are constructed from unob-
served combinations. We train a Support Vector Ma-
chine (SVM) classifier to distinguish the positives
from the negatives. We refer to our model?s scores
as Discriminative Selectional Preference (DSP). By
creating training vectors automatically, DSP enjoys
all the advantages of supervised learning, but with-
out the need for manual annotation of examples.
We evaluate DSP on the task of assigning verb-
object selectional preference. We encode a noun?s
textual distribution as feature information. The
learned feature weights are linguistically interesting,
yielding high-quality similar-word lists as latent in-
formation. Despite its representational power, DSP
scales to real-world data sizes: examples are parti-
tioned by predicate, and a separate SVM is trained
for each partition. This allows us to efficiently learn
with over 57 thousand features and 6.5 million ex-
amples. DSP outperforms recently proposed alterna-
tives in a range of experiments, and better correlates
with human plausibility judgments. It also shows
strong gains over a Mutual Information-based co-
59
occurrence model on two tasks: identifying objects
of verbs in an unseen corpus and finding pronominal
antecedents in coreference data.
2 Related Work
Most approaches to SPs generalize from observed
predicate-argument pairs to semantically similar
ones by modeling the semantic class of the argu-
ment, following Resnik (1996). For example, we
might have a class Mexican Food and learn that the
entire class is suitable for eating. Usually, the classes
are from WordNet (Miller et al, 1990), although
they can also be inferred from clustering (Rooth et
al., 1999). Brockmann and Lapata (2003) compare
a number of WordNet-based approaches, including
Resnik (1996), Li and Abe (1998), and Clark and
Weir (2002), and found that the more sophisticated
class-based approaches do not always outperform
simple frequency-based models.
Another line of research generalizes using simi-
lar words. Suppose we are calculating the proba-
bility of a particular noun, n, occurring as the ob-
ject argument of a given verbal predicate, v. Let
Pr(n|v) be the empirical maximum-likelihood esti-
mate from observed text. Dagan et al (1999) define
the similarity-weighted probability, PrSIM, to be:
PrSIM(n|v) =
?
v??SIMS(v)
Sim(v?, v)Pr(n|v?) (1)
where Sim(v?, v) returns a real-valued similarity be-
tween two verbs v? and v (normalized over all pair
similarities in the sum). In contrast, Erk (2007)
generalizes by substituting similar arguments, while
Wang et al (2005) use the cross-product of simi-
lar pairs. One key issue is how to define the set
of similar words, SIMS(w). Erk (2007) compared a
number of techniques for creating similar-word sets
and found that both the Jaccard coefficient and Lin
(1998a)?s information-theoretic metric work best.
Similarity-smoothed models are simple to compute,
potentially adaptable to new domains, and require
no manually-compiled resources such as WordNet.
Selectional Preferences have also been a recent
focus of researchers investigating the learning of
paraphrases and inference rules (Pantel et al, 2007;
Roberto et al, 2007). Inferences such as ?[X wins
Y] ? [X plays Y]? are only valid for certain argu-
ments X and Y. We follow Pantel et al (2007) in us-
ing automatically-extracted semantic classes to help
characterize plausible arguments.
Discriminative techniques are widely used in NLP
and have been applied to the related tasks of word
prediction and language modeling. Even-Zohar and
Roth (2000) use a classifier to predict the most likely
word to fill a position in a sentence (in their ex-
periments: a verb) from a set of candidates (sets
of verbs), by inspecting the context of the target
token (e.g., the presence or absence of a particu-
lar nearby word in the sentence). This approach
can therefore learn which specific arguments occur
with a particular predicate. In comparison, our fea-
tures are second-order: we learn what kinds of argu-
ments occur with a predicate by encoding features
of the arguments. Recent distributed and latent-
variable models also represent words with feature
vectors (Bengio et al, 2003; Blitzer et al, 2005).
Many of these approaches learn both the feature
weights and the feature representation. Vectors must
be kept low-dimensional for tractability, while learn-
ing and inference on larger scales is impractical. By
partitioning our examples by predicate, we can effi-
ciently use high-dimensional, sparse vectors.
Our technique of generating negative examples
is similar to the approach of Okanohara and Tsujii
(2007). They learn a classifier to disambiguate ac-
tual sentences from pseudo-negative examples sam-
pled from an N-gram language model. Smith and
Eisner (2005) also automatically generate negative
examples. They perturb their input sequence (e.g.
the sentence word order) to create a neighborhood of
implicit negative evidence. We create negatives by
substitution rather than perturbation, and use corpus-
wide statistics to choose our negative instances.
3 Methodology
3.1 Creating Examples
To learn a discriminative model of selectional pref-
erence, we create positive and negative training ex-
amples automatically from raw text. To create the
positives, we automatically parse a large corpus, and
then extract the predicate-argument pairs that have
a statistical association in this data. We measure
this association using pointwise Mutual Information
(MI) (Church and Hanks, 1990). The MI between a
60
verb predicate, v, and its object argument, n, is:
MI(v, n) = log Pr(v, n)
Pr(v)Pr(n) = log
Pr(n|v)
Pr(n) (2)
If MI>0, the probability v and n occur together is
greater than if they were independently distributed.
We create sets of positive and negative examples
separately for each predicate, v. First, we extract all
pairs where MI(v, n)>? as positives. For each pos-
itive, we create pseudo-negative examples, (v, n?),
by pairing v with a new argument, n?, that either has
MI below the threshold or did not occur with v in the
corpus. We require each negative n? to have a similar
frequency to its corresponding n. This prevents our
learning algorithm from focusing on any accidental
frequency-based bias. We mix in K negatives for
each positive, sampling without replacement to cre-
ate all the negatives for a particular predicate. For
each v, 1K+1 of its examples will be positive. The
threshold ? represents a trade-off between capturing
a large number of positive pairs and ensuring these
pairs have good association. Similarly, K is a trade-
off between the number of examples and the com-
putational efficiency. Ultimately, these parameters
should be optimized for task performance.
Of course, some negatives will actually be plau-
sible arguments that were unobserved due to sparse-
ness. Fortunately, modern discriminative methods
like soft-margin SVMs can learn in the face of label
error by allowing slack, subject to a tunable regular-
ization penalty (Cortes and Vapnik, 1995).
If MI is a sparse and imperfect model of SP, what
can DSP gain by training on MI?s scores? We can
regard DSP as learning a view of SP that is or-
thogonal to MI, in a co-training sense (Blum and
Mitchell, 1998). MI labels the data based solely
on co-occurrence; DSP uses these labels to iden-
tify other regularities ? ones that extend beyond co-
occurring words. For example, many instances of
n where MI(eat, n)>? also have MI(buy, n)>? and
MI(cook, n)>? . Also, compared to other nouns,
a disproportionate number of eat-nouns are lower-
case, single-token words, and they rarely contain
digits, hyphens, or begin with a human first name
like Bob. DSP encodes these interdependent prop-
erties as features in a linear classifier. This classi-
fier can score any noun as a plausible argument of
eat if indicative features are present; MI can only
assign high plausibility to observed (eat,n) pairs.
Similarity-smoothed models can make use of the
regularities across similar verbs, but not the finer-
grained string- and token-based features.
Our training examples are similar to the data cre-
ated for pseudodisambiguation, the usual evalua-
tion task for SP models (Erk, 2007; Keller and La-
pata, 2003; Rooth et al, 1999). This data con-
sists of triples (v, n, n?) where v, n is a predicate-
argument pair observed in the corpus and v, n? has
not been observed. The models score correctly
if they rank observed (and thus plausible) argu-
ments above corresponding unobserved (and thus
likely implausible) ones. We refer to this as Pair-
wise Disambiguation. Unlike this task, we classify
each predicate-argument pair independently as plau-
sible/implausible. We also use MI rather than fre-
quency to define the positive pairs, ensuring that the
positive pairs truly have a statistical association, and
are not simply the result of parser error or noise.1
3.2 Partitioning for Efficient Training
After creating our positive and negative training
pairs, we must select a feature representation for our
examples. Let ? be a mapping from a predicate-
argument pair (v, n) to a feature vector, ? :
(v, n) ? ??1...?k?. Predictions are made based
on a weighted combination of the features, y =
? ??(v, n), where ? is our learned weight vector.
We can make training significantly more efficient
by using a special form of attribute-value features.
Let every feature ?i be of the form ?i(v, n) = ?v =
v??f(n)?. That is, every feature is an intersection of
the occurrence of a particular predicate, v?, and some
feature of the argument f(n). For example, a fea-
ture for a verb-object pair might be, ?the verb is eat
and the object is lower-case.? In this representation,
features for one predicate will be completely inde-
pendent from those for every other predicate. Thus
rather than a single training procedure, we can actu-
ally partition the examples by predicate, and train a
1For a fixed verb, MI is proportional to Keller and Lapata
(2003)?s conditional probability scores for pseudodisambigua-
tion of (v, n, n?) triples: Pr(v|n) = Pr(v, n)/Pr(n), which was
shown to be a better measure of association than co-occurrence
frequency f(v, n). Normalizing by Pr(v) (yielding MI) allows
us to use a constant threshold across all verbs. MI was also
recently used for inference-rule SPs by Pantel et al (2007).
61
classifier for each predicate independently. The pre-
diction becomes yv = ?v ??v(n), where ?v are the
learned weights corresponding to predicate v and all
features ?v(n)=f(n) depend on the argument only.
Some predicate partitions may have insufficient
examples for training. Also, a predicate may oc-
cur in test data that was unseen during training. To
handle these instances, we decided to cluster low-
frequency predicates. In our experiments assigning
SP to verb-object pairs, we cluster all verbs that have
less than 250 positive examples, using clusters gen-
erated by the CBC algorithm (Pantel and Lin, 2002).
For example, the low-frequency verbs incarcerate,
parole, and court-martial are all mapped to the same
partition, while more-frequent verbs like arrest and
execute each have their own partition. About 5.5%
of examples are clustered, corresponding to 30% of
the 7367 total verbs. 40% of verbs (but only 0.6% of
examples) were not in any CBC cluster; these were
mapped to a single backoff partition.
The parameters for each partition, ?v, can be
trained with any supervised learning technique. We
use SVM (Section 4.1) because it is effective in simi-
lar high-dimensional, sparse-vector settings, and has
an efficient implementation (Joachims, 1999). In
SVM, the sign of yv gives the classification. We can
also use the scalar yv as our DSP score (i.e. the posi-
tive distance from the separating SVM hyperplane).
3.3 Features
This section details our argument features, f(n), for
assigning verb-object selectional preference. For a
verb predicate (or partition) v and object argument
n, the form of our classifier is yv = ?i ?vi fi(n).
3.3.1 Verb co-occurrence
We provide features for the empirical probability
of the noun occurring as the object argument of other
verbs, Pr(n|v?). If we were to only use these features
(indexing the feature weights by each verb v?), the
form of our classifier would be:
yv =
?
v?
?vv?Pr(n|v?) (3)
Note the similarity between Equation (3) and Equa-
tion (1). Now the feature weights, ?vv? , take the role
of the similarity function, Sim(v?, v). Unlike Equa-
tion (1), however, these weights are not set by an
external similarity algorithm, but are optimized to
discriminate the positive and negative training ex-
amples. We need not restrict ourselves to a short list
of similar verbs; we include Probj(n|v?) features for
every verb that occurs more than 10 times in our cor-
pus. ?vv? may be positive or negative, depending on
the relation between v? and v. We also include fea-
tures for the probability of the noun occurring as the
subject of other verbs, Prsubj(n|v?). For example,
nouns that can be the object of eat will also occur as
the subject of taste and contain. Other contexts, such
as adjectival and nominal predicates, could also aid
the prediction, but have not yet been investigated.
The advantage of tuning similarity to the appli-
cation of interest has been shown previously by
Weeds and Weir (2005). They optimize a few meta-
parameters separately for the tasks of thesaurus gen-
eration and pseudodisambiguation. Our approach,
on the other hand, discriminatively sets millions of
individual similarity values. Like Weeds and Weir
(2005), our similarity values are asymmetric.
3.3.2 String-based
We include several simple character-based fea-
tures of the noun string: the number of tokens, the
case, and whether it contains digits, hyphens, an
apostrophe, or other punctuation. We also include a
feature for the first and last token, and fire indicator
features if any token in the noun occurs on in-house
lists of given names, family names, cities, provinces,
countries, corporations, languages, etc. We also fire
a feature if a token is a corporate designation (like
inc. or ltd.) or a human one (like Mr. or Sheik).
3.3.3 Semantic classes
Motivated by previous SP models that make use
of semantic classes, we generated word clusters us-
ing CBC (Pantel and Lin, 2002) on a 10 GB corpus,
giving 3620 clusters. If a noun belongs in a cluster,
a corresponding feature fires. If a noun is in none of
the clusters, a no-class feature fires.
As an example, CBC cluster 1891 contains:
sidewalk, driveway, roadway, footpath,
bridge, highway, road, runway, street, alley,
path, Interstate, . . .
In our training data, we have examples like widen
highway, widen road and widen motorway. If we
62
see that we can widen a highway, we learn that we
can also widen a sidewalk, bridge, runway, etc.
We also made use of the person-name/instance
pairs automatically extracted by Fleischman et al
(2003).2 This data provides counts for pairs such
as ?Edwin Moses, hurdler? and ?William Farley, in-
dustrialist.? We have features for all concepts and
therefore learn their association with each verb.
4 Experiments and Results
4.1 Set up
We parsed the 3 GB AQUAINT corpus (Voorhees,
2002) using Minipar (Lin, 1998b), and collected
verb-object and verb-subject frequencies, building
an empirical MI model from this data. Verbs and
nouns were converted to their (possibly multi-token)
root, and string case was preserved. Passive sub-
jects (the car was bought) were converted to objects
(bought car). We set the MI-threshold, ? , to be 0,
and the negative-to-positive ratio, K, to be 2.
Numerous previous pseudodisambiguation evalu-
ations only include arguments that occur between 30
and 3000 times (Erk, 2007; Keller and Lapata, 2003;
Rooth et al, 1999). Presumably the lower bound is
to help ensure the negative argument is unobserved
because it is unsuitable, not because of data sparse-
ness. We wish to use our model on arguments of
any frequency, including those that never occurred
in the training corpus (and therefore have empty co-
occurrence features (Section 3.3.1)). We proceed as
follows: first, we exclude pairs whenever the noun
occurs less than 3 times in our corpus, removing
many misspellings and other noun noise. Next, we
omit verb co-occurrence features for nouns that oc-
cur less than 10 times, and instead fire a low-count
feature. When we move to a new corpus, previously-
unseen nouns are treated like these low-count train-
ing nouns.
This processing results in a set of 6.8 million
pairs, divided into 2318 partitions (192 of which
are verb clusters (Section 3.2)). For each parti-
tion, we take 95% of the examples for training,
2.5% for development and 2.5% for a final unseen
test set. We provide full results for two models:
DSPcooc which only uses the verb co-occurrence fea-
tures, and DSPall which uses all the features men-
2Available at http://www.mit.edu/?mbf/instances.txt.gz
tioned in Section 3.3. Feature values are normalized
within each feature type. We train our (linear kernel)
discriminative models using SVMlight (Joachims,
1999) on each partition, but set meta-parameters C
(regularization) and j (cost of positive vs. nega-
tive misclassifications: max at j=2) on the macro-
averaged score across all development partitions.
Note that we can not use the development set to op-
timize ? and K because the development examples
are obtained after setting these values.
4.2 Feature weights
It is interesting to inspect the feature weights re-
turned by our system. In particular, the weights
on the verb co-occurrence features (Section 3.3.1)
provide a high-quality, argument-specific similarity-
ranking of other verb contexts. The DSP parameters
for eat, for example, place high weight on features
like Pr(n|braise), Pr(n|ration), and Pr(n|garnish).
Lin (1998a)?s similar word list for eat misses these
but includes sleep (ranked 6) and sit (ranked 14), be-
cause these have similar subjects to eat. Discrimina-
tive, context-specific training seems to yield a bet-
ter set of similar predicates, e.g. the highest-ranked
contexts for DSPcooc on the verb join,3
lead 1.42, rejoin 1.39, form 1.34, belong to
1.31, found 1.31, quit 1.29, guide 1.19, induct
1.19, launch (subj) 1.18, work at 1.14
give a better SIMS(join) for Equation (1) than the
top similarities returned by (Lin, 1998a):
participate 0.164, lead 0.150, return to 0.148,
say 0.143, rejoin 0.142, sign 0.142, meet
0.142, include 0.141, leave 0.140, work 0.137
Other features are also weighted intuitively. Note
that case is a strong indicator for some arguments,
for example the weight on being lower-case is high
for become (0.972) and eat (0.505), but highly nega-
tive for accuse (-0.675) and embroil (-0.573) which
often take names of people and organizations.
4.3 Pseudodisambiguation
We first evaluate DSP on disambiguating posi-
tives from pseudo-negatives, comparing to recently-
3Which all correspond to nouns occurring in the object po-
sition of the verb (e.g. Probj(n|lead)), except ?launch (subj)?
which corresponds to Prsubj(n|launch).
63
System MacroAvg MicroAvg PairwiseP R F P R F Acc Cov
Dagan et al (1999) 0.36 0.90 0.51 0.68 0.92 0.78 0.58 0.98
Erk (2007) 0.49 0.66 0.56 0.70 0.82 0.76 0.72 0.83
Keller and Lapata (2003) 0.72 0.34 0.46 0.80 0.50 0.62 0.80 0.57
DSPcooc 0.53 0.72 0.61 0.73 0.94 0.82 0.77 1.00
DSPall 0.60 0.71 0.65 0.77 0.90 0.83 0.81 1.00
Table 1: Pseudodisambiguation results averaged across each example (MacroAvg), weighted by word frequency (Mi-
croAvg), plus coverage and accuracy of pairwise competition (Pairwise).
proposed systems that also require no manually-
compiled resources like WordNet. We convert Da-
gan et al (1999)?s similarity-smoothed probability
to MI by replacing the empirical Pr(n|v) in Equa-
tion (2) with the smoothed PrSIM from Equation (1).
We also test an MI model inspired by Erk (2007):
MISIM(n, v) = log
?
n??SIMS(n)
Sim(n?, n) Pr(v, n
?)
Pr(v)Pr(n?)
We gather similar words using Lin (1998a), mining
similar verbs from a comparable-sized parsed cor-
pus, and collecting similar nouns from a broader 10
GB corpus of English text.4
We also use Keller and Lapata (2003)?s approach
to obtaining web-counts. Rather than mining parse
trees, this technique retrieves counts for the pattern
?V Det N? in raw online text, where V is any in-
flection of the verb, Det is the, a, or the empty
string, and N is the singular or plural form of the
noun. We compute a web-based MI by collecting
Pr(n, v), Pr(n), and Pr(v) using all inflections, ex-
cept we only use the root form of the noun. Rather
than using a search engine, we obtain counts from
the Google Web 5-gram Corpus.5
All systems are thresholded at zero to make a clas-
sification. Unlike DSP, the comparison systems may
4For both the similar-noun and similar-verb smoothing, we
only smooth over similar pairs that occurred in the corpus.
While averaging over all similar pairs tends to underestimate
the probability, averaging over only the observed pairs tends to
overestimate it. We tested both and adopt the latter because it
resulted in better performance on our development set.
5Available from the LDC as LDC2006T13. This collection
was generated from approximately 1 trillion tokens of online
text. Unfortunately, tokens appearing less than 200 times have
been mapped to the ?UNK? symbol, and only N-grams appear-
ing more than 40 times are included. Unlike results from search
engines, however, experiments with this corpus are replicable.
not be able to provide a score for each example.
The similarity-smoothed examples will be undefined
if SIMS(w) is empty. Also, the Keller and Lapata
(2003) approach will be undefined if the pair is un-
observed on the web. As a reasonable default for
these cases, we assign them a negative decision.
We evaluate disambiguation using precision (P),
recall (R), and their harmonic mean, F-Score (F).
Table 1 gives the results of our comparison. In the
MacroAvg results, we weight each example equally.
For MicroAvg, we weight each example by the fre-
quency of the noun. To more directly compare with
previous work, we also reproduced Pairwise Disam-
biguation by randomly pairing each positive with
one of the negatives and then evaluating each system
by the percentage it ranks correctly (Acc). For the
comparison approaches, if one score is undefined,
we choose the other one. If both are undefined, we
abstain from a decision. Coverage (Cov) is the per-
cent of pairs where a decision was made.6
Our simple system with only verb co-occurrence
features, DSPcooc, outperforms all comparison ap-
proaches. Using the richer feature set in DSPall
results in a statistically significant gain in perfor-
mance, up to an F-Score of 0.65 and a pairwise
disambiguation accuracy of 0.81.7 DSPall has both
broader coverage and better accuracy than all com-
peting approaches. In the remainder of the experi-
ments, we use DSPall and refer to it simply as DSP.
Some errors are because of plausible but unseen
arguments being used as test-set pseudo-negatives.
For example, for the verb damage, DSP?s three most
high-scoring false positives are the nouns jetliner,
carpet, and gear. While none occur with damage in
6I.e. we use the ?half coverage? condition from Erk (2007).
7The differences between DSPall and all comparison sys-
tems are statistically significant (McNemar?s test, p<0.01).
64
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 10  100  1000  10000  100000  1e+06
F-
Sc
or
e
Noun Frequency
DSPall
Erk (2007)
Keller and Lapata (2003)
Figure 1: Disambiguation results by noun frequency.
our corpus, all intuitively satisfy the verb?s SPs.
MacroAvg performance is worse than MicroAvg
because all systems perform better on frequent
nouns. When we plot F-Score by noun frequency
(Figure 1), we see that DSP outperforms comparison
approaches across all frequencies, but achieves its
biggest gains on the low-frequency nouns. A richer
feature set alows DSP to make correct inferences on
examples that provide minimal co-occurrence data.
These are also the examples for which we would ex-
pect co-occurrence models like MI to fail.
As a further experiment, we re-trained DSP but
with only the string-based features removed. Overall
macro-averaged F-score dropped from 0.65 to 0.64
(a statistically significant reduction in performance).
The system scored nearly identically to DSP on the
high-frequency nouns, but performed roughly 15%
worse on the nouns that occurred less than ten times.
This shows that the string-based features are impor-
tant for selectional preference, and particularly help-
ful for low-frequency nouns.
4.4 Human Plausibility
Table 2 compares some of our systems on data used
by Resnik (1996) (also Appendix 2 in Holmes et al
(1989)). The plausibility of these pairs was initially
judged based on the experimenters? intuitions, and
later confirmed in a human experiment. We include
the scores of Resnik?s system, and note that its errors
were attributed to sense ambiguity and other limi-
tations of class-based approaches (Resnik, 1996).8
8For example, warn-engine scores highly because engines
are in the class entity, and physical entities (e.g. people) are
often objects of warn. Unlike DSP, Resnik?s approach cannot
learn that for warn, ?the property of being a person is more
Seen Criteria Unseen Verb-Object Freq.All = 1 = 2 = 3 > 3
MI > 0 0.44 0.33 0.57 0.70 0.82
Freq. > 0 0.57 0.45 0.76 0.89 0.96
DSP > 0 0.73 0.69 0.80 0.85 0.88
Table 3: Recall on identification of Verb-Object pairs
from an unseen corpus (divided by pair frequency).
The other comparison approaches also make a num-
ber of mistakes, which can often be traced to a mis-
guided choice of similar word to smooth with.
We also compare to our empirical MI model,
trained on our parsed corpus. Although Resnik
(1996) reported that 10 of the 16 plausible pairs did
not occur in his training corpus, all of them occurred
in ours and hence MI gives very reasonable scores
on the plausible objects. It has no statistics, however,
for many of the implausible ones. DSP can make
finer decisions than MI, recognizing that ?warning
an engine? is more absurd than ?judging a climate.?
4.5 Unseen Verb-Object Identification
We next compare MI and DSP on a much larger set
of plausible examples, and also test how well the
models generalize across data sets. We took the MI
and DSP systems trained on AQUAINT and asked
them to rate observed (and thus likely plausible)
verb-object pairs taken from an unseen corpus. We
extracted the pairs by parsing the San Jose Mercury
News (SJM) section of the TIPSTER corpus (Har-
man, 1992). Each unique verb-object pair is a single
instance in this evaluation.
Table 3 gives recall across all pairs (All) and
grouped by pair-frequency in the unseen corpus (1,
2, 3, >3). DSP accepts far more pairs than MI
(73% vs. 44%), even far more than a system that
accepts any previously observed verb-object combi-
nation as plausible (57%). Recall is higher on more
frequent verb-object pairs, but 70% of the pairs oc-
curred only once in the corpus. Even if we smooth
MI by smoothing Pr(n|v) in Equation 2 using modi-
fied KN-smoothing (Chen and Goodman, 1998), the
recall of MI>0 on SJM only increases from 44.1%
to 44.9%, still far below DSP. Frequency-based
models have fundamentally low coverage. As fur-
important than the property of being an entity? (Resnik, 1996).
65
Verb Plaus./Implaus. Resnik Dagan et al Erk MI DSP
see friend/method 5.79/-0.01 0.20/1.40* 0.46/-0.07 1.11/-0.57 0.98/0.02
read article/fashion 6.80/-0.20 3.00/0.11 3.80/1.90 4.00/? 2.12/-0.65
find label/fever 1.10/0.22 1.50/2.20* 0.59/0.01 0.42/0.07 1.61/0.81
hear story/issue 1.89/1.89* 0.66/1.50* 2.00/2.60* 2.99/-1.03 1.66/0.67
write letter/market 7.26/0.00 2.50/-0.43 3.60/-0.24 5.06/-4.12 3.08/-1.31
urge daughter/contrast 1.14/1.86* 0.14/1.60* 1.10/3.60* -0.95/? -0.34/-0.62
warn driver/engine 4.73/3.61 1.20/0.05 2.30/0.62 2.87/? 2.00/-0.99
judge contest/climate 1.30/0.28 1.50/1.90* 1.70/1.70* 3.90/? 1.00/0.51
teach language/distance 1.87/1.86 2.50/1.30 3.60/2.70 3.53/? 1.86/0.19
show sample/travel 1.44/0.41 1.60/0.14 0.40/-0.82 0.53/-0.49 1.00/-0.83
expect visit/mouth 0.59/5.93* 1.40/1.50* 1.40/0.37 1.05/-0.65 1.44/-0.15
answer request/tragedy 4.49/3.88 2.70/1.50 3.10/-0.64 2.93/? 1.00/0.01
recognize author/pocket 0.50/0.50* 0.03/0.37* 0.77/1.30* 0.48/? 1.00/0.00
repeat comment/journal 1.23/1.23* 2.30/1.40 2.90/? 2.59/? 1.00/-0.48
understand concept/session 1.52/1.51 2.70/0.25 2.00/-0.28 3.96/? 2.23/-0.46
remember reply/smoke 1.31/0.20 2.10/1.20 0.54/2.60* 1.13/-0.06 1.00/-0.42
Table 2: Selectional ratings for plausible/implausible direct objects (Holmes et al, 1989). Mistakes are marked with
an asterisk (*), undefined scores are marked with a dash (?). Only DSP is completely defined and completely correct.
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0  0.2  0.4  0.6  0.8  1
In
te
rp
ol
at
ed
 P
re
cis
io
n
Recall
DSP>T
MI>T
DSP>0
MI>0
Figure 2: Pronoun resolution precision-recall on MUC.
ther evidence, if we build a model of MI on the SJM
corpus and use it in our pseudodisambiguation ex-
periment (Section 4.3), MI>0 gets a MacroAvg pre-
cision of 86% but a MacroAvg recall of only 12%.9
4.6 Pronoun Resolution
Finally, we evaluate DSP on a common application
of selectional preferences: choosing the correct an-
tecedent for pronouns in text (Dagan and Itai, 1990;
Kehler et al, 2004). We study the cases where a
9Recall that even the Keller and Lapata (2003) system, built
on the world?s largest corpus, achieves only 34% recall (Table 1)
(with only 48% of positives and 27% of all pairs previously
observed, but see Footnote 5).
pronoun is the direct object of a verb predicate, v. A
pronoun?s antecedent must obey v?s selectional pref-
erences. If we have a better model of SP, we should
be able to better select pronoun antecedents.
We parsed the MUC-7 (1997) coreference corpus
and extracted all pronouns in a direct object rela-
tion. For each pronoun, p, modified by a verb, v, we
extracted all preceding nouns within the current or
previous sentence. Thirty-nine anaphoric pronouns
had an antecedent in this window and are used in
the evaluation. For each p, let N(p)+ by the set of
preceding nouns coreferent with p, and let N(p)?
be the remaining non-coreferent nouns. We take
all (v, n+) where n+ ? N(p)+ as positive, and all
other pairs (v, n?), n? ? N(p)? as negative.
We compare MI and DSP on this set, classifying
every (v, n) with MI>T (or DSP>T ) as positive.
By varying T , we get a precision-recall curve (Fig-
ure 2). Precision is low because, of course, there
are many nouns that satisfy the predicate?s SPs that
are not coreferent. DSP>0 has both a higher recall
and higher precision than accepting every pair pre-
viously seen in text (the right-most point on MI>T ).
The DSP>T system achieves higher precision than
MI>T for points where recall is greater than 60%
(where MI<0). Interestingly, the recall of MI>0 is
66
System Acc
Most-Recent Noun 17.9%
Maximum MI 28.2%
Maximum DSP 38.5%
Table 4: Pronoun resolution accuracy on nouns in current
or previous sentence in MUC.
higher here than it is for general verb-objects (Sec-
tion 4.5). On the subset of pairs with strong empir-
ical association (MI>0), MI generally outperforms
DSP at equivalent recall values.
We next compare MI and DSP as stand-alone pro-
noun resolution systems (Table 4). As a standard
baseline, for each pronoun, we choose the most
recent noun in text as the pronoun?s antecedent,
achieving 17.9% resolution accuracy. This baseline
is quite low because many of the most-recent nouns
are subjects of the pronoun?s verb phrase, and there-
fore resolution violates syntactic coreference con-
straints. If instead we choose the previous noun with
the highest MI as antecedent, we get an accuracy of
28.2%, while choosing the previous noun with the
highest DSP achieves 38.5%. DSP resolves 37%
more pronouns correctly than MI. We leave as fu-
ture work a full-scale pronoun resolution system that
incorporates both MI and DSP as backed-off, inter-
polated, or separate semantic features.
5 Conclusions and Future Work
We have presented a simple, effective model of se-
lectional preference based on discriminative train-
ing. Supervised techniques typically achieve higher
performance than unsupervised models, and we du-
plicate these gains with DSP. Here, however, these
gains come at no additional labeling cost, as train-
ing examples are generated automatically from un-
labeled text. DSP allows an arbitrary combination of
features, including verb co-occurrence features that
yield high-quality similar-word lists as latent output.
This work only scratches the surface of possible fea-
ture mining; information from WordNet relations,
Wikipedia categories, or parallel corpora could also
provide valuable clues to SP. Also, if any other sys-
tem were to exceed DSP?s performance, it could also
be included as one of DSP?s features.
It would be interesting to expand our co-
occurrence features, including co-occurrence counts
across more grammatical relations and using counts
from external, unparsed corpora like the world wide
web. We could also reverse the role of noun and verb
in our training, having verb-specific features and
discriminating separately for each argument noun.
The latent information would then be lists of similar
nouns.
Finally, note that while we focused on word-word
co-occurrences, sense-sense SPs can also be learned
with our algorithm. If our training corpus was sense-
labeled, we could run our algorithm over the senses
rather than the words. The resulting model would
then require sense-tagged input if it were to be used
within an application like parsing or coreference res-
olution. Also, like other models of SP, our technique
can also be used for sense disambiguations: the
weightings on our semantic class features indicate,
for a particular noun, which of its senses (classes) is
most compatible with each verb.
Acknowledgments
We gratefully acknowledge support from the Natu-
ral Sciences and Engineering Research Council of
Canada, the Alberta Ingenuity Fund, and the Alberta
Informatics Circle of Research Excellence.
References
Yoshua Bengio, Re?jean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Research,
3:1137?1155.
Daniel M. Bikel. 2004. Intricacies of Collins? parsing
model. Computational Linguistics, 30(4):479?511.
John Blitzer, Amir Globerson, and Fernando Pereira.
2005. Distributed latent variable models of lexical co-
occurrences. In AISTATS.
Avrim Blum and Tom Mitchell. 1998. Combining la-
beled and unlabeled data with co-training. In Proceed-
ings of COLT, pages 92?100.
Carsten Brockmann and Mirella Lapata. 2003. Evalu-
ating and combining approaches to selectional prefer-
ence acquisition. In EACL, pages 27?34.
Stanley F. Chen and Joshua Goodman. 1998. An empir-
ical study of smoothing techniques for language mod-
eling. TR-10-98, Harvard University.
Kenneth Ward Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicogra-
phy. Computational Linguistics, 16(1):22?29.
67
Stephen Clark and David Weir. 2002. Class-based prob-
ability estimation using a semantic hierarchy. Compu-
tational Linguistics, 28(2):187?206.
Corinna Cortes and Vladimir Vapnik. 1995. Support-
vector networks. Machine Learning, 20(3):273?297.
Ido Dagan and Alan Itai. 1990. Automatic processing of
large corpora for the resolution of anaphora references.
In COLING, volume 3, pages 330?332.
Ido Dagan, Lillian Lee, and Fernando C. N. Pereira.
1999. Similarity-based models of word cooccurrence
probabilities. Machine Learning, 34(1-3):43?69.
Katrin Erk. 2007. A simple, similarity-based model for
selectional preference. In ACL, pages 216?223.
Yair Even-Zohar and Dan Roth. 2000. A classification
approach to word prediction. In NAACL, pages 124?
131.
Michael Fleischman, Eduard Hovy, and Abdessamad
Echihabi. 2003. Offline strategies for online question
answering: answering questions before they are asked.
In ACL, pages 1?7.
Donna Harman. 1992. The DARPA TIPSTER project.
ACM SIGIR Forum, 26(2):26?28.
Virginia M. Holmes, Laurie Stowe, and Linda Cupples.
1989. Lexical expectations in parsing complement-
verb sentences. Journal of Memory and Language,
28:668?689.
Thorsten Joachims. 1999. Making large-scale Support
Vector Machine learning practical. In B. Scho?lkopf
and C. Burges, editors, Advances in Kernel Methods:
Support Vector Machines, pages 169?184. MIT-Press.
Andrew Kehler, Douglas Appelt, Lara Taylor, and Alek-
sandr Simma. 2004. The (non)utility of predicate-
argument frequencies for pronoun interpretation. In
HLT/NAACL, pages 289?296.
Frank Keller and Mirella Lapata. 2003. Using the web to
obtain frequencies for unseen bigrams. Computational
Linguistics, 29(3):459?484.
Hang Li and Naoki Abe. 1998. Generalizing case frames
using a thesaurus and the MDL principle. Computa-
tional Linguistics, 24(2):217?244.
Dekang Lin. 1998a. Automatic retrieval and clustering
of similar words. In COLING-ACL, pages 768?773.
Dekang Lin. 1998b. Dependency-based evaluation of
MINIPAR. In LREC Workshop on the Evaluation of
Parsing Systems.
George A. Miller, Richard Beckwith, Christiane Fell-
baum, Derek Gross, and Katherine J. Miller. 1990.
Introduction to WordNet: an on-line lexical database.
International Journal of Lexicography, 3(4):235?244.
MUC-7. 1997. Coreference task definition (v3.0, 13 Jul
97). In Proceedings of the Seventh Message Under-
standing Conference (MUC-7).
Daisuke Okanohara and Jun?ichi Tsujii. 2007. A
discriminative language model with pseudo-negative
samples. In ACL, pages 73?80.
Patrick Pantel and Dekang Lin. 2002. Discovering word
senses from text. In KDD, pages 613?619.
Patrick Pantel, Rahul Bhagat, Bonaventura Coppola,
Timothy Chklovski, and Eduard Hovy. 2007. ISP:
Learning inferential selectional preferences. In
NAACL-HLT, pages 564?571.
Philip Resnik. 1996. Selectional constraints: An
information-theoretic model and its computational re-
alization. Cognition, 61:127?159.
Basili Roberto, Diego De Cao, Paolo Marocco, and
Marco Pennacchiotti. 2007. Learning selectional
preferences for entailment or paraphrasing rules. In
RANLP.
Mats Rooth, Stefan Riezler, Detlef Prescher, Glenn Car-
roll, and Franz Beil. 1999. Inducing a semantically
annotated lexicon via EM-based clustering. In ACL,
pages 104?111.
Noah A. Smith and Jason Eisner. 2005. Contrastive esti-
mation: training log-linear models on unlabeled data.
In ACL, pages 354?362.
Ellen Voorhees. 2002. Overview of the TREC 2002
question answering track. In Proceedings of the
Eleventh Text REtrieval Conference (TREC).
Qin Iris Wang, Dale Schuurmans, and Dekang Lin. 2005.
Strictly lexical dependency parsing. In International
Workshop on Parsing Technologies, pages 152?159.
Julie Weeds and David Weir. 2005. Co-occurrence re-
trieval: a flexible framework for lexical distributional
similarity. Computational Linguistics, 31(4):439?475.
68
Proceedings of NAACL HLT 2007, pages 516?523,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Automatic Answer Typing for How-Questions
Christopher Pinchak and Shane Bergsma
Department of Computing Science
University of Alberta
Edmonton, Alberta, T6G 2E8, Canada
 
pinchak,bergsma  @cs.ualberta.ca
Abstract
We introduce an answer typing strategy
specific to quantifiable how questions. Us-
ing the web as a data source, we auto-
matically collect answer units appropri-
ate to a given how-question type. Exper-
imental results show answer typing with
these units outperforms traditional fixed-
category answer typing and other strate-
gies based on the occurrences of numeri-
cal entities in text.
1 Introduction
Question answering (QA) systems are emerging as
a viable means of obtaining specific information in
the face of large availability. Answer typing is an
important part of QA because it allows the system
to greatly reduce the number of potential answers,
using general knowledge of the answer form for a
specific question. For example, for what, where, and
who questions like ?What is the capital of Canada??,
answer typing can filter the phrases which might be
proposed as candidate answers, perhaps only identi-
fying those textual entities known to be cities.
We focus on answer typing for how-questions, a
subset of questions which have received little spe-
cific attention in the QA community. Rather than
seeking an open-ended noun or verb phrase, how-
questions often seek a numerical measurement ex-
pressed in terms of a certain kind of unit, as in the
following example:
Example 1: ?How heavy is a grizzly bear??
An answer typing system might expect answers to
include units like kilograms, pounds, or tons. Enti-
ties with inappropriate units, such as feet, meters, or
honey pots, would be excluded as candidate answers.
We specifically handle the subset of how-
questions that we call how-adjective questions; that
is, questions of the form ?How adjective...?? such
as Example 1. In particular, we do not address ?how
many? questions, which usually specify the units di-
rectly following many, nor ?how much? questions,
which generally seek a monetary value.
Hand-crafting a comprehensive list of units ap-
propriate to many different adjectives is time-
consuming and likely to miss important units. For
example, an annotator might miss gigabytes for a
measure of ?how large.? Instead of compiling a list
manually, we propose a means of automatically gen-
erating lists of appropriate units for a number of real-
world questions.
How-adjective questions represent a significant
portion of queries sent to search engines; of the
35 million queries in the AOL search query data
set (Pass et al, 2006), over 11,000 are of the form
?how adjective...? ? close to one in every three thou-
sand queries. Of those 11,000 queries, 152 different
adjectives are used, ranging from the expected ?how
old? and ?how far? to the obscure ?how orwellian.?
This high proportion of queries is especially strik-
ing given that search engines provide little sup-
port for answering how-adjective questions. Indeed,
most IR systems work by keyword matching. En-
tering Example 1 into a search engine returns doc-
uments discussing the grizzly?s ?heavy fur,? ?heavy,
shaggy coat? and ?heavy stout body.? When faced
516
with such results, a smart search engine user knows
to inject answer units into their query to refine their
search, perhaps querying ?grizzly pounds.? They
may also convert their adjective (heavy) to a related
concept (weight), for the query ?grizzly weight.?
Similarly, our approach discovers unit types by
first converting the adjective to a related concept, us-
ing information in a structured ontology. For exam-
ple, ?big? can be used to obtain ?size,? and ?tall? can
derive ?height.? We then use an online search engine
to automatically find units appropriate to the con-
cept, given the assumption that the concept is explic-
itly measured in terms of specific units, e.g., height
can be measured in feet, weight can be measured in
pounds, and size can be measured in gigabytes.
By automatically extracting units, we do not re-
quire a set of prior questions with associated an-
swers. Instead, we use actual questions as a source
of realistic adjectives only. This is important be-
cause while large sets of existing questions can be
obtained (Li and Roth, 2002), there are many fewer
questions with available answers.
Our experiments demonstrate that how-question-
specific unit lists consistently achieve higher answer
identification performance than fixed-type, general-
purpose answer typing (which propose all numeri-
cal entities as answer candidates). Furthermore, our
precomputed, automatically-generated unit lists are
shown to consistently achieve better performance
than baseline systems which derive unit lists at run-
time from documents relevant to the answer query,
even when such documents are gathered using per-
fect knowledge of the answer distribution.
The outline of the paper is as follows. In Section 2
we outline related work. In Section 3 we provide the
framework of our answer-typing model. Section 4
describes the implementation details of the model.
Section 5 describes our experimental methodology,
while Section 6 shows the benefits of using auto-
matic how-question answer-typing. We conclude
with possible directions of future research opened
by this novel problem formulation.
2 Previous Work
Answer typing is an important component of any
QA system, but varies greatly in the approach
taken (Prager et al, 2003; Harabagiu et al, 2005).
Basically, answer typing provides a means of filter-
ing answer candidates as either appropriate or in-
appropriate to the question. For example, Li and
Roth (2002) assign one of fifty possible types to a
question based on features present in the question.
Answer candidates can then be selected from text
by finding entities whose type matches that of the
input question. Similarly, Ittycheriah et al (2000)
assign one of the MUC named-entity types to each
input question. In these fixed-category approaches,
how-questions are assigned a fixed type in the same
manner as other questions. For how-questions, this
corresponds to a numerical type. However, retriev-
ing all numerical entities will provide lower answer
identification precision than a system that only pro-
vides those specified with the expected answer units.
Pinchak and Lin (2006) propose a dynamic an-
swer typing system which computes a unique score
for the appropriateness of any word to a particu-
lar question. Unfortunately, their question context-
mapping is limited to what, where, and who ques-
tions, and thus is not defined for how-questions.
Wu et al (2005) handle how-questions differently
than other questions. They use special hand-crafted
rules to assign a particular answer target during the
answer typing phase. In this way, they take advan-
tage of the structure inherent in how-questions rather
than just treating them as general queries. However,
manually hand-crafting types is costly, and would
have to be repeated if the system was moved to a
new language or a new query domain. Our auto-
matic approach does not suffer from this drawback.
Light et al (2001) showed that for a small fixed
set of answer types, multiple words tagged with
the same type will exist even with perfect passage
retrieval, sentence retrieval, and type assignment.
For example, Example 1 may be answered with a
sentence such as ?bears range in weight from the
smaller black bear at 400 pounds to the gigantic griz-
zly at over 1200 pounds? in which two answers have
appropriate units but only one of which is correct.
We provide results in Section 6 confirming the lim-
its of answer typing at narrowing answer focus, us-
ing varying levels of perfect information.
Our approach makes use of the web as a large
corpus of useful information. Exploiting the vast
amount of data on the web is part of a growing trend
in Natural Language Processing (Keller and Lapata,
517
2003). Indeed, many QA systems have been devel-
oped using the web (to varying degrees) to assist in
finding a correct answer (Brill et al, 2001; Cucerzan
and Agichtein, 2005; Radev et al, 2001), as the web
is the largest available corpus even if its information
can be difficult to harness. Rather than relying on
the web to find the answer to a question, we rely on it
as a source of information on appropriate units only.
Should the domain of the question answering system
change from general factoid questions, units may be
extracted from a smaller, domain-specific corpus.
3 Model Framework
The objective of our model is to create a list of rel-
evant units for an adjective that may be found in a
how-question. We wish to create these lists a pri-
ori and off-line so that they are applicable to future
questions. Although the model described here can
be applied on-line at the time of question answering,
the resources and time required make off-line gener-
ation of unit lists the preferred approach.
We wish to automatically learn a mapping  
 
in which

is a set of adjectives derived
from how-questions and
	
is a set of lists of units
associated with these adjectives. For example, an
element of this mapping might be:
high 


 
feet, meter, foot, inches, ... 


which assigns height measurements to ?how high?
questions. Inducing this mapping means establish-
ing a connection, or co-occurrence, between each
adjective  and its units 	 . In the following sub-
sections, we show how to establish this connection.
3.1 Using WordNet for Adjective Expansion
In common documents, such as news articles or
web pages, the co-occurrence of an adjective and
its units may be unlikely. For example, the co-
occurrence between ?heavy? and ?pounds? may
not be as prevalent as the co-occurrence between
?weight? and ?pounds.? We therefore propose us-
ing WordNet (Fellbaum, 1998) to expand the how-
adjective  to a set of related concepts the adjective
may be used to describe. We denote a related con-
cept of  as  . In the above example, ?heavy? can be
used to describe a ?weight.? Two useful WordNet re-
lations are the attribute relation, in which the adjec-
tive is an attribute of the concept, and in cases where
no attribute exists, the derivationally-related words.
?Heavy? is an attribute of ?weight? whereas the
derivationally-related form is ?heaviness,? a plausi-
ble but less useful concept. Next we describe how
the particular co-occurrence of the related concept 
and unit  is obtained.
3.2 Using Google to Obtain Counts
We selected the Google search engine as a source
of co-occurrence data due to the large number of in-
dexed documents from which co-occurrence counts
can be derived. To further enhance the quality of
co-occurrence data, we search on the specific phrase
?  is measured in? in which  is one of the related
concepts of  . This allows for the simultaneous dis-
covery of unknown units and the retrieval of their
co-occurrence counts.
Sentences in which the pattern occurs are parsed
using Minipar (Lin, 1998b) so that we can obtain
the word related to ?measured? via the preposi-
tional in relation. This allows us to handle senten-
tial constructions that may intervene between ?mea-
sured? and a meaningful unit. For each unit  that
is related to ?measured? via in, we increment the
co-occurrence count Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 33?40,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Bootstrapping Path-Based Pronoun Resolution
Shane Bergsma
Department of Computing Science
University of Alberta
Edmonton, Alberta, Canada, T6G 2E8
bergsma@cs.ualberta.ca
Dekang Lin
Google, Inc.
1600 Amphitheatre Parkway,
Mountain View, California, 94301
lindek@google.com
Abstract
We present an approach to pronoun reso-
lution based on syntactic paths. Through a
simple bootstrapping procedure, we learn
the likelihood of coreference between a
pronoun and a candidate noun based on the
path in the parse tree between the two en-
tities. This path information enables us to
handle previously challenging resolution
instances, and also robustly addresses tra-
ditional syntactic coreference constraints.
Highly coreferent paths also allow mining
of precise probabilistic gender/number in-
formation. We combine statistical knowl-
edge with well known features in a Sup-
port Vector Machine pronoun resolution
classifier. Significant gains in performance
are observed on several datasets.
1 Introduction
Pronoun resolution is a difficult but vital part of the
overall coreference resolution task. In each of the
following sentences, a pronoun resolution system
must determine what the pronoun his refers to:
(1) John needs his friend.
(2) John needs his support.
In (1), John and his corefer. In (2), his refers
to some other, perhaps previously evoked entity.
Traditional pronoun resolution systems are not de-
signed to distinguish between these cases. They
lack the specific world knowledge required in the
second instance ? the knowledge that a person
does not usually explicitly need his own support.
We collect statistical path-coreference informa-
tion from a large, automatically-parsed corpus to
address this limitation. A dependency path is de-
fined as the sequence of dependency links between
two potentially coreferent entities in a parse tree.
A path does not include the terminal entities; for
example, ?John needs his support? and ?He needs
their support? have the same syntactic path. Our
algorithm determines that the dependency path
linking the Noun and pronoun is very likely to con-
nect coreferent entities for the path ?Noun needs
pronoun?s friend,? while it is rarely coreferent for
the path ?Noun needs pronoun?s support.?
This likelihood can be learned by simply count-
ing how often we see a given path in text with
an initial Noun and a final pronoun that are from
the same/different gender/number classes. Cases
such as ?John needs her support? or ?They need
his support? are much more frequent in text than
cases where the subject noun and pronoun termi-
nals agree in gender/number. When there is agree-
ment, the terminal nouns are likely to be corefer-
ent. When they disagree, they refer to different en-
tities. After a sufficient number of occurrences of
agreement or disagreement, there is a strong sta-
tistical indication of whether the path is coreferent
(terminal nouns tend to refer to the same entity) or
non-coreferent (nouns refer to different entities).
We show that including path coreference in-
formation enables significant performance gains
on three third-person pronoun resolution experi-
ments. We also show that coreferent paths can pro-
vide the seed information for bootstrapping other,
even more important information, such as the gen-
der/number of noun phrases.
2 Related Work
Coreference resolution is generally conducted as
a pairwise classification task, using various con-
straints and preferences to determine whether two
33
expressions corefer. Coreference is typically only
allowed between nouns matching in gender and
number, and not violating any intrasentential syn-
tactic principles. Constraints can be applied as a
preprocessing step to scoring candidates based on
distance, grammatical role, etc., with scores devel-
oped either manually (Lappin and Leass, 1994), or
through a machine-learning algorithm (Kehler et
al., 2004). Constraints and preferences have also
been applied together as decision nodes on a deci-
sion tree (Aone and Bennett, 1995).
When previous resolution systems handle cases
like (1) and (2), where no disagreement or syntac-
tic violation occurs, coreference is therefore de-
termined by the weighting of features or learned
decisions of the resolution classifier. Without
path coreference knowledge, a resolution process
would resolve the pronouns in (1) and (2) the
same way. Indeed, coreference resolution research
has focused on the importance of the strategy
for combining well known constraints and prefer-
ences (Mitkov, 1997; Ng and Cardie, 2002), devot-
ing little attention to the development of new fea-
tures for these difficult cases. The application of
world knowledge to pronoun resolution has been
limited to the semantic compatibility between a
candidate noun and the pronoun?s context (Yang
et al, 2005). We show semantic compatibility can
be effectively combined with path coreference in-
formation in our experiments below.
Our method for determining path coreference
is similar to an algorithm for discovering para-
phrases in text (Lin and Pantel, 2001). In that
work, the beginning and end nodes in the paths
are collected, and two paths are said to be similar
(and thus likely paraphrases of each other) if they
have similar terminals (i.e. the paths occur with a
similar distribution). Our work does not need to
store the terminals themselves, only whether they
are from the same pronoun group. Different paths
are not compared in any way; each path is individ-
ually assigned a coreference likelihood.
3 Path Coreference
We define a dependency path as the sequence of
nodes and dependency labels between two poten-
tially coreferent entities in a dependency parse
tree. We use the structure induced by the minimal-
ist parser Minipar (Lin, 1998) on sentences from
the news corpus described in Section 4. Figure 1
gives the parse tree of (2). As a short-form, we
Johnneedshissupport
subj gen
obj
Figure 1: Example dependency tree.
write the dependency path in this case as ?Noun
needs pronoun?s support.? The path itself does not
include the terminal nouns ?John? and ?his.?
Our algorithm finds the likelihood of coref-
erence along dependency paths by counting the
number of times they occur with terminals that
are either likely coreferent or non-coreferent. In
the simplest version, we count paths with termi-
nals that are both pronouns. We partition pronouns
into seven groups of matching gender, number,
and person; for example, the first person singular
group contains I, me, my, mine, and myself. If the
two terminal pronouns are from the same group,
coreference along the path is likely. If they are
from different groups, like I and his, then they are
non-coreferent. Let NS(p) be the number of timesthe two terminal pronouns of a path, p, are from
the same pronoun group, and let ND(p) be thenumber of times they are from different groups.
We define the coreference of p as:
C(p) = NS(p)NS(p) + ND(p)
Our statistics indicate the example path, ?Noun
needs pronoun?s support,? has a low C(p) value.
We could use this fact to prevent us from resolv-
ing ?his? to ?John? when ?John needs his support?
is presented to a pronoun resolution system.
To mitigate data sparsity, we represent the path
with the root form of the verbs and nouns. Also,
we use Minipar?s named-entity recognition to re-
place named-entity nouns by the semantic cate-
gory of their named-entity, when available. All
modifiers not on the direct path, such as adjectives,
determiners and adverbs, are not considered. We
limit the maximum path length to eight nodes.
Tables 1 and 2 give examples of coreferent and
non-coreferent paths learned by our algorithm and
identified in our test sets. Coreferent paths are
defined as paths with a C(p) value (and overall
number of occurrences) above a certain threshold,
indicating the terminal entities are highly likely
34
Table 1: Example coreferent paths: Italicized entities generally corefer.
Pattern Example
1. Noun left ... to pronoun?s wife Buffett will leave the stock to his wife.
2. Noun says pronoun intends... The newspaper says it intends to file a lawsuit.
3. Noun was punished for pronoun?s crime. The criminal was punished for his crime.
4. ... left Noun to fend for pronoun-self They left Jane to fend for herself.
5. Noun lost pronoun?s job. Dick lost his job.
6. ... created Noun and populated pronoun. Nzame created the earth and populated it
7. Noun consolidated pronoun?s power. The revolutionaries consolidated their power.
8. Noun suffered ... in pronoun?s knee ligament. The leopard suffered pain in its knee ligament.
to corefer. Non-coreferent paths have a C(p) be-
low a certain cutoff; the terminals are highly un-
likely to corefer. Especially note the challenge of
resolving most of the examples in Table 2 with-
out path coreference information. Although these
paths encompass some cases previously covered
by Binding Theory (e.g. ?Mary suspended her,?
her cannot refer to Mary by Principle B (Haege-
man, 1994)), most have no syntactic justification
for non-coreference per se. Likewise, although
Binding Theory (Principle A) could identify the
reflexive pronominal relationship of Example 4 in
Table 1, most cases cannot be resolved through
syntax alone. Our analysis shows that successfully
handling cases that may have been handled with
Binding Theory constitutes only a small portion of
the total performance gain using path coreference.
In any case, Binding Theory remains a chal-
lenge with a noisy parser. Consider: ?Alex gave
her money.? Minipar parses her as a possessive,
when it is more likely an object, ?Alex gave money
to her.? Without a correct parse, we cannot rule
out the link between her and Alex through Bind-
ing Theory. Our algorithm, however, learns that
the path ?Noun gave pronoun?s money,? is non-
coreferent. In a sense, it corrects for parser errors
by learning when coreference should be blocked,
given any consistent parse of the sentence.
We obtain path coreference for millions of paths
from our parsed news corpus (Section 4). While
Tables 1 and 2 give test set examples, many other
interesting paths are obtained. We learn corefer-
ence is unlikely between the nouns in ?Bob mar-
ried his mother,? or ?Sue wrote her obituary.? The
fact you don?t marry your own mother or write
your own obituary is perhaps obvious, but this
is the first time this kind of knowledge has been
made available computationally. Naturally, ex-
ceptions to the coreference or non-coreference of
some of these paths can be found; our patterns
represent general trends only. And, as mentioned
above, reliable path coreference is somewhat de-
pendent on consistent parsing.
Paths connecting pronouns to pronouns are dif-
ferent than paths connecting both nouns and pro-
nouns to pronouns ? the case we are ultimately in-
terested in resolving. Consider ?Company A gave
its data on its website.? The pronoun-pronoun
path coreference algorithm described above would
learn the terminals in ?Noun?s data on pronoun?s
website? are often coreferent. But if we see the
phrase ?Company A gave Company B?s data on
its website,? then ?its? is not likely to refer to
?Company B,? even though we identified this as
a coreferent path! We address this problem with a
two-stage extraction procedure. We first bootstrap
gender/number information using the pronoun-
pronoun paths as described in Section 4.1. We
then use this gender/number information to count
paths where an initial noun (with probabilistically-
assigned gender/number) and following pronoun
are connected by the dependency path, record-
ing the agreement or disagreement of their gen-
der/number category.1 These superior paths are
then used to re-bootstrap our final gender/number
information used in the evaluation (Section 6).
We also bootstrap paths where the nodes in
the path are replaced by their grammatical cate-
gory. This allows us to learn general syntactic con-
straints not dependent on the surface forms of the
words (including, but not limited to, the Binding
Theory principles). A separate set of these non-
coreferent paths is also used as a feature in our sys-
1As desired, this modification allows the first example to
provide two instances of noun-pronoun paths with terminals
from the same gender/number group, linking each ?its? to the
subject noun ?Company A?, rather than to each other.
35
Table 2: Example non-coreferent paths: Italicized entities do not generally corefer
Pattern Example
1. Noun thanked ... for pronoun?s assistance John thanked him for his assistance.
2. Noun wanted pronoun to lie. The president wanted her to lie.
3. ... Noun into pronoun?s pool Max put the floaties into their pool.
4. ... use Noun to pronoun?s advantage The company used the delay to its advantage.
5. Noun suspended pronoun Mary suspended her.
6. Noun was pronoun?s relative. The Smiths were their relatives.
7. Noun met pronoun?s demands The players? association met its demands.
8. ... put Noun at the top of pronoun?s list. The government put safety at the top of its list.
tem. We also tried expanding our coverage by us-
ing paths similar to paths with known path coref-
erence (based on distributionally similar words),
but this did not generally increase performance.
4 Bootstrapping in Pronoun Resolution
Our determination of path coreference can be con-
sidered a bootstrapping procedure. Furthermore,
the coreferent paths themselves can serve as the
seed for bootstrapping additional coreference in-
formation. In this section, we sketch previous ap-
proaches to bootstrapping in coreference resolu-
tion and explain our new ideas.
Coreference bootstrapping works by assuming
resolutions in unlabelled text, acquiring informa-
tion from the putative resolutions, and then mak-
ing inferences from the aggregate statistical data.
For example, we assumed two pronouns from the
same pronoun group were coreferent, and deduced
path coreference from the accumulated counts.
The potential of the bootstrapping approach can
best be appreciated by imagining millions of doc-
uments with coreference annotations. With such a
set, we could extract fine-grained features, perhaps
tied to individual words or paths. For example, we
could estimate the likelihood each noun belongs to
a particular gender/number class by the proportion
of times this noun was labelled as the antecedent
for a pronoun of this particular gender/number.
Since no such corpus exists, researchers have
used coarser features learned from smaller sets
through supervised learning (Soon et al, 2001;
Ng and Cardie, 2002), manually-defined corefer-
ence patterns to mine specific kinds of data (Bean
and Riloff, 2004; Bergsma, 2005), or accepted the
noise inherent in unsupervised schemes (Ge et al,
1998; Cherry and Bergsma, 2005).
We address the drawbacks of these approaches
Table 3: Gender classification performance (%)
Classifier F-Score
Bergsma (2005) Corpus-based 85.4
Bergsma (2005) Web-based 90.4
Bergsma (2005) Combined 92.2
Duplicated Corpus-based 88.0
Coreferent Path-based 90.3
by using coreferent paths as the assumed resolu-
tions in the bootstrapping. Because we can vary
the threshold for defining a coreferent path, we can
trade-off coverage for precision. We now outline
two potential uses of bootstrapping with coref-
erent paths: learning gender/number information
(Section 4.1) and augmenting a semantic compat-
ibility model (Section 4.2). We bootstrap this data
on our automatically-parsed news corpus. The
corpus comprises 85 GB of news articles taken
from the world wide web over a 1-year period.
4.1 Probabilistic Gender/Number
Bergsma (2005) learns noun gender (and num-
ber) from two principal sources: 1) mining it
from manually-defined lexico-syntactic patterns in
parsed corpora, and 2) acquiring it on the fly by
counting the number of pages returned for various
gender-indicating patterns by the Google search
engine. The web-based approach outperformed
the corpus-based approach, while a system that
combined the two sets of information resulted in
the highest performance (Table 3). The combined
gender-classifying system is a machine-learned
classifier with 20 features.
The time delay of using an Internet search en-
gine within a large-scale anaphora resolution ef-
fort is currently impractical. Thus we attempted
36
Table 4: Example gender/number probability (%)
Word masc fem neut plur
company 0.6 0.1 98.1 1.2
condoleeza rice 4.0 92.7 0.0 3.2
pat 58.3 30.6 6.2 4.9
president 94.1 3.0 1.5 1.4
wife 9.9 83.3 0.8 6.1
to duplicate Bergsma?s corpus-based extraction of
gender and number, where the information can be
stored in advance in a table, but using a much
larger data set. Bergsma ran his extraction on
roughly 6 GB of text; we used roughly 85 GB.
Using the test set from Bergsma (2005), we
were only able to boost performance from an F-
Score of 85.4% to one of 88.0% (Table 3). This
result led us to re-examine the high performance
of Bergsma?s web-based approach. We realized
that the corpus-based and web-based approaches
are not exactly symmetric. The corpus-based ap-
proaches, for example, would not pick out gender
from a pattern such as ?John and his friends...? be-
cause ?Noun and pronoun?s NP? is not one of the
manually-defined gender extraction patterns. The
web-based approach, however, would catch this
instance with the ?John * his/her/its/their? tem-
plate, where ?*? is the Google wild-card opera-
tor. Clearly, there are patterns useful for capturing
gender and number information beyond the pre-
defined set used in the corpus-based extraction.
We thus decided to capture gender/number in-
formation from coreferent paths. If a noun is con-
nected to a pronoun of a particular gender along a
coreferent path, we count this as an instance of that
noun being that gender. In the end, the probability
that the noun is a particular gender is the propor-
tion of times it was connected to a pronoun of that
gender along a coreferent path. Gender informa-
tion becomes a single intuitive, accessible feature
(i.e. the probability of the noun being that gender)
rather than Bergsma?s 20-dimensional feature vec-
tor requiring search-engine queries to instantiate.
We acquire gender and number data for over 3
million nouns. We use add-one smoothing for data
sparsity. Some example gender/number probabil-
ities are given in Table 4 (cf. (Ge et al, 1998;
Cherry and Bergsma, 2005)). We get a perfor-
mance of 90.3% (Table 3), again meeting our re-
quirements of high performance and allowing for
a fast, practical implementation. This is lower
than Bergsma?s top score of 92.2% (Figure 3),
but again, Bergsma?s top system relies on Google
search queries for each new word, while ours are
all pre-stored in a table for fast access.
We are pleased to be able to share our gender
and number data with the NLP community.2 In
Section 6, we show the benefit of this data as a
probabilistic feature in our pronoun resolution sys-
tem. Probabilistic data is useful because it allows
us to rapidly prototype resolution systems with-
out incurring the overhead of large-scale lexical
databases such as WordNet (Miller et al, 1990).
4.2 Semantic Compatibility
Researchers since Dagan and Itai (1990) have var-
iously argued for and against the utility of col-
location statistics between nouns and parents for
improving the performance of pronoun resolution.
For example, can the verb parent of a pronoun be
used to select antecedents that satisfy the verb?s se-
lectional restrictions? If the verb phrase was shat-
ter it, we would expect it to refer to some kind
of brittle entity. Like path coreference, semantic
compatibility can be considered a form of world
knowledge needed for more challenging pronoun
resolution instances.
We encode the semantic compatibility between
a noun and its parse tree parent (and grammatical
relationship with the parent) using mutual infor-
mation (MI) (Church and Hanks, 1989). Suppose
we are determining whether ham is a suitable an-
tecedent for the pronoun it in eat it. We calculate
the MI as:
MI(eat:obj, ham) = log Pr(eat:obj:ham)Pr(eat:obj)Pr(ham)
Although semantic compatibility is usually only
computed for possessive-noun, subject-verb, and
verb-object relationships, we include 121 differ-
ent kinds of syntactic relationships as parsed in
our news corpus.3 We collected 4.88 billion par-
ent:rel:node triples, including over 327 million
possessive-noun values, 1.29 billion subject-verb
and 877 million verb-direct object. We use small
probability values for unseen Pr(parent:rel:node),
Pr(parent:rel), and Pr(node) cases, as well as a de-
fault MI when no relationship is parsed, roughly
optimized for performance on the training set. We
2Available at http://www.cs.ualberta.ca/?bergsma/Gender/
3We convert prepositions to relationships to enhance our
model?s semantics, e.g. Joan:of:Arc rather than Joan:prep:of
37
include both the MI between the noun and the pro-
noun?s parent as well as the MI between the pro-
noun and the noun?s parent as features in our pro-
noun resolution classifier.
Kehler et al (2004) saw no apparent gain from
using semantic compatibility information, while
Yang et al (2005) saw about a 3% improvement
with compatibility data acquired by searching on
the world wide web. Section 6 analyzes the con-
tribution of MI to our system.
Bean and Riloff (2004) used bootstrapping to
extend their semantic compatibility model, which
they called contextual-role knowledge, by identi-
fying certain cases of easily-resolved anaphors and
antecedents. They give the example ?Mr. Bush
disclosed the policy by reading it.? Once we iden-
tify that it and policy are coreferent, we include
read:obj:policy as part of the compatibility model.
Rather than using manually-defined heuristics
to bootstrap additional semantic compatibility in-
formation, we wanted to enhance our MI statistics
automatically with coreferent paths. Consider the
phrase, ?Saddam?s wife got a Jordanian lawyer for
her husband.? It is unlikely we would see ?wife?s
husband? in text; in other words, we would not
know that husband:gen:wife is, in fact, semanti-
cally compatible and thereby we would discour-
age selection of ?wife? as the antecedent at res-
olution time. However, because ?Noun gets ...
for pronoun?s husband? is a coreferent path, we
could capture the above relationship by adding a
parent:rel:node for every pronoun connected to a
noun phrase along a coreferent path in text.
We developed context models with and with-
out these path enhancements, but ultimately we
could find no subset of coreferent paths that im-
prove the semantic compatibility?s contribution to
training set accuracy. A mutual information model
trained on 85 GB of text is fairly robust on its own,
and any kind of bootstrapped extension seems to
cause more damage by increased noise than can be
compensated by increased coverage. Although we
like knowing audiences have noses, e.g. ?the audi-
ence turned up its nose at the performance,? such
phrases are apparently quite rare in actual test sets.
5 Experimental Design
The noun-pronoun path coreference can be used
directly as a feature in a pronoun resolution sys-
tem. However, path coreference is undefined for
cases where there is no path between the pro-
noun and the candidate noun ? for example, when
the candidate is in the previous sentence. There-
fore, rather than using path coreference directly,
we have features that are true if C(p) is above or
below certain thresholds. The features are thus set
when coreference between the pronoun and candi-
date noun is likely (a coreferent path) or unlikely
(a non-coreferent path).
We now evaluate the utility of path coreference
within a state-of-the-art machine-learned resolu-
tion system for third-person pronouns with nom-
inal antecedents. A standard set of features is used
along with the bootstrapped gender/number, se-
mantic compatibility, and path coreference infor-
mation. We refer to these features as our ?proba-
bilistic features? (Prob. Features) and run experi-
ments using the full system trained and tested with
each absent, in turn (Table 5). We have 29 features
in total, including measures of candidate distance,
frequency, grammatical role, and different kinds
of parallelism between the pronoun and the can-
didate noun. Several reliable features are used as
hard constraints, removing candidates before con-
sideration by the scoring algorithm.
All of the parsing, noun-phrase identification,
and named-entity recognition are done automat-
ically with Minipar. Candidate antecedents are
considered in the current and previous sentence
only. We use SVMlight (Joachims, 1999) to learn
a linear-kernel classifier on pairwise examples in
the training set. When resolving pronouns, we
select the candidate with the farthest positive dis-
tance from the SVM classification hyperplane.
Our training set is the anaphora-annotated por-
tion of the American National Corpus (ANC) used
in Bergsma (2005), containing 1270 anaphoric
pronouns4 . We test on the ANC Test set (1291 in-
stances) also used in Bergsma (2005) (highest res-
olution accuracy reported: 73.3%), the anaphora-
labelled portion of AQUAINT used in Cherry and
Bergsma (2005) (1078 instances, highest accu-
racy: 71.4%), and the anaphoric pronoun subset
of the MUC7 (1997) coreference evaluation for-
mal test set (169 instances, highest precision of
62.1 reported on all pronouns in (Ng and Cardie,
2002)). These particular corpora were chosen so
we could test our approach using the same data
as comparable machine-learned systems exploit-
ing probabilistic information sources. Parameters
4See http://www.cs.ualberta.ca/?bergsma/CorefTags/ for
instructions on acquiring annotations
38
Table 5: Resolution accuracy (%)
Dataset ANC AQT MUC
1 Previous noun 36.7 34.5 30.8
2 No Prob. Features 58.1 60.9 49.7
3 No Prob. Gender 65.8 71.0 68.6
4 No MI 71.3 73.5 69.2
5 No C(p) 72.3 73.7 69.8
6 Full System 73.9 75.0 71.6
7 Upper Bound 93.2 92.3 91.1
were set using cross-validation on the training set;
test sets were used only once to obtain the final
performance values.
Evaluation Metric: We report results in terms of
accuracy: Of all the anaphoric pronouns in the test
set, the proportion we resolve correctly.
6 Results and Discussion
We compare the accuracy of various configura-
tions of our system on the ANC, AQT and MUC
datasets (Table 5). We include the score from pick-
ing the noun immediately preceding the pronoun
(after our hard filters are applied). Due to the hard
filters and limited search window, it is not possi-
ble for our system to resolve every noun to a cor-
rect antecedent. We thus provide the performance
upper bound (i.e. the proportion of cases with a
correct answer in the filtered candidate list). On
ANC and AQT, each of the probabilistic features
results in a statistically significant gain in perfor-
mance over a model trained and tested with that
feature absent.5 On the smaller MUC set, none of
the differences in 3-6 are statistically significant,
however, the relative contribution of the various
features remains reassuringly constant.
Aside from missing antecedents due to the hard
filters, the main sources of error include inaccurate
statistical data and a classifier bias toward preced-
ing pronouns of the same gender/number. It would
be interesting to see whether performance could be
improved by adding WordNet and web-mined fea-
tures. Path coreference itself could conceivably be
determined with a search engine.
Gender is our most powerful probabilistic fea-
ture. In fact, inspecting our system?s decisions,
gender often rules out coreference regardless of
path coreference. This is not surprising, since we
based the acquisition of C(p) on gender. That is,
5We calculate significance with McNemar?s test, p=0.05.
 0.7
 0.75
 0.8
 0.85
 0.9
 0.95
 1
 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
Recall
Pr
ec
is
io
n
Top-1
Pr
ec
is
io
n
Top-2
Pr
ec
is
io
n
Top-3
Figure 2: ANC pronoun resolution accuracy for
varying SVM-thresholds.
our bootstrapping assumption was that the major-
ity of times these paths occur, gender indicates
coreference or lack thereof. Thus when they oc-
cur in our test sets, gender should often sufficiently
indicate coreference. Improving the orthogonality
of our features remains a future challenge.
Nevertheless, note the decrease in performance
on each of the datasets when C(p) is excluded
(#5). This is compelling evidence that path coref-
erence is valuable in its own right, beyond its abil-
ity to bootstrap extensive and reliable gender data.
Finally, we can add ourselves to the camp of
people claiming semantic compatibility is useful
for pronoun resolution. Both the MI from the pro-
noun in the antecedent?s context and vice-versa
result in improvement. Building a model from
enough text may be the key.
The primary goal of our evaluation was to as-
sess the benefit of path coreference within a com-
petitive pronoun resolution system. Our system
does, however, outperform previously published
results on these datasets. Direct comparison of
our scoring system to other current top approaches
is made difficult by differences in preprocessing.
Ideally we would assess the benefit of our prob-
abilistic features using the same state-of-the-art
preprocessing modules employed by others such
as (Yang et al, 2005) (who additionally use a
search engine for compatibility scoring). Clearly,
promoting competitive evaluation of pronoun res-
olution scoring systems by giving competitors
equivalent real-world preprocessing output along
the lines of (Barbu and Mitkov, 2001) remains the
best way to isolate areas for system improvement.
Our pronoun resolution system is part of a larger
information retrieval project where resolution ac-
39
curacy is not necessarily the most pertinent mea-
sure of classifier performance. More than one can-
didate can be useful in ambiguous cases, and not
every resolution need be used. Since the SVM
ranks antecedent candidates, we can test this rank-
ing by selecting more than the top candidate (Top-
n) and evaluating coverage of the true antecedents.
We can also resolve only those instances where the
most likely candidate is above a certain distance
from the SVM threshold. Varying this distance
varies the precision-recall (PR) of the overall res-
olution. A representative PR curve for the Top-n
classifiers is provided (Figure 2). The correspond-
ing information retrieval performance can now be
evaluated along the Top-n / PR configurations.
7 Conclusion
We have introduced a novel feature for pronoun
resolution called path coreference, and demon-
strated its significant contribution to a state-of-the-
art pronoun resolution system. This feature aids
coreference decisions in many situations not han-
dled by traditional coreference systems. Also, by
bootstrapping with the coreferent paths, we are
able to build the most complete and accurate ta-
ble of probabilistic gender information yet avail-
able. Preliminary experiments show path coref-
erence bootstrapping can also provide a means of
identifying pleonastic pronouns, where pleonastic
neutral pronouns are often followed in a depen-
dency path by a terminal noun of different gender,
and cataphoric constructions, where the pronouns
are often followed by nouns of matching gender.
References
Chinatsu Aone and Scott William Bennett. 1995. Evaluating
automated and manual acquisition of anaphora resolution
strategies. In Proceedings of the 33rd Annual Meeting of
the Association for Computational Linguistics, pages 122?
129.
Catalina Barbu and Ruslan Mitkov. 2001. Evaluation tool for
rule-based anaphora resolution methods. In Proceedings
of the 39th Annual Meeting of the Association for Compu-
tational Linguistics, pages 34?41.
David L. Bean and Ellen Riloff. 2004. Unsupervised learn-
ing of contextual role knowledge for coreference resolu-
tion. In HLT-NAACL, pages 297?304.
Shane Bergsma. 2005. Automatic acquisition of gender in-
formation for anaphora resolution. In Proceedings of the
Eighteenth Canadian Conference on Artificial Intelligence
(Canadian AI?2005), pages 342?353.
Colin Cherry and Shane Bergsma. 2005. An expectation
maximization approach to pronoun resolution. In Pro-
ceedings of the Ninth Conference on Natural Language
Learning (CoNLL-2005), pages 88?95.
Kenneth Ward Church and Patrick Hanks. 1989. Word asso-
ciation norms, mutual information, and lexicography. In
Proceedings of the 27th Annual Meeting of the Association
for Computational Linguistics (ACL?89), pages 76?83.
Ido Dagan and Alan Itai. 1990. Automatic processing
of large corpora for the resolution of anaphora refer-
ences. In Proceedings of the 13th International Con-
ference on Computational Linguistics (COLING-90), vol-
ume 3, pages 330?332, Helsinki, Finland.
Niyu Ge, John Hale, and Eugene Charniak. 1998. A statisti-
cal approach to anaphora resolution. In Proceedings of the
Sixth Workshop on Very Large Corpora, pages 161?171.
Liliane Haegeman. 1994. Introduction to Government &
Binding theory: Second Edition. Basil Blackwell, Cam-
bridge, UK.
Thorsten Joachims. 1999. Making large-scale SVM learn-
ing practical. In B. Scho?lkopf and C. Burges, editors, Ad-
vances in Kernel Methods. MIT-Press.
Andrew Kehler, Douglas Appelt, Lara Taylor, and Aleksandr
Simma. 2004. The (non)utility of predicate-argument fre-
quencies for pronoun interpretation. In Proceedings of
HLT/NAACL-04, pages 289?296.
Shalom Lappin and Herbert J. Leass. 1994. An algorithm for
pronominal anaphora resolution. Computational Linguis-
tics, 20(4):535?561.
Dekang Lin and Patrick Pantel. 2001. Discovery of infer-
ence rules for question answering. Natural Language En-
gineering, 7(4):343?360.
Dekang Lin. 1998. Dependency-based evaluation of MINI-
PAR. In Proceedings of the Workshop on the Evalua-
tion of Parsing Systems, First International Conference on
Language Resources and Evaluation.
George A. Miller, Richard Beckwith, Christiane Fellbaum,
Derek Gross, and Katherine J. Miller. 1990. Introduction
to WordNet: an on-line lexical database. International
Journal of Lexicography, 3(4):235?244.
Ruslan Mitkov. 1997. Factors in anaphora resolution: they
are not the only things that matter. a case study based on
two different approaches. In Proceedings of the ACL ?97 /
EACL ?97 Workshop on Operational Factors in Practical,
Robust Anaphora Resolution, pages 14?21.
MUC-7. 1997. Coreference task definition (v3.0, 13 Jul
97). In Proceedings of the Seventh Message Understand-
ing Conference (MUC-7).
Vincent Ng and Claire Cardie. 2002. Improving machine
learning approaches to coreference resolution. In Pro-
ceedings of the 40th Annual Meeting of the Association
for Computational Linguistics, pages 104?111.
Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong
Lim. 2001. A machine learning approach to coreference
resolution of noun phrases. Computational Linguistics,
27(4):521?544.
Xiaofeng Yang, Jian Su, and Chew Lim Tan. 2005. Im-
proving pronoun resolution using statistics-based seman-
tic compatibility information. In Proceedings of the 43rd
Annual Meeting of the Association for Computational Lin-
guistics (ACL?05), pages 165?172, June.
40
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 656?663,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Alignment-Based Discriminative String Similarity
Shane Bergsma and Grzegorz Kondrak
Department of Computing Science
University of Alberta
Edmonton, Alberta, Canada, T6G 2E8
{bergsma,kondrak}@cs.ualberta.ca
Abstract
A character-based measure of similarity is
an important component of many natu-
ral language processing systems, including
approaches to transliteration, coreference,
word alignment, spelling correction, and the
identification of cognates in related vocabu-
laries. We propose an alignment-based dis-
criminative framework for string similarity.
We gather features from substring pairs con-
sistent with a character-based alignment of
the two strings. This approach achieves
exceptional performance; on nine separate
cognate identification experiments using six
language pairs, we more than double the pre-
cision of traditional orthographic measures
like Longest Common Subsequence Ratio
and Dice?s Coefficient. We also show strong
improvements over other recent discrimina-
tive and heuristic similarity functions.
1 Introduction
String similarity is often used as a means of quan-
tifying the likelihood that two pairs of strings have
the same underlying meaning, based purely on the
character composition of the two words. Strube et
al. (2002) use Edit Distance as a feature for de-
termining if two words are coreferent. Taskar et
al. (2005) use French-English common letter se-
quences as a feature for discriminative word align-
ment in bilingual texts. Brill and Moore (2000) learn
misspelled-word to correctly-spelled-word similari-
ties for spelling correction. In each of these exam-
ples, a similarity measure can make use of the recur-
rent substring pairings that reliably occur between
words having the same meaning.
Across natural languages, these recurrent sub-
string correspondences are found in word pairs
known as cognates: words with a common form
and meaning across languages. Cognates arise ei-
ther from words in a common ancestor language
(e.g. light/Licht, night/Nacht in English/German)
or from foreign word borrowings (e.g. trampo-
line/toranporin in English/Japanese). Knowledge of
cognates is useful for a number of applications, in-
cluding sentence alignment (Melamed, 1999) and
learning translation lexicons (Mann and Yarowsky,
2001; Koehn and Knight, 2002).
We propose an alignment-based, discriminative
approach to string similarity and evaluate this ap-
proach on cognate identification. Section 2 de-
scribes previous approaches and their limitations. In
Section 3, we explain our technique for automati-
cally creating a cognate-identification training set. A
novel aspect of this set is the inclusion of competitive
counter-examples for learning. Section 4 shows how
discriminative features are created from a character-
based, minimum-edit-distance alignment of a pair
of strings. In Section 5, we describe our bitext and
dictionary-based experiments on six language pairs,
including three based on non-Roman alphabets. In
Section 6, we show significant improvements over
traditional approaches, as well as significant gains
over more recent techniques by Ristad and Yiani-
los (1998), Tiedemann (1999), Kondrak (2005), and
Klementiev and Roth (2006).
2 Related Work
String similarity is a fundamental concept in a va-
riety of fields and hence a range of techniques
656
have been developed. We focus on approaches
that have been applied to words, i.e., uninterrupted
sequences of characters found in natural language
text. The most well-known measure of the simi-
larity of two strings is the Edit Distance or Lev-
enshtein Distance (Levenshtein, 1966): the number
of insertions, deletions and substitutions required to
transform one string into another. In our experi-
ments, we use Normalized Edit Distance (NED):
Edit Distance divided by the length of the longer
word. Other popular measures include Dice?s Coef-
ficient (DICE) (Adamson and Boreham, 1974), and
the length-normalized measures Longest Common
Subsequence Ratio (LCSR) (Melamed, 1999), and
Longest Common Prefix Ratio (PREFIX) (Kondrak,
2005). These baseline approaches have the impor-
tant advantage of not requiring training data. We
can also include in the non-learning category Kon-
drak (2005)?s Longest Common Subsequence For-
mula (LCSF), a probabilistic measure designed to
mitigate LCSR?s preference for shorter words.
Although simple to use, the untrained measures
cannot adapt to the specific spelling differences be-
tween a pair of languages. Researchers have there-
fore investigated adaptive measures that are learned
from a set of known cognate pairs. Ristad and Yiani-
los (1998) developed a stochastic transducer version
of Edit Distance learned from unaligned string pairs.
Mann and Yarowsky (2001) saw little improvement
over Edit Distance when applying this transducer to
cognates, even when filtering the transducer?s proba-
bilities into different weight classes to better approx-
imate Edit Distance. Tiedemann (1999) used various
measures to learn the recurrent spelling changes be-
tween English and Swedish, and used these changes
to re-weight LCSR to identify more cognates, with
modest performance improvements. Mulloni and
Pekar (2006) developed a similar technique to im-
prove NED for English/German.
Essentially, all these techniques improve on the
baseline approaches by using a set of positive (true)
cognate pairs to re-weight the costs of edit op-
erations or the score of sequence matches. Ide-
ally, we would prefer a more flexible approach that
can learn positive or negative weights on substring
pairings in order to better identify related strings.
One system that can potentially provide this flexi-
bility is a discriminative string-similarity approach
to named-entity transliteration by Klementiev and
Roth (2006). Although not compared to other simi-
larity measures in the original paper, we show that
this discriminative technique can strongly outper-
form traditional methods on cognate identification.
Unlike many recent generative systems, the Kle-
mentiev and Roth approach does not exploit the
known positions in the strings where the characters
match. For example, Brill and Moore (2000) com-
bine a character-based alignment with the Expec-
tation Maximization (EM) algorithm to develop an
improved probabilistic error model for spelling cor-
rection. Rappoport and Levent-Levi (2006) apply
this approach to learn substring correspondences for
cognates. Zelenko and Aone (2006) recently showed
a Klementiev and Roth (2006)-style discriminative
approach to be superior to alignment-based genera-
tive techniques for name transliteration. Our work
successfully uses the alignment-based methodology
of the generative approaches to enhance the feature
set for discriminative string similarity.
3 The Cognate Identification Task
Given two string lists, E and F , the task of cog-
nate identification is to find all pairs of strings (e, f)
that are cognate. In other similarity-driven applica-
tions, E and F could be misspelled and correctly
spelled words, or the orthographic and the phonetic
representation of words, etc. The task remains to
link strings with common meaning in E and F us-
ing only the string similarity measure.
We can facilitate the application of string simi-
larity to cognates by using a definition of cognation
not dependent on etymological analysis. For ex-
ample, Mann and Yarowsky (2001) define a word
pair (e, f) to be cognate if they are a translation
pair (same meaning) and their Edit Distance is less
than three (same form). We adopt an improved
definition (suggested by Melamed (1999) for the
French-English Canadian Hansards) that does not
over-propose shorter word pairs: (e, f) are cog-
nate if they are translations and their LCSR ?
0.58. Note that this cutoff is somewhat conser-
vative: the English/German cognates light/Licht
(LCSR=0.8) are included, but not the cognates
eight/acht (LCSR=0.4).
If two words must have LCSR ? 0.58 to be cog-
657
Foreign Language F Words f ? F Cognates Ef+ False Friends Ef?
Japanese (Ro?maji) napukin napkin nanking, pumpkin, snacking, sneaking
French abondamment abundantly abandonment, abatement, ... wonderment
German prozyklische procyclical polished, prophylactic, prophylaxis
Table 1: Foreign-English cognates and false friend training examples.
nate, then for a given word f ? F , we need only
consider as possible cognates the subset of words in
E having an LCSR with f larger than 0.58, a set we
call Ef . The portion of Ef with the same meaningas f , Ef+, are cognates, while the part with differ-ent meanings, Ef?, are not cognates. The words
Ef? with similar spelling but different meaning aresometimes called false friends. The cognate identi-
fication task is, for every word f ? F , and a list of
similarly spelled words Ef , to distinguish the cog-nate subset Ef+ from the false friend set Ef?.
To create training data for our learning ap-
proaches, and to generate a high-quality labelled test
set, we need to annotate some of the (f, ef ? Ef )word pairs for whether or not the words share a
common meaning. In Section 5, we explain our
two high-precision automatic annotation methods:
checking if each pair of words (a) were aligned in
a word-aligned bitext, or (b) were listed as transla-
tion pairs in a bilingual dictionary.
Table 1 provides some labelled examples with
non-empty cognate and false friend lists. Note that
despite these examples, this is not a ranking task:
even in highly related languages, most words in F
have empty Ef+ lists, and many have empty Ef?as well. Thus one natural formulation for cognate
identification is a pairwise (and symmetric) cogna-
tion classification that looks at each pair (f, ef ) sep-arately and individually makes a decision:
+(napukin,napkin)
? (napukin,nanking)
? (napukin,pumpkin)
In this formulation, the benefits of a discrimina-
tive approach are clear: it must find substrings that
distinguish cognate pairs from word pairs with oth-
erwise similar form. Klementiev and Roth (2006),
although using a discriminative approach, do not
provide their infinite-attribute perceptron with com-
petitive counter-examples. They instead use translit-
erations as positives and randomly-paired English
and Russian words as negative examples. In the fol-
lowing section, we also improve on Klementiev and
Roth (2006) by using a character-based string align-
ment to focus the features for discrimination.
4 Features for Discriminative Similarity
Discriminative learning works by providing a train-
ing set of labelled examples, each represented as a
set of features, to a module that learns a classifier. In
the previous section we showed how labelled word
pairs can be collected. We now address methods of
representing these word pairs as sets of features use-
ful for determining cognation.
Consider the Ro?maji Japanese/English cognates:
(sutoresu,stress). The LCSR is 0.625. Note that the
LCSR of sutoresu with the English false friend sto-
ries is higher: 0.75. LCSR alone is too weak a fea-
ture to pick out cognates. We need to look at the
actual character substrings.
Klementiev and Roth (2006) generate features for
a pair of words by splitting both words into all pos-
sible substrings of up to size two:
sutoresu ? { s, u, t, o, r, e, s, u, su, ut, to, ... su }
stress ? { s, t, r, e, s, s, st, tr, re, es, ss }
Then, a feature vector is built from all substring pairs
from the two words such that the difference in posi-
tions of the substrings is within one:
{s-s, s-t, s-st, su-s, su-t, su-st, su-tr... r-s, r-s, r-es...}
This feature vector provides the feature representa-
tion used in supervised machine learning.
This example also highlights the limitations of the
Klementiev and Roth approach. The learner can pro-
vide weight to features like s-s or s-st at the begin-
ning of the word, but because of the gradual accu-
mulation of positional differences, the learner never
sees the tor-tr and es-es correspondences that really
help indicate the words are cognate.
Our solution is to use the minimum-edit-distance
alignment of the two strings as the basis for fea-
ture extraction, rather than the positional correspon-
dences. We also include beginning-of-word (?) and
end-of-word ($) markers (referred to as boundary
658
markers) to highlight correspondences at those po-
sitions. The pair (sutoresu, stress) can be aligned:
For the feature representation, we only extract sub-
string pairs that are consistent with this alignment.1
That is, the letters in our pairs can only be aligned to
each other and not to letters outside the pairing:
{ ?-?,?s-?s, s-s, su-s, ut-t, t-t,... es-es, s-s, su-ss...}
We define phrase pairs to be the pairs of substrings
consistent with the alignment. A similar use of the
term ?phrase? exists in machine translation, where
phrases are often pairs of word sequences consistent
with word-based alignments (Koehn et al, 2003).
By limiting the substrings to only those pairs
that are consistent with the alignment, we gener-
ate fewer, more-informative features. Using more
precise features allows a larger maximum substring
size L than is feasible with the positional approach.
Larger substrings allow us to capture important re-
curring deletions like the ?u? in sut-st.
Tiedemann (1999) and others have shown the im-
portance of using the mismatching portions of cog-
nate pairs to learn the recurrent spelling changes be-
tween two languages. In order to capture mismatch-
ing segments longer than our maximum substring
size will allow, we include special features in our
representation called mismatches. Mismatches are
phrases that span the entire sequence of unaligned
characters between two pairs of aligned end char-
acters (similar to the ?rules? extracted by Mulloni
and Pekar (2006)). In the above example, su$-ss$
is a mismatch with ?s? and ?$? as the aligned end
characters. Two sets of features are taken from each
mismatch, one that includes the beginning/ending
aligned characters as context and one that does not.
For example, for the endings of the French/English
pair (e?conomique,economic), we include both the
substring pairs ique$:ic$ and que:c as features.
One consideration is whether substring features
should be binary presence/absence, or the count of
the feature in the pair normalized by the length of
the longer word. We investigate both of these ap-
1If the words are from different alphabets, we can get the
alignment by mapping the letters to their closest Roman equiv-
alent, or by using the EM algorithm to learn the edits (Ristad
and Yianilos, 1998).
proaches in our experiments. Also, there is no rea-
son not to include the scores of baseline approaches
like NED, LCSR, PREFIX or DICE as features in
the representation as well. Features like the lengths
of the two words and the difference in lengths of the
words have also proved to be useful in preliminary
experiments. Semantic features like frequency simi-
larity or contextual similarity might also be included
to help determine cognation between words that are
not present in a translation lexicon or bitext.
5 Experiments
Section 3 introduced two high-precision methods for
generating labelled cognate pairs: using the word
alignments from a bilingual corpus or using the en-
tries in a translation lexicon. We investigate both of
these methods in our experiments. In each case, we
generate sets of labelled word pairs for training, test-
ing, and development. The proportion of positive ex-
amples in the bitext-labelled test sets range between
1.4% and 1.8%, while ranging between 1.0% and
1.6% for the dictionary data.2
For the discriminative methods, we use a popu-
lar Support Vector Machine (SVM) learning pack-
age called SVMlight (Joachims, 1999). SVMs are
maximum-margin classifiers that achieve good per-
formance on a range of tasks. In each case, we
learn a linear kernel on the training set pairs and
tune the parameter that trades-off training error and
margin on the development set. We apply our classi-
fier to the test set and score the pairs by their pos-
itive distance from the SVM classification hyper-
plane (also done by Bilenko and Mooney (2003)
with their token-based SVM similarity measure).
We also score the test sets using traditional ortho-
graphic similarity measures PREFIX, DICE, LCSR,
and NED, an average of these four, and Kondrak
(2005)?s LCSF. We also use the log of the edit prob-
ability from the stochastic decoder of Ristad and
Yianilos (1998) (normalized by the length of the
longer word) and Tiedemann (1999)?s highest per-
forming system (Approach #3). Both use only the
positive examples in our training set. Our evaluation
metric is 11-pt average precision on the score-sorted
pair lists (also used by Kondrak and Sherif (2006)).
2The cognate data sets used in our experiments are available
at http://www.cs.ualberta.ca/?bergsma/Cognates/
659
5.1 Bitext Experiments
For the bitext-based annotation, we use publicly-
available word alignments from the Europarl corpus,
automatically generated by GIZA++ for French-
English (Fr), Spanish-English (Es) and German-
English (De) (Koehn and Monz, 2006). Initial clean-
ing of these noisy word pairs is necessary. We thus
remove all pairs with numbers, punctuation, a capi-
talized English word, and all words that occur fewer
than ten times. We also remove many incorrectly
aligned words by filtering pairs where the pairwise
Mutual Information between the words is less than
7.5. This processing leaves vocabulary sizes of 39K
for French, 31K for Spanish, and 60K for German.
Our labelled set is then generated from pairs
with LCSR ? 0.58 (using the cutoff from Melamed
(1999)). Each labelled set entry is a triple of a) the
foreign word f , b) the cognates Ef+ and c) the falsefriends Ef?. For each language pair, we randomlytake 20K triples for training, 5K for development
and 5K for testing. Each triple is converted to a set
of pairwise examples for learning and classification.
5.2 Dictionary Experiments
For the dictionary-based cognate identification, we
use French, Spanish, German, Greek (Gr), Japanese
(Jp), and Russian (Rs) to English translation pairs
from the Freelang program.3 The latter three pairs
were chosen so that we can evaluate on more distant
languages that use non-Roman alphabets (although
the Ro?maji Japanese is Romanized by definition).
We take 10K labelled-set triples for training, 2K for
testing and 2K for development.
The baseline approaches and our definition of
cognation require comparison in a common alpha-
bet. Thus we use a simple context-free mapping to
convert every Russian and Greek character in the
word pairs to their nearest Roman equivalent. We
then label a translation pair as cognate if the LCSR
between the words? Romanized representations is
greater than 0.58. We also operate all of our com-
parison systems on these Romanized pairs.
6 Results
We were interested in whether our working defini-
tion of cognation (translations and LCSR ? 0.58)
3http://www.freelang.net/dictionary/
Figure 1: LCSR histogram and polynomial trendline
of French-English dictionary pairs.
System Prec
Klementiev-Roth (KR) L?2 58.6
KR L?2 (normalized, boundary markers) 62.9
phrases L?2 61.0
phrases L?3 65.1
phrases L?3 + mismatches 65.6
phrases L?3 + mismatches + NED 65.8
Table 2: Bitext French-English development set cog-
nate identification 11-pt average precision (%).
reflects true etymological relatedness. We looked at
the LCSR histogram for translation pairs in one of
our translation dictionaries (Figure 1). The trendline
suggests a bimodal distribution, with two distinct
distributions of translation pairs making up the dic-
tionary: incidental letter agreement gives low LCSR
for the larger, non-cognate portion and high LCSR
characterizes the likely cognates. A threshold of
0.58 captures most of the cognate distribution while
excluding non-cognate pairs. This hypothesis was
confirmed by checking the LCSR values of a list
of known French-English cognates (randomly col-
lected from a dictionary for another project): 87.4%
were above 0.58. We also checked cognation on
100 randomly-sampled, positively-labelled French-
English pairs (i.e. translated or aligned and having
LCSR ? 0.58) from both the dictionary and bitext
data. 100% of the dictionary pairs and 93% of the
bitext pairs were cognate.
Next, we investigate various configurations of the
discriminative systems on one of our cognate iden-
tification development sets (Table 2). The origi-
nal Klementiev and Roth (2006) (KR) system can
660
Bitext Dictionary
System Fr Es De Fr Es De Gr Jp Rs
PREFIX 34.7 27.3 36.3 45.5 34.7 25.5 28.5 16.1 29.8
DICE 33.7 28.2 33.5 44.3 33.7 21.3 30.6 20.1 33.6
LCSR 34.0 28.7 28.5 48.3 36.5 18.4 30.2 24.2 36.6
NED 36.5 31.9 32.3 50.1 40.3 23.3 33.9 28.2 41.4
PREFIX+DICE+LCSR+NED 38.7 31.8 39.3 51.6 40.1 28.6 33.7 22.9 37.9
Kondrak (2005): LCSF 29.8 28.9 29.1 39.9 36.6 25.0 30.5 33.4 45.5
Ristad & Yanilos (1998) 37.7 32.5 34.6 56.1 46.9 36.9 38.0 52.7 51.8
Tiedemann (1999) 38.8 33.0 34.7 55.3 49.0 24.9 37.6 33.9 45.8
Klementiev & Roth (2006) 61.1 55.5 53.2 73.4 62.3 48.3 51.4 62.0 64.4
Alignment-Based Discriminative 66.5 63.2 64.1 77.7 72.1 65.6 65.7 82.0 76.9
Table 3: Bitext, Dictionary Foreign-to-English cognate identification 11-pt average precision (%).
be improved by normalizing the feature count by
the longer string length and including the bound-
ary markers. This is therefore done with all the
alignment-based approaches. Also, because of the
way its features are constructed, the KR system
is limited to a maximum substring length of two
(L?2). A maximum length of three (L?3) in the KR
framework produces millions of features and pro-
hibitive training times, while L?3 is computation-
ally feasible in the phrasal case, and increases pre-
cision by 4.1% over the phrases L?2 system.4 In-
cluding mismatches results in another small boost in
performance (0.5%), while using an Edit Distance
feature again increases performance by a slight mar-
gin (0.2%). This ranking of configurations is consis-
tent across all the bitext-based development sets; we
therefore take the configuration of the highest scor-
ing system as our Alignment-Based Discriminative
system for the remainder of this paper.
We next compare the Alignment-Based Discrim-
inative scorer to the various other implemented ap-
proaches across the three bitext and six dictionary-
based cognate identification test sets (Table 3). The
table highlights the top system among both the
non-adaptive and adaptive similarity scorers.5 In
4Preliminary experiments using even longer phrases (be-
yond L?3) currently produce a computationally prohibitive
number of features for SVM learning. Deploying current fea-
ture selection techniques might enable the use of even more ex-
pressive and powerful feature sets with longer phrase lengths.
5Using the training data and the SVM to weight the com-
ponents of the PREFIX+DICE+LCSR+NED scorer resulted in
negligible improvements over the simple average on our devel-
opment data.
each language pair, the alignment-based discrimi-
native approach outperforms all other approaches,
but the KR system also shows strong gains over
non-adaptive techniques and their re-weighted ex-
tensions. This is in contrast to previous compar-
isons which have only demonstrated minor improve-
ments with adaptive over traditional similarity mea-
sures (Kondrak and Sherif, 2006).
We consistently found that the original KR perfor-
mance could be surpassed by a system that normal-
izes the KR feature count and adds boundary mark-
ers. Across all the test sets, this modification results
in a 6% average gain in performance over baseline
KR, but is still on average 5% below the Alignment-
Based Discriminative technique, with a statistically
significantly difference on each of the nine sets.6
Figure 2 shows the relationship between train-
ing data size and performance in our bitext-based
French-English data. Note again that the Tiedemann
and Ristad & Yanilos systems only use the positive
examples in the training data. Our alignment-based
similarity function outperforms all the other systems
across nearly the entire range of training data. Note
also that the discriminative learning curves show no
signs of slowing down: performance grows logarith-
mically from 1K to 846K word pairs.
For insight into the power of our discrimina-
tive approach, we provide some of our classifiers?
highest and lowest-weighted features (Table 4).
6Following Evert (2004), significance was computed using
Fisher?s exact test (at p = 0.05) to compare the n-best word pairs
from the scored test sets, where n was taken as the number of
positive pairs in the set.
661
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 1000  10000  100000  1e+06
11
-p
t A
ve
ra
ge
 P
re
cis
io
n
Number of training pairs
NED
Tiedemann
Ristad-Yanilos
Klementiev-Roth
Alignment-Based Discrim.
Figure 2: Bitext French-English cognate identifica-
tion learning curve.
Lang. Feat. Wt. Example
Fr (Bitext) e?es-ed +8.0 ve?rifie?es:verified
Jp (Dict.) ru-l +5.9 penaruti:penalty
De (Bitext) k-c +5.5 kreativ:creative
Rs (Dict.) irov- +4.9 motivirovat:motivate
Gr (Dict.) f-ph +4.1 symfonia:symphony
Gr (Dict.) kos-c +3.3 anarchikos:anarchic
Gr (Dict.) os$-y$ -2.5 anarchikos:anarchy
Jp (Dict.) ou-ou -2.6 handoutai:handout
Es (Dict.) -un -3.1 balance:unbalance
Fr (Dict.) er$-er$ -5.0 former:former
Es (Bitext) mos-s -5.1 toleramos:tolerates
Table 4: Example features and weights for var-
ious Alignment-Based Discriminative classifiers
(Foreign-English, negative pairs in italics).
Note the expected correspondences between foreign
spellings and English (k-c, f-ph), but also features
that leverage derivational and inflectional morphol-
ogy. For example, Greek-English pairs with the
adjective-ending correspondence kos-c, e.g. anar-
chikos:anarchic, are favoured, but pairs with the ad-
jective ending in Greek and noun ending in English,
os$-y$, are penalized; indeed, by our definition, an-
archikos:anarchy is not cognate. In a bitext, the
feature e?es-ed captures that feminine-plural inflec-
tion of past tense verbs in French corresponds to
regular past tense in English. On the other hand,
words ending in the Spanish first person plural verb
suffix -amos are rarely translated to English words
ending with the suffix -s, causing mos-s to be pe-
Gr-En (Dict.) Es-En (Bitext)
alkali:alkali agenda:agenda
makaroni:macaroni natural:natural
adrenalini:adrenaline ma?rgenes:margins
flamingko:flamingo hormonal:hormonal
spasmodikos:spasmodic rado?n:radon
amvrosia:ambrosia higie?nico:hygienic
Table 5: Highest scored pairs by Alignment-Based
Discriminative classifier (negative pairs in italics).
nalized. The ability to leverage negative features,
learned from appropriate counter examples, is a key
innovation of our discriminative framework.
Table 5 gives the top pairs scored by our system
on two of the sets. Notice that unlike traditional sim-
ilarity measures that always score identical words
higher than all other pairs, by virtue of our feature
weighting, our discriminative classifier prefers some
pairs with very characteristic spelling changes.
We performed error analysis by looking at all the
pairs our system scored quite confidently (highly
positive or highly negative similarity), but which
were labelled oppositely. Highly-scored false pos-
itives arose equally from 1) actual cognates not
linked as translations in the data, 2) related words
with diverged meanings, e.g. the error in Table 5:
makaroni in Greek actually means spaghetti in En-
glish, and 3) the same word stem, a different part
of speech (e.g. the Greek/English adjective/noun
synonymos:synonym). Meanwhile, inspection of the
highly-confident false negatives revealed some (of-
ten erroneously-aligned in the bitext) positive pairs
with incidental letter match (e.g. the French/English
recettes:proceeds) that we would not actually deem
to be cognate. Thus the errors that our system makes
are often either linguistically interesting or point out
mistakes in our automatically-labelled bitext and (to
a lesser extent) dictionary data.
7 Conclusion
This is the first research to apply discriminative
string similarity to the task of cognate identification.
We have introduced and successfully applied an
alignment-based framework for discriminative sim-
ilarity that consistently demonstrates improved per-
formance in both bitext and dictionary-based cog-
662
nate identification on six language pairs. Our im-
proved approach can be applied in any of the di-
verse applications where traditional similarity mea-
sures like Edit Distance and LCSR are prevalent. We
have also made available our cognate identification
data sets, which will be of interest to general string
similarity researchers.
Furthermore, we have provided a natural frame-
work for future cognate identification research. Pho-
netic, semantic, or syntactic features could be in-
cluded within our discriminative infrastructure to aid
in the identification of cognates in text. In particu-
lar, we plan to investigate approaches that do not re-
quire the bilingual dictionaries or bitexts to generate
training data. For example, researchers have auto-
matically developed translation lexicons by seeing
if words from each language have similar frequen-
cies, contexts (Koehn and Knight, 2002), bursti-
ness, inverse document frequencies, and date dis-
tributions (Schafer and Yarowsky, 2002). Semantic
and string similarity might be learned jointly with a
co-training or bootstrapping approach (Klementiev
and Roth, 2006). We may also compare alignment-
based discriminative string similarity with a more
complex discriminative model that learns the align-
ments as latent structure (McCallum et al, 2005).
Acknowledgments
We gratefully acknowledge support from the Natu-
ral Sciences and Engineering Research Council of
Canada, the Alberta Ingenuity Fund, and the Alberta
Informatics Circle of Research Excellence.
References
George W. Adamson and Jillian Boreham. 1974. The use of
an association measure based on character structure to iden-
tify semantically related pairs of words and document titles.
Information Storage and Retrieval, 10:253?260.
Mikhail Bilenko and Raymond J. Mooney. 2003. Adaptive du-
plicate detection using learnable string similarity measures.
In KDD, pages 39?48.
Eric Brill and Robert Moore. 2000. An improved error model
for noisy channel spelling correction. In ACL. 286?293.
Stefan Evert. 2004. Significance tests for the evaluation of
ranking methods. In COLING, pages 945?951.
Thorsten Joachims. 1999. Making large-scale Support Vector
Machine learning practical. In Advances in Kernel Methods:
Support Vector Machines, pages 169?184. MIT-Press.
Alexandre Klementiev and Dan Roth. 2006. Named entity
transliteration and discovery from multilingual comparable
corpora. In HLT-NAACL, pages 82?88.
Philipp Koehn and Kevin Knight. 2002. Learning a transla-
tion lexicon from monolingual corpora. In ACL Workshop
on Unsupervised Lexical Acquistion.
Philipp Koehn and Christof Monz. 2006. Manual and auto-
matic evaluation of machine translation between European
languages. In NAACL Workshop on Statistical Machine
Translation, pages 102?121.
Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In HLT-NAACL, pages
127?133.
Grzegorz Kondrak and Tarek Sherif. 2006. Evaluation of
several phonetic similarity algorithms on the task of cog-
nate identification. In COLING-ACL Workshop on Linguis-
tic Distances, pages 37?44.
Grzegorz Kondrak. 2005. Cognates and word alignment in
bitexts. In MT Summit X, pages 305?312.
Vladimir I. Levenshtein. 1966. Binary codes capable of cor-
recting deletions, insertions, and reversals. Soviet Physics
Doklady, 10(8):707?710.
Gideon S. Mann and David Yarowsky. 2001. Multipath trans-
lation lexicon induction via bridge languages. In NAACL,
pages 151?158.
Andrew McCallum, Kedar Bellare, and Fernando Pereira.
2005. A conditional random field for discriminatively-
trained finite-state string edit distance. In UAI. 388?395.
I. Dan Melamed. 1999. Bitext maps and alignment via pattern
recognition. Computational Linguistics, 25(1):107?130.
Andrea Mulloni and Viktor Pekar. 2006. Automatic detec-
tion of orthographic cues for cognate recognition. In LREC,
pages 2387?2390.
Ari Rappoport and Tsahi Levent-Levi. 2006. Induction of
cross-language affix and letter sequence correspondence. In
EACL Workshop on Cross-Language Knowledge Induction.
Eric Sven Ristad and Peter N. Yianilos. 1998. Learning string-
edit distance. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 20(5):522?532.
Charles Schafer and David Yarowsky. 2002. Inducing transla-
tion lexicons via diverse similarity measures and bridge lan-
guages. In CoNLL, pages 207?216.
Michael Strube, Stefan Rapp, and Christoph Mu?ller. 2002. The
influence of minimum edit distance on reference resolution.
In EMNLP, pages 312?319.
Ben Taskar, Simon Lacoste-Julien, and Dan Klein. 2005. A
discriminative matching approach to word alignment. In
HLT-EMNLP, pages 73?80.
Jo?rg Tiedemann. 1999. Automatic construction of weighted
string similarity measures. In EMNLP-VLC, pages 213?219.
Dmitry Zelenko and Chinatsu Aone. 2006. Discriminative
methods for transliteration. In EMNLP, pages 612?617.
663
Proceedings of ACL-08: HLT, pages 10?18,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Distributional Identification of Non-Referential Pronouns
Shane Bergsma
Department of Computing Science
University of Alberta
Edmonton, Alberta
Canada, T6G 2E8
bergsma@cs.ualberta.ca
Dekang Lin
Google, Inc.
1600 Amphitheatre Parkway
Mountain View
California, 94301
lindek@google.com
Randy Goebel
Department of Computing Science
University of Alberta
Edmonton, Alberta
Canada, T6G 2E8
goebel@cs.ualberta.ca
Abstract
We present an automatic approach to deter-
mining whether a pronoun in text refers to
a preceding noun phrase or is instead non-
referential. We extract the surrounding tex-
tual context of the pronoun and gather, from
a large corpus, the distribution of words that
occur within that context. We learn to reliably
classify these distributions as representing ei-
ther referential or non-referential pronoun in-
stances. Despite its simplicity, experimental
results on classifying the English pronoun it
show the system achieves the highest perfor-
mance yet attained on this important task.
1 Introduction
The goal of coreference resolution is to determine
which noun phrases in a document refer to the same
real-world entity. As part of this task, coreference
resolution systems must decide which pronouns re-
fer to preceding noun phrases (called antecedents)
and which do not. In particular, a long-standing
challenge has been to correctly classify instances of
the English pronoun it. Consider the sentences:
(1) You can make it in advance.
(2) You can make it in Hollywood.
In sentence (1), it is an anaphoric pronoun refer-
ring to some previous noun phrase, like ?the sauce?
or ?an appointment.? In sentence (2), it is part of the
idiomatic expression ?make it? meaning ?succeed.?
A coreference resolution system should find an an-
tecedent for the first it but not the second. Pronouns
that do not refer to preceding noun phrases are called
non-anaphoric or non-referential pronouns.
The word it is one of the most frequent words in
the English language, accounting for about 1% of
tokens in text and over a quarter of all third-person
pronouns.1 Usually between a quarter and a half of
it instances are non-referential (e.g. Section 4, Ta-
ble 3). As with other pronouns, the preceding dis-
course can affect it?s interpretation. For example,
sentence (2) can be interpreted as referential if the
preceding sentence is ?You want to make a movie??
We show, however, that we can reliably classify a
pronoun as being referential or non-referential based
solely on the local context surrounding the pronoun.
We do this by turning the context into patterns and
enumerating all the words that can take the place of
it in these patterns. For sentence (1), we can ex-
tract the context pattern ?make * in advance? and
for sentence (2) ?make * in Hollywood,? where ?*?
is a wildcard that can be filled by any token. Non-
referential distributions tend to have the word it fill-
ing the wildcard position. Referential distributions
occur with many other noun phrase fillers. For ex-
ample, in our n-gram collection (Section 3.4), ?make
it in advance? and ?make them in advance? occur
roughly the same number of times (442 vs. 449), in-
dicating a referential pattern. In contrast, ?make it in
Hollywood? occurs 3421 times while ?make them in
Hollywood? does not occur at all.
These simple counts strongly indicate whether an-
other noun can replace the pronoun. Thus we can
computationally distinguish between a) pronouns
that refer to nouns, and b) all other instances: includ-
ing those that have no antecedent, like sentence (2),
1e.g. http://ucrel.lancs.ac.uk/bncfreq/flists.html
10
and those that refer to sentences, clauses, or implied
topics of discourse. Beyond the practical value of
this distinction, Section 3 provides some theoretical
justification for our binary classification.
Section 3 also shows how to automatically extract
and collect counts for context patterns, and how to
combine the information using a machine learned
classifier. Section 4 describes our data for learning
and evaluation, It-Bank: a set of over three thousand
labelled instances of the pronoun it from a variety
of text sources. Section 4 also explains our com-
parison approaches and experimental methodology.
Section 5 presents our results, including an interest-
ing comparison of our system to human classifica-
tion given equivalent segments of context.
2 Related Work
The difficulty of non-referential pronouns has been
acknowledged since the beginning of computational
resolution of anaphora. Hobbs (1978) notes his algo-
rithm does not handle pronominal references to sen-
tences nor cases where it occurs in time or weather
expressions. Hirst (1981, page 17) emphasizes the
importance of detecting non-referential pronouns,
?lest precious hours be lost in bootless searches
for textual referents.? Mu?ller (2006) summarizes
the evolution of computational approaches to non-
referential it detection. In particular, note the pio-
neering work of Paice and Husk (1987), the inclu-
sion of non-referential it detection in a full anaphora
resolution system by Lappin and Leass (1994), and
the machine learning approach of Evans (2001).
There has recently been renewed interest in
non-referential pronouns, driven by three primary
sources. First of all, research in coreference resolu-
tion has shown the benefits of modules for general
noun anaphoricity determination (Ng and Cardie,
2002; Denis and Baldridge, 2007). Unfortunately,
these studies handle pronouns inadequately; judg-
ing from the decision trees and performance fig-
ures, Ng and Cardie (2002)?s system treats all pro-
nouns as anaphoric by default. Secondly, while
most pronoun resolution evaluations simply exclude
non-referential pronouns, recent unsupervised ap-
proaches (Cherry and Bergsma, 2005; Haghighi and
Klein, 2007) must deal with all pronouns in unre-
stricted text, and therefore need robust modules to
automatically handle non-referential instances. Fi-
nally, reference resolution has moved beyond writ-
ten text into in spoken dialog. Here, non-referential
pronouns are pervasive. Eckert and Strube (2000)
report that in the Switchboard corpus, only 45%
of demonstratives and third-person pronouns have a
noun phrase antecedent. Handling the common non-
referential instances is thus especially vital.
One issue with systems for non-referential detec-
tion is the amount of language-specific knowledge
that must be encoded. Consider a system that jointly
performs anaphora resolution and word alignment
in parallel corpora for machine translation. For this
task, we need to identify non-referential anaphora in
multiple languages. It is not always clear to what
extent the features and modules developed for En-
glish systems apply to other languages. For exam-
ple, the detector of Lappin and Leass (1994) labels a
pronoun as non-referential if it matches one of sev-
eral syntactic patterns, including: ?It is Cogv-ed that
Sentence,? where Cogv is a ?cognitive verb? such
as recommend, think, believe, know, anticipate, etc.
Porting this approach to a new language would re-
quire not only access to a syntactic parser and a list
of cognitive verbs in that language, but the devel-
opment of new patterns to catch non-referential pro-
noun uses that do not exist in English.
Moreover, writing a set of rules to capture this
phenomenon is likely to miss many less-common
uses. Alternatively, recent machine-learning ap-
proaches leverage a more general representation of
a pronoun instance. For example, Mu?ller (2006)
has a feature for ?distance to next complementizer
(that, if, whether)? and features for the tokens and
part-of-speech tags of the context words. Unfor-
tunately, there is still a lot of implicit and explicit
English-specific knowledge needed to develop these
features, including, for example, lists of ?seem?
verbs such as appear, look, mean, happen. Sim-
ilarly, the machine-learned system of Boyd et al
(2005) uses a set of ?idiom patterns? like ?on the
face of it? that trigger binary features if detected in
the pronoun context. Although machine learned sys-
tems can flexibly balance the various indicators and
contra-indicators of non-referentiality, a particular
feature is only useful if it is relevant to an example
in limited labelled training data.
Our approach avoids hand-crafting a set of spe-
11
cific indicator features; we simply use the distribu-
tion of the pronoun?s context. Our method is thus
related to previous work based on Harris (1985)?s
distributional hypothesis.2 It has been used to deter-
mine both word and syntactic path similarity (Hin-
dle, 1990; Lin, 1998a; Lin and Pantel, 2001). Our
work is part of a trend of extracting other important
information from statistical distributions. Dagan and
Itai (1990) use the distribution of a pronoun?s con-
text to determine which candidate antecedents can fit
the context. Bergsma and Lin (2006) determine the
likelihood of coreference along the syntactic path
connecting a pronoun to a possible antecedent, by
looking at the distribution of the path in text. These
approaches, like ours, are ways to inject sophisti-
cated ?world knowledge? into anaphora resolution.
3 Methodology
3.1 Definition
Our approach distinguishes contexts where pro-
nouns cannot be replaced by a preceding noun
phrase (non-noun-referential) from those where
nouns can occur (noun-referential). Although coref-
erence evaluations, such as the MUC (1997) tasks,
also make this distinction, it is not necessarily
used by all researchers. Evans (2001), for exam-
ple, distinguishes between ?clause anaphoric? and
?pleonastic? as in the following two instances:
(3) The paper reported that it had snowed. It was
obvious. (clause anaphoric)
(4) It was obvious that it had snowed. (pleonastic)
The word It in sentence (3) is considered referen-
tial, while the word It in sentence (4) is considered
non-referential.3 From our perspective, this inter-
pretation is somewhat arbitrary. One could also say
that the It in both cases refers to the clause ?that it
had snowed.? Indeed, annotation experiments using
very fine-grained categories show low annotation re-
liability (Mu?ller, 2006). On the other hand, there
is no debate over the importance nor the definition
of distinguishing pronouns that refer to nouns from
those that do not. We adopt this distinction for our
2Words occurring in similar contexts have similar meanings
3The it in ?it had snowed? is, of course, non-referential.
work, and show it has good inter-annotator reliabil-
ity (Section 4.1). We henceforth refer to non-noun-
referential simply as non-referential, and thus con-
sider the word It in both sentences (3) and (4) as
non-referential.
Non-referential pronouns are widespread in nat-
ural language. The es in the German ?Wie geht es
Ihnen? and the il in the French ?S?il vous pla??t? are
both non-referential. In pro-drop languages that may
omit subject pronouns, there remains the question
of whether an omitted pronoun is referential (Zhao
and Ng, 2007). Although we focus on the English
pronoun it, our approach should differentiate any
words that have both a structural and a referential
role in language, e.g. words like this, there and
that (Mu?ller, 2007). We believe a distributional ap-
proach could also help in related tasks like identify-
ing the generic use of you (Gupta et al, 2007).
3.2 Context Distribution
Our method extracts the context surrounding a pro-
noun and determines which other words can take the
place of the pronoun in the context. The extracted
segments of context are called context patterns. The
words that take the place of the pronoun are called
pattern fillers. We gather pattern fillers from a large
collection of n-gram frequencies. The maximum
size of a context pattern depends on the size of n-
grams available in the data. In our n-gram collection
(Section 3.4), the lengths of the n-grams range from
unigrams to 5-grams, so our maximum pattern size
is five. For a particular pronoun in text, there are five
possible 5-grams that span the pronoun. For exam-
ple, in the following instance of it:
... said here Thursday that it is unnecessary to continue ...
We can extract the following 5-gram patterns:
said here Thursday that *
here Thursday that * is
Thursday that * is unnecessary
that * is unnecessary to
* is unnecessary to continue
Similarly, we extract the four 4-gram patterns.
Shorter n-grams were not found to improve perfor-
mance on development data and hence are not ex-
tracted. We only use context within the current sen-
tence (including the beginning-of-sentence and end-
of-sentence tokens) so if a pronoun occurs near a
sentence boundary, some patterns may be missing.
12
Pattern Filler Type String
#1: 3rd-person pron. sing. it/its
#2: 3rd-person pron. plur. they/them/their
#3: any other pronoun he/him/his/,
I/me/my, etc.
#4: infrequent word token ?UNK?
#5: any other token *
Table 1: Pattern filler types
We take a few steps to improve generality. We
change the patterns to lower-case, convert sequences
of digits to the # symbol, and run the Porter stem-
mer4 (Porter, 1980). To generalize rare names, we
convert capitalized words longer than five charac-
ters to a special NE tag. We also added a few simple
rules to stem the irregular verbs be, have, do, and
said, and convert the common contractions ?nt, ?s,
?m, ?re, ?ve, ?d, and ?ll to their most likely stem.
We do the same processing to our n-gram corpus.
We then find all n-grams matching our patterns, al-
lowing any token to match the wildcard in place of
it. Also, other pronouns in the pattern are allowed
to match a corresponding pronoun in an n-gram, re-
gardless of differences in inflection and class.
We now discuss how to use the distribution of pat-
tern fillers. For identifying non-referential it in En-
glish, we are interested in how often it occurs as a
pattern filler versus other nouns. However, deter-
mining part-of-speech in a large n-gram corpus is
not simple, nor would it easily extend to other lan-
guages. Instead, we gather counts for five differ-
ent classes of words that fill the wildcard position,
easily determined by string match (Table 1). The
third-person plural they (#2) reliably occurs in pat-
terns where referential it also resides. The occur-
rence of any other pronoun (#3) guarantees that at
the very least the pattern filler is a noun. A match
with the infrequent word token ?UNK? (#4) (ex-
plained in Section 3.4) will likely be a noun because
nouns account for a large proportion of rare words in
a corpus. Gathering any other token (#5) also mostly
finds nouns; inserting another part-of-speech usually
4Adapted from the Bow-toolkit (McCallum, 1996). Our
method also works without the stemmer; we simply truncate
the words in the pattern at a given maximum length (see Sec-
tion 5.1). With simple truncation, all the pattern processing can
be easily applied to other languages.
Pattern Filler Counts#1 #2 #3 #5
sai here NE that * 84 0 291 3985
here NE that * be 0 0 0 93
NE that * be unnecessari 0 0 0 0
that * be unnecessari to 16726 56 0 228
* be unnecessari to continu 258 0 0 0
Table 2: 5-gram context patterns and pattern-filler counts
for the Section 3.2 example.
results in an unlikely, ungrammatical pattern.
Table 2 gives the stemmed context patterns for our
running example. It also gives the n-gram counts
of pattern fillers matching the first four filler types
(there were no matches of the ?UNK? type, #4).
3.3 Feature Vector Representation
There are many possible ways to use the above
counts. Intuitively, our method should identify as
non-referential those instances that have a high pro-
portion of fillers of type #1 (i.e., the word it), while
labelling as referential those with high counts for
other types of fillers. We would also like to lever-
age the possibility that some of the patterns may be
more predictive than others, depending on where the
wildcard lies in the pattern. For example, in Table 2,
the cases where the it-position is near the beginning
of the pattern best reflect the non-referential nature
of this instance. We can achieve these aims by or-
dering the counts in a feature vector, and using a la-
belled set of training examples to learn a classifier
that optimally weights the counts.
For classification, we define non-referential as
positive and referential as negative. Our feature rep-
resentation very much resembles Table 2. For each
of the five 5-gram patterns, ordered by the position
of the wildcard, we have features for the logarithm
of counts for filler types #1, #2, ... #5. Similarly,
for each of the four 4-gram patterns, we provide the
log-counts corresponding to types #1, #2, ... #5 as
well. Before taking the logarithm, we smooth the
counts by adding a fixed number to all observed val-
ues. We also provide, for each pattern, a feature that
indicates if the pattern is not available because the
it-position would cause the pattern to span beyond
the current sentence. There are twenty-five 5-gram,
twenty 4-gram, and nine indicator features in total.
13
Our classifier should learn positive weights on the
type #1 counts and negative weights on the other
types, with higher absolute weights on the more pre-
dictive filler types and pattern positions. Note that
leaving the pattern counts unnormalized automati-
cally allows patterns with higher counts to contribute
more to the prediction of their associated instances.
3.4 N-Gram Data
We now describe the collection of n-grams and their
counts used in our implementation. We use, to our
knowledge, the largest publicly available collection:
the Google Web 1T 5-gram Corpus Version 1.1.5
This collection was generated from approximately 1
trillion tokens of online text. In this data, tokens ap-
pearing less than 200 times have been mapped to the
?UNK? symbol. Also, only n-grams appearing more
than 40 times are included. For languages where
such an extensive n-gram resource is not available,
the n-gram counts could also be taken from the page-
counts returned by an Internet search engine.
4 Evaluation
4.1 Labelled It Data
We need labelled data for training and evaluation of
our system. This data indicates, for every occurrence
of the pronoun it, whether it refers to a preceding
noun phrase or not. Standard coreference resolution
data sets annotate all noun phrases that have an an-
tecedent noun phrase in the text. Therefore, we can
extract labelled instances of it from these sets. We
do this for the dry-run and formal sets from MUC-7
(1997), and merge them into a single data set.
Of course, full coreference-annotated data is a
precious resource, with the pronoun it making up
only a small portion of the marked-up noun phrases.
We thus created annotated data specifically for the
pronoun it. We annotated 1020 instances in a col-
lection of Science News articles (from 1995-2000),
downloaded from the Science News website. We
also annotated 709 instances in the WSJ portion of
the DARPA TIPSTER Project (Harman, 1992), and
279 instances in the English portion of the Europarl
Corpus (Koehn, 2005).
A single annotator (A1) labelled all three datasets, while two additional annotators not connected
5Available from the LDC as LDC2006T13
Data Set Number of It % Non-Referential
Europarl 279 50.9
Sci-News 1020 32.6
WSJ 709 25.1
MUC 129 31.8
Train 1069 33.2
Test 1067 31.7
Test-200 200 30.0
Table 3: Data sets used in experiments.
with the project (A2 and A3) were asked to sepa-rately re-annotate a portion of each, so that inter-
annotator agreement could be calculated. A1 and
A2 agreed on 96% of annotation decisions, while
A1-A3, and A2-A3, agreed on 91% and 93% of de-cisions, respectively. The Kappa statistic (Jurafsky
and Martin, 2000, page 315), with P(E) computed
from the confusion matrices, was a high 0.90 for A1-
A2, and 0.79 and 0.81 for the other pairs, around the0.80 considered to be good reliability. These are,
perhaps surprisingly, the only known it-annotation-
agreement statistics available for written text. They
contrast favourably with the low agreement seen on
categorizing it in spoken dialog (Mu?ller, 2006).
We make all the annotations available in It-Bank,
an online repository for annotated it-instances.6
It-Bank also allows other researchers to distribute
their it annotations. Often, the full text of articles
containing annotations cannot be shared because of
copyright. However, sharing just the sentences con-
taining the word it, randomly-ordered, is permissible
under fair-use guidelines. The original annotators
retain their copyright on the annotations.
We use our annotated data in two ways. First
of all, we perform cross-validation experiments on
each of the data sets individually, to help gauge the
difficulty of resolution on particular domains and
volumes of training data. Secondly, we randomly
distribute all instances into two main sets, a training
set and a test set. We also construct a smaller test
set, Test-200, containing only the first 200 instances
in the Test set. We use Test-200 for human experi-
ments and error analysis (Section 5.2). Table 3 sum-
marizes all the sets used in the experiments.
6www.cs.ualberta.ca/?bergsma/ItBank/. It-Bank also con-
tains an additional 1,077 examples used as development data.
14
4.2 Comparison Approaches
We represent feature vectors exactly as described
in Section 3.3. We smooth by adding 40 to all
counts, equal to the minimum count in the n-gram
data. For classification, we use a maximum entropy
model (Berger et al, 1996), from the logistic re-
gression package in Weka (Witten and Frank, 2005),
with all default parameter settings. Results with
our distributional approach are labelled as DISTRIB.
Note that our maximum entropy classifier actually
produces a probability of non-referentiality, which
is thresholded at 50% to make a classification.
As a baseline, we implemented the non-referential
it detector of Lappin and Leass (1994), labelled as
LL in the results. This is a syntactic detector, a
point missed by Evans (2001) in his criticism: the
patterns are robust to intervening words and modi-
fiers (e.g. ?it was never thought by the committee
that...?) provided the sentence is parsed correctly.7
We automatically parse sentences with Minipar, a
broad-coverage dependency parser (Lin, 1998b).
We also use a separate, extended version of
the LL detector, implemented for large-scale non-
referential detection by Cherry and Bergsma (2005).
This system, also for Minipar, additionally detects
instances of it labelled with Minipar?s pleonastic cat-
egory Subj. It uses Minipar?s named-entity recog-
nition to identify time expressions, such as ?it was
midnight,? and provides a number of other patterns
to match common non-referential it uses, such as
in expressions like ?darn it,? ?don?t overdo it,? etc.
This extended detector is labelled as MINIPL (for
Minipar pleonasticity) in our results.
Finally, we tested a system that combines the
above three approaches. We simply add the LL and
MINIPL decisions as binary features in the DISTRIB
system. This system is called COMBO in our results.
4.3 Evaluation Criteria
We follow Mu?ller (2006)?s evaluation criteria. Pre-
cision (P) is the proportion of instances that we la-
bel as non-referential that are indeed non-referential.
Recall (R) is the proportion of true non-referentials
that we detect, and is thus a measure of the coverage
7Our approach, on the other hand, would seem to be suscep-
tible to such intervening material, if it pushes indicative context
tokens out of the 5-token window.
System P R F Acc
LL 93.4 21.0 34.3 74.5
MINIPL 66.4 49.7 56.9 76.1
DISTRIB 81.4 71.0 75.8 85.7
COMBO 81.3 73.4 77.1 86.2
Table 4: Train/Test-split performance (%).
of the system. F-Score (F) is the geometric average
of precision and recall; it is the most common non-
referential detection metric. Accuracy (Acc) is the
percentage of instances labelled correctly.
5 Results
5.1 System Comparison
Table 4 gives precision, recall, F-score, and accu-
racy on the Train/Test split. Note that while the LL
system has high detection precision, it has very low
recall, sharply reducing F-score. The MINIPL ap-
proach sacrifices some precision for much higher
recall, but again has fairly low F-score. To our
knowledge, our COMBO system, with an F-Score
of 77.1%, achieves the highest performance of any
non-referential system yet implemented. Even more
importantly, DISTRIB, which requires only minimal
linguistic processing and no encoding of specific in-
dicator patterns, achieves 75.8% F-Score. The dif-
ference between COMBO and DISTRIB is not statis-
tically significant, while both are significantly bet-
ter than the rule-based approaches.8 This provides
strong motivation for a ?light-weight? approach to
non-referential it detection ? one that does not re-
quire parsing or hand-crafted rules and ? is easily
ported to new languages and text domains.
Since applying an English stemmer to the con-
text words (Section 3.2) reduces the portability of
the distributional technique, we investigated the use
of more portable pattern abstraction. Figure 1 com-
pares the use of the stemmer to simply truncating the
words in the patterns at a certain maximum length.
Using no truncation (Unaltered) drops the F-Score
by 4.3%, while truncating the patterns to a length of
four only drops the F-Score by 1.4%, a difference
which is not statistically significant. Simple trunca-
tion may be a good option for other languages where
stemmers are not readily available. The optimum
8All significance testing uses McNemar?s test, p<0.05
15
 68
 70
 72
 74
 76
 78
 80
 1  2  3  4  5  6  7  8  9  10
F-
Sc
or
e
Truncated word length
Stemmed patterns
Truncated patterns
Unaltered patterns
Figure 1: Effect of pattern-word truncation on non-
referential it detection (COMBO system, Train/Test split).
System Europl. Sci-News WSJ MUC
LL 44.0 39.3 21.5 13.3
MINIPL 70.3 61.8 22.0 50.7
DISTRIB 79.7 77.2 69.5 68.2
COMBO 76.2 78.7 68.1 65.9
COMBO4 83.6 76.5 67.1 74.7
Table 5: 10-fold cross validation F-Score (%).
truncation size will likely depend on the length of
the base forms of words in that language. For real-
world application of our approach, truncation also
reduces the table sizes (and thus storage and look-
up costs) of any pre-compiled it-pattern database.
Table 5 compares the 10-fold cross-validation F-
score of our systems on the four data sets. The
performance of COMBO on Europarl and MUC is
affected by the small number of instances in these
sets (Section 4, Table 3). We can reduce data frag-
mentation by removing features. For example, if we
only use the length-4 patterns in COMBO (labelled as
COMBO4), performance increases dramatically on
Europarl and MUC, while dipping slightly for the
larger Sci-News and WSJ sets. Furthermore, select-
ing just the three most useful filler type counts as
features (#1,#2,#5), boosts F-Score on Europarl to
86.5%, 10% above the full COMBO system.
5.2 Analysis and Discussion
In light of these strong results, it is worth consid-
ering where further gains in performance might yet
be found. One key question is to what extent a lim-
ited context restricts identification performance. We
first tested the importance of the pattern length by
System P R F Acc
DISTRIB 80.0 73.3 76.5 86.5
COMBO 80.7 76.7 78.6 87.5
Human-1 92.7 63.3 75.2 87.5
Human-2 84.0 70.0 76.4 87.0
Human-3 72.2 86.7 78.8 86.0
Table 6: Evaluation on Test-200 (%).
using only the length-4 counts in the DISTRIB sys-
tem (Train/Test split). Surprisingly, the drop in F-
Score was only one percent, to 74.8%. Using only
the length-5 counts drops F-Score to 71.4%. Neither
are statistically significant; however there seems to
be diminishing returns from longer context patterns.
Another way to view the limited context is to ask,
given the amount of context we have, are we mak-
ing optimum use of it? We answer this by seeing
how well humans can do with the same information.
As explained in Section 3.2, our system uses 5-gram
context patterns that together span from four-to-the-
left to four-to-the-right of the pronoun. We thus pro-
vide these same nine-token windows to our human
subjects, and ask them to decide whether the pro-
nouns refer to previous noun phrases or not, based
on these contexts. Subjects first performed a dry-
run experiment on separate development data. They
were shown their errors and sources of confusion
were clarified. They then made the judgments unas-
sisted on the final Test-200 data. Three humans per-
formed the experiment. Their results show a range
of preferences for precision versus recall, with both
F-Score and Accuracy on average below the perfor-
mance of COMBO (Table 6). Foremost, these results
show that our distributional approach is already get-
ting good leverage from the limited context informa-
tion, around that achieved by our best human.
It is instructive to inspect the twenty-five Test-200
instances that the COMBO system classified incor-
rectly, given human performance on this same set.
Seventeen of the twenty-five COMBO errors were
also made by one or more human subjects, suggest-
ing system errors are also mostly due to limited con-
text. For example, one of these errors was for the
context: ?it takes an astounding amount...? Here, the
non-referential nature of the instance is not apparent
without the infinitive clause that ends the sentence:
?... of time to compare very long DNA sequences
16
with each other.?
Six of the eight errors unique to the COMBO sys-
tem were cases where the system falsely said the
pronoun was non-referential. Four of these could
have referred to entire sentences or clauses rather
than nouns. These confusing cases, for both hu-
mans and our system, result from our definition
of a referential pronoun: pronouns with verbal or
clause antecedents are considered non-referential
(Section 3.1). If an antecedent verb or clause is
replaced by a nominalization (Smith researched...
to Smith?s research), a referring pronoun, in the
same context, becomes referential. When we inspect
the probabilities produced by the maximum entropy
classifier (Section 4.2), we see only a weak bias for
the non-referential class on these examples, reflect-
ing our classifier?s uncertainty. It would likely be
possible to improve accuracy on these cases by en-
coding the presence or absence of preceding nomi-
nalizations as a feature of our classifier.
Another false non-referential decision is for the
phrase ?... machine he had installed it on.? The it is
actually referential, but the extracted patterns (e.g.
?he had install * on?) are nevertheless usually filled
with it.9 Again, it might be possible to fix such ex-
amples by leveraging the preceding discourse. No-
tably, the first noun-phrase before the context is the
word ?software.? There is strong compatibility be-
tween the pronoun-parent ?install? and the candidate
antecedent ?software.? In a full coreference resolu-
tion system, when the anaphora resolution module
has a strong preference to link it to an antecedent
(which it should when the pronoun is indeed refer-
ential), we can override a weak non-referential prob-
ability. Non-referential it detection should not be
a pre-processing step, but rather part of a globally-
optimal configuration, as was done for general noun
phrase anaphoricity by Denis and Baldridge (2007).
The suitability of this kind of approach to correct-
ing some of our system?s errors is especially obvious
when we inspect the probabilities of the maximum
entropy model?s output decisions on the Test-200
set. Where the maximum entropy classifier makes
mistakes, it does so with less confidence than when
it classifies correct examples. The average predicted
9This example also suggests using filler counts for the word
?the? as a feature when it is the last word in the pattern.
probability of the incorrect classifications is 76.0%
while the average probability of the correct classi-
fications is 90.3%. Many incorrect decisions are
ready to switch sides; our next step will be to use
features of the preceding discourse and the candi-
date antecedents to help give them a push.
6 Conclusion
We have presented an approach to detecting non-
referential pronouns in text based on the distribu-
tion of the pronoun?s context. The approach is sim-
ple to implement, attains state-of-the-art results, and
should be easily ported to other languages. Our tech-
nique demonstrates how large volumes of data can
be used to gather world knowledge for natural lan-
guage processing. A consequence of this research
was the creation of It-Bank, a collection of thou-
sands of labelled examples of the pronoun it, which
will benefit other coreference resolution researchers.
Error analysis reveals that our system is getting
good leverage out of the pronoun context, achiev-
ing results comparable to human performance given
equivalent information. To boost performance fur-
ther, we will need to incorporate information from
preceding discourse. Future research will also test
the distributional classification of other ambiguous
pronouns, like this, you, there, and that. Another
avenue of study will look at the interaction between
coreference resolution and machine translation. For
example, if a single form in English (e.g. that)
is separated into different meanings in another lan-
guage (e.g., Spanish demonstrative ese, nominal ref-
erence e?se, abstract or statement reference eso, and
complementizer que), then aligned examples pro-
vide automatically-disambiguated English data. We
could extract context patterns and collect statistics
from these examples like in our current approach.
In general, jointly optimizing translation and coref-
erence is an exciting and largely unexplored re-
search area, now partly enabled by our portable non-
referential detection methodology.
Acknowledgments
We thank Kristin Musselman and Christopher Pinchak for as-
sistance preparing the data, and we thank Google Inc. for shar-
ing their 5-gram corpus. We gratefully acknowledge support
from the Natural Sciences and Engineering Research Council
of Canada, the Alberta Ingenuity Fund, and the Alberta Infor-
matics Circle of Research Excellence.
17
References
Adam L. Berger, Stephen A. Della Pietra, and Vincent
J. Della Pietra. 1996. A maximum entropy approach
to natural language processing. Computational Lin-
guistics, 22(1):39?71.
Shane Bergsma and Dekang Lin. 2006. Bootstrap-
ping path-based pronoun resolution. In COLING-
ACL, pages 33?40.
Adrianne Boyd, Whitney Gegg-Harrison, and Donna By-
ron. 2005. Identifying non-referential it: a machine
learning approach incorporating linguistically moti-
vated patterns. In ACL Workshop on Feature Engi-
neering for Machine Learning in NLP, pages 40?47.
Colin Cherry and Shane Bergsma. 2005. An expecta-
tion maximization approach to pronoun resolution. In
CoNLL, pages 88?95.
Ido Dagan and Alan Itai. 1990. Automatic processing of
large corpora for the resolution of anaphora references.
In COLING, volume 3, pages 330?332.
Pascal Denis and Jason Baldridge. 2007. Joint determi-
nation of anaphoricity and coreference using integer
programming. In NAACL-HLT, pages 236?243.
Miriam Eckert and Michael Strube. 2000. Dialogue acts,
synchronizing units, and anaphora resolution. Journal
of Semantics, 17(1):51?89.
Richard Evans. 2001. Applying machine learning to-
ward an automatic classification of it. Literary and
Linguistic Computing, 16(1):45?57.
Surabhi Gupta, Matthew Purver, and Dan Jurafsky. 2007.
Disambiguating between generic and referential ?you?
in dialog. In ACL Demo and Poster Sessions, pages
105?108.
Aria Haghighi and Dan Klein. 2007. Unsupervised
coreference resolution in a nonparametric Bayesian
model. In ACL, pages 848?855.
Donna Harman. 1992. The DARPA TIPSTER project.
ACM SIGIR Forum, 26(2):26?28.
Zellig Harris. 1985. Distributional structure. In J.J.
Katz, editor, The Philosophy of Linguistics, pages 26?
47. Oxford University Press, New York.
Donald Hindle. 1990. Noun classification from
predicate-argument structures. In ACL, pages 268?
275.
Graeme Hirst. 1981. Anaphora in Natural Language
Understanding: A Survey. Springer Verlag.
Jerry Hobbs. 1978. Resolving pronoun references. Lin-
gua, 44(311):339?352.
Daniel Jurafsky and James H. Martin. 2000. Speech and
language processing. Prentice Hall.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In MT Summit X, pages
79?86.
Shalom Lappin and Herbert J. Leass. 1994. An algo-
rithm for pronominal anaphora resolution. Computa-
tional Linguistics, 20(4):535?561.
Dekang Lin and Patrick Pantel. 2001. Discovery of infer-
ence rules for question answering. Natural Language
Engineering, 7(4):343?360.
Dekang Lin. 1998a. Automatic retrieval and clustering
of similar words. In COLING-ACL, pages 768?773.
Dekang Lin. 1998b. Dependency-based evaluation of
MINIPAR. In LREC Workshop on the Evaluation of
Parsing Systems.
Andrew Kachites McCallum. 1996. Bow:
A toolkit for statistical language modeling,
text retrieval, classification and clustering.
http://www.cs.cmu.edu/?mccallum/bow.
MUC-7. 1997. Coreference task definition (v3.0, 13 Jul
97). In Proceedings of the Seventh Message Under-
standing Conference (MUC-7).
Christoph Mu?ller. 2006. Automatic detection of non-
referential It in spoken multi-party dialog. In EACL,
pages 49?56.
Christoph Mu?ller. 2007. Resolving It, This, and That in
unrestricted multi-party dialog. In ACL, pages 816?
823.
Vincent Ng and Claire Cardie. 2002. Identifying
anaphoric and non-anaphoric noun phrases to improve
coreference resolution. In COLING, pages 730?736.
Chris D. Paice and Gareth D. Husk. 1987. Towards the
automatic recognition of anaphoric features in English
text: the impersonal pronoun ?it?. Computer Speech
and Language, 2:109?132.
Martin F. Porter. 1980. An algorithm for suffix stripping.
Program, 14(3):130?137.
Ian H. Witten and Eibe Frank. 2005. Data Mining: Prac-
tical machine learning tools and techniques. Morgan
Kaufmann, second edition.
Shanheng Zhao and Hwee Tou Ng. 2007. Identification
and resolution of Chinese zero pronouns: A machine
learning approach. In EMNLP, pages 541?550.
18
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 118?126,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
A Ranking Approach to Stress Prediction
for Letter-to-Phoneme Conversion
Qing Dou, Shane Bergsma, Sittichai Jiampojamarn and Grzegorz Kondrak
Department of Computing Science
University of Alberta
Edmonton, AB, T6G 2E8, Canada
{qdou,bergsma,sj,kondrak}@cs.ualberta.ca
Abstract
Correct stress placement is important in
text-to-speech systems, in terms of both
the overall accuracy and the naturalness of
pronunciation. In this paper, we formu-
late stress assignment as a sequence pre-
diction problem. We represent words as
sequences of substrings, and use the sub-
strings as features in a Support Vector Ma-
chine (SVM) ranker, which is trained to
rank possible stress patterns. The rank-
ing approach facilitates inclusion of arbi-
trary features over both the input sequence
and output stress pattern. Our system ad-
vances the current state-of-the-art, predict-
ing primary stress in English, German, and
Dutch with up to 98% word accuracy on
phonemes, and 96% on letters. The sys-
tem is also highly accurate in predicting
secondary stress. Finally, when applied in
tandem with an L2P system, it substan-
tially reduces the word error rate when
predicting both phonemes and stress.
1 Introduction
In many languages, certain syllables in words are
phonetically more prominent in terms of duration,
pitch, and loudness. This phenomenon is referred
to as lexical stress. In some languages, the loca-
tion of stress is entirely predictable. For example,
lexical stress regularly falls on the initial syllable
in Hungarian, and on the penultimate syllable in
Polish. In other languages, such as English and
Russian, any syllable in the word can be stressed.
Correct stress placement is important in text-
to-speech systems because it affects the accuracy
of human word recognition (Tagliapietra and Ta-
bossi, 2005; Arciuli and Cupples, 2006). How-
ever, the issue has often been ignored in previ-
ous letter-to-phoneme (L2P) systems. The sys-
tems that do generate stress markers often do not
report separate figures on stress prediction accu-
racy, or they only provide results on a single lan-
guage. Some only predict primary stress mark-
ers (Black et al, 1998; Webster, 2004; Demberg
et al, 2007), while those that predict both primary
and secondary stress generally achieve lower ac-
curacy (Bagshaw, 1998; Coleman, 2000; Pearson
et al, 2000).
In this paper, we formulate stress assignment as
a sequence prediction problem. We divide each
word into a sequence of substrings, and use these
substrings as features for a Support Vector Ma-
chine (SVM) ranker. For a given sequence length,
there is typically only a small number of stress
patterns in use. The task of the SVM is to rank
the true stress pattern above the small number of
acceptable alternatives. This is the first system
to predict stress within a powerful discriminative
learning framework. By using a ranking approach,
we enable the use of arbitrary features over the en-
tire (input) sequence and (output) stress pattern.
We show that the addition of a feature for the en-
tire output sequence improves prediction accuracy.
Our experiments on English, German, and
Dutch demonstrate that our ranking approach sub-
stantially outperforms previous systems. The
SVM ranker achieves exceptional 96.2% word ac-
curacy on the challenging task of predicting the
full stress pattern in English. Moreover, when
combining our stress predictions with a state-of-
the-art L2P system (Jiampojamarn et al, 2008),
we set a new standard for the combined prediction
of phonemes and stress.
The paper is organized as follows. Section 2
provides background on lexical stress and a task
definition. Section 3 presents our automatic stress
prediction algorithm. In Section 4, we confirm the
power of the discriminative approach with experi-
ments on three languages. Section 5 describes how
stress is integrated into L2P conversion.
118
2 Background and Task Definition
There is a long history of research into the prin-
ciples governing lexical stress placement. Zipf
(1929) showed that stressed syllables are of-
ten those with low frequency in speech, while
unstressed syllables are usually very common.
Chomsky and Halle (1968) proposed a set of
context-sensitive rules for producing English
stress from underlying word forms. Due to its
importance in text-to-speech, there is also a long
history of computational stress prediction sys-
tems (Fudge, 1984; Church, 1985; Williams,
1987). While these early approaches depend
on human definitions of vowel tensity, syllable
weight, word etymology, etc., our work follows
a recent trend of purely data-driven approaches to
stress prediction (Black et al, 1998; Pearson et al,
2000; Webster, 2004; Demberg et al, 2007).
In many languages, only two levels of stress
are distinguished: stressed and unstressed. How-
ever, some languages exhibit more than two levels
of stress. For example, in the English word eco-
nomic, the first and the third syllable are stressed,
with the former receiving weaker emphasis than
the latter. In this case, the initial syllable is said
to carry a secondary stress. Although each word
has only one primary stress, it may have any num-
ber of secondary stresses. Predicting the full stress
pattern is therefore inherently more difficult than
predicting the location of primary stress only.
Our objective is to automatically assign primary
and, where possible, secondary stress to out-of-
vocabulary words. Stress is an attribute of sylla-
bles, but syllabification is a non-trivial task in it-
self (Bartlett et al, 2008). Rather than assuming
correct syllabification of the input word, we in-
stead follow Webster (2004) in placing the stress
on the vowel which constitutes the nucleus of the
stressed syllable. If the syllable boundaries are
known, the mapping from the vowel to the cor-
responding syllable is straightforward.
We investigate the assignment of stress to two
related but different entities: the spoken word
(represented by its phonetic transcription), and
the written word (represented by its orthographic
form). Although stress is a prosodic feature, as-
signing stress to written words (?stressed orthog-
raphy?) has been utilized as a preprocessing stage
for the L2P task (Webster, 2004). This prepro-
cessing is motivated by two factors. First, stress
greatly influences the pronunciation of vowels in
English (c.f., allow vs. alloy). Second, since
phoneme predictors typically utilize only local
context around a letter, they do not incorporate the
global, long-range information that is especially
predictive of stress, such as penultimate syllable
emphasis associated with the suffix -ation. By tak-
ing stressed orthography as input, the L2P system
is able to implicitly leverage morphological infor-
mation beyond the local context.
Indicating stress on letters can also be help-
ful to humans, especially second-language learn-
ers. In some languages, such as Spanish, ortho-
graphic markers are obligatory in words with ir-
regular stress. The location of stress is often ex-
plicitly marked in textbooks for students of Rus-
sian. In both languages, the standard method of
indicating stress is to place an acute accent above
the vowel bearing primary stress, e.g., adio?s. The
secondary stress in English can be indicated with
a grave accent (Coleman, 2000), e.g., pre`ce?de.
In summary, our task is to assign primary and
secondary stress markers to stress-bearing vowels
in an input word. The input word may be either
phonemes or letters. If a stressed vowel is repre-
sented by more than one letter, we adopt the con-
vention of marking the first vowel of the vowel se-
quence, e.g., me?eting. In this way, we are able to
focus on the task of stress prediction, without hav-
ing to determine at the same time the exact sylla-
ble boundaries, or whether a vowel letter sequence
represents one or more spoken vowels (e.g., beat-
ing vs. be-at-i-fy).
3 Automatic Stress Prediction
Our stress assignment system maps a word, w, to a
stressed-form of the word, w?. We formulate stress
assignment as a sequence prediction problem. The
assignment is made in three stages:
(1) First, we map words to substrings (s), the ba-
sic units in our sequence (Section 3.1).
(2) Then, a particular stress pattern (t) is chosen
for each substring sequence. We use a sup-
port vector machine (SVM) to rank the possi-
ble patterns for each sequence (Section 3.2).
(3) Finally, the stress pattern is used to produce
the stressed-form of the word (Section 3.3).
Table 1 gives examples of words at each stage of
the algorithm. We discuss each step in more detail.
119
Word Substrings Pattern Word?
w ? s ? t ? w?
worker ? wor-ker ? 1-0 ? wo?rker
overdo ? ov-ver-do ? 2-0-1 ? o`verdo?
react ? re-ac ? 0-1 ? rea?ct
?bstr?kt ? ?b-r?k ? 0-1 ? ?bstr??kt
prisid ? ri-sid ? 2-1 ? pr?`s??d
Table 1: The steps in our stress prediction sys-
tem (with orthographic and phonetic prediction
examples): (1) word splitting, (2) support vector
ranking of stress patterns, and (3) pattern-to-vowel
mapping.
3.1 Word Splitting
The first step in our approach is to represent the
word as a sequence of N individual units: w ?
s = {s1-s2-...-sN}. These units are used to define
the features and outputs used by the SVM ranker.
Although we are ultimately interested in assigning
stress to individual vowels in the phoneme and let-
ter sequence, it is beneficial to represent the task in
units larger than individual letters.
Our substrings are similar to syllables; they
have a vowel as their nucleus and include con-
sonant context. By approximating syllables, our
substring patterns will allow us to learn recur-
rent stress regularities, as well as dependencies
between neighboring substrings. Since determin-
ing syllable breaks is a non-trivial task, we in-
stead adopt the following simple splitting tech-
nique. Each vowel in the word forms the nucleus
of a substring. Any single preceding or follow-
ing consonant is added to the substring unit. Thus,
each substring consists of at most three symbols
(Table 1).
Using shorter substrings reduces the sparsity of
our training data; words like cryer, dryer and fryer
are all mapped to the same form: ry-er. The
SVM can thus generalize from observed words to
similarly-spelled, unseen examples.
Since the number of vowels equals the num-
ber of syllables in the phonetic form of the word,
applying this approach to phonemes will always
generate the correct number of syllables. For let-
ters, splitting may result in a different number of
units than the true syllabification, e.g., pronounce
? ron-no-un-ce. This does not prevent the system
from producing the correct stress assignment after
the pattern-to-vowel mapping stage (Section 3.3)
is complete.
3.2 Stress Prediction with SVM Ranking
After creating a sequence of substring units, s =
{s1-s2-...-sN}, the next step is to choose an out-
put sequence, t = {t1-t2-...-tN}, that encodes
whether each unit is stressed or unstressed. We
use the number ?1? to indicate that a substring re-
ceives primary stress, ?2? for secondary stress, and
?0? to indicate no stress. We call this output se-
quence the stress pattern for a word. Table 1 gives
examples of words, substrings, and stress patterns.
We use supervised learning to train a system to
predict the stress pattern. We generate training
(s, t) pairs in the obvious way from our stress-
marked training words, w?. That is, we first ex-
tract the letter/phoneme portion, w, and use it
to create the substrings, s. We then create the
stress pattern, t, using w??s stress markers. Given
the training pairs, any sequence predictor can be
used, for example a Conditional Random Field
(CRF) (Lafferty et al, 2001) or a structured per-
ceptron (Collins, 2002). However, we can take
advantage of a unique property of our problem to
use a more expressive framework than is typically
used in sequence prediction.
The key observation is that the output space of
possible stress patterns is actually fairly limited.
Clopper (2002) shows that people have strong
preferences for particular sequences of stress, and
this is confirmed by our training data (Section 4.1).
In English, for example, we find that for each set
of spoken words with the same number of sylla-
bles, there are no more than fifteen different stress
patterns. In total, among 55K English training ex-
amples, there are only 70 different stress patterns.
In both German and Dutch there are only about
50 patterns in 250K examples.1 Therefore, for a
particular input sequence, we can safely limit our
consideration to only the small set of output pat-
terns of the same length.
Thus, unlike typical sequence predictors, we do
not have to search for the highest-scoring output
according to our model. We can enumerate the
full set of outputs and simply choose the highest-
scoring one. This enables a more expressive rep-
resentation. We can define arbitrary features over
the entire output sequence. In a typical CRF or
structured perceptron approach, only output fea-
tures that can be computed incrementally during
search are used (e.g. Markov transition features
that permit Viterbi search). Since search is not
1See (Dou, 2009) for more details.
120
needed here, we can exploit longer-range features.
Choosing the highest-scoring output from a
fixed set is a ranking problem, and we provide the
full ranking formulation below. Unlike previous
ranking approaches (e.g. Collins and Koo (2005)),
we do not rely on a generative model to produce
a list of candidates. Candidates are chosen in ad-
vance from observed training patterns.
3.2.1 Ranking Formulation
For a substring sequence, s, of length N , our task
is to select the correct output pattern from the set
of all length-N patterns observed in our training
data, a set we denote as TN . We score each possi-
ble input-output combination using a linear model.
Each substring sequence and possible output pat-
tern, (s, t), is represented with a set of features,
?(s, t). The score for a particular (s, t) combina-
tion is a weighted sum of these features, ???(s, t).
The specific features we use are described in Sec-
tion 3.2.2.
Let tj be the stress pattern for the jth training
sequence sj , both of length N . At training time,
the weights, ?, are chosen such that for each sj ,
the correct output pattern receives a higher score
than other patterns of the same length: ?u ?
TN ,u 6= tj,
? ??(sj, tj) > ? ??(sj ,u) (1)
The set of constraints generated by Equation 1
are called rank constraints. They are created sep-
arately for every (sj , tj) training pair. Essen-
tially, each training pair is matched with a set
of automatically-created negative examples. Each
negative has an incorrect, but plausible, stress pat-
tern, u.
We adopt a Support Vector Machine (SVM) so-
lution to these ranking constraints as described by
Joachims (2002). The learner finds the weights
that ensure a maximum (soft) margin separation
between the correct scores and the competitors.
We use an SVM because it has been successful in
similar settings (learning with thousands of sparse
features) for both ranking and classification tasks,
and because an efficient implementation is avail-
able (Joachims, 1999).
At test time we simply score each possible out-
put pattern using the learned weights. That is,
for an input sequence s of length N , we compute
? ??(s, t) for all t ? TN , and we take the highest
scoring t as our output. Note that because we only
Substring si, ti
si, i, ti
Context si?1, ti
si?1si, ti
si+1, ti
sisi+1, ti
si?1sisi+1, ti
Stress Pattern t1t2 . . . tN
Table 2: Feature Template
consider previously-observed output patterns, it is
impossible for our system to produce a nonsensi-
cal result, such as having two primary stresses in
one word. Standard search-based sequence pre-
dictors need to be specially augmented with hard
constraints in order to prevent such output (Roth
and Yih, 2005).
3.2.2 Features
The power of our ranker to identify the correct
stress pattern depends on how expressive our fea-
tures are. Table 2 shows the feature templates used
to create the features ?(s, t) for our ranker. We
use binary features to indicate whether each com-
bination occurs in the current (s,t) pair.
For example, if a substring tion is unstressed in
a (s, t) pair, the Substring feature {si, ti = tion,0}
will be true.2 In English, often the penultimate
syllable is stressed if the final syllable is tion.
We can capture such a regularity with the Con-
text feature si+1, ti. If the following syllable is
tion and the current syllable is stressed, the fea-
ture {si+1, ti = tion,1} will be true. This feature
will likely receive a positive weight, so that out-
put sequences with a stress before tion receive a
higher rank.
Finally, the full Stress Pattern serves as an im-
portant feature. Note that such a feature would
not be possible in standard sequence predictors,
where such information must be decomposed into
Markov transition features like ti?1ti. In a ranking
framework, we can score output sequences using
their full output pattern. Thus we can easily learn
the rules in languages with regular stress rules. For
languages that do not have a fixed stress rule, pref-
erences for particular patterns can be learned using
this feature.
2tion is a substring composed of three phonemes but we
use its orthographic representation here for clarity.
121
3.3 Pattern-to-Vowel Mapping
The final stage of our system uses the predicted
pattern t to create the stress-marked form of the
word, w?. Note the number of substrings created
by our splitting method always equals the number
of vowels in the word. We can thus simply map
the indicator numbers in t to markers on their cor-
responding vowels to produce the stressed word.
For our example, pronounce ? ron-no-un-ce,
if the SVM chooses the stress pattern, 0-1-0-
0, we produce the correct stress-marked word,
prono?unce. If we instead stress the third vowel, 0-
0-1-0, we produce an incorrect output, pronou?nce.
4 Stress Prediction Experiments
In this section, we evaluate our ranking approach
to stress prediction by assigning stress to spoken
and written words in three languages: English,
German, and Dutch. We first describe the data and
the various systems we evaluate, and then provide
the results.
4.1 Data
The data is extracted from CELEX (Baayen et al,
1996). Following previous work on stress predic-
tion, we randomly partition the data into 85% for
training, 5% for development, and 10% for test-
ing. To make results on German and Dutch com-
parable with English, we reduce the training, de-
velopment, and testing set by 80% for each. Af-
ter removing all duplicated items as well as abbre-
viations, phrases, and diacritics, each training set
contains around 55K words.
In CELEX, stress is labeled on syllables in the
phonetic form of the words. Since our objec-
tive is to assign stress markers to vowels (as de-
scribed in Section 2) we automatically map the
stress markers from the stressed syllables in the
phonetic forms onto phonemes and letters rep-
resenting vowels. For phonemes, the process is
straightforward: we move the stress marker from
the beginning of a syllable to the phoneme which
constitutes the nucleus of the syllable. For let-
ters, we map the stress from the vowel phoneme
onto the orthographic forms using the ALINE al-
gorithm (Dwyer and Kondrak, 2009). The stress
marker is placed on the first letter within the sylla-
ble that represents a vowel sound.3
3Our stand-off stress annotations for English, German,
and Dutch CELEX orthographic data can be downloaded at:
http://www.cs.ualberta.ca/?kondrak/celex.html.
System Eng Ger Dut
P+S P P P
SUBSTRING 96.2 98.0 97.1 93.1
ORACLESYL 95.4 96.4 97.1 93.2
TOPPATTERN 66.8 68.9 64.1 60.8
Table 3: Stress prediction word accuracy (%) on
phonemes for English, German, and Dutch. P:
predicting primary stress only. P+S: primary and
secondary.
CELEX also provides secondary stress annota-
tion for English. We therefore evaluate on both
primary and secondary stress (P+S) in English and
on primary stress assignment alone (P) for En-
glish, German, and Dutch.
4.2 Comparison Approaches
We evaluate three different systems on the letter
and phoneme sequences in the experimental data:
1) SUBSTRING is the system presented in Sec-
tion 3. It uses the vowel-based splitting
method, followed by SVM ranking.
2) ORACLESYL splits the input word into sylla-
bles according to the CELEX gold-standard,
before applying SVM ranking. The output
pattern is evaluated directly against the gold-
standard, without pattern-to-vowel mapping.
3) TOPPATTERN is our baseline system. It uses
the vowel-based splitting method to produce a
substring sequence of length N . Then it simply
chooses the most common stress pattern among
all the stress patterns of length N .
SUBSTRING and ORACLESYL use scores pro-
duced by an SVM ranker trained on the training
data. We employ the ranking mode of the popular
learning package SVMlight (Joachims, 1999). In
each case, we learn a linear kernel ranker on the
training set stress patterns and tune the parameter
that trades-off training error and margin on the de-
velopment set.
We evaluate the systems using word accuracy:
the percent of words for which the output form of
the word, w?, matches the gold standard.
4.3 Results
Table 3 provides results on English, German, and
Dutch phonemes. Overall, the performance of our
automatic stress predictor, SUBSTRING, is excel-
lent. It achieves 98.0% accuracy for predicting
122
System Eng Ger Dut
P+S P P P
SUBSTRING 93.5 95.1 95.9 91.0
ORACLESYL 94.6 96.0 96.6 92.8
TOPPATTERN 65.5 67.6 64.1 60.8
Table 4: Stress prediction word accuracy (%) on
letters for English, German, and Dutch. P: pre-
dicting primary stress only. P+S: primary and sec-
ondary.
primary stress in English, 97.1% in German, and
93.1% in Dutch. It also predicts both primary and
secondary stress in English with high accuracy,
96.2%. Performance is much higher than our base-
line accuracy, which is between 60% and 70%.
ORACLESYL, with longer substrings and hence
sparser data, does not generally improve perfor-
mance. This indicates that perfect syllabification
is unnecessary for phonetic stress assignment.
Our system is a major advance over the pre-
vious state-of-the-art in phonetic stress assign-
ment. For predicting stressed/unstressed syllables
in English, Black et al (1998) obtained a per-
syllable accuracy of 94.6%. We achieve 96.2%
per-word accuracy for predicting both primary and
secondary stress. Others report lower numbers
on English phonemes. Bagshaw (1998) obtained
65%-83.3% per-syllable accuracy using Church
(1985)?s rule-based system. For predicting both
primary and secondary stress, Coleman (2000)
and Pearson et al (2000) report 69.8% and 81.0%
word accuracy, respectively.
The performance on letters (Table 4) is also
quite encouraging. SUBSTRING predicts primary
stress with accuracy above 95% for English and
German, and equal to 91% in Dutch. Performance
is 1-3% lower on letters than on phonemes. On
the other hand, the performance of ORACLESYL
drops much less on letters. This indicates that
most of SUBSTRING?s errors are caused by the
splitting method. Letter vowels may or may not
represent spoken vowels. By creating a substring
for every vowel letter we may produce an incorrect
number of syllables. Our pattern feature is there-
fore less effective.
Nevertheless, SUBSTRING?s accuracy on letters
also represents a clear improvement over previ-
ous work. Webster (2004) reports 80.3% word
accuracy on letters in English and 81.2% in Ger-
man. The most comparable work is Demberg et al
 84
 86
 88
 90
 92
 94
 96
 98
 100
 10000  100000
W
or
d 
Ac
cu
ra
cy
 (%
)
Number of training examples
German
Dutch
English
Figure 1: Stress prediction accuracy on letters.
(2007), which achieves 90.1% word accuracy on
letters in German CELEX, assuming perfect letter
syllabification. In order to reproduce their strict
experimental setup, we re-partition the full set of
German CELEX data to ensure that no overlap of
word stems exists between the training and test
sets. Using the new data sets, our system achieves
a word accuracy of 92.3%, a 2.2% improvement
over Demberg et al (2007)?s result. Moreover, if
we also assume perfect syllabification, the accu-
racy is 94.3%, a 40% reduction in error rate.
We performed a detailed analysis to understand
the strong performance of our system. First of all,
note that an error could happen if a test-set stress
pattern was not observed in the training data; its
correct stress pattern would not be considered as
an output. In fact, no more than two test errors in
any test set were so caused. This strongly justi-
fies the reduced set of outputs used in our ranking
formulation.
We also tested all systems with the Stress Pat-
tern feature removed. Results were worse in all
cases. As expected, it is most valuable for pre-
dicting primary and secondary stress. On English
phonemes, accuracy drops from 96.2% to 95.3%
without it. On letters, it drops from 93.5% to
90.0%. The gain from this feature also validates
our ranking framework, as such arbitrary features
over the entire output sequence can not be used in
standard search-based sequence prediction.
Finally, we examined the relationship between
training data size and performance by plotting
learning curves for letter stress accuracy (Fig-
ure 1). Unlike the tables above, here we use the
123
full set of data in Dutch and German CELEX to
create the largest-possible training sets (255K ex-
amples). None of the curves are levelling off; per-
formance grows log-linearly across the full range.
5 Lexical stress and L2P conversion
In this section, we evaluate various methods of
combining stress prediction with phoneme gener-
ation. We first describe the specific system that we
use for letter-to-phoneme (L2P) conversion. We
then discuss the different ways stress prediction
can be integrated with L2P, and define the systems
used in our experiments. Finally, we provide the
results.
5.1 The L2P system
We combine stress prediction with a state-of-the-
art L2P system (Jiampojamarn et al, 2008). Like
our stress ranker, their system is a data-driven se-
quence predictor that is trained with supervised
learning. The score for each output sequence is
a weighted combination of features. The feature
weights are trained using the Margin Infused Re-
laxed Algorithm (MIRA) (Crammer and Singer,
2003), a powerful online discriminative training
framework. Like other recent L2P systems (Bisani
and Ney, 2002; Marchand and Damper, 2007; Ji-
ampojamarn et al, 2007), this approach does not
generate stress, nor does it consider stress when it
generates phonemes.
For L2P experiments, we use the same training,
testing, and development data as was used in Sec-
tion 4. For all experiments, we use the develop-
ment set to determine at which iteration to stop
training in the online algorithm.
5.2 Combining stress and phoneme
generation
Various methods have been used for combining
stress and phoneme generation. Phonemes can be
generated without regard to stress, with stress as-
signed as a post-process (Bagshaw, 1998; Cole-
man, 2000). Both van den Bosch (1997) and
Black et al (1998) argue that stress should be pre-
dicted at the same time as phonemes. They ex-
pand the output set to distinguish between stressed
and unstressed phonemes. Similarly, Demberg et
al. (2007) produce phonemes, stress, and syllable-
boundaries within a single joint n-gram model.
Pearson et al (2000) generate phonemes and stress
together by jointly optimizing a decision-tree
phoneme-generator and a stress predictor based on
stress pattern counts. In contrast, Webster (2004)
first assigns stress to letters, creating an expanded
input set, and then predicts both phonemes and
stress jointly. The system marks stress on let-
ter vowels by determining the correspondence be-
tween affixes and stress in written words.
Following the above approaches, we can expand
the input or output symbols of our L2P system to
include stress. However, since both decision tree
systems and our L2P predictor utilize only local
context, they may produce invalid global output.
One option, used by Demberg et al (2007), is to
add a constraint to the output generation, requiring
each output sequence to have exactly one primary
stress.
We enhance this constraint, based on the obser-
vation that the number of valid output sequences
is fairly limited (Section 3.2). The modified sys-
tem produces the highest-scoring sequence such
that the output?s corresponding stress pattern has
been observed in our training data. We call this
the stress pattern constraint. This is a tighter
constraint than having only one primary stress.4
Another advantage is that it provides some guid-
ance for the assignment of secondary stress.
Inspired by the aforementioned strategies, we
evaluate the following approaches:
1) JOINT: The L2P system?s input sequence is let-
ters, the output sequence is phonemes+stress.
2) JOINT+CONSTR: Same as JOINT, except it se-
lects the highest scoring output that obeys the
stress pattern constraint.
3) POSTPROCESS: The L2P system?s input is let-
ters, the output is phonemes. It then applies the
SVM stress ranker (Section 3) to the phonemes
to produce the full phoneme+stress output.
4) LETTERSTRESS: The L2P system?s input is
letters+stress, the output is phonemes+stress.
It creates the stress-marked letters by applying
the SVM ranker to the input letters as a pre-
process.
5) ORACLESTRESS: The same input/output as
LETTERSTRESS , except it uses the gold-
standard stress on letters (Section 4.1).
4In practice, the L2P system generates a top-N list, and
we take the highest-scoring output on the list that satisfies
the constraint. If none satisfy the constraint, we take the top
output that has only one primary stress.
124
System Eng Ger Dut
P+S P P P
JOINT 78.9 80.0 86.0 81.1
JOINT+CONSTR 84.6 86.0 90.8 88.7
POSTPROCESS 86.2 87.6 90.9 88.8
LETTERSTRESS 86.5 87.2 90.1 86.6
ORACLESTRESS 91.4 91.4 92.6 94.5
Festival 61.2 62.5 71.8 65.1
Table 5: Combined phoneme and stress predic-
tion word accuracy (%) for English, German, and
Dutch. P: predicting primary stress only. P+S:
primary and secondary.
Note that while the first approach uses only
local information to make predictions (features
within a context window around the current let-
ter), systems 2 to 5 leverage global information in
some manner: systems 3 and 4 use the predictions
of our stress ranker, while 2 uses a global stress
pattern constraint.5
We also generated stress and phonemes using
the popular Festival Speech Synthesis System6
(version 1.96, 2004) and report its accuracy.
5.3 Results
Word accuracy results for predicting both
phonemes and stress are provided in Table 5.
First of all, note that the JOINT approach,
which simply expands the output set, is 4%-
8% worse than all other comparison systems
across the three languages. These results clearly
indicate the drawbacks of predicting stress us-
ing only local information. In English, both
LETTERSTRESS and POSTPROCESS perform
best, while POSTPROCESS and the constrained
system are highest on German and Dutch. Results
using the oracle letter stress show that given
perfect stress assignment on letters, phonemes
and stress can be predicted very accurately, in all
cases above 91%.
We also found that the phoneme prediction ac-
curacy alone (i.e., without stress) is quite simi-
lar for all the systems. The gains over JOINT
on combined stress and phoneme accuracy are
almost entirely due to more accurate stress as-
signment. Utilizing the oracle stress on letters
markedly improves phoneme prediction in English
5This constraint could also help the other systems. How-
ever, since they already use global information, it yields only
marginal improvements.
6http://www.cstr.ed.ac.uk/projects/festival/
(from 88.8% to 91.4%). This can be explained by
the fact that English vowels are often reduced to
schwa when unstressed (Section 2).
Predicting both phonemes and stress is a chal-
lenging task, and each of our globally-informed
systems represents a major improvement over pre-
vious work. The accuracy of Festival is much
lower even than our JOINT approach, but the rel-
ative performance on the different languages is
quite similar.
A few papers report accuracy on the combined
stress and phoneme prediction task. The most di-
rectly comparable work is van den Bosch (1997),
which also predicts primary and secondary stress
using English CELEX data. However, the re-
ported word accuracy is only 62.1%. Three other
papers report word accuracy on phonemes and
stress, using different data sets. Pearson et al
(2000) report 58.5% word accuracy for predicting
phonemes and primary/secondary stress. Black et
al. (1998) report 74.6% word accuracy in English,
while Webster (2004) reports 68.2% on English
and 82.9% in German (all primary stress only).
Finally, Demberg et al (2007) report word accu-
racy on predicting phonemes, stress, and syllab-
ification on German CELEX data. They achieve
86.3% word accuracy.
6 Conclusion
We have presented a discriminative ranking ap-
proach to lexical stress prediction, which clearly
outperforms previously developed systems. The
approach is largely language-independent, appli-
cable to both orthographic and phonetic repre-
sentations, and flexible enough to handle multi-
ple stress levels. When combined with an exist-
ing L2P system, it achieves impressive accuracy
in generating pronunciations together with their
stress patterns. In the future, we will investigate
additional features to leverage syllabic and mor-
phological information, when available. Kernel
functions could also be used to automatically cre-
ate a richer feature space; preliminary experiments
have shown gains in performance using polyno-
mial and RBF kernels with our stress ranker.
Acknowledgements
This research was supported by the Natural
Sciences and Engineering Research Council of
Canada, the Alberta Ingenuity Fund, and the Al-
berta Informatics Circle of Research Excellence.
125
References
Joanne Arciuli and Linda Cupples. 2006. The pro-
cessing of lexical stress during visual word recog-
nition: Typicality effects and orthographic corre-
lates. Quarterly Journal of Experimental Psychol-
ogy, 59(5):920?948.
Harald Baayen, Richard Piepenbrock, and Leon Gu-
likers. 1996. The CELEX2 lexical database.
LDC96L14.
Paul C. Bagshaw. 1998. Phonemic transcription by
analogy in text-to-speech synthesis: Novel word
pronunciation and lexicon compression. Computer
Speech and Language, 12(2):119?142.
Susan Bartlett, Grzegorz Kondrak, and Colin Cherry.
2008. Automatic syllabification with structured
SVMs for letter-to-phoneme conversion. In ACL-
08: HLT, pages 568?576.
Maximilian Bisani and Hermann Ney. 2002. Investi-
gations on joint-multigram models for grapheme-to-
phoneme conversion. In ICSLP, pages 105?108.
Alan W Black, Kevin Lenzo, and Vincent Pagel. 1998.
Issues in building general letter to sound rules. In
The 3rd ESCA Workshop on Speech Synthesis, pages
77?80.
Noam Chomsky and Morris Halle. 1968. The sound
pattern of English. New York: Harper and Row.
Kenneth Church. 1985. Stress assignment in letter
to sound rules for speech synthesis. In ACL, pages
246?253.
Cynthia G. Clopper. 2002. Frequency of stress pat-
terns in English: A computational analysis. IULC
Working Papers Online.
John Coleman. 2000. Improved prediction of stress in
out-of-vocabulary words. In IEEE Seminar on the
State of the Art in Speech Synthesis.
Michael Collins and Terry Koo. 2005. Discriminative
reranking for natural language parsing. Computa-
tional Linguistics, 31(1):25?70.
Michael Collins. 2002. Discriminative training meth-
ods for Hidden Markov Models: Theory and ex-
periments with perceptron algorithms. In EMNLP,
pages 1?8.
Koby Crammer and Yoram Singer. 2003. Ultracon-
servative online algorithms for multiclass problems.
Journal of Machine Learning Research, 3:951?991.
Vera Demberg, Helmut Schmid, and Gregor Mo?hler.
2007. Phonological constraints and morphologi-
cal preprocessing for grapheme-to-phoneme conver-
sion. In ACL, pages 96?103.
Qing Dou. 2009. An SVM ranking approach to stress
assignment. Master?s thesis, University of Alberta.
Kenneth Dwyer and Grzegorz Kondrak. 2009. Reduc-
ing the annotation effort for letter-to-phoneme con-
version. In ACL-IJCNLP.
Erik C. Fudge. 1984. English word-stress. London:
Allen and Unwin.
Sittichai Jiampojamarn, Grzegorz Kondrak, and Tarek
Sherif. 2007. Applying many-to-many alignments
and Hidden Markov Models to letter-to-phoneme
conversion. In NAACL-HLT 2007, pages 372?379.
Sittichai Jiampojamarn, Colin Cherry, and Grzegorz
Kondrak. 2008. Joint processing and discriminative
training for letter-to-phoneme conversion. In ACL-
08: HLT, pages 905?913.
Thorsten Joachims. 1999. Making large-scale Support
Vector Machine learning practical. In B. Scho?lkopf
and C. Burges, editors, Advances in Kernel Meth-
ods: Support Vector Machines, pages 169?184.
MIT-Press.
Thorsten Joachims. 2002. Optimizing search engines
using clickthrough data. In KDD, pages 133?142.
John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional Random Fields:
Probabilistic models for segmenting and labeling se-
quence data. In ICML, pages 282?289.
Yannick Marchand and Robert I. Damper. 2007. Can
syllabification improve pronunciation by analogy of
English? Natural Language Engineering, 13(1):1?
24.
Steve Pearson, Roland Kuhn, Steven Fincke, and Nick
Kibre. 2000. Automatic methods for lexical stress
assignment and syllabification. In ICSLP, pages
423?426.
Dan Roth and Wen-tau Yih. 2005. Integer linear pro-
gramming inference for conditional random fields.
In ICML, pages 736?743.
Lara Tagliapietra and Patrizia Tabossi. 2005. Lexical
stress effects in Italian spoken word recognition. In
The XXVII Annual Conference of the Cognitive Sci-
ence Society, pages 2140?2144.
Antal van den Bosch. 1997. Learning to pronounce
written words: A study in inductive language learn-
ing. Ph.D. thesis, Universiteit Maastricht.
Gabriel Webster. 2004. Improving letter-
to-pronunciation accuracy with automatic
morphologically-based stress prediction. In
ICSLP, pages 2573?2576.
Briony Williams. 1987. Word stress assignment in a
text-to-speech synthesis system for British English.
Computer Speech and Language, 2:235?272.
George Kingsley Zipf. 1929. Relative frequency as a
determinant of phonetic change. Harvard Studies in
Classical Philology, 15:1?95.
126
Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),
pages 88?95, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
An Expectation Maximization Approach to Pronoun Resolution
Colin Cherry and Shane Bergsma
Department of Computing Science
University of Alberta
Edmonton, Alberta, Canada, T6G 2E8
{colinc,bergsma}@cs.ualberta.ca
Abstract
We propose an unsupervised Expectation
Maximization approach to pronoun reso-
lution. The system learns from a fixed
list of potential antecedents for each pro-
noun. We show that unsupervised learn-
ing is possible in this context, as the per-
formance of our system is comparable to
supervised methods. Our results indicate
that a probabilistic gender/number model,
determined automatically from unlabeled
text, is a powerful feature for this task.
1 Introduction
Coreference resolution is the process of determin-
ing which expressions in text refer to the same real-
world entity. Pronoun resolution is the important yet
challenging subset of coreference resolution where a
system attempts to establish coreference between a
pronominal anaphor, such as a third-person pronoun
like he, she, it, or they, and a preceding noun phrase,
called an antecedent. In the following example, a
pronoun resolution system must determine the cor-
rect antecedent for the pronouns ?his? and ?he.?
(1) When the president entered the arena with his
family, he was serenaded by a mariachi band.
Pronoun resolution has applications across many
areas of Natural Language Processing, particularly
in the field of information extraction. Resolving a
pronoun to a noun phrase can provide a new inter-
pretation of a given sentence, giving a Question An-
swering system, for example, more data to consider.
Our approach is a synthesis of linguistic and sta-
tistical methods. For each pronoun, a list of an-
tecedent candidates derived from the parsed corpus
is presented to the Expectation Maximization (EM)
learner. Special cases, such as pleonastic, reflex-
ive and cataphoric pronouns are dealt with linguisti-
cally during list construction. This allows us to train
on and resolve all third-person pronouns in a large
Question Answering corpus. We learn lexicalized
gender/number, language, and antecedent probabil-
ity models. These models, tied to individual words,
can not be learned with sufficient coverage from la-
beled data. Pronouns are resolved by choosing the
most likely antecedent in the candidate list accord-
ing to these distributions. The resulting resolution
accuracy is comparable to supervised methods.
We gain further performance improvement by ini-
tializing EM with a gender/number model derived
from special cases in the training data. This model
is shown to perform reliably on its own. We also
demonstrate how the models learned through our un-
supervised method can be used as features in a su-
pervised pronoun resolution system.
2 Related Work
Pronoun resolution typically employs some com-
bination of constraints and preferences to select
the antecedent from preceding noun phrase candi-
dates. Constraints filter the candidate list of improb-
able antecedents, while preferences encourage se-
lection of antecedents that are more recent, frequent,
etc. Implementation of constraints and preferences
can be based on empirical insight (Lappin and Le-
ass, 1994), or machine learning from a reference-
88
annotated corpus (Ge et al, 1998). The majority
of pronoun resolution approaches have thus far re-
lied on manual intervention in the resolution pro-
cess, such as using a manually-parsed corpus, or
manually removing difficult non-anaphoric cases;
we follow Mitkov et al?s approach (2002) with a
fully-automatic pronoun resolution method. Pars-
ing, noun-phrase identification, and non-anaphoric
pronoun removal are all done automatically.
Machine-learned, fully-automatic systems are
more common in noun phrase coreference resolu-
tion, where the method of choice has been deci-
sion trees (Soon et al, 2001; Ng and Cardie, 2002).
These systems generally handle pronouns as a subset
of all noun phrases, but with limited features com-
pared to systems devoted solely to pronouns. Kehler
used Maximum Entropy to assign a probability dis-
tribution over possible noun phrase coreference re-
lationships (1997). Like his approach, our system
does not make hard coreference decisions, but re-
turns a distribution over candidates.
The above learning approaches require anno-
tated training data for supervised learning. Cardie
and Wagstaff developed an unsupervised approach
that partitions noun phrases into coreferent groups
through clustering (1999). However, the partitions
they generate for a particular document are not use-
ful for processing new documents, while our ap-
proach learns distributions that can be used on un-
seen data. There are also approaches to anaphora
resolution using unsupervised methods to extract
useful information, such as gender and number (Ge
et al, 1998), or contextual role-knowledge (Bean
and Riloff, 2004). Co-training can also leverage
unlabeled data through weakly-supervised reference
resolution learning (Mu?ller et al, 2002). As an alter-
native to co-training, Ng and Cardie (2003) use EM
to augment a supervised coreference system with
unlabeled data. Their feature set is quite different, as
it is designed to generalize from the data in a labeled
set, while our system models individual words. We
suspect that the two approaches can be combined.
Our approach is inspired by the use of EM in
bilingual word alignment, which finds word-to-word
correspondences between a sentence and its transla-
tion. The prominent statistical methods in this field
are unsupervised. Our methods are most influenced
by IBM?s Model 1 (Brown et al, 1993).
3 Methods
3.1 Problem formulation
We will consider our training set to consist of
(p, k, C) triples: one for each pronoun, where p is
the pronoun to be resolved, k is the pronoun?s con-
text, and C is a candidate list containing the nouns p
could potentially be resolved to. Initially, we take k
to be the parsed sentence that p appears in.
C consists of all nouns and pronouns that precede
p, looking back through the current sentence and the
sentence immediately preceding it. This small win-
dow may seem limiting, but we found that a cor-
rect candidate appeared in 97% of such lists in a
labeled development text. Mitkov et al also limit
candidate consideration to the same window (2002).
Each triple is processed with non-anaphoric pronoun
handlers (Section 3.3) and linguistic filters (Sec-
tion 3.4), which produce the final candidate lists.
Before we pass the (p, k, C) triples to EM, we
modify them to better suit our EM formulation.
There are four possibilities for the gender and num-
ber of third-person pronouns in English: masculine,
feminine, neutral and plural (e.g., he, she, it, they).
We assume a noun is equally likely to corefer with
any member of a given gender/number category, and
reduce each p to a category label accordingly. For
example, he, his, him and himself are all labeled as
masc for masculine pronoun. Plural, feminine and
neutral pronouns are handled similarly. We reduce
the context term k to p?s immediate syntactic con-
text, including only p?s syntactic parent, the parent?s
part of speech, and p?s relationship to the parent, as
determined by a dependency parser. Incorporating
context only through the governing constituent was
also done in (Ge et al, 1998). Finally, each candi-
date in C is augmented with ordering information,
so we know how many nouns to ?step over? before
arriving at a given candidate. We will refer to this or-
dering information as a candidate?s j term, for jump.
Our example sentence in Section 1 would create the
two triples shown in Figure 1, assuming the sentence
began the document it was found in.
3.2 Probability model
Expectation Maximization (Dempster et al, 1977) is
a process for filling in unobserved data probabilisti-
cally. To use EM to do unsupervised pronoun reso-
89
his: p = masc k = p?s family
C = arena (0), president (1)
he: p = masc k = serenade p
C = family (0), masc (1), arena (2),
president (3)
Figure 1: EM input for our example sentence.
j-values follow each lexical candidate.
lution, we phrase the resolution task in terms of hid-
den variables of an observed process. We assume
that in each case, one candidate from the candidate
list is selected as the antecedent before p and k are
generated. EM?s role is to induce a probability dis-
tribution over candidates to maximize the likelihood
of the (p, k) pairs observed in our training set:
Pr(Dataset) =
?
(p,k)?Dataset
Pr(p, k) (1)
We can rewrite Pr(p, k) so that it uses a hidden can-
didate (or antecedent) variable c that influences the
observed p and k:
Pr(p, k) =
?
c?C
Pr(p, k, c) (2)
Pr(p, k, c) = Pr(p, k|c)Pr(c) (3)
To improve our ability to generalize to future cases,
we use a na??ve Bayes assumption to state that the
choices of pronoun and context are conditionally in-
dependent, given an antecedent. That is, once we
select the word the pronoun represents, the pronoun
and its context are no longer coupled:
Pr(p, k|c) = Pr(p|c)Pr(k|c) (4)
We can split each candidate c into its lexical com-
ponent l and its jump value j. That is, c = (l, j).
If we assume that l and j are independent, and that
p and k each depend only on the l component of c,
we can combine Equations 3 and 4 to get our final
formulation for the joint probability distribution:
Pr(p, k, c) = Pr(p|l)Pr(k|l)Pr(l)Pr(j) (5)
The jump term j, though important when resolving
pronouns, is not likely to be correlated with any lex-
ical choices in the training set.
Table 1: Examples of learned pronoun probabilities.
Word (l) masc fem neut plur
company 0.03 0.01 0.95 0.01
president 0.94 0.01 0.03 0.02
teacher 0.19 0.71 0.09 0.01
This results in four models that work together to
determine the likelihood of a given candidate. The
Pr(p|l) distribution measures the likelihood of a pro-
noun given an antecedent. Since we have collapsed
the observed pronouns into groups, this models a
word?s affinity for each of the four relevant gen-
der/number categories. We will refer to this as our
pronoun model. Pr(k|l) measures the probability of
the syntactic relationship between a pronoun and its
parent, given a prospective antecedent for the pro-
noun. This is effectively a language model, grading
lexical choice by context. Pr(l) measures the prob-
ability that the word l will be found to be an an-
tecedent. This is useful, as some entities, such as
?president? in newspaper text, are inherently more
likely to be referenced with a pronoun. Finally,
Pr(j) measures the likelihood of jumping a given
number of noun phrases backward to find the cor-
rect candidate. We represent these models with ta-
ble look-up. Table 1 shows selected l-value entries
in the Pr(p|l) table from our best performing EM
model. Note that the probabilities reflect biases in-
herent in our news domain training set.
Given models for the four distributions above,
we can assign a probability to each candidate in
C according to the observations p and k; that is,
Pr(c|p, k) can be obtained by dividing Equation 5
by Equation 2. Remember that c = (l, j).
Pr(c|p, k) =
Pr(p|l)Pr(k|l)Pr(l)Pr(j)
?
c??C Pr(p|l?)Pr(k|l?)Pr(l?)Pr(j?)(6)
Pr(c|p, k) allows us to get fractional counts of
(p, k, c) triples in our training set, as if we had actu-
ally observed c co-occurring with (p, k) in the pro-
portions specified by Equation 6. This estimation
process is effectively the E-step in EM.
The M-step is conducted by redefining our mod-
els according to these fractional counts. For exam-
ple, after assigning fractional counts to candidates
90
according to Pr(c|p, k), we re-estimate Pr(p|l) with
the following equation for a specific (p, l) pair:
Pr(p|l) =
N(p, l)
N(l)
(7)
where N() counts the number of times we see a
given event or joint event throughout the training set.
Given trained models, we resolve pronouns by
finding the candidate c? that is most likely for the
current pronoun, that is c? = argmaxc?CPr(c|p, k).
Because Pr(p, k) is constant with respect to c,
c? = argmaxc?CPr(p, k, c).
3.3 Non-anaphoric Pronouns
Not every pronoun in text refers anaphorically to a
preceding noun phrase. There are a frequent num-
ber of difficult cases that require special attention,
including pronouns that are:
? Pleonastic: pronouns that have a grammatical
function but do not reference an entity. E.g. ?It
is important to observe it is raining.?
? Cataphora: pronouns that reference a future
noun phrase. E.g. ?In his speech, the president
praised the workers.?
? Non-noun referential: pronouns that refer to a
verb phrase, sentence, or implicit concept. E.g.
?John told Mary they should buy a car.?
If we construct them na??vely, the candidate lists
for these pronouns will be invalid, introducing noise
in our training set. Manual handling or removal
of these cases is infeasible in an unsupervised ap-
proach, where the input is thousands of documents.
Instead, pleonastics are identified syntactically us-
ing an extension of the detector developed by Lap-
pin and Leass (1994). Roughly 7% of all pronouns
in our labeled test data are pleonastic. We detect
cataphora using a pattern-based method on parsed
sentences, described in (Bergsma, 2005b). Future
nouns are only included when cataphora are iden-
tified. This approach is quite different from Lap-
pin and Leass (1994), who always include all fu-
ture nouns from the current sentence as candidates,
with a constant penalty added to possible cataphoric
resolutions. The cataphora module identifies 1.4%
of test data pronouns to be cataphoric; in each in-
stance this identification is correct. Finally, we know
of no approach that handles pronouns referring to
verb phrases or implicit entities. The unavoidable
errors for these pronouns, occurring roughly 4% of
the time, are included in our final results.
3.4 Candidate list modifications
It would be possible for C to include every noun
phrase in the current and previous sentence, but per-
formance can be improved by automatically remov-
ing improbable antecedents. We use a standard set of
constraints to filter candidates. If a candidate?s gen-
der or number is known, and does not match the pro-
noun?s, the candidate is excluded. Candidates with
known gender include other pronouns, and names
with gendered designators (such as ?Mr.? or ?Mrs.?).
Our parser also identifies plurals and some gendered
first names. We remove from C all times, dates, ad-
dresses, monetary amounts, units of measurement,
and pronouns identified as pleonastic.
We use the syntactic constraints from Binding
Theory to eliminate candidates (Haegeman, 1994).
For the reflexives himself, herself, itself and them-
selves, this allows immediate syntactic identification
of the antecedent. These cases become unambigu-
ous; only the indicated antecedent is included in C.
We improve the quality of our training set by re-
moving known noisy cases before passing the set
to EM. For example, we anticipate that sentences
with quotation marks will be problematic, as other
researchers have observed that quoted text requires
special handling for pronoun resolution (Kennedy
and Boguraev, 1996). Thus we remove pronouns
occurring in the same sentences as quotes from the
learning process. Also, we exclude triples where
the constraints removed all possible antecedents, or
where the pronoun was deemed to be pleonastic.
Performing these exclusions is justified for training,
but in testing we state results for all pronouns.
3.5 EM initialization
Early in the development of this system, we were
impressed with the quality of the pronoun model
Pr(p|l) learned by EM. However, we found we could
construct an even more precise pronoun model for
common words by examining unambiguous cases in
our training data. Unambiguous cases are pronouns
having only one word in their candidate list C. This
could be a result of the preprocessors described in
91
Sections 3.3 and 3.4, or the pronoun?s position in
the document. A PrU (p|l) model constructed from
only unambiguous examples covers far fewer words
than a learned model, but it rarely makes poor gen-
der/number choices. Furthermore, it can be obtained
without EM. Training on unambiguous cases is sim-
ilar in spirit to (Hindle and Rooth, 1993). We found
in our development and test sets that, after applying
filters, roughly 9% of pronouns occur with unam-
biguous antecedents.
When optimizing a probability function that is not
concave, the EM algorithm is only guaranteed to
find a local maximum; therefore, it can be helpful
to start the process near the desired end-point in pa-
rameter space. The unambiguous pronoun model
described above can provide such a starting point.
When using this initializer, we perform our ini-
tial E-step by weighting candidates according to
PrU (p|l), instead of weighting them uniformly. This
biases the initial E-step probabilities so that a strong
indication of the gender/number of a candidate from
unambiguous cases will either boost the candidate?s
chances or remove it from competition, depending
on whether or not the predicted category matches
that of the pronoun being resolved.
To deal with the sparseness of the PrU (p|l) dis-
tribution, we use add-1 smoothing (Jeffreys, 1961).
The resulting effect is that words with few unam-
biguous occurrences receive a near-uniform gen-
der/number distribution, while those observed fre-
quently will closely match the observed distribution.
During development, we also tried clever initializers
for the other three models, including an extensive
language model initializer, but none were able to im-
prove over PrU (p|l) alone.
3.6 Supervised extension
Even though we have justified Equation 5 with rea-
sonable independence assumptions, our four mod-
els may not be combined optimally for our pronoun
resolution task, as the models are only approxima-
tions of the true distributions they are intended to
represent. Following the approach in (Och and Ney,
2002), we can view the right-hand-side of Equa-
tion 5 as a special case of:
exp
(
?1 log Pr(p|l) + ?2 log Pr(k|l)+
?3 log Pr(l) + ?4 log Pr(j)
)
(8)
where ?i : ?i = 1. Effectively, the log proba-
bilities of our models become feature functions in
a log-linear model. When labeled training data is
available, we can use the Maximum Entropy princi-
ple (Berger et al, 1996) to optimize the ? weights.
This provides us with an optional supervised ex-
tension to the unsupervised system. Given a small
set of data that has the correct candidates indicated,
such as the set we used while developing our unsu-
pervised system, we can re-weight the final models
provided by EM to maximize the probability of ob-
serving the indicated candidates. To this end, we
follow the approach of (Och and Ney, 2002) very
closely, including their handling of multiple correct
answers. We use the limited memory variable met-
ric method as implemented in Malouf?s maximum
entropy package (2002) to set our weights.
4 Experimental Design
4.1 Data sets
We used two training sets in our experiments, both
drawn from the AQUAINT Question Answering
corpus (Vorhees, 2002). For each training set, we
manually labeled pronoun antecedents in a corre-
sponding key containing a subset of the pronouns
in the set. These keys are drawn from a collection
of complete documents. For each document, all pro-
nouns are included. With the exception of the super-
vised extension, the keys are used only to validate
the resolution decisions made by a trained system.
Further details are available in (Bergsma, 2005b).
The development set consists of 333,000 pro-
nouns drawn from 31,000 documents. The devel-
opment key consists of 644 labeled pronouns drawn
from 58 documents; 417 are drawn from sentences
without quotation marks. The development set and
its key were used to guide us while designing the
probability model, and to fine-tune EM and smooth-
ing parameters. We also use the development key as
labeled training data for our supervised extension.
The test set consists of 890,000 pronouns drawn
from 50,000 documents. The test key consists of
1209 labeled pronouns drawn from 118 documents;
892 are drawn from sentences without quotation
marks. All of the results reported in Section 5 are
determined using the test key.
92
4.2 Implementation Details
To get the context values and implement the syntac-
tic filters, we parsed our corpora with Minipar (Lin,
1994). Experiments on the development set indi-
cated that EM generally began to overfit after 2 it-
erations, so we stop EM after the second iteration,
using the models from the second M-step for test-
ing. During testing, ties in likelihood are broken by
taking the candidate closest to the pronoun.
The EM-produced models need to be smoothed,
as there will be unseen words and unobserved (p, l)
or (k, l) pairs in the test set. This is because prob-
lematic cases are omitted from the training set, while
all pronouns are included in the key. We han-
dle out-of-vocabulary events by replacing words or
context-values that occur only once during training
with a special unknown symbol. Out-of-vocabulary
events encountered during testing are also treated
as unknown. We handle unseen pairs with additive
smoothing. Instead of adding 1 as in Section 3.5, we
add ?p = 0.00001 for (k, l) pairs, and ?w = 0.001
for (p, l) pairs. These ? values were determined ex-
perimentally with the development key.
4.3 Evaluation scheme
We evaluate our work in the context of a fully auto-
matic system, as was done in (Mitkov et al, 2002).
Our evaluation criteria is similar to their resolution
etiquette. We define accuracy as the proportion of
pronouns correctly resolved, either to any coreferent
noun phrase in the candidate list, or to the pleonas-
tic category, which precludes resolution. Systems
that handle and state performance for all pronouns
in unrestricted text report much lower accuracy than
most approaches in the literature. Furthermore, au-
tomatically parsing and pre-processing texts causes
consistent degradation in performance, regardless of
the accuracy of the pronoun resolution algorithm. To
have a point of comparison to other fully-automatic
approaches, note the resolution etiquette score re-
ported in (Mitkov et al, 2002) is 0.582.
5 Results
5.1 Validation of unsupervised method
The key concern of our work is whether enough
useful information is present in the pronoun?s cat-
egory, context, and candidate list for unsupervised
learning of antecedents to occur. To that end, our
first set of experiments compare the pronoun resolu-
tion accuracy of our EM-based solutions to that of a
previous-noun baseline on our test key. The results
are shown in Table 2. The columns split the results
into three cases: all pronouns with no exceptions;
all cases where the pronoun was found in a sentence
containing no quotation marks (and therefore resem-
bling the training data provided to EM); and finally
all pronouns excluded by the second case. We com-
pare the following methods:
1. Previous noun: Pick the candidate from the fil-
tered list with the lowest j value.
2. EM, no initializer: The EM algorithm trained
on the test set, starting from a uniform E-step.
3. Initializer, no EM: A model that ranks candi-
dates using only a pronoun model built from
unambiguous cases (Section 3.5).
4. EM w/ initializer: As in (2), but using the ini-
tializer in (3) for the first E-step.
5. Maxent extension: The models produced by
(4) are used as features in a log-linear model
trained on the development key (Section 3.6).
6. Upper bound: The percentage of cases with a
correct answer in the filtered candidate list.
For a reference point, picking the previous noun be-
fore applying any of our candidate filters receives an
accuracy score of 0.281 on the ?All? task.
Looking at the ?All? column in Table 2, we see
EM can indeed learn in this situation. Starting from
uniform parameters it climbs from a 40% baseline
to a 60% accurate model. However, the initializer
can do slightly better with precise but sparse gen-
der/number information alone. As we hoped, com-
bining the initializer and EM results in a statistically
significant1 improvement over EM with a uniform
starting point, but it is not significantly better than
the initializer alone. The advantage of the EM pro-
cess is that it produces multiple models, which can
be re-weighted with maximum entropy to reach our
highest accuracy, roughly 67%. The ? weights that
achieve this score are shown in Table 3.
Maximum entropy leaves the pronoun model
Pr(p|l) nearly untouched and drastically reduces the
1Significance is determined throughout Section 5 using Mc-
Nemar?s test with a significance level ? = 0.05.
93
Table 2: Accuracy for all cases, all excluding sen-
tences with quotes, and only sentences with quotes.
Method All No? ? Only? ?
1 Previous noun 0.397 0.399 0.391
2 EM, no initializer 0.610 0.632 0.549
3 Initializer, no EM 0.628 0.642 0.587
4 EM w/ initializer 0.632 0.663 0.546
5 Maxent extension 0.669 0.696 0.593
6 Upper bound 0.838 0.868 0.754
influence of all other models (Table 3). This, com-
bined with the success of the initializer alone, leads
us to believe that a strong notion of gender/number
is very important in this task. Therefore, we im-
plemented EM with several models that used only
pronoun category, but none were able to surpass the
initializer in accuracy on the test key. One factor
that might help explain the initializer?s success is
that despite using only a PrU (p|l) model, the ini-
tializer also has an implicit factor resembling a Pr(l)
model: when two candidates agree with the category
of the pronoun, add-1 smoothing ensures the more
frequent candidate receives a higher probability.
As was stated in Section 3.4, sentences with quo-
tations were excluded from the learning process be-
cause the presence of a correct antecedent in the can-
didate list was less frequent in these cases. This is
validated by the low upper bound of 0.754 in the
only-quote portion of the test key. We can see that
all methods except for the previous noun heuris-
tic score noticeably better when ignoring those sen-
tences that contain quotation marks. In particular,
the difference between our three unsupervised solu-
tions ((2), (3) and (4)) are more pronounced. Much
of the performance improvements that correspond
to our model refinements are masked in the overall
task because adding the initializer to EM does not
improve EM?s performance on quotes at all. Devel-
oping a method to construct more robust candidate
lists for quotations could improve our performance
on these cases, and greatly increase the percentage
of pronouns we are training on for a given corpus.
Table 3: Weights set by maximum entropy.
Model Pr(p|l) Pr(k|l) Pr(l) Pr(j)
Lambda 0.931 0.056 0.070 0.167
Table 4: Comparison to SVM.
Method Accuracy
Previous noun 0.398
EM w/ initializer 0.664
Maxent extension 0.708
SVM 0.714
5.2 Comparison to supervised system
We put our results in context by comparing our
methods to a recent supervised system. The compar-
ison system is an SVM that uses 52 linguistically-
motivated features, including probabilistic gen-
der/number information obtained through web
queries (Bergsma, 2005a). The SVM is trained
with 1398 separate labeled pronouns, the same train-
ing set used in (Bergsma, 2005a). This data is
also drawn from the news domain. Note the su-
pervised system was not constructed to handle all
pronoun cases, so non-anaphoric pronouns were re-
moved from the test key and from the candidate lists
in the test key to ensure a fair comparison. As ex-
pected, this removal of difficult cases increases the
performance of our system on the test key (Table 4).
Also note there is no significant difference in per-
formance between our supervised extension and the
SVM. The completely unsupervised EM system per-
forms worse, but with only a 7% relative reduction
in performace compared to the SVM; the previous
noun heuristic shows a 44% reduction.
5.3 Analysis of upper bound
If one accounts for the upper bound in Table 2, our
methods do very well on those cases where a cor-
rect answer actually appears in the candidate list: the
best EM solution scores 0.754, and the supervised
extension scores 0.800. A variety of factors result in
the 196 candidate lists that do not contain a true an-
tecedent. 21% of these errors arise from our limited
candidate window (Section 3.1). Incorrect pleonas-
tic detection accounts for another 31% while non-
94
noun referential pronouns cause 25% (Section 3.3).
Linguistic filters (Section 3.4) account for most of
the remainder. An improvement in any of these com-
ponents would result in not only higher final scores,
but cleaner EM training data.
6 Conclusion
We have demonstrated that unsupervised learning is
possible for pronoun resolution. We achieve accu-
racy of 63% on an all-pronoun task, or 75% when
a true antecedent is available to EM. There is now
motivation to develop cleaner candidate lists and
stronger probability models, with the hope of sur-
passing supervised techniques. For example, incor-
porating antecedent context, either at the sentence
or document level, may boost performance. Further-
more, the lexicalized models learned in our system,
especially the pronoun model, are potentially pow-
erful features for any supervised pronoun resolution
system.
References
David L. Bean and Ellen Riloff. 2004. Unsupervised learning
of contextual role knowledge for coreference resolution. In
HLT-NAACL, pages 297?304.
Adam L. Berger, Stephen A. Della Pietra, and Vincent J. Della
Pietra. 1996. A maximum entropy approach to natural lan-
guage processing. Computational Linguistics, 22(1):39?71.
Shane Bergsma. 2005a. Automatic acquisition of gender infor-
mation for anaphora resolution. In Proceedings of the 18th
Conference of the Canadian Society for Computational Intel-
ligence (Canadian AI 2005), pages 342?353, Victoria, BC.
Shane Bergsma. 2005b. Corpus-based learning for pronom-
inal anaphora resolution. Master?s thesis, Department
of Computing Science, University of Alberta, Edmonton.
http://www.cs.ualberta.ca/?bergsma/Pubs/thesis.pdf.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra,
and Robert L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Computational
Linguistics, 19(2):263?312.
Claire Cardie and Kiri Wagstaff. 1999. Noun phrase corefer-
ence as clustering. In Proceedings of the 1999 Joint SIGDAT
Conference on Empirical Methods in Natural Language Pro-
cessing and Very Large Corpora, pages 82?89.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977. Maxi-
mum likelihood from incomplete data via the EM algorithm.
Journal of the Royal Statistical Society, 39(1):1?38.
Niyu Ge, John Hale, and Eugene Charniak. 1998. A statistical
approach to anaphora resolution. In Proceedings of the Sixth
Workshop on Very Large Corpora, pages 161?171.
L. Haegeman. 1994. Introduction to Government & Binding
theory: Second Edition. Basil Blackwell, Cambridge, UK.
Donald Hindle and Mats Rooth. 1993. Structural ambiguity
and lexical relations. Computational Linguistics, 19(1):103?
120.
Harold Jeffreys, 1961. Theory of Probability, chapter 3.23. Ox-
ford: Clarendon Press, 3rd edition.
Andrew Kehler. 1997. Probabilistic coreference in informa-
tion extraction. In Proceedings of the Second Conference on
Empirical Methods in Natural Language Processing, pages
163?173.
Christopher Kennedy and Branimir Boguraev. 1996. Anaphora
for everyone: Pronominal anaphora resolution without a
parser. In Proceedings of the 16th Conference on Compu-
tational Linguistics, pages 113?118.
Shalom Lappin and Herbert J. Leass. 1994. An algorithm for
pronominal anaphora resolution. Computational Linguis-
tics, 20(4):535?561.
Dekang Lin. 1994. Principar - an efficient, broad-coverage,
principle-based parser. In Proceedings of COLING-94,
pages 42?48, Kyoto, Japan.
Robert Malouf. 2002. A comparison of algorithms for max-
imum entropy parameter estimation. In Proceedings of the
Sixth Conference on Natural Language Learning (CoNLL-
2002), pages 49?55.
Ruslan Mitkov, Richard Evans, and Constantin Orasan. 2002.
A new, fully automatic version of Mitkov?s knowledge-poor
pronoun resolution method. In Proceedings of the Third
International Conference on Computational Linguistics and
Intelligent Text Processing, pages 168?186.
Christoph Mu?ller, Stefan Rapp, and Michael Strube. 2002. Ap-
plying co-training to reference resolution. In Proceedings
of the 40th Annual Meeting of the Association for Computa-
tional Linguistics, pages 352?359.
Vincent Ng and Claire Cardie. 2002. Improving machine learn-
ing approaches to coreference resolution. In Proceedings of
the 40th Annual Meeting of the Association for Computa-
tional Linguistics, pages 104?111.
Vincent Ng and Claire Cardie. 2003. Weakly supervised nat-
ural language learning without redundant views. In HLT-
NAACL 2003: Proceedings of the Main Conference, pages
173?180.
Franz J. Och and Hermann Ney. 2002. Discriminative training
and maximum entropy models for statistical machine trans-
lation. In Proceedings of the 40th Annual Meeting of the
Association for Computational Linguistics, pages 295?302,
Philadelphia, PA, July.
Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong Lim.
2001. A machine learning approach to coreference resolu-
tion of noun phrases. Computational Linguistics, 27(4):521?
544.
Ellen Vorhees. 2002. Overview of the TREC 2002 question an-
swering track. In Proceedings of the Eleventh Text REtrieval
Conference (TREC).
95
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 120?128,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Glen, Glenda or Glendale:
Unsupervised and Semi-supervised Learning of English Noun Gender
Shane Bergsma
Department of Computing Science
University of Alberta
Edmonton, Alberta
Canada, T6G 2E8
bergsma@cs.ualberta.ca
Dekang Lin
Google, Inc.
1600 Amphitheatre Parkway
Mountain View
California, 94301
lindek@google.com
Randy Goebel
Department of Computing Science
University of Alberta
Edmonton, Alberta
Canada, T6G 2E8
goebel@cs.ualberta.ca
Abstract
English pronouns like he and they reliably re-
flect the gender and number of the entities to
which they refer. Pronoun resolution systems
can use this fact to filter noun candidates that
do not agree with the pronoun gender. In-
deed, broad-coverage models of noun gender
have proved to be the most important source
of world knowledge in automatic pronoun res-
olution systems.
Previous approaches predict gender by count-
ing the co-occurrence of nouns with pronouns
of each gender class. While this provides use-
ful statistics for frequent nouns, many infre-
quent nouns cannot be classified using this
method. Rather than using co-occurrence in-
formation directly, we use it to automatically
annotate training examples for a large-scale
discriminative gender model. Our model col-
lectively classifies all occurrences of a noun
in a document using a wide variety of con-
textual, morphological, and categorical gender
features. By leveraging large volumes of un-
labeled data, our full semi-supervised system
reduces error by 50% over the existing state-
of-the-art in gender classification.
1 Introduction
Pronoun resolution is the process of determining
which preceding nouns are referred to by a partic-
ular pronoun in text. Consider the sentence:
(1) Glen told Glenda that she was wrong about
Glendale.
A pronoun resolution system should determine that
the pronoun she refers to the noun Glenda. Pro-
noun resolution is challenging because it requires a
lot of world knowledge (general knowledge of word
types). If she is replaced with the pronoun he in (1),
Glen becomes the antecedent. Pronoun resolution
systems need the knowledge of noun gender that ad-
vises that Glen is usually masculine (and thus re-
ferred to by he) while Glenda is feminine.
English third-person pronouns are grouped in four
gender/number categories: masculine (he, his, him,
himself ), feminine (she, her, herself ), neutral (it, its,
itself ), and plural (they, their, them, themselves). We
broadly refer to these gender and number classes
simply as gender. The objective of our work is to
correctly assign gender to English noun tokens, in
context; to determine which class of pronoun will
refer to a given noun.
One successful approach to this problem is to
build a statistical gender model from a noun?s asso-
ciation with pronouns in text. For example, Ge et al
(1998) learn Ford has a 94% chance of being neu-
tral, based on its frequent co-occurrence with neu-
tral pronouns in text. Such estimates are noisy but
useful. Both Ge et al (1998) and Bergsma and Lin
(2006) show that learned gender is the most impor-
tant feature in their pronoun resolution systems.
English differs from other languages like French
and German in that gender is not an inherent gram-
matical property of an English noun, but rather a
property of a real-world entity that is being referred
to. A common noun like lawyer can be (semanti-
cally) masculine in one document and feminine in
another. While previous statistical gender models
learn gender for noun types only, we use document
context to correctly determine the current gender
class of noun tokens, making dynamic decisions on
common nouns like lawyer and ambiguous names
like Ford. Furthermore, if a noun type has not yet
120
been observed (an unknown word), previous ap-
proaches cannot estimate the gender. Our system,
on the other hand, is able to correctly determine that
unknown words corroborators and propeller-heads
are plural, while Pope Formosus is masculine, using
learned contextual and morphological cues.
Our approach is based on the key observation that
while gender information from noun-pronoun co-
occurrence provides imperfect noun coverage, it can
nevertheless provide rich and accurate training data
for a large-scale discriminative classifier. The clas-
sifier leverages a wide variety of noun properties to
generalize from the automatically-labeled examples.
The steps in our approach are:
1. Training:
(a) Automatically extract a set of seed
(noun,gender) pairs from high-quality in-
stances in a statistical gender database.
(b) In a large corpus of text, find documents con-
taining these nouns.
(c) For all instances of each noun in each document,
create a single, composite feature vector repre-
senting all the contexts of the noun in the docu-
ment, as well as encoding other selected proper-
ties of the noun type.
(d) Label each feature vector with the seed noun?s
corresponding gender.
(e) Train a 4-way gender classifier (masculine, fem-
inine, neutral, plural) from the automatically-
labeled vectors.
2. Testing:
(a) Given a new document, create a composite fea-
ture vector for all occurrences of each noun.
(b) Use the learned classifier to assign gender to
each feature vector, and thus all occurrences of
all nouns in the document.
This algorithm achieves significantly better per-
formance than the existing state-of-the-art statisti-
cal gender classifier, while requiring no manually-
labeled examples to train. Furthermore, by training
on a small number of manually-labeled examples,
we can combine the predictions of this system with
the counts from the original gender database. This
semi-supervised extension achieves 95.5% accuracy
on final unseen test data, an impressive 50% reduc-
tion in error over previous work.
2 Path-based Statistical Noun Gender
Seed (noun,gender) examples can be extracted re-
liably and automatically from raw text, providing
the training data for our discriminative classifier.
We call these examples pseudo-seeds because they
are created fully automatically, unlike the small set
of manually-created seeds used to initialize other
bootstrapping approaches (cf. the bootstrapping ap-
proaches discussed in Section 6).
We adopt a statistical approach to acquire the
pseudo-seed (noun,gender) pairs. All previous sta-
tistical approaches rely on a similar observation: if
a noun like Glen is often referred to by masculine
pronouns, like he or his, then Glen is likely a mas-
culine noun. But for most nouns we have no an-
notated data recording their coreference with pro-
nouns, and thus no data from which we can ex-
tract the co-occurrence statistics. Thus previous ap-
proaches rely on either hand-crafted coreference-
indicating patterns (Bergsma, 2005), or iteratively
guess and improve gender models through expec-
tation maximization of pronoun resolution (Cherry
and Bergsma, 2005; Charniak and Elsner, 2009). In
statistical approaches, the more frequent the noun,
the more accurate the assignment of gender.
We use the approach of Bergsma and Lin (2006),
both because it achieves state-of-the-art gender
classification performance, and because a database
of the obtained noun genders is available online.1
Bergsma and Lin (2006) use an unsupervised
algorithm to identify syntactic paths along which a
noun and pronoun are highly likely to corefer. To
extract gender information, they processed a large
corpus of news text, and obtained co-occurrence
counts for nouns and pronouns connected with these
paths in the corpus. In their database, each noun is
listed with its corresponding masculine, feminine,
neutral, and plural pronoun co-occurrence counts,
e.g.:
glen 555 42 32 34
glenda 8 102 0 11
glendale 24 2 167 18
glendalians 0 0 0 1
glenn 3182 207 95 54
glenna 0 6 0 0
1Available at http://www.cs.ualberta.ca/?bergsma/Gender/
121
This sample of the gender data shows that the
noun glenda, for example, occurs 8 times with mas-
culine pronouns, 102 times with feminine pronouns,
0 times with neutral pronouns, and 11 times with
plural pronouns; 84% of the time glenda co-occurs
with a feminine pronoun. Note that all nouns in the
data have been converted to lower-case.2
There are gender counts for 3.1 million English
nouns in the online database. These counts form the
basis for the state-of-the-art gender classifier. We
can either take the most-frequent pronoun-gender
(MFPG) as the class (e.g. feminine for glenda), or
we can supply the logarithm of the counts as features
in a 4-way multi-class classifier. We implement the
latter approach as a comparison system and refer to
it as PATHGENDER in our experiments.
In our approach, rather than using these counts
directly, we process the database to automatically
extract a high-coverage but also high-quality set of
pseudo-seed (noun,gender) pairs. First, we filter
nouns that occur less than fifty times and whose
MFPG accounts for less than 85% of counts. Next,
we note that the most reliable nouns should occur
relatively often in a coreferent path. For exam-
ple, note that importance occurs twice as often on
the web as Clinton, but has twenty-four times less
counts in the gender database. This is because im-
portance is unlikely to be a pronoun?s antecedent.
We plan to investigate this idea further in future
work as a possible filter on antecedent candidates
for pronoun resolution. For the present work, sim-
ply note that a high ratio of database-count to web-
count provides a good indication of the reliability of
a noun?s gender counts, and thus we filter nouns that
have such ratios below a threshold.3 After this fil-
tering, we have about 45 thousand nouns to which
we automatically assign gender according to their
MFPG. These (noun,gender) pairs provide the seed
examples for the training process described in the
2Statistical approaches can adapt to the idiosyncrasies of the
particular text domain. In the news text from which this data
was generated, for example, both the word ships and specific
instances of ships (the USS Cole, the Titanic, etc.) are neutral.
In Wikipedia, on the other hand, feminine pronouns are often
used for ships. Such differences can be learned automatically.
3We roughly tuned all the thresholds to obtain the highest
number of seeds such that almost all of them looked correct
(e.g. Figure 1). Further work is needed to determine whether a
different precision/recall tradeoff can improve performance.
. . .
stefanie
steffi graf
steinem
stella mccartney
stellar jayne
stepdaughter
stephanie
stephanie herseth
stephanie white
stepmother
stewardess
. . .
Figure 1: Sample feminine seed nouns
following section. Figure 1 provides a portion of the
ordered feminine seed nouns that we extracted.
3 Discriminative Learning of Gender
Once we have extracted a number of pseudo-seed
(noun,gender) pairs, we use them to automatically-
label nouns (in context) in raw text. The auto-
labeled examples provide training data for discrimi-
native learning of noun gender.
Since the training pairs are acquired from a
sparse and imperfect model of gender, what can
we gain by training over them? We can regard the
Bergsma and Lin (2006) approach and our discrim-
inative system as two orthogonal views of gender,
in a co-training sense (Blum and Mitchell, 1998).
Some nouns can be accurately labeled by noun-
pronoun co-occurrence (a view based on pronoun
co-occurrence), and these examples can be used to
deduce other gender-indicating regularities (a view
based on other features, described below).
We presently explain how examples are extracted
using our pseudo-seed pairs, turned into auto-
labeled feature vectors, and then used to train a su-
pervised classifier.
3.1 Automatic example extraction
Our example-extraction module processes a large
collection of documents (roughly a million docu-
ments in our experiments). For each document, we
extract all the nouns, including context words within
?5 tokens of each noun. We then group the nouns by
122
Class=masculine String=?Lee?
Contexts =
?led some to suggest that ? , who was born in?
?? also downloaded secret files to?
?? says he was just making?
?by mishandling the investigation of ? .?
. . .
Figure 2: Sample noun training instance
their (lower-case) string. If a group?s noun-string is
in our set of seed (noun,gender) pairs, we assign the
corresponding gender to be the class of the group.
Otherwise, we discard the group. To prevent fre-
quent nouns from dominating our training data, we
only keep the first 200 groups corresponding to each
noun string. Figure 2 gives an example training noun
group with some (selected) context sentences. At
test time, all nouns in the test documents are con-
verted to this format for further processing.
We group nouns because there is a strong ten-
dency for nouns to have only one sense (and hence
gender) per discourse. We extract contexts because
nearby words provide good clues about which gen-
der is being used. The notion that nouns have only
one sense per discourse/collocation was also ex-
ploited by Yarowsky (1995) in his seminal work on
bootstrapping for word sense disambiguation.
3.2 Feature vectors
Once the training instances are extracted, they are
converted to labeled feature vectors for supervised
learning. The automatically-determined gender pro-
vides the class label (e.g., masculine for the group
in Figure 2). The features identify properties of the
noun and its context that potentially correlate with a
particular gender category. We divide the features
into two sets: those that depend on the contexts
within the document (Context features: features of
the tokens in the document), and those that depend
on the noun string only (Type features). In both
cases we induce the feature space from the train-
ing examples, keeping only those features that occur
more than 5 times.
3.2.1 Context features
The first set of features represent the contexts of
the word, using all the contexts in the noun group.
To illustrate the potential utility of the context infor-
mation, consider the context sentences for the mas-
culine noun in Figure 2. Even if these snippets were
all the information we were given, it would be easy
to guess the gender of the noun.
We use binary attribute-value features to flag, for
any of the contexts, the presence of all words at con-
text positions ?1,?2, etc. (sometimes called col-
location features (Golding and Roth, 1999)). For
example, feature 255920 flags that the word two-to-
the-right of the noun is he. We also provide fea-
tures for the presence of all words anywhere within
?5 tokens of the noun (sometimes called context
words). We also parse the sentence and provide a
feature for the noun?s parent (and relationship with
the parent) in the parse tree. For example, the in-
stance in Figure 2 has features downloaded(subject),
says(subject), etc. Since plural nouns should be gov-
erned by plural verbs, this feature is likely to be es-
pecially helpful for number classification.
3.2.2 Type features
The next group of features represent morpholog-
ical properties of the noun. Binary features flag the
presence of all prefixes and suffixes of one-to-four
characters. For multi-token nouns, we have features
for the first and last token in the noun. Thus we hope
to learn that Bob begins masculine nouns while inc.
ends neutral ones.
Finally, we have features that indicate if the noun
or parts of the noun occur on various lists. Indica-
tor features specify if any token occurs on in-house
lists of given names, family names, cities, provinces,
countries, corporations, languages, etc. A feature
also indicates if a token is a corporate designation
(like inc. or ltd.) or a human one (like Mr. or Sheik).
We also made use of the person-name/instance
pairs automatically extracted by Fleischman et al
(2003).4 This data provides counts for pairs such
as (Zhang Qiyue, spokeswoman) and (Thorvald
Stoltenberg, mediator). We have features for all con-
cepts (like spokeswoman and mediator) and there-
fore learn their association with each gender.
3.3 Supervised learning and classification
Once all the feature vectors have been extracted,
they are passed to a supervised machine learn-
4Available at http://www.mit.edu/?mbf/instances.txt.gz
123
ing algorithm. We train and classify using a
multi-class linear-kernel Support Vector Machine
(SVM) (Crammer and Singer, 2001). SVMs are
maximum-margin classifiers that achieve good per-
formance on a range of tasks. At test time, nouns in
test documents are processed exactly as the training
instances described above, converting them to fea-
ture vectors. The test vectors are classified by the
SVM, providing gender classes for all the nouns in
the test document. Since all training examples are
labeled automatically (auto-trained), we denote sys-
tems using this approach as -AUTO.
3.4 Semi-supervised extension
Although a good gender classifier can be learned
from the automatically-labeled examples alone, we
can also use a small quantity of gold-standard la-
beled examples to achieve better performance.
Combining information from our two sets of la-
beled data is akin to a domain adaptation prob-
lem. The gold-standard data can be regarded as
high-quality in-domain data, and the automatically-
labeled examples can be regarded as the weaker, but
larger, out-of-domain evidence.
There is a simple but effective method for com-
bining information from two domains using predic-
tions as features. We train a classifier on the full set
of automatically-labeled data (as described in Sec-
tion 3.3), and then use this classifier?s predictions as
features in a separate classifier, which is trained on
the gold-standard data. This is like the competitive
Feats domain-adaptation system in Daume? III and
Marcu (2006).
For our particular SVM classifier (Section 4.1),
predictions take the form of four numerical scores
corresponding to the four different genders. Our
gold-standard classifier has features for these four
predictions plus features for the original path-based
gender counts (Section 2).5 Since this approach uses
both automatically-labeled and gold-standard data in
a semi-supervised learning framework, we denote
systems using this approach as -SEMI.
5We actually use 12 features for the path-based counts: the
4 original, and then 4 each for counts for the first and last token
in the noun string. See PATHGENDER+ in Section 4.2.
4 Experiments
4.1 Set-up
We parsed the 3 GB AQUAINT corpus (Vorhees,
2002) using Minipar (Lin, 1998) to create our un-
labeled data. We process this data as described in
Section 3, making feature vectors from the first 4
million noun groups. We train from these exam-
ples using a linear-kernel SVM via the the efficient
SVMmulticlass instance of the SVMstruct software
package (Tsochantaridis et al, 2004).
To create our gold-standard gender data, we fol-
low Bergsma (2005) in extracting gender informa-
tion from the anaphora-annotated portion6 of the
American National Corpus (ANC) (Ide and Sud-
erman, 2004). In each document, we first group
all nouns with a common lower-case string (exactly
as done for our example extraction (Section 3.1)).
Next, for each group we determine if a third-person
pronoun refers to any noun in that group. If so, we
label all nouns in the group with the gender of the
referring pronoun. For example, if the pronoun he
refers to a noun Brown, then all instances of Brown
in the document are labeled as masculine. We ex-
tract the genders for 2794 nouns in the ANC train-
ing set (in 798 noun groups) and 2596 nouns in the
ANC test set (in 642 groups). We apply this method
to other annotated corpora (including MUC corpora)
to create a development set.
The gold standard ANC training set is used to
set the weights on the counts in the PATHGENDER
classifiers, and to train the semi-supervised ap-
proaches. We also use an SVM to learn these
weights. We use the development set to tune the
SVM?s regularization parameter, both for systems
trained on automatically-generated data, and for sys-
tems trained on gold-standard data. We also opti-
mize each automatically-trained system on the de-
velopment set when we include this system?s pre-
dictions as features in the semi-supervised exten-
sion. We evaluate and state performance for all ap-
proaches on the final unseen ANC test set.
4.2 Evaluation
The primary purpose of our experiments is to de-
termine if we can improve on the existing state-of-
the-art in gender classification (path-based gender
6Available at http://www.cs.ualberta.ca/?bergsma/CorefTags/
124
counts). We test systems both trained purely on
automatically-labeled data (Section 3.3), and those
that leverage some gold-standard annotations in a
semi-supervised setting (Section 3.4). Another pur-
pose of our experiments is to investigate the relative
value of our context-based features and type-based
features. We accomplish these objectives by imple-
menting and evaluating the following systems:
1. PATHGENDER:
A classifier with the four path-based gender
counts as features (Section 2).
2. PATHGENDER+:
A method of back-off to help classify unseen
nouns: For multi-token nouns (like Bob John-
son), we also include the four gender counts
aggregated over all nouns sharing the first to-
ken (Bob .*), and the four gender counts over
all nouns sharing the last token (.* Johnson).
3. CONTEXT-AUTO:
Auto-trained system using only context fea-
tures (Section 3.2.1).
4. TYPE-AUTO:
Auto-trained system using only type features
(Section 3.2.2).
5. FULL-AUTO:
Auto-trained system using all features.
6. CONTEXT-SEMI:
Semi-sup. combination of the PATHGENDER+
features and the CONTEXT-AUTO predictions.
7. TYPE-SEMI:
Semi-sup. combination of the PATHGENDER+
features and the TYPE-AUTO predictions.
8. FULL-SEMI:
Semi-sup. combination of the PATHGENDER+
features and the FULL-AUTO predictions.
We evaluate using accuracy: the percentage of
labeled nouns that are correctly assigned a gender
class. As a baseline, note that always choosing
neutral achieves 38.1% accuracy on our test data.
5 Results and Discussion
5.1 Main results
Table 1 provides our experimental results. The orig-
inal gender counts already do an excellent job clas-
sifying the nouns; PATHGENDER achieves 91.0%
accuracy by looking for exact noun matches. Our
1. PATHGENDER 91.0
2. PATHGENDER+ 92.1
3. CONTEXT-AUTO 79.1
4. TYPE-AUTO 89.1
5. FULL-AUTO 92.6
6. CONTEXT-SEMI 92.4
7. TYPE-SEMI 91.3
8. FULL-SEMI 95.5
Table 1: Noun gender classification accuracy (%)
simple method of using back-off counts for the first
and last token, PATHGENDER+, achieves 92.1%.
While PATHGENDER+ uses gold standard data to
determine optimum weights on the twelve counts,
FULL-AUTO achieves 92.6% accuracy using no
gold standard training data. This confirms that our
algorithm, using no manually-labeled training data,
can produce a competitive gender classifier.
Both PATHGENDER and PATHGENDER+ do
poorly on the noun types that have low counts in
the gender database, achieving only 63% and 66%
on nouns with less than ten counts. On these
same nouns, FULL-AUTO achieves 88% perfor-
mance, demonstrating the robustness of the learned
classifier on the most difficult examples for previ-
ous approaches (FULL-SEMI achieves 94% on these
nouns).
If we break down the contribution of the two fea-
ture types in FULL-AUTO, we find that we achieve
89.1% accuracy by only using type features, while
we achieve 79.1% with only context features. While
not as high as the type-based accuracy, it is impres-
sive that almost four out of five nouns can be classi-
fied correctly based purely on the document context,
using no information about the noun itself. This is
information that has not previously been systemati-
cally exploited in gender classification models.
We examine the relationship between training
data size and accuracy by plotting a (logarithmic-
scale) learning curve for FULL-AUTO (Figure 3).
Although using four million noun groups originally
seemed sufficient, performance appears to still be in-
creasing. Since more training data can be generated
automatically, it appears we have not yet reached the
full power of the FULL-AUTO system. Of course,
even with orders of magnitude more data, the system
125
 70
 75
 80
 85
 90
 95
 100
 1000  10000  100000  1e+06  1e+07
Ac
cu
ra
cy
 (%
)
Number of training examples
Figure 3: Noun gender classification learning curve for
FULL-AUTO
does not appear destined to reach the performance
obtained through other means described below.
We achieve even higher accuracy when the output
of the -AUTO systems are combined with the orig-
inal gender counts (the semi-supervised extension).
The relative value of the context and type-based fea-
tures is now reversed: using only context-based fea-
tures (CONTEXT-SEMI) achieves 92.4%, while us-
ing only type-based features (TYPE-SEMI) achieves
91.3%. This is because much of the type informa-
tion is already implicit in the PATHGENDER counts.
The TYPE-AUTO predictions contribute little infor-
mation, only fragmenting the data and leading to
over-training and lower accuracy. On the other hand,
the CONTEXT-AUTO predictions improve accuracy,
as these scores provide orthogonal and hence helpful
information for the semi-supervised classifier.
Combining FULL-AUTO with our enhanced path
gender counts, PATHGENDER+, results in the over-
all best performance, 95.5% for FULL-SEMI, signif-
icantly better than PATHGENDER+ alone.7 This is
a 50% error reduction over the PATHGENDER sys-
tem, strongly confirming the benefit of our semi-
supervised approach.
To illustrate the importance of the unlabeled data,
we created a system that uses all features, including
the PATHGENDER+ counts, and trained this system
using only the gold standard training data. This sys-
tem was unable to leverage the extra features to im-
prove performance; its accuracy was 92.0%, roughly
equal to PATHGENDER+ alone. While SVMs work
7We evaluate significance using McNemar?s test, p<0.01.
Since McNemar?s test assumes independent classifications, we
apply the test to the classification of noun groups, not instances.
well with high-dimensional data, they simply cannot
exploit features that do not occur in the training set.
5.2 Further improvements
We can improve performance further by doing some
simple coreference before assigning gender. Cur-
rently, we only group nouns with the same string,
and then decide gender collectively for the group.
There are a few cases, however, where an ambiguous
surname, such as Willey, can only be classified cor-
rectly if we link the surname to an earlier instance of
the full name, e.g. Katherine Willey. We thus added
the following simple post-processing rule: If a noun
is classified as masculine or feminine (like the am-
biguous Willey), and it was observed earlier as the
last part of a larger noun, then re-assign the gender
to masculine or feminine if one of these is the most
common path-gender count for the larger noun. We
back off to counts for the first name (e.g. Kathleen
.*) if the full name is unobserved.
This enhancement improved the PATHGENDER
and PATHGENDER+ systems to 93.3% and 94.3%,
respectively, while raising the accuracy of our
FULL-SEMI system to 96.7%. This demonstrates
that the surname-matching post-processor is a sim-
ple but worthwhile extension to a gender predictor.8
The remaining errors represent a number of chal-
lenging cases: United States, group, and public la-
beled as plural but classified as neutral ; spectator
classified as neutral , etc. Some of these may yield
to more sophisticated joint classification of corefer-
ence and gender, perhaps along the lines of work in
named-entity classification (Bunescu and Mooney,
2004) or anaphoricity (Denis and Baldridge, 2007).
While gender has been shown to be the key fea-
ture for statistical pronoun resolution (Ge et al,
1998; Bergsma and Lin, 2006), it remains to be
seen whether the exceptional accuracy obtained here
will translate into improvements in resolution per-
formance. However, given the clear utility of gender
in coreference, substantial error reductions in gender
8One might wonder, why not provide special features so that
the system can learn how to handle ambiguous nouns that oc-
curred as sub-phrases in earlier names? The nature of our train-
ing data precludes this approach. We only include unambiguous
examples as pseudo-seeds in the learning process. Without
providing ambiguous (but labeled) surnames in some way, the
learner will not take advantage of features to help classify them.
126
assignment will likely be a helpful contribution.
6 Related Work
Most coreference and pronoun resolution papers
mention that they use gender information, but few
explain how it is acquired. Kennedy and Boguraev
(1996) use gender information produced by their en-
hanced part-of-speech tagger. Gender mistakes ac-
count for 35% of their system?s errors. Gender is
less crucial in some genres, like computer manuals;
most nouns are either neutral or plural and gender
can be determined accurately based solely on mor-
phological information (Lappin and Leass, 1994).
A number of researchers (Evans and Ora?san,
2000; Soon et al, 2001; Harabagiu et al, 2001) use
WordNet classes to infer gender knowledge. Unfor-
tunately, manually-constructed databases like Word-
Net suffer from both low coverage and rare senses.
Pantel and Ravichandran (2004) note that the nouns
computer and company both have a WordNet sense
that is a hyponym of person, falsely indicating these
nouns would be compatible with pronouns like he
or she. In addition to using WordNet classes, Soon
et al (2001) assign gender if the noun has a gen-
dered designator (like Mr. or Mrs.) or if the first
token is present on a list of common human first
names. Note that we incorporate such contextual
and categorical information (among many other in-
formation sources) automatically in our discrimina-
tive classifier, while they manually specify a few
high-precision rules for particular gender cues.
Ge et al (1998) pioneered the statistical approach
to gender determination. Like others, they consider
gender and number separately, only learning statis-
tical gender for the masculine, feminine, and neu-
tral classes. While gender and number can be han-
dled together for pronoun resolution, it might be use-
ful to learn them separately for other applications.
Other statistical approaches to English noun gender
are discussed in Section 2.
In languages with ?grammatical? gender and plen-
tiful gold standard data, gender can be tagged along
with other word properties using standard super-
vised tagging techniques (Hajic? and Hladka?, 1997).
While our approach is the first to exploit a dual
or orthogonal representation of English noun gen-
der, a bootstrapping approach has been applied to
determining grammatical gender in other languages
by Cucerzan and Yarowsky (2003). In their work,
the two orthogonal views are: 1) the context of the
noun, and 2) the noun?s morphological properties.
Bootstrapping with these views is possible in other
languages where context is highly predictive of gen-
der class, since contextual words like adjectives and
determiners inflect to agree with the grammatical
noun gender. We initially attempted a similar system
for English noun gender but found context alone to
be insufficiently predictive.
Bootstrapping is also used in general information
extraction. Brin (1998) shows how to alternate be-
tween extracting instances of a class and inducing
new instance-extracting patterns. Collins and Singer
(1999) and Cucerzan and Yarowsky (1999) apply
bootstrapping to the related task of named-entity
recognition. Our approach was directly influenced
by the hypernym-extractor of Snow et al (2005) and
we provided an analogous summary in Section 1.
While their approach uses WordNet to label hyper-
nyms in raw text, our initial labels are generated au-
tomatically. Etzioni et al (2005) also require no la-
beled data or hand-labeled seeds for their named-
entity extractor, but by comparison their classifier
only uses a very small number of both features and
automatically-generated training examples.
7 Conclusion
We have shown how noun-pronoun co-occurrence
counts can be used to automatically annotate the
gender of millions of nouns in unlabeled text. Train-
ing from these examples produced a classifier that
clearly exceeds the state-of-the-art in gender classi-
fication. We incorporated thousands of useful but
previously unexplored indicators of noun gender as
features in our classifier. By combining the pre-
dictions of this classifier with the original gender
counts, we were able to produce a gender predic-
tor that achieves 95.5% classification accuracy on
2596 test nouns, a 50% reduction in error over the
current state-of-the-art. A further name-matching
post-processor reduced error even further, resulting
in 96.7% accuracy on the test data. Our final system
is the broadest and most accurate gender model yet
created, and should be of value to many pronoun and
coreference resolution systems.
127
References
Shane Bergsma and Dekang Lin. 2006. Bootstrap-
ping path-based pronoun resolution. In COLING-
ACL, pages 33?40.
Shane Bergsma. 2005. Automatic acquisition of gen-
der information for anaphora resolution. In Canadian
Conference on Artificial Intelligence, pages 342?353.
Avrim Blum and Tom Mitchell. 1998. Combining la-
beled and unlabeled data with co-training. In COLT,
pages 92?100.
Sergey Brin. 1998. Extracting patterns and relations
from the world wide web. In WebDB Workshop at
6th International Conference on Extending Database
Technology, pages 172?183.
Razvan Bunescu and Raymond J. Mooney. 2004. Col-
lective information extraction with relational Markov
networks. In ACL, pages 438?445.
Eugene Charniak and Micha Elsner. 2009. EM works for
pronoun anaphora resolution. In EACL.
Colin Cherry and Shane Bergsma. 2005. An expecta-
tion maximization approach to pronoun resolution. In
CoNLL, pages 88?95.
Michael Collins and Yoram Singer. 1999. Unsupervised
models for named entity classification. In EMNLP-
VLC, pages 100?110.
Koby Crammer and Yoram Singer. 2001. On the al-
gorithmic implementation of multiclass kernel-based
vector machines. Journal of Machine Learning Re-
search, 2:265?292.
Silviu Cucerzan and David Yarowsky. 1999. Language
independent named entity recognition combining mor-
phological and contextual evidence. In EMNLP-VLC,
pages 90?99.
Silviu Cucerzan and David Yarowsky. 2003. Mini-
mally supervised induction of grammatical gender. In
NAACL, pages 40?47.
Hal Daume? III and Daniel Marcu. 2006. Domain adap-
tation for statistical classifiers. Journal of Artificial
Intelligence Research, 26:101?126.
Pascal Denis and Jason Baldridge. 2007. Joint determi-
nation of anaphoricity and coreference using integer
programming. In NAACL-HLT, pages 236?243.
Oren Etzioni, Michael Cafarella, Doug Downey, Ana-
Maria Popescu, Tal Shaked, Stephen Soderland,
Daniel S. Weld, and Alexander Yates. 2005. Unsu-
pervised named-entity extraction from the web: an ex-
perimental study. Artif. Intell., 165(1):91?134.
Richard Evans and Constantin Ora?san. 2000. Improving
anaphora resolution by identifying animate entities in
texts. In DAARC, pages 154?162.
Michael Fleischman, Eduard Hovy, and Abdessamad
Echihabi. 2003. Offline strategies for online question
answering: answering questions before they are asked.
In ACL, pages 1?7.
Niyu Ge, John Hale, and Eugene Charniak. 1998. A sta-
tistical approach to anaphora resolution. In Proceed-
ings of the Sixth Workshop on Very Large Corpora,
pages 161?171.
Andrew R. Golding and Dan Roth. 1999. A Winnow-
based approach to context-sensitive spelling correc-
tion. Machine Learning, 34(1-3):107?130.
Jan Hajic? and Barbora Hladka?. 1997. Probabilistic and
rule-based tagger of an inflective language: a compar-
ison. In ANLP, pages 111?118.
Sanda Harabagiu, Razvan Bunescu, and Steven Maio-
rano. 2001. Text and knowledge mining for coref-
erence resolution. In NAACL, pages 55?62.
Nancy Ide and Keith Suderman. 2004. The American
National Corpus first release. In LREC, pages 1681?
84.
Christopher Kennedy and Branimir Boguraev. 1996.
Anaphora for everyone: Pronominal anaphora resolu-
tion without a parser. In COLING, pages 113?118.
Shalom Lappin and Herbert J. Leass. 1994. An algo-
rithm for pronominal anaphora resolution. Computa-
tional Linguistics, 20(4):535?561.
Dekang Lin. 1998. Dependency-based evaluation of
MINIPAR. In LREC Workshop on the Evaluation of
Parsing Systems.
Patrick Pantel and Deepak Ravichandran. 2004. Auto-
matically labeling semantic classes. In HLT-NAACL,
pages 321?328.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2005.
Learning syntactic patterns for automatic hypernym
discovery. In NIPS, pages 1297?1304.
Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong
Lim. 2001. A machine learning approach to corefer-
ence resolution of noun phrases. Computational Lin-
guistics, 27(4):521?544.
Ioannis Tsochantaridis, Thomas Hofmann, Thorsten
Joachims, and Yasemin Altun. 2004. Support vec-
tor machine learning for interdependent and structured
output spaces. In ICML.
Ellen Vorhees. 2002. Overview of the TREC 2002 ques-
tion answering track. In Proceedings of the Eleventh
Text REtrieval Conference (TREC).
David Yarowsky. 1995. Unsupervised word sense disam-
biguation rivaling supervised methods. In ACL, pages
189?196.
128
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 53?61,
Beijing, August 2010
Fast and Accurate Arc Filtering for Dependency Parsing
Shane Bergsma
Department of Computing Science
University of Alberta
sbergsma@ualberta.ca
Colin Cherry
Institute for Information Technology
National Research Council Canada
colin.cherry@nrc-cnrc.gc.ca
Abstract
We propose a series of learned arc fil-
ters to speed up graph-based dependency
parsing. A cascade of filters identify im-
plausible head-modifier pairs, with time
complexity that is first linear, and then
quadratic in the length of the sentence.
The linear filters reliably predict, in con-
text, words that are roots or leaves of de-
pendency trees, and words that are likely
to have heads on their left or right. We
use this information to quickly prune arcs
from the dependency graph. More than
78% of total arcs are pruned while retain-
ing 99.5% of the true dependencies. These
filters improve the speed of two state-of-
the-art dependency parsers, with low over-
head and negligible loss in accuracy.
1 Introduction
Dependency parsing finds direct syntactic rela-
tionships between words by connecting head-
modifier pairs into a tree structure. Depen-
dency information is useful for a wealth of nat-
ural language processing tasks, including ques-
tion answering (Wang et al, 2007), semantic pars-
ing (Poon and Domingos, 2009), and machine
translation (Galley and Manning, 2009).
We propose and test a series of arc filters for
graph-based dependency parsers, which rule out
potential head-modifier pairs before parsing be-
gins. In doing so, we hope to eliminate im-
plausible links early, saving the costs associated
with them, and speeding up parsing. In addi-
tion to the scaling benefits that come with faster
processing, we hope to enable richer features
for parsing by constraining the set of arcs that
need to be considered. This could allow ex-
tremely large feature sets (Koo et al, 2008), or the
look-up of expensive corpus-based features such
as word-pair mutual information (Wang et al,
2006). These filters could also facilitate expen-
sive learning algorithms, such as semi-supervised
approaches (Wang et al, 2008).
We propose three levels of filtering, which are
applied in a sequence of increasing complexity:
Rules: A simple set of machine-learned rules
based only on parts-of-speech. They prune over
25% of potential arcs with almost no loss in cover-
age. Rules save on the wasted effort for assessing
implausible arcs such as DT? DT.
Linear: A series of classifiers that tag words ac-
cording to their possible roles in the dependency
tree. By treating each word independently and en-
suring constant-time feature extraction, they oper-
ate in linear time. We view these as a dependency-
parsing analogue to the span-pruning proposed by
Roark and Hollingshead (2008). Our fast linear
filters prune 54.2% of potential arcs while recov-
ering 99.7% of true pairs.
Quadratic: A final stage that looks at pairs of
words to prune unlikely arcs from the dependency
tree. By employing a light-weight feature set, this
high-precision filter can enable more expensive
processing on the remaining plausible dependen-
cies.
Collectively, we show that more than 78% of
total arcs can be pruned while retaining 99.5% of
the true dependencies. We test the impact of these
filters at both train and test time, using two state-
of-the-art discriminative parsers, demonstrating
speed-ups of between 1.9 and 5.6, with little im-
pact on parsing accuracy.
53
Investors continue to pour cash into money funds
Figure 1: An example dependency parse.
2 Dependency Parsing
A dependency tree represents the syntactic struc-
ture of a sentence as a directed graph (Figure 1),
with a node for each word, and arcs indicat-
ing head-modifier pairs (Me?lc?uk, 1987). Though
dependencies can be extracted from many for-
malisms, there is a growing interest in predict-
ing dependency trees directly. To that end, there
are two dominant approaches: graph-based meth-
ods, characterized by arc features in an exhaus-
tive search, and transition-based methods, char-
acterized by operational features in a greedy
search (McDonald and Nivre, 2007). We focus on
graph-based parsing, as its exhaustive search has
the most to gain from our filters.
Graph-based dependency parsing finds the
highest-scoring tree according to a scoring func-
tion that decomposes under an exhaustive search
(McDonald et al, 2005). The most natural de-
composition scores individual arcs, represented as
head-modifier pairs [h,m]. This enables search
by either minimum spanning tree (West, 2001) or
by Eisner?s (1996) projective parser. This paper
focuses on the projective case, though our tech-
niques transfer to spanning tree parsing. With a
linear scoring function, the parser solves:
parse(s) = argmaxt?s
?
[h,m]?t
w? ? f?(h,m, s)
The weights w? are typically learned using an
online method, such as an averaged percep-
tron (Collins, 2002) or MIRA (Crammer and
Singer, 2003). 2nd-order searches, which consider
two siblings at a time, are available with no in-
crease in asymptotic complexity (McDonald and
Pereira, 2006; Carreras, 2007).
The complexity of graph-based parsing is
bounded by two processes: parsing (carrying out
the argmax) and arc scoring (calculating w? ?
f?(h,m, s)). For a sentence with n words, pro-
jective parsing takes O(n3) time, while the span-
ning tree algorithm is O(n2). Both parsers require
scores for arcs connecting each possible [h,m]
pair in s; therefore, the cost of arc scoring is also
O(n2), and may become O(n3) if the features in-
clude words in s between h and m (Galley and
Manning, 2009). Arc scoring also has a signif-
icant constant term: the number of features ex-
tracted for an [h,m] pair. Our in-house graph-
based parser collects on average 62 features for
each potential arc, a number larger than the length
of most sentences. With the cluster-based features
suggested by Koo et al (2008), this could easily
grow by a factor of 3 or 4.
The high cost of arc scoring, coupled with
the parsing stage?s low grammar constant, means
that graph-based parsers spend much of their time
scoring potential arcs. Johnson (2007) reports that
when arc scores have been precomputed, the dy-
namic programming component of his 1st-order
parser can process an amazing 3,580 sentences per
second.1 Beyond reducing the number of features,
the easiest way to reduce the computational bur-
den of arc scoring is to score only plausible arcs.
3 Related Work
3.1 Vine Parsing
Filtering dependency arcs has been explored pri-
marily in the form of vine parsing (Eisner and
Smith, 2005; Dreyer et al, 2006). Vine pars-
ing establishes that, since most dependencies are
short, one can parse quickly by placing a hard
constraint on arc length. As this coarse fil-
ter quickly degrades the best achievable perfor-
mance, Eisner and Smith (2005) also consider
conditioning the constraint on the part-of-speech
(PoS) tags being linked and the direction of the
arc, resulting in a separate threshold for each
[tag(h), tag(m),dir(h,m)] triple. They sketch
an algorithm where the thresholded length for
each triple starts at the highest value seen in the
training data. Thresholds are then decreased in
a greedy fashion, with each step producing the
smallest possible reduction in reachable training
arcs. We employ this algorithm as a baseline in
our experiments. To our knowledge, vine parsing
1To calibrate this speed, consider that the publicly avail-
able 1st-order MST parser processes 16 sentences per second
on modern hardware. This includes I/O costs in addition to
the costs of arc scoring and parsing.
54
has not previously been tested with a state-of-the-
art, discriminative dependency parser.
3.2 CFG Cell Classification
Roark and Hollingshead (2008) speed up another
exhaustive parsing algorithm, the CKY parser for
CFGs, by classifying each word in the sentence
according to whether it can open (or close) a
multi-word constituent. With a high-precision
tagger that errs on the side of permitting con-
stituents, they show a significant improvement in
speed with no reduction in accuracy.
It is difficult to port their idea directly to depen-
dency parsing without committing to a particular
search algorithm,2 and thereby sacrificing some
of the graph-based formalism?s modularity. How-
ever, some of our linear filters (see Section 4.3)
were inspired by their constraints.
3.3 Coarse-to-fine Parsing
Another common method employed to speed up
exhaustive parsers is a coarse-to-fine approach,
where a cheap, coarse model prunes the search
space for later, more expensive models (Charniak
et al, 2006; Petrov and Klein, 2007). This ap-
proach assumes a common forest or chart repre-
sentation, shared by all granularities, where one
can efficiently track the pruning decisions of the
coarse models. One could imagine applying such
a solution to dependency parsing, but the exact
implementation of the coarse pass would vary ac-
cording to the choice in search algorithm. Our fil-
ters are much more modular: they apply to both
1st-order spanning tree parsing and 2nd-order pro-
jective parsing, with no modification.
Carreras et al (2008) use coarse-to-fine pruning
with dependency parsing, but in that case, a graph-
based dependency parser provides the coarse pass,
with the fine pass being a far-more-expensive tree-
adjoining grammar. Our filters could become a
0th pass, further increasing the efficiency of their
approach.
4 Arc Filters
We propose arc filtering as a preprocessing step
for dependency parsing. An arc filter removes im-
2Johnson?s (2007) split-head CFG could implement this
idea directly with little effort.
plausible head-modifier arcs from the complete
dependency graph (which initially includes all
head-modifier arcs). We use three stages of filters
that operate in sequence on progressively sparser
graphs: 1) rule-based, 2) linear: a single pass
through the n nodes in a sentence (O(n) complex-
ity), and 3) quadratic: a scoring of all remaining
arcs (O(n2)). The less intensive filters are used
first, saving time by leaving fewer arcs to be pro-
cessed by the more intensive systems.
Implementations of our rule-based, linear, and
quadratic filters are publicly available at:
http://code.google.com/p/arcfilter/
4.1 Filter Framework
Our filters assume the input sentences have been
PoS-tagged. We also add an artificial root node
to each sentence to be the head of the tree?s root.
Initially, this node is a potential head for all words
in the sentence.
Each filter is a supervised classifier. For exam-
ple, the quadratic filter directly classifies whether
a proposed head-modifier pair is not a link in the
dependency tree. Training data is created from an-
notated trees. All possible arcs are extracted for
each training sentence, and those that are present
in the annotated tree are labeled as class?1, while
those not present are +1. A similar process gener-
ates training examples for the other filters. Since
our goal is to only filter very implausible arcs, we
bias the classifier to high precision, increasing the
cost for misclassifying a true arc during learning.3
Class-specific costs are command-line parame-
ters for many learning packages. One can inter-
pret the learning objective as minimizing regular-
ized, weighted loss:
min
w?
1
2 ||w?||
2 + C1
?
i:yi=1
l(w?, yi, x?i)
+C2
?
i:yi=?1
l(w?, yi, x?i) (1)
where l() is the learning method?s loss function,
x?i and yi are the features and label for the ith
3Learning with a cost model is generally preferable to
first optimizing error rate and then thresholding the predic-
tion values to select a high-confidence subset (Joachims,
2005), but the latter approach was used successfully for cell
classification in Roark and Hollingshead (2008).
55
not a h ? ? , . ; | CC PRP$ PRP EX
-RRB- -LRB-
no ? ? m EX LS POS PRP$
no m? ? . RP
not a root , DT
no h?m DT?{DT,JJ,NN,NNP,NNS,.}
CD?CD NN?{DT,NNP}
NNP?{DT,NN,NNS}
no m?h {DT,IN,JJ,NN,NNP}?DT
NNP?IN IN?JJ
Table 1: Learned rules for filtering dependency
arcs using PoS tags. The rules filter 25% of pos-
sible arcs while recovering 99.9% of true links.
training example, w? is the learned weight vector,
and C1 and C2 are the class-specific costs. High
precision is obtained when C2 >> C1. For an
SVM, l(w?, yi, x?i) is the standard hinge loss.
We solve the SVM objective using LIBLIN-
EAR (Fan et al, 2008). In our experiments, each
filter is a linear SVM with the typical L1 loss and
L2 regularization.4 We search for the best com-
bination of C1 and C2 using a grid search on de-
velopment data. At test time, an arc is filtered if
w? ? x? > 0.
4.2 Rule-Based Filtering
Our rule-based filters seek to instantly remove
those arcs that are trivially implausible on the ba-
sis of their head and modifier PoS tags. We first
extract labeled examples from gold-standard trees
for whenever a) a word is not a head, b) a word
does not have a head on the left (resp. right), and
c) a pair of words is not linked. We then trained
high-precision SVM classifiers. The only features
in x? are the PoS tag(s) of the head and/or modi-
fier. The learned feature weights identify the tags
and tag-pairs to be filtered. For example, if a tag
has a positive weight in the not-a-head classifier,
all arcs having that node as head are filtered.
The classier selects a small number of high-
4We also tried L1-regularized filters. L1 encourages most
features to have zero weight, leading to more compact and
hence faster models. We found the L1 filters to prune fewer
arcs at a given coverage level, providing less speed-up at
parsing time. Both L1 and L2 models are available in our
publicly available implementation.
precision rules, shown in Table 1. Note that the
rules tend to use common tags with well-defined
roles. By focusing on weighted loss as opposed
to arc frequency, the classifier discovers struc-
tural zeros (Mohri and Roark, 2006), events which
could have been observed, but were not. We
consider this an improvement over the frequency-
based length thresholds employed previously in
tag-specific vine parsing.
4.3 Linear-Time Filtering
In the linear filtering stage, we filter arcs on the
basis of single nodes and their contexts, passing
through the sentences in linear time. For each
node, eight separate classifiers decide whether:
1. It is not a head (i.e., it is a leaf of the tree).
2. Its head is on the left/right.
3. Its head is within 5 nodes on the left/right.
4. Its head is immediately on the left/right.
5. It is the root.
For each of these decisions, we again train high-
precision SVMs with C2 >> C1, and filter di-
rectly based on the classifier output.
If a word is not a head, all arcs with the given
word as head can be pruned. If a word is deemed
to have a head within a certain range on the left
or right, then all arcs that do not obey this con-
straint can be pruned. If a root is found, no other
words should link to the artificial root node. Fur-
thermore, in a projective dependency tree, no arc
will cross the root, i.e., there will be no arcs where
a head and a modifier lie on either side of the root.
We can therefore also filter arcs that violate this
constraint when parsing projectively.
S?gaard and Kuhn (2009) previously proposed
a tagger to further constrain a vine parser. Their
tags are a subset of our decisions (items 4 and 5
above), and have not yet been tested in a state-of-
the-art system.
Development experiments show that if we
could perfectly make decisions 1-5 for each word,
we could remove 91.7% of the total arcs or 95%
of negative arcs, close to the upper bound.
Features
Unlike rule-based filtering, linear filtering uses
a rich set of features (Table 2). Each feature is a
56
PoS-tag features Other features
tagi wordi
tagi, tagi?1 wordi+1
tagi, tagi+1 wordi?1
tagi?1, tagi+1 shapei
tagi?2, tagi?1 prefixi
tagi+1, tagi+2 suffixi
tagj , Left, j=i?5...i?1 i
tagj , Right, j=i+1...i+5 i, n
tagj , (i-j), j=i?5...i?1 n - i
tagj , (i-j), j=i+1...i+5
Table 2: Linear filter features for a node at po-
sition i in a sentence of length n. Each feature
is also conjoined (unless redundant) with wordi,
tagi, shapei, prefixi, and suffixi (both 4 letters).
The shape is the word normalized using the regu-
lar expressions [A-Z]+? A and [a-z]+? a.
binary indicator feature. To increase the speed of
applying eight classifiers, we use the same feature
vector for each of the decisions; learning gives
eight different weight vectors, one corresponding
to each decision function. Feature extraction is
constrained to be O(1) for each node, so that over-
all feature extraction and classification remain a
fast O(n) complexity. Feature extraction would
be O(n2) if, for example, we had a feature for ev-
ery tag on the left or right of a node.
Combining linear decisions
We originally optimized the C1 and C2 param-
eter separately for each linear decision function.
However, we found we could substantially im-
prove the collective performance of the linear fil-
ters by searching for the optimal combination of
the component decisions, testing different levels
of precision for each component. We selected a
few of the best settings for each decision when op-
timized separately, and then searched for the best
combination of these candidates on development
data (testing 12960 combinations in all).
4.4 Quadratic-Time Filtering
In the quadratic filtering stage, a single classifier
decides whether each head-modifier pair should
be filtered. It is trained and applied as described
in Section 4.1.
Binary features
sign(h-m) tagshm
tagm?1, tagshm tagm+1, tagshm
tagh?1, tagshm tagh+1, tagshm
sign(h-m), tagh, wordm
sign(h-m), wordh, tagm
Real features? values
sign(h-m)? h-m
tagh, tagm ? h-m
tagk , tagshm ? Count(tagk ? tagsh...m)
wordk , tagshm ? Count(wordk ? wordsh...m)
Table 3: Quadratic filter features for a head at po-
sition h and a modifier at position m in a sentence
of length n. Here tagshm = (sign(h-m), tagh,
tagm), while tagsh...m and wordsh...m are all the
tags (resp. words) between h and m, but within
?5 positions of h or m.
While theoretically of the same complexity as
the parser?s arc-scoring function (O(n2)), this
process can nevertheless save time by employing
a compact feature set. We view quadratic filter-
ing as a light preprocessing step, using only a por-
tion of the resources that might be used in the final
scoring function.
Features
Quadratic filtering uses both binary and real-
valued features (Table 3). Real-valued features
promote a smaller feature space. For example,
one value can encode distance rather than separate
features for different distances. We also general-
ize the ?between-tag features? used in McDonald
et al (2005) to be the count of each tag between
the head and modifier. The count may be more in-
formative than tag presence alone, particularly for
high-precision filters. We follow Galley and Man-
ning (2009) in using only between-tags within a
fixed range of the head or modifier, so that the ex-
traction for each pair is O(1) and the overall fea-
ture extraction is O(n2).
Using only a subset of the between-tags as fea-
tures has been shown to improve speed but im-
pair parser performance (Galley and Manning,
2009). By filtering quickly first, then scoring all
remaining arcs with a cubic scoring function in the
parser, we hope to get the best of both worlds.
57
5 Filter Experiments
Data
We extract dependency structures from the
Penn Treebank using the Penn2Malt extraction
tool,5 which implements the head rules of Yamada
and Matsumoto (2003). Following convention, we
divide the Treebank into train (sections 2?21), de-
velopment (22) and test sets (23). The develop-
ment and test sets are re-tagged using the Stanford
tagger (Toutanova et al, 2003).
Evaluation Metrics
To measure intrinsic filter quality, we define
Reduction as the proportion of total arcs re-
moved, and Coverage as the proportion of true
head-modifier arcs retained. Our evaluation asks,
for each filter, what Reduction can be obtained at
a given Coverage level? We also give Time: how
long it takes to apply the filters to the test set (ex-
cluding initialization).
We compute an Upper Bound for Reduction on
development data. There are 1.2 million poten-
tial dependency links in those sentences, 96.5%
of which are not present in a gold standard depen-
dency tree. Therefore, the maximum achievable
Reduction is 96.5%.
Systems
We evaluate the following systems:
? Rules: the rule-based filter (Section 4.2)
? Lin.: the linear-time filters (Section 4.3)
? Quad.: the quadratic filter (Section 4.4)
The latter two approaches run on the output of the
previous stage. We compare to the two vine pars-
ing approaches described in Section 3.1:
? Len-Vine uses a hard limit on arc length.
? Tag-Vine (later, Vine) learns a maxi-
mum length for dependency arcs for every
head/modifier tag-combination and order.
5.1 Results
We set each filter?s parameters by selecting
a Coverage-Reduction tradeoff on development
5http://w3.msi.vxu.se/?nivre/research/Penn2Malt.
html
 20
 30
 40
 50
 60
 70
 80
 90
 100
 99.3 99.4 99.5 99.6 99.7 99.8 99.9
R
ed
uc
tio
n 
(%
)
Coverage (%)
Upper Bd
Lin-Orac.
Quad
Lin
Tag-Vine
Len-Vine
Figure 2: Filtering performance for different fil-
ters and cost parameters on development data.
Lin-Orac indicates the percentage filtered using
perfect decisions by the linear components.
Filter Coverage Reduct. Time (s)
Vine 99.62 44.0 2.9s
Rules 99.86 25.8 1.3s
Lin. 99.73 54.2 7.3s
Quad. 99.50 78.4 16.1s
Table 4: Performance (%) of filters on test data.
data (Figure 2). The Lin curve is obtained by vary-
ing both the C1/C2 cost parameters and the combi-
nation of components (plotting the best Reduction
at each Coverage level). We chose the linear fil-
ters with 99.8% Coverage at a 54.2% Reduction.
We apply Quad on this output, varying the cost
parameters to produce its curve. Aside from Len-
Vine, all filters remove a large number of arcs with
little drop in Coverage.
After selecting a desired trade-off for each clas-
sifier, we move to final filtering experiments on
unseen test data (Table 4). The linear filter re-
moves well over half the links but retains an as-
tounding 99.7% of correct arcs. Quad removes
78.4% of arcs at 99.5% Coverage. It thus reduces
the number of links to be scored by a dependency
parser by a factor of five.
The time for filtering the 2416 test sentences
varies from almost instantaneous for Vine and
Rules to around 16 seconds for Quad. Speed num-
bers are highly machine, design, and implemen-
58
Decision Precision Recall
No-Head 99.9 44.8
Right-? 99.9 28.7
Left-? 99.9 39.0
Right-5 99.8 31.5
Left-5 99.9 19.7
Right-1 99.7 6.2
Left-1 99.7 27.3
Root 98.6 25.5
Table 5: Linear Filters: Test-set performance (%)
on decisions for components of the combined 54.2
Reduct./99.73 Coverage linear filter.
Type Coverage Reduct. Oracle
All 99.73 54.2 91.8
All\No-Head 99.76 46.4 87.2
All\Left-? 99.74 53.2 91.4
All\Right-? 99.75 53.6 90.7
All\Left-5 99.74 53.2 89.7
All\Right-5 99.74 51.6 90.4
All\Left-1 99.75 53.5 90.8
All\Right-1 99.73 53.9 90.6
All\Root 99.76 50.2 90.0
Table 6: Contribution of different linear filters to
test set performance (%). Oracle indicates the per-
centage filtered by perfect decisions.
tation dependent, and thus we have stressed the
asymptotic complexity of the filters. However, the
timing numbers show that arc filtering can be done
quite quickly. Section 6 confirms that these are
very reasonable costs in light of the speed-up in
overall parsing.
5.2 Linear Filtering Analysis
It is instructive to further analyze the components
of the linear filter. Table 5 gives the performance
of each classifier on its specific decision. Preci-
sion is the proportion of positive classifications
that are correct. Recall is the proportion of pos-
itive instances that are classified positively (e.g.
the proportion of actual roots that were classified
as roots). The decisions correspond to items 1-5 in
Section 4.3. For example, Right-? is the decision
that a word has no head on the right.
Most notably, the optimum Root decision has
much lower Precision than the others, but this has
little effect on its overall accuracy as a filter (Ta-
ble 6). This is perhaps because the few cases of
false positives are still likely to be main verbs or
auxiliaries, and thus still still likely to have few
links crossing them. Thus many of the filtered
links are still correct.
Table 6 provides the performance of the classi-
fier combination when each linear decision is ex-
cluded. No-Head is the most important compo-
nent in the oracle and the actual combination.
6 Parsing Experiments
6.1 Set-up
In this section, we investigate the impact of our fil-
ters on graph-based dependency parsers. We train
each parser unfiltered, and then measure its speed
and accuracy once filters have been applied. We
use the same training, development and test sets
described in Section 5. We evaluate unlabeled de-
pendency parsing using head accuracy: the per-
centage of words (ignoring punctuation) that are
assigned the correct head.
The filters bypass feature extraction for each fil-
tered arc, and replace its score with an extremely
low negative value. Note that 2nd-order features
consider O(n3) [h,m1,m2] triples. These triples
are filtered if at least one component arc ([h,m1]
or [h,m2]) is filtered.
In an optimal implementation, we might also
have the parser re-use features extracted during
filtering when scoring the remaining arcs. We did
not do this. Instead, filtering was treated as a pre-
processing step, which maximizes the portability
of the filters across parsers. We test on two state-
of-the art parsers:
MST We modified the publicly-available MST
parser (McDonald et al, 2005)6 to employ our fil-
ters before carrying out feature extraction. MST
is trained with 5-best MIRA.
DepPercep We also test an in-house depen-
dency parser, which conducts projective first and
2nd-order searches using the split-head CFG de-
scribed by Johnson (2007), with a weight vec-
tor trained using an averaged perceptron (Collins,
6http://sourceforge.net/projects/mstparser/
59
DepPercep-1 DepPercep-2 MST-1 MST-2
Filter Cost Acc. Time Acc. Time Acc. Time Acc. Time
None +0 91.8 348 92.5 832 91.2 153 91.9 200
Vine +3 91.7 192 92.3 407 91.2 99 91.8 139
Rules +1 91.7 264 92.4 609 91.2 125 91.9 167
Linear +7 91.7 168 92.4 334 91.2 88 91.8 121
Quad. +16 91.7 79 92.3 125 91.2 58 91.8 80
Table 7: The effect of filtering on the speed and accuracy on 1st and 2nd-order dependency parsing.
2002). Its features are a mixture of those de-
scribed by McDonald et al (2005), and those used
in the Koo et al (2008) baseline system; we do not
use word-cluster features.
DepPercep makes some small improvements to
MST?s 1st-order feature set. We carefully de-
termined which feature types should have dis-
tance appended in addition to direction. Also, in-
spired by the reported utility of mixing PoS tags
and word-clusters (Koo et al, 2008), we created
versions of all of the ?Between? and ?Surround-
ing Word? features described by McDonald et al
(2005) where we mix tags and words.7
DepPercep was developed with quadratic filters
in place, which enabled a fast development cycle
for feature engineering. As a result, it does not
implement many of the optimizations in place in
MST, and is relatively slow unfiltered.
6.2 Results
The parsing results are shown in Table 7, where
times are given in seconds, and Cost indicates the
additional cost of filtering. Note that the impact
of all filters on accuracy is negligible, with a de-
crease of at most 0.2%. In general, parsing speed-
ups mirror the amount of arc reduction measured
in our filter analysis (Section 5.1).
Accounting for filter costs, the benefits of
quadratic filtering depend on the parser. The extra
benefit of quadratic over linear is substantial for
DepPercep, but less so for 1st-order MST.
MST shows more modest speed-ups than Dep-
Percep, but MST is already among the fastest
publicly-available data-driven parsers. Under
quadratic filtering, MST-2 goes from processing
7This was enabled by using word features only when the
word is among the 800 most frequent in the training set.
12 sentences per second to 23 sentences.8
DepPercep-2 starts slow, but benefits greatly
from filtering. This is because, unlike MST-2,
it does not optimize feature extraction by fac-
toring its ten 2nd-order features into two triple
([h,m1,m2]) and eight sibling ([m1,m2]) fea-
tures. This suggests that filtering could have a dra-
matic effect on a parser that uses more than a few
triple features, such as Koo et al (2008).
7 Conclusion
We have presented a series of arc filters that speed
up graph-based dependency parsing. By treat-
ing filtering as weighted classification, we learn a
cascade of increasingly complex filters from tree-
annotated data. Linear-time filters prune 54%
of total arcs, while quadratic-time filters prune
78%. Both retain at least 99.5% of true dependen-
cies. By testing two state-of-the-art dependency
parsers, we have shown that our filters produce
substantial speed improvements in even carefully-
optimized parsers, with negligible losses in ac-
curacy. In the future we hope to leverage this
reduced search space to explore features derived
from large corpora.
References
Carreras, Xavier, Michael Collins, and Terry Koo.
2008. TAG, dynamic programming, and the percep-
tron for efficient, feature-rich parsing. In CoNLL.
Carreras, Xavier. 2007. Experiments with a higher-
order projective dependency parser. In EMNLP-
CoNLL.
8This speed accounts for 25 total seconds to apply the
rules, linear, and quadratic filters.
60
Charniak, Eugene, Mark Johnson, Micha Elsner,
Joseph Austerweil, David Ellis, Isaac Haxton,
Catherine Hill, R. Shrivaths, Jeremy Moore,
Michael Pozar, and Theresa Vu. 2006. Multilevel
coarse-to-fine PCFG parsing. In HLT-NAACL.
Collins, Michael. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In EMNLP.
Crammer, Koby and Yoram Singer. 2003. Ultracon-
servative online algorithms for multiclass problems.
JMLR, 3:951?991.
Dreyer, Markus, David A. Smith, and Noah A. Smith.
2006. Vine parsing and minimum risk reranking for
speed and precision. In CoNLL.
Eisner, Jason and Noah A. Smith. 2005. Parsing with
soft and hard constraints on dependency length. In
IWPT.
Eisner, Jason. 1996. Three new probabilistic models
for dependency parsing: An exploration. In COL-
ING.
Fan, Rong-En, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A library for large linear classification. JMLR,
9:1871?1874.
Galley, Michel and Christopher D. Manning. 2009.
Quadratic-time dependency parsing for machine
translation. In ACL-IJCNLP.
Joachims, Thorsten. 2005. A support vector method
for multivariate performance measures. In ICML.
Johnson, Mark. 2007. Transforming projective bilex-
ical dependency grammars into efficiently-parsable
CFGs with unfold-fold. In ACL.
Koo, Terry, Xavier Carreras, and Michael Collins.
2008. Simple semi-supervised dependency parsing.
In ACL-08: HLT.
McDonald, Ryan and Joakim Nivre. 2007. Character-
izing the errors of data-driven dependency parsing
models. In EMNLP-CoNLL.
McDonald, Ryan and Fernando Pereira. 2006. Online
learning of approximate dependency parsing algo-
rithms. In EACL.
McDonald, Ryan, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In ACL.
Me?lc?uk, Igor A. 1987. Dependency syntax: theory
and practice. State University of New York Press.
Mohri, Mehryar and Brian Roark. 2006. Probabilistic
context-free grammar induction based on structural
zeros. In HLT-NAACL.
Petrov, Slav and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In HLT-NAACL.
Poon, Hoifung and Pedro Domingos. 2009. Unsuper-
vised semantic parsing. In EMNLP.
Roark, Brian and Kristy Hollingshead. 2008. Classi-
fying chart cells for quadratic complexity context-
free inference. In COLING.
S?gaard, Anders and Jonas Kuhn. 2009. Using a max-
imum entropy-based tagger to improve a very fast
vine parser. In IWPT.
Toutanova, Kristina, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In NAACL.
Wang, Qin Iris, Colin Cherry, Dan Lizotte, and Dale
Schuurmans. 2006. Improved large margin depen-
dency parsing via local constraints and Laplacian
regularization. In CoNLL.
Wang, Mengqiu, Noah A. Smith, and Teruko Mita-
mura. 2007. What is the Jeopardy model? A quasi-
synchronous grammar for QA. In EMNLP-CoNLL.
Wang, Qin Iris, Dale Schuurmans, and Dekang Lin.
2008. Semi-supervised convex training for depen-
dency parsing. In ACL-08: HLT.
West, D. 2001. Introduction to Graph Theory. Pren-
tice Hall, 2nd edition.
Yamada, Hiroyasu and Yuji Matsumoto. 2003. Statis-
tical dependency analysis with support vector ma-
chines. In IWPT.
61
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 886?894,
Beijing, August 2010
Using Web-scale N-grams to Improve Base NP Parsing Performance
Emily Pitler
Computer and Information Science
University of Pennsylvania
epitler@seas.upenn.edu
Shane Bergsma
Department of Computing Science
University of Alberta
sbergsma@ualberta.ca
Dekang Lin
Google, Inc.
lindek@google.com
Kenneth Church
Human Language Technology Center of Excellence
Johns Hopkins University
kenneth.church@jhu.edu
Abstract
We use web-scale N-grams in a base NP
parser that correctly analyzes 95.4% of the
base NPs in natural text. Web-scale data
improves performance. That is, there is no
data like more data. Performance scales
log-linearly with the number of parame-
ters in the model (the number of unique
N-grams). The web-scale N-grams are
particularly helpful in harder cases, such
as NPs that contain conjunctions.
1 Introduction
Noun phrases (NPs) provide an index to the
world?s information. About 70% of web queries
are NPs (Barr et al, 2008). A robust NP parser
could help search engines improve retrieval per-
formance on multi-word NP queries (Zhai, 1997).
For example, by knowing the correct parse of
?washed (baby carrots),? a search engine could
ensure that returned pages (and advertisements)
concern clean carrots rather than clean babies. NP
structure is also helpful for query expansion and
substitution (Jones et al, 2006).
This paper is concerned with base NP pars-
ing. We are given a base NP string as input,
and the task is to produce a parse tree as output.
Base NPs are NPs that do not contain embedded
noun phrases. These are sometimes called NP
chunks, or core/non-recursive NPs (Church, 1988;
Ramshaw and Marcus, 1995). Correctly parsing
(or, equivalently, bracketing) base NPs is chal-
lenging because the same part-of-speech (POS)
sequence can be parsed differently depending on
the specific words involved. For example, ?retired
(science teacher)? and ?(social science) teacher?
have different structures even though they have
identical POS sequences.
Lexical statistics are therefore needed in order
to parse the above examples, and they must be
computed over a lot of text to avoid sparsity. All
of our lexical statistics are derived from a new
and improved web-scale N-gram corpus (Lin et
al., 2010), which we call Google V2.
Despite the importance of base NPs, most
sentence parsers do not parse base NPs, since
the main training corpus for parsers, the Penn
Treebank (PTB) (Marcus et al, 1994), leaves a
flat structure for base NPs. Recent annotations
by Vadas and Curran (2007a) added NP structure
to the PTB. We use these annotations (described
in Section 3) for our experiments.
NP parsers usually focus on bracketing three-
word noun compounds. Parsing three-word noun
compounds is a fairly artificial task; we show that
sequences of three nouns make up less than 1%
of the three-word-or-longer base NPs in natural
text. As the NP length increases, the number of
possible binary trees (parses) increases with the
Catalan numbers (Church and Patil, 1982). NPs of
length three have just two possible parses (chance
is 50%), while NPs of length six already have
forty-two possible parses (chance is 2%). Long
NPs therefore provide much more opportunity to
improve performance over the baseline. In Table
1 (Section 7), we show the distribution of base NP
length in the PTB. While most NPs are of length
three, NP length has a long tail.
886
The three-word noun compound assumption
also restricts research to the case in which all
words are nouns, while base NPs also contain de-
terminers, possessives, adjectives, and conjunc-
tions. Conjunctions and their scopes are particu-
larly challenging. For example, in the NP, ?French
television and movie producers,? a parser should
conjoin ?(television) and (movie),? as opposed to
?(French television) and (movie),? ?(French tele-
vision) and (movie producers)? or ?(television)
and (movie producers).?
To resolve these issues, we train a classifier
which uses contextual information from the entire
NP and lexical statistics derived from the web-
scale N-gram corpus to predict if a given span
is a constituent. Our parser then uses this clas-
sifier to produce a score for every possible NP-
internal bracketing and creates a chart of bracket-
ing scores. This chart can be used as features in a
full sentence parser or parsed directly with a chart
parser. Our parses are highly accurate, creating a
strong new standard for this task.
Finally, we present experiments that investigate
the effects of N-gram frequency cutoffs and vari-
ous sources of N-gram data. We show an interest-
ing relationship between accuracy and the number
of unique N-gram types in the data.
2 Related Work
2.1 Three-Word Noun Compounds
The most commonly used data for NP parsing is
from Lauer (1995), who extracted 244 three-word
noun compounds from the Grolier encyclopedia.
When there are only three words, this task reduces
to a binary decision:
? Left Branching: * [retired science] teacher
? Right Branching: retired [science teacher]
In Lauer (1995)?s set of noun compounds, two-
thirds are left branching.
The main approach to these three-word noun
compounds has been to compute association
statistics between pairs of words and then choose
the bracketing that corresponds to the more highly
associated pair. The two main models are the
adjacency model (Marcus, 1980; Liberman and
Sproat, 1992; Pustejovsky et al, 1993; Resnik,
1993) and the dependency model (Lauer, 1995).
Under the adjacency model, the bracketing deci-
sion is made by comparing the associations be-
tween words one and two versus words two and
three (i.e. comparing retired science versus sci-
ence teacher). In contrast, the dependency model
compares the associations between one and two
versus one and three (retired science versus retired
teacher). Lauer (1995) compares the two models
and finds the dependency model to be more accu-
rate.
Nakov and Hearst (2005) compute the associ-
ation scores using frequencies, conditional proba-
bilities, ?2, and mutual information, for both pairs
of words and for linguistically-motivated para-
phrases. Lapata and Keller (2005) found that us-
ing web-scale data for associations is better than
using the (smaller) 100M-word British National
Corpus.
2.2 Longer NPs
Focusing on only the three word case misses a
large opportunity for base NP parsing. NPs longer
than three words commonly occur, making up
29% of our test set. In addition, a chance baseline
does exponentially worse as the length of the NP
increases. These longer NPs are therefore a major
opportunity to improve overall base NP parsing.
Since in the general case, NP parsing can no
longer be thought of as a single binary classifica-
tion problem, different strategies are required.
Barker (1998) reduces the task of parsing
longer NPs to making sequential three-word de-
cisions, moving a sliding window along the NP.
The window is first moved from right-to-left, in-
serting right bracketings, and then again from left-
to-right, finalizing left bracketings. While Barker
(1998) assumes that these three-word decisions
can be made in isolation, this is not always valid.1
Vadas and Curran (2007b) employ Barker?s algo-
rithm, but use a supervised classifier to make the
sequential bracketing decisions. Because these
approaches rely on a sequence of binary decisions,
1E.g., although the right-most three words are identical
in 1) ?soap opera stars and television producers,? and 2)
?movie and television producers,? the initial right-bracketing
decision for ?and television producers? should be different
in each.
887
early mistakes can cascade and lead to a chain of
incorrect bracketings.
Our approach differs from previous work in NP
parsing; rather than greedily inserting brackets as
in Barker?s algorithm, we use dynamic program-
ming to find the global maximum-scoring parse.
In addition, unlike previous approaches that have
used local features to make local decisions, we use
the full NP to score each potential bracketing.
A related line of research aims to segment
longer phrases that are queried on Internet search
engines (Bergsma and Wang, 2007; Guo et al,
2008; Tan and Peng, 2008). Bergsma and Wang
(2007) focus on NP queries of length four or
greater. They use supervised learning to make
segmentation decisions, with features derived
from the noun compound bracketing literature.
Evaluating the benefits of parsing NP queries,
rather than simply segmenting them, is a natural
application of our system.
3 Annotated Data
Our training and testing data are derived from re-
cent annotations by Vadas and Curran (2007a).
The original PTB left a flat structure for base noun
phrases. For example, ?retired science teacher,?
would be represented as:
(NP (JJ retired) (NN science) (NN teacher))
Vadas and Curran (2007a) annotated NP-internal
structure by adding annotations whenever there is
a left-bracketing. If no annotations were added,
right-branching is assumed. The inter-annotator
agreement for exactly matching the brackets on an
NP was 98.5%.
This data provides a valuable new resource for
parsing research, but little work has so far made
use of it. Vadas and Curran (2007b) perform
some preliminary experiments on NP bracketing,
but use gold standard part-of-speech and named-
entity annotations as features in their classifier.
Our work establishes a strong and realistic stan-
dard on this data; our results will serve as a basis
for further research on this topic.
4 Unlabeled N-gram Data
All of our N-gram features described in Sec-
tion 6.1 rely on probabilities derived from unla-
beled data. To use the largest amount of data
possible, we exploit web-scale N-gram corpora.
N-gram counts are an efficient way to compress
large amounts of data (such as all the text on the
web) into a manageable size. An N-gram corpus
records how often each unique sequence of words
occurs. Co-occurrence probabilities can be calcu-
lated directly from the N-gram counts. To keep
the size manageable, N-grams that occur with a
frequency below a particular threshold can be fil-
tered.
The corpus we use is Google V2 (Lin et al,
2010): a new N-gram corpus with N-grams of
length 1-5 that we created from the same 1 tril-
lion word snapshot of the web as Google N-grams
Version 1 (Brants and Franz, 2006), but with sev-
eral enhancements. Duplicate sentences are re-
moved, as well as ?sentences? which are probably
noise (indicated by having a large proportion of
non-alphanumeric characters, being very long, or
being very short). Removing duplicate sentences
is especially important because automatically-
generated websites, boilerplate text, and legal dis-
claimers skew the source web data, with sentences
that may have only been authored once occurring
millions of times. We use the suffix array tools
described in Lin et al (2010) to quickly extract
N-gram counts.
5 Base NP Parsing Approach
Our goal is to take a base NP string as input and
produce a parse tree as output. In practice, it
would be most useful if the NP parse could be
integrated into a sentence parser. Previous NP
parsers are difficult to apply in practice.2 Work
in prepositional phrase attachment that assumes
gold-standard knowledge of the competing attach-
ment sites has been criticized as unrealistic (At-
terer and Schu?tze, 2007).
Our system can easily be integrated into full
parsers. Its input can be identified quickly and
reliably and its output is compatible with down-
stream parsers.
2For example, Vadas and Curran (2007b) report results on
NP parsing, but these results include NPs containing preposi-
tional or adverbial phrases (confirmed by personal communi-
cation). Practical application of their system would therefore
require resolving prepositional phrase attachment as a pre-
processing step.
888
Our parser?s input is base NPs, which can be
identified with very high accuracy. Kudo and Mat-
sumoto (2001) report 95.8% NP chunking accu-
racy on PTB data.
Once provided with an NP, our system uses a
supervised classifier to predict the probability of
a particular contiguous subsequence (span) of the
NP being a constituent, given the entire NP as con-
text. This probability can be inserted into the chart
that a standard chart parser would use.
For example, the base NP ?French television
and movie producers? would be decomposed into
nine different classification problems, scoring the
following potential bracketings:
(French television) and movie producers
French (television and) movie producers
(French television and) movie producers ...
French television and (movie producers)
In Section 6, we detail the set of statistical and
structural features used by the classifier.
The output of our classifier can be easily used
as a feature in a full-sentence structured prediction
parser, as in Taskar et al (2004). Alternatively,
our work could be integrated into a full-sentence
parser by using our feature representations di-
rectly in a discriminative CFG parser (Finkel et
al., 2008), or in a parse re-ranker (Ratnaparkhi et
al., 1994; Collins and Koo, 2005; Charniak and
Johnson, 2005).
While our main objective is to use web-scale
lexical statistics to create an accurate classifier for
base NP-internal constituents, we do produce a
parse tree for evaluation purposes. The probabil-
ity of a parse tree is defined as the product of the
probabilities of all the spans (constituents) in the
tree. The most probable tree is computed with the
CYK algorithm.
6 Features
Over the course of development experiments, we
discovered that the more position-specific our fea-
tures were, the more effectively we could parse
NPs. We define a word?s position as its distance
from the right of the full NP, as the semantic head
of NPs is most often the right-most word. Ulti-
mately, we decided to conjoin each feature with
the position of the proposed bracketing. Since
the features for differing proposed bracketings are
now disjoint, this is equivalent to scoring bracket-
ings with different classifiers, with each classifier
chosen according to the bracketing position. We
now outline the feature types that are common,
but weighted differently, in each proposed brack-
eting?s feature set.
6.1 N-gram Features
All of the features described in this section require
estimates of the probability of specific words or
sequences of words. All probabilities are com-
puted using Google V2 (Section 4).
6.1.1 PMI
Recall that the adjacency model for the three-
word task uses the associations of the two pairs of
adjacent words, while the dependency model uses
the associations of the two pairs of attachment
sites for the initial noun. We generalize the ad-
jacency and dependency models by including the
pointwise mutual information (Church and Hanks,
1990) between all pairs of words in the NP:
PMI(x, y) = log p(?x y?)p(?x?)p(?y?) (1)
For NPs of length n, for each proposed bracket-
ing, we include separate features for the PMI be-
tween all
(n
2
)
pairs of words in the NP. For NPs in-
cluding conjunctions, we include additional PMI
features (Section 6.1.2).
Since these features are also tied to the pro-
posed bracketing positions (as explained above),
this allows us to learn relationships between var-
ious associations within the NP and each poten-
tial bracketing. For example, consider a proposed
bracketing from word 4 to word 5. We learn that
a high association of words inside a bracketing
(here, a high association between word 4 and word
5) indicates a bracketing is likely, while a high
association between words that cross a proposed
bracketing (e.g., a high association between word
3 and word 4) indicates the bracketing is unlikely.
The value of these features is the PMI, if it is
defined. If the PMI is undefined, we include one
of two binary features:
p(?x y?) = 0 or p(?x?) ? p(?y?) = 0.
889
We illustrate the PMI features with an example.
In deciding whether (movie producers) is a rea-
sonable bracketing within ?French television and
movie producers,? the classifier weighs features
for all of:
PMI(French, television)
PMI(French, and)
. . .
PMI(television, producers)
PMI(and, producers)
PMI(movie, producers)
6.1.2 Conjunctions
Properly handling NPs containing conjunc-
tions (NP+conj) requires special statistical fea-
tures. For example, television and movie are
commonly conjoined, but the relevant statistics
that suggest placing brackets around the phrase
?television and movie? are not provided by the
above PMI features (i.e., this is not clear from
PMI(television, and), PMI(television, movie), nor
PMI(and, movie)). Rather, we want to know if the
full phrase ?television and movie? is common.
We thus have additional NP+conj features that
consider the PMI association across the word and:
PMIand(x, y) = log
p(?x and y?)
p(?x and?)p(?and y?) (2)
When PMIand between a pair of words is high,
they are likely to be the constituents of a conjunc-
tion.
Let NP=(w1 . . . wi?1, ?and?, wi+1 . . . wn) be
an NP+conj. We include the PMIand features be-
tween wi?1 and all w ? wi+1 . . . wn. In the exam-
ple ?French television and movie producers,? we
would include features PMIand(television, movie)
and PMIand(television, producers).
In essence, we are assuming wi?1 is the head
of one of the items being conjoined, and we score
the likelihood of each of the words to the right
of the and being the head for the other item. In
our running example, the conjunction has narrow
scope, and PMIand(television, movie) is greater
than PMIand(television, producers), indicating to
our classifier that (television and movie) is a good
bracketing. In other examples the conjunction will
join heads that are further apart, as in ((French TV)
and (British radio)) stars, where both of the fol-
lowing hold:
PMIand(TV, radio) > PMIand(TV, British)
PMIand(TV, radio) > PMIand(TV, stars)
6.2 Lexical
We include a binary feature to indicate the pres-
ence of a particular word at each position in the
NP. We learn that, for instance, the word Inc. in
names tends to occur outside of brackets.
6.3 Shape
Previous work on NP bracketing has used gold-
standard named entity tags (Vadas and Curran,
2007b) as features. We did not want to use any
gold-standard features in our experiments, how-
ever NER information is helpful in separating pre-
modifiers from names, i.e. (news reporter) (Wal-
ter Cronkite).
As an expedient way to get both NER informa-
tion and useful information from hyphenated ad-
jectives, abbreviations, and other punctuation, we
normalize each string using the following regular
expressions:
[A-Z]+ ? A [a-z]+ ? a
We use this normalized string as an indicator
feature. E.g. the word ?Saudi-born? will fire the
binary feature ?Aa-a.?
6.4 Position
We also include the position of the proposed
bracketing as a feature. This represents the prior
of a particular bracketing, regardless of the actual
words.
7 Experiments
7.1 Experimental Details
We use Vadas and Curran (2007a)?s annotations
(Section 3) to create training, development and
testing data for base NPs, using standard splits of
the Penn Treebank (Table 1). We consider all non-
trivial base NPs, i.e., those longer than two words.
For training, we expand each NP in our train-
ing set into independent examples corresponding
to all the possible internal NP-bracketings, and
represent these examples as feature vectors (Sec-
tion 5). Each example is positively labeled if it is
890
Data Set Train Dev Test Chance
PTB Section 2-22 24 23
Length=3 41353 1428 2498 50%
Length=4 12067 445 673 20%
Length=5 3930 148 236 7%
Length=6 1272 34 81 2%
Length>6 616 29 34 < 1%
Total NPs 59238 2084 3522
Table 1: Breakdown of the PTB base NPs used in
our experiments. Chance = 1/Catalan(length).
Features All NPs NP+conj NP-conj
All features 95.4 89.7 95.7
-N-grams 94.0 84.0 94.5
-lexical 92.2 87.4 92.5
-shape 94.9 89.7 95.2
-position 95.3 89.7 95.6
Right 72.6 58.3 73.5bracketing
Table 2: Accuracy (%) of base NPs parsing; abla-
tion of different feature classes.
consistent with the gold-standard bracketing, oth-
erwise it is a negative example.
We train using LIBLINEAR, an efficient linear
Support Vector Machine (SVM).3 We use an L2-
loss function, and optimize the regularization pa-
rameter on the development set (reaching an opti-
mum at C=1). We converted the SVM output to
probabilities.4 Perhaps surprisingly, since SVMs
are not probabilistic, performance on the devel-
opment set with these SVM-derived probabilities
was higher than using probabilities from the LIB-
LINEAR logistic regression solver.
At test time, we again expand the NPs and cal-
culate the probability of each constituent, insert-
ing the score into a chart. We run the CYK algo-
rithm to find the most probable parse of the entire
NP according to the chart. Our evaluation metric
is Accuracy: the proportion of times our proposed
parse of the NP exactly matches the gold standard.
8 Results
8.1 Base NPs
Our method improves substantially over the base-
line of assuming a completely right-branching
structure, 95.4% versus 72.6% (Table 2). The ac-
curacy of the constituency classifier itself (before
the CYK parser is used) is 96.1%.
The lexical features are most important, but all
feature classes are somewhat helpful. In particu-
lar, including N-gram PMI features significantly
improves the accuracy, from 94.0% to 95.4%.5
Correctly parsing more than 19 base NPs out of 20
is an exceptional level of accuracy, and provides a
strong new standard on this task. The most com-
parable result is by Vadas and Curran (2007b),
who achieved 93.0% accuracy on a different set of
PTB noun phrases (see footnote 2), but their clas-
sifier used features based on gold-standard part-
of-speech and named-entity information.
Exact match is a tough metric for parsing, and
the difficulty increases as the length of the NP
increases (because there are more decisions to
make correctly). At three word NPs, our accu-
racy is 98.5%; by six word NPs, our accuracy
drops to 79.0% (Figure 1). Our method?s accu-
racy decreases as the length of the NP increases,
but much less rapidly than a right-bracketing or
chance baseline.
8.2 Base NPs with Conjunctions
N-gram PMI features help more on NP+conj than
on those that do not contain conjunctions (NP-
conj) (Table 2). N-gram PMI features are the most
important features for NP+conj, increasing accu-
racy from 84.0% to 89.7%, a 36% relative reduc-
tion in error.
8.3 Effect of Thresholding N-gram data
We now address two important related questions:
1) how does our parser perform as the amount
of unlabeled auxiliary data varies, and 2) what
is the effect of thresholding an N-gram corpus?
The second question is of widespread relevance as
3www.csie.ntu.edu.tw/
?
cjlin/liblinear/
4Following instructions in http://www.csie.ntu.
edu.tw/
?
cjlin/liblinear/FAQ.html
5McNemar?s test, p < 0.05
891
 1
 10
 100
6543
Ac
cu
ra
cy
 (%
)
Length of Noun Compound (words)
Proposed
Right-bracketing
Chance
Figure 1: Accuracy (log scale) over different NP
lengths, of our method, the right-bracketing base-
line, and chance (1/Catalan(length)).
thresholded N-gram corpora are now widely used
in NLP. Without thresholds, web-scale N-gram
data can be unmanageable.
While we cannot lower the threshold after cre-
ating the N-gram corpus, we can raise it, filtering
more N-grams, and then measure the relationship
between threshold and performance.
Threshold Unique N-grams Accuracy
10 4,145,972,000 95.4%
100 391,344,991 95.3%
1,000 39,368,488 95.2%
10,000 3,924,478 94.8%
100,000 386,639 94.8%
1,000,000 37,567 94.4%
10,000,000 3,317 94.0%
Table 3: There is no data like more data. Accuracy
improves with the number of parameters (unique
N-grams).
We repeat the parsing experiments while in-
cluding in our PMI features only N-grams with
a count ?10 (the whole data set), ?100, ?1000,
. . ., ?107. All other features (lexical, shape, posi-
tion) remain unchanged. The N-gram data almost
perfectly exhibits Zipf?s power law: raising the
threshold by a factor of ten decreases the number
of unique N-grams by a factor of ten (Table 3).
The improvement in accuracy scales log-linearly
with the number of unique N-grams. From a prac-
tical standpoint, we see a trade-off between stor-
Corpus # of tokens ? # of types
NEWS 3.2 B 1 3.7 B
Google V1 1,024.9 B 40 3.4 B
Google V2 207.4 B 10 4.1 B
Table 4: N-gram data, with total number of words
(tokens) in the original corpus (in billions, B), fre-
quency threshold used to filter the data, ? , and to-
tal number of unique N-grams (types) remaining
in the data after thresholding.
age and accuracy. There are consistent improve-
ments in accuracy from lowering the threshold
and increasing the amount of auxiliary data. If for
some application it is necessary to reduce storage
by several orders of magnitude, then one can eas-
ily estimate the resulting impact on performance.
We repeat the thresholding experiments using
two other N-gram sources:
NEWS: N-gram data created from a large set
of news articles including the Reuters and Giga-
word (Graff, 2003) corpora, not thresholded.
Google V1: The original web-scale N-gram
corpus (Section 4).
Details of these sources are given in Table 4.
For a given number of unique N-grams, using
any of the three sources does about the same (Fig-
ure 2). It does not matter that the source corpus
for Google V1 is about five times larger than the
source corpus for Google V2, which in turn is
sixty-five times larger than NEWS (Table 4). Ac-
curacies increase linearly with the log of the num-
ber of types in the auxiliary data set.
Google V1 is the one data source for which
the relationship between accuracy and number of
N-grams is not monotonic. After about 100 mil-
lion unique N-grams, performance starts decreas-
ing. This drop shows the need for Google V2.
Since Google V1 contains duplicated web pages
and sentences, mistakes that should be rare can
appear to be quite frequent. Google V2, which
comes from the same snapshot of the web as
Google V1, but has only unique sentences, does
not show this drop.
We regard the results in Figure 2 as a compan-
ion to Banko and Brill (2001)?s work on expo-
nentially increasing the amount of labeled train-
ing data. Here we see that varying the amount of
892
 94
 94.5
 95
 95.5
 96
1e91e81e71e61e51e4
Ac
cu
ra
cy
 (%
)
Number of Unique N-grams
Google V1
Google V2
NEWS
Figure 2: There is no data like more data. Ac-
curacy improves with the number of parameters
(unique N-grams). This trend holds across three
different sources of N-grams.
unlabeled data can cause an equally predictable
improvement in classification performance, with-
out the cost of labeling data.
Suzuki and Isozaki (2008) also found a log-
linear relationship between unlabeled data (up to
a billion words) and performance on three NLP
tasks. We have shown that this trend continues
well beyond Gigaword-sized corpora. Brants et
al. (2007) also found that more unlabeled data (in
the form of input to a language model) leads to
improvements in BLEU scores for machine trans-
lation.
Adding noun phrase parsing to the list of prob-
lems for which there is a ?bigger is better? rela-
tionship between performance and unlabeled data
shows the wide applicability of this principle. As
both the amount of text on the web and the power
of computer architecture continue to grow expo-
nentially, collecting and exploiting web-scale aux-
iliary data in the form of N-gram corpora should
allow us to achieve gains in performance linear in
time, without any human annotation, research, or
engineering effort.
9 Conclusion
We used web-scale N-grams to produce a new
standard in performance of base NP parsing:
95.4%. The web-scale N-grams substantially im-
prove performance, particularly in long NPs that
include conjunctions. There is no data like more
data. Performance improves log-linearly with the
number of parameters (unique N-grams). One can
increase performance with larger models, e.g., in-
creasing the size of the unlabeled corpora, or by
decreasing the frequency threshold. Alternatively,
one can decrease storage costs with smaller mod-
els, e.g., decreasing the size of the unlabeled cor-
pora, or by increasing the frequency threshold. Ei-
ther way, the log-linear relationship between accu-
racy and model size makes it easy to estimate the
trade-off between performance and storage costs.
Acknowledgments
We gratefully acknowledge the Center for Lan-
guage and Speech Processing at Johns Hopkins
University for hosting the workshop at which this
research was conducted.
References
Atterer, M. and H. Schu?tze. 2007. Prepositional
phrase attachment without oracles. Computational
Linguistics, 33(4):469?476.
Banko, M. and E. Brill. 2001. Scaling to very very
large corpora for natural language disambiguation.
In ACL.
Barker, K. 1998. A trainable bracketer for noun mod-
ifiers. In Twelfth Canadian Conference on Artificial
Intelligence (LNAI 1418).
Barr, C., R. Jones, and M. Regelson. 2008. The lin-
guistic structure of English web-search queries. In
EMNLP.
Bergsma, S. and Q.I. Wang. 2007. Learning noun
phrase query segmentation. In EMNLP-CoNLL.
Brants, T. and A. Franz. 2006. The Google Web 1T
5-gram Corpus Version 1.1. LDC2006T13.
Brants, T., A.C. Popat, P. Xu, F.J. Och, and J. Dean.
2007. Large language models in machine transla-
tion. In EMNLP.
Charniak, E. and M. Johnson. 2005. Coarse-to-fine n-
best parsing and MaxEnt discriminative reranking.
In ACL.
Church, K.W. and P. Hanks. 1990. Word associa-
tion norms, mutual information, and lexicography.
Computational Linguistics, 16(1):22?29.
Church, K. and R. Patil. 1982. Coping with syntactic
ambiguity or how to put the block in the box on the
table. Computational Linguistics, 8(3-4):139?149.
893
Church, K.W. 1988. A stochastic parts program and
noun phrase parser for unrestricted text. In ANLP.
Collins, M. and T. Koo. 2005. Discriminative rerank-
ing for natural language parsing. Computational
Linguistics, 31(1):25?70.
Finkel, J.R., A. Kleeman, and C.D. Manning. 2008.
Efficient, feature-based, conditional random field
parsing. In ACL.
Graff, D. 2003. English Gigaword. LDC2003T05.
Guo, J., G. Xu, H. Li, and X. Cheng. 2008. A unified
and discriminative model for query refinement. In
SIGIR.
Jones, R., B. Rey, O. Madani, and W. Greiner. 2006.
Generating query substitutions. In WWW.
Kudo, T. and Y. Matsumoto. 2001. Chunking with
support vector machines. In NAACL.
Lapata, M. and F. Keller. 2005. Web-based models for
natural language processing. ACM Transactions on
Speech and Language Processing, 2(1):1?31.
Lauer, M. 1995. Corpus statistics meet the noun com-
pound: some empirical results. In ACL.
Liberman, M. and R. Sproat. 1992. The stress and
structure of modified noun phrases in English. Lex-
ical matters, pages 131?181.
Lin, D., K. Church, H. Ji, S. Sekine, D. Yarowsky,
S. Bergsma, K. Patil, E. Pitler, R. Lathbury, V. Rao,
K. Dalwani, and S. Narsale. 2010. New tools for
web-scale n-grams. In LREC.
Marcus, M.P., B. Santorini, and M.A. Marcinkiewicz.
1994. Building a Large Annotated Corpus of En-
glish: The Penn Treebank. Computational Linguis-
tics, 19(2):313?330.
Marcus, M.P. 1980. Theory of Syntactic Recogni-
tion for Natural Languages. MIT Press, Cambridge,
MA, USA.
Nakov, P. and M. Hearst. 2005. Search engine statis-
tics beyond the n-gram: Application to noun com-
pound bracketing. In CoNLL.
Pustejovsky, J., P. Anick, and S. Bergler. 1993. Lex-
ical semantic techniques for corpus analysis. Com-
putational Linguistics, 19(2):331?358.
Ramshaw, L.A. and M.P. Marcus. 1995. Text chunk-
ing using transformation-based learning. In 3rd
ACL Workshop on Very Large Corpora.
Ratnaparkhi, A., S. Roukos, and R.T. Ward. 1994.
A maximum entropy model for parsing. In Third
International Conference on Spoken Language Pro-
cessing.
Resnik, P. 1993. Selection and information: a class-
based approach to lexical relationships. Ph.D. the-
sis, University of Pennsylvania.
Suzuki, J. and H. Isozaki. 2008. Semi-supervised se-
quential labeling and segmentation using giga-word
scale unlabeled data. In ACL.
Tan, B. and F. Peng. 2008. Unsupervised query
segmentation using generative language models and
Wikipedia. In WWW.
Taskar, B., D. Klein, M. Collins, D. Koller, and
C. Manning. 2004. Max-margin parsing. In
EMNLP.
Vadas, D. and J.R. Curran. 2007a. Adding noun
phrase structure to the Penn Treebank. In ACL.
Vadas, D. and J.R. Curran. 2007b. Large-scale su-
pervised models for noun phrase bracketing. In PA-
CLING.
Zhai, C. 1997. Fast statistical parsing of noun phrases
for document indexing. In ANLP.
894
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 293?303,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Predicting the Semantic Compositionality of Prefix Verbs
Shane Bergsma, Aditya Bhargava, Hua He, Grzegorz Kondrak
Department of Computing Science
University of Alberta
{bergsma,abhargava,hhe,kondrak}@cs.ualberta.ca
Abstract
In many applications, replacing a complex
word form by its stem can reduce sparsity, re-
vealing connections in the data that would not
otherwise be apparent. In this paper, we focus
on prefix verbs: verbs formed by adding a pre-
fix to an existing verb stem. A prefix verb is
considered compositional if it can be decom-
posed into a semantically equivalent expres-
sion involving its stem. We develop a clas-
sifier to predict compositionality via a range
of lexical and distributional features, includ-
ing novel features derived from web-scale N-
gram data. Results on a new annotated cor-
pus show that prefix verb compositionality can
be predicted with high accuracy. Our system
also performs well when trained and tested on
conventional morphological segmentations of
prefix verbs.
1 Introduction
Many verbs are formed by adding prefixes to exist-
ing verbs. For example, remarry is composed of a
prefix, re-, and a stem, marry. We present an ap-
proach to predicting the compositionality of prefix
verbs. The verb remarry is compositional; it means
to marry again. On the other hand, retire is gener-
ally non-compositional; it rarely means to tire again.
There is a continuum of compositionality in prefix
verbs, as in other complex word forms and multi-
word expressions (Bannard et al, 2003; Creutz and
Lagus, 2005; Fazly et al, 2009; Xu et al, 2009).
We adopt a definition of compositionality specifi-
cally designed to support downstream applications
that might benefit from knowledge of verb stems.
For example, suppose our corpus contains the fol-
lowing sentence: ?Pope Clement VII denied Henry
VIII permission to marry again before a decision
was given in Rome.? A user might submit the ques-
tion, ?Which pope refused Henry VIII permission to
remarry?? If we can determine that the meaning of
remarry could also be provided via the stem marry,
we could add marry to our search terms. This is
known as morphological query expansion (Bilotti et
al., 2004). Here, such an expansion leads to a better
match between question and answer.
Previous work has shown that ?full morpholog-
ical analysis provides at most very modest bene-
fits for retrieval? (Manning et al, 2008). Stem-
ming, lemmatization, and compound-splitting often
increase recall at the expense of precision, but the
results depend on the morphological complexity of
the text?s language (Hollink et al, 2004).
The lack of success in applying morphological
analysis in IR is unsurprising given that most pre-
vious systems are not designed with applications
in mind. For example, the objective of the influ-
ential Linguistica program is ?to produce an out-
put that matches as closely as possible the analy-
sis that would be given by a human morphologist?
(Goldsmith, 2001). Unsupervised systems achieve
this aim by exploiting learning biases such as min-
imum description length for lexicons (Goldsmith,
2001; Creutz and Lagus, 2007) and high entropy
across morpheme boundaries (Keshava and Pitler,
2006). Supervised approaches learn directly from
words annotated by morphologists (Van den Bosch
and Daelemans, 1999; Toutanova and Cherry, 2009),
often using CELEX, a lexical database that includes
293
morphological information (Baayen et al, 1996).
The conventional approach in morphology is to
segment words into separate morphemes even when
the words are not entirely compositional combina-
tions of their parts (Creutz and Lagus, 2005). For
example, while co- is considered a separate mor-
pheme in the verb cooperate, the meaning of coop-
erate is not simply to operate jointly. These forms
are sometimes viewed as perturbations of compo-
sition (de Marken, 1996). In practice, a user may
query, ?Which nations do not cooperate with the In-
ternational Criminal Court?? An expansion of the
query to include operate may have undesirable con-
sequences.
Rather than relying on conventional standards, we
present an algorithm whose objective is to find only
those prefix verbs that exhibit semantic composi-
tionality; i.e., prefix verbs that are fully meaning-
preserving, sums-of-their-parts. We produce a new
corpus, annotated according to this definition. We
use these annotated examples to learn a discrimina-
tive model of semantic compositionality.
Our classifier relies on a variety of features that
exploit the distributional patterns of verbs and stems.
We build on previous work that applies semantics
to morphology (Yarowsky and Wicentowski, 2000;
Schone and Jurafsky, 2001; Baroni et al, 2002), and
also on work that exploits web-scale data for seman-
tic analysis (Turney, 2001; Nakov, 2007; Kummer-
feld and Curran, 2008). For example, we measure
how often a prefix verb appears with a hyphen be-
tween the prefix and stem. We also look at the dis-
tribution of the stem as a separate word: we calculate
the probability of the prefix verb and the separated
stem?s co-occurrence in a segment of discourse; we
also calculate the distributional similarity between
the verb and the separated stem. High scores for
these measures indicate compositionality. We ex-
tract counts from a web-scale N-gram corpus, allow-
ing us to efficiently leverage huge volumes of unla-
beled text.
Our system achieves 93.6% accuracy on held-out
data, well above several baselines and comparison
systems. We also train and test our system on con-
ventional morphological segmentations. Our clas-
sifier remains reliable in this setting, making half
as many errors as the state-of-the-art unsupervised
Morfessor system (Creutz and Lagus, 2007).
2 Problem Definition and Setting
A prefix verb is a derived word with a bound mor-
pheme as prefix. While derivation can change both
the meaning and part-of-speech of a word (as op-
posed to inflection, which does not change ?referen-
tial or cognitive meaning? (Katamba, 1993)), here
the derived form remains a verb.
We define prefix-verb compositionality as a se-
mantic equivalence between a verb and a paraphrase
involving the verb?s stem. The stem must be used as
a verb in the paraphrase. Words can be introduced,
if needed, to account for the meaning contributed by
the prefix, e.g., outbuild?build more/better/faster
than. A bidirectional entailment between the prefix
verb and the paraphrase is required.
Words can have different meanings in different
contexts. For example, a nation might ?resort to
force,? (non-compositional) while a computer pro-
gram can ?resort a linked list? (compositional). We
therefore define prefix-verb compositionality as a
context-specific property of verb tokens rather than
a global property of verb types. However, it is worth
noting that we ultimately found the compositionality
of types to be very consistent across contexts (Sec-
tion 5.1.2), and we were unable to leverage contex-
tual information to improve classification accuracy;
our final system is essentially type-based. Other re-
cent morphological analyzers have also been type-
based (Keshava and Pitler, 2006; Poon et al, 2009).
Our system takes as input a verb token in unin-
flected form along with its sentence as context. The
verb must be divisible into an initial string and a fol-
lowing remainder such that the initial string is on
our list of prefixes and the remainder is on our list of
stems. Hyphenation is allowed, e.g., both re-enter
and reenter are acceptable inputs. The system deter-
mines whether the prefix/stem combination is com-
positional in the current context. For example, the
verb unionize in, ?The workers must unionize,? can
be divided into a prefix un- and a stem ionize. The
system should determine that here unionize is not a
compositional combination of these parts.
The algorithm requires a list of prefixes and stems
in a given language. For our experiments, we use
both dictionary and corpus-based methods to con-
struct these lists (Section 4).
294
3 Supervised Compositionality Detection
We use a variety of lexical and statistical informa-
tion when deciding whether a prefix verb is compo-
sitional. We adopt a discriminative approach. We
assume some labeled examples are available to train
a classifier. Relevant information is encoded in a
feature vector, and a learning algorithm determines
a set of weights for the features using the training
data. As compositionality is a binary decision, we
can adopt any standard package for binary classifi-
cation. In our experiments we use support vector
machines.
Our features include both local information that
depends only on the verb string (sometimes referred
to as lexical features) and also global information
that depends on the verb and the stem?s distribution
in text. Our approach can therefore be regarded as a
simple form of semi-supervised learning; we lever-
age both a small number of labeled examples and a
large volume of unlabeled text.
If a frequency or similarity is undefined in our cor-
pus, we indicate this with a separate feature; weights
on these features act as a kind of smoothing.
3.1 Features based on Web-Scale N-gram Data
We use web-scale N-gram data to extract distribu-
tional features. The most widely-used N-gram cor-
pus is the Google 5-gram Corpus (Brants and Franz,
2006). We use Google V2: a new N-gram corpus
(also with N-grams of length one-to-five) created
from the same one-trillion-word snapshot of the web
as the Google 5-gram Corpus, but with enhanced fil-
tering and processing of the source text (Lin et al,
2010). For Google V2, the source text was also part-
of-speech tagged, and the resulting part-of-speech
tag distribution is included for each N-gram. There
are 4.1 billion N-grams in the corpus.
The part-of-speech tag distributions are particu-
larly useful, as they allow us to collect verb-specific
counts. For example, while a string like reuse oc-
curs 1.1 million times in the web corpus, it is only
tagged as a verb 270 thousand times. Conflating the
noun/verb senses can lead to misleading scores for
certain features. E.g., the hyphenation frequency
of re-use would appear relatively low, even though
reuse is semantically compositional.
Lin et al (2010) also provide a high-coverage,
10-million-phrase set of clusters extracted from the
N-grams; we use these for our similarity features
(Section 3.1.3). There are 1000 clusters in total.
The data does not provide the context vectors for
each phrase; rather, each phrase is listed with its 20
most similar clusters, measured by cosine similar-
ity with the cluster centroid. We use these centroid
similarities as values in a 1000-dimensional cluster-
membership feature space. To calculate the similar-
ity between two verbs, we calculate the cosine simi-
larity between their cluster-membership vectors.
The feature classes in the following four subsec-
tions each make use of web-scale N-gram data.
3.1.1 HYPH features
Hyphenated verbs are usually compositional (e.g.,
re-elect). Of course, a particular instance of a com-
positional verb may or may not occur in hyphenated
form. However, across a large corpus, compositional
prefix verbs tend to occur in a hyphenated form more
often than do non-compositional prefix verbs. We
therefore provide real-valued features for how often
the verb was hyphenated and unhyphenated on the
web. For example, we collect counts for the fre-
quencies of re-elect (33K) and reelect (9K) in our
web corpus, and we convert the frequencies to log-
counts. We also give real-valued features for the hy-
phenated/unhyphenated log-counts using only those
occurrences of the verb that were tagged as a verb,
exploiting the tag distributions in our web corpus as
described above.
Nakov and Hearst (2005) previously used hy-
phenation counts as an indication of a syntactic re-
lationship between nouns. In contrast, we leverage
hyphenation counts as an indication of a semantic
property of verbs.
3.1.2 COOC features
COOC features, and also the SIM (Section 3.1.3)
and YAH (Section 3.2.2) features, concern the asso-
ciation in text between the prefix verb and its stem,
where the stem occurs as a separate word. We call
this the separated stem.
If a prefix verb is compositional, it is more likely
to occur near its separated stem in text. We often
see agree and disagree, read and reread, etc. occur-
ring in the same segment of discourse. We create
features for the association of the prefix verb and its
295
separated stem in a discourse. We include the log-
count of how often the verb and stem occur in the
same N-gram (of length 2-to-5) in our N-gram cor-
pus. Note that the 2-to-4-gram counts are not strictly
a subset of the 5-gram counts, since fewer 5-grams
pass the data?s minimum frequency threshold.
We also include a real-valued pointwise mutual
information (PMI) feature for the verb and separated
stem?s co-occurrence in an N-gram. For the PMI, we
regard occurrence in an N-gram as an event, and cal-
culate the probability that a verb and separated stem
jointly occur in an N-gram, divided by the probabil-
ity of their occurring in an N-gram independently.
3.1.3 SIM features
If a prefix verb is compositional, it should oc-
cur in similar contexts to its stem. The idea that
a stem and stem+affix should be semantically sim-
ilar has been exploited previously for morphological
analysis (Schone and Jurafsky, 2000). We include
a real-valued feature for the distributional similar-
ity of the verb and stem using Lin?s thesaurus (Lin,
1998). The coverage of this measure was low: it
was non-zero for only 93 of the 1000 prefix verbs in
our training set. We therefore also include distribu-
tional similarity calculated using the web-scale 10-
million-phrase clustering as described above. Us-
ing this data, similarity is defined for 615 of the
1000 training verbs. We also explored a variety of
WordNet-based similarity measures, but these ulti-
mately did not prove helpful on development data.
3.1.4 FRQ features
We include real-valued features for the raw fre-
quencies of the verb and the stem on the web. If
these frequencies are widely different, it may in-
dicate a non-compositional usage. Yarowsky and
Wicentowski (2000) use similar statistics to iden-
tify words related by inflection, but they gather their
counts from a much smaller corpus. In addition,
higher-frequency prefix verbs may be a priori more
likely to be non-compositional. A certain frequency
is required for an irregular usage to become famil-
iar to language speakers. The potential correlation
between frequency and non-compositionality could
thus also be exploited by the classifier via the FRQ
features.
3.2 Other Features
3.2.1 LEX features
We provide lexical features for various aspects
of a prefix verb. Binary features indicate the oc-
currence of particular verbs, prefixes, and stems,
and whether the prefix verb is hyphenated. While
hyphenated prefix verbs are usually compositional,
even non-compositional prefix verbs may be hy-
phenated if the prefix and stem terminate and be-
gin with a vowel, respectively. For example, non-
compositional uses of co-operate are often hyphen-
ated, whereas the compositional remarry is rarely
hyphenated. We therefore have indicator features
for the conjunction of the prefix and the first letter
of the stem (e.g., co-o), and also for the prefix con-
joined with a flag indicating whether the stem begins
with a vowel (e.g., co+vowel).
3.2.2 YAH features
While the COOC features capture many cases
where the verb and separated stem occur in close
proximity (especially, but not limited to, conjunc-
tions), there are many other cases where a longer
distance might separate a compositional verb and
its separated stem. For example, consider the sen-
tence, ?Brush the varnish on, but do not overbrush.?
Here, the verb and separated stem do not co-occur
within a 5-gram window, and their co-occurrence
will therefore not be recorded in our N-gram cor-
pus. As an approximation for co-occurrence counts
within a longer segment of discourse, we count the
number of pages on the web where the verb and sep-
arated stem co-occur. We use hit-counts returned
by the Yahoo search engine API.1 Similar to our
COOC features, we include a real-valued feature for
the pointwise mutual information of the prefix verb
and separated stem?s co-occurrence on a web page,
i.e., we use Turney?s PMI-IR (Turney, 2001).
Baroni et al (2002) use similar statistics to help
discover morphologically-related words. In contrast
to our features, however, their counts are derived
from source text that is several orders of magnitude
smaller in size.
1http://developer.yahoo.com/search/boss/
296
3.2.3 DIC features
One potentially useful resource, when available,
is a dictionary of the conventional morphological
segmentations of words in the language. Although
these segmentations have been created for a differ-
ent objective than that of our annotations, we hy-
pothesize that knowledge of morphology can help
inform our system?s predictions. For each prefix
verb, we include features for whether or not the pre-
fix and stem are conventionally segmented into sep-
arate morphemes, according to a morphological dic-
tionary. Similar to the count-based features, we in-
clude a DIC-undefined feature for the verbs that are
not in the dictionary; any precompiled dictionary
will have imperfect coverage of actual test examples.
Interestingly, DIC features are found to be among
our least useful features in the final evaluation.
4 Experiments
4.1 Resources
We use CELEX (Baayen et al, 1996) as our dictio-
nary for the DIC features. We also use CELEX to help
extract our lists of prefixes and stems. We take ev-
ery prefix that is marked in CELEX as forming a new
verb by attaching to an existing verb. For stems, we
use every verb that occurs in CELEX, but we also
extend this list by automatically collecting a large
number of words that were automatically tagged as
verbs in the NYT section of Gigaword (Graff, 2003).
To be included in the extra-verb list, a verb must oc-
cur more than ten times and be tagged as a verb more
than 70% of the time by a part-of-speech tagger. We
thereby obtain 43 prefixes and 6613 stems.2 We
aimed for an automatic, high-precision list for our
initial experiments. This procedure is also amenable
to human intervention; one could alternatively cast a
wider net for possible stems and then manually filter
false positives.
4.2 Annotated Data
We carried out a medium-scale annotation to provide
training and evaluation data for our experiments.3
2The 43 prefixes are: a- ab- ac- ad- as- be- circum- co- col-
com- con- cor- counter- cross- de- dis- e- em- en- ex- fore- im-
in- inter- ir- mis- out- over- per- photo- post- pre- pro- psycho-
re- sub- super- sur- tele- trans- un- under- with-
3Our annotated data is publicly available at:
http://www.cs.ualberta.ca/?ab31/verbcomp/
The data for our annotations also comes from the
NYT section of Gigaword. We first build a list of
possible prefix verbs. We include any verb that a) is
composed of a valid prefix and stem; and b) occurs
at least twice in the corpus.4 If the verb occurs less
than 50 times in the corpus, we also require that it
was tagged as a verb in at least 70% of cases. This
results in 2077 possible prefix verbs for annotation.
For each verb type in our list of possible prefix
verbs, we randomly select for annotation sentences
from Gigaword containing the verb. We take at most
three sentences for each verb type so that a few very
common types (such as become, understand, and im-
prove) do not comprise the majority of annotated ex-
amples. The resulting set of sentences includes a
small number of sentences with incorrectly-tagged
non-verbs; these are simply marked as non-verbs
by our annotators and excluded from our final data
sets. A graphical program was created for the an-
notation; the program automatically links to the on-
line Merriam-Webster dictionary entries for the pre-
fix verb and separated stem. When in doubt about
a verb?s meaning, our annotators adhere to the dic-
tionary definitions. A single annotator labeled 1718
examples, indicating for each sentence whether the
prefix verb was compositional. A second annota-
tor then labeled a random subset of 150 of these ex-
amples, and agreement was calculated. The annota-
tors agreed on 137 of the 150 examples. The Kappa
statistic (Jurafsky and Martin, 2000, page 315), with
P(E) computed from the confusion matrices, is 0.82,
above the 0.80 level considered to indicate good re-
liability.
For our experiments, the 1718 annotated exam-
ples are randomly divided into 1000 training, 359
development, and 359 held-out test examples.
4.3 Classifier Settings
We train a linear support vector machine classifier
using the efficient LIBLINEAR package (Fan et al,
2008). We use L2-loss and L2-regularization. We
4We found that the majority of single-occurrence verbs in
the Gigaword data were typos. We would expect true hapax
legomena to be largely compositional, and we could potentially
derive better statistics if we include them (Baayen and Sproat,
1996). One possible option, employed in previous work, is to
ensure words of interest are ?manually corrected for typing er-
rors before further analysis? (Baayen and Renouf, 1996).
297
optimize the choice of features and regularization
hyperparameter on development data, attaining a
maximum when C = 0.1.
4.4 Evaluation
We compare the following systems:
1. Base1: always choose compositional (the ma-
jority class).
2. Base2: for each prefix, choose the majority
class over the verbs having that prefix in train-
ing data.
3. Morf: the unsupervised Morfessor sys-
tem (Creutz and Lagus, 2007) (Categories-
ML, from 110K-word corpus). If Morfessor
splits the prefix and stem into separate mor-
phemes, we take the prediction as composi-
tional. If it does anything else, we take it as
non-compositional.
4. SCD: Supervised Compositionality Detection:
the system proposed in this paper.
We evaluate using accuracy: the percentage of ex-
amples classified correctly in held-out test data.
5 Results
We first analyze our annotations, gaining insight into
the relation between our definition and conventional
segmentations. We also note the consistency of our
annotations across contexts. We then provide the
main results of our system. Finally, we provide the
results of our system when trained and tested on con-
ventional morphological segmentations.
5.1 Analysis of Annotations
5.1.1 Annotation consistency with dictionaries
The majority of our examples are not present in
a morphological dictionary, even in one as compre-
hensive as CELEX. The prefix verbs are in CELEX
for only 670 of the 1718 total annotated instances.
For those that are in CELEX, Table 1 provides
the confusion matrix that relates the CELEX seg-
mentations to our annotations. The table shows
that the major difference between our annotations
and CELEX is that our definition of compositionality
is more strict than conventional morphological seg-
mentations. When CELEX does not segment the pre-
fix from the stem (case 0), our annotations agree in
CELEX segmentation
1 0
Compositionality 1 227 10
annotation 0 250 183
Table 1: Confusion matrix on the subset of prefix verb
annotations that are also in CELEX. 1 indicates that the
prefix and stem are segmented into separate morphemes,
0 indicates otherwise.
183 of 193 cases. When CELEX does split the prefix
from the stem (case 1), the meaning is semantically
compositional in less than half the cases. This is
a key difference between conventional morphology
and our semantic definition.
It is also instructive to analyze the 10 cases that
are semantically compositional but which CELEX
did not segment. Most of these are verbs that are
conventionally viewed as single morphemes because
they entered English as complete words. For exam-
ple, await comes from the Old North French await-
ier, itself from waitier. In practice, it is useful to
know that await is compositional, i.e. that it can be
rephrased as wait for. Downstream applications can
exploit the compositionality of await, but miss the
opportunity if using the conventional lack of seg-
mentation.
5.1.2 Annotation consistency across contexts
We next analyze our annotated data to determine
the consistency of compositionality across different
occurrences of the same prefix-verb type. There are
1248 unique prefix verbs in our 1718 labeled exam-
ples: 45 verbs occur three times, 380 occur twice
and 823 occur only once. Of the 425 verbs that oc-
cur multiple times, only 6 had different annotations
in different examples (i.e., six verbs occur in both
compositional and non-compositional usages in our
dataset). These six instances are subtle, debatable,
and largely uninteresting, depending on distinctions
like whether the proclaim sense of blazon can sub-
stitute for the celebrate sense of emblazon, etc.
It is easy to find clearer ambiguities online,
such as compositional examples of typically non-
compositional verbs (how to recover a couch, when
to redress a wound, etc.). However, in our data verbs
like recover and redress always occur in their more
dominant non-compositional sense. People may
298
Set # Base1 Base2 Morf SCD
Test 359 65.7 87.2 73.8 93.6
? CELEX 128 30.5 73.4 50.8 89.8
/? CELEX 231 85.3 94.8 86.6 95.7
? train 107 69.2 93.5 74.8 97.2
/? train 252 64.3 84.5 73.4 92.1
Table 2: Number of examples (#) and accuracy (%) on
test data, and on in-CELEX vs. not-in-CELEX, and in-
training-data vs. not-in-training splits.
consciously or unconsciously recognize the possi-
bility for confusion and systematically hyphenate
prefixes from the stem if a less-common composi-
tional usage is employed. For example, our data has
?repress your feelings? for the non-compositional
case but the hyphenated ?re-press the center? for the
compositional usage.5
Due to the consistency of compositionality across
contexts, context-based features may simply not be
very useful for classification. All the features we de-
scribe in Section 3 depend only on the prefix verb
itself and not the verb context. Various context-
dependent features did not improve accuracy on our
development data and were thus excluded from the
final system.
5.2 Main Results
The first row of Table 2 gives the results of all
systems on test data. SCD achieves 93.6% ac-
curacy, making one fifth as many errors as the
majority-class baseline (Base1) and half as many er-
rors as the more competitive prefix-based predictor
(Base2). The substantial difference between SCD
and Base2 shows that SCD is exploiting much infor-
mation beyond the trivial memorization of a deci-
sion for each prefix. Morfessor performs better than
Base1 but significantly worse than Base2. This indi-
cates that state-of-the-art unsupervised morpholog-
ical segmentation is not yet practical for semantic
preprocessing. Of course, Morfessor was also de-
signed with a different objective; in Section 5.3 we
compare Morfessor and SCD on conventional mor-
5Note that many examples like recover, repress and redress
are only ambiguous in text, not in speech. Pronunciation re-
duces ambiguity in the same way that hyphens do in text. Con-
versely, observe that knowledge of compositionality could po-
tentially help speech synthesis.
Prefix # Tot # Comp SCD
re- 166 147 95.8
over- 26 25 96.2
out- 23 18 91.3
de- 21 0 100.0
pre- 19 16 94.7
un- 17 1 94.1
dis- 10 0 90.0
under- 9 7 77.8
co- 7 6 100.0
en- 5 2 60.0
Table 3: Total number of examples (# Tot), number of
examples that are compositional (# Comp), and accuracy
(%) of SCD on test data, by prefix.
phological segmentations.
We further analyzed the systems by splitting the
test data two ways.
First, we separate verbs that occur in our mor-
phological dictionary (? CELEX) from those that
do not (/? CELEX). Despite using the dictionary
segmentation itself as a feature, the performance
of SCD is worse on the ? CELEX verbs (89.8%).
The comparison systems drop even more dramati-
cally on this subset. The ? CELEX verbs comprise
the more frequent, irregular verbs in English. Non-
compositionality is the majority class on the exam-
ples that are in the dictionary.
On the other hand, one would expect verbs that
are not in a comprehensive dictionary to be largely
compositional, and indeed most of the /? CELEX
verbs are compositional. However, there is still
much to be gained from applying SCD, which makes
a third as many errors as the system which always
assigns compositional (95.7% for SCD vs. 85.3%
for Base1).
Our second way of splitting the data is to divide
our test set into prefix verbs that also occurred in
training sentences (? train) and those that did not (/?
train). Over 70% did not occur in training. SCD
scores 97.2% accuracy on those that did. The clas-
sifier is thus able to exploit the consistency of anno-
tations across different contexts (Section 5.1.2). The
92.1% accuracy on the /?-train portion also shows
the features allow the system to generalize well to
new, previously-unseen verbs.
Table 3 gives the results of our system on sets of
299
-LEX -HYPH -COOC -SIM -YAH -FRQ -DIC
85.0 92.8 92.5 93.6 93.6 93.6 93.6
85.5 93.6 92.8 93.0 93.3 93.9
86.9 90.5 93.3 93.6 93.6
84.1 90.3 93.3 93.6
87.5 90.5 93.0
85.5 89.4
Table 4: Accuracy (%) of SCD as different feature classes
are removed. Performance with all features is 93.6%.
verbs divided according to their prefix. The table in-
cludes those prefixes that occurred at least 5 times
in the test set. Note that the prefixes have a long
tail: these ten prefixes cover only 303 of the 359
test examples. Accuracy is fairly high across all the
different prefixes. Note also that the three prefixes
de-, un-, and dis- almost always correspond to non-
compositional verbs. Each of these prefixes corre-
sponds to a subtle form of negation, and it is usually
difficult to paraphrase the negation using the stem.
For example, to demilitarize does not mean to not
militarize (or any other simple re-phrasing using the
stem as a verb), and so our annotation marks it as
non-compositional. Whether such a strict strategy is
ultimately best may depend on the target application.
Feature Analysis
We perform experiments to evaluate which features
are most useful for this task. Table 4 gives the ac-
curacy of our system as different feature classes are
removed. A similar table was previously used for
feature analysis in Daume? III and Marcu (2005).
Each row corresponds to performance with a group
of features; each entry is performance with a par-
ticular feature class individually removed the group.
We remove the least helpful feature class from each
group in succession moving group-to-group down
the rows.
We first remove the DIC features. These do not
impact performance on test data. The last row gives
the performance with only HYPH features (85.5, re-
moving LEX), and only LEX features (89.4, remov-
ing HYPH). These are found to be the two most ef-
fective features for this task, followed by the COOC
statistics. The other features, while marginally help-
ful on development data, are relatively ineffective on
the test set. In all cases, removing LEX features hurts
Base1 Base2 Morf SCD
76.0 79.6 72.4 86.4
Table 5: Accuracy (%) on CELEX.
the most. Removing LEX not only removes useful
stem, prefix, and hyphen information, but it also im-
pairs the ability of the classifier to use the other fea-
tures to separate the examples.
5.3 CELEX Experiments and Results
Finally, we train and test our system on prefix verbs
where the segmentation decisions are provided by
a morphological dictionary. We are interested in
whether the strong results of our system could trans-
fer to conventional morphological segmentations.
We extract all verbs in CELEX that are valid verbs
for our system (divisible into a prefix and verb stem),
and take the CELEX segmentation as the label; i.e.,
whether the prefix and stem are separated into dis-
tinct morphemes. We extract 1006 total verbs.
We take 506 verbs for training, 250 verbs as a
development set (to tune our classifier?s regulariza-
tion parameter) and 250 verbs as a final held-out test
set. We use the same features and classifier as in
our main results, except we remove the DIC features
which are now the instance labels.
Table 5 shows the performance of our two base-
line systems along with Morfessor and SCD. While
the majority-class baseline is much higher, the
prefix-based baseline is 7% lower, indicating that
knowledge of prefixes, and lexical features in gen-
eral, are less helpful for conventional segmentations.
In fact, performance only drops 2% when we re-
move the LEX features, showing that web-scale in-
formation alone can enable solid performance on
this task. Surprisingly, Morfessor performs worse
here, below both baselines and substantially below
the supervised system. We confirmed our Morfessor
program was generating the same segmentations as
the online demo. We also experimented with Lin-
guistica (Goldsmith, 2001), training on a large cor-
pus, but results were worse than with Morfessor.
Accurate segmentation of prefix verbs is clearly
part of the mandate of these systems; prefix verb
segmentation is simply a very challenging task. Un-
like other, less-ambiguous tasks in morphology, a
prefix/stem segmentation is plausible for all of our
300
input verbs, since the putative morphemes are by
definition valid morphemes in the language.
Overall, the results confirm and extend previous
studies that show semantic information is helpful in
morphology (Schone and Jurafsky, 2000; Yarowsky
and Wicentowski, 2000). However, we reiterate that
optimizing systems according to conventional mor-
phology may not be optimal for downstream ap-
plications. Furthermore, accuracy is substantially
lower in this setting than in our main results. Target-
ing conventional segmentations may be both more
challenging and less useful than focusing on seman-
tic compositionality.
6 Related Work
There is a large body of work on morphological
analysis of English, but most of this work does not
handle prefixes. Porter?s stemmer is a well-known
suffix-stripping algorithm (Porter, 1980), while
publicly-available lemmatizers like morpha (Min-
nen et al, 2001) and PC-KIMMO (Karp et al, 1992)
only process inflectional morphology. FreeLing (At-
serias et al, 2006) comes with a few simple rules
for deterministically stripping prefixes in some lan-
guages, but not English (e.g., only semi- and re- can
be stripped when analyzing OOV Spanish verbs).
A number of modern morphological analyzers use
supervised machine learning. These systems could
all potentially benefit from the novel distributional
features used in our model. Van den Bosch and
Daelemans (1999) use memory-based learning to
analyze Dutch. Wicentowski (2004)?s supervised
WordFrame model includes a prefixation compo-
nent. Results are presented on over 30 languages.
Erjavec and Dz?eroski (2004) present a supervised
lemmatizer for Slovene. Dreyer et al (2008) per-
form supervised lemmatization on Basque, English,
Irish and Tagalog; like us they include results when
the set of lemmas is given. Toutanova and Cherry
(2009) present a discriminative lemmatizer for En-
glish, Bulgarian, Czech and Slovene, but only han-
dle suffix morphology. Poon et al (2009) present an
unsupervised segmenter, but one that is based on a
log-linear model that can include arbitrary and in-
terdependent features of the type proposed in our
work. We see potential in combining the best el-
ements of both approaches to obtain a system that
does not need annotated training data, but can make
use of powerful web-scale features.
Our approach follows previous systems for mor-
phological analysis that leverage semantic as well
as orthographic information (Yarowsky and Wicen-
towski, 2000; Schone and Jurafsky, 2001; Baroni et
al., 2002). Similar problems also arise in core se-
mantics, such as how to detect the compositionality
of multi-word expressions (Lin, 1999; Baldwin et
al., 2003; Fazly et al, 2009). Our problem is sim-
ilar to the analysis of verb-particle constructions or
VPCs (e.g., round up, sell off, etc.) (Bannard et al,
2003). Web-scale data can be used for a variety of
problems in semantics (Lin et al, 2010), including
classifying VPCs (Kummerfeld and Curran, 2008).
We motivated our work by describing applications
in information retrieval, and here Google is clearly
the elephant in the room. It is widely reported that
Google has been using stemming since 2003; for ex-
ample, a search today for Porter stemming returns
pages describing the Porter stemmer, and the re-
turned snippets have words like stemming, stem-
mer, and stem in bold text. Google can of course
develop high-quality lists of morphological variants
by paying attention to how users reformulate their
queries. User query sessions have previously been
used to expand queries using similar terms, such as
substituting feline for cat (Jones et al, 2006). We
show that high-quality, IR-friendly stemming is pos-
sible even without query data. Furthermore, query
data could be combined with our other features for
highly discriminative word stemming in context.
Beyond information retrieval, suffix-based stem-
ming and lemmatization have been used in a range
of NLP applications, including text categorization,
textual entailment, and statistical machine transla-
tion. We believe accurate prefix-stripping can also
have an impact in these areas.
7 Conclusions and Future Work
We presented a system for predicting the semantic
compositionality of prefix verbs. We proposed a
new, well-defined and practical definition of compo-
sitionality, and we annotated a corpus of sentences
according to this definition. We trained a discrimina-
tive model to predict compositionality using a range
of lexical and web-scale statistical features. Novel
301
features include measures of the frequency of prefix-
stem hyphenation, and statistics for the likelihood of
the verb and stem co-occurring as separate words in
an N-gram. The classifier is highly accurate across a
range of prefixes, correctly predicting composition-
ality for 93.6% of examples.
Our preliminary results provide strong motiva-
tion for investigating and applying new distribu-
tional features in the prediction of both conventional
morphology and in task-directed semantic composi-
tionality. Our techniques could be used on a variety
of other complex word forms. In particular, many
of our features extend naturally to identifying stem-
stem compounds (like panfry or healthcare). Also, it
would be possible for our system to handle inflected
forms by first converting them to their lemmas us-
ing a morphological analyzer. We could also jointly
learn the compositionality of words across their in-
flections, along the lines of Yarowsky and Wicen-
towski (2000).
There are also other N-gram-derived features that
warrant further investigation. One source of in-
formation that has not previously been exploited is
the ?lexical fixedness? (Fazly et al, 2009) of non-
compositional prefix verbs. If prefix verbs are rarely
rephrased in another form, they are likely to be non-
compositional. For example, in our N-gram data,
the count of quest again is relatively low compared
to the count of request, indicating request is non-
compositional. On the other hand, marry again is
relatively frequent, indicating that remarry is com-
positional. Incorporation of these and other N-gram
counts could further improve classification accuracy.
References
Jordi Atserias, Bernardino Casas, Elisabet Comelles,
Meritxell Gonza?lez, Llu??s Padro?, and Muntsa Padro?.
2006. FreeLing 1.3: Syntactic and semantic services
in an open-source NLP library. In LREC.
R. Harald Baayen and Antoinette Renouf. 1996. Chron-
icling the Times: Productive lexical innovations in an
English newspaper. Language, 72(1).
Harald Baayen and Richard Sproat. 1996. Estimating
lexical priors for low-frequency morphologically am-
biguous forms. Comput. Linguist., 22(2):155?166.
R. Harald Baayen, Richard Piepenbrock, and Leon
Gulikers. 1996. The CELEX2 lexical database.
LDC96L14.
Timothy Baldwin, Colin Bannard, Takaaki Tanaka, and
Dominic Widdows. 2003. An empirical model of
multiword expression decomposability. In ACL 2003
Workshop on Multiword Expressions.
Colin Bannard, Timothy Baldwin, and Alex Lascarides.
2003. A statistical approach to the semantics of verb-
particles. In ACL 2003 Workshop on Multiword Ex-
pressions.
Marco Baroni, Johannes Matiasek, and Harald Trost.
2002. Unsupervised discovery of morphologically re-
lated words based on orthographic and semantic sim-
ilarity. In ACL-02 Workshop on Morphological and
Phonological Learning (SIGPHON), pages 48?57.
Matthew W. Bilotti, Boris Katz, and Jimmy Lin. 2004.
What works better for question answering: Stemming
or morphological query expansion? In Information
Retrieval for Question Answering (IR4QA) Workshop
at SIGIR 2004.
Thorsten Brants and Alex Franz. 2006. The Google Web
1T 5-gram Corpus Version 1.1. LDC2006T13.
Mathias Creutz and Krista Lagus. 2005. Inducing
the morphological lexicon of a natural language from
unannotated text. In International and Interdisci-
plinary Conference on Adaptive Knowledge Represen-
tation and Reasoning.
Mathias Creutz and Krista Lagus. 2007. Unsupervised
models for morpheme segmentation and morphology
learning. ACM Trans. Speech Lang. Process., 4(1):1?
34.
Hal Daume? III and Daniel Marcu. 2005. A large-scale
exploration of effective global features for a joint en-
tity detection and tracking model. In HLT-EMNLP.
Carl de Marken. 1996. Linguistic structure as composi-
tion and perturbation. In ACL.
Markus Dreyer, Jason Smith, and Jason Eisner. 2008.
Latent-variable modeling of string transductions with
finite-state methods. In EMNLP.
Tomaz? Erjavec and Sas?o Dz?eroski. 2004. Machine learn-
ing of morphosyntactic structure: Lemmatising un-
known Slovene words. Applied Artificial Intelligence,
18:17?41.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A li-
brary for large linear classification. JMLR, 9:1871?
1874.
Afsaneh Fazly, Paul Cook, and Suzanne Stevenson.
2009. Unsupervised type and token identification of
idiomatic expressions. Comput. Linguist., 35(1):61?
103.
John Goldsmith. 2001. Unsupervised learning of the
morphology of a natural language. Comput. Linguist.,
27(2):153?198.
David Graff. 2003. English Gigaword. LDC2003T05.
302
Vera Hollink, Jaap Kamps, Christof Monz, and Maarten
de Rijke. 2004. Monolingual document retrieval for
European languages. IR, 7(1):33?52.
Rosie Jones, Benjamin Rey, Omid Madani, and Wiley
Greiner. 2006. Generating query substitutions. In
WWW.
Daniel Jurafsky and James H. Martin. 2000. Speech and
language processing. Prentice Hall.
Daniel Karp, Yves Schabes, Martin Zaidel, and Dania
Egedi. 1992. A freely available wide coverage mor-
phological analyzer for English. In COLING.
Francis Katamba. 1993. Morphology. MacMillan Press.
Samarth Keshava and Emily Pitler. 2006. A simpler, in-
tuitive approach to morpheme induction. In 2nd Pas-
cal Challenges Workshop.
Jonathan K. Kummerfeld and James R. Curran. 2008.
Classification of verb particle constructions with the
Google Web1T Corpus. In Australasian Language
Technology Association Workshop.
Dekang Lin, Kenneth Church, Heng Ji, Satoshi Sekine,
David Yarowsky, Shane Bergsma, Kailash Patil, Emily
Pitler, Rachel Lathbury, Vikram Rao, Kapil Dalwani,
and Sushant Narsale. 2010. New tools for web-scale
N-grams. In LREC.
Dekang Lin. 1998. Automatic retrieval and clustering of
similar words. In COLING-ACL.
Dekang Lin. 1999. Automatic identification of non-
compositional phrases. In ACL.
Christopher D. Manning, Prabhakar Raghavan, and Hin-
rich Schu?tze. 2008. Introduction to Information Re-
trieval. Cambridge University Press.
Guido Minnen, John Carroll, and Darren Pearce. 2001.
Applied morphological processing of English. Nat.
Lang. Eng., 7(3):207?223.
Preslav Nakov and Marti Hearst. 2005. Search en-
gine statistics beyond the n-gram: Application to noun
compound bracketing. In CoNLL.
Preslav Ivanov Nakov. 2007. Using the Web as an Im-
plicit Training Set: Application to Noun Compound
Syntax and Semantics. Ph.D. thesis, University of Cal-
ifornia, Berkeley.
Hoifung Poon, Colin Cherry, and Kristina Toutanova.
2009. Unsupervised morphological segmentation with
log-linear models. In HLT-NAACL.
Martin F. Porter. 1980. An algorithm for suffix stripping.
Program, 14(3).
Patrick Schone and Daniel Jurafsky. 2000. Knowledge-
free induction of morphology using latent semantic
analysis. In LLL/CoNLL.
Patrick Schone and Daniel Jurafsky. 2001. Knowledge-
free induction of inflectional morphologies. In
NAACL.
Kristina Toutanova and Colin Cherry. 2009. A global
model for joint lemmatization and part-of-speech pre-
diction. In ACL-IJCNLP.
Peter D. Turney. 2001. Mining the web for synonyms:
PMI-IR versus LSA on TOEFL. In European Confer-
ence on Machine Learning.
Antal Van den Bosch and Walter Daelemans. 1999.
Memory-based morphological analysis. In ACL.
Richard Wicentowski. 2004. Multilingual noise-robust
supervised morphological analysis using the word-
frame model. In ACL SIGPHON.
Ying Xu, Christoph Ringlstetter, and Randy Goebel.
2009. A continuum-based approach for tightness anal-
ysis of Chinese semantic units. In PACLIC.
David Yarowsky and Richard Wicentowski. 2000. Min-
imally supervised morphological analysis by multi-
modal alignment. In ACL.
303
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 327?337,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Stylometric Analysis of Scientific Articles
Shane Bergsma, Matt Post, David Yarowsky
Department of Computer Science and Human Language Technology Center of Excellence
Johns Hopkins University
Baltimore, MD 21218, USA
sbergsma@jhu.edu, post@cs.jhu.edu, yarowsky@cs.jhu.edu
Abstract
We present an approach to automatically re-
cover hidden attributes of scientific articles,
such as whether the author is a native English
speaker, whether the author is a male or a fe-
male, and whether the paper was published in
a conference or workshop proceedings. We
train classifiers to predict these attributes in
computational linguistics papers. The classi-
fiers perform well in this challenging domain,
identifying non-native writing with 95% accu-
racy (over a baseline of 67%). We show the
benefits of using syntactic features in stylom-
etry; syntax leads to significant improvements
over bag-of-words models on all three tasks,
achieving 10% to 25% relative error reduction.
We give a detailed analysis of which words
and syntax most predict a particular attribute,
and we show a strong correlation between our
predictions and a paper?s number of citations.
1 Introduction
Stylometry aims to recover useful attributes of doc-
uments from the style of the writing. In some do-
mains, statistical techniques have successfully de-
duced author identity (Mosteller and Wallace, 1984),
gender (Koppel et al, 2003), native language (Kop-
pel et al, 2005), and even whether an author has de-
mentia (Le et al, 2011). Stylometric analysis is im-
portant to marketers, analysts and social scientists
because it provides demographic data directly from
raw text. There has been growing interest in apply-
ing stylometry to the content generated by users of
Internet applications, e.g., detecting author ethnic-
ity in social media (Eisenstein et al, 2011; Rao et
Native/Non-native
Male/Female
Conference/Workshop
Stylometric Analysis of Scientific Articles
Abstract
We present an approach to automatically re-
cover hidden attributes of scientific articles,
such as whether the author is a native En-
glish speaker. We train classifiers to predict
these attributes in computational linguistics
papers. The classifiers perform well in this
challenging domain, identifying non-native
writing with 95% accuracy (over a baseline
of 67%), and outperforming reasonable base-
lines on two other difficult tasks. We show the
benefits of using syntactic features in stylom-
etry; syntax leads to significant improvements
over bag-of-words models on all three tasks,
achieving 10% to 25% relative error reduction.
We give an insightful analysis of which words
and syntax most predict a particular attribute,
and we show a strong correlation between our
predictions and a paper?s number of citations.
1 Introduction
Stylometry aims to recover useful attributes of doc-
uments from the style of their writing. In some
domains, statistical techniques have successfully
deduced author identities (Mosteller and Wallace,
1984), gender (Koppel et al, 2003), native language
(Koppel et al, 2005), and even whether an author
has dementia (Le et al, 2011). Stylometric analysis
is important to marketers, analysts and social scien-
tists because it provides demographic data directly
from raw text. There has been growing interest in
applying stylometry in Web 2.0 applications, e.g.,
detecting the ethnicity of Twitter users (Eisenstein
et al, 2011; Rao et al, 2011), or whether a person is
writing deceptive online reviews (Ott et al, 2011).
We evaluate stylometric techniques in the novel
domain of scientific writing. Science is a diffi-
cult domain; authors are compelled, often explic-
itly by reviewers/submission guidelines, to comply
!"#$%&'!(!)!"#$%&
*"+&',&*"+&
*"$!'-(./01(2
!"#$%&'"()* +,-$#.). %/ !*)',")0* +(")*$'.
+1."(-*"
!" #$"%"&' (& (##$)(*+ ') (,')-('.*(//0 $"1
*)2"$ +.33"& (''$.4,'"% )5 %*."&'.6* ($'.*/"%7
%,*+ (% 8+"'+"$ '+" (,'+)$ .% ( &('.2" 9&1
:/.%+ %#"(;"$< !" '$(.& */(%%.6"$% ') #$"3.*'
'+"%" (''$.4,'"% .& *)-#,'('.)&(/ /.&:,.%'.*%
#(#"$%< =+" */(%%.6"$% #"$5)$- 8"// .& '+.%
*+(//"&:.&: 3)-(.&7 .3"&'.50.&: &)&1&('.2"
8$.'.&: 8.'+ >?@ (**,$(*0 A)2"$ ( 4(%"/.&"
)5 BC@D7 (&3 ),'#"$5)$-.&: $"(%)&(4/" 4(%"1
/.&"% )& '8) )'+"$ 3.56*,/' '(%;%< !" %+)8 '+"
4"&"6'% )5 ,%.&: !"#$%&$'& ()%$*+)! .& %'0/)-1
"'$0E %0&'(F /"(3% ') %.:&.6*(&' .-#$)2"-"&'%
)2"$ 4(:1)518)$3% -)3"/% )& (// '+$"" '(%;%7
(*+."2.&: GH@ ') I?@ $"/('.2" "$$)$ $"3,*'.)&<
!" :.2" (& .&%.:+'5,/ (&(/0%.% )5 8+.*+ 8)$3%
(&3 %0&'(F -)%' #$"3.*' ( #($'.*,/($ (''$.4,'"7
(&3 8" %+)8 ( %'$)&: *)$$"/('.)& 4"'8""& ),$
#$"3.*'.)&% (&3 ( #(#"$J% &,-4"$ )5 *.'('.)&%<
2 3,"(%45*")%,
K'0/)-"'$0 (.-% ') $"*)2"$ ,%"5,/ (''$.4,'"% )5 3)*1
,-"&'% 5$)- '+" %'0/" )5 '+".$ 8$.'.&:< L& %)-"
3)-(.&%7 %'('.%'.*(/ '"*+&.M,"% +(2" %,**"%%5,//0
3"3,*"3 (,'+)$ .3"&'.'."% AN)%'"//"$ (&3 !(//(*"7
G>OPD7 :"&3"$ AQ)##"/ "' (/<7 IHHRD7 &('.2" /(&:,(:"
AQ)##"/ "' (/<7 IHH?D7 (&3 "2"& 8+"'+"$ (& (,'+)$
+(% 3"-"&'.( AS" "' (/<7 IHGGD< K'0/)-"'$.* (&(/0%.%
.% .-#)$'(&' ') -($;"'"$%7 (&(/0%'% (&3 %)*.(/ %*."&1
'.%'% 4"*(,%" .' #$)2.3"% 3"-):$(#+.* 3('( 3.$"*'/0
5$)- $(8 '"F'< =+"$" +(% 4""& :$)8.&: .&'"$"%' .&
(##/0.&: %'0/)-"'$0 .& !"4 I<H (##/.*('.)&%7 "<:<7
3"'"*'.&: '+" "'+&.*.'0 )5 =8.''"$ ,%"$% A9.%"&%'".&
"' (/<7 IHGGE T() "' (/<7 IHGGD7 )$ 8+"'+"$ ( #"$%)& .%
8$.'.&: 3"*"#'.2" )&/.&" $"2."8% AU'' "' (/<7 IHGGD<
!" "2(/,('" %'0/)-"'$.* '"*+&.M,"% .& '+" &)2"/
3)-(.& )5 !&')#$',& -+'$'#.< K*."&*" .% ( 3.561
*,/' 3)-(.&E (,'+)$% ($" *)-#"//"37 )5'"& "F#/.*1
.'/0 40 $"2."8"$%V%,4-.%%.)& :,.3"/.&"%7 ') *)-#/0
!"#$%&'!(!)!"#$%&
*"+&',&*"+&
*"$!'-(./01(2
!"#$%&'"()* +,-$#.). %/ !*)',")0* +(")*$'.
+1."(-*"!" #$"%"&' (& (##$)(*+ ') (,')-('.*(//0 $"1*)2"$ +.33"& (''$.4,'"% )5 %*."&'.6* ($'.*/"%7%,*+ (% 8+"'+"$ '+" (,'+)$ .% ( &('.2" 9&1:/.%+ %#"(;"$< !" '$(.& */(%%.6"$% ') #$"3.*''+"%" (''$.4,'"% .& *)-#,'('.)&(/ /.&:,.%'.*%#(#"$%< =+" */(%%.6"$% #"$5)$- 8"// .& '+.%*+(//"&:.&: 3)-(.&7 .3"&'.50.&: &)&1&('.2"8$.'.&: 8.'+ >?@ (**,$(*0 A)2"$ ( 4(%"/.&")5 BC@D7 (&3 ),'#"$5)$-.&: $"(%)&(4/" 4(%"1/.&"% )& '8) )'+"$ 3.56*,/' '(%;%< !" %+)8 '+"4"&"6'% )5 ,%.&: !"#$%&$'& ()%$*+)! .& %'0/)-1"'$0E %0&'(F /"(3% ') %.:&.6*(&' .-#$)2"-"&'%)2"$ 4(:1)518)$3% -)3"/% )& (// '+$"" '(%;%7(*+."2.&: GH@ ') I?@ $"/('.2" "$$)$ $"3,*'.)&<!" :.2" (& .&%.:+'5,/ (&(/0%.% )5 8+.*+ 8)$3%(&3 %0&'(F -)%' #$"3.*' ( #($'.*,/($ (''$.4,'"7(&3 8" %+)8 ( %'$)&: *)$$"/('.)& 4"'8""& ),$#$"3.*'.)&% (&3 ( #(#"$J% &,-4"$ )5 *.'('.)&%<2 3,"(%45*")%,K'0/)-"'$0 (.-% ') $"*)2"$ ,%"5,/ (''$.4,'"% )5 3)*1,-"&'% 5$)- '+" %'0/" )5 '+".$ 8$.'.&:< L& %)-"3)-(.&%7 %'('.%'.*(/ '"*+&.M,"% +(2" %,**"%%5,//03"3,*"3 (,'+)$ .3"&'.'."% AN)%'"//"$ (&3 !(//(*"7G>OPD7 :"&3"$ AQ)##"/ "' (/<7 IHHRD7 &('.2" /(&:,(:"AQ)##"/ "' (/<7 IHH?D7 (&3 "2"& 8+"'+"$ (& (,'+)$+(% 3"-"&'.( AS" "' (/<7 IHGGD< K'0/)-"'$.* (&(/0%.%.% .-#)$'(&' ') -($;"'"$%7 (&(/0%'% (&3 %)*.(/ %*."&1'.%'% 4"*(,%" .' #$)2.3"% 3"-):$(#+.* 3('( 3.$"*'/05$)- $(8 '"F'< =+"$" +(% 4""& :$)8.&: .&'"$"%' .&(##/0.&: %'0/)-"'$0 .& !"4 I<H (##/.*('.)&%7 "<:<73"'"*'.&: '+" "'+&.*.'0 )5 =8.''"$ ,%"$% A9.%"&%'".&"' (/<7 IHGGE T() "' (/<7 IHGGD7 )$ 8+"'+"$ ( #"$%)& .%8$.'.&: 3"*"#'.2" )&/.&" $"2."8% AU'' "' (/<7 IHGGD<!" "2(/,('" %'0/)-"'$.* '"*+&.M,"% .& '+" &)2"/3)-(.& )5 !&')#$',& -+'$'#.< K*."&*" .% ( 3.561*,/' 3)-(.&E (,'+)$% ($" *)-#"//"37 )5'"& "F#/.*1.'/0 40 $"2."8"$%V%,4-.%%.)& :,.3"/.&"%7 ') *)-#/0
!"#$%&'!(!)!"#$%&*"+&',&*"+&*"$!'-(./01(2
!"#$%&'"()* +,-$#.). %/ !*)',")0* +(")*$'.+1."(-*"!" #$"%"&' (& (##$)(*+ ') (,')-('.*(//0 $"1*)2"$ +.33"& (''$.4,'"% )5 %*."&'.6* ($'.*/"%7%,*+ (% 8+"'+"$ '+" (,'+)$ .% ( &('.2" 9&1:/.%+ %#"(;"$< !" '$(.& */(%%.6"$% ') #$"3.*''+"%" (''$.4,'"% .& *)-#,'('.)&(/ /.&:,.%'.*%#(#"$%< =+" */(%%.6"$% #"$5)$- 8"// .& '+.%*+(//"&:.&: 3)-(.&7 .3"&'.50.&: &)&1&('.2"8$.'.&: 8.'+ >?@ (**,$(*0 A)2"$ ( 4(%"/.&")5 BC@D7 (&3 ),'#"$5)$-.&: $"(%)&(4/" 4(%"1/.&"% )& '8) )'+"$ 3.56*,/' '(%;%< !" %+)8 '+"4"&"6'% )5 ,%.&: !"#$%&$'& ()%$*+)! .& %'0/)-1"'$0E %0&'(F /"(3% ') %.:&.6*(&' .-#$)2"-"&'%)2"$ 4(:1)518)$3% -)3"/% )& (// '+$"" '(%;%7(*+."2.&: GH@ ') I?@ $"/('.2" "$$)$ $"3,*'.)&<!" :.2" (& .&%.:+'5,/ (&(/0%.% )5 8+.*+ 8)$3%(&3 %0&'(F -)%' #$"3.*' ( #($'.*,/($ (''$.4,'"7(&3 8" %+)8 ( %'$)&: *)$$"/('.)& 4"'8""& ),$#$"3.*'.)&% (&3 ( #(#"$J% &,-4"$ )5 *.'('.)&%<2 3,"(%45*")%,K'0/)-"'$0 (.-% ') $"*)2"$ ,%"5,/ (''$.4,'"% )5 3)*1,-"&'% 5$)- '+" %'0/" )5 '+".$ 8$.'.&:< L& %)-"3)-(.&%7 %'('.%'.*(/ '"*+&.M,"% +(2" %,**"%%5,//03"3,*"3 (,'+)$ .3"&'.'."% AN)%'"//"$ (&3 !(//(*"7G>OPD7 :"&3"$ AQ)##"/ "' (/<7 IHHRD7 &('.2" /(&:,(:"AQ)##"/ "' (/<7 IHH?D7 (&3 "2"& 8+"'+"$ (& (,'+)$+(% 3"-"&'.( AS" "' (/<7 IHGGD< K'0/)-"'$.* (&(/0%.%.% .-#)$'(&' ') -($;"'"$%7 (&(/0%'% (&3 %)*.(/ %*."&1'.%'% 4"*(,%" .' #$)2.3"% 3"-):$(#+.* 3('( 3.$"*'/05$)- $(8 '"F'< =+"$" +(% 4""& :$)8.&: .&'"$"%' .&(##/0.&: %'0/)-"'$0 .& !"4 I<H (##/.*('.)&%7 "<:<73"'"*'.&: '+" "'+&.*.'0 )5 =8.''"$ ,%"$% A9.%"&%'".&"' (/<7 IHGGE T() "' (/<7 IHGGD7 )$ 8+"'+"$ ( #"$%)& .%8$.'.&: 3"*"#'.2" )&/.&" $"2."8% AU'' "' (/<7 IHGGD<!" "2(/,('" %'0/)-"'$.* '"*+&.M,"% .& '+" &)2"/3)-(.& )5 !&')#$',& -+'$'#.< K*."&*" .% ( 3.561*,/' 3)-(.&E (,'+)$% ($" *)-#"//"37 )5'"& "F#/.*1.'/0 40 $"2."8"$%V%,4-.%%.)& :,.3"/.&"%7 ') *)-#/0
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`$%'7 G>>?D< U,$ 8)$; .&*/,3"%X6'7 !"#$%&'"()* 8-.9.:!" #$"3.*' 8+"'+"$ (#(#"$ .% 8$.''"&X AGD 40 ( &('.2" )$ &)&1&('.2" 9&1:/.%+ %#"(;"$7 AID 40 ( -(/" )$ 5"-(/"7 (&3 ARD .& '+"%'0/" )5 ( *)&5"$"&*" )$ (8)$;%+)# #(#"$< =+" /(''"$.% ( &)2"/ %'0/)-"'$.* (&3 4.4/.)-"'$.* #$"3.*'.)&<6'7 !"#$%&'"()* ;'-"5('.:!" %+)8 '+" 2(/,")5!"#$%&$'& ()%$*+)!5)$ %'0/)-"'$0< ^-)&: )'+"$%78" 3"%*$.4"$+)) !*/!$'$*$'0# .+%11%+ 5$(:-"&'%78+.*+ +(2" &)' #$"2.),%/0 4""& ,%"3 .& %'0/)-"'$0<=K_ 5$(:-"&'% ($" .&'"$#$"'(4/"7 "56*."&'7 (&3 #($1'.*,/($/0 "55"*'.2" 5)$ 3"'"*'.&: &)&1&('.2" 8$.'.&:<!+./" $"*"&' %',3."% +(2" -)%'/0 "2(/,('"3 %.&1:/" #$"3.*'.)& '(%;%7 8" *)-#($" 3.55"$"&' %'$('":."%(*$)%% 3.55"$"&' '(%;% )& ( *)--)& 3('(%"' (&3 8.'+( *)--)& .&5$(%'$,*',$"< L& (33.'.)& ') *)&'$(%'.&:3.55"$"&' 5"(',$" '0#"%7 8" (/%) *)-#($" 3.55"$"&'W.:,$" GX Y$"3.*'.&: +.33"& (''$.4,'"% .& %*."&'.6* ($'.*/"%8.'+ &)$-('.2" #$(*'.*"% .& %#"//.&: (&3 :$(--($<N)$")2"$7 ')#.*(/ */,"% ($" /"%% %(/."&' '+(& .& 3)1-(.&% /.;" %)*.(/ -"3.(< Z"' %*."&*" .% -)$" '+(&[,%' ( :))3 *+(//"&:" 5)$ %'0/)-"'$0E .' .% (& .-#)$1'(&' ($"( .& .'%"/5< K0%'"-% 5)$ %*."&'.6* %'0/)-"'$08),/3 :.2" %)*.)/):.%'% &"8 '))/% 5)$ (&(/0\.&: (*(13"-.* *)--,&.'."%7 (&3 &"88(0% ') $"%)/2" '+" &(1',$" )5 *)//(4)$('.)& .& %#"*.6* ($'.*/"% A])+$. "' (/<7IHGGD< ^,'+)$% -.:+' (/%) ,%" '+"%" '))/%7 "<:< ')+"/# "&%,$" ( *)&%.%'"&' %'0/" .& -,/'.1(,'+)$"3 #(1#"$% A_/)2"$ (&3 `.$%'7 G>>?D< U,$ 8)$; .&*/,3"%X6'7 !"#$%&'"()* 8-.9.: !" #$"3.*' 8+"'+"$ (#(#"$ .% 8$.''"&X AGD 40 ( &('.2" )$ &)&1&('.2" 9&1:/.%+ %#"(;"$7 AID 40 ( -(/" )$ 5"-(/"7 (&3 ARD .& '+"%'0/" )5 ( *)&5"$"&*" )$ ( 8)$;%+)# #(#"$< =+" /(''"$.% ( &)2"/ %'0/)-"'$.* (&3 4.4/.)-"'$.* #$"3.*'.)&<6'7 !"#$%&'"()* ;'-"5('.: !" %+)8 '+" 2(/,")5 !"#$%&$'& ()%$*+)! 5)$ %'0/)-"'$0< ^-)&: )'+"$%78" 3"%*$.4" $+)) !*/!$'$*$'0# .+%11%+ 5$(:-"&'%78+.*+ +(2" &)' #$"2.),%/0 4""& ,%"3 .& %'0/)-"'$0<=K_ 5$(:-"&'% ($" .&'"$#$"'(4/"7 "56*."&'7 (&3 #($1'.*,/($/0 "55"*'.2" 5)$ 3"'"*'.&: &)&1&('.2" 8$.'.&:<!+./" $"*"&' %',3."% +(2" -)%'/0 "2(/,('"3 %.&1:/" #$"3.*'.)& '(%;%7 8" *)-#($" 3.55"$"&' %'$('":."%(*$)%% 3.55"$"&' '(%;% )& ( *)--)& 3('(%"' (&3 8.'+( *)--)& .&5$(%'$,*',$"< L& (33.'.)& ') *)&'$(%'.&:3.55"$"&' 5"(',$" '0#"%7 8" (/%) *)-#($" 3.55"$"&'W.:,$" GX Y$"3.*'.&: +.33"& (''$.4,'"% .& %*."&'.6* ($'.*/"%8.'+ &)$-('.2" #$(*'.*"% .& %#"//.&: (&3 :$(--($<N)$")2"$7 ')#.*(/ */,"% ($" /"%% %(/."&' '+(& .& 3)1-(.&% /.;" %)*.(/ -"3.(< Z"' %*."&*" .% -)$" '+(&[,%' ( :))3 *+(//"&:" 5)$ %'0/)-"'$0E .' .% (& .-#)$1'(&' ($"( .& .'%"/5< K0%'"-% 5)$ %*."&'.6* %'0/)-"'$08),/3 :.2" %)*.)/):.%'% &"8 '))/% 5)$ (&(/0\.&: (*(13"-.* *)--,&.'."%7 (&3 &"8 8(0% ') $"%)/2" '+" &(1',$" )5 *)//(4)$('.)& .& %#"*.6* ($'.*/"% A])+$. "' (/<7IHGGD< ^,'+)$% -.:+' (/%) ,%" '+"%" '))/%7 "<:< ')+"/# "&%,$" ( *)&%.%'"&' %'0/" .& -,/'.1(,'+)$"3 #(1#"$% A_/)2"$ (&3 `.$%'7 G>>?D< U,$ 8)$; .&*/,3"%X6'7 !"#$%&'"()* 8-.9.: !" #$"3.*' 8+"'+"$ (#(#"$ .% 8$.''"&X AGD 40 ( &('.2" )$ &)&1&('.2" 9&1:/.%+ %#"(;"$7 AID 40 ( -(/" )$ 5"-(/"7 (&3 ARD .& '+"%'0/" )5 ( *)&5"$"&*" )$ ( 8)$;%+)# #(#"$< =+" /(''"$.% ( &)2"/ %'0/)-"'$.* (&3 4.4/.)-"'$.* #$"3.*'.)&<6'7 !"#$%&'"()* ;'-"5('.: !" %+)8 '+" 2(/,")5 !"#$%&$'& ()%$*+)! 5)$ %'0/)-"'$0< ^-)&: )'+"$%78" 3"%*$.4" $+)) !*/!$'$*$'0# .+%11%+ 5$(:-"&'%78+.*+ +(2" &)' #$"2.),%/0 4""& ,%"3 .& %'0/)-"'$0<=K_ 5$(:-"&'% ($" .&'"$#$"'(4/"7 "56*."&'7 (&3 #($1'.*,/($/0 "55"*'.2" 5)$ 3"'"*'.&: &)&1&('.2" 8$.'.&:<!+./" $"*"&' %',3."% +(2" -)%'/0 "2(/,('"3 %.&1:/" #$"3.*'.)& '(%;%7 8" *)-#($" 3.55"$"&' %'$('":."%(*$)%% 3.55"$"&' '(%;% )& ( *)--)& 3('(%"' (&3 8.'+( *)--)& .&5$(%'$,*',$"< L& (33.'.)& ') *)&'$(%'.&:3.55"$"&' 5"(',$" '0#"%7 8" (/%) *)-#($" 3.55"$"&'
W.:,$" GX Y$"3.*'.&: +.33"& (''$.4,'"% .& %*."&'.6* ($'.*/"%
8.'+ &)$-('.2" #$(*'.*"% .& %#"//.&: (&3 :$(--($<
N)$")2"$7 ')#.*(/ */,"% ($" /"%% %(/."&' '+(& .& 3)1
-(.&% /.;" %)*.(/ -"3.(< Z"' %*."&*" .% -)$" '+(&
[,%' ( :))3 *+(//"&:" 5)$ %'0/)-"'$0E .' .% (& .-#)$1
'(&' ($"( .& .'%"/5< K0%'"-% 5)$ %*."&'.6* %'0/)-"'$0
8),/3 :.2" %)*.)/):.%'% &"8 '))/% 5)$ (&(/0\.&: (*(1
3"-.* *)--,&.'."%7 (&3 &"8 8(0% ') $"%)/2" '+" &(1
',$" )5 *)//(4)$('.)& .& %#"*.6* ($'.*/"% A])+$. "' (/<7
IHGGD< ^,'+)$% -.:+' (/%) ,%" '+"%" '))/%7 "<:< ')
+"/# "&%,$" ( *)&%.%'"&' %'0/" .& -,/'.1(,'+)$"3 #(1
#"$% A_/)2"$ (&3 `.$%'7 G>>?D< U,$ 8)$; .&*/,3"%X
6'7 !"#$%&'"()* 8-.9.: !" #$"3.*' 8+"'+"$ (
#(#"$ .% 8$.''"&X AGD 40 ( &('.2" )$ &)&1&('.2" 9&1
:/.%+ %#"(;"$7 AID 40 ( -(/" )$ 5"-(/"7 (&3 ARD .& '+"
%'0/" )5 ( *)&5"$"&*" )$ ( 8)$;%+)# #(#"$< =+" /(''"$
.% ( &)2"/ %'0/)-"'$.* (&3 4.4/.)-"'$.* #$"3.*'.)&<
6'7 !"#$%&'"()* ;'-"5('.: !" %+)8 '+" 2(/,"
)5 !"#$%&$'& ()%$*+)! 5)$ %'0/)-"'$0< ^-)&: )'+"$%7
8" 3"%*$.4" $+)) !*/!$'$*$'0# .+%11%+ 5$(:-"&'%7
8+.*+ +(2" &)' #$"2.),%/0 4""& ,%"3 .& %'0/)-"'$0<
=K_ 5$(:-"&'% ($" .&'"$#$"'(4/"7 "56*."&'7 (&3 #($1
'.*,/($/0 "55"*'.2" 5)$ 3"'"*'.&: &)&1&('.2" 8$.'.&:<
!+./" $"*"&' %',3."% +(2" -)%'/0 "2(/,('"3 %.&1
:/" #$"3.*'.)& '(%;%7 8" *)-#($" 3.55"$"&' %'$('":."%
(*$)%% 3.55"$"&' '(%;% )& ( *)--)& 3('(%"' (&3 8.'+
( *)--)& .&5$(%'$,*',$"< L& (33.'.)& ') *)&'$(%'.&:
3.55"$"&' 5"(',$" '0#"%7 8" (/%) *)-#($" 3.55"$"&'
Figure 1: Predicting hidden attributes in scientific articles
with normative practices in spelling and grammar.
Moreover, topical clues are less salient than in do-
mains like social media. Yet science is more than
just a good challenge for stylometry; it is an impor-
tant area in itself. Systems for scientific stylometry
would give sociologists new tools for analyzing aca-
demic communities, and new ways to resolve the na-
ture of collaboration in specific articles (Johri et al,
2011). Authors might also use these tools, e.g. to
help ensure a consistent style in multi-authored pa-
pers (Glover and Hirst, 1995). Our work includes:
New Stylometric Tasks: We predict whether a
paper is written: (1) by a native or non-native En-
glish speaker, (2) by a male or female, and (3) in the
style of a conference or a workshop paper. The latter
is a novel stylometric and bibliometric prediction.
New Stylometric Features: We show the value
of syntactic features for stylometry. Among others,
we describe tree substitution grammar fragments,
which have not previously been used in stylometry.
TSG fragments are interpretable, efficient, and par-
ticularly effective for detecting non-native writing.
While recent studies have mostly evaluated sin-
gle prediction tasks, we compare different strategies
across different tasks on a common dataset and with
a common infrastructure. In addition to contrasting
different feature types, we also compare different
Figure 1: Predicting hidden attributes in scientific articles
al., 2011), or whether someone is writing deceptive
online reviews (Ott et al, 2011).
We evaluate stylometric techniques in the novel
domain of scientific writing. Science is a difficult
domain; authors are encouraged, often explicitly
by reviewers/submission-guidelines, to comply with
normative practices in style, spelling and grammar.
Moreover, topical clues are less salient than in do-
mains like social media. Success in this challenging
domain can bring us closer to correctly analyzing
the huge volumes of online text that are currently
unmarked for useful author attributes such as gender
and native-language.
Yet science is more than just a good stepping-
stone for stylometry; it is an important area in itself.
Systems for scientific stylometry would give sociol-
ogists new tools for analyzing academic communi-
ties, and new ways to resolve the nature of collab-
oration in specific articles (Johri et al, 2011). Au-
thors might also use these tools, e.g., to help ensure
a consistent style in multi-authored papers (Glover
and Hirst, 1995), or to determine sections of a paper
needing revision.
327
The contributions of our paper include:
New Stylometric Tasks: We predict whether
a paper is written: (1) by a native or non-native
speaker, (2) by a male or female, and (3) in the style
of a conference or workshop paper. The latter is a
fully novel stylometric and bibliometric prediction.
New Stylometric Features: We show the value
of syntactic features for stylometry. Among others,
we describe tree substitution grammar fragments,
which have not previously been used in stylometry.
TSG fragments are interpretable, efficient, and par-
ticularly effective for detecting non-native writing.
While recent studies have mostly evaluated sin-
gle prediction tasks, we compare different strategies
across different tasks on a common dataset and with
a common infrastructure. In addition to contrasting
different feature types, we compare different train-
ing strategies, exploring ways to make use of train-
ing instances with label uncertainty.
We also provide a detailed analysis that is inter-
esting from a sociolinguistic standpoint. Precisely
what words distinguish non-native writing? How
does the syntax of female authors differ from males?
What are the hallmarks of top-tier papers? Finally,
we identify some strong correlations between our
predictions and a paper?s citation count, even when
controlling for paper venue and origin.
2 Related Work
Bibliometrics is the empirical analysis of scholarly
literature; citation analysis is a well-known bib-
liometric approach for ranking authors and papers
(Borgman and Furner, 2001). Bibliometry and sty-
lometry can share goals but differ in techniques.
For example, in a work questioning the blindness
of double-blind reviewing, Hill and Provost (2003)
predict author identities. They ignore the article
body and instead consider (a) potential self-citations
and (b) similarity between the article?s citation list
and the citation lists of known papers. Radev et al
(2009a) perform a bibliometric analysis of compu-
tational linguistics. Teufel and Moens (2002) and
Qazvinian and Radev (2008) summarize scientific
articles, the latter by automatically finding and fil-
tering sentences in other papers that cite the target
article.
Our system does not consider citations; it is most
similar to work that uses raw article text. Hall et
al. (2008) build per-year topic models over scientific
literature to track the evolution of scientific ideas.
Gerrish and Blei (2010) assess the influence of indi-
vidual articles by modeling their impact on the con-
tent of future papers. Yogatama et al (2011) pre-
dict whether a paper will be cited based on both its
content and its meta-data such as author names and
publication venues. Johri et al (2011) use per-author
topic models to assess the nature of collaboration in
a particular article (e.g., apprenticeship or synergy).
One of the tasks in Sarawgi et al (2011) concerned
predicting gender in scientific writing, but they use a
corpus of only ten ?highly established? authors and
make the prediction using twenty papers for each.
Finally, Dale and Kilgarriff (2010) initiated a shared
task on automatic editing of scientific papers written
by non-native speakers, with the objective of devel-
oping ?tools which can help non-native speakers of
English (NNSs) (and maybe some native ones) write
academic English prose of the kind that helps a pa-
per get accepted.?
Lexical and pragmatic choices in academic writ-
ing have also been analyzed within the applied lin-
guistics community (Myers, 1989; Vassileva, 1998).
3 ACL Dataset and Preprocessing
We use papers from the ACL Anthology Network
(Radev et al, 2009b, Release 2011) and exploit its
manually-curated meta-data such as normalized au-
thor names, affiliations (including country, avail-
able up to 2009), and citation counts. We con-
vert each PDF to text1 but remove text before the
Abstract (to anonymize) and after the Acknowledg-
ments/References headings. We split the text into
sentences2 and filter any documents with fewer than
100 (this removes some short/demo papers, mal-
converted PDFs, etc. ? about 23% of the 13K pa-
pers with affiliation information). In case the text
was garbled, we then filtered the first 3 lines from
every file and any line with an ?@? symbol (which
might be part of an affiliation). We remove foot-
ers like Proceedings of ..., table/figure captions, and
any lines with non-ASCII characters (e.g. math
equations). Papers are then parsed via the Berke-
1Via the open-source utility pdftotext
2Splitter from cogcomp.cs.illinois.edu/page/tools
328
Task Training Set: Dev Test
Strict Lenient Set Set
NativeL 2127 3963 450 477
Venue 2484 3991 400 421
Gender 2125 3497 400 409
Table 1: Number of documents for each task
ley parser (Petrov et al, 2006), and part-of-speech
(PoS) tagged using CRFTagger (Phan, 2006).
Training sets always comprise papers from 2001-
2007, while test sets are created by randomly shuf-
fling the 2008-2009 portion and then dividing it into
development/test sets. We also use papers from
1990-2000 for experiments in ?7.3 and ?7.4.
4 Stylometric Tasks
Each task has both a Strict training set, using only
the data for which we are most confident in the la-
bels (as described below), and a Lenient set, which
forcibly assigns every paper in the training period
to some class (Table 1). All test papers are anno-
tated using a Strict rule. While our approaches for
automatically-assigning labels can be coarse, they
allow us to scale our analysis to a realistic cross-
section of academic papers, letting us discover some
interesting trends.
4.1 NativeL: Native vs. Non-Native English
We introduce the task of predicting whether a sci-
entific paper is written by a native English speaker
(NES) or non-native speaker (NNS). Prior work has
mostly made this prediction in learner corpora (Kop-
pel et al, 2005; Tsur and Rappoport, 2007; Wong
and Dras, 2011), although there have been attempts
in elicited speech transcripts (Tomokiyo and Jones,
2001) and e-mail (Estival et al, 2007). There has
also been a large body of work on correcting er-
rors in non-native writing, with a specific focus on
difficulties in preposition and article usage (Han et
al., 2006; Chodorow et al, 2007; Felice and Pul-
man, 2007; Tetreault and Chodorow, 2008; Gamon,
2010).
We annotate papers using two pieces of associated
meta-data: (1) author first names and (2) countries
of affiliation. We manually marked each country for
whether English is predominantly spoken there. We
then built a list of common first names of English
speakers via the top 150 male and female names
from the U.S. census.3 If the first author of a pa-
per has an English first name and English-speaking-
country affiliation, we mark as NES.4 If none of the
authors have an English first name nor an English-
speaking-country affiliation, we mark as NNS. We
use this rule to label our development and test data,
as well as our Strict training set. For Lenient train-
ing, we decide based solely on whether the first au-
thor is from an English-speaking country.
4.2 Venue: Top-Tier vs. Workshop
This novel task aims to distinguish top-tier papers
from those at workshops, based on style. We use
the annual meeting of the ACL as our canonical top-
tier venue. For evaluation and Strict training, we la-
bel all main-session ACL papers as top-tier, and all
workshop papers as workshop. For Lenient training,
we assign all conferences (LREC, Coling, EMNLP,
etc.) to be top-tier except for their non-main-session
papers, which we label as workshop.
4.3 Gender: Male vs. Female
Because we are classifying an international set of
authors, U.S. census names (the usual source of
gender ground-truth) provide incomplete informa-
tion. We therefore use the data of Bergsma and Lin
(2006).5 This data has been widely used in corefer-
ence resolution but never in stylometry. Each line
in the data lists how often a noun co-occurs with
male, female, neutral and plural pronouns; this is
commonly taken as an approximation of the true
gender distribution. E.g., ?bill clinton? is 98% male
(in 8344 instances) while ?elsie wayne? is 100% fe-
male (in 23). The data also has aggregate counts
over all nouns with the same first token, e.g., ?elsie
...? is 94% female (in 255 instances). For Strict
training/evaluation, we label papers with the fol-
lowing rule based on the first author?s first name:
3
www.census.gov/genealogy/names/names_files.
html We also manually added common nicknames for these,
e.g. Rob for Robert, Chris for Christopher, Dan for Daniel, etc.
4Of course, assuming the first author writes each paper is
imperfect. In fact, for some native/non-native collaborations,
our system ultimately predicts the 2nd (non-native) author to be
the main writer; in one case we confirmed the accuracy of this
prediction by personal communication with the authors.
5
www.clsp.jhu.edu/
?
sbergsma/Gender/
329
if the name has an aggregate count >30 and fe-
male probability >0.85, label as female; otherwise
if the aggregate count is >30 and male probabil-
ity >0.85, label male. This rule captures many of
ACL?s unambiguously-gendered names, both male
(Nathanael, Jens, Hiroyuki) and female (Widad,
Yael, Sunita). For Lenient training, we assign all
papers based only on whether the male or female
probability for the first author is higher. While po-
tentially noisy, there is precedent for assigning a sin-
gle gender to papers ?co-authored by researchers of
mixed gender? (Sarawgi et al, 2011).
5 Models and Training Strategies
Model: We take a discriminative approach to sty-
lometry, representing articles as feature vectors (?6)
and classifying them using a linear, L2-regularized
SVM, trained via LIBLINEAR (Fan et al, 2008).
SVMs are state-of-the-art and have been used pre-
viously in stylometry (Koppel et al, 2005).
Strategy: We test whether it?s better to train with
a smaller, more accurate Strict set, or a larger but
noisier Lenient set. We also explore a third strategy,
motivated by work in learning from noisy web im-
ages (Bergamo and Torresani, 2010), in which we
fix the Strict labels, but also include the remaining
examples as unlabeled instances. We then optimize
a Transductive SVM, solving an optimization prob-
lem where we not only choose the feature weights,
but also labels for unlabeled training points. Like
a regular SVM, the goal is to maximize the margin
between the positive and negative vectors, but now
the vectors have both fixed and imputed labels. We
optimize using Joachims (1999)?s software. While
the classifier is trained using a transductive strategy,
it is still tested inductively, i.e., on unseen data.
6 Stylometric Features
Koppel et al (2003) describes a range of features
that have been used in stylometry, ranging from
early manual selection of potentially discriminative
words, to approaches based on automated text cat-
egorization (Sebastiani, 2002). We use the follow-
ing three feature classes; the particular features were
chosen based on development experiments.
6.1 Bow Features
A variety of ?discouraging results? in the text cate-
gorization literature have shown that simple bag-of-
words (Bow) representations usually perform better
than ?more sophisticated? ones (e.g. using syntax)
(Sebastiani, 2002). This was also observed in sen-
timent classification (Pang et al, 2002). One key
aim of our research is to see whether this is true of
scientific stylometry. Our Bow representation uses
a feature for each unique lower-case word-type in
an article. We also preprocess papers by making all
digits ?0?. Normalizing digits and filtering capital-
ized words helps ensure citations and named-entities
are excluded from our features. The feature value is
the log-count of how often the corresponding word
occurs in the document.
6.2 Style Features
While text categorization relies on keywords, sty-
lometry focuses on topic-independent measures like
function word frequency (Mosteller and Wallace,
1984), sentence length (Yule, 1939), and PoS (Hirst
and Feiguina, 2007). We define a style-word to be:
(1) punctuation, (2) a stopword, or (3) a Latin abbre-
viation.6 We create Style features for all unigrams
and bigrams, replacing non-style-words separately
with both PoS-tags and spelling signatures.7 Each
feature is an N-gram, the value is its log-count in the
article. We also include stylistic meta-features such
as mean-words-per-sentence and mean-word-length.
6.3 Syntax Features
Unlike recent work using generative PCFGs (Ragha-
van et al, 2010; Sarawgi et al, 2011), we use syntax
directly as features in discriminative models, which
can easily incorporate arbitrary and overlapping syn-
tactic clues. For example, we will see that one indi-
cator of native text is the use of certain determin-
ers as stand-alone noun phrases (NPs), like this in
Figure 2. This contrasts with a proposed non-native
phrase, ?this/DT growing/VBG area/NN,? where this
instead modifies a noun. The Bow features are
clearly unhelpful: this occurs in both cases. The
6The stopword list is the standard set of 524 SMART-system
stopwords (following Tomokiyo and Jones (2001)). Latin ab-
breviations are i.e., e.g., etc., c.f., et or al.
7E.g., signature ?LC-ing? means lower-case, ending in ing.
These are created via a script included with the Berkeley parser.
330
we did this using . . .
PRP VBD DT VBG
NPNP
VP
. . .
Figure 2: Motivating deeper syntactic features: The
shaded TSG fragment indicates native English, but is not
directly encoded in Bow, Style, nor standard CFG-rules.
Style features are likewise unhelpful; this-VBG also
occurs in both cases. We need the deeper knowledge
that a specific determiner is used as a complete NP.
We evaluate three feature types that aim to cap-
ture such knowledge. In each case, we aggregate the
feature counts over all the parse trees constituting a
document. The feature value is the log-count of how
often each feature occurs. To remove content infor-
mation from the features, we preprocess the parse
tree terminals: all non-style-word terminals are re-
placed with their spelling signature (see ?6.2).
CFG Rules: We include a feature for every unique,
single-level context-free-grammar (CFG) rule appli-
cation in a paper (following Baayen et al (1996),
Gamon (2004), Hirst and Feiguina (2007), Wong
and Dras (2011)). The Figure 2 tree would have
features: NP?PRP, NP?DT, DT?this, etc. Such fea-
tures do capture that a determiner was used as an NP,
but they do not jointly encode which determiner was
used. This is an important omission; we?ll see that
other determiners acting as stand-alone NPs indicate
non-native writing (e.g., the word that, see ?7.2).
TSG Fragments: A tree-substitution grammar is a
generalization of CFGs that allow rewriting to tree
fragments rather than sequences of non-terminals
(Joshi and Schabes, 1997). Figure 2 gives the exam-
ple NP?(DT this). This fragment captures both the
identity of the determiner and its syntactic function
as an NP, as desired. Efficient Bayesian procedures
have recently been developed that enable the train-
ing of large-scale probabilistic TSG grammars (Post
and Gildea, 2009; Cohn et al, 2010).
While TSGs have not been used previously in sty-
lometry, Post (2011) uses them to predict sentence
grammaticality (i.e. detecting pseudo-sentences fol-
lowing Okanohara and Tsujii (2007) and Cherry and
Quirk (2008)). We use Post?s TSG training settings
and his public code.8 We parse with the TSG gram-
mar and extract the fragments as features. We also
follow Post by having features for aggregate TSG
statistics, e.g., how many fragments are of a given
size, tree-depth, etc. These syntactic meta-features
are somewhat similar to the manually-defined stylo-
metric features of Stamatatos et al (2001).
C&J Reranking Features: We also extracted the
reranking features of Charniak and Johnson (2005).
These features were hand-crafted for reranking the
output of a parser, but have recently been used for
other NLP tasks (Post, 2011; Wong and Dras, 2011).
They include lexicalized features for sub-trees and
head-to-head dependencies, and aggregate features
for conjunct parallelism and the degree of right-
branching. We get the features using another script
from Post.9 While TSG fragments tile a parse tree
into a few useful fragments, C&J features can pro-
duce thousands of features per sentence, and are thus
much more computationally-demanding.
7 Experiments and Results
We take the minority class as the positive class:
NES for NativeL, top-tier for Venue and female for
Gender, and calculate the precision/recall of these
classes. We tune three hyperparameters for F1-
score on development data: (1) the SVM regular-
ization parameter, (2) the threshold for classifying
an instance as positive (using the signed hyperplane-
distance as the score), and (3) for transductive train-
ing (?5), the fraction of unlabeled data to label as
positive. Statistical significance on held-out test data
is assessed with McNemar?s test, p<0.05. For F1-
score, we use the following reasonable Baseline: we
label all instances with the label of the minority class
(achieving 100% recall but low precision).
7.1 Selection of Syntax and Training Strategy
Development experiments showed that using all fea-
tures, Bow+Style+Syntax, works best on all tasks,
but there was no benefit in combining different
8http://github.com/mjpost/dptsg
9http://github.com/mjpost/extract-spfeatures.
331
Syntax Strategy NativeL Venue Gender
Baseline 50.5 45.0 28.7
CFG Strict 93.5 59.9 42.5
CFG Lenient 89.9 64.9 39.5
TSG Strict 93.6 60.7 40.0
TSG Lenient 90.9 64.4 39.1
C&J Strict 90.5 62.3 37.1
C&J Lenient 86.2 65.2 39.0
Table 2: F1 scores for Bow+Style+Syntax system on de-
velopment data: The best training strategy and the best
syntactic features depend on the task.
Syntax features. We also found no gain from trans-
ductive training, but greater cost, with more hyper-
parameter tuning and a slower SVM solver. The
best Syntax features depend on the task (Table 2).
Whether Strict or Lenient training: TSG was best
for NativeL, C&J was best for Venue, and CFG was
best for Gender. These trends continue on test data,
where TSG exceeds CFG (91.6% vs. 91.2%). For
the training strategy, Strict was best on NativeL and
Gender, while Lenient was best on Venue (Table 2).
This latter result is interesting: recall that for Venue,
Lenient training considers all conferences to be top-
tier, but evaluation is just on detecting ACL papers.
We suggest some reasons for this below, highlight-
ing some general features of conference papers that
extend beyond particular venues.
For the remainder of experiments on each task,
we fix the syntactic features and training strategy to
those that performed best on development data.
7.2 Test Results and Feature Analysis
Gender remains the most difficult task on test data,
but our F1 still substantially outperforms the base-
line (Table 3). Results on NativeL are particu-
larly impressive; in terms of accuracy, we classify
94.6% of test articles correctly (the majority-class
baseline is 66.9%). Regarding features, just using
Style+Syntax always works better than using Bow.
Combining all features always works better still.
The gains of Bow+Style+Syntax over vanilla Bow are
statistically significant in each case.
We also highlight important individual features:
NativeL: Table 4 gives Bow and Style features
for NativeL. Some reflect differences in common
Features NativeL Venue Gender
Baseline 49.8 45.5 33.1
Bow 88.8 60.7 42.5
Style 90.6 61.9 39.8
Syntax 88.7 64.6 41.2
Bow+Style 90.4 64.0 45.1
Bow+Syntax 90.3 65.8 42.9
Style+Syntax 89.4 65.5 43.3
Bow+Style+Syntax 91.6 66.7 48.2
Table 3: F1 scores with different features on held-out test
data: Including style and syntactic features is superior to
standard Bow features in all cases.
native/non-native topics; e.g., ?probabilities? pre-
dicts native while ?morphological? predicts non-
native. Several features, like ?obtained?, indicate L1
interference; i.e., many non-natives have a cognate
for obtain in their native language and thus adopt the
English word. As an example, the word obtained
occurs 3.7 times per paper from Spanish-speaking
areas (cognate obtenir) versus once per native paper
and 0.8 times per German-authored paper.
Natives also prefer certain abbreviations (e.g.
?e.g.?) while non-natives prefer others (?i.e.?, ?c.f.?,
?etc.?). Exotic punctuation also suggests native text:
the semi-colon, exclamation and question mark all
predict NES. Note this also varies by region; semi-
colons are most popular in NES countries but papers
from Israel and Italy are close behind.
Table 5 gives highly-weighted TSG features for
predicting NativeL. Note the determiner-as-NP us-
age described earlier (? 6.3): these, this and each
predict native when used as an NP; that-as-an-NP
predicts non-native. Furthermore, while not all na-
tive speakers use a comma before a conjunction in
a list, it?s nevertheless a good flag for native writ-
ing (?NP?NP, NP, (CC and) NP?). In terms of non-
native syntax, the passive voice is more common
(?VP?(VBZ is) VP? and ?VP?VBN (PP (IN as) NP)?).
We also looked for features involving determiners
since correct determiner usage is a common diffi-
culty for non-native speakers. We found cases where
determiners were missing where natives might have
used one (?NP?JJ JJ NN?), but also those where a de-
terminer might be optional and skipped by a native
speaker (?NP?(DT the) NN NNS?). Note that Table 5
332
Predicts native Predicts non-native
Bow feature Wt. Bow feature Wt.
initial 2.25 obtained -2.15
techniques 2.11 proposed -2.06
probabilities 1.38 method -2.06
additional 1.23 morphological -1.96
fewer 1.02 languages -1.23
Style feature Wt. Style feature Wt.
used to 1.92 , i.e. -2.60
JJR NN 1.90 have to -1.65
has VBN 1.90 the xxxx-ing -1.61
example , 1.75 thus -1.61
all of 1.73 usually -1.24
?s 1.69 mainly -1.21
allow 1.47 , because -1.12
has xxxx-ed 1.45 the VBN -1.12
may be 1.35 JJ for -1.11
; and 1.21 cf -0.97
e.g. 1.10 etc. -0.55
must VB 0.99 associated to -0.23
Table 4: NativeL: Examples of highly-weighted style and
content features in the Bow+Style+Syntax system.
examples are based on actual usage in ACL papers.
We also found that complex NPs were more asso-
ciated with native text. Features such as ?NP?DT JJ
NN NN NN?, and ?NP?DT NN NN NNS? predict native
writing.
Non-natives also rely more on boilerplate. For
example, the exact phrase ?The/This paper is orga-
nized as follows? occurs 3 times as often in non-
native compared to native text (in 7.5% of all non-
native papers). Sentence re-use is only indirectly
captured by our features; it would be interesting to
encode flags for it directly.
In general, we found very few highly-weighted
features that pinpoint ?ungrammatical? non-native
writing (the feature ?associated to? in Table 4 is a
rare example). Our classifiers largely detect non-
native writing on a stylistic rather than grammatical
basis.
Venue: Table 6 provides important Bow and Style
features for the Venue task (syntactic features omit-
ted due to space). While some features are topical
(e.g. ?biomedical?), the table gives a blueprint for
writing a solid main-conference paper. That is, good
papers often have an explicit probability model (or
algorithm), experimental baselines, error analysis,
TSG Fragment Example
Predicts native English author:
NP?NNP CD (Model) (1)
NP?(DT these) six of (these)
NP?(DT that) NN in (that) (language)
NP?(DT this) we did (this) using ...
VP?(VBN used) S (used) (to describe it)
NP?NP, NP, (CC and) NP (X), (Y), (and) (Z)
NP?(DT each) (each) consists of ...
Predicts non-native English author:
VP?(VBZ is) VP it (is) (shown below)
VP?VBN (PP (IN as) NP) (considered) (as) (a term)
NP?JJ JJ NN in (other) (large) (corpus)
NP?DT JJ (CD one) (a) (correct) (one)
NP?(DT the) NN NNS seen in (the) (test) (data)
NP?(DT that) larger than (that) of ...
QP?(IN about) CD (about) (200,000) words
Table 5: NativeL: Highly-weighted syntactic features
(descending order of absolute weight) and examples in
the Bow+Style+Syntax system.
and statistical significance checking. On the other
hand, there might be a bias at main conferences for
focused, incremental papers; features of workshop
papers highlight the exploration of ?interesting? new
ideas/domains. Here, the objective might only be to
show what is ?possible? or what one is ?able to? do.
Main conference papers prefer work that improves
?performance? by ?#%? on established tasks.
Gender: The CFG features for Gender are given
in Table 7. Several of the most highly-weighted
female features include pronouns (e.g. PRP$). A
higher frequency of pronouns in female writing has
been attested previously (Argamon et al, 2003), but
has not been traced to particular syntactic construc-
tions. Likewise, we observe a higher frequency of
not just negation (noted previously) but adverbs (RB)
in general (e.g. ?VP?MD RB VP?). In terms of Bow
features (not shown), the words contrast and com-
parison highly predict female, as do topical clues
like verb and resource. The top-three male Bow fea-
tures are (in order): simply, perform, parsing.
7.3 Author Rankings
While our objective is to predict attributes of pa-
pers, we also show how that we can identify author
attributes using a larger body of work. We make
NativeL and Gender predictions for all papers in the
333
Predicts ACL Predicts Workshop
Bow feature Wt. Bow feature Wt.
model 2.64 semantic -2.16
probability 1.66 analysis -1.65
performance 1.40 verb -1.35
baseline 1.36 lexical -1.33
= 1.26 study -0.92
algorithm 1.18 biomedical -0.87
large 1.16 preliminary -0.69
error 1.15 interesting -0.69
outperforms 1.02 aim -0.64
significant 0.96 manually -0.62
statistically 0.75 appears -0.54
Style feature Wt. Style feature Wt.
by VBG 1.04 able to -0.99
#% 0.82 xxxx-ed out -0.77
NN over 0.79 further NN -0.71
than the 0.79 NN should -0.69
improvement 0.75 will be -0.61
best 0.71 possible -0.57
xxxx-s by 0.70 have not -0.56
much JJR 0.67 currently -0.56
Table 6: Venue: Examples of highly-weighted style con-
tent features in the Bow+Style+Syntax system.
1990-2000 era using our Bow+Style+Syntax system.
For each author+affiliation with ?3 first-authored
papers, we take the average classifier score on these
papers.
Table 8 shows cases where our model strongly
predicts native, showing top authors with foreign af-
filiations and top authors in English-speaking coun-
tries.10 While not perfect, the predictions correctly
identify some native authors that would be difficult
to detect using only name and location data. For ex-
ample, Dekai Wu (Hong Kong) speaks English na-
tively; Christer Samuelsson lists near-native English
on his C.V.; etc. Likewise, we have also been able
to accurately identify a set of non-native speakers
with common American names that were working
at American universities.
Table 9 provides some of the extreme predictions
of our system on Gender. The extreme male and fe-
male predictions are based on both style and content;
females tend to work on summarization, discourse,
10Note again that this is based on the affiliation of these au-
thors during the 1990s; e.g. Gerald Penn published three papers
while at the University of Tu?bingen.
CFG Rule Example
Predicts female author:
NP?PRP$ NN NN (our) (upper) (bound)
QP?RB CD (roughly) (6000)
NP?NP, CC NP (a new NE tag), (or) (no NE tag)
NP?PRP$ JJ JJ NN (our) (first) (new) (approach)
VP?MD RB VP (may) (not) (be useful)
ADVP?RB RBR (significantly) (more)
Predicts male author:
ADVP?RB RB (only) (superficially)
NP?NP, SBAR we use (XYZ), (which is ...)
S?S: S. (Trust me): (I?m a doctor)
S?S, NP VP (To do so), (it) (needs help)
WHNP?WP NN depending on (what) (path) is ...
PP?IN PRN (in) ((Jelinek, 1976))
Table 7: Gender: Highly-weighted syntactic features
(descending order of weight) and examples in the
Bow+Style+Syntax system.
Highest NES Scores, non-English-country: Gerald
Penn,10 Ezra W. Black, Nigel Collier, Jean-Luc Gauvain,
Dan Cristea, Graham J. Russell, Kenneth R. Beesley,
Dekai Wu, Christer Samuelsson, Raquel Martinez
Highest NES Scores, English-country: Eric V. Siegel,
Lance A. Ramshaw, Stephanie Seneff, Victor W. Zue,
Joshua Goodman, Patti J. Price, Stuart M. Shieber, Jean
Carletta, Lynn Lambert, Gina-Anne Levow
Table 8: Authors scoring highest on NativeL, in descend-
ing order, based exclusively on article text.
etc., while many males focus on parsing. We also
tried making these lists without Bow features, but
the extreme examples still reflect topic to some ex-
tent. Topics themselves have their own style, which
the style features capture; it is difficult to fully sepa-
rate style from topic.
7.4 Correlation with Citations
We also test whether our systems? stylometric scores
correlate with the most common bibliometric mea-
sure: citation count. To reduce the impact of topic,
we only use Style+Syntax features. We plot re-
sults separately for ACL, Coling and Workshop pa-
pers (1990-2000 era). Papers at each venue are
sorted by their classifier scores and binned into five
score bins. Each point in the plot is the mean-
score/mean-number-of-citations for papers in a bin
(within-community citation data is via the AAN ?3
334
Highest Model Scores (Male): John Aberdeen,
Chao-Huang Chang, Giorgio Satta, Stanley F. Chen,
GuoDong Zhou, Carl Weir, Akira Ushioda, Hideki
Tanaka, Koichi Takeda, Douglas B. Paul, Hideo Watan-
abe, Adam L. Berger, Kevin Knight, Jason M. Eisner
Highest Model Scores (Female): Julia B. Hirschberg,
Johanna D. Moore, Judy L. Delin, Paola Merlo,
Rebecca J. Passonneau, Bonnie Lynn Webber, Beth
M. Sundheim, Jennifer Chu-Carroll, Ching-Long Yeh,
Mary Ellen Okurowski, Erik-Jan Van Der Linden
Table 9: Authors scoring highest (absolute values) on
Gender, in descending order, based exclusively on arti-
cle text.
and excludes self citations). We use a truncated
mean for citation counts, leaving off the top/bottom
five papers in each bin.
For NativeL, we only plot papers marked as na-
tive by our Strict rule (i.e. English name/country).
Papers with the lowest NativeL-scores receive many
fewer citations, but they soon level off (Figure 3(a)).
Many junior researchers at English universities are
non-native speakers; early-career non-natives might
receive fewer citations than well-known peers. The
correlation between citations and Venue-scores is
even stronger (Figure 3(b)); the top-ranked work-
shop papers receive five times as many citations
as the lowest ones, and are cited better than a
good portion of ACL papers. These figures sug-
gest that citation-predictors can get useful informa-
tion beyond typical Bow features (Yogatama et al,
2011). Although we focused on a past era, stylis-
tic/syntactic features should also be more robust to
the evolution of scientific topics; we plan to next test
whether we can better forecast future citations. It
would also be interesting to see whether these trends
transfer to other academic disciplines.
7.5 Further Experiments on NativeL
For NativeL, we also created a special test corpus of
273 papers written by first-time ACL authors (2008-
2009 era). This set closely aligns with the system?s
potential use as a tool to help new authors compose
papers. Two (native-speaking) annotators manually
annotated each paper for whether it was primarily
written by a native or non-native speaker (consid-
ering both content and author names/affiliations).
The annotators agreed on 90% of decisions, with an
 1
 10
 0.3  0.4  0.5  0.6  0.7  0.8
NativeL-Score
ACL
Coling
Workshop
(a)
 1
 10
 0.2  0.3  0.4  0.5  0.6
Venue-Score
ACL
Coling
Workshop
(b)
Figure 3: Correlation between predictions (x-axis) and
mean number of citations (y-axis, log-scale).
inter-annotator kappa of 66%. We divided the papers
into a test set and a development set. We applied our
Bow+Style+Syntax system exactly as trained above,
except we tuned its hyperparameters on the new de-
velopment data. The system performed quite well
on this set, reaching 68% F1 over a baseline of only
27%. Moreover, the system also reached 90% accu-
racy, matching the level of human agreement.
8 Conclusion
We have proposed, developed and successfully eval-
uated significant new tasks and methods in the sty-
lometric analysis of scientific articles, including the
novel resolution of publication venue based on pa-
per style, and novel syntactic features based on tree
substitution grammar fragments. In all cases, our
syntactic and stylistic features significantly improve
over a bag-of-words baseline, achieving 10% to 25%
relative error reduction in all three major tasks. We
have included a detailed and insightful analysis of
discriminative stylometric features, and we showed
a strong correlation between our predictions and a
paper?s number of citations. We observed evidence
for L1-interference in non-native writing, for dif-
ferences in topic between males and females, and
for distinctive language usage which can success-
fully identify papers published in top-tier confer-
ences versus wokrshop proceedings. We believe that
this work can stimulate new research at the intersec-
tion of computational linguistics and bibliometrics.
335
References
Shlomo Argamon, Moshe Koppel, Jonathan Fine, and
Anat Rachel Shimoni. 2003. Gender, genre, and writ-
ing style in formal written texts. Text, 23(3), August.
Harald Baayen, Fiona Tweedie, and Hans van Halteren.
1996. Outside the cave of shadows: Using syntactic
annotation to enhance authorship attribution. Literary
and Linguistic Computing, 11(3):121?132.
Alessandro Bergamo and Lorenzo Torresani. 2010. Ex-
ploiting weakly-labeled web images to improve object
classification: a domain adaptation approach. In Proc.
NIPS, pages 181?189.
Shane Bergsma and Dekang Lin. 2006. Bootstrapping
path-based pronoun resolution. In Proc. Coling-ACL,
pages 33?40.
Christine L. Borgman and Jonathan Furner. 2001. Schol-
arly communication and bibliometrics. Annual Review
of Information Science and Technology, 36:3?72.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and MaxEnt discriminative rerank-
ing. In Proc. ACL, pages 173?180.
Colin Cherry and Chris Quirk. 2008. Discriminative,
syntactic language modeling through latent SVMs. In
Proc. AMTA.
Martin Chodorow, Joel R. Tetreault, and Na-Rae Han.
2007. Detection of grammatical errors involving
prepositions. In Proc. ACL-SIGSEM Workshop on
Prepositions, pages 25?30.
Trevor Cohn, Phil Blunsom, and Sharon Goldwater.
2010. Inducing tree-substitution grammars. J. Mach.
Learn. Res., 11:3053?3096.
Robert Dale and Adam Kilgarriff. 2010. Helping our
own: Text massaging for computational linguistics as
a new shared task. In Proc. 6th International Natural
Language Generation Conference, pages 261?265.
Jacob Eisenstein, Noah A. Smith, and Eric P. Xing.
2011. Discovering sociolinguistic associations with
structured sparsity. In Proc. ACL, pages 1365?1374.
Dominique Estival, Tanja Gaustad, Son-Bao Pham, Will
Radford, and Ben Hutchinson. 2007. Author profiling
for English emails. In Proc. PACLING, pages 263?
272.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A li-
brary for large linear classification. J. Mach. Learn.
Res., 9:1871?1874.
Rachele De Felice and Stephen G. Pulman. 2007. Au-
tomatically acquiring models of preposition use. In
Proc. ACL-SIGSEM Workshop on Prepositions, pages
45?50.
Michael Gamon. 2004. Linguistic correlates of style:
authorship classification with deep linguistic analysis
features. In Proc. Coling, pages 611?617.
Michael Gamon. 2010. Using mostly native data to cor-
rect errors in learners? writing: a meta-classifier ap-
proach. In Proc. HLT-NAACL, pages 163?171.
Sean Gerrish and David M. Blei. 2010. A language-
based approach to measuring scholarly impact. In
Proc. ICML, pages 375?382.
Angela Glover and Graeme Hirst. 1995. Detecting
stylistic inconsistencies in collaborative writing. In
Writers at work: Professional writing in the comput-
erized environment, pages 147?168.
David Hall, Daniel Jurafsky, and Christopher D. Man-
ning. 2008. Studying the history of ideas using topic
models. In Proc. EMNLP, pages 363?371.
Na-Rae Han, Martin Chodorow, and Claudia Leacock.
2006. Detecting errors in English article usage by non-
native speakers. Nat. Lang. Eng., 12(2):115?129.
Shawndra Hill and Foster Provost. 2003. The myth of
the double-blind review?: Author identification using
only citations. SIGKDD Explor. Newsl., 5:179?184.
Graeme Hirst and Ol?ga Feiguina. 2007. Bigrams of
syntactic labels for authorship discrimination of short
texts. Literary and Linguistic Computing, 22(4):405?
417.
Thorsten Joachims. 1999. Transductive inference for
text classification using support vector machines. In
Proc. ICML, pages 200?209.
Nikhil Johri, Daniel Ramage, Daniel McFarland, and
Daniel Jurafsky. 2011. A study of academic collabo-
rations in computational linguistics using a latent mix-
ture of authors model. In Proc. 5th ACL-HLT Work-
shop on Language Technology for Cultural Heritage,
Social Sciences, and Humanities, pages 124?132.
Aravind K. Joshi and Yves Schabes. 1997. Tree-
adjoining grammars. In G. Rozenberg and A. Salo-
maa, editors, Handbook of Formal Languages: Beyond
Words, volume 3, pages 71?122.
Moshe Koppel, Shlomo Argamon, and Anat Rachel Shi-
moni. 2003. Automatically categorizing written texts
by author gender. Literary and Linguistic Computing,
17(4):401?412.
Moshe Koppel, Jonathan Schler, and Kfir Zigdon. 2005.
Determining an author?s native language by mining a
text for errors. In Proc. KDD, pages 624?628.
Xuan Le, Ian Lancashire, Graeme Hirst, and Regina
Jokel. 2011. Longitudinal detection of dementia
through lexical and syntactic changes in writing: A
case study of three British novelists. Literary and Lin-
guistic Computing, 26(4):435?461.
Frederick Mosteller and David L. Wallace. 1984. Ap-
plied Bayesian and Classical Inference: The Case of
the Federalist Papers. Springer-Verlag.
Greg Myers. 1989. The pragmatics of politeness in sci-
entific articles. Applied Linguistics, 10(1):1?35.
336
Daisuke Okanohara and Jun?ichi Tsujii. 2007. A
discriminative language model with pseudo-negative
samples. In Proc. ACL, pages 73?80.
Myle Ott, Yejin Choi, Claire Cardie, and Jeffrey T. Han-
cock. 2011. Finding deceptive opinion spam by any
stretch of the imagination. In Proc. ACL, pages 309?
319.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: Sentiment classification using
machine learning techniques. In Proc. EMNLP, pages
79?86.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proc. Coling-ACL, pages
433?440.
Xuan-Hieu Phan. 2006. CRFTagger: CRF English POS
Tagger. crftagger.sourceforge.net.
Matt Post and Daniel Gildea. 2009. Bayesian learning
of a tree substitution grammar. In Proc. ACL-IJCNLP,
pages 45?48.
Matt Post. 2011. Judging grammaticality with tree sub-
stitution grammar derivations. In Proc. ACL, pages
217?222.
Vahed Qazvinian and Dragomir R. Radev. 2008. Scien-
tific paper summarization using citation summary net-
works. In Proc. Coling, pages 689?696.
Dragomir R. Radev, Mark Thomas Joseph, Bryan Gib-
son, and Pradeep Muthukrishnan. 2009a. A biblio-
metric and network analysis of the field of computa-
tional linguistics. Journal of the American Society for
Information Science and Technology.
Dragomir R. Radev, Pradeep Muthukrishnan, and Vahed
Qazvinian. 2009b. The ACL anthology network cor-
pus. In Proc. ACL Workshop on Natural Language
Processing and Information Retrieval for Digital Li-
braries, pages 54?61.
Sindhu Raghavan, Adriana Kovashka, and Raymond
Mooney. 2010. Authorship attribution using proba-
bilistic context-free grammars. In Proc. ACL, pages
38?42.
Delip Rao, Michael Paul, Clay Fink, David Yarowsky,
Timothy Oates, and Glen Coppersmith. 2011. Hierar-
chical bayesian models for latent attribute detection in
social media. In Proc. ICWSM, pages 598?601.
Ruchita Sarawgi, Kailash Gajulapalli, and Yejin Choi.
2011. Gender attribution: tracing stylometric evidence
beyond topic and genre. In Proc. CoNLL, pages 78?
86.
Fabrizio Sebastiani. 2002. Machine learning in auto-
mated text categorization. ACM Comput. Surv., 34:1?
47.
Efstathios Stamatatos, Nikos Fakotakis, and George
Kokkinakis. 2001. Automatic text categorization in
terms of genre and author. Computational Linguistics,
26(4):471?495.
Joel R. Tetreault and Martin Chodorow. 2008. The ups
and downs of preposition error detection in ESL writ-
ing. In Proc. Coling, pages 865?872.
Simone Teufel and Marc Moens. 2002. Summariz-
ing scientific articles - experiments with relevance
and rhetorical status. Computational Linguistics,
28(4):409?445.
Laura Mayfield Tomokiyo and Rosie Jones. 2001.
You?re not from ?round here, are you? Naive Bayes
detection of non-native utterances. In Proc. NAACL.
Oren Tsur and Ari Rappoport. 2007. Using classi-
fier features for studying the effect of native language
on the choice of written second language words. In
Proc. Workshop on Cognitive Aspects of Computa-
tional Language Acquisition, pages 9?16.
Irena Vassileva. 1998. Who am I/who are we in aca-
demic writing? International Journal of Applied Lin-
guistics, 8(2):163?185.
Sze-Meng Jojo Wong and Mark Dras. 2011. Exploiting
parse structures for native language identification. In
Proc. EMNLP, pages 1600?1610.
Dani Yogatama, Michael Heilman, Brendan O?Connor,
Chris Dyer, Bryan R. Routledge, and Noah A. Smith.
2011. Predicting a scientific community?s response to
an article. In Proc. EMNLP, pages 594?604.
G. Udny Yule. 1939. On sentence-length as a statis-
tical characteristic of style in prose: With applica-
tion to two cases of disputed authorship. Biometrika,
30(3/4):363?390.
337
Proceedings of NAACL-HLT 2013, pages 1010?1019,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Broadly Improving User Classification via
Communication-Based Name and Location Clustering on Twitter
Shane Bergsma, Mark Dredze, Benjamin Van Durme, Theresa Wilson, David Yarowsky
Department of Computer Science and Human Language Technology Center of Excellence
Johns Hopkins University
Baltimore, MD 21218, USA
shane.a.bergsma@gmail.com, mdredze@cs.jhu.edu, vandurme@cs.jhu.edu, taw@jhu.edu, yarowsky@cs.jhu.edu
Abstract
Hidden properties of social media users, such
as their ethnicity, gender, and location, are of-
ten reflected in their observed attributes, such
as their first and last names. Furthermore,
users who communicate with each other of-
ten have similar hidden properties. We pro-
pose an algorithm that exploits these insights
to cluster the observed attributes of hundreds
of millions of Twitter users. Attributes such
as user names are grouped together if users
with those names communicate with other
similar users. We separately cluster millions
of unique first names, last names, and user-
provided locations. The efficacy of these clus-
ters is then evaluated on a diverse set of clas-
sification tasks that predict hidden users prop-
erties such as ethnicity, geographic location,
gender, language, and race, using only pro-
file names and locations when appropriate.
Our readily-replicable approach and publicly-
released clusters are shown to be remarkably
effective and versatile, substantially outper-
forming state-of-the-art approaches and hu-
man accuracy on each of the tasks studied.
1 Introduction
There is growing interest in automatically classify-
ing users in social media by various hidden prop-
erties, such as their gender, location, and language
(e.g. Rao et al (2010), Cheng et al (2010), Bergsma
et al (2012)). Predicting these and other proper-
ties for users can enable better advertising and per-
sonalization, as well as a finer-grained analysis of
user opinions (O?Connor et al, 2010), health (Paul
and Dredze, 2011), and sociolinguistic phenomena
(Eisenstein et al, 2011). Classifiers for user prop-
erties often rely on information from a user?s social
network (Jernigan and Mistree, 2009; Sadilek et al,
2012) or the textual content they generate (Pennac-
chiotti and Popescu, 2011; Burger et al, 2011).
Here, we propose and evaluate classifiers that bet-
ter exploit the attributes that users explicitly provide
in their user profiles, such as names (e.g., first names
like Mary, last names like Smith) and locations (e.g.,
Brasil). Such attributes have previously been used as
?profile features? in supervised user classifiers (Pen-
nacchiotti and Popescu, 2011; Burger et al, 2011;
Bergsma et al, 2012). There are several motivations
for exploiting these data. Often the only informa-
tion available for a user is a name or location (e.g.
for a new user account). Profiles also provide an
orthogonal or complementary source of information
to a user?s social network and textual content; gains
based on profiles alone should therefore add to gains
based on other data. The decisions of profile-based
classifiers could also be used to bootstrap training
data for other classifiers that use complementary fea-
tures.
Prior work has encoded profile attributes via lex-
ical or character-based features (e.g. Pennacchiotti
and Popescu (2011), Burger et al (2011), Bergsma
et al (2012)). Unfortunately, due to the long-tailed
distribution of user attributes, a profile-based classi-
fier will encounter many examples at test time that
were not observed during training. For example,
suppose a user wassim hassan gives their location as
tanger. If the attribute tokens wassim, hassan, and
tanger do not occur in training (nor indicative sub-
1010
strings), then a classifier can only guess at the user?s
ethnicity and location. In social media, the preva-
lence of fake names and large variations in spelling,
slang, and language make matters worse.
Our innovation is to enhance attribute-based clas-
sifiers with new data, derived from the communica-
tions of Twitter users with those attributes. Users
with the name tokens wassim and hassan often talk
to users with Arab names like abdul and hussein.
Users listing their location as tanger often talk to
users from morocco. Since users who communicate
often share properties such as ethnicity and location
(?8), the user wassim hassan might be an Arab who
uses the French spelling of the city Tangier.
Our challenge is to encode these data in a form
readily usable by a classifier. Our approach is to
represent each unique profile attribute (e.g. tanger
or hassan) as a vector that encodes the communi-
cation pattern of users with that attribute (e.g. how
often they talk to users from morocco, etc.); we then
cluster the vectors to discover latent groupings of
similar attributes. Based on transitive (third party)
connections, tanger and tangier can appear in the
same cluster, even if no two users from these loca-
tions talk directly. To use the clusters in an attribute-
based classifier, we add new features that indicate
the cluster memberships of the attributes. Clustering
thus lets us convert a high-dimensional space of all
attribute pairs to a low-dimensional space of cluster
memberships. This makes it easier to share our data,
yields fewer parameters for learning, and creates at-
tribute groups that are interpretable to humans.
We cluster names and locations in a very large
corpus of 168 million Twitter users (?2) and use a
distributed clustering algorithm to separately clus-
ter millions of first names, last names, and user-
provided locations (?3). We evaluate the use of our
cluster data as a novel feature in supervised classi-
fiers, and compare our result to standard classifiers
using character and token-level features (?4). The
cluster data enables significantly improved perfor-
mance in predicting the gender, location, and lan-
guage of social media users, exceeding both ex-
isting state-of-the-art machine and human perfor-
mance (?6). Our cluster data can likewise im-
prove performance in other domains, on both es-
tablished and new NLP tasks as further evaluated
in this paper (?6). We also propose a way to
First names: maria, david, ana, daniel, michael, john,
alex, jessica, carlos, jose, chris, sarah, laura, juan
Last names: silva, santos, smith, garcia, oliveira, ro-
driguez, jones, williams, johnson, brown, gonzalez
Locations: brasil, indonesia, philippines, london,
jakarta, s?o paulo, rio de janeiro, venezuela, brazil
Table 1: Most frequent profile attributes for our collection
of 168 million Twitter users, in descending order
enhance a geolocation system by using commu-
nication patterns, and show strong improvements
over a hand-engineered baseline (?7). We share
our clusters with the community to use with other
tasks. The clusters, and other experimental data, are
available for download from www.clsp.jhu.edu/
~sbergsma/TwitterClusters/.
2 Attribute Associations on Twitter
Data and Processing Our raw Twitter data com-
prises the union of 2.2 billion tweets from 05/2009
to 10/2010 (O?Connor et al, 2010), 1.8 billion
tweets collected from 07/2011 to 08/2012, and 80
million tweets collected from followers of 10 thou-
sand location and language-specific Twitter feeds.
We implemented each stage of processing using
MapReduce (Dean and Ghemawat, 2008). The total
computation (from extracting profiles to clustering
attributes) was 1300 days of wall-clock CPU time.
Attribute Extraction Tweets provide the name
and self-reported location of the tweeter. We find
126M unique users with these attributes in our data.
When tweets mention other users via an @user con-
struction, Twitter also includes the profile name of
the mentioned user; we obtain a further 42M users
from these cases. We then normalize the extracted
attributes by converting to lower-case, deleting sym-
bols, numbers, and punctuation, and removing com-
mon honorifics and suffixes like mr/mrs and jr/sr.
Common prefixes like van and de la are joined to
the last-name token.1 This processing yields 8.3M
1www.clsp.jhu.edu/~sbergsma/TwitterClusters/
also provides our scripts for normalizing attributes. The scripts
can be used to ensure consistency/compatibility between
arbitrary datasets and our shared cluster data. Note we use no
special processing for the companies, organizations, and spam-
mers among our users, nor for names arising from different
conventions (e.g. 1-word names, reversed first/last names).
1011
henrik: fredrik 5.87, henrik 5.82, anders 5.73, johan
5.69, andreas 5.59, martin 5.54, magnus 5.41
courtney: taylor 8.03, ashley 7.92, courtney 7.92,
emily 7.91, lauren 7.82, katie 7.72, brittany 7.69
ilya: sergey 5.85, alexey 5.62, alexander 5.59, dmitry
5.51, ????????? 5.46, anton 5.44, andrey 5.40
Table 2: Top associates and PMIs for three first names.
unique locations, 7.4M unique last names, and 5.5M
unique first names. These three sets provide the tar-
get attributes that we cluster in ?3. Table 1 shows
the most frequent names in each of these three sets.
User-User Links We extract each user mention as
an undirected communication link between the user
tweeting and the mentioned user (including self-
mentions but not retweets). We consider each user-
user link as a single event; we count it once no mat-
ter how often two specific users interact. We extract
436M user-user links in total.
Attribute-Attribute Pairs We use our profile data
to map each user-user link to an attribute-attribute
pair; we separately count each pair of first names,
last names, and locations. For example, the first-
name pair (henrik, fredrik) occurs 181 times. Rather
than using the raw count, we calculate the associa-
tion between attributes a1 and a2 via their pointwise
mutual information (PMI), following prior work in
distributional clustering (Lin and Wu, 2009):
PMI(a1, a2) = log
P(a1, a2)
P(a1)P(a2)
PMI essentially normalizes the co-occurrence by
what we would expect if the attributes were indepen-
dently distributed. We smooth the PMI by adding a
count of 0.5 to all co-occurrence events.
The most highly-associated name attributes re-
flect similarities in ethnicity and gender (Table 2).
The most highly-ranked associates for locations are
often nicknames and alternate/misspellings of those
locations. For example, the locations charm city,
bmore, balto, westbaltimore, b a l t i m o r e, bal-
timoreee, and balitmore each have the U.S. city of
baltimore as their highest-PMI associate. We show
how this can be used to help geolocate users (?7).
3 Attribute Clustering
Representation We first represent each target at-
tribute as a feature vector, where each feature corre-
sponds to another attribute of the same type as the
target and each value gives the PMI between this at-
tribute and the target (as in Table 2).2 To help cluster
the long-tail of infrequent attributes, we also include
orthographic features. For first and last names, we
have binary features for the last 2 characters in the
string. For locations, we have binary features for
(a) any ideographic characters in the string and (b)
each token (with diacritics removed) in the string.
We normalize the feature vectors to unit length.
Distributed K-Means Clustering Our approach
to clustering follows Lin and Wu (2009) who used k-
means to cluster tens of millions of phrases. We also
use cosine similarity to compute the closest centroid
(i.e., we use the spherical k-means clustering algo-
rithm (Dhillon and Modha, 2001)). We keep track
of the average cosine similarity between each vector
and its nearest centroid; this average is guaranteed
to increase at each iteration.
Like Lin and Wu (2009), we parallelize the al-
gorithm using MapReduce. Each mapper finds the
nearest centroids for a portion of the vectors, while
also computing the partial sums of the vectors as-
signed to each centroid. The mappers emit the cen-
troid IDs as keys and the partial sums as values.
The Reducer aggregates the partial sums from each
partition and re-normalizes each sum vector to unit
length to obtain the new centroids. We also use an
inverted index at each iteration that, for each input
feature, lists which centroids each feature belongs
to. Using this index greatly speeds up the centroid
similarity computations.
Clustering Details We cluster with nine separate
configurations: over first names, last names, and lo-
cations, and each with 50, 200, and 1000 cluster
centroids (denoted C50, C200, and C1000). Since k-
2We decided to restrict the features for a target to be at-
tributes of the same type (e.g., we did not use last name as-
sociations for a first name target) because each attribute type
conveys distinct information. For example, first names convey
gender and age more than last names. By separately cluster-
ing representations using first names, last names, and locations,
each clustering can capture its own distinct latent-class associa-
tions.
1012
Cluster 463 (Serbian): pavlovic?, jovanovic, jo-
vanovic?, stankovic?, srbija, markovic?, petrovic?,
radovic, nenad, milenkovic, nikolic, sekulic, todor-
ovic, stojanovic, petrovic, aleksic, ilic, markovic
Cluster 544 (Black South African): ngcobo, nkosi,
dlamini, ndlovu, mkhize, mtshali, sithole, mathebula,
mthembu, khumalo, ngwenya, shabangu, nxumalo,
buthelezi, radebe, mabena, zwane, mbatha, sibiya
Cluster 449 (Turkish): s?ahin, ?elik, ?zt?rk, ko?, ?ak?r,
karatas?, aktas?, g?ng?r, ?zkan, balc?, g?m?s?, akkaya,
gen?, sar?, y?ksel, g?nes?, yig?it, yal??n, orhan, sag?lam,
g?ler, demirci, k???k, yavuz, bayrak, ?zcan, altun
Cluster 656 (Indonesian): utari, oktaviana, apriani,
mustika, septiana, febrianti, kurniawati, indriani, nur-
janah, septian, cahya, anggara, yuliani, purnamasari,
sukma, wijayanti, pramesti, ningrum, yanti, wulansari
Table 3: Example C1000 last-name clusters
Cluster 56 [sim=0.497]: gregg, bryn, bret, stewart,
lyndsay, howie, elyse, jacqui, becki, rhett, meaghan,
kirstie, russ, jaclyn, zak, katey, seamus, brennan,
fraser, kristie, stu, jaimie, kerri, heath, carley, griffin
Cluster 104 [sim=0.442]: stephon, devonte, deion,
demarcus, janae, tyree, jarvis, donte, dewayne, javon,
destinee, tray, janay, tyrell, jamar, iesha, chyna,
jaylen, darion, lamont, marquise, domonique, alexus
Cluster 132 [sim=0.292]: moustafa, omnya, menna-
tallah, ?C?@, shorouk, ragab, ?


??, radwa, moemen,
mohab, hazem, yehia, ? K
Q k, Z @Q?? @, mennah, ?
 Q?? ?,
abdelrahman, ?


	
????, H. 	Qk, Q?A

K, nermeen, hebatallah
...
Table 4: C200 soft clustering for first name yasmeen
means is not guaranteed to reach a global optimum,
we use ten different random initializations for each
configuration, and select the one with the highest av-
erage similarity after 20 iterations. We run this one
for an additional 30 iterations and take the output as
our final set of centroids for that configuration.
The resulting clusters provide data that could help
classify hidden properties of social media users. For
example, Table 3 shows that last names often clus-
ter by ethnicity, even at the sub-national level (e.g.
Zulu tribe surnames nkosi, dlamini, mathebula, etc.).
Note the Serbian names include two entries that are
not last names: srbija, the Serbian word for Serbia,
and nenad, a common Serbian first name.
Soft Clustering Rather than assigning each at-
tribute to its single highest-similarity cluster, we can
assign each vector to its N most similar clusters.
These soft-cluster assignments often reflect different
social groups where a name or location is used. For
example, the name yasmeen is similar to both com-
mon American names (Cluster 56), African Ameri-
can names (Cluster 104), and Arabic names (Clus-
ter 132) (Table 4). As another example, the C1000
assignments for the location trujillo comprise sep-
arate clusters containing towns and cities in Peru,
Venezuela, Colombia, etc., reflecting the various
places in the Latin world with this name. In general,
the soft cluster assignment is a low-dimensional rep-
resentation of each of our attributes. Although it can
be interpretable to humans, it need not be in order to
be useful to a classifier.
4 Classification with Cluster Features
Our motivating problem is to classify users for hid-
den properties such as their gender, location, race,
ethnicity, and language. We adopt a discriminative
solution. We encode the relevant data for each in-
stance in a feature vector and train a (linear) support
vector machine classifier (Cortes and Vapnik, 1995).
SVMs represent the state-of-the-art on many NLP
classification tasks, but other classifiers could also
be used. For multi-class classification, we use a one-
versus-all strategy, a competitive approach on most
multi-class problems (Rifkin and Klautau, 2004).
The input to our system is one or more observed
user attributes (e.g. name and location fields from
a user profile). We now describe how features are
created from these attributes in both state-of-the-art
systems and via our new cluster data.
Token Features (Tok) are binary features that in-
dicate the presence of a specific attribute (e.g., first-
name=bob). Burger et al (2011) and Bergsma et al
(2012) used Tok features to encode user profile fea-
tures. For multi-token fields (e.g. location), our Tok
features also indicate the specific position of each
token (e.g., loc1=s?o, loc2=paulo, locN=brasil).
Character N-gram Features (Ngm) give the
count of all character n-grams of length 1-to-4 in the
input. Ngm features have been used in user classifi-
cation (Burger et al, 2011) and represent the state-
1013
of-the-art in detecting name ethnicity (Bhargava and
Kondrak, 2010). We add special begin/end charac-
ters to the attributes to mark the prefix and suffix po-
sitions. We also use a smoothed log-count; we found
this to be most effective in preliminary work.
Cluster Features (Clus) indicate the soft-cluster
memberships of the attributes. We have features for
the top-2, 5, and 20 most similar clusters in the C50,
C200, and C1000 clusterings, respectively. Like Lin
and Wu (2009), we ?side-step the matter of choos-
ing the optimal value k in k-means? by using fea-
tures from clusterings at different granularities. Our
feature dimensions correspond to cluster IDs; fea-
ture values give the similarity to the cluster centroid.
Other strategies (e.g. hard clustering, binary fea-
tures) were less effective in preliminary work.
5 Classification Experiments
5.1 Methodology
Our main objective is to assess the value of us-
ing cluster features (Clus). We add these features
to classifiers using Tok+Ngm features, which repre-
sents the current state-of-the-art. We compare these
feature settings on both Twitter tasks (?5.2) and
tasks not related to social-media (?5.3). For each
task, we randomly divide the gold standard data into
50% train, 25% development and 25% test, unless
otherwise noted. As noted above, the gold-standard
datasets for all of our experiments are available for
download. We train our SVM classifiers using the
LIBLINEAR package (Fan et al, 2008). We optimize
the classifier?s regularization parameter on develop-
ment data, and report our final results on the held-
out test examples. We report accuracy: the propor-
tion of test examples classified correctly. For com-
parison, we report the accuracy of a majority-class
baseline on each task (Base).
Classifying hidden properties of social media
users is challenging (Table 5). Pennacchiotti and
Popescu (2011) even conclude that ?profile fields do
not contain enough good-quality information to be
directly used for user classification.? To provide in-
sight into the difficulty of the tasks, we had two hu-
mans annotate 120 examples from each of the test
sets, and we average their results to give a ?Human?
performance number. The two humans are experts in
Country: 53 possible countries
United States courtland dante cali baby
United States tinas twin on the court
Brazil thamires gomez macap? ap
Denmark marte clason NONE
Lang. ID: 9 confusable languages
Bulgarian valentina getova NONE
Russian borisenko yana edinburgh
Bulgarian NONE blagoevgrad
Ukrainian andriy kupyna ternopil
Farsi kambiz barahouei NONE
Urdu musadiq sanwal jammu
Ethnicity: 13 European ethnicities
German dennis hustadt
Dutch bernhard hofstede
French david coste
Swedish mattias bjarsmyr
Portuguese helder costa
Race: black or white
black kerry swain
black darrell foskey
white ty j larocca
black james n jones
white sean p farrell
Table 5: Examples of class (left) and input (names, loca-
tions) for some of our evaluation tasks.
this domain and have very wide knowledge of global
names and locations.
5.2 Twitter Applications
Country A number of recent papers have consid-
ered the task of predicting the geolocation of users,
using both user content (Cheng et al, 2010; Eisen-
stein et al, 2010; Hecht et al, 2011; Wing and
Baldridge, 2011; Roller et al, 2012) and social net-
work (Backstrom et al, 2010; Sadilek et al, 2012).
Here, we first predict user location at the level of
the user?s location country. To our knowledge, we
are the first to exploit user locations and names for
this prediction. For this task, we obtain gold data
from the portion of Twitter users who have GPS en-
abled (geocoded tweets). We were able to obtain a
very large number of gold instances for this task, so
selected only 10K for testing, 10K for development,
and retained the remaining 782K for training.
Language ID Identifying the language of users
is an important prerequisite for building language-
specific social media resources (Tromp and Pech-
1014
enizkiy, 2011; Carter et al, 2013). Bergsma et al
(2012) recently released a corpus of tweets marked
for one of nine languages grouped into three confus-
able character sets: Arabic, Farsi, and Urdu tweets
written in Arabic characters; Hindi, Nepali, and
Marathi written in Devanagari, and Russian, Bulgar-
ian, and Ukrainian written in Cyrillic. The tweets
were marked for language by native speakers via
Amazon Mechanical Turk. We again discard the
tweet content and extract each user?s first name, last
name, and user location as our input data, while tak-
ing the annotated language as the class label.
Gender We predict whether a Twitter user is male
or female using data from Burger et al (2011). This
data was created by linking Twitter users to struc-
tured profile pages on other websites where users
must select their gender. Unlike prior systems using
this data (Burger et al, 2011; Van Durme, 2012), we
make the predictions using only user names.
5.3 Other Applications
Origin Knowing the origin of a name can improve
its automatic pronunciation (Llitjos and Black,
2001) and transliteration (Bhargava and Kondrak,
2010). We evaluate our cluster data on name-origin
prediction using a corpus of names marked as ei-
ther Indian or non-Indian by Bhargava and Kondrak
(2010). Since names in this corpus are not marked
for entity type, we include separate cluster features
from both our first and last name clusters.
Ethnicity We also evaluate on name-origin data
from Konstantopoulos (2007). This data derives
from lists of football players on European national
teams; it marks each name (with diacritics removed)
as arising from one of 13 European languages. Fol-
lowing prior work, we test in two settings: (1) using
last names only, and (2) using first and last names.
Race We also evaluate our ability to identify eth-
nic groups at a sub-national level. To obtain data
for this task, we mined the publicly-available arrest
records on mugshots.com for the U.S. state of New
Jersey (a small but diverse and densely-populated
area). Over 99% of users were listed as either black
or white, and we structure the task as a binary clas-
sification problem between these two classes. We
predict the race of each person based purely on their
name; this contrasts with prior work in social media
which looked at identifying African Americans on
the basis of their Twitter content (Eisenstein et al,
2011; Pennacchiotti and Popescu, 2011).
6 Classification Results
Table 6 gives the results on each task. The system in-
corporating our novel Clus features consistently im-
proves over the Ngm+Tok system; all differences be-
tween All and Ngm+Tok are significant (McNemar?s,
p<0.01). The relative reduction in error from adding
Clus features ranges between 7% and 51%. The All
system including Clus features also exceeds human
performance on all studied tasks.
On Country, the U.S. is the majority class, oc-
curring in 42.5% of cases.3 It is impressive that
All so significantly exceeds Tok+Ngm (86.7% vs.
84.8%); with 782K training examples, we did not
expect such room for improvement. Both names and
locations play an important role: All achieves 66%
using names alone and 70% with only location. On
the subset of data where all three attributes are non-
empty, the full system achieves 93% accuracy.
Both feature classes are likewise important for
Lang. ID; All achieves 67% with only first+last
names, 72% with just locations, but 83% with both.
Our smallest improvement is on Gender. This
task is easier (with higher human/system accuracy)
and has plenty of training data (more data per class
than any other task); there is thus less room to im-
prove. Looking at the feature weights, the strongest-
weighted female cluster apparently captures a sub-
community of Justin Bieber fans (showing loyalty
with ?first names? jbieber, belieb, biebz, beliebing,
jbiebs, etc.). Just because a first name like madison
has a high similarity to this cluster does not imply
girls named Madison are Justin Bieber fans; it sim-
ply means that Madisons have similar names to the
friends of Justin Bieber fans (who tend to be girls).
Also, note that while the majority of the 34K users in
our training data are assigned this cluster somewhere
in their soft clustering, only 6 would be assigned this
3We tried other baselines: e.g., we predict countries if they
are substrings of the location (otherwise predicting U.S.); and
we predict countries if they often occur as a string following
the given location in our profile data (e.g., we predict Spain for
Madrid since Madrid, Spain is common). Variations on these
approaches consistently performed between 48% and 56%.
1015
Task Input
Num. Num.
Base Human Tok Ngm Clus
Tok+
All ?
Train Class Ngm
Country first+last+loc 781920 53 42.5 71.7 83.0 84.5 80.2 84.8 86.7 12.5
Lang. ID first+last+loc 2492 9 27.0 74.2 74.6 80.6 71.1 80.4 82.7 11.7
Gender first+last 33805 2 52.4 88.3 85.3 88.6 79.5 89.5 90.2 6.7
Origin entity name 500 2 52.4 80.4 - 75.6 81.2 75.6 88.0 50.8
Ethnicity last 6026 13 20.8 47.9 - 54.6 48.5 54.6 62.4 17.2
Ethnicity first+last 7457 13 21.2 53.3 67.6 77.5 73.6 78.4 81.3 13.4
Race first+last 7977 2 54.7 71.4 80.4 81.6 84.6 82.4 84.6 12.5
Table 6: Task details and accuracy (%) for attribute-based classification tasks. ? = relative error reduction (%) of All
(Tok+Ngm+Clus) over Ngm+Tok. All always exceeds both Tok+Ngm and the human performance.
cluster in a hard clustering. This clearly illustrates
the value of the soft clustering representation.
Note the All system performed between 83% and
90% on each Twitter task. This level of performance
strongly refutes the prevailing notion that Twitter
profile information is useless in general (Pennac-
chiotti and Popescu, 2011) and especially for geolo-
cation (Cheng et al, 2010; Hecht et al, 2011).
We now move to applications beyond social me-
dia. Bhargava and Kondrak (2010) have the current
state-of-the-art on Origin and Ethnicity based on an
SVM using character-n-gram features; we reimple-
mented this as Ngm. We obtain a huge improvement
over their work using Clus, especially on Origin
where we reduce error by >50%.4 This improve-
ment can partly be attributed to the small amount of
training data; with fewer parameters to learn, Clus
learns more from limited data than Ngm. We like-
wise see large improvements over the state-of-the-
art on Ethnicity, on both last name and full name
settings.
Finally, Clus features also significantly improve
accuracy on the new Race task. Our cluster data can
therefore help to classify names into sub-national
groups, and could potentially be used to infer other
interesting communities such as castes in India and
religious divisions in many countries.
In general, the relative value of our cluster models
varies with the amount of training data; we see huge
gains on the smaller Origin data but smaller gains
on the large Gender set. Figure 1 shows how per-
formance of Clus and Ngm varies with training data
on Race. Again, Clus is especially helpful with less
4Note Tok is not used here because the input is a single token
and training and test splits have distinct instances.
 60
 65
 70
 75
 80
 85
 10  100  1000  10000
A
cc
ur
ac
y
Number of training examples
Clus
Ngm
Figure 1: Learning curve on Race: Clus perform as well
with 30 training examples as Ngm features do with 1000.
data; thousands of training examples are needed for
Ngm to rival the performance of Clus using only a
handful. Since labeled data is generally expensive
to obtain or in short supply, our method for exploit-
ing unlabeled Twitter data can both save money and
improve top-end performance.
7 Geolocation by Association
There is a tradition in computational linguistics of
grouping words both by the similarity of their con-
text vectors (Hindle, 1990; Pereira et al, 1993; Lin,
1998) and directly by their statistical association in
text (Church and Hanks, 1990; Brown et al, 1992).
While the previous sections explored clusters built
by vector similarity, we now explore a direct appli-
cation of our attribute association data (?2).
We wish to use this data to improve an existing
Twitter geolocation system based on user profile lo-
cations. The system operates as follows: 1) normal-
1016
ize user-provided locations using a set of regular ex-
pressions (e.g. remove extra spacing, punctuation);
2) look up the normalized location in an alias list;
3) if found, map the alias to a unique string (target
location), corresponding to a structured location ob-
ject that includes geo-coordinates.
The alias list we are currently using is based on
extensive work in hand-writing aliases for the most
popular Twitter locations. For example, the current
aliases for Nashville, Tennessee include nashville,
nashville tn, music city, etc. Our objective is to im-
prove on this human-designed list by automatically
generating aliases using our association data.
Aliases by Association For each target, we pro-
pose new aliases from the target?s top-PMI asso-
ciates (?2). To become an alias, the PMI between
the alias and target must be above a threshold,
the alias must occur more than a fixed number of
times in our profile data, the alias must be within
the top-N1 associates of the target, and the target
must be within the top-N2 associates of the alias.
We merge our automatic aliases with the manually-
written aliases. The new aliases for Nashville, Ten-
nessee include east nashville, nashville tenn, music
city usa, nashvegas, cashville tn, etc.
Experiments To evaluate the geolocation system,
we use tweets from users with GPS enabled (?5.2).
For each tweet, we resolve the location using the
system and compare to the gold coordinates. The
system can skip a location if it does not match the
alias list; more than half of the locations are skipped,
which is consistent with prior work (Hecht et al,
2011). We evaluate the alias lists using two mea-
sures: (1) its coverage: the percentage of locations it
resolves, and (2) its precision: of the ones resolved,
the percentage that are correct. We define a correct
resolution to be one where the resolved coordinates
are within 50 miles of the gold coordinates.
We use 56K gold tweets to tune the parameters of
our automatic alias-generator, trading off coverage
and precision. We tune such that the system using
these aliases obtains the highest possible coverage,
while being at least as precise as the baseline system.
We then evaluate both the baseline set of aliases and
our new set on 56K held-out examples.
Results On held-out test data, the geolocation sys-
tem using baseline aliases has a coverage of 38.7%
and a precision of 59.5%. Meanwhile, the system
using the new aliases has a coverage of 44.6% and
a precision of 59.4%. With virtually the same pre-
cision, the new aliases are thus able to resolve 15%
more users. This provides an immediate benefit to
our existing Twitter research efforts.
Note that our alias lists can be viewed as clus-
ters of locations. In ongoing work, we are exploring
techniques based on discriminative learning to infer
alias lists using not only Clus information but also
Ngm and Tok features as in the previous sections.
8 Related Work
In both real-world and online social networks, ?peo-
ple socialize with people who are like them in terms
of gender, sexual orientation, age, race, education,
and religion? (Jernigan and Mistree, 2009). So-
cial media research has exploited this for two main
purposes: (1) to predict friendships based on user
properties, and (2) to predict user properties based
on friendships. Friendship prediction systems (e.g.
Facebook?s friend suggestion tool) use features such
as whether both people are computer science ma-
jors (Taskar et al, 2003) or whether both are at the
same location (Crandall et al, 2010; Sadilek et al,
2012). The inverse problem has been explored in the
prediction of a user?s location given the location of
their peers (Backstrom et al, 2010; Cho et al, 2011;
Sadilek et al, 2012). Jernigan and Mistree (2009)
predict a user?s sexuality based on the sexuality of
their Facebook friends, while Garera and Yarowsky
(2009) predict a user?s gender partly based on the
gender of their conversational partner. Jha and El-
hadad (2010) predict the cancer stage of users of
an online cancer discussion board; they derive com-
plementary information for prediction from both the
text a user generates and the cancer stage of the peo-
ple that a user interacts with.
The idea of clustering data in order to provide fea-
tures for supervised systems has been successfully
explored in a range of NLP tasks, including named-
entity-recognition (Miller et al, 2004; Lin and Wu,
2009; Ratinov and Roth, 2009), syntactic chunking
(Turian et al, 2010), and dependency parsing (Koo
et al, 2008; T?ckstr?m et al, 2012). In each case,
1017
the clusters are derived from the distribution of the
words or phrases in text, not from their communica-
tion pattern. It would be interesting to see whether
prior distributional clusters can be combined with
our communication-based clusters to achieve even
better performance. Indeed, there is evidence that
features derived from text can improve the predic-
tion of name ethnicity (Pervouchine et al, 2010).
There has been an explosion of work in recent
years in predicting user properties in social net-
works. Aside from the work mentioned above that
analyzes a user?s social network, a large amount
of work has focused on inferring user properties
based on the content they generate (e.g. Burger
and Henderson (2006), Schler et al (2006), Rao
et al (2010), Mukherjee and Liu (2010), Pennac-
chiotti and Popescu (2011), Burger et al (2011), Van
Durme (2012)).
9 Conclusion and Future Work
We presented a highly effective and readily repli-
cable algorithm for generating language resources
from Twitter communication patterns. We clustered
user attributes based on both the communication of
users with those attributes as well as substring sim-
ilarity. Systems using our clusters significantly out-
perform state-of-the-art algorithms on each of the
tasks investigated, and exceed human performance
on each task as well. The power and versatility of
our clusters is exemplified by the fact we reduce er-
ror by a larger margin on each of the non-Twitter
tasks than on any Twitter task itself.
Twitter provides a remarkably large sample and
effectively a partial census of much of the world?s
population, with associated metadata, descriptive
content and sentiment information. Our ability to
accurately assign numerous often unspecified prop-
erties such as race, gender, language and ethnicity to
such a large user sample substantially increases the
sociological insights and correlations one can derive
from such data.
References
Lars Backstrom, Eric Sun, and Cameron Marlow. 2010.
Find me if you can: improving geographical predic-
tion with social and spatial proximity. In Proc. WWW,
pages 61?70.
Shane Bergsma, Paul McNamee, Mossaab Bagdouri,
Clayton Fink, and Theresa Wilson. 2012. Language
identification for creating language-specific Twitter
collections. In Proceedings of the Second Workshop
on Language in Social Media, pages 65?74.
Aditya Bhargava and Grzegorz Kondrak. 2010. Lan-
guage identification of names with SVMs. In Proc.
HLT-NAACL, pages 693?696.
Peter F. Brown, Vincent J. Della Pietra, Peter V. de Souza,
Jennifer C. Lai, and Robert L. Mercer. 1992. Class-
based n-gram models of natural language. Computa-
tional Linguistics, 18(4):467?479.
John D. Burger and John C. Henderson. 2006. An ex-
ploration of observable features related to blogger age.
In Proc. AAAI Spring Symposium: Computational Ap-
proaches to Analyzing Weblogs, pages 15?20.
John D. Burger, John Henderson, George Kim, and Guido
Zarrella. 2011. Discriminating gender on Twitter. In
Proc. EMNLP, pages 1301?1309.
Simon Carter, Wouter Weerkamp, and Manos Tsagkias.
2013. Microblog Language Identification: Overcom-
ing the Limitations of Short, Unedited and Idiomatic
Text. Language Resources and Evaluation Journal.
(forthcoming).
Zhiyuan Cheng, James Caverlee, and Kyumin Lee. 2010.
You are where you tweet: a content-based approach
to geo-locating Twitter users. In Proc. CIKM, pages
759?768.
Eunjoon Cho, Seth A. Myers, and Jure Leskovec. 2011.
Friendship and mobility: user movement in location-
based social networks. In Proc. KDD, pages 1082?
1090.
Kenneth W. Church and Patrick Hanks. 1990. Word as-
sociation norms, mutual information, and lexicogra-
phy. Computational Linguistics, 16(1):22?29.
Corinna Cortes and Vladimir Vapnik. 1995. Support-
vector networks. Mach. Learn., 20(3):273?297.
David J. Crandall, Lars Backstrom, Dan Cosley, Sid-
dharth Suri, Daniel Huttenlocher, and Jon Kleinberg.
2010. Inferring social ties from geographic coinci-
dences. Proceedings of the National Academy of Sci-
ences, 107(52):22436?22441.
Jeffrey Dean and Sanjay Ghemawat. 2008. MapReduce:
simplified data processing on large clusters. Commun.
ACM, 51(1):107?113.
Inderjit S. Dhillon and Dharmendra S. Modha. 2001.
Concept decompositions for large sparse text data us-
ing clustering. Mach. Learn., 42(1-2):143?175.
Jacob Eisenstein, Brendan O?Connor, Noah A. Smith,
and Eric P. Xing. 2010. A latent variable model for
geographic lexical variation. In Proc. EMNLP, pages
1277?1287.
1018
Jacob Eisenstein, Noah A. Smith, and Eric P. Xing. 2011.
Discovering sociolinguistic associations with struc-
tured sparsity. In Proc. ACL, pages 1365?1374.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A li-
brary for large linear classification. J. Mach. Learn.
Res., 9:1871?1874.
Nikesh Garera and David Yarowsky. 2009. Modeling la-
tent biographic attributes in conversational genres. In
Proc. ACL-IJCNLP, pages 710?718.
Brent Hecht, Lichan Hong, Bongwon Suh, and Ed H. Chi.
2011. Tweets from Justin Bieber?s heart: the dynamics
of the location field in user profiles. In Proc. CHI,
pages 237?246.
Donald Hindle. 1990. Noun classification from
predicate-argument structures. In Proc. ACL, pages
268?275.
Carter Jernigan and Behram F. T. Mistree. 2009. Gaydar:
Facebook friendships expose sexual orientation. First
Monday, 14(10). [Online].
Mukund Jha and Noemie Elhadad. 2010. Cancer stage
prediction based on patient online discourse. In Proc.
2010 Workshop on Biomedical Natural Language Pro-
cessing, pages 64?71.
Stasinos Konstantopoulos. 2007. What?s in a name? In
Proc. Computational Phonology Workshop, RANLP.
Terry Koo, Xavier Carreras, and Michael Collins. 2008.
Simple semi-supervised dependency parsing. In Proc.
ACL-08: HLT, pages 595?603.
Dekang Lin and Xiaoyun Wu. 2009. Phrase clustering
for discriminative learning. In Proc. ACL-IJCNLP,
pages 1030??1038.
Dekang Lin. 1998. Automatic retrieval and clustering of
similar words. In Proc. Coling-ACL, pages 768?774.
Ariadna Font Llitjos and Alan W. Black. 2001. Knowl-
edge of language origin improves pronunciation accu-
racy of proper names. In Proceedings of EuroSpeech-
01, pages 1919?1922.
Scott Miller, Jethran Guinness, and Alex Zamanian.
2004. Name tagging with word clusters and discrimi-
native training. In Proc. HLT-NAACL, pages 337?342.
Arjun Mukherjee and Bing Liu. 2010. Improving gender
classification of blog authors. In Proc. EMNLP, pages
207?217.
Brendan O?Connor, Ramnath Balasubramanyan,
Bryan R. Routledge, and Noah A. Smith. 2010.
From tweets to polls: Linking text sentiment to public
opinion time series. In Proc. ICWSM, pages 122?129.
Michael Paul and Mark Dredze. 2011. You are what you
tweet: Analyzing Twitter for public health. In Proc.
ICWSM, pages 265?272.
Marco Pennacchiotti and Ana-Maria Popescu. 2011. A
machine learning approach to Twitter user classifica-
tion. In Proc. ICWSM, pages 281?288.
Fernando Pereira, Naftali Tishby, and Lillian Lee. 1993.
Distributional clustering of English words. In Proc.
ACL, pages 183?190.
Vladimir Pervouchine, Min Zhang, Ming Liu, and
Haizhou Li. 2010. Improving name origin recogni-
tion with context features and unlabelled data. In Col-
ing 2010: Posters, pages 972?978.
Delip Rao, David Yarowsky, Abhishek Shreevats, and
Manaswi Gupta. 2010. Classifying latent user at-
tributes in Twitter. In Proc. International Workshop on
Search and Mining User-Generated Contents, pages
37?44.
Lev Ratinov and Dan Roth. 2009. Design challenges and
misconceptions in named entity recognition. In Proc.
CoNLL, pages 147?155.
Ryan Rifkin and Aldebaro Klautau. 2004. In defense of
one-vs-all classification. J. Mach. Learn. Res., 5:101?
141.
Stephen Roller, Michael Speriosu, Sarat Rallapalli, Ben-
jamin Wing, and Jason Baldridge. 2012. Supervised
text-based geolocation using language models on an
adaptive grid. In Proc. EMNLP-CoNLL, pages 1500?
1510.
Adam Sadilek, Henry Kautz, and Jeffrey P. Bigham.
2012. Finding your friends and following them to
where you are. In Proc. WSDM, pages 723?732.
Jonathan Schler, Moshe Koppel, Shlomo Argamon, and
James W. Pennebaker. 2006. Effects of age and
gender on blogging. In Proc. AAAI Spring Sympo-
sium: Computational Approaches to Analyzing We-
blogs, pages 199?205.
Oscar T?ckstr?m, Ryan McDonald, and Jakob Uszkoreit.
2012. Cross-lingual word clusters for direct transfer
of linguistic structure. In Proc. NAACL-HLT, pages
477?487.
Ben Taskar, Ming-Fai Wong, Pieter Abbeel, and Daphne
Koller. 2003. Link prediction in relational data. In
Proc. NIPS, volume 15.
Erik Tromp and Mykola Pechenizkiy. 2011. Graph-
based n-gram language identication on short texts. In
Proc. 20th Machine Learning conference of Belgium
and The Netherlands, pages 27?34.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: A simple and general method
for semi-supervised learning. In Proc. ACL, pages
384?394.
Benjamin Van Durme. 2012. Streaming analysis of dis-
course participants. In Proc. EMNLP-CoNLL, pages
48?58.
Benjamin Wing and Jason Baldridge. 2011. Simple su-
pervised document geolocation with geodesic grids.
In Proc. ACL, pages 955?964.
1019
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 865?874,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Creating Robust Supervised Classifiers via Web-Scale N-gram Data
Shane Bergsma
University of Alberta
sbergsma@ualberta.ca
Emily Pitler
University of Pennsylvania
epitler@seas.upenn.edu
Dekang Lin
Google, Inc.
lindek@google.com
Abstract
In this paper, we systematically assess the
value of using web-scale N-gram data in
state-of-the-art supervised NLP classifiers.
We compare classifiers that include or ex-
clude features for the counts of various
N-grams, where the counts are obtained
from a web-scale auxiliary corpus. We
show that including N-gram count features
can advance the state-of-the-art accuracy
on standard data sets for adjective order-
ing, spelling correction, noun compound
bracketing, and verb part-of-speech dis-
ambiguation. More importantly, when op-
erating on new domains, or when labeled
training data is not plentiful, we show that
using web-scale N-gram features is essen-
tial for achieving robust performance.
1 Introduction
Many NLP systems use web-scale N-gram counts
(Keller and Lapata, 2003; Nakov and Hearst,
2005; Brants et al, 2007). Lapata and Keller
(2005) demonstrate good performance on eight
tasks using unsupervised web-based models. They
show web counts are superior to counts from a
large corpus. Bergsma et al (2009) propose un-
supervised and supervised systems that use counts
from Google?s N-gram corpus (Brants and Franz,
2006). Web-based models perform particularly
well on generation tasks, where systems choose
between competing sequences of output text (such
as different spellings), as opposed to analysis
tasks, where systems choose between abstract la-
bels (such as part-of-speech tags or parse trees).
In this work, we address two natural and related
questions which these previous studies leave open:
1. Is there a benefit in combining web-scale
counts with the features used in state-of-the-
art supervised approaches?
2. How well do web-based models perform on
new domains or when labeled data is scarce?
We address these questions on two generation
and two analysis tasks, using both existing N-gram
data and a novel web-scale N-gram corpus that
includes part-of-speech information (Section 2).
While previous work has combined web-scale fea-
tures with other features in specific classification
problems (Modjeska et al, 2003; Yang et al,
2005; Vadas and Curran, 2007b), we provide a
multi-task, multi-domain comparison.
Some may question why supervised approaches
are needed at all for generation problems. Why
not solely rely on direct evidence from a giant cor-
pus? For example, for the task of prenominal ad-
jective ordering (Section 3), a system that needs
to describe a ball that is both big and red can sim-
ply check that big red is more common on the web
than red big, and order the adjectives accordingly.
It is, however, suboptimal to only use N-gram
data. For example, ordering adjectives by direct
web evidence performs 7% worse than our best
supervised system (Section 3.2). No matter how
large the web becomes, there will always be plau-
sible constructions that never occur. For example,
there are currently no pages indexed by Google
with the preferred adjective ordering for bedrag-
gled 56-year-old [professor]. Also, in a particu-
lar domain, words may have a non-standard usage.
Systems trained on labeled data can learn the do-
main usage and leverage other regularities, such as
suffixes and transitivity for adjective ordering.
With these benefits, systems trained on labeled
data have become the dominant technology in aca-
demic NLP. There is a growing recognition, how-
ever, that these systems are highly domain de-
pendent. For example, parsers trained on anno-
tated newspaper text perform poorly on other gen-
res (Gildea, 2001). While many approaches have
adapted NLP systems to specific domains (Tsu-
ruoka et al, 2005; McClosky et al, 2006; Blitzer
865
et al, 2007; Daume? III, 2007; Rimell and Clark,
2008), these techniques assume the system knows
on which domain it is being used, and that it has
access to representative data in that domain. These
assumptions are unrealistic in many real-world sit-
uations; for example, when automatically process-
ing a heterogeneous collection of web pages. How
well do supervised and unsupervised NLP systems
perform when used uncustomized, out-of-the-box
on new domains, and how can we best design our
systems for robust open-domain performance?
Our results show that using web-scale N-gram
data in supervised systems advances the state-of-
the-art performance on standard analysis and gen-
eration tasks. More importantly, when operating
out-of-domain, or when labeled data is not plen-
tiful, using web-scale N-gram data not only helps
achieve good performance ? it is essential.
2 Experiments and Data
2.1 Experimental Design
We evaluate the benefit of N-gram data on multi-
class classification problems. For each task, we
have some labeled data indicating the correct out-
put for each example. We evaluate with accuracy:
the percentage of examples correctly classified in
test data. We use one in-domain and two out-of-
domain test sets for each task. Statistical signifi-
cance is assessed with McNemar?s test, p<0.01.
We provide results for unsupervised approaches
and the majority-class baseline for each task.
For our supervised approaches, we represent the
examples as feature vectors, and learn a classi-
fier on the training vectors. There are two fea-
ture classes: features that use N-grams (N-GM)
and those that do not (LEX). N-GM features are
real-valued features giving the log-count of a par-
ticular N-gram in the auxiliary web corpus. LEX
features are binary features that indicate the pres-
ence or absence of a particular string at a given po-
sition in the input. The name LEX emphasizes that
they identify specific lexical items. The instantia-
tions of both types of features depend on the task
and are described in the corresponding sections.
Each classifier is a linear Support Vector Ma-
chine (SVM), trained using LIBLINEAR (Fan et al,
2008) on the standard domain. We use the one-vs-
all strategy when there are more than two classes
(in Section 4). We plot learning curves to mea-
sure the accuracy of the classifier when the num-
ber of labeled training examples varies. The size
of the N-gram data and its counts remain constant.
We always optimize the SVM?s (L2) regulariza-
tion parameter on the in-domain development set.
We present results with L2-SVM, but achieve sim-
ilar results with L1-SVM and logistic regression.
2.2 Tasks and Labeled Data
We study two generation tasks: prenominal ad-
jective ordering (Section 3) and context-sensitive
spelling correction (Section 4), followed by two
analysis tasks: noun compound bracketing (Sec-
tion 5) and verb part-of-speech disambiguation
(Section 6). In each section, we provide refer-
ences to the origin of the labeled data. For the
out-of-domain Gutenberg and Medline data used
in Sections 3 and 4, we generate examples our-
selves.1 We chose Gutenberg and Medline in order
to provide challenging, distinct domains from our
training corpora. Our Gutenberg corpus consists
of out-of-copyright books, automatically down-
loaded from the Project Gutenberg website.2 The
Medline data consists of a large collection of on-
line biomedical abstracts. We describe how la-
beled adjective and spelling examples are created
from these corpora in the corresponding sections.
2.3 Web-Scale Auxiliary Data
The most widely-used N-gram corpus is the
Google 5-gram Corpus (Brants and Franz, 2006).
For our tasks, we also use Google V2: a new
N-gram corpus (also with N-grams of length one-
to-five) that we created from the same one-trillion-
word snapshot of the web as the Google 5-gram
Corpus, but with several enhancements. These in-
clude: 1) Reducing noise by removing duplicate
sentences and sentences with a high proportion
of non-alphanumeric characters (together filtering
about 80% of the source data), 2) pre-converting
all digits to the 0 character to reduce sparsity for
numeric expressions, and 3) including the part-of-
speech (POS) tag distribution for each N-gram.
The source data was automatically tagged with
TnT (Brants, 2000), using the Penn Treebank tag
set. Lin et al (2010) provide more details on the
1http://webdocs.cs.ualberta.ca/?bergsma/Robust/
provides our Gutenberg corpus, a link to Medline, and also
the generated examples for both Gutenberg and Medline.
2
www.gutenberg.org. All books just released in 2009 and
thus unlikely to occur in the source data for our N-gram cor-
pus (from 2006). Of course, with removal of sentence dupli-
cates and also N-gram thresholding, the possible presence of
a test sentence in the massive source data is unlikely to affect
results. Carlson et al (2008) reach a similar conclusion.
866
N-gram data and N-gram search tools.
The third enhancement is especially relevant
here, as we can use the POS distribution to collect
counts for N-grams of mixed words and tags. For
example, we have developed an N-gram search en-
gine that can count how often the adjective un-
precedented precedes another adjective in our web
corpus (113K times) and how often it follows one
(11K times). Thus, even if we haven?t seen a par-
ticular adjective pair directly, we can use the posi-
tional preferences of each adjective to order them.
Early web-based models used search engines to
collect N-gram counts, and thus could not use cap-
italization, punctuation, and annotations such as
part-of-speech (Kilgarriff and Grefenstette, 2003).
Using a POS-tagged web corpus goes a long way
to addressing earlier criticisms of web-based NLP.
3 Prenominal Adjective Ordering
Prenominal adjective ordering strongly affects text
readability. For example, while the unprecedented
statistical revolution is fluent, the statistical un-
precedented revolution is not. Many NLP systems
need to handle adjective ordering robustly. In ma-
chine translation, if a noun has two adjective mod-
ifiers, they must be ordered correctly in the tar-
get language. Adjective ordering is also needed
in Natural Language Generation systems that pro-
duce information from databases; for example, to
convey information (in sentences) about medical
patients (Shaw and Hatzivassiloglou, 1999).
We focus on the task of ordering a pair of adjec-
tives independently of the noun they modify and
achieve good performance in this setting. Follow-
ing the set-up of Malouf (2000), we experiment
on the 263K adjective pairs Malouf extracted from
the British National Corpus (BNC). We use 90%
of pairs for training, 5% for testing, and 5% for
development. This forms our in-domain data.3
We create out-of-domain examples by tokeniz-
ing Medline and Gutenberg (Section 2.2), then
POS-tagging them with CRFTagger (Phan, 2006).
We create examples from all sequences of two ad-
jectives followed by a noun. Like Malouf (2000),
we assume that edited text has adjectives ordered
fluently. We extract 13K and 9.1K out-of-domain
pairs from Gutenberg and Medline, respectively.4
3BNC is not a domain per se (rather a balanced corpus),
but has a style and vocabulary distinct from our OOD data.
4Like Malouf (2000), we convert our pairs to lower-case.
Since the N-gram data includes case, we merge counts from
the upper and lower case combinations.
The input to the system is a pair of adjectives,
(a1, a2), ordered alphabetically. The task is to
classify this order as correct (the positive class) or
incorrect (the negative class). Since both classes
are equally likely, the majority-class baseline is
around 50% on each of the three test sets.
3.1 Supervised Adjective Ordering
3.1.1 LEX features
Our adjective ordering model with LEX features is
a novel contribution of this paper.
We begin with two features for each pair: an in-
dicator feature for a1, which gets a feature value of
+1, and an indicator feature for a2, which gets a
feature value of ?1. The parameters of the model
are therefore weights on specific adjectives. The
higher the weight on an adjective, the more it is
preferred in the first position of a pair. If the alpha-
betic ordering is correct, the weight on a1 should
be higher than the weight on a2, so that the clas-
sifier returns a positive score. If the reverse order-
ing is preferred, a2 should receive a higher weight.
Training the model in this setting is a matter of as-
signing weights to all the observed adjectives such
that the training pairs are maximally ordered cor-
rectly. The feature weights thus implicitly produce
a linear ordering of all observed adjectives. The
examples can also be regarded as rank constraints
in a discriminative ranker (Joachims, 2002). Tran-
sitivity is achieved naturally in that if we correctly
order pairs a ? b and b ? c in the training set,
then a ? c by virtue of the weights on a and c.
While exploiting transitivity has been shown
to improve adjective ordering, there are many
conflicting pairs that make a strict linear order-
ing of adjectives impossible (Malouf, 2000). We
therefore provide an indicator feature for the pair
a1a2, so the classifier can memorize exceptions
to the linear ordering, breaking strict order tran-
sitivity. Our classifier thus operates along the lines
of rankers in the preference-based setting as de-
scribed in Ailon and Mohri (2008).
Finally, we also have features for all suffixes of
length 1-to-4 letters, as these encode useful infor-
mation about adjective class (Malouf, 2000). Like
the adjective features, the suffix features receive a
value of +1 for adjectives in the first position and
?1 for those in the second.
3.1.2 N-GM features
Lapata and Keller (2005) propose a web-based
approach to adjective ordering: take the most-
867
System IN O1 O2
Malouf (2000) 91.5 65.6 71.6
web c(a1, a2) vs. c(a2, a1) 87.1 83.7 86.0
SVM with N-GM features 90.0 85.8 88.5
SVM with LEX features 93.0 70.0 73.9
SVM with N-GM + LEX 93.7 83.6 85.4
Table 1: Adjective ordering accuracy (%). SVM
and Malouf (2000) trained on BNC, tested on
BNC (IN), Gutenberg (O1), and Medline (O2).
frequent order of the words on the web, c(a1, a2)
vs. c(a2, a1). We adopt this as our unsupervised
approach. We merge the counts for the adjectives
occurring contiguously and separated by a comma.
These are indubitably the most important N-GM
features; we include them but also other, tag-based
counts from Google V2. Raw counts include cases
where one of the adjectives is not used as a mod-
ifier: ?the special present was? vs. ?the present
special issue.? We include log-counts for the
following, more-targeted patterns:5 c(a1 a2 N.*),
c(a2 a1 N.*), c(DT a1 a2 N.*), c(DT a2 a1 N.*).
We also include features for the log-counts of
each adjective preceded or followed by a word
matching an adjective-tag: c(a1 J.*), c(J.* a1),
c(a2 J.*), c(J.* a2). These assess the positional
preferences of each adjective. Finally, we include
the log-frequency of each adjective. The more fre-
quent adjective occurs first 57% of the time.
As in all tasks, the counts are features in a clas-
sifier, so the importance of the different patterns is
weighted discriminatively during training.
3.2 Adjective Ordering Results
In-domain, with both feature classes, we set a
strong new standard on this data: 93.7% accuracy
for the N-GM+LEX system (Table 1). We trained
and tested Malouf (2000)?s program on our data;
our LEX classifier, which also uses no auxiliary
corpus, makes 18% fewer errors than Malouf?s
system. Our web-based N-GM model is also su-
perior to the direct evidence web-based approach
of Lapata and Keller (2005), scoring 90.0% vs.
87.1% accuracy. These results show the benefit
of our new lexicalized and web-based features.
Figure 1 gives the in-domain learning curve.
With fewer training examples, the systems with
N-GM features strongly outperform the LEX-only
system. Note that with tens of thousands of test
5In this notation, capital letters (and regular expressions)
are matched against tags while a1 and a2 match words.
 60
 65
 70
 75
 80
 85
 90
 95
 100
1e51e41e3100
Ac
cu
ra
cy
 (%
)
Number of training examples
N-GM+LEX
N-GM
LEX
Figure 1: In-domain learning curve of adjective
ordering classifiers on BNC.
 60
 65
 70
 75
 80
 85
 90
 95
 100
1e51e41e3100
Ac
cu
ra
cy
 (%
)
Number of training examples
N-GM+LEX
N-GM
LEX
Figure 2: Out-of-domain learning curve of adjec-
tive ordering classifiers on Gutenberg.
examples, all differences are highly significant.
Out-of-domain, LEX?s accuracy drops a shock-
ing 23% on Gutenberg and 19% on Medline (Ta-
ble 1). Malouf (2000)?s system fares even worse.
The overlap between training and test pairs helps
explain. While 59% of the BNC test pairs were
seen in the training corpus, only 25% of Gutenberg
and 18% of Medline pairs were seen in training.
While other ordering models have also achieved
?very poor results? out-of-domain (Mitchell,
2009), we expected our expanded set of LEX fea-
tures to provide good generalization on new data.
Instead, LEX is very unreliable on new domains.
N-GM features do not rely on specific pairs in
training data, and thus remain fairly robust cross-
domain. Across the three test sets, 84-89% of
examples had the correct ordering appear at least
once on the web. On new domains, the learned
N-GM system maintains an advantage over the un-
supervised c(a1, a2) vs. c(a2, a1), but the differ-
ence is reduced. Note that training with 10-fold
868
cross validation, the N-GM system can achieve up
to 87.5% on Gutenberg (90.0% for N-GM + LEX).
The learning curve showing performance on
Gutenberg (but still training on BNC) is particu-
larly instructive (Figure 2, performance on Med-
line is very similar). The LEX system performs
much worse than the web-based models across
all training sizes. For our top in-domain sys-
tem, N-GM + LEX, as you add more labeled ex-
amples, performance begins decreasing out-of-
domain. The system disregards the robust N-gram
counts as it is more and more confident in the LEX
features, and it suffers the consequences.
4 Context-Sensitive Spelling Correction
We now turn to the generation problem of context-
sensitive spelling correction. For every occurrence
of a word in a pre-defined set of confusable words
(like peace and piece), the system must select the
most likely word from the set, flagging possible
usage errors when the predicted word disagrees
with the original. Contextual spell checkers are
one of the most widely used NLP technologies,
reaching millions of users via compressed N-gram
models in Microsoft Office (Church et al, 2007).
Our in-domain examples are from the New York
Times (NYT) portion of Gigaword, from Bergsma
et al (2009). They include the 5 confusion sets
where accuracy was below 90% in Golding and
Roth (1999). There are 100K training, 10K devel-
opment, and 10K test examples for each confusion
set. Our results are averages across confusion sets.
Out-of-domain examples are again drawn from
Gutenberg and Medline. We extract all instances
of words that are in one of our confusion sets,
along with surrounding context. By assuming the
extracted instances represent correct usage, we la-
bel 7.8K and 56K out-of-domain test examples for
Gutenberg and Medline, respectively.
We test three unsupervised systems: 1) Lapata
and Keller (2005) use one token of context on the
left and one on the right, and output the candidate
from the confusion set that occurs most frequently
in this pattern. 2) Bergsma et al (2009) measure
the frequency of the candidates in all the 3-to-5-
gram patterns that span the confusable word. For
each candidate, they sum the log-counts of all pat-
terns filled with the candidate, and output the can-
didate with the highest total. 3) The baseline pre-
dicts the most frequent member of each confusion
set, based on frequencies in the NYT training data.
System IN O1 O2
Baseline 66.9 44.6 60.6
Lapata and Keller (2005) 88.4 78.0 87.4
Bergsma et al (2009) 94.8 87.7 94.2
SVM with N-GM features 95.7 92.1 93.9
SVM with LEX features 95.2 85.8 91.0
SVM with N-GM + LEX 96.5 91.9 94.8
Table 2: Spelling correction accuracy (%). SVM
trained on NYT, tested on NYT (IN) and out-of-
domain Gutenberg (O1) and Medline (O2).
 70
 75
 80
 85
 90
 95
 100
1e51e41e3100
Ac
cu
ra
cy
 (%
)
Number of training examples
N-GM+LEX
N-GM
LEX
Figure 3: In-domain learning curve of spelling
correction classifiers on NYT.
4.1 Supervised Spelling Correction
Our LEX features are typical disambiguation fea-
tures that flag specific aspects of the context. We
have features for the words at all positions in
a 9-word window (called collocation features by
Golding and Roth (1999)), plus indicators for a
particular word preceding or following the con-
fusable word. We also include indicators for all
N-grams, and their position, in a 9-word window.
For N-GM count features, we follow Bergsma
et al (2009). We include the log-counts of all
N-grams that span the confusable word, with each
word in the confusion set filling the N-gram pat-
tern. These features do not use part-of-speech.
Following Bergsma et al (2009), we get N-gram
counts using the original Google N-gram Corpus.
While neither our LEX nor N-GM features are
novel on their own, they have, perhaps surpris-
ingly, not yet been evaluated in a single model.
4.2 Spelling Correction Results
The N-GM features outperform the LEX features,
95.7% vs. 95.2% (Table 2). Together, they
achieve a very strong 96.5% in-domain accuracy.
869
This is 2% higher than the best unsupervised ap-
proach (Bergsma et al, 2009). Web-based models
again perform well across a range of training data
sizes (Figure 3).
The error rate of LEX nearly triples on Guten-
berg and almost doubles on Medline (Table 2). Re-
moving N-GM features from the N-GM + LEX sys-
tem, errors increase around 75% on both Guten-
berg and Medline. The LEX features provide no
help to the combined system on Gutenberg, while
they do help significantly on Medline. Note the
learning curves for N-GM+LEX on Gutenberg and
Medline (not shown) do not display the decrease
that we observed in adjective ordering (Figure 2).
Both the baseline and LEX perform poorly on
Gutenberg. The baseline predicts the majority
class from NYT, but it?s not always the majority
class in Gutenberg. For example, while in NYT
site occurs 87% of the time for the (cite, sight,
site) confusion set, sight occurs 90% of the time in
Gutenberg. The LEX classifier exploits this bias as
it is regularized toward a more economical model,
but the bias does not transfer to the new domain.
5 Noun Compound Bracketing
About 70% of web queries are noun phrases (Barr
et al, 2008) and methods that can reliably parse
these phrases are of great interest in NLP. For
example, a web query for zebra hair straightener
should be bracketed as (zebra (hair straightener)),
a stylish hair straightener with zebra print, rather
than ((zebra hair) straightener), a useless product
since the fur of zebras is already quite straight.
The noun compound (NC) bracketing task is
usually cast as a decision whether a 3-word NC
has a left or right bracketing. Most approaches are
unsupervised, using a large corpus to compare the
statistical association between word pairs in the
NC. The adjacency model (Marcus, 1980) pro-
poses a left bracketing if the association between
words one and two is higher than between two
and three. The dependency model (Lauer, 1995a)
compares one-two vs. one-three. We include de-
pendency model results using PMI as the associ-
ation measure; results were lower with the adja-
cency model.
As in-domain data, we use Vadas and Curran
(2007a)?s Wall-Street Journal (WSJ) data, an ex-
tension of the Treebank (which originally left NPs
flat). We extract all sequences of three consec-
utive common nouns, generating 1983 examples
System IN O1 O2
Baseline 70.5 66.8 84.1
Dependency model 74.7 82.8 84.4
SVM with N-GM features 89.5 81.6 86.2
SVM with LEX features 81.1 70.9 79.0
SVM with N-GM + LEX 91.6 81.6 87.4
Table 3: NC-bracketing accuracy (%). SVM
trained on WSJ, tested on WSJ (IN) and out-of-
domain Grolier (O1) and Medline (O2).
 60
 65
 70
 75
 80
 85
 90
 95
 100
1e310010
Ac
cu
ra
cy
 (%
)
Number of labeled examples
N-GM+LEX
N-GM
LEX
Figure 4: In-domain NC-bracketer learning curve
from sections 0-22 of the Treebank as training, 72
from section 24 for development and 95 from sec-
tion 23 as a test set. As out-of-domain data, we
use 244 NCs from Grolier Encyclopedia (Lauer,
1995a) and 429 NCs from Medline (Nakov, 2007).
The majority class baseline is left-bracketing.
5.1 Supervised Noun Bracketing
Our LEX features indicate the specific noun at
each position in the compound, plus the three pairs
of nouns and the full noun triple. We also add fea-
tures for the capitalization pattern of the sequence.
N-GM features give the log-count of all subsets
of the compound. Counts are from Google V2.
Following Nakov and Hearst (2005), we also in-
clude counts of noun pairs collapsed into a single
token; if a pair occurs often on the web as a single
unit, it strongly indicates the pair is a constituent.
Vadas and Curran (2007a) use simpler features,
e.g. they do not use collapsed pair counts. They
achieve 89.9% in-domain on WSJ and 80.7% on
Grolier. Vadas and Curran (2007b) use compara-
ble features to ours, but do not test out-of-domain.
5.2 Noun Compound Bracketing Results
N-GM systems perform much better on this task
(Table 3). N-GM+LEX is statistically significantly
870
better than LEX on all sets. In-domain, errors
more than double without N-GM features. LEX
performs poorly here because there are far fewer
training examples. The learning curve (Figure 4)
looks much like earlier in-domain curves (Fig-
ures 1 and 3), but truncated before LEX becomes
competitive. The absence of a sufficient amount of
labeled data explains why NC-bracketing is gen-
erally regarded as a task where corpus counts are
crucial.
All web-based models (including the depen-
dency model) exceed 81.5% on Grolier, which
is the level of human agreement (Lauer, 1995b).
N-GM + LEX is highest on Medline, and close
to the 88% human agreement (Nakov and Hearst,
2005). Out-of-domain, the LEX approach per-
forms very poorly, close to or below the base-
line accuracy. With little training data and cross-
domain usage, N-gram features are essential.
6 Verb Part-of-Speech Disambiguation
Our final task is POS-tagging. We focus on one
frequent and difficult tagging decision: the distinc-
tion between a past-tense verb (VBD) and a past
participle (VBN). For example, in the troops sta-
tioned in Iraq, the verb stationed is a VBN; troops
is the head of the phrase. On the other hand, for
the troops vacationed in Iraq, the verb vacationed
is a VBD and also the head. Some verbs make the
distinction explicit (eat has VBD ate, VBN eaten),
but most require context for resolution.
Conflating VBN/VBD is damaging because it af-
fects downstream parsers and semantic role la-
belers. The task is difficult because nearby POS
tags can be identical in both cases. When the
verb follows a noun, tag assignment can hinge on
world-knowledge, i.e., the global lexical relation
between the noun and verb (E.g., troops tends to
be the object of stationed but the subject of vaca-
tioned).6 Web-scale N-gram data might help im-
prove the VBN/VBD distinction by providing rela-
tional evidence, even if the verb, noun, or verb-
noun pair were not observed in training data.
We extract nouns followed by a VBN/VBD in the
WSJ portion of the Treebank (Marcus et al, 1993),
getting 23K training, 1091 development and 1130
test examples from sections 2-22, 24, and 23, re-
spectively. For out-of-domain data, we get 21K
6HMM-style taggers, like the fast TnT tagger used on our
web corpus, do not use bilexical features, and so perform es-
pecially poorly on these cases. One motivation for our work
was to develop a fast post-processor to fix VBN/VBD errors.
examples from the Brown portion of the Treebank
and 6296 examples from tagged Medline abstracts
in the PennBioIE corpus (Kulick et al, 2004).
The majority class baseline is to choose VBD.
6.1 Supervised Verb Disambiguation
There are two orthogonal sources of information
for predicting VBN/VBD: 1) the noun-verb pair,
and 2) the context around the pair. Both N-GM
and LEX features encode both these sources.
6.1.1 LEX features
For 1), we use indicators for the noun and verb,
the noun-verb pair, whether the verb is on an in-
house list of said-verb (like warned, announced,
etc.), whether the noun is capitalized and whether
it?s upper-case. Note that in training data, 97.3%
of capitalized nouns are followed by a VBD and
98.5% of said-verbs are VBDs. For 2), we provide
indicator features for the words before the noun
and after the verb.
6.1.2 N-GM features
For 1), we characterize a noun-verb relation via
features for the pair?s distribution in Google V2.
Characterizing a word by its distribution has a
long history in NLP; we apply similar techniques
to relations, like Turney (2006), but with a larger
corpus and richer annotations. We extract the 20
most-frequent N-grams that contain both the noun
and the verb in the pair. For each of these, we con-
vert the tokens to POS-tags, except for tokens that
are among the most frequent 100 unigrams in our
corpus, which we include in word form. We mask
the noun of interest as N and the verb of interest
as V . This converted N-gram is the feature label.
The value is the pattern?s log-count. A high count
for patterns like (N that V), (N have V) suggests
the relation is a VBD, while patterns (N that were
V), (N V by), (V some N) indicate a VBN. As al-
ways, the classifier learns the association between
patterns and classes.
For 2), we use counts for the verb?s context co-
occurring with a VBD or VBN tag. E.g., we see
whether VBD cases like troops ate or VBN cases
like troops eaten are more frequent. Although our
corpus contains many VBN/VBD errors, we hope
the errors are random enough for aggregate counts
to be useful. The context is an N-gram spanning
the VBN/VBD. We have log-count features for all
five such N-grams in the (previous-word, noun,
verb, next-word) quadruple. The log-count is in-
871
System IN O1 O2
Baseline 89.2 85.2 79.6
ContextSum 92.5 91.1 90.4
SVM with N-GM features 96.1 93.4 93.8
SVM with LEX features 95.8 93.4 93.0
SVM with N-GM + LEX 96.4 93.5 94.0
Table 4: Verb-POS-disambiguation accuracy (%)
trained on WSJ, tested on WSJ (IN) and out-of-
domain Brown (O1) and Medline (O2).
 80
 85
 90
 95
 100
1e41e3100
Ac
cu
ra
cy
 (%
)
Number of training examples
N-GM (N,V+context)
LEX (N,V+context)
N-GM (N,V)
LEX (N,V)
Figure 5: Out-of-domain learning curve of verb
disambiguation classifiers on Medline.
dexed by the position and length of the N-gram.
We include separate count features for contexts
matching the specific noun and for when the noun
token can match any word tagged as a noun.
ContextSum: We use these context counts in an
unsupervised system, ContextSum. Analogously
to Bergsma et al (2009), we separately sum the
log-counts for all contexts filled with VBD and
then VBN, outputting the tag with the higher total.
6.2 Verb POS Disambiguation Results
As in all tasks, N-GM+LEX has the best in-domain
accuracy (96.4%, Table 4). Out-of-domain, when
N-grams are excluded, errors only increase around
14% on Medline and 2% on Brown (the differ-
ences are not statistically significant). Why? Fig-
ure 5, the learning curve for performance on Med-
line, suggests some reasons. We omit N-GM+LEX
from Figure 5 as it closely follows N-GM.
Recall that we grouped the features into two
views: 1) noun-verb (N,V) and 2) context. If we
use just (N,V) features, we do see a large drop out-
of-domain: LEX (N,V) lags N-GM (N,V) even us-
ing all the training examples. The same is true us-
ing only context features (not shown). Using both
views, the results are closer: 93.8% for N-GM and
93.0% for LEX. With two views of an example,
LEX is more likely to have domain-neutral fea-
tures to draw on. Data sparsity is reduced.
Also, the Treebank provides an atypical num-
ber of labeled examples for analysis tasks. In a
more typical situation with less labeled examples,
N-GM strongly dominates LEX, even when two
views are used. E.g., with 2285 training exam-
ples, N-GM+LEX is statistically significantly bet-
ter than LEX on both out-of-domain sets.
All systems, however, perform log-linearly with
training size. In other tasks we only had a handful
of N-GM features; here there are 21K features for
the distributional patterns of N,V pairs. Reducing
this feature space by pruning or performing trans-
formations may improve accuracy in and out-of-
domain.
7 Discussion and Future Work
Of all classifiers, LEX performs worst on all cross-
domain tasks. Clearly, many of the regularities
that a typical classifier exploits in one domain do
not transfer to new genres. N-GM features, how-
ever, do not depend directly on training examples,
and thus work better cross-domain. Of course, us-
ing web-scale N-grams is not the only way to cre-
ate robust classifiers. Counts from any large auxil-
iary corpus may also help, but web counts should
help more (Lapata and Keller, 2005). Section 6.2
suggests that another way to mitigate domain-
dependence is having multiple feature views.
Banko and Brill (2001) argue ?a logical next
step for the research community would be to di-
rect efforts towards increasing the size of anno-
tated training collections.? Assuming we really do
want systems that operate beyond the specific do-
mains on which they are trained, the community
also needs to identify which systems behave as in
Figure 2, where the accuracy of the best in-domain
system actually decreases with more training ex-
amples. Our results suggest better features, such
as web pattern counts, may help more than ex-
panding training data. Also, systems using web-
scale unlabeled data will improve automatically as
the web expands, without annotation effort.
In some sense, using web counts as features
is a form of domain adaptation: adapting a web
model to the training domain. How do we ensure
these features are adapted well and not used in
domain-specific ways (especially with many fea-
tures to adapt, as in Section 6)? One option may
872
be to regularize the classifier specifically for out-
of-domain accuracy. We found that adjusting the
SVM misclassification penalty (for more regular-
ization) can help or hurt out-of-domain. Other
regularizations are possible. In each task, there
are domain-neutral unsupervised approaches. We
could encode these systems as linear classifiers
with corresponding weights. Rather than a typical
SVM that minimizes the weight-norm ||w|| (plus
the slacks), we could regularize toward domain-
neutral weights. This regularization could be opti-
mized on creative splits of the training data.
8 Conclusion
We presented results on tasks spanning a range of
NLP research: generation, disambiguation, pars-
ing and tagging. Using web-scale N-gram data
improves accuracy on each task. When less train-
ing data is used, or when the system is used on a
different domain, N-gram features greatly improve
performance. Since most supervised NLP systems
do not use web-scale counts, further cross-domain
evaluation may reveal some very brittle systems.
Continued effort in new domains should be a pri-
ority for the community going forward.
Acknowledgments
We gratefully acknowledge the Center for Lan-
guage and Speech Processing at Johns Hopkins
University for hosting the workshop at which part
of this research was conducted.
References
Nir Ailon and Mehryar Mohri. 2008. An efficient re-
duction of ranking to classification. In COLT.
Michele Banko and Eric Brill. 2001. Scaling to very
very large corpora for natural language disambigua-
tion. In ACL.
Cory Barr, Rosie Jones, and Moira Regelson. 2008.
The linguistic structure of English web-search
queries. In EMNLP.
Shane Bergsma, Dekang Lin, and Randy Goebel.
2009. Web-scale N-gram models for lexical disam-
biguation. In IJCAI.
John Blitzer, Mark Dredze, and Fernando Pereira.
2007. Biographies, bollywood, boom-boxes and
blenders: Domain adaptation for sentiment classi-
fication. In ACL.
Thorsten Brants and Alex Franz. 2006. The Google
Web 1T 5-gram Corpus Version 1.1. LDC2006T13.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Large language mod-
els in machine translation. In EMNLP.
Thorsten Brants. 2000. TnT ? a statistical part-of-
speech tagger. In ANLP.
Andrew Carlson, Tom M. Mitchell, and Ian Fette.
2008. Data analysis project: Leveraging massive
textual corpora using n-gram statistics. Technial Re-
port CMU-ML-08-107.
Kenneth Church, Ted Hart, and Jianfeng Gao. 2007.
Compressing trigram language models with Golomb
coding. In EMNLP-CoNLL.
Hal Daume? III. 2007. Frustratingly easy domain adap-
tation. In ACL.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A library for large linear classification. Journal of
Machine Learning Research, 9.
Dan Gildea. 2001. Corpus variation and parser perfor-
mance. In EMNLP.
Andrew R. Golding and Dan Roth. 1999. A Winnow-
based approach to context-sensitive spelling correc-
tion. Machine Learning, 34(1-3):107?130.
Thorsten Joachims. 2002. Optimizing search engines
using clickthrough data. In KDD.
Frank Keller and Mirella Lapata. 2003. Using the web
to obtain frequencies for unseen bigrams. Computa-
tional Linguistics, 29(3):459?484.
Adam Kilgarriff and Gregory Grefenstette. 2003. In-
troduction to the special issue on the Web as corpus.
Computational Linguistics, 29(3):333?347.
Seth Kulick, Ann Bies, Mark Liberman, Mark Mandel,
Ryan McDonald, Martha Palmer, Andrew Schein,
Lyle Ungar, Scott Winters, and Pete White. 2004.
Integrated annotation for biomedical information ex-
traction. In BioLINK 2004: Linking Biological Lit-
erature, Ontologies and Databases.
Mirella Lapata and Frank Keller. 2005. Web-based
models for natural language processing. ACM
Transactions on Speech and Language Processing,
2(1):1?31.
Mark Lauer. 1995a. Corpus statistics meet the noun
compound: Some empirical results. In ACL.
Mark Lauer. 1995b. Designing Statistical Language
Learners: Experiments on Compound Nouns. Ph.D.
thesis, Macquarie University.
Dekang Lin, Kenneth Church, Heng Ji, Satoshi Sekine,
David Yarowsky, Shane Bergsma, Kailash Patil,
Emily Pitler, Rachel Lathbury, Vikram Rao, Kapil
Dalwani, and Sushant Narsale. 2010. New tools for
web-scale N-grams. In LREC.
873
Robert Malouf. 2000. The order of prenominal adjec-
tives in natural language generation. In ACL.
Mitchell P. Marcus, Beatrice Santorini, and Mary
Marcinkiewicz. 1993. Building a large annotated
corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2):313?330.
Mitchell P. Marcus. 1980. Theory of Syntactic Recog-
nition for Natural Languages. MIT Press, Cam-
bridge, MA, USA.
David McClosky, Eugene Charniak, and Mark John-
son. 2006. Reranking and self-training for parser
adaptation. In COLING-ACL.
Margaret Mitchell. 2009. Class-based ordering of
prenominal modifiers. In 12th European Workshop
on Natural Language Generation.
Natalia N. Modjeska, Katja Markert, and Malvina Nis-
sim. 2003. Using the Web in machine learning for
other-anaphora resolution. In EMNLP.
Preslav Nakov and Marti Hearst. 2005. Search engine
statistics beyond the n-gram: Application to noun
compound bracketing. In CoNLL.
Preslav Ivanov Nakov. 2007. Using the Web as an Im-
plicit Training Set: Application to Noun Compound
Syntax and Semantics. Ph.D. thesis, University of
California, Berkeley.
Xuan-Hieu Phan. 2006. CRFTagger: CRF English
POS Tagger. crftagger.sourceforge.net.
Laura Rimell and Stephen Clark. 2008. Adapting a
lexicalized-grammar parser to contrasting domains.
In EMNLP.
James Shaw and Vasileios Hatzivassiloglou. 1999. Or-
dering among premodifiers. In ACL.
Yoshimasa Tsuruoka, Yuka Tateishi, Jin-Dong Kim,
Tomoko Ohta, John McNaught, Sophia Ananiadou,
and Jun?ichi Tsujii. 2005. Developing a robust part-
of-speech tagger for biomedical text. In Advances in
Informatics.
Peter D. Turney. 2006. Similarity of semantic rela-
tions. Computational Linguistics, 32(3):379?416.
David Vadas and James R. Curran. 2007a. Adding
noun phrase structure to the Penn Treebank. In ACL.
David Vadas and James R. Curran. 2007b. Large-scale
supervised models for noun phrase bracketing. In
PACLING.
Xiaofeng Yang, Jian Su, and Chew Lim Tan. 2005.
Improving pronoun resolution using statistics-based
semantic compatibility information. In ACL.
874
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1346?1355,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Using Large Monolingual and Bilingual Corpora to
Improve Coordination Disambiguation
Shane Bergsma, David Yarowsky, Kenneth Church
Deptartment of Computer Science and Human Language Technology Center of Excellence
Johns Hopkins University
sbergsma@jhu.edu, yarowsky@cs.jhu.edu, kenneth.church@jhu.edu
Abstract
Resolving coordination ambiguity is a clas-
sic hard problem. This paper looks at co-
ordination disambiguation in complex noun
phrases (NPs). Parsers trained on the Penn
Treebank are reporting impressive numbers
these days, but they don?t do very well on this
problem (79%). We explore systems trained
using three types of corpora: (1) annotated
(e.g. the Penn Treebank), (2) bitexts (e.g. Eu-
roparl), and (3) unannotated monolingual (e.g.
Google N-grams). Size matters: (1) is a mil-
lion words, (2) is potentially billions of words
and (3) is potentially trillions of words. The
unannotated monolingual data is helpful when
the ambiguity can be resolved through associ-
ations among the lexical items. The bilingual
data is helpful when the ambiguity can be re-
solved by the order of words in the translation.
We train separate classifiers with monolingual
and bilingual features and iteratively improve
them via co-training. The co-trained classifier
achieves close to 96% accuracy on Treebank
data and makes 20% fewer errors than a su-
pervised system trained with Treebank anno-
tations.
1 Introduction
Determining which words are being linked by a co-
ordinating conjunction is a classic hard problem.
Consider the pair:
+ellipsis rocket\w1 and mortar\w2 attacks\h
?ellipsis asbestos\w1 and polyvinyl\w2 chloride\h
+ellipsis is about both rocket attacks and mortar at-
tacks, unlike ?ellipsis which is not about asbestos
chloride. We use h to refer to the head of the phrase,
and w1 and w2 to refer to the other two lexical items.
Natural Language Processing applications need to
recognize NP ellipsis in order to make sense of new
sentences. For example, if an Internet search en-
gine is given the phrase rocket attacks as a query, it
should rank documents containing rocket and mor-
tar attacks highly, even though rocket and attacks
are not contiguous in the document. Furthermore,
NPs with ellipsis often require a distinct type of re-
ordering when translated into a foreign language.
Since coordination is both complex and produc-
tive, parsers and machine translation (MT) systems
cannot simply memorize the analysis of coordinate
phrases from training text. We propose an approach
to recognizing ellipsis that could benefit both MT
and other NLP technology that relies on shallow or
deep syntactic analysis.
While the general case of coordination is quite
complicated, we focus on the special case of com-
plex NPs. Errors in NP coordination typically ac-
count for the majority of parser coordination errors
(Hogan, 2007). The information needed to resolve
coordinate NP ambiguity cannot be derived from
hand-annotated data, and we follow previous work
in looking for new information sources to apply
to this problem (Resnik, 1999; Nakov and Hearst,
2005; Rus et al, 2007; Pitler et al, 2010).
We first resolve coordinate NP ambiguity in a
word-aligned parallel corpus. In bitexts, both mono-
lingual and bilingual information can indicate NP
structure. We create separate classifiers using mono-
lingual and bilingual feature views. We train the
two classifiers using co-training, iteratively improv-
ing the accuracy of one classifier by learning from
the predictions of the other. Starting from only two
1346
initial labeled examples, we are able to train a highly
accurate classifier using only monolingual features.
The monolingual classifier can then be used both
within and beyond the aligned bitext. In particular,
it achieves close to 96% accuracy on both bitext data
and on out-of-domain examples in the Treebank.
2 Problem Definition and Related Tasks
Our system operates over a part-of-speech tagged in-
put corpus. We attempt to resolve the ambiguity in
all tag sequences matching the expression:
[DT|PRP$] (N.*|J.*) and [DT|PRP$] (N.*|J.*) N.*
e.g. [the] rocket\w1 and [the] mortar\w2 attacks\h
Each example ends with a noun, h. Preceding h
are a pair of possibly-conjoined words, w1 and w2,
either nouns (rocket and mortar), adjectives, or a
mix of the two. We allow determiners or possessive
pronouns before w1 and/or w2. This pattern is very
common. Depending on the domain, we find it in
roughly one of every 10 to 20 sentences. We merge
identical matches in our corpus into a single exam-
ple for labeling. Roughly 38% of w1,w2 pairs are
both adjectives, 26% are nouns, and 36% are mixed.
The task is to determine whether w1 and w2 are
conjoined or not. When they are not conjoined, there
are two cases: 1) w1 is actually conjoined with w2 h
as a whole (e.g. asbestos and polyvinyl chloride),
or 2) The conjunction links something higher up in
the parse tree, as in, ?farmers are getting older\w1
and younger\w2 people\h are reluctant to take up
farming.? Here, and links two separate clauses.
Our task is both narrower and broader than pre-
vious work. It is broader than previous approaches
that have focused only on conjoined nouns (Resnik,
1999; Nakov and Hearst, 2005). Although pairs
of adjectives are usually conjoined (and mixed tags
are usually not), this is not always true, as in
older/younger above. For comparison, we also state
accuracy on the noun-only examples (? 8).
Our task is more narrow than the task tackled
by full-sentence parsers, but most parsers do not
bracket NP-internal structure at all, since such struc-
ture is absent from the primary training corpus for
statistical parsers, the Penn Treebank (Marcus et al,
1993). We confirm that standard broad-coverage
parsers perform poorly on our task (? 7).
Vadas and Curran (2007a) manually annotated NP
structure in the Penn Treebank, and a few custom NP
parsers have recently been developed using this data
(Vadas and Curran, 2007b; Pitler et al, 2010). Our
task is more narrow than the task handled by these
parsers since we do not handle other, less-frequent
and sometimes more complex constructions (e.g.
robot arms and legs). However, such constructions
are clearly amenable to our algorithm. In addition,
these parsers have only evaluated coordination res-
olution within base NPs, simplifying the task and
rendering the aforementioned older/younger prob-
lem moot. Finally, these custom parsers have only
used simple count features; for example, they have
not used the paraphrases we describe below.
3 Supervised Coordination Resolution
We adopt a discriminative approach to resolving co-
ordinate NP ambiguity. For each unique coordinate
NP in our corpus, we encode relevant information
in a feature vector, x?. A classifier scores these vec-
tors with a set of learned weights, w?. We assume N
labeled examples {(y1, x?1), ..., (yN , x?N )} are avail-
able to train the classifier. We use ?y = 1? as the
class label for NPs with ellipsis and ?y = 0? for
NPs without. Since our particular task requires a bi-
nary decision, any standard learning algorithm can
be used to learn the feature weights on the train-
ing data. We use (regularized) logistic regression
(a.k.a. maximum entropy) since it has been shown
to perform well on a range of NLP tasks, and also
because its probabilistic interpretation is useful for
co-training (? 4). In binary logistic regression, the
probability of a positive class takes the form of the
logistic function:
Pr(y = 1) = exp(w? ? x?)1 + exp(w? ? x?)
Ellipsis is predicted if Pr(y = 1) > 0.5 (equiva-
lently, w? ? x? > 0), otherwise we predict no ellipsis.
Supervised classifiers easily incorporate a range
of interdependent information into a learned deci-
sion function. The cost for this flexibility is typically
the need for labeled training data. The more features
we use, the more labeled data we need, since for
linear classifiers, the number of examples needed to
reach optimum performance is at most linear in the
1347
Phrase Evidence Pattern
dairy and meat English: ... production of dairy and meat... h of w1 and w2
production English: ... dairy production and meat production... w1 h and w2 h
(ellipsis) English: ... meat and dairy production... w2 and w1 h
Spanish: ... produccio?n la?ctea y ca?rnica... h w1 ... w2
? production dairy and meat
Finnish: ... maidon- ja lihantuotantoon... w1- ... w2h
? dairy- and meatproduction
French: ... production de produits laitiers et de viande... h ... w1 ... w2
? production of products dairy and of meat
asbestos and English: ... polyvinyl chloride and asbestos... w2 h and w1
polyvinyl English: ... asbestos , and polyvinyl chloride... w1 , and w2 h
chloride English: ... asbestos and chloride... w1 and h
(no ellipsis) Portuguese: ... o amianto e o cloreto de polivinilo... w1 ... h ... w2
? the asbestos and the chloride of polyvinyl
Italian: ... l? asbesto e il polivinilcloruro... w1 ... w2h
? the asbestos and the polyvinylchloride
Table 1: Monolingual and bilingual evidence for ellipsis or lack-of-ellipsis in coordination of [w1 and w2 h] phrases.
number of features (Vapnik, 1998). In ? 4, we pro-
pose a way to circumvent the need for labeled data.
We now describe the particular monolingual and
bilingual information we use for this problem. We
refer to Table 1 for canonical examples of the two
classes and also to provide intuition for the features.
3.1 Monolingual Features
Count features These real-valued features encode
the frequency, in a large auxiliary corpus, of rel-
evant word sequences. Co-occurrence frequencies
have long been used to resolve linguistic ambigui-
ties (Dagan and Itai, 1990; Hindle and Rooth, 1993;
Lauer, 1995). With the massive volumes of raw
text now available, we can look for very specific
and indicative word sequences. Consider the phrase
dairy and meat production (Table 1). A high count
in raw text for the paraphrase ?production of dairy
and meat? implies ellipsis in the original example.
In the third column of Table 1, we suggest a pat-
tern that generalizes the particular piece of evidence.
It is these patterns and other English paraphrases
that we encode in our count features (Table 2). We
also use (but do not list) count features for the four
paraphrases proposed in Nakov and Hearst (2005,
? 3.2.3). Such specific paraphrases are more com-
mon than one might think. In our experiments, at
least 20% of examples have non-zero counts for a
5-gram pattern, while over 70% of examples have
counts for a 4-gram pattern.
Our features also include counts for subsequences
of the full phrase. High counts for ?dairy produc-
tion? alone or just ?dairy and meat? also indicate el-
lipsis. On the other hand, like Pitler et al (2010), we
have a feature for the count of ?dairy and produc-
tion.? Frequent conjoining of w1 and h is evidence
that there is no ellipsis, that w1 and h are compatible
and heads of two separate and conjoined NPs.
Many of our patterns are novel in that they include
commas or determiners. The presence of these of-
ten indicate that there are two separate NPs. E.g.
seeing asbestos , and polyvinyl chloride or the as-
bestos and the polyvinyl chloride suggests no ellip-
sis. We also propose patterns that include left-and-
right context around the NP. These aim to capture
salient information about the NP?s distribution as an
entire unit. Finally, patterns involving prepositions
look for explicit paraphrasing of the nominal rela-
tions; the presence of ?h PREP w1 and w2? in a cor-
pus would suggest ellipsis in the original NP.
In total, we have 48 separate count features, re-
quiring counts for 315 distinct N-grams for each ex-
ample. We use log-counts as the feature value, and
use a separate binary feature to indicate if a partic-
ular count is zero. We efficiently acquire the counts
using custom tools for managing web-scale N-gram
1348
Real-valued count features. C(p) ? count of p
C(w1) C(w2) C(h)
C(w1 CC w2) C(w1 h) C(w2 h)
C(w2 CC w1) C(w1 CC h) C(h CC w1)
C(DT w1 CC w2) C(w1 , CC w2)
C(DT w2 CC w1) C(w2 , CC w1)
C(DT w1 CC h) C(w1 CC w2 ,)
C(DT h CC w1) C(w2 CC w1 ,)
C(DT w1 and DT w2) C(w1 CC DT w2)
C(DT w2 and DT w1) C(w2 CC DT w1)
C(DT h and DT w1) C(w1 CC DT h)
C(DT h and DT w2) C(h CC DT w1)
C(?L-CTXTi? w1 and w2 h) C(w1 CC w2 h)
C(w1 and w2 h ?R-CTXTi?) C(h PREP w1)
C(h PREP w1 CC w2) C(h PREP w2)
Count feature filler sets
DT = {the, a, an, its, his} CC = {and, or, ?,?}
PREP = {of, for, in, at, on, from, with, about}
Binary features and feature templates ? {0, 1}
wrd1=?wrd(w1)? tag1=?tag(w1)?
wrd2=?wrd(w2)? tag2=?tag(w2)?
wrdh=?wrd(h)? tagh=?tag(h)?
wrd12=?wrd(w1),wrd(w2)? wrd(w1)=wrd(w2)
tag12=?tag(w1),tag(w2)? tag(w1)=tag(w2)
tag12h=?tag(w1),tag(w1),tag(h)?
Table 2: Monolingual features. For counts using the
filler sets CC, DT and PREP, counts are summed across
all filler combinations. In contrast, feature templates are
denoted with ???, where the feature label depends on the
?bracketed argument?. E.g., we have separate count fea-
ture for each item in the L/R context sets, where
{L-CTXT} = {with, and, as, including, on, is, are, &},
{R-CTXT} = {and, have, of, on, said, to, were, &}
data (? 5). Previous approaches have used search
engine page counts as substitutes for co-occurrence
information (Nakov and Hearst, 2005; Rus et al,
2007). These approaches clearly cannot scale to use
the wide range of information used in our system.
Binary features Table 2 gives the binary features
and feature templates. These are templates in the
sense that every unique word or tag fills the tem-
plate and corresponds to a unique feature. We can
thus learn if particular words or tags are associated
with ellipsis. We also include binary features to flag
the presence of any optional determiners before w1
or w2. We also have binary features for the context
words that precede and follow the tag sequence in
the source corpus. These context features are analo-
gous to the L/R-CTXT features that were counted in
the auxiliary corpus. Our classifier learns, for exam-
Monolingual: x?m Bilingual: x?b
C(w1):14.4 C(detl=h * w1 * w2),Dutch:1
C(w2):15.4 C(detl=h * * w1 * * w2),Fr.:1
C(h):17.2 C(detl=h w1 h * w2),Greek:1
C(w1 CC w2):9.0 C(detl=h w1 * w2),Spanish:1
C(w1 h):9.8 C(detl=w1- * w2h),Swedish:1
C(w2 h):10.2 C(simp=h w1 w2),Dutch:1
C(w2 CC w1):10.5 C(simp=h w1 w2),French:1
C(w1 CC h):3.5 C(simp=h w1 h w2),Greek:1
C(h CC w1):6.8 C(simp=h w1 w2),Spanish:1
C(DT w2 CC w1:7.8 C(simp=w1 w2h),Swedish:1
C(w1 and w2 h and):2.4 C(span=5),Dutch:1
C(h PREP w1 CC w2):2.6 C(span=7),French:1
wrd1=dairy:1 C(span=5),Greek:1
wrd2=meat:1 C(span=4),Spanish:1
wrdh=production:1 C(span=3),Swedish:1
tag1=NN:1 C(ord=h w1 w2),Dutch:1
tag2=NN:1 C(ord=h w1 w2),French:1
tagh=NN:1 C(ord=h w1 h w2),Greek:1
wrd12=dairy,meat:1 C(ord=h w1 w2),Spanish:1
tag12=NN,NN:1 C(ord=w1 w2 h),Swedish:1
tag(w1)=tag(w2):1 C(ord=h w1 w2):4
tag12h=NN,NN,NN:1 C(ord=w1 w2 h):1
Table 3: Example of actual instantiated feature vectors
for dairy and meat production (in label:value format).
Monolingual feature vector, x?m, on the left (both count
and binary features, see Table 2), Bilingual feature vec-
tor, x?b, on the right (see Table 4).
ple, that instances preceded by the words its and in
are likely to have ellipsis: these words tend to pre-
cede single NPs as opposed to conjoined NP pairs.
Example Table 3 provides part of the actual in-
stantiated monolingual feature vector for dairy and
meat production. Note the count features have log-
arithmic values, while only the non-zero binary fea-
tures are included.
A later stage of processing extracts a list of feature
labels from the training data. This list is then used
to map feature labels to integers, yielding the stan-
dard (sparse) format used by most machine learning
software (e.g., 1:14.4 2:15.4 3:17.2 ... 7149:1 24208:1).
3.2 Bilingual Features
The above features represent the best of the infor-
mation available to a coordinate NP classifier when
operating on an arbitrary text. In some domains,
however, we have additional information to inform
our decisions. We consider the case where we seek
to predict coordinate structure in parallel text: i.e.,
English text with a corresponding translation in one
1349
or more target languages. A variety of mature NLP
tools exists in this domain, allowing us to robustly
align the parallel text first at the sentence and then
at the word level. Given a word-aligned parallel cor-
pus, we can see how the different types of coordinate
NPs are translated in the target languages.
In Romance languages, examples with ellipsis,
such as dairy and meat production (Table 1), tend to
correspond to translations with the head in the first
position, e.g. ?produccio?n la?ctea y ca?rnica? in Span-
ish (examples taken from Europarl (Koehn, 2005)).
When there is no ellipsis, the head-first syntax leads
to the ?w1 and h w2? ordering, e.g. amianto e o
cloreto de polivinilo in Portuguese. Another clue
for ellipsis is the presence of a dangling hyphen, as
in the Finnish maidon- ja lihantuotantoon. We find
such hyphens especially common in Germanic lan-
guages like Dutch. In addition to language-specific
clues, a translation may resolve an ambiguity by
paraphrasing the example in the same way it may
be paraphrased in English. E.g., we see hard and
soft drugs translated into Spanish as drogas blandas
y drogas duras with the head, drogas, repeated (akin
to soft drugs and hard drugs in English).
One could imagine manually defining the rela-
tionship between English NP coordination and the
patterns in each language, but this would need to be
repeated for each language pair, and would likely
miss many useful patterns. In contrast, by represent-
ing the translation patterns as features in a classifier,
we can instead automatically learn the coordination-
translation correspondences, in any language pair.
For each occurrence of a coordinate NP in a word-
aligned bitext, we inspect the alignments and de-
termine the mapping of w1, w2 and h. Recall that
each of our examples represents all the occurrences
of a unique coordinate NP in a corpus. We there-
fore aggregate translation information over all the
occurrences. Since the alignments in automatically-
aligned parallel text are noisy, the more occurrences
we have, the more translations we have, and the
more likely we are to make a correct decision. For
some common instances in Europarl, like Agricul-
ture and Rural Development, we have thousands of
translations in several languages.
Table 4 provides the bilingual feature templates.
The notation indicates that, for a given coordi-
nate NP, we count the frequency of each transla-
C?detl(w1,w2,h)?,?LANG?
C?simp(w1,w2,h)?,?LANG?
C?span(w1,w2,h)?,?LANG?
C?ord(w1,w2,h)?,?LANG?
C?ord(w1,w2,h)?
Table 4: Real-valued bilingual feature templates. The
shorthand is detl=?detailed pattern,? simp=?simple pat-
tern,? span=?span of pattern,? ord=?order of words.? The
notation C?p?,?LANG?means the number of times we see
the pattern (or span) ?p? as the aligned translation of the
coordinate NP in the target language ?LANG?.
tion pattern in each target language, and generate
real-valued features for these counts. The feature
counts are indexed to the particular pattern and lan-
guage. We also have one language-independent fea-
ture, C?ord(w1,w2,h)?, which gives the frequency of
each ordering across all languages. The span is the
number of tokens collectively spanned by the trans-
lations of w1, w2 and h. The ?detailed pattern? rep-
resents the translation using wildcards for all other
foreign words, but maintains punctuation. Letting
?*? stand for the wildcard, the detailed patterns for
the translations of dairy and meat production in Ta-
ble 1 would be [h w1 * w2] (Spanish), [w1- * w2h]
(Finnish) and [h * * w1 * * w2] (French). Four
or more consecutive wildcards are converted to ?...?.
For the ?simple pattern,? we remove the wildcards
and punctuation. Note that our aligner allows the
English word to map to multiple target words. The
simple pattern differs from the ordering in that it de-
notes how many tokens each of w1, w2 and h span.
Example Table 3 also provides part of the actual
instantiated bilingual feature vector for dairy and
meat production.
4 Bilingual Co-training
We exploit the orthogonality of the monolingual
and bilingual features using semi-supervised learn-
ing. These features are orthogonal in the sense that
they look at different sources of information for each
example. If we had enough training data, a good
classifier could be trained using either monolingual
or bilingual features on their own. With classifiers
trained on even a little labeled data, it?s feasible that
for a particular example, the monolingual classifier
might be confident when the bilingual classifier is
1350
Algorithm 1 The bilingual co-training algorithm: subscript m corresponds to monolingual, b to bilingual
Given: ? a set L of labeled training examples in the bitext, {(x?i, yi)}
? a set U of unlabeled examples in the bitext, {x?j}
? hyperparams: k (num. iterations), um and ub (size smaller unlabeled pools), nm and nb
(num. new labeled examples each iteration), C: regularization param. for classifier training
Create Lm ? L
Create Lb ? L
Create a pool Um by choosing um examples randomly from U .
Create a pool Ub by choosing ub examples randomly from U .
for i = 0 to k do
Use Lm to train a classifier hm using only x?m, the monolingual features of x?
Use Lb to train a classifier hb using only x?b, the bilingual features of x?
Use hm to label Um, move the nm most-confident examples to Lb
Use hb to label Ub, move the nb most-confident examples to Lm
Replenish Um and Ub randomly from U with nm and nb new examples
end for
uncertain, and vice versa. This suggests using a
co-training approach (Yarowsky, 1995; Blum and
Mitchell, 1998). We train separate classifiers on the
labeled data. We use the predictions of one classi-
fier to label new examples for training the orthogo-
nal classifier. We iterate this training and labeling.
We outline how this procedure can be applied to
bitext data in Algorithm 1 (above). We follow prior
work in drawing predictions from smaller pools, Um
and Ub, rather than from U itself, to ensure the la-
beled examples ?are more representative of the un-
derlying distribution? (Blum and Mitchell, 1998).
We use a logistic regression classifier for hm and
hb. Like Blum and Mitchell (1998), we also create
a combined classifier by making predictions accord-
ing to argmaxy=1,0 Pr(y|xm)Pr(y|xb).
The hyperparameters of the algorithm are 1) k,
the number of iterations, 2) um and ub, the size of
the smaller unlabeled pools, 3) nm and nb, the num-
ber of new labeled examples to include at each itera-
tion, and 4) the regularization parameter of the logis-
tic regression classifier. All such parameters can be
tuned on a development set. Like Blum and Mitchell
(1998), we ensure that we maintain roughly the true
class balance in the labeled examples added at each
iteration; we also estimate this balance using devel-
opment data.
There are some differences between our approach
and the co-training algorithm presented in Blum and
Mitchell (1998, Table 1). One of our key goals is to
produce an accurate classifier that uses only mono-
lingual features, since only this classifier can be ap-
plied to arbitrary monolingual text. We thus break
the symmetry in the original algorithm and allow hb
to label more examples for hm than vice versa, so
that hm will improve faster. This is desirable be-
cause we don?t have unlimited unlabeled examples
to draw from, only those found in our parallel text.
5 Data
Web-scale text data is used for monolingual feature
counts, parallel text is used for classifier co-training,
and labeled data is used for training and evaluation.
Web-scale N-gram Data We extract our counts
from Google V2: a new N-gram corpus (with
N-grams of length one-to-five) created from the
same one-trillion-word snapshot of the web as the
Google 5-gram Corpus (Brants and Franz, 2006),
but with enhanced filtering and processing of the
source text (Lin et al, 2010, Section 5). We get
counts using the suffix array tools described in (Lin
et al, 2010). We add one to all counts for smooth-
ing.
Parallel Data We use the Danish, German, Greek,
Spanish, Finnish, French, Italian, Dutch, Por-
tuguese, and Swedish portions of Europarl (Koehn,
2005). We also use the Czech, German, Span-
ish and French news commentary data from WMT
1351
2010.1 Word-aligned English-Foreign bitexts are
created using the Berkeley aligner.2 We run 5 itera-
tions of joint IBM Model 1 training, followed by 3-
to-5 iterations of joint HMM training, and align with
the competitive-thresholding heuristic. The English
portions of all bitexts are part-of-speech tagged with
CRFTagger (Phan, 2006). 94K unique coordinate
NPs and their translations are then extracted.
Labeled Data For experiments within the paral-
lel text, we manually labeled 1320 of the 94K co-
ordinate NP examples. We use 605 examples to set
development parameters, 607 examples as held-out
test data, and 2, 10 or 100 examples for training.
For experiments on the WSJ portion of the Penn
Treebank, we merge the original Treebank annota-
tions with the NP annotations provided by Vadas and
Curran (2007a). We collect all coordinate NP se-
quences matching our pattern and collapse them into
a single example. We label these instances by deter-
mining whether the annotations have w1 and w2 con-
joined. In only one case did the same coordinate NP
have different labels in different occurrences; this
was clearly an error and resolved accordingly. We
collected 1777 coordinate NPs in total, and divided
them into 777 examples for training, 500 for devel-
opment and 500 as a final held-out test set.
6 Evaluation and Settings
We evaluate using accuracy: the percentage of ex-
amples classified correctly in held-out test data.
We compare our systems to a baseline referred to
as the Tag-Triple classifier. This classifier has a
single feature: the tag(w1), tag(w2), tag(h) triple.
Tag-Triple is therefore essentially a discriminative,
unlexicalized parser for our coordinate NPs.
All classifiers use L2-regularized logistic regres-
sion training via LIBLINEAR (Fan et al, 2008). For
co-training, we fix regularization at C = 0.1. For all
other classifiers, we optimize the C parameter on the
development data. At each iteration, i, classifier hm
annotates 50 new examples for training hb, from a
pool of 750 examples, while hb annotates 50 ? i new
examples for hm, from a pool of 750 ? i examples.
This ensures hm gets the majority of automatically-
labeled examples.
1
www.statmt.org/wmt10/translation-task.html
2
nlp.cs.berkeley.edu/pages/wordaligner.html
 86
 88
 90
 92
 94
 96
 98
 100
 0  10  20  30  40  50  60
Ac
cu
ra
cy
 (%
)
Co-training iteration
Bilingual View
Monolingual View
Combined
Figure 1: Accuracy on Bitext development data over the
course of co-training (from 10 initial seed examples).
We also set k, the number of co-training itera-
tions. The monolingual, bilingual, and combined
classifiers reach their optimum levels of perfor-
mance after different numbers of iterations (Fig-
ure 1). We therefore set k separately for each, stop-
ping around 16 iterations for the combined, 51 for
the monolingual, and 57 for the bilingual classifier.
7 Bitext Experiments
We evaluate our systems on our held-out bitext data.
The majority class is ellipsis, in 55.8% of exam-
ples. For comparison, we ran two publicly-available
broad-coverage parsers and analyzed whether they
correctly predicted ellipsis. The parsers were the
C&C parser (Curran et al, 2007) and Minipar (Lin,
1998). They achieved 78.6% and 77.6%.3
Table 5 shows that co-training results in much
more accurate classifiers than supervised training
alone, regardless of the features or amount of ini-
tial training data. The Tag-Triple system is the
weakest system in all cases. This shows that better
monolingual features are very important, but semi-
supervised training can also make a big difference.
3We provided the parsers full sentences containing the NPs. We
directly extracted the labels from the C&C bracketing, while
for Minipar we checked whether w1 was the head of w2. Of
course, the parsers performed very poorly on ellipsis involving
two nouns (partly because NP structure is absent from their
training corpora (see ? 2 and also Vadas and Curran (2008)),
but neither exceeded 88% on adjective or mixed pairs either.
1352
# of Examples
System 2 10 100
Tag-Triple classifier 67.4 79.1 82.9
Monolingual classifier 69.9 90.8 91.6
Co-trained Mono. classifier 96.4 95.9 96.0
Relative error reduction via co-training 88% 62% 52%
Bilingual classifier 76.8 85.5 92.1
Co-trained Bili. classifier 93.2 93.2 93.9
Relative error reduction via co-training 71% 53% 23%
Mono.+Bili. classifier 69.9 91.4 94.9
Co-trained Combo classifier 96.7 96.7 96.7
Relative error reduction via co-training 89% 62% 35%
Table 5: Co-training improves accuracy (%) over stan-
dard supervised learning on Bitext test data for different
feature types and number of training examples.
System Accuracy ?
Monolingual alone 91.6 -
+ Bilingual 94.9 39%
+ Co-training 96.0 54%
+ Bilingual & Co-training 96.7 61%
Table 6: Net benefits of bilingual features and co-training
on Bitext data, 100-training-example setting. ? = rela-
tive error reduction over Monolingual alone.
Table 6 shows the net benefit of our main contri-
butions. Bilingual features clearly help on this task,
but not as much as co-training. With bilingual fea-
tures and co-training together, we achieve 96.7% ac-
curacy. This combined system could be used to very
accurately resolve coordinate ambiguity in parallel
data prior to training an MT system.
8 WSJ Experiments
While we can now accurately resolve coordinate NP
ambiguity in parallel text, it would be even better
if this accuracy carried over to new domains, where
bilingual features are not available. We test the ro-
bustness of our co-trained monolingual classifier by
evaluating it on our labeled WSJ data.
The Penn Treebank and the annotations added by
Vadas and Curran (2007a) comprise a very special
corpus; such data is clearly not available in every
domain. We can take advantage of the plentiful la-
beled examples to also test how our co-trained sys-
tem compares to supervised systems trained with in-
System Training WSJ Acc.Set # Nouns All
Nakov & Hearst - - 79.2 84.8
Tag-Triple WSJ 777 76.1 82.4
Pitler et al WSJ 777 92.3 92.8
MonoWSJ WSJ 777 92.3 94.4
Co-trained Bitext 2 93.8 95.6
Table 7: Coordinate resolution accuracy (%) on WSJ.
domain labeled examples, and also other systems,
like Nakov and Hearst (2005), which although un-
supervised, are tuned on WSJ data.
We reimplemented Nakov and Hearst (2005)4 and
Pitler et al (2010)5 and trained the latter on WSJ an-
notations. We compare these systems to Tag-Triple
and also to a supervised system trained on the WSJ
using only our monolingual features (MonoWSJ).
The (out-of-domain) bitext co-trained system is the
best system on the WSJ data, both on just the ex-
amples where w1 and w2 are nouns (Nouns), and on
all examples (All) (Table 7).6 It is statistically sig-
nificantly better than the prior state-of-the-art Pitler
et al system (McNemar?s test, p<0.05) and also
exceeds the WSJ-trained system using monolingual
features (p<0.2). This domain robustness is less sur-
prising given its key features are derived from web-
scale N-gram data; such features are known to gen-
eralize well across domains (Bergsma et al, 2010).
We tried co-training without the N-gram features,
and performance was worse on the WSJ (85%) than
supervised training on WSJ data alone (87%).
9 Related Work
Bilingual data has been used to resolve a range of
ambiguities, from PP-attachment (Schwartz et al,
2003; Fossum and Knight, 2008), to distinguishing
grammatical roles (Schwarck et al, 2010), to full
dependency parsing (Huang et al, 2009). Related
4Nakov and Hearst (2005) use an unsupervised algorithm that
predicts ellipsis on the basis of a majority vote over a number
of pattern counts and established heuristics.
5Pitler et al (2010) uses a supervised classifier to predict brack-
etings; their count and binary features are a strict subset of the
features used in our Monolingual classifier.
6For co-training, we tuned k on the WSJ dev set but left other
parameters the same. We start from 2 training instances; results
were the same or slightly better with 10 or 100 instances.
1353
work has also focused on projecting syntactic an-
notations from one language to another (Yarowsky
and Ngai, 2001; Hwa et al, 2005), and jointly pars-
ing the two sides of a bitext by leveraging the align-
ments during training and testing (Smith and Smith,
2004; Burkett and Klein, 2008) or just during train-
ing (Snyder et al, 2009). None of this work has fo-
cused on coordination, nor has it combined bitexts
with web-scale monolingual information.
Most prior work has focused on leveraging the
alignments between a single pair of languages. Da-
gan et al (1991) first articulated the need for ?a mul-
tilingual corpora based system, which exploits the
differences between languages to automatically ac-
quire knowledge about word senses.? Kuhn (2004)
used alignments across several Europarl bitexts to
devise rules for identifying parse distituents. Ban-
nard and Callison-Burch (2005) used multiple bi-
texts as part of a system for extracting paraphrases.
Our co-training algorithm is well suited to using
multiple bitexts because it automatically learns the
value of alignment information in each language. In
addition, our approach copes with noisy alignments
both by aggregating information across languages
(and repeated occurrences within a language), and
by only selecting the most confident examples at
each iteration. Burkett et al (2010) also pro-
posed exploiting monolingual-view and bilingual-
view predictors. In their work, the bilingual view
encodes the per-instance agreement between mono-
lingual predictors in two languages, while our bilin-
gual view encodes the alignment and target text to-
gether, across multiple instances and languages.
The other side of the coin is the use of syntax to
perform better translation (Wu, 1997). This is a rich
field of research with its own annual workshop (Syn-
tax and Structure in Translation).
Our monolingual model is most similar to pre-
vious work using counts from web-scale text, both
for resolving coordination ambiguity (Nakov and
Hearst, 2005; Rus et al, 2007; Pitler et al, 2010),
and for syntax and semantics in general (Lapata
and Keller, 2005; Bergsma et al, 2010). We do
not currently use semantic similarity (either tax-
onomic (Resnik, 1999) or distributional (Hogan,
2007)) which has previously been found useful for
coordination. Our model can easily include such in-
formation as additional features. Adding new fea-
tures without adding new training data is often prob-
lematic, but is promising in our framework, since the
bitexts provide so much indirect supervision.
10 Conclusion
Resolving coordination ambiguity is hard. Parsers
are reporting impressive numbers these days, but
coordination remains an area with room for im-
provement. We focused on a specific subcase, com-
plex NPs, and introduced a new evaluation set. We
achieved a huge performance improvement from
79% for state-of-the-art parsers to 96%.7
Size matters. Most parsers are trained on a mere
million words of the Penn Treebank. In this work,
we show how to take advantage of billions of words
of bitexts and trillions of words of unlabeled mono-
lingual text. Larger corpora make it possible to
use associations among lexical items (compare dairy
production vs. asbestos chloride) and precise para-
phrases (production of dairy and meat). Bitexts are
helpful when the ambiguity can be resolved by some
feature in another language (such as word order).
The Treebank is convenient for supervised train-
ing because it has annotations. We show that even
without such annotations, high-quality supervised
models can be trained using co-training and features
derived from huge volumes of unlabeled data.
References
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Proc. ACL,
pages 597?604.
Shane Bergsma, Emily Pitler, and Dekang Lin. 2010.
Creating robust supervised classifiers via web-scale n-
gram data. In Proc. ACL, pages 865?874.
Avrim Blum and Tom Mitchell. 1998. Combining la-
beled and unlabeled data with co-training. In Proc.
COLT, pages 92?100.
Thorsten Brants and Alex Franz. 2006. The Google Web
1T 5-gram Corpus Version 1.1. LDC2006T13.
David Burkett and Dan Klein. 2008. Two languages
are better than one (for syntactic parsing). In Proc.
EMNLP, pages 877?886.
David Burkett, Slav Petrov, John Blitzer, and Dan Klein.
2010. Learning better monolingual models with unan-
notated bilingual text. In Proc. CoNLL, pages 46?53.
7Evaluation scripts and data are available online:
www.clsp.jhu.edu/?sbergsma/coordNP.ACL11.zip
1354
James Curran, Stephen Clark, and Johan Bos. 2007. Lin-
guistically motivated large-scale NLP with C&C and
Boxer. In Proc. ACL Demo and Poster Sessions, pages
33?36.
Ido Dagan and Alan Itai. 1990. Automatic processing of
large corpora for the resolution of anaphora references.
In Proc. COLING, pages 330?332.
Ido Dagan, Alon Itai, and Ulrike Schwall. 1991. Two
languages are more informative than one. In Proc.
ACL, pages 130?137.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A li-
brary for large linear classification. JMLR, 9:1871?
1874.
Victoria Fossum and Kevin Knight. 2008. Using bilin-
gual Chinese-English word alignments to resolve PP-
attachment ambiguity in English. In Proc. AMTA Stu-
dent Workshop, pages 48?53.
Donald Hindle and Mats Rooth. 1993. Structural ambi-
guity and lexical relations. Computational Linguistics,
19(1):103?120.
Deirdre Hogan. 2007. Coordinate noun phrase disam-
biguation in a generative parsing model. In Proc. ACL,
pages 680?687.
Liang Huang, Wenbin Jiang, and Qun Liu. 2009.
Bilingually-constrained (monolingual) shift-reduce
parsing. In Proc. EMNLP, pages 1222?1231.
Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrapping
parsers via syntactic projection across parallel texts.
Natural Language Engineering, 11(3):311?325.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Proc. MT Summit X.
Jonas Kuhn. 2004. Experiments in parallel-text based
grammar induction. In Proc. ACL, pages 470?477.
Mirella Lapata and Frank Keller. 2005. Web-based
models for natural language processing. ACM Trans.
Speech and Language Processing, 2(1):1?31.
Mark Lauer. 1995. Corpus statistics meet the noun com-
pound: Some empirical results. In Proc. ACL, pages
47?54.
Dekang Lin, Kenneth Church, Heng Ji, Satoshi Sekine,
David Yarowsky, Shane Bergsma, Kailash Patil, Emily
Pitler, Rachel Lathbury, Vikram Rao, Kapil Dalwani,
and Sushant Narsale. 2010. New tools for web-scale
N-grams. In Proc. LREC.
Dekang Lin. 1998. Dependency-based evaluation of
MINIPAR. In Proc. LREC Workshop on the Evalu-
ation of Parsing Systems.
Mitchell P. Marcus, Beatrice Santorini, and Mary
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19(2):313?330.
Preslav Nakov and Marti Hearst. 2005. Using the web as
an implicit training set: application to structural ambi-
guity resolution. In Proc. HLT-EMNLP, pages 17?24.
Xuan-Hieu Phan. 2006. CRFTagger: CRF English POS
Tagger. crftagger.sourceforge.net.
Emily Pitler, Shane Bergsma, Dekang Lin, and Kenneth
Church. 2010. Using web-scale N-grams to improve
base NP parsing performance. In In Proc. COLING,
pages 886?894.
Philip Resnik. 1999. Semantic similarity in a taxonomy:
An information-based measure and its application to
problems of ambiguity in natural language. Journal of
Artificial Intelligence Research, 11:95?130.
Vasile Rus, Sireesha Ravi, Mihai C. Lintean, and
Philip M. McCarthy. 2007. Unsupervised method for
parsing coordinated base noun phrases. In Proc. CI-
CLing, pages 229?240.
Florian Schwarck, Alexander Fraser, and Hinrich
Schu?tze. 2010. Bitext-based resolution of German
subject-object ambiguities. In Proc. HLT-NAACL,
pages 737?740.
Lee Schwartz, Takako Aikawa, and Chris Quirk. 2003.
Disambiguation of English PP attachment using mul-
tilingual aligned data. In Proc. MT Summit IX, pages
330?337.
David A. Smith and Noah A. Smith. 2004. Bilingual
parsing with factored estimation: Using English to
parse Korean. In Proc. EMNLP, pages 49?56.
Benjamin Snyder, Tahira Naseem, and Regina Barzilay.
2009. Unsupervised multilingual grammar induction.
In Proc. ACL-IJCNLP, pages 1041?1050.
David Vadas and James R. Curran. 2007a. Adding noun
phrase structure to the Penn Treebank. In Proc. ACL,
pages 240?247.
David Vadas and James R. Curran. 2007b. Large-scale
supervised models for noun phrase bracketing. In PA-
CLING, pages 104?112.
David Vadas and James R. Curran. 2008. Parsing noun
phrase structure with CCG. In Proc. ACL, pages 104?
112.
Vladimir N. Vapnik. 1998. Statistical Learning Theory.
John Wiley & Sons.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403.
David Yarowsky and Grace Ngai. 2001. Inducing multi-
lingual POS taggers and NP bracketers via robust pro-
jection across aligned corpora. In Proc. NAACL, pages
1?8.
David Yarowsky. 1995. Unsupervised word sense disam-
biguation rivaling supervised methods. In Proc. ACL,
pages 189?196.
1355
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 200?205,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Joint Training of Dependency Parsing Filters through
Latent Support Vector Machines
Colin Cherry
Institute for Information Technology
National Research Council Canada
colin.cherry@nrc-cnrc.gc.ca
Shane Bergsma
Center for Language and Speech Processing
Johns Hopkins University
sbergsma@jhu.edu
Abstract
Graph-based dependency parsing can be sped
up significantly if implausible arcs are elim-
inated from the search-space before parsing
begins. State-of-the-art methods for arc fil-
tering use separate classifiers to make point-
wise decisions about the tree; they label tokens
with roles such as root, leaf, or attaches-to-
the-left, and then filter arcs accordingly. Be-
cause these classifiers overlap substantially in
their filtering consequences, we propose to
train them jointly, so that each classifier can
focus on the gaps of the others. We inte-
grate the various pointwise decisions as latent
variables in a single arc-level SVM classifier.
This novel framework allows us to combine
nine pointwise filters, and adjust their sensi-
tivity using a shared threshold based on arc
length. Our system filters 32% more arcs than
the independently-trained classifiers, without
reducing filtering speed. This leads to faster
parsing with no reduction in accuracy.
1 Introduction
A dependency tree represents syntactic relationships
between words using directed arcs (Mel?c?uk, 1987).
Each token in the sentence is a node in the tree,
and each arc connects a head to its modifier. There
are two dominant approaches to dependency pars-
ing: graph-based and transition-based, where graph-
based parsing is understood to be slower, but often
more accurate (McDonald and Nivre, 2007).
In the graph-based setting, a complete search
finds the highest-scoring tree under a model that de-
composes over one or two arcs at a time. Much of
the time for parsing is spent scoring each poten-
tial arc in the complete dependency graph (John-
son, 2007), one for each ordered word-pair in the
sentence. Potential arcs are scored using rich linear
models that are discriminatively trained to maximize
parsing accuracy (McDonald et al, 2005). The vast
majority of these arcs are bad; in an n-word sen-
tence, only n of the n2 potential arcs are correct. If
many arcs can be filtered before parsing begins, then
the entire process can be sped up substantially.
Previously, we proposed a cascade of filters to
prune potential arcs (Bergsma and Cherry, 2010).
One stage of this cascade operates one token at a
time, labeling each token t according to various roles
in the tree:
? Not-a-head (NaH ): t is not the head of any arc
? Head-to-left (HtL{1/5/*}): t?s head is to its
left within 1, 5 or any number of words
? Head-to-right (HtR{1/5/*}): as head-to-left
? Root (Root): t is the root node, which elimi-
nates arcs according to projectivity
Similar to Roark and Hollingshead (2008), each role
has a corresponding binary classifier. These token-
role classifierswere shown to be more effective than
vine parsing (Eisner and Smith, 2005; Dreyer et
al., 2006), a competing filtering scheme that filters
arcs based on their length (leveraging the observa-
tion that most dependencies are short).
In this work, we propose a novel filtering frame-
work that integrates all the information used in
token-role classification and vine parsing, but of-
fers a number of advantages. In our previous work,
classifier decisions would often overlap: different
token-role classifiers would agree to filter the same
arc. Based on this observation, we propose a joint
training framework where only the most confident
200
HtR16	 ?
NaH3	 ? HtR*6	 ?
HtR56	 ?
Bob1	 ? ate2	 ? the3	 ? pizza4	 ? with5	 ? his6	 ? fork8	 ?	 ?
NN	 ? VBD	 ? DT	 ? NN	 ? IN	 ? POS	 ? NN	 ?
HtL16	 ?
salad7	 ?
NN	 ?
(T)	 ? (T)	 ?
(T)	 ?
(F)	 ?
(F)	 ?
Figure 1: The dotted arc can be filtered by labeling any of the
boxed roles as True; i.e., predicting that the head the3 is not the
head of any arc, or that the modifier his6 attaches elsewhere.
Role truth values, derived from the gold-standard tree (in grey),
are listed adjacent to the boxes, in parentheses.
classifier is given credit for eliminating an arc. The
identity of the responsible classifier is modeled as
a latent variable, which is filled in during training
using a latent SVM (LSVM) formulation. Our use
of an LSVM to assign credit during joint training
differs substantially from previous LSVM applica-
tions, which have induced latent linguistic structures
(Cherry and Quirk, 2008; Chang et al, 2010) or sen-
tence labels (Yessenalina et al, 2010).
In our framework, each classifier learns to fo-
cus on the cases where the other classifiers are less
confident. Furthermore, the integrated approach di-
rectly optimizes for arc-filtering accuracy (rather
than token-labeling fidelity). We trade-off filtering
precision/recall using two hyperparameters, while
the previous approach trained classifiers for eight
different tasks resulting in sixteen hyperparameters.
Ultimately, the biggest gains in filter quality are
achieved when we jointly train the token-role classi-
fiers together with a dynamic threshold that is based
on arc length and shared across all classifiers.
2 Joint Training of Token Roles
In our previous system, filtering is conducted by
training a separate SVM classifier for each of the
eight token-roles described in Section 1. Each clas-
sifier uses a training set with one example per tree-
bank token, where each token is assigned a binary
label derived from the gold-standard tree. Figure 1
depicts five of the eight token roles, along with their
truth values. The role labelers can be tuned for high
precision with label-specific cost parameters; these
are tuned separately for each classifier. At test time,
each of the eight classifiers assigns a binary label
to each of the n tokens in the sentence. Potential
arcs are then filtered from the complete dependency
graph according to these token labels. In Figure 1,
a positive assignment to any of the indicated token-
roles is sufficient to filter the dotted arc.
In the current work, we maintain almost the same
test-time framework, but we alter training substan-
tially, so that the various token-role classifiers are
trained jointly. To do so, we propose a classifica-
tion scheme focused on arcs.1 During training, each
arc is assigned a filtering event as a latent variable.
Events generalize the token-roles from our previous
system (e.g. NaH 3,HtR?6). Events are assigned bi-
nary labels during filtering; positive events are said
to be detected. In general, events can correspond
to any phenomenon, so long as the following holds:
For each arc a, we must be able to deterministically
construct the set Za of all events that would filter
a if detected.2 Figure 1 shows that Zthe3?his6 =
{NaH 3,HtR?6,HtR56,HtR16,HtL16}.
To detect events, we maintain the eight token-role
classifiers from the previous system, but they be-
come subclassifiers of our joint system. For no-
tational convenience, we pack them into a single
weight vector w?. Thus, the event z = NaH 3 is de-
tected only if w? ? ??(NaH 3) > 0, where ??(z) is z?s
feature vector. Given this notation, we can cast the
filtering decision for an arc a as a maximum. We
filter a only if:
f(Za) > 0 where f(Za) = max
z?Za
[
w? ? ??(z)
]
(1)
We have reformulated our problem, which previ-
ously involved a number of independent token clas-
sifiers, as a single arc classifier f()with an innermax
over latent events. Note the asymmetry inherent in
(1). To filter an arc,
[
w? ? ??(z) > 0
]
must hold for at
least one z ? Za; but to keep an arc,
[
w? ? ??(z) ? 0
]
must hold for all z ? Za. Also note that tokens
have completely disappeared from our formalism:
the classifier is framed only in terms of events and
arcs; token-roles are encapsulated inside events.
To provide a large-margin training objective for
our joint classifier, we adapt the latent SVM (Felzen-
1A joint filtering formalism for CFG parsing or SCFG trans-
lation would likewise focus on hyper-edges or spans.
2This same requirement is also needed by the previous,
independently-trained filters at test time, so that arcs can be fil-
tered according to the roles assigned to tokens.
201
szwalb et al, 2010; Yu and Joachims, 2009) to our
problem. Given a training set A of (a, y) pairs,
where a is an arc in context and y is the correct filter
label for a (1 to filter, 0 otherwise), LSVM training
selects w? to minimize:
1
2
||w?||2+
?
(a,y)?A
Cy max
[
0, 1 + f(Za|?y)? f(Za|y)
]
(2)
where Cy is a label-specific regularization parame-
ter, and the event set Z is now conditioned on the
label y: Za|1 = Za, and Za|0 = {Nonea}. Nonea
is a rejection event, which indicates that a is not
filtered. The rejection event slightly alters our de-
cision rule; rather than thresholding at 0, we now
filter a only if f(Za) > w? ? ??(Nonea). One can set
??(Nonea)? ? for all a to fix the threshold at 0.
Though not convex, (2) can be solved to a lo-
cal minimum with an EM-like alternating minimiza-
tion procedure (Felzenszwalb et al, 2010; Yu and
Joachims, 2009). The learner alternates between
picking the highest-scoring latent event z?a ? Za|y
for each example (a, y), and training a multiclass
SVM to solve an approximation to (2) where Za|y is
replaced with {z?a}. Intuitively, the first step assigns
the event z?a to a, making z?a responsible for a?s ob-
served label. The second step optimizes the model to
ensure that each z?a is detected, leading to the desired
arc-filtering decisions. As the process iterates, event
assignment becomes increasingly refined, leading to
a more accurate joint filter.
The resulting joint filter has only two hyper-
parameters: the label-specific cost parameters C1
and Co. These allow us to tune our system for high
precision by increasing the cost of misclassifying an
arc that should not be filtered (C1  Co).
Joint training also implicitly affects the relative
costs of subclassifier decisions. By minimizing an
arc-level hinge loss with latent events (which in turn
correspond to token-roles), we assign costs to token-
roles based on arc accuracy. Consequently, 1) A
token-level decision that affects multiple arcs im-
pacts multiple instances of hinge loss, and 2) No
extra credit (penalty) is given for multiple decisions
that (in)correctly filter the same arc. Therefore, an
NaH decision that filters thirty arcs is given more
weight than an HtL5 decision that filters only one
(Item 1), unless those thirty arcs are already filtered
NaH3	 ?=	 ?0.5	 ?
The1	 ? big2	 ? dog3	 ? chased4	 ? the5	 ? cat6	 ?
DT	 ? ADJ	 ? NN	 ? VBD	 ? DT	 ? NN	 ?
1.0	 ? 1.1	 ? 0.6	 ? 0.3	 ? 0.2	 ?
Figure 2: A hypothetical example of dynamic threshold-
ing, where a weak assertion that dog3 should not be a head`
w? ? ??(NaH 3) = 0.5
?
is sufficient to rule out two arcs. Each
arc?s threshold
`
w? ? ??(Nonea)
?
is shown next to its arrow.
by higher-scoring subclassifiers (Item 2).
3 Accounting for Arc Length
We can extend our system by expanding our event
set Z. By adding an arc-level event Vinea to each
Za, we can introduce a vine filter to prune long arcs.
Similarly, we have already introduced another arc-
level event, the rejection event Nonea. By assign-
ing features to Nonea, we learn a dynamic thresh-
old on all filters, which considers properties of the
arc before acting on any other event. We parameter-
ize both Vinea and Nonea with the same two fea-
tures, inspired by tag-specific vine parsing (Eisner
and Smith, 2005):
{
Bias : 1
HeadTag ModTag Dir(a) : Len(a)
}
where HeadTag ModTag Dir(a) concatenates the
part-of-speech tags of a?s head and modifier tokens
to its direction (left or right), and Len(a) gives the
unsigned distance between a?s head and modifier.
In the context of Vinea, these two features al-
low the system to learn tag-pair-specific limits on
arc length. In the context of Nonea, these features
protect short arcs and arcs that connect frequently-
linked tag-pairs, allowing our token-role filters to be
more aggressive on arcs that do not have these char-
acteristics. The dynamic threshold also alters our
interpretation of filtering events: where before they
were either active or inactive, events are now as-
signed scores, which are compared with the thresh-
old to make final filtering decisions (Figure 2).3
3Because tokens and arcs are scored independently and cou-
pled only through score comparison, the impact of Vinea and
Nonea on classification speed should be no greater than doing
vine and token-role filtering in sequence. In practice, it is no
slower than running token-role filtering on its own.
202
4 Experiments
We extract dependency structures from the Penn
Treebank using the head rules of Yamada and Mat-
sumoto (2003).4 We divide the Treebank into train
(sections 2?21), development (22) and test (23). We
part-of-speech tag our data using a perceptron tagger
similar to the one described by Collins (2002). The
training set is tagged with jack-knifing: the data is
split into 10 folds and each fold is tagged by a sys-
tem trained on the other 9 folds. Development and
test sets are tagged using the entire training set.
We train our joint filter using an in-house latent
SVM framework, which repeatedly calls a multi-
class exponentiated gradient SVM (Collins et al,
2008). LSVM training was stopped after 4 itera-
tions, as determined during development.5 For the
token-role classifiers, we re-implement the Bergsma
and Cherry (2010) feature set, initializing w? with
high-precision subclassifiers trained independently
for each token-role. Vine and None subclassifiers
are initialized with a zero vector. At test time, we
extract subclassifiers from the joint weight vector,
and use them as parameters in the filtering tools of
Bergsma and Cherry (2010).6
Parsing experiments are carried out using the
MST parser (McDonald et al, 2005),7 which we
have modified to filter arcs before carrying out fea-
ture extraction. It is trained using 5-best MIRA
(Crammer and Singer, 2003).
Following Bergsma and Cherry (2010), we mea-
sure intrinsic filter quality with reduction, the pro-
portion of total arcs removed, and coverage, the pro-
portion of true arcs retained. For parsing results, we
present dependency accuracy, the percentage of to-
kens that are assigned the correct head.
4.1 Impact of Joint Training
Our technical contribution consists of our proposed
joint training scheme for token-role filters, along
4As implemented at http://w3.msi.vxu.se/?nivre/
research/Penn2Malt.html
5The LSVM is well on its way to convergence: fewer than
3% of arcs have event assignments that are still in flux.
6http://code.google.com/p/arcfilter/. Since our
contribution is mainly in better filter training, we were able to
use the arcfilter (testing) code with only small changes. We have
added our new joint filter, along with the Joint P1 model to the
arcfilter package, labeled as ultra filters.
7http://sourceforge.net/projects/mstparser/
Indep. Joint
System Cov. Red. Cov. Red.
Token 99.73 60.5 99.71 59.0
+ Vine 99.62 68.6 99.69 63.3
+ None N/A 99.76 71.6
Table 1: Ablation analysis of intrinsic filter quality.
with two extensions: the addition of vine filters
(Vine) and a dynamic threshold (None). Using pa-
rameters determined to perform well during devel-
opment,8 we examine test-set performance as we in-
corporate each of these components. For the token-
role and vine subclassifiers, we compare against an
independently-trained ensemble of the same classi-
fiers.9 Note that None cannot be trained indepen-
dently, as its shared dynamic threshold considers arc
and token views of the data simultaneously. Results
are shown in Table 1.
Our complete system outperforms all variants in
terms of both coverage and reduction. However, one
can see that neither joint system is able to outper-
form its independently-trained counter-part without
the dynamic threshold provided by None. This is
because the desirable credit-assignment properties
of our joint training procedure are achieved through
duplication (Zadrozny et al, 2003). That is, the
LSVM knows that a specific event is important be-
cause it appears in event sets Za for many arcs from
the same sentence. WithoutNone, the filtering deci-
sions implied by each copy of an event are identical.
Because these replicated events are associated with
arcs that are presented to the LSVM as independent
examples, they appear to be not only important, but
also low-variance, and therefore easy. This leads to
overfitting. We had hoped that the benefits of joint
training would outweigh this drawback, but our re-
sults show that they do not. However, in addition to
its other desirable properties (protecting short arcs),
the dynamic threshold imposed byNone restores in-
dependence between arcs that share a common event
(Figure 2). This alleviates overfitting and enables
strong performance.
8C0=1e-2, C1=1e-5
9Each subclassifier is a token-level SVM trained with token-
role labels extracted from the training treebank. Using develop-
ment data, we search over regularization parameters so that each
classifier yields more than 99.93% arc-level coverage.
203
Filter Intrinsic MST-1 MST-2
Filter Cov. Red. Time Acc. Sent/sec* Acc. Sent/sec*
None 100.00 00.0 0s 91.28 16 92.05 10
B&C R+L 99.70 54.1 7s 91.24 29 92.00 17
Joint P1 99.76 71.6 7s 91.28 38 92.06 22
B&C R+L+Q 99.43 78.3 19s 91.23 35 91.98 22
Joint P2 99.56 77.9 7s 91.29 44 92.05 25
Table 2: Parsing with jointly-trained filters outperforms independently-trained filters (R+L), as well as a more complex
cascade (R+L+Q). *Accounts for total time spent parsing and applying filters, averaged over five runs.
4.2 Comparison to the state of the art
We directly compare our filters to those of Bergsma
and Cherry (2010) in terms of both intrinsic fil-
ter quality and impact on the MST parser. The
B&C system consists of three stages: rules (R), lin-
ear token-role filters (L) and quadratic arc filters
(Q). The Q stage uses rich arc-level features simi-
lar to those of the MST parser. We compare against
independently-trained token-role filters (R+L), as
well as the complete cascade (R+L+Q), using the
models provided online.10 Our comparison points,
Joint P1 and P2 were built by tuning our complete
joint system to roughly match the coverage values
of R+L and R+L+Q on development data.11 Results
are shown in Table 2.
Comparing Joint P1 to R+L, we can see that for
a fixed set of pointwise filters, joint training with
a dynamic threshold outperforms independent train-
ing substantially. We achieve a 32% improvement
in reduction with no impact on coverage and no in-
crease in filtering overhead (time).
Comparing Joint P2 to R+L+Q, we see that Joint
P2 achieves similar levels of reduction with far less
filtering overhead; our filters take only 7 seconds
to apply instead of 19. This increases the speed of
the (already fast) filtered MST-1 parser from 35 sen-
tences per second to 44, resulting in a total speed-
up of 2.75 with respect to the unfiltered parser. The
improvement is less impressive for MST-2, where
the overhead for filter application is a less substan-
tial fraction of parsing time; however, our training
framework also has other benefits with respect to
R+L+Q, including a single unified training algo-
10Results are not identical to those reported in our previous
paper, due to our use of a different part-of-speech tagger. Note
that parsing accuracies for the B&C systems have improved.
11P1: C0=1e-2, C1=1e-5, P2: C0=1e-2, C1=2e-5
rithm, fewer hyper-parameters and a smaller test-
time memory footprint. Finally, the jointly trained
filters have no impact on parsing accuracy, where
both B&C filters have a small negative effect.
The performance of Joint-P2+MST-2 is compa-
rable to the system of Huang and Sagae (2010),
who report a parsing speed of 25 sentences per
second and an accuracy of 92.1 on the same test
set, using a transition-based parser enhanced with
dynamic-programming state combination.12 Graph-
based and transition-based systems tend to make dif-
ferent types of errors (McDonald and Nivre, 2007).
Therefore, having fast, accurate parsers for both ap-
proaches presents an opportunity for large-scale, ro-
bust parser combination.
5 Conclusion
We have presented a novel use of latent SVM
technology to train a number of filters jointly,
with a shared dynamic threshold. By training a
family of dependency filters in this manner, each
subclassifier focuses on the examples where it is
most needed, with our dynamic threshold adjust-
ing filter sensitivity based on arc length. This al-
lows us to outperform a 3-stage filter cascade in
terms of speed-up, while also reducing the im-
pact of filtering on parsing accuracy. Our filter-
ing code and trained models are available online at
http://code.google.com/p/arcfilter. In
the future, we plan to apply our joint training tech-
nique to other rich filtering regimes (Zhang et al,
2010), and to other NLP problems that combine the
predictions of overlapping classifiers.
12The usual caveats for cross-machine, cross-implementation
speed comparisons apply.
204
References
Shane Bergsma and Colin Cherry. 2010. Fast and accu-
rate arc filtering for dependency parsing. In COLING.
Ming-Wei Chang, Dan Goldwasser, Dan Roth, and Vivek
Srikumar. 2010. Discriminative learning over con-
strained latent representations. In HLT-NAACL.
Colin Cherry and Chris Quirk. 2008. Discriminative,
syntactic language modeling through latent SVMs. In
AMTA.
Michael Collins, Amir Globerson, Terry Koo, Xavier
Carreras, and Peter L. Bartlett. 2008. Exponentiated
gradient algorithms for conditional random fields and
max-margin markov networks. JMLR, 9:1775?1822.
Michael Collins. 2002. Discriminative training methods
for hidden markov models: Theory and experiments
with perceptron algorithms. In EMNLP.
Koby Crammer and Yoram Singer. 2003. Ultraconserva-
tive online algorithms for multiclass problems. JMLR,
3:951?991.
Markus Dreyer, David A. Smith, and Noah A. Smith.
2006. Vine parsing and minimum risk reranking for
speed and precision. In CoNLL.
Jason Eisner and Noah A. Smith. 2005. Parsing with soft
and hard constraints on dependency length. In IWPT.
Pedro F. Felzenszwalb, Ross B. Girshick, David
McAllester, and Deva Ramanan. 2010. Object detec-
tion with discriminatively trained part based models.
IEEE Transactions on Pattern Analysis and Machine
Intelligence, 32(9).
Liang Huang and Kenji Sagae. 2010. Dynamic program-
ming for linear-time incremental parsing. In ACL.
Mark Johnson. 2007. Transforming projective bilexical
dependency grammars into efficiently-parsable CFGs
with unfold-fold. In ACL.
Ryan McDonald and Joakim Nivre. 2007. Characteriz-
ing the errors of data-driven dependency parsing mod-
els. In EMNLP-CoNLL.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In ACL.
Igor A. Mel?c?uk. 1987. Dependency syntax: theory and
practice. State University of New York Press.
Brian Roark and Kristy Hollingshead. 2008. Classifying
chart cells for quadratic complexity context-free infer-
ence. In COLING.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical
dependency analysis with support vector machines. In
IWPT.
Ainur Yessenalina, Yisong Yue, and Claire Cardie. 2010.
Multi-level structured models for document-level sen-
timent classification. In EMNLP.
Chun-Nam John Yu and Thorsten Joachims. 2009.
Learning structural SVMs with latent variables. In
ICML.
Bianca Zadrozny, John Langford, and Naoki Abe. 2003.
Cost-sensitive learning by cost-proportionate example
weighting. In Third IEEE International Conference on
Data Mining.
Yue Zhang, Byung-Gyu Ahn, Stephen Clark, Curt Van
Wyk, James R. Curran, and Laura Rimell. 2010.
Chart pruning for fast lexicalised-grammar parsing. In
EMNLP.
205
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 710?720,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Using Conceptual Class Attributes to Characterize Social Media Users
Shane Bergsma and Benjamin Van Durme
Department of Computer Science and Human Language Technology Center of Excellence
Johns Hopkins University
Baltimore, MD 21218, USA
Abstract
We describe a novel approach for automat-
ically predicting the hidden demographic
properties of social media users. Building
on prior work in common-sense knowl-
edge acquisition from third-person text,
we first learn the distinguishing attributes
of certain classes of people. For exam-
ple, we learn that people in the Female
class tend to have maiden names and en-
gagement rings. We then show that this
knowledge can be used in the analysis of
first-person communication; knowledge of
distinguishing attributes allows us to both
classify users and to bootstrap new train-
ing examples. Our novel approach enables
substantial improvements on the widely-
studied task of user gender prediction, ob-
taining a 20% relative error reduction over
the current state-of-the-art.
1 Introduction
There has been growing interest in characteriz-
ing social media users based on the content they
generate; that is, automatically labeling users with
demographic categories such as age and gender
(Burger and Henderson, 2006; Schler et al, 2006;
Rao et al, 2010; Mukherjee and Liu, 2010; Pen-
nacchiotti and Popescu, 2011; Burger et al, 2011;
Van Durme, 2012). Automatic user character-
ization has applications in targeted advertising
and personalization, and could also lead to finer-
grained assessment of public opinion (O?Connor
et al, 2010) and health (Paul and Dredze, 2011).
Consider the following tweet and suppose we
wish to predict the user?s gender:
Dirac was one of my boyhood heroes.
I?m glad I met him once. RT Paul Dirac
image by artist Eric Handy: http:...
State-of-the-art approaches cast this problem as a
classification task and train classifiers using super-
vised learning (Section 2). The features of the
classifier are indicators of specific words in the
user-generated text. While a human would as-
sume that someone with boyhood heroes is male,
a standard classifier has no way of exploiting such
knowledge unless the phrase occurs in training
data. We present an algorithm that improves user
characterization by collecting and exploiting such
common-sense knowledge.
Our work is inspired by algorithms that pro-
cesses large text corpora in order to discover the
attributes of semantic classes, e.g. (Berland and
Charniak, 1999; Schubert, 2002; Almuhareb and
Poesio, 2004; Tokunaga et al, 2005; Girju et al,
2006; Pas?ca and Van Durme, 2008; Alfonseca et
al., 2010). We learn the distinguishing attributes
of different demographic groups (Section 3), and
then automatically assign users to these groups
whenever they refer to a distinguishing attribute in
their writings (Section 4). Our approach obviates
the need for expensive annotation efforts, and al-
lows us to rapidly bootstrap training data for new
classification tasks.
We validate our approach by advancing the
state-of-the-art on the most well-studied user clas-
sification task: predicting user gender (Section 5).
Our bootstrapped system, trained purely from
automatically-annotated Twitter data, significantly
reduces error over a state-of-the-art system trained
on thousands of gold-standard training examples.
2 Supervised User Characterization
The current state-of-the-art in user characteriza-
tion is to use supervised classifiers trained on an-
notated data. For each instance to be classified, the
output is a decision about a distinct demographic
property, such as Male/Female or Over/Under-18.
A variety of classification algorithms have been
employed, including SVMs (Rao et al, 2010), de-
710
cision trees (Pennacchiotti and Popescu, 2011), lo-
gistic regression (Van Durme, 2012), and the Win-
now algorithm (Burger et al, 2011).
Content Features: BoW Prior classifiers use a
set of features encoding the presence of specific
words in the user-generated text. We call these
features BoW features as they encode the stan-
dard Bag-of-Words representation which has been
highly effective in text categorization and informa-
tion retrieval (Sebastiani, 2002).
User-Profile Features: Usr Some researchers
have explored features for user-profile meta-
information in addition to user content. This may
include the user?s communication behavior and
network of contacts (Rao et al, 2010), their full
name (Burger et al, 2011) and whether they pro-
vide a profile picture (Pennacchiotti and Popescu,
2011). We focus on the case where we only
have access to the user?s screen-name (a.k.a. user-
name). Using a combination of content and user-
name features ?represents a use case common to
many different social media sites, such as chat
rooms and news article comment streams? (Burger
et al, 2011). We refer to features derived from a
username as Usr features in our experiments.
3 Learning Class Attributes
We aim to improve the automated classification
of users into various demographic categories by
learning and applying the distinguishing attributes
of those categories, e.g. that males have boyhood
heroes. Our approach builds on lexical-semantic
research on the topic of class-attribute extraction.
In this research, the objective is to discover vari-
ous attributes or parts of classes of entities. For
example, Berland and Charniak (1999) learn that
the class car has parts such as headlight, wind-
shield, dashboard, etc. Berland and Charniak ex-
tract these attributes by mining a corpus for fillers
of patterns such as ?car?s X? or ?X of a car?. Note
their patterns explicitly include the class itself
(car). Another approach is to use patterns that are
based on instances (i.e. hyponyms or sub-classes)
of the class. For example, Pas?ca and Van Durme
(2007) learn the attributes of the class car via pat-
terns involving instances of cars, e.g. Chevrolet
Corvette?s X and X of a Honda Civic. For these ap-
proaches, lists of instances are typically collected
from publicly-available resources such as Word-
Net or Wikipedia (Pas?ca and Van Durme, 2007;
Van Durme et al, 2008), acquired automatically
from corpora (Pas?ca and Van Durme, 2008; Al-
fonseca et al, 2010), or simply specified by hand
(Schubert, 2002).
Creation of Instance Lists We use an instance-
based approach; our instances are derived from
collections of common nouns that are associated
with roles and occupations of people. For the
gender task that we study in our experiments, we
acquire class instances by filtering the dataset of
nouns and their genders created by Bergsma and
Lin (2006). This dataset indicates how often a
noun is referenced by a male, female, neutral or
plural pronoun. We extract prevalent common
nouns for males and females by selecting only
those nouns that (a) occur more than 200 times
in the dataset, (b) mostly occur with male or fe-
male pronouns, and (c) occur as lower-case more
often than upper-case in a web-scale N-gram cor-
pus (Lin et al, 2010). We then classify a noun as
Male (resp. Female) if the noun is indicated to
occur with male (resp. female) pronouns at least
85% of the time. Since the gender data is noisy,
we also quickly pruned by hand any instances that
were malformed or obviously incorrectly assigned
by our automatic process. This results in 652 in-
stances in total. Table 1 provides some examples.
Male: bouncer, altar boy, army officer, dictator,
assailant, cameraman, drifter, chauffeur, bad guy
Female: young lady, lesbian, ballerina, waitress,
granny, chairwoman, heiress, soprano, socialite
Table 1: Example instances used for extraction of
class attributes for the gender classification task
Attribute Extraction We next collect and rank
attributes for each class. We first look for fillers of
attribute-patterns involving each of the instances.
Let I represent an instance of one of our classes.
We find fillers of the single high-precision pattern:
{word=I ,tag=NN}
| {z }
instance
{word=?s}
| {z }
?s
[{word=.*}* {tag=N.*}]
| {z }
attribute
(E.g. dictator ?s [former mistress]). The expres-
sion ?tag=NN? means that I must be tagged as
a noun. The expression in square brackets is the
filler, i.e. the extracted attribute, A. The notation
?{word=.*}* tag=N.*? means that A can be any
sequence of tokens ending in a noun. We use an
711
equivalent pattern when I is multi-token. The out-
put of this process is a set of (I ,A) pairs.
In attribute extraction, typically one must
choose between the precise results of rich pat-
terns (involving punctuation and parts-of-speech)
applied to small corpora (Berland and Charniak,
1999) and the high-coverage results of superficial
patterns applied to web-scale data, e.g. via the
Google API (Almuhareb and Poesio, 2004). We
obtain the best of both worlds by matching our
precise pattern against a version of the Google N-
gram Corpus that includes the part-of-speech tag
distributions for every N-gram (Lin et al, 2010).
We found that applying this pattern to web-scale
data is effective in extracting useful attributes. We
acquired around 20,000 attributes in total.
Finding Distinguishing Attributes Unlike
prior work, we aim to find distinguishing proper-
ties of each class; that is, the kinds of properties
that uniquely distinguish a particular category.
Prior work has mostly focused on finding ?rel-
evant? attributes (Alfonseca et al, 2010) or
?correct? parts (Berland and Charniak, 1999). A
leg is a relevant and correct part of both a male and
a female (and many other living and inanimate
objects), but it does not help us distinguish males
from females in social media. We therefore rank
our attributes for each class by their strength of
association with instances of that specific class.1
To calculate the association, we first disregard
the count of each (I ,A) pair and consider each
unique pair to be a single probabilistic event.
We then convert the (I ,A) pairs to corresponding
(C,A) pairs by replacing I with the corresponding
class, C. We then calculate the pointwise mutual
information (Church and Hanks, 1990) between
each C and A over the set of events:
PMI(C,A) = log p(C,A)p(C)p(A) (1)
If the PMI>0, the observed probability of a class
and attribute co-occurring is greater than the prob-
ability of co-occurrence that we would expect if C
and A were independently distributed. For each
class, we rank the attributes by their PMI scores.
1Reisinger and Pas?ca (2009) considered the related prob-
lem of finding the most appropriate class for each attribute;
they take an existing ontology of concepts (WordNet) as a
class hierarchy and use a Bayesian approach to decide ?the
correct level of abstraction for each attribute.?
Filtering Attributes We experimented with two
different methods to select a final set of distin-
guishing attributes for each class: (1) we used
a threshold to select the top-ranked attributes for
each class, and (2) we manually filtered the at-
tributes. For the gender classification task, we
manually filtered the entire set of attributes to se-
lect around 1000 attributes that were judged to be
discriminative (two thirds of which are female).
This filtering took one annotator only a few hours
to complete. Because this process was so trivial,
we did not invest in developing annotation guide-
lines or measuring inter-annotator agreement. We
make these filter attributes available online as an
attachment to this article, available through the
ACL Anthology.
Ultimately, we discovered that manual filter-
ing was necessary to avoid certain pathological
cases in our Twitter data. For example, our PMI
scoring finds homepage to be strongly associated
with males. In our gold-standard gender data
(Section 5), however, every user has a home-
page [by dataset construction]; we might there-
fore incorrectly classify every user as Male. We
agree with Richardson et al (1998) that ?auto-
matic procedures ... provide the only credible
prospect for acquiring world knowledge on the
scale needed to support common-sense reasoning?
but ?hand vetting? might be needed to ensure ?ac-
curacy and consistency in production level sys-
tems.? Since our approach requires manual in-
volvement in the filtering of the attribute list, one
might argue that one should simply manually enu-
merate the most relevant attributes directly. How-
ever, the manual generation of conceptual features
by a single researcher results in substantial vari-
ability both across and within participants (McRae
et al, 2005). Psychologists therefore generate
such lists by pooling the responses across many
participants: future work may compare our ?auto-
matically generate, manually prune? approach to
soliciting attributes via crowdsourcing.2
Table 2 gives examples of our extracted at-
2One can also view the work of manually filtering at-
tributes as a kind of ?feature labeling.? There is evidence
from Zaidan et al (2007) that a few hours of feature labeling
can be more productive than annotating new training exam-
ples. In fact, since Zaidan et al (2007) label features at the
token level (e.g., in our case one would highlight ?handbag?
in a given tweet), while we label features at the type level
(e.g., deciding whether to mark the word ?handbag? as fem-
inine in general), our process is likely even more efficient.
Future work may also wish to consider this connection to so-
called ?annotator rationales? more deeply.
712
Male: wife, widow, wives, ex-girlfriend, erec-
tion, testicles, wet dream, bride, buddies, ex-
wife, first-wife, penis, death sentence, manhood
Female: vagina, womb, maiden name, dresses,
clitoris, wedding dress, uterus, shawl, necklace,
ex-husband, ex-boyfriend, dowry, nightgown
Table 2: Example attributes for gender classes, in
descending order of class-association score
tributes. Our approach captures many multi-token
attributes; these are often distinguishing even
though the head noun is ambiguous (e.g. name
is ambiguous, maiden name is not). Our attributes
also go beyond the traditional meronyms that were
the target of earlier work. As we discuss further
in Related Work (Section 7), previous researchers
have worried about a proper definition of parts or
attributes and relied on human judgments for eval-
uation (Berland and Charniak, 1999; Girju et al,
2006; Van Durme et al, 2008). For us, whether
a property such as dowry should be considered
an ?attribute? of the class Female is immaterial;
we echo Almuhareb and Poesio (2004) who (on a
different task) noted that ?while the notion of ?at-
tribute? is not completely clear... our results sug-
gest that trying to identify attributes is beneficial.?
4 Applying Class Attributes
To classify users using the extracted attributes, we
look for cases where users refer to such attributes
in their first-person writings. We performed a pre-
liminary analysis of a two-week sample of tweets
from the TREC Tweets2011 Corpus.3 We found
that users most often reveal their attributes in the
possessive construction, ?my X? where X is an at-
tribute, quality or event that they possess (in a lin-
guistic sense). For example, we found over 1000
tweets with the phrase ?my wife.? In contrast, ?I
have a wife? occurs only 5 times.4
We therefore assign a user to a demographic
category as follows: We first part-of-speech tag
our data using CRFTagger (Phan, 2006) and then
look for ?my X? patterns where X is a sequence
of tokens terminating in a noun, analogous to our
3http://trec.nist.gov/data/tweets/ This corpus was de-
veloped for the TREC Microblog track (Soboroff et al, 2012).
4Note that ?I am a man? occurs only 20 times. Users
also reveal their names in ?my name is X? patterns in several
hundred tweets, but this is small compared to cases of self-
distinguishing attributes. Exploiting these alternative pat-
terns could nevertheless be a possible future direction.
attribute-extraction pattern (Section 3).5 When a
user uses such a ?my X? construction, we match
the filler X against our attribute lists for each
class. If the filler is on a list, we call it a self-
distinguishing attribute of a user. We then apply
our knowledge of the self-distinguishing attribute
and its corresponding class in one of the following
three ways:
(1) ARules: Using Attribute-Based Rules to
Override a Classifier When human-annotated
data is available for training and testing a su-
pervised classifier, we refer to it as gold stan-
dard data. Our first technique provides a sim-
ple way to use our identified self-distinguishing
attributes in conjunction with a classifier trained
on gold-standard data. If the user has any self-
distinguishing attributes, we assign the user to the
corresponding class; otherwise, we trust the output
of the classifier.
(2) Bootstrapped: Automatic Labeling of Train-
ing Examples Even without gold standard train-
ing data, we can use our self-distinguishing at-
tributes to automatically bootstrap annotations.
We collect a large pool of unlabeled users and their
tweets, and we apply the ARules described above
to label those users that have self-distinguishing
attributes. Once an example is auto-annotated,
we delete the self-distinguishing attributes from
the user?s content. This prevents the subsequent
learning algorithm from trivially learning the rules
with which we auto-annotated the data. Next, the
auto-annotated examples are used as training data
for a supervised system.6 Finally, when applying
the Bootstrapped classifiers, we can still apply the
ARules as a post-process (although in practice this
made little difference in our final results).
(3) BootStacked: Gold Standard and Boot-
strapped Combination Although we show that
an accurate classifier can be trained using auto-
annotated Bootstrapped data alone, we also test
whether we can combine this data with any gold-
standard training examples to achieve even better
performance. We use the following simple but
5While we used an ?off the shelf? POS tagger in this
work, we note that taggers optimized specifically for social
media are now available and would likely have resulted in
higher tagging accuracy (e.g. Owoputi et al (2013)).
6Note that while our target gender task presents mutually-
exclusive output classes, we can still train classifiers for other
categories without clear opposites (e.g. for labeling users
as Parents or Doctors) by using the 1-class classification
paradigm (Koppel and Schler, 2004).
713
effective method for combining data from these
two sources, inspired by prior techniques used in
the domain adaptation literature (Daume? III and
Marcu, 2006). We first use the trained Boot-
strapped system to make predictions on the entire
set of gold standard data (gold train, development,
and test sets). We then use these predictions as
features in a classifier trained on the gold standard
data. We refer to this system as the BootStacked
system in our evaluation.
5 Twitter Gender Prediction
To test the use of self-distinguishing attributes
in user classification, we apply our methods to
the task of gender classification on Twitter. This
is an important and intensely-studied task within
academia and industry. Furthermore, for this task
it is possible to semi-automatically acquire large
amounts of ground truth (Burger et al, 2011).
We can therefore benchmark our approach against
state-of-the-art supervised systems trained with
plentiful gold-standard data, giving us an idea of
how well our Bootstrapped system might compare
to theoretically top-performing systems on other
tasks, domains, and social media platforms where
such gold-standard training data is not available.
Gold Data Our data is derived from the corpus
created by Burger et al (2011). Burger et al ob-
served that many Twitter users link their Twitter
profile to homepages on popular blogging web-
sites. Since ?many of these [sites] have well-
structured profile pages [where users] must se-
lect gender and other attributes from dropdown
menus,? they were able to link these attributes to
the Twitter users. Using this process, they created
a large multi-lingual corpus of Twitter users and
genders.
We filter non-English tweets from this corpus
using the LID system of Bergsma et al (2012)
and also tweets containing URLs (since many of
these are spam) and re-tweets. We then filter users
with <40 tweets and randomly divide the remain-
ing users into 2282 training, 1140 development,
and 1141 test examples.
Classifier Set-up We train logistic-regression
classifiers on this gold standard data via the LI-
BLINEAR package (Fan et al, 2008). We optimize
the classifier?s regularization parameter on devel-
opment data and report final results on the held-
out test examples. We also report the results of
our new attribute-based strategies (Section 4) on
the test data. We report accuracy: the percentage
of examples labeled correctly.
Our classifiers use both BoW and Usr features
(Section 2). To increase the generality of our
BoW features, we preprocess the text by lower-
casing and converting all digits to special ?#? sym-
bols. We then create real-valued features that
encode the log-count of each word in the input.
While Burger et al (2011) found ?no apprecia-
ble difference in performance? when using either
binary presence/absence features or encoding the
frequency of the word, we found real-valued fea-
tures worked better in development experiments.
For the Usr features, we add special beginning and
ending characters to the username, and then create
features for all character n-grams of length two-
to-four in the modified username string. We in-
clude n-gram features with the original capitaliza-
tion pattern and separate features with the n-grams
lower-cased.
Unlabeled Data For Bootstrapped training, we
also use a pool of unlabeled Twitter data. This
pool comprises the union of 2.2 billion tweets
from 05/2009 to 10/2010 (O?Connor et al, 2010),
1.9 billion tweets collected from 07/2011 to
11/2012, and 80 million tweets collected from the
followers of 10-thousand location and language-
specific Twitter feeds. We filter this corpus as
above, except we do not put any restrictions on the
number of tweets needed per user. We also filter
any users that overlap with our gold standard data.
Bootstrapping Analysis We apply our Boot-
strapped auto-annotation strategy to this unlabeled
data, yielding 789,285 auto-annotated examples
of users and their tweets. The decisions of our
bootstrapping process reflect the true gender dis-
tribution; the auto-annotated data is 60.5% Fe-
male, remarkably close to the 60.9% proportion
in our gold standard test set. Figure 1 shows that
a wide range of self-distinguishing attributes are
used in the auto-annotation process. This is impor-
tant because if only a few attributes are used (e.g.
wife/husband or penis/vagina), we might system-
atically miss a segment of users (e.g. young people
that don?t have husbands or wives, or people that
don?t frequently talk about their genitalia). Thus a
wide range of common-sense knowledge is useful
for bootstrapping, which is one reason why auto-
matic approaches are needed to acquire it.
714
050000
100000
150000
200000
engagement ring ? 
Note: showing only first 10% of attributes used boyfriend ? 
hubby ? 
bra ? future wife ?  
natural hair ? 
jewelry ? 
bride ? beard 
? 
due date ? 
wife ? 
husband ? 
tux ? 
purse ? 
Figure 1: Frequency with which attributes are used to auto-annotate examples in the bootstrapping ap-
proach. The plot identifies some attributes and their corresponding class (labeled via gender symbol).
Majority-class baseline 60.9
Supervised on 100 examples 72.0
Supervised on 2282 examples 84.0
Supervised on 100 examples + ARules 74.7
Supervised on 2282 examples + ARules 84.7
Bootstrapped 86.0
BootStacked 87.2
Table 3: Classification accuracy (%) on gold stan-
dard test data for user gender prediction on Twitter
6 Results
Our main classification results are presented in Ta-
ble 3. The majority-class baseline for this task
is to always choose Female; this achieves an ac-
curacy of 60.9%. A standard classifier trained
on 100 gold-standard training examples improves
over this baseline, to 72.0%, while one with 2282
training examples achieves 84.0%. This latter re-
sult represents the current state-of-the-art: a clas-
sifier trained on thousands of gold standard exam-
ples, making use of both Usr and BoW features.
Our performance compares favourably to Burger
et al (2011), who achieved 81.4% using the same
features, but on a very different subset of the data
(also including tweets in other languages).7
Applying the ARules as a post-process signifi-
cantly improves performance in both cases (Mc-
Nemar?s, p<0.05). It is also possible to use the
ARules as a stand-alone system rather than as a
post-process, however the coverage is low: we find
a distinguishing attribute in 18.3% of the 695 Fe-
male instances in the test data, and make the cor-
7Note that it is possible to achieve even higher perfor-
mance on gender classification in social media if you have
further information about a user, such as their full first and
last name (Burger et al, 2011; Bergsma et al, 2013).
rect decision in 96.9% of these cases. We find a
distinguishing attribute in 11.4% of the 446 Male
instances, with 86.3% correct decisions.
The Bootstrapped system substantially im-
proves over the state-of-the-art, achieving 86% ac-
curacy and doing so without using any gold stan-
dard training data. This is important because hav-
ing thousands of gold standard annotations for ev-
ery possible user characterization task, in every
domain and social media platform, is not realis-
tic. Combining the bootstrapped classifier with
the gold standard annotations in the BootStacked
model results in further gains in performance.8
These results provide strong validation for both
the inherent utility of class-attributes knowledge in
user characterization and the effectiveness of our
specific strategies for exploiting such knowledge.
Figure 2 shows the learning curve of the Boot-
strapped classifier. Performance rises consistently
across all the auto-annotated training data; this
is encouraging because there is theoretically no
reason not to vastly increase the amount of auto-
annotated data by collecting an even larger col-
lection of tweets. Finally, note that most of the
gains of the Bootstrapped system appear to derive
from the tweet content itself, i.e. the BoW fea-
tures. However, the Usr features are also helpful
at most training sizes.
We provide some of the top-ranked features of
the Bootstrapped system in Table 4. We see that
a variety of other common-sense knowledge is
learned by the system (e.g., the association be-
tween males and urinals, boxers, fatherhood, etc.),
as well as stylistic clues (e.g. Female users using
betcha and xox in their writing). The username
8We observed no further gains in accuracy when applying
the ARules as a post-process on top of these systems.
715
 60
 65
 70
 75
 80
 85
 90
 100  1000  10000  100000  1e+06
A
cc
u
ra
cy
Number of auto-annotated training pts.
BoW+Usr
BoW
Usr
Figure 2: Learning curve for Bootstrapped
logistic-regression classifier, with automatically-
labeled data, for different feature classes.
features capture reasonable associations between
gender classes and particular names (such as mike,
tony, omar, etc.) and also between gender classes
and common nouns (such as guy, dad, sir, etc.).
7 Related Work
User Characterization The field of sociolin-
guistics has long been concerned with how various
morphological, phonological and stylistic aspects
of language can vary with a person?s age, gender,
social class, etc. (Fischer, 1968; Labov, 1972).
This early work therefore had an emphasis on ana-
lyzing the form of language, as opposed to its con-
tent. This emphasis continued into early machine
learning approaches, which predicted author prop-
erties based on the usage of function words, parts-
of-speech, punctuation (Koppel et al, 2002) and
spelling/grammatical errors (Koppel et al, 2005).
Recently, researchers have focused less on the
sociolinguistic implications and more on the tasks
themselves, naturally leading to classifiers with
feature representations capturing content in ad-
dition to style (Schler et al, 2006; Garera and
Yarowsky, 2009; Mukherjee and Liu, 2010). Our
work represents a logical next step for content-
based classification, a step partly suggested by
Schler et al (2006) who noted that ?those who
are interested in automatically profiling bloggers
for commercial purposes would be well served by
considering additional features - which we delib-
erately ignore in this study - such as author self-
identification.?
Male BoW features: wife, wifey, sucked, shave,
boner, boxers, missus, installed, manly, in-laws,
brah, urinal, kickoff, golf, comics, ubuntu, homo,
nhl, jedi, fatherhood, nigga, movember, algebra
Male Usr features: boy, mike, ben, guy, mr, dad,
jr, kid, tony, dog, lord, sir, omar, dude, man, big
Female BoW features: hubby, hubs, jewelry,
sewing, mascara, fabulous, bf, softball, betcha,
motherhood, perky, cozy, zumba, xox, cuddled,
belieber, bridesmaid, anorexic, jammies, pad
Female Usr features: mrs, mom, jen, lady, wife,
mary, joy, mama, pink, kim, diva, elle, woma, ms
Table 4: Examples of highly-weighted BoW (con-
tent) and Usr (username) features (in descending
order of weight) in the Bootstrapped system for
predicting user gender in Twitter.
Many recent papers have analyzed the lan-
guage of social media users, along dimensions
such as ethnicity (Eisenstein et al, 2011; Rao et
al., 2011; Pennacchiotti and Popescu, 2011; Fink
et al, 2012) time zone (Kiciman, 2010), polit-
ical orientation (Rao et al, 2010; Pennacchiotti
and Popescu, 2011) and gender (Rao et al, 2010;
Burger et al, 2011; Van Durme, 2012).
Class-Attribute Extraction The idea of using
simple patterns to extract useful semantic relations
goes back to Hearst (1992) who focused on hy-
ponyms. Hearst reports that she ?tried applying
this technique to meronymy (i.e., the part/whole
relation), but without great success.? Berland and
Charniak (1999) did have success using Hearst-
style patterns for part-whole detection, which they
attribute to their ?very large corpus and the use of
more refined statistical measures for ranking the
output.? Girju et al (2006) devised a supervised
classification scheme for part/whole relation dis-
covery that integrates the evidence from multiple
patterns. These efforts focused exclusively on the
meronymy relation as used in WordNet (Miller et
al., 1990). Indeed, Berland and Charniak (1999)
attempted to filter out attributes that were regarded
as qualities (like driveability) rather than parts
(like steering wheels) by removing words end-
ing with the suffixes -ness, -ing, and -ity. In our
work, such qualities are not filtered and are ulti-
mately valuable in classification; for example, the
attributes peak fertility and loveliness are highly
716
associated with females.
As subsequent research became more focused
on applications, looser definitions of class at-
tributes were adopted. Almuhareb and Poesio
(2004) automatically mined class attributes that in-
clude parts, qualities, and those with an ?agen-
tive? or ?telic? role with the class. Their ex-
tended set of attributes was shown to enable an
improved representation of nouns for the purpose
of clustering these nouns into semantic concepts.
Tokunaga et al (2005) define attributes as prop-
erties that can serve as focus words in questions
about a target class; e.g. director is an attribute
of a movie since one might ask, ?Who is the di-
rector of this movie?? Another line of research
has been motivated by the observation that much
of Internet search consists of people looking for
values of various class attributes (Bellare et al,
2007; Pas?ca and Van Durme, 2007; Pas?ca and Van
Durme, 2008; Alfonseca et al, 2010). By knowing
the attributes of different classes, search engines
can better recognize that queries such as ?altitude
guadalajara? or ?population guadalajara? are seek-
ing values for a particular city?s ?altitude? and
?population? attributes (Pas?ca and Van Durme,
2007). Finally, note that Van Durme et al (2008)
compared instance-based and class-based patterns
for broad-definition attribute extraction, and found
both to be effective.
Of course, text-mining with custom-designed
patterns is not the only way to extract class-
attribute information. Experts can manually spec-
ify the attributes of entities, as in the WordNet
project (Miller et al, 1990). Others have auto-
matically extracted attribute relations from dictio-
nary definitions (Richardson et al, 1998), struc-
tured online sources such as Wikipedia infoboxes,
(Wu and Weld, 2007) and large-scale collections
of high-quality tabular web data (Cafarella et al,
2008). Attribute extraction has also been viewed
as a sub-component or special case of the infor-
mation obtained by general-purpose knowledge
extractors (Schubert, 2002; Pantel and Pennac-
chiotti, 2006).
NLP Applications of Common-Sense Knowl-
edge The kind of information derived from
class-attribute extraction is sometimes referred to
as a type of common-sense knowledge. The need
for computer programs to represent common-
sense knowledge has been recognized since the
work of McCarthy (1959). Lenat et al (1990)
defines common sense as ?human consensus re-
ality knowledge: the facts and concepts that you
and I know and which we each assume the other
knows.?
While we are the first to exploit common-
sense knowledge in user characterization, com-
mon sense has been applied to a range of other
problems in natural language processing. In many
ways WordNet can be regarded as a collection of
common-sense relationships. WordNet has been
applied in a myriad of NLP applications, includ-
ing in seminal works on semantic-role labeling
(Gildea and Jurafsky, 2002), coreference resolu-
tion (Soon et al, 2001) and spelling correction
(Budanitsky and Hirst, 2006). Also, many ap-
proaches to the task of sentiment analysis ?be-
gin with a large lexicon of words marked with
their prior polarity? (Wilson et al, 2009). Like
our class-attribute associations, the common-sense
knowledge that the word cool is positive while
unethical is negative can be learned from asso-
ciations in web-scale data (Turney, 2002). We
might also view information about synonyms or
conceptually-similar words as a kind of common-
sense knowledge. In this perspective, our work
is related to recent work that has extracted
distributionally-similar words from web-scale data
and applied this knowledge in tasks such as
named-entity recognition (Lin and Wu, 2009) and
dependency parsing (Ta?ckstro?m et al, 2012).
8 Conclusion
We have proposed, developed and successfully
evaluated a novel approach to user characteriza-
tion based on exploiting knowledge of user class
attributes. The knowledge is obtained using a new
algorithm that discovers distinguishing attributes
of particular classes. Our approach to discovering
distinguishing attributes represents a significant
new direction for research in class-attribute extrac-
tion, and provides a valuable bridge between the
fields of user characterization and lexical knowl-
edge extraction.
We presented three effective techniques for
leveraging this knowledge within the framework
of supervised user characterization: rule-based
post-processing, a learning-by-bootstrapping ap-
proach, and a stacking approach that integrates the
predictions of the bootstrapped system into a sys-
tem trained on annotated gold-standard training
data. All techniques lead to significant improve-
717
ments over state-of-the-art supervised systems on
the task of Twitter gender classification.
While our technique has advanced the state-of-
the-art on this important task, our approach may
prove even more useful on other tasks where train-
ing on thousands of gold-standard examples is not
even an option. Currently we are exploring the
prediction of finer-grained user roles, such as stu-
dent, waitress, parent, and so forth, based on ex-
tensions to the process laid out here.
References
Enrique Alfonseca, Marius Pas?ca, and Enrique
Robledo-Arnuncio. 2010. Acquisition of instance
attributes via labeled and related instances. In Proc.
SIGIR, pages 58?65.
Abdulrahman Almuhareb and Massimo Poesio. 2004.
Attribute-based and value-based clustering: An
evaluation. In Proc. EMNLP, pages 158?165.
Kedar Bellare, Partha P. Talukdar, Giridhar Kumaran,
Fernando Pereira, Mark Liberman, Andrew McCal-
lum, and Mark Dredze. 2007. Lightly-Supervised
Attribute Extraction. In NIPS Workshop on Machine
Learning for Web Search.
Shane Bergsma and Dekang Lin. 2006. Bootstrapping
path-based pronoun resolution. In Proc. Coling-
ACL, pages 33?40.
Shane Bergsma, Paul McNamee, Mossaab Bagdouri,
Clayton Fink, and Theresa Wilson. 2012. Language
identification for creating language-specific Twitter
collections. In Proceedings of the Second Workshop
on Language in Social Media, pages 65?74.
Shane Bergsma, Mark Dredze, Benjamin Van
Durme, Theresa Wilson, and David Yarowsky.
2013. Broadly improving user classification via
communication-based name and location clustering
on twitter. In Proc. NAACL.
Matthew Berland and Eugene Charniak. 1999. Find-
ing parts in very large corpora. In Proc. ACL, pages
57?64.
Alexander Budanitsky and Graeme Hirst. 2006. Eval-
uating WordNet-based measures of lexical semantic
relatedness. Computational Linguistics, 32(1):13?
47.
John D. Burger and John C. Henderson. 2006. An
exploration of observable features related to blogger
age. In Proc. AAAI Spring Symposium: Computa-
tional Approaches to Analyzing Weblogs, pages 15?
20.
John D. Burger, John Henderson, George Kim, and
Guido Zarrella. 2011. Discriminating gender on
Twitter. In Proc. EMNLP, pages 1301?1309.
Michael J. Cafarella, Alon Y. Halevy, Daisy Zhe Wang,
Eugene Wu, and Yang Zhang. 2008. WebTables:
exploring the power of tables on the web. Proc.
PVLDB, 1(1):538?549.
Kenneth W. Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicog-
raphy. Computational Linguistics, 16(1).
Hal Daume? III and Daniel Marcu. 2006. Domain adap-
tation for statistical classifiers. Journal of Artificial
Intelligence Research, 26.
Jacob Eisenstein, Noah A. Smith, and Eric P. Xing.
2011. Discovering sociolinguistic associations with
structured sparsity. In Proc. ACL, pages 1365?1374.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A library for large linear classification. J. Mach.
Learn. Res., 9:1871?1874.
Clayton Fink, Jonathon Kopecky, Nathan Bos, and
Max Thomas. 2012. Mapping the Twitterverse in
the developing world: An analysis of social media
use in Nigeria. In Proc. International Conference on
Social Computing, Behavioral Modeling, and Pre-
diction, pages 164?171.
John L. Fischer. 1968. Social influences on the choice
of a linguistic variant. Word, 14:47?56.
Nikesh Garera and David Yarowsky. 2009. Modeling
latent biographic attributes in conversational genres.
In Proc. ACL-IJCNLP, pages 710?718.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational Linguis-
tics, 28:245?288.
Roxana Girju, Adriana Badulescu, and Dan Moldovan.
2006. Automatic discovery of part-whole relations.
Computational Linguistics, 32(1):83?135.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proc. Coling,
pages 539?545.
Emre Kiciman. 2010. Language differences and meta-
data features on Twitter. In Proc. SIGIR 2010 Web
N-gram Workshop, pages 47?51.
Moshe Koppel and Jonathan Schler. 2004. Authorship
verification as a one-class classification problem. In
Proc. ICML, pages 489?495.
Moshe Koppel, Shlomo Argamon, and Anat Rachel
Shimoni. 2002. Automatically categorizing writ-
ten texts by author gender. Literary and Linguistic
Computing, 17(4):401?412.
Moshe Koppel, Jonathan Schler, and Kfir Zigdon.
2005. Determining an author?s native language by
mining a text for errors. In Proc. KDD, pages 624?
628.
718
William Labov. 1972. Sociolinguistic Patterns. Uni-
versity of Pennsylvania Press.
Douglas B. Lenat, R. V. Guha, Karen Pittman, Dex-
ter Pratt, and Mary Shepherd. 1990. CYC: toward
programs with common sense. Commun. ACM,
33(8):30?49.
Dekang Lin and Xiaoyun Wu. 2009. Phrase clustering
for discriminative learning. In Proc. ACL-IJCNLP,
pages 1030?1038.
Dekang Lin, Kenneth Church, Heng Ji, Satoshi Sekine,
David Yarowsky, Shane Bergsma, Kailash Patil,
Emily Pitler, Rachel Lathbury, Vikram Rao, Kapil
Dalwani, and Sushant Narsale. 2010. New tools for
web-scale N-grams. In Proc. LREC, pages 2221?
2227.
John McCarthy. 1959. Programs with common sense.
In Proc. Teddington Conference on the Mechaniza-
tion of Thought Processes, pages 75?91. London:
Her Majesty?s Stationery Office.
Ken McRae, George S. Cree, Mark S. Seidenberg, and
Chris McNorgan. 2005. Semantic feature pro-
duction norms for a large set of living and nonliv-
ing things. Behavior Research Methods, 37(4):547?
559.
George A. Miller, Richard Beckwith, Christiane Fell-
baum, Derek Gross, and Katherine J. Miller.
1990. Introduction to WordNet: an on-line lexical
database. International Journal of Lexicography,
3(4).
Arjun Mukherjee and Bing Liu. 2010. Improving gen-
der classification of blog authors. In Proc. EMNLP,
pages 207?217.
Brendan O?Connor, Ramnath Balasubramanyan,
Bryan R. Routledge, and Noah A. Smith. 2010.
From tweets to polls: Linking text sentiment to
public opinion time series. In Proc. ICWSM, pages
122?129.
Olutobi Owoputi, Brendan O?Connor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A.
Smith. 2013. Improved part-of-speech tagging for
online conversational text with word clusters. In
Proc. of NAACL.
Patrick Pantel and Marco Pennacchiotti. 2006.
Espresso: leveraging generic patterns for automati-
cally harvesting semantic relations. In Proc. Coling-
ACL, pages 113?120.
Marius Pas?ca and Benjamin Van Durme. 2007. What
you seek is what you get: extraction of class at-
tributes from query logs. In Proc. IJCAI, pages
2832?2837.
Marius Pas?ca and Benjamin Van Durme. 2008.
Weakly-supervised acquisition of open-domain
classes and class attributes from web documents and
query logs. In Proc. ACL-08: HLT, pages 19?27.
Michael Paul and Mark Dredze. 2011. You are what
you tweet: Analyzing Twitter for public health. In
Proc. ICWSM, pages 265?272.
Marco Pennacchiotti and Ana-Maria Popescu. 2011.
A machine learning approach to Twitter user classi-
fication. In Proc. ICWSM, pages 281?288.
Xuan-Hieu Phan. 2006. CRFTagger: CRF English
POS Tagger. crftagger.sourceforge.net.
Delip Rao, David Yarowsky, Abhishek Shreevats, and
Manaswi Gupta. 2010. Classifying latent user at-
tributes in Twitter. In Proc. International Work-
shop on Search and Mining User-Generated Con-
tents, pages 37?44.
Delip Rao, Michael Paul, Clay Fink, David Yarowsky,
Timothy Oates, and Glen Coppersmith. 2011. Hi-
erarchical bayesian models for latent attribute detec-
tion in social media. In Proc. ICWSM, pages 598?
601.
Joseph Reisinger and Marius Pas?ca. 2009. Latent
variable models of concept-attribute attachment. In
Proc. ACL-IJCNLP, pages 620?628.
Stephen D. Richardson, William B. Dolan, and Lucy
Vanderwende. 1998. MindNet: Acquiring and
structuring semantic information from text. In Proc.
ACL-Coling, pages 1098?1102.
Jonathan Schler, Moshe Koppel, Shlomo Argamon,
and James W. Pennebaker. 2006. Effects of age and
gender on blogging. In Proc. AAAI Spring Sympo-
sium: Computational Approaches to Analyzing We-
blogs, pages 199?205.
Lenhart Schubert. 2002. Can we derive general world
knowledge from texts? In Proc. HLT, pages 84?87.
Fabrizio Sebastiani. 2002. Machine learning in au-
tomated text categorization. ACM Comput. Surv.,
34:1?47.
Ian Soboroff, Dean McCullough, Jimmy Lin, Craig
Macdonald, Iadh Ounis, and Richard McCreadie.
2012. Evaluating real-time search over tweets. In
Proc. ICWSM.
Wee Meng Soon, Hwee Tou Ng, and Daniel
Chung Yong Lim. 2001. A machine learning ap-
proach to coreference resolution of noun phrases.
Computational Linguistics, 27(4).
Oscar Ta?ckstro?m, Ryan McDonald, and Jakob Uszko-
reit. 2012. Cross-lingual word clusters for direct
transfer of linguistic structure. In Proc. NAACL-
HLT, pages 477?487.
Kosuke Tokunaga, Jun?ichi Kazama, and Kentaro Tori-
sawa. 2005. Automatic discovery of attribute words
from web documents. In Proc. IJCNLP, pages 106?
118.
719
Peter D. Turney. 2002. Thumbs up or thumbs down?
Semantic orientation applied to unsupervised classi-
fication of reviews. In Proc. ACL, pages 417?424.
Benjamin Van Durme, Ting Qian, and Lenhart Schu-
bert. 2008. Class-driven attribute extraction. In
Proc. Coling, pages 921?928.
Benjamin Van Durme. 2012. Streaming analysis of
discourse participants. In Proc. EMNLP-CoNLL,
pages 48?58.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2009. Recognizing contextual polarity: An explo-
ration of features for phrase-level sentiment analy-
sis. Computational Linguistics., 35(3):399?433.
Fei Wu and Daniel S. Weld. 2007. Autonomously se-
mantifying Wikipedia. In Proc. CIKM, pages 41?
50.
Omar Zaidan, Jason Eisner, and Christine Piatko.
2007. Using ?annotator rationales? to improve ma-
chine learning for text categorization. In Proc.
NAACL-HLT.
720
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 866?872,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Explicit and Implicit Syntactic Features for Text Classification
Matt Post1 and Shane Bergsma1,2
1Human Language Technology Center of Excellence
2Center for Language and Speech Processing
Johns Hopkins University
Baltimore, MD
Abstract
Syntactic features are useful for many
text classification tasks. Among these,
tree kernels (Collins and Duffy, 2001)
have been perhaps the most robust and
effective syntactic tool, appealing for
their empirical success, but also be-
cause they do not require an answer
to the difficult question of which tree
features to use for a given task. We
compare tree kernels to different ex-
plicit sets of tree features on five diverse
tasks, and find that explicit features of-
ten perform as well as tree kernels on
accuracy and always in orders of mag-
nitude less time, and with smaller mod-
els. Since explicit features are easy to
generate and use (with publicly avail-
able tools), we suggest they should al-
ways be included as baseline compar-
isons in tree kernel method evaluations.
1 Introduction
Features computed over parse trees are use-
ful for a range of discriminative tasks, in-
cluding authorship attribution (Baayen et al,
1996), parse reranking (Collins and Duffy,
2002), language modeling (Cherry and Quirk,
2008), and native-language detection (Wong
and Dras, 2011). A major distinction among
these uses of syntax is how the features are rep-
resented. The implicit approach uses tree
kernels (Collins and Duffy, 2001), which make
predictions with inner products between tree
pairs. These products can be computed effi-
ciently with a dynamic program that produces
weighted counts of all the shared tree frag-
ments between a pair of trees, essentially in-
corporating all fragments without representing
any of them explicitly. Tree kernel approaches
have been applied successfully in many areas
of NLP (Collins and Duffy, 2002; Moschitti,
2004; Pighin and Moschitti, 2009).
Tree kernels were inspired in part by ideas
from Data-Oriented Parsing (Scha, 1990; Bod,
1993), which was in turn motivated by uncer-
tainty about which fragments to include in a
grammar. However, manual and automatic
approaches to inducing tree fragments have
recently been found to be useful in an ex-
plicit approach to text classification, which
employs specific tree fragments as features in
standard classifiers (Post, 2011; Wong and
Dras, 2011; Swanson and Charniak, 2012).
These feature sets necessarily represent only a
small subset of all possible tree patterns, leav-
ing open the question of what further gains
might be had from the unusued fragments.
Somewhat surprisingly, explicit and implicit
syntactic features have been explored largely
independently. Here, we compare them on a
range of classification tasks: (1,2) grammati-
cal classification (is a sentence written by a hu-
man?), (3) question classification (what type
of answer is sought by this question?), and
(4,5) native language prediction (what is the
native language of a text?s author?).
Our main contribution is to show that an ex-
plicit syntactic feature set performs as well or
better than tree kernels on each tested task,
and in orders of magnitude less time. Since
explicit features are simple to generate (with
publicly available tools) and flexible to use, we
recommend they be included as baseline com-
parisons in tree kernel method evaluations.
2 Experimental setup
We used the following feature sets:
N-grams All unigrams and bigrams.1
1Experiments with trigrams did not show any im-
866
CFG rules Counts of depth-one context-
free grammar (CFG) productions obtained
from the Berkeley parser (Petrov et al, 2006).
C&J features The parse-tree reranking
feature set of Charniak and Johnson (2005),
extracted from the Berkeley parse trees.
TSG features We also parsed with a
Bayesian tree substitution grammar (Post and
Gildea, 2009, TSG)2 and extracted fragment
counts from Viterbi derivations.
We build classifiers with Liblinear3 (Fan
et al, 2008). We divided each dataset into
training, dev, and test sets. We then trained
an L2-regularized L1-loss support vector ma-
chine (-s 3) with a bias parameter of 1 (-B 1),
optimizing the regularization parameter (-c)
on the dev set over the range {0.0001 . . . 100}
by multiples of 10. The best model was then
used to classify the test set. A sentence length
feature was included for every sentence.
For tree kernels, we used SVM-light-TK4
(Moschitti, 2004; Moschitti, 2006) with the
default settings (-t 5 -D 1 -L 0.4),5 which
also solves an L2-regularized L1-loss SVM op-
timization problem. We tuned the regulariza-
tion parameter (-c) on the dev set in the same
manner as described above, providing 4 GB of
memory to the kernel cache (-m 4000).6 We
used subset tree kernels, which compute the
similarity between two trees by implicitly enu-
merating all possible fragments of the trees (in
contrast with subtree kernels, where all frag-
ments fully extend to the leaves).
3 Tasks
Table 1 summarizes our datasets.
3.1 Coarse grammatical classification
Our first comparison is coarse grammatical
classification, where the goal is to distin-
guish between human-written sentences and
?pseudo-negative? sentences sampled from a
trigram language model constructed from in-
provement.
2github.com/mjpost/dptsg
3www.csie.ntu.edu.tw/~cjlin/liblinear/
4disi.unitn.it/moschitti/Tree-Kernel.htm
5Optimizing SVM-TK?s decay parameter (-L) did
not improve test-set accuracy, but did increase training
time (squaring the number of hyperparameter combi-
nations to evaluate), so we stuck with the default.
6Increased from the default of 40 MB, which halves
the running time.
train dev test
Coarse grammaticality (BLLIP)
sentences 100,000 6,000 6,000
Fine grammaticality (PTB)
sentences 79,664 3,978 3,840
Question classification (TREC-10)
sentences 4,907 545 500
Native language (ICLE; 7 languages)
documents 490 105 175
sentences 17,715 3,968 6,777
Native language (ACL; 5 languages)
documents 987 195 185
sentences 146,257 28,139 28,403
Table 1: Datasets.
system accuracy CPU time
Chance 50.0 -
N-gram 68.4 minutes
CFG 86.3 minutes
TSG 89.8 minutes
C&J 92.9 an hour
SVM-TK 91.0 a week
Table 2: Coarse grammaticality. CPU time is
for classifier setup, training, and testing.
domain data (Okanohara and Tsujii, 2007).
Cherry and Quirk (2008) first applied syn-
tax to this task, learning weighted parameters
for a CFG with a latent SVM. Post (2011)
found further improvements with fragment-
based representations (TSGs and C&J) with a
regular SVM. Here, we compare their results
to kernel methods. We repeat Post?s experi-
ments on the BLLIP dataset,7 using his exact
data splits (Table 2). To our knowledge, tree
kernels have not been applied to this task.
3.2 Fine grammatical classification
Real-world grammaticality judgments require
much finer-grained distinctions than the
coarse ones of the previous section (for exam-
ple, marking dropped determiners or wrong
verb inflections). For this task, we too pos-
itive examples from all sentences of sections
2?21 of the WSJ portion of the Penn Tree-
bank (Marcus et al, 1993). Negative exam-
ples were created by inserting one or two errors
7LDC Catalog No. LDC2000T43
867
system accuracy CPU time
Wong & Dras 60.6 -
Chance 50.0 -
N-gram 61.4 minutes
CFG 64.5 minutes
TSG 67.0 minutes
C&J 71.9 an hour
SVM-TK 67.8 weeks
Table 3: Fine-grained classification accuracy
(the Wong and Dras (2010) score is the highest
score from the last column of their Table 3).
system accuracy CPU time
Pighin & Moschitti 86.6 -
Bigram 73.2 seconds
CFG 90.0 seconds
TSG 85.6 seconds
C&J 89.6 minutes
SVM-TK 87.7 twenty min.
Table 4: Question classification (6 classes).
into the parse trees from the positive data us-
ing GenERRate (Foster and Andersen, 2009).
An example sentence pair is But the ballplay-
ers disagree[ing], where the negative exam-
ple incorrectly inflects the verb. Wong and
Dras (2010) reported good results with parsers
trained separately on the positive and negative
sides of the training data and classifiers built
from comparisons between the CFG produc-
tions of those parsers. We obtained their data
splits (described as NoisyWSJ in their paper)
and repeat their experiments here (Table 3).
3.3 Question Classification
We look next at question classification (QC).
Li and Roth (2002) introduced the TREC-10
dataset,8 a set of questions paired with labels
that categorize the question by the type of an-
swer it seeks. The labels are organized hi-
erarchically into six (coarse) top-level labels
and fifty (fine) refinements. An example ques-
tion from the ENTY/animal category is What
was the first domesticated bird?. Table 4 con-
tains results predicting just the coarse labels.
We compare to Pighin and Moschitti (2009),
and also repeat their experiments, finding a
slightly better result for them.
8cogcomp.cs.illinois.edu/Data/QA/QC/
system sent. voting whole
Wong & Dras - - 80.0
Style 42.0 75.3 86.8
CFG 39.5 73.2 83.7
TSG 38.7 72.1 83.2
C&J 42.9 76.3 86.3
SVM-TK 40.7 69.5 -
Style 42.5 65.3 83.7
CFG 39.2 52.6 86.3
TSG 40.4 56.8 84.7
C&J 49.2 66.3 81.1
SVM-TK 42.1 52.6 -
Table 5: Accuracy on ICLE (7 languages, top)
and ACL (five, bottom) datasets at the sen-
tence and document levels. All documents
were signature-stylized (?3.4).
We also experimented with the refined ver-
sion of the task, where we directly predict one
of the fifty refined categories, and found nearly
identical relative results, with the best explicit
feature set (CFG) returning an accuracy of
83.6% (in seconds), and the tree kernel system
69.8% (in an hour). For reference, Zhang and
Lee (2003) report 80.2% accuracy when train-
ing on the full training set (5,500 examples)
with an SVM and bag-of-words features.9
3.4 Native language identification
Native language identification (NLI) is the
task of determining a text?s author?s native
language. This is usually cast as a document-
level task, since there are often not enough
cues to identify native languages at smaller
granularities. As such, this task presents a
challenge to tree kernels, which are defined at
the level of a single parse tree and have no ob-
vious document-level extension. Table 5 there-
fore presents three evaluations: (a) sentence-
level accuracy, and document-level accuracy
from (b) sentence-level voting and (c) direct,
whole-document classification.
We perform these experiments on two
datasets. In order to mitigate topic bias10 and
other problems that have been reported with
9Pighin and Moschitti (2009) did not report results
on this version of the task.
10E.g., when we train with all words, the keyword
?Japanese? is a strong indicator for Japanese authors,
while ?Arabic? is a strong indicator for English ones.
868
the ICLE dataset (Tetreault et al, 2012),11 we
preprocessed each dataset into two signature-
stylized versions by replacing all words not in a
stopword list.12 The first version replaces non-
stopwords with word classes computed from
surface-form signatures,13 and the second with
POS tags.14 N-gram features are then taken
from both stylized versions of the corpus.
Restricting the feature representation to be
topic-independent is standard-practice in sty-
lometric tasks like authorship attribution, gen-
der identification, and native-language identi-
fication (Mosteller and Wallace, 1984; Koppel
et al, 2003; Tomokiyo and Jones, 2001).
3.4.1 ICLE v.2
The first dataset is a seven-language subset
of the International Corpus of Learner En-
glish, Version 2 (ICLE) (Granger et al, 2009),
which contains 3.7 million words of English
documents written by people with sixteen dif-
ferent native languages. Table 1 contains
scores, including one reported by Wong and
Dras (2011), who used the CFG and C&J fea-
tures, and whose data splits we mirror.15
3.4.2 ACL Anthology Network
We also experimented with native language
classification on scientific documents using
a version of the ACL Anthology Network
(Radev et al, 2009, AAN) annotated for ex-
periments in stylemetric tasks, including a
native/non-native author judgment (Bergsma
et al, 2012). For NLI, we further anno-
tated this dataset in a semi-automatic fash-
ion for the five most-common native languages
of ACL authors in our training era: English,
Japanese, German, Chinese, and French. The
annotation heuristics, designed to favor pre-
cision over recall, provided annotations for
1,959 of 8,483 papers (23%) in the 2001?2009
AAN.16
11Including prompts, characters, and special tokens
that correlate strongly with particular outcomes.
12The stopword list contains the set of 524 SMART-
system stopwords used by Tomokiyo and Jones (2001),
plus punctuation and Latin abbreviations.
13For example, suffix and capitalization.
14Via CRFTagger (Phan, 2006).
15Tetreault et al reported accuracies up to 90.1 in a
cross-validation setting that isn?t directly comparable.
16Details and data at old-site.clsp.jhu.edu/
~sbergsma/Stylo/.
60
70
80
90
100
0 0.01 0.1 1 10 100 1,000
a
c
c
u
r
a
c
y
training time (thousands of seconds)
size CFG CFG TSG TSG TSG+ TSG+ C&J C&J SVM-TK SVM-TK uSVM-TK USVM-TK
100 7 62.6 6 61.0 8 73.1 407 72.8 13 62.9 27 62.7
300 7 68.0 6 65.0 8 77.9 412 77.5 46 70.8 174 70.9
1000 7 73.3 6 70.9 9 78.4 433 82.2 227 77.1 1475 77.4
3000 9 75.8 7 77.5 12 82.3 465 87.1 1034 81.4 4394 81.2
10000 13 80.8 11 82.5 32 85.2 708 89.9 8984 85.5 6691 85.3
30000 37 83.5 29 85.8 108 87.7 1276 92.7 72859 88.8 7789 87.8
100000 133 86.3 85 89.1 406 89.8 3152 93.0 873969 91.0 8488 89.0
CFG
TSG
C&J
SVM-TK
uSVM-TK
uSVM-TK USVM-TK
1010.35 62.7
2628.84 70.9
7264.65 77.4
25447.47 81.2
29298.76 85.3
45938.05 87.8
48570.46 89.0
OLD VALUES
Figure 1: Train ng time (1000 econds) vs.
test accuracy for coarse grammaticality, plot-
ting test scores from models trained on 100,
300, 1k, 3k, 10k, 30k, and 100k instances.
4 Discussion
Syntactic features improve upon the n-gram
baseline for all tasks except whole-document
classification for ICLE. Tree kernels are often
among the best, but always trail (by orders
of magnitude) when runtime is considered.
Constructing the multi-class SVM-TK models
for the NLI tasks in particular was computa-
tionally burdensome, requiring cpu-months of
time. The C&J features are similarly often the
best, but incur a runtime cost due to the large
models. CFG and TSG features balance per-
formance, model size, and runtime. We now
compare these approaches in more depth.
4.1 Training time versus accuracy
Tree kernel training is quadratic in the size of
the training data, and its empirical slowness
is known. It is informative to examine learn-
ing curves to see how the time-accuracy trade-
offs extrapolate. We compared models trained
on the first 100, 300, 1k, 3k, 10k, 30k, and
100k data points of the coarse grammaticality
dataset, split evenly between positive and neg-
ative examples (Figure 1). SVM-TK improves
over the TSG and CFG models in the limit,
but at an extraordinary cost in training time:
100k training examples is already pushing the
bounds of practicality for tree kernel learning,
and generating curve?s next point would re-
quire several months of time. Kernel methods
also produce large models that result in slow
test-time performance, a problem dubbed the
?curse of kernelization? (Wang et al, 2010).
Approximate kernel methods designed to
scale to large datasets address this (Severyn
869
and Moschitti, 2010). We investigated the
uSVM-TK toolkit,17 which enables tuning the
tradeoff between training time and accuracy.
While faster than SVM-TK, its performance
was never better than explicit methods along
both dimensions (time and accuracy).
4.2 Overfitting
Overfitting is also a problem for kernel meth-
ods. The best models often had a huge number
of support vectors, achieving near-perfect ac-
curacy on the training set but making many
errors on the dev. and test sets. On the ICLE
task, close to 75% of all the training exam-
ples were used as support vectors. We found
only half as many support vectors used for the
explicit representations, implying less error
(Vapnik, 1998), and saw much lower variance
between training and testing performance.
4.3 Which fragments?
Our findings support the observations of
Cumby and Roth (2003), who point out that
kernels introduce a large number of irrelevant
features that may be especially harmful in
small-data settings, and that, when possible, it
is often better to have a set of explicit, relevant
features. In other words, it is better to have
the right features than all of them. Tree ker-
nels provide a robust, efficiently-computable
measure of comparison, but they also skirt the
difficult question, Which fragments?
So what are the ?right? features? Table 6)
presents an intuitive list from the coarse gram-
maticality task: phenomena such as balanced
parenthetical phrases and quotations are asso-
ciated with grammaticality, while small, flat,
abstract rules indicate samples from the n-
gram model. Similar intuitive results hold for
the other tasks. The immediate interpretabil-
ity of the explicit formalisms is another ad-
vantage, although recent work has shown that
weights on the implicit features can also be ob-
tained after a kind of linearization of the tree
kernel (Pighin and Moschitti, 2009).
Ultimately, which features matter is task-
dependent, and skirting the question is ad-
vantageous in many settings. But it is also
encouraging that methods for selecting frag-
ments and other tree features work so well,
17disi.unitn.it/~severyn/code.html
(TOP (S ? S , ? NP (VP (VBZ says) ADVP) .))
(FRAG (X SYM) VP .)
(PRN (-LRB- -LRB-) S (-RRB- -RRB-))
(PRN (-LRB- -LRB-) NP (-RRB- -RRB-))
(S NP VP .)
(NP (NP DT CD (NN %)) PP)
(NP DT)
(PP (IN of))
(TOP (NP NP PP PP .))
(NP DT JJ NNS)
Table 6: The highest- and lowest-weighted
TSG features (coarse grammaticality).
yielding quick, light-weight models that con-
trast with the heavy machinery of tree kernels.
5 Conclusion
Tree kernels provide a robust measure of com-
parison between trees, effectively making use
of all fragments. We have shown that for
some tasks, it is sufficient (and advantageous)
to instead use an explicitly-represented subset
of them. In addition to their flexibility and
interpetability, explicit syntactic features of-
ten outperformed tree kernels in accuracy, and
even where they did not, the cost was multiple
orders of magnitude increase in both training
and testing time. These results were consistent
across a range of task types, dataset sizes, and
classification arities (binary and multiclass).
There are a number of important caveats.
We explored a range of data settings, but
there are many others where tree kernels have
been proven useful, such as parse tree rerank-
ing (Collins and Duffy, 2002; Shen and Joshi,
2003), sentence subjectivity (Suzuki et al,
2004), pronoun resolution (Yang et al, 2006),
relation extraction (Culotta and Sorensen,
2004), machine translation evaluation (Liu
and Gildea, 2005), predicate-argument recog-
nition, and semantic role labeling (Pighin and
Moschitti, 2009). There are also tree ker-
nel variations such as dependency tree kernels
(Culotta and Sorensen, 2004) and shallow se-
mantic tree kernels (Moschitti et al, 2007).
These variables provide a rich environment for
future work; in the meantime, we take these re-
sults as compelling motivation for the contin-
ued development of explicit syntactic features
(both manual and automatically induced), and
suggest that such features should be part of
the baseline systems on applicable discrimina-
tive NLP tasks.
870
References
Harald Baayen, Hans Van Halteren, and Fiona
Tweedie. 1996. Outside the cave of shadows:
Using syntactic annotation to enhance author-
ship attribution. Literary and Linguistic Com-
puting, 11(3):121.
Shane Bergsma, Matt Post, and David Yarowsky.
2012. Stylometric analysis of scientific arti-
cles. In Proc. of NAACL-HLT, pages 327?337,
Montre?al, Canada, June. Association for Com-
putational Linguistics.
Rens Bod. 1993. Using an annotated corpus as a
stochastic grammar. In Proc. of ACL, Colum-
bus, Ohio, USA, June.
Eugene Charniak and Mark Johnson. 2005.
Coarse-to-fine n-best parsing and MaxEnt dis-
criminative reranking. In Proc. of ACL, pages
173?180, Ann Arbor, Michigan, USA, June.
Colin Cherry and Chris Quirk. 2008. Discrimi-
native, syntactic language modeling through la-
tent SVMs. In Proc. of AMTA, Waikiki, Hawaii,
USA, October.
Michael Collins and Nigel Duffy. 2001. Convolu-
tion kernels for natural language. In Proc. of
NIPS.
Michael Collins and Nigel Duffy. 2002. New rank-
ing algorithms for parsing and tagging: kernels
over discrete structures, and the voted percep-
tron. In Proc. of ACL, pages 173?180, Philadel-
phia, Pennsylvania, USA, July.
Aron Culotta and Jeffrey Sorensen. 2004. Depen-
dency tree kernels for relation extraction. In
Proc. of ACL, pages 423?429.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh,
Xiang-Rui Wang, and Chih-Jen Lin. 2008. LIB-
LINEAR: A library for large linear classification.
Journal of Machine Learning Research, 9:1871?
1874.
Jennifer Foster and ?istein E. Andersen. 2009.
GenERRate: Generating errors for use in gram-
matical error detection. In Proceedings of the
fourth workshop on innovative use of NLP for
building educational applications, pages 82?90.
Sylviane Granger, Estelle Dagneaux, Fanny Me-
unier, and Magali Paquot. 2009. The Inter-
national Corpus of Learner English. Version 2.
Handbook and CD-Rom.
Moshe Koppel, Shlomo Argamon, and Anat Rachel
Shimoni. 2003. Automatically categorizing
written texts by author gender. Literary and
Linguistic Computing, 17(4):401?412.
Xin Li and Dan Roth. 2002. Learning question
classifiers. In Proc. of COLING, pages 1?7.
Ding Liu and Daniel Gildea. 2005. Syntactic fea-
tures for evaluation of machine translation. In
Proceedings of the ACL Workshop on Intrinsic
and Extrinsic Evaluation Measures for Machine
Translation and/or Summarization, pages 25?
32.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large an-
notated corpus of English: The Penn Treebank.
Computational Linguistics, 19(2):330.
Alessandro Moschitti, Silvia Quarteroni, Roberto
Basili, and Suresh Manandhar. 2007. Exploit-
ing syntactic and shallow semantic kernels for
question answer classification. In Proc. of ACL,
pages 776?783, Prague, Czech Republic, June.
Alessandro Moschitti. 2004. A study on convo-
lution kernels for shallow semantic parsing. In
Proc. of ACL.
Alessandro Moschitti. 2006. Making tree kernels
practical for natural language learning. In Proc.
of EACL, volume 6, pages 113?120.
Frederick Mosteller and David L. Wallace. 1984.
Applied Bayesian and Classical Inference: The
Case of the Federalist Papers. Springer-Verlag.
Daisuke Okanohara and Jun?ichi Tsujii. 2007.
A discriminative language model with pseudo-
negative samples. In Proc. of ACL, Prague,
Czech Republic, June.
Slav Petrov, Leon Barrett, Romain Thibaux, and
Dan Klein. 2006. Learning accurate, compact,
and interpretable tree annotation. In Proc. of
ACL, Sydney, Australia, July.
Xuan-Hieu Phan. 2006. CRFTagger: CRF En-
glish POS Tagger. crftagger.sourceforge.net.
Daniele Pighin and Alessandro Moschitti. 2009.
Reverse engineering of tree kernel feature spaces.
In Proc. of EMNLP, pages 111?120, Singapore,
August.
Matt Post and Daniel Gildea. 2009. Bayesian
learning of a tree substitution grammar. In
Proc. of ACL (short paper track), Suntec, Sin-
gapore, August.
Matt Post. 2011. Judging grammaticality with
tree substitution grammar derivations. In Proc.
of ACL, Portland, Oregon, USA, June.
Dragomir R. Radev, Pradeep Muthukrishnan, and
Vahed Qazvinian. 2009. The ACL anthology
network corpus. In Proc. of ACL Workshop on
Natural Language Processing and Information
Retrieval for Digital Libraries, pages 54?61.
Remko Scha. 1990. Taaltheorie en taaltechnologie;
competence en performance. In R. de Kort and
G.L.J. Leerdam, editors, Computertoepassingen
in de neerlandistiek, pages 7?22, Almere, the
Netherlands. De Vereniging.
871
Aliaksei Severyn and Alessandro Moschitti. 2010.
Large-scale support vector learning with struc-
tural kernels. In Proc. of ECML/PKDD, pages
229?244.
Libin Shen and Aravind K. Joshi. 2003. An SVM-
based voting algorithm with application to parse
reranking. In Proc. of CoNLL, pages 9?16.
Jun Suzuki, Hideki Isozaki, and Eisaku Maeda.
2004. Convolution kernels with feature selection
for natural language processing tasks. In Proc.
of ACL, pages 119?126.
Benjamin Swanson and Eugene Charniak. 2012.
Native language detection with tree substitu-
tion grammars. In Proc. of ACL (short papers),
pages 193?197, Jeju Island, Korea, July.
Joel Tetreault, Daniel Blanchard, Aoife Cahill, and
Martin Chodorow. 2012. Native tongues, lost
and found: Resources and empirical evaluations
in native language identification. In Proc. of
COLING, pages 2585?2602, Mumbai, India, De-
cember.
Laura Mayfield Tomokiyo and Rosie Jones. 2001.
You?re not from ?round here, are you? Naive
Bayes detection of non-native utterances. In
Proc. of NAACL.
Vladimir N. Vapnik. 1998. Statistical Learning
Theory. John Wiley & Sons.
Zhuang Wang, Koby Crammer, and Slobodan
Vucetic. 2010. Multi-class pegasos on a bud-
get. In ICML, pages 1143?1150.
Sze-Meng Jojo Wong and Mark Dras. 2010. Parser
features for sentence grammaticality classifica-
tion. In Proceedings of the Australasian Lan-
guage Technology Association Workshop, Mel-
bourne, Australia, December.
Sze-Meng Jojo Wong and Mark Dras. 2011. Ex-
ploiting parse structures for native language
identification. In Proc. of EMNLP, pages 1600?
1610, Edinburgh, Scotland, UK., July.
Xiaofeng Yang, Jian Su, and Chew Lim Tan.
2006. Kernel-based pronoun resolution with
structured syntactic knowledge. In Proc. of
Coling-ACL, pages 41?48.
Dell Zhang and Wee Sun Lee. 2003. Question
classification using support vector machines. In
Proceedings of the 26th annual international
ACM SIGIR conference on Research and de-
velopment in informaion retrieval, SIGIR ?03,
pages 26?32, New York, NY, USA. ACM.
872
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 181?186,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
I?m a Belieber:
Social Roles via Self-identification and Conceptual Attributes
Charley Beller, Rebecca Knowles, Craig Harman
Shane Bergsma
?
, Margaret Mitchell
?
, Benjamin Van Durme
Human Language Technology Center of Excellence
Johns Hopkins University, Baltimore, MD USA
?
University of Saskatchewan, Saskatoon, Saskatchewan Canada
?
Microsoft Research, Redmond, Washington USA
charleybeller@jhu.edu, rknowles@jhu.edu, craig@craigharman.net,
shane.a.bergsma@gmail.com, memitc@microsoft.com, vandurme@cs.jhu.edu
Abstract
Motivated by work predicting coarse-
grained author categories in social me-
dia, such as gender or political preference,
we explore whether Twitter contains infor-
mation to support the prediction of fine-
grained categories, or social roles. We
find that the simple self-identification pat-
tern ?I am a ? supports significantly
richer classification than previously ex-
plored, successfully retrieving a variety of
fine-grained roles. For a given role (e.g.,
writer), we can further identify character-
istic attributes using a simple possessive
construction (e.g., writer?s ). Tweets
that incorporate the attribute terms in first
person possessives (my ) are confirmed
to be an indicator that the author holds the
associated social role.
1 Introduction
With the rise of social media, researchers have
sought to induce models for predicting latent au-
thor attributes such as gender, age, and politi-
cal preferences (Garera and Yarowsky, 2009; Rao
et al, 2010; Burger et al, 2011; Van Durme,
2012b; Zamal et al, 2012). Such models are
clearly in line with the goals of both computa-
tional advertising (Wortman, 2008) and the grow-
ing area of computational social science (Conover
et al, 2011; Nguyen et al, 2011; Paul and Dredze,
2011; Pennacchiotti and Popescu, 2011; Moham-
mad et al, 2013) where big data and computa-
tion supplement methods based on, e.g., direct hu-
man surveys. For example, Eisenstein et al (2010)
demonstrated a model that predicted where an au-
thor was located in order to analyze regional dis-
tinctions in communication. While some users ex-
plicitly share their GPS coordinates through their
Twitter clients, having a larger collection of au-
tomatically identified users within a region was
preferable even though the predictions for any
given user were uncertain.
We show that media such as Twitter can sup-
port classification that is more fine-grained than
gender or general location. Predicting social roles
such as doctor, teacher, vegetarian, christian,
may open the door to large-scale passive surveys
of public discourse that dwarf what has been pre-
viously available to social scientists. For exam-
ple, work on tracking the spread of flu infections
across Twitter (Lamb et al, 2013) might be en-
hanced with a factor based on aggregate predic-
tions of author occupation.
We present two studies showing that first-
person social content (tweets) contains intuitive
signals for such fine-grained roles. We argue that
non-trivial classifiers may be constructed based
purely on leveraging simple linguistic patterns.
These baselines suggest a wide range of author
categories to be explored further in future work.
Study 1 In the first study, we seek to determine
whether such a signal exists in self-identification:
we rely on variants of a single pattern, ?I am a ?,
to bootstrap data for training balanced-class binary
classifiers using unigrams observed in tweet con-
tent. As compared to prior research that required
actively polling users for ground truth in order to
construct predictive models for demographic in-
formation (Kosinski et al, 2013), we demonstrate
that some users specify such properties publicly
through direct natural language.
Many of the resultant models show intuitive
strongly-weighted features, such as a writer be-
ing likely to tweet about a story, or an ath-
lete discussing a game. This demonstrates self-
identification as a viable signal in building predic-
tive models of social roles.
181
Role Tweet
artist I?m an Artist..... the last of a dying breed
belieber @justinbieber I will support you in ev-
erything you do because I am a belieber
please follow me I love you 30
vegetarian So glad I?m a vegetarian.
Table 1: Examples of self-identifying tweets.
# Role # Role # Role
29,924 little 5,694 man 564 champion
21,822 big ... ... 559 teacher
18,957 good 4,007 belieber 556 writer
13,069 huge 3,997 celebrity 556 awful
13,020 bit 3,737 virgin ... ...
12,816 fan 3,682 pretty 100 cashier
10,832 bad ... ... 100 bro
10,604 girl 2,915 woman ... ...
9,981 very 2,851 beast 10 linguist
... ... ... ... ... ...
Table 2: Number of self-identifying users per ?role?. While
rich in interesting labels, cases such as very highlight the pur-
poseful simplicity of the current approach.
Study 2 In the second study we exploit a com-
plementary signal based on characteristic con-
ceptual attributes of a social role, or concept
class (Schubert, 2002; Almuhareb and Poesio,
2004; Pas?ca and Van Durme, 2008). We identify
typical attributes of a given social role by collect-
ing terms in the Google n-gram corpus that occur
frequently in a possessive construction with that
role. For example, with the role doctor we extract
terms matching the simple pattern ?doctor?s ?.
2 Self-identification
All role-representative users were drawn from
the free public 1% sample of the Twitter Fire-
hose, over the period 2011-2013, from the sub-
set that selected English as their native language
(85,387,204 unique users). To identify users of
a particular role, we performed a case-agnostic
search of variants of a single pattern: I am a(n)
, and I?m a(n) , where all single tokens filling
the slot were taken as evidence of the author self-
reporting for the given ?role?. Example tweets can
be seen in Table 1, examples of frequency per role
in Table 2. This resulted in 63,858 unique roles
identified, of which 44,260 appeared only once.
1
We manually selected a set of roles for fur-
ther exploration, aiming for a diverse sample
across: occupation (e.g., doctor, teacher), family
(mother), disposition (pessimist), religion (chris-
1
Future work should consider identifying multi-word role
labels (e.g., Doctor Who fan, or dog walker).
0.60
0.65
0.70
0.75
0.80
ll
l
l
l
l l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
ll
l
l
l
l l
l
l
l
l
l
l
direc
tione
r
belie
ber
optim
ist
sold
ier
soph
omo
re
pess
imis
t
ran
dom
.0
danc
er
hips
ter
ran
dom
.2
sing
er
fresh
man
mo
ther
ran
dom
.1
chee
rlead
er
rapp
er
chris
tian
artis
t
sm
oker acto
r
vege
taria
n
wo
ma
n
athle
te
geek engi
neer
wa
itres
s
nur
se
ma
n
stud
ent doct
or poet writ
er
athe
ist
gran
dmalawy
er
teac
her
Role
Cha
nce 
of S
ucce
ss
Figure 1: Success rate for querying a user. Random.0,1,2
are background draws from the population, with the mean of
those three samples drawn horizontally. Tails capture 95%
confidence intervals.
tian), and ?followers? (belieber, directioner).
2
We filtered users via language ID (Bergsma et al,
2012) to better ensure English content.
3
For each selected role, we randomly sampled up
to 500 unique self-reporting users and then queried
Twitter for up to 200 of their recent publicly
posted tweets.
4
These tweets served as represen-
tative content for that role, with any tweet match-
ing the self-reporting patterns filtered. Three sets
of background populations were extracted based
on randomly sampling users that self-reported En-
glish (post-filtered via LID).
Twitter users are empowered to at any time
delete, rename or make private their accounts.
Any given user taken to be representative based on
a previously posted tweet may no longer be avail-
able to query on. As a hint of the sort of user stud-
ies one might explore given access to social role
prediction, we see in Figure 1 a correlation be-
tween self-reported role and the chance of an ac-
count still being publicly visible, with roles such
as belieber and directioner on the one hand, and
doctor and teacher on the other.
The authors examined the self-identifying tweet
of 20 random users per role. The accuracy of the
self-identification pattern varied across roles and
is attributable to various factors including quotes,
e.g. @StarTrek Jim, I?m a DOCTOR not a down-
load!. While these samples are small (and thus
estimates of quality come with wide variance), it
2
Those that follow the music/life of the singer Justin
Bieber and the band One Direction, respectively.
3
This removes users that selected English as their primary
language, used a self-identification phrase, e.g. I am a be-
lieber, but otherwise tended to communicate in non-English.
4
Roughly half of the classes had less than 500 self-
reporting users in total, in those cases we used all matches.
182
actorartistatheist
athletebeliebercheerleader
christiandancerdirectioner
doctorengineerfreshman
geekgrandmahipster
lawyermanmother
nurseoptimistpessimist
poetrappersinger
smokersoldiersophomore
studentteachervegetarian
waitresswomanwriter
0 5 10 15
Figure 2: Valid self-identifying tweets from sample of 20.
is noteworthy that a non-trivial number for each
were judged as actually self-identifying.
Indicative Language Most work in user clas-
sification relies on featurizing language use,
most simply through binary indicators recording
whether a user did or did not use a particular word
in a history of n tweets. To explore whether lan-
guage provides signal for future work in fine-grain
social role prediction, we constructed a set of ex-
periments, one per role, where training and test
sets were balanced between users from a random
background sample and self-reported users. Base-
line accuracy in these experiments was thus 50%.
Each training set had a target of 600 users (300
background, 300 self-identified); for those roles
with less than 300 users self-identifying, all users
were used, with an equal number background. We
used the Jerboa (Van Durme, 2012a) platform
to convert data to binary feature vectors over a un-
igram vocabulary filtered such that the minimum
frequency was 5 (across unique users). Training
and testing was done with a log-linear model via
LibLinear (Fan et al, 2008). We used the pos-
itively annotated data to form test sets, balanced
with data from the background set. Each test set
had a theoretical maximum size of 40, but for sev-
eral classes it was in the single digits (see Fig-
ure 2). Despite the varied noisiness of our simple
pattern-bootstrapped training data, and the small
size of our annotated test set, we see in Figure 3
that we are able to successfully achieve statisti-
cally significant predictions of social role for the
majority of our selected examples.
Table 3 highlights examples of language indica-
tive of role, as determined by the most positively
weighted unigrams in the classification experi-
0.2
0.4
0.6
0.8
1.0
l
l l l
l
ll
l ll
l
l l
l
ll
l ll
l
l
ll l
l
l
l
l l l
l
l
l
sold
ier
wo
ma
n
pess
imis
t
chris
tian
gran
dma
nur
se
rapp
er
ma
n poet
chee
rlead
er
stud
ent
engi
neer acto
r
teac
her
vege
taria
n
mo
ther sing
er
lawy
er
optim
ist
wa
itres
s
sm
oker hips
ter doct
or
danc
er
artis
t
fresh
man
direc
tione
r
geek
soph
omo
re
athe
ist
athle
te
writ
er
belie
ber
Role
Acc
urac
y
Figure 3: Accuracy in classifying social roles.
Role :: Feature ( Rank)
artist morning, summer, life, most, amp, studio
atheist fuck, fucking, shit, makes, dead, ..., religion
19
athlete lol, game, probably, life, into, ..., team
9
belieber justin, justinbeiber, believe, beliebers, bieber
cheerleader cheer, best, excited, hate, mom, ..., prom
16
christian lol, ..., god
12
, pray
13
, ..., bless
17
, ..., jesus
20
dancer dance, since, hey, never, been
directioner harry, d, follow, direction, never, liam, niall
doctor sweet, oh, or, life, nothing
engineer (, then, since, may, ), test
9
, -
17
, =
18
freshman summer, homework, na, ..., party
19
, school
20
geek trying, oh, different, dead, been
grandma morning, baby, around, night, excited
hipster fucking, actually, thing, fuck, song
lawyer did, never, his, may, pretty, law, even, office
man man, away, ai, young, since
mother morning, take, fuck, fucking, trying
nurse lol, been, morning, ..., night
10
, nursing
11
, shift
13
optimist morning, enough, those, everything, never
poet feel, song, even, say, yo
rapper fuck, morning, lol, ..., mixtape
8
, songs
15
singer sing, song, music, lol, never
smoker fuck, shit, fucking, since, ass, smoke, weed
20
solider ai, beautiful, lol, wan, trying
sophmore summer, >, ..., school
11
, homework
12
student anything, summer, morning, since, actually
teacher teacher, morning, teach, ..., students
7
, ..., school
20
vegetarian actually, dead, summer, oh, morning
waitress man, try, goes, hate, fat
woman lol, into, woman, morning, never
writer write, story, sweet, very, working
Table 3: Most-positively weighted features per role, along
with select features within the top 20. Surprising mother
features come from ambigious self-identification, as seen in
tweets such as: I?m a mother f!cking starrrrr.
ment. These results qualitatively suggest many
roles under consideration may be teased out from a
background population by focussing on language
that follows expected use patterns. For example
the use of the term game by athletes, studio by
artists, mixtape by rappers, or jesus by Christians.
3 Characteristic Attributes
Bergsma and Van Durme (2013) showed that the
183
task of mining attributes for conceptual classes can
relate straightforwardly to author attribute predic-
tion. If one views a role, in their case gender, as
two conceptual classes, male and female, then ex-
isting attribute extraction methods for third-person
content (e.g., news articles) can be cheaply used to
create a set of bootstrapping features for building
classifiers over first-person content (e.g., tweets).
For example, if we learn from news corpora that:
a man may have a wife, then a tweet saying: ...my
wife... can be taken as potential evidence of mem-
bership in the male conceptual class.
In our second study, we test whether this idea
extends to our wider set of fine-grained roles. For
example, we aimed to discover that a doctor may
have a patient, while a hairdresser may have a
salon; these properties can be expressed in first-
person content as possessives like my patient or my
salon. We approached this task by selecting target
roles from the first experiment and ranking charac-
teristic attributes for each using pointwise mutual
information (PMI) (Church and Hanks, 1990).
First, we counted all terms matching a target
social role?s possessive pattern (e.g., doctor?s )
in the web-scale n-gram corpus Google V2 (Lin
et al, 2010)
5
. We ranked the collected terms
by computing PMI between classes and attribute
terms. Probabilities were estimated from counts of
the class-attribute pairs along with counts match-
ing the generic possessive patterns his and
her which serve as general background cate-
gories. Following suggestions by Bergsma and
Van Durme, we manually filtered the ranked list.
6
We removed attributes that were either (a) not
nominal, or (b) not indicative of the social role.
This left fewer than 30 attribute terms per role,
with many roles having fewer than 10.
We next performed a precision test to identify
potentially useful attributes in these lists. We ex-
amined tweets with a first person possessive pat-
tern for each attribute term from a small corpus
of tweets collected over a single month in 2013,
discarding those attribute terms with no positive
matches. This precision test is useful regardless
of how attribute lists are generated. The attribute
5
In this corpus, follower-type roles like belieber and di-
rectioner are not at all prevalent. We therefore focused on
occupational and habitual roles (e.g., doctor, smoker).
6
Evidence from cognitive work on memory-dependent
tasks suggests that such relevance based filtering (recogni-
tion) involves less cognitive effort than generating relevant
attributes (recall) see (Jacoby et al, 1979). Indeed, this filter-
ing step generally took less than a minute per class.
term chart, for example, had high PMI with doc-
tor; but a precision test on the phrase my chart
yielded a single tweet which referred not to a med-
ical chart but to a top ten list (prompting removal
of this attribute). Using this smaller high-precision
set of attribute terms, we collected tweets from the
Twitter Firehose over the period 2011-2013.
4 Attribute-based Classification
Attribute terms are less indicative overall than
self-ID, e.g., the phrase I?m a barber is a clearer
signal than my scissors. We therefore include a
role verification step in curating a collection of
positively identified users. We use the crowd-
sourcing platform Mechanical Turk
7
to judge
whether the person tweeting held a given role
Tweets were judged 5-way redundantly. Me-
chanical Turk judges (?Turkers?) were presented
with a tweet and the prompt: Based on this
tweet, would you think this person is a BAR-
BER/HAIRDRESSER? along with four response
options: Yes, Maybe, Hard to tell, and No.
We piloted this labeling task on 10 tweets per
attribute term over a variety of classes. Each an-
swer was associated with a score (Yes = 1, Maybe
= .5, Hard to tell = No = 0) and aggregated across
the five judges. We found in development that an
aggregate score of 4.0 (out of 5.0) led to an ac-
ceptable agreement rate between the Turkers and
the experimenters, when the tweets were randomly
sampled and judged internally. We found that
making conceptual class assignments based on a
single tweet was often a subtle task. The results of
this labeling study are shown in Figure 4, which
gives the percent of tweets per attribute that were
4.0 or above. Attribute terms shown in red were
manually discarded as being inaccurate (low on
the y-axis) or non-prevalent (small shape).
From the remaining attribute terms, we identi-
fied users with tweets scoring 4.0 or better as posi-
tive examples of the associated roles. Tweets from
those users were scraped via the Twitter API to
construct corpora for each role. These were split
intro train and test, balanced with data from the
same background set used in the self-ID study.
Test sets were usually of size 40 (20 positive, 20
background), with a few classes being sparse (the
smallest had only 16 instances). Results are shown
in Figure 5. Several classes in this balanced setup
can be predicted with accuracies in the 70-90%
7
https://www.mturk.com/mturk/
184
l l l l ll lll l l l
l l l l l l
l
l l ll
l
l l l l l
l
l l l
Actor/Actress Athlete Barber/Hairdresser Bartender Blogger Cheerleader
Christian College Student Dancer Doctor/Nurse Drummer Hunter
Jew Mom Musician Photographer Professor Rapper/Songwriter
Reporter Sailor Skier Smoker Soldier Student
Swimmer Tattoo Artist Waiter/Waitress Writer
0.00.2
0.40.6
0.8
0.00.2
0.40.6
0.8
0.00.2
0.40.6
0.8
0.00.2
0.40.6
0.8
0.00.2
0.40.6
0.8
rehear
sal theater directo
r lines
conc
ussionplayin
gprotein sport squadcondit
ioningjerseypositioncoach calves clien
t
scissor
s
shears salon bar blog bloggi
ng pom
hope testimo
ny
church bible schola
rship
syllabu
s
adviso
r
tuition campu
s
univer
sity
college tu
tu
scrub patient stethos
cope drum stand
shul angel deliver
y kid parent
ing set alum guitar piano shoot shutter lecture faculty studen
t lyrics
cove
rage editor article shi
p
goggle
s pipe smokin
g
tobacc
o
smoke cigaret
te
billet comba
t duffel orders bunk deploy
mentbarrac
ks stats cap lab philoso
phy
pool ink station tip apron script memo
ir poemKeyword
Above
 Thres
hold
log10(Count)
l l l l1 2 3 4Keep
l FALSE TRUE
Figure 4: Turker judged quality of attributes selected as
candidate features for bootstrapping positive instances of the
given social role.
0.5
0.6
0.7
0.8
acto
r
athle
te
barbe
r
blogg
er
chee
rlead
er
chris
tian docto
r
drum
mer
mo
m
mu
sicia
n
photo
graph
er
profe
ssor
repor
ter
smo
ker
soldi
er
stude
nt
waite
r
write
r
Accu
racy
Figure 5: Classifier accuracy on balanced set contrasting
agreed upon Twitter users of a given role against users pulled
at random from the 1% stream.
range, supporting our claim that there is discrimi-
nating content for a variety of these social roles.
Conditional Classification How accurately we
can predict membership in a given class when a
Twitter user sends a tweet matching one of the tar-
geted attributes? For example, if one sends a tweet
saying my coach, then how likely is it that author
Figure 6: Results of positive vs negative by attribute term.
Given that a user tweets . . . my lines . . . we are nearly 80%
accurate in identifying whether or not the user is an actor.
is an athlete?
Using the same collection as the previous ex-
periment, we trained classifiers conditioned on a
given attribute term. Positive instances were taken
to be those with a score of 4.0 or higher, with neg-
ative instances taken to be those with scores of 1.0
or lower (strong agreement by judges that the orig-
inal tweet did not provide evidence of the given
role). Classification results are shown in Figure 6.
5 Conclusion
We have shown that Twitter contains sufficiently
robust signal to support more fine-grained au-
thor attribute prediction tasks than have previously
been attempted. Our results are based on simple,
intuitive search patterns with minimal additional
filtering: this establishes the feasibility of the task,
but leaves wide room for future work, both in the
sophistication in methodology as well as the diver-
sity of roles to be targeted. We exploited two com-
plementary types of indicators: self-identification
and self-possession of conceptual class (role) at-
tributes. Those interested in identifying latent de-
mographics can extend and improve these indica-
tors in developing ways to identify groups of inter-
est within the general population of Twitter users.
Acknowledgements This material is partially
based on research sponsored by the NSF un-
der grants DGE-123285 and IIS-1249516 and by
DARPA under agreement number FA8750-13-2-
0017 (the DEFT program).
185
References
Abdulrahman Almuhareb and Massimo Poesio. 2004.
Attribute-based and value-based clustering: an eval-
uation. In Proceedings of EMNLP.
Shane Bergsma and Benjamin Van Durme. 2013. Us-
ing Conceptual Class Attributes to Characterize So-
cial Media Users. In Proceedings of ACL.
Shane Bergsma, Paul McNamee, Mossaab Bagdouri,
Clay Fink, and Theresa Wilson. 2012. Language
identification for creating language-specific twitter
collections. In Proceedings of the NAACL Workshop
on Language and Social Media.
John D. Burger, John Henderson, George Kim, and
Guido Zarrella. 2011. Discriminating gender on
twitter. In Proceedings of EMNLP.
Kenneth Ward Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicog-
raphy. Computational linguistics, 16(1):22?29.
Michael Conover, Jacob Ratkiewicz, Matthew Fran-
cisco, Bruno Gonc?alves, Filippo Menczer, and
Alessandro Flammini. 2011. Political polarization
on twitter. In ICWSM.
Jacob Eisenstein, Brendan O?Connor, Noah Smith, and
Eric P. Xing. 2010. A latent variable model of
geographical lexical variation. In Proceedings of
EMNLP.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsief, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A
library for large linear classification. Journal of Ma-
chine Learning Research, (9).
Nikesh Garera and David Yarowsky. 2009. Modeling
latent biographic attributes in conversational genres.
In Proceedings of ACL.
Larry L Jacoby, Fergus IM Craik, and Ian Begg. 1979.
Effects of decision difficulty on recognition and re-
call. Journal of Verbal Learning and Verbal Behav-
ior, 18(5):585?600.
Michal Kosinski, David Stillwell, and Thore Graepel.
2013. Private traits and attributes are predictable
from digital records of human behavior. Proceed-
ings of the National Academy of Sciences.
Alex Lamb, Michael J. Paul, and Mark Dredze. 2013.
Separating fact from fear: Tracking flu infections on
twitter. In Proceedings of NAACL.
Dekang Lin, Kenneth Church, Heng Ji, Satoshi Sekine,
David Yarowsky, Shane Bergsma, Kailash Patil,
Emily Pitler, Rachel Lathbury, Vikram Rao, Kapil
Dalwani, and Sushant Narsale. 2010. New tools for
web-scale n-grams. In Proc. LREC, pages 2221?
2227.
Saif M. Mohammad, Svetlana Kiritchenko, and Joel
Martin. 2013. Identifying purpose behind elec-
toral tweets. In Proceedings of the Second Interna-
tional Workshop on Issues of Sentiment Discovery
and Opinion Mining, WISDOM ?13, pages 1?9.
Dong Nguyen, Noah A Smith, and Carolyn P Ros?e.
2011. Author age prediction from text using lin-
ear regression. In Proceedings of the 5th ACL-
HLT Workshop on Language Technology for Cul-
tural Heritage, Social Sciences, and Humanities,
pages 115?123. Association for Computational Lin-
guistics.
Marius Pas?ca and Benjamin Van Durme. 2008.
Weakly-Supervised Acquisition of Open-Domain
Classes and Class Attributes from Web Documents
and Query Logs. In Proceedings of ACL.
Michael J Paul and Mark Dredze. 2011. You are what
you tweet: Analyzing twitter for public health. In
ICWSM.
Marco Pennacchiotti and Ana-Maria Popescu. 2011.
Democrats, Republicans and Starbucks afficionados:
User classification in Twitter. In Proceedings of
the 17th ACM SIGKDD International Conference on
Knowledge Discovery and Data mining, pages 430?
438. ACM.
Delip Rao, David Yarowsky, Abhishek Shreevats, and
Manaswi Gupta. 2010. Classifying latent user at-
tributes in twitter. In Proceedings of the Work-
shop on Search and Mining User-generated Con-
tents (SMUC).
Lenhart K. Schubert. 2002. Can we derive general
world knowledge from texts? In Proceedings of
HLT.
Benjamin Van Durme. 2012a. Jerboa: A toolkit for
randomized and streaming algorithms. Technical
Report 7, Human Language Technology Center of
Excellence, Johns Hopkins University.
Benjamin Van Durme. 2012b. Streaming analysis of
discourse participants. In Proceedings of EMNLP.
Jennifer Wortman. 2008. Viral marketing and the
diffusion of trends on social networks. Technical
Report MS-CIS-08-19, University of Pennsylvania,
May.
Faiyaz Al Zamal, Wendy Liu, and Derek Ruths. 2012.
Homophily and latent attribute inference: Inferring
latent attributes of Twitter users from neighbors. In
Proceedings of ICWSM.
186
Proceedings of the 2010 Named Entities Workshop, ACL 2010, pages 39?47,
Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational Linguistics
Transliteration Generation and Mining with Limited Training Resources
Sittichai Jiampojamarn, Kenneth Dwyer, Shane Bergsma, Aditya Bhargava,
Qing Dou, Mi-Young Kim, Grzegorz Kondrak
Department of Computing Science
University of Alberta
Edmonton, AB, T6G 2E8, Canada
{sj,dwyer,bergsma,abhargava,qdou,miyoung2,kondrak}@cs.ualberta.ca
Abstract
We present DIRECTL+: an online dis-
criminative sequence prediction model
based on many-to-many alignments,
which is further augmented by the in-
corporation of joint n-gram features.
Experimental results show improvement
over the results achieved by DIRECTL in
2009. We also explore a number of diverse
resource-free and language-independent
approaches to transliteration mining,
which range from simple to sophisticated.
1 Introduction
Many out-of-vocabulary words in statistical ma-
chine translation and cross-language information
retrieval are named entities. If the languages in
question use different writing scripts, such names
must be transliterated. Transliteration can be de-
fined as the conversion of a word from one writ-
ing script to another, which is usually based on the
phonetics of the original word.
DIRECTL+ is our current approach to name
transliteration which is an extension of the DI-
RECTL system (Jiampojamarn et al, 2009). We
augmented the feature set with joint n-gram fea-
tures which allow the discriminative model to uti-
lize long dependencies of joint information of
source and target substrings (Jiampojamarn et al,
2010). Experimental results suggest an improve-
ment over the results achieved by DIRECTL in
2009.
Transliteration mining aims at automatically
obtaining bilingual lists of names written in differ-
ent scripts. We explore a number of different ap-
proaches to transliteration mining in the context of
the NEWS 2010 Shared Task.1 The sole resource
that is provided for each language pair is a ?seed?
1http://translit.i2r.a-star.edu.sg/
news2010
dataset that contains 1K transliteration word pairs.
The objective is then to mine transliteration pairs
from a collection of Wikipedia titles/topics that are
given in both languages.
We explore a number of diverse resource-free
and language-independent approaches to translit-
eration mining. One approach is to bootstrap the
seed data by generating pseudo-negative exam-
ples, which are combined with the positives to
form a dataset that can be used to train a clas-
sifier. We are particularly interested in achiev-
ing good performance without utilizing language-
specific resources, so that the same approach can
be applied with minimal or no modifications to an
array of diverse language pairs.
This paper is divided in two main parts that cor-
respond to the two tasks of transliteration genera-
tion and transliteration mining.
2 Transliteration generation
The structure of this section is as follows. In Sec-
tion 2.1, we describe the pre-processing steps that
were applied to all datasets. Section 2.2 reviews
two methods for aligning the source and target
symbols in the training data. We provide details
on the DIRECTL+ systems in Section 2.3. In Sec-
tion 2.4, we discuss extensions of DIRECTL+ that
incorporate language-specific information. Sec-
tion 2.5 summarizes our results.
2.1 Pre-processing
For all generation tasks, we pre-process the pro-
vided data as follows. First, we convert all char-
acters in the source word to lower case. Then,
we remove non-alphabetic characters unless they
appear in both the source and target words. We
normalize whitespace that surrounds a comma, so
that there are no spaces before the comma and ex-
actly one space following the comma. Finally, we
separate multi-word titles into single words, using
whitespace as the separator. We assume a mono-
39
tonic matching and ignore the titles that have a dif-
ferent number of words on both sides.
We observed that in the ArAe task there are
cases where an extra space is added to the target
when transliterating from Arabic names to their
English equivalents; e.g., ?Al Riyad?, ?El Sayed?,
etc. In order to prevent the pre-processing from
removing too many title pairs, we allow non-equal
matching if the source title is a single word.
For the English-Chinese (EnCh) task, we con-
vert the English letter ?x? to ?ks? to facilitate bet-
ter matching with its Chinese targets.
During testing, we pre-process test data in the
same manner, except that we do not remove non-
alphabetic characters. After the pre-processing
steps, our system proposes 10-best lists for single
word titles in the test data. For multi-word titles,
we construct 10-best lists by ranking the combina-
tion scores of single words that make up the test
titles.
2.2 Alignment
In the transliteration tasks, training data consist
of pairs of names written in source and target
scripts without explicit character-level alignment.
In our experiments, we applied two different algo-
rithms to automatically generate alignments in the
training data. The generated alignments provide
hypotheses of substring mappings in the training
data. Given aligned training data, a transliteration
model is trained to generate names in the target
language given names in the source language.
The M2M-aligner (Jiampojamarn et al, 2007)
is based on the expectation maximization (EM)
algorithm. It allows us to create alignments be-
tween substrings of various lengths. We opti-
mized the maximum substring sizes for the source
and target based on the performance of the end
task on the development sets. We allowed empty
strings (nulls) only on the target side. We used the
M2M-aligner for all alignment tasks, except for
English-Pinyin alignment. The source code of the
M2M-aligner is publicly available.2
An alternative alignment algorithm is based on
the phonetic similarity of graphemes. The key idea
of this approach is to represent each grapheme by a
phoneme or a sequence of phonemes that is likely
to be represented by the grapheme. The sequences
of phonemes on the source side and the target
side can then be aligned on the basis of phonetic
2http://code.google.com/p/m2m-aligner/
b a r c - l a y
| | | | | | | |
b a - k u r - i
Figure 1: An alignment example.
similarity between phonemes. The main advan-
tage of the phonetic alignment is that it requires
no training data. We use the ALINE phonetic
aligner (Kondrak, 2000), which aligns two strings
of phonemes. The example in Figure 1 shows
the alignment of the word Barclay to its Katakana
transliteration ba-ku-ri. The one-to-one alignment
can then be converted to a many-to-many align-
ment by grouping the Japanese phonemes that cor-
respond to individual Katakana symbols.
2.3 DIRECTL+
We refer to our present approach to transliteration
as DIRECTL+. It is an extension of our DIRECTL
system (Jiampojamarn et al, 2009). It includes ad-
ditional ?joint n-gram? features that allow the dis-
criminative model to correlate longer source and
target substrings. The additional features allow
our discriminative model to train on information
that is present in generative joint n-gram models,
and additionally train on rich source-side context,
transition, and linear-chain features that have been
demonstrated to be important in the transliteration
task (Jiampojamarn et al, 2010).
Our model is based on an online discriminative
framework. At each training iteration, the model
generates an m-best list for each given source
name based on the current feature weights. The
feature weights are updated according to the gold-
standard answers and the generated m-best an-
swer lists using the Margin Infused Relaxed Algo-
rithm (MIRA) (Crammer and Singer, 2003). This
training process iterates over the training examples
until the model converges. For m-best and n-gram
parameters, we set m = 10 and n = 6 for all lan-
guage pairs. These parameters as well as others
were optimized on the development sets.
We trained our models directly on the data
that were provided by the organizers, with three
exceptions. In order to improve performance,
we gave special treatment to English-Korean
(EnKo), English-Chinese (EnCh), and English-
Hindi (EnHi). These special cases are described
in the next section.
40
2.4 Beyond DIRECTL+
2.4.1 Korean Jaso
A Korean syllable can be decomposed into two
or three components called Jaso: an initial con-
sonant, a middle vowel, and optionally a final con-
sonant. The Korean generation for EnKo involves
the following three steps: (1) English-to-Jaso gen-
eration, (2) correction of illegal Jaso sequences,
and (3) Jaso-to-Korean conversion.
In order to correct illegal Jaso sequences that
cannot be combined into Korean syllables in step
2, we consider both vowel and consonant rules.
A Korean vowel can be either a simple vowel or
a complex vowel that combines two simple vow-
els. We can use this information in order to replace
double vowels with one complex vowel. We also
use the silent consonant o (i-eung) when we need
to insert a consonant between double vowels. A
Korean vowel - (eu) is most commonly inserted
between two English consonants in transliteration.
In order to resolve three consecutive consonants, it
can be placed into the most probable position ac-
cording to the probability distribution of the train-
ing data.
2.4.2 Japanese Katakana
In the Japanese Katakana generation task, we re-
place each Katakana symbol with one or two let-
ters using standard romanization tables. This has
the effect of expressing the target side in Latin let-
ters, which facilitates the alignment. DIRECTL+
is trained on the converted data to generate the tar-
get from the source. A post-processing program
then attempts to convert the generated letters back
into Katakana symbols. Sequences of letters that
cannot be converted into Katakana are removed
from the output m-best lists and replaced by lower
scoring sequences that pass the back-conversion
filter. Otherwise, there is usually a single valid
mapping because most Katakana symbols are rep-
resented by single vowels or a consonant-vowel
pair. The only apparent ambiguity involves the
letter n, which can either stand by itself or clus-
ter with the following vowel letter. We resolve the
ambiguity by always assuming the latter case un-
less the letter n occurs at the end of the word.
2.4.3 Chinese Pinyin
Following (Jiampojamarn et al, 2009), we experi-
mented with converting the original Chinese char-
acters to Pinyin as an intermediate representation.
Pinyin is the most commonly known romanization
system for Standard Mandarin and many free tools
are available for converting Chinese characters to
Pinyin. Its alphabet contains the same 26 letters
as English. Each Chinese character can be tran-
scribed phonetically into Pinyin. A small percent-
age of Chinese characters have multiple pronunci-
ations, and are thus represented by different Pinyin
sequences. For those characters, we manually se-
lected the pronunciations that are normally used
for names. This pre-processing step significantly
reduces the size of the target symbols: from 370
distinct Chinese characters to 26 Pinyin symbols.
This allows our system to produce better align-
ments.
We developed three models: (1) trained on the
original Chinese characters, (2) trained on Pinyin,
and (3) the model that incorporates the phonetic
alignment described in Section 2.2. The combi-
nation of the predictions of the different systems
was performed using the following simple algo-
rithm (Jiampojamarn et al, 2009). First, we rank
the individual systems according to their top-1 ac-
curacy on the development set. To obtain the top-
1 prediction for each input word, we use simple
voting, with ties broken according to the ranking
of the systems. We generalize this approach to
handle n-best lists by first ordering the candidate
transliterations according to the rank assigned by
each individual system, and then similarly break-
ing ties by voting and using the ranking of the sys-
tems.
2.4.4 Language identification for Hindi
Bhargava and Kondrak (2010) apply support vec-
tor machines (SVMs) to the task of identifying
the language of names. The intuition here is that
language information can inform transliteration.
Bhargava and Kondrak (2010) test this hypothe-
sis on the NEWS 2009 English-Hindi transliter-
ation data by training language identification on
data manually tagged as being of either Indian or
non-Indian origin. It was found that splitting the
data disjointly into two sets and training separate
transliteration models yields no performance in-
crease due to the decreased size of the data for the
models.
We adopt this approach for the NEWS 2010
task, but here we do not use disjoint splits. In-
stead, we use the SVMs to generate probabilities,
and then we apply a threshold to these probabili-
ties to generate two datasets. For example, if we
set the threshold to be 0.05, then we determine the
41
probabilities of a given name being of Indian ori-
gin (phi) and of being of non-Indian origin (pen).
If phi < 0.05 then the name is excluded from the
Indian set, and if pen < 0.05 then the name is
excluded from the non-Indian set. Using the two
obtained non-disjoint sets, we then train a translit-
eration model for each set using DIRECTL+.
Since the two sets are not disjoint, we must de-
cide how to combine the two results. Given that a
name occurs in both sets, and both models provide
a ranked list of possible targets for that name, we
obtain a combined ranking using a linear combi-
nation over the mean reciprocal ranks (MRRs) of
the two lists. The weights used are phi and pen so
that the more likely a name is considered to be of
Indian origin, the more strongly the result from the
Indian set is considered relative to the result from
the non-Indian set.
2.5 Evaluation
In the context of the NEWS 2010 Machine
Transliteration Shared Task we tested our sys-
tem on all twelve datasets: from English to Chi-
nese (EnCh), Thai (EnTh), Hindi (EnHi), Tamil
(EnTa), Bangla (EnBa), Kannada (EnKa), Ko-
rean Hangul (EnKo), Japanese Katakana (EnJa),
Japanese Kanji (JnJk); and, in the opposite di-
rection, to English from Arabic (ArAe), Chi-
nese (ChEn), and Thai (ThEn). For all datasets,
we trained transliteration models on the provided
training and development sets without additional
resources.
Table 1 shows our best results obtained on the
datasets in terms of top-1 accuracy and mean F-
score. We also include the rank in standard runs
ordered by top-1 word accuracy. The EnCh re-
sult presented in the table refers to the output of
the three-system combination, using the combi-
nation algorithm described in Section 2.4.3. The
respective results for the three component EnCh
systems were: 0.357, 0.360, and 0.363. The
EnJa result in the table refers the system described
in Section 2.4.2 that applied specific treatment
to Japanese Katakana. Based on our develop-
ment results, this specific treatment improves as
much as 2% top-1 accuracy over the language-
independent model. The EnHi system that in-
corporates language identification obtained ex-
actly the same top-1 accuracy as the language-
independent model. However, the EnKo system
with Jaso correction produced the top-1 accu-
Task top-1 F-score Rank
EnCh 0.363 0.707 2
ChEn 0.137 0.740 1
EnTh 0.378 0.866 2
ThEn 0.352 0.861 2
EnHi 0.456 0.884 1
EnTa 0.390 0.891 2
EnKa 0.341 0.867 2
EnJa 0.398 0.791 1
EnKo 0.554 0.770 1
JnJk 0.126 0.426 1
ArAe 0.464 0.924 1
EnBa 0.395 0.877 2
Table 1: Transliteration generation results
racy of 0.554, which is a significant improvement
over 0.387 achieved by the language-independent
model.
3 Transliteration mining
This section is structured as follows. In Sec-
tion 3.1, we describe the method of extracting
transliteration candidates that serves as the input
to the subsequently presented mining approaches.
Two techniques for generating negative exam-
ples are discussed in Section 3.2. Our language-
independent approaches to transliteration mining
are described in Section 3.3, and a technique for
mining English-Chinese pairs is proposed in Sec-
tion 3.4. In Section 3.5, we address the issue of
overlapping predictions. Finally, Section 3.6 and
Section 3.7 summarize our results.
3.1 Extracting transliteration candidates
We cast the transliteration mining task as a bi-
nary classification problem. That is, given a word
in the source language and a word in the target
language, a classifier predicts whether or not the
pair constitutes a valid transliteration. As a pre-
processing step, we extract candidate translitera-
tions from the pairs of Wikipedia titles. Word seg-
mentation is performed based on sequences of one
or more spaces and/or punctuation symbols, which
include hyphens, underscores, brackets, and sev-
eral other non-alphanumeric characters. Apostro-
phes and single quotes are not used for segmenta-
tion (and therefore remain in a given word); how-
ever, all single quote-like characters are converted
into a generic apostrophe. Once an English ti-
tle and its target language counterpart have been
42
segmented into words, we form the candidate set
for this title as the cross product of the two sets
of words after discarding any words that contain
fewer than two characters.
After the candidates have been extracted, indi-
vidual words are flagged for certain attributes that
may be used by our supervised learner as addi-
tional features. Alternatively, the flags may serve
as criteria for filtering the list of candidate pairs
prior to classification. We identify words that are
capitalized, consist of all lowercase (or all capital)
letters, and/or contain one or more digits. We also
attempt to encode each word in the target language
as an ASCII string, and flag that word if the opera-
tion succeeds. This can be used to filter out words
that are written in English on both the source and
target side, which are not transliterations by defi-
nition.
3.2 Generating negative training examples
The main issue with applying a supervised learn-
ing approach to the NEWS 2010 Shared Task is
that annotated task-specific data is not available
to train the system. However, the seed pairs do
provide example transliterations, and these can be
used as positive training examples. The remaining
issue is how to select the negative examples.
We adopt two approaches for selecting nega-
tives. First, we generate all possible source-target
pairs in the seed data, and take as negatives those
pairs which are not transliterations but have a
longest common subsequence ratio (LCSR) above
0.58; this mirrors the approach used by Bergsma
and Kondrak (2007). The method assumes that
the source and target words are written in the same
script (e.g., the foreign word has been romanized).
A second possibility is to generate all seed pair-
ings as above, but then randomly select negative
examples, thus mirroring the approach in Klemen-
tiev and Roth (2006). In this case, the source and
target scripts do not need to be the same. Com-
pared with the LCSR technique, random sampling
in this manner has the potential to produce nega-
tive examples that are very ?easy? (i.e., clearly not
transliterations), and which may be of limited util-
ity when training a classifier. On the other hand, at
test time, the set of candidates extracted from the
Wikipedia data will include pairs that have very
low LCSR scores; hence, it can be argued that dis-
similar pairs should also appear as negative exam-
ples in the training set.
3.3 Language-independent approaches
In this section, we describe methods for transliter-
ation mining that can, in principle, be applied to a
wide variety of language pairs without additional
modification. For the purposes of the Shared Task,
however, we convert all source (English) words to
ASCII by removing diacritics and making appro-
priate substitutions for foreign letters. This is done
to mitigate sparsity in the relatively small seed sets
when training our classifiers.
3.3.1 Alignment-derived romanization
We developed a simple method of performing ro-
manization of foreign scripts. Initially, the seed set
of transliterations is aligned using the one-to-one
option of the M2M-aligner approach (Jiampoja-
marn et al, 2007). We allow nulls on both the
source and target sides. The resulting alignment
model contains pairs of Latin letters and foreign
script symbols (graphemes) sorted by their con-
ditional probability. Then, for each grapheme,
we select a letter (or a null symbol) that has the
highest conditional probability. The process pro-
duces an approximate romanization table that can
be obtained without any knowledge of the target
script. This method of romanization was used by
all methods described in the remainder of Sec-
tion 3.3.
3.3.2 Normalized edit distance
Normalized edit distance (NED) is a measure of
the similarity of two strings. We define a uniform
edit cost for each of the three operations: substitu-
tion, insertion, and deletion. NED is computed by
dividing the minimum edit distance by the length
of the longer string, and subtracting the resulting
fraction from 1. Thus, the extreme values of NED
are 1 for identical strings, and 0 for strings that
have no characters in common.
Our baseline method, NED+ is simply the NED
measure augmented with filtering of the candidate
pairs described in Section 3.1. In order to address
the issue of morphological variants, we also fil-
ter out the pairs in which the English word ends
in a consonant and the foreign word ends with a
vowel. With no development set provided, we set
the similarity thresholds for individual languages
on the basis of the average word length in the seed
sets. The values were 0.38, 0.48, 0.52, and 0.58
for Hindi, Arabic, Tamil, and Russian, respec-
tively, with the last number taken from Bergsma
and Kondrak (2007).
43
3.3.3 Alignment-based string similarity
NED selects transliteration candidates when the
romanized foreign strings have high character
overlap with their English counterparts. The mea-
sure is independent of the language pair. This
is suboptimal for several reasons. First of all,
phonetically unrelated words can share many in-
cidental character matches. For example, the
French word ?recettes? and the English word
?proceeds? share the letters r,c,e,e,s as a com-
mon subsequence, but the words are phonetically
unrelated. Secondly, many reliable, recurrent,
language-specific substring matches are prevalent
in true transliterations. These pairings may or may
not involve matching characters. NED can not
learn or adapt to these language-specific patterns.
In light of these drawbacks, researchers have
proposed string similarity measures that can learn
from provided example pairs and adapt the simi-
larity function to a specific task (Ristad and Yiani-
los, 1998; Bilenko and Mooney, 2003; McCallum
et al, 2005; Klementiev and Roth, 2006).
One particularly successful approach is by
Bergsma and Kondrak (2007), who use discrim-
inative learning with an improved feature repre-
sentation. The features are substring pairs that are
consistent with a character-level alignment of the
two strings. This approach strongly improved per-
formance on cognate identification, while varia-
tions of it have also proven successful in transliter-
ation discovery (Goldwasser and Roth, 2008). We
therefore adopted this approach for the translitera-
tion mining task.
We produce negative training examples using
the LCSR threshold approach described in Sec-
tion 3.2. For features, we extract from the aligned
word pairs all substring pairs up to a maximum
length of three. We also append characters mark-
ing the beginning and end of words, as described
in Bergsma and Kondrak (2007). For our clas-
sifier, we use a Support Vector Machine (SVM)
training with the very efficient LIBLINEAR pack-
age (Fan et al, 2008). We optimize the SVM?s
regularization parameter using 10-fold cross vali-
dation on the generated training data. At test time,
we apply our classifier to all the transliteration
candidates extracted from the Wikipedia titles,
generating transliteration pairs whenever there is
a positive classification.
3.3.4 String kernel classifier
The alignment-based classifier described in the
preceding section is limited to using substring fea-
tures that are up to (roughly) three or four letters
in length, due to the combinatorial explosion in the
number of unique features as the substring length
increases. It is natural to ask whether longer sub-
strings can be utilized to learn a more accurate pre-
dictor.
This question inspired the development of a sec-
ond SVM-based learner that uses a string kernel,
and therefore does not have to explicitly repre-
sent feature vectors. Our kernel is a standard n-
gram (or spectrum) kernel that implicitly embeds
a string in a feature space that has one co-ordinate
for each unique n-gram (see, e.g., (Shawe-Taylor
and Cristianini, 2004)). Let us denote the alphabet
over input strings as A. Given two input strings x
and x?, this kernel function computes:
k(x, x?) =
?
s?An
#(s, x)#(s, x?)
where s is an n-gram and #(a, b) counts the num-
ber of times a appears as a substring of b.
An extension of the n-gram kernel that we em-
ploy here is to consider all n-grams of length
1 ? n ? k, and weight each n-gram as a func-
tion of its length. In particular, we specify a value
? and weight each n-gram by a factor of ?n. We
implemented this kernel in the LIBSVM software
package (Chang and Lin, 2001). Optimal values
for k, ?, and the SVM?s regularization parame-
ter were estimated for each dataset using 5-fold
cross-validation. The values of (k, ?) that we ul-
timately used were: EnAr (3, 0.8), EnHi (8, 0.8),
EnRu (5, 1.2), and EnTa (5, 1.0).
Our input string representation for a candidate
pair is formed by first aligning the source and tar-
get words using M2M-aligner (Jiampojamarn et
al., 2007). Specifically, an alignment model is
trained on the seed examples, which are subse-
quently aligned and used as positive training ex-
amples. We then generate 20K negative examples
by random sampling (cf. Section 3.2) and apply
the alignment model to this set. Not all of these
20K word pairs will necessarily be aligned; we
randomly select 10K of the successfully aligned
pairs to use as negative examples in the training
set.
Each aligned pair is converted into an ?align-
ment string? by placing the letters that appear in
44
Word pair zubtsov z u b ov
Aligned pair z|u|b|t|s|o|v| z|u|b|| |o|v|
Align?t string zz|uu|bb|t|s |oo|vv
Table 2: An example showing how an alignment
string (the input representation for the string ker-
nel) is created from a word pair.
the same position in the source and target next to
one another, while retaining the separator charac-
ters (see Table 2). We also appended beginning
and end of word markers. Note that no romaniza-
tion of the target words is necessary for this pro-
cedure.
At test time, we apply the alignment model to
the candidate word pairs that have been extracted
from the train data, and retain all the successfully
aligned pairs. Here, M2M-aligner also acts as a
filter, since we cannot form alignment strings from
unaligned pairs ? these yield negative predictions
by default. We also filter out pairs that met any of
the following conditions: 1) the English word con-
sists of all all capital or lowercase letters, 2) the
target word can be converted to ASCII (cf. Sec-
tion 3.1), or 3) either word contains a digit.
3.3.5 Generation-based approach
In the mining tasks, we are interested in whether a
candidate pair (x, y) is a transliteration pair. One
approach is to determine if the generated translit-
erations of a source word y? = ?(x) and a target
word x? = ?(y) are similar to the given candi-
date pair. We applied DIRECTL+ to the mining
tasks by training transliteration generation models
on the provided seed data in forward and back-
ward transliteration directions, creating ?(x) and
?(y) models. We now define a transliteration
score function in Eq. 1. N(x?, x) is the normal-
ized edit distance between string x? and x, and w1
and w2 are combination weights to favor forward
and backward transliteration models.
S(x, y) = w1 ? N(y?, y) + w2 ? N(x?, x)w1 + w2
(1)
A candidate pair is considered a transliteration
pair if its S(x, y) > ? . Ideally, we would like
to optimize these parameters, ?, w1, w2 based on
a development set for each language pair. Unfor-
tunately, no development sets were provided for
the Shared Task. Therefore, following Bergsma
and Kondrak (2007), we adopt the threshold of
? = 0.58. We experimented with three sets of val-
ues for w1 and w2: (1, 0), (0.5, 0.5), and (0, 1).
Our final predictions were made using w0 = 0
and w1 = 1, which appeared to produce the best
results. Thus, only the backward transliteration
model was ultimately employed.
3.4 English-Chinese string matching
Due to the fact that names transliterated into Chi-
nese consist of multiple Chinese characters and
that the Chinese text provided in this shared task
is not segmented, we have to adopt a different ap-
proach to the English-Chinese mining task (Unlike
many other languages, there are no clear bound-
aries between Chinese words). We first train a
generation model using the seed data and then ap-
ply a greedy string matching algorithm to extract
transliteration pairs.
The generation model is built using the discrim-
inative training framework described in (Jiampoja-
marn et al, 2008). Two models are learned: one
is trained using English and Chinese characters,
while the other is trained on English and Pinyin (a
standard phonetic representation of Chinese char-
acters). In order to mine transliteration pairs from
Wikipedia titles, we first use the generation model
to produce transliterations for each English token
on the source side as both Chinese characters and
Pinyin. The generated Chinese characters are ul-
timately converted to Pinyin during string match-
ing. We also convert all the Chinese characters on
the target side to their Pinyin representations when
performing string matching.
The transliteration pairs are then mined by com-
bining two different strategies. First of all, we ob-
serve that most of the titles that contain a separa-
tion symbol ? ? ? on the target side are translit-
erations. In this case, the number of tokens on
both sides is often equal. Therefore, the mining
task can be formulated as a matching problem.
We use a competitive linking approach (Melamed,
2000) to find the best match. First, we select
links between all possible pairs if similarity of
strings on both sides is above a threshold (0.6 ?
length(Pinyin)). We then greedily extract the
pairs with highest similarity until the number of
unextracted segments on either side becomes zero.
The problem becomes harder when there is no
indication of word segmentation for Chinese. In-
stead of trying to segment the Chinese characters
first, we use an incremental string matching strat-
45
egy. For each token on the source side, the algo-
rithm calculates its similarity with all possible n-
grams (2 ? n ? L) on the target side, where L
is the length of the Chinese title (i.e., the number
of characters). If the similarity score of n-gram
with the highest similarity surpasses a threshold
(0.5 ? length(Pinyin)), the n-gram sequence is
proposed as a possible transliteration for the cur-
rent source token.
3.5 Resolving overlapping predictions
Given a set of candidate word pairs that have been
extracted from a given Wikipedia title according to
the procedure described in Section 3.1, our clas-
sifiers predict a class label for each pair inde-
pendently of the others. Pairs that receive neg-
ative predictions are discarded immediately and
are never reported as mined pairs. However, it
is sometimes necessary to arbitrate between pos-
itive predictions, since it is possible for a classifier
to mark as transliterations two or more pairs that
involve the same English word or the same target
word in the title. Clearly, mining multiple overlap-
ping pairs will lower the system?s precision, since
there is (presumably) at most one correct translit-
eration in the target language version of the title
for each English word.3
Our solution is to apply a greedy algorithm that
sorts the word pair predictions for a given title
in descending order according to the scores that
were assigned by the classifier. We make one pass
through the sorted list and report a pair of words as
a mined pair unless the English word or the target
language word has already been reported (for this
particular title).4
3.6 Results
In the context of the NEWS 2010 Shared Task
on Transliteration Generation we tested our sys-
tem on all five data sets: from English to Rus-
sian (EnRu), Hindi (EnHi), Tamil (EnTa), Arabic
(EnAr), and Chinese (EnCh). The EnCh set dif-
fers from the remaining sets in the lack of transpar-
ent word segmentation on the Chinese side. There
were no development sets provided for any of the
language pairs.
3On the other hand, mining all such pairs might improve
recall.
4A bug was later discovered in our implementation of this
algorithm, which had failed to add the words in a title?s first
mined pair to the ?already reported? list. This sometimes
caused up to two additional mined pairs per title to be re-
ported in the prediction files that were submitted.
Task System F P R
EnRu NED+ .875 .880 .869
BK-2007 .778 .684 .902
StringKernel* .811 .746 .889
DIRECTL+ .786 .778 .795
EnHi NED+ .907 .875 .941
BK-2007 .882 .883 .880
StringKernel .924 .954 .895
DIRECTL+ .904 .945 .866
EnTa NED+ .791 .916 .696
BK-2007 .829 .808 .852
StringKernel .914 .923 .906
DIRECTL+ .801 .919 .710
EnAr NED+ .800 .818 .783
BK-2007 .816 .834 .798
StringKernel* .827 .917 .753
DIRECTL+ .742 .861 .652
EnCh GreedyMatch .530 .698 .427
DIRECTL+ .009 .045 .005
Table 3: Transliteration mining results. An aster-
isk (*) indicates an unofficial result.
Table 3 shows the results obtained by our var-
ious systems on the final test sets, measured in
terms of F-score (F), precision (P), and recall
(R). The systems referred to as NED+, BK-2007,
StringKernel, DIRECTL+, and GreedyMatch are
described in Section 3.3.2, Section 3.3.3, Sec-
tion 3.3.4, Section 3.3.5, and Section 3.4 respec-
tively. The runs marked with an asterisk (*)
were produced after the Shared Task deadline, and
therefore are not included in the official results.
3.7 Discussion
No fixed ranking of the four approaches emerges
across the four alphabetic language pairs (all ex-
cept EnCh). However, StringKernel appears to be
the most robust, achieving the highest F-score on
three language pairs. This suggests that longer
substring features are indeed useful for classifying
candidate transliteration pairs. The simple NED+
method is a clear winner on EnRu, and obtains de-
cent scores on the remaining alphabetic language
pairs. The generation-based DIRECTL+ approach
ranks no higher than third on any language pair,
and it fails spectacularly on EnCh because of the
word segmentation ambiguity.
Finally, we observe that there are a number of
cases where the results for our discriminatively
trained classifiers, BK-2007 and StringKernel, are
46
not significantly better than those of the simple
NED+ approach. We conjecture that automatically
generating training examples is suboptimal for this
task. A more effective strategy may be to filter all
possible word pairs in the seed data to only those
with NED above a fixed threshold. We would then
apply the same threshold to the Wikipedia candi-
dates, only passing to the classifier those pairs that
surpass the threshold. This would enable a better
match between the training and test operation of
the system.
4 Conclusion
The results obtained in the context of the NEWS
2010 Machine Transliteration Shared Task con-
firm the effectiveness of our discriminative ap-
proaches to transliteration generation and mining.
Acknowledgments
This research was supported by the Alberta Inge-
nuity Fund, Informatics Circle of Research Excel-
lence (iCORE), and the Natural Sciences and En-
gineering Research Council of Canada (NSERC).
References
Shane Bergsma and Grzegorz Kondrak. 2007.
Alignment-based discriminative string similarity. In
Proc. ACL.
Aditya Bhargava and Grzegorz Kondrak. 2010. Lan-
guage identification of names with SVMs. In Proc.
NAACL-HLT.
Mikhail Bilenko and Raymond J. Mooney. 2003.
Adaptive duplicate detection using learnable string
similarity measures. In Proc. KDD.
Chih-Chung Chang and Chih-Jen Lin. 2001. LIB-
SVM: a library for support vector machines.
Software available at http://www.csie.ntu.
edu.tw/?cjlin/libsvm.
Koby Crammer and Yoram Singer. 2003. Ultracon-
servative online algorithms for multiclass problems.
Journal of Machine Learning Research, 3:951?991.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLIN-
EAR: A library for large linear classification. JMLR,
9:1871?1874.
Dan Goldwasser and Dan Roth. 2008. Transliteration
as constrained optimization. In Proc. EMNLP.
Sittichai Jiampojamarn, Grzegorz Kondrak, and Tarek
Sherif. 2007. Applying many-to-many alignments
and Hidden Markov Models to letter-to-phoneme
conversion. In Proc. HLT-NAACL.
Sittichai Jiampojamarn, Colin Cherry, and Grzegorz
Kondrak. 2008. Joint processing and discriminative
training for letter-to-phoneme conversion. In Proc.
ACL.
Sittichai Jiampojamarn, Aditya Bhargava, Qing Dou,
Kenneth Dwyer, and Grzegorz Kondrak. 2009. Di-
recTL: a language-independent approach to translit-
eration. In NEWS ?09: Proceedings of the 2009
Named Entities Workshop: Shared Task on Translit-
eration, pages 28?31.
Sittichai Jiampojamarn, Colin Cherry, and Grzegorz
Kondrak. 2010. Integrating joint n-gram features
into a discriminative training framework. In Proc.
NAACL-HLT.
Alexandre Klementiev and Dan Roth. 2006. Named
entity transliteration and discovery from multilin-
gual comparable corpora. In Proc. HLT-NAACL.
Grzegorz Kondrak. 2000. A new algorithm for the
alignment of phonetic sequences. In Proc. NAACL,
pages 288?295.
Andrew McCallum, Kedar Bellare, and Fernando
Pereira. 2005. A conditional random field for
discriminatively-trained finite-state string edit dis-
tance. In Proc. UAI.
I. Dan Melamed. 2000. Models of translational equiv-
alence among words. Computational Linguistics,
26(2):221?249.
Eric Sven Ristad and Peter N. Yianilos. 1998. Learn-
ing string-edit distance. IEEE Trans. Pattern Analy-
sis and Machine Intelligence, 20(5).
John Shawe-Taylor and Nello Cristianini. 2004. Ker-
nel Methods for Pattern Analysis. Cambridge Uni-
versity Press.
47
Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 172?181,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Improved Natural Language Learning via
Variance-Regularization Support Vector Machines
Shane Bergsma
University of Alberta
sbergsma@ualberta.ca
Dekang Lin
Google, Inc.
lindek@google.com
Dale Schuurmans
University of Alberta
dale@cs.ualberta.ca
Abstract
We present a simple technique for learn-
ing better SVMs using fewer training ex-
amples. Rather than using the standard
SVM regularization, we regularize toward
low weight-variance. Our new SVM ob-
jective remains a convex quadratic func-
tion of the weights, and is therefore com-
putationally no harder to optimize than a
standard SVM. Variance regularization is
shown to enable dramatic improvements
in the learning rates of SVMs on three lex-
ical disambiguation tasks.
1 Introduction
Discriminative training is commonly used in NLP
and speech to scale the contribution of different
models or systems in a combined predictor. For
example, discriminative training can be used to
scale the contribution of the language model and
translation model in machine translation (Och and
Ney, 2002). Without training data, it is often rea-
sonable to weight the different models equally. We
propose a simple technique that exploits this intu-
ition for better learning with fewer training exam-
ples. We regularize the feature weights in a Sup-
port Vector Machine (Cortes and Vapnik, 1995) to-
ward a low-variance solution. Since the new SVM
quadratic program is convex, it is no harder to op-
timize than the standard SVM objective.
When training data is generated through hu-
man effort, faster learning saves time and money.
When examples are labeled automatically, through
user feedback (Joachims, 2002) or from tex-
tual pseudo-examples (Smith and Eisner, 2005;
Okanohara and Tsujii, 2007), faster learning can
reduce the lag before a new system is useful.
We demonstrate faster learning on lexical dis-
ambiguation tasks. For these tasks, a system pre-
dicts a label for a word in text, based on the
word?s context. Possible labels include part-of-
speech tags, named-entity types, and word senses.
A number of disambiguation systems make pre-
dictions with the help of N-gram counts from a
web-scale auxiliary corpus, typically via a search-
engine (Lapata and Keller, 2005) or N-gram cor-
pus (Bergsma et al, 2009). When discriminative
training is used to weigh the counts for classifi-
cation, many of the learned feature weights have
similar values. Good weights have low variance.
For example, consider the task of preposition
selection. A system selects the most likely prepo-
sition given the context, and flags a possible error
if it disagrees with the user?s choice:
? I worked in Russia from 1997 to 2001.
? I worked in Russia *during 1997 to 2001.
Bergsma et al (2009) use a variety of web counts
to predict the correct preposition. They have fea-
tures for COUNT(in Russia from), COUNT(Russia
from 1997), COUNT(from 1997 to), etc. If these are
high, from is predicted. Similarly, they have fea-
tures for COUNT(in Russia during), COUNT(Russia
during 1997), COUNT(during 1997 to). These fea-
tures predict during. All counts are in the log
domain. The task has thirty-four different prepo-
sitions to choose from. A 34-way classifier is
trained on examples of correct preposition usage;
it learns which context positions and sizes are most
reliable and assigns feature weights accordingly.
A very strong unsupervised baseline, however,
is to simply weight all the count features equally.
In fact, in Bergsma et al (2009), the supervised
approach requires over 30,000 training examples
before it outperforms this baseline. In contrast,
we show that by regularizing a classifier toward
equal weights, a supervised predictor outperforms
the unsupervised approach after only ten exam-
ples, and does as well with 1000 examples as the
standard classifier does with 100,000.
172
Section 2 first describes a general multi-class
SVM. We call the base vector of information
used by the SVM the attributes. A standard
multi-class SVM creates features for the cross-
product of attributes and classes. E.g., the attribute
COUNT(Russia during 1997) is not only a feature
for predicting the preposition during, but also for
predicting the 33 other prepositions. The SVM
must therefore learn to disregard many irrelevant
features. We observe that this is not necessary,
and develop an SVM that only uses the relevant
attributes in the score for each class. Building on
this efficient framework, we incorporate variance
regularization into the SVM?s quadratic program.
We apply our algorithms to three tasks: prepo-
sition selection, context-sensitive spelling correc-
tion, and non-referential pronoun detection (Sec-
tion 4). We reproduce Bergsma et al (2009)?s
results using a multi-class SVM. Our new mod-
els achieve much better accuracy with fewer train-
ing examples. We also exceed the accuracy of a
reasonable alternative technique for increasing the
learning rate: including the output of the unsuper-
vised system as a feature in the SVM.
Variance regularization is an elegant addition to
the suite of methods in NLP that improve perfor-
mance when access to labeled data is limited. Sec-
tion 5 discusses some related approaches. While
we motivate our algorithm as a way to learn better
weights when the features are counts from an aux-
iliary corpus, there are other potential uses of our
method. We outline some of these in Section 6,
and note other directions for future research.
2 Three Multi-Class SVM Models
We describe three max-margin multi-class classi-
fiers and their corresponding quadratic programs.
Although we describe linear SVMs, they can be
extended to nonlinear cases in the standard way
by writing the optimal function as a linear combi-
nation of kernel functions over the input examples.
In each case, after providing the general tech-
nique, we relate the approach to our motivating
application: learning weights for count features in
a discriminative web-scale N-gram model.
2.1 Standard Multi-Class SVM
We define a K-class SVM following Crammer and
Singer (2001). This is a generalization of binary
SVMs (Cortes and Vapnik, 1995). We have a set
{(x?1, y1), ..., (x?M , yM )} of M training examples.
Each x? is an N -dimensional attribute vector, and
y ? {1, ...,K} are classes. A classifier, H , maps
an attribute vector, x?, to a class, y. H is parame-
terized by a K-by-N matrix of weights, W:
HW(x?) =
Kargmax
r=1
{W?r ? x?} (1)
where W?r is the rth row of W. That is, the pre-
dicted label is the index of the row of W that has
the highest inner-product with the attributes, x?.
We seek weights such that the classifier makes
few errors on training data and generalizes well
to unseen data. There are KN weights to learn,
for the cross-product of attributes and classes.
The most common approach is to train K sep-
arate one-versus-all binary SVMs, one for each
class. The weights learned for the rth SVM pro-
vide the weights W?r in (1). We call this approach
OvA-SVM. Note in some settings various one-
versus-one strategies may be more effective than
one-versus-all (Hsu and Lin, 2002).
The weights can also be found using a single
constrained optimization (Vapnik, 1998; Weston
and Watkins, 1998). Following the soft-margin
version in Crammer and Singer (2001):
min
W,?1,...,?M
1
2
K
?
i=1
||W?i||2 + C
m
?
i=1
?i
subject to : ?i ?0
?r 6= yi, W?yi ? x?i ? W?r ? x?i ?1 ? ?i (2)
The constraints require the correct class to be
scored higher than other classes by a certain mar-
gin, with slack for non-separable cases. Minimiz-
ing the weights is a form of regularization. Tuning
the C-parameter controls the emphasis on regular-
ization versus separation of training examples.
We call this the K-SVM. The K-SVM out-
performed the OvA-SVM in Crammer and Singer
(2001), but see Rifkin and Klautau (2004). The
popularity of K-SVM is partly due to convenience;
it is included in popular SVM software like SVM-
multiclass1 and LIBLINEAR (Fan et al, 2008).
Note that with two classes, K-SVM is less effi-
cient than a standard binary SVM. A binary classi-
fier outputs class 1 if (w? ? x? > 0) and class 2 other-
wise. The K-SVM encodes a binary classifier using
W?1 = w? and W?2 = ?w?, therefore requiring twice
the memory of a binary SVM. However, both bi-
nary and 2-class formulations have the same solu-
tion (Weston and Watkins, 1998).
1http://svmlight.joachims.org/svm multiclass.html
173
2.1.1 Web-Scale N-gram K-SVM
K-SVM was used with N-gram models in Bergsma
et al (2009). For preposition selection, attributes
were web counts of patterns filled with 34 preposi-
tions, corresponding to the 34 classes. Each prepo-
sition serves as the filler of each context pattern.
Fourteen patterns were used for each filler: all five
5-grams, four 4-grams, three 3-grams, and two 2-
grams spanning the position to be predicted. There
are N = 14?34 = 476 total attributes, and therefore
KN = 476 ? 34 = 16184 weights in W.
This K-SVM classifier can potentially exploit
very subtle information. Let W?in and W?before
be weights for the classes in and before. Notice
some of the attributes weighted in the inner prod-
ucts W?before ? x? and W?in ? x? will be for counts of
the preposition after. Relatively high counts for a
context with after should deter us from choosing
in more than from choosing before. These cor-
relations can be encoded in the classifier via the
corresponding weights on after-counts in W?in and
W?before. How useful are these correlations and
how much training data is needed before they can
be learned and exploited effectively?
We next develop a model that, for each class,
only scores those attributes deemed to be directly
relevant to the class. Our experiments thus empir-
ically address these questions for different tasks.
2.2 SVM with Class-Specific Attributes
Suppose we can partition our attribute vectors into
sub-vectors that only include attributes that we de-
clare as relevant to the corresponding class: x? =
(x?1, ..., x?K). We develop a classifier that only
uses the class-specific attributes in the score for
each class. The classifier uses an N -dimensional
weight vector, w?, which follows the attribute par-
tition, w? = (w?1, ..., w?K). The classifier is:
Hw?(x?) =
Kargmax
r=1
{w?r ? x?r} (3)
We call this classifier the CS-SVM (an SVM with
Class-Specific attributes).
The weights can be determined using the follow
(soft-margin) optimization:
min
w?,?1,...,?m
1
2 w?
T w? + C
m
?
i=1
?i
subject to : ?i ?0
?r 6= yi, w?yi ? x?iyi ? w?r ? x?ir ?1 ? ?i (4)
There are several advantages to this formula-
tion. Foremost, rather than having KN weights,
it can have only N . For linear classifiers, the
number of examples needed to reach optimum
performance is at most linear in the number of
weights (Vapnik, 1998; Ng and Jordan, 2002). In
fact, both the total number and number of active
features per example decrease by K. Thus this re-
duction saves far more memory than what could
be obtained by an equal reduction in dimensional-
ity via pruning infrequent attributes.
Also, note that unlike the K-SVM (Section 2.1),
in the binary case the CS-SVM is completely equiv-
alent (thus equally efficient) to a standard SVM.
We will not always a priori know the class as-
sociated with each attribute. Also, some attributes
may be predictive of multiple classes. In such
cases, we can include ambiguous attributes in ev-
ery sub-vector (needing N+D(K-1) total weights
if D attributes are duplicated). In the degenerate
case where every attribute is duplicated, CS-SVM
is equivalent to K-SVM; both have KN weights.
2.2.1 Optimization as a Binary SVM
We could solve the optimization problem in (4)
directly using a quadratic programming solver.
However, through an equivalent transformation
into a binary SVM, we can take advantage of effi-
cient, custom SVM optimization algorithms.
We follow Har-Peled et al (2003) in transform-
ing a multi-class example into a set of binary
examples, each specifying a constraint from (4).
We extend the attribute sub-vector corresponding
to each class to be N -dimensional. We do this
by substituting zero-vectors for all the other sub-
vectors in the partition. The attribute vector for the
rth class is then z?r = (0?, ..., 0?, x?r, 0?, ..., 0?). This is
known as Kesler?s Construction and has a long his-
tory in classification (Duda and Hart, 1973; Cram-
mer and Singer, 2003). We then create binary rank
constraints for a ranking SVM (Joachims, 2002)
(ranking SVMs reduce to standard binary SVMs).
We create K instances for each multi-class exam-
ple (x?i, yi), with the transformed vector of the true
class, z?yi , assigned a higher-rank than all the other,
equally-ranked classes, z?{r 6=yi}. Training a rank-
ing SVM using these constraints gives the same
weights as solving (4), but allows us to use effi-
cient, custom SVM software.2 Note the K-SVM
2One subtlety is whether to use a single slack, ?i, for all
K-1 constraints per example i (Crammer and Singer, 2001),
or a different slack for each constraint (Joachims, 2002). Us-
174
can also be trained this way, by including every
attribute in every sub-vector, as described earlier.
2.2.2 Web-Scale N-gram CS-SVM
Returning to our preposition selection example, an
obvious attribute partition for the CS-SVM is to
include as attributes for predicting preposition r
only those counts for patterns filled with preposi-
tion r. Thus x?in will only include counts for con-
text patterns filled with in and x?before will only
include counts for context patterns filled with be-
fore. With 34 sub-vectors and 14 attributes in each,
there are only 14 ? 34 = 476 total weights. In con-
trast, K-SVM had 16184 weights to learn.
It is instructive to compare the CS-SVM in (3) to
the unsupervised SUMLM approach in Bergsma et
al. (2009). That approach can be written as:
H(x?) = Kargmax
r=1
{1? ? x?r} (5)
where 1? is an N -dimensional vector of ones. This
is CS-SVM with all weights set to unity. The
counts for each preposition are simply summed,
and whichever one scores the highest is taken as
the output (actually only a subset of the counts are
used, see Section 4.1). As mentioned earlier, this
system performs remarkably well on several tasks.
2.3 Variance Regularization SVMs
Suppose we choose our attribute partition well and
train the CS-SVM on a sufficient number of exam-
ples to achieve good performance. It is a reason-
able hypothesis that the learned weights will be
predominantly positive. This is because each sub-
vector x?r was chosen to only include attributes
that are predictive of class r. Unlike the classifier
in (1) which weighs positive and negative evidence
together for each class, in CS-SVM, negative evi-
dence only plays a roll as it contributes to the score
of competing classes.
If all the attributes are equally important, the
weights should be equal, as in the unsupervised
approach in (5). If some are more important than
others, the training examples should reflect this
and the learner can adjust the weights accord-
ingly.3 In the absence of this training evidence, it
is reasonable to bias the classifier toward an equal-
weight solution.
ing the former may be better as it results in a tighter bound
on empirical risk (Tsochantaridis et al, 2005).
3E.g., the true preposition might be better predicted by the
counts of patterns that tend to include the preposition?s gram-
matical object, i.e., patterns that include more right-context.
Rather than the standard SVM regularization
that minimizes the norm of the weights as in (4),
we therefore regularize toward weights that have
low variance. More formally, we can regard the
set of weights, w1, ..., wN , as the distribution of a
discrete random variable, W . We can calculate the
mean and variance of this variable from its distri-
bution. We seek a variable that has low variance.
We begin with a more general objective and
then explain how a specific choice of covariance
matrix, C, minimizes the variance of the weights.
We propose the regularizer:
min
w?,?1,...,?m
1
2 w?
TCw? +C
m
?
i=1
?i
subject to : ?i ?0
?r 6= yi, w?yi ? x?iyi ? w?r ? x?ir ?1 ? ?i (6)
where C is a normalized covariance matrix such
that
?
i,j Ci,j = 0. This ensures uniform weight
vectors receive zero regularization penalty. Since
all covariance matrices are positive semi-definite,
the quadratic program (QP) remains convex in w?,
and thus amenable to general purpose QP-solvers.
Since the unsupervised system in (5) has zero
weight variance, the SVM learned in (6) should do
as least as well as (5) as we tune the C-parameter
on development data. That is, as C approaches
zero, variance minimization becomes the sole ob-
jective of (6), and uniform weights are produced.
We use covariance matrices of the form:
C = diag(p?) ? p?p?T (7)
where diag(p?) is the matrix constructed by putting
p? on the main diagonal. Here, p? is an arbitrary
N -dimensional weighting vector, such that p ?
0 and ?i pi = 1. p? dictates the contribution of
each wi to the mean and variance of the weights
in w?. It is easy to see that
?
i,j Ci,j =
?
i pi ?
?
i
?
j pipj = 0.
We now show that w?T (diag(p?) ? p?p?T )w? ex-
presses the variance of the weights in w? with re-
spect to the probability weighting p?. The variance
of a random variable with mean E[W ] = ? is:
Var[W ] = E[(W ? ?)2] = E[W 2] ? E[W ]2
The mean of the weights using probability weight-
ing p? is E[W ] = w?T p? = p?w?. Also, E[W 2] =
w?T diag(p?)w?. Thus:
Var[W ] = w?T diag(p?)w? ? (w?T p?)(p?w?)
= w?T (diag(p?) ? p?p?)w?
175
In our experiments, we deem each weight to be
equally important to the variance calculation, and
set pi = 1N ,?i = 1, . . . , N .
The goal of the regularization in (6) using C
from (7) can be regarded as directing the SVM to-
ward a good unsupervised system, regardless of
the constraints (training examples). In some un-
supervised systems, however, only a subset of the
attributes are used. In other cases, distinct subsets
of weights should have low variance, rather than
minimizing the variance across all weights. There
are examples of these situations in Section 4.
We can account for these cases in our QP. We
provide separate terms in our quadratic function
for the subsets of w? that should have low vari-
ance. Suppose we create L subsets of w?: ??1, ...??L,
where ??j is w? with elements set to zero that are not
in subset j. We then minimize 12(??T1 C1??1 + ... +
??TLCL??L). If the terms in subset j have low vari-
ance, Cj = C from (7) is used. If the subset corre-
sponds to attributes that are not a priori known to
be useful, an identity matrix can instead be used,
Cj = I, and these weights will be regularized to-
ward zero as in a standard SVM.4
Variance regularization therefore exploits extra
knowledge by the system designer. The designer
decides which weights should have similar values,
and the SVM is biased to prefer this solution.
One consequence of being able to regularize
different subsets of weights is that we can also ap-
ply variance regularization to the standard multi-
class SVM (Section 2.1). We can use an identity
Ci matrix for all irrelevant weights, i.e., weights
that correspond to class-attribute pairs where the
attribute is not directly relevant to the class. In our
experiments, however, we apply variance regular-
ization to the more efficient CS-SVM.
We refer to a CS-SVM trained using the variance
minimization quadratic program as the VAR-SVM.
2.3.1 Web-Scale N-gram VAR-SVM
If variance regularization is applied to all weights,
attributes COUNT(in Russia during), COUNT(Russia
during 1997), and COUNT(during 1997 to) will be
encouraged to have similar weights in the score for
class during. Furthermore, these will be weighted
similarly to other patterns, filled with other prepo-
sitions, used in the scores for other classes.
4Weights must appear in ?1 subsets (possibly only in the
Cj = I subset). Each occurs in at most one in our experi-
ments. Note it is straightforward to express this as a single
covariance matrix regularizer over w?; we omit the details.
Alternatively, we could minimize the variance
separately over all 5-gram patterns, then over all
4-gram patterns, etc., or over all patterns with a
filler in the same position. In our experiments, we
took a very simple approach: we minimized the
variance of all attributes that are weighted equally
in the unsupervised baselines. If a feature is not in-
cluded in a baseline, it is regularized toward zero.
3 Experimental Details
We use the data sets from Bergsma et al (2009).
These are the three tasks where web-scale N-gram
counts were previously used as features in a stan-
dard K-SVM. In each case a classifier makes a de-
cision for a particular word based on the word?s
surrounding context. The attributes of the classi-
fier are the log counts of different fillers occurring
in the context patterns. We retrieve counts from
the web-scale Google Web 5-gram Corpus (Brants
and Franz, 2006), which includes N-grams of
length one to five. We apply add-one smoothing
to all counts. Every classifier also has bias fea-
tures (for every class). We simply include, where
appropriate, attributes that are always unity.
We use LIBLINEAR (Fan et al, 2008) to train
K-SVM and OvA-SVM, and SVMrank (Joachims,
2006) to train CS-SVM. For VAR-SVM, we solve
the primal form of the quadratic program directly
in CPLEX (2005), a general optimization package.
We vary the number of training examples for
each classifier. The C-parameters of all SVMs are
tuned on development data. We evaluate using ac-
curacy: the percentage of test examples that are
classified correctly. We also provide the accuracy
of the majority-class baseline and best unsuper-
vised system, as defined in Bergsma et al (2009).
As an alternative way to increase the learning
rate, we augment a classifier?s features using the
output of the unsupervised system: For each class,
we include one feature for the sum of all counts (in
the unsupervised system) that predict that class.
We denote these augmented systems with a + as
in K-SVM+ and CS-SVM+.
4 Applications
4.1 Preposition Selection
Preposition errors are common among new En-
glish speakers (Chodorow et al, 2007). Systems
that can reliably identify these errors are needed
in word processing and educational software.
176
Training Examples
System 10 100 1K 10K 100K
OvA-SVM 16.0 50.6 66.1 71.1 73.5
K-SVM 13.7 50.0 65.8 72.0 74.7
K-SVM+ 22.2 56.8 70.5 73.7 75.2
CS-SVM 27.1 58.8 69.0 73.5 74.2
CS-SVM+ 39.6 64.8 71.5 74.0 74.4
VAR-SVM 73.8 74.2 74.7 74.9 74.9
Table 1: Accuracy (%) of preposition-selection
SVMs. Unsupervised accuracy is 73.7%.
In our experiments, a classifier must choose the
correct preposition among 34 candidates, using
counts for filled 2-to-5-gram patterns. We use
100K training, 10K development, and 10K test
examples. The unsupervised approach sums the
counts of all 3-to-5-gram patterns for each prepo-
sition. We therefore regularize the variance of the
3-to-5-gram weights in VAR-SVM, and simultane-
ously minimize the norm of the 2-gram weights.
4.1.1 Results
The majority-class is the preposition of; it occurs
in 20.3% of test examples. The unsupervised sys-
tem scores 73.7%. For further perspective on these
results, note Chodorow et al (2007) achieved 69%
with 7M training examples, while Tetreault and
Chodorow (2008) found the human performance
was around 75%. However, these results are not
directly comparable as they are on different data.
Table 1 gives the accuracy for different amounts
of training data. Here, as in the other tasks, K-SVM
mirrors the learning rate in Bergsma et al (2009).
There are several distinct phases among the rela-
tive ranking of the systems. For smaller amounts
of training data (?1000 examples) K-SVM per-
forms worst, while VAR-SVM is statistically sig-
nificantly better than all other systems, and al-
ways exceeds the performance of the unsupervised
approach.5 Augmenting the attributes with sum
counts (the + systems) strongly helps with fewer
examples, especially in conjunction with the more
efficient CS-SVM. However, VAR-SVM clearly
helps more. We noted earlier that VAR-SVM is
guaranteed to do as well as the unsupervised sys-
tem on the development data, but here we confirm
that it can also exploit even small amounts of train-
ing data to further improve accuracy.
CS-SVM outperforms K-SVM except with 100K
5Significance is calculated using a ?2 test over the test set
correct/incorrect contingency table.
Training Examples
System 10 100 1K 10K 100K
CS-SVM 86.0 93.5 95.1 95.7 95.7
CS-SVM+ 91.0 94.9 95.3 95.7 95.7
VAR-SVM 94.9 95.3 95.6 95.7 95.8
Table 2: Accuracy (%) of spell-correction SVMs.
Unsupervised accuracy is 94.8%.
examples, while OvA-SVM is better than K-SVM
for small amounts of data.6 K-SVM performs best
with all the data; it uses the most expressive repre-
sentation, but needs 100K examples to make use
of it. On the other hand, feature augmentation
and variance regularization provide diminishing
returns as the amount of training data increases.
4.2 Context-Sensitive Spelling Correction
Context-sensitive spelling correction, or real-word
error/malapropism detection (Golding and Roth,
1999; Hirst and Budanitsky, 2005), is the task of
identifying errors when a misspelling results in a
real word in the lexicon, e.g., using site when sight
or cite was intended. Contextual spell checkers are
among the most widely-used NLP technology, as
they are included in commercial word processing
software (Church et al, 2007).
For every occurrence of a word in a pre-defined
confusion set (e.g. {cite, sight, cite}), the clas-
sifier selects the most likely word from the set.
We use the five confusion sets from Bergsma et al
(2009); four are binary and one is a 3-way classi-
fication. We use 100K training, 10K development,
and 10K test examples for each, and average ac-
curacy across the sets. All 2-to-5 gram counts are
used in the unsupervised system, so the variance
of all weights is regularized in VAR-SVM.
4.2.1 Results
On this task, the majority-class baseline is much
higher, 66.9%, and so is the accuracy of the top un-
supervised system: 94.8%. Since four of the five
sets are binary classifications, where K-SVM and
CS-SVM are equivalent, we only give the accuracy
of the CS-SVM (it does perform better on the one
3-way set). VAR-SVM again exceeds the unsuper-
vised accuracy for all training sizes, and generally
6Rifkin and Klautau (2004) argue OvA-SVM is as good
as K-SVM, but this is ?predicated on the assumption that the
classes are ?independent?,? i.e., that examples from class 0
are no closer to class 1 than to class 2. This is not true of this
task (e.g. x?before is closer to x?after than x?in, etc.).
177
Training Examples
System 10 100 1K
CS-SVM 59.0 71.0 84.3
CS-SVM+ 59.4 74.9 84.5
VAR-SVM 70.2 76.2 84.5
VAR-SVM+FreeB 64.2 80.3 84.5
Table 3: Accuracy (%) of non-referential detection
SVMs. Unsupervised accuracy is 80.1%.
performs as well as the augmented CS-SVM+ us-
ing an order of magnitude less training data (Ta-
ble 2). Differences from ?1K are significant.
4.3 Non-Referential Pronoun Detection
Non-referential detection predicts whether the En-
glish pronoun it refers to a preceding noun (?it
lost money?) or is used as a grammatical place-
holder (?it is important to...?). This binary clas-
sification is a necessary but often neglected step
for noun phrase coreference resolution (Paice and
Husk, 1987; Bergsma et al, 2008; Ng, 2009).
Bergsma et al (2008) use features for the counts
of various fillers in the pronoun?s context patterns.
If it is the most common filler, the pronoun is
likely non-referential. If other fillers are common
(like they or he), it is likely a referential instance.
For example, ?he lost money? is common on the
web, but ?he is important to? is not. We use the
same fillers as in previous work, and preprocess
the N-gram corpus in the same way.
The unsupervised system picks non-referential
if the difference between the summed count of
it fillers and the summed count of they fillers is
above a threshold (note this no longer fits (5),
with consequences discussed below). We thus
separately minimize the variance of the it pattern
weights and the they pattern weights. We use 1K
training, 533 development, and 534 test examples.
4.3.1 Results
The most common class is referential, occurring
in 59.4% of test examples. The unsupervised sys-
tem again does much better, at 80.1%.
Annotated training examples are much harder
to obtain for this task and we experiment with a
smaller range of training sizes (Table 3). The per-
formance of VAR-SVM exceeds the performance
of K-SVM across all training sizes (bold accura-
cies are significantly better than either CS-SVM for
?100 examples). However, the gains were not
as large as we had hoped, and accuracy remains
worse than the unsupervised system when not us-
ing all the training data. When using all the data,
a fairly large C-parameter performs best on devel-
opment data, so regularization plays less of a role.
After development experiments, we speculated
that the poor performance relative to the unsuper-
vised approach was related to class bias. In the
other tasks, the unsupervised system chooses the
highest summed score. Here, the difference in it
and they counts is compared to a threshold. Since
the bias feature is regularized toward zero, then,
unlike the other tasks, using a low C-parameter
does not produce the unsupervised system, so per-
formance can begin below the unsupervised level.
Since we wanted the system to learn this thresh-
old, even when highly regularized, we removed
the regularization penalty from the bias weight,
letting the optimization freely set the weight to
minimize training error. With more freedom, the
new classifier (VAR-SVM+FreeB) performs worse
with 10 examples, but exceeds the unsupervised
approach with 100 training points. Although
this was somewhat successful, developing better
strategies for bias remains useful future work.
5 Related Work
There is a large body of work on regularization in
machine learning, including work that uses posi-
tive semi-definite matrices in the SVM quadratic
program. The graph Laplacian has been used to
encourage geometrically-similar feature vectors to
be classified similarly (Belkin et al, 2006). An ap-
pealing property of these approaches is that they
incorporate information from unlabeled examples.
Wang et al (2006) use Laplacian regularization
for the task of dependency parsing. They regular-
ize such that features for distributionally-similar
words have similar weights. Rather than penal-
ize pairwise differences proportional to a similar-
ity function, we simply penalize weight variance.
In the field of computer vision, Tefas et al
(2001) (binary) and Kotsia et al (2009) (multi-
class) also regularize weights with respect to a co-
variance matrix. They use labeled data to find the
sum of the sample covariance matrices from each
class, similar to linear discriminant analysis. We
propose the idea in general, and instantiate with
a different C matrix: a variance regularizer over
w?. Most importantly, our instantiated covariance
matrix does not require labeled data to generate.
In a Bayesian setting, Raina et al (2006) model
178
feature correlations in a logistic regression clas-
sifier. They propose a method to construct a co-
variance matrix for a multivariate Gaussian prior
on the classifier?s weights. Labeled data for other,
related tasks is used to infer potentially correlated
features on the target task. Like in our results, they
found that the gains from modeling dependencies
diminish as more training data is available.
We also mention two related online learning ap-
proaches. Similar to our goal of regularizing to-
ward a good unsupervised system, Crammer et al
(2006) regularize w? toward a (different) target vec-
tor at each update, rather than strictly minimizing
||w?||2. The target vector is the vector learned from
the cumulative effect of previous updates. Dredze
et al (2008) maintain the variance of each weight
and use this to guide the online updates. However,
covariance between weights is not considered.
We believe new SVM regularizations in gen-
eral, and variance regularization in particular, will
increasingly be used in combination with related
NLP strategies that learn better when labeled data
is scarce. These may include: using more-general
features, e.g. ones generated from raw text (Miller
et al, 2004; Koo et al, 2008), leveraging out-of-
domain examples to improve in-domain classifi-
cation (Blitzer et al, 2007; Daume? III, 2007), ac-
tive learning (Cohn et al, 1994; Tong and Koller,
2002), and approaches that treat unlabeled data as
labeled, such as bootstrapping (Yarowsky, 1995),
co-training (Blum and Mitchell, 1998), and self-
training (McClosky et al, 2006).
6 Future Work
The primary direction of future research will be
to apply the VAR-SVM to new problems and tasks.
There are many situations where a system designer
has an intuition about the role a feature will play in
prediction; the feature was perhaps added with this
role in mind. By biasing the SVM to use features
as intended, VAR-SVM may learn better with fewer
training examples. The relationship between at-
tributes and classes may be explicit when, e.g.,
a rule-based system is optimized via discrimina-
tive learning, or annotators justify their decisions
by indicating the relevant attributes (Zaidan et al,
2007). Also, if features are a priori thought to
have different predictive worth, the attribute val-
ues could be scaled such that variance regulariza-
tion, as we formulated it, has the desired effect.
Other avenues of future work will be to extend
the VAR-SVM in three directions: efficiency, rep-
resentational power, and problem domain.
While we optimized the VAR-SVM objective in
CPLEX, general purpose QP-solvers ?do not ex-
ploit the special structure of [the SVM optimiza-
tion] problem,? and consequently often train in
time super-linear with the number of training ex-
amples (Joachims et al, 2009). It would be useful
to fit our optimization problem to efficient SVM
training methods, especially for linear classifiers.
VAR-SVM?s representational power could be ex-
tended by using non-linear SVMs. Kernels can
be used with a covariance regularizer (Kotsia et
al., 2009). Since C is positive semi-definite, the
square root of its inverse is defined. We can there-
fore map the input examples using (C? 12 x?), and
write an equivalent objective function in terms of
kernel functions over the transformed examples.
Also, since structured-prediction SVMs build
on the multi-class framework (Tsochantaridis et
al., 2005), variance regularization can be incor-
porated naturally into more complex prediction
tasks, such as parsers, taggers, and aligners.
VAR-SVM may also help in new domains where
annotated data is lacking. VAR-SVM should be
stronger cross-domain than K-SVM; regulariza-
tion with domain-neutral prior-knowledge can off-
set domain-specific biases. Learned weight vec-
tors from other domains may also provide cross-
domain regularization guidance.
7 Conclusion
We presented variance-regularization SVMs, an
approach to learning that creates better classi-
fiers using fewer training examples. Variance reg-
ularization incorporates a bias for known good
weights into the SVM?s quadratic program. The
VAR-SVM can therefore exploit extra knowledge
by the system designer. Since the objective re-
mains a convex quadratic function of the weights,
the program is computationally no harder to opti-
mize than a standard SVM. We also demonstrated
how to design multi-class SVMs using only class-
specific attributes, and compared the performance
of this approach to standard multi-class SVMs on
the task of preposition selection.
While variance regularization is most helpful on
tasks with many classes and features, like prepo-
sition selection, it achieved gains on all our tasks
when training with smaller sample sizes. It should
be useful on a variety of other NLP problems.
179
References
Mikhail Belkin, Partha Niyogi, and Vikas Sindhwani.
2006. Manifold regularization: A geometric frame-
work for learning from labeled and unlabeled exam-
ples. JMLR, 7:2399?2434.
Shane Bergsma, Dekang Lin, and Randy Goebel.
2008. Distributional identification of non-referential
pronouns. In ACL-08: HLT.
Shane Bergsma, Dekang Lin, and Randy Goebel.
2009. Web-scale N-gram models for lexical disam-
biguation. In IJCAI.
John Blitzer, Mark Dredze, and Fernando Pereira.
2007. Biographies, bollywood, boom-boxes and
blenders: Domain adaptation for sentiment classi-
fication. In ACL.
Avrim Blum and Tom Mitchell. 1998. Combining la-
beled and unlabeled data with co-training. In COLT.
Thorsten Brants and Alex Franz. 2006. The Google
Web 1T 5-gram Corpus Version 1.1. LDC2006T13.
Martin Chodorow, Joel R. Tetreault, and Na-Rae Han.
2007. Detection of grammatical errors involving
prepositions. In ACL-SIGSEM Workshop on Prepo-
sitions.
Kenneth Church, Ted Hart, and Jianfeng Gao. 2007.
Compressing trigram language models with Golomb
coding. In EMNLP-CoNLL.
David Cohn, Les Atlas, and Richard Ladner. 1994. Im-
proving generalization with active learning. Mach.
Learn., 15(2):201?221.
Corinna Cortes and Vladimir Vapnik. 1995. Support-
vector networks. Mach. Learn., 20(3):273?297.
CPLEX. 2005. IBM ILOG CPLEX 9.1. www.ilog.
com/products/cplex/.
Koby Crammer and Yoram Singer. 2001. On the algo-
rithmic implementation of multiclass kernel-based
vector machines. JMLR, 2:265?292.
Koby Crammer and Yoram Singer. 2003. Ultracon-
servative online algorithms for multiclass problems.
JMLR, 3:951?991.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. 2006. Online
passive-aggressive algorithms. JMLR, 7:551?585.
Hal Daume? III. 2007. Frustratingly easy domain adap-
tation. In ACL.
Mark Dredze, Koby Crammer, and Fernando Pereira.
2008. Confidence-weighted linear classification. In
ICML.
Richard O. Duda and Peter E. Hart. 1973. Pattern
Classification and Scene Analysis. John Wiley &
Sons.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLIN-
EAR: A library for large linear classification. JMLR,
9:1871?1874.
Andrew R. Golding and Dan Roth. 1999. A Winnow-
based approach to context-sensitive spelling correc-
tion. Mach. Learn., 34(1-3):107?130.
Sariel Har-Peled, Dan Roth, and Dav Zimak. 2003.
Constraint classification for multiclass classification
and ranking. In NIPS.
Graeme Hirst and Alexander Budanitsky. 2005. Cor-
recting real-word spelling errors by restoring lexical
cohesion. Nat. Lang. Eng., 11(1):87?111.
Chih-Wei Hsu and Chih-Jen Lin. 2002. A comparison
of methods for multiclass support vector machines.
IEEE Trans. Neur. Networks, 13(2):415?425.
Thorsten Joachims, Thomas Finley, and Chun-
Nam John Yu. 2009. Cutting-plane training of
structural SVMs. Mach. Learn., 77(1):27?59.
Thorsten Joachims. 2002. Optimizing search engines
using clickthrough data. In KDD.
Thorsten Joachims. 2006. Training linear SVMs in
linear time. In KDD.
Terry Koo, Xavier Carreras, and Michael Collins.
2008. Simple semi-supervised dependency parsing.
In ACL-08: HLT.
Irene Kotsia, Stefanos Zafeiriou, and Ioannis Pitas.
2009. Novel multiclass classifiers based on the min-
imization of the within-class variance. IEEE Trans.
Neur. Networks, 20(1):14?34.
Mirella Lapata and Frank Keller. 2005. Web-based
models for natural language processing. ACM
Trans. Speech and Language Processing, 2(1):1?31.
David McClosky, Eugene Charniak, and Mark John-
son. 2006. Effective self-training for parsing. In
HLT-NAACL.
Scott Miller, Jethran Guinness, and Alex Zamanian.
2004. Name tagging with word clusters and discrim-
inative training. In HLT-NAACL.
Andrew Y. Ng and Michael I. Jordan. 2002. Discrim-
inative vs. generative classifiers: A comparison of
logistic regression and naive bayes. In NIPS.
Vincent Ng. 2009. Graph-cut-based anaphoricity de-
termination for coreference resolution. In NAACL-
HLT.
Franz J. Och and Hermann Ney. 2002. Discriminative
training and maximum entropy models for statistical
machine translation. In ACL.
Daisuke Okanohara and Jun?ichi Tsujii. 2007. A dis-
criminative language model with pseudo-negative
samples. In ACL.
180
Chris D. Paice and Gareth D. Husk. 1987. Towards the
automatic recognition of anaphoric features in En-
glish text: the impersonal pronoun ?it?. Computer
Speech and Language, 2:109?132.
Rajat Raina, Andrew Y. Ng, and Daphne Koller. 2006.
Constructing informative priors using transfer learn-
ing. In ICML.
Ryan Rifkin and Aldebaro Klautau. 2004. In defense
of one-vs-all classification. JMLR, 5:101?141.
Noah A. Smith and Jason Eisner. 2005. Contrastive
estimation: training log-linear models on unlabeled
data. In ACL.
Anastasios Tefas, Constantine Kotropoulos, and Ioan-
nis Pitas. 2001. Using support vector machines to
enhance the performance of elastic graph matching
for frontal face authentication. IEEE Trans. Pattern
Anal. Machine Intell., 23:735?746.
Joel R. Tetreault and Martin Chodorow. 2008. The
ups and downs of preposition error detection in ESL
writing. In COLING.
Simon Tong and Daphne Koller. 2002. Support vec-
tor machine active learning with applications to text
classification. JMLR, 2:45?66.
Ioannis Tsochantaridis, Thorsten Joachims, Thomas
Hofmann, and Yasemin Altun. 2005. Large mar-
gin methods for structured and interdependent out-
put variables. JMLR, 6:1453?1484.
Vladimir N. Vapnik. 1998. Statistical Learning The-
ory. John Wiley & Sons.
Qin Iris Wang, Colin Cherry, Dan Lizotte, and Dale
Schuurmans. 2006. Improved large margin depen-
dency parsing via local constraints and Laplacian
regularization. In CoNLL.
Jason Weston and Chris Watkins. 1998. Multi-class
support vector machines. Technical Report CSD-
TR-98-04, Department of Computer Science, Royal
Holloway, University of London.
David Yarowsky. 1995. Unsupervised word sense dis-
ambiguation rivaling supervised methods. In ACL.
Omar Zaidan, Jason Eisner, and Christine Piatko.
2007. Using ?annotator rationales? to improve ma-
chine learning for text categorization. In NAACL-
HLT.
181
Proceedings of the 2012 Workshop on Language in Social Media (LSM 2012), pages 65?74,
Montre?al, Canada, June 7, 2012. c?2012 Association for Computational Linguistics
Language Identification for Creating Language-Specific Twitter Collections
Shane Bergsma? Paul McNamee?,? Mossaab Bagdouri? Clayton Fink? Theresa Wilson?
?Human Language Technology Center of Excellence, Johns Hopkins University
?Johns Hopkins University Applied Physics Laboratory, Laurel, MD
?Department of Computer Science, University of Maryland, College Park, MD
sbergsma@jhu.edu, mcnamee@jhu.edu, mossaab@umd.edu, clayton.fink@jhuapl.edu, taw@jhu.edu
Abstract
Social media services such as Twitter offer an
immense volume of real-world linguistic data.
We explore the use of Twitter to obtain authen-
tic user-generated text in low-resource lan-
guages such as Nepali, Urdu, and Ukrainian.
Automatic language identification (LID) can
be used to extract language-specific data from
Twitter, but it is unclear how well LID per-
forms on short, informal texts in low-resource
languages. We address this question by an-
notating and releasing a large collection of
tweets in nine languages, focusing on confus-
able languages using the Cyrillic, Arabic, and
Devanagari scripts. This is the first publicly-
available collection of LID-annotated tweets
in non-Latin scripts, and should become a
standard evaluation set for LID systems. We
also advance the state-of-the-art by evaluat-
ing new, highly-accurate LID systems, trained
both on our new corpus and on standard ma-
terials only. Both types of systems achieve
a huge performance improvement over the
existing state-of-the-art, correctly classifying
around 98% of our gold standard tweets. We
provide a detailed analysis showing how the
accuracy of our systems vary along certain di-
mensions, such as the tweet-length and the
amount of in- and out-of-domain training data.
1 Introduction
Twitter is an online social-networking service that
lets users send and receive short texts called tweets.
Twitter is enormously popular; more than 50 mil-
lion users log in daily and billions of tweets are sent
each month.1 Tweets are publicly-available by de-
1http://mashable.com/2011/09/08/
Twitter-has-100-million-active-users/
fault and thus provide an enormous and growing free
resource of authentic, unedited text by ordinary peo-
ple. Researchers have used Twitter to study how hu-
man language varies by time zone (Kiciman, 2010),
census area (Eisenstein et al, 2011), gender (Burger
et al, 2011), and ethnicity (Fink et al, 2012). Twit-
ter also provides a wealth of user dialog, and a vari-
ety of dialog acts have been observed (Ritter et al,
2010) and predicted (Ritter et al, 2011).
Of course, working with Twitter is not all roses
and rainbows. Twitter is a difficult domain because
unlike, for example, news articles, tweets are short
(limited to 140 characters), vary widely in style,
and contain many spelling and grammatical errors.
Moreover, unlike articles written by a particular
news organization, a corpus constructed from Twit-
ter will contain tweets in many different languages.
This latter point is particularly troubling because
the majority of language-processing technology is
predicated on knowing which language is being pro-
cessed. We are pursuing a long-term effort to build
social media collections in a variety of low-resource
languages, and we need robust language identifica-
tion (LID) technology. While LID is often viewed
as a solved problem (McNamee, 2005), recent re-
search has shown that LID can be made arbitrarily
difficult by choosing domains with (a) informal writ-
ing, (b) lots of languages to choose from, (c) very
short texts, and (d) unbalanced data (Hughes et al,
2006; Baldwin and Lui, 2010). Twitter exhibits all
of these properties. While the problem of LID on
Twitter has been considered previously (Tromp and
Pechenizkiy, 2011; Carter et al, 2013), these studies
have only targeted five or six western European lan-
guages, and not the diversity of languages and writ-
ing systems that we would like to process.
65
Our main contribution is the release of a large col-
lection of tweets in nine languages using the Cyril-
lic, Arabic, and Devanagari alphabets. We test dif-
ferent methods for obtaining tweets in a given tar-
get language (?2). We then use an online crowd-
sourcing platform to have these tweets annotated by
fluent speakers of that language (?3). We generate
over 18,000 triple-consensus tweets, providing the
first publicly-available collection of LID-annotated
tweets in non-Latin scripts. The annotated cor-
pus is available online at: http://apl.jhu.edu/
?
paulmac/lid.html. We anticipate our multilin-
gual Twitter collection becoming a standard evalua-
tion set for LID systems.
We also implement two LID approaches and eval-
uate these approaches against state-of-the-art com-
petitors. ?4.1 describes a discriminative classifier
that leverages both the tweet text and the tweet meta-
data (such as the user name, location, and landing
pages for shortened URLs). ?4.2 describes an effi-
cient tool based on compression language models.
Both types of systems achieve a huge improvement
over existing state-of-the-art approaches, including
the Google Compact Language Detector (part of the
Chrome browser), and a recent LID system from
Lui and Baldwin (2011). Finally, we provide further
analysis of our systems in this unique domain, show-
ing how accuracy varies with the tweet-length and
the amount of in-domain and out-of-domain train-
ing data. In addition to the datasets, we are releasing
our compression language model tool for public use.
2 Acquiring Language-Specific Tweets
We use two strategies to collect tweets in specific
languages: (?2.1) we collect tweets by users who
follow language-specific Twitter sources, and (?2.2)
we use the Twitter API to collect tweets from users
who are likely to speak the target language.
2.1 Followers of Language-Specific Sources
Our first method is called the Sources method and
involves a three-stage process. First, Twitter sources
for the target language are manually identified.
Sources are Twitter users or feeds who: (a) tweet
in the target language, (b) have a large number of
followers, and (c) act as hubs (i.e., have a high
followers-to-following ratio). Twitter sources are
typically news or media outlets (e.g. BBC News),
celebrities, politicians, governmental organizations,
but they may just be prominent bloggers or tweeters.
Once sources are identified, we use the Twitter
API (dev.twitter.com) to query each source for
its list of followers. We then query the user data for
the followers in batches of 100 tweets. For users
whose data is public, a wealth of information is
returned, including the total number of tweets and
their most recent tweet. For users who had tweeted
above a minimum number of times. and whose
most-recent-tweet tweet was in the character set for
the target language, we obtained their most recent
100-200 tweets and added them to our collection.2
While we have used the above approach to ac-
quire data in a number of different languages, for the
purposes of our annotated corpus (?3), we select the
subsets of users who exclusively follow sources in
one of our nine target languages (Table 1). We also
filter tweets that do not contain at least one charac-
ter in the target?s corresponding writing system (we
plan to address romanized tweets in future work).
2.2 Direct Twitter-API Collection
While we are most interested in users who follow
news articles, we also tested other methods for ob-
taining language-specific tweets. First, we used the
Twitter API to collect tweets from locations where
we expected to get some number of tweets in the tar-
get language. We call this method the Twit-API col-
lection method. To geolocate our tweets, the Twit-
ter API?s geotag method allowed us to collect tweets
within a specified radius of a given set of coordi-
nates in latitude and longitude. To gather a sam-
ple of tweets in our target languages, we queried
for tweets from cities with populations of at least
200,000 where speakers of the target language are
prominent (e.g., Karachi, Pakistan for Urdu; Tehran,
Iran for Farsi; etc.). We collected tweets within a ra-
dius of 25 miles of the geocoordinates. We also used
the Search API to persistently poll for tweets from
users identified by Twitter as being in the queried
location. For Urdu, we also relied on the language-
2Tromp and Pechenizkiy (2011) also manually identified
language-specific Twitter feeds, but they use tweets from these
sources directly as gold standard data, while we target the users
who simply follow such sources. We expect our approach to
obtain more-authentic and less-edited user language.
66
identification code returned by the API for each
tweet; we filter all our geolocated Urdu tweets that
are not marked as Urdu.
We also obtained tweets through an information-
retrieval approach that has been used elsewhere for
creating minority language corpora (Ghani et al,
2001). We computed the 25 most frequent unique
words in a number of different languages (that is,
words that do not occur in the vocabularies of other
languages). Unfortunately, we found no way to en-
force that the Twitter API return only tweets con-
taining one or more of our search terms (e.g., re-
turned tweets for Urdu were often in Arabic and did
not contain our Urdu search terms). There is a lack
of documentation on what characters are supported
by the search API; it could be that the API cannot
handle certain of our terms. We thus leave further
investigation of this method for future work.
3 Annotating Tweets by Language
The general LID task is to take as input some piece
of text, and to produce as output a prediction of what
language the text is written in. Our annotation and
prediction systems operate at the level of individual
tweets. An alternative would have been to assume
that each user only tweets in a single language, and
to make predictions on an aggregation of multiple
tweets. We operate on individual tweets mainly be-
cause (A) we would like to quantify how often users
switch between languages and (B) we are also inter-
ested in domains and cases where only tweet-sized
amounts of text are available. When we do have
multiple tweets per user, we can always aggregate
the scores on individual predictions (?6 has some ex-
perimental results using prediction aggregation).
Our human annotation therefore also focuses on
validating the language of individual tweets. Tweets
verified by three independent annotators are ac-
cepted into our final gold-standard data.
3.1 Amazon Mechanical Turk
To access annotators with fluency in each language,
we crowdsourced the annotation using Amazon Me-
chanical Turk (mturk.com). AMT is an online la-
bor marketplace that allows requesters to post tasks
for completion by paid human workers. Crowd-
sourcing via AMT has been shown to provide high-
quality data for a variety of NLP tasks (Snow et al,
2008; Callison-Burch and Dredze, 2010), including
multilingual annotation efforts in translation (Zaidan
and Callison-Burch, 2011b), dialect identification
(Zaidan and Callison-Burch, 2011a), and building
bilingual lexicons (Irvine and Klementiev, 2010).
3.2 Annotation Task
From the tweets obtained in ?2, we took a random
sample in each target language, and posted these
tweets for annotation on AMT. Each tweet in the
sample was assigned to a particular AMT job; each
job comprised the annotation of 20 tweets. The job
description requested workers that are fluent in the
target language and gave an example of valid and
invalid tweets in that language. The job instructions
asked workers to mark whether each tweet was writ-
ten for speakers of the target language. If the tweet
combines multiple languages, workers were asked
to mark as the target language if ?most of the text is
in [that language] excluding URLs, hash-tags, etc.?
Jobs were presented to workers as HTML pages with
three buttons alongside each tweet for validating the
language. For example, for Nepali, a Worker can
mark that a tweet is ?Nepali?, ?Not Nepali?, or ?Not
sure.? We paid $0.05 per job and requested that each
job be completed by three workers.
3.3 Quality Control
To ensure high annotation quality, we follow our
established practices in only allowing our tasks to
be completed by workers who have previously com-
pleted at least 50 jobs on AMT, and who have had at
least 85% of their jobs approved. Our jobs also dis-
play each tweet as an image; this prevents workers
from pasting the tweet into existing online language
processing services (like Google Translate).
We also have control tweets in each job to allow
us to evaluate worker performance. A positive con-
trol is a tweet known to be in the target language;
a negative control is a tweet known to be in a dif-
ferent language. Between three to six of the twenty
tweets in each job were controls. The controls are
taken from the sources used in our Sources method
(?2.1); e.g., our Urdu controls come from sources
like BBC Urdu?s Twitter feed. To further validate
the controls, we also applied our open-domain LID
system (?4.2) and filtered any Source tweets whose
67
Language Method Purity Gold Tweets
Arabic Sources 100% 1174
Farsi Sources 100% 2512
Urdu Sources 55.4% 1076
Arabic Twit-API 99.9% 1254
Farsi Twit-API 99.7% 2366
Urdu Twit-API 61.0% 1313
Hindi Sources 97.5% 1214
Nepali Sources 97.3% 1681
Marathi Sources 91.4% 1157
Russian Sources 99.8% 2005
Bulgarian Sources 92.2% 1886
Ukrainian Sources 14.3% 631
Table 1: Statistics of the Annotated Multilingual Twitter
Corpus: 18,269 total tweets in nine languages.
predicted language was not the expected language.
Our negative controls are validated tweets in a lan-
guage that uses the same alphabet as the target (e.g.,
our negative controls for Ukrainian were taken from
our LID-validated Russian and Bulgarian sources).
We collect aggregate statistics for each Worker
over the control tweets of all their completed jobs.
We conservatively discard any annotations by work-
ers who get below 80% accuracy on either the posi-
tive or negative control tweets.
3.4 Dataset Statistics
Table 1 gives the number of triple-validated ?Gold?
tweets in each language, grouped into those using
the Arabic, Devanagari and Cyrillic writing sys-
tems. The Arabic data is further divided into tweets
acquired using the Sources and Twit-API methods.
Table 1 also gives the Purity of the acquired re-
sults; that is, the percentage of acquired tweets that
were indeed in the target language. The Purity
is calculated as the number of triple-verified gold
tweets divided by the total number of tweets where
the three annotators agreed in the annotation (thus
triply-marked either Yes, No, or Not sure).
For major languages (e.g. Arabic and Russian),
we can accurately obtain tweets in the target lan-
guage, perhaps obviating the need for LID. For the
Urdu sets, however, a large percentage of tweets are
not in Urdu, and thus neither collection method is
reliable. An LID tool is needed to validate the data.
A native Arabic speaker verified that most of our
invalid Urdu tweets were Arabic. Ukrainian is the
most glaringly impure language that we collected,
with less than 15% of our intended tweets actually
in Ukrainian. Russian is widely spoken in Ukraine
and seems to be the dominant language on Twitter,
but more analysis is required. Finally, Marathi and
Bulgarian also have significant impurities.
The complete annotation of all nine languages
cost only around $350 USD. While not insignificant,
this was a small expense relative to the total human
effort we are expending on this project. Scaling our
approach to hundreds of languages would only cost
on the order of a few thousand dollars, and we are
investigating whether such an effort could be sup-
ported by enough fluent AMT workers.
4 Language Identification Systems
We now describe the systems we implemented
and/or tested on our annotated data. All the ap-
proaches are supervised learners, trained from a col-
lection of language-annotated texts. At test time, the
systems choose an output language based on the in-
formation they have derived from the annotated data.
4.1 LogR: Discriminative LID
We first adopt a discriminative approach to LID.
Each tweet to be classified has its relevant informa-
tion encoded in a feature vector, x?. The annotated
training data can be represented as N pairs of la-
bels and feature vectors: {(y1, x?1), ..., (yN , x?N )}.
To train our model, we use (regularized) logistic re-
gression (a.k.a. maximum entropy) since it has been
shown to perform well on a range of NLP tasks
and its probabilistic outputs are useful for down-
stream processing (such as aggregating predictions
over multiple tweets). In multi-class logistic regres-
sion, the probability of each class takes the form of
exponential functions over features:
p(y = k|x?) = exp(w?k ? x?)?
j exp(w?j ? x?)
For LID, the classifier predicts the language k that
has the highest probability (this is also the class with
highest weighted combination of features, w?k ? x?).
The training procedure tunes the weights to optimize
for correct predictions on training data, subject to a
tunable L2-regularization penalty on the weight vec-
tor norm. For our experiments, we train and test our
logistic regression classifier (LogR) using the effi-
cient LIBLINEAR package (Fan et al, 2008).
68
We use two types of features in our classifier:
Character Features encode the character
N-grams in the input text; characters are the
standard information source for most LID systems
(Cavnar and Trenkle, 1994; Baldwin and Lui, 2010).
We have a unique feature for each unique N-gram in
our training data. N-grams of up-to-four characters
were optimal on development data. Each feature
value is the (smoothed) log-count of how often
the corresponding N-gram occurs in that instance.
Prior to extracting the N-grams, we preprocess each
tweet to remove URLs, hash-tags, user mentions,
punctuation and we normalize all digits to 0.
Meta features encode user-provided information
beyond the tweet text. Similar information has pre-
viously been used to improve the accuracy of LID
classifiers on European-language tweets (Carter et
al., 2013). We have features for the tokens in
the Twitter user name, the screen name, and self-
reported user location. We also have features for
prefixes of these tokens, and flags for whether the
name and location are in the Latin script. Our meta
features also include features for the hash-tags, user-
mentions, and URLs in the tweet. We provide fea-
tures for the protocol (e.g. http), hostname, and top-
level domain (e.g. .com) of each link in a tweet. For
shortened URLs (e.g. via bit.ly), we query the
URL server to obtain the final link destination, and
provide the URL features for this destination link.
4.2 PPM: Compression-Based LID
Our next tool uses compression language models,
which have been proposed for a variety of NLP
tasks including authorship attribution (Pavelec et al,
2009), text classification (Teahan, 2000; Frank et al,
2000), spam filtering (Bratko et al, 2006), and LID
(Benedetto et al, 2002). Our method is based on the
prediction by partial matching (PPM) family of al-
gorithms and we use the PPM-A variant (Cleary et
al., 1984). The algorithm processes a string and de-
termines the number of bits required to encode each
character using a variable-length context. It requires
only a single parameter, the maximal order, n; we
use n = 5 for the experiments in this paper. Given
training data for a number of languages, the method
seeks to minimize cross-entropy and thus selects the
Language Wikip. All
Arabic 372 MB 1058 MB
Farsi 229 MB 798 MB
Urdu 30 MB 50 MB
Hindi 235 MB 518 MB
Nepali 31 MB 31 MB
Marathi 32 MB 66 MB
Russian 563 MB 564 MB
Bulgarian 301 MB 518 MB
Ukrainian 461 MB 463 MB
Table 2: Size of other PPM training materials.
language which would most compactly encode the
text we are attempting to classify.
We train this method both on our Twitter data and
on large collections of other material. These ma-
terials include corpora obtained from news sources,
Wikipedia, and government bodies. For our ex-
periments we divide these materials into two sets:
(1) just Wikipedia and (2) all sources, including
Wikipedia. Table 2 gives the sizes of these sets.
4.3 Comparison Systems
We compare our two new systems with the best-
available commercial and academic software.
TextCat: TextCat3 is a widely-used stand-alone
LID program. Is is an implementation of the
N-gram-based algorithm of Cavnar and Trenkle
(1994), and supports identification in ?about 69 lan-
guages? in its downloadable form. Unfortunately,
the available models do not support all of our target
languages, nor are they compatible with the standard
UTF-8 Unicode character encoding. We therefore
modified the code to process UTF-8 characters and
re-trained the system on our Twitter data (?5).
Google CLD: Google?s Chrome browser includes
a tool for language-detection (the Google Compact
Language Detector), and this tool is included as a li-
brary within Chrome?s open-source code. Mike Mc-
Candless ported this library to its own open source
project.4 The CLD tool makes predictions using text
4-grams. It is designed for detecting the language
of web pages, and can take meta-data hints from the
domain of the webpage and/or the declared webpage
3http://odur.let.rug.nl/vannoord/TextCat/
4http://code.google.com/p/
chromium-compact-language-detector/
69
Dataset Train Development Test
Arabic 2254 1171 1191
Devanagari 2099 991 962
Cyrillic 2243 1133 1146
Table 3: Number of tweets used in experiments, by writ-
ing system/classification task
encoding, but it also works on stand-alone text.5 We
use it in its original, unmodified form. While there
are few details in the source code itself, the train-
ing data for this approach was apparently obtained
through Google?s internal data collections.
Lui and Baldwin ?11: Lui and Baldwin (2011) re-
cently released a stand-alone LID tool, which they
call langid.py.6 They compared this system to
state-of-the-art LID methods and found it ?to be
faster whilst maintaining competitive accuracy.? We
use this system with its provided models only, as
the software readme notes ?training a model for
langid.py is a non-trivial process, due to the large
amount of computations required.? The sources of
the provided models are described in Lui and Bald-
win (2011). Although many languages are sup-
ported, we restrict the system to only choose be-
tween our data?s target languages (?5).
5 Experiments
The nine languages in our annotated data use one of
three different writing systems: Arabic, Devanagari,
or Cyrillic. We therefore define three classification
tasks, each choosing between three languages that
have the same writing system. We divide our an-
notated corpus into training, development and test
data for these experiments (Table 3). For the Ara-
bic data, we merge the tweets obtained via our two
collection methods (?2); for Devanagari/Cyrillic, all
tweets are obtained using the Sources method. We
ensure that tweets by a unique Twitter user occur
in at most only one of the sets. The proportion of
each language in each set is roughly the same as the
proportions of gold tweets in Table 1. All of our
Twitter-trained systems learn their models from this
training data, while all hyperparameter tuning (such
5Google once offered an online language-detection API, but
this service is now deprecated; moreover, it was rate-limited and
not licensed for research use (Lui and Baldwin, 2011).
6https://github.com/saffsd/langid.py
System Arab. Devan. Cyrill.
Trained on Twitter Corpus:
LogR: meta 79.8 74.7 82.0
LogR: chars 97.1 96.2 96.1
LogR: chars+meta 97.4 96.9 98.3
PPM 97.1 95.3 95.8
TextCat 96.3 89.1 90.3
Open-Domain: Trained on Other Materials:
Google CLD 90.5 N/A 91.4
Lui and Baldwin ?11 91.4 78.4 88.8
PPM (Wikip.) 97.6 95.8 95.7
PPM (All) 97.6 97.1 95.8
Trained on both Twitter and Other Materials:
PPM (Wikip.+Twit) 97.9 97.0 95.9
PPM (All+Twit) 97.6 97.9 96.0
Table 4: LID accuracy (%) of different systems on held-
out tweets. High LID accuracy on tweets is obtainable,
whether training in or out-of-domain.
as tuning the regularization parameter of the LogR
classifier) is done on the development set. Our eval-
uation metric is Accuracy: what proportion of tweets
in each held-out test set are predicted correctly.
6 Results
For systems trained on the Twitter data, both our
LogR and PPM system strongly outperform TextCat,
showing the effectiveness of our implemented ap-
proaches (Table 4). Meta features improve LogR
on each task. For systems trained on external data,
PPM strongly outperforms other systems, making
fewer than half the errors on each task. We also
trained PPM on both the relatively small number of
Twitter training samples and the much larger number
of other materials. The combined system is as good
or better than the separate models on each task.
We get more insight into our systems by seeing
how they perform as we vary the amount of train-
ing data. Figure 1 shows that with only a few hun-
dred annotated tweets, the LogR system gets over
90% accuracy, while performance seems to plateau
shortly afterwards. A similar story holds as we
vary the amount of out-of-domain training data for
the PPM system; performance improves fairly lin-
early as exponentially more training data is used, but
eventually begins to level off. Not only is PPM an
effective system, it can leverage a lot of training ma-
70
 50
 60
 70
 80
 90
 100
 10  100  1000
Ac
cu
ra
cy
 (%
)
Number of training tweets
Arabic
Devanagari
Cyrillic
Figure 1: The more training data the better, but accuracy
levels off: learning curve for LogR-chars (note log-scale).
 50
 60
 70
 80
 90
 100
 100  1000  10000 100000 1e+06  1e+07
Ac
cu
ra
cy
 (%
)
Number of characters of training data
Arabic
Devanagari
Cyrillic
Figure 2: Accuracy of PPM classifier using varying
amounts of Wikipedia training text (also on log-scale).
terials in order to obtain its high accuracy.
In Figure 3, we show how the accuracy of our sys-
tems varies over tweets grouped into bins by their
length. Performance on short tweets is much worse
than those closer to 140 characters in length.
We also examined aggregating predictions over
multiple tweets by the same user. We extracted all
users with ?4 tweets in the Devanagari test set (87
users in total). We then averaged the predictions of
the LogR system on random subsets of a user?s test
tweets, making a single decision for all tweets in a
subset. We report the mean accuracy of running this
approach 100 times with random subsets of 1, 2, 3,
and all 4 tweets used in the prediction. Even with
only 2 tweets per user, aggregating predictions can
reduce relative error by almost 60% (Table 5).
Encouraged by the accuracy of our systems on an-
notated data, we used our PPM system to analyze
a large number of un-annotated tweets. We trained
PPM models for 128 languages using data that in-
cludes Wikipedia (February 2012), news (e.g., BBC
News, Voice of America), and standard corpora such
 88
 90
 92
 94
 96
 98
 100
 40  60  80  100  120  140
Ac
cu
ra
cy
 (%
)
Avgerage length of tweet (binned)
Arabic
Devanagari
Cyrillic
Figure 3: The longer the tweet, the better: mean accuracy
of LogR by average length of tweet, with tweets grouped
into five bins by length in characters.
Number of Tweets 1 2 3 4
Accuracy 97.0 98.7 98.8 98.9
Table 5: The benefits of aggregating predictions by user:
Mean accuracy of LogR-chars as you make predictions
on multiple Devanagari tweets at a time
as Europarl, JRC-Acquis, and various LDC releases.
We then made predictions in the TREC Tweets2011
Corpus.7
We observed 65 languages in roughly 10 million
tweets. We calculated two other proportions using
auxiliary data:8 (1) the proportion of Wikipedia arti-
cles written in each language, and (2) the proportion
of speakers that speak each language. We use these
proportions to measure a language?s relative repre-
sentation on Twitter: we divide the tweet-proportion
by the Wikipedia and speaker proportions. Table 6
shows some of the most over-represented Twitter
languages compared to Wikipedia. E.g., Indonesian
is predicted to be 9.9 times more relatively com-
mon on Twitter than Wikipedia. Note these are pre-
dictions only; some English tweets may be falsely
marked as other languages due to English impurities
in our training sources. Nevertheless, the good rep-
resentation of languages with otherwise scarce elec-
tronic resources shows the potential of using Twitter
to build language-specific social media collections.
7http://trec.nist.gov/data/tweets/ This corpus, de-
veloped for the TREC Microblog track (Soboroff et al, 2012), contains
a two-week Twitter sample from early 2011. We processed all tweets
that were obtained with a ?200? response code using the twitter-corpus-
tools package.
8From http://meta.wikimedia.org/wiki/List_of_
Wikipedias_by_speakers_per_article
71
Language Num. % of Tweets/ Tweets/
Tweets Tot. Wikip. Speakers
Indonesian 1055 9.0 9.9 3.1
Thai 238 2.0 5.7 1.9
Japanese 2295 19.6 5.0 8.8
Korean 446 3.8 4.0 3.2
Swahili 46 0.4 3.4 0.4
Portuguese 1331 11.4 3.2 2.8
Marathi 58 0.5 2.9 0.4
Malayalam 30 0.3 2.2 0.4
Nepali 23 0.2 2.1 0.8
Macedonian 61 0.5 1.9 13.9
Bengali 25 0.2 1.9 0.1
Turkish 174 1.5 1.7 1.1
Arabic 162 1.4 1.6 0.3
Chinese 346 3.0 1.4 0.2
Spanish 696 5.9 1.4 0.7
Telugu 39 0.3 1.4 0.3
Croatian 79 0.7 1.3 6.1
English 2616 22.3 1.2 2.1
Table 6: Number of tweets (1000s) and % of total for lan-
guages that appear to be over-represented on Twitter (vs.
proportion of Wikipedia and proportion of all speakers).
7 Related Work
Researchers have tackled language identification us-
ing statistical approaches since the early 1990s.
Cavnar and Trenkle (1994) framed LID as a text
categorization problem and made their influential
TextCat tool publicly-available. The related problem
of identifying the language used in speech signals
has also been well-studied; for speaker LID, both
phonetic and sequential information may be help-
ful (Berkling et al, 1994; Zissman, 1996). Insights
from LID have also been applied to related problems
such as dialect determination (Zaidan and Callison-
Burch, 2011a) and identifying the native language
of non-native speakers (Koppel et al, 2005).
Recently, LID has received renewed interest as a
mechanism to help extract language-specific corpora
from the growing body of linguistic materials on the
web (Xia et al, 2009; Baldwin and Lui, 2010). Work
along these lines has found LID to be far from a
solved problem (Hughes et al, 2006; Baldwin and
Lui, 2010; Lui and Baldwin, 2011); the web in gen-
eral has exactly the uneven mix of style, languages,
and lengths-of-text that make the real problem quite
difficult. New application areas have also arisen,
each with their own unique challenges, such as LID
for search engine queries (Gottron and Lipka, 2010),
or person names (Bhargava and Kondrak, 2010).
The multilinguality of Twitter has led to the de-
velopment of ways to ensure language purity. Rit-
ter et al (2010) use ?a simple function-word-driven
filter. . . to remove non-English [Twitter] conversa-
tions,? but it?s unclear how much non-English sur-
vives the filtering and how much English is lost.
Tromp and Pechenizkiy (2011) and Carter et al
(2013) perform Twitter LID, but only targeting six
common European languages. We focus on low-
resource languages, where training data is scarce.
Our data and systems could enable better LID
for services like indigenoustweets.com, which
aims to ?strengthen minority languages through so-
cial media.?
8 Conclusions
Language identification is a key technology for ex-
tracting authentic, language-specific user-generated
text from social media. We addressed a previously
unexplored issue: LID performance on Twitter text
in low-resource languages. We have created and
made available a large corpus of human-annotated
tweets in nine languages and three non-Latin writ-
ing systems, and presented two systems that can pre-
dict tweet language with very high accuracy.9 While
challenging, LID on Twitter is perhaps not as diffi-
cult as first thought (Carter et al, 2013), although
performance depends on the amount of training data,
the length of the tweet, and whether we aggregate
information across multiple tweets by the same user.
Our next step will be to develop a similar approach
to handle romanized text. We also plan to develop
tools for identifying code-switching (switching lan-
guages) within a tweet.
Acknowledgments
We thank Chris Callison-Burch for his help with the
crowdsourcing. The first author was supported by the
Natural Sciences and Engineering Research Council of
Canada. The third author was supported by the BOLT
program of the Defense Advanced Research Projects
Agency, Contract No. HR0011-12-C-0015
9The annotated corpus and PPM system are available online
at: http://apl.jhu.edu/
?
paulmac/lid.html
72
References
Timothy Baldwin and Marco Lui. 2010. Language iden-
tification: The long and the short of the matter. In
Proc. HLT-NAACL, pages 229?237.
Dario Benedetto, Emanuele Caglioti, and Vittorio Loreto.
2002. Language trees and zipping. Physical Review
Letters, 88(4):2?5.
Kay Berkling, Takayuki Arai, and Etienne Barnard.
1994. Analysis of phoneme-based features for lan-
guage identification. In Proc. ICASSP, pages 289?
292.
Aditya Bhargava and Grzegorz Kondrak. 2010. Lan-
guage identification of names with SVMs. In Proc.
HLT-NAACL, pages 693?696.
Andrej Bratko, Gordon V. Cormack, Bogdan Filipic,
Thomas R. Lynam, and Blaz Zupan. 2006. Spam
filtering using statistical data compression models.
JMLR, 6:2673?2698.
John D. Burger, John Henderson, George Kim, and Guido
Zarrella. 2011. Discriminating gender on Twitter. In
Proc. EMNLP, pages 1301?1309.
Chris Callison-Burch and Mark Dredze. 2010. Creating
speech and language data with amazon?s mechanical
turk. In Proc. NAACL HLT 2010 Workshop on Cre-
ating Speech and Language Data with Amazon?s Me-
chanical Turk, pages 1?12.
Simon Carter, Wouter Weerkamp, and Manos Tsagkias.
2013. Microblog Language Identification: Overcom-
ing the Limitations of Short, Unedited and Idiomatic
Text. Language Resources and Evaluation Journal.
(forthcoming).
William B. Cavnar and John M. Trenkle. 1994. N-gram-
based text categorization. In Proc. Symposium on
Document Analysis and Information Retrieval, pages
161?175.
John G. Cleary, Ian, and Ian H. Witten. 1984. Data
compression using adaptive coding and partial string
matching. IEEE Transactions on Communications,
32:396?402.
Jacob Eisenstein, Noah A. Smith, and Eric P. Xing.
2011. Discovering sociolinguistic associations with
structured sparsity. In Proc. ACL, pages 1365?1374.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A li-
brary for large linear classification. JMLR, 9:1871?
1874.
Clayton Fink, Jonathon Kopecky, Nathan Bos, and Max
Thomas. 2012. Mapping the Twitterverse in the devel-
oping world: An analysis of social media use in Nige-
ria. In Proc. International Conference on Social Com-
puting, Behavioral Modeling, and Prediction, pages
164?171.
Eibe Frank, Chang Chui, and Ian H. Witten. 2000. Text
categorization using compression models. In Proc.
DCC-00, IEEE Data Compression Conference, Snow-
bird, US, pages 200?209. IEEE Computer Society
Press.
Rayid Ghani, Rosie Jones, and Dunja Mladenic. 2001.
Automatic web search query generation to create mi-
nority language corpora. In Proceedings of the 24th
annual international ACM SIGIR conference on Re-
search and development in information retrieval, SI-
GIR ?01, pages 432?433, New York, NY, USA. ACM.
Thomas Gottron and Nedim Lipka. 2010. A comparison
of language identification approaches on short, query-
style texts. In Proc. ECIR, pages 611?614.
Baden Hughes, Timothy Baldwin, Steven Bird, Jeremy
Nicholson, and Andrew Mackinlay. 2006. Reconsid-
ering language identification for written language re-
sources. In Proc. LREC, pages 485?488.
Ann Irvine and Alexandre Klementiev. 2010. Using Me-
chanical Turk to annotate lexicons for less commonly
used languages. In Proc. NAACL HLT 2010 Workshop
on Creating Speech and Language Data with Ama-
zon?s Mechanical Turk, pages 108?113.
Emre Kiciman. 2010. Language differences and meta-
data features on Twitter. In Proc. SIGIR 2010 Web
N-gram Workshop, pages 47?51.
Moshe Koppel, Jonathan Schler, and Kfir Zigdon. 2005.
Determining an author?s native language by mining a
text for errors. In Proc. KDD, pages 624?628.
Marco Lui and Timothy Baldwin. 2011. Cross-domain
feature selection for language identification. In Proc.
IJCNLP, pages 553?561.
Paul McNamee. 2005. Language identification: a solved
problem suitable for undergraduate instruction. J.
Comput. Sci. Coll., 20(3):94?101.
D. Pavelec, L. S. Oliveira, E. Justino, F. D. Nobre Neto,
and L. V. Batista. 2009. Compression and stylometry
for author identification. In Proc. IJCNN, pages 669?
674, Piscataway, NJ, USA. IEEE Press.
Alan Ritter, Colin Cherry, and Bill Dolan. 2010. Unsu-
pervised modeling of twitter conversations. In Proc.
HLT-NAACL, pages 172?180.
Alan Ritter, Colin Cherry, and William B. Dolan. 2011.
Data-driven response generation in social media. In
Proc. EMNLP, pages 583?593.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-
drew Y. Ng. 2008. Cheap and fast?but is it good?:
evaluating non-expert annotations for natural language
tasks. In Proc. EMNLP, pages 254?263.
Ian Soboroff, Dean McCullough, Jimmy Lin, Craig Mac-
donald, Iadh Ounis, and Richard McCreadie. 2012.
Evaluating real-time search over tweets. In Proc.
ICWSM.
73
William John Teahan. 2000. Text classification and
segmentation using minimum cross-entropy. In Proc.
RIAO, pages 943?961.
Erik Tromp and Mykola Pechenizkiy. 2011. Graph-
based n-gram language identication on short texts. In
Proc. 20th Machine Learning conference of Belgium
and The Netherlands, pages 27?34.
Fei Xia, William Lewis, and Hoifung Poon. 2009. Lan-
guage ID in the context of harvesting language data off
the web. In Proc. EACL, pages 870?878.
Omar F. Zaidan and Chris Callison-Burch. 2011a.
The arabic online commentary dataset: an annotated
dataset of informal arabic with high dialectal content.
In Proc. ACL, pages 37?41.
Omar F. Zaidan and Chris Callison-Burch. 2011b.
Crowdsourcing translation: Professional quality from
non-professionals. In Proc. ACL, pages 1220?1229.
Marc A. Zissman. 1996. Comparison of four ap-
proaches to automatic language identification of tele-
phone speech. IEEE Transactions on Speech and Au-
dio Processing, 4(1):31?44.
74
