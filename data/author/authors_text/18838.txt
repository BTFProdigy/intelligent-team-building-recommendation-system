Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 193?202, Dublin, Ireland, August 23-29 2014.
A Study of using Syntactic and Semantic Structures
for Concept Segmentation and Labeling
Iman Saleh
?
, Shafiq Joty, Llu??s M
`
arquez,
Alessandro Moschitti, Preslav Nakov
ALT Research Group
Qatar Computing Research Institute
{sjoty,lmarquez,amoschitti,pnakov}
@qf.org.qa
Scott Cyphers, Jim Glass
MIT CSAIL
Cambridge, Massachusetts 02139
USA
{cyphers,glass}@mit.edu
Abstract
This paper presents an empirical study on using syntactic and semantic information for Concept
Segmentation and Labeling (CSL), a well-known component in spoken language understand-
ing. Our approach is based on reranking N -best outputs from a state-of-the-art CSL parser. We
perform extensive experimentation by comparing different tree-based kernels with a variety of
representations of the available linguistic information, including semantic concepts, words, POS
tags, shallow and full syntax, and discourse trees. The results show that the structured representa-
tion with the semantic concepts yields significant improvement over the base CSL parser, much
larger compared to learning with an explicit feature vector representation. We also show that
shallow syntax helps improve the results and that discourse relations can be partially beneficial.
1 Introduction
Spoken Language Understanding aims to interpret user utterances and to convert them to logical forms,
or, equivalently, database queries, which can then be used to satisfy the user?s information needs. This
process is known as Concept Segmentation and Labeling (CSL): it maps utterances into meaning repre-
sentations based on semantic constituents. The latter are basically sequences of semantic entities, often
referred to as concepts, attributes or semantic tags. Traditionally, grammar-based methods have been
used for CSL, but more recently machine learning approaches to semantic structure computation have
been shown to yield higher accuracy. However, most previous work did not exploit syntactic/semantic
structures of the utterances, and the state-of-the-art is represented by conditional models for sequence la-
beling, such as Conditional Random Fields (Lafferty et al., 2001) trained with simple morphological and
lexical features. In our study, we measure the impact of syntactic and discourse structures by also com-
bining them with innovative features. In the following subsections, we present the application context
for our CSL task and then we outline the challenges and the findings of our research.
1.1 Semantic parsing for the ?restaurant? domain
We experiment with the dataset of McGraw et al. (2012), containing spoken and typed questions about
restaurants, which are to be answered using a database of free text such as reviews, categorical data such
as names and locations, and semi-categorical data such as user-reported cuisines and amenities.
Semantic parsing, in the form of sequential segmentation and labeling, makes it easy to convert spoken
and typed questions such as ?cheap lebanese restaurants in doha with take out? into database queries.
First, a language-specific semantic parser tokenizes, segments and labels the question:
[
Price
cheap] [
Cuisine
lebanese] [
Other
restaurants in] [
City
doha] [
Other
with] [
Amenity
take out]
Then, label-specific normalizers are applied to the segments, with the option to possibly relabel mis-
labeled segments; at this point, discourse history may be incorporated as well.
[
Price
low] [
Cuisine
lebanese] [
City
doha] [
Amenity
carry out]
?
Iman Saleh (iman.saleh@fci-cu.edu.eg) is affiliated to Faculty of Computers and Information, Cairo University.
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings
footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
193
Finally, a database query is formed from the list of labels and values, and is then executed against the
database, e.g., MongoDB; a backoff mechanism may be used if the query does not succeed.
{$and [{cuisine:"lebanese"}, {city:"doha"}, {price:"low"}, {amenity:"carry out"}]}
1.2 Related work on CSL
Pieraccini et al. (1991) used Hidden Markov Models (HMMs) for CSL, where the observations were
word sequences and the hidden states were meaning units, i.e, concepts. In subsequent work (Rubinstein
and Hastie, 1997; Santaf?e et al., 2007; Raymond and Riccardi, 2007; De Mori et al., 2008), other genera-
tive models were applied, which model the joint probability of a word sequence and a concept sequence,
as well as discriminative models, which directly model a conditional probability over the concepts in the
input text.
Seneff (1989) and Miller et al. (1994) used stochastic grammars for CSL. In particular, they applied
stochastic Finite State Transducers (FST) for recognizing constituent annotations. FSTs describe local
syntactic structures with a sequence of words, e.g., noun phrases or even constituents. Papineni et al.
(1998) proposed and evaluated exponential models, but, nowadays, Conditional Random Fields (Lafferty
et al., 2001) are considered to be the state-of-the-art. More recently, Wang et al. (2009) illustrated an
approach for CSL that is specific to query understanding for web applications. A general survey of CSL
approaches can be found in (De Mori et al., 2008). CSL is also connected to a large body of work on
shallow semantic parsing; see (Gildea and Jurafsky, 2002; M`arquez et al., 2008) for an overview.
Another relevant line of research with a considerable body of work is reranking in NLP. Tree kernels
for reranking syntactic parse trees were first proposed in (Collins and Duffy, 2002). Some variants used
explicit spaces (Kudo et al., 2005), and feature vector approaches were proposed in (Koo and Collins,
2005). Other reranking work using tree kernels regards predicate argument structures (Moschitti et al.,
2006) and named entities (Nguyen and Moschitti, 2012). In (Dinarelli et al., 2011), we rerank CSL
hypotheses using structures built on top of concepts, words and features that are simpler than those
studied in this paper. The work of Ge and Mooney (2006) and Kate and Mooney (2006) is also similar
to ours, as it models the extraction of semantics as a reranking task using string kernels.
1.3 Syntactic and semantic structures for CSL
The related work has highlighted that automatic CSL is mostly based on powerful machine learning al-
gorithms and simple feature representations based on word and tag n-grams. In this paper, we study the
impact of more advanced linguistic processing on CSL, such as shallow and full syntactic parsing and
discourse structure. We use a reranking approach to select the best hypothesis annotated with concepts
derived by a local model, where the hypotheses are represented as trees enriched with semantic con-
cepts similarly to (Dinarelli et al., 2011). These tree-based structures can capture dependencies between
sentence constituents and concepts. However, extracting features from them is rather difficult as their
number is exponentially large. Thus, we rely on structural kernels (e.g., see (Moschitti, 2006)) for au-
tomatically encoding tree fragments, which represent syntactic and semantic dependencies from words
and concepts, and we train the reranking functions with Support Vector Machines (e.g., see (Joachims,
1999)). Additionally, we experiment with several types of kernels and newly designed feature vectors.
We test our models on the above-mentioned Restaurant domain. The results show that (i) the basic
CRF model, in fact semi-CRF (see below), is very accurate, achieving more than 83% in F
1
-score, which
indicates that improving over the semi-CRF approach is very hard; (ii) the upper-bound performance
of the reranking approach is very high as well, i.e., the correct annotation is generated in the first 100
hypotheses in 98.72% of the cases; (iii) our feature vectors show improvement only when all feature
groups are used together; otherwise, we only observe marginal improvement; (iv) structural kernels yield
a 10% relative error reduction from the semi-CRF baseline, which is more than double the feature vector
result; (v) syntactic information significantly improves on the best model, but only when using shallow
syntax; and finally, (vi) although, discourse structures provide good improvement over the semi-CRF
model, they perform lower than shallow syntax (thus, a valuable use of discourse features is still an open
problem that we plan to pursue in future work).
194
2 CSL reranking
Reranking is based on a list of N annotation hypotheses, which are generated and sorted by probability
using local classifiers. Then a reranker, typically a meta-classifier, tries to select the best hypothesis from
the list. The reranker can exploit global information, and, specifically, the dependencies between the
different concepts that are made available by the local model. We use semi-CRF as our local model since
it yields the highest accuracy in CSL (when using a single model), and preference reranking with kernel
machines to rerank the N hypotheses generated by the semi-CRF.
2.1 Basic parser using semi-CRF
We use a semi-Markov CRF (Sarawagi and Cohen, 2004), or semi-CRF, a variation of a linear-chain
CRF (Lafferty et al., 2001), to produce the N -best list of labeled segment hypotheses that serve as the
input to reranking. In a linear-chain CRF, with a sequence of tokens x and labels y, we approximate
p(y|x) as a product of factors of the form p(y
i
|y
i?1
, x), which corresponds to features of the form
f
j
(y
i?1
, y
i
, i, x), where i iterates over the token/label positions. This supports a Viterbi search for the
approximateN best values of y. WithM label values, if for each label y
m
we know the bestN sequences
of labels y
1
, y
2
, . . . , y
i?1
= y
m
, then we can use p(y
i
|y
i?1
, x) to get the probability for extending each
path by each possible label y
i
= y
?
m
. Then for each label y
?
m
, we will have MN paths and scores, one
from each of the paths of length i? 1 ending with y
m
. For each y
?
m
, we pick the N best extended paths.
With semi-CRF, we want a labeled segmentation s rather than a sequence of labels. Each segment
s
i
= (y
i
, t
i
, u
i
) has a label y
i
as well as a starting and ending token position for the segment, t
i
and
u
i
respectively, where u
i
+ 1 = t
i+1
. We approximate p(s|x), with factors of the form p(s
i
|s
i?1
, x),
which we simplify to p(y
i
, u
i
|y
i?1
, t
i
), so features take the form f
j
(y
i?1
, y
i
, t
i
, u
i
), i.e., they can use the
previous segment?s label and the current segment?s label and endpoints. The Viterbi search is extended
to search for a pair of label and segment end. Whereas for M labels we kept track of MN paths, we
must keep track of MLN paths, where L is the maximum segment length.
We use token n-gram features relative to the segment boundaries, n-grams within the segment, token
regular expression and lexicon features within a segment. Each of these features also includes the labels
of the previous and current segment, and the segment length.
2.2 Preference reranking with kernel machines
Preference reranking (PR) uses a classifier C of pairs of hypotheses ?H
i
, H
j
?, which decides if H
i
is
better thanH
j
. Given each training question Q, positive and negative examples are generated for training
the classifier. We adopt the following approach for example generation: the pairs ?H
1
, H
i
? constitute
positive examples, where H
1
has the lowest error rate with respect to the gold standard among the
hypotheses for Q, and vice versa, ?H
i
, H
1
? are considered as negative examples. At testing time, given
a new question Q
?
, C classifies all pairs ?H
i
, H
j
? generated from the annotation hypotheses of Q
?
: a
positive classification is a vote for H
i
, otherwise the vote is for H
j
. Also, the classifier score can be used
as a weighted vote. H
k
are then ranked according to the number (sum) of the (weighted) votes they get.
We build our reranker with kernel machines. The latter, e.g., SVMs, classify an input object o using
the following function: C(o) =
?
i
?
i
y
i
K(o, o
i
), where ?
i
are model parameters estimated from the
training data, o
i
are support objects and y
i
are the labels of the support objects. K(?, ?) is a kernel
function, which computes the scalar product between the two objects in an implicit vector space. In the
case of the reranker, the objects o are ?H
i
, H
j
?, and the kernel is defined as follow:
K(?H
1
, H
2
?, ?H
?
1
, H
?
2
?) = S(H
1
, H
?
1
) + S(H
2
, H
?
2
)? S(H
1
, H
?
2
)? S(H
2
, H
?
1
).
Our reranker also includes traditional feature vectors in addition to the trees. Therefore, we define each
hypothesis H as a tuple ?T,~v? composed of a tree T and a feature vector ~v. We then define a structural
kernel (similarity) between two hypotheses H and H
?
as follows: S(H,H
?
) = S
TK
(T, T
?
) + S
v
(~v,~v
?
),
where S
TK
is one of the tree kernel functions defined in Section 3.1, and S
v
is a kernel over feature
vectors (see Section 3.3), e.g., linear, polynomial, gaussian, etc.
195
(a) Basic Tree (BT). (b) Discourse Tree (DT).
(c) Shallow Syntactic Tree (ShT).
(d) Syntactic Tree (ST).
(e) BT with POS (BTP).
Figure 1: Syntactic/semantic trees. The numeric semantic tagset is defined in Table 7.
3 Structural kernels for semantic parsing
In this section, we briefly describe the kernels we use in S(H,H
?
) for preference reranking. We engineer
them by combining three aspects: (i) different types of existing tree kernels, (ii) new syntactic/semantic
structures for representing CSL, and (iii) new feature vectors.
3.1 Tree kernels
Structural kernels, e.g., tree and sequence kernels, measure the similarity between two structures in terms
of their shared substructures. One interesting aspect is that these kernels correspond to a scalar product
in the fragment space, where each substructure is a feature. Therefore, they can be used in the training
and testing algorithms of kernel machines (see Section 2.2). Below, we briefly describe different types of
kernels we tested in our study, which are made available in the SVM-Light-TK toolkit (Moschitti, 2006).
Subtree Kernel (K0) is one of the simplest tree kernels, as it only generates complete subtrees, i.e., tree
fragments that, given any arbitrary starting node, necessarily include all its descendants.
Syntactic Tree Kernel (K1), also known as a subset tree kernel (Collins and Duffy, 2002), maps ob-
jects in the space of all possible tree fragments constrained by the rule that the sibling nodes cannot
be separated from their parents. In other words, substructures are composed of atomic building blocks
corresponding to nodes, along with all of their direct children. In the case of a syntactic parse tree, these
are complete production rules for the associated parser grammar.
Syntactic Tree Kernel + BOW (K2) extends ST by allowing leaf nodes to be part of the feature space.
The leaves of the trees correspond to words, i.e., we allow bag-of-words (BOW).
Partial Tree Kernel (K3) can be effectively applied to both constituency and dependency parse trees.
It generates all possible connected tree fragments, e.g., sibling nodes can be also separated and be part
of different tree fragments. In other words, a fragment is any possible tree path from whose nodes other
tree paths can depart. Thus, it can generate a very rich feature space.
Sequence Kernel (K4) is the traditional string kernel applied to the words of a sentence. In our case, we
apply it to the sequence of concepts.
3.2 Semantic/syntactic structures
As mentioned before, tree kernels allow us to compute structural similarities between two trees without
explicitly representing them as feature vectors. For the CSL task, we experimented with a number of tree
representations that incorporate different levels of syntactic and semantic information.
To capture the structural dependencies between the semantic tags, we use a basic tree (Figure 1a)
where the words of a sentence are tagged with their semantic tags. More specifically, the words in the
sentence constitute the leaves of the tree, which are in turn connected to the pre-terminals containing the
semantic tags in BIO notation (?B?=begin, ?I?=inside, ?O?=outside). The BIO tags are then generalized
in the upper level, and so on. The basic tree does not include any syntactic information.
196
However, part-of-speech (POS) and phrasal information could be informative for both segmentation
and labeling in semantic parsing. To incorporate this information, we use two extensions of the basic
tree: one that includes the POS tags of the words (Figure 1e), and another one that includes both POS
tags and syntactic chunks (Figure 1c). The POS tags are children of the semantic tags, whereas the
chunks (i.e., phrasal information) are included as parents of the semantic tags.
We also experiment with full syntactic trees (Figure 1d) to see the impact of deep syntactic informa-
tion. The semantic tags are attached to the pre-terminals (i.e., POS tags) in the syntactic tree. We use the
Stanford POS tagger and syntactic parser and the Twitter NLP tool
1
for the shallow trees.
A sentence containing multiple clauses exhibits a coherence structure. For instance, in our example,
the first clause ?along my route tell me the next steak house? is elaborated by the second clause ?that is
within a mile?. The relations by which clauses in a text are linked are called coherence relations (e.g.,
Elaboration, Contrast). Discourse structures capture this coherence structure of text and provide addi-
tional semantic information that could be useful for the CSL task (Stede, 2011). To build the discourse
structure of a sentence, we use a state-of-the-art discourse parser (Joty et al., 2012) which generates
discourse trees in accordance with the Rhetorical Structure Theory of discourse (Mann and Thompson,
1988), as exemplified in Figure 1b. Notice that a text span linked by a coherence relation can be either a
nucleus (i.e., the core part) or a satellite (i.e., a supportive one) depending on how central the claim is.
3.3 New features
In order to compare to the structured representation, we also devoted significant effort towards engineer-
ing a set of features to be used in a flat feature-vector representation; they can be used in isolation or in
combination with the kernel-based approach (as a composite kernel using a linear combination):
CRF-based: these include the basic features used to train the initial semi-CRF model (cf. Section 2.1).
n-gram based: we collected 3- and 4-grams of the output label sequence at the level of concepts, with
artificial tags inserted to identify the start (?S?) and end (?E?) of the sequence.
2
Probability-based: two features computing the probability of the label sequence as an average of the
probabilities at the word level p(l
i
|w
i
) (i.e., assuming independence between words). The unigram prob-
abilities are estimated by frequency counts using maximum likelihood in two ways: (i) from the complete
100-best list of hypotheses; (ii) from the training set (according to the gold standard annotation).
DB-based: a single feature encoding the number of results returned from the database when constructing
a query using the conjunction of all semantic segments in the hypothesis. Three possible values are
considered by using a threshold t: 0 (if the query result is void), 1 (if the number of results is in [1, t]),
and 2 (if the number of results is greater than t). In our case, t is empirically set to 10,000.
4 Experiments
The experiments aim at investigating which structures, and thus which linguistic models and combination
with other models, are the most appropriate for our reranker. We first calculate the oracle accuracy in
order to compute an upper bound of the reranker. Then we present experiments with the feature vectors,
tree kernels, and representations of linguistic information introduced in the previous sections.
4.1 Experimental setup
In our experiments, we use questions annotated with semantic tags in the restaurant domain,
3
which were
collected by McGraw et al. (2012) through crowdsourcing on Amazon Mechanical Turk.
4
We split the
dataset into training, development and test sets. Table 1 shows statistics about the dataset and about the
size of the parts we used for training, development and testing (see the semi-CRF line).
We subsequently split the training data randomly into ten folds. We generated the N -best lists on
the training set in a cross-validation fashion, i.e., iteratively training on nine folds and annotating the
remaining fold. We computed the 100-best hypotheses for each example.
1
Available from http://nlp.stanford.edu/software/index.shtml and https://github.com/aritter/twitter nlp, respectively.
2
For instance, if the output sequence is Other-Rating-Other-Amenity the 3-gram patterns would be: S-Other-Rating, Other-
Rating-Other, Rating-Other-Amenity, and Other-Amenity-E.
3
http://www.sls.csail.mit.edu/downloads/restaurant
4
We could not use the datasets used by Dinarelli et al. (2011), because they use French and Italian corpora for which there
are no reliable syntactic and discourse parsers.
197
Train Devel. Test Total
semi-CRF 6,922 739 1,521 9,182
Reranker 28,482 3,695 7,605 39,782
Table 1: Number of instances and pairs used to
train the semi-CRF and rerankers, respectively.
N 1 2 5 10 100
F
1
83.03 87.76 92.63 95.23 98.72
Table 2: Oracle F
1
-score for N -best lists
of different lengths.
We used the development set to experiment and tune the hyper-parameters of the reranking model. The
results on the development set presented in Section 4.2 were obtained by semi-CRF and reranking models
learned on the training set. The results on the test set were obtained by models trained on the training
plus development sets. Similarly, the N -best lists for the development and test sets were generated using
a single semi-CRF model trained on the training set and the training+development sets, respectively.
Each generated hypothesis is represented using a semantic tree and a feature vector (explained in
Section 3) and two extra features accounting for (i) the semi-CRF probability of the hypothesis, and
(ii) the hypothesis reciprocal rank in the N -best list. SVM-Light-TK
5
is used to train the reranker with
a combination of tree kernels and feature vectors (Moschitti, 2006; Joachims, 1999). Although we
tried several parameters on the validation set, we observed that the default values yielded the highest
results. Thus, we used the default c (trade-off) and tree kernel parameters and a linear kernel for the
feature vectors. Table 1 shows the sizes of the train, the development and the test sets used for the
semi-CRF as well as the number of pairs generated for the reranker. As a baseline, we picked the best-
scored hypothesis in the list, according to the semi-CRF tagger. The evaluation measure used in all
the experiments is the harmonic mean of precision and recall, i.e., the F
1
-score (van Rijsbergen, 1979),
computed at the token level and micro-averaged over the different semantic types.
6
We used paired t-test
to measure the statistical significance of the improvements: we split the test set into 31 equally-sized
samples and performed t-tests based on the F
1
-scores of different models on the resulting samples.
4.2 Results
Oracle accuracy. Table 2 shows the oracle F
1
-score for N -best lists of different lengths, i.e., which
can be achieved by picking the best candidate of the N -best list for various values of N . We can see that
going to 5-best increases the oracle F
1
-score by almost ten points absolute. Going down to 10-best only
adds 2.5 extra F
1
points absolute, and a 100-best list adds 3.5 F
1
points more to yield a respectable F
1
-
score of 98.72. This high result can be explained considering that the size of the complete hypothesis set
is smaller than 100 for most questions. Thus, we can conclude that theN -best lists do include many good
options and do offer quite a large space for potential improvement. We can further observe that going to
5-best lists offers a good balance between the length of the list and the possibility to improve F
1
-score:
generally, we do not want too long N -best lists since they slow down computation and also introduce
more opportunities to make the wrong choice for a reranker (since there are just more candidates to
choose from). In our experiments with larger N , we observed improvements only for 10 and only on the
development set; thus, we will focus on 5-best lists in our experiments below.
K0 K1 K2 K3 K4
Dev 84.21 82.92 83.07 85.07 83.78
Test 84.08 83.19 83.20 84.61 82.93
Table 3: Results for using different tree kernels on the basic tree (BT) representation.
Choosing the best tree kernel. We first select the most appropriate tree kernel to limit the number
of experiment variables. Table 3 shows the results of different tree kernels using the basic tree (BT)
representation (see Figure 1a). We can observe that for both the development set and the test set, kernel
K3 (see Section 3.1) yields the highest F
1
-score.
Impact of feature vectors. Table 4 presents the results for the feature vector experiments in terms
of F
1
-scores and relative error reductions (row RER). The first column shows the baseline, when no
reranking is used; the following four columns contain the results when using vectors including different
5
http://disi.unitn.it/moschitti/Tree-Kernel.htm
6
?Other? is not considered a semantic type, thus ?Other? tokens are not included in the F
1
calculation.
198
Baseline n-grams CRF features Count DB ProbBased AllFeat
Dev 83.86 83.79 83.96 83.80 83.86 83.87 84.49
RER -0.4 0.6 -0.4 0.0 0.0 3.9
Test 83.03 82.90 83.44 82.90 83.01 83.09 83.86
RER -0.7 2.4 -0.7 -0.1 0.3 4.8
Table 4: Feature vector experiments: F
1
score and relative error reduction (in %).
Combining AllFeat and
Baseline BT BTP ShT ST AllFeat +BT +ShT +ShT +BT
Dev 83.86 85.07 85.41 85.06 84.30 84.49 85.57 85.58 85.33
RER 7.5 9.6 7.4 2.8 3.9 10.6 10.7 9.1
Test 83.03 84.61 84.63 84.07 83.81 83.86 84.67 84.79 84.76
RER 9.3 9.4 6.1 4.5 4.8 9.6 10.2 10.2
p.v. 0.00049 0.0002 0.012 0.032 0.00018 0.00028 0.00004 0.000023
Table 5: Tree kernel experiments: F
1
-score, relative error reduction (in %), and p-values.
kinds of features: (i) n-gram features, (ii) all features used by the semi-CRF, (iii) count features, and
(iv) database (DB) features. In each case, we include two additional features: the semi-CRF score
(i.e., the probability) and the reciprocal rank of the hypothesis in the N -best list. Among (i)?(iv), only
the semi-CRF features seem to help; the rest either show no improvements or degrade the performance.
However, putting all these features together (AllFeat) yields sizable gains in terms of F
1
-score and a
relative error reduction of 4-5%; the improvement is statistically significant, and it is slightly larger on
the test dataset compared to the development dataset.
Impact of structural kernels and combinations. Table 5 shows the results when experimenting with
various tree structures (see columns 2-5): (i) the basic tree (BT), (ii) the basic tree augmented with
part-of-speech information (BTP), (iii) shallow syntactic tree (ShT), and (iv) syntactic tree (ST). We
can see that the basic tree works rather well, yielding +1.6 F
1
-score on the test dataset, but adding POS
information can help a bit more, especially for the tuning dataset. Interestingly, the syntactic tree kernels,
ShT and ST, perform worse than BT and BTP, especially on the test dataset. The last three columns in the
table show the results when we combine the AllFeat feature vector (see Table 4) with BT and ShT. We can
see that combining AllFeat with ShT works better, on both development and test sets, than combining it
with BT or with both ShT and BT. Also note the big jump in performance from AllFeat to AllFeat+ShT.
Overall, we can conclude that shallow syntax has a lot to offer over AllFeat, and it is preferable over BT
in the combination with AllFeat. The improvements reported in Tables 5 and 6 are statistically significant
when compared to the semi-CRF baseline as shown by the p.v. (value) row. Moreover, the improvement
of AllFeat + ShT over BT is also statistically significant (p.v.<0.05).
Combining AllFeat and
Baseline DS +DS +DS +BT +DS +ShT
Dev 83.86 84.61 85.14 85.43 85.46
RER 4.7 7.9 9.7 9.9
Test 83.03 84.38 84.55 84.63 84.67
RER 7.9 8.9 9.4 9.6
p.v. 0.0005 0.0001 0.00066 0.00015
Table 6: Experiments with discourse kernels: F
1
score, relative error reduction (in %), and p-values.
Discourse structure. Finally, Table 6 shows the results for the discourse tree kernel (DS), which we
designed and experimented with for the first time in this paper. We see that DS yields sizable improve-
ments over the baseline. We also see that further gains can be achieved by combining DS with AllFeat,
and also with BT and ShT, the best combination being AllFeat+DS+ShT (see last column). However,
comparing to Table 5, we see that it is better to use just AllFeat+ShT and leave DS out. We would like
to note though that the discourse parser produced non-trivial trees for only 30% of the hypotheses (due
to the short, simple nature of the questions); in the remaining cases, it probably hurt rather than helped.
We conclude that discourse structure has clear potential, but how to make best use of it, especially in the
case of short simple questions, remains an open question that deserves further investigation.
199
Tag ID Other Rating Restaurant Amenity Cuisine Dish Hours Location Price
0 Other 8260 35 43 110 15 19 55 113 9
1 Rating 29 266 0 14 3 6 0 0 8
2 Restaurant 72 6 657 20 19 15 0 5 0
3 Amenity 117 9 10 841 27 27 7 12 7
4 Cuisine 36 2 12 26 543 44 3 1 0
5 Dish 23 0 4 20 33 324 1 4 0
6 Hours 61 0 1 2 6 1 426 9 1
7 Location 104 1 14 20 2 1 1 1457 0
8 Price 22 1 0 7 0 2 0 1 204
Table 7: Confusion matrix for the output of the best performing system.
4.3 Error analysis and discussion
Table 7 shows the confusion matrix for our best-performing model AllFeat+ShT (rows = gold standard
tags; columns = system predicted tags). Given the good results of the semantic parser, the numbers in the
diagonal are clearly dominating the weight of the matrix. The largest errors correspond to missed (first
column) and over-generated (first row) entity tokens. Among the proper confusions between semantic
types, Dish and Cuisine tend to mislead each other most. This is due to the fact that these two tags
are semantically similar, thus making them hard to distinguish. We can also notice that it is difficult to
identify Amenity correctly, and the model mistakenly tags many other tags as Amenity. We looked into
some examples to further investigate the errors. Our findings are as follow:
Inaccuracies and inconsistencies in human annotations. Since the annotations were done in Me-
chanical Turk, they have many inaccuracies and inconsistencies. For example, the word good with
exactly the same sense was tagged as both Other and Rating by the Turkers in the following examples:
Gold: [
Other
any good] [
Price
cheap] [
Cuisine
german] [
Other
restaurants] [
Location
nearby]
Model: [
Other
any] [
Rating
good] [
Price
cheap] [
Cuisine
german] [
Other
restaurants] [
Location
nearby]
Gold: [
Other
any place] [
Location
along the road] [
Other
has a] [
Rating
good] [
Dish
beer] [
Other
selection that also serves] ...
Requires lexical semantics and more coverage. In some cases our model fails to generalize well. For
instance, it fails to correctly tag establishments and tameles for the following examples. This suggests
that we need to consider other forms of semantic information, e.g., distributional and compositional
semantics computed from large corpora and/or using Web resources such as Wikipedia.
Gold: [
Other
any] [
Location
dancing establishments] [
Other
with] [
Price
reasonable] [
Other
pricing]
Model: [
Other
any] [
Amenity
dancing] [
Other
establishments] [
Other
with] [
Price
reasonable] [
Other
pricing]
Gold: [
Other
any] [
Cuisine
mexican] [
Other
places have a] [
Dish
tameles] [
Amenity
special today]
Model: [
Other
any] [
Cuisine
mexican] [
Other
places have a] [
Amenity
tameles] [
Other
special] [
Hours
today]
5 Conclusions
We have presented a study on the usage of syntactic and semantic structured information for improved
Concept Segmentation and Labeling (CSL). Our approach is based on reranking a set of N -best se-
quences generated by a state-of-the-art semi-CRF model for CSL. The syntactic and semantic informa-
tion was encoded in tree-based structures, which we used to train a reranker with kernel-based Support
Vector Machines. We empirically compared several variants of syntactic/semantic structured representa-
tions and kernels, including also a vector of manually engineered features.
The first and foremost conclusion from our study is that structural kernels yield significant improve-
ment over the strong baseline system, with a relative error reduction of ?10%. This more than doubles
the improvement when using the explicit feature vector. Second, we observed that shallow syntactic
information also improves results significantly over the best model. Unfortunately, the results obtained
using full syntax and discourse trees are not so clear. This is probably explained by the fact that user
queries are rather short and linguistically not very complex. We also observed that the upper bound per-
formance for the reranker still leaves large room for improvement. Thus, it remains to be seen whether
some alternative kernel representations can be devised to make better use of discourse and other syntac-
tic/semantic information. Also, we think that some innovative features based on analyzing the results
obtained from our database (or the Web) when querying with the segments represented in each hypothe-
ses have the potential to improve the results. All these concerns will be addressed in future work.
200
Acknowledgments
This research is developed by the Arabic Language Technologies (ALT) group at Qatar Computing Re-
search Institute (QCRI) within the Qatar Foundation in collaboration with MIT. It is part of the Interactive
sYstems for Answer Search (Iyas) project.
References
Michael Collins and Nigel Duffy. 2002. New ranking algorithms for parsing and tagging: Kernels over discrete
structures, and the voted perceptron. In Proceedings of the 40th Annual Meeting on Association for Computa-
tional Linguistics, ACL ?02, pages 263?270, Philadelphia, PA, USA.
Renato De Mori, Dilek Hakkani-T?ur, Michael McTear, Giuseppe Riccardi, and Gokhan Tur. 2008. Spoken
language understanding: a survey. IEEE Signal Processing Magazine, 25:50?58.
Marco Dinarelli, Alessandro Moschitti, and Giuseppe Riccardi. 2011. Discriminative reranking for spoken lan-
guage understanding. IEEE Transactions on Audio, Speech and Language Processing, 20(2):526?539.
Ruifang Ge and Raymond Mooney. 2006. Discriminative reranking for semantic parsing. In Proceedings of the
21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association
for Computational Linguistics, COLING-ACL?06, pages 263?270, Sydney, Australia.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic labeling of semantic roles. Computational Linguistics,
28(3):245?288.
Thorsten Joachims. 1999. Advances in kernel methods. In Bernhard Sch?olkopf, Christopher J. C. Burges, and
Alexander J. Smola, editors, Making Large-scale Support Vector Machine Learning Practical, pages 169?184,
Cambridge, MA, USA. MIT Press.
Shafiq Joty, Giuseppe Carenini, and Raymond Ng. 2012. A novel discriminative framework for sentence-level dis-
course analysis. In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing
and Computational Natural Language Learning, EMNLP-CoNLL ?12, pages 904?915, Jeju Island, Korea.
Rohit Kate and Raymond Mooney. 2006. Using string-kernels for learning semantic parsers. In Proceedings of
the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association
for Computational Linguistics, COLING-ACL ?06, pages 913?920, Sydney, Australia.
Terry Koo and Michael Collins. 2005. Hidden-variable models for discriminative reranking. In Proceedings
of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing,
HLT-EMNLP ?05, pages 507?514, Vancouver, British Columbia, Canada.
Taku Kudo, Jun Suzuki, and Hideki Isozaki. 2005. Boosting-based parse reranking with subtree features. In
Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ?05, pages 189?
196, Ann Arbor, MI, USA.
John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. In Proceedings of the 18th International Conference on Machine
Learning, ICML ?01, pages 282?289, Williamstown, MA, USA.
William Mann and Sandra Thompson. 1988. Rhetorical structure theory: Toward a functional theory of text
organization. Text, 8(3):243?281.
Llu??s M`arquez, Xavier Carreras, Kenneth Litkowski, and Suzanne Stevenson. 2008. Semantic Role Labeling: An
Introduction to the Special Issue. Computational Linguistics, 34(2):145?159.
Ian McGraw, Scott Cyphers, Panupong Pasupat, Jingjing Liu, and Jim Glass. 2012. Automating crowd-supervised
learning for spoken language systems. In Proceedings of 13th Annual Conference of the International Speech
Communication Association, INTERSPEECH ?12, Portland, OR, USA.
Scott Miller, Richard Schwartz, Robert Bobrow, and Robert Ingria. 1994. Statistical language processing using
hidden understanding models. In Proceedings of the workshop on Human Language Technology, HLT ?94,
pages 278?282, Morristown, NJ, USA.
Alessandro Moschitti, Daniele Pighin, and Roberto Basili. 2006. Semantic role labeling via tree kernel joint
inference. In Proceedings of the Tenth Conference on Computational Natural Language Learning (CoNLL-X),
pages 61?68, New York City, June.
201
Alessandro Moschitti. 2006. Efficient convolution kernels for dependency and constituent syntactic trees. In
Proceedings of the 17th European Conference on Machine Learning, ECML?06, pages 318?329, Berlin, Hei-
delberg. Springer-Verlag.
Truc-Vien T. Nguyen and Alessandro Moschitti. 2012. Structural reranking models for named entity recognition.
Intelligenza Artificiale, 6(2):177?190.
Kishore Papineni, Salim Roukos, and Todd Ward. 1998. Maximum likelihood and discriminative training of
direct translation models. In Proceedings of the IEEE International Conference on Acoustics, Speech and
Signal Processing, volume 1, pages 189?192, Seattle, WA, USA.
Roberto Pieraccini, Esther Levin, and Chin-Hui Lee. 1991. Stochastic representation of conceptual structure in
the ATIS task. In Proceedings of the Workshop on Speech and Natural Language, HLT ?91, pages 121?124,
Pacific Grove, CA, USA.
Christian Raymond and Giuseppe Riccardi. 2007. Generative and discriminative algorithms for spoken language
understanding. In Proceedings of 8th Annual Conference of the International Speech Communication Associa-
tion, INTERSPEECH ?07, pages 1605?1608, Antwerp, Belgium.
Yigal Dan Rubinstein and Trevor Hastie. 1997. Discriminative vs informative learning. In Proceedings of the
Third International Conference on Knowledge Discovery and Data Mining, KDD ?97, pages 49?53, Newport
Beach, CA, USA.
Guzm?an Santaf?e, Jose Lozano, and Pedro Larra?naga. 2007. Discriminative vs. generative learning of Bayesian
network classifiers. Lecture Notes in Computer Science, 4724:453?464.
Sunita Sarawagi and William Cohen. 2004. Semi-Markov conditional random fields for information extraction.
In Proceedings of the 18th Annual Conference on Neural Information Processing Systems, NIPS ?04, pages
1185?1192, Vancouver, British Columbia, Canada.
Stephanie Seneff. 1989. TINA: A probabilistic syntactic parser for speech understanding systems. In Proceedings
of the Workshop on Speech and Natural Language, HLT ?89, pages 168?178, Philadelphia, PA, USA.
Manfred Stede. 2011. Discourse Processing. Synthesis Lectures on Human Language Technologies. Morgan and
Claypool Publishers.
Cornelis Joost van Rijsbergen. 1979. Information Retrieval. Butterworth.
Ye-Yi Wang, Raphael Hoffmann, Xiao Li, and Jakub Szymanski. 2009. Semi-supervised learning of semantic
classes for query understanding: from the web and for the web. In Proceedings of the 18th ACM Conference on
Information and Knowledge Management, CIKM ?09, pages 37?46, New York, NY, USA.
202
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 214?220,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Learning to Differentiate Better from Worse Translations
Francisco Guzm
?
an Shafiq Joty Llu??s M
`
arquez
Alessandro Moschitti Preslav Nakov Massimo Nicosia
ALT Research Group
Qatar Computing Research Institute ? Qatar Foundation
{fguzman,sjoty,lmarquez,amoschitti,pnakov,mnicosia}@qf.org.qa
Abstract
We present a pairwise learning-to-rank
approach to machine translation evalua-
tion that learns to differentiate better from
worse translations in the context of a given
reference. We integrate several layers
of linguistic information encapsulated in
tree-based structures, making use of both
the reference and the system output simul-
taneously, thus bringing our ranking closer
to how humans evaluate translations. Most
importantly, instead of deciding upfront
which types of features are important, we
use the learning framework of preference
re-ranking kernels to learn the features au-
tomatically. The evaluation results show
that learning in the proposed framework
yields better correlation with humans than
computing the direct similarity over the
same type of structures. Also, we show
our structural kernel learning (SKL) can
be a general framework for MT evaluation,
in which syntactic and semantic informa-
tion can be naturally incorporated.
1 Introduction
We have seen in recent years fast improvement
in the overall quality of machine translation (MT)
systems. This was only possible because of the
use of automatic metrics for MT evaluation, such
as BLEU (Papineni et al., 2002), which is the de-
facto standard; and more recently: TER (Snover et
al., 2006) and METEOR (Lavie and Denkowski,
2009), among other emerging MT evaluation met-
rics. These automatic metrics provide fast and in-
expensive means to compare the output of differ-
ent MT systems, without the need to ask for hu-
man judgments each time the MT system has been
changed.
As a result, this has enabled rapid develop-
ment in the field of statistical machine translation
(SMT), by allowing to train and tune systems as
well as to track progress in a way that highly cor-
relates with human judgments.
Today, MT evaluation is an active field of re-
search, and modern metrics perform analysis at
various levels, e.g., lexical (Papineni et al., 2002;
Snover et al., 2006), including synonymy and
paraphrasing (Lavie and Denkowski, 2009); syn-
tactic (Gim?enez and M`arquez, 2007; Popovi?c
and Ney, 2007; Liu and Gildea, 2005); semantic
(Gim?enez and M`arquez, 2007; Lo et al., 2012);
and discourse (Comelles et al., 2010; Wong and
Kit, 2012; Guzm?an et al., 2014; Joty et al., 2014).
Automatic MT evaluation metrics compare the
output of a system to one or more human ref-
erences in order to produce a similarity score.
The quality of such a metric is typically judged
in terms of correlation of the scores it produces
with scores given by human judges. As a result,
some evaluation metrics have been trained to re-
produce the scores assigned by humans as closely
as possible (Albrecht and Hwa, 2008). Unfortu-
nately, humans have a hard time assigning an ab-
solute score to a translation. Hence, direct hu-
man evaluation scores such as adequacy and flu-
ency, which were widely used in the past, are
now discontinued in favor of ranking-based eval-
uations, where judges are asked to rank the out-
put of 2 to 5 systems instead. It has been shown
that using such ranking-based assessments yields
much higher inter-annotator agreement (Callison-
Burch et al., 2007).
While evaluation metrics still produce numeri-
cal scores, in part because MT evaluation shared
tasks at NIST and WMT ask for it, there has also
been work on a ranking formulation of the MT
evaluation task for a given set of outputs. This
was shown to yield higher correlation with human
judgments (Duh, 2008; Song and Cohn, 2011).
214
Learning automatic metrics in a pairwise set-
ting, i.e., learning to distinguish between two al-
ternative translations and to decide which of the
two is better (which is arguably one of the easiest
ways to produce a ranking), emulates closely how
human judges perform evaluation assessments in
reality. Instead of learning a similarity function
between a translation and the reference, they learn
how to differentiate a better from a worse trans-
lation given a corresponding reference. While the
pairwise setting does not provide an absolute qual-
ity scoring metric, it is useful for most evaluation
and MT development scenarios.
In this paper, we propose a pairwise learning
setting similar to that of Duh (2008), but we extend
it to a new level, both in terms of feature represen-
tation and learning framework. First, we integrate
several layers of linguistic information encapsu-
lated in tree-based structures; Duh (2008) only
used lexical and POS matches as features. Second,
we use information about both the reference and
two alternative translations simultaneously, thus
bringing our ranking closer to how humans rank
translations. Finally, instead of deciding upfront
which types of features between hypotheses and
references are important, we use a our structural
kernel learning (SKL) framework to generate and
select them automatically.
The structural kernel learning (SKL) framework
we propose consists in: (i) designing a struc-
tural representation, e.g., using syntactic and dis-
course trees of translation hypotheses and a refer-
ences; and (ii) applying structural kernels (Mos-
chitti, 2006; Moschitti, 2008), to such representa-
tions in order to automatically inject structural fea-
tures in the preference re-ranking algorithm. We
use this method with translation-reference pairs
to directly learn the features themselves, instead
of learning the importance of a predetermined set
of features. A similar learning framework has
been proven to be effective for question answer-
ing (Moschitti et al., 2007), and textual entailment
recognition (Zanzotto and Moschitti, 2006).
Our goals are twofold: (i) in the short term, to
demonstrate that structural kernel learning is suit-
able for this task, and can effectively learn to rank
hypotheses at the segment-level; and (ii) in the
long term, to show that this approach provides a
unified framework that allows to integrate several
layers of linguistic analysis and information and to
improve over the state-of-the-art.
Below we report the results of some initial ex-
periments using syntactic and discourse structures.
We show that learning in the proposed framework
yields better correlation with humans than apply-
ing the traditional translation?reference similarity
metrics using the same type of structures. We
also show that the contributions of syntax and dis-
course information are cumulative. Finally, de-
spite the limited information we use, we achieve
correlation at the segment level that outperforms
BLEU and other metrics at WMT12, e.g., our met-
ric would have been ranked higher in terms of cor-
relation with human judgments compared to TER,
NIST, and BLEU in the WMT12 Metrics shared
task (Callison-Burch et al., 2012).
2 Kernel-based Learning from Linguistic
Structures
In our pairwise setting, each sentence s in
the source language is represented by a tuple
?t
1
, t
2
, r?, where t
1
and t
2
are two alternative
translations and r is a reference translation. Our
goal is to develop a classifier of such tuples that
decides whether t
1
is a better translation than t
2
given the reference r.
Engineering features for deciding whether t
1
is
a better translation than t
2
is a difficult task. Thus,
we rely on the automatic feature extraction en-
abled by the SKL framework, and our task is re-
duced to choosing: (i) a meaningful structural rep-
resentation for ?t
1
, t
2
, r?, and (ii) a feature func-
tion ?
mt
that maps such structures to substruc-
tures, i.e., our feature space. Since the design
of ?
mt
is complex, we use tree kernels applied
to two simpler structural mappings ?
M
(t
1
, r) and
?
M
(t
2
, r). The latter generate the tree representa-
tions for the translation-reference pairs (t
1
, r) and
(t
2
, r). The next section shows such mappings.
2.1 Representations
To represent a translation-reference pair (t, r), we
adopt shallow syntactic trees combined with RST-
style discourse trees. Shallow trees have been
successfully used for question answering (Severyn
and Moschitti, 2012) and semantic textual sim-
ilarity (Severyn et al., 2013b); while discourse
information has proved useful in MT evaluation
(Guzm?an et al., 2014; Joty et al., 2014). Com-
bined shallow syntax and discourse trees worked
well for concept segmentation and labeling (Saleh
et al., 2014a).
215
DIS:ELABORATION
EDU:NUCLEUS EDU:SATELLITE-REL
VP NP-REL NP VP-REL o-REL o-REL
RB TO-REL VB-REL PRP-REL DT NN-REL TO-REL VB-REL .-REL ''-REL
not to give them the time to think . "
VP NP-REL NP VP-REL o-REL o-REL
TO-REL `` VB-REL PRP-REL DT NN-REL TO-REL VB-REL .-REL ''-REL
to " give them no time to think . "
a) Hypothesis
b) Reference DIS:ELABORATION
EDU:NUCLEUS EDU:SATELLITE-REL
Bag-of-words relations 
rela
tion
 pro
pag
atio
n di
rect
ion
Figure 1: Hypothesis and reference trees combining discourse, shallow syntax and POS.
Figure 1 shows two example trees combining
discourse, shallow syntax and POS: one for a
translation hypothesis (top) and the other one for
the reference (bottom). To build such structures,
we used the Stanford POS tagger (Toutanova et
al., 2003), the Illinois chunker (Punyakanok and
Roth, 2001), and the discourse parser
1
of (Joty et
al., 2012; Joty et al., 2013).
The lexical items constitute the leaves of the
tree. The words are connected to their respec-
tive POS tags, which are in turn grouped into
chunks. Then, the chunks are grouped into el-
ementary discourse units (EDU), to which the
nuclearity status is attached (i.e., NUCLEUS or
SATELLITE). Finally, EDUs and higher-order dis-
course units are connected by discourse relations
(e.g., DIS:ELABORATION).
2.2 Kernels-based modeling
In the SKL framework, the learning objects are
pairs of translations ?t
1
, t
2
?. Our objective is to
automatically learn which pair features are impor-
tant, independently of the source sentence. We
achieve this by using kernel machines (KMs) over
two learning objects ?t
1
, t
2
?, ?t
?
1
, t
?
2
?, along with
an explicit and structural representation of the
pairs (see Fig. 1).
1
The discourse parser can be downloaded from
http://alt.qcri.org/tools/
More specifically, KMs carry out learning using
the scalar product
K
mt
(?t
1
, t
2
?, ?t
?
1
, t
?
2
?) = ?
mt
(t
1
, t
2
) ??
mt
(t
?
1
, t
?
2
),
where ?
mt
maps pairs into the feature space.
Considering that our task is to decide whether
t
1
is better than t
2
, we can conveniently rep-
resent the vector for the pair in terms of the
difference between the two translation vectors,
i.e., ?
mt
(t
1
, t
2
) = ?
K
(t
1
) ? ?
K
(t
2
). We can
approximate K
mt
with a preference kernel PK to
compute this difference in the kernel space K:
PK(?t
1
, t
2
?, ?t
?
1
, t
?
2
?) (1)
= K(t
1
)? ?
K
(t
2
)) ? (?
K
(t
?
1
)? ?
K
(t
?
2
))
= K(t
1
, t
?
1
) +K(t
2
, t
?
2
)?K(t
1
, t
?
2
)?K(t
2
, t
?
1
)
The advantage of this is that now K(t
i
, t
?
j
) =
?
K
(t
i
) ? ?
K
(t
?
j
) is defined between two transla-
tions only, and not between two pairs of transla-
tions. This simplification enables us to map trans-
lations into simple trees, e.g., those in Figure 1,
and then to apply them tree kernels, e.g., the Par-
tial Tree Kernel (Moschitti, 2006), which carry out
a scalar product in the subtree space.
We can further enrich the representation ?
K
, if
we consider all the information available to the
human judges when they are ranking translations.
That is, the two alternative translations along with
their corresponding reference.
216
In particular, let r and r
?
be the references for
the pairs ?t
1
, t
2
? and ?t
?
1
, t
?
2
?, we can redefine all
the members of Eq. 1, e.g., K(t
1
, t
?
1
) becomes
K(?t
1
, r?, ?t
?
1
, r
?
?) = PTK(?
M
(t
1
, r), ?
M
(t
?
1
, r
?
))
+ PTK(?
M
(r, t
1
), ?
M
(r
?
, t
?
1
)),
where ?
M
maps a pair of texts to a single tree.
There are several options to produce the bitext-
to-tree mapping for ?
M
. A simple approach is
to only use the tree corresponding to the first ar-
gument of ?
M
. This leads to the basic model
K(?t
1
, r?, ?t
?
1
, r
?
?) = PTK(?
M
(t
1
), ?
M
(t
?
1
)) +
PTK(?
M
(r), ?
M
(r
?
)), i.e., the sum of two tree
kernels applied to the trees constructed by ?
M
(we
previously informally mentioned it).
However, this simple mapping may be ineffec-
tive since the trees within a pair, e.g., (t
1
, r), are
treated independently, and no meaningful features
connecting t
1
and r can be derived from their
tree fragments. Therefore, we model ?
M
(r, t
1
) by
using word-matching relations between t
1
and r,
such that connections between words and con-
stituents of the two trees are established using
position-independent word matching. For exam-
ple, in Figure 1, the thin dashed arrows show the
links connecting the matching words between t
1
and r. The propagation of these relations works
from the bottom up. Thus, if all children in a con-
stituent have a link, their parent is also linked.
The use of such connections is essential as it en-
ables the comparison of the structural properties
and relations between two translation-reference
pairs. For example, the tree fragment [ELABORA-
TION [SATELLITE]] from the translation is con-
nected to [ELABORATION [SATELLITE]] in the
reference, indicating a link between two entire dis-
course units (drawn with a thicker arrow), and pro-
viding some reliability to the translation
2
.
Note that the use of connections yields a graph
representation instead of a tree. This is problem-
atic as effective models for graph kernels, which
would be a natural fit to this problem, are not cur-
rently available for exploiting linguistic informa-
tion. Thus, we simply use K, as defined above,
where the mapping ?
M
(t
1
, r) only produces a tree
for t
1
annotated with the marker REL represent-
ing the connections to r. This marker is placed on
all node labels of the tree generated from t
1
that
match labels from the tree generated from r.
2
Note that a non-pairwise model, i.e., K(t
1
, r), could
also be used to match the structural information above, but
it would not learn to compare it to a second pair (t
2
, r).
In other words, we only consider the trees en-
riched by markers separately, and ignore the edges
connecting both trees.
3 Experiments and Discussion
We experimented with datasets of segment-level
human rankings of system outputs from the
WMT11 and the WMT12 Metrics shared tasks
(Callison-Burch et al., 2011; Callison-Burch et al.,
2012): we used the WMT11 dataset for training
and the WMT12 dataset for testing. We focused
on translating into English only, for which the
datasets can be split by source language: Czech
(cs), German (de), Spanish (es), and French (fr).
There were about 10,000 non-tied human judg-
ments per language pair per dataset. We scored
our pairwise system predictions with respect to
the WMT12 human judgments using the Kendall?s
Tau (? ), which was official at WMT12.
Table 1 presents the ? scores for all metric vari-
ants introduced in this paper: for the individual
language pairs and overall. The left-hand side of
the table shows the results when using as sim-
ilarity the direct kernel calculation between the
corresponding structures of the candidate transla-
tion and the reference
3
, e.g., as in (Guzm?an et al.,
2014; Joty et al., 2014). The right-hand side con-
tains the results for structured kernel learning.
We can make the following observations:
(i) The overall results for all SKL-trained metrics
are higher than the ones when applying direct sim-
ilarity, showing that learning tree structures is bet-
ter than just calculating similarity.
(ii) Regarding the linguistic representation, we see
that, when learning tree structures, syntactic and
discourse-based trees yield similar improvements
with a slight advantage for the former. More in-
terestingly, when both structures are put together
in a combined tree, the improvement is cumula-
tive and yields the best results by a sizable margin.
This provides positive evidence towards our goal
of a unified tree-based representation with multi-
ple layers of linguistic information.
(iii) Comparing to the best evaluation metrics
that participated in the WMT12 Metrics shared
task, we find that our approach is competitive and
would have been ranked among the top 3 partici-
pants.
3
Applying tree kernels between the members of a pair to
generate one feature (for each different kernel function) has
become a standard practice in text similarity tasks (Severyn et
al., 2013b) and in question answering (Severyn et al., 2013a).
217
Similarity Structured Kernel Learning
Structure cs-en de-en es-en fr-en all cs-en de-en es-en fr-en all
1 SYN 0.169 0.188 0.203 0.222 0.195 0.190 0.244 0.198 0.158 0.198
2 DIS 0.130 0.174 0.188 0.169 0.165 0.176 0.235 0.166 0.160 0.184
3 DIS+POS 0.135 0.186 0.190 0.178 0.172 0.167 0.232 0.202 0.133 0.183
4 DIS+SYN 0.156 0.205 0.206 0.203 0.192 0.210 0.251 0.240 0.223 0.231
Table 1: Kendall?s (? ) correlation with human judgements on WMT12 for each language pair.
Furthermore, our result (0.237) is ahead of the
correlation obtained by popular metrics such as
TER (0.217), NIST (0.214) and BLEU (0.185) at
WMT12. This is very encouraging and shows the
potential of our new proposal.
In this paper, we have presented only the first
exploratory results. Our approach can be easily
extended with richer linguistic structures and fur-
ther combined with some of the already existing
strong evaluation metrics.
Testing
Train cs-en de-en es-en fr-en all
1 cs-en 0.210 0.204 0.217 0.204 0.209
2 de-en 0.196 0.251 0.203 0.202 0.213
3 es-en 0.218 0.204 0.240 0.223 0.221
4 fr-en 0.203 0.218 0.224 0.223 0.217
5 all 0.231 0.258 0.226 0.232 0.237
Table 2: Kendall?s (? ) on WMT12 for cross-
language training with DIS+SYN.
Note that the results in Table 1 were for train-
ing on WMT11 and testing on WMT12 for each
language pair in isolation. Next, we study the im-
pact of the choice of training language pair. Ta-
ble 2 shows cross-language evaluation results for
DIS+SYN: lines 1-4 show results when training on
WMT11 for one language pair, and then testing for
each language pair of WMT12.
We can see that the overall differences in perfor-
mance (see the last column: all) when training on
different source languages are rather small, rang-
ing from 0.209 to 0.221, which suggests that our
approach is quite independent of the source lan-
guage used for training. Still, looking at individ-
ual test languages, we can see that for de-en and
es-en, it is best to train on the same language; this
also holds for fr-en, but there it is equally good
to train on es-en. Interestingly, training on es-en
improves a bit for cs-en.
These somewhat mixed results have motivated
us to try tuning on the full WMT11 dataset; as line
5 shows, this yielded improvements for all lan-
guage pairs except for es-en. Comparing to line
4 in Table 1, we see that the overall Tau improved
from 0.231 to 0.237.
4 Conclusions and Future Work
We have presented a pairwise learning-to-rank ap-
proach to MT evaluation, which learns to differen-
tiate good from bad translations in the context of
a given reference. We have integrated several lay-
ers of linguistic information (lexical, syntactic and
discourse) in tree-based structures, and we have
used the structured kernel learning to identify rel-
evant features and learn pairwise rankers.
The evaluation results have shown that learning
in the proposed SKL framework is possible, yield-
ing better correlation (Kendall?s ? ) with human
judgments than computing the direct kernel sim-
ilarity between translation and reference, over the
same type of structures. We have also shown that
the contributions of syntax and discourse informa-
tion are cumulative, indicating that this learning
framework can be appropriate for the combination
of different sources of information. Finally, de-
spite the limited information we used, we achieved
better correlation at the segment level than BLEU
and other metrics in the WMT12 Metrics task.
In the future, we plan to work towards our long-
term goal, i.e., including more linguistic informa-
tion in the SKL framework and showing that this
can help. This would also include more semantic
information, e.g., in the form of Brown clusters or
using semantic similarity between the words com-
posing the structure calculated with latent seman-
tic analysis (Saleh et al., 2014b).
We further want to show that the proposed
framework is flexible and can include information
in the form of quality scores predicted by other
evaluation metrics, for which a vector of features
would be combined with the structured kernel.
Acknowledgments
This research is part of the Interactive sYstems
for Answer Search (Iyas) project, conducted by
the Arabic Language Technologies (ALT) group
at Qatar Computing Research Institute (QCRI)
within the Qatar Foundation.
218
References
Joshua Albrecht and Rebecca Hwa. 2008. Regression
for machine translation evaluation at the sentence
level. Machine Translation, 22(1-2):1?27.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. 2007.
(Meta-) evaluation of machine translation. In Pro-
ceedings of the Second Workshop on Statistical
Machine Translation, WMT ?07, pages 136?158,
Prague, Czech Republic.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Omar Zaidan. 2011. Findings of the 2011
workshop on statistical machine translation. In Pro-
ceedings of the Sixth Workshop on Statistical Ma-
chine Translation, WMT ?11, pages 22?64, Edin-
burgh, Scotland, UK.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 workshop on statistical ma-
chine translation. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, WMT
?12, pages 10?51, Montr?eal, Canada.
Elisabet Comelles, Jes?us Gim?enez, Llu??s M`arquez,
Irene Castell?on, and Victoria Arranz. 2010.
Document-level automatic MT evaluation based on
discourse representations. In Proceedings of the
Joint Fifth Workshop on Statistical Machine Trans-
lation and MetricsMATR, WMT ?10, pages 333?
338, Uppsala, Sweden.
Kevin Duh. 2008. Ranking vs. regression in machine
translation evaluation. In Proceedings of the Third
Workshop on Statistical Machine Translation, WMT
?08, pages 191?194, Columbus, Ohio, USA.
Jes?us Gim?enez and Llu??s M`arquez. 2007. Linguis-
tic features for automatic evaluation of heterogenous
MT systems. In Proceedings of the Second Work-
shop on Statistical Machine Translation, WMT ?07,
pages 256?264, Prague, Czech Republic.
Francisco Guzm?an, Shafiq Joty, Llu??s M`arquez, and
Preslav Nakov. 2014. Using discourse structure
improves machine translation evaluation. In Pro-
ceedings of 52nd Annual Meeting of the Association
for Computational Linguistics, ACL ?14, pages 687?
698, Baltimore, Maryland, USA.
Shafiq Joty, Giuseppe Carenini, and Raymond Ng.
2012. A Novel Discriminative Framework for
Sentence-Level Discourse Analysis. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, EMNLP-CoNLL ?12,
pages 904?915, Jeju Island, Korea.
Shafiq Joty, Giuseppe Carenini, Raymond Ng, and
Yashar Mehdad. 2013. Combining Intra- and
Multi-sentential Rhetorical Parsing for Document-
level Discourse Analysis. In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics, ACL ?13, pages 486?496, Sofia,
Bulgaria.
Shafiq Joty, Francisco Guzm?an, Llu??s M`arquez, and
Preslav Nakov. 2014. DiscoTK: Using discourse
structure for machine translation evaluation. In Pro-
ceedings of the Ninth Workshop on Statistical Ma-
chine Translation, WMT ?14, pages 402?408, Balti-
more, Maryland, USA.
Alon Lavie and Michael Denkowski. 2009. The ME-
TEOR metric for automatic evaluation of machine
translation. Machine Translation, 23(2?3):105?115.
Ding Liu and Daniel Gildea. 2005. Syntactic fea-
tures for evaluation of machine translation. In Pro-
ceedings of the ACL Workshop on Intrinsic and Ex-
trinsic Evaluation Measures for Machine Transla-
tion and/or Summarization, pages 25?32, Ann Ar-
bor, Michigan, USA.
Chi-kiu Lo, Anand Karthik Tumuluru, and Dekai Wu.
2012. Fully automatic semantic MT evaluation. In
Proceedings of the Seventh Workshop on Statisti-
cal Machine Translation, WMT ?12, pages 243?252,
Montr?eal, Canada.
Alessandro Moschitti, Silvia Quarteroni, Roberto
Basili, and Suresh Manandhar. 2007. Exploiting
syntactic and shallow semantic kernels for ques-
tion answer classification. In Proceedings of the
45th Annual Meeting of the Association of Computa-
tional Linguistics, ACL ?07, pages 776?783, Prague,
Czech Republic.
Alessandro Moschitti. 2006. Efficient convolution ker-
nels for dependency and constituent syntactic trees.
In Proceedings of 17th European Conference on Ma-
chine Learning and the 10th European Conference
on Principles and Practice of Knowledge Discovery
in Databases, ECML/PKDD ?06, pages 318?329,
Berlin, Germany.
Alessandro Moschitti. 2008. Kernel methods, syn-
tax and semantics for relational text categorization.
In Proceedings of the 17th ACM Conference on In-
formation and Knowledge Management, CIKM ?08,
pages 253?262, Napa Valley, California, USA.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of 40th Annual Meting of the Association for Com-
putational Linguistics, ACL ?02, pages 311?318,
Philadelphia, Pennsylvania, USA.
Maja Popovi?c and Hermann Ney. 2007. Word error
rates: Decomposition over POS classes and applica-
tions for error analysis. In Proceedings of the Sec-
ond Workshop on Statistical Machine Translation,
WMT ?07, pages 48?55, Prague, Czech Republic.
Vasin Punyakanok and Dan Roth. 2001. The use of
classifiers in sequential inference. In Advances in
Neural Information Processing Systems 14, NIPS
?01, pages 995?1001, Vancouver, Canada.
219
Iman Saleh, Scott Cyphers, Jim Glass, Shafiq Joty,
Llu??s M`arquez, Alessandro Moschitti, and Preslav
Nakov. 2014a. A study of using syntactic and se-
mantic structures for concept segmentation and la-
beling. In Proceedings of the 25th International
Conference on Computational Linguistics, COLING
?14, pages 193?202, Dublin, Ireland.
Iman Saleh, Alessandro Moschitti, Preslav Nakov,
Llu??s M`arquez, and Shafiq Joty. 2014b. Semantic
kernels for semantic parsing. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP ?14, Doha, Qatar.
Aliaksei Severyn and Alessandro Moschitti. 2012.
Structural relationships for large-scale learning of
answer re-ranking. In Proceedings of the 35th Inter-
national ACM SIGIR Conference on Research and
Development in Information Retrieval, SIGIR ?12,
pages 741?750, Portland, Oregon, USA.
Aliaksei Severyn, Massimo Nicosia, and Alessandro
Moschitti. 2013a. Learning adaptable patterns for
passage reranking. In Proceedings of the Seven-
teenth Conference on Computational Natural Lan-
guage Learning, CoNLL ?13, pages 75?83, Sofia,
Bulgaria.
Aliaksei Severyn, Massimo Nicosia, and Alessandro
Moschitti. 2013b. Learning semantic textual sim-
ilarity with structural representations. In Proceed-
ings of the 51st Annual Meeting of the Association
for Computational Linguistics (Volume 2: Short Pa-
pers), ACL ?13, pages 714?718, Sofia, Bulgaria.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of the 7th Biennial Conference of the
Association for Machine Translation in the Ameri-
cas, AMTA ?06, Cambridge, Massachusetts, USA.
Xingyi Song and Trevor Cohn. 2011. Regression and
ranking based optimisation for sentence-level MT
evaluation. In Proceedings of the Sixth Workshop
on Statistical Machine Translation, WMT ?11, pages
123?129, Edinburgh, Scotland, UK.
Kristina Toutanova, Dan Klein, Christopher Manning,
and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology
- Volume 1, HLT-NAACL ?03, pages 173?180, Ed-
monton, Canada.
Billy Wong and Chunyu Kit. 2012. Extending ma-
chine translation evaluation metrics with lexical co-
hesion to document level. In Proceedings of the
2012 Joint Conference on Empirical Methods in
Natural Language Processing and Computational
Natural Language Learning, EMNLP-CoNLL ?12,
pages 1060?1068, Jeju Island, Korea.
Fabio Massimo Zanzotto and Alessandro Moschitti.
2006. Automatic learning of textual entailments
with cross-pair similarities. In Proceedings of the
21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Associ-
ation for Computational Linguistics, COLING-ACL
?06, pages 401?408, Sydney, Australia.
220
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 430?435,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
A Shortest-path Method for Arc-factored Semantic Role Labeling
Xavier Llu??s
TALP Research Center
Universitat Polit`ecnica de
Catalunya
xlluis@cs.upc.edu
Xavier Carreras
Xerox Research Centre
Europe
xavier.carreras@xrce.xerox.com
Llu??s M
`
arquez
ALT Research Group
Qatar Computing Research
Institute
lmarquez@qf.org.qa
Abstract
We introduce a Semantic Role Labeling
(SRL) parser that finds semantic roles for a
predicate together with the syntactic paths
linking predicates and arguments. Our
main contribution is to formulate SRL in
terms of shortest-path inference, on the as-
sumption that the SRL model is restricted
to arc-factored features of the syntactic
paths behind semantic roles. Overall, our
method for SRL is a novel way to ex-
ploit larger variability in the syntactic re-
alizations of predicate-argument relations,
moving away from pipeline architectures.
Experiments show that our approach im-
proves the robustness of the predictions,
producing arc-factored models that per-
form closely to methods using unrestricted
features from the syntax.
1 Introduction
Semantic role labeling (SRL) consists of finding
the arguments of a predicate and labeling them
with semantic roles (Gildea and Jurafsky, 2002;
M`arquez et al., 2008). The arguments fill roles that
answer questions of the type ?who? did ?what? to
?whom?, ?how?, and ?why? for a given sentence
predicate. Most approaches to SRL are based on
a pipeline strategy, first parsing the sentence to
obtain a syntactic tree and then identifying and
classifying arguments (Gildea and Jurafsky, 2002;
Carreras and M`arquez, 2005).
SRL methods critically depend on features of
the syntactic structure, and consequently parsing
mistakes can harm the quality of semantic role
predictions (Gildea and Palmer, 2002). To allevi-
ate this dependence, previous work has explored
k-best parsers (Johansson and Nugues, 2008),
combination systems (Surdeanu et al., 2007) or
joint syntactic-semantic models (Johansson, 2009;
Henderson et al., 2008; Llu??s et al., 2013).
In this paper we take a different approach. In
our scenario SRL is the end goal, and we as-
sume that syntactic parsing is only an intermedi-
ate step to extract features to support SRL predic-
tions. In this setting we define a model that, given
a predicate, identifies each of the semantic roles
together with the syntactic path that links the pred-
icate with the argument. Thus, following previous
work (Moschitti, 2004; Johansson, 2009), we take
the syntactic path as the main source of syntac-
tic features, but instead of just conditioning on it,
we predict it together with the semantic role. The
main contribution of this paper is a formulation of
SRL parsing in terms of efficient shortest-path in-
ference, under the assumption that the SRL model
is restricted to arc-factored features of the syntac-
tic path linking the argument with the predicate.
Our assumption ?that features of an SRL
model should factor over dependency arcs? is
supported by some empirical frequencies. Table 1
shows the most frequent path patterns on CoNLL-
2009 (Haji?c et al., 2009) data for several lan-
guages, where a path pattern is a sequence of as-
cending arcs from the predicate to some ancestor,
followed by descending arcs to the argument. For
English the distribution of path patterns is rather
simple: the majority of paths consists of a num-
ber of ascending arcs followed by zero or one de-
scending arc. Thus a common strategy in SRL sys-
tems, formulated by Xue and Palmer (2004), is to
look for arguments in the ancestors of the pred-
icate and their direct descendants. However, in
Czech and Japanese data we observe a large por-
tion of paths with two or more descending arcs,
which makes it difficult to characterize the syn-
tactic scope in which arguments are found. Also,
in the datasets for German, Czech and Chinese the
three most frequent patterns cover over the 90% of
all arguments. In contrast, Japanese exhibits much
more variability and a long tail of infrequent types
430
English German Czech Chinese Japanese
?
% % path
?
% % path
?
% % path
?
% % path
?
% % path
63.63 63.6298 ? 77.22 77.2202 ? 63.90 63.8956 ? 78.09 78.0949 ? 37.20 37.1977 ??
73.97 10.3429 ?? 93.51 16.2854 ?? 86.26 22.3613 ?? 85.36 7.26962 ?? 51.52 14.3230 ?
80.63 6.65915 ? 97.43 3.92111 ??? 90.24 3.98078 ?? 91.27 5.90333 ??? 60.79 9.27270 ???
85.97 5.33352 ? 98.19 0.76147 ?? 93.95 3.71713 ??? 95.93 4.66039 ?? 70.03 9.23857 ?
90.78 4.81104 ??? 98.70 0.51640 ???? 95.48 1.52168 ??? 97.53 1.60392 ? 74.17 4.13359 ????
93.10 2.31928 ???? 99.17 0.46096 ? 96.92 1.44091 ? 98.28 0.75086 ???? 76.76 2.59117 ??
95.19 2.09043 ?? 99.43 0.26841 ??? 97.68 0.76714 ??? 98.77 0.48734 ?? 78.82 2.06111 ????
96.26 1.07468 ????? 99.56 0.12837 ???? 98.28 0.59684 ???? 99.13 0.36270 ??? 80.85 2.03381 ?????
97.19 0.92482 ?? 99.67 0.10503 ????? 98.60 0.31759 ???? 99.45 0.31699 ????? 82.66 1.80631 ???
97.93 0.74041 ??? 99.77 0.10503 ?? 98.88 0.28227 ???? 99.72 0.27041 ???? 83.71 1.05558 ???
98.41 0.48565 ?????? 99.82 0.04960 ??? 99.15 0.26721 ???? 99.82 0.10049 ??? 84.74 1.02828 ?????
98.71 0.29769 ???? 99.87 0.04960 ??? 99.27 0.12430 ????? 99.86 0.03623 ??? 85.68 0.93500 ?????
98.94 0.22733 ??????? 99.90 0.02626 ? 99.37 0.10103 ????? 99.89 0.02890 ???? 86.61 0.93273 ??????
99.11 0.17805 ??? 99.92 0.02042 ????? 99.47 0.09747 ?? 99.92 0.02890 ?????? 87.29 0.68249 ??????
99.27 0.15316 ??? 99.94 0.02042 ?????? 99.56 0.08515 ????? 99.94 0.02846 ? 87.90 0.60969 ????
99.39 0.12065 ????? 99.95 0.01459 ????? 99.63 0.07419 ????? 99.96 0.02070 ????? 88.47 0.56646 ??????
99.50 0.11024 ???? 99.96 0.01167 ???? 99.69 0.05667 ????? 99.97 0.00992 ????? 89.01 0.53689 ???????
99.60 0.09931 ???????? 99.97 0.00875 ???? 99.73 0.04216 ?????? 99.98 0.00733 ??????? 89.49 0.48684 ??????
99.65 0.05283 ???? 99.98 0.00875 ??????? 99.76 0.02875 ?????? 99.99 0.00431 ?????? 89.94 0.45044 ????
Table 1: Summary of the most frequent paths on the CoNLL-2009 Shared Task datasets. ? indicates that we traverse a syntactic
dependency upwards from a modifier to a head. ? is for dependencies following a descending head to modifier edge. The
symbol ? represents that the argument is the predicate itself. We exclude from this table Catalan and Spanish as predicates and
arguments are always trivially related by a single syntactic dependency that descends.
of patterns. In general it is not feasible to capture
path patterns manually, and it is not desirable that
a statistical system depends on rather sparse non-
factored path features. For this reason in this paper
we explore arc-factored models for SRL.
Our method might be specially useful in appli-
cations were we are interested in some target se-
mantic role, i.e. retrieving agent relations for some
verb, since it processes semantic roles indepen-
dently of each other. Our method might also be
generalizable to other kinds of semantic relations
which strongly depend on syntactic patterns such
as relation extraction in information extraction or
discourse parsing.
2 Arc-factored SRL
We define an SRL parsing model that re-
trieves predicate-argument relations based on arc-
factored syntactic representations of paths con-
necting predicates with their arguments. Through-
out the paper we assume a fixed sentence x =
x
1
, . . . , x
n
and a fixed predicate index p. The
SRL output is an indicator vector z, where
z
r,a
= 1 indicates that token a is filling role
r for predicate p. Our SRL parser performs
argmax
z?Z(x,p)
s(x, p, z), where Z(x, p) defines
the set of valid argument structures for p, and
s(x, p, z) computes a plausibility score for z given
x and p. Our first assumption is that the score
function factors over role-argument pairs:
s(x, p, z) =
?
z
r,a
=1
s(x, p, r, a) . (1)
Then we assume two components in the model,
one that scores the role-argument pair alone, and
another that considers the best (max) syntactic de-
pendency pathpi that connects the predicate pwith
the argument a:
s(x, p, r, a) = s
0
(x, p, r, a) +
max
pi
s
syn
(x, p, r, a,pi) . (2)
The model does not assume access to the syntac-
tic structure of x, hence in Eq. (2) we locally re-
trieve the maximum-scoring path for an argument-
role pair. A path pi is a sequence of dependencies
?h,m, l? where h is the head, m the modifier and l
the syntactic label. We further assume that the syn-
tactic component factors over the dependencies in
the path:
s
syn
(x, p, r, a,pi)=
?
?h,m,l??pi
s
syn
(x, p, r, a, ?h,m, l?) .
(3)
This will allow to employ efficient shortest-path
inference, which is the main contribution of this
paper and is described in the next section. Note
that since paths are locally retrieved per role-
argument pair, there is no guarantee that the set
of paths across roles forms a (sub)tree.
As a final note, in this paper we follow Llu??s
et al. (2013) and consider a constrained space of
valid argument structures Z(x, p): (a) each role is
realized at most once, and (b) each token fills at
most one role. As shown by Llu??s et al. (2013),
this can be efficiently solved as a linear assign-
431
Figure 1: Graph representing all possible syntactic paths
from a single predicate to their arguments. We find in this
graph the best SRL using a shortest-path algorithm. Note that
many edges are omitted for clarity reasons. We labeled the
nodes and arcs as follows: p is the predicate and source ver-
tex; u
1
, . . . , u
n
are tokens reachable by an ascending path;
v
1
, . . . , v
n
are tokens reachable by a ascending path (possi-
bly empty) followed by a descending path (possibly empty);
a
i?j
is an edge related to an ascending dependency from
node u
i
to node u
j
; d
i?j
is a descending dependency from
node v
i
to node v
j
; 0
i?i
is a 0-weighted arc that connects the
ascending portion of the path ending at u
i
with the descend-
ing portion of the path starting at v
i
.
ment problem as long as the SRL model factors
over role-argument pairs, as in Eq. (1).
3 SRL as a Shortest-path Problem
We now focus on solving the maximization over
syntactic paths in Eq. (2). We will turn it into a
minimization problem which can be solved with a
polynomial-cost algorithm, in our case a shortest-
path method. Assume a fixed argument and role,
and define ?
?h,m,l?
to be a non-negative penalty for
the syntactic dependency ?h,m, l? to appear in the
predicate-argument path. We describe a shortest-
path method that finds the path of arcs with the
smaller penalty:
min
pi
?
?h,m,l??pi
?
?h,m,l?
. (4)
We find these paths by appropriately constructing
a weighted graph G = (V,E) that represents the
problem. Later we show how to adapt the arc-
factored model scores to be non-negative penal-
ties, such that the solution to Eq. (4) will be the
negative of the maximizer of Eq. (2).
It remains only to define the graph construc-
tion where paths correspond to arc-factored edges
weighted by ? penalties. We start by noting that
any path from a predicate p to an argument v
i
is
formed by a number of ascending syntactic arcs
followed by a number of descending arcs. The as-
cending segment connects p to some ancestor q (q
might be p itself, which implies an empty ascend-
ing segment); the descending segment connects q
with v
i
(which again might be empty). To com-
pactly represent all these possible paths we define
the graph as follows (see Figure 1):
1. Add node p as the source node of the graph.
2. Add nodes u
1
, . . . , u
n
for every token of the
sentence except p.
3. Link every pair of these nodes u
i
, u
j
with a
directed edge a
i?j
weighted by the corre-
sponding ascending arc, namely min
l
?
?j,i,l?
.
Also add ascending edges from p to any u
i
weighted by min
l
?
?i,p,l?
. So far we have
a connected component representing all as-
cending path segments.
4. Add nodes v
1
, . . . , v
n
for every token of the
sentence except p, and add edges d
i?j
be-
tween them weighted by descending arcs,
namely min
l
?
?i,j,l?
. This adds a second
strongly-connected component representing
descending path segments.
5. For each i, add an edge from u
i
to v
i
with
weight 0. This ensures that ascending and
descending path segments are connected con-
sistently.
6. Add direct descending edges from p to all the
v
i
nodes to allow for only-descending paths,
weighted by min
l
?
?p,i,l?
.
Dijkstra?s algorithm (Dijkstra, 1959) will find
the optimal path from predicate p to all tokens in
time O(V
2
) (see Cormen et al. (2009) for an in-
depth description). Thus, our method runs this
algorithm for each possible role of the predicate,
obtaining the best paths to all arguments at each
run.
4 Adapting and Training Model Scores
The shortest-path problem is undefined if a nega-
tive cycle is found in the graph as we may indefi-
nitely decrease the cost of a path by looping over
this cycle. Furthermore, Dijkstra?s algorithm re-
quires all arc scores to be non-negative penalties.
However, the model in Eq. (3) computes plausibil-
ity scores for dependencies, not penalties. And, if
we set this model to be a standard feature-based
linear predictor, it will predict unrestricted real-
valued scores.
One approach to map plausibility scores to
penalties is to assume a log-linear form for our
432
model. Let us denote by x? the tuple ?x, p, r, a?,
which we assume fixed in this section. The log-
linear model predicts:
Pr(?h,m, l? | x?) =
exp{w ? f(x?, ?h,m, l?)}
Z(x?)
,
(5)
where f(x?, ?h,m, l?) is a feature vector for an
arc in the path, w are the parameters, and Z(x?)
is the normalizer. We can turn predictions into
non-negative penalties by setting ?
?h,m,l?
to be
the negative log-probability of ?h,m, l?; namely
?
?h,m,l?
= ?w ? f(x?, ?h,m, l?) + logZ(x?). Note
that logZ(x?) shifts all values to the non-negative
side.
However, log-linear estimation of w is typically
expensive since it requires to repeatedly com-
pute feature expectations. Furthermore, our model
as defined in Eq. (2) combines arc-factored path
scores with path-independent scores, and it is de-
sirable to train these two components jointly. We
opt for a mistake-driven training strategy based
on the Structured Averaged Perceptron (Collins,
2002), which directly employs shortest-path infer-
ence as part of the training process.
To do so we predict plausibility scores for a de-
pendency directly as w ? f(x?, ?h,m, l?). To map
scores to penalties, we define
?
0
= max
?h,m,l?
w ? f(x?, ?h,m, l?)
and we set
?
?h,m,l?
= ?w ? f(x?, ?h,m, l?) + ?
0
.
Thus, ?
0
has a similar purpose as the log-
normalizer Z(x?) in a log-linear model, i.e., it
shifts the negated scores to the positive side; but
in our version the normalizer is based on the max
value, not the sum of exponentiated predictions as
in log-linear models. If we set our model function
to be
s
syn
(x?, ?h,m, l?) = w ? f(x?, ?h,m, l?)? ?
0
then the shortest-path method is exact.
5 Experiments
We present experiments using the CoNLL-2009
Shared Task datasets (Haji?c et al., 2009), for the
verbal predicates of English. Evaluation is based
on precision, recall and F
1
over correct predicate-
argument relations
1
. Our system uses the fea-
ture set of the state-of-the-art system by Johansson
(2009), but ignoring the features that do not factor
over single arcs in the path.
The focus of these experiments is to see the per-
formance of the shortest-path method with respect
to the syntactic variability. Rather than running
the method with the full set of possible depen-
dency arcs in a sentence, i.e. O(n
2
), we only con-
sider a fraction of the most likely dependencies.
To do so employ a probabilistic dependency-based
model, following Koo et al. (2007), that computes
the distribution over head-label pairs for a given
modifier, Pr(h, l | x,m). Specifically, for each
modifier token we only consider the dependencies
or heads whose probability is above a factor ? of
the most likely dependency for the given modi-
fier. Thus, ? = 1 selects only the most likely de-
pendency (similar to a pipeline system, but with-
out enforcing tree constraints), and as ? decreases
more dependencies are considered, to the point
where ? = 0 would select all possible dependen-
cies. Table 2 shows the ratio of dependencies in-
cluded with respect to a pipeline system for the de-
velopment set. As an example, if we set ? = 0.5,
for a given modifier we consider the most likely
dependency and also the dependencies with proba-
bility larger than 1/2 of the probability of the most
likely one. In this case the total number of depen-
dencies is 10.3% larger than only considering the
most likely one.
Table 3 shows results of the method on develop-
ment data, when training and testing with different
? values. The general trend is that testing with the
most restricted syntactic graph results in the best
performance. However, we observe that as we al-
low for more syntactic variability during training,
the results largely improve. Setting ? = 1 for both
training and testing gives a semantic F
1
of 75.9.
This configuration is similar to a pipeline approach
but considering only factored features. If we allow
to train with ? = 0.1 and we test with ? = 1 the
results improve by 1.96 points to a semantic F
1
of 77.8 points. When syntactic variability is too
large, e.g., ? = 0.01, no improvements are ob-
served.
Finally, table 4 shows results on the verbal En-
glish WSJ test set using our best configuration
1
Unlike in the official CoNLL-2009 evaluation, in this
work we exclude the predicate sense from the features and
the evaluation.
433
Threshold ? 1 0.9 0.5 0.1 0.01
Ratio 1 1.014 1.103 1.500 2.843
Table 2: Ratio of additional dependencies in the graphs with
respect to a single-tree pipeline model (? = 1) on develop-
ment data.
Threshold prec (%) rec (%) F
1
training ? = 1
1 77.91 73.97 75.89
0.9 77.23 74.17 75.67
0.5 73.30 75.03 74.16
0.1 58.22 68.75 63.05
0.01 32.83 53.69 40.74
training ? = 0.5
1 81.17 73.57 77.18
0.9 80.74 73.78 77.10
0.5 78.40 74.79 76.55
0.1 65.76 71.61 68.56
0.01 42.95 57.68 49.24
training ? = 0.1
1 84.03 72.52 77.85
0.9 83.76 72.66 77.82
0.5 82.75 73.33 77.75
0.1 77.25 72.20 74.64
0.01 63.90 65.98 64.92
training ? = 0.01
1 81.62 69.06 74.82
0.9 81.45 69.19 74.82
0.5 80.80 69.80 74.90
0.1 77.92 68.94 73.16
0.01 74.12 65.92 69.78
Table 3: Results of our shortest-path system for different
number of allowed dependencies showing precision, recall
and F
1
on development set for the verbal predicates of the
English language.
from the development set. We compare to the
state-of-the art system by Zhao et al. (2009) that
was the top-performing system for the English lan-
guage in SRL at the CoNLL-2009 Shared Task.
We also show the results for a shortest-path system
trained and tested with ? = 1. In addition we in-
clude an equivalent pipeline system using all fea-
tures, both factored and non-factored, as defined
in Johansson (2009). We observe that by not be-
ing able to capture non-factored features the final
performance drops by 1.6 F
1
points.
6 Conclusions
We have formulated SRL in terms of shortest-
path inference. Our model predicts semantic roles
together with associated syntactic paths, and as-
sumes an arc-factored representation of the path.
This property allows for efficient shortest-path al-
System prec(%) rec(%) F
1
Zhao et al. 2009 86.91 81.22 83.97
Non-factored 86.96 75.92 81.06
Factored ? = 1 79.88 76.12 77.96
Factored best 85.26 74.41 79.46
Table 4: Test set results for verbal predicates of the in-domain
English dataset. The configurations are labeled as follows.
Factored ? = 1: our shortest-path system trained and tested
with ? = 1, similar to a pipeline system but without en-
forcing tree constraints and restricted to arc-factored features.
Factored best: our shortest-path system with the best results
from table 3. Non-factored: an equivalent pipeline system
that includes both factored and non-factored features.
gorithms that, given a predicate and a role, retrieve
the most likely argument and its path.
In the experimental section we prove the fea-
sibility of the approach. We observe that arc-
factored models are in fact more restricted, with a
drop in accuracy with respect to unrestricted mod-
els. However, we also observe that our method
largely improves the robustness of the arc-factored
method when training with a degree of syntac-
tic variability. Overall, ours is a simple strategy
to bring arc-factored models close to the perfor-
mance of unrestricted models. Future work should
explore further approaches to parse partial syntac-
tic structure specific to some target semantic rela-
tions.
Acknowledgments
This work was financed by the European Com-
mission for the XLike project (FP7-288342); and
by the Spanish Government for projects Tacardi
(TIN2012-38523-C02-00) and Skater (TIN2012-
38584-C06-01). For a large part of this work
Xavier Carreras was at the Universitat Polit`ecnica
de Catalunya under a Ram?on y Cajal contract
(RYC-2008-02223).
References
Xavier Carreras and Llu??s M`arquez. 2005. Intro-
duction to the CoNLL-2005 shared task: Semantic
role labeling. In Proceedings of the Ninth Confer-
ence on Computational Natural Language Learning
(CoNLL-2005), pages 152?164, Ann Arbor, Michi-
gan, June. Association for Computational Linguis-
tics.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings
of the 2002 Conference on Empirical Methods in
434
Natural Language Processing, pages 1?8. Associ-
ation for Computational Linguistics, July.
Thomas H. Cormen, Charles E. Leiserson, Ronald L.
Rivest, and Clifford Stein. 2009. Introduction to
Algorithms. The MIT Press.
Edsger W. Dijkstra. 1959. A note on two problems
in connexion with graphs. Numerische Mathematik,
1(1):269?271.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational Linguis-
tics, 28(3):245?288, September.
Daniel Gildea and Martha Palmer. 2002. The necessity
of parsing for predicate argument recognition. In
Proceedings of 40th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 239?246,
Philadelphia, Pennsylvania, USA, July. Association
for Computational Linguistics.
Jan Haji?c, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Ant`onia Mart??, Llu??s
M`arquez, Adam Meyers, Joakim Nivre, Sebastian
Pad?o, Jan
?
St?ep?anek, Pavel Stra?n?ak, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The CoNLL-
2009 shared task: Syntactic and semantic depen-
dencies in multiple languages. In Proceedings of
the 13th Conference on Computational Natural Lan-
guage Learning (CoNLL-2009), June 4-5, Boulder,
Colorado, USA.
James Henderson, Paola Merlo, Gabriele Musillo, and
Ivan Titov. 2008. A latent variable model of syn-
chronous parsing for syntactic and semantic depen-
dencies. In Proceedings of CoNLL-2008 Shared
Task.
Richard Johansson and Pierre Nugues. 2008.
Dependency-based syntactic?semantic analysis with
propbank and nombank. In CoNLL 2008: Pro-
ceedings of the Twelfth Conference on Computa-
tional Natural Language Learning, pages 183?187,
Manchester, England, August. Coling 2008 Orga-
nizing Committee.
Richard Johansson. 2009. Statistical bistratal depen-
dency parsing. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing, pages 561?569, Singapore, August. As-
sociation for Computational Linguistics.
Terry Koo, Amir Globerson, Xavier Carreras, and
Michael Collins. 2007. Structured prediction mod-
els via the matrix-tree theorem. In Proceedings
of the 2007 Joint Conference on Empirical Meth-
ods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 141?150, Prague, Czech Republic,
June. Association for Computational Linguistics.
Xavier Llu??s, Xavier Carreras, and Llu??s M`arquez.
2013. Joint Arc-factored Parsing of Syntactic and
Semantic Dependencies. Transactions of the As-
sociation for Computational Linguistics (TACL),
1(1):219?230, May.
Llu??s M`arquez, Xavier Carreras, Kenneth C.
Litkowski, and Suzanne Stevenson. 2008. Semantic
Role Labeling: An Introduction to the Special Issue.
Computational Linguistics, 34(2):145?159, June.
Alessandro Moschitti. 2004. A study on convolution
kernels for shallow statistic parsing. In Proceedings
of the 42nd Meeting of the Association for Compu-
tational Linguistics (ACL?04), Main Volume, pages
335?342, Barcelona, Spain, July.
Mihai Surdeanu, Llu??s M`arquez, Xavier Carreras, and
Pere R. Comas. 2007. Combination strategies for
semantic role labeling. Journal of Artificial Intelli-
gence Research.
Nianwen Xue and Martha Palmer. 2004. Calibrat-
ing features for semantic role labeling. In Dekang
Lin and Dekai Wu, editors, Proceedings of EMNLP
2004, pages 88?94, Barcelona, Spain, July. Associ-
ation for Computational Linguistics.
Hai Zhao, Wenliang Chen, Jun?ichi Kazama, Kiyotaka
Uchimoto, and Kentaro Torisawa. 2009. Multi-
lingual dependency learning: Exploiting rich fea-
tures for tagging syntactic and semantic dependen-
cies. In Proceedings of the Thirteenth Confer-
ence on Computational Natural Language Learning
(CoNLL 2009): Shared Task, pages 61?66, Boulder,
Colorado, June. Association for Computational Lin-
guistics.
435
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 436?442,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Semantic Kernels for Semantic Parsing
Iman Saleh
Faculty of Computers and Information
Cairo University
iman.saleh@fci-cu.edu.eg
Alessandro Moschitti, Preslav Nakov,
Llu??s M
`
arquez, Shafiq Joty
ALT Research Group
Qatar Computing Research Institute
{amoschitti,pnakov,lmarquez,sjoty}@qf.org.qa
Abstract
We present an empirical study on the use
of semantic information for Concept Seg-
mentation and Labeling (CSL), which is
an important step for semantic parsing.
We represent the alternative analyses out-
put by a state-of-the-art CSL parser with
tree structures, which we rerank with a
classifier trained on two types of seman-
tic tree kernels: one processing structures
built with words, concepts and Brown
clusters, and another one using semantic
similarity among the words composing the
structure. The results on a corpus from the
restaurant domain show that our semantic
kernels exploiting similarity measures out-
perform state-of-the-art rerankers.
1 Introduction
Spoken Language Understanding aims to inter-
pret user utterances and to convert them to logical
forms or, equivalently, to database queries, which
can then be used to satisfy the user?s information
needs. This process is known as Concept Segmen-
tation and Labeling (CSL), also called semantic
parsing in the speech community: it maps utter-
ances into meaning representations based on se-
mantic constituents. The latter are basically word
sequences, often referred to as concepts, attributes
or semantic tags. CSL makes it easy to convert
spoken questions such as ?cheap lebanese restau-
rants in doha with take out? into database queries.
First, a language-specific semantic parser tok-
enizes, segments and labels the question:
[
Price
cheap] [
Cuisine
lebanese] [
Other
restaurants in]
[
City
doha] [
Other
with] [
Amenity
take out]
Then, label-specific normalizers are applied to
the segments, with the option to possibly relabel
mislabeled segments:
[
Price
low] [
Cuisine
lebanese] [
City
doha] [
Amenity
carry out]
Finally, a database query is formed from the list
of labels and values, and is then executed against
the database, e.g., MongoDB; a backoff mecha-
nism may be used if the query has not succeeded.
{$and [{cuisine:"lebanese"},{city:"doha"},
{price:"low"},{amenity:"carry out"}]}
The state-of-the-art of CSL is represented by
conditional models for sequence labeling such as
Conditional Random Fields (CRFs) (Lafferty et
al., 2001) trained with simple morphological and
lexical features. The basic CRF model was im-
proved by means of reranking (Moschitti et al.,
2006; Dinarelli et al., 2012) using structural ker-
nels (Moschitti, 2006). Although these meth-
ods exploited sentence structure, they did not use
syntax at all. More recently, we applied shal-
low syntactic structures and discourse parsing with
slightly better results (Saleh et al., 2014). How-
ever, the most obvious models for semantic pars-
ing, i.e., rerankers based on semantic structural
kernels (Bloehdorn and Moschitti, 2007b), had not
been applied to semantic structures yet.
In this paper, we study the impact of semantic
information conveyed by Brown Clusters (BCs)
(Brown et al., 1992) and semantic similarity, while
also combining them with innovative features. We
use reranking, similarly to (Saleh et al., 2014),
to select the best hypothesis annotated with con-
cepts predicted by a local model. The competing
hypotheses are represented as innovative trees en-
riched with the semantic concepts and BC labels.
The trees can capture dependencies between sen-
tence constituents, concepts and BCs. However,
extracting explicit features from them is rather
difficult as their number is exponentially large.
Thus, we rely on (i) Support Vector Machines
(Joachims, 1999) to train the reranking functions
and on (ii) structural kernels (Moschitti, 2010;
Moschitti, 2012; Moschitti, 2013) to automatically
encode tree fragments that represent syntactic and
semantic dependencies from words and concepts.
436
(a) Semantic Kernel Structure (SKS)
(b) SKS with Brown Clusters
Figure 1: CSL structures: standard and with Brown Clusters.
We further apply a semantic kernel (SK),
namely the Smoothed Partial Tree Kernel (Croce
et al., 2011), which uses the lexical similarity be-
tween the tree nodes, while computing the sub-
structure space. This is the first time that SKs are
applied to reranking hypotheses. This (i) makes
the global sentence structure along with concepts
available to the learning algorithm, and (ii) enables
computing the similarity between lexicals in syn-
tactic patterns that are enriched by concepts.
We tested our models on the Restaurant do-
main. Our results show that: (i) The basic CRF
parser, which uses semi-Markov CRF, or semi-
CRF (Sarawagi and Cohen, 2004), is already very
accurate; it achieves F
1
scores over 83%, mak-
ing any further improvement very hard. (ii) The
upper-bound performance of the reranker is very
high as well, i.e., the correct annotation is gen-
erated in the list of the first 100 hypotheses in
98.72% of the cases. (iii) SKs significantly im-
prove over the semi-CRF baseline and our pre-
vious state-of-the-art reranker exploiting shallow
syntactic patterns (Saleh et al., 2014), as shown
by extensive comparisons using several systems.
(iv) Making BCs effective requires a deeper study.
2 Related Work
One of the early approaches to CSL was that
of Pieraccini et al. (1991), where the word se-
quences and concepts were modeled using Hid-
den Markov Models (HMMs) as observations and
hidden states, respectively. Generative models
were exploited by Seneff (1989) and Miller et
al. (1994), who used stochastic grammars for
CSL. Other discriminative models followed such
preliminary work, e.g., (Rubinstein and Hastie,
1997; Santaf?e et al., 2007; Raymond and Riccardi,
2007). CRF-based models are considered to be the
state of the art in CSL (De Mori et al., 2008).
Another relevant line of research are the seman-
tic kernels, i.e., kernels that use lexical similarity
between features. One of the first that applyed
LSA was (Cristianini et al., 2002), whereas (Bloe-
hdorn et al., 2006; Basili et al., 2006) used Word-
Net. Semantic structural kernels of the type we
use in this paper were first introduced in (Bloe-
hdorn and Moschitti, 2007a; Bloehdorn and Mos-
chitti, 2007b). The most advanced model based on
tree kernels, which we also use in this paper, is the
Smoothed PTK (Croce et al., 2011).
3 Reranking for CSL
Reranking is applied to a list of N annotation hy-
potheses, which are generated and sorted by the
probability to be globally correct as estimated us-
ing local classifiers or global classifiers that only
use local features. Then, a reranker, typically a
meta-classifier, tries to select the best hypothe-
sis from the list. The reranker can exploit global
information, and specifically, the dependencies
between the different concepts, which are made
available by the local model. We use semi-CRFs
for the local model as they yield the highest ac-
curacy in CSL (when using a single model) and
preference reranking for the global reranker.
3.1 Preference Reranking (PR)
PR uses a classifier C, which takes a pair of hy-
potheses ?H
i
, H
j
? and decides whether H
i
is bet-
ter than H
j
. Given a training question Q, posi-
tive and negative examples are built for training
the classifier. Let H
1
be the hypothesis with the
lowest error rate with respect to the gold standard
among all hypotheses generated for question Q.
We adopt the following approach for example gen-
eration: the pairs ?H
1
, H
i
? (i = 2, 3, . . . , N ) are
positive examples, while ?H
i
, H
1
? are considered
negative.
437
At testing time, given a new question Q
?
, C clas-
sifies all pairs ?H
i
, H
j
? generated from the anno-
tation hypotheses of Q
?
: a positive classification is
a vote for H
i
, otherwise the vote is for H
j
, where
the classifier score can be used as a weighted vote.
H
k
are then ranked according to the number (sum)
of the votes (weighted by score) they receive.
We build our reranker with SVMs using the
following kernel: K(?H
1
, H
2
?, ?H
?
1
, H
?
2
?) =
?(?H
1
, H
2
?) ? ?(?H
?
1
, H
?
2
?) ,
(
?(H
1
) ?
?(H
2
)
)
?
(
?(H
?
1
) ? ?(H
?
2
)
)
= ?(H
1
)?(H
?
1
) +
?(H
2
)?(H
?
2
) ? ?(H
1
)?(H
?
2
) ? ?(H
2
)?(H
?
1
) =
S(H
1
, H
?
1
) + S(H
2
, H
?
2
) ? S(H
1
, H
?
2
) ?
S(H
2
, H
?
1
). We consider H as a tuple ?T,~v? com-
posed of a tree T and a feature vector ~v. Then, we
define S(H,H
?
) = S
TK
(T, T
?
)+S
v
(~v,~v
?
), where
S
TK
computes one of the tree kernel functions
defined in 3.2 and 3.3; and S
v
is a kernel (see 3.4),
e.g., linear, polynomial, Gaussian, etc.
3.2 Tree kernels (TKs)
TKs measure the similarity between two structures
in terms of the number of substructures they share.
We use two types of tree kernels: (i) Partial Tree
Kernel (PTK), which can be effectively applied
to both constituency and dependency parse trees
(Moschitti, 2006). It generates all possible con-
nected tree fragments, e.g., sibling nodes can be
also separated and can be part of different tree
fragments: a fragment is any possible tree path,
and other tree paths are allowed to depart from its
nodes. Thus, it can generate a very rich feature
space. (ii) The smoothed PTK or semantic kernel
(SK) (Croce et al., 2011), which extends PTK by
allowing soft matching (i.e., via similarity compu-
tation) between nodes associated with different but
related lexical items. The node similarity can be
derived from manually annotated resources, e.g.,
WordNet or Wikipedia, as well as using corpus-
based clustering approaches, e.g., latent semantic
analysis (LSA), as we do in this paper.
3.3 Semantic structures
Tree kernels allow us to compute structural simi-
larities between two trees; thus, we engineered a
special structure for the CSL task. In order to cap-
ture the structural dependencies between the se-
mantic tags,
1
we use a basic tree (see for exam-
ple Figure 1a), where the words of a sentence are
tagged with their semantic tags.
1
They are associated with the following IDs: 0-Other,
1-Rating, 2-Restaurant, 3-Amenity, 4-Cuisine, 5-Dish, 6-
Hours, 7-Location, and 8-Price.
More specifically, the words in the sentence
constitute the leaves of the tree, which are in
turn connected to the pre-terminals containing
the semantic tags in BIO notation (?B?=begin,
?I?=inside, ?O?=outside). The BIO tags are then
generalized in the upper level, and joined to the
Root node. Additionally, part-of-speech (POS)
tags
2
are added to each word by concatenating
it with the string ?::L?, where L is the first let-
ter of the POS-tags of the words, e.g., along, my
and route, receive i, p and n, which are the first
letters of the POS-tags IN, PRN and NN, respec-
tively. SK applied to the above structure can gen-
erate powerful semantic patterns such as [Root
[4-Cuisine [similar to(stake house)]][7-Loc [simi-
lar to(within a mile)]]], e.g., for correctly labeling
new clauses like Pizza Parlor in three kilometers.
The BC labels, represented as cluster IDs, are sim-
ply added as siblings of words as shown in Fig. 1b.
3.4 Feature Vectors
For the sake of comparison, we also devoted
some effort towards engineering a set of features
to be used in a flat feature-vector representation.
These features can be used in isolation to learn
the reranking function, or in combination with the
kernel-based approach (as a composite kernel us-
ing a linear combination). They belong to the fol-
lowing four categories: (i) CRF-based: these in-
clude the basic features used to train the initial
semi-CRF model; (ii) n-gram based: we collected
3- and 4-grams of the output label sequence at
the level of concepts, with artificial tags inserted
to identify the start (?S?) and end (?E?) of the se-
quence.
3
(iii) Probability-based, computing the
probability of the label sequence as an average of
the probabilities at the word level in the N -best
list; and (iv) DB-based: a single feature encoding
the number of results returned from the database
when constructing a query using the conjunction
of all semantic segments in the hypothesis.
4 Experiments
The experiments aim at investigating the role of
feature vectors, PTK, SK and BCs in reranking.
We first describe the experimental setting and then
we move into the analysis of the results.
2
We use the Stanford tagger (Toutanova et al., 2003).
3
For instance, if the output sequence is Other-Rating-
Other-Amenity the 3-gram patterns would be: S-Other-
Rating, Other-Rating-Other, Rating-Other-Amenity, and
Other-Amenity-E.
438
Train Devel. Test Total
semi-CRF 6,922 739 1,521 9,182
Reranker 7,000 3,695 7,605 39,782
Table 1: Number of instances and pairs used to
train the semi-CRF and rerankers, respectively.
4.1 Experimental setup
Dataset. In our experiments, we used questions
annotated with semantic tags, which were col-
lected through crowdsourcing on Amazon Me-
chanical Turk and made available
4
by McGraw et
al. (2012). We split the dataset into training, de-
velopment and test sets. Table 1 shows the num-
ber of examples and example pairs we used for
the semi-CRF and the reranker, respectively. We
subsequently split the training data randomly into
10 folds. We used cross-validation, i.e., iteratively
training with 9 folds and annotating the remaining
fold, in order to generate the N -best lists of hy-
potheses for the entire training dataset. We com-
puted the 100-best hypotheses for each example.
We then used the development dataset to test and
tune the hyper-parameters of our reranking model.
The results on the development set, which we will
present in Section 4.2 below, were obtained us-
ing semi-CRF and reranking models trained on the
training set.
Data representation. Each hypothesis is repre-
sented by a semantic tree, a feature vector (ex-
plained in Section 3), and two extra features:
(i) the semi-CRF probability of the hypothesis,
and (ii) its reciprocal rank in the N -best list.
Learning algorithm. We used the SVM-Light-
TK
5
to train the reranker with a combination of
tree kernels and feature vectors (Moschitti, 2006;
Joachims, 1999). We used the default parameters
and a linear kernel for the feature vectors. As a
baseline, we picked the best-scoring hypothesis in
the list, i.e., the output by the regular semi-CRF
parser. The setting is exactly the same as that de-
scribed in (Saleh et al., 2014).
Evaluation measure. In all experiments, we used
the harmonic mean of precision and recall (F
1
)
(van Rijsbergen, 1979), computed at the token
level and micro-averaged across the different se-
mantic types.
6
4
http://groups.csail.mit.edu/sls/downloads/restaurant/
5
http://disi.unitn.it/moschitti/Tree-Kernel.htm
6
We do not consider ?Other? to be a semantic type; thus,
we did not include it in the F
1
calculation.
N 1 2 5 10 100
F
1
83.03 87.76 92.63 95.23 98.72
Table 2: Oracle F
1
score for N -best lists.
Brown Clusters. Clustering groups of similar
words together provides a way of generalizing
them. In this work, we explore the use of Brown
clusters (Brown et al., 1992) in both feature vec-
tors and tree kernels. The Brown clustering al-
gorithm uses an n-gram class model. It first as-
signs each word to a distinct cluster, and then it
merges different clusters in a bottom-up fashion.
The merge step is done in a way that minimizes the
loss in average mutual information between clus-
ters. The outcome is hierarchical clustering, which
we use in our reranking algorithm. To create the
Brown clusters, we used the Yelp dataset of re-
views.
7
It contains 335,022 reviews about 15,585
businesses; 5,575 of the businesses and 233,839 of
the reviews are restaurant-related. This dataset is
very similar to the dataset of queries about restau-
rants we use in our experiments.
Similarity matrix for SK. We compute the lexi-
cal similarity for SK by applying LSA (Furnas et
al., 1988) to Tripadvisor data. The dataset and the
exact procedure for creating the LSA matrix are
described in (Castellucci et al., 2013; Croce and
Previtali, 2010).
4.2 Results
Oracle accuracy. Table 2 shows the oracle F
1
score for N -best lists of different lengths, i.e., the
F
1
that is achieved by picking the best candidate
in the N -best list for various values of N . Con-
sidering 5-best lists yields an increase in oracle F
1
of almost ten absolute points. Going up to 10-best
lists only adds 2.5 extra F
1
points. The complete
100-best lists add 3.5 extra F
1
points, for a total
of 98.72. This very high value is explained by the
fact that often the total number of different anno-
tations for a given question is smaller than 100. In
our experiments, we will focus on 5-best lists.
Baseline accuracy. We computed F
1
for the semi-
CRF model on both the development and the test
sets, obtaining 83.86 and 83.03, respectively.
Learning Curves. The semantic information in
terms of BCs or semantic similarity derived by
LSA can have a major impact in case of data
scarcity. Therefore, we trained our reranking mod-
els with increasing sizes of training data.
7
http://www.yelp.com/dataset challenge/
439
Development set
79	 ?
80	 ?
81	 ?
82	 ?
83	 ?
84	 ?
85	 ?
86	 ?
0	 ? 1000	 ? 2000	 ? 3000	 ? 4000	 ? 5000	 ? 6000	 ? 7000	 ?
F1-??m
easu
re	 ?
Training	 ?data	 ?size	 ?
PTK	 ? SK	 ?PTK+BC	 ? PTK+all	 ?PTK+BC+all	 ? Baseline	 ?
79	 ?
80	 ?
81	 ?
82	 ?
83	 ?
84	 ?
85	 ?
86	 ?
0	 ? 1000	 ? 2000	 ? 3000	 ? 4000	 ? 5000	 ? 6000	 ? 7000	 ?
F1-??m
easu
re	 ?
Training	 ?data	 ?size	 ?
PTK	 ? SK	 ?SK+BC	 ? PTK+all	 ?SK+all	 ? SK+BC+all	 ?Baseline	 ?
Test set
74	 ?
76	 ?
78	 ?
80	 ?
82	 ?
84	 ?
86	 ?
0	 ? 1000	 ? 2000	 ? 3000	 ? 4000	 ? 5000	 ? 6000	 ? 7000	 ?
F1-??m
easu
re	 ?
Training	 ?data	 ?size	 ?
PTK	 ? SK	 ?
PTK+BC	 ? PTK+all	 ?
PTK+BC+all	 ? Baseline	 ?
79	 ?
80	 ?
81	 ?
82	 ?
83	 ?
84	 ?
85	 ?
0	 ? 1000	 ? 2000	 ? 3000	 ? 4000	 ? 5000	 ? 6000	 ? 7000	 ?
F1-??m
easu
re	 ?
Training	 ?data	 ?size	 ?
PTK	 ? SK	 ?SK+BC	 ? PTK+all	 ?SK+all	 ? SK+BC+all	 ?Baseline	 ?
Figure 2: Learning curves for different reranking models on the development and on the testing sets.
The first two graphs in Fig. 2 show the plots
on the development set whereas the last two are
computed on the test set. The reranking models
reported are Baseline, PTK, PTK+BC, PTK+all
(features), PTK+BC+all, SK, SK+BC, SK+all and
SK+BC+all.
8
We can see that: (i) PTK alone, i.e.,
without semantic information, has the lowest ac-
curacy; (ii) BCs do not improve significantly any
model; (iii) SK almost always achieves the high-
est accuracy; (iv) PTK+all (i.e., the model also us-
ing features) improves on PTK, but its accuracy
is lower than for any model using SK, i.e., us-
ing semantic similarity; and (v) all features pro-
vide an initial boost to SK, but as soon as the data
increases, their impact decreases.
5 Conclusion and Future Work
In summary, the learning curves clearly show the
good generalization ability of SK, which improve
the CRF baseline using little data (?3,000). The
semantic kernel significantly improves over the
semi-CRF baseline and our previous state-of-the-
art reranker exploiting shallow syntactic patterns
(Saleh et al., 2014), which corresponds to PTK+all
in the above comparison.
8
Models are split between 2 plots in order to ease reading.
The improvement falls between 1-2 absolute
percent points. This is remarkable as (i) it corre-
sponds to ?10% relative error reduction, and (ii)
the state-of-the-art baseline system is very difficult
to beat, as confirmed by the low impact of tradi-
tional features and BCs. Although the latter can
generalize over concepts and words, their use is
not straightforward, resulting in no improvement.
In the future, we plan to investigate the use of
semantic similarity from distributional and other
sources (Mihalcea et al., 2006; Pad?o and Lapata,
2007), e.g., Wikipedia (Strube and Ponzetto, 2006;
Mihalcea and Csomai, 2007), Wiktionary (Zesch
et al., 2008), WordNet (Pedersen et al., 2004;
Agirre et al., 2009), FrameNet, VerbNet (Shi and
Mihalcea, 2005), BabelNet (Navigli and Ponzetto,
2010), and LSA, and for different domains.
Acknowledgments
This research is part of the Interactive sYstems
for Answer Search (Iyas) project, conducted by
the Arabic Language Technologies (ALT) group
at Qatar Computing Research Institute (QCRI)
within the Qatar Foundation. We would like to
thank Danilo Croce, Roberto Basili and Giuseppe
Castellucci for helping and providing us with the
similarity matrix for the semantic kernels.
440
References
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Pasca, and Aitor Soroa. 2009.
A study on similarity and relatedness using distribu-
tional and wordnet-based approaches. In Proceed-
ings of Human Language Technologies: The 2009
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 19?27, Boulder, Colorado, June.
Roberto Basili, Marco Cammisa, and Alessandro Mos-
chitti. 2006. A semantic kernel to classify texts with
very few training examples. Informatica (Slovenia),
30(2):163?172.
Stephan Bloehdorn and Alessandro Moschitti. 2007a.
Combined syntactic and semantic kernels for text
classification. In Advances in Information Retrieval
- Proceedings of the 29th European Conference on
Information Retrieval (ECIR 2007), pages 307?318,
Rome, Italy.
Stephan Bloehdorn and Alessandro Moschitti. 2007b.
Structure and semantics for expressive text kernels.
In Proceedings of the 16th ACM Conference on
Information and Knowledge Management (CIKM
2007), pages 861?864, Lisbon, Portugal.
Stephan Bloehdorn, Roberto Basili, Marco Cammisa,
and Alessandro Moschitti. 2006. Semantic kernels
for text classification based on topological measures
of feature similarity. In Proceedings of the 6th IEEE
International Conference on Data Mining (ICDM
06), pages 808?812, Hong Kong.
Peter F. Brown, Peter V. deSouza, Robert L. Mer-
cer, Vincent J. Della Pietra, and Jenifer C. Lai.
1992. Class-based n-gram models of natural lan-
guage. Computational Linguistics, 18(4):467?479.
Giuseppe Castellucci, Simone Filice, Danilo Croce,
and Roberto Basili. 2013. UNITOR: Combining
Syntactic and Semantic Kernels for Twitter Senti-
ment Analysis. In Second Joint Conference on Lex-
ical and Computational Semantics (*SEM), Volume
2: Proceedings of the Seventh International Work-
shop on Semantic Evaluation (SemEval 2013), pages
369?374, Atlanta, Georgia, USA.
Nello Cristianini, John Shawe-Taylor, and Huma
Lodhi. 2002. Latent Semantic Kernels. Journal
of Intelligent Information Systems, 18(2):127?152.
Danilo Croce and Daniele Previtali. 2010. Mani-
fold learning for the semi-supervised induction of
framenet predicates: An empirical investigation. In
Proceedings of the 2010 Workshop on GEometrical
Models of Natural Language Semantics, pages 7?16,
Uppsala, Sweden.
Danilo Croce, Alessandro Moschitti, and Roberto
Basili. 2011. Structured lexical similarity via con-
volution kernels on dependency trees. In Proceed-
ings of the 2011 Conference on Empirical Methods
in Natural Language Processing, pages 1034?1046,
Edinburgh, Scotland, UK.
Renato De Mori, Frederic B?echet, Dilek Hakkani-T?ur,
Michael McTear, Giuseppe Riccardi, and Gokhan
Tur. 2008. Spoken Language Understanding. IEEE
Signal Processing Magazine, 25:50?58.
Marco Dinarelli, Alessandro Moschitti, and Giuseppe
Riccardi. 2012. Discriminative reranking for
spoken language understanding. IEEE Transac-
tions on Audio, Speech and Language Processing,
20(2):526?539.
G. W. Furnas, S. Deerwester, S. T. Dumais, T. K. Lan-
dauer, R. A. Harshman, L. A. Streeter, and K. E.
Lochbaum. 1988. Information retrieval using a sin-
gular value decomposition model of latent semantic
structure. In Proceedings of the 11th annual inter-
national ACM SIGIR conference on Research and
development in information retrieval (SIGIR ?88),
pages 465?480, New York, USA.
Thorsten Joachims. 1999. Making large-scale SVM
learning practical. In B. Schlkopf, C. Burges, and
A. Smola, editors, Advances in Kernel Methods -
Support Vector Learning. MIT Press, Cambridge,
MA, USA.
John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling se-
quence data. In Proceedings of the Eighteenth Inter-
national Conference on Machine Learning (ICML
2001), pages 282?289, Williamstown, MA, USA.
Ian McGraw, Scott Cyphers, Panupong Pasupat,
Jingjing Liu, and Jim Glass. 2012. Automating
crowd-supervised learning for spoken language sys-
tems. In Proceedings of the 13th Annual Conference
of the International Speech Communication Asso-
ciation (INTERSPEECH 2012), pages 2473?2476,
Portland, OR, USA.
Rada Mihalcea and Andras Csomai. 2007. Wikify!
linking documents to encyclopedic knowledge. In
Proceedings of the sixteenth ACM conference on
Conference on information and knowledge manage-
ment (CIKM 2007), pages 233?242, Lisbon, Portu-
gal.
Rada Mihalcea, Courtney Corley, and Carlo Strappa-
rava. 2006. Corpus-based and knowledge-based
measures of text semantic similarity. In Proceed-
ings of the 21st National Conference on Artificial In-
telligence - Volume 1 (AAAI 2006), pages 775?780,
Boston, MA, USA.
Scott Miller, Richard Schwartz, Robert Bobrow, and
Robert Ingria. 1994. Statistical Language Process-
ing using Hidden Understanding Models. In Pro-
ceedings of the workshop on Human Language Tech-
nology (HLT 1994), pages 278?282, Plainsboro, NJ,
USA.
Alessandro Moschitti, Daniele Pighin, and Roberto
Basili. 2006. Semantic role labeling via tree kernel
441
joint inference. In Proceedings of the Tenth Confer-
ence on Computational Natural Language Learning
(CoNLL-X), pages 61?68, New York City, USA.
Alessandro Moschitti. 2006. Efficient convolution ker-
nels for dependency and constituent syntactic trees.
In Proceedings of the 17th European Conference on
Machine Learning (ECML 2006), pages 318?329,
Berlin, Germany.
Alessandro Moschitti. 2010. Kernel engineering
for fast and easy design of natural language ap-
plications. In Coling 2010: Kernel Engineering
for Fast and Easy Design of Natural Language
Applications?Tutorial notes, pages 1?91, Beijing,
China.
Alessandro Moschitti. 2012. State-of-the-art kernels
for natural language processing. In Tutorial Ab-
stracts of ACL 2012, page 2, Jeju Island, Korea.
Alessandro Moschitti. 2013. Kernel-based learning to
rank with syntactic and semantic structures. In Tu-
torial abstracts of the 36th Annual ACM SIGIR Con-
ference, page 1128, Dublin, Ireland.
Roberto Navigli and Simone Paolo Ponzetto. 2010.
Babelnet: Building a very large multilingual seman-
tic network. In Proceedings of the 48th annual meet-
ing of the association for computational linguistics
(ACL 2010), pages 216?225, Uppsala, Sweden.
Sebastian Pad?o and Mirella Lapata. 2007.
Dependency-based construction of semantic space
models. Computational Linguistics, 33(2):161?199.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. Wordnet::similarity - measuring the
relatedness of concepts. In HLT-NAACL 2004:
Demonstration Papers, pages 38?41, Boston, Mas-
sachusetts, USA.
Roberto Pieraccini, Esther Levin, and Chin-Hui Lee.
1991. Stochastic Representation of Conceptual
Structure in the ATIS Task. In Proceedings of the
Fourth Joint DARPA Speech and Natural Language
Workshop, pages 121?124, Los Altos, CA, USA.
Christian Raymond and Giuseppe Riccardi. 2007.
Generative and Discriminative Algorithms for Spo-
ken Language Understanding. In Proceedings
of the 8th Annual Conference of the Interna-
tional Speech Communication Association (INTER-
SPEECH 2007), pages 1605?1608, Antwerp, Bel-
gium, August.
Y. Dan Rubinstein and Trevor Hastie. 1997. Discrimi-
native vs Informative Learning. In Proceedings of
the Third International Conference on Knowledge
Discovery and Data Mining (KDD-1997), pages 49?
53, Newport Beach, CA, USA.
Iman Saleh, Scott Cyphers, Jim Glass, Shafiq Joty,
Llu??s M`arquez, Alessandro Moschitti, and Preslav
Nakov. 2014. A study of using syntactic and seman-
tic structures for concept segmentation and labeling.
In Proceedings of the 25th International Conference
on Computational Linguistics, COLING ?14, pages
193?202, Dublin, Ireland.
G. Santaf?e, J.A. Lozano, and P. Larra?naga. 2007.
Discriminative vs. Generative Learning of Bayesian
Network Classifiers. In Proceedings of the 9th Euro-
pean Conference on Symbolic and Quantitative Ap-
proaches to Reasoning with Uncertainty (ECSQARU
2007), pages 453?546, Hammamet, Tunisia.
Sunita Sarawagi and William W. Cohen. 2004. Semi-
markov conditional random fields for information
extraction. In Advances in Neural Information Pro-
cessing Systems 17 (NIPS 2004), Vancouver, British
Columbia, Canada.
Stephanie Seneff. 1989. TINA: A Probabilistic Syn-
tactic Parser for Speech Understanding Systems.
In Proceedings of the International Conference on
Acoustics, Speech, and Signal Processing (ICASSP-
89), pages 711?714, Glasgow, UK.
Lei Shi and Rada Mihalcea. 2005. Putting pieces to-
gether: Combining framenet, verbnet and wordnet
for robust semantic parsing. In Computational Lin-
guistics and Intelligent Text Processing, pages 100?
111. Springer Berlin Heidelberg.
Michael Strube and Simone Paolo Ponzetto. 2006.
Wikirelate! computing semantic relatedness using
wikipedia. In Proceedings of the 21st National Con-
ference on Artificial Intelligence (AAAI?06), pages
1419?1424, Boston, Massachusetts, USA.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of the 2003 Human Language Tech-
nology Conference of the North American Chap-
ter of the Association for Computational Linguis-
tics (HLT-NAACL 2003), pages 173?180, Edmon-
ton, Canada.
Cornelis J. van Rijsbergen. 1979. Information
Retrieval. Butterworth-Heinemann Newton, MA,
USA.
Torsten Zesch, Christof M?uller, and Iryna Gurevych.
2008. Using wiktionary for computing semantic re-
latedness. In Proceedings of the 23rd National Con-
ference on Artificial Intelligence (AAAI?08), pages
861?866, Chicago, Illinois,USA.
442
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 687?698,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Using Discourse Structure Improves Machine Translation Evaluation
Francisco Guzm
?
an Shafiq Joty Llu??s M
`
arquez and Preslav Nakov
ALT Research Group
Qatar Computing Research Institute ? Qatar Foundation
{fguzman,sjoty,lmarquez,pnakov}@qf.org.qa
Abstract
We present experiments in using dis-
course structure for improving machine
translation evaluation. We first design
two discourse-aware similarity measures,
which use all-subtree kernels to compare
discourse parse trees in accordance with
the Rhetorical Structure Theory. Then,
we show that these measures can help
improve a number of existing machine
translation evaluation metrics both at the
segment- and at the system-level. Rather
than proposing a single new metric, we
show that discourse information is com-
plementary to the state-of-the-art evalu-
ation metrics, and thus should be taken
into account in the development of future
richer evaluation metrics.
1 Introduction
From its foundations, Statistical Machine Transla-
tion (SMT) had two defining characteristics: first,
translation was modeled as a generative process at
the sentence-level. Second, it was purely statisti-
cal over words or word sequences and made lit-
tle to no use of linguistic information. Although
modern SMT systems have switched to a discrim-
inative log-linear framework, which allows for ad-
ditional sources as features, it is generally hard to
incorporate dependencies beyond a small window
of adjacent words, thus making it difficult to use
linguistically-rich models.
Recently, there have been two promising re-
search directions for improving SMT and its eval-
uation: (a) by using more structured linguistic
information, such as syntax (Galley et al, 2004;
Quirk et al, 2005), hierarchical structures (Chi-
ang, 2005), and semantic roles (Wu and Fung,
2009; Lo et al, 2012), and (b) by going beyond
the sentence-level, e.g., translating at the docu-
ment level (Hardmeier et al, 2012).
Going beyond the sentence-level is important
since sentences rarely stand on their own in a
well-written text. Rather, each sentence follows
smoothly from the ones before it, and leads into
the ones that come afterwards. The logical rela-
tionship between sentences carries important in-
formation that allows the text to express a meaning
as a whole beyond the sum of its separate parts.
Note that sentences can be made of several
clauses, which in turn can be interrelated through
the same logical relations. Thus, in a coherent text,
discourse units (sentences or clauses) are logically
connected: the meaning of a unit relates to that of
the previous and the following units.
Discourse analysis seeks to uncover this coher-
ence structure underneath the text. Several formal
theories of discourse have been proposed to de-
scribe the coherence structure (Mann and Thomp-
son, 1988; Asher and Lascarides, 2003; Webber,
2004). For example, the Rhetorical Structure The-
ory (Mann and Thompson, 1988), or RST, repre-
sents text by labeled hierarchical structures called
Discourse Trees (DTs), which can incorporate sev-
eral layers of other linguistic information, e.g.,
syntax, predicate-argument structure, etc.
Modeling discourse brings together the above
research directions (a) and (b), which makes it an
attractive goal for MT. This is demonstrated by the
establishment of a recent workshop dedicated to
Discourse in Machine Translation (Webber et al,
2013), collocated with the 2013 annual meeting of
the Association of Computational Linguistics.
The area of discourse analysis for SMT is still
nascent and, to the best of our knowledge, no
previous research has attempted to use rhetorical
structure for SMT or machine translation evalua-
tion. One possible reason could be the unavailabil-
ity of accurate discourse parsers. However, this
situation is likely to change given the most recent
advances in automatic discourse analysis (Joty et
al., 2012; Joty et al, 2013).
687
We believe that the semantic and pragmatic in-
formation captured in the form of DTs (i) can help
develop discourse-aware SMT systems that pro-
duce coherent translations, and (ii) can yield bet-
ter MT evaluation metrics. While in this work we
focus on the latter, we think that the former is also
within reach, and that SMT systems would bene-
fit from preserving the coherence relations in the
source language when generating target-language
translations.
In this paper, rather than proposing yet another
MT evaluation metric, we show that discourse
information is complementary to many existing
evaluation metrics, and thus should not be ignored.
We first design two discourse-aware similarity
measures, which use DTs generated by a publicly-
available discourse parser (Joty et al, 2012); then,
we show that they can help improve a number of
MT evaluation metrics at the segment- and at the
system-level in the context of the WMT11 and the
WMT12 metrics shared tasks (Callison-Burch et
al., 2011; Callison-Burch et al, 2012).
These metrics tasks are based on sentence-level
evaluation, which arguably can limit the benefits
of using global discourse properties. Fortunately,
several sentences are long and complex enough to
present rich discourse structures connecting their
basic clauses. Thus, although limited, this setting
is able to demonstrate the potential of discourse-
level information for MT evaluation. Furthermore,
sentence-level scoring (i) is compatible with most
translation systems, which work on a sentence-by-
sentence basis, (ii) could be beneficial to mod-
ern MT tuning mechanisms such as PRO (Hop-
kins and May, 2011) and MIRA (Watanabe et al,
2007; Chiang et al, 2008), which also work at
the sentence-level, and (iii) could be used for re-
ranking n-best lists of translation hypotheses.
2 Related Work
Addressing discourse-level phenomena in ma-
chine translation is relatively new as a research di-
rection. Some recent work has looked at anaphora
resolution (Hardmeier and Federico, 2010) and
discourse connectives (Cartoni et al, 2011; Meyer,
2011), to mention two examples.
1
However, so
far the attempts to incorporate discourse-related
knowledge in MT have been only moderately suc-
cessful, at best.
1
We refer the reader to (Hardmeier, 2012) for an in-depth
overview of discourse-related research for MT.
A common argument, is that current automatic
evaluation metrics such as BLEU are inadequate
to capture discourse-related aspects of translation
quality (Hardmeier and Federico, 2010; Meyer et
al., 2012). Thus, there is consensus that discourse-
informed MT evaluation metrics are needed in or-
der to advance research in this direction. Here we
suggest some simple ways to create such metrics,
and we also show that they yield better correlation
with human judgments.
The field of automatic evaluation metrics for
MT is very active, and new metrics are contin-
uously being proposed, especially in the context
of the evaluation campaigns that run as part of
the Workshops on Statistical Machine Transla-
tion (WMT 2008-2012), and NIST Metrics for
Machine Translation Challenge (MetricsMATR),
among others. For example, at WMT12, 12 met-
rics were compared (Callison-Burch et al, 2012),
most of them new.
There have been several attempts to incorpo-
rate syntactic and semantic linguistic knowledge
into MT evaluation. For instance, at the syn-
tactic level, we find metrics that measure the
structural similarity between shallow syntactic se-
quences (Gim?enez and M`arquez, 2007; Popovic
and Ney, 2007) or between constituency trees (Liu
and Gildea, 2005). In the semantic case, there are
metrics that exploit the similarity over named en-
tities and predicate-argument structures (Gim?enez
and M`arquez, 2007; Lo et al, 2012).
In this work, instead of proposing a new metric,
we focus on enriching current MT evaluation met-
rics with discourse information. Our experiments
show that many existing metrics can benefit from
additional knowledge about discourse structure.
In comparison to the syntactic and semantic ex-
tensions of MT metrics, there have been very few
attempts to incorporate discourse information so
far. One example are the semantics-aware metrics
of Gim?enez and M`arquez (2009) and Comelles et
al. (2010), which use the Discourse Representa-
tion Theory (Kamp and Reyle, 1993) and tree-
based discourse representation structures (DRS)
produced by a semantic parser. They calculate the
similarity between the MT output and references
based on DRS subtree matching, as defined in (Liu
and Gildea, 2005), DRS lexical overlap, and DRS
morpho-syntactic overlap. However, they could
not improve correlation with human judgments, as
evaluated on the MetricsMATR dataset.
688
Compared to the previous work, (i) we use a
different discourse representation (RST), (ii) we
compare discourse parses using all-subtree ker-
nels (Collins and Duffy, 2001), (iii) we evaluate
on much larger datasets, for several language pairs
and for multiple metrics, and (iv) we do demon-
strate better correlation with human judgments.
Wong and Kit (2012) recently proposed an
extension of MT metrics with a measure of
document-level lexical cohesion (Halliday and
Hasan, 1976). Lexical cohesion is achieved using
word repetitions and semantically similar words
such as synonyms, hypernyms, and hyponyms.
For BLEU and TER, they observed improved
correlation with human judgments on the MTC4
dataset when linearly interpolating these metrics
with their lexical cohesion score. Unlike their
work, which measures lexical cohesion at the
document-level, here we are concerned with co-
herence (rhetorical) structure, primarily at the
sentence-level.
3 Our Discourse-Based Measures
Our working hypothesis is that the similarity be-
tween the discourse structures of an automatic and
of a reference translation provides additional in-
formation that can be valuable for evaluating MT
systems. In particular, we believe that good trans-
lations should tend to preserve discourse relations.
As an example, consider the three discourse
trees (DTs) shown in Figure 1: (a) for a reference
(human) translation, and (b) and (c) for transla-
tions of two different systems on the WMT12 test
dataset. The leaves of a DT correspond to con-
tiguous atomic text spans, called Elementary Dis-
course Units or EDUs (three in Figure 1a). Ad-
jacent spans are connected by certain coherence
relations (e.g., Elaboration, Attribution), forming
larger discourse units, which in turn are also sub-
ject to this relation linking. Discourse units linked
by a relation are further distinguished based on
their relative importance in the text: nuclei are
the core parts of the relation while satellites are
supportive ones. Note that the nuclearity and re-
lation labels in the reference translation are also
realized in the system translation in (b), but not
in (c), which makes (b) a better translation com-
pared to (c), according to our hypothesis. We ar-
gue that existing metrics that only use lexical and
syntactic information cannot distinguish well be-
tween (b) and (c).
In order to develop a discourse-aware evalua-
tion metric, we first generate discourse trees for
the reference and the system-translated sentences
using a discourse parser, and then we measure the
similarity between the two discourse trees. We de-
scribe these two steps below.
3.1 Generating Discourse Trees
In Rhetorical Structure Theory, discourse analysis
involves two subtasks: (i) discourse segmentation,
or breaking the text into a sequence of EDUs, and
(ii) discourse parsing, or the task of linking the
units (EDUs and larger discourse units) into la-
beled discourse trees. Recently, Joty et al (2012)
proposed discriminative models for both discourse
segmentation and discourse parsing at the sen-
tence level. The segmenter uses a maximum en-
tropy model that achieves state-of-the-art accuracy
on this task, having an F
1
-score of 90.5%, while
human agreement is 98.3%.
The discourse parser uses a dynamic Condi-
tional Random Field (Sutton et al, 2007) as a pars-
ing model in order to infer the probability of all
possible discourse tree constituents. The inferred
(posterior) probabilities are then used in a proba-
bilistic CKY-like bottom-up parsing algorithm to
find the most likely DT. Using the standard set
of 18 coarse-grained relations defined in (Carlson
and Marcu, 2001), the parser achieved an F
1
-score
of 79.8%, which is very close to the human agree-
ment of 83%. These high scores allowed us to de-
velop successful discourse similarity metrics.
2
3.2 Measuring Similarity
A number of metrics have been proposed to mea-
sure the similarity between two labeled trees, e.g.,
Tree Edit Distance (Tai, 1979) and Tree Kernels
(Collins and Duffy, 2001; Moschitti and Basili,
2006). Tree kernels (TKs) provide an effective
way to integrate arbitrary tree structures in kernel-
based machine learning algorithms like SVMs.
In the present work, we use the convolution TK
defined in (Collins and Duffy, 2001), which effi-
ciently calculates the number of common subtrees
in two trees. Note that this kernel was originally
designed for syntactic parsing, where the subtrees
are subject to the constraint that their nodes are
taken with either all or none of the children. This
constraint of the TK imposes some limitations on
the type of substructures that can be compared.
2
The discourse parser is freely available from
http://alt.qcri.org/tools/
689
ElaborationROOT
SPAN NucleusAttributionSatellite
Voices are coming from Germany , SPANSatellite SPANNucleus
suggesting that ECB be the last resort creditor .
(a) A reference (human) translation.
	

 	
		Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 394?401,
Baltimore, Maryland USA, June 26?27, 2014.
c?2014 Association for Computational Linguistics
IPA and STOUT: Leveraging Linguistic and Source-based
Features for Machine Translation Evaluation
Meritxell Gonz
`
alez, Alberto Barr
?
on-Cede
?
no
TALP Research Center,
Technical University of Catalonia
{mgonzalez,albarron}@lsi.upc.edu
Llu??s M
`
arquez
Qatar Computing Research Institute
Qatar Foundation
lmarquez@qf.org.qa
Abstract
This paper describes the UPC submissions
to the WMT14 Metrics Shared Task: UPC-
IPA and UPC-STOUT. These metrics
use a collection of evaluation measures in-
tegrated in ASIYA, a toolkit for machine
translation evaluation. In addition to some
standard metrics, the two submissions take
advantage of novel metrics that consider
linguistic structures, lexical relationships,
and semantics to compare both source and
reference translation against the candidate
translation. The new metrics are available
for several target languages other than En-
glish. In the the official WMT14 evalua-
tion, UPC-IPA and UPC-STOUT scored
above the average in 7 out of 9 language
pairs at the system level and 8 out of 9 at
the segment level.
1 Introduction
Evaluating Machine Translation (MT) quality is a
difficult task, in which even human experts may
fail to achieve a high degree of agreement when
assessing translations. Conducting manual evalu-
ations is impractical during the development cy-
cle of MT systems or for transation applications
addressed to general users, such as online transla-
tion portals. Automatic evaluation measures bring
valuable benefits in such situations. Compared to
manual evaluation, automatic measures are cheap,
more objective, and reusable across different test
sets and domains.
Nonetheless, automatic metrics are far from
perfection: when used in isolation, they tend to
stress specific aspects of the translation quality and
neglect others (particularly during tuning); they
are often unable to capture little system improve-
ments (enhancements in very specific aspects of
the translation process); and they may make un-
fair comparisons when they are not able to reflect
real differences among the quality of different MT
systems (Gim?enez, 2008).
ASIYA, the core of our approach, is an open-
source suite for automatic machine translation
evaluation and output analysis.
1
It provides a rich
set of heterogeneous metrics and tools to evalu-
ate and analyse the quality of automatic transla-
tions. The ASIYA core toolkit was first released
in 2009 (Gim?enez and M`arquez, 2010a) and has
been continuously improved and extended since
then (Gonz`alez et al., 2012; Gonz`alez et al., 2013).
In this paper we first describe the most recent
enhancements to ASIYA: (i) linguistic-based met-
rics for French and German; (ii) an extended set
of source-based metrics for English, Spanish, Ger-
man, French, Russian, and Czech; and (iii) the in-
tegration of mechanisms to exploit the alignments
between sources and translations. These enhance-
ments are all available in ASIYA since version 3.0.
We have used them to prepare the UPC submis-
sions to the WMT14 Metrics Task: UPC-IPA and
UPC-STOUT, which serve the purpose of testing
their usefulness in a real comparative setting.
The rest of the paper is structured as follows.
Section 2 describes the new reference-based met-
rics developed, including syntactic parsers for lan-
guages other than English. Section 3 gives the
details of novel source-based metrics, developed
for almost all the language pairs in this challenge.
Section 4 explains our simple metrics combina-
tion strategy and analyses the results obtained with
both approaches, UPC-IPA and UPC-STOUT,
when applied to the WMT13 dataset. Finally, Sec-
tion 5 summarises our main contributions.
2 Reference-based Metrics
We recently added a new set of metrics to ASIYA,
which estimate the similarity between reference
(ref ) and candidate (cand) translations. The met-
1
http://asiya.lsi.upc.edu
394
rics rely either on structural linguistic informa-
tion (Section 2.1), on a semantic mapping (Sec-
tion 2.2), or on word n-grams (Section 2.3).
2.1 Parsing-based Metrics
Our initial set of parsing-based metrics is a follow-
up of the proposal by Gim?enez and M`arquez
(2010b): it leverages the structural information
provided by linguistic processors to compute sev-
eral similarity cues between two analyzed sen-
tences. ASIYA includes plenty of metrics that cap-
ture syntactic and semantic aspects of a transla-
tion. New metrics based on linguistic structural
information for French and German and upgraded
versions of the parsers for English and Spanish are
available since version 3.0.
2
In the WMT14 evaluation, we opt for metrics
based on shallow parsing (SP), constituency pars-
ing (CP), and dependency parsing (DPm)
3
. Mea-
sures based on named entities (NE) and semantic
roles (SR) were used to analyse translations into
English as well. The nomenclature used below
follows the same patterns as in the ASIYA?s man-
ual (Gonz`alez and Gim?enez, 2014). The manual
describes every family of metrics in detail. Next,
we briefly depict the concrete metrics involved in
our submissions to the WMT14 Shared Task.
The set of SP metrics is available for English,
German, French, Spanish and Catalan. They
measure the lexical overlapping between parts-of-
speech elements in the candidate and reference
translations. For instance, SP-Op(VB) measures
the proportion of correctly translated verbs; and
the coarser SP-Op(*) averages the overlapping be-
tween the words for each part of speech. We also
use NIST (Doddington, 2002) to compute accu-
mulated scores over sequences of n = 1..5 parts
of speech (SP-pNIST).
Similarly, CP metrics analyse similarities be-
tween constituent parse trees associated to can-
didate and reference translations. For instance,
CP-STMi5 and CP-STM4 compute, respectively,
the proportion of (individual) length-5 and accu-
mulated up to length-4 matching sub-paths of the
syntactic tree (Liu and Gildea, 2005). CP-Oc(*)
computes the lexical overlap averaged over all the
phrase constituents. Constituent trees are obtained
using the parsers of Charniak and Johnson (2005),
2
Equivalent resources were previously available for En-
glish, Catalan, and Spanish.
3
ASIYA includes two dependency parsers; the m identifies
the metrics calculated using the MALT parser.
Bonsai v3.2 (Candito et al., 2010b), and Berke-
ley Parser (Petrov et al., 2006; Petrov and Klein,
2007) for English, French, and German, respec-
tively.
Measures based on dependency parsing (DPm)
? available for English and French thanks to
the MALT parser (Nivre et al., 2007)? capture
the similarities between dependency tree items
(i.e., heads and modifiers). The pre-trained mod-
els for French were obtained from the French
Treebank (Candito et al., 2010a) and used to
train the Bonsai parser, which in turn uses the
MALT parser. For instance, DPm-HWCM w-3 re-
trieves average accumulated proportion of match-
ing word-chains (Liu and Gildea, 2005) up
to length 3; and DPm-HWCMi c-3 computes
the proportion of matching category-chains of
length 3.
2.2 Explicit-Semantics Metric
Additionally, we borrowed a metric originally pro-
posed in the field of Information Retrieval: ex-
plicit semantic analysis (ESA) (Gabrilovich and
Markovitch, 2007). ESA is a similarity metric
that relies on a large corpus of general knowl-
edge to represent texts. Our knowledge corpora
are composed of ? 100K Wikipedia articles from
2010 for the following target languages: English,
French and German. In this case, ref and cand
translations are both mapped onto the Wikipedia
collection W . The similarities between each text
and every article a ? W are computed on the ba-
sis of the cosine measure in order to compose a
similarities vector that represents the text. That is:
~
ref = {sim(ref, a) ?a ?W} , (1)
~
cand = {sim(cand, a)?a ?W} . (2)
As the i-th elements in both
~
ref and
~
cand represent
the similarity of ref and cand sentences to a com-
mon article, the similarity between ref and cand
can be estimated by computing sim(
~
ref,
~
cand).
2.3 Language-Independent Resource-Free
Metric
We consider a simple characterisation based on
word n-grams. Texts are broken down into over-
lapping word sequences of length n, with 1-word
shifting. The similarity between cand and ref
is computed on the basis of the Jaccard coeffi-
cient (Jaccard, 1901). We used this metric for the
pairs English?Russian and Russian-English, con-
sidering n = 2 (NGRAM-jacTok2ngram). For the
395
rest of the pairs we opt for the character-n-gram
metrics described in Section 3.1, but they showed
no positive results in the English?Russian pair dur-
ing our tuning experiments.
3 Source-based Metrics
We enhance our evaluation module by including
a set of new metrics that compare the source text
against the translations. The metrics can be di-
vided into two subsets: those that do not require
any external resources (Section 3.1) and those that
depend on a parallel corpus (Section 3.2).
3.1 Language-Independent Resource-Free
Metrics
We opted for two characterisations that allow for
the comparison of texts across languages without
external resources nor language-related knowl-
edge ?as far as the languages use the same writ-
ing system.
4
The first characterisation is character n-grams;
proposed by McNamee and Mayfield (2004) for
cross-language information retrieval between Eu-
ropean languages. Texts are broken down into
overlapping character sequences of length n, with
1-character shifting. We opt for case-folded bi-
grams (NGRAM-cosChar2ngrams), as they al-
lowed for the best performance across all the pairs
(except for From/To Russian pairs) during tuning.
The second characterisation (NGRAM-
jacCognates) is based on the concept of
cognateness; originally proposed for bitexts
alignment (Simard et al., 1992). A word is a
pseudo-cognate candidate if (i) it has only letters
and |w| ? 4, (ii) it contains at least one digit, or
(iii) it is a single punctuation mark. src and cand
sentences are then represented as word vectors,
containing only those words fulfilling one of the
previous conditions. In the case of (i) the word is
cut down to its leading four characters only.
In both cases (character n-grams and cognate-
ness) cand translations are compared against src
sentences on the basis of the cosine similarity
measure.
3.2 Parallel-Corpus Metrics
We consider two metrics that make use of parallel
corpora: length factor and alignment.
4
Previous research showed that transliteration is a
good short-cut when dealing with different writing sys-
tems (Barr?on-Cede?no et al., 2014).
Table 1: Length factor parameters as estimated on
the WMT13 parallel corpora.
pair ? ? pair ? ?
en?cs 0.972 0.245 cs?en 1.085 0.273
en?de 1.176 0.926 de?en 0.961 0.463
en?fr 1.158 0.411 fr?en 0.914 0.313
en?ru 1.157 0.678 ru?en 1.069 0.668
The length factor (LeM) is rooted in the fact that
the length of a text and its translation tend to pre-
serve a certain length correlation. For instance,
translations from English into Spanish or French
tend to be longer than their source. Similar mea-
sures were proposed during the statistical machine
translation early days, both considering character-
and word-level lengths (Gale and Church, 1993;
Brown et al., 1991). Pouliquen et al. (2003) de-
fines the length factor as:
%(d
?
) = e
?0.5
(
|d
?
|
|d
q
|
??
?
)
2
, (3)
where ? and ? represent the mean and standard
deviation of the character lengths between trans-
lations of texts from L into L
?
. This is a stochas-
tic normal distribution that results in higher values
as the length of the target text approaches the ex-
pected value given the source. Table 1 includes
the values for each language pair, as estimated on
the WMT13 parallel corpora. Note that this metric
was not applied to Hindi?English since this lan-
guage pair was not present in the WMT13 chal-
lenge.
The last of our newly-added measures relies
on the word alignments calculated between the
sentence pairs src?cand and src?ref. We trained
alignment models for each language pair using the
Berkeley Aligner
5
, and devised three variants of
an ALGN metric, which compute: (i) the propor-
tion of aligned words between src and cand (AL-
GNs); (ii) the proportion of aligned words between
cand and ref, calculated as the combination of the
alignments src?cand and src?ref (ALGNr); and
(iii) the ratio of shared alignments between src?
cand and src?ref (ALGNp).
4 Experimental Results
The tuning and selection of the different met-
rics to build UPC-IPA and UPC-STOUT was
5
https://code.google.com/p/berkeleyaligner
396
conducted considering the WMT13 Metrics Task
dataset (Mach?a?cek and Bojar, 2013), and the re-
sources distributed for the WMT13 Translation
Task (Bojar et al., 2013). Table 2 gives a
complete list of these metrics grouped by fami-
lies. First, we calculated the Pearson?s correla-
tion with the human judgements for all the met-
rics in the current version of the ASIYA repos-
itory, including standard MT evaluation metrics,
such as METEOR (Denkowski and Lavie, 2011),
GTM (Melamed et al., 2003), -TERp-A (Snover
et al., 2009) (a variant of TER tuned towards ade-
quacy), WER (Nie?en et al., 2000) and PER (Till-
mann et al., 1997). We selected the best perform-
ing metrics (i.e., those resulting in high Pearson
coefficients) in each family across all the From/To
English translation language pairs, including the
newly developed measures ?even if they per-
formed poorly compared to others (see This is how
the UPC-STOUT metrics sets for both from En-
glish and To English translation pairs were com-
posed
6
(see Table 3).
Table 2: Metrics considered in the experiments
separated by families according to the type of
grammatical items they use.
1. -WER 17. DPm-HWCM r-1
2. -PER 18. DPm-Or(*)
3. -TERp-A 19. SR-Or(*)
4. METEOR-ex 20. SR-Or
5. METEOR-pa 21. SR-Orv(*)
6. GTM-3 22. SR-Orv
7. SP-Op(*) 23. NE-Oe(*)
8. SP-pNIST 24. NE-Oe(**)
9. CP-STMi-5 25. ESA
10. CP-STMi-2 26. NGRAM-jacTok2ngrams
11. CP-STMi-3 27. NGRAM-jacCognates
12. CP-STM-4 28. NGRAM-cosChar2ngrams
13. CP-Oc(*) 29. LeM
14. DPm-HWCM w-3 30. ALGNp
15. DPm-HWCM c-3 31. ALGNs
16. DPm-HWCMi c-3 32. ALGNr
Table 3: Metrics considered in each system.
7
BAS: 1?6 SYN: 7?18
SEM: 19?25 SRC: 26?32
IPA: 1?9, 25?31 STOUT: 1?32
6
Parser-based measures are not present in Czech nor Rus-
sian as target languages, ALGN is not available for French
pairs, and ESA is not applied to Russian as target.
The metric sets included in UPC-IPA are light
versions of the UPC-STOUT ones. They were
composed following different criteria, depending
on the translation direction. Parsing-based mea-
sures were already available in the previous ver-
sion of ASIYA when translating into English ?
they are known to be robust across domains and
are usually good indicators of translation qual-
ity (Gim?enez and M`arquez, 2007). So, in order
to assess the gain achieved with these measures
with respect the new ones, UPC-IPA neglects the
measures based on structural information obtained
from parsers. In contrast, this distinction was not
suitable for the From English pairs since the num-
ber of resources and measures varies for each lan-
guage. Hence, in this latter case, UPC-IPA used
only the subset of measures from UPC-STOUT
that required no or little resources.
In summary, when English is the target lan-
guage, UPC-IPA uses the baseline evaluation
metrics along with the length factor, alignments-
based metrics, character n-grams, and ESA. In ad-
dition to the above metrics, UPC-STOUT uses
the linguistic-based metrics over parsing trees,
named entities, and semantic roles. When English
is the source language, UPC-IPA relies on the
basic collection of metrics and character n-grams
only. UPC-STOUT includes the alignment-based
metrics, length factor, ESA, and the syntactic
parsers applied to both German and French.
In all cases (metric sets and language pairs),
the translation quality score is computed as the
uniformly-averaged linear combination (ULC) of
all the individual metrics for each sentence in the
testset. Its calculation implies the normalization
of heterogeneous scores (some of them are neg-
ative or unbounded), into the range [0, 1]. As a
consequence, the scores of UPC-IPA and UPC-
STOUT constitute a natural way of ranking dif-
ferent translations, rather than an overall quality
estimation measure. We opt for this linear combi-
nation for simplicity. The discussion below sug-
gests that a more sophisticated method for weight
tuning (e.g., relying on machine learning methods)
would be required for each language pair, domain
and/or task since different metric families perform
notably different for each subtask.
We complete our experimentation by eval-
uating more configurations: BAS, a baseline
7
These are the full sets of measures for each configura-
tion. However, each specific subset for From/To English can
vary slightly depending on the available resources.
397
Table 4: System-level Pearson correlation for automatic metrics over translations From/To English.
WMT13 en?fr en?de en?es en?cs en?ru fr?en de?en es?en cs?en ru?en
UPC-IPA 93.079 85.147 88.702 85.259 70.345 96.755 94.660 95.065 94.316 72.083
UPC-STOUT 94.274 90.193 73.314 84.743 70.544 96.916 96.208 96.704 96.666 74.050
BAS 92.502 84.251 90.051 86.584 67.655 95.777 96.506 95.98 96.539 71.536
SYN 95.68 87.297 96.965 n/a n/a 96.291 96.592 96.052 95.238 73.083
BAS+SYN 94.584 87.786 95.162 n/a n/a 96.684 97.057 96.101 96.402 72.800
SEM 89.735 83.647 35.694 95.067 n/a 95.629 96.601 98.021 96.595 76.158
BAS+SEM 92.254 87.005 47.321 89.107 n/a 96.337 97.534 97.568 97.371 74.804
SRC 14.465 -16.796 -22.466 -49.981 39.527 13.405 -51.371 71.64 -73.254 68.766
BAS+SRC 93.637 76.401 83.754 64.742 54.128 95.395 90.889 93.299 89.216 71.882
WMT13-Best 94.745 93.813 96.446 86.036 81.194 98.379 97.789 99.171 83.734 94.768
WMT13-Worst 78.787 -45.461 87.677 69.151 61.075 95.118 92.239 79.957 60.918 82.058
Table 5: Segment-level Kendall?s ? correlation for automatic metrics over translations From/To English.
WMT13 en?fr en?de en?es en?cs en?ru fr?en de?en es?en cs?en ru?en
UPC-IPA 18.625 14.901 17.057 7.805 15.132 22.832 25.769 26.907 21.207 19.904
UPC-STOUT 19.488 15.012 17.166 8.545 15.279 23.090 27.117 26.848 21.332 19.100
BAS 19.477 13.589 16.975 8.449 15.599 24.060 28.259 28.381 23.346 20.983
SYN 16.554 14.970 16.444 n/a n/a 22.365 24.289 23.889 20.232 17.679
BAS+SYN 19.112 16.016 18.122 n/a n/a 23.940 28.068 27.988 23.180 19.659
SEM 12.184 9.249 10.871 3.808 n/a 17.282 19.083 20.859 15.186 14.971
BAS+SEM 19.167 13.291 15.857 7.732 n/a 22.024 25.788 26.360 21.427 19.117
SRC 2.745 2.481 1.152 1.992 5.247 2.181 1.154 8.700 -4.023 16.267
BAS+SRC 18.32 13.017 15.698 7.666 13.619 22.292 24.948 26.780 17.603 20.707
WMT13-Best 21.897 19.459 20.699 11.283 18.899 26.836 29.565 24.271 21.665 25.584
WMT13-Worst 16.753 13.910 3.024 4.431 13.166 14.008 14.542 14.494 9.667 13.178
with standard and commonly used MT metrics;
SYN, the reference-based syntactic metrics; SEM,
the reference-based semantic metrics; SRC, the
source-based metrics; and the combination of
BAS with every other configuration: BAS+SYN,
BAS+SEM, and BAS+SRC. Their purpose is to
evaluate the contribution of the newly developed
sets of metrics with respect to the baseline. The
composition of the different configurations is sum-
marised in Tables 2 and 3.
Evaluation results are shown in Tables 4 and 5.
For each configuration and language pair, we show
the correlation coefficients obtained at the system-
and the segment-level, respectively. As customary
with the WMT13 dataset, Pearson correlation was
computed at the system-level, whereas Kendall?s
? was used to estimate segment-level rank correla-
tions. Additionally to the two submitted and seven
extra configurations, we include the coefficients
obtained with the Best and Worst systems reported
in the official WMT13 evaluation for each lan-
guage pair.
Although the results of our two submitted sys-
tems are not radically different to each other,
UPC-STOUT consistently outperforms UPC-
IPA. The currently available version of ASIYA, in-
cluding the new metrics, allows for a performance
close to the top-performing evaluation measures in
last year?s challenge, even with our na??ve combi-
nation strategy.
It is worth noting that no configuration be-
haves the same way throughout the different lan-
guages. In some cases (e.g., with the SRC config-
uration), the bad performance can be explained by
the weaknesses of the necessary resources when
computing certain metrics. When analysed in de-
tail, the cause can be ascribed to different metric
families in each case. As a result, it is clear that
specific configurations are necessary for evaluat-
ing different languages and domains. We plan to
approach these issues as part of our future work.
When looking at the system-level figures, one
can observe that the SEM set allows for a con-
siderable improvement over the baseline system.
The further inclusion of the SYN set ?when
available?, tends to increase the quality of the
estimations, mainly when English is the source
language. These properties impact on some of
the UPC-STOUT configurations. In contrast,
when looking at the segment-level scores, while
398
Table 6: System-level Pearson correlation results
in the WMT14 Metrics shared task
en?fr en?de en?cs en?ru
UPC-IPA 93.7 13.0 96.8 92.2
UPC-STOUT 93.8 14.8 93.8 92.1
WMT14-Best 95.9 19.8 98.8 94.2
WMT14-Worst 88.8 1.1 93.8 90.3
fr?en de?en hi?en cs?en ru?en
UPC-IPA 96.6 89.4 91.5 82.4 80.0
UPC-STOUT 96.8 91.4 89.8 94.7 82.5
WMT14-Best 98.1 94.2 97.6 99.3 86.1
WMT14-Worst 94.5 76.0 41.1 74.1 -41.7
the SYN measures still tend to provide some gain
over the baseline, the SEM ones do not. Finally, it
merits some attention the good results achieved by
the baseline for translations into English. We may
remark here that our baseline included also the
best performing state-of-the-art metrics, including
all the variants of METEOR, that reported good
results in the WMT13 challenge.
Tables 6 and 7 show the official results obtained
by UPC-IPA and UPC-STOUT in WMT14.
8
The best and worst figures for each language pair
are included for comparison ?the worst perform-
ing submission at segment level is neglected as it
seems to be a dummy (Mach?a?cek and Bojar, 2014
to appear). Both UPC-IPA and UPC-STOUT
configurations resulted in different performances
depending on the language pair. UPC-STOUT
scored above the average for all the language pairs
except for en?cs at both system and segment level,
and en?ru at system level. Although the evaluation
results are not directly comparable to the WMT13
ones, one can note that the results were notably
better for pairs that involved Czech and Russian,
and worse for those that involved French and Ger-
man at system level. Analysing the impact of the
evaluation methods and building comparable re-
sults in order to address a study on configurations
for different languages is part of our future work.
5 Conclusions
This paper describes the UPC submission to the
WMT14 metrics for automatic machine transla-
tion evaluation task. The core of our evaluation
system is ASIYA, a toolkit for MT evaluation. Be-
sides the formerly available metrics in ASIYA, we
experimented with new metrics for machine trans-
8
At the time of submitting this paper, the evaluation re-
sults for WMT14 Metrics Task were provisional.
Table 7: Segment-level Kendall?s ? correlation re-
sults in the WMT14 Metrics shared task
en?fr en?de en?cs en?ru
UPC-IPA 26.3 21.7 29.7 42.6
UPC-STOUT 27.8 22.4 28.1 42.5
WMT14-Best 29.7 25.8 34.4 44.0
WMT14-Worst 25.4 18.5 28.1 38.1
fr?en de?en hi?en cs?en ru?en
UPC-IPA 41.2 34.1 36.7 27.4 32.4
UPC-STOUT 40.3 34.5 35.1 27.5 32.4
WMT14-Best 43.3 38.1 43.8 32.8 36.4
WMT14-Worst 31.1 22.5 23.7 18.7 21.2
lation evaluation, with especial focus on transla-
tion from English into other languages.
As previous work on English as target language
has proven, syntactic and semantic analysis can
contribute positively to the evaluation of automatic
translations. For this reason, we integrated a set of
new metrics for different languages, aimed at eval-
uating a translation from different perspectives.
Among the novelties, (i) new shallow metrics, bor-
rowed from Information Retrieval, were included
to compare the candidate translation against both
the reference translation (monolingual compari-
son) and the source sentence (cross-language com-
parison), including explicit semantic analysis and
the lexical-based characterisations character n-
grams and pseudo-cognates; (ii) new parsers for
other languages than English were applied to com-
pare automatic and reference translation at the
syntactic level; (iii) an experimental metric based
on alignments; and (iv) a metric based on the cor-
relation of the translations? expected lengths was
included as well. Our preliminary experiments
showed that the combination of these and standard
MT evaluation metrics allows for a performance
close to the best in last year?s competition for some
language pairs.
The new set of metrics is already available
in the current version of the toolkit ASIYA
v3.0 (Gonz`alez and Gim?enez, 2014). Our current
efforts are focused on the exploitation of more so-
phisticated methods to combine the contributions
of each metric, and the extension of the list of sup-
ported languages.
Acknowledgements
This work was funded by the Spanish Ministry
of Education and Science (TACARDI project,
TIN2012-38523-C02-00).
399
References
Alberto Barr?on-Cede?no, Monica Lestari Paramita, Paul
Clough, and Paolo Rosso. 2014. A Comparison
of Approaches for Measuring Cross-Lingual Sim-
ilarity of Wikipedia Articles. Advances in Infor-
mation Retrieval. Proceedings of the 36th European
Conference on IR Research, LNCS (8416):424?429.
Springer-Verlag.
Ond?rej Bojar, Christian Buck, Chris Callison-Burch,
Christian Federmann, Barry Haddow, Philipp
Koehn, Christof Monz, Matt Post, Radu Soricut, and
Lucia Specia. 2013. Findings of the 2013 Work-
shop on Statistical Machine Translation. In Pro-
ceedings of the Eighth Workshop on Statistical Ma-
chine Translation, pages 1?44, Sofia, Bulgaria, Au-
gust. Association for Computational Linguistics.
Peter F. Brown, Jennifer C. Lai, and Robert L. Mer-
cer. 1991. Aligning Sentences in Parallel Corpora.
In Douglas E. Appelt, editor, Proceedings of the
29th Annual Meeting of the Association for Com-
putational Linguistics (ACL 1991), pages 169?176,
Berkeley, CA, USA. Association for Computational
Linguistics.
Marie Candito, Benot Crabb, and Pascal Denis. 2010a.
Statistical French dependency parsing: treebank
conversion and first results. In The seventh interna-
tional conference on Language Resources and Eval-
uation (LREC), Valletta, Malta.
Marie Candito, Joakim Nivre, Pascal Denis, and En-
rique Henestroza Anguiano. 2010b. Benchmark-
ing of Statistical Dependency Parsers for French. In
Proc. 23rd Intl. COLING Conference on Computa-
tional Linguistics: Poster Volume, pages 108?116,
Beijing, China.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-Fine N-best Parsing and MaxEnt Discriminative
Reranking. In Proc. 43rd Annual Meeting of the
Association for Computational Linguistics (ACL),
pages 173?180, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Michael Denkowski and Alon Lavie. 2011. Meteor
1.3: Automatic Metric for Reliable Optimization
and Evaluation of Machine Translation Systems. In
Proceedings of the 6th Workshop on Statistical Ma-
chine Translation, pages 85?91, Stroudsburg, PA,
USA. Association for Computational Linguistics.
George Doddington. 2002. Automatic Evaluation
of Machine Translation Quality Using N-gram Co-
Occurrence Statistics. In Proceedings of the 2nd In-
ternational Conference on Human Language Tech-
nology, pages 138?145, San Francisco, CA, USA.
Morgan Kaufmann Publishers Inc.
E. Gabrilovich and S. Markovitch. 2007. Computing
Semantic Relatedness Using Wikipedia-based Ex-
plicit Semantic Analysis. In 20th International Joint
Conference on Artificial Intelligence, pages 1606?
1611, San Francisco, CA, USA.
William A. Gale and Kenneth, W. Church. 1993. A
Program for Aligning Sentences in Bilingual Cor-
pora. Computational Linguistics, 19:75?102.
Jes?us Gim?enez and Llu??s M`arquez. 2007. Linguis-
tic Features for Automatic Evaluation of Heteroge-
neous MT Systems. In Proc. of 2nd Workshop on
statistical Machine Translation (WMT07), ACL?07,
Prague, Czech Republic.
Jes?us Gim?enez and Llu??s M`arquez. 2010a. Asiya:
An Open Toolkit for Automatic Machine Translation
(Meta-)Evaluation. The Prague Bulletin of Mathe-
matical Linguistics, 94:77?86.
Jes?us Gim?enez and Llu??s M`arquez. 2010b. Linguistic
Measures for Automatic Machine Translation Eval-
uation. Machine Translation, 24(3?4):77?86.
Jes?us Gim?enez. 2008. Empirical Machine Translation
and its Evaluation. Ph.D. thesis, Universitat Politc-
nica de Catalunya, July.
Meritxell Gonz`alez and Jes?us Gim?enez. 2014.
Asiya: An Open Toolkit for Automatic Ma-
chine Translation (Meta-)Evaluation, v3.0, Febru-
ary. http://asiya.lsi.upc.edu.
Meritxell Gonz`alez, Jes?us Gim?enez, and Llu??s
M`arquez. 2012. A Graphical Interface for MT Eval-
uation and Error Analysis. In Proc. Annual Meet-
ing of the Association for Computational Linguis-
tics (ACL). System Demonstration, pages 139?144,
Jeju, South Korea, July. Association for Computa-
tional Linguistics.
Meritxell Gonz`alez, Laura Mascarell, and Llu??s
M`arquez. 2013. tSearch: Flexible and Fast
Search over Automatic translation for Improved
Quality/Error Analysis. In Proc. 51st Annual Meet-
ing of the Association for Computational Linguis-
tics (ACL). System Demonstration, pages 181?186,
Sofia, Bulgaria, August.
Paul Jaccard. 1901.
?
Etude comparative de la distribu-
tion florale dans une portion des Alpes et des Jura.
Bulletin del la Soci?et?e Vaudoise des Sciences Na-
turelles, 37:547?579.
Ding Liu and Daniel Gildea. 2005. Syntactic Fea-
tures for Evaluation of Machine Translation. In Pro-
ceedings of ACL Workshop on Intrinsic and Extrin-
sic Evaluation Measures for MT and/or Summariza-
tion, pages 25?32, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Matou?s Mach?a?cek and Ond?rej Bojar. 2013. Results of
the WMT13 metrics shared task. In Proceedings of
the Eighth Workshop on Statistical Machine Trans-
lation, pages 45?51, Sofia, Bulgaria, August. Asso-
ciation for Computational Linguistics.
Matou?s Mach?a?cek and Ond?rej Bojar. 2014 (to appear).
Results of the WMT14 metrics shared task. In Pro-
ceedings of the Ninth Workshop on Statistical Ma-
chine Translation, Baltimare, US, June. Association
for Computational Linguistics.
400
Paul McNamee and James Mayfield. 2004. Character
N-Gram Tokenization for European Language Text
Retrieval. Information Retrieval, 7(1-2):73?97.
I. Dan Melamed, Ryan Green, and Joseph P. Turian.
2003. Precision and Recall of Machine Transla-
tion. In Proceedings of the Joint Conference on Hu-
man Language Technology and the North American
Chapter of the Association for Computational Lin-
guistics (HLT-NAACL), pages 61?63, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Sonja Nie?en, Franz Josef Och, Gregor Leusch, and
Hermann Ney. 2000. An Evaluation Tool for Ma-
chine Translation: Fast Evaluation for MT Research.
In Proceedings of the 2nd International Conference
on Language Resources and Evaluation (LREC).
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas
Chanev, G?ulsen Eryigit, Sandra K?ubler, Svetoslav
Marinov, and Erwin Marsi. 2007. MaltParser:
A language-independent system for data-driven de-
pendency parsing. Natural Language Engineering,
13(2):95?135.
Slav Petrov and Dan Klein. 2007. Improved Infer-
ence for Unlexicalized Parsing. In Proc. Human
Language Technologies (HLT), pages 404?411. As-
sociation for Computational Linguistics, April.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and in-
terpretable tree annotation. In 21st International
Conference on Computational Linguistics and the
44th annual meeting of the Association for Com-
putational Linguistics, ACL-44, pages 433?440,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Bruno Pouliquen, Ralf Steinberger, and Camelia Ignat.
2003. Automatic Identification of Document Trans-
lations in Large Multilingual Document Collections.
In Proceedings of the International Conference on
Recent Advances in Natural Language Processing
(RANLP-2003), pages 401?408, Borovets, Bulgaria.
Michel Simard, George F. Foster, and Pierre Isabelle.
1992. Using Cognates to Align Sentences in Bilin-
gual Corpora. In Proceedings of the Fourth Interna-
tional Conference on Theoretical and Methodologi-
cal Issues in Machine Translation.
Matthew Snover, Nitin Madnani, Bonnie Dorr, and
Richard Schwartz. 2009. Fluency, adequacy, or
HTER? Exploring different human judgments with a
tunable MT metric. In Proceedings of the 4th Work-
shop on Statistical Machine Translation, pages 259?
268, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Christoph Tillmann, Stefan Vogel, Hermann Ney,
A. Zubiaga, and H. Sawaf. 1997. Accelerated DP
based Search for Statistical Translation. In Proceed-
ings of European Conference on Speech Communi-
cation and Technology, pages 2667?2670.
401
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 402?408,
Baltimore, Maryland USA, June 26?27, 2014.
c
?2014 Association for Computational Linguistics
DiscoTK: Using Discourse Structure for Machine Translation Evaluation
Shafiq Joty Francisco Guzm
?
an Llu??s M
`
arquez and Preslav Nakov
ALT Research Group
Qatar Computing Research Institute ? Qatar Foundation
{sjoty,fguzman,lmarquez,pnakov}@qf.org.qa
Abstract
We present novel automatic metrics for
machine translation evaluation that use
discourse structure and convolution ker-
nels to compare the discourse tree of an
automatic translation with that of the hu-
man reference. We experiment with five
transformations and augmentations of a
base discourse tree representation based
on the rhetorical structure theory, and we
combine the kernel scores for each of them
into a single score. Finally, we add other
metrics from the ASIYA MT evaluation
toolkit, and we tune the weights of the
combination on actual human judgments.
Experiments on the WMT12 and WMT13
metrics shared task datasets show corre-
lation with human judgments that outper-
forms what the best systems that partici-
pated in these years achieved, both at the
segment and at the system level.
1 Introduction
The rapid development of statistical machine
translation (SMT) that we have seen in recent
years would not have been possible without au-
tomatic metrics for measuring SMT quality. In
particular, the development of BLEU (Papineni
et al., 2002) revolutionized the SMT field, al-
lowing not only to compare two systems in a
way that strongly correlates with human judg-
ments, but it also enabled the rise of discrimina-
tive log-linear models, which use optimizers such
as MERT (Och, 2003), and later MIRA (Watanabe
et al., 2007; Chiang et al., 2008) and PRO (Hop-
kins and May, 2011), to optimize BLEU, or an ap-
proximation thereof, directly. While over the years
other strong metrics such as TER (Snover et al.,
2006) and Meteor (Lavie and Denkowski, 2009)
have emerged, BLEU remains the de-facto stan-
dard, despite its simplicity.
Recently, there has been steady increase in
BLEU scores for well-resourced language pairs
such as Spanish-English and Arabic-English.
However, it was also observed that BLEU-like n-
gram matching metrics are unreliable for high-
quality translation output (Doddington, 2002;
Lavie and Agarwal, 2007). In fact, researchers al-
ready worry that BLEU will soon be unable to dis-
tinguish automatic from human translations.
1
This
is a problem for most present-day metrics, which
cannot tell apart raw machine translation output
from a fully fluent professionally post-edited ver-
sion thereof (Denkowski and Lavie, 2012).
Another concern is that BLEU-like n-gram
matching metrics tend to favor phrase-based SMT
systems over rule-based systems and other SMT
paradigms. In particular, they are unable to cap-
ture the syntactic and semantic structure of sen-
tences, and are thus insensitive to improvement
in these aspects. Furthermore, it has been shown
that lexical similarity is both insufficient and not
strictly necessary for two sentences to convey
the same meaning (Culy and Riehemann, 2003;
Coughlin, 2003; Callison-Burch et al., 2006).
The above issues have motivated a large amount
of work dedicated to design better evaluation met-
rics. The Metrics task at the Workshop on Ma-
chine Translation (WMT) has been instrumental in
this quest. Below we present QCRI?s submission
to the Metrics task of WMT14, which consists of
the DiscoTK family of discourse-based metrics.
In particular, we experiment with five different
transformations and augmentations of a discourse
tree representation, and we combine the kernel
scores for each of them into a single score which
we call DISCOTK
light
. Next, we add to the com-
bination other metrics from the ASIYA MT eval-
uation toolkit (Gim?enez and M`arquez, 2010), to
produce the DISCOTK
party
metric.
1
This would not mean that computers have achieved hu-
man proficiency; it would rather show BLEU?s inadequacy.
402
Finally, we tune the relative weights of the met-
rics in the combination using human judgments
in a learning-to-rank framework. This proved
to be quite beneficial: the tuned version of the
DISCOTK
party
metric was the best performing
metric in the WMT14 Metrics shared task.
The rest of the paper is organized as follows:
Section 2 introduces our basic discourse metrics
and the tree representations they are based on.
Section 3 describes our metric combinations. Sec-
tion 4 presents our experiments and results on
datasets from previous years. Finally, Section 5
concludes and suggests directions for future work.
2 Discourse-Based Metrics
In our recent work (Guzm?an et al., 2014), we used
the information embedded in the discourse-trees
(DTs) to compare the output of an MT system to
a human reference. More specifically, we used
a state-of-the-art sentence-level discourse parser
(Joty et al., 2012) to generate discourse trees for
the sentences in accordance with the Rhetorical
Structure Theory (RST) of discourse (Mann and
Thompson, 1988). Then, we computed the simi-
larity between DTs of the human references and
the system translations using a convolution tree
kernel (Collins and Duffy, 2001), which efficiently
computes the number of common subtrees. Note
that this kernel was originally designed for syntac-
tic parsing, and the subtrees are subject to the con-
straint that their nodes are taken with all or none
of their children, i.e., if we take a direct descen-
dant of a given node, we must also take all siblings
of that descendant. This imposes some limitations
on the type of substructures that can be compared,
and motivates the enriched tree representations ex-
plained in subsections 2.1?2.4.
The motivation to compare discourse trees, is
that translations should preserve the coherence re-
lations. For example, consider the three discourse
trees (DTs) shown in Figure 1. Notice that the
Attribution relation in the reference translation is
also realized in the system translation in (b) but not
in (c), which makes (b) a better translation com-
pared to (c), according to our hypothesis.
In (Guzm?an et al., 2014), we have shown that
discourse structure provides additional informa-
tion for MT evaluation, which is not captured by
existing metrics that use lexical, syntactic and se-
mantic information; thus, discourse should be con-
sidered when developing new rich metrics.
Here, we extend our previous work by devel-
oping metrics that are based on new representa-
tions of the DTs. In the remainder of this section,
we will focus on the individual DT representations
that we will experiment with; then, the following
section will describe the metric combinations and
tuning used to produce the DiscoTK metrics.
2.1 DR-LEX
1
Figure 2a shows our first representation of the DT.
The lexical items, i.e., words, constitute the leaves
of the tree. The words in an Elementary Discourse
Unit (EDU) are grouped under a predefined tag
EDU, to which the nuclearity status of the EDU
is attached: nucleus vs. satellite. Coherence re-
lations, such as Attribution, Elaboration, and En-
ablement, between adjacent text spans constitute
the internal nodes of the tree. Like the EDUs, the
nuclearity statuses of the larger discourse units are
attached to the relation labels. Notice that with
this representation the tree kernel can easily be ex-
tended to find subtree matches at the word level,
i.e., by including an additional layer of dummy
leaves as was done in (Moschitti et al., 2007). We
applied the same solution in our representations.
2.2 DR-NOLEX
Our second representation DR-NOLEX (Figure 2b)
is a simple variation of DR-LEX
1
, where we ex-
clude the lexical items. This allows us to measure
the similarity between two translations in terms of
their discourse structures alone.
2.3 DR-LEX
2
One limitation of DR-LEX
1
and DR-NOLEX is that
they do not separate the structure, i.e., the skele-
ton, of the tree from its labels. Therefore, when
measuring the similarity between two DTs, they
do not allow the tree kernel to give partial credit
to subtrees that differ in labels but match in their
structures. DR-LEX
2
, a variation of DR-LEX
1
, ad-
dresses this limitation as shown in Figure 2c. It
uses predefined tags SPAN and EDU to build the
skeleton of the tree, and considers the nuclearity
and/or relation labels as properties (added as chil-
dren) of these tags. For example, a SPAN has two
properties, namely its nuclearity and its relation,
and an EDU has one property, namely its nucle-
arity. The words of an EDU are placed under the
predefined tag NGRAM.
403
Elaboration ROOT
SPANNucleus AttributionSatellite
Voices are coming from Germany , SPANSatellite SPANNucleus
suggesting that ECB be the last resort creditor .
(a) A reference (human-written) translation.
AttributionROOT
SPANSatellite SPANNucleus
In Germany voices , the ECB should be the lender of last resort .
(b) A higher quality (system-generated) translation.
SPANROOT
In Germany the ECB should be for the creditors of last resort .
(c) A lower quality (system-generated) translation.
Figure 1: Three discourse trees for the translations of a source sentence: (a) the reference, (b) a higher
quality automatic translation, and (c) a lower quality automatic translation.
2.4 DR-LEX
1.1
and DR-LEX
2.1
Although both DR-LEX
1
and DR-LEX
2
allow the
tree kernel to find matches at the word level, the
words are compared in a bag-of-words fashion,
i.e., if the trees share a common word, the ker-
nel will find a match regardless of its position in
the tree. Therefore, a word that has occurred in
an EDU with status Nucleus in one tree could be
matched with the same word under a Satellite in
the other tree. In other words, the kernel based
on these representations is insensitive to the nu-
clearity status and the relation labels under which
the words are matched. DR-LEX
1.1
, an exten-
sion of DR-LEX
1
, and DR-LEX
2.1
, an extension
of DR-LEX
2
, are sensitive to these variations at
the lexical level. DR-LEX
1.1
(Figure 2d) and DR-
LEX
2.1
(Figure 2e) propagate the nuclearity sta-
tuses and/or the relation labels to the lexical items
by including three more subtrees at the EDU level.
3 Metric Combination and Tuning
In this section, we describe our Discourse Tree
Kernel (DiscoTK) metrics. We have two main
versions: DISCOTK
light
, which combines the five
DR-based metrics, and DISCOTK
party
, which fur-
ther adds the Asiya metrics.
3.1 DISCOTK
light
In the previous section, we have presented several
discourse tree representations that can be used to
compare the output of a machine translation sys-
tem to a human reference. Each representation
stresses a different aspect of the discourse tree.
In order to make our estimations more robust,
we propose DISCOTK
light
, a metric that takes ad-
vantage of all the previous discourse representa-
tions by linearly interpolating their scores. Here
are the processing steps needed to compute this
metric:
(i) Parsing: We parsed each sentence in order to
produce discourse trees for the human references
and for the outputs of the systems.
(ii) Tree enrichment/simplification: For each
sentence-level discourse tree, we generated the
five different tree representations: DR-NOLEX,
DR-LEX
1
, DR-LEX
1.1
, DR-LEX
2
, DR-LEX
2.1
.
(iii) Estimation: We calculated the per-sentence
similarity scores between tree representations of
the system hypothesis and the human reference
using the extended convolution tree kernel as de-
scribed in the previous section. To compute the
system-level similarity scores, we calculated the
average sentence-level similarity; note that this en-
sures that our metric is ?the same? at the system
and at the segment level.
(iv) Normalization: In order to make the scores of
the different representations comparable, we per-
formed a min?max normalization
2
for each met-
ric and for each language pair.
(v) Combination: Finally, for each sentence, we
computed DISCOTK
light
as the average of the
normalized similarity scores of the different repre-
sentations. For system-level experiments, we per-
formed linear interpolation of system-level scores.
2
Where x
?
= (x?min)/(max?min).
404
	


 

Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 132?134,
October 25, 2014, Doha, Qatar. c?2014 Association for Computational Linguistics
Word?s Vector Representations meet Machine Translation
Eva Mart??nez Garcia
Cristina Espan?a-Bonet
TALP Research Center
Univesitat Polite`cnica de Catalunya
emartinez@lsi.upc.edu
cristinae@lsi.upc.edu
Jo?rg Tiedemann
Uppsala University
Department of Linguistics
and Philology
jorg.tiedemann@lingfil.uu.se
Llu??s Ma`rquez
Qatar Computing Research Institute
Qatar Foundation
lluism@lsi.upc.edu
Abstract
Distributed vector representations of
words are useful in various NLP tasks.
We briefly review the CBOW approach
and propose a bilingual application of
this architecture with the aim to improve
consistency and coherence of Machine
Translation. The primary goal of the bilin-
gual extension is to handle ambiguous
words for which the different senses are
conflated in the monolingual setup.
1 Introduction
Machine Translation (MT) systems are nowadays
achieving a high-quality performance. However,
they are typically developed at sentence level
using only local information and ignoring the
document-level one. Recent work claims that
discourse-wide context can help to translate indi-
vidual words in a way that leads to more coherent
translations (Hardmeier et al., 2013; Hardmeier et
al., 2012; Gong et al., 2011; Xiao et al., 2011).
Standard SMT systems use n-gram models to
represent words in the target language. How-
ever, there are other word representation tech-
niques that use vectors of contextual information.
Recently, several distributed word representation
models have been introduced that have interesting
properties regarding to the semantic information
that they capture. In particular, we are interested
in the word2vec package available in (Mikolov et
al., 2013a). These models proved to be robust
and powerful for predicting semantic relations be-
tween words and even across languages. However,
they are not able to handle lexical ambiguity as
they conflate word senses of polysemous words
into one common representation. This limitation is
already discussed in (Mikolov et al., 2013b) and in
(Wolf et al., 2014), in which bilingual extensions
of the word2vec architecture are proposed. In con-
trast to their approach, we are not interested in
monolingual applications but instead like to con-
centrate directly on the bilingual case in connec-
tion with MT.
We built bilingual word representation mod-
els based on word-aligned parallel corpora by
an application of the Continuous Bag-of-Words
(CBOW) algorithm to the bilingual case (Sec-
tion 2). We made a twofold preliminary evalua-
tion of the acquired word-pair representations on
two different tasks (Section 3): predicting seman-
tically related words (3.1) and cross-lingual lexical
substitution (3.2). Section 4 draws the conclusions
and sets the future work in a direct application of
these models to MT.
2 Semantic Models using CBOW
The basic architecture that we use to build our
models is CBOW (Mikolov et al., 2013a). The
algorithm uses a neural network (NN) to predict
a word taking into account its context, but without
considering word order. Despite its drawbacks, we
chose to use it since we presume that the transla-
tion task applies the same strategy as the CBOW
architecture, i.e., from a set of context words try to
predict a translation of a specific given word.
In the monolingual case, the NN is trained using
a monolingual corpus to obtain the corresponding
projection matrix that encloses the vector repre-
sentations of the words. In order to introduce the
semantic information in a bilingual scenario, we
use a parallel corpus and automatic word align-
ment to extract a training corpus of word pairs:
(w
i,S
|w
i,T
). This approach is different from (Wolf
et al., 2014) who build an independent model for
each language. With our method, we try to cap-
ture simultaneously the semantic information as-
sociated to the source word and the information
in the target side of the translation. In this way,
we hope to better capture the semantic informa-
tion that is implicitly given by translating a text.
132
Model Accuracy Known words
mono en 32.47 % 64.67 %
mono es 10.24 % 44.96 %
bi en-es 23.68 % 13.74 %
Table 1: Accuracy on the Word Relationship set.
3 Experiments
The semantic models are built using a combination
of freely available corpora for English and Span-
ish (EuropalV7, United Nations and Multilingual
United Nations, and Subtitles2012). They can
be found in the Opus site (Tiedemann, 2012).We
trained vectors to represent word pairs forms us-
ing this corpora with the word2vec CBOW imple-
mentation. We built a training set of almost 600
million words and used 600-dimension vectors in
the training. Regarding to the alignments, we only
used word-to-word ones to avoid noise.
3.1 Accuracy of the Semantic Model
We first evaluate the quality of the models based
on the task of predicting semantically related
words. A Spanish native speaker built the bilin-
gual test set similarly to the process done to the
training data from a list of 19, 544 questions intro-
duced by (Mikolov et al., 2013c). In our bilingual
scenario, the task is to predict a pair of words given
two pairs of related words. For instance, given the
pair Athens|Atenas Greece|Grecia and
the question London|Londres, the task is to
predict England|Inglaterra.
Table 1 shows the results, both overall accuracy
and accuracy over the known words for the mod-
els. Using the first 30, 000 entries of the model
(the most frequent ones), we obtain 32% of ac-
curacy for English (mono en) and 10% for Span-
ish (mono es). We chose these parameters for our
system to obtain comparable results to the ones
in (Mikolov et al., 2013a) for a CBOW architec-
ture but trained with 783 million words (50.4%).
Decay for the model in Spanish can be due to the
fact that it was built from automatic translations.
In the bilingual case (bi en-es), the accuracy is
lower than for English probably due to the noise
in translations and word alignment.
3.2 Cross-Lingual Lexical Substitution
Another way to evaluate the semantic models is
through the effect they have in translation. We im-
plemented the Cross-Lingual Lexical Substitution
task carried out in SemEval-2010 (Task2, 2010)
and applied it to a test set of news data from the
News Commentary corpus of 2011.
We identify those content words which are
translated in more than one way by a baseline
translation system (Moses trained with Europarl
v7). Given one of these content words, we take the
two previous and two following words and look
for their vector representations using our bilingual
models. We compute a linear combination of these
vectors to obtain a context vector. Then, to chose
the best translation option, we calculate a score
based on the similarity among the vector of every
possible translation option seen in the document
and the context vector.
In average there are 615 words per document
within the test set and 7% are translated in more
than one way by the baseline system. Our bilin-
gual models know in average 87.5% of the words
and 83.9% of the ambiguous ones, so although
there is a good coverage for this test set, still, some
of the candidates cannot be retranslated or some
of the options cannot be used because they are
missing in the models. The accuracy obtained af-
ter retranslation of the known ambiguous words
is 62.4% and this score is slightly better than the
result obtained by using the most frequent transla-
tion for ambiguous words (59.8%). Even though
this improvement is rather modest, it shows poten-
tial benefits of our model in MT.
4 Conclusions
We implemented a new application of word vec-
tor representations for MT. The system uses word
alignments to build bilingual models with the final
aim to improve the lexical selection for words that
can be translated in more than one sense.
The models have been evaluated regarding their
accuracy when trying to predict related words
(Section 3.1) and also regarding its possible effect
within a translation system (Section 3.2). In both
cases one observes that the quality of the transla-
tion and alignments previous to building the se-
mantic models are bottlenecks for the final perfor-
mance: part of the vocabulary, and therefore trans-
lation pairs, are lost in the training process.
Future work includes studying different kinds
of alignment heuristics. We plan to develop
new features based on the semantic models to
use them inside state-of-the-art SMT systems like
Moses (Koehn et al., 2007) or discourse-oriented
decoders like Docent (Hardmeier et al., 2013).
133
References
Z. Gong, M. Zhang, and G. Zhou. 2011. Cache-based
document-level statistical machine translation. In
Proc. of the 2011 Conference on Empirical Methods
in NLP, pages 909?919, UK.
C. Hardmeier, J. Nivre, and J. Tiedemann. 2012.
Document-wide decoding for phrase-based statisti-
cal machine translation. In Proc. of the Joint Con-
ference on Empirical Methods in NLP and Compu-
tational Natural Language Learning, pages 1179?
1190, Korea.
C. Hardmeier, S. Stymne, J. Tiedemann, and J. Nivre.
2013. Docent: A document-level decoder for
phrase-based statistical machine translation. In
Proc. of the 51st ACL Conference, pages 193?198,
Bulgaria.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: open source toolkit for
statistical machine translation. In Proc. of the 45th
ACL Conference, pages 177?180, Czech Republic.
T. Mikolov, K. Chen, G. Corrado, and J. Dean. 2013a.
Efficient estimation of word representations in vec-
tor space. In Proceedings of Workshop at ICLR.
http://code.google.com/p/word2vec.
T. Mikolov, Q. V. Le, and I. Sutskever. 2013b. Ex-
ploiting similarities among languages for machine
translation. In arXiv.
T. Mikolov, I. Sutskever, G. Corrado, and J. Dean.
2013c. Distributed representations of words and
phrases and their compositionality. In Proceedings
of NIPS.
Task2. 2010. Cross-lingual lexi-
cal substitution task, semeval-2010.
http://semeval2.fbk.eu/semeval2.php?location=tasksT24.
J. Tiedemann. 2009. News from opus - a collection
of multilingual parallel corpora with tools and in-
terfaces. In N. Nicolov and K. Bontcheva and G.
Angelova and R. Mitkov (eds.) Recent Advances in
Natural Language Processing (vol V), pages 237?
248, Amsterdam/Philadelphia. John Benjamins.
J. Tiedemann. 2012. Parallel data, tools and interfaces
in opus. In Proceedings of the 8th International
Conference on Language Resources and Evaluation
(LREC?2012). http://opus.lingfil.uu.se/.
L. Wolf, Y. Hanani, K. Bar, and N. Derschowitz. 2014.
Joint word2vec networks for bilingual semantic rep-
resentations. In Poster sessions at CICLING.
T. Xiao, J. Zhu, S. Yao, and H. Zhang. 2011.
Document-level consistency verification in machine
translation. In Proc. of Machine Translation Summit
XIII, pages 131?138, China.
134
