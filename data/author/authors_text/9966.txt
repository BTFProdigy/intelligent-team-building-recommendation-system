Proceedings of the IJCNLP-08 Workshop on NER for South and South East Asian Languages, pages 89?96,
Hyderabad, India, January 2008. c?2008 Asian Federation of Natural Language Processing
Named Entity Recognition for South Asian Languages 
Amit Goyal
University of Utah, School of Computing
Salt Lake City, Utah
amitg@cs.utah.edu
Abstract
Much work has already been done on 
building named entity recognition systems. 
However most of this work has been con-
centrated on English and other European
languages. Hence, building a named entity 
recognition (NER) system for South Asian 
Languages (SAL) is still an open problem
because they exhibit characteristics differ-
ent from English. This paper builds a 
named entity recognizer which also identi-
fies nested name entities for the Hindi lan-
guage using machine learning algorithm, 
trained on an annotated corpus. However,
the algorithm is designed in such a manner 
that it can easily be ported to other South
Asian Languages provided the necessary 
NLP tools like POS tagger and chunker are 
available for that language. I compare re-
sults of Hindi data with English data of 
CONLL shared task of 2003.
1 Introduction
Identifying and classifying named-entities into 
person, location, organization or other names in a 
text is an important task for numerous applications.
I focus here on building a named entity recognition 
system that will automatically mark the boundaries 
and labels of the named entities (NEs) in running 
text. The system also identifies nested named enti-
ties which are a superset of the maximal entities.
E.g. ?Lal Bahadur Shastri National Academy of 
Administration? is an organization name and is 
referred as maximal entity. However it also con-
tains ?Lal Bahadur Shastri? as a person name pre-
sent inside an organization name and which is re-
ferred as a part of nested entity along with ?Lal 
Bahadur Shastri National Academy of Administra-
tion? as an organization name.
To make the problem simpler, I split the prob-
lem into three sub tasks. The first (NER module) of 
which identifies whether an entity is a NE or not;
the second (NEC module) identifies the type of 
label associated with each entity; the third (NNE 
module) identifies the nested name entities (NNE). 
Labels considered for this task are: person, organi-
zation and location names, measure, time, number, 
domain specific terms, abbreviation, title and
designation.
Conditional random fields (CRFs) (Lafferty et 
al. 2001) with a variety of novel and traditional 
features have been used as a classifier for above 
three modules. CRFs are undirected graphical 
models, a special case of which is linear chains 
which are well suited to sequence labeling tasks. 
They have shown to be useful in part of speech 
tagging (Lafferty et al 2001), shallow parsing (Sha 
and Pereira 2003), and named entity recognition 
for Hindi newswire data (Li and McCallum 2003).
2 Related Work
Named Entity Recognition (NER) has been con-
sidered as subtask of Information Extraction. Dif-
ferent NER systems were evaluated as a part of the 
Sixth Message Understanding Conference in 1995 
(MUC6). The target language was English. Palmer 
and Day (1997) have worked on Chinese, English, 
French, Japanese, Portuguese and Spanish and 
found that the difficulty of the NER task was dif-
ferent for the six languages but that a large part of 
the task could be performed with simple methods. 
89
Cucerzan et al (1999) used both morphological 
and contextual clues for identifying named entities 
in English, Greek, Hindi, Rumanian and Turkish. 
With minimal supervision, they obtained overall F 
measures between 40 and 70, depending on the 
languages used. Collins (1999) showed that use of 
unlabelled data for NER can reduce the require-
ments for supervision to just 7 simple seed rules. 
The CoNLL shared task of 2002 and 2003 focused 
on language independent NER and has performed 
evaluations on English, Spanish, Dutch and Ger-
man and participating systems have performed 
well. Li and McCallum (2003) used CRFs and fea-
ture induction (McCallum 2003) to get an F-score 
of 71.50 for Hindi language on test-set. May et al 
(2003) used HMM to create NER for Hindi and 
Cebuano. Ekbal et al (2007) used lexical pattern 
learning from corpus data for NER for Bangla lan-
guage.
3 My Contributions
I focus here on building a NER system for the
Hindi language using conditional random fields 
(CRFs) using NLPAI Machine Learning Contest 
2007 data. The system is built in such a manner 
that it could be easily ported to other languages. 
This method was evaluated on test set 1 and test set 
2 and attains a maximal F1 measure around 49.2 
and nested F1 measure around 50.1 for test-set 1; 
maximal F1 measure around 44.97 and nested F1 
measure 43.70 around  for test-set 2. However the 
system achieves an F-measure of 58.85 on devel-
opment set. The great difference in the numbers 
could be due to some difference in test and devel-
opment set. I have also compared my results on 
Hindi data with English data of CONLL shared 
task of 2003 by introducing interesting phenomena
which are not present in English. I perform ex-
periments on English after removing capitalization
since Hindi lacks such overt marking. Also there is 
another interesting phenomenon in Hindi or any 
other SAL i.e. a word can be a common noun as 
well as a proper noun. For example ?sambhab 
sinha? is a name of a person but when I use ?samb-
hab? in a sentence ?yaha kaam mujse sambhab 
nahi? It acts as a common noun meaning ?possible? 
in English. Hindi is full of such cases making the 
task more difficult. Hence it becomes very difficult 
for NER system to classify it as person or not.
4 Features
The success of any machine learning algorithm 
depends on finding an appropriate combination of 
features.  This section outlines three types of fea-
tures.
4.1 Contextual features
? Word Window: A word window of size n 
centered in position iw is the sequence of 
words in the sentence placed at   iw + jw po-
sitions, with jw ? [-n , +n]. For each word in 
the window, word and it?s POS + its relative 
position jw forms a feature
? Chunk window: A chunk window of con-
text size n centered in position ic is the se-
quence of chunks in the sentence placed ic + 
jc positions, with jc ? [-n , +n]. The tags (la-
bels) of the chunks in the window + its rela-
tive position jc form a feature. 
4.2 Statistical features
? Binary features: As name suggests these 
features have value 0 or 1. These features 
are not mutually exclusive features that test 
whether the following predicates hold in the 
word: all digits, 4 digit number, contains 
hyphen, punctuation mark, acronym, alpha-
numeric etc. I also modeled whether a par-
ticular word is a noun or not using the POS 
information.
? Trigger words: Using the annotated train-
ing data I find all those words which have a 
high probability of being a number, meas-
ure, abbreviation and time. I model 4 binary 
features giving value 1 to high probable 
words and 0 to the rest. For example, high 
probable words for number would be ?eka?,
?xo?, ?wIna?, ?cAra? etc. (words here are in 
wx-notation) and will get a value as 1.
4.3 Word Internal Feature
? Affixes: Some prefixes and suffixes are 
good indicators for identifying certain 
classes of entities.  Suffixes are typically 
even more informative. For example, suf-
fixes like -bad , -pur, -pally are good indica-
tors of a name of a location.
90
? Words are also assigned a generalized 
?word class (WC)? similar to Collins (2002), 
which replaces all letters with ?a?, digits 
with ?0?, punctuation marks with ?p?, and 
other characters with ?-?. There is a similar 
?brief class (BWC) (Settles 2004)? which 
collapses consecutive characters into one. 
Thus the words ?D.D.T.? and ?AB-1946? 
would both be given the features 
WC=apapap, BWC=apapap and
WC=aap0000, BWC=ap0 respectively, in 
above example hyphen forms the part of 
punctuation marks.  This feature has been 
modeled since this feature can be useful for 
both unseen words as well as solving the 
data sparsity problem.
? Stem of the Word was also obtained using 
a morph analyzer.
We have tried to use the different combination of 
all these features for all three modules which I am
going to discuss in the next section. But before 
ending there are few features which I haven?t used 
and would like to use in future. Bag of words i.e. 
form of the words in the window without consider-
ing their position. Gazetteer Features can also be 
useful. These features couldn?t be used due to 
computational reasons, lack of resources and time.
5 Modules
5.1 NER module
This module identifies whether an entity is a NE or 
not. I use well-known BIO model. B denotes begin
of an entity, I denotes inside an entity; O denotes
outside and is not part of any entity. Here I have 
only one label i.e. NE. Hence it becomes a three 
class problem with B-NE, I-NE and O as output 
labels. Here I am identifying NEs as it?s an easier 
task as compare to classifying them among named-
entity tag-set. It is also done with a hope that this 
information can be useful for NEC module. For 
example in entity like ?Raja Ram Mohun Roy?
tags would be ?Raja/B-NE Ram/I-NE Mohun/I-NE 
Roy/I-NE.? Similarly for ?Microsoft Corp.? tags 
would be ?Microsoft/B-NE Corp./I-NE.? Words 
like ?tiger?, ?eat?, ?happy? etc which are not NEs 
are tagged as O.
5.2 NEC module
Here I try to classify the NEs among various 
classes/labels like person (like Mahatma Gandhi), 
location(like Delhi) and organization(like Micro-
soft Corp.) names, number (like one, two etc), time
(like one day), measure (like 5 kg), domain spe-
cific terms (Botany, zoology etc), title (Mr., The 
Seven Year Itch), abbreviation (D.D.T.) and desig-
nation (Emperor). Hence it becomes a 10 (la-
bels/classes) * 2(B+I) = 20 + 1 (O which denotes
remaining words) =21 class problem. This module 
is independent from the previous module. For ex-
ample in entity like ?Raja Ram Mohun Roy? tags 
would be ?Raja/B-NEP Ram/I-NEP Mohun/I-NEP
Roy/I-NEP.? Similarly for ?Microsoft Corp.? tags 
would be ?Microsoft/B-NEO Corp./I-NEO.?
I could have tried labeling the identified named-
entities from NER However; I found that this re-
sults in a drop in accuracy. Hence I use the output 
of the NER module as one of the features for NEC.
5.3 NNE module
The length of nested named entities is unbounded 
but the majority contains at most 3 words. There-
fore, I try to train three classifiers to learn entities 
of length 1, 2 and 3 independently. This allows us 
to learn nested entities since the bigger entities can 
have different tags when compared to smaller enti-
ties. For example, Srinivas Bangalore will be 
tagged as a name of a person by a classifier who is 
trained to classify NEs of length 2. However, Srini-
vas and Bangalore will be tagged as a name of a 
person and location respectively by a classifier 
which is trained to classify entities of length 1. 
In this module also I use the same BIO model 
and there will be 21 classes for each of the three 
classifiers.
6 Experiments and Discussion
In this section I describe the experiments I per-
formed to evaluate presented algorithm with its 
variations.
NLPAI 2007 NER contest Corpus, I was pro-
vided annotated training and development data 
comprising of 19825 and 4812 sentences respec-
tively for Hindi. The data is labeled with 10 labels 
described above in NEC module. The average sen-
tence length of the corpus is 24.5. The first step 
was to enrich the data with POS, chunk informa-
tion and root of the word using POS tagger, Chun-
91
ker (Avinesh et al 2007) and IIIT-Hyderabad
morph analyzer. Hence porting this algorithm to 
any other SAL would require these tools for that 
language.
In the training data, in about 50% sentences
(i.e.10524 sentences) there was not even a single 
NE. Experimentally I found that the inclusion or 
exclusion of these sentences did not have a signifi-
cant effect on system performance. Hence I carried 
all the remaining experiments with sentences con-
taining NEs. The reason for choosing it is it takes 
less time to train and more experiments could be 
performed given the time constraints.
Then I tried to find an appropriate set of features 
for NER and NEC module. For NNE I used the 
same features as used in NEC module since I don?t 
have explicitly labeled data for nested entities. 
Tweaking and tuning of feature doesn?t affect the 
accuracy significantly. 
For NER module, where I am trying to identify 
name entities; context information seems to be 
more informative than statistical features. I use a 
window of -1 to +1 for words, -2 to +2 POS and 
also use features which are combinations of con-
secutive POS tags and words. For example 
Ram/NNP eat/VB mangoes/NNS. Combination 
features for word ?eat? would be NNP/VB, 
VB/NNS, Ram/eat, eat/mangoes, NNP/VB/NNS, 
Ram/eat/mangoes. The stem of the word and chunk 
information also doesn?t affect the accuracy. The 
prefixes and suffixes of length 3 and 4 are found to 
improve the accuracy of the classifier. For example 
Hyderabad will have Hyd, Hyde, bad, abad as pre-
fixes and suffixes of length 3 and 4 respectively.
The word class (WC) and Brief word class (BWC)
features are also very useful features for recogniz-
ing named-entities. I have achieved an F-measure 
of 64.28 by combination of all these features for
identifying name-entities on development set. Ta-
ble 1 shows the detailed results of named entity
recognition (NER) module.
For NEC module, the contextual features as well 
as statistical features are helpful in deciding to 
which class a name-entity belongs. I use word and 
POS window of -1 to +1 as context. No combina-
tion features are being used as introduction of such 
features degrades the accuracy rather than improv-
ing it. However the statistical features are found to
be more useful in this case as compared to NER.
Here also prefixes and suffixes of length 3 and 4
are found to be useful. BWC feature alone is suffi-
Features  Precision  Recall  F-measure
Contextual 64.19 60.53 62.31
Contextual+
Word Internal
64.84 63.73 64.28
Table1: Detailed performance of NER module us-
ing only contextual features and combining word 
internal features.
    Entity Precision Recall F-measure
Abbreviation 43.21 36.46 39.55
Designation 69.61 46.84 56.00
Location 67.51 63.08 65.22
Measure 73.98 72.84 73.41
Number 70.41 87.74 78.13
organization 49.71 39.73 44.16
Person 61.18 47.37 53.40
Title 31.82 14.00 19.44
Terms 30.81 16.72 21.67
Time 67.30 58.53 62.61
Overall 62.60 55.52 58.85
Table2: Detailed performance of the best feature 
set on development set for maximal/nested named 
entities.
-cient for classification, we don?t need to use WC 
feature for improving the accuracy. Chunk infor-
mation and stem of the word doesn?t improve the 
accuracy.
I have modeled NER module so that the output 
of that module can be used as feature for NEC. But 
using it as a feature doesn?t improve the classifica-
tion accuracy. Also, I tried using the boundary in-
formation from the NER module and combining it 
with labels learned from NEC module. It also 
seems to be a futile attempt.
I have used unlabelled data i.e. 24630 sentences
provided during the contest and used bootstrapping 
to make use of it. I have doubled the data i.e. 50% 
manually annotated data and rest is system output
on unlabelled data i.e. 12323 sentences; we have 
used only those sentences which contains at least 
one NE. With this data I almost get the same accu-
racy as I got with only manually annotated data. 
Table 2 shows the detailed performance of the best 
feature set on development set for maximal/nested
named entities using evaluation script of CONLL
shared task of 2003. I have used the evaluation 
script of NLPAI contest to report results on Test 
set-1 and Test set-2 (which contains 1091 and 744
sentences) for two systems in Table 3 and 4. One
92
trained using only annotated data and the other 
trained on annotated and bootstrapped data for the 
same feature set which performed best on devel-
opment set. For test-set 2, system trained using 
annotated and bootstrapped data performs better 
than the system trained using only annotated data.
However, for test set1 both the systems perform 
almost same. One of the reasons for less results as 
compared to development set is I haven?t further 
classified title tag into title object and title person 
tag and Test sets contain many such instances.
I have trained a single classifier for all the enti-
ties but we can use more classifiers and divide the 
tags in such a fashion that those which are closer to 
one another fall in one group. For example we can 
club number, time and measure in one group and 
call them as number group since these are closer to 
each other and train a classifier to automatically 
annotate these entities in running text. Similarly, 
we can group person, number, and location and 
call them as name group. I have attempted a simi-
lar experiment using the same features of NEC 
module for number and name group but still there 
is no improvement. 
For NNE module, I have used the same set of 
features which I have used in NEC module and I 
am handling nested entities up to length of 3. Since 
the development set is not enriched with nested 
entities, it is difficult to optimize the features for 
this module and the results would be same as NER 
module since nested entities are superset of maxi-
mal entities. For Test set-1 and Test set-2 Table 3 
and 4 are used to report results.
For NEs like title there are fewer instances in 
training data which is a reason for its low F-
measure i.e. 19.44 on development set which is 
even less than terms (i.e. 21.67) which are most 
difficult to learn. Also here I have focused on a 
large tag set but it would be interesting to concen-
trate only on person, location and organization 
names, since most of the systems report accuracy 
for these entities. Hence I did some experiments 
with Hindi data concentrating only on person, loca-
tion and Organization but there is not so much in-
crease in the performance.
When I trained my system on English data 
(which I have made mono case) of Conll-2003 
shared task, with only contextual features, system 
gets an overall F-measure of 84.09 on development
set and 75.81 on test set which is far better than 
Hindi. I have just used contextual features with 
    Entity Test set1 Test set 2
Maximal 
Precision
70.78 55.24
Maximal 
Recall
37.69 35.75
Maximal   
F-Measure
49.19 43.41
Nested 
Precision
74.28 58.62
Nested 
Recall
37.73 33.07
Nested 
F-Measure
50.04 42.29
Table3: System trained using only annotated data
    Entity Test set1 Test set 2
Maximal 
Precision
70.28 57.60
Maximal 
Recall
37.62 36.88
Maximal   
F-Measure
49.00 44.97
Nested 
Precision
73.90 60.98
Nested 
Recall
37.93 34.05
Nested 
F-Measure
50.13 43.70
Table 4: System trained using annotated and boot-
strapped data
window size of -1 to +1 for words, POS and chunk
to achieve the results reported in Table 5 for test 
set. The reason for using only contextual informa-
tion is that these features give the maximum accu-
racy and the rest of the features don?t increase the 
accuracy by such a great amount. Also the aim 
over here is to compare results with Hindi lan-
guage and not to make the best NER system for 
English language.
    Entity Precision Recall F-measure
Person 82.05 79.16 80.58
Location 84.16 79.32 81.67  
Organization 70.76 67.01 68.83  
Misc. 73.71 61.11 66.82  
Overall 78.40 73.39 75.81
Table 5: System trained on English mono case data 
using contextual features
93
Also to include common noun phenomena in Eng-
lish I have taken 10 random person names from the 
data and replaced them with common nouns and 
the results are really surprising. By introducing 
this, system achieves an F-measure of 84.32 on 
development set and 76.19 on test set which is bet-
ter than the results on normal system. The number 
of tokens corresponding to these names in training 
data is 500. Table 6 contains the detailed results.
    Entity Precision Recall F-measure
Person 81.92 79.84 80.86
Location 84.18 80.10 82.09
Organization 71.98 67.13 69.47
Misc. 73.04 60.97 66.46
Overall 78.71 73.83 76.19
Table 6: System trained on English mono case data 
with common noun phenomena using contextual 
features 
The results for English are far better than Hindi 
language. The reason is English already has tools 
like POS tagger and chunker which achieves an F 
measure around 95 whereas for Hindi we only 
have an F-measure of 85 for tagger and 80 for 
chunker. This is the reason why the accuracy of 
English system didn?t fall when I removed capi-
talization and introduced common noun phenom-
ena since POS context and chunk context helps a 
lot. Since CONLL 2003 data is already POS 
tagged and chunked, hence POS and chunks corre-
spond to capitalized data. To make it more even, I 
ran Stanford POS tagger (Toutanova et al 2003)
on the same mono case CONLL 2003 data and 
then train the model using only word and POS con-
text. The numbers drop on test set by more than
15% as shown in Table 7. For development set the 
overall F-measure is around 74%.
    Entity Precision Recall F-measure
Person 66.97 53.93 59.75
Location 68.57 56.54 61.98  
Organization 71.64 53.55 61.29  
Misc. 74.71 55.98 64.01  
Overall 69.69 54.84 61.38
Table7: System trained on POS tagger ran on 
mono-case data 
These numbers are comparable to Hindi data. The 
reason is POS tagger performs badly after remov-
ing capitalization. Now the POS tagged data marks 
proper noun i.e. NNP as common noun i.e. NN or 
foreign word as FW. The reason is it uses capitali-
zation to mark NNP tag. We still haven?t included
common noun phenomena. So to do that, I take the 
common noun phenomenon English data and train 
the model using the same features as used above. 
Here also the system performs in the same way.
There is just a decrease of 1% in F-measure of per-
son class. Table 8 contains the detailed results. The 
introduction of common noun phenomena doesn?t
seem to affect the performance too much. The rea-
son can be context helps in disambiguating be-
tween the real ?cheese? and the ?cheese? which has 
been made up by replacing it with ?John?.  
    Entity Precision Recall F-measure
Person 65.48 53.37 58.81
Location 68.23 56.18 61.62
Organization 73.95 53.01 61.75
Misc. 74.81 56.27 64.23
Overall 69.74 54.45 61.16
Table8: System trained on POS tagger ran on 
mono case data which contains common noun 
phenomenon
After looking at these results, we can easily say 
that if we can improve the performance of POS 
tagger, we can do very well on the NER task. 
Without that it?s even difficult for English to give 
good numbers. It is correct that Hindi and SAL 
don?t have capitalization but we could make use of 
morphological features since most of SAL are 
morphologically rich. A hybrid approach involving 
rules along with machine learning approach could 
help us to improve POS tagger and NER systems.
After seeing results on English we ask what are 
the actual reasons for lower numbers on Hindi 
data?  Inconsistency of annotated data is one of the 
big problems but it?s very difficult to create 100%
correct manual data since we have chosen a finely 
grained tagset. Also the data used for Hindi is from 
different domains. Hence due to which the lot of 
terms doesn?t occur in corpus more than once. One 
of the plausible reasons for bad results on test set 
for Hindi compared to development set could be 
94
difference in domain of test set. Also due to lack of 
resources like gazetteer for SAL the task becomes 
more challenging to create everything from
scratch. Also the accuracy of tagger, chunker and 
morph analyzer are not as good as when we com-
pare results with English.
7 Conclusion
In conclusion, I have confirmed that use of ma-
chine learning algorithm on annotated data for 
Hindi language can be useful and the same algo-
rithm can be useful for other languages. I only 
need to tune and tweak the features for a particular 
language. I have described some traditional and 
novel features for Hindi language. I have also 
shown that it?s better to directly classify name-
entities into various labels or classes rather than 
first recognizing them. Also the attempt to make 
use of unlabelled data didn?t help much.
Also I have showed that capitalization is one of 
the important clues for high performance of Eng-
lish on various NLP applications. But we could 
also recognize some other important clues in SAL 
and can hope to do better than English without 
having capitalization.
Directions for future work include concentrating 
on a smaller tag set and trying to improve accuracy 
for each of the label. Since still we don?t have 
enough labeled data for other SAL, it would be 
interesting to try out some unsupervised or semi-
supervised approaches. Also I haven?t tried rule 
based approach which could be very handy when 
combined with some machine learning approach. 
Hence adopting a hybrid approach should help in 
improving the accuracy of the system but still it?s 
an open question.  
8 Acknowledgements
I would like to thank Prof. Rajeev Sangal, Dr. Hal 
Daume III, Dr. Dipti Misra Sharma and Anil 
Kumar Singh for their helpful comments and sug-
gestions.
References
Andrew McCallum. 2003. Efficiently Inducing Features 
of Conditional Random Fields. In Proceedings of the 
19th Conference in UAI.
Andrew McCallum and Wei Li. 2003. Early results for 
named entity recognition with conditional random 
fields, feature induction and web-enhanced lexicons. 
In Proceedings CoNLL 2003.
Avinesh.PVS. and Karthik G. 2007. Part-Of-Speech 
Tagging and Chunking using Conditional Random 
Fields and Transformation Based Learning. In Pro-
ceedings of SPSAL2007
Asif Ekbal and Sivaji Bandyopadhyay. 2007. Lexical 
Pattern Learning from Corpus Data for Named entity 
recognition. In Proceedings of ICON 2007.
Burr Settles. 2004. Biomedical Named Entity Recogni-
tion Using Conditional Random Fields and Rich Fea-
ture Sets. In Proceedings of the International Joint 
Workshop on NLPBA.
David D. Palmer and David S. Day. 1997. A Statistical 
Profile of the Named Entity Task. In Proceedings of 
Fifth ACL Conference for ANLP. 
Erik F. Tjong Kim Sang. 2002. Introduction to the
CoNLL-2002 Shared Task: Language-Independent 
Named Entity Recognition. In Proceedings of the 
CoNLL 2002.
Erik F. Tjong Kim Sang and Fien De Meulder. 2003. 
Introduction to the CoNLL-2003 Shared Task: Lan-
guage-Independent Named Entity Recognition. In
Proceedings of the CoNLL 2003.
Fei Sha and Fernando Pereira. 2003. Shallow parsing-
with conditional random fields. In Proceedings of the
HLT and NAACL 2003.
John Lafferty, Andrew McCallum, and Fernando  
Pereira. 2001. Conditional random fields: Probabilis-
tic models for segmenting and labeling sequence 
data. In Proceedings of ICML 2001.
Kristina Toutanova and Christopher D. Manning. 2000. 
Enriching the Knowledge Sources Used in a Maxi-
mum Entropy Part-of-Speech Tagger. Proceedings of 
the Joint SIGDAT Conference on (EMNLP/VLC-
2000), Hong Kong.
Kristina Toutanova, Dan Klein, Christopher Manning, 
and Yoram Singer. 2003. Feature-Rich Part-of-
Speech Tagging with a Cyclic Dependency Network.
In Proceedings of HLT-NAACL 2003. 
Michael Collins. 2002. Ranking algorithms for named-
entity extraction: Boosting and the voted perceptron. 
In Proceedings of ACL 2002.
Michael Collins and Yoram Singer. 1999. Unsupervised 
models for named entity classification. In Proceed-
ings of the Joint SIGDAT Conference on EMNLP
and Very Large Corpora.
Silviu Cucerzan and David Yarowsky. 1999. Language 
independent named entity recognition combining 
95
morphological and contextual evidence. In Proceed-
ings of 1999 Joint SIGDAT Conference on EMNLP 
and VLC.
Wei Li and Andrew McCallum. 2003. Rapid Develop-
ment of Hindi Named Entity Recognition Using Con-
ditional Random Fields and Feature Induction. In 
Proceedings of ACM TALIP.
96
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 512?520,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Streaming for large scale NLP: Language Modeling
Amit Goyal, Hal Daume? III, and Suresh Venkatasubramanian
University of Utah, School of Computing
{amitg,hal,suresh}@cs.utah.edu
Abstract
In this paper, we explore a streaming al-
gorithm paradigm to handle large amounts
of data for NLP problems. We present an
efficient low-memory method for construct-
ing high-order approximate n-gram frequency
counts. The method is based on a determinis-
tic streaming algorithm which efficiently com-
putes approximate frequency counts over a
stream of data while employing a small mem-
ory footprint. We show that this method eas-
ily scales to billion-word monolingual corpora
using a conventional (8 GB RAM) desktop
machine. Statistical machine translation ex-
perimental results corroborate that the result-
ing high-n approximate small language model
is as effective as models obtained from other
count pruning methods.
1 Introduction
In many NLP problems, we are faced with the chal-
lenge of dealing with large amounts of data. Many
problems boil down to computing relative frequen-
cies of certain items on this data. Items can be
words, patterns, associations, n-grams, and others.
Language modeling (Chen and Goodman, 1996),
noun-clustering (Ravichandran et al, 2005), con-
structing syntactic rules for SMT (Galley et al,
2004), and finding analogies (Turney, 2008) are
examples of some of the problems where we need
to compute relative frequencies. We use language
modeling as a canonical example of a large-scale
task that requires relative frequency estimation.
Computing relative frequencies seems like an
easy problem. However, as corpus sizes grow,
it becomes a highly computational expensive task.
Cutoff Size BLEU NIST MET
Exact 367.6m 28.73 7.691 56.32
2 229.8m 28.23 7.613 56.03
3 143.6m 28.17 7.571 56.53
5 59.4m 28.33 7.636 56.03
10 18.3m 27.91 7.546 55.64
100 1.1m 28.03 7.607 55.91
200 0.5m 27.62 7.550 55.67
Table 1: Effect of count-based pruning on SMT per-
formance using EAN corpus. Results are according to
BLEU, NIST and METEOR (MET) metrics. Bold #s are
not statistically significant worse than exact model.
Brants et al (2007) used 1500 machines for a
day to compute the relative frequencies of n-grams
(summed over all orders from 1 to 5) from 1.8TB
of web data. Their resulting model contained 300
million unique n-grams.
It is not realistic using conventional computing re-
sources to use all the 300 million n-grams for ap-
plications like speech recognition, spelling correc-
tion, information extraction, and statistical machine
translation (SMT). Hence, one of the easiest way to
reduce the size of this model is to use count-based
pruning which discards all n-grams whose count is
less than a pre-defined threshold. Although count-
based pruning is quite simple, yet it is effective for
machine translation. As we do not have a copy of
the web, we will use a portion of gigaword i.e. EAN
(see Section 4.1) to show the effect of count-based
pruning on performance of SMT (see Section 5.1).
Table 1 shows that using a cutoff of 100 produces a
model of size 1.1 million n-grams with a Bleu score
of 28.03. If we compare this with an exact model
of size 367.6 million n-grams, we see an increase of
0.8 points in Bleu (95% statistical significance level
512
? Size BLEU NIST MET
Exact 367.6m 28.73 7.691 56.32
1e-10 218.4m 28.64 7.669 56.33
5e-10 171.0m 28.48 7.666 56.38
1e-9 148.0m 28.56 7.646 56.51
5e-9 91.9m 28.27 7.623 56.16
1e-8 69.4m 28.15 7.609 56.19
5e-7 28.5m 28.08 7.595 55.91
Table 2: Effect of entropy-based pruning on SMT perfor-
mance using EAN corpus. Results are as in Table 1
is ? 0.53 Bleu). However, we need 300 times big-
ger model to get such an increase. Unfortunately, it
is not possible to integrate such a big model inside a
decoder using normal computation resources.
A better way of reducing the size of n-grams is to
use entropy pruning (Stolcke, 1998). Table 2 shows
the results with entropy pruning with different set-
tings of ?. We see that for three settings of ? equal to
1e-10, 5e-10 and 1e-9, we get Bleu scores compara-
ble to the exact model. However, the size of all these
models is not at all small. The size of smallest model
is 25% of the exact model. Even with this size it is
still not feasible to integrate such a big model inside
a decoder. If we take a model of size comparable to
count cutoff of 100, i.e., with ? = 5e-7, we see both
count-based pruning as well as entropy pruning per-
forms the same.
There also have been prior work on maintain-
ing approximate counts for higher-order language
models (LMs) ((Talbot and Osborne, 2007a; Tal-
bot and Osborne, 2007b; Talbot and Brants, 2008))
operates under the model that the goal is to store a
compressed representation of a disk-resident table of
counts and use this compressed representation to an-
swer count queries approximately.
There are two difficulties with scaling all the
above approaches as the order of the LM increases.
Firstly, the computation time to build the database of
counts increases rapidly. Secondly, the initial disk
storage required to maintain these counts, prior to
building the compressed representation is enormous.
The method we propose solves both of these prob-
lems. We do this by making use of the streaming al-
gorithm paradigm (Muthukrishnan, 2005). Working
under the assumption that multiple-GB models are
infeasible, our goal is to instead of estimating a large
model and then compressing it, we directly estimate
a small model. We use a deterministic streaming al-
gorithm (Manku and Motwani, 2002) that computes
approximate frequency counts of frequently occur-
ring n-grams. This scheme is considerably more ac-
curate in getting the actual counts as compared to
other schemes (Demaine et al, 2002; Karp et al,
2003) that find the set of frequent items without car-
ing about the accuracy of counts.
We use these counts directly as features in an
SMT system, and propose a direct way to integrate
these features into an SMT decoder. Experiments
show that directly storing approximate counts of fre-
quent 5-grams compared to using count or entropy-
based pruning counts gives equivalent SMT perfor-
mance, while dramatically reducing the memory us-
age and getting rid of pre-computing a large model.
2 Background
2.1 n-gram Language Models
Language modeling is based on assigning probabil-
ities to sentences. It can either compute the proba-
bility of an entire sentence or predict the probability
of the next word in a sequence. Let wm1 denote a se-
quence of words (w1, . . . , wm). The probability of
estimating word wm depends on previous n-1 words
where n denotes the size of n-gram. This assump-
tion that probability of predicting a current word de-
pends on the previous words is called a Markov as-
sumption, typically estimated by relative frequency:
P (wm | wm?1m?n+1) =
C(wm?1m?n+1wm)
C(wm?1m?n+1)
(1)
Eq 1 estimates the n-gram probability by taking the
ratio of observed frequency of a particular sequence
and the observed frequency of the prefix. This is
precisely the relative frequency estimate we seek.
2.2 Large-scale Language modeling
Using higher order LMs to improve the accuracy
of SMT is not new. (Brants et al, 2007; Emami
et al, 2007) built 5-gram LMs over web using dis-
tributed cluster of machines and queried them via
network requests. Since the use of cluster of ma-
chines is not always practical, (Talbot and Osborne,
2007b; Talbot and Osborne, 2007a) showed a ran-
domized data structure called Bloom filter, that can
be used to construct space efficient language models
513
for SMT. (Talbot and Brants, 2008) presented ran-
domized language model based on perfect hashing
combined with entropy pruning to achieve further
memory reductions. A problem mentioned in (Tal-
bot and Brants, 2008) is that the algorithm that com-
putes the compressed representation might need to
retain the entire database in memory; in their paper,
they design strategies to work around this problem.
(Federico and Bertoldi, 2006) also used single ma-
chine and fewer bits to store the LM probability by
using efficient prefix trees.
(Uszkoreit and Brants, 2008) used partially class-
based LMs together with word-based LMs to im-
prove SMT performance despite the large size of
the word-based models used. (Schwenk and Koehn,
2008; Zhang et al, 2006) used higher language mod-
els at time of re-ranking rather than integrating di-
rectly into the decoder to avoid the overhead of
keeping LMs in the main memory since disk lookups
are simply too slow. Now using higher order LMs at
time of re-ranking looks like a good option. How-
ever, the target n-best hypothesis list is not diverse
enough. Hence if possible it is always better to inte-
grate LMs directly into the decoder.
2.3 Streaming
Consider an algorithm that reads the input from a
read-only stream from left to right, with no ability
to go back to the input that it has already processed.
This algorithm has working storage that it can use to
store parts of the input or other intermediate compu-
tations. However, (and this is a critical constraint),
this working storage space is significantly smaller
than the input stream length. For typical algorithms,
the storage size is of the order of logk N , where N
is the input size and k is some constant.
Stream algorithms were first developed in the
early 80s, but gained in popularity in the late 90s
as researchers first realized the challenges of dealing
with massive data sets. A good survey of the model
and core challenges can be found in (Muthukrish-
nan, 2005). There has been considerable work on the
problem of identifying high-frequency items (items
with frequency above a threshold), and a detailed re-
view of these methods is beyond the scope of this ar-
ticle. A new survey by (Cormode and Hadjielefthe-
riou, 2008) comprehensively reviews the literature.
3 Space-Efficient Approximate Frequency
Estimation
Prior work on approximate frequency estimation for
language models provide a ?no-false-negative? guar-
antee, ensuring that counts for n-grams in the model
are returned exactly, while working to make sure the
false-positive rate remains small (Talbot and Os-
borne, 2007a). The notion of approximation we use
is different: in our approach, it is the actual count
values that will be approximated. We also exploit
the fact that low-frequency n-grams, while consti-
tuting the vast majority of the set of unique n-grams,
are usually smoothed away and are less likely to in-
fluence the language model significantly. Discard-
ing low-frequency n-grams is particularly important
in a stream setting, because it can be shown in gen-
eral that any algorithm that generates approximate
frequency counts for all n-grams requires space lin-
ear in the input stream (Alon et al, 1999).
We employ an algorithm for approximate fre-
quency counting proposed by (Manku and Motwani,
2002) in the context of database management. Fix
parameters s ? (0, 1), and ? ? (0, 1), ? ? s. Our
goal is to approximately find all n-grams with fre-
quency at least sN . For an input stream of n-grams
of length N , the algorithm outputs a set of items
(and frequencies) and guarantees the following:
? All items with frequencies exceeding sN are
output (no false negatives).
? No item with frequency less than (s ? ?)N is
output (few false positives).
? All reported frequencies are less than the true
frequencies by at most ?N (close-to-exact fre-
quencies).
? The space used by the algorithm is
O(1? log ?N).
A simple example illustrates these properties. Let
us fix s = 0.01, ? = 0.001. Then the algorithm guar-
antees that all n-grams with frequency at least 1%
will be returned, no element with frequency less than
0.9% will be returned, and all frequencies will be no
more than 0.1% away from the true frequencies. The
space used by the algorithm is O(logN), which can
be compared to the much larger (close to N ) space
514
needed to store the initial frequency counts. In addi-
tion, the algorithm runs in linear time by definition,
requiring only one pass over the input. Note that
there might be 1? elements with frequency at least
?N , and so the algorithm uses optimal space (up to
a logarithmic factor).
3.1 The Algorithm
We present a high-level overview of the algorithm;
for more details, the reader is referred to (Manku
and Motwani, 2002). The algorithm proceeds by
conceptually dividing the stream into epochs, each
containing 1/? elements. Note that there are ?N
epochs. Each such epoch has an ID, starting from
1. The algorithm maintains a list of tuples1 of the
form (e, f,?), where e is an n-gram, f is its re-
ported frequency, and ? is the maximum error in the
frequency estimation. While the algorithm reads n-
grams associated with the current epoch, it does one
of two things: if the new element e is contained in
the list of tuples, it merely increments the frequency
count f . If not, it creates a new tuple of the form
(e, 1, T ?1), where T is the ID of the current epoch.
After each epoch, the algorithm ?cleans house? by
eliminating tuples whose maximum true frequency
is small. Formally, if the epoch that just ended
has ID T , then the algorithm deletes all tuples sat-
isfying condition f + ? ? T . Since T ? ?N ,
this ensures that no low-frequency tuples are re-
tained. When all elements in the stream have been
processed, the algorithm returns all tuples (e, f,?)
where f ? (s??)N . In practice, however we do not
care about s and return all tuples. At a high level,
the reason the algorithm works is that if an element
has high frequency, it shows up more than once each
epoch, and so its frequency gets updated enough to
stave off elimination.
4 Intrinsic Evaluation
We conduct a set of experiments with approxi-
mate n-gram counts (stream counts) produced by
the stream algorithm. We define various metrics on
which we evaluate the quality of stream counts com-
pared with exact n-gram counts (true counts). To
1We use hash tables to store tuples; however smarter data
structures like suffix trees could also be used.
Corpus Gzip-MB M-wrds Perplexity
EP 63 38 1122.69
afe 417 171 1829.57
apw 1213 540 1872.96
nyt 2104 914 1785.84
xie 320 132 1885.33
Table 3: Corpus Statistics and perplexity of LMs made
with each of these corpuses on development set
evaluate the quality of stream counts on these met-
rics, we carry out three experiments.
4.1 Experimental Setup
The freely available English side of Europarl (EP)
and Gigaword corpus (Graff, 2003) is used for
computing n-gram counts. We only use EP along
with two sections of the Gigaword corpus: Agence
France Press English Service(afe) and The New
York Times Newswire Service (nyt). The unigram
language models built using these corpuses yield
better perplexity scores on the development set (see
Section 5.1) compared to The Xinhua News Agency
English Service (xie) and Associated Press World-
stream English Service (apw) as shown in Table 3.
The LMs are build using the SRILM language mod-
elling toolkit (Stolcke, 2002) with modified Kneser-
Ney discounting and interpolation. The evaluation
of stream counts is done on EP+afe+nyt (EAN) cor-
pus, consisting of 1.1 billion words.
4.2 Description of the metrics
To evaluate the quality of counts produced by our
stream algorithm four different metrics are used.
The accuracy metric measures the quality of top N
stream counts by taking the fraction of top N stream
counts that are contained in the top N true counts.
Accuracy = Stream Counts ? True Counts
True Counts
Spearman?s rank correlation coefficient or Spear-
man?s rho(?) computes the difference between the
ranks of each observation (i.e. n-gram) on two vari-
ables (that are top N stream and true counts). This
measure captures how different the stream count or-
dering is from the true count ordering.
? = 1? 6
? d2i
N(N2 ? 1)
515
di is the difference between the ranks of correspond-
ing elements Xi and Yi; N is the number of elements
found in both sets; Xi and Yi in our case denote the
stream and true counts.
Mean square error (MSE) quantifies the amount
by which a predicted value differs from the true
value. In our case, it estimates how different the
stream counts are from the true counts.
MSE = 1N
N?
i=1
(truei ? predictedi)2
true and predicted denotes values of true and stream
counts; N denotes the number of stream counts con-
tained in true counts.
4.3 Varying ? experiments
In our first experiment, we use accuracy, ? and MSE
metrics for evaluation. Here, we compute 5-gram
stream counts with different settings of ? on the EAN
corpus. ? controls the number of stream counts pro-
duced by the algorithm. The results in Table 4 sup-
port the theory that decreasing the value of ? im-
proves the quality of stream counts. Also, as ex-
pected, the algorithm produces more stream counts
with smaller values of ?. The evaluation of stream
counts obtained with ? = 50e-8 and 20e-8 reveal that
the stream counts learned with this large value are
more susceptible to errors.
If we look closely at the counts for ? = 50e-8, we
see that we get at least 30% of the stream counts
from 245k true counts. This number is not signifi-
cantly worse than the 36% of stream counts obtained
from 4, 018k true counts for the smallest value of
? = 5e-8. However, if we look at the other two met-
rics, the ranking correlation ? of stream counts com-
pared with true counts on ? = 50e-8 and 20e-8 is low
compared to other ? values. For the MSE, the error
with stream counts on these ?m values is again high
compared to other values. As we decrease the value
of ? we continually get better results: decreasing ?
pushes the stream counts towards the true counts.
However, using a smaller ? increases the memory
usage. Looking at the evaluation, it is therefore ad-
visable to use 5-gram stream counts produced with
at most ? ? 10e-7 for the EAN corpus.
Since it is not possible to compute true 7-grams
counts on EAN with available computing resources,
? 5-gram Acc ? MSEproduced
50e-8 245k 0.294 -3.6097 0.4954
20e-8 726k 0.326 -2.6517 0.1155
10e-8 1655k 0.352 -1.9960 0.0368
5e-8 4018k 0.359 -1.7835 0.0114
Table 4: Evaluating quality of 5-gram stream counts for
different settings of ? on EAN corpus
? 7-gram Acc ? MSEproduced
50e-8 44k 0.509 0.3230 0.0341
20e-8 128k 0.596 0.5459 0.0063
10e-8 246k 0.689 0.7413 0.0018
5e-8 567k 0.810 0.8599 0.0004
Table 5: Evaluating quality of 7-gram stream counts for
different settings of ? on EP corpus
we carry out a similar experiment for 7-grams on EP
to verify the results for higher order n-grams 2. The
results in Table 5 tell a story similar to our results for
7-grams. The size of EP corpus is much smaller than
EAN and so we see even better results on each of the
metrics with decreasing the value of ?. The overall
trend remains the same; here too, setting ? ? 10e-
8 is the most effective strategy. The fact that these
results are consistent across two datasets of different
sizes and different n-gram sizes suggests that they
will carry over to other tasks.
4.4 Varying top K experiments
In the second experiment, we evaluate the quality
of the top K (sorted by frequency) 5-gram stream
counts. Here again, we use accuracy, ? and MSE for
evaluation. We fix the value of ? to 5e-8 and com-
pute 5-gram stream counts on the EAN corpus. We
vary the value of K between 100k and 4, 018k (i.e
all the n-gram counts produced by the stream algo-
rithm). The experimental results in Table 6 support
the theory that stream count algorithm computes the
exact count of most of the high frequency n-grams.
Looking closer, we see that if we evaluate the algo-
rithm on just the top 100k 5-grams (roughly 5% of
all 5-grams produced), we see almost perfect results.
Further, if we take the top 1, 000k 5-grams (approx-
imately 25% of all 5-grams) we again see excellent
2Similar evaluation scores are observed for 9-gram stream
counts with different values of ? on EP corpus.
516
Top K Accuracy ? MSE
100k 0.994 0.9994 0.01266
500k 0.934 0.9795 0.0105
1000k 0.723 0.8847 0.0143
2000k 0.504 0.2868 0.0137
4018k 0.359 -1.7835 0.0114
Table 6: Evaluating top K sorted 5-gram stream counts
for ?=5e-8 on EAN corpus
performance on all metrics. The accuracy of the re-
sults decrease slightly, but the ? and MSE metrics
are not decreased that much in comparison. Perfor-
mance starts to degrade as we get to 2, 000k (over
50% of all 5-grams), a result that is not too surpris-
ing. However, even here we note that the MSE is
low, suggesting that the frequencies of stream counts
(found in top K true counts) are very close to the
true counts. Thus, we conclude that the quality of
the 5-gram stream counts produced for this value of
? is quite high (in relation to the true counts).
As before, we corroborate our results with higher
order n-grams. We evaluate the quality of top K 7-
gram stream counts on EP.3 Since EP is a smaller
corpus, we evaluate the stream counts produced by
setting ? to 10e-8. Here we vary the value of K be-
tween 10k and 246k (the total number produced by
the stream algorithm). Results are shown in Table
7. As we saw earlier with 5-grams, the top 10k (i.e.
approximately 5% of all 7-grams) are of very high
quality. Results, and this remains true even when
we increase K to 100k. There is a drop in the accu-
racy and a slight drop in ?, while the MSE remains
the same. Taking all counts again shows a signifi-
cant decrease in both accuracy and ? scores, but this
does not affect MSE scores significantly. Hence, the
7-gram stream counts i.e. 246k counts produced by
? = 10e-8 are quite accurate when compared to the
top 246k true counts.
4.5 Analysis of tradeoff between coverage and
space
In our third experiment, we investigate whether a
large LM can help MT performance. We evaluate
the coverage of stream counts built on the EAN cor-
pus on the test data for SMT experiments (see Sec-
3Similar evaluation scores are observed for different top K
sorted 9-gram stream counts with ?=10e-8 on EP corpus.
Top K Accuracy ? MSE
10k 0.996 0.9997 0.0015
20k 0.989 0.9986 0.0016
50k 0.950 0.9876 0.0016
100k 0.876 0.9493 0.0017
246k 0.689 0.7413 0.0018
Table 7: Evaluating top K sorted 7-gram stream counts
for ?=10e-8 on EP corpus
tion 5.1) with different values of ?m. We compute
the recall of each model against 3071 sentences of
test data where recall is the fraction of number of
n-grams of a dataset found in stream counts.
Recall = Number of n-grams found in stream counts
Number of n-grams in dataset
We build unigram, bigram, trigram, 5-gram and
7-gram with four different values of ?. Table 8 con-
tains the gzip size of the count file and the recall
of various different stream count n-grams. As ex-
pected, the recall with respect to true counts is max-
imum for unigrams, bigrams, trigrams and 5-grams.
However the amount of space required to store all
true counts in comparison to stream counts is ex-
tremely high: we need 4.8GB of compressed space
to store all the true counts for 5-grams.
For unigram models, we see that the recall scores
are good for all values of ?. If we compare the
approximate stream counts produced by largest ?
(which is worst) to all true counts, we see that the
stream counts compressed size is 50 times smaller
than the true counts size, and is only three points
worse in recall. Similar trends hold for bigrams,
although the loss in recall is higher. As with uni-
grams, the loss in recall is more than made up for by
the memory savings (a factor of nearly 150). For
trigrams, we see a 14 point loss in recall for the
smallest ?, but a memory savings of 400 times. For
5-grams, the best recall value is .020 (1.2k out of
60k 5-gram stream counts are found in the test set).
However, compared with the true counts we only
loss a recall of 0.05 (4.3k out of 60k) points but
memory savings of 150 times. In extrinsic evalua-
tions, we will show that integrating 5-gram stream
counts with an SMT system performs slightly worse
than the true counts, while dramatically reducing the
memory usage.
517
N -gram unigram bigram trigram 5-gram 7-gram
? Gzip Recall Gzip Recall Gzip Recall Gzip Recall Gzip RecallMB MB MB MB MB
50e-8 .352 .785 2.3 .459 3.3 .167 1.9 .006 .864 5.6e-5
20e-8 .568 .788 4.5 .494 7.6 .207 5.3 .011 2.7 1.3e-4
10e-8 .824 .791 7.6 .518 15 .237 13 .015 9.7 4.1e-4
5e-8 1.3 .794 13 .536 30 .267 31 .020 43 5.9e-4
all 17 .816 228 .596 1200 .406 4800 .072 NA
Table 8: Gzipped space required to store n-gram counts on disk and their coverage on a test set with different ?m
For 7-gram we can not compute the true n-gram
counts due to limitations of available computational
resources. The memory requirements with smallest
value of ? are similar to those of 5-gram, but the re-
call values are quite small. For 7-grams, the best re-
call value is 5.9e-4 which means that stream counts
contains only 32 out of 54k 7-grams contained in
test set. The small recall value for 7-grams suggests
that these counts may not be that useful in SMT.
We further substantiate our findings in our extrinsic
evaluations. There we show that integrating 7-gram
stream counts with an SMT system does not affect
its overall performance significantly.
5 Extrinsic Evaluation
5.1 Experimental Setup
All the experiments conducted here make use of
publicly available resources. Europarl (EP) corpus
French-English section is used as parallel data. The
publicly available Moses4 decoder is used for train-
ing and decoding (Koehn and Hoang, 2007). The
news corpus released for ACL SMT workshop in
2007 consisting of 1057 sentences5 is used as the de-
velopment set. Minimum error rate training (MERT)
is used on this set to obtain feature weights to opti-
mize translation quality. The final SMT system per-
formance is evaluated on a uncased test set of 3071
sentences using the BLEU (Papineni et al, 2002),
NIST (Doddington, 2002) and METEOR (Banerjee
and Lavie, 2005) scores. The test set is the union of
the 2007 news devtest and 2007 news test data from
ACL SMT workshop 2007.6
4http://www.statmt.org/moses/
5http://www.statmt.org/wmt07/
6We found that testing on Parliamentary test data was com-
pletely insensitive to large n-gram LMs, even when these LMs
are exact. This suggests that for SMT performance, more data
5.2 Integrating stream counts feature into
decoder
Our method only computes high-frequency n-gram
counts; it does not estimate conditional probabili-
ties. We can either turn these counts into conditional
probabilities (by using SRILM) or use the counts di-
rectly. We observed no significant difference in per-
formance between these two approaches. However,
using the counts directly consumes significantly less
memory at run-time and is therefore preferable. Due
to space constraints, SRILM results are omitted.
The only remaining open question is: how should
we turn the counts into a feature that can be used in
an SMT system? We considered several alternatives;
the most successful was a simple weighted count
of n-gram matches of varying size, appropriately
backed-off. Specifically, consider an n-gram model.
For every sequence of words wi, . . . , wi+N?1, we
obtain a feature score computed recursively accord-
ing to Eq (2).
f(wi) = log
?
C(wi)
Z
?
(2)
f(wi, . . . , wi+k) = log
?
C(wi, . . . , wi+k)
Z
?
+ 12f(wi+1, . . . , wi+k)
Here, 12 is the backoff factor and Z is the largest
count in the count set (the presence of Z is simply to
ensure that these values remain manageable). In or-
der to efficiently compute these features, we store
the counts in a suffix-tree. The computation pro-
ceeds by first considering wi+N?1 alone and then
?expanding? to consider the bigram, then trigram
and so on. The advantage to this order of computa-
tion is that the recursive calls can cease whenever a
is better only if it comes from the right domain.
518
n-gram(?) BLEU NIST MET MemGB
3 EP(exact) 25.57 7.300 54.48 2.7
5 EP(exact) 25.79 7.286 54.44 2.9
3 EAN(exact) 27.04 7.428 55.07 4.6
5 EAN(exact) 28.73 7.691 56.32 20.5
4(10e-8) 27.36 7.506 56.19 2.7
4(5e-8) 27.40 7.507 55.90 2.8
5(10e-8) 27.97 7.605 55.52 2.8
5(5e-8) 27.98 7.611 56.07 2.8
7(10e-8) 27.97 7.590 55.88 2.9
7(5e-8) 27.88 7.577 56.01 2.9
9(10e-8) 28.18 7.611 55.95 2.9
9(5e-8) 27.98 7.608 56.08 2.9
Table 9: Evaluating SMT with different LMs on EAN.
Results are according to BLEU, NIST and MET metrics.
Bold #s are not statistically significant worse than exact.
zero count is reached. (Extending Moses to include
this required only about 100 lines of code.)
5.3 Results
Table 9 summarizes SMT results. We have 4 base-
line LMs that are conventional LMs smoothed using
modified Kneser-Ney smoothing. The first two tri-
gram and 5-gram LMs are built on EP corpus and
the other two are built on EAN corpus. Table 9
show that there is not much significant difference
in SMT results of 5-gram and trigram LM on EP.
As expected, the trigram built on the large corpus
EAN gets an improvement of 1.5 Bleu Score. How-
ever, unlike the EP corpus, building a 5-gram LM
on EAN (huge corpus) gets an improvement of 3.2
Bleu Score. (The 95% statistical significance bound-
ary is about ? 0.53 Bleu on the test data, 0.077 Nist
and 0.16 Meteor according to bootstrap resampling)
We see similar gains in Nist and Meteor metrics as
shown in Table 9.
We use stream counts computed with two values
of ?, 5e-8 and 10e-8 on EAN corpus. We use all
the stream counts produced by the algorithm. 4, 5, 7
and 9 order n-gram stream counts are computed with
these settings of ?. These counts are used along with
a trigram LM built on EP to improve SMT perfor-
mance. The memory usage (Mem) shown in Table
9 is the full memory size required to run on the test
data (including phrase tables).
Adding 4-gram and 5-gram stream counts as fea-
ture helps the most. The performance gain by using
5-gram stream counts is slightly worse than com-
pared to true 5-gram LM on EAN. However, using
5-gram stream counts directly is more memory ef-
ficient. Also, the gains for stream counts are ex-
actly the same as we saw for same sized count-
based and entropy-based pruning counts in Table 1
and 2 respectively. Moreover, unlike the pruning
methods, our algorithm directly computes a small
model, as opposed to compressing a pre-computed
large model.
Adding 7-gram and 9-gram does not help signifi-
cantly, a fact anticipated by the low recall of 7-gram-
based counts that we saw in Section 4.5. The results
with two different settings of ? are largely the same.
This validates our intrinsic evaluation results in Sec-
tion 4.3 that stream counts learned using ? ? 10e-8
are of good quality, and that the quality of the stream
counts is high.
6 Conclusion
We have proposed an efficient, low-memory method
to construct high-order approximate n-gram LMs.
Our method easily scales to billion-word monolin-
gual corpora on conventional (8GB) desktop ma-
chines. We have demonstrated that approximate n-
gram features could be used as a direct replacement
for conventional higher order LMs in SMT with
significant reductions in memory usage. In future,
we will be looking into building streaming skip n-
grams, and other variants (like cluster n-grams).
In NLP community, it has been shown that having
more data results in better performance (Ravichan-
dran et al, 2005; Brants et al, 2007; Turney, 2008).
At web scale, we have terabytes of data and that can
capture broader knowledge. Streaming algorithm
paradigm provides a memory and space-efficient
platform to deal with terabytes of data. We hope
that other NLP applications (where we need to com-
pute relative frequencies) like noun-clustering, con-
structing syntactic rules for SMT, finding analogies,
and others can also benefit from streaming methods.
We also believe that stream counts can be applied to
other problems involving higher order LMs such as
speech recognition, information extraction, spelling
correction and text generation.
519
References
Noga Alon, Yossi Matias, and Mario Szegedy. 1999. The
space complexity of approximating the frequency mo-
ments. J. Comput. Syst. Sci., 58(1).
Satanjeev Banerjee and Alon Lavie. 2005. Meteor:
An automatic metric for MT evaluation with improved
correlation with human judgments. In Proceedings of
the ACL Workshop on Intrinsic and Extrinsic Evalu-
ation Measures for Machine Translation and/or Sum-
marization.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och,
and Jeffrey Dean. 2007. Large language models in
machine translation. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL).
S. Chen and J. Goodman. 1996. An Empirical Study
of Smoothing Techniques for Language Modeling. In
Proceedings of 34th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 310?318,
Santa Cruz, CA, June.
Graham Cormode and Marios Hadjieleftheriou. 2008.
Finding frequent items in data streams. In VLDB.
E.D. Demaine, A. Lopez-Ortiz, and J.I. Munro. 2002.
Frequency estimation of internet packet streams with
limited space.
George Doddington. 2002. Automatic evaluation of ma-
chine translation quality using n-gram co-occurrence
statistics. In Proceedings of the second interna-
tional conference on Human Language Technology
Research.
Ahmad Emami, Kishore Papineni, and Jeffrey Sorensen.
2007. Large-scale distributed language modeling. In
Proceedings of the 2007 IEEE International Confer-
ence on Acoustics, Speech, and Signal Processing
(ICASSP), volume 4, pages 37?40.
Marcello Federico and Nicola Bertoldi. 2006. How
many bits are needed to store probabilities for phrase-
based translation? In Proceedings on the Workshop on
Statistical Machine Translation at ACL06.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In Pro-
ceedings of HLT/NAACL-04.
D. Graff. 2003. English Gigaword. Linguistic Data Con-
sortium, Philadelphia, PA, January.
Richard M. Karp, Christos H. Papadimitriou, and Scott
Shenker. 2003. A simple algorithm for finding fre-
quent elements in streams and bags.
Philipp Koehn and Hieu Hoang. 2007. Factored transla-
tion models. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing (EMNLP-CoNLL), pages 868?876.
G. S. Manku and R. Motwani. 2002. Approximate fre-
quency counts over data streams. In Proceedings of
the 28th International Conference on Very Large Data
Bases.
S. Muthukrishnan. 2005. Data streams: Algorithms and
applications. Foundations and Trends in Theoretical
Computer Science, 1(2).
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation.
Deepak Ravichandran, Patrick Pantel, and Eduard Hovy.
2005. Randomized algorithms and nlp: using locality
sensitive hash function for high speed noun clustering.
In ACL ?05: Proceedings of the 43rd Annual Meeting
on Association for Computational Linguistics.
Holger Schwenk and Philipp Koehn. 2008. Large and
diverse language models for statistical machine trans-
lation. In Proceedings of The Third International Joint
Conference on Natural Language Processing (IJCNP).
Andreas Stolcke. 1998. Entropy-based pruning of back-
off language models. In In Proc. DARPA Broadcast
News Transcription and Understanding Workshop.
A. Stolcke. 2002. SRILM ? An Extensible Language
Modeling Toolkit. In Proceedings of the 7th Inter-
national Conference on Spoken Language Processing,
pages 901?904, Denver, CO, September.
David Talbot and Thorsten Brants. 2008. Randomized
language models via perfect hash functions. In Pro-
ceedings of ACL-08: HLT.
David Talbot and Miles Osborne. 2007a. Randomised
language modelling for statistical machine translation.
In Proceedings of the 45th Annual Meeting of the As-
sociation of Computational Linguistics.
David Talbot and Miles Osborne. 2007b. Smoothed
Bloom filter language models: Tera-scale LMs on the
cheap. In Proceedings of the 2007 Joint Conference on
Empirical Methods in Natural Language Processing
and Computational Natural Language Learning (EM
NLP-CoNLL).
Peter D. Turney. 2008. A uniform approach to analogies,
synonyms, antonyms, and associations. In Proceed-
ings of COLING 2008.
Jakob Uszkoreit and Thorsten Brants. 2008. Distributed
word clustering for large scale class-based language
modeling in machine translation. In Proceedings of
ACL-08: HLT.
Ying Zhang, Almut Silja Hildebrand, and Stephan Vogel.
2006. Distributed language modeling for n-best list
re-ranking. In Proceedings of the 2006 Conference on
Empirical Methods in Natural Language Processing.
520
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 77?86,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Automatically Producing Plot Unit Representations for Narrative Text
Amit Goyal
Dept. of Computer Science
University of Maryland
College Park, MD 20742
amit@umiacs.umd.edu
Ellen Riloff
School of Computing
University of Utah
Salt Lake City, UT 84112
riloff@cs.utah.edu
Hal Daume? III
Dept. of Computer Science
University of Maryland
College Park, MD 20742
hal@umiacs.umd.edu
Abstract
In the 1980s, plot units were proposed as a
conceptual knowledge structure for represent-
ing and summarizing narrative stories. Our
research explores whether current NLP tech-
nology can be used to automatically produce
plot unit representations for narrative text. We
create a system called AESOP that exploits
a variety of existing resources to identify af-
fect states and applies ?projection rules? to
map the affect states onto the characters in a
story. We also use corpus-based techniques
to generate a new type of affect knowledge
base: verbs that impart positive or negative
states onto their patients (e.g., being eaten is
an undesirable state, but being fed is a desir-
able state). We harvest these ?patient polar-
ity verbs? from a Web corpus using two tech-
niques: co-occurrence with Evil/Kind Agent
patterns, and bootstrapping over conjunctions
of verbs. We evaluate the plot unit representa-
tions produced by our system on a small col-
lection of Aesop?s fables.
1 Introduction
In the 1980s, plot units (Lehnert, 1981) were pro-
posed as a knowledge structure for representing nar-
rative stories and generating summaries. Plot units
are fundamentally different from the story represen-
tations that preceded them because they focus on the
affect states of characters and the tensions between
them as the driving force behind interesting and co-
hesive stories. Plot units were used in narrative sum-
marization studies, both in computer science and
psychology (Lehnert et al, 1981), but previous com-
putational models of plot units relied on tremendous
amounts of manual knowledge engineering.
The last few decades have seen tremendous ad-
vances in NLP and the emergence of many resources
that could be useful for plot unit analysis. So we em-
barked on a project to see whether plot unit repre-
sentations can be generated automatically using cur-
rent NLP technology. We created a system called
AESOP that uses a variety of resources to iden-
tify words that correspond to positive, negative, and
mental affect states. AESOP uses affect projection
rules to map the affect states onto the characters in
the story based on verb argument structure. Addi-
tionally, affect states are inferred based on syntactic
properties, and causal and cross-character links are
created using simple heuristics.
Affect states often arise from actions that produce
good or bad states for the character that is acted
upon. For example, ?the cat ate the mouse? pro-
duces a negative state for the mouse because being
eaten is bad. Similarly, ?the man fed the dog? pro-
duces a positive state for the dog because being fed
is generally good. Knowledge about the effects of
actions (i.e., state changes) on patients is not readily
available in existing semantic resources. We create
a new type of lexicon consisting of patient polarity
verbs (PPVs) that impart positive or negative states
on their patients. These verbs reflect world knowl-
edge about desirable/undesirable states for animate
beings; for example, being fed, paid or adopted are
generally desirable states, while being eaten, chased
or hospitalized are generally undesirable states.
We automatically generate a lexicon of ?patient
polarity verbs? from a Web corpus using two tech-
77
The Father and His Sons
(s1) A father had a family of sons who were perpetually
quarreling among themselves. (s2) When he failed to
heal their disputes by his exhortations, he determined
to give them a practical illustration of the evils of dis-
union; and for this purpose he one day told them to
bring him a bundle of sticks. (s3) When they had done
so, he placed the faggot into the hands of each of them
in succession, and ordered them to break it in pieces.
(s4) They tried with all their strength, and were not
able to do it. (s5) He next opened the faggot, took the
sticks separately, one by one, and again put them into
his sons? hands, upon which they broke them easily.
(s6) He then addressed them in these words: ?My sons,
if you are of one mind, and unite to assist each other,
you will be as this faggot, uninjured by all the attempts
of your enemies; but if you are divided among your-
selves, you will be broken as easily as these sticks.?
(a) ?Father and Sons? Fable
Father Sons
(quarreling)a1
(stop quarreling)a3
(annoyed)a2
(exhortations)a4
(exhortations fail)a5
m
m
a
(teach lesson)a6
m
(get sticks & break)a7m (get sticks & break)a8
(cannot break sticks)a9
a
(cannot break sticks)a10
a
(bundle & break)a11 (bundle & break)a12
(break sticks)a13
a
(break sticks)a14
a
m
a
shared
request
request
mixed
shared
s2
s2
s2
s2
s2
s2
s4
s5
s5
s1
s2
s4
s5
s5
(lesson succeeds)a15s5
(b) Plot Unit Analysis for ?Father and Sons? Fable
Figure 1: Sample Fable and Plot Unit Representation
niques: patterns that identify co-occurrence with
stereotypically evil or kind agents, and a bootstrap-
ping algorithm that learns from conjunctions of
verbs. We evaluate the plot unit representations pro-
duced by our system on a small collection of fables.
2 Overview of Plot Units
Plot unit structures consist of affect states for each
character, and links defining the relationships be-
tween them. Plot units include three types of affect
states: positive (+), negative (-), and mental (M).
Affect states can be connected by causal links and
cross-character links, which explain how the nar-
rative hangs together. Causal links exist between
affect states for the same character and have four
types: motivation (m), actualization (a), termination
(t) and equivalence (e). Cross-character links indi-
cate that a single event affects multiple characters.
For instance, if one character requests something of
another, then each character is assigned an M state
and a cross-character link connects the states.
To see a concrete example of a plot unit represen-
tation, a short fable, ?The Father and His Sons,? is
shown in Figure 1(a) and our annotation of its plot
unit structure is shown in Figure 1(b). In this fable,
there are two characters, the ?Father? and (collec-
tively) the ?Sons?, who go through a series of affect
states depicted chronologically in the two columns.
The first affect state (a1) is produced from sen-
tence #1 (s1) and is a negative state for the sons be-
cause they are quarreling. This state is shared by the
father (via a cross-character link) who has a nega-
tive annoyance state (a2). The father decides that
he wants to stop the sons from quarreling, which
is a mental event (a3). The causal link from a2 to
a3 with an m label indicates that his annoyed state
?motivated? this decision. His first attempt is by ex-
hortations (a4). The first M (a3) is connected to the
second M (a4) with an m (motivation) link, which
represents subgoaling. The father?s overall goal is
to stop the quarreling (a3), and to do so he creates a
subgoal of exhorting the sons to stop (a4). The ex-
hortations fail, which produces a negative state (a5)
for the father. The a causal link indicates an ?actu-
alization?, representing the failure of his plan (a4).
This failure motivates a new subgoal: teach the
sons a lesson (a6). At a high level, this subgoal
has two parts, indicated by the two gray regions
(a7 ? a10 and a11 ? a14). The first gray region
begins with a cross-character link (M to M), which
indicates a request (in this case, to break a bundle
of sticks). The sons fail at this, which upsets them
(a9) but pleases the father (a10). The second gray
region depicts the second part of the father?s sub-
goal; he makes a second request (a11 to a12) to sep-
arate the bundle and break the sticks, which the sons
successfully do, making them happy (a13) and the
father happy (a14) as well. This latter structure (the
second gray region) is an HONORED REQUEST plot
unit structure. At the end, the father?s plan succeeds
(a15) which is an actualization (a link) of his goal
to teach the sons a lesson (a6).
78
3 Where Do Affect States Come From?
We briefly overview the variety of situations that can
be represented by affect states in plot units.
Direct Expressions of Emotion: Affect states can
correspond to positive/negative emotional states, as
have been studied in the realm of sentiment anal-
ysis. For example, ?Max was disappointed? pro-
duces a negative affect state for Max, and ?Max was
pleased? produces a positive affect state for Max.
Situational Affect States: Positive and negative af-
fect states can represent good and bad situational
states that characters find themselves in. These
states do not represent emotion, but indicate whether
a situation (state) is good or bad for a character
based on world knowledge. e.g., ?The wolf had a
bone stuck in his throat.? produces a negative affect
state for the wolf. Similarly, ?The woman recovered
her sight.? produces a positive affect state for the
woman.
Plans and Goals: The existence of a plan or goal is
represented as a mental state (M). Plans and goals
can be difficult to detect automatically and can be
revealed in many ways, such as:
? Direct expressions of plans/goals: a plan/goal
may be explicitly stated (e.g., ?John wants food?).
? Speech acts: a plan or goal may be revealed
through a speech act. For example, ?the wolf asked
an eagle to extract the bone? is a directive speech
act that indicates the wolf?s plan to resolve its
negative state (having a bone stuck). This example
illustrates how a negative state (bone stuck) can
motivate a mental state (plan). When a speech act
involves multiple characters, it produces multiple
mental states.
? Inferred plans/goals: plans and goals are some-
times inferred from actions. e.g., ?the lion hunted
deer? implies that the lion has a plan to obtain food.
Similarly, ?the serpent spat poison at John? implies
that the serpent wants to kill John.
? Plan/Goal completion: Plans and goals produce
+/- affect states when they succeed or fail. For
example, if the eagle successfully extracts the bone
from the wolf?s throat, then both the wolf and the
eagle will have positive affect states because both
were successful in their respective goals.
We observed that situational and plan/goal states
often originate from an action. When a character is
acted upon (the patient of a verb), then the charac-
ter may be in a positive or negative state depend-
ing upon whether the action was good or bad for
them based on world knowledge. For example, be-
ing fed, paid or adopted is generally desirable, but
being chased, eaten, or hospitalized is usually unde-
sirable. Consequently, we decided to create a lex-
icon of patient polarity verbs that produce positive
or negative states for their patients. In Section 4.2,
we present two methods for automatically harvest-
ing these verbs from a Web corpus.
4 AESOP: Automatically Generating Plot
Unit Representations
Our system, AESOP, automatically creates plot unit
representations for narrative text. AESOP has four
main steps: affect state recognition, character iden-
tification, affect state projection, and link creation.
During affect state recognition, AESOP identifies
words that may be associated with positive, nega-
tive, and mental states. AESOP then identifies the
main characters in the story and applies affect pro-
jection rules to map the affect states onto these char-
acters. During this process, some additional affect
states are inferred based on verb argument structure.
Finally, AESOP creates cross-character links and
causal links between affect states. We also present
two corpus-based methods to automatically produce
a new resource for affect state recognition: a patient
polarity verb lexicon.
4.1 Plot Unit Creation
4.1.1 Recognizing Affect States
The basic building blocks of plot units are af-
fect states which come in three flavors: positive,
negative, and mental. In recent years, many pub-
licly available resources have been created for sen-
timent analysis and other types of semantic knowl-
edge. We considered a wide variety of resources and
ultimately decided to experiment with five resources
that most closely matched our needs:
? FrameNet (Baker et al, 1998): We manually
identified 87 frame classes that seem to be associ-
ated with affect: 43 mental classes (e.g., COMMU-
NICATION and NEEDING), 22 positive classes (e.g.,
ACCOMPLISHMENT and SUPPORTING), and 22 neg-
ative classes (e.g., CAUSE HARM and PROHIBIT-
79
ING). We use the verbs listed for these classes to
produce M, +, and - affect states.
?MPQA Lexicon (Wilson et al, 2005b): We used
the words listed as having positive or negative senti-
ment polarity to produce +/- states, when they occur
with the designated part-of-speech.
? OpinionFinder (Wilson et al, 2005a) (Version
1.4) : We used the +/- labels assigned by its con-
textual polarity classifier (Wilson et al, 2005b) to
create +/- states and the MPQASD tags produced
by its Direct Subjective and Speech Event Identifier
(Choi et al, 2006) to produce mental (M) states.
? Semantic Orientation Lexicon (Takamura et al,
2005): We used the words listed as having posi-
tive or negative polarity to produce +/- affect states,
when they occur with the designated part-of-speech.
? Speech Act Verbs: We used 228 speech act
verbs from (Wierzbicka, 1987) to produce M states.
4.1.2 Identifying the Characters
For the purposes of this work, we made two sim-
plifying assumptions: (1) There are only two char-
acters per fable1, and (2) Both characters are men-
tioned in the fable?s title. The problem of corefer-
ence resolution for fables is somewhat different than
for other genres, primarily because the characters
are often animals (e.g., he=owl). So we hand-crafted
a simple rule-based coreference system. First, we
apply heuristics to determine number and gender
based on word lists, WordNet (Miller, 1990) and
part-of-speech tags. If no determination of a char-
acter?s gender or number can be made, we employ a
process of elimination. Given the two character as-
sumption, if one character is known to be male, but
there are female pronouns in the fable, then the other
character is assumed to be female. The same is done
for number agreement. Finally, if there is only one
character between a pronoun and the beginning of
a document, then we resolve the pronoun with that
character and the character assumes the gender and
number of the pronoun. Lastly, WordNet provides
some additional resolutions by exploiting hypernym
relations, for instance, linking peasant with man.
4.1.3 Mapping Affect States onto Characters
Plot unit representations are not just a set of af-
fect states, but they are structures that capture the
1We only selected fables that had two main characters.
chronological ordering of states for each character
as the narrative progresses. Consequently, every af-
fect state needs to be attributed to a character. Since
most plots revolve around events, we use verb argu-
ment structure as the primary means for projecting
affect states onto characters.
We developed four affect projection rules that or-
chestrate how affect states are assigned to the char-
acters. We used the Sundance parser (Riloff and
Phillips, 2004) to produce a shallow parse of each
sentence, which includes syntactic chunking, clause
segmentation, and active/passive voice recognition.
We normalized the verb phrases with respect to ac-
tive/passive voice to simplify the rules. We made the
assumption that the Subject of the VP is its AGENT
and the Direct Object of the VP is its PATIENT.2
The rules only project affect states onto AGENTS
and PATIENTS that refer to a character in the story.
The four projection rules are presented below.
1. AGENT VP : This rule applies when the VP
has no PATIENT or the PATIENT corefers with the
AGENT. All affect tags assigned to the VP are pro-
jected onto the AGENT. Example: ?Mary laughed
(+)? projects a + affect state onto Mary.
2. VP PATIENT : This rule applies when the VP
has no agent, which is common in passive voice con-
structions. All affect tags assigned to the VP are
projected onto the PATIENT. Example: ?John was
rewarded (+), projects a + affect state onto John.
3. AGENT VP PATIENT : This rules applies
when both an AGENT and PATIENT are present, do
not corefer, and at least one of them is a character. If
the PATIENT is a character, then all affect tags asso-
ciated with the VP are projected onto the PATIENT.
If the AGENT is a character and the VP has an M
tag, then we also project an M tag onto the AGENT
(representing a shared, cross-character mental state).
4. AGENT VERB1 to VERB2 PATIENT : This
rule has two cases: (a) If the AGENT and PATIENT
refer to the same character, then we apply Rule #1.
Example: ?Bo decided to teach himself...? (b) If the
AGENT and PATIENT are different, then we apply
Rule #1 to VERB1 and Rule #2 to VERB2.
Finally, if an adverb or adjectival phrase has af-
fect, then that affect is mapped onto the preceding
VP and the rules above are applied. For all of the
2This is not always correct, but worked ok in our fables.
80
rules, if a clause contains a negation word, then we
flip the polarity of all words in that clause.
4.1.4 Inferring Affect States
Recognizing plans and goals depends on world
knowledge and inference, and is beyond the scope
of this paper. However, we identified two cases
where affect states often can be inferred based on
syntactic properties. The first case involves verb
phrases (VPs) that have both an AGENT and PA-
TIENT, which corresponds to projection rule #3. If
the VP has polarity, then rule #3 assigns that po-
larity to the PATIENT, not the AGENT. For exam-
ple, ?John killed Paul? imparts negative polarity on
Paul, but not necessarily on John. Unless we are
told otherwise, one assumes that John intentionally
killed Paul, and so in a sense, John accomplished
his goal. Consequently, this action should produce a
positive affect state for John. We capture this notion
of accomplishment as a side effect of projection rule
#3: if the VP has +/- polarity, then we produce an
inferred positive state for the AGENT.
The second case involves infinitive verb phrases
of the form: ?AGENT VERB1 TO VERB2 PA-
TIENT? (e.g., ?Susan tried to warn Mary?). The
infinitive VP construction suggests that the AGENT
has a goal or plan that is being put into motion (e.g.,
tried to, wanted to, attempted to, hoped to, etc.). To
capture this intuition, in rule #4 if VERB1 does not
already have an affect state assigned to it then we
produce an inferred mental state for the AGENT.
4.1.5 Causal and Cross-Character Links
Our research is focused primarily on creating af-
fect states for characters, but plot unit structures
also include cross-character links to connect states
that are shared across characters and causal links
between states for a single character. As an ini-
tial attempt to create complete plot units, AESOP
produces links using simple heuristics. A cross-
character link is created when two characters in a
clause have affect states that originated from the
same word. A causal link is created between each
pair of (chronologically) consecutive affect states
for the same character. Currently, AESOP only pro-
duces forward causal links (motivation (m), actual-
ization (a)) and does not produce backward causal
links (equivalence (e), termination (t)). For forward
links, the causal syntax only allows for five cases:
M m? M , + m? M , ? m? M , M a? +, M a? ?.
So when AESOP produces a causal link between
two affect states, the order and types of the two states
uniquely determine which label it gets (m or a).
4.2 Generating PPV Lexicons
During the course of this research, we identified a
gap in currently available knowledge: we are not
aware of existing resources that identify verbs which
produce a desirable/undesirable state for their pa-
tients even though the verb itself does not carry po-
larity. For example, the verb eat describes an action
that is generally neutral, but being eaten is clearly
an undesirable state. Similarly, the verb fed does not
have polarity, but being fed is a desirable state for the
patient. In the following sections, we try to fill this
gap by using corpus-based techniques to automati-
cally acquire a Patient Polarity Verb (PPV) Lexicon.
4.2.1 PPV Harvesting with Evil/Kind Agents
The key idea behind our first approach is to iden-
tify verbs that frequently occur with evil or kind
agents. Our intuition was that an ?evil? agent will
typically perform actions that are bad for the patient,
while a ?kind? agent will typically perform actions
that are good for the patient.
We manually identified 40 stereotypically evil
agent words, such as monster, villain, terrorist, and
murderer, and 40 stereotypically kind agent words,
such as hero, angel, benefactor, and rescuer. We
searched the Google Web 1T N-gram corpus to
identify verbs that co-occur with these words as
probable agents. For each agent term, we applied
the pattern ?* by [a,an,the] AGENT? and extracted
the matching N-grams. Then we applied a part-of-
speech tagger to each N-gram and saved the words
that were tagged as verbs (i.e., the words in the *
position).3 This process produced 811 negative (evil
agent) PPVs and 1362 positive (kind agent) PPVs.
4.2.2 PPV Bootstrapping over Conjunctions
Our second approach for acquiring PPVs is based
on an observation from sentiment analysis research
that conjoined adjectives typically have the same po-
larity (e.g. (Hatzivassiloglou and McKeown, 1997)).
3The POS tagging quality is undoubtedly lower than if tag-
ging complete sentences but it seemed reasonable.
81
Our hypothesis was that conjoined verbs often share
the same polarity as well (e.g., ?abducted and
killed? or ?rescued and rehabilitated?). We exploit
this idea inside a bootstrapping algorithm to itera-
tively learn verbs that co-occur in conjunctions.
Bootstrapping begins with 10 negative and 10
positive PPV seeds. First, we extracted triples of
the form ?w1 and w2? from the Google Web 1T
N -gram corpus that had frequency ? 100 and were
lower case. We separated each conjunction into
two parts: a primary VERB (?w1?) and a CONTEXT
(?and w2?), and created a copy of the conjunction
with the roles of w1 and w2 reversed. For example,
?rescued and adopted? produces:
VERB=?rescued? CONTEXT=?and adopted?
VERB=?adopted? CONTEXT=?and rescued?
Next, we applied the Basilisk bootstrapping al-
gorithm (Thelen and Riloff, 2002) to learn PPVs.
Basilisk identifies semantically similar words based
on their co-occurrence with seeds in contextual pat-
terns. Basilisk was originally designed for semantic
class induction using lexico-syntactic patterns, but
has also been used to learn subjective and objective
nouns (Riloff et al, 2003).
Basilisk first identifies the pattern contexts that
are most strongly associated with the seed words.
Words that occur in those contexts are labeled as
candidates and scored based on the strength of their
contexts. The top 5 candidates are selected and the
bootstrapping process repeats. Basilisk produces a
lexicon of learned words as well as a ranked list of
pattern contexts. Since we bootstrapped over verb
conjunctions, we also extracted new PPVs from the
contexts. We ran the bootstrapping process to create
a lexicon of 500 words, and we collected verbs from
the top 500 contexts as well.
5 Evaluation
Plot unit analysis of narrative text is enormously
complex ? the idea of creating gold standard plot
unit annotations seemed like a monumental task.
So we began with relatively simple and constrained
texts that seemed appropriate: fables. Fables have
two desirable attributes: (1) they have a small cast
of characters, and (2) they typically revolve around
a moral, which is exemplified by a short and concise
plot. Even so, fables are challenging for NLP due to
anthropomorphic characters, flowery language, and
sometimes archaic vocabulary.
We collected 34 Aesop?s fables from a web site4,
choosing fables that have a true plot (some only con-
tain quotes) and exactly two characters. We divided
them into a development set of 11 stories, a tuning
set of 8 stories, and a test set of 15 stories.
Creating a gold standard was itself a substantial
undertaking, and training non-experts to produce
them did not seem feasible in the short term. So
the authors discussed and iteratively refined manual
annotations for the development and tuning sets un-
til we produced similar results and had a common
understanding of the task. Then two authors inde-
pendently created annotations for the test set, and a
third author adjudicated the differences.
5.1 Evaluation Procedure
For evaluation, we used recall (R), precision (P),
and F-measure (F). In our gold standard, each af-
fect state is annotated with the set of clauses that
could legitimately produce it. In most cases (75%),
we were able to ascribe the existence of a state to
precisely one clause. During evaluation, the system-
produced affect states must be generated from the
correct clause. However, for affect states that could
be ascribed to multiple clauses in a sentence, the
evaluation was done at the sentence level. In this
case, the system-produced affect state must come
from the sentence that contains one of those clauses.
Coreference resolution is far from perfect, so we
created gold standard coreference annotations for
our fables and used them for most of our experi-
ments. This allowed us to evaluate our approach
without coreference mistakes factoring in. In Sec-
tion 5.5, we re-evaluate our final results using auto-
matic coreference resolution.
5.2 Evaluation of Affect States using External
Resources
Our first set of experiments evaluates the quality of
the affect states produced by AESOP using only the
external resources. The top half of Table 1 shows the
results for each resource independently. FrameNet
produced the best results, yielding much higher re-
call than any other resource. The bottom half of Ta-
4www.pacificnet.net/?johnr/aesop/
82
Affect State M (59) + (47) - (37) All (143)
Resource(s) R P F R P F R P F R P F
FrameNet .49 .51 .50 .17 .57 .26 .14 .42 .21 .29 .51 .37
MPQA Lexicon .07 .50 .12 .21 .24 .22 .22 .38 .28 .15 .31 .20
OpinionFinder .42 .40 .41 .00 .00 .00 .03 .17 .05 .18 .35 .24
Semantic Orientation Lexicon .07 .44 .12 .17 .40 .24 .08 .38 .13 .10 .41 .16
Speech Act Verbs .36 .53 .43 .00 .00 .00 .00 .00 .00 .15 .53 .23
FrameNet+MPQA Lexicon .44 .52 .48 .30 .28 .29 .27 .38 .32 .35 .40 .37
FrameNet+OpinionFinder .53 .39 .45 .17 .38 .23 .16 .33 .22 .31 .38 .34
FrameNet+Semantic Orientation Lexicon .49 .51 .50 .26 .36 .30 .22 .42 .29 .34 .45 .39
FrameNet+Speech Act Verbs .51 .48 .49 .17 .57 .26 .14 .42 .21 .30 .49 .37
Table 1: Evaluation results for AESOP using external resources. The # in parentheses is the # of gold affect states.
Affect State M (59) + (47) - (37) All (143)
Resource(s) R P F R P F R P F R P F
- Evil Agent PPVs .07 .50 .12 .21 .40 .28 .46 .46 .46 .22 .44 .29
- Neg Basilisk PPVs .07 .44 .12 .11 .45 .18 .24 .45 .31 .13 .45 .20
- Evil Agent and Neg Basilisk PPVs .05 .43 .09 .21 .38 .27 .46 .40 .43 .21 .39 .27
+ Kind Agent PPVs (?>1) .03 .33 .06 .28 .17 .21 .00 .00 .00 .10 .19 .13
+ Pos Basilisk PPVs .08 .56 .14 .02 .12 .03 .03 1.00 .06 .05 .39 .09
FrameNet+SOLex+EvilAgentPPVs .49 .54 .51 .30 .38 .34 .46 .42 .44 .42 .46 .44
FrameNet+EvilAgentPPVs .49 .54 .51 .28 .45 .35 .46 .46 .46 .41 .49 .45
FrameNet+EvilAgentPPVs+PosBasiliskPPVs .49 .53 .51 .30 .41 .35 .49 .49 .49 .43 .48 .45
Table 2: Evaluation results for AESOP with PPVs. The # in parentheses is the # of gold affect states.
ble 1 shows the results when combining FrameNet
with other resources. In terms of F score, the only
additive benefit came from the Semantic Orientation
Lexicon, which produced a better balance of recall
and precision and an F score gain of +2.
5.3 Evaluation of Affect States using PPVs
Our second set of experiments evaluates the quality
of the automatically generated PPV lexicons. The
top portion of Table 2 shows the results for the neg-
ative PPVs. The PPVs harvested by the Evil Agent
patterns produced the best results, yielding recall
and precision of .46 for negative states. Note that
M and + states are also generated from the negative
PPVs because they are inferred during affect projec-
tion (Section 4.1.4). The polarity of a negative PPV
can also be flipped by negation to produce a + state.
Basilisk?s negative PPVs achieved similar preci-
sion but lower recall. We see no additional recall
and some precision loss when the Evil Agent and
Basilisk PPV lists are combined. The precision drop
is likely due to redundancy, which creates spurious
affect states. If two different words have negative
polarity but refer to the same event, then only one
negative affect state should be generated. But AE-
SOP will generate two affect states, so one will be
spurious.
The middle section of Table 2 shows the results
for the positive PPVs. Both positive PPV lexicons
were of dubious quality, so we tried to extract a high-
quality subset of each list. For the Kind Agent PPVs,
we computed the ratio of the frequency of the verb
with Evil Agents versus Kind Agents and only saved
verbs with an Evil:Kind ratio (?) > 1, which yielded
1203 PPVs. For the positive Basilisk PPVs, we used
only the top 100 lexicon and top 100 context verbs,
which yielded 164 unique verbs. The positive PPVs
did generate several correct affect states (including
a - state when a positive PPV was negated), but also
many spurious states.
The bottom section of Table 2 shows the impact
of the learned PPVs when combined with FrameNet
and the Semantic Orientation Lexicon (SOLex).
Adding the Evil Agent PPVs improved AESOP?s F
score from 39% to 44%, mainly due to a +8 recall
gain. The recall of the - states increased from 22%
to 46% with no loss of precision. Interestingly, if
we remove SOLex and use only FrameNet with our
PPVs, precision increases from 46% to 49% and re-
call only drops by -1. Finally, the last row of Table
83
2 shows that adding Basilisk?s positive PPVs pro-
duces a small recall boost (+2) with a slight drop in
precision (-1).
Evaluating the impact of PPVs on plot unit struc-
tures is an indirect way of assessing their quality be-
cause creating plot units involves many steps. Also,
our test set is small so many verbs will never appear.
To directly measure the quality of our PPVs, we re-
cruited 3 people to manually review them. We devel-
oped annotation guidelines that instructed each an-
notator to judge whether a verb is generally good or
bad for its patient, assuming the patient is animate.
They assigned each verb to one of 6 categories: ?
(not a verb), 2 (always good), 1 (usually good), 0
(neutral, mixed, or requires inanimate patient), -1
(usually bad), -2 (always bad). Each annotator la-
beled 250 words: 50 words randomly sampled from
each of our 4 PPV lexicons5 (Evil Agent PPVs, Kind
Agent PPVs, Positive Basilisk PPVs, and Negative
Basilisk PPVs) plus 50 verbs labeled as neutral in
the MPQA lexicon.
First, we measured agreement based on three
groupings: negative (-2 and -1), neutral (0), or pos-
itive (1 and 2). We computed ? scores to measure
inter-annotator agreement for each pair of annota-
tors.6, but the ? scores were relatively low because
the annotators had trouble distinguishing the posi-
tive cases from the neutral ones. So we re-computed
agreement using two groupings: negative (-2 and -
1) and not-negative (0 through 2), and obtained ?
scores of .69, .71, and .74. We concluded that peo-
ple largely agree on whether a verb is bad for the
patient, but they do not necessarily agree if a verb is
good for the patient. One possible explanation is that
many ?bad? verbs represent physical harm or dan-
ger: these verbs are both plentiful and easy to rec-
ognize. In contrast, ?good? verbs are often more ab-
stract and open to interpretation (e.g., is being ?en-
vied? or ?feared? a good thing?).
We used the labels produced by the two an-
notators with the highest ? score to measure the
accuracy of our PPVs. Both the Evil Agent and
Negative Basilisk PPVs were judged to be 72.5%
accurate, averaged over the judges. The Kind Agent
5The top-ranked Evil/Kind Agent PPV lists (? > 1) which
yields 1203 kind PPVs, and 477 evil PPVs, the top 164 positive
Basilisk verbs, and the 678 (unique) negative Basilisk verbs.
6We discarded words labeled as not a verb.
PPVs were only about 39% accurate, while the
Positive Basilisk PPVs were nearly 50% accurate.
These results are consistent with our impressions
that the negative PPVs are of relatively high quality,
while the positive PPVs are mixed. Some examples
of learned PPVs that were not present in our other
resources are:
- : censor, chase, fire, orphan, paralyze, scare, sue
+ : accommodate, harbor, nurse, obey, respect, value
5.4 Evaluation of Links
We represented each link as a 5-tuple
?src-clause, src-state, tgt-clause, tgt-state, link-type?,
where source/target denotes the direction of the
link, the source/target-states are the affect state type
(+,-,M) and link-type is one of 3 types: actualization
(a), motivation (m), or cross-character (xchar). A
system-produced link is considered correct if all 5
elements of the tuple match the human annotation.
Gold Aff States System Aff States
Links R P F R P F
xchar (56) .79 .85 .82 .18 .43 .25
a (51) .90 .94 .92 .04 .07 .05
m (26) 1.0 .57 .72 .15 .10 .12
Table 3: Link results; parentheses show # of gold links.
The second column of Table 3 shows the perfor-
mance of AESOP when using gold standard affect
states. Our simple heuristics for creating links work
surprisingly well for xchar and a links when given
perfect affect states. However, these heuristics pro-
duce relatively low precision for m links, albeit with
100% recall. This reveals that m links primarily do
connect adjacent states, but we need to be more dis-
criminating when connecting them. The third col-
umn of Table 3 shows the results when using system-
generated affect states. We see that performance is
much lower. This is not particularly surprising, since
AESOP?s F-score is 45%, so over half of the indi-
vidual states are wrong, which means that less than
a quarter of the pairs are correct. From that perspec-
tive, the xchar link performance is reasonable, but
the causal a and m links need improvement.
5.5 Analysis
We performed additional experiments to evaluate
some assumptions and components. First, we cre-
ated a Baseline system that is identical to AESOP
84
except that it does not use the affect projection rules.
Instead, it naively projects every affect state in a
clause onto every character in that clause. The first
two rows of the table below show that AESOP?s pre-
cision is double the Baseline, with nearly the same
recall. This illustrates the importance of the projec-
tion rules for mapping affect states onto characters.
R P F
Baseline .44 .24 .31
AESOP, gold coref .43 .48 .45
AESOP, gold coref, infstates .39 .48 .43
AESOP, auto coref, infstates .24 .56 .34
Our gold standard includes pure inference affect
states that are critical to the plot unit structure but
come from world knowledge outside the story itself.
Of 157 affect states in our test set, 14 were pure in-
ference states. We ignored these states in our previ-
ous experiments because our system has no way to
generate them. The third row of the table shows that
including them lowers recall by -4. Generating pure
inferences is an interesting challenge, but they seem
to be a relatively small part of the problem.
The last row of the table shows AESOP?s perfor-
mance when we use our automated coreference re-
solver (Section 4.1.2) instead of gold standard coref-
erence annotations. We see a -15 recall drop coupled
with a +8 precision gain. We were initially puz-
zled by the precision gain but believe that it is pri-
marily due to the handling of quotations. Our gold
standard includes annotations for characters men-
tioned in quotations, but our automated coreference
resolver ignores quotations. Most fables end with
a moral, which is often a quote that may not men-
tion the plot. Consequently, AESOP generates more
spurious affect states from the quotations when us-
ing the gold standard annotations.
6 Related Work and Conclusions
Our research is the first effort to fully automate
the creation of plot unit structures. Other prelimi-
nary work has begun to look at plot unit modelling
for single character stories (Appling and Riedl,
2009). More generally, our work is related to re-
search in narrative story understanding (e.g., (El-
son and McKeown, 2009)), automatic affect state
analysis (Alm, 2009), and automated learning of
scripts (Schank and Abelson, 1977) and other con-
ceptual knowledge structures (e.g., (Mooney and
DeJong, 1985; Fujiki et al, 2003; Chambers and Ju-
rafsky, 2008; Chambers and Jurafsky, 2009; Kasch
and Oates, 2010)). Our work benefitted from prior
research in creating semantic resources such as
FrameNet (Baker et al, 1998) and sentiment lex-
icons and classifiers (e.g., (Takamura et al, 2005;
Wilson et al, 2005b; Choi et al, 2006)). We showed
that affect projection rules can effectively assign af-
fect states to characters. This task is similar to, but
not the same as, associating opinion words with their
targets or topics (Kim and Hovy, 2006; Stoyanov
and Cardie, 2008). Some aspects of affect state iden-
tification are closely related to Hopper and Thomp-
son?s (1980) theory of transitivity. In particular, their
notions of aspect (has an action completed?), benefit
and harm (how much does an object gain/lose from
an action?) and volition (did the subject make a con-
scious choice to act?).
AESOP produces affect states with an F score of
45%. Identifying positive states appears to be more
difficult than negative or mental states. Our sys-
tem?s biggest shortcoming currently seems to hinge
around identifying plans and goals. This includes
the M affect states that initiate plans, the +/- com-
pletion states, as well as their corresponding links.
We suspect that the relatively low recall on positive
affect states is due to our inability to accurately iden-
tify successful plan completions. Finally, these re-
sults are based on fables; plot unit analysis of other
types of texts will pose additional challenges.
Acknowledgments
The authors gratefully acknowledge the support of
Department of Homeland Security Grant N0014-07-
1-0152, NSF grant IIS-0712764, and the Defense
Advanced Research Projects Agency (DARPA) Ma-
chine Reading Program under Air Force Research
Laboratory (AFRL) prime contract no. FA8750-09-
C-0172. Any opinions, findings, and conclusion
or recommendations expressed in this material are
those of the author(s) and do not necessarily reflect
the view of the DARPA, AFRL, or the U.S. gov-
ernment. Thanks to Peter Jensen, Emily Schlichter,
and Clay Templeton for PPV annotations, Nathan
Gilbert for help with the coreference resolver, and
the anonymous reviewers for many helpful com-
ments.
85
References
Cecilia Ovesdotter Alm. 2009. Affect in Text and Speech.
VDM Verlag Dr. Mller.
D. Scott Appling and Mark O. Riedl. 2009. Representa-
tions for learning to summarize plots. In Proceedings
of the AAAI Spring Symposium on Intelligent Narra-
tive Technologies II.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet Project. In In Proceed-
ings of COLING/ACL, pages 86?90.
Nathanael Chambers and Dan Jurafsky. 2008. Unsuper-
vised learning of narrative event chains. In Proceed-
ings of the Association for Computational Linguistics.
Nathanael Chambers and Dan Jurafsky. 2009. Unsuper-
vised learning of narrative schemas and their partici-
pants. In Proceedings of the Association for Compu-
tational Linguistics.
Yejin Choi, Eric Breck, and Claire Cardie. 2006. Joint
extraction of entities and relations for opinion recogni-
tion. In EMNLP ?06: Proceedings of the 2006 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 431?439, Morristown, NJ, USA. Asso-
ciation for Computational Linguistics.
David Elson and Kathleen McKeown. 2009. Extending
and evaluating a platform for story understanding. In
Proceedings of the AAAI 2009 Spring Symposium on
Intelligent Narrative Technologies II.
Toshiaki Fujiki, Hidetsugu Nanba, and Manabu Oku-
mura. 2003. Automatic acquisition of script knowl-
edge from a text collection. In Proceedings of the Eu-
ropean Association for Computational Linguistics.
Vasileios Hatzivassiloglou and Kathy McKeown. 1997.
Predicting the semantic orientation of adjectives. In
Proceedings of the 35th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 174?181,
Madrid, Spain.
Paul J. Hopper and Sandra A. Thompson. 1980.
Transitivity in grammar and discourse. Language,
56:251299.
Niels Kasch and Tim Oates. 2010. Mining script-like
structures from the web. In NAACL-10 Workshop on
Formalisms and Methodology for Learning by Read-
ing (FAM-LbR).
S. Kim and E. Hovy. 2006. Extracting Opinions, Opin-
ion Holders, and Topics Expressed in Online News
Media Text. In Proceedings of ACL/COLING Work-
shop on Sentiment and Subjectivity in Text.
W. Lehnert, J. Black, and B. Reiser. 1981. Summariz-
ing Narratives. In Proceedings of the Seventh Interna-
tional Joint Conference on Artificial Intelligence.
W. G. Lehnert. 1981. Plot Units and Narrative Summa-
rization. Cognitive Science, 5(4):293?331.
G. Miller. 1990. Wordnet: An On-line Lexical Database.
International Journal of Lexicography, 3(4).
Raymond Mooney and Gerald DeJong. 1985. Learning
Schemata for Natural Language Processing. In Pro-
ceedings of the Ninth International Joint Conference
on Artificial Intelligence, pages 681?687.
E. Riloff and W. Phillips. 2004. An Introduction to the
Sundance and AutoSlog Systems. Technical Report
UUCS-04-015, School of Computing, University of
Utah.
E. Riloff, J. Wiebe, and T. Wilson. 2003. Learning Sub-
jective Nouns using Extraction Pattern Bootstrapping.
In Proceedings of the Seventh Conference on Natural
Language Learning (CoNLL-2003), pages 25?32.
Roger C. Schank and Robert P. Abelson. 1977. Scripts,
plans, goals and understanding. Lawrence Erlbaum.
V. Stoyanov and C. Cardie. 2008. Topic Identification
for Fine-Grained Opinion Analysis. In Conference on
Computational Linguistics (COLING 2008).
Hiroya Takamura, Takashi Inui, and Manabu Okumura.
2005. Extracting semantic orientations of words using
spin model. In ACL ?05: Proceedings of the 43rd An-
nual Meeting on Association for Computational Lin-
guistics.
M. Thelen and E. Riloff. 2002. A Bootstrapping Method
for Learning Semantic Lexicons Using Extraction Pa
ttern Contexts. In Proceedings of the 2002 Conference
on Empirical Methods in Natural Language Process-
ing, pages 214?221.
A. Wierzbicka. 1987. English speech act verbs: a se-
mantic dictionary. Academic Press, Sydney, Orlando.
T. Wilson, P. Hoffmann, S. Somasundaran, J. Kessler,
J. Wiebe, Y. Choi, C. Cardie, E. Riloff, and S. Patward-
han. 2005a. OpinionFinder: A system for subjectivity
analysis. In Proceedings of HLT/EMNLP 2005 Inter-
active Demonstrations.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005b. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of Human
Language Technology Conference and Conference on
Empirical Methods in Natural Language Processing,
pages 347?354. Association for Computational Lin-
guistics.
86
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 250?261,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Approximate Scalable Bounded Space Sketch for Large Data NLP
Amit Goyal and Hal Daume? III
Dept. of Computer Science
University of Maryland
College Park, MD 20742
{amit,hal}@umiacs.umd.edu
Abstract
We exploit sketch techniques, especially the
Count-Min sketch, a memory, and time effi-
cient framework which approximates the fre-
quency of a word pair in the corpus without
explicitly storing the word pair itself. These
methods use hashing to deal with massive
amounts of streaming text. We apply Count-
Min sketch to approximate word pair counts
and exhibit their effectiveness on three im-
portant NLP tasks. Our experiments demon-
strate that on all of the three tasks, we get
performance comparable to Exact word pair
counts setting and state-of-the-art system. Our
method scales to 49 GB of unzipped web data
using bounded space of 2 billion counters (8
GB memory).
1 Introduction
There is more data available today on the web than
there has ever been and it keeps increasing. Use
of large data in the Natural Language Processing
(NLP) community is not new. Many NLP problems
(Brants et al, 2007; Turney, 2008; Ravichandran et
al., 2005) have benefited from having large amounts
of data. However, processing large amounts of data
is still challenging.
This has motivated NLP community to use com-
modity clusters. For example, Brants et al (2007)
used 1500 machines for a day to compute the rela-
tive frequencies of n-grams from 1.8TB of web data.
In another work, a corpus of roughly 1.6 Terawords
was used by Agirre et al (2009) to compute pair-
wise similarities of the words in the test sets using
the MapReduce infrastructure on 2, 000 cores. How-
ever, the inaccessibility of clusters to an average user
has attracted the NLP community to use streaming,
randomized, and approximate algorithms to handle
large amounts of data (Goyal et al, 2009; Levenberg
et al, 2010; Van Durme and Lall, 2010).
Streaming approaches (Muthukrishnan, 2005)
provide memory and time-efficient framework to
deal with terabytes of data. However, these ap-
proaches are proposed to solve a singe problem.
For example, our earlier work (Goyal et al, 2009)
and Levenberg and Osborne (2009) build approxi-
mate language models and show their effectiveness
in Statistical Machine Translation (SMT). Stream-
based translation models (Levenberg et al, 2010)
has been shown effective to handle large parallel
streaming data for SMT. In Van Durme and Lall
(2009b), a Talbot Osborne Morris Bloom (TOMB)
Counter (Van Durme and Lall, 2009a) was used to
find the top-K verbs ?y? given verb ?x? using the
highest approximate online Pointwise Mutual Infor-
mation (PMI) values.
In this paper, we explore sketch techniques,
especially the Count-Min sketch (Cormode and
Muthukrishnan, 2004) to build a single model to
show its effectiveness on three important NLP tasks:
? Predicting the Semantic Orientation of words
(Turney and Littman, 2003)
? Distributional Approaches for word similarity
(Agirre et al, 2009)
? Unsupervised Dependency Parsing (Cohen and
Smith, 2010) with a little linguistics knowl-
edge.
In all these tasks, we need to compute association
measures like Pointwise Mutual Information (PMI),
250
and Log Likelihood ratio (LLR) between words. To
compute association scores (AS), we need to count
the number of times pair of words appear together
within a certain window size. However, explicitly
storing the counts of all word pairs is both computa-
tionally expensive and memory intensive (Agirre et
al., 2009; Pantel et al, 2009). Moreover, the mem-
ory usage keeps increasing with increase in corpus
size.
We explore Count-Min (CM) sketch to address
the issue of efficient storage of such data. The
CM sketch stores counts of all word pairs within a
bounded space. Storage space saving is achieved
by approximating the frequency of word pairs in
the corpus without explicitly storing the word pairs
themselves. Both updating (adding a new word pair
or increasing the frequency of existing word pair)
and querying (finding the frequency of a given word
pair) are constant time operations making it efficient
online storage data structure for large data. Sketches
are scalable and can easily be implemented in dis-
tributed setting.
We use CM sketch to store counts of word pairs
(except word pairs involving stop words) within a
window of size1 7 over different size corpora. We
store exact counts of words (except stop words) in
hash table (since the number of unique words is
not large that is quadratically less than the num-
ber of unique word pairs). The approximate PMI
and LLR scores are computed using these approxi-
mate counts and are applied to solve our three NLP
tasks. Our experiments demonstrate that on all of
the three tasks, we get performance comparable to
Exact word pair counts setting and state-of-the-art
system. Our method scales to 49 GB of unzipped
web data using bounded space of 2 billion counters
(8 GB memory). This work expands upon our ear-
lier workshop papers (Goyal et al, 2010a; Goyal et
al., 2010b).
2 Sketch Techniques
A sketch is a compact summary data structure to
store the frequencies of all items in the input stream.
Sketching techniques use hashing to map items in
streaming data onto a small sketch vector that can
be updated and queried in constant time. These tech-
17 is chosen from intuition and not tuned.
niques generally process the input stream in one di-
rection, say from left to right, without re-processing
previous input. The main advantage of using these
techniques is that they require a storage which is
sub-linear in size of the input stream. The following
surveys comprehensively review the streaming liter-
ature: (Rusu and Dobra, 2007; Cormode and Had-
jieleftheriou, 2008).
There exists an extensive literature on sketch tech-
niques (Charikar et al, 2004; Li et al, 2008; Cor-
mode and Muthukrishnan, 2004; Rusu and Dobra,
2007) in algorithms community for solving many
large scale problems. However, in practice, re-
searchers have preferred Count-Min (CM) sketch
over other sketch techniques in many application ar-
eas, such as Security (Schechter et al, 2010), Ma-
chine Learning (Shi et al, 2009; Aggarwal and Yu,
2010), and Privacy (Dwork et al, 2010). This moti-
vated us to explore CM sketch to solve three impor-
tant NLP problems.2
2.1 Count-Min Sketch
The Count-Min sketch (Cormode and Muthukrish-
nan, 2004) is a compact summary data structure
used to store the frequencies of all items in the in-
put stream. The sketch allows fundamental queries
on the data stream such as point, range and inner
product queries to be approximately answered very
quickly. It can also be applied to solve the finding
frequent items problem (Manku and Motwani, 2002)
in a data stream. In this paper, we are only interested
in point queries. The aim of a point query is to es-
timate the count of an item in the input stream. For
other details, the reader is referred to (Cormode and
Muthukrishnan, 2004).
Given an input stream of word pairs of length N
and user chosen parameters ? and , the algorithm
stores the frequencies of all the word pairs with the
following guarantees:
? All reported frequencies are within the true fre-
quencies by at most N with a probability of at
least 1-?.
? The space used by the algorithm is O(1 log 1? ).
2In future, in another line of research, we will explore com-
paring different sketch techniques for NLP problems.
251
? Constant time of O(log(1? )) per each update andquery operation.
2.1.1 CM Data Structure
A Count-Min sketch (CM) with parameters (,?)
is represented by a two-dimensional array with
width w and depth d :
?
??
sketch[1, 1] ? ? ? sketch[1, w]
... . . . ...
sketch[d, 1] ? ? ? sketch[d,w]
?
??
Among the user chosen parameters,  controls the
amount of tolerable error in the returned count and
? controls the probability with which the returned
count is not within the accepted error. These val-
ues of  and ? determine the width and depth of the
two-dimensional array respectively. To achieve the
guarantees mentioned in the previous section, we
set w=2 and d=log(1? ). The depth d denotes thenumber of pairwise-independent hash functions em-
ployed by the algorithm and there exists a one-to-
one correspondence between the rows and the set
of hash functions. Each of these hash functions
hk:{x1 . . . xN} ? {1 . . . w}, 1 ? k ? d, takes a
word pair from the input stream and maps it into a
counter indexed by the corresponding hash function.
For example, h2(x) = 10 indicates that the word
pair ?x? is mapped to the 10th position in the second
row of the sketch array.
Initialize the entire sketch array with zeros.
Update Procedure: When a new word pair ?x?
with count c arrives, one counter in each row (as de-
cided by its corresponding hash function) is updated
by c.
sketch[k, hk(x)]? sketch[k, hk(x)] + c, ?1 ? k ? d
Query Procedure: Since multiple word pairs can
get hashed to the same position, the frequency stored
by each position is guaranteed to overestimate the
true count. Thus, to answer the point query for a
given word pair, we return minimum over all the po-
sitions indexed by the k hash functions. The answer
to Query(x): c? = mink sketch[k, hk(x)]
Both update and query procedures involve evalu-
ating d hash functions and reading of all the values
in those indices and hence both these procedures are
linear in the number of hash functions. Hence both
these steps require O(log(1? )) time. In our experi-ments (see Section 3.1), we found that a small num-
ber of hash functions are sufficient and we use d=5.
Hence, the update and query operations take only a
constant time. The space used by the algorithm is
the size of the array i.e. wd counters, where w is the
width of each row.
2.1.2 Properties
Apart from the advantages of being space ef-
ficient, and having constant update and constant
querying time, the Count-Min sketch has also other
advantages that makes it an attractive choice for
NLP applications.
? Linearity: Given two sketches s1 and s2 com-
puted (using the same parameters w and d)
over different input streams, the sketch of the
combined data stream can be easily obtained
by adding the individual sketches in O(1 log 1? )time which is independent of the stream size.
? The linearity is especially attractive because it
allows the individual sketches to be computed
independent of each other, which means that it
is easy to implement it in distributed setting,
where each machine computes the sketch over
a sub set of corpus.
2.2 Conservative Update
Estan and Varghese introduced the idea of conserva-
tive update (Estan and Varghese, 2002) in the con-
text of computer networking. This can easily be used
with CM sketch to further improve the estimate of a
point query. To update a word pair ?x? with fre-
quency c, we first compute the frequency c? of this
word pair from the existing data structure and the
counts are updated according to:
c? = mink sketch[k, hk(x)], ?1 ? k ? d
sketch[k, hk(x)]? max{sketch[k, hk(x)], c?+ c}
The intuition is that, since the point query returns
the minimum of all the d values, we will update a
counter only if it is necessary as indicated by the
above equation. Though this is a heuristic, it avoids
the unnecessary updates of counter values and thus
reduces the error.
In our experiments, we found that employing the
conservative update reduces the Average Relative
252
Error (ARE) of these counts approximately by a fac-
tor of 1.5. (see Section 3.1). But unfortunately,
this update can only be maintained over individual
sketches in distributed setting.
3 Intrinsic Evaluations
To show the effectiveness of the CM sketch and CM
sketch with conservative update (CU) in the context
of NLP, we perform intrinsic evaluations. First, the
intrinsic evaluations are designed to measure the er-
ror in the approximate counts returned by CM sketch
compared to their true counts. Second, we compare
the word pairs association rankings obtained using
PMI and LLR with sketch and exact counts.
It is memory and time intensive to perform many
intrinsic evaluations on large data (Ravichandran et
al., 2005; Brants et al, 2007; Goyal et al, 2009).
Hence, we use a subset of corpus of 2 million sen-
tences (Subset) from Gigaword (Graff, 2003) for it.
We generate words and word pairs over a window
of size 7. We store exact counts of words (except
stop words) in a hash table and store approximate
counts of word pairs (except word pairs involving
stop words) in the sketch.
3.1 Evaluating approximate sketch counts
To evaluate the amount of over-estimation error (see
Section 2.1) in CM and CU counts compared to the
true counts, we first group all word pairs with the
same true frequency into a single bucket. We then
compute the average relative error in each of these
buckets. Since low-frequency word pairs are more
prone to errors, making this distinction based on fre-
quency lets us understand the regions in which the
algorithm is over-estimating. Moreover, to focus on
errors on low frequency counts, we have only plot-
ted word pairs with count at most 100. Average Rel-
ative error (ARE) is defined as the average of abso-
lute difference between the predicted and the exact
value divided by the exact value over all the word
pairs in each bucket.
ARE = 1N
N?
i=1
|Exacti ? Predictedi|
Exacti
Where Exact and Predicted denotes values of ex-
act and CM/CU counts respectively; N denotes the
number of word pairs with same counts in a bucket.
In Fig. 1(a), we fixed the number of counters to 20
million (20M ) with four bytes of memory per each
counter (thus it only requires 80 MB of main mem-
ory). Keeping the total number of counters fixed,
we try different values of depth (2, 3, 5 and 7) of the
sketch array and in each case the width is set to 20Md .The ARE curves in each case are shown in Fig. 1(a).
We can make three main observations from Figure
1(a): First it shows that most of the errors occur on
low frequency word pairs. For frequent word pairs,
in almost all the different runs the ARE is close to
zero. Secondly, it shows that ARE is significantly
lower (by a factor of 1.5) for the runs which use
conservative update (CUx run) compared to the runs
that use direct CM sketch (CMx run). The encourag-
ing observation is that, this holds true for almost all
different (width,depth) settings. Thirdly, in our ex-
periments, it shows that using depth of 3 gets com-
paratively less ARE compared to other settings.
To be more certain about this behavior with re-
spect to different settings of width and depth, we
tried another setting by increasing the number of
counters to 50 million. The curves in 1(b) follow a
pattern which is similar to the previous setting. Low
frequency word pairs are more prone to error com-
pared to the frequent ones and employing conserva-
tive update reduces the ARE by a factor of 1.5. In
this setting, depth 5 does slightly better than depth 3
and gets lowest ARE.
We use CU counts and depth of 5 for the rest of
the paper. As 3 and 5 have lowest ARE in different
settings and using 5 hash functions, we get ? = 0.01
(d = log(1? ) refer Section 2.1) that is probability offailure is 1 in 100, making the algorithm more robust
to false positives compared with 3 hash functions,
? = 0.1 with probability of failure 1 in 10.
Fig. 1(c) studies the effect of the number of coun-
ters in the sketch (the size of the two-dimensional
sketch array) on the ARE with fixed depth 5. As ex-
pected, using more number of counters decreases the
ARE in the counts. This is intuitive because, as the
length of each row in the sketch increases, the prob-
ability of collision decreases and hence the array is
more likely to contain true counts. By using 100
million counters, which is comparable to the length
of the stream 88 million, we are able to achieve al-
most zero ARE over all the counts including the rare
253
100 101 102
0
0.5
1
1.5
2
2.5
3
True frequency counts of word pairs (log scale)
Av
era
ge 
Re
lati
ve 
Err
or
 
 
CM?7
CM?5
CM?3
CM?2
CU?7
CU?5
CU?3
CU?2
(a) 20M counters
100 101 1020
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
True frequency counts of word pairs (log scale)
Ave
rag
e R
ela
tive
 Er
ror
 
 CM?7CM?5CM?3CM?2CU?7CU?5CU?3CU?2
(b) 50M counters
100 101 102
0
1
2
3
4
5
True frequency counts of word pairs (log scale)
Av
era
ge
 R
ela
tiv
e E
rro
r
 
 
10M
20M
50M
100M
(c) Different size models with depth 5
Figure 1: Compare 20 and 50 million counter models with different (width,depth) settings. The notation CMx represents the
Count Min sketch with a depth of ?x? and CUx represents the CM sketch along with conservative update and depth ?x?.
ones3. Note that the space we save by not storing the
exact counts is almost four times the memory that
we use here because on an average each word pair
is twelve characters long and requires twelve bytes
(thrice the size of an integer) and 4 bytes for storing
the integer count. Note, we get even bigger space
savings if we work with longer phrases (phrase clus-
tering), phrase pairs (paraphrasing/translation), and
varying length n-grams (Information Extraction).
3.2 Evaluating word pairs association ranking
In this experiment, we compare the word pairs asso-
ciation rankings obtained using PMI and LLR with
CU and exact word pair counts. We use two kinds of
measures, namely recall and Spearman?s correlation
to measure the overlap in the rankings obtained by
exact and CU counts. Intuitively, recall captures the
number of word pairs that are found in both the sets
and then Spearman?s correlation captures if the rela-
tive order of these common word pairs is preserved
in both the rankings. In our experimental setup, if
the rankings match exactly, then we get a recall (R)
of 100% and a correlation (?) of 1.
The results with respect to different sized counter
(20 million (20M ), 50 million (50M )) models are
shown in Table 1. If we compare the second and
third column of the table using PMI and LLR for
20M counters, we get exact rankings for LLR com-
pared to PMI while comparing TopK word pairs.
The explanation for such a behavior is: since we are
3Even with other datasets we found that using counters lin-
ear in the size of the stream leads to ARE close to zero ? counts.
# Cs 20M 50M
AS PMI LLR PMI LLR
TopK R ? R ? R ? R ?
50 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0
100 .98 .94 1.0 1.0 1.0 1.0 1.0 1.0
500 .80 .98 1.0 1.0 .98 1.0 1.0 1.0
1000 .56 .99 1.0 1.0 .96 .99 1.0 1.0
5000 .35 .90 1.0 1.0 .85 .99 1.0 1.0
10000 .38 .55 1.0 1.0 .81 .95 1.0 1.0
Table 1: Evaluating the PMI and LLR rankings obtained using
CM sketch with conservative update (CU) and Exact counts
not throwing away any infrequent word pairs, PMI
will rank pairs with low frequency counts higher
(Church and Hanks, 1989). Hence, we are evaluat-
ing the PMI values for rare word pairs and we need
counters linear in size of stream to get alost perfect
ranking. This is also evident from the fourth column
for 50M of the Table 1, where CU PMI ranking gets
close to the optimal as the number of counters ap-
proaches stream size.
However, in some NLP problems, we are not in-
terested in low-frequency items. In such cases, even
using space less than linear in number of counters
would suffice. In our extrinsic evaluations, we show
that using space less than the length of the stream
does not degrade the performance.
4 Extrinsic Evaluations
4.1 Data
Gigaword corpus (Graff, 2003) and a 50% portion
of a copy of web crawled by (Ravichandran et al,
254
2005) are used to compute counts of words and word
pairs. For both the corpora, we split the text into
sentences, tokenize and convert into lower-case. We
generate words and word pairs over a window of size
7. We use four different sized corpora: SubSet (used
for intrinsic evaluations in Section 3), Gigaword
(GW), GigaWord + 20% of web data (GWB20), and
GigaWord + 50% of web data (GWB50). Corpus
Statistics are shown below. We store exact counts of
words in a hash table and store approximate counts
of word pairs in the sketch. Hence, the stream size
in our case is the total number of word pairs in a
corpus.
Corpus Subset GW GWB20 GWB50
Unzipped .32 9.8 22.8 49Size (GB)
# of sentences 2.00 56.78 191.28 462.60(Million)
Stream Size .088 2.67 6.05 13.20(Billion)
4.2 Semantic Orientation
Given a word, the task of finding the Semantic Ori-
entation (SO) (Turney and Littman, 2003) of the
word is to identify if the word is more likely to be
used in positive or negative sense. We use a similar
framework as used by the authors to infer the SO.
We take the seven positive words (good, nice, excel-
lent, positive, fortunate, correct, and superior) and
the seven negative words (bad, nasty, poor, negative,
unfortunate, wrong, and inferior) used in (Turney
and Littman, 2003) work. The SO of a given word
is calculated based on the strength of its association
with the seven positive words, and the strength of
its association with the seven negative words. We
compute the SO of a word ?w? as follows:
SO-AS(W) =
?
p?Pwords
AS(p, w)?
?
n?Nwords
AS(n,w)
Where, Pwords and Nwords denote the seven pos-
itive and negative prototype words respectively. We
use PMI and LLR to compute association scores
(AS). If this score is positive, we predict the word
as positive. Otherwise, we predict it as negative.
We use the General Inquirer lexicon4 (Stone et
al., 1966) as a benchmark to evaluate the semantic
4The General Inquirer lexicon is freely available at http:
//www.wjh.harvard.edu/?inquirer/
orientation scores similar to (Turney and Littman,
2003) work. Words with multiple senses have multi-
ple entries in the lexicon, we merge these entries for
our experiment. Our test set consists of 1597 posi-
tive and 1980 negative words. Accuracy is used as
an evaluation metric and is defined as the percentage
of number of correctly identified SO words.
0 500M 1B 1.5B 2B60
65
70
75
Model Size
Accu
racy
 
 
CUExact
(a) SO PMI
0 500M 1B 1.5B 2B55
60
65
70
Model Size
Acc
urac
y
 
 
CUExact
(b) SO LLR
Figure 2: Evaluating Semantic Orientation using PMI and LLR
with different number of counters of CU sketch built using Gi-
gaword.
4.2.1 Varying sketch size
We evaluate SO of words using PMI and LLR
on Gigaword (9.8GB). We compare approximate
SO computed using varying sizes of CU sketches:
50 million (50M ), 100M , 200M , 500M , 1 billion
(1B) and 2 billion (2B) counters with Exact SO. To
compute these scores, we count the number of indi-
vidual words w1 and w2 and the pair (w1,w2) within
a window of size 7. Note that computing the exact
counts of all word pairs on these corpora is com-
putationally expensive and memory intensive, so we
consider only those pairs in which one word appears
in the prototype list and the other word appears in
the test set.
First, if we look at the Exact SO using PMI and
LLR in Figure 2(a) and 2(b) respectively, it shows
that using PMI, we get about 6 points higher ac-
curacy than LLR on this task (The 95% statistical
significance boundary for accuracy is about ? 1.5.).
Second, for both PMI and LLR, having more num-
ber of counters improve performance.5 Using 2B
counters, we get the same accuracy as Exact.
5We use maximum of 2B counters (8GB main memory), as
most of the current desktop machines have at most 8GB RAM.
255
4.2.2 Effect of Increasing Corpus Size
We evaluate SO of words on three different sized
corpora (see Section 4.1): GW (9.8GB), GWB20
(22.8GB), and GWB50 (49GB). First, since for this
task using PMI performs better than LLR, so we will
use PMI for this experiment. Second, we will fix
number of counters to 2B (CU-2B) as it performs
the best in Section 4.2.1. Third, we will compare the
CU-2B counter model with the Exact over increas-
ing corpus size.
We can make several observations from the Fig-
ure 3: ? It shows that increasing the amount of data
improves the accuracy of identifying the SO of a
word. We get an absolute increase of 5.5 points in
accuracy, when we add 20% Web data to GigaWord
(GW). Adding 30% more Web data (GWB50), gives
a small increase of 1.3 points in accuracy which is
not even statistically significant. ? Second, CU-2B
performs as good as exact for all corpus sizes. ?
Third, the number of 2B counters (bounded space)
is less than the length of stream for GWB20 (6.05B
), and GWB50 (13.2B). Hence, it shows that using
counters less than the stream length does not degrade
the performance. ? These results are also compara-
ble to Turney?s (2003) state-of-the-art work where
they report an accuracy of 82.84%. Note, they use a
billion word corpus which is larger than GWB50.
0 10GB 20GB 30GB 40GB 50GB72
74
76
78
80
82
Corpus Size
Accu
racy
 
 
CU?2BExact
Figure 3: Evaluating Semantic Orientation of words with Ex-
act and CU counts with increase in corpus size
4.3 Distributional Similarity
Distributional similarity is based on the distribu-
tional hypothesis that similar terms appear in simi-
lar contexts (Firth, 1968; Harris, 1954). The context
vector for each term is represented by the strength
of association between the term and each of the lex-
ical, semantic, syntactic, and/or dependency units
that co-occur with it6. We use PMI and LLR to com-
pute association score (AS) between the term and
each of the context to generate the context vector.
Once, we have context vectors for each of the terms,
cosine similarity measure returns distributional sim-
ilarity between terms.
4.3.1 Efficient Distributional Similarity
We propose an efficient approach for computing
distributional similarity between word pairs using
CU sketch. In the first step, we traverse the corpus
and store counts of all words (except stop words) in
hash table and all word pairs (except word pairs in-
volving stop words) in sketch. In the second step,
for a target word ?x?, we consider all words (except
infrequent contexts which appear less than or equal
to 10.) as plausible context (since it is faster than
traversing the whole corpus.), and query the sketch
for vocabulary number of word pairs, and compute
approximate AS between word-context pairs. We
maintain only top K AS scores7 contexts using pri-
ority queue for every target word ?x? and save them
onto the disk. In the third step, we use cosine simi-
larity using these approximate topK context vectors
to compute efficient distributional similarity.
The efficient distributional similarity using
sketches has following advantages:
? It can return semantic similarity between any
word pairs that are stored in the sketch.
? It can return the similarity between word pairs
in time O(K).
? We do not store word pairs explicitly, and use
fixed number of counters, hence the overall
space required is bounded.
? The additive property of sketch (Sec. 2.1.2) en-
ables us to parallelize most of the steps in the
algorithm. Thus it can be easily extended to
very large amounts of text data.
We use two test sets which consist of word pairs,
and their corresponding human rankings. We gen-
erate the word pair rankings using efficient distri-
butional similarity. We report the spearman?s rank
6Here, the context for a target word ?x? is defined as words
appear within a window of size 7.
7For this work, we use K = 1000 which is not tuned.
256
correlation8 coefficient (?) between the human and
distributional similarity rankings. The two test sets
are:
1. WS-353 (Finkelstein et al, 2002) is a set of 353
word pairs.
2. RG-65: (Rubenstein and Goodenough, 1965)
is set of 65 word pairs.
0 500M 1B 1.5B 2B0.1
0.15
0.2
0.25
Model Size
Acc
urac
y
 
 
CUExact
(a) Word Similarity PMI
0 500M 1B 1.5B 2B0.4
0.45
0.5
0.55
Model Size
Acc
urac
y
 
 
CUExact
(b) Word Similarity LLR
Figure 4: Evaluating Distributional Similarity between word
pairs on WS-353 test set using PMI and LLR with different
number of counters of CU sketch built using Gigaword data-set.
4.3.2 Varying sketch size
We evaluate efficient distributional similarity be-
tween between word pairs on WS-353 test set us-
ing PMI and LLR association scores on Giga-
word (9.8GB). We compare different sizes of CU
sketch (similar to SO evaluation): 50 million (50M ),
100M , 200M , 500M , 1 billion (1B) and 2 bil-
lion (2B) counters with the Exact word pair counts.
Here again, computing the exact counts of all word-
context pairs on these corpora is time, and memory
intensive, we generate context vectors for only those
words which are present in the test set.
First, if we look at word pair ranking using exact
PMI and LLR across Figures 4(a) and 4(b) respec-
tively, it shows that using LLR, we get better ? of
.55 compared to ? of .25 using PMI on this task (The
95% statistical significance boundary on ? for WS-
353 is about ? .08). The explanation for such a be-
havior is: PMI rank context pairs with low frequency
counts higher (Church and Hanks, 1989) compared
to frequent ones which are favored by LLR. Second,
8To calculate the Spearman correlations values are trans-
formed into ranks (if tied ranks exist, average of ranks is taken),
and we calculate the Pearson correlation on them.
Test Set WS-353 RG-65
Model GW GWB20 GWB50 GW GWB20 GWB50
Agirre .64 .75
Exact .55 .55 .62 .65 .72 .74
CU-2B .53 .58 .62 .66 .72 .74
Table 2: Evaluating word pairs ranking with Exact and
CU counts. Scores are evaluated using ? metric.
for PMI in Fig. 4(a), having more counters does not
improve ?. Third, for LLR in Fig. 4(b), having more
number of counters improve performance and using
2B counters, we get ? close to the Exact.
4.3.3 Effect of Increasing Corpus Size
We evaluate efficient distributional similarity be-
tween word pairs using three different sized cor-
pora: GW (9.8GB), GWB20 (22.8GB), and GWB50
(49GB) on two test sets: WS-353, and RG-65. First,
since for this task using LLR performs better than
PMI, so we will use LLR for this experiment. Sec-
ond, we will fix number of counters to 2B (CU-
2B) as it performs the best in Section 4.2.1. Third,
we will compare the CU-2B counter model with the
Exact over increasing corpus size. We also com-
pare our results against the state-of-the-art results
(Agirre) for distributional similarity (Agirre et al,
2009). We report their results of context window of
size 7.
We can make several observations from the Ta-
ble 2: ? It shows that increasing the amount of
data is not substantially improving the accuracy of
word pair rankings over both the test sets. ? Here
again, CU-2B performs as good as exact for all cor-
pus sizes. ? CU-2B and Exact performs same as the
state-of-the-art system. ? The number of 2B coun-
ters (bounded space) is less than the length of stream
for GWB20 (6.05B ), and GWB50 (13.2B). Hence,
here again it shows that using counters less than the
stream length does not degrade the performance.
5 Dependency Parsing
Recently, maximum spanning tree (MST) algo-
rithms for dependency parsing (McDonald et al,
2005) have shown great promise, primarily in su-
pervised settings. In the MST framework, words in
a sentence form nodes in a graph, and connections
between nodes indicate how ?related? they are. A
maximum spanning tree algorithm constructs a de-
257
pendency parse by linking together ?most similar?
words. Typically the weights on edges in the graph
are parameterized as a linear function of features,
with weight learned by some supervised learning al-
gorithm. In this section, we ask the question: can
word association scores be used to derive syntactic
structures in an unsupervised manner?
A first pass answer is: clearly not. Metrics like
PMI would assign high association scores to rare
word pairs (mostly content words) leading to incor-
rect parses. Metrics like LLR would assign high
association scores to frequent words, also leading
to incorrect parses. However, with a small amount
of linguistic side information (Druck et al, 2009;
Naseem et al, 2010), we see that these issues can
be overcome. In particular, we see that large data
+ a little linguistics > fancy unsupervised learning
algorithms.
5.1 Graph Definition
Our approach is conceptually simple. We construct
a graph over nodes in the sentence with a unique
?root? node. The graph is directed and fully con-
nected, and for any two words in positions i and j,
the weight from word i to word j is defined as:
wij = ?ascasc(wi, wj)??distdist(i?j)+?lingling(ti, tj)
Here, asc(wi, wj) is a association score such as
PMI or LLR computed using approximate counts
from the sketch. Similarly, dist(i ? j) is a simple
parameterized model of distances that favors short
dependencies. We use a simple unnormalized (log)
Laplacian prior of the form dist(i?j) = ?|i?j?1|,
centered around 1 (encouraging short links to the
right). It is negated because we need to convert dis-
tances to similarities.
The final term, ling(ti, tj) asks: according to
some simple linguistic knowledge, how likely is if
that the (gold standard) part of speech tag associated
with word i points at that associated with word j?
For this, we use the same linguistic information
used by (Naseem et al, 2010), which does not
encode direction information. These rules are:
root? { aux, verb }; verb? { noun,
pronoun, adverb, verb }; aux ? {
verb }; noun ? { adj, art, noun,
num }; prep? { noun }; adj ? { adv
len ? 10 len ? 20 all
COHEN-DIRICHLET 45.9 39.4 34.9
COHEN-BEST 59.4 45.9 40.5
ORACLE 75.1 66.6 63.0
BASELINE+LING 42.4 33.8 29.7
BASELINE 33.5 30.4 28.9
CU-2B LLR OPTIMAL 62.4 ? 7.7 51.1 ? 3.2 41.1 ? 1.9
CU-2B PMI OPTIMAL 63.3 ? 7.8 52.0 ? 3.2 41.1 ? 2.0
CU-2B LLR BALANCED 49.1 ? 7.6 43.6 ? 3.3 37.2 ? 1.9
CU-2B PMI BALANCED 49.5 ? 8.0 45.0 ? 3.2 38.3 ? 2.0
CU-2B LLR SEMISUP 55.7 ? 0.0 44.1 ? 0.0 39.4 ? 0.0
CU-2B PMI SEMISUP 56.5 ? 0.0 45.8 ? 0.0 39.9 ? 0.0
Table 3: Comparing CU-2B build on GWB50 + a little lin-
guistics v/s fancy unsupervised learning algorithms.
}. We simply give an additional weight of 1 to any
edge that agrees with one of these linguistic rules.
5.2 Parameter Setting
The remaining issue is setting the interpolation pa-
rameters ? associated with each of these scores.
This is a difficult problem in purely unsupervised
learning. We report results on three settings. First,
the OPTIMAL setting is based on grid search for op-
timal parameters. This is an oracle result based on
grid search over two of the three parameters (hold-
ing the third fixed at 1). In our second approach,
BALANCED, we normalize the three components to
?compete? equally. In particular, we scale and trans-
late all three components to have zero mean and unit
variance, and set the ?s to all be equal to one. Fi-
nally, our third approach, SEMISUP, is based on us-
ing a small amount of labeled data to set the param-
eters. In particular, we use 10 labeled sentences to
select parameters based on the same grid search as
the OPTIMAL setting. Since this relies heavily on
which 10 sentences are used, we repeat this experi-
ment 20 times and report averages.
5.3 Experiments
Our experiments are on a dependency-converted ver-
sion of section 23 of the Penn Treebank using mod-
ified Collins? head finding rules. We measure accu-
racies as directed, unlabeled dependency accuracy.
We separately report results of sentences of length
at most 10, at most 20 and finally of all length. Note
that there is no training or cross-validation: we sim-
ply run our MST parser on test data directly.
The results of the parsing experiments are shown
258
in Table 3. We compare against the following al-
ternative systems. The first, Cohen-Dirichlet and
Cohen-Best, are previously reported state-of-the-art
results for unsupervised Bayesian dependency pars-
ing (Cohen and Smith, 2010). The first is results
using a simple Dirichlet prior; the second is the best
reported results for any system from that paper.
Next, we compare against an ?oracle? system that
uses LLR extracted from the training data for the
Penn Treebank, where the LLR is based on the prob-
ability of observing an edge given two words. This
is not a true oracle in the sense that we might be
able to do better, but it is unlikely. The next two
baseline system are simple right branching base-
line trees. The Baseline system is a purely right-
branching tree. The Baseline+Ling system is one
that is right branching except that it can only create
edges that are compatible with the linguistic rules,
provided a relevant rule exists. For short sentences,
this is competitive with the Dirichlet prior results.
Finally we report variants of our approach using
association scores computed on the GWB50 using
CU sketch with 2 billion counters. We experiment
with two association scores: LLR and PMI. For each
measure, we report results based on the three ap-
proaches described earlier for setting the ? hyper-
parameters. Error bars for our approaches are 95%
confidence intervals based on bootstrap resampling.
The results show that, for this task, PMI seems
slightly better than LLR, across the board. The OP-
TIMAL performance (based on tuning two hyperpa-
rameters) is amazingly strong: clearly beating out
all the baselines, and only about 15 points behind
the ORACLE system. Using the BALANCED ap-
proach causes a degradation of only 3 points from
the OPTIMAL on sentences of all lengths. In general,
the balancing approach seems to be slightly worse
than the semi-supervised approach, except on very
short sentences: for those, it is substantially better.
Overall, though, the results for both Balanced and
Semisup are competitive with state-of-the-art unsu-
pervised learning algorithms.
6 Discussion and Conclusion
The advantage of using sketch in addition to being
memory and time efficient is that it contains counts
for all word pairs and hence can be used to com-
pute association scores like PMI and LLR between
any word pairs. We show that using sketch counts in
our experiments, on the three tasks, we get perfor-
mance comparable to Exact word pair counts setting
and state-of-the-art system. Our method scales to 49
GB of unzipped web data using bounded space of 2
billion counters (8 GB memory). Moreover, the lin-
earity property of the sketch makes it scalable and
usable in distributed setting. Association scores and
counts from sketch can be used for more NLP tasks
like small-space randomized language models, word
sense disambiguation, spelling correction, relation
learning, paraphrasing, and machine translation.
Acknowledgments
The authors gratefully acknowledge the support of
NSF grant IIS-0712764 and Google Research Grant
for Large-Data NLP. Thanks to Suresh Venkatasub-
ramanian and Jagadeesh Jagarlamudi for useful dis-
cussions and the anonymous reviewers for many
helpful comments.
References
Charu C. Aggarwal and Philip S. Yu. 2010. On classi-
fication of high-cardinality data streams. In SDM?10,
pages 802?813.
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Pas?ca, and Aitor Soroa. 2009. A
study on similarity and relatedness using distributional
and wordnet-based approaches. In NAACL ?09: Pro-
ceedings of HLT-NAACL.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Large language mod-
els in machine translation. In Proceedings of EMNLP-
CoNLL.
Moses Charikar, Kevin Chen, and Martin Farach-Colton.
2004. Finding frequent items in data streams. Theor.
Comput. Sci., 312:3?15, January.
K. Church and P. Hanks. 1989. Word Associa-
tion Norms, Mutual Information and Lexicography.
In Proceedings of ACL, pages 76?83, Vancouver,
Canada, June.
S. B. Cohen and N. A. Smith. 2010. Covariance in unsu-
pervised learning of probabilistic grammars. Journal
of Machine Learning Research, 11:3017?3051.
Graham Cormode and Marios Hadjieleftheriou. 2008.
Finding frequent items in data streams. In VLDB.
Graham Cormode and S. Muthukrishnan. 2004. An im-
proved data stream summary: The count-min sketch
and its applications. J. Algorithms.
259
Gregory Druck, Gideon Mann, and Andrew McCal-
lum. 2009. Semi-supervised learning of dependency
parsers using generalized expectation criteria. In Pro-
ceedings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International Joint
Conference on Natural Language Processing of the
AFNLP: Volume 1 - Volume 1, ACL ?09, pages 360?
368, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Cynthia Dwork, Moni Naor, Toniann Pitassi, Guy N.
Rothblum, and Sergey Yekhanin. 2010. Pan-private
streaming algorithms. In In Proceedings of ICS.
Cristian Estan and George Varghese. 2002. New di-
rections in traffic measurement and accounting. SIG-
COMM Comput. Commun. Rev., 32(4).
L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin,
Z. Solan, G. Wolfman, and E. Ruppin. 2002. Plac-
ing search in context: The concept revisited. In ACM
Transactions on Information Systems.
J. Firth. 1968. A synopsis of linguistic theory 1930-
1955. In F. Palmer, editor, Selected Papers of J. R.
Firth. Longman.
Amit Goyal, Hal Daume? III, and Suresh Venkatasubra-
manian. 2009. Streaming for large scale NLP: Lan-
guage modeling. In NAACL.
Amit Goyal, Jagadeesh Jagarlamudi, Hal Daume? III, and
Suresh Venkatasubramanian. 2010a. Sketch tech-
niques for scaling distributional similarity to the web.
In GEMS workshop at ACL, Uppsala, Sweden.
Amit Goyal, Jagadeesh Jagarlamudi, Hal Daume? III, and
Suresh Venkatasubramanian. 2010b. Sketching tech-
niques for Large Scale NLP. In 6th WAC Workshop at
NAACL-HLT.
D. Graff. 2003. English Gigaword. Linguistic Data Con-
sortium, Philadelphia, PA, January.
Z. Harris. 1954. Distributional structure. Word 10 (23),
pages 146?162.
Abby Levenberg and Miles Osborne. 2009. Stream-
based randomised language models for SMT. In
EMNLP, August.
Abby Levenberg, Chris Callison-Burch, and Miles Os-
borne. 2010. Stream-based translation models for
statistical machine translation. In Human Language
Technologies: The 2010 Annual Conference of the
North American Chapter of the Association for Com-
putational Linguistics, HLT ?10, pages 394?402. As-
sociation for Computational Linguistics.
Ping Li, Kenneth Ward Church, and Trevor Hastie. 2008.
One sketch for all: Theory and application of condi-
tional random sampling. In Neural Information Pro-
cessing Systems, pages 953?960.
G. S. Manku and R. Motwani. 2002. Approximate fre-
quency counts over data streams. In VLDB.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic?. 2005. Non-projective dependency parsing
using spanning tree algorithms. In Proceedings of the
conference on Human Language Technology and Em-
pirical Methods in Natural Language Processing, HLT
?05, pages 523?530, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
S. Muthukrishnan. 2005. Data streams: Algorithms and
applications. Foundations and Trends in Theoretical
Computer Science, 1(2).
Tahira Naseem, Harr Chen, Regina Barzilay, and Mark
Johnson. 2010. Using universal linguistic knowl-
edge to guide grammar induction. In Proceedings of
the 2010 Conference on Empirical Methods in Natural
Language Processing, EMNLP ?10, pages 1234?1244.
Association for Computational Linguistics.
Patrick Pantel, Eric Crestan, Arkady Borkovsky, Ana-
Maria Popescu, and Vishnu Vyas. 2009. Web-scale
distributional similarity and entity set expansion. In
Proceedings of EMNLP.
Deepak Ravichandran, Patrick Pantel, and Eduard Hovy.
2005. Randomized algorithms and nlp: using locality
sensitive hash function for high speed noun clustering.
In Proceedings of ACL.
H. Rubenstein and J.B. Goodenough. 1965. Contextual
correlates of synonymy. Computational Linguistics,
8:627?633.
Florin Rusu and Alin Dobra. 2007. Statistical analysis of
sketch estimators. In SIGMOD ?07. ACM.
Stuart Schechter, Cormac Herley, and Michael Mitzen-
macher. 2010. Popularity is everything: a new
approach to protecting passwords from statistical-
guessing attacks. In Proceedings of the 5th USENIX
conference on Hot topics in security, HotSec?10, pages
1?8, Berkeley, CA, USA. USENIX Association.
Qinfeng Shi, James Petterson, Gideon Dror, John Lang-
ford, Alex Smola, and S.V.N. Vishwanathan. 2009.
Hash kernels for structured data. J. Mach. Learn. Res.,
10:2615?2637, December.
Philip J. Stone, Dexter C. Dunphy, Marshall S. Smith,
and Daniel M. Ogilvie. 1966. The General Inquirer:
A Computer Approach to Content Analysis. MIT
Press.
Peter D. Turney and Michael L. Littman. 2003. Measur-
ing praise and criticism: Inference of semantic orienta-
tion from association. ACM Trans. Inf. Syst., 21:315?
346, October.
Peter D. Turney. 2008. A uniform approach to analogies,
synonyms, antonyms, and associations. In Proceed-
ings of COLING 2008.
Benjamin Van Durme and Ashwin Lall. 2009a. Prob-
abilistic counting with randomized storage. In IJ-
CAI?09: Proceedings of the 21st international jont
conference on Artifical intelligence.
260
Benjamin Van Durme and Ashwin Lall. 2009b. Stream-
ing pointwise mutual information. In Advances in
Neural Information Processing Systems 22.
Benjamin Van Durme and Ashwin Lall. 2010. Online
generation of locality sensitive hash signatures. In
Proceedings of the ACL 2010 Conference Short Pa-
pers, pages 231?235, July.
261
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1069?1080, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Fast Large-Scale Approximate Graph Construction for NLP
Amit Goyal and Hal Daume? III
Dept. of Computer Science
University of Maryland
College Park, MD
{amit,hal}@umiacs.umd.edu
Raul Guerra
Dept. of Computer Science
University of Maryland
College Park, MD
rguerra@cs.umd.edu
Abstract
Many natural language processing problems
involve constructing large nearest-neighbor
graphs. We propose a system called FLAG
to construct such graphs approximately from
large data sets. To handle the large amount
of data, our algorithm maintains approximate
counts based on sketching algorithms. To
find the approximate nearest neighbors, our
algorithm pairs a new distributed online-PMI
algorithm with novel fast approximate near-
est neighbor search algorithms (variants of
PLEB). These algorithms return the approxi-
mate nearest neighbors quickly. We show our
system?s efficiency in both intrinsic and ex-
trinsic experiments. We further evaluate our
fast search algorithms both quantitatively and
qualitatively on two NLP applications.
1 Introduction
Many natural language processing (NLP) prob-
lems involve graph construction. Examples in-
clude constructing polarity lexicons based on lexi-
cal graphs from WordNet (Rao and Ravichandran,
2009), constructing polarity lexicons from web data
(Velikovich et al 2010) and unsupervised part-of-
speech tagging using label propagation (Das and
Petrov, 2011). The later two approaches con-
struct nearest-neighbor graphs between word pairs
by computing nearest neighbors between word pairs
from large corpora. These nearest neighbors form
the edges of the graph, with weights given by the
distributional similarity (Turney and Pantel, 2010)
between terms. Unfortunately, computing the distri-
butional similarity between all words in a large vo-
cabulary is computationally and memory intensive
when working with large amounts of data (Pantel et
al., 2009). This bottleneck is typically addressed by
means of commodity clusters. For example, Pantel
et al(2009) compute distributional similarity be-
tween 500 million terms over a 200 billion words in
50 hours using 100 quad-core nodes, explicitly stor-
ing a similarity matrix between 500 million terms.
In this work, we propose Fast Large-Scale Ap-
proximate Graph (FLAG) construction, a sys-
tem that constructs a fast large-scale approximate
nearest-neighbor graph from a large text corpus. To
build this system, we exploit recent developments
in the area of approximation, randomization and
streaming for large-scale NLP problems (Ravichan-
dran et al 2005; Goyal et al 2009; Levenberg et
al., 2010). More specifically we exploit work on Lo-
cality Sensitive Hashing (LSH) (Charikar, 2002) for
computing word-pair similarities from large text col-
lections (Ravichandran et al 2005; Van Durme and
Lall, 2010). However, Ravichandran et al(2005)
approach stored an enormous matrix of all unique
words and their contexts in main memory, which is
infeasible for very large data sets. A more efficient
online framework to locality sensitive hashing (Van
Durme and Lall, 2010; Van Durme and Lall, 2011)
computes distributional similarity in a streaming set-
ting. Unfortunately, their approach can handle only
additive features like raw-counts, and not non-linear
association scores like pointwise mutual information
(PMI), which generates better context vectors for
distributional similarity (Ravichandran et al 2005;
Pantel et al 2009; Turney and Pantel, 2010).
In FLAG, we first propose a novel distributed
online-PMI algorithm (Section 3.1). It is a stream-
ing method that processes large data sets in one pass
while distributing the data over commodity clusters
1069
and returns context vectors weighted by pointwise
mutual information (PMI) for all the words. Our
distributed online-PMI algorithm makes use of the
Count-Min (CM) sketch algorithm (Cormode and
Muthukrishnan, 2004) (previously shown effective
for computing distributional similarity in our ear-
lier work (Goyal and Daume? III, 2011)) to store the
counts of all words, contexts and word-context pairs
using only 8GB of main memory. The main motiva-
tion for using the CM sketch comes from its linear-
ity property (see last paragraph of Section 2) which
makes CM sketch to be implemented in distributed
setting for large data sets. In our implementation,
FLAG scaled up to 110 GB of web data with 866
million sentences in less than 2 days using 100 quad-
core nodes. Our intrinsic and extrinsic experiments
demonstrate the effectiveness of distributed online-
PMI.
After generating context vectors from distributed
online-PMI algorithm, our goal is to use them to find
fast approximate nearest neighbors for all words. To
achieve this goal, we exploit recent developments in
the area of existing randomized algorithms for ran-
dom projections (Achlioptas, 2003; Li et al 2006),
Locality Sensitive Hashing (LSH) (Charikar, 2002)
and improve on previous work done on PLEB (Point
Location in Equal Balls) (Indyk and Motwani, 1998;
Charikar, 2002). We propose novel variants of PLEB
to address the issue of reducing the pre-processing
time for PLEB. One of the variants of PLEB (FAST-
PLEB) with considerably less pre-processing time
has effectiveness comparable to PLEB. We evaluate
these variants of PLEB both quantitatively and qual-
itatively on large data sets. Finally, we show the ap-
plicability of large-scale graphs built from FLAG on
two applications: the Google-Sets problem (Ghahra-
mani and Heller, 2005), and learning concrete and
abstract words (Turney et al 2011).
2 Count-Min sketch
The Count-Min (CM) sketch (Cormode and
Muthukrishnan, 2004) belongs to a class of ?sketch?
algorithms that represents a large data set with a
compact summary, typically much smaller than the
full size of the input by processing the data in
one pass. The following surveys comprehensively
review the streaming literature (Rusu and Dobra,
2007; Cormode and Hadjieleftheriou, 2008) and
sketch techniques (Charikar et al 2004; Li et al
2008; Cormode and Muthukrishnan, 2004; Rusu
and Dobra, 2007). In our another recent paper
(Goyal et al 2012), we conducted a systematic
study and compare many sketch techniques which
answer point queries with focus on large-scale NLP
tasks. In that paper, we empirically demonstrated
that CM sketch performs the best among all the
sketches on three large-scale NLP tasks.
CM sketch uses hashing to store the approximate
frequencies of all items from the large data set onto a
small sketch vector that can be updated and queried
in constant time. CM has two parameters  and ?: 
controls the amount of tolerable error in the returned
count and ? controls the probability with which the
error exceeds the bound .
CM sketch with parameters (,?) is represented
as a two-dimensional array with width w and depth
d; where w and d depends on  and ? respectively.
We set w=2 and d=log(
1
? ). The depth d denotes
the number of pairwise-independent hash functions
employed by the CM sketch; and the width w de-
notes the range of the hash functions. Given an
input stream of items of length N (x1, x2 . . . xN ),
each of the hash functions hk:{x1, x2 . . . xN} ?
{1 . . . w},?1 ? k ? d, takes an item from the in-
put stream and maps it into a position indexed by the
corresponding hash function.
UPDATE: For each new item ?x? with count c, the
sketch is updated as:
sketch[k, hk(x)]? sketch[k, hk(x)]+c, ?1 ? k ? d.
QUERY: Since multiple items can be hashed to the
same index for each row of the array, hence the
stored frequency in each row is guaranteed to over-
estimate the true count, which makes it a biased esti-
mator. Therefore, to answer the point query (QUERY
(x)), CM returns the minimum over all the d posi-
tions indexed by the hash functions.
c?(x) = mink sketch[k, hk(x)], ?1 ? k ? d.
All reported frequencies by CM exceed the true
frequencies by at most N with probability of at
least 1 ? ?. The space used by the algorithm is
O(1 log
1
? ). Constant time of O(log(
1
? )) per each
update and query operation.
CM sketch has a linearity property which states
that: Given two sketches s1 and s2 computed (us-
1070
ing the same parameters w and d, and the same set
of d hash functions) over different input streams; the
sketch of the combined data stream can be easily ob-
tained by adding the individual sketches in O(d?w)
time which is independent of the stream size. This
property enables sketches to be implemented in dis-
tributed setting, where each machine computes the
sketch over a small portion of the corpus and makes
it scalable to large datasets.
The idea of conservative update (Estan and Vargh-
ese, 2002) is to only increase counts in the sketch
by the minimum amount needed to ensure that the
estimate remains accurate. We (Goyal and Daume?
III, 2011) used CM sketch with conservative update
(CM-CU sketch) to show that the update reduces
the amount of over-estimation error by a factor of
at least 1.5 on NLP data and showed the effective-
ness of CM-CU on three important NLP tasks. The
QUERY procedure for CM-CU is identical to Count-
Min. However, to UPDATE an item ?x? with fre-
quency c, first we compute the frequency c?(x) of this
item from the existing data structure:
(?1 ? k ? d, c?(x) = mink sketch[k, hk(x)])
and the counts are updated according to:
sketch[k, hk(x)]? max{sketch[k, hk(x)], c?(x) + c}.
The intuition is that, since the point query returns
the minimum of all the d values, we will update
a counter only if it is necessary as indicated by
the above equation. This heuristic avoids the
unnecessary updating of counter values to reduce
the over-estimation error.
3 FLAG: Fast Large-Scale Approximate
Graph Construction
We describe a system, FLAG, for generating a near-
est neighbor graph from a large corpus. For ev-
ery node (word), our system returns top l approxi-
mate nearest neighbors, which implicitly defines the
graph. Our system operates in four steps. First, for
every word ?z?, our system generates a sparse con-
text vector (?(c1, v1); (c2, v2) . . . ; (cd, vd)?) of size
d where cd denotes the context and vd denotes the
PMI (strength of association) between the context
cd and the word ?z?. The context can be lexical,
semantic, syntactic, and/or dependency units that
co-occur with the word ?z?. We compute this ef-
ficiently using a new distributed online Pointwise
Mutual Information algorithm (Section 3.1). Sec-
ond, we project all the words with context vector
size d onto k random vectors and then binarize these
random projection vectors (Section 3.2). Third, we
propose novel variants of PLEB (Section 3.3) with
less pre-processing time to represent data for fast
query retrieval. Fourth, using the output of vari-
ants of PLEB, we generate a small set of potential
nearest neighbors for every word ?z? (Section 3.4).
From this small set, we can compute the Hamming
distance between every word ?z? and its potential
nearest neighbors to return the l nearest-neighbors
for all unique words.
3.1 Distributed online-PMI
We propose a new distributed online Pointwise Mu-
tual Information (PMI) algorithm motivated by the
online-PMI algorithm (Van Durme and Lall, 2009b)
(page 5). This is a streaming algorithm which pro-
cesses the input corpus in one pass. After one
pass over the data set, it returns the context vec-
tors for all query words. The original online-PMI
algorithm was used to find the top-d verbs for a
query verb using the highest approximate online-
PMI values using a Talbot-Osborne-Morris-Bloom1
(TOMB) Counter (Van Durme and Lall, 2009a).
Unfortunately, this algorithm is prohibitively slow
when computing contexts for all words, rather than
just a small query set. This motivates us to propose
a distributed variant that enables us to scale to large
data and large vocabularies.2
We make three modifications to the original
online-PMI algorithm and refer to it as the ?modified
online-PMI algorithm? shown in Algorithm 1. First,
we use Count-Min with conservative update (CM-
CU) sketch (Goyal and Daume? III, 2011) instead of
TOMB. We prefer CM because it enables distribu-
tion due to its linearity property (Section 2) and foot-
note #1. Distribution using TOMB is not known in
literature and we will like to explore that direction in
future. Second, we store the counts of words (?z?),
contexts (?y?) and word-context pairs all together in
1TOMB is a variant of CM sketch which focuses on reduc-
ing the bit size of each counter (in addition to the number of
counters) at the cost of incurring more error in the counts.
2The serialized online-PMI algorithm took a week to gener-
ate context vectors for all the words from GW (Section 4.1).
1071
Algorithm 1 Modified online-PMI
Require: Data set D, buffer size B
Ensure: context vectors V , mapping word z to d-best
contexts in priority queue ? y,PMI(z, y)?
1: initialize CM-CU sketch to store approximate counts
of words, context and word-context pairs
2: for each buffer B in the data set D do
3: initialize S to store ?z,y? observed in B
4: for ?z,y? in B do
5: set S (?z,y?) =1
6: insert z, y and pair ?z,y? in sketch
7: end for
8: for x in set S do
9: recompute vectors V(x) using current contexts
in priority queue and {y|S(?z,y?)=1}
10: end for
11: end for
12: return context vectors V
the CM-CU sketch (in the original online-PMI al-
gorithm, exact counts of words and contexts were
stored in a hash table; only the pairs were stored in
the TOMB data structure). Third, in the original al-
gorithm, for each ?z? a vector of top-d contexts are
modified at the end of each buffer (refer Algorithm
1). However, in our algorithm, we only modify the
list of those ?z??s which appeared in the recent buffer
rather than modifying for all the ?z??s (Note, if ?z?
does not appear in the recent buffer, then its top-d
contexts cannot be changed. Hence, we only modify
those ?z?s which appear in the recent buffer).
In our distributed online-PMI algorithm, first we
split the data into chunks of 10 million sentences.
Second, we run the modified online-PMI algorithm
on each chunk in distributed setting. This stores
counts of all words (?z?), contexts (?y?) and word-
context pairs in the CM-CU sketch, and store top-d
contexts for each word in priority queues. In third
step, we merge all the sketches using linearity prop-
erty to sum the counts of the words, contexts and
word-context pairs. Additionally we merge the lists
of top-d contexts for each word. In the last step, we
use the single merged sketch and merged top-d con-
texts list to generate the final distributed online-PMI
top-d contexts list.
It takes around one day to compute context vec-
tors for all the words from a chunk of 10 million
sentences using first step of distributed online-PMI.
We generated context vectors for all the 87 chunks
(110 GB data with 866 million sentences: see Table
1) in one day by running one process per chunk over
a cluster. The first step of the algorithm involves
traversing the data set and is the most time intensive
step. For the second step, the merging of sketches is
fast, since sketches are two dimensional array data
structures (we used the sketch of size 2 billion coun-
ters with 3 hash functions). Merging the lists of top-
d contexts for each word is embarrassingly parallel
and fast. The last step to generate the final top-d
contexts list is again embarrassingly parallel and fast
and takes couple of hours to generate the top-d con-
texts for all the words from all the chunks. If im-
plemented serially the ?modified online-PMI algo-
rithm? on 110 GB data with 866 million sentences
would take approximately 3 months.
The downside of the distributed online-PMI is that
it splits the data into small chunks and loses infor-
mation about the global best contexts for a word
over all the chunks. The algorithm locally computes
the best contexts for each chunk, that can be bad if
the algorithm misses out globally good contexts and
that can affect the accuracy of downstream applica-
tion. We will demonstrate in our experiments (Sec-
tion 4.2) by using distributed online-PMI, we do not
lose any significant information about global con-
texts and perform comparable to offline-PMI over
an intrinsic and extrinsic evaluation.
3.2 Dimensionality Reduction from RD to Rk
We are given context vectors for Z words, our goal
is to use k random projections to project the con-
text vectors from RD to Rk. There are total D
unique contexts (D >> k) for all Z words. Let
(?(c1, v1); (c2, v2) . . . ; (cd, vd)?) be sparse context
vectors of size d for Z words. For each word, we use
hashing to project the context vectors onto k direc-
tions. We use k pairwise independent hash functions
that maps each of the d context (cd) dimensions onto
?d,k ? {?1,+1}; and compute inner product be-
tween ?d,k and vd. Next, ?k,
?
d ?d,k.vd returns the
k random projections for each word ?z?. We store
the k random projections for all words (mapped to
integers) as a matrix A of size of k ? Z.
The mechanism described above generates ran-
dom projections by implicitly creating a random
projection matrix from a set of {?1,+1}. This
idea of creating implicit random projection matrix
1072
?????
1 2 ? ? ? Z
k1 ?z1, 26? ?z2, 80? ? ? ? ?zZ , 3?
k2 ?z1,?28? ?z2, 6? ? ? ? ?zZ , 111?... ... ... . . . ...
kK ?z1, 78? ?z2, 69? ? ? ? ?zZ , 92?
?
????
Sort=?
(a) Matrix A
?
????
Smallest to Largest
?zZ , 3? ?z1, 26? ? ? ? ?zm, 700?
?zr,?50? ?z2, 6? ? ? ? ?zZ , 111?... ... . . . ...
?z1, 78? ?zZ , 92? ? ? ? ?zu, 432?
?
????
?
(b) Matrix A
?
????
1 2 ? ? ? Z
zZ z1 ? ? ? zm
zr z2 ? ? ? zZ... ... . . . ...
z1 zZ ? ? ? zu
?
????
?
(c) Matrix A
?
???
z1 z2 ? ? ? zZ
2 60 ? ? ? 1
55 2 ? ? ? Z... ... . . . ...
1 90 ? ? ? 2
?
???
(d) Matrix C
Figure 1: First matrix pairs the words 1 ? ? ?Z and their random projection values. Second matrix sorts each row by the random
projection values from smallest to largest. Third matrix throws away the projection values leaving only the words. Fourth matrix
maps the words 1 ? ? ?Z to their sorted position in the third matrix for each k. This allows constant query time for all the words.
is motivated by the work on stable random projec-
tions (Li et al 2006; Li et al 2008), Count sketch
(Charikar et al 2004), feature hashing (Weinberger
et al 2009) and online Locality Sensitive Hashing
(LSH) (Van Durme and Lall, 2010). The idea of gen-
erating random projections from the set {?1,+1}
was originally proposed by Achlioptas (2003).
Next we create a binary matrix B using matrix
A by taking sign of each of the entries of the ma-
trix A. If A(i, j) ? 0, then B(i, j) = 1; else
B(i, j) = 0. This binarization creates Locality Sen-
sitive Hash (LSH) function that preserves the cosine
similarity between every pair of word vectors. This
idea was first proposed by Charikar (2002) and used
in NLP for large-scale noun clustering (Ravichan-
dran et al 2005). However, in large-scale noun
clustering work, their approach had to store the ran-
dom projection matrix of size D ? k; where D de-
notes the number of all unique contexts (which is
generally large and D >> Z) and in this paper, we
do not explicitly require storing a random projection
matrix.
3.3 Representation for Fast-Search
We describe three approaches to represent the data
(matrix A and B from Section 3.2) in such a manner
that finding nearest neighbors is fast. These three
approaches differ in amount of pre-processing time.
First, we propose a naive baseline approach using
random projections independently with the best pre-
processing time. Second, we describe PLEB (Point
Location in Equal Balls) (Indyk and Motwani, 1998;
Charikar, 2002) with the worst pre-processing time.
Third, we propose a variant of PLEB to reduce its
pre-processing time.
3.3.1 Independent Random Projections (IRP)
Here, we describe a naive baseline approach to
arrange nearest neighbors next to each other by us-
ing Independent Random Projections (IRP). In this
approach, we pre-process the matrix A. First for
matrix A, we pair the words z1 ? ? ? zZ and their ran-
dom projection values as shown in Fig. 1(a). Sec-
ond, we sort the elements of each row of matrix A
by their random projection values from smallest to
largest (shown in Fig. 1(b)). The sorting step takes
O(ZlogZ) time (We can assume k to be a constant).
The sorting operation puts all the nearest neighbor
words (for each k independent projections) next to
each other. After sorting the matrix A, we throw
away the projection values leaving only the words
(see Fig. 1(c)). To search for a word in matrix A
in constant time, we create another matrix C of size
(k ? Z) (see Fig. 1(d)). Matrix C maps the words
z1 ? ? ? zZ to their sorted position in the matrix A (see
Fig. 1(c)) for each k.
3.3.2 PLEB
PLEB (Point Location in Equal Balls) was first
proposed by Indyk and Motwani (1998) and further
improved by Charikar (2002). The improved PLEB
algorithm puts in operation all k random projections
together. It randomly permutes the ordering of k bi-
nary LSH bits (stored in matrix B) for all the words
p times. For each permutation it sorts all the words
lexicographically based on their permuted LSH rep-
resentation of size k. The sorting operation puts all
the nearest neighbor words (using k projections to-
gether) next to each other for all the permutations.
In practice p is generally large, Ravichandran et al
(2005) used p = 1000 in their work.
In our implementation of PLEB, we have a matrix
A of size (p ? Z) similar to the first matrix in Fig.
1(a). The main difference to the first matrix in Fig.
1(a) is that bit vectors of size k are used for sorting
rather than using scalar projection values. Similar to
Fig. 1(c) after sorting, bit vectors are discarded and
1073
a matrix C of size (p? Z) is used to map the words
1 ? ? ?Z to their sorted position in the matrixA. Note,
in IRP approach, the size of A and C matrix is (k ?
Z). In PLEB generating random permutations and
sorting the bit vectors of size k involves worse pre-
processing time than using IRP. However, spending
more time in pre-processing leads to finding better
approximate nearest neighbors.
3.3.3 FAST-PLEB
To reduce the pre-processing time for PLEB, we
propose a variant of PLEB (FAST-PLEB). In PLEB,
while generating random permutations, it uses all
the k bits. In this variant, for each random permu-
tation we randomly sample without replacement q
(q << k) bits out of k. We use q bits to repre-
sent each permutation and sort based on these q bits.
This makes pre-processing faster for PLEB. Section
4.3 shows that FAST-PLEB only needs q = 10 to
perform comparable to PLEB with q = 3000 (that
makes FAST-PLEB 300 times faster than PLEB).
Here, again we store matrices A and C of size
(p? Z).
3.4 Finding Approximate Nearest Neighbors
The goal here is to exploit three representations dis-
cussed in Section 3.3 to find approximate nearest
neighbors quickly. For all the three methods (IRP,
PLEB, FAST-PLEB), we can use the same fast ap-
proximate search which is simple and fast. To search
a word ?z?, first, we can look up matrix C to locate
the k positions where ?z? is stored in matrix A. This
can be done in constant time (Again assuming k (for
IRP) and p (for PLEB and FAST-PLEB) to be a con-
stant.). Once, we find ?z? in each row, we can select
b (beam parameter) neighbors (b/2 neighbors from
left and b/2 neighbors from right of the query word.)
for all the k or p rows. This can be done in constant
time (Assuming k, p and b to be constants.). This
search procedure produces a set of bk (IRP) or bp
(PLEB and FAST-PLEB) potential nearest neighbors
for a query word ?z?. Next, we compute Hamming
distance between query word ?z? and the set of po-
tential nearest neighbors from matrix B to return l
closest nearest neighbors. For computing hamming
distance, all the approaches discussed in Section 3.3
require all k random projection bits.
4 Experiments
We evaluate our system FLAG for fast large-scale
approximate graph construction. First, we show that
using distributed online-PMI algorithm is as effec-
tive as offline-PMI. Second, we compare the approx-
imate nearest neighbors lists generated by FLAG
against the exact nearest neighbor lists. Finally, we
show the quality of our approximate similarity lists
generated by FLAG from the web corpus.
4.1 Experimental Setup
Data sets: We use two data sets: Gigaword (Graff,
2003) and a copy of news web (Ravichandran et
al., 2005). For both the corpora, we split the text
into sentences, tokenize and convert into lower-case.
To evaluate our approximate graph construction, we
evaluate on three data sets: Gigaword (GW), Giga-
word + 50% of web data (GWB50) and Gigaword
+ 100% ((GWB100)) of web data. Corpus statistics
are shown in Table 1. We define the context for a
given word ?z? as the surrounding words appearing
in a window of 2 words to the left and 2 words to
the right. The context words are concatenated along
with their positions -2, -1, +1, and +2.
Corpus GW GWB50 GWB100
Unzipped
12 60 110
Size (GB)
# of sentences
57 463 866
(Million)
# of tokens
2.1 10.9 20.0
(Billion)
Table 1: Corpus Description
4.2 Evaluating Distributed online-PMI
Experimental Setup: First we do an intrinsic
evaluation to quantitatively evaluate the distributed
online-PMI vectors against the offline-PMI vectors
computed from Gigaword (GW). Offline-PMI com-
puted from the sketches have been shown as effec-
tive as exact PMI by Goyal and Daume? III (2011).
To compute offline-PMI vectors, we do two passes
over the corpus. In the first pass, we store the counts
of words, contexts and word-context pairs computed
from GW in the Count-Min with conservative up-
date (CM-CU) sketch. We use the CM-CU sketch
of size 2 billion counters (bounded 8 GB memory)
1074
with 3 hash functions. In second pass, using the
aggregated counts from the sketch, we generate the
offline-PMI vectors of size d = 1000 for every word.
For rest of this paper for distributed online-PMI, we
set d = 1000 and the size of the buffer=10, 000 and
we split the data sets into small chunks of 10 million
sentences.
Intrinsic Evaluation: We use four kinds of mea-
sures: precision (P), recall (R), f-measure (F1) and
Pearson?s correlation (?) to measure the overlap in
the context vectors obtained using online and offline
PMI. ? is computed between contexts that are found
in offline and online context vectors. We do this
evaluation on 447 words selected from the concate-
nation of four test-sets mentioned in the next para-
graph. On these 447 words, we achieve an average P
of .97, average R of .96 and average F1 of .97 and a
perfect average ? of 1. This evaluation show that the
vectors obtained using online-PMI are as effective
as offline-PMI.
Extrinsic Evaluation: We also compare online-
PMI effectiveness on four test sets which consist of
word pairs, and their corresponding human rank-
ings. We generate the word pair rankings using
online-PMI and offline-PMI strategies. We report
the Pearson?s correlation (?) between the human and
system generated similarity rankings. The four test
sets are: WS-353 (Finkelstein et al 2002) is a set
of 353 word pairs. WS-203: A subset of WS-353
with 203 word pairs (Agirre et al 2009). RG-65:
(Rubenstein and Goodenough, 1965) has 65 word
pairs. MC-30: A subset of RG-65 dataset with 30
word pairs (Miller and Charles, 1991).
The results in Table 2 shows that by using dis-
tributed online-PMI (by making a single pass over
the corpus) is comparable to offline-PMI (which is
computed by making two passes over the corpus).
For generating context vectors from GW, for both
offline-PMI and online-PMI, we use a frequency
cutoff of 5 for word-context pairs to throw away the
rare terms as they are sensitive to PMI (Church and
Hanks, 1989). Next, FLAG generates online-PMI
vectors from GWB50 and GWB100 and uses fre-
quency cutoffs of 15 and 25. The higher frequency
cutoffs are selected based on the intuition that, with
more data, we get more noise, and hence not con-
sidering word-context pairs with frequency less than
25 will be better for the system. As FLAG is go-
ing to use the context vectors to find nearest neigh-
bors, we also throw away all those words which have
? 50 contexts associated with them. This generates
context vectors for 57, 930 words from GW; 95, 626
from GWB50 and 106, 733 from GWB100.
Test Set WS-353 WS-203 RG-65 MC-30
Offline-PMI .41 .55 .40 .52
Online-PMI .41 .56 .39 .51
Table 2: Evaluating word pairs ranking with online and offline
PMI. Scores are evaluated using ? metric.
10 25 50 100
R ? R ? R ? R ?
IRP .40 .53 .38 .51 .35 .54 .34 .51
q=1 .24 .62 .20 .63 .18 .59 .17 .54
q=5 .47 .60 .43 .57 .40 .57 .37 .53
q=10 .53 .58 .49 .56 .45 .55 .42 .53
q=100 .53 .60 .50 .59 .46 .56 .43 .53
q=3000 .54 .58 .50 .59 .46 .56 .43 .54
Table 4: Varying parameter q for FAST-PLEB with fixed p =
1000, k = 3000 and b = 40. Results reported on recall and ?.
4.3 Evaluating Approximate Nearest Neighbor
Experimental Setup: To evaluate approximate
nearest neighbor similarity lists generated by
FLAG, we conduct three experiments. We evaluate
all the three experiments on 447 words (test set) as
used in Section 4.2. For each word, both exact and
approximate methods return l = 100 nearest neigh-
bors. The exact similarity lists for 447 test words is
computed by calculating cosine similarity between
447 test words with respect to all other words. We
also compare the LSH (computed using Hamming
distance between all words and test set.) approxi-
mate nearest neighbor similarity lists against the ex-
act similarity lists. LSH provides an upper bound
on the performance of our approximate search rep-
resentations (IRP, PLEB, and FAST-PLEB) for fast-
search from Section 3.3) . We set the number of
projections k = 3000 for all three methods and for
PLEB and FAST-PLEB, we set number of permuta-
tions p = 1000 as used in large-scale noun cluster-
ing work (Ravichandran et al 2005).
Evaluation Metric: We use two kinds of mea-
sures, recall and Pearson?s correlation to measure
the overlap in the approximate and exact similarity
lists. Intuitively, recall (R) captures the number of
1075
IRP PLEB FAST-PLEB
10 25 50 100 10 25 50 100 10 25 50 100
R ? R ? R ? R ? R ? R ? R ? R ? R ? R ? R ? R ?
LSH .55 .57 .52 .56 .49 .54 .46 .52 .55 .57 .52 .56 .49 .54 .46 .52 .55 .57 .52 .56 .49 .54 .46 .52
20 .29 .50 .26 .55 .25 .54 .24 .50 .50 .59 .45 .60 .41 .57 .37 .55 .48 .58 .42 .58 .38 .58 .35 .55
30 .36 .55 .33 .56 .31 .55 .30 .52 .53 .59 .48 .59 .44 .56 .41 .54 .51 .57 .47 .57 .42 .56 .40 .54
40 .40 .53 .38 .51 .35 .54 .34 .51 .54 .58 .50 .59 .46 .56 .43 .54 .53 .58 .49 .56 .45 .55 .42 .53
50 .44 .56 .42 .54 .39 .54 .37 .52 .54 .58 .51 .57 .47 .56 .44 .53 .54 .58 .50 .56 .46 .55 .44 .53
100 .53 .59 .49 .54 .46 .55 .43 .53 .55 .56 .52 .56 .48 .54 .46 .53 .55 .57 .52 .56 .48 .54 .46 .53
Table 3: Evaluation results on comparing LSH, IRP, PLEB, and FAST-PLEB with k = 3000 and b = {20, 30, 40, 50, 100} with
exact nearest neighbors over GW data set. For PLEB and FAST-PLEB, we set p = 1000 and for FAST-PLEB, we set q = 10. We
report results on recall (R) and ? metric. For IRP, we sample first p rows and only use p rows rather than k.
GW GWB50 GWB100
10 25 50 100 10 25 50 100 10 25 50 100
R ? R ? R ? R ? R ? R ? R ? R ? R ? R ? R ? R ?
LSH .55 .57 .52 .56 .49 .54 .46 .52 .51 .55 .46 .54 .44 .52 .42 .48 .48 .58 .45 .52 .42 .49 .40 .47
IRP .40 .53 .37 .53 .35 .54 .34 .51 .29 .50 .27 .51 .25 .51 .24 .47 .26 .57 .24 .49 .23 .48 .22 .45
PLEB .54 .58 .50 .59 .46 .56 .43 .54 .46 .58 .42 .56 .38 .53 .36 .51 .44 .57 .40 .56 .36 .52 .33 .49
FAST-PLEB .53 .58 .49 .56 .45 .55 .42 .53 .46 .56 .41 .56 .37 .54 .35 .51 .43 .57 .38 .55 .35 .52 .32 .50
Table 5: Evaluation results on comparing LSH, IRP, PLEB, and FAST-PLEB with k = 3000, b = 40, p = 1000 and q = 10 with
exact nearest neighbors across three different data sets: GW, GWB50, and GWB100. We report results on recall (R) and ? metric.
The gray color row is the system that we use for further evaluations.
nearest neighbors that are found in both the lists and
then Pearson?s (?) correlation captures if the rela-
tive order of these lists is preserved in both the sim-
ilarity lists. We also compute R and ? at various
l = {10, 25, 50, 100}.
Results: For the first experiment, we evaluate
IRP, PLEB, and FAST-PLEB against the exact near-
est neighbor similarity lists. For IRP, we sample
first p rows and only use p rather than k, this en-
sures that all the three methods (IRP, PLEB, and
FAST-PLEB) take the same query time. We vary
the approximate nearest neighbor beam parameter
b = {20, 30, 40, 50, 100} that controls the number
of closest neighbors for a word with respect to each
independent random projection. Note, with increas-
ing b, our algorithm approaches towards LSH (com-
puting Hamming distance with respect to all the
words). For FAST-PLEB, we set q = 10 (q << k)
that is the number of random bits selected out of k to
generate p permuted bit vectors of size q. The results
are reported in Table 3, where the first row com-
pares the LSH approach against the exact similar-
ity list for test set words. Across three columns we
compare IRP, PLEB, and FAST-PLEB. For all meth-
ods, increasing b means better recall. If we move
down the table, with b = 100, IRP, PLEB, and FAST-
PLEB get results comparable to LSH (reaches an up-
per bound). However, using large b implies gener-
ating a long potential nearest neighbor list close to
the size of the unique context vectors. If we focus
on the gray color row with b = 40 (This will have
comparatively small potential list and return nearest
neighbors in less time), IRP has worse recall with
best pre-processing time. FAST-PLEB (q = 10) is
comparable to PLEB (using all bits q = 3000) with
pre-processing time 300 times faster than PLEB. For
rest of this work, FLAG will use FAST-PLEB as it
has best recall and pre-processing time with fixed
b = 40.
For the second experiment, we vary parameter
q = {1, 5, 10, 100, 3000} for FAST-PLEB in Table
4. Table 4 demonstrates using q = {1, 5} result in
worse recall, however using q = 5 for FAST-PLEB
is better than IRP. q = 10 has comparable recall
to q = {100, 3000}. For rest of this work, we fix
q = 10 as it has best recall and pre-processing time.
For the third experiment, we increase the size of
the data set across the Table 5. With the increase
in size of the data set, LSH, IRP, PLEB, and FAST-
PLEB (q = 10) have worse recall. The reason for
such a behavior is that the number of unique context
vectors is greater for big data sets. Across all the
1076
jazz yale soccer physics wednesday
reggae harvard basketball chemistry tuesday
rockabilly cornell hockey mathematics thursday
rock fordham lacrosse biology monday
bluegrass rutgers handball biochemistry friday
indie dartmouth badminton science saturday
baroque nyu softball microbiology sunday
ska ucla football geophysics yesterday
funk princeton tennis economics tues
banjo stanford wrestling psychology october
blues loyola rugby neuroscience week
Table 6: Sample Top 10 similarity lists returned by FAST-PLEB
with k = 3000, p = 1000, b = 40 and q = 10 from GWB100.
three data sets, FAST-PLEB has recall comparable to
PLEB with best pre-processing time. Hence, for the
next evaluation to show the quality of final lists we
use FAST-PLEB with q = 10 for GWB100 data set.
In Table 6, we list the top 10 most similar words
for some words found by our system FLAG using
GWB100 data set. Even though FLAG?s approxi-
mate nearest neighbor algorithm has less recall with
respect to exact but still the quality of these nearest
neighbor lists is excellent.
For the final experiment, we demonstrate the pre-
processing and query time results comparing LSH,
IRP, PLEB, and FAST-PLEB with k = 3000, p =
1000, b = 40 and q = 10 parameter settings. For
pre-processing timing results, we perform all the ex-
periments (averaged over 5 runs) on GWB100 data
set with 106, 733 words. The second pre-processing
step of the system FLAG (Section 3.2) that is di-
mensionality reduction from RD to Rk took 8.8
hours. The pre-processing time differences among
IRP, PLEB, and FAST-PLEB from third step (Section
3.3) are shown in second column of Table 7. Ex-
perimental results show that the naive baseline IRP
is the fastest and FAST-PLEB has 120 times faster
pre-processing time compared to PLEB.
For comparing query time among several meth-
ods, we evaluate over 447 words (Section 4.2). We
report average timing results (averaged over 10 runs
and 447 words) to find top 100 nearest neighbors for
single query word. The results are shown in third
column of Table 7. Comparing first and second rows
show that LSH is 87 times faster than computing
exact top-100 (cosine similarity) nearest neighbors.
Comparing second, third, fourth and fifth rows of the
table demonstrate that IRP, PLEB and FAST-PLEB
Methods Preprocessing Query (seconds)
Exact n/a 87
LSH 8.8 hours 0.59
IRP 7.5 minutes 0.28
PLEB 1.8 days 0.28
FAST-PLEB 22 minutes 0.26
Table 7: Preprocessing and query time results compar-
ing exact, LSH, IRP, PLEB, and FAST-PLEB methods on
GWB100 data set.
Language english chinese japanese spanish russian
Place africa america washington london pacific
Nationality american european french british western
Date january may december october june
Organization ford microsoft sony disneyland google
Table 8: Query terms for Google Sets Problem evaluation
methods are twice as fast as LSH.
5 Applications
We use the graph constructed by FLAG from
GWB100 data set (110 GB) by applying FAST-
PLEB with parameters k = 3000, p = 1000, q = 10
and b = 40. The graph has 106, 733 nodes (words),
with each node having 100 edges that denote the top
l = 100 approximate nearest neighbors associated
with each node. However, FLAG applied FAST-
PLEB (approximate search) to find these neighbors.
Therefore many of these edges can be noisy for our
applications. Hence for each node, we only consider
top 10 edges. In general for graph-based NLP prob-
lems; for example, constructing web-derived polar-
ity lexicons (Velikovich et al 2010), top 25 edges
were used, and for unsupervised part-of-speech tag-
ging using label propagation (Das and Petrov, 2011),
top 5 edges were used.
5.1 Google Sets Problem
Google Sets problem (Ghahramani and Heller,
2005) can be defined as: given a set of query words,
return top t similar words with respect to query
words. To evaluate the quality of our approximate
large-scale graph, we return top 25 words which
have best aggregated similarity scores with respect
to query words. We take 5 classes and their query
terms (McIntosh and Curran, 2008) shown in Table
8 and our goal is to learn 25 new words which are
similar with these 5 query words.
1077
Language: german, french, estonian, hungarian, bulgarian
Place: scandinavia, mongolia, mozambique, zambia, namibia
Nationality: german, hungarian, estonian, latvian, lithuanian
Date: september, february, august, july, november
Organization: looksmart, hotbot, lycos, webcrawler, alltheweb
Table 9: Learned terms for Google Sets Problem
Concrete car, house, tree, horse, animal
seeds man, table, bottle, woman, computer
Abstract idea, bravery, deceit, trust, dedication
seeds anger, humour, luck, inflation, honesty
Table 10: Example seeds for bootstrapping.
We conduct a manual evaluation to directly mea-
sure the quality of returned words. We recruited 1
annotator and developed annotation guidelines that
instructed each recruiter to judge whether learned
values are similar to query words or not. Overall the
annotator found almost all the learned words to be
similar to the query words. However, the algorithm
can not differentiate between different senses of the
word. For example, ?French? can be a language and
a nationality. Table 9 shows the top ranked words
with respect to query words.
5.2 Learning Concrete and Abstract Words
Our goal is to automatically learn concrete and ab-
stract words (Turney et al 2011). We apply boot-
strapping (Kozareva et al 2008) on the word graphs
by manually selecting 10 seeds for concrete and ab-
stract words (see Table 10). We use in-degree (sum
of weights of incoming edges) to compute the score
for each node which has connections with known
(seeds) or automatically labeled nodes, previously
exploited to learn hyponymy relations from the web
(Kozareva et al 2008). We learn concrete and ab-
stract words together (known as mutual exclusion
principle in bootstrapping (Thelen and Riloff, 2002;
McIntosh and Curran, 2008)), and each word is as-
signed to only one class. Moreover, after each it-
eration, we harmonically decrease the weight of the
in-degree associated with instances learned in later
iterations. We add 25 new instances at each itera-
tion and ran 100 iterations of bootstrapping, yielding
2506 concrete nouns and 2498 abstract nouns. To
evaluate our learned words, we searched in WordNet
whether they had ?abstraction? or ?physical? as their
hypernym. Out of 2506 learned concrete nouns,
Concrete: girl, person, bottles, wife, gentleman, mi-
crocomputer, neighbor, boy, foreigner, housewives,
texan, granny, bartender, tables, policeman, chubby,
mature, trees, mainframe, backbone, truck
Abstract: perseverance, tenacity, sincerity, profes-
sionalism, generosity, heroism, compassion, commit-
ment, openness, resentment, treachery, deception, no-
tion, jealousy, loathing, hurry, valour
Table 11: Learned concrete/abstract words.
1655 were found in WordNet. According to Word-
Net, 74% of those are concrete and 26% are ab-
stract. Out of 2498 learned abstract nouns, 942 were
found in WordNet. According to WordNet, 5% of
those are concrete and 95% are abstract. Table 11
shows the top ranked concrete and abstract words.
6 Conclusion
We proposed a system, FLAG which constructs
fast large-scale approximate graphs from large data
sets. To build this system we proposed a distributed
online-PMI algorithm that scaled up to 110 GB of
web data with 866 million sentences in less than 2
days using 100 quad-core nodes. Our both intrinsic
and extrinsic experiments demonstrated that online-
PMI algorithm not at all loses globally good con-
texts and perform comparable to offline-PMI. Next,
we proposed FAST-PLEB (a variant of PLEB) and
empirically demonstrated that it has recall compa-
rable to PLEB with 120 times faster pre-processing
time. Finally, we show the applicability of FLAG on
two applications: Google-Sets problem and learning
concrete and abstract words.
In future, we will apply FLAG to construct graphs
using several kinds of contexts like lexical, seman-
tic, syntactic and dependency relations or a combi-
nation of them. Moreover, we will apply graph theo-
retic models on graphs constructed using FLAG for
solving a large variety of NLP applications.
Acknowledgments
This work was partially supported by NSF Award
IIS-1139909. Thanks to Graham Cormode and
Suresh Venkatasubramanian for useful discussions
and the anonymous reviewers for many helpful com-
ments.
1078
References
Dimitris Achlioptas. 2003. Database-friendly random
projections: Johnson-lindenstrauss with binary coins.
J. Comput. Syst. Sci., 66(4):671?687.
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Pas?ca, and Aitor Soroa. 2009. A
study on similarity and relatedness using distributional
and wordnet-based approaches. In NAACL ?09: Pro-
ceedings of HLT-NAACL.
Moses Charikar, Kevin Chen, and Martin Farach-Colton.
2004. Finding frequent items in data streams. Theor.
Comput. Sci., 312:3?15, January.
Moses S. Charikar. 2002. Similarity estimation tech-
niques from rounding algorithms. In In Proc. of 34th
STOC, pages 380?388. ACM.
K. Church and P. Hanks. 1989. Word Associa-
tion Norms, Mutual Information and Lexicography.
In Proceedings of ACL, pages 76?83, Vancouver,
Canada, June.
Graham Cormode and Marios Hadjieleftheriou. 2008.
Finding frequent items in data streams. In VLDB.
Graham Cormode and S. Muthukrishnan. 2004. An im-
proved data stream summary: The count-min sketch
and its applications. J. Algorithms.
Dipanjan Das and Slav Petrov. 2011. Unsupervised
part-of-speech tagging with bilingual graph-based pro-
jections. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies, pages 600?609, Port-
land, Oregon, USA, June. Association for Computa-
tional Linguistics.
Cristian Estan and George Varghese. 2002. New di-
rections in traffic measurement and accounting. SIG-
COMM Comput. Commun. Rev., 32(4).
L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin,
Z. Solan, G. Wolfman, and E. Ruppin. 2002. Plac-
ing search in context: The concept revisited. In ACM
Transactions on Information Systems.
Zoubin Ghahramani and Katherine A. Heller. 2005.
Bayesian Sets. In in Advances in Neural Information
Processing Systems, volume 18.
Amit Goyal and Hal Daume? III. 2011. Approximate
scalable bounded space sketch for large data NLP. In
Empirical Methods in Natural Language Processing
(EMNLP).
Amit Goyal, Hal Daume? III, and Suresh Venkatasubra-
manian. 2009. Streaming for large scale NLP: Lan-
guage modeling. In NAACL.
Amit Goyal, Graham Cormode, and Hal Daume? III.
2012. Sketch algorithms for estimating point queries
in NLP. In Empirical Methods in Natural Language
Processing (EMNLP).
D. Graff. 2003. English Gigaword. Linguistic Data Con-
sortium, Philadelphia, PA, January.
Piotr Indyk and Rajeev Motwani. 1998. Approximate
nearest neighbors: towards removing the curse of di-
mensionality. In Proceedings of the thirtieth annual
ACM symposium on Theory of computing, STOC ?98,
pages 604?613. ACM.
Zornitsa Kozareva, Ellen Riloff, and Eduard Hovy. 2008.
Semantic class learning from the web with hyponym
pattern linkage graphs. In Proceedings of ACL-08:
HLT, pages 1048?1056, Columbus, Ohio, June. As-
sociation for Computational Linguistics.
Abby Levenberg, Chris Callison-Burch, and Miles Os-
borne. 2010. Stream-based translation models for
statistical machine translation. In Human Language
Technologies: The 2010 Annual Conference of the
North American Chapter of the Association for Com-
putational Linguistics, HLT ?10, pages 394?402. As-
sociation for Computational Linguistics.
Ping Li, Trevor J. Hastie, and Kenneth W. Church. 2006.
Very sparse random projections. In Proceedings of
the 12th ACM SIGKDD international conference on
Knowledge discovery and data mining, KDD ?06,
pages 287?296. ACM.
Ping Li, Kenneth Ward Church, and Trevor Hastie. 2008.
One sketch for all: Theory and application of condi-
tional random sampling. In Neural Information Pro-
cessing Systems, pages 953?960.
Tara McIntosh and James R Curran. 2008. Weighted
mutual exclusion bootstrapping for domain indepen-
dent lexicon and template acquisition. In Proceedings
of the Australasian Language Technology Association
Workshop 2008, pages 97?105, December.
G.A. Miller and W.G. Charles. 1991. Contextual corre-
lates of semantic similarity. Language and Cognitive
Processes, 6(1):1?28.
Patrick Pantel, Eric Crestan, Arkady Borkovsky, Ana-
Maria Popescu, and Vishnu Vyas. 2009. Web-scale
distributional similarity and entity set expansion. In
Proceedings of EMNLP.
Delip Rao and Deepak Ravichandran. 2009. Semi-
supervised polarity lexicon induction. In Proceedings
of the 12th Conference of the European Chapter of the
ACL (EACL 2009), pages 675?682, Athens, Greece,
March. Association for Computational Linguistics.
Deepak Ravichandran, Patrick Pantel, and Eduard Hovy.
2005. Randomized algorithms and nlp: using locality
sensitive hash function for high speed noun clustering.
In Proceedings of ACL.
H. Rubenstein and J.B. Goodenough. 1965. Contextual
correlates of synonymy. Computational Linguistics,
8:627?633.
Florin Rusu and Alin Dobra. 2007. Statistical analysis of
sketch estimators. In SIGMOD ?07. ACM.
1079
M. Thelen and E. Riloff. 2002. A Bootstrapping Method
for Learning Semantic Lexicons Using Extraction Pat-
tern Contexts. In Proceedings of the Empirical Meth-
ods in Natural Language Processing, pages 214?221.
Peter D. Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of seman-
tics. JOURNAL OF ARTIFICIAL INTELLIGENCE
RESEARCH, 37:141.
Peter Turney, Yair Neuman, Dan Assaf, and Yohai Co-
hen. 2011. Literal and metaphorical sense identifi-
cation through concrete and abstract context. In Pro-
ceedings of the 2011 Conference on Empirical Meth-
ods in Natural Language Processing, pages 680?690.
Association for Computational Linguistics.
Benjamin Van Durme and Ashwin Lall. 2009a. Prob-
abilistic counting with randomized storage. In IJ-
CAI?09: Proceedings of the 21st international jont
conference on Artifical intelligence.
Benjamin Van Durme and Ashwin Lall. 2009b. Stream-
ing pointwise mutual information. In Advances in
Neural Information Processing Systems 22.
Benjamin Van Durme and Ashwin Lall. 2010. Online
generation of locality sensitive hash signatures. In
Proceedings of the ACL 2010 Conference Short Pa-
pers, pages 231?235, July.
Benjamin Van Durme and Ashwin Lall. 2011. Efficient
online locality sensitive hashing via reservoir count-
ing. In Proceedings of the ACL 2011 Conference Short
Papers, June.
Leonid Velikovich, Sasha Blair-Goldensohn, Kerry Han-
nan, and Ryan McDonald. 2010. The viability of web-
derived polarity lexicons. In Human Language Tech-
nologies: The 2010 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 777?785, Los Angeles, Cal-
ifornia, June. Association for Computational Linguis-
tics.
Kilian Weinberger, Anirban Dasgupta, John Langford,
Alex Smola, and Josh Attenberg. 2009. Feature hash-
ing for large scale multitask learning. In Proceedings
of the 26th Annual International Conference on Ma-
chine Learning, ICML ?09, pages 1113?1120. ACM.
1080
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1093?1103, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Sketch Algorithms for Estimating Point Queries in NLP
Amit Goyal and Hal Daume? III
University of Maryland
{amit,hal}@umiacs.umd.edu
Graham Cormode
AT&T Labs?Research
graham@research.att.com
Abstract
Many NLP tasks rely on accurate statis-
tics from large corpora. Tracking com-
plete statistics is memory intensive, so recent
work has proposed using compact approx-
imate ?sketches? of frequency distributions.
We describe 10 sketch methods, including ex-
isting and novel variants. We compare and
study the errors (over-estimation and under-
estimation) made by the sketches. We evaluate
several sketches on three important NLP prob-
lems. Our experiments show that one sketch
performs best for all the three tasks.
1 Introduction
Since the emergence of the World Wide Web, so-
cial media and mobile devices, we have ever larger
and richer examples of text data. Such vast cor-
pora have led to leaps in the performance of many
language-based tasks: the concept is that simple
models trained on big data can outperform more
complex models with fewer examples. However,
this new view comes with its own challenges: prin-
cipally, how to effectively represent such large data
sets so that model parameters can be efficiently ex-
tracted? One answer is to adopt compact summaries
of corpora in the form of probabilistic ?sketches?.
In recent years, the field of Natural Language Pro-
cessing (NLP) has seen tremendous growth and in-
terest in the use of approximation, randomization,
and streaming techniques for large-scale problems
(Brants et al 2007; Turney, 2008). Much of this
work relies on tracking very many statistics. For ex-
ample, storing approximate counts (Talbot and Os-
borne, 2007; Van Durme and Lall, 2009a; Goyal
and Daume? III, 2011a), computing approximate as-
sociation scores like Pointwise Mutual Information
(Li et al 2008; Van Durme and Lall, 2009b; Goyal
and Daume? III, 2011a), finding frequent items (like
n-grams) (Goyal et al 2009), building streaming
language models (Talbot and Brants, 2008; Leven-
berg and Osborne, 2009), and distributional similar-
ity (Ravichandran et al 2005; Van Durme and Lall,
2010). All these problems ultimately depend on ap-
proximate counts of items (such as n-grams, word
pairs and word-context pairs). Thus we focus on
solving this central problem in the context of NLP
applications.
Sketch algorithms (Charikar et al 2004; Cor-
mode, 2011) are a memory- and time-efficient so-
lution to answering point queries. Recently in NLP,
we (Goyal and Daume? III, 2011a) demonstrated that
a version of the Count-Min sketch (Cormode and
Muthukrishnan, 2004) accurately solves three large-
scale NLP problems using small bounded memory
footprint. However, there are several other sketch al-
gorithms, and it is not clear why this instance should
be preferred amongst these. In this work, we con-
duct a systematic study and compare many sketch
techniques which answer point queries with focus
on large-scale NLP tasks. While sketches have been
evaluated within the database community for find-
ing frequent items (Cormode and Hadjieleftheriou,
2008) and join-size estimation (Rusu and Dobra,
2007), this is the first comparative study for NLP
problems.
Our work includes three contributions: (1)
We propose novel variants of existing sketches
by extending the idea of conservative update to
them. We propose Count sketch (Charikar et al
2004) with conservative update (COUNT-CU) and
Count-mean-min sketch with conservative update
1093
(CMM-CU). The motivation behind proposing new
sketches is inspired by the success of Count-Min
sketch with conservative update in our earlier work
(Goyal and Daume? III, 2011a). (2) We empirically
compare and study the errors in approximate counts
for several sketches. Errors can be over-estimation,
under-estimation, or a combination of the two. We
also evaluate their performance via Pointwise Mu-
tual Information and LogLikelihood Ratio. (3) We
use sketches to solve three important NLP problems.
Our experiments show that sketches can be very ef-
fective for these tasks, and that the best results are
obtained using the ?conservative update? technique.
Across all the three tasks, one sketch (CM-CU) per-
forms best.
2 Sketches
In this section, we review existing sketch algorithms
from the literature, and propose novel variants based
on the idea of conservative update (Estan and Vargh-
ese, 2002). The term ?sketch? refers to a class of
algorithm that represents a large data set with a
compact summary, typically much smaller than the
full size of the input. Given an input of N items
(x1, x2 . . . xN ), each item x (where x is drawn from
some domain U ) is mapped via hash functions into
a small sketch vector that records frequency infor-
mation. Thus, the sketch does not store the items
explicitly, but only information about the frequency
distribution. Sketches support fundamental queries
on their input such as point, range and inner product
queries to be quickly answered approximately. In
this paper, we focus on point queries, which ask for
the (approximate) count of a given item.
The algorithms we consider are randomized and
approximate. They have two user-chosen parame-
ters  and ?.  controls the amount of tolerable error
in the returned count and ? controls the probability
with which the error exceeds the bound . These
values of  and ? determine respectively the width
w and depth d of a two-dimensional array sk[?, ?] of
count information. The depth d denotes the num-
ber of hash functions employed by the sketch algo-
rithms.
Sketch Operations. Every sketch has two opera-
tions: UPDATE and QUERY to update and estimate
the count of an item. They all guarantee essentially
constant time operation (technically, this grows as
O(log(1? ) but in practice this is set to a constant)
per UPDATE and QUERY. Moreover, sketches can be
combined: given two sketches s1 and s2 computed
(using the same parameters w and d, and same set
of d hash functions) over different inputs, a sketch
of the combined input is obtained by adding the in-
dividual sketches, entry-wise. The time to perform
the COMBINE operation on sketches is O(d ? w),
independent of the data size. This property enables
sketches to be implemented in distributed setting,
where each machine computes the sketch over a
small portion of the corpus and makes it scalable
to large datasets.
2.1 Existing sketch algorithms
This section describes sketches from the literature:
Count-Min sketch (CM): The CM (Cormode and
Muthukrishnan, 2004) sketch has been used effec-
tively for many large scale problems across sev-
eral areas, such as Security (Schechter et al 2010),
Machine Learning (Shi et al 2009; Aggarwal
and Yu, 2010), Privacy (Dwork et al 2010), and
NLP (Goyal and Daume? III, 2011a). The sketch
stores an array of size d ? w counters, along with d
hash functions (drawn from a pairwise-independent
family), one for each row of the array. Given an in-
put ofN items (x1, x2 . . . xN ), each of the hash func-
tions hk:U ? {1 . . . w}, ?1 ? k ? d, takes an item
from the input and maps it into a counter indexed by
the corresponding hash function.
UPDATE: For each new item ?x? with count c, the
sketch is updated as:
sk[k, hk(x)]? sk[k, hk(x)] + c, ?1 ? k ? d.
QUERY: Since multiple items are hashed to the
same index for each array row, the stored frequency
in each row is guaranteed to overestimate the true
count, making it a biased estimator. Therefore, to
answer the point query (QUERY (x)), CM returns the
minimum over all the d positions x is stored.
c?(x) = mink sk[k, hk(x)], ?1 ? k ? d.
Setting w=2 and d=log(
1
? ) ensures all reported
frequencies by CM exceed the true frequencies by
at most N with probability of at least 1 ? ?. This
makes the space used by the algorithm O(1 log
1
? ).
Spectral Bloom Filters (SBF): Cohen and Matias
(2003) proposed SBF, an extension to Bloom
Filters (Bloom, 1970) to answer point queries. The
1094
UPDATE and QUERY procedures for SBF are the
same as Count-Min (CM) sketch, except that the
range of all the hash functions for SBF are the full
array: hk:U ? {1 . . . w ? d},?1 ? k ? d. While
CM and SBF are very similar, only CM provides
guarantees on the query error.
Count-mean-min (CMM): The motivation behind
the CMM (Deng and Rafiei, 2007) sketch is to
provide an unbiased estimator for Count-Min
(CM) sketch. The construction of CMM sketch
is identical to the CM sketch, while the QUERY
procedure differs. Instead of returning the minimum
value over the d counters (indexed by d hash
functions), CMM deducts the value of estimated
noise from each of the d counters, and return the
median of the d residues. The noise is estimated
as (N ? sk[k, hk(x)])/(w ? 1). Nevertheless,
the median estimate (f?1) over the d residues can
overestimate more than the original CM sketch min
estimate (f?2), so we return min (f?1,f?2) as the final
estimate for CMM sketch. CMM gives the same
theoretical guarantees as Count sketch (below).
Count sketch (COUNT) (Charikar et al 2004):
COUNT (aka Fast-AGMS) keeps two hash
functions for each row, hk maps items onto
[1, w], and gk maps items onto {?1,+1}. UP-
DATE: For each new item ?x? with count c:
sk[k, hk(x)]? sk[k, hk(x)] + c ? gk(x), ?1 ? k ? d.
QUERY: the median over the d rows is an unbiased
estimator of the point query:
?c(x) = mediank sk[k, hk(x)] ? gk(x), ?1 ? k ? d.
Setting w= 22 and d=log(
4
? ) ensures that all re-
ported frequencies have error at most (
?N
i=1 f
2
i )
1/2
? N with probability at least 1??. The space used
by the algorithm is O( 12 log
1
? ).
2.2 Conservative Update sketch algorithms
In this section, we propose novel variants of existing
sketches (see Section 2) by combining them with the
conservative update process (Estan and Varghese,
2002). The idea of conservative update (also known
as Minimal Increase (Cohen and Matias, 2003)) is to
only increase counts in the sketch by the minimum
amount needed to ensure the estimate remains accu-
rate. It can easily be applied to Count-Min (CM)
sketch and Spectral Bloom Filters (SBF) to further
improve the estimate of a point query. Goyal and
Daume? III (2011a) showed that CM sketch with
conservative update reduces the amount of over-
estimation error by a factor of at least 1.5, and also
improves performance on three NLP tasks.
Note that while conservative update for CM and
SBF never increases the error, there is no guaranteed
improvement. The method relies on seeing multiple
updates in sequence. When a large corpus is being
summarized in a distributed setting, we can apply
conservative update on each sketch independently
before combining the sketches together (see ?Sketch
Operations? in Section 2).
Count-Min sketch with conservative update
(CM-CU): The QUERY procedure for CM-CU
(Cormode, 2009; Goyal and Daume? III, 2011a) is
identical to Count-Min. However, to UPDATE an
item ?x? with frequency c, we first compute the fre-
quency c?(x) of this item from the existing data struc-
ture (?1 ? k ? d, c?(x) = mink sk[k, hk(x)]) and
the counts are updated according to:
sk[k, hk(x)]? max{sk[k, hk(x)], c?(x) + c} (?).
The intuition is that, since the point query returns
the minimum of all the d values, we update a
counter only if it is necessary as indicated by (?).
This heuristic avoids unnecessarily updating counter
values to reduce the over-estimation error.
Spectral Bloom Filters with conservative update
(SBF-CU): The QUERY procedure for SBF-CU
(Cohen and Matias, 2003) is identical to SBF.
SBF-CU UPDATE procedure is similar to CM-CU,
with the difference that all d hash functions have the
common range d? w.
Count-mean-min with conservative update
(CMM-CU): We propose a new variant to reduce
the over-estimation error for CMM sketch. The
construction of CMM-CU is identical to CM-CU.
However, due to conservative update, each row of
the sketch is not updated for every update, hence
the sum of counts over each row (
?
i sk[k, i],
?1 ? k ? d) is not equal to input size N .
Hence, the estimated noise to be subtracted here is
(
?
i sk[k, i]? sk[k, hk(x)]) / (w ? 1). CMM-CU
deducts the value of estimated noise from each of
the d counters, and returns the median of the d
residues as the point query.
Count sketch with conservative update (COUNT-
CU): We propose a new variant to reduce over-
estimation error for the COUNT sketch. The
QUERY procedure for COUNT-CU is the same as
1095
COUNT. The UPDATE procedure follows the same
outline as CM-CU, but uses the current estimate
c?(x) from the COUNT sketch, i.e.
c?(x) = mediank sk[k, hk(x)] ? gk(x), ?1 ? k ? d.
Note, this heuristic is not as strong as for CM-CU
and SBF-CU because COUNT can have both over-
estimate and under-estimate errors.
Lossy counting with conservative update (LCU-
WS): LCU-WS (Goyal and Daume? III, 2011b) was
proposed to reduce the amount of over-estimation
error for CM-CU sketch, without incurring too
much under-estimation error. This scheme is in-
spired by lossy counting (Manku and Motwani,
2002). In this approach, the input sequence is con-
ceptually divided into windows, each containing 1/?
items. The size of each window is equal to size of
the sketch i.e. d ? w. Note that there are ?N win-
dows; let t denote the index of current window. At
window boundaries, ? 1 ? i ? d, 1 ? j ? w,
if (sk[i, j] > 0 and sk[i, j] ? t), then sk[i, j] ?
sk[i, j]?1. The idea is to remove the contribution of
small items colliding in the same entry, while not al-
tering the count of frequent items. The current win-
dow index is used to draw this distinction. Here, all
reported frequencies f? have both under and over es-
timation error: f ? ?N ? f? ? f + N .
Lossy counting with conservative update II
(LCU-SWS): This is a variant of the previous
scheme, where the counts of the sketch are de-
creased more conservatively. Hence, this scheme
has worse over-estimation error compared to LCU-
WS, with better under-estimation. Here, only those
counts are decremented which are at most the square
root of current window index, t. At window bound-
aries, ? 1 ? i ? d, 1 ? j ? w, if (sk[i, j] > 0 and
sk[i, j] ? d
?
te), then sk[i, j]? sk[i, j]? 1. LCU-
SWS has similar analytical bounds to LCU-WS.
3 Intrinsic Evaluations
We empirically compare and study the errors in ap-
proximate counts for all 10 sketches. Errors can be
over-estimation, under-estimation, or a combination
of the two. We also study the behavior of approxi-
mate Pointwise Mutual Information and Log Likeli-
hood Ratio for the sketches.
3.1 Experimental Setup
DATA: We took 50 million random sentences from
Gigaword (Graff, 2003). We split this data in
10 chunks of 5 million sentences each. Since all
sketches have probabilistic bounds, we report aver-
age results over these 10 chunks. For each chunk,
we generate counts of all word pairs within a win-
dow size 7. This results in an average stream size of
194 million word pair tokens and 33.5 million word
pair types per chunk.
To compare error in various sketch counts, first we
compute the exact counts of all the word pairs. Sec-
ond, we store the counts of all the word pairs in all
the sketches. Third, we query sketches to generate
approximate counts of all the word pairs. Recall, we
do not store the word pairs explicitly in sketches but
only a compact summary of the associated counts.
We fix the size of each sketch to be w = 20?10
6
3
and d = 3. We keep the size of sketches equal
to allow fair comparison among them. Prior work
(Deng and Rafiei, 2007; Goyal and Daume? III,
2011a) showed with fixed sketch size, a small num-
ber of hash functions (d=number of hash functions)
with large w (or range) give rise to small error over
counts. Next, we group all word pairs with the
same true frequency into a single bucket. We then
compute the Mean Relative Error (MRE) in each of
these buckets. Because different sketches have dif-
ferent accuracy behavior on low, mid, and high fre-
quency counts, making this distinction based on fre-
quency lets us determine the regions in which dif-
ferent sketches perform best. Mean Relative Error
(MRE) is defined as the average of absolute differ-
ence between the predicted and the exact value di-
vided by the exact value over all the word pairs in
each bucket.
3.2 Studying the Error in Counts
We study the errors produced by all 10 sketches.
Since various sketches result in different errors on
low, mid, and high frequency counts, we plot the re-
sults with a linear error scale (Fig. 1(a)) to highlight
the performance for low frequency counts, and with
a log error scale (Fig. 1(b)) for mid and high fre-
quency counts.
We make several observations on low frequency
counts from Fig. 1(a). (1) Count-Min (CM) and
1096
100 101 1020
2
4
6
8
True frequency counts of word pairs (log scale)
Mea
n R
elat
ive 
Erro
r
 
 CMCM?CUSBFSBF?CUCOUNTCOUNT?CUCMMCMM?CULCU?SWSLCU?WS
(a) Focusing on low frequency counts
100 102 104
10?4
10?2
100
True frequency counts of word pairs (log scale)
Mea
n R
elat
ive 
Erro
r (log
 scal
e)
 
 
CMCM?CUCOUNTCOUNT?CUCMMCMM?CULCU?SWSLCU?WS
(b) Focusing on mid and high frequency counts
Figure 1: Comparing several sketches for input size of 75 million word pairs. Size of each sketch: w = 20?10
6
3 and d = 3. All
items with same exact count are put in one bucket and we plot Mean Relative Error on the y-axis with exact counts on the x-axis.
Spectral Bloom Filters (SBF) have identical MRE
for word pairs. Using conservative update with CM
(CM-CU) and SBF (SBF-CU) reduces the MRE
by a factor of 1.5. MRE for CM-CU and SBF-
CU is also identical. (2) COUNT has better MRE
than CM-CU and using conservative update with
COUNT (COUNT-CU) further reduces the MRE.
(3) CMM has better MRE than COUNT and using
conservative update with CMM (CMM-CU) fur-
ther reduces the MRE. (4) Lossy counting with con-
servative update variants (LCU-SWS, LCU-WS)
have comparable MRE to COUNT-CU and CMM-
CU respectively.
In Fig. 1(b), we do not plot the SBF variants
as SBF and CM variants had identical MRE in
Fig. 1(a). From Figure 1(b), we observe that,
CM, COUNT, COUNT-CU, CMM, CMM-CU
sketches have worse MRE than CM-CU, LCU-
SWS, and LCU-WS for mid and high frequency
counts. CM-CU, LCU-SWS, and LCU-WS have
zero MRE for all the counts > 1000.
To summarize the above observations, for those
NLP problems where we cannot afford to make
errors on mid and high frequency counts, we
should employ CM-CU, LCU-SWS, and LCU-
WS sketches. If we want to reduce the error on
low frequency counts, LCU-WS generates least er-
ror. For NLP tasks where we can allow error on mid
and high frequency counts but not on low frequency
100 101 102 103?1
?0.5
0
0.5
1
1.5
2
True frequency counts of word pairs (log scale)
Mea
n R
elat
ive 
Erro
r
 
 COUNT?CU?OECOUNT?CU?UECMM?CU?OECMM?CU?UELCU?SWS?OELCU?SWS?UELCU?WS?OELCU?WS?UE
Figure 2: Compare several sketches on over-estimation and
under-estimation errors with respect to exact counts.
counts, CMM-CU sketch is best.
3.3 Examining OE and UE errors
In many NLP applications, we are willing to tolerate
either over-estimation or under-estimation errors.
Hence we breakdown the error into over-estimation
(OE) and under-estimation (UE) errors for the six
best-performing sketches (COUNT, COUNT-CU,
CMM, CMM-CU, LCU-SWS, and LCU-WS). To
accomplish that, rather than using absolute error val-
ues, we divide the values into over-estimation (pos-
itive), and under-estimation (negative) error buck-
1097
101 102 103 104 1050
0.2
0.4
0.6
0.8
1
Top?K
Rec
all
 
 
CM?CUCOUNT?CUCMM?CULCU?SWSLCU?WS
(a) PMI
101 102 103 104 1050.5
0.6
0.7
0.8
0.9
1
Top?K
Rec
all
 
 
CM?CUCOUNT?CUCMM?CULCU?SWSLCU?WS
(b) LLR
Figure 3: Evaluate the approximate PMI and LLR rankings (obtained using various sketches) with the exact rankings.
ets. Hence, to compute the over-estimation MRE,
we take the average of positive values over all the
items in each bucket. For under-estimation, we
take the average over the negative values. We
can make several interesting observations from Fig-
ure 2: (1) Comparing COUNT-CU and LCU-
SWS, we learn that both have the same over-
estimation errors. However, LCU-SWS has less
under-estimation error than COUNT-CU. There-
fore, LCU-SWS is always better than COUNT-
CU. (2) LCU-WS has less over-estimation than
LCU-SWS but with more under-estimation error on
mid frequency counts. LCU-WS has less under-
estimation error than COUNT-CU. (3) CMM-CU
has the least over-estimation error and most under-
estimation error among all the compared sketches.
From the above experiments, we conclude that
tasks sensitive to under-estimation should use the
CM-CU sketch, which guarantees over-estimation.
However, if we are willing to make some under-
estimation error with less over-estimation error,
then LCU-WS and LCU-SWS are recommended.
Lastly, to have minimal over-estimation error with
willingness to accept large under-estimation error,
CMM-CU is recommended.
3.4 Evaluating association scores ranking
Last, in many NLP problems, we are interested in as-
sociation rankings obtained using Pointwise Mutual
Information (PMI) and Log Likelihood Ratio (LLR).
In this experiment, we compare the word pairs asso-
ciation rankings obtained using PMI and LLR from
several sketches and exact word pair counts. We use
recall to measure the number of top-K sorted word
pairs that are found in both the rankings.
In Figure 3(a), we compute the recall for CM-
CU, COUNT-CU, CMM-CU, LCU-SWS, and
LCU-WS sketches at several top-K thresholds of
word pairs for approximate PMI ranking. We
can make several observations from Figure 3(a).
COUNT-CU has the worst recall for almost all the
top-K settings. For top-K values less than 750, all
sketches except COUNT-CU have comparable re-
call. Meanwhile, for K greater than 750, LCU-WS
has the best recall. The is because PMI is sensitive
to low frequency counts (Church and Hanks, 1989),
over-estimation of the counts of low frequency word
pairs can make their approximate PMI scores worse.
In Figure 3(b), we compare the LLR rankings. For
top-K values less than 1000, all the sketches have
comparable recall. For top-K values greater than
1000, CM-CU, LCU-SWS, and LCU-WS perform
better. The reason for such a behavior is due to LLR
favoring high frequency word pairs, and COUNT-
CU and CMM-CU making under-estimation error
on high frequency word pairs.
To summarize, to maintain top-K PMI rank-
ings making over-estimation error is not desirable.
Hence, LCU-WS is recommended for PMI rank-
ings. For LLR, producing under-estimation error is
not preferable and therefore, CM-CU, LCU-WS,
and LCU-SWS are recommended.
1098
Test Set Random Buckets Neighbor
Model CM-CU CMM-CU LCU-WS CM-CU CMM-CU LCU-WS CM-CU CMM-CU LCU-WS
50M 87.2 74.3 86.5 83.9 72.9 83.2 71.7 64.7 72.1
100M 90.4 79.0 91.0 86.5 76.9 86.9 73.4 67.2 74.7
200M 93.3 83.1 92.9 88.3 80.1 88.4 75.0 69.0 75.4
500M 94.4 86.6 94.1 89.3 83.4 89.3 75.7 70.8 75.5
1B 94.4 88.7 94.4 89.5 85.1 89.5 75.8 71.9 75.8
Exact 94.5 89.5 75.8
Table 1: Pseudo-words evaluation on accuracy metric for selectional preferences using several sketches of different sizes against
the exact. There is no statistically significant difference (at p < 0.05 using bootstrap resampling) among bolded numbers.
4 Extrinsic Evaluation
4.1 Experimental Setup
We study three important NLP applications, and
compare the three best-performing sketches: Count-
Min sketch with conservative update (CM-CU),
Count-mean-min with conservative update (CMM-
CU), and Lossy counting with conservative update
(LCU-WS). The above mentioned 3 sketches are se-
lected from 10 sketches (see Section 2) considering
these sketches make errors on different ranges of the
counts: low, mid and, high frequency counts as seen
in our intrinsic evaluations in Section 3. The goal
of this experiment is to show the effectiveness of
sketches on large-scale language processing tasks.
These adhere to the premise that simple methods
using large data can dominate more complex mod-
els. We purposefully select simple methods as they
use approximate counts and associations directly to
solve these tasks. This allows us to have a fair com-
parison among different sketches, and to more di-
rectly see the impact of different choices of sketch
on the task outcome. Of course, sketches are still
broadly applicable to many NLP problems where we
want to count (many) items or compute associations:
e.g. language models, Statistical Machine Transla-
tion, paraphrasing, bootstrapping and label propaga-
tion for automatically creating a knowledge base and
finding interesting patterns in social media.
Data: We use Gigaword (Graff, 2003) and a 50%
portion of a copy of news web (GWB50) crawled
by (Ravichandran et al 2005). The raw size of
Gigaword (GW) and GWB50 is 9.8 GB and 49
GB with 56.78 million and 462.60 sentences respec-
tively. For both the corpora, we split the text into
sentences, tokenize and convert into lower-case.
4.2 Pseudo-Words Evaluation
In NLP, it is difficult and time consuming to create
annotated test sets. This problem has motivated the
use of pseudo-words to automatically create the test
sets without human annotation. The pseudo-words
are a common way to evaluate selectional prefer-
ences models (Erk, 2007; Bergsma et al 2008) that
measure the strength of association between a predi-
cate and its argument filler, e.g., that the noun ?song?
is likely to be the object of the verb ?sing?.
A pseudo-word is the conflation of two words
(e.g. song/dance). One word is the original in a sen-
tence, and the second is the confounder. For exam-
ple, in our task of selectional preferences, the system
has to decide for the verb ?sing? which is the correct
object between ?song?/?dance?. Recently, Cham-
bers and Jurafsky (2010) proposed a simple baseline
based on co-occurrence counts of words, which has
state-of-the-art performance on pseudo-words eval-
uation for selectional preferences.
We use a simple approach (without any typed de-
pendency data) similar to Chambers and Jurafsky
(2010), where we count all word pairs (except word
pairs involving stop words) that appear within a win-
dow of size 3 from Gigaword (9.8 GB). That gen-
erates 970 million word pair tokens (stream size)
and 94 million word pair types. Counts of all the
94 million unique word pairs are stored in CM-
CU, CMM-CU, and LCU-WS. For a target verb,
we return that noun which has higher co-occurrence
count with it, as the correct selectional preference.
We evaluate on Chambers and Jurafsky?s three test
sets1 (excluding instances involving stop words) that
are based on different strategies in selecting con-
founders: Random (4081 instances), Buckets (4028
1http://www.usna.edu/Users/cs/nchamber/
data/pseudowords/
1099
100 102 104 1060.4
0.6
0.8
1
Cum
ulativ
e Pro
portio
n
True Frequency of word pairs (log?scale) 
 
Pseudo test setsSO test set
Figure 4: Determining the proportion of low, mid and high
frequency test word pairs in Gigaword (GW).
instances), and Neighbor (3881 instances). To eval-
uate against the exact counts, we compute exact
counts for only those word pairs that are present in
the test sets. Accuracy is used for evaluation and
is defined as the percentage of number of correctly
identified pseudo words.
In Fig. 4, we plot the cumulative proportion of
true frequency counts of all word pairs (from the
three tests) in Gigaword (GW). To include unseen
word pairs from test set in GW on log-scale in Fig.
4, we increment the true counts of all the word pairs
by 1. This plot demonstrates that 45% of word-
pairs are unseen in GW, and 67% of word pairs have
counts less than 10. Hence, to perform better on this
task, it is essential to accurately maintain counts of
rare word pairs.
In Table 1, we vary the size of all sketches (50
million (M ), 100M , 200M , 500M and 1 billion
(1B) counters) with 3 hash functions to compare
them against the exact counts. It takes 1.8 GB un-
compressed space to maintain the exact counts on
the disk. Table 1 shows that with sketches of size
> 200M on all the three test sets, CM-CU and
LCU-WS are comparable to exact. However, the
CMM-CU sketch performs less well. We conjec-
ture the reason for such a behavior is due to loss of
recall (information about low frequency word pairs)
by under-estimation error. For this task CM-CU and
LCU-WS scales to storing 94M unique word pairs
using 200M integer (4 bytes each) counters (using
800 MB) < 1.8 GB to maintain exact counts. More-
over, these results are comparable to Chambers and
Jurafsky?s state-of-the-art framework.
Data Exact CM-CU CMM-CU LCU-WS
GW 74.2 74.0 65.3 72.9
GWB50 81.2 80.9 74.9 78.3
Table 2: Evaluating Semantic Orientation on accuracy metric
using several sketches of 2 billion counters against exact. Bold
and italic numbers denote no statistically significant difference.
4.3 Finding Semantic Orientation of a word
Given a word, the task of finding its Semantic Ori-
entation (SO) (Turney and Littman, 2003) is to de-
termine if the word is more probable to be used in
positive or negative connotation. We use Turney and
Littman?s (2003) state-of-the-art framework to com-
pute the SO of a word. We use same seven pos-
itive words (good, nice, excellent, positive, fortu-
nate, correct, and superior) and same seven nega-
tive words (bad, nasty, poor, negative, unfortunate,
wrong, and inferior) from their framework as seeds.
The SO of a given word is computed based on the
strength of its association with the seven positive
words and the seven negative words. Association
scores are computed via Pointwise Mutual Informa-
tion (PMI). We compute the SO of a word ?w? as:
SO(W) =
?
p?Pos PMI(p,w)?
?
n?Neg PMI(n,w)
where, Pos and Neg denote the seven positive and
negative seeds respectively. If this score is negative,
we predict the word as negative; otherwise, we pre-
dict it as positive. We use the General Inquirer lex-
icon2 (Stone et al 1966) as a benchmark to eval-
uate the semantic orientation similar to Turney and
Littman?s (2003) work. Our test set consists of 1611
positive and 1987 negative words. Accuracy is used
for evaluation and is defined as the percentage of
number of correctly identified SO words.
We evaluate SO of words on two different
sized corpora (see Section 4.1): Gigaword (GW)
(9.8GB), and GW with 50% news web corpus
(GWB50) (49GB). We fix the size of all sketches
to 2 billion (2B) counters with 5 hash functions. We
store exact counts of all words in a hash table for
both GW and GWB50. We count all word pairs
(except word pairs involving stop words) that appear
within a window of size 7 from GW and GWB50.
This yields 2.67 billion(B) tokens and .19B types
2The General Inquirer lexicon is freely available at http:
//www.wjh.harvard.edu/?inquirer/
1100
Test Set WS-203 MC-30
Model C
M
-C
U
C
M
M
-C
U
L
C
U
-W
S
C
M
-C
U
C
M
M
-C
U
L
C
U
-W
S
P
M
I
10M .58 .25 .28 .67 .20 .16
50M .44 .23 .41 .61 .22 .31
200M .53 .44 .47 .57 .28 .43
Exact .52 .50
L
L
R
10M .47 .27 .29 .50 .29 .10
50M .42 .31 .34 .48 .32 .35
200M .41 .35 .39 .40 .31 .40
Exact .42 .41
Table 3: Evaluating distributional similarity using sketches.
Scores are evaluated using rank correlation ?. Bold and italic
numbers denote no statistically significant difference.
from GW and 13.20B tokens and 0.8B types from
GWB50. Next, we compare the sketches against the
exact counts over two different size corpora.
Table 2 shows that increasing the amount of data
improves the accuracy of identifying the SO of a
word. We get an absolute increase of 7 percentage
points (with exact counts) in accuracy (The 95% sta-
tistical significance boundary for accuracy is about
? 1.5.), when we add 50% web data (GWB50).
CM-CU results are equivalent to exact counts for all
the corpus sizes. These results are also comparable
to Turney?s (2003) accuracy of 82.84%. However,
CMM-CU results are worse by absolute 8.7 points
and 6 points on GW and GWB50 respectively with
respect to CM-CU. LCU-WS is better than CMM-
CU but worse than CM-CU. Using 2B integer (4
bytes each) counters (bounded memory footprint of
8 GB), CM-CU scales to 0.8B word pair types (It
takes 16 GB uncompressed disk space to store exact
counts of all the unique word pair types.).
Figure 4 has similar frequency distribution of
word pairs3 in SO test set as pseudo-words evalu-
ation test sets word pairs. Hence, CMM-CU again
has substantially worse results than CM-CU due to
loss of recall (information about low frequency word
pairs) by under-estimation error. We can conclude
that for this task CM-CU is best.
4.4 Distributional Similarity
Distributional similarity is based on the distribu-
tional hypothesis that similar terms appear in simi-
3Consider only those pairs in which one word appears in the
seed list and the other word appears in the test set.
lar contexts (Firth, 1968; Harris, 1954). The context
vector for each term is represented by the strength
of association between the term and each of the lex-
ical, semantic, syntactic, and/or dependency units
that co-occur with it. For this work, we define con-
text for a given term as the surrounding words ap-
pearing in a window of 2 words to the left and 2
words to the right. The context words are concate-
nated along with their positions -2, -1, +1, and +2.
We use PMI and LLR to compute the association
score (AS) between the term and each of the context
to generate the context vector. We use the cosine
similarity measure to find the distributional similar-
ity between the context vectors for each of the terms.
We use two test sets which consist of word pairs,
and their corresponding human rankings. We gener-
ate the word pair rankings using distributional sim-
ilarity. We report the Spearman?s rank correlation
coefficient (?) between the human and distributional
similarity rankings. We report results on two test
sets: WS-203: A set of 203 word pairs marked ac-
cording to similarity (Agirre et al 2009). MC-30:
A set of 30 noun pairs (Miller and Charles, 1991).
We evaluate distributional similarity on Giga-
word (GW) (9.8GB) (see Section 4.1). First, we
store exact counts of all words and contexts in a hash
table from GW. Next, we count all the word-context
pairs and store them in CM-CU, CMM-CU, and
LCU-WS sketches. That generates a stream of
size 3.35 billion (3.35B) word-context pair tokens
and 215 million unique word-context pair types (It
takes 4.6 GB uncompressed disk space to store exact
counts of all these unique word-context pair types.).
For every target word in the test set, we maintain
top-1000 approximate AS scores contexts using a
priority queue, by passing over the corpus a second
time. Finally, we use cosine similarity with these
approximate top-K context vectors to compute dis-
tributional similarity.
In Table 3, we vary the size of all sketches across
10 million (M ), 50M , and 200M counters with 3
hash functions. The results using PMI shows that
CM-CU has best ? on both WS-203 and MC-30
test sets. The results for LLR in Table 3 show simi-
lar trends with CM-CU having best results on small
size sketches. Thus, CM-CU scales using 10M
counters (using fixed memory of 40 MB versus 4.6
GB to store exact counts). These results are compa-
1101
rable against the state-of-the-art results for distribu-
tional similarity (Agirre et al 2009).
On this task CM-CU is best as it avoids loss
of recall (information about low frequency word
pairs) due to under-estimation error. For a target
word that has low frequency, using CMM-CU will
not generate any contexts for it, as it will have
large under-estimation error for word-context pairs
counts. This phenomenon is demonstrated in Ta-
ble 3, where CMM-CU and LCU-WS have worse
result with small size sketches.
5 Conclusion
In this work, we systematically studied the problem
of estimating point queries using different sketch al-
gorithms. As far as we know, this represents the
first comparative study to demonstrate the relative
behavior of sketches in the context of NLP applica-
tions. We proposed two novel sketch variants: Count
sketch (Charikar et al 2004) with conservative up-
date (COUNT-CU) and Count-mean-min sketch
with conservative update (CMM-CU). We empiri-
cally showed that CMM-CU has under-estimation
error with small over-estimation error, CM-CU has
only over-estimation error, and LCU-WS has more
under-estimation error than over-estimation error.
Finally, we demonstrated CM-CU has better re-
sults on all three tasks: pseudo-words evaluation
for selectional preferences, finding semantic orien-
tation task, and distributional similarity. This shows
that maintaining information about low frequency
items (even with over-estimation error) is better than
throwing away information (under-estimation error)
about rare items.
Future work is to reduce the bit size of each
counter (instead of the number of counters), as has
been tried for other summaries (Talbot and Osborne,
2007; Talbot, 2009; Van Durme and Lall, 2009a) in
NLP. However, it may be challenging to combine
this with conservative update.
Acknowledgments
This work was partially supported by NSF Award
IIS-1139909. Thanks to Suresh Venkatasubrama-
nian for useful discussions and the anonymous re-
viewers for many helpful comments.
References
Charu C. Aggarwal and Philip S. Yu. 2010. On classi-
fication of high-cardinality data streams. In SDM?10,
pages 802?813.
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Pas?ca, and Aitor Soroa. 2009. A
study on similarity and relatedness using distributional
and wordnet-based approaches. In NAACL ?09: Pro-
ceedings of HLT-NAACL.
Shane Bergsma, Dekang Lin, and Randy Goebel. 2008.
Discriminative learning of selectional preference from
unlabeled text. In Proc. EMNLP, pages 59?68, Hon-
olulu, Hawaii, October.
Burton H. Bloom. 1970. Space/time trade-offs in hash
coding with allowable errors. Communications of the
ACM, 13:422?426.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Large language mod-
els in machine translation. In Proceedings of EMNLP-
CoNLL.
Nathanael Chambers and Dan Jurafsky. 2010. Improv-
ing the use of pseudo-words for evaluating selectional
preferences. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguistics,
ACL ?10, pages 445?453. Association for Computa-
tional Linguistics.
Moses Charikar, Kevin Chen, and Martin Farach-Colton.
2004. Finding frequent items in data streams. Theor.
Comput. Sci., 312:3?15, January.
K. Church and P. Hanks. 1989. Word Associa-
tion Norms, Mutual Information and Lexicography.
In Proceedings of ACL, pages 76?83, Vancouver,
Canada, June.
Saar Cohen and Yossi Matias. 2003. Spectral bloom fil-
ters. In Proceedings of the 2003 ACM SIGMOD in-
ternational conference on Management of data, SIG-
MOD ?03, pages 241?252. ACM.
Graham Cormode and Marios Hadjieleftheriou. 2008.
Finding frequent items in data streams. In VLDB.
Graham Cormode and S. Muthukrishnan. 2004. An im-
proved data stream summary: The count-min sketch
and its applications. J. Algorithms.
Graham Cormode. 2009. Encyclopedia entry on ?Count-
Min Sketch?. In Encyclopedia of Database Systems,
pages 511?516. Springer.
Graham Cormode. 2011. Sketch techniques for approx-
imate query processing. Foundations and Trends in
Databases. NOW publishers.
Fan Deng and Davood Rafiei. 2007. New estimation al-
gorithms for streaming data: Count-min can do more.
http://webdocs.cs.ualberta.ca/.
Cynthia Dwork, Moni Naor, Toniann Pitassi, Guy N.
Rothblum, and Sergey Yekhanin. 2010. Pan-private
streaming algorithms. In Proceedings of ICS.
1102
Katrin Erk. 2007. A simple, similarity-based model for
selectional preferences. In Proceedings of the 45th An-
nual Meeting of the Association of Computational Lin-
guistics, volume 45, pages 216?223. Association for
Computational Linguistics.
Cristian Estan and George Varghese. 2002. New di-
rections in traffic measurement and accounting. SIG-
COMM Comput. Commun. Rev., 32(4).
J. Firth. 1968. A synopsis of linguistic theory 1930-
1955. In F. Palmer, editor, Selected Papers of J. R.
Firth. Longman.
Amit Goyal and Hal Daume? III. 2011a. Approximate
scalable bounded space sketch for large data NLP. In
Empirical Methods in Natural Language Processing
(EMNLP).
Amit Goyal and Hal Daume? III. 2011b. Lossy con-
servative update (LCU) sketch: Succinct approximate
count storage. In Conference on Artificial Intelligence
(AAAI).
Amit Goyal, Hal Daume? III, and Suresh Venkatasubra-
manian. 2009. Streaming for large scale NLP: Lan-
guage modeling. In NAACL.
D. Graff. 2003. English Gigaword. Linguistic Data Con-
sortium, Philadelphia, PA, January.
Z. Harris. 1954. Distributional structure. Word 10 (23),
pages 146?162.
Abby Levenberg and Miles Osborne. 2009. Stream-
based randomised language models for SMT. In
EMNLP, August.
Ping Li, Kenneth Ward Church, and Trevor Hastie. 2008.
One sketch for all: Theory and application of condi-
tional random sampling. In Neural Information Pro-
cessing Systems, pages 953?960.
G. S. Manku and R. Motwani. 2002. Approximate fre-
quency counts over data streams. In VLDB.
G.A. Miller and W.G. Charles. 1991. Contextual corre-
lates of semantic similarity. Language and Cognitive
Processes, 6(1):1?28.
Deepak Ravichandran, Patrick Pantel, and Eduard Hovy.
2005. Randomized algorithms and nlp: using locality
sensitive hash function for high speed noun clustering.
In Proceedings of ACL.
Florin Rusu and Alin Dobra. 2007. Statistical analysis of
sketch estimators. In SIGMOD ?07. ACM.
Stuart Schechter, Cormac Herley, and Michael Mitzen-
macher. 2010. Popularity is everything: a new
approach to protecting passwords from statistical-
guessing attacks. In Proceedings of the 5th USENIX
conference on Hot topics in security, HotSec?10, pages
1?8, Berkeley, CA, USA. USENIX Association.
Qinfeng Shi, James Petterson, Gideon Dror, John Lang-
ford, Alex Smola, and S.V.N. Vishwanathan. 2009.
Hash kernels for structured data. Journal Machine
Learning Research, 10:2615?2637, December.
Philip J. Stone, Dexter C. Dunphy, Marshall S. Smith,
and Daniel M. Ogilvie. 1966. The General Inquirer:
A Computer Approach to Content Analysis. MIT
Press.
David Talbot and Thorsten Brants. 2008. Randomized
language models via perfect hash functions. In Pro-
ceedings of ACL-08: HLT.
David Talbot and Miles Osborne. 2007. Smoothed
Bloom filter language models: Tera-scale LMs on the
cheap. In Proceedings of the 2007 Joint Conference on
Empirical Methods in Natural Language Processing
and Computational Natural Language Learning (EM
NLP-CoNLL).
David Talbot. 2009. Succinct approximate counting of
skewed data. In IJCAI?09: Proceedings of the 21st
international jont conference on Artifical intelligence.
Peter D. Turney and Michael L. Littman. 2003. Measur-
ing praise and criticism: Inference of semantic orienta-
tion from association. ACM Trans. Inf. Syst., 21:315?
346, October.
Peter D. Turney. 2008. A uniform approach to analogies,
synonyms, antonyms, and associations. In Proceed-
ings of COLING 2008.
Benjamin Van Durme and Ashwin Lall. 2009a. Prob-
abilistic counting with randomized storage. In IJ-
CAI?09: Proceedings of the 21st international jont
conference on Artifical intelligence.
Benjamin Van Durme and Ashwin Lall. 2009b. Stream-
ing pointwise mutual information. In Advances in
Neural Information Processing Systems 22.
Benjamin Van Durme and Ashwin Lall. 2010. Online
generation of locality sensitive hash signatures. In
Proceedings of the ACL 2010 Conference Short Pa-
pers, pages 231?235, July.
1103
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 747?756,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Midge: Generating Image Descriptions From Computer Vision
Detections
Margaret Mitchell?
Xufeng Han?
Jesse Dodge??
Alyssa Mensch??
Amit Goyal??
Alex Berg?
Kota Yamaguchi?
Tamara Berg?
Karl Stratos?
Hal Daume? III??
?U. of Aberdeen and Oregon Health and Science University, m.mitchell@abdn.ac.uk
? Stony Brook University, {aberg,tlberg,xufhan,kyamagu}@cs.stonybrook.edu
??U. of Maryland, {hal,amit}@umiacs.umd.edu
?Columbia University, stratos@cs.columbia.edu
??U. of Washington, dodgejesse@gmail.com, ??MIT, acmensch@mit.edu
Abstract
This paper introduces a novel generation
system that composes humanlike descrip-
tions of images from computer vision de-
tections. By leveraging syntactically in-
formed word co-occurrence statistics, the
generator filters and constrains the noisy
detections output from a vision system to
generate syntactic trees that detail what
the computer vision system sees. Results
show that the generation system outper-
forms state-of-the-art systems, automati-
cally generating some of the most natural
image descriptions to date.
1 Introduction
It is becoming a real possibility for intelligent sys-
tems to talk about the visual world. New ways of
mapping computer vision to generated language
have emerged in the past few years, with a fo-
cus on pairing detections in an image to words
(Farhadi et al 2010; Li et al 2011; Kulkarni et
al., 2011; Yang et al 2011). The goal in connect-
ing vision to language has varied: systems have
started producing language that is descriptive and
poetic (Li et al 2011), summaries that add con-
tent where the computer vision system does not
(Yang et al 2011), and captions copied directly
from other images that are globally (Farhadi et al
2010) and locally similar (Ordonez et al 2011).
A commonality between all of these ap-
proaches is that they aim to produce natural-
sounding descriptions from computer vision de-
tections. This commonality is our starting point:
We aim to design a system capable of producing
natural-sounding descriptions from computer vi-
sion detections that are flexible enough to become
more descriptive and poetic, or include likely in-
The bus by the road with a clear blue sky
Figure 1: Example image with generated description.
formation from a language model, or to be short
and simple, but as true to the image as possible.
Rather than using a fixed template capable of
generating one kind of utterance, our approach
therefore lies in generating syntactic trees. We
use a tree-generating process (Section 4.3) simi-
lar to a Tree Substitution Grammar, but preserv-
ing some of the idiosyncrasies of the Penn Tree-
bank syntax (Marcus et al 1995) on which most
statistical parsers are developed. This allows us
to automatically parse and train on an unlimited
amount of text, creating data-driven models that
flesh out descriptions around detected objects in a
principled way, based on what is both likely and
syntactically well-formed.
An example generated description is given in
Figure 1, and example vision output/natural lan-
guage generation (NLG) input is given in Fig-
ure 2. The system (?Midge?) generates descrip-
tions in present-tense, declarative phrases, as a
na??ve viewer without prior knowledge of the pho-
tograph?s content.1
Midge is built using the following approach:
An image processed by computer vision algo-
rithms can be characterized as a triple <Ai, Bi,
Ci>, where:
1Midge is available to try online at:
http://recognition.cs.stonybrook.edu:8080/?mitchema/midge/.
747
stuff: sky .999
id: 1
atts: clear:0.432, blue:0.945
grey:0.853, white:0.501 ...
b. box: (1,1 440,141)
stuff: road .908
id: 2
atts: wooden:0.722 clear:0.020 ...
b. box: (1,236 188,94)
object: bus .307
id: 3
atts: black:0.872, red:0.244 ...
b. box: (38,38 366,293)
preps: id 1, id 2: by id 1, id 3: by id 2, id 3: below
Figure 2: Example computer vision output and natu-
ral language generation input. Values correspond to
scores from the vision detections.
? Ai is the set of object/stuff detections with
bounding boxes and associated ?attribute?
detections within those bounding boxes.
? Bi is the set of action or pose detections as-
sociated to each ai ? Ai.
? Ci is the set of spatial relationships that hold
between the bounding boxes of each pair
ai, aj ? Ai.
Similarly, a description of an image can be char-
acterized as a triple <Ad, Bd, Cd> where:
? Ad is the set of nouns in the description with
associated modifiers.
? Bd is the set of verbs associated to each ad ?
Ad.
? Cd is the set of prepositions that hold be-
tween each pair of ad, ae ? Ad.
With this representation, mapping <Ai, Bi, Ci>
to <Ad, Bd, Cd> is trivial. The problem then
becomes: (1) How to filter out detections that
are wrong; (2) how to order the objects so that
they are mentioned in a natural way; (3) how to
connect these ordered objects within a syntacti-
cally/semantically well-formed tree; and (4) how
to add further descriptive information from lan-
guage modeling alone, if required.
Our solution lies in usingAi andAd as descrip-
tion anchors. In computer vision, object detec-
tions form the basis of action/pose, attribute, and
spatial relationship detections; therefore, in our
approach to language generation, nouns for the
object detections are used as the basis for the de-
scription. Likelihood estimates of syntactic struc-
ture and word co-occurrence are conditioned on
object nouns, and this enables each noun head in
a description to select for the kinds of structures it
tends to appear in (syntactic constraints) and the
other words it tends to occur with (semantic con-
straints). This is a data-driven way to generate
likely adjectives, prepositions, determiners, etc.,
taking the intersection of what the vision system
predicts and how the object noun tends to be de-
scribed.
2 Background
Our approach to describing images starts with
a system from Kulkarni et al(2011) that com-
poses novel captions for images in the PASCAL
sentence data set,2 introduced in Rashtchian et
al. (2010). This provides multiple object detec-
tions based on Felzenszwalb?s mixtures of multi-
scale deformable parts models (Felzenszwalb et
al., 2008), and stuff detections (roughly, mass
nouns, things like sky and grass) based on linear
SVMs for low level region features.
Appearance characteristics are predicted using
trained detectors for colors, shapes, textures, and
materials, an idea originally introduced in Farhadi
et al(2009). Local texture, Histograms of Ori-
ented Gradients (HOG) (Dalal and Triggs, 2005),
edge, and color descriptors inside the bounding
box of a recognized object are binned into his-
tograms for a vision system to learn to recognize
when an object is rectangular, wooden, metal,
etc. Finally, simple preposition functions are used
to compute the spatial relations between objects
based on their bounding boxes.
The original Kulkarni et al(2011) system gen-
erates descriptions with a template, filling in slots
by combining computer vision outputs with text
based statistics in a conditional random field to
predict the most likely image labeling. Template-
based generation is also used in the recent Yang et
al. (2011) system, which fills in likely verbs and
prepositions by dependency parsing the human-
written UIUC Pascal-VOC dataset (Farhadi et al
2010) and selecting the dependent/head relation
with the highest log likelihood ratio.
Template-based generation is useful for auto-
matically generating consistent sentences, how-
ever, if the goal is to vary or add to the text pro-
duced, it may be suboptimal (cf. Reiter and Dale
(1997)). Work that does not use template-based
generation includes Yao et al(2010), who gener-
ate syntactic trees, similar to the approach in this
2http://vision.cs.uiuc.edu/pascal-sentences/
748
Kulkarni et al This is a pic-
ture of three persons, one bot-
tle and one diningtable. The
first rusty person is beside the
second person. The rusty bot-
tle is near the first rusty per-
son, and within the colorful
diningtable. The second per-
son is by the third rusty per-
son. The colorful diningtable
is near the first rusty person,
and near the second person,
and near the third rusty person.
Kulkarni et al This is
a picture of two potted-
plants, one dog and one
person. The black dog is
by the black person, and
near the second feathered
pottedplant.
Yang et al Three people
are showing the bottle on the
street
Yang et al The person is
sitting in the chair in the
room
Midge: people with a bottle at
the table
Midge: a person in black
with a black dog by potted
plants
Figure 3: Descriptions generated by Midge, Kulkarni
et al(2011) and Yang et al(2011) on the same images.
Midge uses the Kulkarni et al(2011) front-end, and so
outputs are directly comparable.
paper. However, their system is not automatic, re-
quiring extensive hand-coded semantic and syn-
tactic details. Another approach is provided in
Li et al(2011), who use image detections to se-
lect and combine web-scale n-grams (Brants and
Franz, 2006). This automatically generates de-
scriptions that are either poetic or strange (e.g.,
?tree snowing black train?).
A different line of work transfers captions of
similar images directly to a query image. Farhadi
et al(2010) use <object,action,scene> triples
predicted from the visual characteristics of the
image to find potential captions. Ordonez et al
(2011) use global image matching with local re-
ordering from a much larger set of captioned pho-
tographs. These transfer-based approaches result
in natural captions (they are written by humans)
that may not actually be true of the image.
This work learns and builds from these ap-
proaches. Following Kulkarni et aland Li et al
the system uses large-scale text corpora to esti-
mate likely words around object detections. Fol-
lowing Yang et al the system can hallucinate
likely words using word co-occurrence statistics
alone. And following Yao et al the system aims
black, blue, brown, colorful, golden, gray,
green, orange, pink, red, silver, white, yel-
low, bare, clear, cute, dirty, feathered, flying,
furry, pine, plastic, rectangular, rusty, shiny,
spotted, striped, wooden
Table 1: Modifiers used to extract training corpus.
for naturally varied but well-formed text, generat-
ing syntactic trees rather than filling in a template.
In addition to these tasks, Midge automatically
decides what the subject and objects of the de-
scription will be, leverages the collected word co-
occurrence statistics to filter possible incorrect de-
tections, and offers the flexibility to be as de-
scriptive or as terse as possible, specified by the
user at run-time. The end result is a fully au-
tomatic vision-to-language system that is begin-
ning to generate syntactically and semantically
well-formed descriptions with naturalistic varia-
tion. Example descriptions are given in Figures 4
and 5, and descriptions from other recent systems
are given in Figure 3.
The results are promising, but it is important to
note that Midge is a first-pass system through the
steps necessary to connect vision to language at
a deep syntactic/semantic level. As such, it uses
basic solutions at each stage of the process, which
may be improved: Midge serves as an illustration
of the types of issues that should be handled to
automatically generate syntactic trees from vision
detections, and offers some possible solutions. It
is evaluated against the Kulkarni et alsystem, the
Yang et alsystem, and human-written descrip-
tions on the same set of images in Section 5, and
is found to significantly outperform the automatic
systems.
3 Learning from Descriptive Text
To train our system on how people describe im-
ages, we use 700,000 (Flickr, 2011) images with
associated descriptions from the dataset in Or-
donez et al(2011). This is separate from our
evaluation image set, consisting of 840 PASCAL
images. The Flickr data is messier than datasets
created specifically for vision training, but pro-
vides the largest corpus of natural descriptions of
images to date.
We normalize the text by removing emoticons
and mark-up language, and parse each caption
using the Berkeley parser (Petrov, 2010). Once
parsed, we can extract syntactic information for
individual (word, tag) pairs.
749
a cow with sheep with a gray sky people with boats a brown cow people at
green grass by the road a wooden table
Figure 4: Example generated outputs.
Awkward Prepositions Incorrect Detections
a person boats under a black bicycle at the sky a yellow bus cows by black sheep
on the dog the sky a green potted plant with people by the road
Figure 5: Example generated outputs: Not quite right
We compute the probabilities for different
prenominal modifiers (shiny, clear, glowing, ...)
and determiners (a/an, the, None, ...) given a
head noun in a noun phrase (NP), as well as the
probabilities for each head noun in larger con-
structions, listed in Section 4.3. Probabilities are
conditioned only on open-class words, specifi-
cally, nouns and verbs. This means that a closed-
class word (such as a preposition) is never used to
generate an open-class word.
In addition to co-occurrence statistics, the
parsed Flickr data adds to our understanding of
the basic characteristics of visually descriptive
text. Using WordNet (Miller, 1995) to automati-
cally determine whether a head noun is a physical
object or not, we find that 92% of the sentences
have no more than 3 physical objects. This in-
forms generation by placing a cap on how many
objects are mentioned in each descriptive sen-
tence: When more than 3 objects are detected,
the system splits the description over several sen-
tences. We also find that many of the descriptions
are not sentences as well (tagged as S, 58% of the
data), but quite commonly noun phrases (tagged
as NP, 28% of the data), and expect that the num-
ber of noun phrases that form descriptions will be
much higher with domain adaptation. This also
informs generation, and the system is capable of
generating both sentences (contains a main verb)
and noun phrases (no main verb) in the final im-
age description. We use the term ?sentence? in the
rest of this paper to refer to both kinds of complex
phrases.
4 Generation
Following Penn Treebank parsing guidelines
(Marcus et al 1995), the relationship between
two head nouns in a sentence can usually be char-
acterized among the following:
1. prepositional (a boy on the table)
2. verbal (a boy cleans the table)
3. verb with preposition (a boy sits on the table)
4. verb with particle (a boy cleans up the table)
5. verb with S or SBAR complement (a boy
sees that the table is clean)
The generation system focuses on the first three
kinds of relationships, which capture a wide range
of utterances. The process of generation is ap-
proached as a problem of generating a semanti-
cally and syntactically well-formed tree based on
object nouns. These serve as head noun anchors
in a lexicalized syntactic derivation process that
we call tree growth.
Vision detections are associated to a {tag
word} pair, and the model fleshes out the tree de-
tails around head noun anchors by utilizing syn-
tactic dependencies between words learned from
the Flickr data discussed in Section 3. The anal-
ogy of growing a tree is quite appropriate here,
where nouns are bundles of constraints akin to
seeds, giving rise to the rest of the tree based on
the lexicalized subtrees in which the nouns are
likely to occur. An example generated tree struc-
ture is shown in Figure 6, with noun anchors in
bold.
750
NP
PP
NP
NN
table
DT
the
IN
at
NP
PP
NP
NN
bottle
DT
a
IN
with
NP
NN
people
DT
-
Figure 6: Tree generated from tree growth process.
Midge was developed using detections run on
Flickr images, incorporating action/pose detec-
tions for verbs as well as object detections for
nouns. In testing, we generate descriptions for
the PASCAL images, which have been used in
earlier work on the vision-to-language connection
(Kulkarni et al 2011; Yang et al 2011), and al-
lows us to compare systems directly. Action and
pose detection for this data set still does not work
well, and so the system does not receive these de-
tections from the vision front-end. However, the
system can still generate verbs when action and
pose detectors have been run, and this framework
allows the system to ?hallucinate? likely verbal
constructions between objects if specified at run-
time. A similar approach was taken in Yang et al
(2011). Some examples are given in Figure 7.
We follow a three-tiered generation process
(Reiter and Dale, 2000), utilizing content determi-
nation to first cluster and order the object nouns,
create their local subtrees, and filter incorrect de-
tections; microplanning to construct full syntactic
trees around the noun clusters, and surface real-
ization to order selected modifiers, realize them as
postnominal or prenominal, and select final out-
puts. The system follows an overgenerate-and-
select approach (Langkilde and Knight, 1998),
which allows different final trees to be selected
with different settings.
4.1 Knowledge Base
Midge uses a knowledge base that stores models
for different tasks during generation. These mod-
els are primarily data-driven, but we also include
a hand-built component to handle a small set of
rules. The data-driven component provides the
syntactically informed word co-occurrence statis-
tics learned from the Flickr data, a model for or-
dering the selected nouns in a sentence, and a
model to change computer vision attributes to at-
tribute:value pairs. Below, we discuss the three
main data-driven models within the generation
Unordered Ordered
bottle, table, person ? person, bottle, table
road, sky, cow ? cow, road, sky
Figure 8: Example nominal orderings.
pipeline. The hand-built component contains plu-
ral forms of singular nouns, the list of possible
spatial relations shown in Table 3, and a map-
ping between attribute values and modifier sur-
face forms (e.g., a green detection for person is to
be realized as the postnominal modifier in green).
4.2 Content Determination
4.2.1 Step 1: Group the Nouns
An initial set of object detections must first be
split into clusters that give rise to different sen-
tences. If more than 3 objects are detected in the
image, the system begins splitting these into dif-
ferent noun groups. In future work, we aim to
compare principled approaches to this task, e.g.,
using mutual information to cluster similar nouns
together. The current system randomizes which
nouns appear in the same group.
4.2.2 Step 2: Order the Nouns
Each group of nouns are then ordered to deter-
mine when they are mentioned in a sentence. Be-
cause the system generates declarative sentences,
this automatically determines the subject and ob-
jects. This is a novel contribution for a general
problem in NLG, and initial evaluation (Section
5) suggests it works reasonably well.
To build the nominal ordering model, we use
WordNet to associate all head nouns in the Flickr
data to all of their hypernyms. A description is
represented as an ordered set [a1...an] where each
ap is a noun with position p in the set of head
nouns in the sentence. For the position pi of each
hypernym ha in each sentence with n head nouns,
we estimate p(pi|n, ha).
During generation, the system greedily maxi-
mizes p(pi|n, ha) until all nouns have been or-
dered. Example orderings are shown in Figure 8.
This model automatically places animate objects
near the beginning of a sentence, which follows
psycholinguistic work in object naming (Branigan
et al 2007).
4.2.3 Step 3: Filter Incorrect Attributes
For the system to be able to extend coverage as
new computer vision attribute detections become
available, we develop a method to automatically
751
A person sitting on a sofa Cows grazing Airplanes flying A person walking a dog
Figure 7: Hallucinating: Creating likely actions. Straightforward to do, but can often be wrong.
COLOR purple blue green red white ...
MATERIAL plastic wooden silver ...
SURFACE furry fluffy hard soft ...
QUALITY shiny rust dirty broken ...
Table 2: Example attribute classes and values.
group adjectives into broader attribute classes,3
and the generation system uses these classes when
deciding how to describe objects. To group adjec-
tives, we use a bootstrapping technique (Kozareva
et al 2008) that learns which adjectives tend to
co-occur, and groups these together to form an at-
tribute class. Co-occurrence is computed using
cosine (distributional) similarity between adjec-
tives, considering adjacent nouns as context (i.e.,
JJ NN constructions). Contexts (nouns) for adjec-
tives are weighted using Pointwise Mutual Infor-
mation and only the top 1000 nouns are selected
for every adjective. Some of the learned attribute
classes are given in Table 2.
In the Flickr corpus, we find that each attribute
(COLOR, SIZE, etc.), rarely has more than a single
value in the final description, with the most com-
mon (COLOR) co-occurring less than 2% of the
time. Midge enforces this idea to select the most
likely word v for each attribute from the detec-
tions. In a noun phrase headed by an object noun,
NP{NN noun}, the prenominal adjective (JJ v) for
each attribute is selected using maximum likeli-
hood.
4.2.4 Step 4: Group Plurals
How to generate natural-sounding spatial rela-
tions and modifiers for a set of objects, as opposed
to a single object, is still an open problem (Fu-
nakoshi et al 2004; Gatt, 2006). In this work, we
use a simple method to group all same-type ob-
jects together, associate them to the plural form
listed in the KB, discard the modifiers, and re-
turn spatial relations based on the first recognized
3What in computer vision are called attributes are called
values in NLG. A value like red belongs to a COLOR at-
tribute, and we use this distinction in the system.
member of the group.
4.2.5 Step 5: Gather Local Subtrees Around
Object Nouns
1 2
NP
NN
n
JJ* ?DT{0,1} ? S
VP{VBZ} ?NP{NN n}
3 4
NP
VP{VB(G|N)} ?NP{NN n}
NP
PP{IN} ?NP{NN n}
5 6
PP
NP{NN n}IN ?
VP
PP{IN} ?VB(G|N|Z) ?
7
VP
NP{NN n}VB(G|N|Z) ?
Figure 9: Initial subtree frames for generation, present-
tense declarative phrases. ? marks a substitution site,
* marks ? 0 sister nodes of this type permitted, {0,1}
marks that this node can be included of excluded.
Input: set of ordered nouns, Output: trees preserving
nominal ordering.
Possible actions/poses and spatial relationships
between objects nouns, represented by verbs and
prepositions, are selected using the subtree frames
listed in Figure 9. Each head noun selects for its
likely local subtrees, some of which are not fully
formed until the Microplanning stage. As an ex-
ample of how this process works, see Figure 10,
which illustrates the combination of Trees 4 and
5. For simplicity, we do not include the selection
of further subtrees. The subject noun duck se-
lects for prepositional phrases headed by different
prepositions, and the object noun grass selects
for prepositions that head the prepositional phrase
in which it is embedded. Full PP subtrees are cre-
ated during Microplanning by taking the intersec-
tion of both.
The leftmost noun in the sequence is given a
rightward directionality constraint, placing it as
the subject of the sentence, and so it will only se-
752
a over b a above b b below a b beneath a a by b b by a a on b b under a
b underneath a a upon b a over b
a by b a against b b against a b around a a around b a at b b at a a beside b
b beside a a by b b by a a near b b near a b with a a with b
a in b a in b b outside a a within b a by b b by a
Table 3: Possible prepositions from bounding boxes.
Subtree frames:
NP
PP{IN} ?NP{NN n1}
PP
NP{NN n2}IN ?
Generated subtrees:
NP
PP
IN
above, on, by
NP
NN
duck
PP
NP
NN
grass
IN
on, by, over
Combined trees:
NP
PP
NP
NN
grass
IN
on
NP
NN
duck
NP
PP
NP
NN
grass
IN
by
NP
NN
duck
Figure 10: Example derivation.
lect for trees that expand to the right. The right-
most noun is given a leftward directionality con-
straint, placing it as an object, and so it will only
select for trees that expand to its left. The noun in
the middle, if there is one, selects for all its local
subtrees, combining first with a noun to its right
or to its left. We now walk through the deriva-
tion process for each of the listed subtree frames.
Because we are following an overgenerate-and-
select approach, all combinations above a proba-
bility threshold ? and an observation cutoff ? are
created.
Tree 1:
Collect all NP? (DT det) (JJ adj)* (NN noun)
and NP? (JJ adj)* (NN noun) subtrees, where:
? p((JJ adj)|(NN noun)) > ? for each adj
? p((DT det)|JJ, (NN noun)) > ?, and the proba-
bility of a determiner for the head noun is higher
than the probability of no determiner.
Any number of adjectives (including none) may
be generated, and we include the presence or ab-
sence of an adjective when calculating which de-
terminer to include.
The reasoning behind the generation of these
subtrees is to automatically learn whether to treat
a given noun as a mass or count noun (not taking a
determiner or taking a determiner, respectively) or
as a given or new noun (phrases like a sky sound
unnatural because sky is given knowledge, requir-
ing the definite article the). The selection of de-
terminer is not independent of the selection of ad-
jective; a sky may sound unnatural, but a blue sky
is fine. These trees take the dependency between
determiner and adjective into account.
Trees 2 and 3:
Collect beginnings of VP subtrees headed by
(VBZ verb), (VBG verb), and (VBN verb), no-
tated here as VP{VBX verb}, where:
? p(VP{VBX verb}|NP{NN noun}=SUBJ) > ?
Tree 4:
Collect beginnings of PP subtrees headed by (IN
prep), where:
? p(PP{IN prep}|NP{NN noun}=SUBJ) > ?
Tree 5:
Collect PP subtrees headed by (IN prep) with
NP complements (OBJ) headed by (NN noun),
where:
? p(PP{IN prep}|NP{NN noun}=OBJ) > ?
Tree 6:
Collect VP subtrees headed by (VBX verb) with
embedded PP complements, where:
? p(PP{IN prep}|VP{VBX verb}=SUBJ) > ?
Tree 7:
Collect VP subtrees headed by (VBX verb) with
embedded NP objects, where:
? p(VP{VBX verb}|NP{NN noun}=OBJ) > ?
4.3 Microplanning
4.3.1 Step 6: Create Full Trees
In Microplanning, full trees are created by tak-
ing the intersection of the subtrees created in Con-
tent Determination. Because the nouns are or-
dered, it is straightforward to combine the sub-
trees surrounding a noun in position 1 with sub-
trees surrounding a noun in position 2. Two
753
VP
VP* ?
NP
NP ?CC
and
NP ?
Figure 11: Auxiliary trees for generation.
further trees are necessary to allow the subtrees
gathered to combine within the Penn Treebank
syntax. These are given in Figure 11. If two
nouns in a proposed sentence cannot be combined
with prepositions or verbs, we backoff to combine
them using (CC and).
Stepping through this process, all nouns will
have a set of subtrees selected by Tree 1. Prepo-
sitional relationships between nouns are created
by substituting Tree 1 subtrees into the NP nodes
of Trees 4 and 5, as shown in Figure 10. Verbal
relationships between nouns are created by substi-
tuting Tree 1 subtrees into Trees 2, 3, and 7. Verb
with preposition relationships are created between
nouns by substituting the VBX node in Tree 6
with the corresponding node in Trees 2 and 3 to
grow the tree to the right, and the PP node in Tree
6 with the corresponding node in Tree 5 to grow
the tree to the left. Generation of a full tree stops
when all nouns in a group are dominated by the
same node, either an S or NP.
4.4 Surface Realization
In the surface realization stage, the system se-
lects a single tree from the generated set of pos-
sible trees and removes mark-up to produce a fi-
nal string. This is also the stage where punctua-
tion may be added. Different strings may be gen-
erated depending on different specifications from
the user, as discussed at the beginning of Section
4 and shown in the online demo. To evaluate the
system against other systems, we specify that the
system should (1) not hallucinate likely verbs; and
(2) return the longest string possible.
4.4.1 Step 7: Get Final Tree, Clear Mark-Up
We explored two methods for selecting a final
string. In one method, a trigram language model
built using the Europarl (Koehn, 2005) data with
start/end symbols returns the highest-scoring de-
scription (normalizing for length). In the second
method, we limit the generation system to select
the most likely closed-class words (determiners,
prepositions) while building the subtrees, over-
generating all possible adjective combinations.
The final string is then the one with the most
words. We find that the second method produces
descriptions that seem more natural and varied
than the n-gram ranking method for our develop-
ment set, and so use the longest string method in
evaluation.
4.4.2 Step 8: Prenominal Modifier Ordering
To order sets of selected adjectives, we use the
top-scoring prenominal modifier ordering model
discussed in Mitchell et al(2011). This is an n-
gram model constructed over noun phrases that
were extracted from an automatically parsed ver-
sion of the New York Times portion of the Giga-
word corpus (Graff and Cieri, 2003). With this
in place, blue clear sky becomes clear blue sky,
wooden brown table becomes brown wooden ta-
ble, etc.
5 Evaluation
Each set of sentences is generated with ? (likeli-
hood cutoff) set to .01 and ? (observation count
cutoff) set to 3. We compare the system against
human-written descriptions and two state-of-the-
art vision-to-language systems, the Kulkarni et al
(2011) and Yang et al(2011) systems.
Human judgments were collected using Ama-
zon?s Mechanical Turk (Amazon, 2011). We
follow recommended practices for evaluating an
NLG system (Reiter and Belz, 2009) and for run-
ning a study on Mechanical Turk (Callison-Burch
and Dredze, 2010), using a balanced design with
each subject rating 3 descriptions from each sys-
tem. Subjects rated their level of agreement on
a 5-point Likert scale including a neutral mid-
dle position, and since quality ratings are ordinal
(points are not necessarily equidistant), we evalu-
ate responses using a non-parametric test. Partici-
pants that took less than 3 minutes to answer all 60
questions and did not include a humanlike rating
for at least 1 of the 3 human-written descriptions
were removed and replaced. It is important to note
that this evaluation compares full generation sys-
tems; many factors are at play in each system that
may also influence participants? perception, e.g.,
sentence length (Napoles et al 2011) and punc-
tuation decisions.
The systems are evaluated on a set of 840
images evaluated in the original Kulkarni et al
(2011) system. Participants were asked to judge
the statements given in Figure 12, from Strongly
Disagree to Strongly Agree.
754
Grammaticality Main Aspects Correctness Order Humanlikeness
Human 4 (3.77, 1.19) 4 (4.09, 0.97) 4 (3.81, 1.11) 4 (3.88, 1.05) 4 (3.88, 0.96)
Midge 3 (2.95, 1.42) 3 (2.86, 1.35) 3 (2.95, 1.34) 3 (2.92, 1.25) 3 (3.16, 1.17)
Kulkarni et al2011 3 (2.83, 1.37) 3 (2.84, 1.33) 3 (2.76, 1.34) 3 (2.78, 1.23) 3 (3.13, 1.23)
Yang et al2011 3 (2.95, 1.49) 2 (2.31, 1.30) 2 (2.46, 1.36) 2 (2.53, 1.26) 3 (2.97, 1.23)
Table 4: Median scores for systems, mean and standard deviation in parentheses. Distance between points on the
rating scale cannot be assumed to be equidistant, and so we analyze results using a non-parametric test.
GRAMMATICALITY:
This description is grammatically correct.
MAIN ASPECTS:
This description describes the main aspects of this
image.
CORRECTNESS:
This description does not include extraneous or in-
correct information.
ORDER:
The objects described are mentioned in a reasonable
order.
HUMANLIKENESS:
It sounds like a person wrote this description.
Figure 12: Mechanical Turk prompts.
We report the scores for the systems in Table
4. Results are analyzed using the non-parametric
Wilcoxon Signed-Rank test, which uses median
values to compare the different systems. Midge
outperforms all recent automatic approaches on
CORRECTNESS and ORDER, and Yang et alad-
ditionally on HUMANLIKENESS and MAIN AS-
PECTS. Differences between Midge and Kulkarni
et alare significant at p< .01; Midge and Yang et
al. at p< .001. For all metrics, human-written de-
scriptions still outperform automatic approaches
(p < .001).
These findings are striking, particularly be-
cause Midge uses the same input as the Kulka-
rni et alsystem. Using syntactically informed
word co-occurrence statistics from a large corpus
of descriptive text improves over state-of-the-art,
allowing syntactic trees to be generated that cap-
ture the variation of natural language.
6 Discussion
Midge automatically generates language that is as
good as or better than template-based systems,
tying vision to language at a syntactic/semantic
level to produce natural language descriptions.
Results are promising, but, there is more work to
be done: Evaluators can still tell a difference be-
tween human-written descriptions and automati-
cally generated descriptions.
Improvements to the generated language are
possible at both the vision side and the language
side. On the computer vision side, incorrect ob-
jects are often detected and salient objects are of-
ten missed. Midge does not yet screen out un-
likely objects or add likely objects, and so pro-
vides no filter for this. On the language side, like-
lihood is estimated directly, and the system pri-
marily uses simple maximum likelihood estima-
tions to combine subtrees. The descriptive cor-
pus that informs the system is not parsed with
a domain-adapted parser; with this in place, the
syntactic constructions that Midge learns will bet-
ter reflect the constructions that people use.
In future work, we hope to address these issues
as well as advance the syntactic derivation pro-
cess, providing an adjunction operation (for ex-
ample, to add likely adjectives or adverbs based
on language alone). We would also like to incor-
porate meta-data ? even when no vision detection
fires for an image, the system may be able to gen-
erate descriptions of the time and place where an
image was taken based on the image file alone.
7 Conclusion
We have introduced a generation system that uses
a new approach to generating language, tying a
syntactic model to computer vision detections.
Midge generates a well-formed description of an
image by filtering attribute detections that are un-
likely and placing objects into an ordered syntac-
tic structure. Humans judge Midge?s output to be
the most natural descriptions of images generated
thus far. The methods described here are promis-
ing for generating natural language descriptions
of the visual world, and we hope to expand and
refine the system to capture further linguistic phe-
nomena.
8 Acknowledgements
Thanks to the Johns Hopkins CLSP summer
workshop 2011 for making this system possible,
and to reviewers for helpful comments. This
work is supported in part by Michael Collins and
by NSF Faculty Early Career Development (CA-
REER) Award #1054133.
755
References
Amazon. 2011. Amazon mechanical turk: Artificial
artificial intelligence.
Holly P. Branigan, Martin J. Pickering, and Mikihiro
Tanaka. 2007. Contributions of animacy to gram-
matical function assignment and word order during
production. Lingua, 118(2):172?189.
Thorsten Brants and Alex Franz. 2006. Web 1T 5-
gram version 1.
Chris Callison-Burch and Mark Dredze. 2010. Creat-
ing speech and language data with Amazon?s Me-
chanical Turk. NAACL 2010 Workshop on Creat-
ing Speech and Language Data with Amazon?s Me-
chanical Turk.
Navneet Dalal and Bill Triggs. 2005. Histograms of
oriented gradients for human detections. Proceed-
ings of CVPR 2005.
Ali Farhadi, Ian Endres, Derek Hoiem, and David
Forsyth. 2009. Describing objects by their at-
tributes. Proceedings of CVPR 2009.
Ali Farhadi, Mohsen Hejrati, Mohammad Amin
Sadeghi, Peter Young, Cyrus Rashtchian, Julia
Hockenmaier, and David Forsyth. 2010. Every pic-
ture tells a story: generating sentences for images.
Proceedings of ECCV 2010.
Pedro Felzenszwalb, David McAllester, and Deva Ra-
maman. 2008. A discriminatively trained, mul-
tiscale, deformable part model. Proceedings of
CVPR 2008.
Flickr. 2011. http://www.flickr.com. Accessed
1.Sep.11.
Kotaro Funakoshi, Satoru Watanabe, Naoko
Kuriyama, and Takenobu Tokunaga. 2004.
Generating referring expressions using perceptual
groups. Proceedings of the 3rd INLG.
Albert Gatt. 2006. Generating collective spatial refer-
ences. Proceedings of the 28th CogSci.
David Graff and Christopher Cieri. 2003. English Gi-
gaword. Linguistic Data Consortium, Philadelphia,
PA. LDC Catalog No. LDC2003T05.
Philipp Koehn. 2005. Europarl: A parallel cor-
pus for statistical machine translation. MT Summit.
http://www.statmt.org/europarl/.
Zornitsa Kozareva, Ellen Riloff, and Eduard Hovy.
2008. Semantic class learning from the web with
hyponym pattern linkage graphs. Proceedings of
ACL-08: HLT.
Girish Kulkarni, Visruth Premraj, Sagnik Dhar, Sim-
ing Li, Yejin Choi, Alexander C. Berg, and Tamara
Berg. 2011. Baby talk: Understanding and gener-
ating image descriptions. Proceedings of the 24th
CVPR.
Irene Langkilde and Kevin Knight. 1998. Gener-
ation that exploits corpus-based statistical knowl-
edge. Proceedings of the 36th ACL.
Siming Li, Girish Kulkarni, Tamara L. Berg, Alexan-
der C. Berg, and Yejin Choi. 2011. Composing
simple image descriptions using web-scale n-grams.
Proceedings of CoNLL 2011.
Mitchell Marcus, Ann Bies, Constance Cooper, Mark
Ferguson, and Alyson Littman. 1995. Treebank II
bracketing guide.
George A. Miller. 1995. WordNet: A lexical
database for english. Communications of the ACM,
38(11):39?41.
Margaret Mitchell, Aaron Dunlop, and Brian Roark.
2011. Semi-supervised modeling for prenomi-
nal modifier ordering. Proceedings of the 49th
ACL:HLT.
Courtney Napoles, Benjamin Van Durme, and Chris
Callison-Burch. 2011. Evaluating sentence com-
pression: Pitfalls and suggested remedies. ACL-
HLT Workshop on Monolingual Text-To-Text Gen-
eration.
Vicente Ordonez, Girish Kulkarni, and Tamara L Berg.
2011. Im2text: Describing images using 1 million
captioned photographs. Proceedings of NIPS 2011.
Slav Petrov. 2010. Berkeley parser. GNU General
Public License v.2.
Cyrus Rashtchian, Peter Young, Micah Hodosh, and
Julia Hockenmaier. 2010. Collecting image anno-
tations using amazon?s mechanical turk. Proceed-
ings of the NAACL HLT 2010 Workshop on Creat-
ing Speech and Language Data with Amazon?s Me-
chanical Turk.
Ehud Reiter and Anja Belz. 2009. An investiga-
tion into the validity of some metrics for automat-
ically evaluating natural language generation sys-
tems. Computational Linguistics, 35(4):529?558.
Ehud Reiter and Robert Dale. 1997. Building ap-
plied natural language generation systems. Journal
of Natural Language Engineering, pages 57?87.
Ehud Reiter and Robert Dale. 2000. Building Natural
Language Generation Systems. Cambridge Univer-
sity Press.
Yezhou Yang, Ching Lik Teo, Hal Daume? III, and
Yiannis Aloimonos. 2011. Corpus-guided sen-
tence generation of natural images. Proceedings of
EMNLP 2011.
Benjamin Z. Yao, Xiong Yang, Liang Lin, Mun Wai
Lee, and Song-Chun Zhu. 2010. I2T: Image pars-
ing to text description. Proceedings of IEEE 2010,
98(8):1485?1508.
756
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 762?772,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Detecting Visual Text
Jesse Dodge1, Amit Goyal2, Xufeng Han3, Alyssa Mensch4, Margaret Mitchell5, Karl Stratos6
Kota Yamaguchi3, Yejin Choi3, Hal Daume? III2, Alexander C. Berg3 and Tamara L. Berg3
1University of Washington, 2University of Maryland, 3Stony Brook University
4MIT, 5Oregon Health & Science University, 6Columbia University
dodgejesse@gmail.com, amit@umiacs.umd.edu, xufhan@cs.stonybrook.edu
acmensch@mit.edu, mitchmar@ohsu.edu, stratos@cs.columbia.edu
kyamagu@cs.stonybrook.edu, ychoi@cs.stonybrook.edu
me@hal3.name, aberg@cs.stonybrook.edu, tlberg@cs.stonybrook.edu
Abstract
When people describe a scene, they often in-
clude information that is not visually apparent;
sometimes based on background knowledge,
sometimes to tell a story. We aim to sepa-
rate visual text?descriptions of what is being
seen?from non-visual text in natural images
and their descriptions. To do so, we first con-
cretely define what it means to be visual, an-
notate visual text and then develop algorithms
to automatically classify noun phrases as vi-
sual or non-visual. We find that using text
alone, we are able to achieve high accuracies
at this task, and that incorporating features
derived from computer vision algorithms im-
proves performance. Finally, we show that we
can reliably mine visual nouns and adjectives
from large corpora and that we can use these
effectively in the classification task.
1 Introduction
People use language to describe the visual world.
Our goal is to: formalize what ?visual text? is (Sec-
tion 2.2); analyze naturally occurring written lan-
guage for occurrences of visual text (Section 2); and
build models that can detect visual descriptions from
raw text or from image/text pairs (Section 3). This
is a challenging problem. One challenge is demon-
strated in Figure 1, which contains two images that
contain the noun ?car? in their human-written cap-
tions. In one case (the top image), there actually is a
car in the image; in the other case, there is not: the
car refers to the state of the speaker.
The ability to automatically identify visual text is
practically useful in a number of scenarios. One can
Another dream car to
add to the list, this one
spotted in Hanbury St.
Shot out my car win-
dow while stuck in traf-
fic because people in
Cincinnati can?t drive in
the rain.
Figure 1: Two image/caption pairs, both containing the
noun ?car? but only the top one in a visual context.
imagine automatically mining image/caption data
(like that in Figure 1) to train object recognition sys-
tems. However, in order to do so reliably, one must
know whether the ?car? actually appears or not.
When building image search engines, it is common
to use text near an image as features; this is more
useful when this text is actually visual. Or when
training systems to automatically generate captions
of images (e.g., for visually impaired users), we
need good language models for visual text.
One of our goals is to define what it means for a
bit of text to be visual. As inspiration, we consider
image/description pairs automatically crawled from
Flickr (Ordonez et al, 2011). A first pass attempt
might be to say ?a phrase in the description of an
image is visual if you can see it in the corresponding
image.? Unfortunately, this is too vague to be useful;
the biggest issues are discussed in Section 2.2.
762
Based on our analysis, we settled on the follow-
ing definition: A piece of text is visual (with re-
spect to a corresponding image) if you can cut out
a part of that image, paste it into any other image,
and a third party could describe that cut-out part in
the same way. In the car example, the claim is that I
could cut out the car, put it in the middle of any other
image, and someone else might still refer to that car
as ?dream car.? The car in the bottom image in Fig-
ure 1 is not visual because there?s nothing you could
cut out that would retain car-ness.
2 Data Analysis
Before embarking on the road to building models of
visual text, it is useful to obtain a better understand-
ing of what visual text is like, and how it compares to
the more standard corpora that we are used to work-
ing with. We describe the two large data sets that we
use (one visual, one non-visual), then describe the
quantitative differences between them, and finally
discuss our annotation effort for labeling visual text.
2.1 Data sets
We use the SBU Captioned Photo Dataset (Ordonez
et al, 2011) as our primary source of image/caption
data. This dataset contains 1 million images with
user associated captions, collected in the wild by in-
telligent filtering of a huge number of Flickr pho-
tos. Past work has made use of this dataset to re-
trieve whole captions for association with a query
image (Ordonez et al, 2011). Their method first
used global image descriptors to retrieve an initial
matched set, and then applied more local estimates
of content to re-rank this (relatively small) set (Or-
donez et al, 2011). This means that content based
matching was relatively constrained by the bottle-
neck of global descriptors, and local content (e.g.,
objects) had relatively small effect on accuracy.
As an auxiliary source of information for (largely)
non-visual text, we consider a large corpus of text
obtained by concatenating ukWaC1 and the New
York Times Newswire Service (NYT) section of the
Gigaword (Graff, 2003) Corpus. The Web-derived
ukWaC is already tokenized and POS-tagged with
the TreeTagger (Schmid, 1995). NYT is tokenized,
1ukWaC is a freely available Wikipedia-derived corpus from
2009; see http://wacky.sslmit.unibo.it/doku.php.
and POS-tagged using TagChunk (Daume? III and
Marcu, 2005). This consists of 171 million sen-
tences (4 billion words). We refer to this generic
text corpus as Large-Data.
2.2 Formalizing visual text
We begin our analysis by revisiting the definition
of visual text from the introduction, and justifying
this particular definition. In order to arrive at a suf-
ficiently specific definition of ?visual text,? we fo-
cused on the applications of visual text that we care
about. As discussed in the introduction, these are:
training object detectors, building image search en-
gines and automatically generating captions for im-
ages. Our definition is based on access to image/text
pairs, but later we discuss how to talk about it purely
based on text. To make things concrete, consider an
image/text pair like that in the top of Figure 1. And
then consider a phrase in the text, like ?dream car.?
The question is: is ?dream car? visual or not?
One of the challenges in arriving at such a defi-
nition is that the description of an image in Flickr
is almost always written by the photographer of that
image. This means the descriptions often contain in-
formation that is not actually pictured in the image,
or contain references that are only relevant to the
photographer (referring to a person/pet by name).
One might think that this is an artifact of this par-
ticular dataset, but it appears to be generic to all cap-
tions, even those written by a viewer (rather than the
photographer). Figure 2 shows an image from the
Pascal dataset (Everingham et al, 2010), together
with captions written by random people collected
via crowd-sourcing (Rashtchian et al, 2010). There
is much in this caption that is clearly made-up by the
author, presumably to make the caption more inter-
esting (e.g., meta-references like ?the camera? or ?A
photo? as well as ?guesses? about the image, such as
?garage? and ?venison?).
Second, there is a question of how much inference
you are allowed to do when you say that you ?see?
something. For example, in the top image in Fig-
ure 1, the street is pictured, but does that mean that
?Hanbury St.? is visual? What if there were a street
sign that clearly read ?Hanbury St.? in the image?
This problem comes up all the time, when people
say things like ?in London? or ?in France? in their
captions. If it?s just a portrait of people ?in France,?
763
1. A distorted photo of a man cutting up a large cut of meat in a garage.
2. A man smiling at the camera while carving up meat.
3. A man smiling while he cuts up a piece of meat.
4. A smiling man is standing next to a table dressing a piece of venison.
5. The man is smiling into the camera as he cuts meat.
Figure 2: An image from the Pascal data with five captions collected via crowd-sourcing. Measurements on the
SMALL and LARGE dataset show that approximately 70% of noun phrases are visual (bolded), while the rest are
non-visual (underlined). See Section 2.4 for details.
it?s hard to say that this is visual. If you see the Eif-
fel tower in the background, this is perhaps better
(though it could be Las Vegas!), but how does this
compare to a photo taken out of an airplane window
in which you actually do see France-the-country?
This problem becomes even more challenging
when you consider things other than nouns. For in-
stance, when is a verb visual? For instance, the most
common non-copula verb in our data is ?sitting,?
which appears in roughly two usages: (1) ?Took this
shot, sitting in a bar and enjoying a Portugese beer.?
and (2) ?Lexy sitting in a basket on top of her cat
tree.? The first one is clearly not visual; the second
probably is. A more nuanced case is for ?playing,?
as in: ?Girls playing in a boat on the river bank?
(probably visual) versus ?Tuckered out from play-
ing in Nannie?s yard.? The corresponding image for
the latter description shows a sleeping cat.
Our final definition, based on cutting out the po-
tentially visual part of the image, allows us to say
that: (1) ?venison? is not visual (because you cannot
actually tell); (2) ?Hanbury St.? and ?Lexy? are not
visual (you can infer them, in the first case because
there is only one street and in the second case be-
cause there is only one cat); (3) that seeing the real
Eiffel tower in the background does not mean that
?France? is visual (but again, may be inferred); etc.
2.3 Most Pronounced Differences
To get an intuitive sense of how Flickr captions (ex-
pected to be predominantly visual) and generic text
(expected not to be so) differ, we computed some
simple statistics on sentences from these. In gen-
eral, the generic text had twice as many main verbs
as the Flickr data, four times as many auxiliaries or
light verbs, and about 50% more prepositions.
Flickr captions tended to have far more references
to physical objects (versus abstract objects) than the
generic text, according to the WordNet hierarchy.
Approximately 64% of the objects in Flickr were
physical (about 22% abstract and 14% unknown).
Whereas in the generic text, only 30% of the objects
were physical, 53% were abstract (17% unknown).
A third major difference between the corpora is
in terms of noun modifiers. In both corpora, nouns
tend not to have any modifiers, but modifiers are still
more prevalent in Flickr than in generic text. In par-
ticular, 60% of nouns in Flickr have zero modifiers,
but 70% of nouns in generic text have zero modi-
fiers. In Flickr, 30% of nouns have exactly one mod-
ifier, as compared to only 22% for generic text.
The breakdown of what those modifiers look like
is even more pronounced, even when restricted just
to physical objects (modifier types are obtained
through the bootstrapping process discussed in Sec-
tion 3.1). Almost 50% of nominal modifiers in the
Flickr data are color modifiers, whereas color ac-
counts for less than 5% of nominal modifiers in
generic text. In Flickr, 10% of modifiers talk about
beauty, in comparison to less than 5% in generic
text. On the other hand, less than 3% of modifiers
in Flickr reference ethnicity, as compared to almost
20% in generic text; and 20% of Flickr modifiers
reference size, versus 50% in generic text.
2.4 Annotating Visual Text
In order to obtain ground truth data, we rely on
crowdsourcing (via Amazon?s Mechanical Turk).
Each instance is an image, a paired caption, and a
highlighted noun phrase in that caption. The anno-
tation for this instance is a label of ?visual,? ?non-
visual? or ?error,? where the error category is re-
764
served for cases where the noun phrase segmenta-
tion was erroneous. Each worker is given five in-
stances to label and paid one cent per annotation.2
For a small amount of data (803 images contain-
ing 2339 instances), we obtained annotations from
three separate workers per instance to obtain higher
quality data. For a large amount of data (48k im-
ages), we obtained annotations from only a sin-
gle worker. Subsequently, we will refer to these
two data sets as the SMALL and LARGE data sets.
In both data sets, approximately 70% of the noun
phrases were visual, 28% were non-visual and 2%
were erroneous. For simplicity, we group erroneous
and non-visual for all learning and evaluation.
In the SMALL data set, the rate of disagreement
between annotators was relatively low. In 74% of the
annotations, there was no disagreement at all. We
reconciled the annotations using the quality manage-
ment technique of Ipeirotis et al (2010); only 14%
of the annotations need to be changed in order to ob-
tain a gold standard.
One immediate question raised in this process is
whether one needs to actually see the image to per-
form the annotation. In particular, if we expect an
NLP system to be able to classify noun phrases as
visual or non-visual, we need to know whether peo-
ple can do this task sans image. We therefore per-
formed the same annotation on the SMALL data set,
but where the workers were not shown the image.
Their task was to imagine an image for this caption
and then annotate the noun phrase based on whether
they thought it would be pictured or not. We ob-
tained three annotations as before and reconciled
them (Ipeirotis et al, 2010). The accuracy of this
reconciled version against the gold standard (pro-
duced by people who did see the image) was 91%.
This suggests that while people are able to do this
task with some reliability, seeing the image is very
important (recall that always guessing ?visual? leads
to an accuracy of 70%).
3 Visual Features from Raw Text
Our first goal is to attempt to obtain relatively large
knowledge bases of terms that are (predominantly)
visual. This is potentially useful in its own right
2Data available at http://hal3.name/dvt/, with direct links
back to the SBU Captioned Photo Dataset.
(for instance, in the context of search, to determine
which query terms are likely to be pictured). We
have explored two techniques for performing this
task, the first based on bootstrapping (Section 3.1)
and the second based on label propagation (Sec-
tion 3.2). We then use these lists to generate features
for a classifier that predicts whether a noun phrase?
in context?is visual or not (Section 4).
In addition, we consider the task of separating ad-
jectives into different visual categories (Section 3.3).
We have already used the results of this in Sec-
tion 2.3 to understand the differences between our
two corpora. It is also potentially useful for the
purpose of building new object detection systems or
even attribute detection systems, to get a vocabulary
of target detections.
3.1 Bootstrapping for Visual Text
In this section, we learn visual and non-visual nouns
and adjectives automatically based on bootstrapping
techniques. First, we construct a graph between ad-
jectives by computing distributional similarity (Tur-
ney and Pantel, 2010) between them. For comput-
ing distributional similarity between adjectives, each
target adjective is defined as a vector of nouns which
are modified by the target adjective. To be exact, we
use only those adjectives as modifiers which appear
adjacent to a noun (that is, in a JJ NN construction).
For example, in ?small red apple,? we consider only
red as a modifier for noun. We use Pointwise Mu-
tual Information (PMI) (Church and Hanks, 1989)
to weight the contexts, and select the top 1000 PMI
contexts for each adjective.3
Next, we apply cosine similarity to find the top
10 distributionally similar adjectives with respect to
each target adjective based on our large generic cor-
pus (Large-Data from Section 2.1). This creates a
graph with adjectives as nodes and cosine similarity
as weight on the edges. Analogously, we construct a
graph with nouns as nodes (here, adjectives are used
as contexts for nouns).
We then apply bootstrapping (Kozareva et al,
2008) on the noun and adjective graphs by select-
ing 10 seeds for visual and non-visual nouns and
adjectives (see Table 1). We use in-degree (sum of
weights of incoming edges) to compute the score for
3We are interested in descriptive adjectives, which ?typi-
cally ascribe to a noun a value of an attribute? (Miller, 1998).
765
Visual car house tree horse animal
nouns man table bottle
seeds woman computer
Non-visual idea bravery deceit trust
nouns dedication anger humour luck
seeds inflation honesty
Visual brown green wooden striped
adjectives orange rectangular furry
seeds shiny rusty feathered
Non-visual public original whole righteous
adjectives political personal intrinsic
seeds individual initial total
Table 1: Example seeds for bootstrapping.
each node that has connections with known (seeds)
or automatically labeled nodes, previously exploited
to learn hyponymy relations from the web (Kozareva
et al, 2008). Intuitively, in-degree captures the pop-
ularity of new instances among instances that have
already been identified as good instances. We learn
visual and non-visual words together (known as the
mutual exclusion principle in bootstrapping (The-
len and Riloff, 2002; McIntosh and Curran, 2008)):
each word (node) is assigned to only one class.
Moreover, after each iteration, we harmonically de-
crease the weight of the in-degree associated with
instances learned in later iterations. We added 25
new instances at each iteration and ran 500 iterations
of bootstrapping, yielding 11955 visual and 11978
non-visual nouns, and 7746 visual and 7464 non-
visual adjectives.
Based on manual inspection, the learned visual
and non-visual lists look great. In the future, we
would like to do a Mechanical Turk evaluation to
directly evaluate the visual and non-visual nouns
and adjectives. For now, we show the coverage of
these classes in the Flickr data-set: Visual nouns:
53.71%; Non-visual nouns: 14.25%; Visual ad-
jectives: 51.79%; Non-visual adjectives: 14.40%.
Overall, we find more visual nouns and adjectives
are covered in the Flickr data-set, which makes
sense, since the Flickr data-set is largely visual.
Second, we show the coverage of these classes
on the large text corpora (Large-Data from Sec-
tion 2.1): Visual nouns: 26.05%; Non-visual nouns:
41.16%; Visual adjectives: 20.02%; Non-visual ad-
Visual: attend, buy, clean, comb, cook, drink, eat,
fry, pack, paint, photograph, smash, spill, steal,
taste, tie, touch, watch, wear, wipe
Non-visual: achieve, admire, admit, advocate, al-
leviate, appreciate, arrange, criticize, eradicate,
induce, investigate, minimize, overcome, pro-
mote, protest, relieve, resolve, review, support,
tolerate
Table 2: Predicates that are visual and non-visual.
Visual: water, cotton, food, pumpkin, chicken,
ring, hair, mouth, meeting, kind, filter, game, oil,
show, tear, online, face, class, car
Non-visual: problem, poverty, pain, issue, use,
symptom, goal, effect, thought, government,
share, stress, work, risk, impact, concern, obsta-
cle, change, disease, dispute
Table 3: Learned visual/non-visual nouns.
jectives: 40.00%. Overall, more non-visual nouns
and adjectives cover text data, since Large-Data is
a non-visual data-set.
3.2 Label Propagation for Visual Text
To propagate visual labels, we construct a bipartite
graph between visually descriptive predicates and
their arguments. Let VP be the set of nodes that cor-
responds to predicates, and let VA be the set of nodes
that corresponds to arguments. To learn the visually
descriptive words, we set VP to 20 visually descrip-
tive predicates shown in the top of Table 2, and VA
to all nouns that appear in the object argument posi-
tion with respect to the seed predicates. We approx-
imate this by taking nouns on the right hand side
of the predicates within a window of 4 words using
the Web 1T Google N-gram data (Brants and Franz.,
2006). For edge weights, we use conditional prob-
abilities between predicates and arguments so that
w(p? a) := pr(a|p) and w(a? p) := pr(p|a).
In order to collectively induce the visually de-
scriptive words from this graph, we apply the graph
propagation algorithm of Velikovich et al (2010),
a variant of label propagation algorithms (Zhu and
Ghahramani, 2002) that has been shown to be ef-
fective for inducing a web-scale polarity lexicon
based on word co-occurrence statistics. This algo-
766
Color purple blue maroon beige green
Material plastic cotton wooden metallic silver
Shape circular square round rectangular triangular
Size small big tiny tall huge
Surface coarse smooth furry fluffy rough
Direction sideways north upward left down
Pattern striped dotted checked plaid quilted
Quality shiny rusty dirty burned glittery
Beauty beautiful cute pretty gorgeous lovely
Age young mature immature older senior
Ethnicity french asian american greek hispanic
Table 4: Attribute Classes with their seed values
rithm iteratively updates the semantic distance be-
tween each pair of nodes in the graph, then produces
a score for each node that represents how visually
descriptive each word is. To learn the words that
are not visually descriptive, we use the predicates
shown in the bottom of Table 2 as VP instead. Ta-
ble 3 shows the top ranked nouns that are visually
descriptive and not visually descriptive.
3.3 Bootstrapping Visual Adjectives
Our goal in this section is to automatically gener-
ate comprehensive lists of adjectives for different at-
tributes, such as color, material, shape, etc. To our
knowledge, this is the first significant effort of this
type for adjectives: most bootstrapping techniques
focus exclusively on nouns, although Almuhareb
and Poesio (2005) populated lists of attributes us-
ing web-based similarity measures. We found that
in some ways adjectives are easier than nouns, but
require slightly different representations.
One might conjecture that listing attributes by
hand is difficult. Colors names are well known to
be quite varied. For instance, our bootstrapping
approach is able to discover colors like ?grayish,?
?chestnut,? ?emerald,? and ?rufous? that would be
hard to list manually (the last is a reddish-brown
color, somewhat like rust). Although perhaps not
easy to create, the Wikipedia list of colors (http:
//en.wikipedia.org/wiki/List of colors) includes all of these
except ?grayish?. On the other hand, it includes
color terms that might be difficult to make use of as
colors, such as ?bisque,? ?bone? and ?bubbles? (the
last is a very light cyan), which might over-generate
hits. For shape, we find ?oblong,? ?hemispherical,?
?quadrangular? and, our favorite, ?convex?.
We use essentially the same bootstrapping process
as described earlier in Section 3.1, but on a slightly
different data representation. The only difference is
that instead of linking adjectives to their 10 most
similar neighbors, we link them only to 25 neigh-
bors to attempt to improve recall.
We begin with seeds for each attribute class from
Table 4. We conduct a manual evaluation to di-
rectly measure the quality of attribute classes. We
recruited 3 annotators and developed annotation
guidelines that instructed each recruiter to judge
whether a learned value belongs to an attribute class
or not. The annotators assigned ?1? if a learned
value belongs to a class, otherwise ?0?.
We conduct an Information Retrieval (IR) Style
human evaluation. Analogous to an IR evaluation,
here the total number of relevant values for attribute
classes can not be computed. Therefore, we assume
the correct output of several systems as the total re-
call which can be produced by any system. Now,
with the help of our 3 manual annotators, we obtain
the correct output of several systems from the total
output produced by these systems.
First, we measured the agreement on whether
each learned value belongs to a semantic class or
not. We computed ? to measure inter-annotator
agreement for each pair of annotators. We focus
our evaluation on 4 classes: age, beauty, color, and
direction; between Human 2 and Human 3 and be-
tween Human 1 and Human 3, the ? value was 0.48;
between Human 1 and Human 2 it was 0.45. These
numbers are somewhat lower than we would like,
but not terrible. If we evaluate the classes individu-
ally, we find that age has the lowest ?. If we remove
?age,? the pairwise ?s rise to 0.59, 0.57 and 0.55.
Second, we compute Precision (Pr), Recall (Rec)
and F-measure (F1) for different bootstrapping sys-
tems (based on the number of iterations and the
number of new words added in each iteration).
Two parameter settings performed consistently bet-
ter than others (10 iterations with 25 items, and 5 it-
erations with 50 items). The former system achieves
a precision/recall/F1 of 0.53, 0.71, 0.60 against Hu-
man 2; the latter achieves scores of 0.54, 0.72, 0.62.
4 Recognizing Visual Text
We train a logistic regression (aka maximum en-
tropy) model (Daume? III, 2004) to classify text as
visual or non-visual. The features we use fall into
767
the following categories: WORDS (the actual lexi-
cal items and stems); BIGRAMS (lexical bigrams);
SPELL (lexical features such as capitalization pat-
tern, and word prefixes and suffixes); WORDNET
(set of hypernyms according to WordNet); and
BOOTSTRAP (features derived from bootstrapping
or label propagation).
For each of these feature categories, we compute
features inside the phrase being categorized (e.g.,
?the car?), before the phrase (two words to the left)
and after the phrase (two words to the right). We
additionally add a feature that computes the num-
ber of words in a phrase, and a feature that com-
putes the position of the phrase in the caption (first
fifth through last fifth of the description). This leads
to seventeen feature templates that are computed for
each example. In the SMALL data set, there are 25k
features (10k non-singletons); in the LARGE data
set, there are 191k features (79k non-singletons).
To train models on the SMALL data set, we use
1500 instances as training, 200 as development and
the remaining 639 as test data. To train models on
the LARGE data set, we use 45000 instances as train-
ing and the remaining 4401 as development. We
always test on the 639 instances from the SMALL
data, since it has been redundantly annotated. The
development data is used only to choose the regular-
ization parameter for a Gaussian prior on the logis-
tic regression model; this parameter is chosen in the
range {0.01, 0.05, 0.1, 0.5, 1, 2, 4, 8, 16, 32, 64}.
Because of the imbalanced data problem, evalu-
ating according to accuracy is not appropriate for
this task. Even evaluating by precision/recall is not
appropriate, because a baseline system that guesses
that everything is visual obtains 100% recall and
70% precision. Due to these issues, we instead
evaluate according to the area under the ROC curve
(AUC). To check statistical significance, we com-
pute standard deviations using bootstrap resampling,
and consider there to be a significant difference if a
result falls outside of two standard deviations of the
baseline (95% confidence).
Figure 3 shows learning curves for the two data
sets. The SMALL data achieves an AUC score of
71.3 in the full data setting (1700 examples); the
LARGE data needs 12k examples to achieve similar
accuracy due to noise. However, with 49k examples,
we are able to achieve a AUC score of 75.3 using the
101 102 103 104 105
0.55
0.6
0.65
0.7
0.75
0.8
Figure 3: Learning curves for training on SMALL data
(blue solid) and LARGE data (black dashed). X-axis (in
log-scale) is number of training examples; Y-axis is AUC.
large data set. By pooling the data (and weighting
the small data), this boosts results to 76.1. The con-
fidence range on these data is approximately ?1.9,
meaning that this boost is likely not significant.
4.1 Using Image Features
As discussed previously, humans are only able to
achieve 90% accuracy on the visual/non-visual task
when they are not allowed to view the image.
This potentially upper-bounds the performance of a
learned system that can only look at text. In order to
attempt to overcome this, we augment our basic sys-
tem with a number of features computed from the
corresponding images. These features are derived
from the output of state of the art vision algorithms
to detect 121 different objects, stuff and scenes.
As our object detectors, we use standard state
of the art deformable part-based models (Felzen-
szwalb et al, 2010) for 89 common object cate-
gories, including: the original 20 objects from Pas-
cal, 49 objects from Object Bank (Li-Jia Li and Fei-
Fei, 2010), and 20 from Im2Text (Ordonez et al,
2011). We additionally use coarse image parsing
to estimate background elements in each database
image. Six possible background (stuff) categories
are considered: sky, water, grass, road, tree, and
building. For this we use detectors (Ordonez et
al., 2011) which compute color, texton, HoG (Dalal
and Triggs, 2005) and Geometric Context (Hoiem
et al, 2005) as input features to a sliding win-
dow based SVM classifier. These detectors are run
on all database images, creating a large pool of
background elements for retrieval. Finally, we ob-
768
Figure 4: (Left) Highest confidence flower detected in an
image; (Right) All detections in the same image.
tain scene descriptors for each image by comput-
ing scene classification scores for 26 common scene
categories, using the features, methods and training
data from the SUN dataset (Xiao et al, 2010).
Figure 4 shows an example image on which sev-
eral detectors have been run. From each image, we
extract the following features: which object detec-
tors fired; how many times they fired; the confidence
of the most-likely firing; the percentage of the image
(in pixels) that the bounding box corresponding to
this object occupies; and the percentage of the width
(and height) of the image that it occupies.
Unfortunately, object detection is a highly noisy
process. The right image in Figure 4 shows all de-
tections for that image, which includes, for instance,
a chair detection that spans nearly the entire image,
and a person detection in the bottom-right corner.
For an average image, if a single detector (e.g., the
flower detector) fires once, it actually fires 40 times
(?? = 1.8). Moreover, of the 120 detectors, on
an average image over 22 (?? = 5.6) of them fire
at least once (though certainly in an average image
only a few objects are actually present). Exacerbat-
ing this problem, although the confidence scores for
a single detector can be compared, the scores be-
tween different detectors are not at all comparable.
In order to attenuate this problem, we include dupli-
cate copies of all the above features restricted to the
most confident object for each object type.
On the SMALL data set, this adds 400 new fea-
CATEGORY POSITION AUC
Bootstrap Phrase 65.2
+ Spell Phrase 68.6
+ Image - 69.2
+ Words Phrase 70.0
+ Length - 69.8
+ Wordnet Phrase 70.4
+ Wordnet Before 70.6
+ Spell Before 71.8
+ Words Before 72.2
+ Bootstrap Before 72.4
+ Spell After 71.5
Table 5: Results of feature ablation on SMALL data set.
Best result is in bold; results that are not statistically sig-
nificantly worse are italicized.
tures (300 of which are non-singletons4); on the
LARGE data set, this adds 500 new features (480
non-singletons). Overall, the AUC scores trained on
the small data set increase from 71.3 to 73.9 (a sig-
nificant improvement). On the large data set, the in-
crease is only from 76.1 to 76.8, which is not likely
to be significant. In general, the improvement ob-
tained by adding image features is most pronounced
in the setting of small training data, perhaps because
these features are more generic than the highly lexi-
calized features used in the textual model. But once
there is a substantial amount of text data, the noisy
image features become less useful.
4.2 Feature Ablations
In order to ascertain the degree to which each feature
template is useful, we perform an ablation study. We
first perform feature selection at the template level
using the information gain criteria, and then train
models using the corresponding subset of features.
The results on the SMALL data set are shown in
Table 5. Here, the bootstrapping features computed
on words within the phrase to be classified were
judged as the most useful, followed by spelling fea-
tures. Image features were judged third most use-
ful. In general, features in the phrase were most use-
ful (not surprisingly), and then features before the
phrase (presumably to give context, for instance as
in ?out of the window?). Features from after the
phrase were not useful.
4Non-singleton features appear more than once in the data.
769
CATEGORY POSITION AUC
Words Phrase 74.7
+ Image - 74.4
+ Bootstrap Phrase 74.3
+ Spell Phrase 75.3
+ Length - 74.7
+ Words Before 76.2
+ Wordnet Phrase 76.1
+ Spell After 76.0
+ Spell Before 76.8
+ Wordnet Before 77.0
+ Wordnet After 75.6
Table 6: Results of feature ablation on LARGE data set.
Corresponding results on the LARGE data set are
shown in Table 6. Note that the order of features
selected is different because the training data is dif-
ferent. Here, the most useful features are simply the
words in the phrase to be classified, which alone al-
ready gives an AUC score of 74.7, only a few points
off from the best performance of 77.0 once image
features, bootstrap features and spelling features are
added. As before, these features are rated as very
useful for classification performance.
Finally, we consider the effect of using Bootstrap-
based features or label-propagation-based features.
In all the above experiments, the features used
are based on the union of word lists created by
these two techniques. We perform three experi-
ments. Beginning with the system that contains all
features (SMALL=73.9, LARGE=76.8), we first re-
move the bootstrap-based features (SMALL?71.8,
LARGE?75.5) or remove the label-propagation-
based features (SMALL?71.2, LARGE?74.9) or
remove both (SMALL?70.7, LARGE?74.2). From
these results, we can see that these techniques are
useful, but somewhat redundant: if you had to
choose one, you should choose label-propagation.
5 Discussion
As connections between language and vision be-
come stronger, for instance in the contexts of ob-
ject detection (Hou and Zhang, 2007; Kim and Tor-
ralba, 2009; Sivic et al, 2008; Alexe et al, 2010;
Gu et al, 2009), attribute detection (Ferrari and Zis-
serman, 2007; Farhadi et al, 2009; Kumar et al,
2009; Berg et al, 2010), visual phrases (Farhadi and
Sadeghi, 2011), and automatic caption generation
(Farhadi et al, 2010; Feng and Lapata, 2010; Or-
donez et al, 2011; Kulkarni et al, 2011; Yang et
al., 2011; Li et al, 2011; Mitchell et al, 2012), it
becomes increasingly important to understand, and
to be able to detect, text that actually refers to ob-
served phenomena. Our results suggest that while
this is a hard problem, it is possible to leverage large
text resources and state-of-the-art computer vision
algorithms to address it with high accuracy.
Acknowledgments
T.L. Berg and K. Yamaguchi were supported in part
by NSF Faculty Early Career Development (CA-
REER) Award #1054133; A.C. Berg and Y. Choi
were partially supported by the Stony Brook Uni-
versity Office of the Vice President for Research; H.
Daume? III and A. Goyal were partially supported by
NSF Award IIS-1139909; all authors were partially
supported by a 2011 JHU Summer Workshop.
References
B. Alexe, T. Deselaers, and V. Ferrari. 2010. What is an
object? In Computer Vision and Pattern Recognition
(CVPR), 2010 IEEE Conference on, pages 73 ?80.
A. Almuhareb and M. Poesio. 2005. Finding concept at-
tributes in the web. In Corpus Linguistics Conference.
Tamara L. Berg, Alexander C. Berg, and Jonathan Shih.
2010. Automatic attribute discovery and characteriza-
tion from noisy web data. In European Conference on
Computer Vision (ECCV).
Thorsten Brants and Alex Franz. 2006. Web 1t 5-gram
version 1. In Linguistic Data Consortium, ISBN: 1-
58563-397-6, Philadelphia.
K. Church and P. Hanks. 1989. Word Associa-
tion Norms, Mutual Information and Lexicography.
In Proceedings of ACL, pages 76?83, Vancouver,
Canada, June.
N. Dalal and B. Triggs. 2005. Histograms of oriented
gradients for human detection. In CVPR.
Hal Daume? III and Daniel Marcu. 2005. Learning as
search optimization: Approximate large margin meth-
ods for structured prediction. In Proceedings of the In-
ternational Conference on Machine Learning (ICML).
Hal Daume? III. 2004. Notes on CG and LM-BFGS
optimization of logistic regression. Paper available
at http://pub.hal3.name/#daume04cg-bfgs, implementation
available at http://hal3.name/megam/, August.
770
M. Everingham, L. Van Gool, C. K. I. Williams,
J. Winn, and A. Zisserman. 2010. The PASCAL
Visual Object Classes Challenge 2010 (VOC2010)
Results. http://www.pascal-network.org/challenges/VOC/
voc2010/workshop/index.html.
Ali Farhadi and Amin Sadeghi. 2011. Recognition us-
ing visual phrases. In Computer Vision and Pattern
Recognition (CVPR).
A. Farhadi, I. Endres, D. Hoiem, and D.A. Forsyth. 2009.
Describing objects by their attributes. In Computer
Vision and Pattern Recognition (CVPR).
A. Farhadi, M. Hejrati, M.A. Sadeghi, P. Young,
C. Rashtchian1, J. Hockenmaier, and D.A. Forsyth.
2010. Every picture tells a story: Generating sentences
from images. In ECCV.
P. F. Felzenszwalb, R. B. Girshick, and D. McAllester.
2010. Discriminatively trained deformable part
models, release 4. http://people.cs.uchicago.edu/?pff/
latent-release4/.
Y. Feng and M. Lapata. 2010. How many words is a
picture worth? automatic caption generation for news
images. In ACL.
V. Ferrari and A. Zisserman. 2007. Learning visual at-
tributes. In Advances in Neural Information Process-
ing Systems (NIPS).
D. Graff. 2003. English Gigaword. Linguistic Data Con-
sortium, Philadelphia, PA, January.
Chunhui Gu, J.J. Lim, P. Arbelaez, and J. Malik. 2009.
Recognition using regions. In Computer Vision and
Pattern Recognition, 2009. CVPR 2009. IEEE Confer-
ence on, pages 1030 ?1037.
Derek Hoiem, Alexei A. Efros, and Martial Hebert.
2005. Geometric context from a single image. In
ICCV.
Xiaodi Hou and Liqing Zhang. 2007. Saliency detection:
A spectral residual approach. In Computer Vision and
Pattern Recognition, 2007. CVPR ?07. IEEE Confer-
ence on, pages 1 ?8.
P. Ipeirotis, F. Provost, and J. Wang. 2010. Quality man-
agement on amazon mechanical turk. In Proceedings
of the Second Human Computation Workshop (KDD-
HCOMP).
Gunhee Kim and Antonio Torralba. 2009. Unsupervised
Detection of Regions of Interest using Iterative Link
Analysis. In Annual Conference on Neural Informa-
tion Processing Systems (NIPS 2009).
Zornitsa Kozareva, Ellen Riloff, and Eduard Hovy. 2008.
Semantic class learning from the web with hyponym
pattern linkage graphs. In Proceedings of ACL-08:
HLT, pages 1048?1056, Columbus, Ohio, June. As-
sociation for Computational Linguistics.
G. Kulkarni, V. Premraj, S. Dhar, S. Li, Y. Choi, A. C
Berg, and T. L Berg. 2011. Babytalk: Understanding
and generating simple image descriptions. In CVPR.
N. Kumar, A.C. Berg, P. Belhumeur, and S.K. Nayar.
2009. Attribute and simile classifiers for face verifi-
cation. In ICCV.
Siming Li, Girish Kulkarni, Tamara L. Berg, Alexan-
der C. Berg, and Yejin Choi. 2011. Composing sim-
ple image descriptions using web-scale n-grams. In
CONLL.
Eric P. Xing Li-Jia Li, Hao Su and Li Fei-Fei. 2010. Ob-
ject bank: A high-level image representation for scene
classification and semantic feature sparsification. In
NIPS.
Tara McIntosh and James R Curran. 2008. Weighted
mutual exclusion bootstrapping for domain indepen-
dent lexicon and template acquisition. In Proceedings
of the Australasian Language Technology Association
Workshop 2008, pages 97?105, December.
K.J. Miller. 1998. Modifiers in WordNet. In C. Fell-
baum, editor, WordNet, chapter 2. MIT Press.
Margaret Mitchell, Jesse Dodge, Amit Goyal, Kota Ya-
maguchi, Karl Stratos, Xufeng Han, Alyssa Mensch,
Alex Berg, Tamara Berg, and Hal Daume? III. 2012.
Midge: Generating image descriptions from computer
vision detections. Proceedings of EACL 2012.
Vicente Ordonez, Girish Kulkarni, and Tamara L. Berg.
2011. Im2Text: Describing Images Using 1 Million
Captioned Photographs. In NIPS.
Cyrus Rashtchian, Peter Young, Micah Hodosh, and Ju-
lia Hockenmaier. 2010. Collecting image annotations
using amazon?s mechanical turk. In Proceedings of
the NAACL HLT 2010 Workshop on Creating Speech
and Language Data with Amazon?s Mechanical Turk.
Association for Computational Linguistics.
H. Schmid. 1995. Improvements in part?of?speech tag-
ging with an application to german. In Proceedings of
the EACL SIGDAT Workshop.
J. Sivic, B.C. Russell, A. Zisserman, W.T. Freeman, and
A.A. Efros. 2008. Unsupervised discovery of visual
object class hierarchies. In Computer Vision and Pat-
tern Recognition, 2008. CVPR 2008. IEEE Conference
on, pages 1 ?8.
M. Thelen and E. Riloff. 2002. A Bootstrapping Method
for Learning Semantic Lexicons Using Extraction Pat-
tern Contexts. In Proceedings of the Empirical Meth-
ods in Natural Language Processing, pages 214?221.
Peter D. Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of seman-
tics. Journal of Artificial Intelligence Research (JAIR),
37:141.
Leonid Velikovich, Sasha Blair-Goldensohn, Kerry Han-
nan, and Ryan McDonald. 2010. The viability of web-
derived polarity lexicons. In Human Language Tech-
nologies: The 2010 Annual Conference of the North
771
American Chapter of the Association for Computa-
tional Linguistics. Association for Computational Lin-
guistics.
J. Xiao, J. Hays, K. Ehinger, A. Oliva, and A. Torralba.
2010. Sun database: Large-scale scene recognition
from abbey to zoo. In CVPR.
Yezhou Yang, Ching Lik Teo, Hal Daume? III, and Yian-
nis Aloimonos. 2011. Corpus-guided sentence gener-
ation of natural images. In EMNLP.
Xiaojin Zhu and Zoubin Ghahramani. 2002. Learn-
ing from labeled and unlabeled data with label prop-
agation. In Technical Report CMU-CALD-02-107.
CarnegieMellon University.
772
Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, pages 17?25,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Toward Plot Units: Automatic Affect State Analysis
Amit Goyal and Ellen Riloff and Hal Daume III and Nathan Gilbert
School of Computing
University of Utah
Salt Lake City, UT 84112
{amitg,riloff,hal,ngilbert}@cs.utah.edu
Abstract
We present a system called AESOP that au-
tomatically produces affect states associated
with characters in a story. This research repre-
sents a first step toward the automatic genera-
tion of plot unit structures from text. AESOP
incorporates several existing sentiment analy-
sis tools and lexicons to evaluate the effective-
ness of current sentiment technology on this
task. AESOP also includes two novel compo-
nents: a method for acquiring patient polar-
ity verbs, which impart negative affect on their
patients, and affect projection rules to propa-
gate affect tags from surrounding words onto
the characters in the story. We evaluate AE-
SOP on a small collection of fables.
1 Introduction
In the 1980s, plot units (Lehnert, 1981) were pro-
posed as a knowledge structure for representing nar-
rative stories and generating summaries. Plot units
are fundamentally different from the story represen-
tations that preceded them because they focus on the
emotional states and tensions between characters as
the driving force behind interesting plots and cohe-
sive stories. Plot units were used in narrative sum-
marization studies, both in computer science and
psychology (Lehnert et al, 1981), but the compu-
tational models of plot units relied on tremendous
amounts of manual knowledge engineering.
Given the recent swell of activity in automated
methods for sentiment analysis, we embarked on a
project to see whether current techniques could auto-
matically detect the affect states needed for plot unit
analysis. Plot units are complex structures that in-
clude affect states, causal links, and cross-character
links, and generating complete plot unit structures is
beyond the scope of this work. As an initial step to-
ward the long-term goal of automatically generating
plot units, we began by creating a system to automat-
ically identify the affect states associated with char-
acters. An affect state represents the emotional state
of a character, based on their perspective of events
in the story. Plots units include three types of af-
fect states: positive (+) states, negative (-) states, and
mental (M) states that have neutral emotion (these
are often associated with plans and goals).
Our system, called AESOP, pulls together a va-
riety of existing technologies in sentiment analy-
sis to automatically identify words and phrases that
have positive/negative polarity or that correspond
to speech acts (for mental states). However, we
needed to develop a method to automatically map
these affect tags onto characters in the story.1 To
address this issue, we created affect projection rules
that propagate affect tags from words and phrases to
characters in the story via syntactic relations.
During the course of our research, we came to ap-
preciate that affect states, of the type required for
plot units, can represent much more than just di-
rect expressions of emotion. A common phenom-
ena are affect states that result from a character be-
ing acted upon in a positive or negative way. For
example, ?the cat ate the mouse? produces a pos-
itive affect state for the cat and a negative affect
1This is somewhat analogous to, but not exactly the same as,
associating opinion words with their targets or topics (Kim and
Hovy, 2006; Stoyanov and Cardie, 2008).
17
The Father and His Sons
(s1) A father had a family of sons who were perpetually
quarreling among themselves. (s2) When he failed to
heal their disputes by his exhortations, he determined to
give them a practical illustration of the evils of disunion;
and for this purpose he one day told them to bring him a
bundle of sticks. (s3) When they had done so, he placed
the faggot into the hands of each of them in succession,
and ordered them to break it in pieces. (s4) They tried
with all their strength, and were not able to do it. (s5) He
next opened the faggot, took the sticks separately, one by
one, and again put them into his sons? hands, upon which
they broke them easily. (s6) He then addressed them in
these words: ?My sons, if you are of one mind, and unite
to assist each other, you will be as this faggot, uninjured
by all the attempts of your enemies; but if you are divided
among yourselves, you will be broken as easily as these
sticks.?
(a) ?Father and Sons? Fable
Father Sons
(quarreling)a1
(stop quarreling)a3
(annoyed)a2
(exhortations)a4
(exhortations fail)a5
m
m
a
(teach lesson)a6
m
(get sticks & break)a7
m
(get sticks & break)a8
(cannot break sticks)a9
a
(cannot break sticks)a10
a
(bundle & break)a11
(bundle & break)a12
(break sticks)a13
a
(break sticks)a14
a
m
a
shared
request
request
shared
shared
s2
s2
s2
s2
s2
s2
s4
s5
s5
s1
s2
s4
s5
s5
(lesson succeeds)a15s5
(b) Plot Unit Analysis for ?Father and Sons? Fable
state for the mouse because obtaining food is good
but being eaten is bad. This type of world knowl-
edge is difficult to obtain, yet essential for plot unit
analysis. In AESOP, we use corpus statistics to au-
tomatically learn a set of negative patient polarity
verbs which impart a negative polarity on their pa-
tient (e.g., eaten, killed, injured, fired). To acquire
these verbs, we queried a large corpus with patterns
to identify verbs that frequently occur with agents
who stereotypically have evil intent.
We evaulate our complete system on a set of AE-
SOP?s fables. In this paper, we also explain and cat-
egorize different types of situations that can produce
affect states, several of which cannot be automati-
cally recognized by existing sentiment analysis tech-
nology. We hope that one contribution of our work
will be to create a better awareness of, and apprecia-
tion for, the different types of language understand-
ing mechanisms that will ultimately be necessary for
comprehensive affect state analysis.
2 Overview of Plot Units
Narratives can often be understood in terms of the
emotional reactions and affect states of the char-
acters therein. The plot unit formalism (Lehnert,
1981) provides a representational mechanism for af-
fect states and the relationships between them. Plot
unit structures can be used for tasks such as narrative
summarization and question answering.
Plot unit structures consist of affect states for each
character in a narrative, and links explaining the re-
lationships between these affect states. The affect
states themselves each have a type: (+) for positive
states, (-) for negative states, and (M) for mental
states (with neutral affect). Although affect states
are not events per se, events often trigger affect
states. If an event affects multiple characters, it can
trigger multiple affect states, one for each character.
Affect states are further connected by causal links,
which explain how the narrative hangs together.
These include motivations (m), actualizations (a),
terminations (t) and equivalences (e). Causal links
exist between affect states for the same character.
Cross-character links explain how single events af-
fect two characters. For instance, if one character
requests something of the other, this is an M-to-M
link, since it spans a shared mental affect for both
characters. Other speech acts can be represented as
M to + (promise) or M to - (threat).
To get a better feeling of the plot unit represen-
tation, a short fable, ?The Father and His Sons,? is
shown in Figure 1(a) and our annotation of its plot
unit structure is shown in Figure 1(b). In this fa-
ble, there are two characters (the ?Father? and the
?Sons?) who go through a series of affect states, de-
picted chronologically in the two columns.
In this example, the first affect state is a negative
state for the sons, who are quarreling (a1). This state
is shared by the father (via a cross-character link)
who has a negative annoyance state (a2). The fa-
ther then decides that he wants to stop the sons from
quarreling, which is a mental event (a3). The causal
link from a2 to a3 with an m label indicates a ?mo-
tivation.? His first attempt is by exhortations (a4).
18
This produces an M (a3) linked to an M (a4) with
a m (motivation) link, which represents subgoaling.
The father?s overall goal is to stop the quarreling
(a3) and in order to do so, he creates a subgoal of
exhorting the sons to stop (a4). The exhortations
fail, which produces a negative state (a5) for the fa-
ther. The a causal link indicates an ?actualization?,
representing the failure of the plan (a4).
The failure of the father?s exhortations leads to a
new subgoal: to teach the sons a lesson (a6). The m
link from a5 to a6 is an example of ?enablement.?
At a high level, this subgoal has two parts, indicated
by the two gray regions (a7 ? a10 and a11 ? a14).
The first gray region begins with a cross-character
link (M to M), which indicates a request (in this case,
to break a bundle of sticks). The sons fail at this,
which upsets them (a9) but pleases the father (a10).
The second gray region depicts the second part of
the father?s subgoal; he makes a second request (a11
to a12) to separate the bundle and break the sticks,
which the sons successfully do, making them happy
(a13) and the father happy (a14). This latter struc-
ture (the second gray region) is an HONORED RE-
QUEST plot unit. At the end, the father?s plan suc-
ceeds (a15) which is an actualization (a link) of his
goal to teach the sons a lesson (a6).
In this example, as well as the others that we an-
notated in our gold standard, (see Section 5.1), we
annotated conservatively. In particular, in reading
the story, we may assume that the father?s origi-
nal plan of stopping the son?s quarrelling also suc-
ceeded. However, this is not mentioned in the story
and therefore we chose not to represent it. It is also
important to note that plot unit representations can
have t (termination) and e (equivalence) links that
point backwards in time, but they do not occur in
the Father and Sons fable.
3 Where Do Affect States Come From?
We began this research with the hope that recent re-
search in sentiment analysis would supply us with
effective tools to recognize affect states. However,
we soon realized that affect states, as required for
plot unit analysis, go well beyond the notions of pos-
itive/negative polarity and private states that have
been studied in recent sentiment analysis work. In
this section, we explain the wide variety of situa-
tions that can produce an affect state, based on our
observations in working with fables. Most likely, an
even wider variety of situations could produce affect
states in other text genres.
3.1 Direct Expressions of Emotion
Plot units can include affect states that correspond to
explicit expressions of positive/negative emotional
states, as has been studied in the realm of sentiment
analysis. For example, ?Max was disappointed?
produces a negative affect state for Max, and ?Max
was pleased? produces a positive affect state for
Max. However, the affect must relate to an event that
occurs in the story?s plot. For example, a hypotheti-
cal expression of emotion would not yield an affect
state (e.g., ?if the rain stops, she will be pleased?).
3.2 Situational Affect States
Positive and negative affect states also frequently
represent good and bad situational states that char-
acters find themselves in. These states do not rep-
resent emotion, but indicate whether a situation is
good or bad for a character based on world knowl-
edge. For example, ?Wolf, who had a bone stuck
in his throat, ...? produces a negative affect state
for the wolf. Similarly, ?The Old Woman recovered
her sight...? produces a positive affect state. Senti-
ment analysis is not sufficient to generate these af-
fect states. Sometimes, however, a direct expression
of emotion will also be present (e.g., ?Wolf was un-
happy because he had a bone stuck...?), providing
redundancy and multiple opportunities to recognize
the correct affect state for a character.
Situational affect states are common and often
motivate plans and goals that are central to the plot.
3.3 Plans and Goals
Plans and goals are another common reason for
affect states. The existence of a plan or goal is
usually represented as a mental state (M). Plans and
goals can be difficult to detect automatically. A
story may reveal that a character has a plan or goal
in a variety of ways, such as:
Direct expressions of plans/goals: a plan or goal
may be explicitly stated (e.g., ?the lion wanted to
find food?). In this case, a mental state (M) should
19
be generated.
Speech acts: a plan or goal may be revealed
through a speech act between characters. For
example, ?the wolf asked an eagle to extract the
bone? is a directive speech act that indicates the
wolf?s plan to resolve its negative state (having a
bone stuck). This example illustrates how a negative
state (bone stuck) can motivate a mental state (plan).
When a speech act involves multiple characters, it
produces multiple mental states. For example, a
mental state should also be produced for the eagle,
because it now has a plan to help the wolf (by virtue
of being asked).
Inferred plans/goals: plans and goals sometimes
must be inferred from actions. For example, ?the
lion hunted deer? reveals the lion?s plan to obtain
food. Similarly, the serpent spat poison into the
man?s water? implies that the serpent had a plan to
kill the man.
Plans and goals also produce positive/negative af-
fect states when they succeed/fail. For example, if
the eagle successfully extracts the bone from the
wolf?s throat, then both the wolf and the eagle will
have positive affect states, because both were suc-
cessful in their respective goals. A directive speech
act between two characters coupled with positive af-
fect states for both characters is a common plot unit
structure called an HONORED REQUEST, depicted
by the second gray block shown in Fig.1(b).
The affect state for a character is always with
respect to its view of the situation. For example,
consider: ?The owl besought a grasshopper to
stop chirping. The grasshopper refused to desist,
and chirped louder and louder.? Both the owl and
the grasshopper have M affect states representing
the request from the owl to the grasshopper (i.e.,
the owl?s plan to stop the chirping is to ask the
grasshopper to knock it off). The grasshopper
refuses the request, so a negative affect state is
produced for the owl, indicating that its plan failed.
However, a positive affect state is produced for
the grasshopper, because its goal was to continue
chirping which was accomplished by refusing the
request. This scenario is also a common plot unit
structure called a DENIED REQUEST.
3.4 Patient Role Affect States
Many affect states come directly from events. In
particular, when a character is acted upon (the theme
or patient of an event), a positive or negative affect
state often results for the character. These affect
states reflect world knowledge about what situations
are good and bad. For example:
Negative patient roles: killed X, ate X, chased X,
captured X, fired X, tortured X
Positive patient roles: rescued X, fed X, adopted X,
housed X, protected X, rewarded X
For example, ?a man captured a bear? indicates a
negative state for the bear. Overall, this sentence
would generate a SUCCESS plot unit consisting of
an M state and a + state for the man (with an actual-
ization a causal link between them representing the
plan?s success) and a - state for the bear (as a cross-
character link indicating that what was good for the
man was bad for the bear). A tremendous amount of
world knowledge is needed to generate these states
from such a seemingly simple sentence. Similarly,
if a character is rescued, fed, or adopted, then a + af-
fect state should be produced for the character based
on knowledge that these events are desirable. We
are not aware of existing resources that can automat-
ically identify affect polarity with respect to event
roles. In Section 4.1.2, we explain how we automat-
ically acquire Patient Polarity Verbs from a corpus
to identify some of these affect states.
4 AESOP: Automatic Affect State Analysis
We created a system, called AESOP, to try to auto-
matically identify the types of affect states that are
required for plot unit analysis. AESOP incorporates
existing resources for sentiment analysis and speech
act recognition, and includes two novel components:
patient polarity verbs, which we automatically gen-
erate using corpus statistics, and affect projection
rules, which automatically project and infer affect
labels via syntactic relations.
AESOP produces affect states in a 3-step process.
First, AESOP labels individual words and phrases
with an M, +, or - affect tag. Second, it identi-
fies all references to the two main characters of the
20
story. Third, AESOP applies affect projection rules
to propagate affect states onto the characters, and in
some cases, to infer new affect states.
4.1 Step 1: Assigning Affect Tags to Words
4.1.1 Sentiment Analysis Resources
AESOP incorporates several existing sentiment
analysis resources to recognize affect states associ-
ated with emotions and speech acts.
? OpinionFinder2 (Wilson et al, 2005) (Version
1.4) is used to identify all three types of states. We
use the +/- labels assigned by its contextual polar-
ity classifier (Wilson, 2005) to create +/- affect tags.
The MPQASD tags produced by its Direct Subjective
and Speech Event Identifier (Choi et al, 2006) are
used as M affect tags.
? Subjectivity Lexicon3 (Wilson, 2005): The pos-
itive/negative words in this list are assigned +/- af-
fect tags, when they occur with the designated part-
of-speech (POS).
? Semantic Orientation Lexicon4 (Takamura et
al., 2005): The positive/negative words in this list
are assigned +/- affect tags, when they occur with
the designated part-of-speech.
? A list of 228 speech act verbs compiled from
(Wierzbicka, 1987)5, which are used for M states.
4.1.2 Patient Polarity Verbs
As we discussed in Section 3.4, existing resources
are not sufficient to identify affect states that arise
from a character being acted upon. Sentiment lexi-
cons, for example, assign polarity to verbs irrespec-
tive of their agents or patients. To fill this gap,
we tried to automatically acquire verbs that have a
strong patient polarity (i.e., the patient will be in a
good or bad state by virtue of being acted upon).
We used corpus statistics to identify verbs that
frequently occur with agents who typically have
evil (negative) or charitable (positive) intent. First,
we identified 40 words that are stereotypically evil
agents, such as monster, villain, terrorist, and mur-
derer, and 40 words that are stereotypically charita-
ble agents, such as hero, angel, benefactor, and res-
cuer. Next, we searched the google Web 1T 5-gram
2http://www.cs.pitt.edu/mpqa/opinionfinderrelease/
3http://www.cs.pitt.edu/mpqa/lexiconrelease/collectinfo1.html
4http://www.lr.pi.titech.ac.jp/?takamura/pndic en.html
5http://openlibrary.org/b/OL2413134M/English speech act verbs
corpus6 using patterns designed to identify verbs
that co-occur with these words as agents. For each
agent term, we applied the pattern ?*ed by [a,an,the]
AGENT? and extracted the list of matching verbs.7
Next, we rank the extracted verbs by computing
the ratio between the frequency of the verb with a
negative agent versus a positive agent. If this ratio
is > 1, then we save the verb as a negative patient
polarity verb (i.e., it imparts negative polarity to its
patient). This process produced 408 negative patient
polarity verbs, most of which seemed clearly neg-
ative for the patient. Table 1 shows the top 20 ex-
tracted verbs. We also tried to identify positive pa-
tient polarity verbs using a positive-to-negative ra-
tio, but the extracted verbs were often neutral for the
patient, so we did not use them.
scammed damaged disrupted ripped
raided corrupted hindered crippled
slammed chased undermined possesed
dogged tainted grounded levied
patched victimized posessed bothered
Table 1: Top 20 negative patient polarity verbs
4.2 Step 2: Identifying the Characters
The problem of coreference resolution in fables
is somewhat different than for other genres, pri-
marily because characters are often animals (e.g.,
?he?=?owl?). So we hand-crafted a simple rule-
based coreference system. For the sake of this task,
we made two assumptions: (1) There are only two
characters per fable, and (2) Both characters are
mentioned in the fable?s title.
We then apply heuristics to determine number and
gender for the characters based on word lists, Word-
Net (Miller, 1990) and POS tags. If no determina-
tion of a character?s gender or number can be made
from these resources, a process of elimination is em-
ployed. Given the two character assumption, if one
character is known to be male, but there are female
pronouns in the fable, then the other character is as-
sumed to be female. The same is done for number
agreement. Finally, if there is only one character be-
tween a pronoun and the beginning of a document,
6http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalogId=LDC2006T13
7The corpus is not POS tagged so there is no guarantee these
will be verbs, but they usually are in this construction.
21
the pronoun is assumed to corefer with that char-
acter. The character then assumes the gender and
number of that pronoun. Lastly, WordNet is used
to obtain a small set of non-pronominal, non-string-
match resolutions by exploiting hypernym relations,
for instance, linking Peasant with the man.
4.3 Step 3: Affect Projection
Our goal is to produce affect states for each char-
acter in the story. Therefore every affect tag needs
to be attributed to a character, or discarded. Since
plots typically revolve around actions, we used the
verbs as the basis for projecting affect tags onto the
characters. In some cases, we also spawn new affect
tags associated with mental states to indicate that an
action is likely the manifestation of a plan.
We developed 6 types of affect projection rules
that orchestrate how affect tags are assigned to the
characters based on verb argument structure. We
use the Sundance shallow parsing toolkit (Riloff and
Phillips, 2004) to generate a syntactic analysis of
each sentence, including syntactic chunking, clause
segmentation, and active/passive voice recognition.
We normalize the verb phrases (VPs) with respect to
voice (i.e., we transform the passive voice construc-
tions into an active voice equivalent) to simplify our
rules. We then make the assumption that the Subject
of the VP is its AGENT and the Direct Object of the
VP is its PATIENT.8 The affect projection rules only
project affect states onto AGENTS and PATIENTS
that correspond to a character in the story. The five
types of rules are described below.
1. AGENT VP : This case applies when the VP
has no PATIENT, or a PATIENT that is not a char-
acter in the story, or the PATIENT corefers with
the AGENT. All affect tags associated with the VP
are projected onto the AGENT. For example, ?Mary
laughed (+)? projects a positive affect state onto
Mary.
2. VP PATIENT9: All affect tags associated with
the VP are projected onto the PATIENT, unless both
M and +/- tags exist, in which case only the +/- tags
are projected. For example, ?loved (+) the cat?,
projects a positive affect state onto the cat.
8We are not actually doing thematic role recognition, so this
will not always be correct, but it is a reasonable approximation.
9Agent is missing or not a character.
3. AGENT VP PATIENT: This case applies when
the AGENT and PATIENT refer to different char-
acters. All affect tags associated with the VP are
projected onto the PATIENT, unless both M and +/-
tags exist, in which case only the +/- tags are pro-
jected (as in Rule #2). If the VP has an M tag, then
we also project an M tag onto the AGENT (repre-
senting a shared, cross-character mental state). If
the VP has a +/- tag, then we project a + tag onto
the agent (as an inference that the AGENT accom-
plished some action).
4. AGENT VERB1 to VERB2 PATIENT. We di-
vide this into two cases: (a) If the agent and patient
refer to the same character, then Rule #1 is applied
(e.g., ?Bo decided to teach himself...?). (b) If the
agent and patient are different, we apply Rule #1 to
VERB1 to agent and Rule #2 to VERB2. If no af-
fect tags are assigned to either verb, then we create
an M affect state for the agent (assuming that the VP
represents some sort of plan).
5. If a noun phrase refers to a character and in-
cludes a modifying adjective with an affect tag, then
the affect is mapped onto the character. For exam-
ple, ?the happy (+) fox?.
Finally, if an adverb or adjectival phrase (e.g.,
predicate adjective) has an affect tag, then that affect
tag is mapped onto the preceding VP and the projec-
tion rules above are applied. For all of the rules, if
a clause contains a negation word, then we flip the
polarity of all words in that clause. Our negation list
contains: no, not, never, fail, failed, fails, don?t, and
didn?t.
5 Evaluation
5.1 Data Set
Plot unit analysis of ordinary text is enormously
complex ? even the idea of manually creating gold
standard annotations seemed like a monumental
task. So we began our exploration with simpler and
more constrained texts that seemed particularly ap-
propriate for plot unit analysis: fables. Fables have
two desirable attributes: (1) they have a small cast
of characters, and (2) they typically revolve around
a moral, which is exemplified by a short and concise
plot. Even so, fables are challenging for NLP due to
anthropomorphic characters, flowery language, and
sometimes archaic vocabulary.
22
State M (66) + (52) - (39) All (157)
System R P F R P F R P F R P F
Bsent baseline .65 .10 .17 .52 .08 .14 .74 .06 .11 .63 .08 .14
Bclause baseline .48 .28 .35 .44 .22 .29 .69 .17 .27 .52 .22 .31
All 4 resources (w/proj. rules) .48 .43 .45 .23 .39 .29 .23 .41 .29 .34 .41 .37
OpinionFinder .36 .42 .39 .00 .00 .00 .00 .00 .00 .15 .35 .21
Subjectivity Lexicon .45 .43 .44 .23 .35 .28 .21 .44 .28 .32 .41 .36
Semantic Dictionary .42 .45 .43 .00 .00 .00 .00 .00 .00 .18 .45 .26
Semantic Orientation Lexicon .41 .43 .42 .17 .53 .26 .08 .43 .13 .25 .45 .32
PPV Lexicon .41 .42 .41 .02 .17 .04 .21 .73 .33 .23 .44 .30
AESOP (All 4 + PPV) .48 .40 .44 .25 .36 .30 .33 .46 .38 .37 .40 .38
Table 2: Evaluation results for 2 baselines, 4 sentiment analysis resources with projection rules, and our PPV lexicon
with projection rules. (The # in parentheses is the number of occurrences of that state in the gold standard).
We collected 34 fables from an Aesop?s Fables
web site10, choosing fables that have a true plot
(some only contain quotes) and exactly two charac-
ters. We divided them into a development set of 11
stories, a tuning set of 8 stories, and a test set of 15
stories. The Father and Sons story from Figure 1(a)
is an example from our set.
Creating a gold standard was itself a substantial
undertaking. Plot units are complex structures, and
training non-experts to produce them did not seem
feasible in the short term. So three of the authors
discussed and iteratively refined manual annotations
for the development and tuning set stories until we
became comfortable that we had a common under-
standing for the annotation task. Then to create our
gold standard test set, two authors independently
created annotations for the test set, and a third au-
thor adjudicated the differences. The gold standard
contains complete plot unit annotations, including
affect states, causal links, and cross-character links.
For the experiments in this paper, however, only the
affect state annotations were used.
5.2 Baselines
We created two baselines to measure what would
happen if we use all 4 sentiment analysis resources
without any projection rules. The first one (Bsent)
operates at the sentence level. It naively projects ev-
ery affect tag that occurs in a sentence onto every
character in the same sentence. The second base-
line (Bclause) operates identically, but at the clause
level.
10http://www.pacificnet.net/?johnr/aesop/
5.3 Evaluation
As our evaluation metrics we used recall (R), preci-
sion (P), and F-measure (F). We evaluate each sys-
tem on individual affect states (+, - and M) as well
as across all affect states. The evaluation is done at
the sentence level. Meaning, if a system produces
the same affect state as present in the gold standard
for a sentence, we count it as a correct affect state.
Our main evaluation also requires each affect state
to be associated with the correct character.
Table 2 shows the coverage of our two baseline
systems as well as the four Sentiment Analysis
Resources used with our projection rules. We can
make several observations:
? As expected, the baselines achieve relatively high
recall, but low precision.
? Each of the sentiment analysis resources alone
is useful, and using them with the projection rules
leads to improved performance over the baselines
(10 points in F score for M and 6 points overall).
This shows that the projection rules are helpful
in identifying the characters associated with each
affect state.
? The PPV Lexicon, alone, is quite good at cap-
turing negative affect states. Together with the
projection rules, this leads to good performance on
identifying mental states as well.
To better assess our projection rules, we evaluated
the systems both with respect to characters and with-
out respect to characters. In this evaluation, system-
produced states are correct even if they are assigned
to the wrong character. Table 3 reveals several re-
sults: (1) For the baseline: there is a large drop when
23
State M (66) + (52) - (39) All (157)
System R P F R P F R P F R P F
Bclause w/o char .65 .37 .47 .50 .25 .33 .77 .19 .30 .63 .26 .37
AESOP w/o char .55 .44 .49 .33 .47 .39 .36 .50 .42 .43 .46 .44
Bclause w/ char .48 .28 .35 .44 .22 .29 .69 .17 .27 .52 .22 .31
AESOP w/ char .48 .40 .44 .25 .36 .30 .33 .46 .38 .37 .40 .38
Table 3: Evaluating affect states with and without respect to character.
State M (66) + (52) - (39) All (157)
System R P F R P F R P F R P F
Bclause PCoref .48 .28 .35 .44 .22 .29 .69 .17 .27 .52 .22 .31
AESOP PCoref .48 .40 .44 .25 .36 .30 .33 .46 .38 .37 .40 .38
Bclause ACoref .42 .45 .43 .25 .34 .29 .54 .24 .33 .39 .33 .36
AESOP ACoref .41 .54 .47 .12 .40 .18 .26 .45 .33 .27 .49 .35
Table 4: Final results of Bclause and AESOP systems with perfect and automated coreference
evaluated with respect to the correct character. (2)
For AESOP: there is a smaller drop in both preci-
sion and recall for M and -, suggesting that our pro-
jection rules are doing well for these affect states.
(3) For AESOP: there is a large drop in both preci-
sion and recall for +, suggesting that there is room
for improvement of our projection rules for positive
affect.
Finally, we wish to understand the role that coref-
erence plays. Table 4 summarizes the results with
perfect coreference and with automated coreference.
AESOP is better than both baselines when we use
perfect coreference (PCoref), which indicates that
the affect projection rules are useful. However,
when we use automated coreference (ACoref), re-
call goes down and precision goes up. Recall goes
down because our automated coreference system is
precision oriented: it only says ?coreferent? if it is
sure.
The increase in precision when moving to auto-
mated coreference is bizarre. We suspect it is pri-
marily due to the handling of quotations. Our perfect
coreference system resolves first and second person
pronouns in quotations, but the automated system
does not. Thus, with automated coreference, we al-
most never produce affect states from quotations.
This is a double-edged sword: sometimes quotes
contain important affect states, sometimes they do
not. For example, from the Father and Sons fable,
?if you are divided among yourselves, you will be
broken as easily as these sticks.? Automated coref-
erence does not produce any character resolutions
and therefore AESOP produces no affect states. In
this case this is the right thing to do. However, in
another well-known fable, a tortoise says to a hare:
?although you be as swift as the wind, I have beaten
you in the race.? Here, perfect coreference produces
multiple affect states, which are related to the plot:
the hare recieves a negative affect state for having
been beaten in the race.
6 Conclusions
AESOP demonstrates that sentiment analysis tools
can successfully recognize many affect states when
coupled with syntax-based projection rules to map
the affect states onto characters. We also showed
that negative patient polarity verbs can be harvested
from a corpus to identify characters that are in a neg-
ative state due to an action. However, performance is
still modest, revealing that much work remains to be
done. In future work, new methods will be needed
to represent affect states associated with plans/goals,
events, and inferences.
7 Acknowledgments
The authors thank the anonymous reviewers for
many helpful comments. This work was sup-
ported in part by the Department of Homeland Se-
curity Grant N0014-07-1-0152, the DARPA Ma-
chine Reading program under contract FA8750-09-
C-0172, and the NSF grant IIS-0712764.
24
References
Yejin Choi, Eric Breck, and Claire Cardie. 2006. Joint
extraction of entities and relations for opinion recogni-
tion. In EMNLP ?06: Proceedings of the 2006 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 431?439, Morristown, NJ, USA. Asso-
ciation for Computational Linguistics.
S. Kim and E. Hovy. 2006. Extracting Opinions, Opin-
ion Holders, and Topics Expressed in Online News
Media Text. In Proceedings of ACL/COLING Work-
shop on Sentiment and Subjectivity in Text.
W. Lehnert, J. Black, and B. Reiser. 1981. Summariz-
ing Narratives. In Proceedings of the Seventh Interna-
tional Joint Conference on Artificial Intelligence.
W. G. Lehnert. 1981. Plot Units and Narrative Summa-
rization. Cognitive Science, 5(4):293?331.
G. Miller. 1990. Wordnet: An On-line Lexical Database.
International Journal of Lexicography, 3(4).
E. Riloff and W. Phillips. 2004. An Introduction to the
Sundance and AutoSlog Systems. Technical Report
UUCS-04-015, School of Computing, University of
Utah.
V. Stoyanov and C. Cardie. 2008. Topic Identification
for Fine-Grained Opinion Analysis. In Conference on
Computational Linguistics (COLING 2008).
Hiroya Takamura, Takashi Inui, and Manabu Okumura.
2005. Extracting semantic orientations of words using
spin model. In ACL ?05: Proceedings of the 43rd An-
nual Meeting on Association for Computational Lin-
guistics.
A. Wierzbicka. 1987. English speech act verbs: a se-
mantic dictionary. Academic Press, Sydney, Orlando.
T. Wilson, P. Hoffmann, S. Somasundaran, J. Kessler,
J. Wiebe, Y. Choi, C. Cardie, E. Riloff, and S. Pat-
wardhan. 2005. OpinionFinder: A system for subjec-
tivity analysis. In Proceedings of HLT/EMNLP 2005
Interactive Demonstrations.
Theresa Wilson. 2005. Recognizing contextual polarity
in phrase-level sentiment analysis. In In Proceedings
of HLT-EMNLP.
25
Proceedings of the NAACL HLT 2010 Sixth Web as Corpus Workshop, pages 17?25,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Sketching Techniques for Large Scale NLP
Amit Goyal, Jagadeesh Jagarlamudi, Hal Daume? III, and Suresh Venkatasubramanian
University of Utah, School of Computing
{amitg,jags,hal,suresh}@cs.utah.edu
Abstract
In this paper, we address the challenges
posed by large amounts of text data by
exploiting the power of hashing in the
context of streaming data. We explore
sketch techniques, especially the Count-
Min Sketch, which approximates the fre-
quency of a word pair in the corpus with-
out explicitly storing the word pairs them-
selves. We use the idea of a conservative
update with the Count-Min Sketch to re-
duce the average relative error of its ap-
proximate counts by a factor of two. We
show that it is possible to store all words
and word pairs counts computed from 37
GB of web data in just 2 billion counters
(8 GB RAM). The number of these coun-
ters is up to 30 times less than the stream
size which is a big memory and space gain.
In Semantic Orientation experiments, the
PMI scores computed from 2 billion coun-
ters are as effective as exact PMI scores.
1 Introduction
Approaches to solve NLP problems (Brants et al,
2007; Turney, 2008; Ravichandran et al, 2005) al-
ways benefited from having large amounts of data.
In some cases (Turney and Littman, 2002; Pat-
wardhan and Riloff, 2006), researchers attempted
to use the evidence gathered from web via search
engines to solve the problems. But the commer-
cial search engines limit the number of automatic
requests on a daily basis for various reasons such
as to avoid fraud and computational overhead.
Though we can crawl the data and save it on disk,
most of the current approaches employ data struc-
tures that reside in main memory and thus do not
scale well to huge corpora.
Fig. 1 helps us understand the seriousness of
the situation. It plots the number of unique word-
s/word pairs versus the total number of words in
5 10 15 20 25
5
10
15
20
25
Log2 of # of words
Lo
g 2 
of 
# o
f u
niq
ue
 Ite
ms
 
 
Items=word?pairs
Items=words
Figure 1: Token Type Curve
a corpus of size 577 MB. Note that the plot is in
log-log scale. This 78 million word corpus gen-
erates 63 thousand unique words and 118 million
unique word pairs. As expected, the rapid increase
in number of unique word pairs is much larger
than the increase in number of words. Hence, it
shows that it is computationally infeasible to com-
pute counts of all word pairs with a giant corpora
using conventional main memory of 8 GB.
Storing only the 118 million unique word pairs
in this corpus require 1.9 GB of disk space. This
space can be saved by avoiding storing the word
pair itself. As a trade-off we are willing to tolerate
a small amount of error in the frequency of each
word pair. In this paper, we explore sketch tech-
niques, especially the Count-Min Sketch, which
approximates the frequency of a word pair in the
corpus without explicitly storing the word pairs
themselves. It turns out that, in this technique,
both updating (adding a new word pair or increas-
ing the frequency of existing word pair) and query-
ing (finding the frequency of a given word pair) are
very efficient and can be done in constant time1.
Counts stored in the CM Sketch can be used to
compute various word-association measures like
1depend only on one of the user chosen parameters
17
Pointwise Mutual Information (PMI), and Log-
Likelihood ratio. These association scores are use-
ful for other NLP applications like word sense
disambiguation, speech and character recognition,
and computing semantic orientation of a word. In
our work, we use computing semantic orientation
of a word using PMI as a canonical task to show
the effectiveness of CM Sketch for computing as-
sociation scores.
In our attempt to advocate the Count-Min
sketch to store the frequency of keys (words or
word pairs) for NLP applications, we perform both
intrinsic and extrinsic evaluations. In our intrinsic
evaluation, first we show that low-frequent items
are more prone to errors. Second, we show that
computing approximate PMI scores from these
counts can give the same ranking as Exact PMI.
However, we need counters linear in size of stream
to achieve that. We use these approximate PMI
scores in our extrinsic evaluation of computing se-
mantic orientation. Here, we show that we do not
need counters linear in size of stream to perform
as good as Exact PMI. In our experiments, by us-
ing only 2 billion counters (8GB RAM) we get the
same accuracy as for exact PMI scores. The num-
ber of these counters is up to 30 times less than the
stream size which is a big memory and space gain
without any loss of accuracy.
2 Background
2.1 Large Scale NLP problems
Use of large data in the NLP community is not
new. A corpus of roughly 1.6 Terawords was used
by Agirre et al (2009) to compute pairwise sim-
ilarities of the words in the test sets using the
MapReduce infrastructure on 2, 000 cores. Pan-
tel et al (2009) computed similarity between 500
million terms in the MapReduce framework over a
200 billion words in 50 hours using 200 quad-core
nodes. The inaccessibility of clusters for every one
has attracted the NLP community to use stream-
ing, randomized, approximate and sampling algo-
rithms to handle large amounts of data.
A randomized data structure called Bloom fil-
ter was used to construct space efficient language
models (Talbot and Osborne, 2007) for Statis-
tical Machine Translation (SMT). Recently, the
streaming algorithm paradigm has been used to
provide memory and space-efficient platform to
deal with terabytes of data. For example, We
(Goyal et al, 2009) pose language modeling as
a problem of finding frequent items in a stream
of data and show its effectiveness in SMT. Subse-
quently, (Levenberg and Osborne, 2009) proposed
a randomized language model to efficiently deal
with unbounded text streams. In (Van Durme and
Lall, 2009b), authors extend Talbot Osborne Mor-
ris Bloom (TOMB) (Van Durme and Lall, 2009a)
Counter to find the highly ranked k PMI response
words given a cue word. The idea of TOMB is
similar to CM Sketch. TOMB can also be used to
store word pairs and further compute PMI scores.
However, we advocate CM Sketch as it is a very
simple algorithm with strong guarantees and good
properties (see Section 3).
2.2 Sketch Techniques
A sketch is a summary data structure that is used
to store streaming data in a memory efficient man-
ner. These techniques generally work on an input
stream, i.e. they process the input in one direc-
tion, say from left to right, without going back-
wards. The main advantage of these techniques
is that they require storage which is significantly
smaller than the input stream length. For typical
algorithms, the working storage is sublinear in N ,
i.e. of the order of logk N , where N is the input
size and k is some constant which is not explicitly
chosen by the algorithm but it is an artifact of it..
Sketch based methods use hashing to map items in
the streaming data onto a small-space sketch vec-
tor that can be easily updated and queried. It turns
out that both updating and querying on this sketch
vector requires only a constant time per operation.
Streaming algorithms were first developed in
the early 80s, but gained in popularity in the late
90s as researchers first realized the challenges of
dealing with massive data sets. A good survey
of the model and core challenges can be found in
(Muthukrishnan, 2005). There has been consid-
erable work on coming up with different sketch
techniques (Charikar et al, 2002; Cormode and
Muthukrishnan, 2004; Li and Church, 2007). A
survey by (Rusu and Dobra, 2007; Cormode and
Hadjieleftheriou, 2008) comprehensively reviews
the literature.
3 Count-Min Sketch
The Count-Min Sketch (Cormode and Muthukr-
ishnan, 2004) is a compact summary data structure
used to store the frequencies of all items in the in-
put stream. The sketch allows fundamental queries
18
on the data stream such as point, range and in-
ner product queries to be approximately answered
very quickly. It can also be applied to solve the
finding frequent items problem (Manku and Mot-
wani, 2002) in a data stream. In this paper, we are
only interested in point queries. The aim of a point
query is to estimate the count of an item in the in-
put stream. For other details, the reader is referred
to (Cormode and Muthukrishnan, 2004).
Given an input stream of word pairs of length N
and user chosen parameters ? and ?, the algorithm
stores the frequencies of all the word pairs with the
following guarantees:
? All reported frequencies are within the true
frequencies by at most ?N with a probability
of at least ?.
? The space used by the algorithm is
O(1? log 1? ).
? Constant time of O(log(1? )) per each update
and query operation.
3.1 CM Data Structure
A Count-Min Sketch with parameters (?,?) is rep-
resented by a two-dimensional array with width w
and depth d :
?
?
?
sketch[1,1] ? ? ? sketch[1,w]
.
.
.
.
.
.
.
.
.
sketch[d,1] ? ? ? sketch[d,w]
?
?
?
Among the user chosen parameters, ? controls the
amount of tolerable error in the returned count and
? controls the probability with which the returned
count is not within the accepted error. These val-
ues of ? and ? determine the width and depth of the
two-dimensional array respectively. To achieve
the guarantees mentioned in the previous section,
we set w=2? and d=log(1? ). The depth d denotes
the number of pairwise-independent hash func-
tions employed by the algorithm and there exists
an one-to-one correspondence between the rows
and the set of hash functions. Each of these hash
functions hk:{1 . . . N} ? {1 . . . w} (1 ? k ? d)
takes an item from the input stream and maps it
into a counter indexed by the corresponding hash
function. For example, h2(w) = 10 indicates that
the word pair w is mapped to the 10th position in
the second row of the sketch array. These d hash
functions are chosen uniformly at random from a
pairwise-independent family.
Figure 2: Update Procedure for CM sketch and conserva-
tive update (CU)
Initially the entire sketch array is initialized
with zeros.
Update Procedure: When a new item (w,c) ar-
rives, where w is a word pair and c is its count2,
one counter in each row, as decided by its corre-
sponding hash function, is updated by c. Formally,
?1 ? k ? d
sketch[k,hk(w)]? sketch[k,hk(w)] + c
This process is illustrated in Fig. 2 CM. The item
(w,2) arrives and gets mapped to three positions,
corresponding to the three hash functions. Their
counts before update were (4,2,1) and after update
they become (6,4,3). Note that, since we are using
a hash to map a word into an index, a collision can
occur and multiple word pairs may get mapped to
the same counter in any given row. Because of
this, the values stored by the d counters for a given
word pair tend to differ.
Query Procedure: The querying involves find-
ing the frequency of a given item in the input
stream. Since multiple word pairs can get mapped
into same counter and the observation that the
counts of items are positive, the frequency stored
by each counter is an overestimate of the true
count. So in answering the point query, we con-
sider all the positions indexed by the hash func-
tions for the given word pair and return the mini-
mum of all these values. The answer to Query(w)
is:
c? = mink sketch[k,hk(w)]
Note that, instead of positive counts if we had neg-
ative counts as well then the algorithm returns the
median of all the counts and the bounds we dis-
cussed in Sec. 3 vary. In Fig. 2 CM, for the word
pair w it takes the minimum over (6,4,3) and re-
turns 3 as the count of word pair w.
2In our setting, c is always 1. However, in other NLP
problem, word pairs can be weighted according to recency.
19
Both update and query procedures involve eval-
uating d hash functions and a linear scan of all the
values in those indices and hence both these pro-
cedures are linear in the number of hash functions.
Hence both these steps require O(log(1? )) time. In
our experiments (see Section 4.2), we found that a
small number of hash functions are sufficient and
we use d=3. Hence, the update and query oper-
ations take only a constant time. The space used
by the algorithm is the size of the array i.e. wd
counters, where w is the width of each row.
3.2 Properties
Apart from the advantages of being space efficient,
and having constant update and constant querying
time, the Count-Min sketch has also other advan-
tages that makes it an attractive choice for NLP
applications.
? Linearity: given two sketches s1 and s2 com-
puted (using the same parameters w and d)
over different input streams, the sketch of
the combined data stream can be easily ob-
tained by adding the individual sketches in
O(1? log 1? ) time which is independent of the
stream size.
? The linearity is especially attractive because,
it allows the individual sketches to be com-
puted independent of each other. Which
means that it is easy to implement it in dis-
tributed setting, where each machine com-
putes the sketch over a sub set of corpus.
? This technique also extends to allow the dele-
tion of items. In this case, to answer a point
query, we should return the median of all the
values instead of the minimum value.
3.3 Conservative Update
Estan and Varghese introduce the idea of conser-
vative update (Estan and Varghese, 2002) in the
context of networking. This can easily be used
with CM Sketch to further improve the estimate
of a point query. To update an item, word pair, w
with frequency c, we first compute the frequency
c? of this item from the existing data structure and
the counts are updated according to: ?1 ? k ? d
sketch[k,hk(w)]? max{sketch[k,hk(w)], c? + c}
The intuition is that, since the point query returns
the minimum of all the d values, we will update
a counter only if it is necessary as indicated by
the above equation. Though this is a heuristic, it
avoids the unnecessary updates of counter values
and thus reduces the error.
The process is also illustrated in Fig. 2CU.
When an item ?w? with a frequency of 2 arrives
in the stream, it gets mapped into three positions
in the sketch data structure. Their counts before
update were (4,2,1) and the frequency of the item
is 1 (the minimum of all the three values). In this
particular case, the update rule says that increase
the counter value only if its updated value is less
than c? + 2 = 3. As a result, the values in these
counters after the update become (4,3,3).
However, if the value in any of the counters
is already greater than 3 e.g. 4, we cannot at-
tempt to correct it by decreasing, as it could con-
tain the count for other items hashed at that posi-
tion. Therefore, in this case, for the first counter
we leave the value 4 unchanged. The query pro-
cedure remains the same as in the previous case.
In our experiments, we found that employing the
conservative update reduces the Average Relative
Error (ARE) of these counts approximately by a
factor of 2. (see Section 4.2). But unfortunately,
this update prevents deletions and items with neg-
ative updates cannot be processed3.
4 Intrinsic Evaluations
To show the effectiveness of the Count-Min sketch
in the context of NLP, we perform intrinsic evalu-
ations. The intrinsic evaluations are designed to
measure the error in the approximate counts re-
turned by CMS compared to their true counts. By
keeping the total size of the data structure fixed,
we study the error by varying the width and the
depth of the data structure to find the best setting
of the parameters for textual data sets. We show
that using conservative update (CU) further im-
proves the quality of counts over CM sketch.
4.1 Corpus Statistics
Gigaword corpus (Graff, 2003) and a copy of web
crawled by (Ravichandran et al, 2005) are used
to compute counts of words and word pairs. For
both the corpora, we split the text into sentences,
tokenize and convert into lower-case. We generate
words and word pairs (items) over a sliding win-
dow of size 14. Unlike previous work (Van Durme
3Here, we are only interested in the insertion case.
20
Corpus Sub Giga 50% 100%
set word Web Web
Size
.15 6.2 15 31GB
# of sentences 2.03 60.30 342.68 686.63(Million)
# of words 19.25 858.92 2122.47 4325.03(Million)
Stream Size 0.25 19.25 18.63 39.0510 (Billion)
Stream Size 0.23 25.94 18.79 40.0014 (Billion)
Table 1: Corpus Description
and Lall, 2009b) which assumes exact frequen-
cies for words, we store frequencies of both the
words and word pairs in the CM sketch4. Hence,
the stream size in our case is the total number of
words and word pairs in a corpus. Table 1 gives
the characteristics of the corpora.
Since, it is not possible to compute exact fre-
quencies of all word pairs using conventional main
memory of 8 GB from a large corpus, we use a
subset of 2 million sentences (Subset) from Giga-
word corpus for our intrinsic evaluation. We store
the counts of all words and word pairs (occurring
in a sliding window of length 14) from Subset us-
ing the sketch and also the exact counts.
4.2 Comparing CM and CU counts and
tradeoff between width and depth
To evaluate the amount of over-estimation in CM
and CU counts compared to the true counts, we
first group all items (words and word pairs) with
same true frequency into a single bucket. We then
compute the average relative error in each of these
buckets. Since low-frequent items are more prone
to errors, making this distinction based on fre-
quency lets us understand the regions in which the
algorithm is over-estimating. Average Relative er-
ror (ARE) is defined as the average of absolute dif-
ference between the predicted and the exact value
divided by the exact value over all the items in
each bucket.
ARE = 1N
N
?
i=1
|Exacti ? Predictedi|
Exacti
Where Exact and Predicted denotes values of exact
and CM/CU counts respectively; N denotes the
number of items with same counts in a bucket.
In Fig. 3(a), we fixed the number of counters
to 50 million with four bytes of memory per each
4Though a minor point, it allows to process more text.
counter (thus it only requires 200 MB of main
memory). Keeping the total number of counters
fixed, we try different values of depth (2, 3, 5 and
7) of the sketch array and in each case the width
is set to 50Md . The ARE curves in each case are
shown in Fig. 3(a). There are three main observa-
tions: First it shows that most of the errors occur
on low frequency items. For frequent items, in al-
most all the different runs the ARE is close to zero.
Secondly, it shows that ARE is significantly lower
(by a factor of two) for the runs which use conser-
vative update (CUx run) compared to the runs that
use direct CM sketch (CMx run). The encouraging
observation is that, this holds true for almost all
different (width,depth) settings. Thirdly, in our ex-
periments, it shows that using depth of 3 gets com-
paratively less ARE compared to other settings.
To be more certain about this behavior with re-
spect to different settings of width and depth, we
tried another setting by increasing the number of
counters to 100 million. The curves in 3(b) follow
a pattern which is similar to the previous setting.
Low frequency items are more prone to error com-
pared to the frequent ones and employing conser-
vative update reduces the ARE by a factor of two.
In this setting, depth 3 and 5 do almost the same
and get lowest ARE. In both the experiments, set-
ting the depth to three did well and thus in the rest
of the paper we fix this parameter to three.
Fig. 4 studies the effect of the number of coun-
ters in the sketch (the size of the two-dimensional
sketch array) on the ARE. Using more number of
counters decreases the ARE in the counts. This is
intuitive because, as the length of each row in the
sketch increases, the probability of collision de-
creases and hence the array is more likely to con-
tain true counts. By using 200 million counters,
which is comparable to the length of the stream
230 million (Table. 1), we are able to achieve al-
most zero ARE over all the counts including the
rare ones5. Note that the actual space required
to represent the exact counts is almost two times
more than the memory that we use here because
there are 230 million word pairs and on an aver-
age each word is eight characters long and requires
eight bytes (double the size of an integer). The
summary of this Figure is that, if we want to pre-
serve the counts of low-frequent items accurately,
then we need counters linear in size of stream.
5Even with other datasets we found that using counters
linear in the size of the stream leads to ARE close to zero ?
counts.
21
0 2 4 6 8 10 12
0
0.5
1
1.5
2
2.5
3
3.5
4
Log2 of true frequency counts of words/word?pairs
Av
era
ge 
Re
lati
ve 
Err
or
 
 
CM7
CM5
CM3
CM2
CU7
CU5
CU3
CU2
(a) 50M counters
0 2 4 6 8 10 12
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
Log2 of true frequency counts of words/word?pairs
Av
era
ge 
Re
lati
ve 
Err
or
 
 
CM7
CM5
CM3
CM2
CU7
CU5
CU3
CU2
(b) 100M counters
Figure 3: Comparing 50 and 100 million counter models with different (width,depth) settings. The notation CMx represents
the Count-Min Sketch with a depth of ?x? and CUx represents the CM sketch along with conservative update and depth ?x?.
0 2 4 6 8 10 12
0
1
2
3
4
5
6
Log2 of true frequency counts of words/word?pairs
Av
er
ag
e 
Re
lat
ive
 E
rro
r
 
 
20M
50M
100M
200M
Figure 4: Comparing different size models with depth 3
4.3 Evaluating the CU PMI ranking
In this experiment, we compare the word pairs as-
sociation rankings obtained using PMI with CU
and exact counts. We use two kinds of measures,
namely accuracy and Spearman?s correlation, to
measure the overlap in the rankings obtained by
both these approaches.
4.3.1 PointWise Mutual Information
The Pointwise Mutual Information (PMI) (Church
and Hanks, 1989) between two words w1 and w2
is defined as:
PMI(w1, w2) = log2
P (w1, w2)
P (w1)P (w2)
Here, P (w1, w2) is the likelihood that w1 and w2
occur together, and P (w1) and P (w2) are their in-
dependent likelihoods respectively. The ratio be-
tween these probabilities measures the degree of
statistical dependence between w1 and w2.
4.3.2 Description of the metrics
Accuracy is defined as fraction of word pairs that
are found in both rankings to the size of top ranked
word pairs.
Accuracy = |CP-WPs ? EP-WPs||EP-WPs|
Where CP-WPs represent the set of top ranked K
word pairs under the counts stored using the CU
sketch and EP-WPs represent the set of top ranked
word pairs with the exact counts.
Spearman?s rank correlation coefficient (?)
computes the correlation between the ranks of
each observation (i.e. word pairs) on two variables
(that are top N CU-PMI and exact-PMI values).
This measure captures how different the CU-PMI
ranking is from the Exact-PMI ranking.
? = 1? 6
? d2i
F (F 2 ? 1)
Where di is the difference between the ranks of
a word pair in both rankings and F is the number
of items found in both sets.
Intuitively, accuracy captures the number of
word pairs that are found in both the sets and then
Spearman?s correlation captures if the relative or-
der of these common items is preserved in both the
rankings. In our experimental setup, both these
measures are complimentary to each other and
measure different aspects. If the rankings match
exactly, then we get an accuracy of 100% and a
correlation of 1.
4.3.3 Comparing CU PMI ranking
The results with respect to different sized counter
(50, 100 and 200 million) models are shown in Ta-
ble 2. Table 2 shows that having counters linear
22
Counters 50M 100M 200M
Top K Acc ? Acc ? Acc ?
50 .20 -0.13 .68 .95 .92 1.00
100 .18 .31 .77 .80 .96 .95
200 .21 .68 .73 .86 .97 .99
500 .24 .31 .71 .97 .95 .99
1000 .33 .17 .74 .87 .95 .98
5000 .49 .38 .82 .82 .96 .97
Table 2: Evaluating the PMI rankings obtained using CM
Sketch with conservative update (CU) and Exact counts
in size of stream (230M ) results in better rank-
ing (i.e. close to the exact ranking). For example,
with 200M counters, among the top 50 word pairs
produced using the CU counts, we found 46 pairs
in the set returned by using exact counts. The ?
score on those word pairs is 1 means that the rank-
ing of these 46 items is exactly the same on both
CU and exact counts. We see the same phenom-
ena for 200M counters with other Top K values.
While both accuracy and the ranking are decent
with 100M counters, if we reduce the number of
counters to say 50M , the performance degrades.
Since, we are not throwing away any infrequent
items, PMI will rank pairs with low frequency
counts higher (Church and Hanks, 1989). Hence,
we are evaluating the PMI values for rare word
pairs and we need counters linear in size of stream
to get alost perfect ranking. Also, using coun-
ters equal to half the length of the stream is decent.
However, in some NLP problems, we are not inter-
ested in low-frequency items. In such cases, even
using space less than linear in number of coun-
ters would suffice. In our extrinsic evaluations, we
show that using space less than the length of the
stream does not degrades the performance.
5 Extrinsic Evaluations
5.1 Experimental Setup
To evaluate the effectiveness of CU-PMI word
association scores, we infer semantic orientation
(S0) of a word from CU-PMI and Exact-PMI
scores. Given a word, the task of finding the SO
(Turney and Littman, 2002) of the word is to iden-
tify if the word is more likely to be used in positive
or negative sense. We use a similar framework as
used by the authors6 to infer the SO. We take the
seven positive words (good, nice, excellent, posi-
tive, fortunate, correct, and superior) and the nega-
tive words (bad, nasty, poor, negative, unfortunate,
6We compute this score slightly differently. However, our
main focus is to show that CU-PMI scores are useful.
wrong, and inferior) used in (Turney and Littman,
2002) work. The SO of a given word is calculated
based on the strength of its association with the
seven positive words, and the strength of its asso-
ciation with the seven negative words. We com-
pute the SO of a word ?w? as follows:
SO-PMI(W) = PMI(+, w)? PMI(?, w)
PMI(+,W) =
?
p?Pwords
log hits(p, w)hits(p) ? hits(w)
PMI(-,W) =
?
n?Nwords
log hits(n,w)hits(n) ? hits(w)
Where, Pwords and Nwords denote the seven pos-
itive and negative prototype words respectively.
We compute SO score from different sized cor-
pora (Section 4.1). We use the General Inquirer
lexicon7 (Stone et al, 1966) as a benchmark to
evaluate the semantic orientation scores similar to
(Turney and Littman, 2002) work. Words with
multiple senses have multiple entries in the lexi-
con, we merge these entries for our experiment.
Our test set consists of 1619 positive and 1989
negative words. Accuracy is used as an evaluation
metric and is defined as the fraction of number of
correctly identified SO words.
Accuracy = Correctly Identified SO Words ? 100Total SO words
5.2 Results
We evaluate SO of words on three different sized
corpora: Gigaword (GW) 6.2GB, GigaWord +
50% of web data (GW+WB1) 21.2GB and Gi-
gaWord + 100% of web data (GW+WB2) 31GB.
Note that computing the exact counts of all word
pairs on these corpora is not possible using main
memory, so we consider only those pairs in which
one word appears in the prototype list and the
other word appears in the test set.
We compute the exact PMI (denoted using Ex-
act) scores for pairs of test-set words w1 and proto-
type words w2 using the above data-sets. To com-
pute PMI, we count the number of hits of individ-
ual words w1 and w2 and the pair (w1,w2) within a
sliding window of sizes 10 and 14 over these data-
sets. After computing the PMI scores, we compute
SO score for a word using SO-PMI equation from
Section 5.1. If this score is positive, we predict
the word as positive. Otherwise, we predict it as
7The General Inquirer lexicon is freely available at
http://www.wjh.harvard.edu/ inquirer/
23
Model Accuracy window 10 Accuracy window 14
#of counters Mem. Usage GW GW+WB1 GW+WB2 GW GW+WB1 GW+WB2
Exact n/a 64.77 75.67 77.11 64.86 74.25 75.30
500M 2GB 62.98 71.09 72.31 63.21 69.21 70.35
1B 4GB 62.95 73.93 75.03 63.95 72.42 72.73
2B 8GB 64.69 75.86 76.96 65.28 73.94 74.96
Table 3: Evaluating Semantic Orientation of words with different # of counters of CU sketch with increasing amount of data
on window size of 10 and 14. Scores are evaluated using Accuracy metric.
negative. The results on inferring correct SO for
a word w with exact PMI (Exact) are summarized
in Table 3. It (the second row) shows that increas-
ing the amount of data improves the accuracy of
identifying the SO of a word with both the win-
dow sizes. The gain is more prominent when we
add 50% of web data in addition to Gigaword as
we get an increase of more than 10% in accuracy.
However, when we add the remaining 50% of web
data, we only see an slight increase of 1% in accu-
racy8. Using words within a window of 10 gives
better accuracy than window of 14.
Now, we use our CU Sketches of 500 million
(500M ), 1 billion (1B) and 2 billion (2B) coun-
ters to compute CU-PMI. These sketches contain
the number of hits of all words/word pairs (not just
the pairs of test-set and prototype words) within a
window size of 10 and 14 over the whole data-
set. The results in Table 3 show that even with
CU-PMI scores, the accuracy improves by adding
more data. Again we see a significant increase in
accuracy by adding 50% of web data to Gigaword
over both window sizes. The increase in accuracy
by adding the rest of the web data is only 1%.
By using 500M counters, accuracy with CU-
PMI are around 4% worse than the Exact. How-
ever, increasing the size to 1B results in only 2
% worse accuracy compared to the Exact. Go-
ing to 2B counters (8 GB of RAM), results in ac-
curacy almost identical to the Exact. These re-
sults hold almost the same for all the data-sets
and for both the window sizes. The increase in
accuracy comes at expense of more memory Us-
age. However, 8GB main memory is not large as
most of the conventional desktop machines have
this much RAM. The number of 2B counters is
less than the length of stream for all the data-sets.
For GW, GW+WB1 and GW+WB2, 2B counters
are 10, 20 and 30 times smaller than the stream
size. This shows that using counters less than the
stream length does not degrade the performance.
8These results are similar to the results reported in (Tur-
ney and Littman, 2002) work.
The advantage of using Sketch is that it con-
tains counts for all words and word pairs. Suppose
we are given a new word to label it as positive or
negative. We can find its exact PMI in two ways:
First, we can go over the whole corpus and com-
pute counts of this word with positive and nega-
tive prototype words. This procedure will return
PMI in time needed to traverse the whole corpus.
If the corpus is huge, this could be too slow. Sec-
ond option is to consider storing counts of all word
pairs but this is not feasible as their number in-
creases rapidly with increase in data (see Fig. 1).
Therefore, using a CM sketch is a very good al-
ternative which returns the PMI in constant time
by using only 8GB of memory. Additionally, this
Sketch can easily be used for other NLP applica-
tions where we need word-association scores.
6 Conclusion
We have explored the idea of the CM Sketch,
which approximates the frequency of a word pair
in the corpus without explicitly storing the word
pairs themselves. We used the idea of a conserva-
tive update with the CM Sketch to reduce the av-
erage relative error of its approximate counts by
a factor of 2. It is an efficient, small-footprint
method that scales to at least 37 GB of web data
in just 2 billion counters (8 GB main memory). In
our extrinsic evaluations, we found that CU Sketch
is as effective as exact PMI scores.
Word-association scores from CU Sketch can be
used for other NLP tasks like word sense disam-
biguation, speech and character recognition. The
counts stored in CU Sketch can be used to con-
struct small-space randomized language models.
In general, this sketch can be used for any applica-
tion where we want to query a count of an item.
Acknowledgments
We thank the anonymous reviewers for helpful
comments. This work is partially funded by NSF
grant IIS-0712764 and Google Research Grant
Grant for Large-Data NLP.
24
References
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Pas?ca, and Aitor Soroa. 2009.
A study on similarity and relatedness using distri-
butional and wordnet-based approaches. In NAACL
?09: Proceedings of Human Language Technolo-
gies: The 2009 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Large language
models in machine translation. In Proceedings of
the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL).
Moses Charikar, Kevin Chen, and Martin Farach-
colton. 2002. Finding frequent items in data
streams.
K. Church and P. Hanks. 1989. Word Association
Norms, Mutual Information and Lexicography. In
Proceedings of the 27th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 76?83,
Vancouver, Canada, June.
Graham Cormode and Marios Hadjieleftheriou. 2008.
Finding frequent items in data streams. In VLDB.
Graham Cormode and S. Muthukrishnan. 2004. An
improved data stream summary: The count-min
sketch and its applications. J. Algorithms.
Cristian Estan and George Varghese. 2002. New direc-
tions in traffic measurement and accounting. SIG-
COMM Comput. Commun. Rev., 32(4).
Amit Goyal, Hal Daume? III, and Suresh Venkatasub-
ramanian. 2009. Streaming for large scale NLP:
Language modeling. In North American Chap-
ter of the Association for Computational Linguistics
(NAACL).
D. Graff. 2003. English Gigaword. Linguistic Data
Consortium, Philadelphia, PA, January.
Abby Levenberg and Miles Osborne. 2009. Stream-
based randomised language models for SMT. In
Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing, pages
756?764, Singapore, August. Association for Com-
putational Linguistics.
Ping Li and Kenneth W. Church. 2007. A sketch algo-
rithm for estimating two-way and multi-way associ-
ations. Comput. Linguist., 33(3).
G. S. Manku and R. Motwani. 2002. Approximate
frequency counts over data streams. In Proceedings
of the 28th International Conference on Very Large
Data Bases.
S. Muthukrishnan. 2005. Data streams: Algorithms
and applications. Foundations and Trends in Theo-
retical Computer Science, 1(2).
Patrick Pantel, Eric Crestan, Arkady Borkovsky, Ana-
Maria Popescu, and Vishnu Vyas. 2009. Web-
scale distributional similarity and entity set expan-
sion. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Process-
ing, pages 938?947, Singapore, August. Association
for Computational Linguistics.
S. Patwardhan and E. Riloff. 2006. Learning Domain-
Specific Information Extraction Patterns from the
Web. In Proceedings of the ACL 2006 Workshop on
Information Extraction Beyond the Document.
Deepak Ravichandran, Patrick Pantel, and Eduard
Hovy. 2005. Randomized algorithms and nlp: using
locality sensitive hash function for high speed noun
clustering. In ACL ?05: Proceedings of the 43rd An-
nual Meeting on Association for Computational Lin-
guistics.
Florin Rusu and Alin Dobra. 2007. Statistical analysis
of sketch estimators. In SIGMOD ?07. ACM.
Philip J. Stone, Dexter C. Dunphy, Marshall S. Smith,
and Daniel M. Ogilvie. 1966. The General In-
quirer: A Computer Approach to Content Analysis.
MIT Press.
David Talbot and Miles Osborne. 2007. Smoothed
Bloom filter language models: Tera-scale LMs on
the cheap. In Proceedings of the 2007 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EM NLP-CoNLL).
Peter D. Turney and Michael L. Littman. 2002.
Unsupervised learning of semantic orientation
from a hundred-billion-word corpus. CoRR,
cs.LG/0212012.
Peter D. Turney. 2008. A uniform approach to analo-
gies, synonyms, antonyms, and associations. In Pro-
ceedings of COLING 2008.
Benjamin Van Durme and Ashwin Lall. 2009a. Prob-
abilistic counting with randomized storage. In IJ-
CAI?09: Proceedings of the 21st international jont
conference on Artifical intelligence, pages 1574?
1579.
Benjamin Van Durme and Ashwin Lall. 2009b.
Streaming pointwise mutual information. In Ad-
vances in Neural Information Processing Systems
22.
25
Proceedings of the 2010 Workshop on GEometrical Models of Natural Language Semantics, ACL 2010, pages 51?56,
Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational Linguistics
Sketch Techniques for Scaling Distributional Similarity to the Web
Amit Goyal, Jagadeesh Jagarlamudi, Hal Daume? III, and Suresh Venkatasubramanian
School of Computing
University of Utah
Salt Lake City, UT 84112
{amitg,jags,hal,suresh}@cs.utah.edu
Abstract
In this paper, we propose a memory, space,
and time efficient framework to scale dis-
tributional similarity to the web. We
exploit sketch techniques, especially the
Count-Min sketch, which approximates
the frequency of an item in the corpus
without explicitly storing the item itself.
These methods use hashing to deal with
massive amounts of the streaming text. We
store all item counts computed from 90
GB of web data in just 2 billion coun-
ters (8 GB main memory) of CM sketch.
Our method returns semantic similarity
between word pairs in O(K) time and
can compute similarity between any word
pairs that are stored in the sketch. In our
experiments, we show that our framework
is as effective as using the exact counts.
1 Introduction
In many NLP problems, researchers (Brants et al,
2007; Turney, 2008) have shown that having large
amounts of data is beneficial. It has also been
shown that (Agirre et al, 2009; Pantel et al, 2009;
Ravichandran et al, 2005) having large amounts
of data helps capturing the semantic similarity be-
tween pairs of words. However, computing distri-
butional similarity (Sec. 2.1) between word pairs
from large text collections is a computationally ex-
pensive task. In this work, we consider scaling dis-
tributional similarity methods for computing se-
mantic similarity between words to Web-scale.
The major difficulty in computing pairwise sim-
ilarities stems from the rapid increase in the num-
ber of unique word-context pairs with the size of
text corpus (number of tokens). Fig. 1 shows that
5 10 15 20 25
5
10
15
20
Log2 of # of words
Lo
g 2
 
o
f #
 o
f u
ni
qu
e 
Ite
m
s
 
 
word?context pairs
words
Figure 1: Token Type Curve
the number of unique word-context pairs increase
rapidly compared to the number words when plot-
ted against the number of tokens1. For example,
a 57 million word corpus2 generates 224 thousand
unique words and 15 million unique word-context
pairs. As a result, it is computationally hard to
compute counts of all word-context pairs with a gi-
ant corpora using conventional machines (say with
main memory of 8 GB). To overcome this, Agirre
et al (2009) used MapReduce infrastructure (with
2, 000 cores) to compute pairwise similarities of
words on a corpus of roughly 1.6 Terawords.
In a different direction, our earlier work (Goyal
et al, 2010) developed techniques to make the
computations feasible on a conventional machines
by willing to accept some error in the counts. Sim-
ilar to that work, this work exploits the idea of
Count-Min (CM) sketch (Cormode and Muthukr-
ishnan, 2004) to approximate the frequency of
word pairs in the corpus without explicitly stor-
ing the word pairs themselves. In their, we stored
1Note that the plot is in log-log scale.
2
?Subset? column of Table 1 in Section 5.1
51
counts of all words/word pairs in fixed amount of
main memory. We used conservative update with
CM sketch (referred as CU sketch) and showed
that it reduces the average relative error of its ap-
proximate counts by a factor of two. The approx-
imate counts returned by CU Sketch were used
to compute approximate PMI between word pairs.
We found their that the approximate PMI values
are as useful as the exact PMI values for com-
puting semantic orientation (Turney and Littman,
2002) of words. In addition, our intrinsic evalua-
tions in their showed that the quality of approxi-
mate counts and approximate PMI is good.
In this work, we use CU-sketch to store counts
of items (words, contexts, and word-context pairs)
using fixed amount of memory of 8 GB by using
only 2B counters. These approximate counts re-
turned by CU Sketch are converted into approx-
imate PMI between word-context pairs. The top
K contexts (based on PMI score) for each word
are used to construct distributional profile (DP) for
each word. The similarity between a pair of words
is computed based on the cosine similarity of their
respective DPs.
The above framework of using CU sketch to
compute semantic similarity between words has
five good properties. First, this framework can re-
turn semantic similarity between any word pairs
that are stored in the CU sketch. Second, it can
return the similarity between word pairs in time
O(K). Third, because we do not store items ex-
plicitly, the overall space required is significantly
smaller. Fourth, the additive property of CU
sketch (Sec. 3.2) enables us to parallelize most
of the steps in the algorithm. Thus it can be easily
extended to very large amounts of text data. Fifth,
this easily generalizes to any kind of association
measure and semantic similarity measure.
2 Background
2.1 Distributional Similarity
Distributional Similarity is based on the distribu-
tional hypothesis (Firth, 1968; Harris, 1985) that
words occur in similar contexts tend to be sim-
ilar. The context of a word is represented by
the distributional profile (DP), which contains the
strength of association between the word and each
of the lexical, syntactic, semantic, and/or depen-
dency units that co-occur with it3. The association
3In this work, we only consider lexical units as context.
is commonly measured using conditional proba-
bility, pointwise mutual information (PMI) or log
likelihood ratios. Then the semantic similarity be-
tween two words, given their DPs, is calculated
using similarity measures such as Cosine, ?-skew
divergence, and Jensen-Shannon divergence. In
our work, we use PMI as association measure and
cosine similarity to compute pairwise similarities.
2.2 Large Scale NLP problems
Pantel et al (2009) computed similarity between
500 million word pairs using the MapReduce
framework from a 200 billion word corpus using
200 quad-core nodes. The inaccessibility of clus-
ters for every one has attracted NLP community to
use streaming, and randomized algorithms to han-
dle large amounts of data.
Ravichandran et al (2005) used locality sensi-
tive hash functions for computing word-pair simi-
larities from large text collections. Their approach
stores a enormous matrix of all unique words and
their contexts in main memory which makes it
hard for larger data sets. In our work, we store
all unique word-context pairs in CU sketch with a
pre-defined size4.
Recently, the streaming algorithm paradigm has
been used to provide memory and time-efficient
platform to deal with terabytes of data. For
example, we (Goyal et al, 2009); Levenberg
and Osborne (2009) build approximate language
models and show their effectiveness in SMT. In
(Van Durme and Lall, 2009b), a TOMB Counter
(Van Durme and Lall, 2009a) was used to find the
top-K verbs ?y? with the highest PMI for a given
verb ?x?. The idea of TOMB is similar to CU
Sketch. However, we use CU Sketch because of
its simplicity and attractive properties (see Sec. 3).
In this work, we go one step further, and compute
semantic similarity between word-pairs using ap-
proximate PMI scores from CU sketch.
2.3 Sketch Techniques
Sketch techniques use a sketch vector as a data
structure to store the streaming data compactly in
a small-memory footprint. These techniques use
hashing to map items in the streaming data onto a
small sketch vector that can be easily updated and
queried. These techniques generally process the
input stream in one direction, say from left to right,
4We use only 2 billion counters which takes up to 8 GB
of main memory.
52
without re-processing previous input. The main
advantage of using these techniques is that they
require a storage which is significantly smaller
than the input stream length. A survey by (Rusu
and Dobra, 2007; Cormode and Hadjieleftheriou,
2008) comprehensively reviews the literature.
3 Count-Min Sketch
The Count-Min Sketch (Cormode and Muthukr-
ishnan, 2004) is a compact summary data struc-
ture used to store the frequencies of all items in
the input stream.
Given an input stream of items of length N
and user chosen parameters ? and ?, the algorithm
stores the frequencies of all the items with the fol-
lowing guarantees:
? All reported frequencies are within ?N of
true frequencies with probability of atleast ?.
? Space used by the algorithm is O(1? log 1? ).
? Constant time of O(log(1? )) per each update
and query operation.
3.1 CM Data Structure
A Count-Min Sketch with parameters (?,?) is rep-
resented by a two-dimensional array with width w
and depth d :
?
?
?
sketch[1, 1] ? ? ? sketch[1, w]
.
.
.
.
.
.
.
.
.
sketch[d, 1] ? ? ? sketch[d,w]
?
?
?
Among the user chosen parameters, ? controls the
amount of tolerable error in the returned count and
? controls the probability with which the returned
count is not within this acceptable error. These
values of ? and ? determine the width and depth
of the two-dimensional array respectively. To
achieve the guarantees mentioned in the previous
section, we set w=2? and d=log(1? ). The depth d
denotes the number of pairwise-independent hash
functions employed by the algorithm and there
exists an one-to-one correspondence between the
rows and the set of hash functions. Each of these
hash functions hk:{x1 . . . xN} ? {1 . . . w}, 1 ?
k ? d takes an item from the input stream and
maps it into a counter indexed by the correspond-
ing hash function. For example, h2(x) = 10 indi-
cates that the item ?x? is mapped to the 10th posi-
tion in the second row of the sketch array. These
d hash functions are chosen uniformly at random
from a pairwise-independent family.
Initialize the entire sketch array with zeros.
Update Procedure: When a new item ?x? with
count c arrives5, one counter in each row, as de-
cided by its corresponding hash function, is up-
dated by c. Formally, ?1 ? k ? d
sketch[k,hk(x)]? sketch[k,hk(x)] + c
Query Procedure: Since multiple items can be
hashed to the same counter, the frequency stored
by each counter is an overestimate of the true
count. Thus, to answer the point query, we con-
sider all the positions indexed by the hash func-
tions for the given item and return the minimum
of all these values. The answer to Query(x) is:
c? = mink sketch[k, hk(x)].
Both update and query procedures involve eval-
uating d hash functions. Hence, both these proce-
dures are linear in the number of hash functions. In
our experiments (see Section5), we use d=3 simi-
lar to our earlier work (Goyal et al, 2010). Hence,
the update and query operations take only constant
time.
3.2 Properties
Apart from the advantages of being space efficient
and having constant update and querying time, the
CM sketch has other advantages that makes it at-
tractive for scaling distributional similarity to the
web:
1. Linearity: given two sketches s1 and s2 com-
puted (using the same parameters w and d)
over different input streams, the sketch of the
combined data stream can be easily obtained
by adding the individual sketches.
2. The linearity allows the individual sketches
to be computed independent of each other.
This means that it is easy to implement it in
distributed setting, where each machine com-
putes the sketch over a subset of the corpus.
3.3 Conservative Update
Estan and Varghese introduce the idea of conserva-
tive update (Estan and Varghese, 2002) in the con-
text of networking. This can easily be used with
CM Sketch (CU Sketch) to further improve the es-
timate of a point query. To update an item, w with
frequency c, we first compute the frequency c? of
5In our setting, c is always 1.
53
this item from the existing data structure and the
counts are updated according to: ?1 ? k ? d
sketch[k,hk(x)]? max{sketch[k,hk(x)], c? + c}
The intuition is that, since the point query returns
the minimum of all the d values, we will update a
counter only if it is necessary as indicated by the
above equation. This heuristic avoids the unneces-
sary updating of counter values and thus reduces
the error.
4 Efficient Distributional Similarity
To compute distributional similarity efficiently, we
store counts in CU sketch. Our algorithm has three
main steps:
1. Store approximate counts of all words, con-
texts, and word-context pairs in CU-sketch
using fixed amount of counters.
2. Convert these counts into approximate PMI
scores between word-context pairs. Use these
PMI scores to store top K contexts for a word
on the disk. Store these top K context vectors
for every word stored in the sketch.
3. Use cosine similarity to compute the similar-
ity between word pairs using these approxi-
mate top K context vectors constructed using
CU sketch.
5 Word pair Ranking Evaluations
As discussed earlier, the DPs of words are used to
compute similarity between a pair of words. We
used the following four test sets and their corre-
sponding human judgements to evaluate the word
pair rankings.
1. WS-353: WordSimilarity-3536 (Finkelstein
et al, 2002) is a set of 353 word pairs.
2. WS-203: A subset of WS-353 containing 203
word pairs marked according to similarity7
(Agirre et al, 2009).
3. RG-65: (Rubenstein and Goodenough, 1965)
is set of 65 word pairs.
4. MC-30: A smaller subset of the RG-65
dataset containing 30 word pairs (Miller and
Charles, 1991).
6http://www.cs.technion.ac.il/ gabr/resources/data/word-
sim353/wordsim353.html
7http://alfonseca.org/pubs/ws353simrel.tar.gz
Each of these data sets come with human ranking
of the word pairs. We rank the word pairs based
on the similarity computed using DPs and evalu-
ate this ranking against the human ranking. We
report the spearman?s rank correlation coefficient
(?) between these two rankings.
5.1 Corpus Statistics
The Gigaword corpus (Graff, 2003) and a copy of
the web crawled by (Ravichandran et al, 2005)
are used to compute counts of all items (Table. 1).
For both the corpora, we split the text into sen-
tences, tokenize, convert into lower-case, remove
punctuations, and collapse each digit to a sym-
bol ?0? (e.g. ?1996? gets collapsed to ?0000?).
We store the counts of all words (excluding num-
bers, and stop words), their contexts, and counts
of word-context pairs in the CU sketch. We de-
fine the context for a given word ?x? as the sur-
rounding words appearing in a window of 2 words
to the left and 2 words to the right. The context
words are concatenated along with their positions
-2, -1, +1, and +2. We evaluate ranking of word
pairs on three different sized corpora: Gigaword
(GW), GigaWord + 50% of web data (GW-WB1),
and GigaWord + 100% of web data (GW-WB2).
Corpus Sub GW GW- GW-
set WB1 WB2
Size
.32 9.8 49 90(GB)
# of sentences 2.00 56.78 462.60 866.02(Million)
Stream Size
.25 7.65 37.93 69.41(Billion)
Table 1: Corpus Description
5.2 Results
We compare our system with two baselines: Ex-
act and Exact1000 which use exact counts. Since
computing the exact counts of all word-context
pairs on these corpora is not possible using main
memory of only 8 GB , we generate context vec-
tors for only those words which appear in the test
set. The former baseline uses all possible contexts
which appear with a test word, while the latter
baseline uses only the top 1000 contexts (based on
PMI value) for each word. In each case, we use
a cutoff (of 10, 60 and 120) on the frequency of
word-context pairs. These cut-offs were selected
based on the intuition that, with more data, you
get more noise, and not considering word-context
pairs with frequency less than 120 might be a bet-
54
Data GW GW-WB1 GW-WB2
Model Frequency cutoff Frequency cutoff Frequency cutoff10 60 120 10 60 120 10 60 120
? ? ?
WS-353
Exact .25 .25 .22 .29 .28 .28 .30 .28 .28
Exact1000 .36 .28 .22 .46 .43 .37 .47 .44 .41
Our Model .39 .28 .22 -0.09 .48 .40 -0.03 .04 .47
WS-203
Exact .35 .36 .33 .38 .38 .37 .40 .38 .38
Exact1000 .49 .40 .35 .57 .55 .47 .56 .56 .52
Our Model .49 .39 .35 -0.08 .58 .47 -0.06 .03 .55
RG-65
Exact .21 .12 .08 .42 .28 .22 .39 .31 .23
Exact1000 .14 .09 .08 .45 .16 .13 .47 .26 .12
Our Model .13 .10 .09 -0.06 .32 .18 -0.05 .08 .31
MC-30
Exact .26 .23 .21 .45 .33 .31 .46 .39 .29
Exact1000 .27 .18 .21 .63 .42 .32 .59 .47 .36
Our Model .36 .20 .21 -0.08 .52 .39 -0.27 -0.29 .52
Table 2: Evaluating word pairs ranking with Exact and CU counts. Scores are evaluated using ? metric.
ter choice than a cutoff of 10. The results are
shown in Table 2
From the above baseline results, first we learn
that using more data helps in better capturing
the semantic similarity between words. Second,
it shows that using top (K) 1000 contexts for
each target word captures better semantic similar-
ity than using all possible contexts for that word.
Third, using a cutoff of 10 is optimal for all differ-
ent sized corpora on all test-sets.
We use approximate counts from CU sketch
with depth=3 and 2 billion (2B) counters (?Our
Model?)8. Based on previous observation, we re-
strict the number of contexts for a target word to
1000. Table 2 shows that using CU counts makes
the algorithm sensitive to frequency cutoff. How-
ever, with appropriate frequency cutoff for each
corpus, approximate counts are nearly as effective
as exact counts. For GW, GW-WB1, and GW-
WB2, the frequency cutoffs of 10, 60, and 120 re-
spectively performed the best. The reason for de-
pendence on frequency cutoffs is due to the over-
estimation of low-frequent items. This is more
pronounced with bigger corpus (GW-WB2) as the
size of CU sketch is fixed to 2B counters and
stream size is much bigger (69.41 billion) com-
pared to GW where the stream size is 7.65 billion.
The advantages of using our model is that the
sketch contains counts for all words, contexts, and
word-context pairs stored in fixed memory of 8
GB by using only 2B counters. Note that it is not
8Our goal is not to build the best distributional similarity
method. It is to show that our framework scales easily to large
corpus and it is as effective as exact method.
feasible to keep track of exact counts of all word-
context pairs since their number increases rapidly
with increase in data (see Fig. 1). We can use our
model to create context vectors of size K for all
possible words stored in the Sketch and computes
semantic similarity between two words in O(K)
time. In addition, the linearity of sketch allows
us to include new incoming data into the sketch
without building the sketch from scratch. Also,
it allows for parallelization using the MapReduce
framework. We can generalize our framework to
any kind of association and similarity measure.
6 Conclusion
We proposed a framework which uses CU Sketch
to scale distributional similarity to the web. It can
compute similarity between any word pairs that
are stored in the sketch and returns similarity be-
tween them in O(K) time. In our experiments, we
show that our framework is as effective as using
the exact counts, however it is sensitive to the fre-
quency cutoffs. In future, we will explore ways to
make this framework robust to the frequency cut-
offs. In addition, we are interested in exploring
this framework for entity set expansion problem.
Acknowledgments
We thank the anonymous reviewers for helpful
comments. This work is partially funded by NSF
grant IIS-0712764 and Google Research Grant
Grant for Large-Data NLP.
55
References
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Pas?ca, and Aitor Soroa. 2009.
A study on similarity and relatedness using distri-
butional and wordnet-based approaches. In NAACL
?09: Proceedings of HLT-NAACL.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Large language
models in machine translation. In Proceedings of
the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL).
Graham Cormode and Marios Hadjieleftheriou. 2008.
Finding frequent items in data streams. In VLDB.
Graham Cormode and S. Muthukrishnan. 2004. An
improved data stream summary: The count-min
sketch and its applications. J. Algorithms.
Cristian Estan and George Varghese. 2002. New direc-
tions in traffic measurement and accounting. SIG-
COMM Comput. Commun. Rev., 32(4).
L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin,
Z. Solan, G. Wolfman, and E. Ruppin. 2002. Plac-
ing search in context: The concept revisited. In
ACM Transactions on Information Systems.
J. Firth. 1968. A synopsis of linguistic theory 1930-
1955. In F. Palmer, editor, Selected Papers of J. R.
Firth. Longman.
Amit Goyal, Hal Daume? III, and Suresh Venkatasub-
ramanian. 2009. Streaming for large scale NLP:
Language modeling. In North American Chap-
ter of the Association for Computational Linguistics
(NAACL).
Amit Goyal, Jagadeesh Jagarlamudi, Hal Daume? III,
and Suresh Venkatasubramanian. 2010. Sketching
techniques for Large Scale NLP. In 6th Web as Cor-
pus Workshop in conjunction with NAACL-HLT.
D. Graff. 2003. English Gigaword. Linguistic Data
Consortium, Philadelphia, PA, January.
Z. Harris. 1985. Distributional structure. In J. J. Katz,
editor, The Philosophy of Linguistics, pages 26?47.
Oxford University Press, New York.
Abby Levenberg and Miles Osborne. 2009. Stream-
based randomised language models for SMT. In
Proceedings of EMNLP, August.
G.A. Miller and W.G. Charles. 1991. Contextual cor-
relates of semantic similarity. Language and Cogni-
tive Processes, 6(1):1?28.
Patrick Pantel, Eric Crestan, Arkady Borkovsky, Ana-
Maria Popescu, and Vishnu Vyas. 2009. Web-scale
distributional similarity and entity set expansion. In
Proceedings of EMNLP.
Deepak Ravichandran, Patrick Pantel, and Eduard
Hovy. 2005. Randomized algorithms and nlp: using
locality sensitive hash function for high speed noun
clustering. In ACL ?05: Proceedings of the 43rd An-
nual Meeting on Association for Computational Lin-
guistics.
H. Rubenstein and J.B. Goodenough. 1965. Contex-
tual correlates of synonymy. Computational Lin-
guistics, 8:627?633.
Florin Rusu and Alin Dobra. 2007. Statistical analysis
of sketch estimators. In SIGMOD ?07. ACM.
P.D. Turney and M.L. Littman. 2002. Unsupervised
learning of semantic orientation from a hundred-
billion-word corpus.
Peter D. Turney. 2008. A uniform approach to analo-
gies, synonyms, antonyms, and associations. In Pro-
ceedings of COLING 2008.
Benjamin Van Durme and Ashwin Lall. 2009a. Prob-
abilistic counting with randomized storage. In IJ-
CAI?09: Proceedings of the 21st international jont
conference on Artifical intelligence.
Benjamin Van Durme and Ashwin Lall. 2009b.
Streaming pointwise mutual information. In Ad-
vances in Neural Information Processing Systems
22.
56
Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, ACL-HLT 2011, pages 37?43,
24 June, 2011, Portland, Oregon, USA c?2011 Association for Computational Linguistics
Generating Semantic Orientation Lexicon using Large Data and Thesaurus
Amit Goyal and Hal Daume? III
Dept. of Computer Science
University of Maryland
College Park, MD 20742
{amit,hal}@umiacs.umd.edu
Abstract
We propose a novel method to construct se-
mantic orientation lexicons using large data
and a thesaurus. To deal with large data, we
use Count-Min sketch to store the approxi-
mate counts of all word pairs in a bounded
space of 8GB. We use a thesaurus (like Roget)
to constrain near-synonymous words to have
the same polarity. This framework can easily
scale to any language with a thesaurus and a
unzipped corpus size ? 50 GB (12 billion to-
kens). We evaluate these lexicons intrinsically
and extrinsically, and they perform compara-
ble when compared to other existing lexicons.
1 Introduction
In recent years, the field of natural language process-
ing (NLP) has seen tremendous growth and inter-
est in the computational analysis of emotions, sen-
timents, and opinions. This work has focused on
many application areas, such as sentiment analy-
sis of consumer reviews e.g., (Pang et al, 2002;
Nasukawa and Yi, 2003), product reputation anal-
ysis e.g., (Morinaga et al, 2002; Nasukawa and Yi,
2003), tracking sentiments toward events e.g., (Das
and Chen, 2001; Tong, 2001), and automatically
producing plot unit representations e.g., (Goyal et
al., 2010b). An important resource in accomplishing
the above tasks is a list of words with semantic ori-
entation (SO): positive or negative. The goal of this
work is to automatically create such a list of words
using large data and a thesaurus structure.
For this purpose, we store exact counts of all
the words in a hash table and use Count-Min (CM)
sketch (Cormode and Muthukrishnan, 2004; Goyal
et al, 2010) to store the approximate counts of all
word pairs for a large corpus in a bounded space of
8GB. (Storing the counts of all word pairs is compu-
tationally expensive and memory intensive on large
data (Agirre et al, 2009; Pantel et al, 2009)). Stor-
age space saving in CM sketch is achieved by ap-
proximating the frequency of word pairs in the cor-
pus without explicitly storing the word pairs them-
selves. Both updating (adding a new word pair or
increasing the frequency of existing word pair) and
querying (finding the frequency of a given word
pair) are constant time operations making it an ef-
ficient online storage data structure for large data.
Once we have these counts, we find semantic
orientation (SO) (Turney and Littman, 2003) of a
word using its association strength with positive
(e.g. good, and nice) and negative (e.g., bad and
nasty) seeds. Next, we make use of a thesaurus (like
Roget) structure in which near-synonymous words
appear in a single group. We compute the SO of
the whole group by computing SO of each individ-
ual word in the group and assign that SO to all the
words in the group. The hypothesis is that near
synonym words should have similar polarity. How-
ever, similar words in a group can still have differ-
ent connotations. For example, one group has ?slen-
der?, ?slim?, ?wiry? and ?lanky?. One can argue
that, first two words have positive connotation and
last two have negative. To remove these ambigu-
ous words errors from the lexicon, we discard those
words which have conflicting SO compared to their
group SO. The idea behind using thesaurus struc-
ture is motivated from the idea of using number of
positive and negative seed words (Mohammad et al,
2009) in thesaurus group to determine the polarity
of words in the group.
In our experiments, we show the effectiveness of
the lexicons created using large data and freely avail-
37
able thesaurus both intrinsically and extrinsically.
2 Background
2.1 Related Work
The literature on sentiment lexicon induction can be
broadly classified into three categories: (1) Corpora
based, (2) using thesaurus structure, and (3) com-
bination of (1) and (2). Pang and Lee (2008) pro-
vide an excellent survey on the literature of sen-
timent analysis. We briefly discuss some of the
works which have motivated our research for this
work. A web-derived lexicon (Velikovich et al,
2010) was constructed for all words and phrases us-
ing graph propagation algorithm which propagates
polarity from seed words to all other words. The
graph was constructed using distributional similar-
ity between the words. The goal of their work was
to create a high coverage lexicon. In a similar direc-
tion (Rao and Ravichandran, 2009), word-net was
used to construct the graph for label propagation.
Our work is most closely related to Mohammad et
al. (2009) which exploits thesaurus structure to de-
termine the polarity of words in the thesaurus group.
2.2 Semantic Orientation
We use (Turney and Littman, 2003) framework to
infer the Semantic Orientation (SO) of a word. We
take the seven positive words (good, nice, excellent,
positive, fortunate, correct, and superior) and the
seven negative words (bad, nasty, poor, negative, un-
fortunate, wrong, and inferior) used in (Turney and
Littman, 2003) work. The SO of a given word is
calculated based on the strength of its association
with the seven positive words, and the strength of
its association with the seven negative words using
pointwise mutual information (PMI). We compute
the SO of a word ?w? as follows:
SO(w) =
?
p?Pwords
PMI(p, w)?
?
n?Nwords
PMI(n,w)
where, Pwords and Nwords denote the seven pos-
itive and seven negative prototype words respec-
tively. If this score is negative, the word is predicted
as negative. Otherwise, it is predicted as positive.
2.3 CM sketch
The Count-Min sketch (Cormode and Muthukrish-
nan, 2004) with user chosen parameters (,?) is
represented by a two-dimensional array with width
w and depth d. Parameters  and ? control the
amount of tolerable error in the returned count ()
and the probability with which the returned count
is not within this acceptable error (?) respectively.
These parameters determine the width and depth
of the two-dimensional array. We set w=2 , and
d=log(1? ). The depth d denotes the number of
pairwise-independent hash functions and there ex-
ists an one-to-one mapping between the rows and
the set of hash functions. Each of these hash func-
tions hk:{x1 . . . xN} ? {1 . . . w}, 1 ? k ? d, takes
an item from the input stream and maps it into a
counter indexed by the corresponding hash function.
For example, h3(x) = 8 indicates that the item ?x?
is mapped to the 8th position in the third row of the
sketch.
Update Procedure: When a new item ?x? with
count c, the sketch is updated by:
sketch[k,hk(x)]? sketch[k,hk(x)] + c, ?1 ? k ? d
Query Procedure: Since multiple items can be
hashed to the same position, the stored frequency in
any one row is guaranteed to overestimate the true
count. Thus, to answer the point query, we return
the minimum over all the positions indexed by the
k hash functions. The answer to Query(x) is: c? =
mink sketch[k, hk(x)].
2.4 CU sketch
The Count-Min sketch with conservative update
(CU sketch) (Goyal et al, 2010) is similar to CM
sketch except the update operation. It is based on
the idea of conservative update (Estan and Vargh-
ese, 2002) introduced in the context of networking.
It is used with CM sketch to further improve the es-
timate of a point query. To update an item, x with
frequency c, we first compute the frequency c? of this
item from the existing data structure and the counts
are updated according to:
c? = mink sketch[k,hk(x)], ?1 ? k ? d
sketch[k,hk(x)]? max{sketch[k,hk(x)], c?+ c}
The intuition is that, since the point query returns
the minimum of all the d values, we will update a
counter only if it is necessary as indicated by the
above equation.
38
3 Generating Polarity Lexicon
Our framework to generate lexicon has three main
steps: First, we compute Semantic Orientation (SO)
of words using a formula defined in Section 2.2 us-
ing a large corpus. Second, we use a thesaurus (like
Roget) to constrain all synonym words in a group
to have the same polarity. Third, we discard words
which do not follow the above constraints. The three
steps are discussed in the following subsections.
3.1 Computing SO of a word
We use CM sketch to store counts of word pairs (ex-
cept word pairs involving stop words and numbers)
within a sliding window of size1 7 using a large cor-
pus: GWB66 of size 64GB (see Section 4.3). We
fix the number of counters of the sketch to 2 bil-
lion (2B) (8GB of memory) with conservative up-
date (CU) as it performs the best for (Goyal et al,
2010) with d = 5 (see Section 2.3) hash functions.
We store exact counts of words in hash table.
Once, we have stored the counts for all words and
word pairs, we can compute the SO of a word using
a formula defined in Section 2.2. Moreover, a word
can have multiple senses, hence it can belong to mul-
tiple paragraphs. To assign a single label to a word,
we combine all its SO scores. We use positive SO
scores to label words as positive and negative SO to
label words as negative. We discard words with SO
equal to zero. We apply this strategy to all the words
in a thesaurus (like Roget) (refer to Section 3.2), we
call the lexicon constructed using SO scores using
thesaurus words as ?SO? lexicon.
3.2 Using Thesaurus structure
Thesaurus like Roget2, Macquarie are available in
several languages. We use freely available version
of Roget thesaurus which has 1046 categories, each
containing on average 64 words and phrases. Terms
within a category are closely related to one another,
and they are further grouped into near-synonymous
words and phrases called paragraphs. There are
about 3117 paragraphs in Roget thesaurus. One
of the examples of paragraphs from the Roget the-
saurus is shown in Table 1. All the words appears to
be near-synonymous with positive polarity.
1Window size 7 is chosen from intuition and not tuned.
2http://www.nzdl.org/ELKB/
pure undefiled modest delicate decent decorous cherry chaste
continent virtuous honest platonic virgin unsullied simonpure
Table 1: A paragraph from the Roget thesaurus
We assign semantic orientation (SO) score to a
thesaurus paragraph3 (SO(TP )) by averaging over
SO scores over all the words in it. The SO(TP )
score constrains all the words in a paragraph to have
same polarity. If SO(TP ) > 0, all the words in a
paragraph are marked as positive. If SO(TP ) < 0,
all the words in a group are marked as negative. For
SO(TP ) = 0, we discard all the words of a para-
graph. For the paragraph in Table 1, the SO(TP )
for the paragraph is 8.72. Therefore, all the words in
this paragraph are labeled as positive. However, the
SO scores for ?virgin? and ?decorous? are negative,
therefore they are marked as negative by previous
lexicon ?SO?, however they seem to be more pos-
itive than negative. Therefore, using the structure
of the lexicon helps us in correcting the polarity of
these words to negative. We apply this strategy to all
the 3117 Roget thesaurus paragraphs and construct
?SO-TP? lexicon using SO(TP ) scores.
3.3 Words and Thesaurus Consensus
Since near-synonymous words could have different
connotation or polarity. Hence, here we use both
SO of word and SO(TP ) of its paragraph to assign
polarity to a word. If SO(w) > 0 and SO(TP ) >
0, then we mark that word as positive. If SO(w) <
0 and SO(TP ) < 0, then we mark that word as
negative. In other cases, we discard the word.
We refer to the lexicon constructed using this
strategy on Roget thesaurus paragraphs as ?SO-
WTP? lexicon. The motivation behind this is to gen-
erate precision orientated lexicon by having consen-
sus over both individual and paragraph scores. For
the paragraph in Table 1, we discard words ?virgin?
and ?decorous? from the lexicon, as they have con-
flicting SO(w) and SO(TP ) scores. In experiments
in Section 5.2.1, we also examine existing lexicons
to constrain the polarity of thesaurus paragraphs.
4 Evaluating SO computed using sketch
We compare the accuracy of computed SO using dif-
ferent sized corpora. We also compare exact counts
with approximate counts using sketch.
3We do not assign polarity to phrases and stop words.
39
4.1 Data
We use Gigaword corpus (Graff, 2003) and a 66%
portion of a copy of web crawled by (Ravichan-
dran et al, 2005). For both the corpora, we split
the text into sentences, tokenize and convert into
lower-case. We generate words and word pairs over
a sliding window of size 7. We use four different
sized corpora: Gigaword (GW), GigaWord + 16%
of web data (GWB16), GigaWord + 50% of web
data (GWB50), and GigaWord + 66% of web data
(GWB66). Corpus Statistics are shown in Table 2.
We store exact counts of words in a hash table and
store approximate counts of word pairs in the sketch.
4.2 Test Set
We use General Inquirer lexicon4 (Stone et al, 1966)
as a benchmark to evaluate the semantic orientation
scores similar to (Turney and Littman, 2003) work.
Our test set consists of 1597 positive and 1980 nega-
tive words. Accuracy is used as an evaluation metric.
Corpus GW GWB16 GWB50 GWB66
Unzipped
9.8 22.8 49 64
Size (GB)
# of sentences
56.78 191.28 462.60 608.74
(Million)
# of Tokens
1.8 4.2 9.1 11.8
(Billion)
Stream Size
2.67 6.05 13.20 17.31
(Billion)
Table 2: Corpus Description
4.3 Effect of Increasing Corpus Size
We evaluate SO of words on four different sized
corpora (see Section 4.1): GW (9.8GB), GWB20
(22.8GB), GWB50 (49GB) and GWB66 (64GB).
First, we will fix number of counters to 2 billion
(2B) (CU-2B) as it performs the best for (Goyal
et al, 2010). Second, we will compare the CU-2B
model with the Exact over increasing corpus size.
We can make several observations from the Fig-
ure 1: ? It shows that increasing the amount of data
improves the accuracy of identifying the SO of a
word. We get an absolute increase of 5.5 points
in accuracy when we add 16% Web data to Giga-
Word (GW). Adding 34% more Web data (GWB50),
gives a small increase of 1.3 points. Adding 16%
4The General Inquirer lexicon which is freely available at
http://www.wjh.harvard.edu/?inquirer/
more Web data (GWB66), give an increase of 0.5
points. ? Second, CU-2B performs as good as Ex-
act. ? These results are also comparable to Turney?s
(2003) state-of-the-art work where they report an ac-
curacy of 82.84%. Note, they use a 100 billion to-
kens corpus which is larger than GWB66 (12 billion
tokens).
This experiments shows that using unzipped cor-
pus size ? 50 GB (12 billion tokens), we get per-
formance comparable to the state-of-the-art. Hence,
this approach is applicable for any language which
has large collection of monolingual data available
in it. Note that these results compared to best re-
sults of (Goyal et al, 2010) that is 77.11 are 4.5
points better; however in their work their goal was
to show their approach scales to large data. We sus-
pect the difference in results is due to difference in
pre-processing and choosing the window size. We
used counts from GWB66 (64GB) to generate lexi-
cons in Section 3.
0 10GB 20GB 30GB 40GB 50GB 60GB 70GB72
74
76
78
80
82
Corpus Size
Accu
racy
 
 
CU?2BExact
Figure 1: Evaluating Semantic Orientation of words with Ex-
act and CU counts with increase in corpus size
5 Lexicon evaluation
We evaluate the lexicons proposed in Section 3
both intrinsically (by comparing their lexicon en-
tries against General Inquirer (GI) lexicon) and ex-
trinsically (by using them in a phrase polarity anno-
tation task). We remove stop words and phrases for
comparison from existing lexicons as our framework
does not assign polarity to them.
5.1 Intrinsic evaluation
We compare the lexicon entries of ?SO?, ?SO-TP? ,
and ?SO-WTP? against entries of GI Lexicon. This
evaluation is similarly used by other authors (Tur-
ney and Littman, 2003; Mohammad et al, 2009) to
evaluate sentiment lexicons.
Table 3 shows the percentage of GI positive (Pos),
negative (Neg) and all (All) lexicon entries that
40
Lexicon (size) Pos (1597) Neg (1980) All (3577)
SO (32.2K) 0.79 0.73 0.76
S0-TP (33.1K) 0.88 0.64 0.75
SO-WTP (22.6K) 0.78 0.65 0.71
Roget-ASL (27.8K) 0.79 0.40 0.57
Table 3: The percentage of GI entries (positive, negative, and
all) that match those of the automatically generated lexicons
match the proposed lexicons. The recall of our pre-
cision orientated lexicon SO-WTP is only 5 and
4 % less compared to SO and SO-TP respectively
which are more recall oriented. We evaluate these
lexicons against Roget-ASL (discussed in Section
5.2.1). Even, Our SO-WTP precision oriented lexi-
con has more recall than Roget-ASL.
5.2 Extrinsic evaluation
In this section, we compare the effectiveness of our
lexicons on a task of phrase polarity identification.
We use the MPQA corpus which contains news ar-
ticles from a wide variety of news sources manually
annotated for opinions and other private states (like
beliefs, emotions, sentiments, speculations, etc.).
Moreover, it has polarity annotations (positive/neg-
ative) at the phrase level. We use MPQA5 version
2.0 collection of 2789 positive and 6079 negative
phrases. We perform an extrinsic evaluation of our
automatic generated lexicons (using large data and
thesaurus) against existing automated and manually
generated lexicons by using them to automatically
determine the phrase polarity. This experimental
setup is similar to Mohammad et al (2009). How-
ever, in their work, they used MPQA version 1.0.
We use a similar algorithm as used by Mohammad
et al (2009) to determine the polarity of the phrase.
If any of the words in the target phrase is labeled in
the lexicon as having negative SO, then the phrase is
marked as negative. If there are no negative words in
the target phrase and it contains one or more positive
words, then the phrase is marked as positive. In all
other cases, do not assign any tag.
The only difference with respect to Mohammad et
al. (2009) is that we use a list of 58 negation words
used in OpinionFinder6 (Wilson et al, 2005b) (Ver-
sion 1.4) to flip the polarity of a phrase if it contains
odd number of negation words. We can get better
5http://www.cs.pitt.edu/mpqa/databaserelease/
6www.cs.pitt.edu/mpqa/opinionfinderrelease
Lexicon # of positives # of negatives # of all
GI 1597 1980 3577
MPQA 2666 4888 7554
ASL 2320 2616 4936
Roget (ASL) 21637 6161 27798
Roget (GI) 10804 16319 27123
Roget (ASL+GI) 16168 12530 28698
MSOL 22088 32712 54800
SO 16620 15582 32202
SO-TP 22959 10117 33076
SO-WTP 14357 8257 22614
SO+GI 8629 9936 18565
SO-TP+GI 12049 9317 21366
Table 4: Summarizes all lexicons size
accuracies on phrase polarity identification using su-
pervised classifiers (Wilson et al, 2005a). However,
the goal of this work is only to show the effective-
ness of large data and thesaurus learned lexicons.
5.2.1 Baselines
We compare our method against the following
baselines: First, MPQA Lexicon7 ((Wilson et al,
2005a)). Second, we use Affix seed lexicon (ASL)
seeds used by Mohammad et al (2009) to assign
labels to Roget thesaurus paragraphs. ASL was
constructed using 11 affix patterns, e.g. honest-
dishonest (X-disX pattern). If ASL matches more
positive words than negative words in a paragraph
then all the words in the paragraph are labeled as
positive. However, if ASL matches more negative
words than positive words in a paragraph, then all
words in the paragraph are labeled as negative. For
other cases, we do not assign any labels. The gen-
erated lexicon is referred as Roget (ASL). Third, we
use GI Lexicon instead of ASL and generate Roget
(GI) Lexicon. Fourth, we use ASL + GI, and gen-
erate Roget (ASL+GI) Lexicon. Fifth, MSOL8 gen-
erated by Mohammad et al (2009) using ASL+GI
lexicon on Macquarie Thesaurus. Note that Mac-
quarie Thesaurus is not freely available and its size
is larger than the freely available Roget?s thesaurus.
5.2.2 GI seeds information with SO Lexicon
We combine the GI seed lexicon with seman-
tic orientation of word computed using large cor-
pus to mark the words positive or negative in the-
saurus paragraphs. We combine the information
7www.cs.pitt.edu/mpqa/lexiconrelease/collectinfo1.html
8http://www.umiacs.umd.edu/?saif/
Release/MSOL-June15-09.txt
41
Polarity + (2789) - (6079) All (8868)
SO Lexicon R P F R P F R P F
MPQA .48 .73 .58 .48 .95 .64 .48 .87 .62
Roget (ASL) .64 .45 .53 .32 .90 .47 .42 .60 .49
Roget (GI) .50 .60 .55 .55 .86 .67 .53 .76 .62
Roget (ASL+GI) .62 .57 .59 .49 .91 .64 .53 .75 .62
MSOL .51 .58 .54 .60 .84 .70 .57 .74 .64
SO .63 .54 .58 .50 .90 .64 .54 .73 .62
SO-TP .68 .51 .58 .44 .93 .60 .52 .69 .59
SO-WTP .65 .54 .59 .44 .93 60 .51 .72 .60
SO+GI .60 .57 .58 .46 .93 .62 .50 .75 .60
SO-TP+GI .62 .58 .60 .45 .93 .61 .51 .76 .61
Table 5: Results on marking polarity of phrases using various
lexicons. The # in parentheses is the # of gold +/-/all phrases.
from large corpus with GI in two forms: ? SO+GI:
If GI matches more number of positive words than
negative words in a paragraph and SO of a word
> 0, then that word is labeled as positive. However,
if GI matches more number of negative words than
positive words in a paragraph and SO of a word< 0,
that word is labeled as negative. For other cases,
we do not assign any labels to words. ? SO-TP+GI:
Here, we use SO(TP ) scores instead of SO scores
and use the same strategy as in previous bullet to
generate the lexicon.
Table 4 summarizes the size of all lexicons.
MPQA has the largest size among manually created
lexicons. It is build on top of GI Lexicon. Ro-
get (ASL) has 78% positive entries. MSOL is the
biggest lexicon and it is about 2.5 times bigger than
our precision oriented SO-WTP lexicon.
5.2.3 Results
Table 5 demonstrates the performance of the algo-
rithm (discussed in Section 5.2) when using different
lexicons. The performance of existing lexicons is
shown in the top part of the table. The performance
of large data and thesaurus lexicons is shown in the
middle of the table. The bottom of the table com-
bines GI information with large data and thesaurus.
In the first part of the Table 5, our results demon-
strate that MPQA in the first row of the table has the
best precision on this task for both positive and neg-
ative phrases. Roget (ASL) in the second row has
the best recall for positives which is double the re-
call for negatives. Hence, this indicates that ASL is
biased towards positive words. Using GI with Ro-
get gives more balanced recall for both positives and
negatives in third row. Roget (ASL+GI) are more
biased towards positive words. MSOL has the best
recall for negatives; however it comes at an expense
of equal drop in precision with respect to MPQA.
In the second part of the Table using large data,
?SO? lexicon has same F-score as MPQA with pre-
cision and recall trade-offs. Using thesaurus along
with large data has comparable F-score; however it
again gives some precision and recall trade-offs with
noticeable 6 points drop in recall for negatives. The
small decrease in F-score for SO-WTP precision-
oriented lexicon (22, 614 entries) is due to its small
size in comparison to SO lexicon (32, 202 entries).
We are currently working with a small sized freely
available thesaurus which is smaller than Macquarie,
hence MSOL performs the best.
Using GI lexicon in bottom part of the Table, we
incorporate another form of information, which pro-
vides overall better precision than SO, SO-TP, and
SO-WTP approaches. Even for languages, where
we have only large amounts of data available, ?SO?
can be beneficial. If we have thesaurus available for
a language, it can be combined with large data to
produce precision oriented lexicons.
6 Discussion and Conclusion
We constructed lexicons automatically using large
data and a thesaurus and evaluated its quality both
intrinsically and extrinsically. This framework can
easily scale to any language with a thesaurus and
a unzipped corpus size of ? 50 GB (12 billion to-
kens). However, if a language does not have the-
saurus, word similarity between words can be used
to generate word clusters. Currently we are explor-
ing using word clusters instead of using thesaurus
in our framework. Moreover, if a language does
not have large collection of data, we like to explore
bilingual lexicons to compute semantic orientation
of a word in another language. Another promising
direction would be to explore the idea of word simi-
larity combined with CM sketch (stores the approx-
imate counts of all word pairs in a bounded space of
8GB) in graph propagation setting without explicitly
representing the graph structure between words.
Acknowledgments
We thank the anonymous reviewers for helpful com-
ments. This work is partially funded by NSF grant
IIS-0712764 and Google Research Grant Grant for
Large-Data NLP.
42
References
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Pas?ca, and Aitor Soroa. 2009. A
study on similarity and relatedness using distributional
and wordnet-based approaches. In NAACL ?09: Pro-
ceedings of HLT-NAACL.
Graham Cormode and S. Muthukrishnan. 2004. An im-
proved data stream summary: The count-min sketch
and its applications. J. Algorithms.
S. R. Das and M. Y. Chen. 2001. Yahoo! for Ama-
zon: Opinion extraction from small talk on the Web.
In Proceedings of the 8th Asia Pacific Finance Associ-
ation Annual Conference (APFA), Bangkok, Thailand.
Cristian Estan and George Varghese. 2002. New di-
rections in traffic measurement and accounting. SIG-
COMM Comput. Commun. Rev., 32(4).
Amit Goyal, Jagadeesh Jagarlamudi, Hal Daume? III, and
Suresh Venkatasubramanian. 2010. Sketching tech-
niques for Large Scale NLP. In 6th WAC Workshop at
NAACL-HLT.
Amit Goyal, Ellen Riloff, and Hal Daume III. 2010b.
Automatically producing plot unit representations for
narrative text. In Proceedings of the 2010 Conference
on Empirical Methods in Natural Language Process-
ing, pages 77?86. Association for Computational Lin-
guistics, October.
D. Graff. 2003. English Gigaword. Linguistic Data Con-
sortium, Philadelphia, PA, January.
Saif Mohammad, Cody Dunne, and Bonnie Dorr. 2009.
Generating high-coverage semantic orientation lexi-
cons from overtly marked words and a thesaurus. In
Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing, pages 599?
608. Association for Computational Linguistics.
Satoshi Morinaga, Kenji Yamanishi, Kenji Tateishi, and
Toshikazu Fukushima. 2002. Mining product reputa-
tions on the Web. In Proceedings of the 8th Associa-
tion for Computing Machinery SIGKDD International
Conference on Knowledge Discovery and Data Mining
(KDD-2002), pages 341?349, Edmonton, Canada.
Tetsuya Nasukawa and Jeonghee Yi. 2003. Senti-
ment analysis: Capturing favorability using natural
language processing. In Proceedings of the 2nd Inter-
national Conference on Knowledge Capture (K-CAP
2003), pages 70?77, Sanibel Island, Florida.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and Trends in Infor-
mation Retrieval, Vol. 2(1-2):pp. 1?135.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment classification using ma-
chine learning techniques. pages 79?86, Philadelphia,
Pennsylvania.
Patrick Pantel, Eric Crestan, Arkady Borkovsky, Ana-
Maria Popescu, and Vishnu Vyas. 2009. Web-scale
distributional similarity and entity set expansion. In
Proceedings of EMNLP.
Delip Rao and Deepak Ravichandran. 2009. Semi-
supervised polarity lexicon induction. In Proceedings
of the 12th Conference of the European Chapter of the
ACL (EACL 2009), pages 675?682, Athens, Greece,
March. Association for Computational Linguistics.
Deepak Ravichandran, Patrick Pantel, and Eduard Hovy.
2005. Randomized algorithms and nlp: using locality
sensitive hash function for high speed noun clustering.
In Proceedings of ACL.
Philip J. Stone, Dexter C. Dunphy, Marshall S. Smith,
and Daniel M. Ogilvie. 1966. The General Inquirer:
A Computer Approach to Content Analysis. MIT
Press.
Richard Tong. 2001. An operational system for detecting
and tracking opinions in on-line discussions. In Work-
ing Notes of the Special Interest Group on Information
Retrieval (SIGIR) Workshop on Operational Text Clas-
sification, pages 1?6, New Orleans, Louisianna.
Peter D. Turney and Michael L. Littman. 2003. Measur-
ing praise and criticism: Inference of semantic orienta-
tion from association. ACM Trans. Inf. Syst., 21:315?
346, October.
Leonid Velikovich, Sasha Blair-Goldensohn, Kerry Han-
nan, and Ryan McDonald. 2010. The viability of web-
derived polarity lexicons. In Human Language Tech-
nologies: The 2010 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 777?785, Los Angeles, Cal-
ifornia, June. Association for Computational Linguis-
tics.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005a. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of Human
Language Technology Conference and Conference on
Empirical Methods in Natural Language Processing,
pages 347?354. Association for Computational Lin-
guistics.
T. Wilson, P. Hoffmann, S. Somasundaran, J. Kessler,
J. Wiebe, Y. Choi, C. Cardie, E. Riloff, and S. Pat-
wardhan. 2005b. OpinionFinder: A system for sub-
jectivity analysis. In Proceedings of Human Language
Technology Conference and Conference on Empirical
Methods in Natural Language Processing Interactive
Demonstrations, pages 34?35.
43
