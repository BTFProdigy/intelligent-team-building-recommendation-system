Proceedings of the Eighth Meeting of the ACL Special Interest Group on Computational Phonology at HLT-NAACL 2006, pages 50?59,
New York City, USA, June 2006. c?2006 Association for Computational Linguistics
Richness of the Base and Probabilistic Unsupervised Learning in  
Optimality Theory 
 
 
Gaja Jarosz 
Department of Cognitive Science 
Johns Hopkins University 
Baltimore, MD 21218 
jarosz@cogsci.jhu.edu 
 
 
 
 
Abstract 
This paper proposes an unsupervised 
learning algorithm for Optimality Theo-
retic grammars, which learns a complete 
constraint ranking and a lexicon given 
only unstructured surface forms and mor-
phological relations. The learning algo-
rithm, which is based on the Expectation-
Maximization algorithm, gradually 
maximizes the likelihood of the observed 
forms by adjusting the parameters of a 
probabilistic constraint grammar and a 
probabilistic lexicon. The paper presents 
the algorithm?s results on three con-
structed language systems with different 
types of hidden structure: voicing neu-
tralization, stress, and abstract vowels. In 
all cases the algorithm learns the correct 
constraint ranking and lexicon.  The paper 
argues that the algorithm?s ability to iden-
tify correct, restrictive grammars is due in 
part to its explicit reliance on the Opti-
mality Theoretic notion of Richness of the 
Base. 
1 Introduction 
In Optimality Theory or OT (Prince and Smolen-
sky, 1993) grammars are defined by a set of ranked 
universal and violable constraints.  The function of 
the grammar is to map underlying or lexical forms 
to valid surface forms. The task of the learner is to 
find the correct grammar, or correct ranking of 
constraints, as well as the set of underlying forms 
that correspond to overt surface forms given only 
the surface forms and the set of universal con-
straints.   
The most well known algorithms for learning 
OT grammars (Tesar, 1995; Tesar and Smolensky, 
1995; Boersma, 1997, 1998; Prince and Tesar, 
1999; Boersma and Hayes, 2001) are supervised 
learners and focus on the task of learning the 
constraint ranking, given training pairs that map 
underlying forms to surface forms.  Recent work 
has focused on the task of unsupervised learning of 
OT grammars, where only unstructured surface 
forms are provided to the learner. Some of this 
work focuses on grammar learning without training 
data (Tesar, 1998; Tesar, 1999; Hayes, 2004; 
Apoussidou and Boersma, 2004). The remainder of 
this work tackles the problem of learning the 
ranking and lexicon simultaneously, the problem 
addressed in the present paper (Tesar et al, 2003; 
Tesar, 2004; Tesar and Prince, to appear; Merchant 
and Tesar, to appear). These proposals adopt an 
algebraic approach wherein learning the lexicon 
involves iteratively eliminating potential 
underlying forms by determining that they have 
become logically impossible, given certain 
assumptions about the learning problem.1 In 
particular, one simplifying assumption of previous 
work requires that mappings be one-to-one and 
onto.  This assumption prohibits input-output 
mappings with deletion and insertion as well as 
                                                        
1
 An alternative algorithm is proposed in Escudero (2005), but 
it has not been tested computationally. 
50
constraints that evaluate such mappings. This work 
represents a leap forward toward the accurate 
modeling of human language acquisition, but the 
identification of a general-purpose, unsupervised 
learner of OT remains an open problem. 
In contrast to previous work, this paper proposes 
a gradual, probabilistic algorithm for unsupervised 
OT learning based on the Expectation Maximiza-
tion algorithm (Dempster et al, 1977). Because the 
algorithm depends on gradually maximizing an 
objective function, rather than on wholly eliminat-
ing logically impossible hypotheses, it is not cru-
cial to prohibit insertion or deletion.   
A major challenge posed by unsupervised learn-
ing of OT is that of learning restrictive grammars 
that generate only grammatical forms.  In previous 
work, the preference for restrictive grammars is 
implemented by encoding a bias into the ranking 
algorithm that favors ranking constraints that pro-
hibit marked structures as high as possible. In con-
trast, the solution proposed here involves a 
combination of likelihood maximization and ex-
plicit reliance on Richness of the Base, an OT prin-
ciple requiring that the set of potential underlying 
forms be universal.  This combination favors re-
strictive grammars because grammars that map a 
?rich? lexicon onto observed forms with high 
probability are preferred. The proposed model is 
tested on three constructed language systems, each 
exemplifying a different type of hidden structure. 
2 Learning Probabilistic OT 
While the primary task of the grammar is to map 
underlying forms to overt forms, the grammar?s 
secondary role is that of a filter ? ruling out un-
grammatical forms no matter what underlying form 
is fed to the grammar. The role of the grammar as 
filter follows from the OT principle of Richness of 
the Base, according to which the set of possible 
underlying forms is universal (Prince and Smolen-
sky 1993). In other words, the grammar must be 
restrictive and not over-generate.  The requirement 
that grammars be restrictive complicates the learn-
ing problem - it is not sufficient to find a combina-
tion of underlying forms and constraint ranking 
that yields the set of observed surface forms: the 
constraint ranking must yield only grammatical 
forms irrespective of the particular lexical items 
selected for the language.  
In classic OT, constraint ranking is categorical 
and non-probabilistic.  In recent years various sto-
chastic versions of OT have been proposed to ac-
count for free variation (Boersma and Hayes, 
2001), lexically conditioned variation (Anttila, 
1997), child language acquisition (Legendre et al, 
2002) and the modeling of frequencies associated 
with these phenomena. In addition to these advan-
tages, probabilistic versions of OT are advanta-
geous from the point of view of learnability. In 
particular, the Gradual Learning Algorithm for 
Stochastic OT (Boersma, 1997, 1998; Boersma and 
Hayes, 2001) is capable of learning in spite of 
noisy training data and is capable of learning vari-
able grammars in a supervised fashion.  In addi-
tion, probabilistic versions of OT and variants of 
OT (Goldwater and Johnson, 2003; Rosenbach and 
Jaeger, 2003) enable learning of OT via likelihood 
maximization, for which there exist many estab-
lished algorithms.  Furthermore, as this paper pro-
poses, unsupervised learning of OT using 
likelihood maximization combined with Richness 
of the Base provides a natural solution to the 
grammar-as-filter problem due to the power of 
probabilistic modeling to use negative evidence 
implicitly. 
The algorithm proposed here relies on a prob-
abilistic extension of OT in which each possible 
constraint ranking is assigned a probability P(r). 
Thus, the OT grammar is a probability distribution 
over constraint rankings rather than a single con-
straint ranking. This notion of probabilistic OT is 
similar to - but less restricted than - Stochastic OT, 
in which the distribution over possible rankings is 
given by the joint probability over independently 
normally distributed constraints with fixed, equal 
variance.  The advantage of the present model is 
computational simplicity, but the proposed learn-
ing algorithm does not depend on any particular 
instantiation of probabilistic OT. 
Tables 1 and 2 illustrate the proposed probabilis-
tic version of OT with an abstract example.   Table 
1 shows the violation marks assigned by three con-
straints, A, B and C, to five candidate outputs O1-
O5 for the underlying form, or input /I/.  To com-
pute the winner of an optimization, constraints are 
applied to the candidate set in order according to 
their rank. Candidates continue to the next con-
straint if they have the fewest (or tie for fewest) 
constraint violation marks (indicated by asterisks). 
In this way the winning or optimal candidate, the 
51
candidate that violates the higher-ranked con-
straints the least, is selected.  
 
  constraints 
input: /I/ A B C 
O1 * *  
O2 **  * 
O3  **  
O4  * ** 
ca
n
di
da
te
s 
O5 *  ** 
Table 1. OT Candidates and Constraint Violations 
 
The third column of Table 2 identifies the win-
ner under each possible ranking of the three con-
straints. For example, if the ranking is A >> B >> 
C, constraint A eliminates all but O3 and O4, then 
constraint B eliminates O3, designating O4 as the 
winner. The remainder of Table 2 illustrates the 
proposed probabilistic instantiation of OT.  The 
first column shows the probability P(r) that the 
grammar assigns to each ranking in this example. 
The probability of each ranking determines the 
probability with which the winner under that rank-
ing will be selected for the given input. In other 
words, it defines the conditional probability Pr(Ok | 
I), shown in the fourth column, of the kth output 
candidate given the input /I/ under the ranking r. 
The last column shows the total conditional prob-
ability for each candidate after summing across 
rankings.  For instance, O3 is the winner under two 
of the rankings, and thus its total conditional prob-
ability P(O3 | I) is found by summing over the con-
ditional probabilities under each ranking. The total 
conditional probability P(O3 | I) refers to the prob-
ability that underlying form /I/ will surface as O3, 
and this probability depends on the grammar. 
 
P(r) ranking winner Pr(Ok | I) P(Ok | I) 
0.20 A>>B>>C O4 0.2 0.2 
0.15 A>>C>>B O3 0.15 
0.05 C>>A>>B O3 0.05 
0.2 
0.10 B>>A>>C O5 0.1 0.1 
0.00 B>>C>>A O2 0.0 0.0 
0.50 C>>B>>A O1 0.5 0.5 
Table 2: Probabilistic OT  
 
In addition to the conditional probability as-
signed by the grammar, this model relies on a 
probability distribution P(I | M) over possible un-
derlying forms for a given morpheme M.  This 
property of the model implements the standard lin-
guistic proposition that each morpheme has a con-
sistent underlying form across contexts, while the 
grammar drives allomorphic variation that may 
result in the morpheme having different surface 
realizations in different contexts.  Rather than iden-
tifying a single underlying form for each mor-
pheme, this model represents the underlying form 
as a distribution over possible underlying forms, 
and this distribution is constant across contexts. To 
determine the probability of an underlying form for 
a morphologically complex word, the product of 
the morpheme?s individual distributions is taken ? 
the probability of an underlying form is taken to be 
independent of morphological context. For exam-
ple, suppose that some morpheme Mk has two pos-
sible underlying forms, I1 and I2, and the two 
underlying forms are equally likely.  This means 
that the conditional probabilities of both underly-
ing forms are 50%: P(I1 | Mk) = P(I2 | Mk) = 50%. 
In sum, the probabilistic model described here 
consists of a grammar and lexicon, both of which 
are probabilistic.  The task of learning involves 
selecting the appropriate parameter settings of both 
the grammar and lexicon simultaneously. 
3 Expectation Maximization and Richness 
of the Base in OT  
This section presents the details of the learning 
algorithm for probabilistic OT.  First, in Section 
3.1 the objective function and its properties are 
discussed.  Next, Section 3.2 proposes the solution 
to the grammar-as-filter problem, which involves 
restricting the search space available to the learn-
ing algorithm. Finally, Section 3.3 describes the 
likelihood maximization algorithm ? the input to 
the algorithm, the initial state, and the form of the 
solution.  
3.1 The Objective Function 
The learning algorithm relies on the following ob-
jective function: 
 
PH (O | M) = [PH (Ok | Mk )]Fk
k
?
= [ PH (Ok & Ik, j | Mk )
j
 
]Fk
k
?
= [ PH (Ok | Ik, j )PH (Ik, j | Mk )
j
 
]Fk
k
?
 
52
The likelihood of the data, or set of overt surface 
forms, PH(O | M) depends on the parameter set-
tings, the probability distributions over rankings 
and underlying forms, under the hypothesis H.  It 
is also conditional on M, the set of observed mor-
phemes, which are annotated in the data provided 
to the algorithm. M is constant, however, and does 
not differ between hypotheses for the same data 
set. Under this model each unique surface form Ok 
is treated independently, and the likelihood of the 
data is simply the product of the probability of 
each surface form, raised to the power correspond-
ing to its observed frequency Fk.  Each surface 
form Ok is composed of a set of morphemes Mk, 
and each of these morphemes has a set of underly-
ing forms Ik,j. The probability of each surface form 
PH(Ok | Mk) is found by summing the joint distribu-
tion PH(Ok & Ik,j | Mk) over all possible underlying 
forms Ik,J for morphemes Mk that compose Ok.  
Finally, the joint probability is simply the product 
of the conditional probability PH(Ok | Ik,j) and lexi-
cal probability PH(IK,j | Mk), both of which were 
defined in the previous section. 
The primary property of this objective function 
is that it is maximal only when the hypothesis gen-
erates the observed data with high probability.  In 
other words, the grammar must map the selected 
lexicon onto observed surface forms without wast-
ing probability mass on unobserved forms.  Be-
cause there are two parameters in the model, this 
can be accomplished by adjusting the ranking dis-
tributions or by adjusting lexicon distributions.  
The probability model itself does not specify 
whether the grammar or the lexicon should be ad-
justed in order to maximize the objective function.  
In other words, the objective function is indifferent 
to whether the restrictions observed in the lan-
guage are accounted for by having a restrictive 
grammar or by selecting a restrictive lexicon.  As 
discussed in Section 2, according to Richness of 
the Base, only the first option is available in OT: 
the grammar must be restrictive and must neutral-
ize noncontrastive distinctions in the language.  
The next subsection addresses the proposed solu-
tion ? a restriction of the search procedure that fa-
vors maximizing probability by restricting the 
grammar rather than the lexicon. 
3.2 Richness of the Base 
Although the notion of a restrictive grammar is 
intuitively clear, it is difficult to implement for-
mally.  Previous work on OT learnability (Tesar, 
1995; Tesar and Smolensky, 1995; Smolensky 
1996; Tesar, 1998, Tesar, 1999; Tesar et al, 2003; 
Tesar and Prince, to appear; Hayes, 2004) has pro-
posed the heuristic of Markedness over Faithful-
ness during learning to favor restrictive grammars. 
In OT there are two basic types of constraints, 
markedness constraints, which penalize dis-
preferred surface structures, and faithfulness con-
straints, which penalize nonidentical mappings 
from underlying to surface forms. In general, a 
restrictive grammar will have markedness con-
straints ranked high, because these constraints will 
restrict the type of surface forms that are allowed 
in a language. On the other hand, if faithfulness 
constraints are ranked high, all the distinctions in-
troduced into the lexicon will surface.  Thus, a 
heuristic preferring markedness constraints to rank 
high whenever possible does in general prefer re-
strictive grammars.  However, the markedness over 
faithfulness heuristic does not exhaust the notion 
of restrictiveness. In particular, markedness over 
faithfulness does not favor grammar restrictiveness 
that follows from particular rankings between 
markedness constraints or between faithfulness 
constraints. 
This work aims to provide a general solution 
that does not require distinguishing various types 
of constraints ? the proposed solution implements 
Richness of the Base explicitly in the initial state 
of the lexicon. Specifically, the solution involves 
requiring that initial distributions over the lexicon 
be uniform, or rich. Although the objective func-
tion alone does not prefer restrictive grammars 
over restrictive lexicons, a lexicon constrained to 
be uniform, or nonrestrictive, will in turn force the 
grammar to be restrictive.  Another way to think 
about it is that a restrictive grammar is one that 
compresses the input distributions maximally by 
mapping as much of the lexicon onto observed sur-
face forms as possible.  By requiring the lexicon to 
be rich the proposed solution relies on the objec-
tive function?s natural preference for grammars 
that maximally compress the lexicon. The objec-
tive function prefers restrictive grammars in this 
situation because restrictive grammars will allow 
the highest probability to be assigned to observed 
53
forms.  In contrast, if the lexicon is not rich, there 
is nothing for the grammar to compress, and the 
objective function?s natural preference for com-
pression will not be employed. The next subsection 
discusses the algorithm and the initialization of the 
parameters in more detail. 
3.3 Likelihood Maximization Algorithm 
As discussed above, the goal of the learning algo-
rithm is to find the probability distributions over 
rankings and lexicons that maximize the probabil-
ity assigned to the observed set of data according 
to the objective function. In addition, any regulari-
ties present in the data should be accommodated by 
the grammar rather than by restricting the lexicon.  
As in previous work on unsupervised learning of 
OT, the algorithm assumes knowledge of OT con-
straints, the possible underlying forms of overt 
forms, and sets of candidate outputs and their con-
straint violation profiles for all possible underlying 
forms.  While the present version of the algorithm 
receives this information as input, recent work in 
computational OT (Riggle, 2004; Eisner, 2000) 
suggests that this information is formally derivable 
from the constraints and overt surface forms and 
can be generated automatically. 
In addition, the algorithm receives information 
about the morphological relations between ob-
served surface forms.  Specifically, output forms 
are segmented into morphemes, and the mor-
phemes are indexed by a unique identifier. This 
information, which has also been assumed in pre-
vious work, cannot be derived directly from the 
constraints and observed forms but is a necessary 
component of a model that refers to underlying 
forms of morphemes. The present work assumes 
this information is available to the learner although 
Section 5 will discuss the possibility of learning 
these morphological relations in conjunction with 
the learning of phonology. 
The set of potential underlying forms is derived 
from observed surface forms, morphological rela-
tions, and the constraint set.  On the one hand the 
set of potential underlying forms, which is initially 
uniformly distributed, should be rich enough to 
constitute a rich base for the reasons discussed ear-
lier.   On the other hand, the set should be re-
stricted enough so that the search space is not too 
large and so that the grammar is not pressured to 
favor mapping underlying forms to completely 
unrelated surface forms.  For this reason, potential 
underlying forms are derived from surface forms 
by considering all featural variants of surface 
forms for features that are evaluated by the gram-
mar.  Of these potential underlying forms, only 
those that can yield each of the observed surface 
allomorphs of the morpheme under some ranking 
of the constraints are included. This formulation 
differs substantially from previous work, which 
aimed to construct the lexicon via discrete steps, 
the first of which involved permanently setting the 
values for features that do not alternate. In contrast, 
the approach taken here aims to create a rich initial 
lexicon, to compel the selection of a restrictive 
grammar. 
In addition to featural variants, variants of sur-
face forms that differ in length are included if they 
are supported by allomorphic alternation.  In par-
ticular, featural variants of all the observed surface 
allomorphs of the morpheme are considered as po-
tential underlying forms for the morpheme if each 
of the observed surface forms can be generated 
under some ranking.  Including these types of un-
derlying forms extends previous work, which did 
not allow segmental insertion or deletion or con-
straints that evaluate these unfaithful mappings, 
such as MAX and DEP. 
The algorithm initializes both the lexicon and 
grammar to uniform probability distributions.  This 
means that all rankings are initially equally likely.  
Likewise, all potential underlying forms for a mor-
pheme are initially equally likely.  Thus, the prob-
ability distributions begin unbiased, but choosing 
an unbiased lexicon initially begins the search 
through parameter space at a position that favors 
restrictive grammars.  The experiments in the fol-
lowing section suggest that this choice of initializa-
tion correctly selects a restrictive final grammar. 
The learning algorithm itself is based on the Ex-
pectation Maximization algorithm (Dempster et al, 
1977) and alternates between an expectation stage 
and a maximization stage.  During the expectation 
stage the algorithm computes the likelihood of the 
observed surface forms under the current hypothe-
sis.  During the maximization stage the algorithm 
adjusts the grammar and lexicon distributions in 
order to increase the likelihood of the data.  The 
probability distribution over rankings is adjusted 
according to the following re-estimation formula: 
 
54
PH +1(r) =
Fk
Fk
k
 
?
PH (Ok | r,Mk )
PH (Ok | Mk )k
 
 
Intuitively, this formula re-estimates the prob-
ability of a ranking for state H+1 in proportion to 
the ranking?s contribution to the overall probability 
at state H. The algorithm re-estimates the probabil-
ity distribution for an underlying form according to 
an analogous formula: 
 
PH +1(Ik, j | M i) = FkFk
k

?
PH (Ok & Ik, j | M i)
PH (Ok | M i)k

 
Intuitively, the re-estimate of the probability of 
an underlying form Ik,j for state H+1 is propor-
tional to the contribution that underlying form 
makes to the total probability due to morpheme Mi 
at state H. The algorithm continues to alternate 
between the two stages until the distributions con-
verge, or until the change between one stage and 
the next reaches some predetermined minimum. At 
this point the resulting distributions are taken to 
correspond to the learned grammar and lexicon.  
4 Experiments  
This section describes the results of experiments 
with three artificial language systems with differ-
ent types of hidden structure. In all experiments 
presented here, each unique surface form is as-
sumed to occur with frequency 1. 
4.1 Voicing Neutralization 
The first test set is an artificial language system 
(Tesar and Prince, to appear) exhibiting voicing 
neutralization. The constraint set includes five con-
straints: 
 
? NOVOI - No voiced obstruents 
? NOSFV- No syllable-final voiced obstruents 
? IVV - No intervocalic voiceless consonants 
? IDVOI - Surface voicing must match underly-
ing voicing 
? MAX - Input segments must have output cor-
respondents 
 
These five constraints can describe a number of 
languages, but of particular interest are languages 
in which voicing contrasts are neutralized in one or 
more positions.  Such languages, three of which 
are shown below, test the algorithm?s ability to 
identify correct and restrictive grammars. The par-
tial rankings shown below correspond to the neces-
sary rankings that must hold for these languages; 
each partial ranking actually corresponds to several 
total rankings of the constraints. Also shown below 
are the morphologically analyzed surface forms for 
each language that are provided as input to the al-
gorithm. The subscripts in these forms indicate 
morpheme identities, while the hyphens segment 
the words into separate morphemes.  For example, 
tat1,2 means that the surface form ?tat? could be 
derived from either morpheme 1 or 2 in this lan-
guage. 
   
? (A) Final devoicing, contrast intervocalically: 
? NOSFV, MAX >> IDVOI >> IVV, NOVOI 
? tat1,2; dat3,4; tat1-e5; tad2-e5; dat3-e5; dad4-e5 
 
? (B) Final devoicing and intervocalic voicing: 
? NOSFV, MAX, IVV >> IDVOI, NOVOI 
? tat1,2; dat3,4; tad1,2-e5; dad3,4-e5 
 
? (C) No voiced obstruents: 
? MAX, NOVOI >> IDVOI, IVV 
? tat1,2,3,4; tat1,2,3,4-e5  
 
In language C, it would be possible to maximize 
the objective function by selecting a restrictive 
lexicon rather than a restrictive grammar.  In par-
ticular, /tat/ could be selected as the underlying 
form for morphemes 1-4 in order to account for the 
lack of voiced obstruents in the observed surface 
forms.  In this case, the objective function could 
just as well be satisfied by an identity grammar 
mapping underlying /tat/ to surface ?tat?. However, 
as discussed in Section 2, such a grammar would 
violate the principle of Richness of the Base by 
putting the restriction against voiced obstruents 
into the lexicon rather than the grammar. Thus, this 
language tests not only whether the algorithm finds 
a maximum, but also whether the maximum corre-
sponds to a restrictive grammar. 
In fact, for all three languages above, the algo-
rithm converges on the correct, restrictive gram-
mars and correct lexicons.  Specifically, the final 
grammars for each of the languages above con-
verge on probability distributions that distribute the 
probability mass equally among the total rankings 
consistent with the partial orders above.  For ex-
ample, for language C the algorithm converges on 
55
a distribution that assigns equal probability to the 
20 total rankings consistent with the partial order 
given by MAX, NOVOI >> IDVOI, IVV.  
The initial uniform lexicon for language C is 
shown in Table 3.  Here the numbers 1-5 refer to 
morpheme indices, and the possible underlying 
forms for each morpheme are uniformly distrib-
uted. This initial lexicon favors a grammar that can 
map as much of the rich lexicon as possible onto 
surface forms with no voiced obstruents. With 
these constraints, this translates into ranking 
NOVOI above IDVOI and IVV.  As the algorithm 
begins learning the lexicon and continues to refine 
its hypothesis for this language, nothing drives the 
algorithm to abandon the initial rich lexicon. Thus, 
in the final state, the lexicon for this language is 
identical to the initial lexicon.  In general, the final 
lexicon will be uniformly distributed over underly-
ing forms that differ in noncontrastive features. 
 
1 /tat/ - 25% /tad/ - 25% /dat/ - 25% /dad/ - 25% 
2 /tat/ - 25% /tad/ - 25% /dat/ - 25% /dad/ - 25% 
3 /tat/ - 25% /tad/ - 25% /dat/ - 25% /dad/ - 25% 
4 /tat/ - 25% /tad/ - 25% /dat/ - 25% /dad/ - 25% 
5 /e/ - 100%    
Table 3. Initial Lexicon for Language C 
4.2 Grammatical and Lexical Stress 
The next set of languages from the PAKA system 
(Tesar et al, 2003) test the ability of the algorithm 
to identify grammatical stress (most restrictive), 
lexical stress (least restrictive), and combinations 
of the two. The constraint set includes: 
 
? MAINLEFT - Stress the leftmost syllable 
? MAINRIGHT - Stress the rightmost syllable 
? FAITHACCENT - Stress an accented syllable 
? FAITHACCENTROOT - Stress an accented root 
syllable 
 
Possible languages and their corresponding par-
tial orders ranging from least restrictive to most 
restrictive are shown below.  In the first two lan-
guages, the least restrictive languages, lexical dis-
tinctions in stress are realized faithfully, while 
grammatical stress surfaces only in forms with no 
underlying stress. In the final two languages stress 
is entirely grammatical; underlying distinctions are 
neutralized in favor of a regular surface stress pat-
tern.  Finally, the middle language is a combination 
of lexical and grammatical stress, requiring that the 
algorithm learn that a contrast in roots is preserved, 
while a contrast in suffixes is neutralized. 
 
? Full contrast: roots and suffixes contrast in 
stress, default left: 
? F >> ML >> MR, FAR 
? p?1-ka3; pa1-g?4; b?2-ka3; b?2-ga4 
? Full contrast: roots and suffixes contrast in 
stress, default right: 
? F >> MR >> ML, FAR 
? pa1-k?3; pa1-g?4; b?2-ka3; ba2-g?4 
? Root contrast only, default right: 
? FAR >> MR >> ML 
? pa1-k?3; pa1-g?4; b?2-ka3; b?2-ga4 
? Predictable left stress: 
? ML >> FAR, F, MR 
? p?1-ka3; p?1-ga4; b?2-ka3; b?2-ga4 
? Predictable right stress: 
? MR >> FAR, F, ML 
? pa1-k?3; pa1-g?4; ba2-k?3; ba2-g?4 
 
In all cases the algorithm learns the correct, re-
strictive grammars corresponding to the partial 
orders shown above.  As before, the final lexicon 
assigns uniform probability to all underlying forms 
that differ in noncontrastive features.  For example, 
in the case of the language with root contrast only, 
the final lexicon selects a unique lexical item for 
root morphemes and maintains a uniform probabil-
ity distribution over stressed and unstressed under-
lying forms for suffixes. 
4.3 Abstract Underlying Vowels 
The final experiment tests the algorithm on an 
artificial language, based on Polish, with abstract 
underlying vowels that never surface faithfully. 
Although the particular phenomenon exhibited by 
Slavic alternating vowels is rare, the general phe-
nomenon wherein underlying forms do not corre-
spond to any surface allomorph is not uncommon 
and should be accommodated by the learning algo-
rithm. This language presents a challenge for pre-
vious work on unsupervised learning of OT 
because alternations in the number of segments are 
observed in morpheme 3. The morphologically 
56
annotated input to the algorithm for this language 
is shown in Table 4.  
 
kater1 vatr2 sater3 
kater1-a4 vatr2-a4 satr3-a4 
Table 4. Yer Language Surface Forms 
 
In this language morphemes 1, 2 and 4 exhibit no 
alternation while morpheme 3 alternates between 
sater and satr depending on the context. The con-
straints for this language, based on Jarosz (2005), 
are shown below:  
 
? *E = *[+HIGH][-ATR] 
? DEP-V 
? MAX-V 
? *COMPLEXCODA 
? IDENT[HIGH] 
 
1 2 3 4 
/kater/ /vatr/ /satEr/ /-a/ 
Table 5. Desired Final Lexicon 
 
In the proposed analysis of this language, the ab-
stract underlying [E], which is a [+high] version of 
[e], is neutralized on the surface and exhibits two 
repairs systematically depending on the context. It 
deletes in general, but if a complex coda is at stake, 
the vowel surfaces as [e] by violating 
IDENT[HIGH]. The required partial ranking for this 
language is shown below while the desired lexicon 
is shown in Table 5. 
 
{*E, {DEP-V >> *COMPLEXCODA }} >> 
IDENT[HIGH] >> MAX-V 
The algorithm successfully learns the correct rank-
ing above and the lexicon in Table 5.  Specifically, 
the final grammar assigns equal probability to all 
the rankings consistent with the above partial or-
der. The final lexicon selects a single underlying 
form for each morpheme as shown in Table 5 be-
cause all underlying distinctions in this language 
are contrastive. 
4.4 Discussion 
In summary, the algorithm is able to find a cor-
rect grammar and lexicon combination for all of 
the language systems discussed.  As discussed in 
Section 3, the objective function itself does not 
favor restrictive grammars, but the ability of the 
algorithm to learn restrictive grammars in these 
experiments suggests that initializing the lexicons 
to uniform distributions does compel the learning 
algorithm to select restrictive grammars rather than 
restrictive lexicons. 
While the experiments presented in this section 
focus on the task of learning a grammar and lexi-
con simultaneously, the proposed algorithm is also 
capable of learning grammars from structurally 
ambiguous forms. The same likelihood maximiza-
tion procedure proposed here could be used for 
unsupervised learning of grammars that assign full 
structural description to overt forms. Future direc-
tions include testing the algorithm on language 
data of this sort. 
5 Conclusion  
In sum, this paper has presented an unsupervised, 
probabilistic algorithm for OT learning. The paper 
argues that combining the OT principle of Rich-
ness of the Base and likelihood maximization pro-
vides a novel and general solution to the problem 
of finding a restrictive grammar.  The proposed 
solution involves explicitly implementing Richness 
of the Base in the initialization of the lexicon in 
order to fully utilize the properties of the objective 
function. By relying on Richness of the Base and 
likelihood maximization, the algorithm is able to 
use negative evidence implicitly to find restrictive 
grammars. The algorithm is shown to be successful 
on three constructed languages featuring different 
types of neutralization and hidden structure. 
One potential extension of the proposed algo-
rithm involves combining a system for unsuper-
vised learning of morphological relations with the 
proposed algorithm for learning phonology.  Sev-
eral algorithms have been proposed for automati-
cally inducing morphological relations, like those 
assumed by the present learner (Goldsmith, 2001; 
Snover and Brent, 2001). The task of uncovering 
morphological relations is complicated by allo-
morphic alternations that obscure the underlying 
identity of related morphemes. While these algo-
rithms are very promising, their performance may 
be significantly enhanced if they were combined 
with an algorithm that models such phonological 
alternations.   
In conclusion, this is the first proposed unsuper-
vised algorithm for OT learning that takes advan-
57
tage of the power of probabilistic modeling to learn 
a grammar and lexicon simultaneously. This paper 
demonstrates that combining OT theoretic princi-
ples with results from computational language 
learning is a worthwhile pursuit that may inform 
both disciplines.  In this case the theoretical princi-
ple of Richness of the Base has provided a novel 
solution to a learning problem, but at the same 
time, this work also informs theoretical OT by 
providing a formal characterization of this theo-
retical principle. Future work includes testing on 
larger, more realistic languages, including lan-
guage data with noise and variation, in order to 
determine the algorithm?s resistance to noise and 
ability to model variable grammars like those ob-
served in natural languages and in human language 
acquisition.  
Acknowledgements 
I would like to thank Paul Smolensky for his in-
valuable feedback on this work and for his sugges-
tions on the preparation of this paper.  I am also 
grateful to Luigi Burzio, Robert Frank, Jason Eis-
ner, and members of the Johns Hopkins Linguistics 
Research Group (especially Joan Chen-Main, 
Adam Wayment, and Sara Finley) for additional 
comments and helpful discussion. 
References  
Apoussidou, Diana and Paul Boersma. 2004. Compar-
ing Different Optimality-Theoretic Learning Algo-
rithms:the Case of Metrical Phonology. Proceedings 
of the 2004 Spring Symposium Series of the Ameri-
can Association for Artificial Intelligence. 
Anttila, Arto. 1997. Deriving variation from grammar. 
In F. Hinskens, R. Van Hout and W. L. Wetzels 
(eds.) Variation, Change and Phonological Theory.  
Amsterdam, John Benjamins. 
Boersma, Paul. 1997. How we Learn Variation, Option-
ality, and Probability. Proc. Institute of Phonetic Sci-
ences of the University of Amsterdam 21:43-58. 
Boersma, P. 1998. Functional Phonology. Doctoral Dis-
sertation, University of Amsterdam. The Hague: Hol-
land Academic Graphics. 
Boersma, P. and B. Hayes. 2001. Empirical Tests of the 
Gradual Learning Algorithm. Linguistic Inquiry 
32(1):45-86. 
Dempster, A.P., N.M. Laird, and D.B. Rubin. 1977. 
Maximum Likelihood from incomplete data via the 
EM Algorithm. Journal of Royal Statistics Society. 
39(B):1-38 
Eisner, Jason. 2000. Easy and hard constraint ranking in 
optimality theory: Algorithms and complexity. In Ja-
son Eisner, Lauri Karttunen and Alain Th?riault 
(eds.), Finite-State Phonology: Proceedings of the 
5th Workshop of the ACL Special Interest Group in 
Computational Phonology (SIGPHON), pages 22-33, 
Luxembourg, August. 
Escudero, Paola. 2005. Linguistic Perception and Sec-
ond Language Acquisition.Explaining the attainment 
of optimal phonological categorization. Doctoral dis-
sertation, Utrecht University. 
Goldsmith, John. 2001. Unsupervised Learning of Mor-
phology of a Natural Language. Computational Lin-
guistics, 27: 153-198. 
Goldwater, Sharon and Mark Johnson. 2003. Learning 
OT constraint rankings using a maximum entropy 
model. In Jennifer Spenader, Anders Eriksson and 
Osten Dahl (eds.), Proceedings of the Stockholm 
Workshop on Variation within Optimality Theory. 
Stockholm University, pages 111-120. 
Hayes, Bruce. 2004. Phonological acquisition in Opti-
mality Theory:  the early stages. Appeared 2004 in 
Kager, Rene, Pater, Joe, and Zonneveld, Wim, (eds.), 
Fixing Priorities: Constraints in Phonological Ac-
quisition. Cambridge University Press.  
Jarosz, Gaja. 2005. Polish Yers and the Finer Structure 
of Output-Output Correspondence. 31st Annual Meet-
ing of the Berkeley Linguistics Society, Berkeley, 
California. 
Lari, K. and S.J. Young. 1990. The estimation of sto-
chastic context-free grammars using the inside-
outside algorithm.  Computer Speech and Language. 
4:35-56 
Legendre, Geraldine, Paul Hagstrom, Anne Vainikka 
and Marina Todorova. 2002. Partial Constraint Or-
dering in Child French Syntax. to appear Language 
Acquisition 10(3). 189-227. 
Merchant, Nazarr?, and Bruce Tesar. to appear. Learn-
ing underlying forms by searching restricted lexical 
subspaces. In The Proceedings of Chicago Linguis-
tics Society 41. ROA-811. 
Pereira, F. and Y. Schabes. 1992.  Inside-Outside re-
estimation from partially bracketed corpora. In Pro-
ceedings of the ACL 1992, Newark, Delaware. 
Prince, Alan and Paul Smolensky. 1993. Optimality 
Theory: Constraint Interaction in Generative Gram-
mar.  Technical Report 2, Center for Cognitive Sci-
ence, Rutgers University. 
58
Prince, Alan, and Bruce Tesar. 1999. Learning phono-
tactic distributions. Technical Report RuCCS-TR-54, 
Rutgers Center for Cognitive Science, Rutgers Uni-
versity. 
Riggle, Jason. 2004. Generation, Recognition, and 
Learning in Finite State Optimality Theory. Ph.D. 
Dissertation, UCLA, Los Angeles, California. 
Rosenbach, Anette and Gerhard Jaeger. 2003. Cumula-
tivity in Variation: testing different versions of Sto-
chastic OT empirically. Presented at the Seventh 
Workshop on Optimality Theoretic Syntax, Univer-
sity of Nijmegen.  
Smolensky, Paul. 1996. The initial state and `richness of 
the base' in Optimality Theory. Technical Report 
JHU-CogSci-96-4, Department of Cognitive Science, 
Johns Hopkins University. 
Snover, Matthew and Michael R. Brent. 2001 A Bayes-
ian Model for Morpheme and Paradigm Identifica-
tion. In Proceedings of the 39th Annual Meeting of 
the ACL, pages 482-490. Association for Computa-
tional Linguistics. 
Tesar, Bruce. 1995. Computational Optimality Theory. 
Ph.D. thesis, University of Colorado at Boulder, 
June. 
Tesar, Bruce. 1998. An iterative strategy for language 
learning. Lingua 104:131-145. ROA-177. 
Tesar, Bruce. 1999. Robust interpretive parsing in met-
rical stress theory. In The Proceedings of Seventeenth 
West Coast Conference on Formal Linguistics, pp. 
625-639. ROA-262. 
Tesar, Bruce. 2004. Contrast analysis in phonological 
learning. Manuscript, Linguistics Dept., Rutgers 
University. ROA-695.  
Tesar, Bruce, John Alderete, Graham Horwood, Nazarr? 
Merchant, Koichi Nishitani, and Alan Prince. 2003. 
?Surgery in language learning?. In The Proceedings 
of Twenty-Second West Coast Conference on Formal 
Linguistics, pp. 477-490. ROA-619. 
Tesar, Bruce and Alan Prince. to appear. ?Using phono-
tactics to learn phonological alternations.?  Revised 
version will appear in The Proceedings of CLS 39, 
Vol. II: The Panels. ROA-620. 
Tesar, Bruce and Paul Smolensky. 1995. ?The Learn-
ability of Optimality Theory?. In Proceedings of the 
Thirteenth West Coast Conference on Formal Lin-
guistics, 122-137. 
 
59
Unsupervised Learning of Morphology Using a Novel Directed Search
Algorithm: Taking the First Step
Matthew G. Snover and Gaja E. Jarosz and Michael R. Brent
Department of Computer Science
Washington University
St Louis, MO, USA, 63130-4809
 
ms9, gaja, brent  @cs.wustl.edu
Abstract
This paper describes a system for the un-
supervised learning of morphological suf-
fixes and stems from word lists. The sys-
tem is composed of a generative probabil-
ity model and a novel search algorithm.
By examining morphologically rich sub-
sets of an input lexicon, the search identi-
fies highly productive paradigms. Quanti-
tative results are shown by measuring the
accuracy of the morphological relations
identified. Experiments in English and
Polish, as well as comparisons with other
recent unsupervised morphology learning
algorithms demonstrate the effectiveness
of this technique.
1 Introduction
There are numerous languages for which no anno-
tated corpora exist but for which there exists an
abundance of unannotated orthographic text. It is
extremely time-consuming and expensive to cre-
ate a corpus annotated for morphological structure
by hand. Furthermore, a preliminary, conservative
analysis of a language?s morphology would be use-
ful in discovering linguistic structure beyond the
word level. For instance, morphology may provide
information about the syntactic categories to which
words belong, knowledge which could be used by
parsing algorithms. From a cognitive perspective, it
is crucial to determine whether the amount of infor-
mation found in pure speech is sufficient for discov-
ering the level of morphological structure that chil-
dren are able to find without any direct supervision.
Thus, we believe the task of automatically discover-
ing a conservative estimate of the orthographically-
based morphological structure in a language inde-
pendent manner is a useful one.
Additionally, an initial description of a lan-
guage?s morphology could provide a starting
point for supervised morphological mod-
els, such as the memory-based algorithm of
Van den Bosch and Daelemans (1999), which can-
not be used on languages for which annotated data
is unavailable.
During the last decade several minimally super-
vised and unsupervised algorithms that address the
problem have been developed. Gaussier (1999) de-
scribes an explicitly probabilistic system that is
based primarily on spellings. It is an unsupervised
algorithm, but requires the tweaking of parameters
to tune it to the target language. Brent (1993) and
Brent et al (1995), described Minimum Description
Length, (MDL), systems. One approach used only
the spellings of the words; another attempted to find
the set of suffixes in the language used the syntactic
categories from a tagged corpus as well. While both
are unsupervised, the latter is not knowledge free
and requires data that is tagged for part of speech,
making it less suitable for analyzing under examined
languages.
A similar MDL approach is described by
Goldsmith (2001). It is ideal in being both knowl-
edge free and unsupervised. The difficulty lies in
Goldsmith?s liberal definition of morphology which
he uses to evaluate with; a more conservative ap-
proach would seem to be a better hypothesis to boot-
strap from.
We previously, Snover and Brent (2001), pre-
sented a very conservative unsupervised system,
                     July 2002, pp. 11-20.  Association for Computational Linguistics.
        ACL Special Interest Group in Computational Phonology (SIGPHON), Philadelphia,
       Morphological and Phonological Learning: Proceedings of the 6th Workshop of the
which uses a generative probability model and a hill
climbing search. No quantitative studies had been
conducted on it, and it appears that the hill-climbing
search used limits that system?s usefulness. We have
developed a system based on a novel search and
an extension of the previous probability model of
Snover and Brent.
The use of probabilistic models is equivalent to
minimum description length models. Searching for
the most probable hypothesis is just as compelling
as searching for the smallest hypothesis and a model
formulated in one framework can, through some
mathematical manipulation, be reformulated into the
other framework. By taking the negative log of a
probability distribution, one can find the number of
bits required to encode a value according to that dis-
tribution. Our system does not use the minimum de-
scription length principle but could easily be refor-
mulated to do so.
Our goal in designing this system, is to be able to
detect the final stem and suffix break of each word
given a list of the most common words in a language.
We do not distinguish between derivational and in-
flectional suffixation or between the notion of a stem
and a base. Our probability model differs slightly
from that of Snover and Brent (2001), but the main
difference is in the search technique. We find and
analyze subsets of the lexicon to find good solutions
for a small set of words. We then combine these sub-
hypotheses to form a morphological analysis of the
entire input lexicon. We do not attempt to learn pre-
fixes, infixes, or other more complex morphological
systems, such as template-based morphology: we
are attempting to discover the component of many
morphological systems that is strictly concatenative.
Finally, our model does not currently have a mecha-
nism to deal with multiple interpretations of a word,
or to deal with morphological ambiguity.
2 Probability Model
This section introduces a prior probability distribu-
tion over the space of all hypotheses, where a hy-
pothesis is a set of words, each with morphological
split separating the stem and suffix. The distribution
is based on a seven-model model for the generation
of hypothesis, which is heavily based upon the prob-
ability model presented in Snover and Brent (2001),
with steps 1-3 of the generative procedure being the
same. The two models diverge at step 4 with the
pairing of stems and suffixes. Whereas the previ-
ous model paired individual stems with suffixes, our
new model uses the abstract structure of paradigms.
A paradigm is a set of suffixes and the stems that
attach to those suffixes and no others. Each stem is
in exactly one paradigm, and each paradigm has at
least one stem. This is an important improvement
to the model as it takes into account the patterns in
which stems and suffixes attach.
The seven steps are presented below, along with
their probability distributions and a running exam-
ple of how a hypothesis could be generated by this
process. By taking the product over the distributions
from all of the steps of the generative process, one
can calculate the prior probability for any given hy-
pothesis. What is described in this section is a math-
ematical model and not an algorithm intended to be
run.
1. Choose the number of stems,   , according to
the distribution:

 
	



 

(1)
The 	 
  term normalizes the inverse-squared
distribution on the positive integers. The num-
ber of suffixes,  is chosen according to the
same probability distribution. The symbols M
for steMs and X for suffiXes are used through-
out this paper.
Example:   = 5.  = 3.
2. For each stem  , choose its length in letters  ,
according to the inverse squared distribution.
Assuming that the lengths are chosen indepen-
dently and multiplying together their probabil-
ities we have:
Proceedings of the 2014 Joint Meeting of SIGMORPHON and SIGFSM, pages 19?28,
Baltimore, Maryland USA, June 27 2014. c?2014 Association for Computational Linguistics
Comparing Models of Phonotactics for Word Segmentation 
   
    Abstract 
Developmental research indicates that infants use low-level statistical regularities, or pho-notactics, to segment words from continuous speech. In this paper, we present a segmenta-tion framework that enables the direct com-parison of different phonotactic models for segmentation. We compare a model using phoneme transitional probabilities, which have been widely used in computational models, to syllable-based bigram models, which have played a prominent role in the developmental literature. We also introduce a novel estimation method, and compare it to other strategies for estimating the parameters of the phonotactic models from unsegmented data. The results show that syllable-based models outperform the phoneme models, specifically in the context of improved unsu-pervised parameter estimation. The syllable-based transitional probability model achieves a word token f-score of nearly 80%, the high-est reported performance for a phonotactic segmentation model with no lexicon. 1 Introduction One of the first language learning tasks infants must solve is the segmentation of fluent speech into words. Extensive experimental work has demonstrated that infants are able to use phono-tactic restrictions (Jusczyk & Luce, 1994; Mattys et al., 1999; Mattys & Jusczyk, 2001) and other low-level statistical regularities (Saffran et al., 1996; Thiessen & Saffran, 2003; Pelucchi et al., 2009) to extract words from fluent speech before the age of one. This work has shown that infants utilize these low-level statistical regularities to segment speech during the second half of the first year of life before they have developed ex-tensive vocabularies that could provide top-down lexical information to guide segmentation. De-
velopmental research indicates that on average infants know fewer than 100 word types during this period (Dale & Fenson, 1996; Daland & Pierrehumbert, 2011).  One statistical cue that has received a great deal of support in experimental work on infant speech segmentation is transitional probability calculated over syllables. In foundational work, Saffran et al. (1996) found that infants are able to segment words from continuous speech using statistical regularities between syllables. Numer-ous subsequent studies have confirmed that in-fants can track transitional probabilities and use them to segment speech (Aslin et al., 1998; Thiessen & Saffran, 2003; Pelucchi et al., 2009).  Despite the extensive experimental literature demonstrating infants? sensitivity to transitional probability in an artificial language learning set-ting, the utility of these statistical cues in a natu-ral language learning context is disputed. Yang (2004) shows that a segmentation strategy rely-ing on transitional probabilities over syllables achieves very poor results on English child-directed speech, even when the input is perfectly syllabified. Yang implements the local minimum segmentation strategy proposed by Saffran et al. (1996) wherein word boundaries are posited at syllable transitions whenever the transitional probabilities at these positions are lower than at the neighboring transitions. He reports that this strategy discovers a mere 23% of target words and posits incorrect words nearly 60% of the time. Swingley (2005) argues that statistical cues calculated over syllables can provide sufficient information for infants to begin building an ini-tial lexicon. However, the learning strategy ex-plored by Swingley is highly conservative, relia-bly detecting only a small proportion of target words in the input. Overall, these results raise questions about whether syllable-based statistics can be reliably used to identify word boundaries in natural language data. 
Natalie M. Schrimpf Department of Linguistics Yale University natalie.schrimpf@yale.edu 
Gaja Jarosz Department of Linguistics Yale University gaja.jarosz@yale.edu 
19
While the experimental work emphasizes syl-lable-level transitional probability, recent com-putational modeling work and corpus analyses have primarily focused on the utility of pho-neme-level statistics. A number of phonotactical-ly-based segmentation models, focusing on the discovery of word boundaries based on pho-neme-level statistics, have achieved more prom-ising results (Adriaans & Kager, 2010; Daland & Pierrehumbert, 2011; see also Brent, 1999). For example, Brent (1999) showed that a local mini-mum strategy relying on phoneme bigrams cor-rectly extracts about 50% of word tokens in Eng-lish child-directed speech. Corpus analyses of child-directed speech have also highlighted the information content of phoneme-level statistics (Hockema, 2006; Jarosz & Johnson, 2013). Re-lated work has shown that phonotactic infor-mation can improve the performance of state-of-the-art segmentation models whose primary ob-jective is to discover the lexicon that underlies the regularities in the continuous speech signal. Again, this work has largely emphasized pho-neme-level statistical cues (Blanchard & Heinz 2008, 2010), and those models that do rely on syllable structure (Johnson, 2008a; Johnson & Goldwater, 2009), do not directly encode sequen-tial statistics between adjacent syllables of the sort investigated in the infant literature. Finally, some models assume computations are per-formed over syllables and that all word bounda-ries in the input are aligned with syllable bounda-ries, but provide no mechanism by which such language-specific syllabification principles could be learned (Yang, 2004; Swingley, 2005; Lignos & Yang, 2010). Overall, the existing evidence clearly shows that there are phonotactic cues to word bounda-ries in spontaneous, child-directed speech. How-ever, there are remaining questions regarding the exact nature of these cues, their reliability, and how they relate to the statistical cues explored in the infant word segmentation literature. In this paper, we investigate the computational mecha-nisms underlying infants? early speech segmenta-tion abilities relying on low-level statistical regu-larities, or phonotactics. We present a computa-tional framework that permits the direct compari-son of segmentation predictions for alternative models of phonotactics. In particular, we com-pare a standard phonotactic model relying on phoneme-level bigrams to two syllable-based phonotactic models relying on transitional prob-abilities. Unlike previous models relying on syl-labified data (Yang, 2004; Swingley, 2005; Lig-
nos & Yang, 2010), we do not assume that word boundaries align with syllable boundaries in the input. Rather, we present a simple syllabification method that can be used to model phonotactic probability for arbitrary strings using statistics estimated from unsyllabified, unsegmented utter-ances. We also compare the local minimum seg-mentation strategy (Saffran et al., 1996; Yang, 2004) to alternatives designed to deal with the challenges of unsupervised estimation of transi-tional probabilities from unsegmented input.  Our focus on the early phonotactic segmenta-tion stage differentiates our approach from many computational models emphasizing the discovery of the lexicon and higher-level language struc-ture (Brent, 1999; Venkataraman, 2001; Swin-gley, 2005; Johnson, 2008a; Goldwater et al., 2009; Johnson & Goldwater, 2009; Blanchard & Heinz 2008, 2010; Lignos & Yang, 2010). It complements that of recent work investigating the use of phoneme-level statistical regularities for segmentation (Adriaans & Kager, 2010; Da-land & Pierrehumbert, 2011). Our work differs from these latter approaches, however, in com-paring several phonotactic models, including ones relying on the syllable-based transitional probability statistics investigated in infant re-search. Our work also contributes to existing segmentation work that assumes a syllabified input (Yang, 2004; Swingley, 2005; Lignos & Yang, 2010) by showing how many aspects of syllable structure can be inferred.  Our results reveal an interaction between es-timation strategy and the choice of phonotactic model. The local minimum segmentation strate-gy works poorly in general for all models con-sidered, but the lowest performance is achieved by the syllable-based models. However, when the same cues are used in the context of a simple, generative probability model with improved un-supervised parameter estimation, the syllable-based models substantially outperform the pho-neme-based models. Indeed, the syllable-based transitional probability phonotactic model achieves a word token segmentation f-score of nearly 80%, which is the highest reported per-formance among purely phonotactically-based segmentation models (Adriaans & Kager, 2010; Daland & Pierrehumbert, 2011). Indeed, this per-formance compares favorably with state-of-the-art segmentation models that involve learning of higher level regularities, such as the lexicon and collocations (Brent, 1999; Venkataraman, 2001; Johnson, 2008a; Goldwater et al., 2009; Johnson & Goldwater, 2009), and demonstrates that good 
20
segmentation performance can be achieved by exploiting simple syllable-level phonotactic cues. 2 Segmentation Model The proposed segmentation model defines the probability of an utterance in terms of an abstract phonotactic probability component that assigns word well-formedness probabilities to phoneme strings. The segmentation algorithm uses those probabilities to determine the maximum likeli-hood segmentation as defined by a simple gener-ative model. Since the phonotactics and segmen-tation components are separate, they can be in-dependently modified. This framework makes it possible to compare models of phonotactics while using the same segmentation strategy.  2.1 Probability Model The segmentation probability model relies on the phonotactic component to assign probabilities to potential words. The probability of a segmenta-tion w is defined in terms of a simple unigram model by multiplying the probabilities of the words !!!!  posited in that segmentation.  1) ! ! ! ! !! ! ! ! !!! !!!!    ! !! !is the probability assigned by the phono-tactic models, which will be defined in the next section. The various phonotactic models change how exactly ! ! ! !is defined, but the segmenta-tion probability always depends directly on the word probabilities given by a particular phono-tactic model. For example, for the utterance [l!k?tmi] ?lookatme?, the segmentation model compares different segmentations, such as [l!k#?#tmi] and [l!k#?t#mi] based on the pho-notactic well-formedness of the posited words. 2.2 Segmentation Algorithm The segmentation algorithm computes and out-puts the segmentation with the highest likeli-hood: !"#$!%! ! ! . The optimal segmenta-tion is found using dynamic programming, as in several previous proposals (Brent, 1999; Venka-taraman, 2001). Given an input utterance, the model considers placing word boundaries at dif-ferent positions within the utterance without re-gard to phonotactics or syllable structure. The phonotactic probability of each posited word is calculated independently as it is considered and used to update the probability of segmentations utilizing that word. In this way, the segmentation component remains entirely divorced from the 
details of the phonotactic models. Crucially, this means the full space of possible segmentations is considered by the segmentation model regardless of the phonotactic model, with no a priori re-strictions imposed by phonotactic or syllable constraints as to where boundaries are permitted.  3 Phonotactic Models We implement and compare several models of phonotactics that are utilized by the segmentation component described above. While all models rely on transitional probabilities, or bigrams, as defined in (2), the unit of analysis varies between the models. One model uses phonemes and pho-neme transitions, and two models incorporate syllable information: we use x to denote a gener-ic unit. The model determines the probability of a word, ! ! !!!! ! ! !  where !!  and !!!! are the word boundary symbol #, by multiplying the probabilities of all bigrams in the word.  2) ! ! ! ! ! !! !! !!!!!! !  The transitional probability for the sequence !!!!! !  can be calculated using relative frequency estimates based on counts ! in the corpus.  3) ! !! !!!! ! !!!!!!!!!!! !!!! ! !   Section 4 describes strategies that we consider for estimating these parameters in an unsuper-vised way from unsegmented data where the on-ly word boundaries are those that coincide with utterance boundaries. 3.1 Phoneme Model The first phonotactic model is a standard pho-neme bigram model that determines the probabil-ity of a word by multiplying the phoneme bi-grams in the word (Jurafsky & Martin, 2008). For example, to calculate the phonotactic proba-bility of the sequence [bot] as a word, this model multiplies together P(b|#)P(o|b)P(t|o)P(#|t).  3.2 Syllable-Based Models The other two phonotactic models use syllables rather than phonemes. One model relies on tran-sitional probabilities over syllables, and the other uses onsets and rhymes as the unit of analysis. 3.2.1 Unsupervised Syllabification The syllabification method relies on the language universal principle of onset maximization to-
21
gether with an inventory of syllable onsets de-rived from the beginnings of utterances. When syllabifying an intervocalic sequence of conso-nants, this method finds the longest legal onset aligned with the right edge and places any re-maining consonants in the coda of the previous syllable. Thus, a sequence like [?tmi] would be syllabified as [?t.mi] in English since [m] but not [tm] occurs utterance-initially. The only lan-guage-particular information required for this approach is knowledge of which phonemes are vowels (syllabic) and which are consonants, a limited type of information also assumed by oth-er syllable inference models for segmentation (Johnson, 2008a; Johnson & Goldwater, 2009). As the segmentation component posits poten-tial words, they are passed to the phonotactic component for syllabification and phonotactic probability calculation. This differs crucially from previous work assuming a fixed syllabifica-tion of the input corpus in which word bounda-ries always align with syllable boundaries (Yang, 2004; Swingley, 2005; Lignos & Yang, 2010). In a setting in which syllabification must be in-ferred from unsegmented utterances, the learner must be capable of assigning syllabification more flexibly since word boundaries do not always align with the syllable boundaries that would be posited for the utterance as a whole.  For exam-ple, the universal onset maximization principle always parses singleton consonants VCV as the onsets V.CV rather than codas VC.V. Therefore, without prior knowledge of word boundaries, the utterance [l!k?tmi] (?look at me?) would be syl-labified as [l!.k?t.mi], and if the segmentation algorithm never considered words that misa-ligned with these syllable boundaries, it would never extract any vowel-initial words like ?at?. Thus, a crucial feature of the current model is that syllabification takes place on a word-by-word basis as potential words are posited. The resulting syllabification for the potential word is used by the syllable-based models to assign pho-notactic probability as discussed below. 3.2.2 Syllable Model The first syllable-based model is one in which bigram transitional probabilities are calculated over syllables. These transitional probabilities are precisely those discussed earlier as having played a prominent role in the infant segmenta-tion literature. The phonotactic probability of a posited word is calculated by multiplying the 
transitional probabilities of all syllable bigrams in the word, including an assumed initial and final #. For example, if the segmentation compo-nent posits a potential word such as [l!k?tmi] ?lookatme?, this sequence is first syllabified us-ing the procedure described earlier as [l!.k?t.mi]. Then the phonotactic probability of this potential word is calculated by multiplying together the syllable-based bigram probabilities: P(l!|#)P(k?t|l!)P(mi|k?t)P(#|mi). As before, rel-ative frequency estimates calculated from un-segmented input data (automatically syllabified using the unsupervised syllabification method described earlier) provide a starting point for pa-rameter estimation. Estimation strategies are dis-cussed in depth in Section 4. 3.2.3 Onset Rhyme Model In addition to the phoneme level and syllable level bigram models, we consider an intermedi-ate model that makes use of the main subconstit-uents of syllables: onsets and rhymes. Recall that the syllabification procedure relies on identifying maximal onsets, whereas rhymes are composed of the remaining material in the syllable. So the-se constituents are already available during the syllabification procedure, and this phonotactic model operates over these smaller constituents, rather than over entire syllables. The syllable-based model operates over indivisible syllable units, while this models treats syllables as com-binations of smaller subconstituents.  Once a sequence is syllabified (separating on-sets and rhymes), this model uses bigrams over these units to determine word probabilities. Con-sider again the potential word [l!k?tmi] ?lookatme?. This sequence is first syllabified into onsets and rhymes as [l.!.k.?t.m.i]. Then its phonotactic probability is calculated by multiply-ing together the bigram probabilities: P(l|#)P(!|l)P(k|!)P(?t|k)P(m|?t)P(i|m)P(#|i).  As before, relative frequency estimates are calculat-ed from an (automatically syllabified) unseg-mented version of the input corpus. 4 Estimation Inferring the parameters of these models in an unsupervised way from unsegmented utterances presents a number of challenges. First, a genera-tive model relying on these parameters must be able to accommodate elements and sequences of elements that have not previously been encoun-
22
tered. This includes unseen phonemes, onsets, rhymes, syllables, and unseen sequences of these units. A second difficulty for the generative model arises specifically in the context of seg-mentation due to the number of boundaries en-countered in the input data. In an unsegmented corpus there are no boundaries within an utter-ance. The only evidence for word boundaries comes from boundaries at the beginnings and ends of utterances. The effect is that the total number of boundaries is lower than the number that must be inferred by the learner, and the overall probability of boundaries is underrepre-sented in the input data. We considered several estimation methods to overcome these effects. 4.1 Local Minimum Strategy In previous research (Saffran et al., 1996) it has been suggested that word boundaries are placed at troughs in transitional probability so that a boundary is inserted between two elements when the transitional probability of those elements is lower than the probability of the neighboring transitions. This strategy captures the fact that word boundaries are more likely to occur be-tween elements that have a low probability of occurring together. Since this strategy does not incorporate transitional probabilities into a gen-erative segmentation model, it provides a simple way around the estimation challenges discussed above. We include it for comparison to previous results relying on syllable-based transitional probabilities (Yang, 2004). 4.2 Adjusted Boundary Count Strategy We also introduce a novel, simple method for adjusting the estimates of transitional probabili-ties based on input data that underrepresents word boundaries. This method directly adjusts the parameter estimates in order to increase the overall likelihood of word boundaries. The main insight behind this estimation strategy is that ob-served bigram counts (of co-occurring pho-nemes, syllables, or onsets and rhymes) in the input data are overestimated since a proportion of them are in reality separated by word bounda-ries in the desired segmentation. For a given pro-portion p# (a parameter of this estimation meth-od), the bigram counts of co-occurring elements (phonemes, syllables, or onsets/rhymes) are sys-tematically decreased by a factor of (1- p#) and for each context c, are reallocated to the transi-tional probability of P(# | c). The formula below illustrates how this adjustment works for arbi-trary contexts c and proportion p#. The probabil-
ity of each possible element !! that can follow c is decreased by a factor of p# as shown in (4). The total probability taken away from all contin-uations of c is used to increase the probability of P(# | c) as shown in (5).  4) ! !! ! ! !!!!! !!!!!!! !! ! !!!  5) ! ! ! ! !!!! ! !!! !!! ! ! !!!! !!! !!!!!! ! !  Consider an example for the context x, with three bigrams observed in the input: c(xy) = 10, c(xz) = 6, and c(x#) = 4. The relative frequency estimates for these transitional probabilities are 0.5, 0.3, and 0.2 respectively. The adjusted count method takes away p# of the xy and xz counts and reallo-cates them to x#. For p# = 0.5, for example, the new estimates would be 0.25, 0.15, and 0.6. The adjustment works analogously for every context for each of the units of analysis. 4.3 Smoothing We also utilized rudimentary smoothing tech-niques to allow the generative model to deal with unknown sequences. We chose a simple method that allocated non-zero probability to unseen se-quences while minimally disrupting the estimates computed using the adjusted boundary count strategy, since our primary concern was in ex-ploring the effects of this novel re-estimation strategy. For all models, add-lambda smoothing (Jurafsky & Martin, 2008) with a value of 0.001 was used. For the syllable-based models this total value was allocated to all unseen bigrams in or-der to avoid over-allocation of probability to the numerous combinations of unseen syllabic units.  4.4 Iterative Re-estimation After estimating the transitional probabilities from the unsegmented corpus, the above strate-gies can be used to compute the optimal segmen-tation of the input corpus in a single pass. In ad-dition to the above strategies, we also investigat-ed a greedy, iterative re-estimation strategy that makes multiple passes through the corpus. This estimation method takes the output of the above methods and uses it to re-estimate (smoothed and adjusted) parameters for the phonotactic models. It then recomputes the optimal segmentation of the corpus based on the new parameters and re-peats until convergence.  This method is moti-vated by previous segmentation work highlight-ing the effectiveness of greedy re-estimation 
23
techniques (Brent, 1999; Venkataraman, 2001; Goldwater et al., 2009; Johnson & Goldwater, 2009). As noted in previous work, such greedy re-estimation has the potential to infer additional word boundaries based on commitments made to word boundaries on earlier passes. 5 Experiments 5.1 Corpus The experiments for all the models were run on the Brent (1999) version of the Bernstein-Ratner (1987) corpus of English child-directed speech consisting of phonetically transcribed utterances. This corpus has been widely used for evaluating segmentation models. Other models evaluated on this corpus include those of Brent (1999), Venka-taraman (2001), Blanchard and Heinz (2008), and Johnson and Goldwater (2009). 5.2 Evaluation Precision, recall, and f-scores of both word to-kens and boundaries were used to evaluate per-formance. For the models with iterative re-estimation, the reported performance scores are taken from the iteration after convergence. This typically happened after 5-10 iterations. 5.3 Results and Discussion Table 1 summarizes the word boundary and word token f-scores for all models, while Table 2 presents the precision and recall scores for the best-performing adjusted count models and the local minimum models.  Focusing first on the local minimum estima-teion strategy, there are several noteworthy ef-fects. First, our results with local minima for the syllable-level transitional probabilities achieves very similar word token precision and recall to that reported by Yang (2004), who examined a different corpus of child-directed English. The word token precision and recall of our model is 40.2% and 23.7%, respectively, while Yang re-ported 41.6% and 23.3%, respectively, for his experiments. This corroborates Yang?s finding that the local minima estimation strategy for syl-lable-level transitional probabilities works very poorly, this time showing that this level of per-formance can be achieved with simultaneous in-ference of syllabification. As Table 2 shows, the poor performance can be attributed to poor re-call, which the low boundary recall and high pre-cision illustrate most clearly. As Yang discusses, the fatal flaw for this approach is that it categori-cally fails to segment monosyllabic words, which 
account for an overwhelming majority of words in child-directed speech. This is because local minima must, by definition, be separated by at least one transition with a higher bigram proba-bility, which is not treated as a boundary. Indeed, the proportion of monosyllables is so high that a baseline strategy that simply posits word bounda-ries at all syllable boundaries achieves a word token f-score of 58.0% using the minimally-supervised syllabification procedure described here1. The high performance of the monosyllabic baseline highlights the ineffectiveness of the lo-cal minimum strategy but also indicates that syl-lable structure provides a significant amount of information about word boundaries in English, even if this syllable structure is automatically inferred from unsegmented input using minimal prior knowledge. Furthermore, our results with the phoneme bigram local minimum strategy (47.1% word token f-score) corroborate Brent?s (1999) finding that this method achieves a roughly 50% word token f-score (Brent did not provide exact num-bers). The improvement in performance is not surprising given the above discussion about the prevalence of monosyllabic words: local minima defined over the smaller phoneme units do not automatically rule out the possibility of segment-ing short words. We also demonstrate that the onset-rhyme model achieves performance similar to that of the syllable bigram model using the local minima strategy. Finally, the results with iterative re-estimation show that further refine-ment of the posited word boundaries can lead to some improvement, but none of the local mini-mum models surpass 53% word token f-score, and the syllable-based models perform substan-tially worse. Overall, these partial results are consistent with the trend suggested by previous work that the syllable-level bigrams examined in the infant studies provide little information about word boundaries in natural language data when the local minimum strategy is used.  However, a different picture emerges when the performance of the adjusted count strategy is considered. The fact that the local minimum strategy is ineffectual is already clear from the comparison with the monosyllabic baseline; however, the results for the adjusted counts esti-mation strategy reveal that it is possible to ex-                                                 1 In contrast, Lignos & Yang (2010) report a word token f-score of 78.9% for this baseline for already syllabified in-put. The difference between these baselines highlights how much more difficult the segmentation task is when the syl-labification must be inferred from unsegmented input. 
24
    p# = 0 p# = 0.35 p# = 0.5 p# = 0.6 p# = 0.75 p# = 0.99 LM  WF BF WF BF WF BF WF BF WF BF WF BF WF BF P 13.0 10.2 34.7 51.9 40.3 60.6 49.9 69.2 45.9 68.8 13.9 50.1 47.1 64.5 OR 15.4 17.9 28.7 43.3 37.1 55.8 42.2 62.0 58.4 76.0 52.3 71.4 27.9 44.1 S 10.7 3.1 12.7 8.6 14.2 12.4 15.9 16.3 20.7 26.1 74.1 84.1 29.8 51.0 P-IR 13.0 10.2 34.7 51.9 40.3 60.6 50.7 69.6 46.9 69.6 9.9 47.0 52.9 70.5 OR-IR 19.8 29.1 36.8 54.7 47.7 67.7 53.4 72.8 63.8 79.8 37.1 62.1 42.3 62.3 S-IR 10.9 3.8 13.3 10.5 15.2 15.0 16.8 18.7 23.1 31.4 79.8 88.0 27.2 43.9 Table 1: Word token (WF) and boundary (BF) f-scores for all models. The columns in the first section of the table represent different settings of the p# parameter, with highest performance for each adjusted count model shown in bold. p# values were selected to show a representative range of performance. P = phoneme model; OR = onset-rhyme model; S = syllable model; IR = iterative re-estimation; LM = local minimum strategy. The best performing local minimum model is shaded.  Adjusted Count Estimation Local Minimum Estimation  WP WR BP BR WP WR BP BR P-IR 50.3 51.1 68.8 70.4 53.4 52.4 71.5 69.5 OR-IR 63.8 63.8 79.9 79.8 44.2 40.5 66.5 58.6 S-IR 85.2 75.0 97.0 80.6 40.4 20.5 94.0 28.6 Table 2: Word precision (WP), word recall (WR), boundary precision (BP), and boundary recall (BR) scores for selected models. For the adjusted count estimation models, the results for the best perform-ing parameter value are shown (P-IR: 0.6; OR-IR: 0.75; S-IR: 0.99).  extract substantially more information about word boundaries from syllable-based models when these cues are used in the context of a gen-erative model and better methods are used for unsupervised estimation of these parameters. In fact, using the adjusted counts estimation method with the optimal parameter settings, the reverse trend is observed, wherein the phoneme-level bigrams perform worse than the syllable-based models, and syllable-level bigrams perform best of all, reaching word token f-scores of nearly 80%. Crucially, both the onset-rhyme and the syllable bigram models achieve levels of perfor-mance that surpass the monosyllabic baseline. In the case of the syllable bigram, the improvement in word token f-score is more than 20% when iterative re-estimation is used and more than 15% when segmentation is performed in only a single pass through the corpus. The phoneme-based models perform about as well whether adjusted counts or local minimum estimation is used. However, compensation for the underrepresentation of word boundaries in the input is crucial to the syllable-based models. These models surpass the local minimum estima-tion models only when the p# parameter compen-sates sufficiently for the input bias against word boundaries. As shown in Table 1, without any compensation (p# = 0), all models perform terri-
bly. This is because utterance boundaries provide very little evidence of word boundaries, and the models estimated directly from such input mas-sively undersegment. It is only at higher settings of the parameter that performance improves. As expected, the optimal parameter value increases with the granularity of the unit over which bi-grams are computed. This makes sense since boundaries are more likely to fall between larger units than between smaller units.  Less expected is the fact that the optimal pa-rameter values are high compared to the empiri-cal rates of word boundaries in the true segmen-tation of the input corpus. For example, the true rate of utterance-internal word boundaries is around 30% at the phoneme level, yet the opti-mal p# value for phoneme bigrams is around 60%. The reason for this is that our generative model, like that of a number of previous models discussed in the literature (Brent, 1999; Venkata-raman, 2001; Goldwater et al., 2009), has an in-herent undersegmentation bias. Due to the way the phonotactic models are defined, there is a cost for every additional word boundary posited in the segmentation. This is because positing a boundary corresponds to the generation of an additional symbol, #, which otherwise does not have to be generated. Since generating a # is never done with 100% probability, doing so al-
25
ways incurs a cost relative to a segmentation where no such # has to be generated. The high optimal settings of the p#  parameter reflect this inherent bias and enable the estimation procedure to compensate not only for the underrepresenta-tion of word boundaries in the input but also for this bias in the generative model.  6 Conclusions We compared segmentation models that rely on phoneme transitions to models that make use of syllable structure. The results indicate that sylla-ble-based statistics are valuable for segmenta-tion. We also showed that it is possible to utilize this structure successfully with limited prior knowledge of the target language by using a simple syllabification strategy inferred from un-segmented utterances. The performance of the syllable-based models also demonstrates that it is possible to achieve good segmentation results without the use of a lexicon. Another contribu-tion of this work is a novel estimation procedure that addresses some challenges of unsupervised segmentation. We showed that adjusting parame-ter estimates inferred from unsegmented input is essential for achieving good performance.  The strong performance of the syllable level bigram phonotactic model has a number of im-plications. First, it demonstrates that the kind of statistical regularities that infants have been con-sistently shown to be sensitive to in artificial ex-perimental stimuli do provide a substantial amount of information about word boundaries in natural language data, at least in English. This lends significant credibility to the claim that sen-sitivity to such statistical regularities plays a cru-cial role in infants? early language development (contra Yang 2004). This result also highlights the role that sensitivity to richer phonological information, beyond the level of phonemes, plays in language learning, a result that is echoed in much recent work on the modeling of phonotac-tic well-formedness of isolated words (Hayes & Wilson, 2008; Albright, 2009; Daland et al., 2011). A consistent finding of this work has been that access to abstract structure and robust gener-alization mechanisms is crucial to the modeling of human phonotactic knowledge. While our re-sults are compatible with these conclusions, our results cannot confirm that it is syllable structure per se that improves segmentation since the syl-lable-based models have several co-occurring advantages. In addition to abstract structure, they can track longer and more complex dependen-
cies. Nonetheless, these results motivate further investigation into the role that richer models of phonotactics may play in word segmentation and into the precise mechanisms responsible for im-proved segmentation using syllable structure. Particularly critical is exploration of phonotacti-cally-based segmentation models for languages besides English, for which phonotactic cues hold significant promise (Jarosz & Johnson, 2013) given the relatively low performance of state-of-the-art lexicon-building models (Johnson 2008b). Another important direction for future work is investigating how early, phonotactically-based segmentation interacts with subsequent learning of higher-level structure, including the lexicon. Johnson (2008a) and Johnson & Goldwater (2009) have already demonstrated that syllable structure provides valuable information in this context; however, their models relied on very different syllable regularities than those investi-gated here, and the consequences of these differ-ences should be explored in future work.  Goldwater et al. (2009) showed that a number of proposed segmentation models have an under-segmentation bias that can be avoided by simul-taneously modeling statistical dependencies be-tween words. They proposed a Bayesian prior to favor a smaller lexicon and showed that other-wise unigram models introduce a severe under-segmentation bias due to the possibility of matching empirical probabilities by memorizing utterances as words. Note that the same is not true of syllable-based models since the hypothe-sis space does not permit memorization of utter-ances, and the size of the syllable inventory, un-like a lexicon, remains relatively stable under different segmentations. Thus, the syllable-based models are not subject to the same kind of under-segmentation bias. Interestingly, the syllable bi-gram model surpasses the performance of the word bigram model proposed by Goldwater et al. (word token f-score 72.3) given sufficient com-pensation for its undersegmentation bias. How-ever, this level of performance requires adjust-ment of the p# parameter to compensate for the cost of generating additional boundaries. Alt-hough parameters are common in computational models (for example, Goldwater et al. used a p# parameter to modulate the prior distributions in their Bayesian models), they do not provide a particularly satisfying explanation for why in-fants are compelled to break up the speech stream into smaller units (words). Further work is needed to determine how undersegmentation biases are ultimately overcome by children. 
26
References Adriaans, Frans and Kager, Ren?. 2010. Adding gen-eralization to statistical learning: The induction of phonotactics from continuous speech. Journal of Memory and Language 62(3): 311-331. Albright, Adam. 2009. Feature-based generalisation as a source of gradient acceptability. Phonology, 26(1): 9-41. Aslin, Richard N., Saffran, Jenny R., & Newport, Elissa L. 1998. Computation of conditional proba-bility statistics by 8-month-old infants. Psychologi-cal Science, 9, 321-324.  Bernstein-Ratner, Nan. 1987. The phonology of par-ent child speech. Children?s Language, 6: 159-174. Blanchard, Daniel and Heinz, Jeffrey. 2008. Improv-ing word segmentation by simultaneously learning phonotactics. In Conll ?08: Proceedings of the 12th Conference on Computational Natural Language Learning. Stroudsburg, PA: Association for Com-putational Linguistics. Blanchard, Daniel, Heinz, Jeffrey and Golinkoff, Roberta. 2010. Modeling the contribution of pho-notactic cues to the problem of word segmentation. Journal of Child Language, 37(3): 487-511. Brent, Michael R. 1999. An efficient, probabilistically sound algorithm for segmentation and word dis-covery. Machine Learning, 34(1-3): 71-105. Daland, Robert and Pierrehumbert, Janet B. 2011. Learning Diphone-Based Segmentation. Cognitive science, 35(1). Wiley Online Library. 119?155. Daland, Robert, Hayes, Bruce, White, James, Garellek, Marc, Davis, Andrea and Norrmann, In-grid. 2011. Explaining sonority projection effects. Phonology, 28(2): 197-234. Dale, P. S., & Fenson, L. 1996. Lexical development norms for young children. Behavior Research Methods, Instruments, & Computers, 28, 125-127. Goldwater, Sharon, Griffiths, Thomas L. and Johnson, Mark. 2009. A Bayesian framework for word seg-mentation: Exploring the effects of context. Cogni-tion, 112(1): 21-54. Hayes, Bruce & Wilson, Colin. 2008. A maximum entropy model of phonotactics and phonotactic learning. Linguistic Inquiry, 39(3): 379-440. Hockema, Stephen A. 2006. Finding Words in Speech: An Investigation of American English. Language Learning and Development, 2(2). Psy-chology Press. 119-146. Jarosz, Gaja and Johnson, J. Alex. 2013. The Rich-ness of Distributional Cues to Word Boundaries in Speech to Young Children. Language Learning and Development, 9(2): 175-210. 
Johnson, Mark. 2008a. Using adaptor grammars to identify synergies in the unsupervised acquisition of linguistic structure. Proceedings of the 46th An-nual Meeting of the Association for Computational Linguistics. Association for Computational Lin-guistics. Johnson, Mark. 2008b. Unsupervised Word Segmen-tation for Sesotho Using Adaptor Grammars. Pro-ceedings of the 10th Meeting of ACL SIGMOR-PHON. Columbus, OH: Association of Computa-tional Linguistics.  Johnson, Mark & Goldwater, Sharon. 2009. Improv-ing nonparametric Bayesian inference: Experi-ments on unsupervised word segmentation with adaptor grammars. In NAACL ?09: Proceedings of the Annual Conference of the North American Chapter of the Association for Computational Lin-guistics. Boulder, CO: Association for Computa-tional Linguistics. Jurafsky, Daniel & Martin, James H. 2008. Speech and language processing, 2nd edition. Upper Sad-dle River, NJ: Prentice-Hall. Jusczyk, Peter W. & Luce, Paul A. 1994. Infants? Sensitivity to Phonotactic Patterns in the Native Language. Journal of Memory and Language, 33(5): 630-645. Lignos, Constantine and Yang, Charles. 2010. Reces-sion Segmentation: Simpler Online Word Segmen-tation Using Limited Resources. Proceedings of the Fourteenth Conference on Computational Nat-ural Language Learning. (CoNLL '10). Associa-tion for Computational Linguistics. . Mattys, Sven L. and Jusczyk, Peter W. 2000. Phono-tactic cues for segmentation of fluent speech by in-fants. Cognition, 78(2): 91-121. Mattys, Sven L., Jusczyk, Peter W., Luce, Paul A., and Morgan, James L. 1999. Phonotactic and pro-sodic effects on word segmentation in infants. Cognitive psychology, 38(4): 465-494. Newport, Elissa L. and Aslin, Richard N. 2004. Learning at a distance I. Statistical learning of non-adjacent dependencies. Cognitive psychology, 48(2): 127-162. Pelucchi, Bruna, Hay, Jessica F., and Saffran, Jenny R. 2009. Learning in reverse: Eight-month-old in-fants track backward transitional probabilities. Cognition, 113(2): 244-247. Saffran, Jenny R., Aslin, Richard N., and Newport, Elissa L. 1996. Statistical learning by 8-month-old infants. Science, 274(5294): 1926-1928. Swingley, Daniel. 2005. Statistical clustering and the contents of the infant vocabulary. Cognitive Psy-chology, 50(1): 86-32. 
27
Thiessen, Erik D. and Saffran, Jenny R. 2003. When cues collide: use of stress and statistical cues to word boundaries by 7-to 9-month-old infants. De-velopmental psychology, 39(4): 706. Venkataraman, Anand 2001. A statistical model for word discovery in transcribed speech. Computa-tional Linguistics, 27(3): 352-372. Yang, Charles D. 2004. Universal Grammar, statistics or both? Trends in Cognitive Sciences, 8(10): 451-456. 
28
