Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 424?432,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Semi-Supervised Lexicon Mining from Parenthetical Expressions
in Monolingual Web Pages
Xianchao Wu? Naoaki Okazaki? Jun?ichi Tsujii??
?Computer Science, Graduate School of Information Science and Technology, University of Tokyo
7-3-1 Hongo, Bunkyo-ku, Tokyo 113-8656, Japan
?School of Computer Science, University of Manchester
National Centre for Text Mining (NaCTeM)
Manchester Interdisciplinary Biocentre, 131 Princess Street, Manchester M1 7DN, UK
{wxc, okazaki, tsujii}@is.s.u-tokyo.ac.jp
Abstract
This paper presents a semi-supervised learn-
ing framework for mining Chinese-English
lexicons from large amount of Chinese Web
pages. The issue is motivated by the ob-
servation that many Chinese neologisms are
accompanied by their English translations in
the form of parenthesis. We classify par-
enthetical translations into bilingual abbrevi-
ations, transliterations, and translations. A
frequency-based term recognition approach is
applied for extracting bilingual abbreviations.
A self-training algorithm is proposed for min-
ing transliteration and translation lexicons. In
which, we employ available lexicons in terms
of morpheme levels, i.e., phoneme correspon-
dences in transliteration and grapheme (e.g.,
suffix, stem, and prefix) correspondences in
translation. The experimental results verified
the effectiveness of our approaches.
1 Introduction
Bilingual lexicons, as lexical or phrasal parallel
corpora, are widely used in applications of multi-
lingual language processing, such as statistical ma-
chine translation (SMT) and cross-lingual informa-
tion retrieval. However, it is a time-consuming task
for constructing large-scale bilingual lexicons by
hand. There are many facts cumber the manual de-
velopment of bilingual lexicons, such as the contin-
uous emergence of neologisms (e.g., new technical
terms, personal names, abbreviations, etc.), the dif-
ficulty of keeping up with the neologisms for lexi-
cographers, etc. In order to turn the facts to a better
way, one of the simplest strategies is to automati-
cally mine large-scale lexicons from corpora such as
the daily updated Web.
Generally, there are two kinds of corpora used
for automatic lexicon mining. One is the purely
monolingual corpora, wherein frequency-based
expectation-maximization (EM, refer to (Dempster
et al, 1977)) algorithms and cognate clues play a
central role (Koehn and Knight, 2002). Haghighi
et al (2008) presented a generative model based
on canonical correlation analysis, in which monolin-
gual features such as the context and orthographic
substrings of words were taken into account. The
other is multilingual parallel and comparable cor-
pora (e.g., Wikipedia1), wherein features such as co-
occurrence frequency and context are popularly em-
ployed (Cheng et al, 2004; Shao and Ng, 2004; Cao
et al, 2007; Lin et al, 2008).
In this paper, we focus on a special type of com-
parable corpus, parenthetical translations. The issue
is motivated by the observation that Web pages and
technical papers written in Asian languages (e.g.,
Chinese, Japanese) sometimes annotate named enti-
ties or technical terms with their translations in En-
glish inside a pair of parentheses. This is considered
to be a traditional way to annotate new terms, per-
sonal names or other named entities with their En-
glish translations expressed in brackets. Formally,
a parenthetical translation can be expressed by the
following pattern,
f1 f2 ... fJ (e1 e2 ... eI). (1)
Here, f1 f2 ... fJ (fJ1 ), the pre-parenthesis text, de-
notes the word sequence of some language other
than English; and e1 e2 ... eI (eI1), the in-parenthesis
text, denotes the word sequence of English. We sep-
arate parenthetical translations into three categories:
1http://en.wikipedia.org/wiki/Main Page
424
Type Examples with translations in italic
?? ?? ?? ?? ?? (GCOS)to Global Climate Observing System (GCOS)
?? ? ?? ???- ???(Shipton-Tilman)brand will be among Shipton-Tilman (Shipton-Tilman)
?????? ???(Cancelbots)time bomb, Cancelbots (Cancelbots)
? ?? ?? ? ??? ???? ??(Bradford University)
the English Bradford University (Bradford University)
that holds lessons in Hongkong
Abbreviation
Transliteration
Translation
Mixture
Table 1: Parenthetical translation categories and exam-
ples extracted from Chinese Web pages. Mixture stands
for the mixture of translation (University) and translitera-
tion (Bradford). ??? denotes the left boundary of fJ1 .
bilingual abbreviation, transliteration, and transla-
tion. Table 1 illustrates examples of these categories.
We address several characteristics of parenthetical
translations that differ from traditional comparable
corpora. The first is that they only appear in mono-
lingual Web pages or documents, and the context
information of eI1 is unknown. Second, frequency
and word number of eI1 are frequently small. This
is because parenthetical translations are only used
when the authors thought that fJ1 contained some
neologism(s) which deserved further explanation in
another popular language (e.g., English). Thus, tra-
ditional context based approaches are not applicable
and frequency based approaches may yield low re-
call while with high precision. Furthermore, cog-
nate clues such as orthographic features are not ap-
plicable between language pairs such as English and
Chinese.
Parenthetical translation mining faces the follow-
ing issues. First, we need to distinguish paren-
thetical translations from parenthetical expressions,
since parenthesis has many functions (e.g., defining
abbreviations, elaborations, ellipsis, citations, anno-
tations, etc.) other than translation. Second, the
left boundary (denoted as ? in Table 1) of the pre-
parenthesis text need to be determined to get rid of
the unrelated words. Third, we need further distin-
guish different translation types, such as bilingual
abbreviation, the mixture of translation and translit-
eration, as shown in Table 1.
In order to deal with these problems, supervised
(Cao et al, 2007) and unsupervised (Li et al, 2008)
methods have been proposed. However, supervised
approaches are restricted by the quality and quantity
of manually constructed training data, and unsuper-
vised approaches are totally frequency-based with-
out using any semantic clues. In contrast, we pro-
pose a semi-supervised framework for mining par-
enthetical translations. We apply a monolingual ab-
breviation extraction approach to bilingual abbrevia-
tion extraction. We construct an English-syllable to
Chinese-pinyin transliteration model which is self-
trained using phonemic similarity measurements.
We further employ our cascaded translation model
(Wu et al, 2008) which is self-trained based on
morpheme-level translation similarity.
This paper is organized as follows. We briefly
review the related work in the next section. Our
system framework and self-training algorithm is de-
scribed in Section 3. Bilingual abbreviation ex-
traction, self-trained transliteration models and cas-
caded translation models are described in Section 4,
5, and 6, respectively. In Section 7, we evaluate our
mined lexicons by Wikipedia. We conclude in Sec-
tion 8 finally.
2 Related Work
Numerous researchers have proposed a variety of
automatic approaches to mine lexicons from the
Web pages or other large-scale corpora. Shao and
Ng (2004) presented a method to mine new transla-
tions from Chinese and English news documents of
the same period from different news agencies, com-
bining both transliteration and context information.
Kuo et al (2006) used active learning and unsu-
pervised learning for mining transliteration lexicon
from the Web pages, in which an EM process was
used for estimating the phonetic similarities between
English syllables and Chinese characters.
Cao et al (2007) split parenthetical translation
mining task into two parts, transliteration detection
and translation detection. They employed a translit-
eration lexicon for constructing a grapheme-based
transliteration model and annotated boundaries man-
ually to train a classifier. Lin et al (2008) applied
a frequency-based word alignment approach, Com-
petitive Link (Melanmed, 2000), to determine the
outer boundary (Section 7).
On the other hand, there have been many semi-
supervised approaches in numerous applications
425
Parenthetical expression extraction{C(E)} 
Chinese word segmentation{c?(e?)} S-MSRSeg 
Heuristic filtering{c?(e?)} 
Chinese Web pages 
Bilingual abbreviation mining 
Section 4 
Transliteration lexicon mining 
Section 5 
Translation lexicon mining 
Section 6 
(Lin et al, 2008) 
Figure 1: The system framework of mining lexicons from
Chinese Web pages.
(Zhu, 2007), such as self-training in word sense
disambiguation (Yarowsky, 2005) and parsing (Mc-
Closky et al, 2008). In this paper, we apply self-
training to a new topic, lexicon mining.
3 System Framework and Self-Training
Algorithm
Figure 1 illustrates our system framework for min-
ing lexicons from Chinese Web pages. First, par-
enthetical expressions matching Pattern 1 are ex-
tracted. Then, pre-parenthetical Chinese sequences
are segmented into word sequences by S-MSRSeg2
(Gao et al, 2006). The initial parenthetical transla-
tion corpus is constructed by applying the heuristic
rules defined in (Lin et al, 2008)3. Based on this
corpus, we mine three lexicons step by step, a bilin-
gual abbreviation lexicon, a transliteration lexicon,
and a translation lexicon. The abbreviation candi-
dates are extracted firstly by using a heuristic rule
(Section 4.1). Then, the transliteration candidates
are selected by employing a transliteration model
(Section 5.1). Specially, fJ1 (eI1) is taken as a translit-
eration candidate only if a word ei in eI1 can be
transliterated. In addition, a transliteration candidate
will also be considered as a translation candidate if
not all ei can be transliterated (refer to the mixture
example in Table1). Finally, after abbreviation filter-
ing and transliteration filtering, the remaining candi-
2http://research.microsoft.com/research/downloads/details/
7a2bb7ee-35e6-40d7-a3f1-0b743a56b424/details.aspx
3e.g., fJ1 is predominantly in Chinese and eI1 is predomi-
nantly in English
Algorithm 1 self-training algorithm
Require: L, U = {fJ1 (eI1)}, T , M ?L, (labeled) train-
ing set; U , (unlabeled) candidate set; T , test set; M, the
transliteration or translation model.
1: Lexicon = {} ? new mined lexicon
2: repeat
3: N = {} ? new mined lexicon during one iteration
4: train M on L
5: evaluate M on T
6: for fJ1 (eI1) ? U do
7: topN = {C?|decode eI1 by M}
8: N = N ? {(c, eI1)|c ? fJ1 ?
?C? ? topN s.t. similarity{c, C?} ? ?}
9: end for
10: U = U ?N
11: L = unified(L ?N)
12: Lexicon = unified(Lexicon ?N)
13: until |N | ? ?
14: return Lexicon ? the output
dates are used for translation lexicon mining.
Algorithm 1 addresses the self-training algorithm
for lexicon mining. The main part is a loop from
Line 2 to Line 13. A given seed lexicon is taken
as labeled data and is split into training and testing
sets (L and T ). U={fJ1 (eI1)}, stands for the (unla-
beled) parenthetical expression set. Initially, a trans-
lation/transliteration model (M) is trained on L and
evaluated on T (Line 4 and 5). Then, the English
phrase eI1 of each unlabeled entry is decoded by M,
and the top-N outputs are stored in set topN (Line
7?8). A similarity function on c (a word substring
of fJ1 ) and a top-N output C ? is employed to make
the decision of classification: the pair (c, eI1) will be
selected as a new entry if the similarity between c
and C ? is no smaller than a threshold value ? (Line
8). After processing each entry in U , the new mined
lexicon N is deleted from U and unified with the
current training set L as the new training set (Line
10 and 11). Also, N is added to the final lexicon
(Line 12). When |N | is lower than a threshold, the
loop stops. Finally, the algorithm returns the mined
lexicon.
One of the open problems in Algorithm 1 is how
to append new mined entries into the existing seed
lexicon, considering they have different distribu-
tions. One way is to design and estimate a weight
function on the frequency of new mined entries. For
simplicity, we use a deficient strategy that takes the
weights of all new mined entries to be one.
426
4 Bilingual Abbreviation Extraction
4.1 Methodology
The method that we use for extracting a bilingual
abbreviation lexicon from parenthetical expressions
is inspired by (Okzaki and Ananiadou, 2006). They
used a term recognition approach to build a monolin-
gual abbreviation dictionary from the Medical Liter-
ature Analysis and Retrieval System Online (MED-
LINE) abstracts, wherein acronym definitions (e.g.,
ADM is short for adriamycin, adrenomedullin, etc.)
are abundant. They reported 99% precision and 82-
95% recall. Through locating a textual fragment
with an acronym and its expanded form in pattern
long form (short form), (2)
they defined a heuristic formula to compute the long-
form likelihood LH(c) for a candidate c:
LH(c) = freq(c)? ?
t?Tc
freq(t)? freq(t)?
t?Tc freq(t)
.
(3)
Here, c is a long-form candidate; freq(c) denotes the
frequency of co-occurrence of c with a short-form;
and Tc is a set of nested long-form candidates, each
of which consists of a preceding word followed by
the candidate c. Obviously, for t ? Tc, Equation 3
can be explained as:
LH(c) = freq(c)? E[freq(t)]. (4)
In this paper, we apply their method on the task
of bilingual abbreviation lexicon extraction. Now,
the long-form is a Chinese word sequence and the
short-form is an English acronym. We filter the par-
enthetical expressions in the Web pages with several
heuristic rules to meet the form of pattern 2 and to
save the computing time:
? the short-form (eI1) should contain only one En-
glish word (I = 1), and all letters in which
should be capital;
? similar with (Lin et al, 2008), the pre-
parenthesis text is trimmed with: |c| ? 10 ?
|eI1|+ 6 when |eI1| ? 6, and |c| ? 2? |eI1|+ 6,
otherwise. |c| and |eI1| are measured in bytes.
We further trim the remaining pre-parenthesis
text by punctuations other than hyphens and
dots, i.e., the right most punctuation and its left
subsequence are discarded.
o. Chinese long-form candidates LH T/F
1 ?? ?? ?? 172.5 T
Tumor-Associated Antigen
2 ? ? ?? ? 79.9 T
thioacetamide
3 ? 33.8 F
amine
4 ?? 24.5 F
antigen
5 ?? ?? 21.2 F
associated antigen
6 ? ?? ?? ?? 16.5 F
's Tumor-Associated Antigen
7 ? ??? 16.2 T
total amino acid
Table 2: Top-7 Chinese long-form candidates for the En-
glish acronym TAA, according to the LH score.
4.2 Experiment
We used SogouT Internet Corpus Version 2.04,
which contains about 13 billion original Web pages
(mainly Chinese) in the form of 252 gigabyte .txt
files. In addition, we used 55 gigabyte (.txt for-
mat) Peking University Chinese Paper Corpus. We
constructed a partially parallel corpus in the form
of Pattern 1 from the union of the two corpora us-
ing the heuristic rules defined in (Lin et al, 2008).
We gained a partially parallel corpus which contains
12,444,264 entries.
We extracted 107,856 distinct English acronyms.
Limiting LH score ? 1.0 in Equation 3, we gained
2,020,012 Chinese long-form candidates for the
107,856 English acronyms. Table 2 illustrates the
top-7 Chinese long-form candidates of the English
acronym TAA. Three candidates are correct (T) long-
forms while the other 4 are wrong (F). Wrong can-
didates from No. 3 to 5 are all subsequences of the
correct candidate No. 1. No. 6 includes No. 1 while
with a Chinese functional word de in the left most
side. These error types can be easily tackled with
some filtering patterns, such as ?remove the left most
functional word in the long-form candidates?, ?only
keep the relatively longer candidates with larger LH
score?, etc.
Since there does not yet exists a common eval-
uation data set for the bilingual abbreviation lexi-
con, we manually evaluated a small sample of it.
4http://www.sogou.com/labs/dl/t.html
427
Of the 107,856 English acronyms, we randomly se-
lected 200 English acronyms and their top-1 Chi-
nese long-form candidates for manually evaluating.
We found, 92 candidates were correct including 3
transliteration examples. Of the 108 wrong candi-
dates, 96 candidates included the correct long-form
with some redundant words on the left side (i.e., c =
(word)+ correct long-form), the other 12 candidates
missed some words of the correct long-form or had
some redundant words right before the left paren-
thesis (i.e., c = (word)? correct long-form (word)+
or c = (word)? subsequence of correct long-form
word)?). We classified the redundant word right be-
fore the correct long-form of each of the 96 candi-
dates, de occupied 32, noun occupied 7, verb occu-
pied 18, prepositions and conjunctions occupied the
remaining ones.
In total, the abbreviation translation accuracy is
44.5%. We improved the accuracy to 60.5% with
an additional de filtering pattern. According to for-
mer mentioned error analysis, the accuracy may fur-
ther be improved if a Chinese part-of-speech tagger
is employed and the non-nominal words in the long-
form are removed beforehand.
5 Self-Training for Transliteration Models
In this section, we first describe and compare three
transliteration models. Then, we select and train the
best model following Algorithm 1 for lexicon min-
ing. We investigate two things, the scalability of the
self-trained model given different amount of initial
training data, and the performance of several strate-
gies for selecting new training samples.
5.1 Model description
We construct and compare three forward translit-
eration models, a phoneme-based model (English
phonemes to Chinese pinyins), a grapheme-based
model (English syllables to Chinese characters)
and a hybrid model (English syllables to Chinese
pinyins). Similar models have been compared in
(Oh et al, 2006) for English-to-Korean and English-
to-Japanese transliteration. All the three models are
phrase-based, i.e., adjacent phonemes or graphemes
are allowable to form phrase-level transliteration
units. Building the correspondences on phrase
level can effectively tackle the missing or redundant
phoneme/grapheme problem during transliteration.
For example, when Aamodt is transliterated into a
mo? te`5, a and d are missing. The problem can be
easily solved when taking Aa and dt as single units
for transliterating.
Making use of Moses (Koehn et al, 2007), a
phrase-based SMT system, Matthews (2007) has
shown that the performance was comparable to re-
cent state-of-the-art work (Jiang et al, 2007) in
English-to-Chinese personal name transliteration.
Matthews (2007) took transliteration as translation
at the surface level. Inspired by his idea, we also
implemented our transliteration models employing
Moses. The main difference is that, while Matthews
(2007) tokenized the English names into individual
letters before training in Moses, we split them into
syllables using the heuristic rules described in (Jiang
et al, 2007), such that one syllable only contains one
vowel letter or a combination of a consonant and a
vowel letter.
English syllable sequences are used in the
grapheme-based and hybrid models. In the
phoneme-based model, we transfer English names
into phonemes and Chinese characters into Pinyins
in virtue of the CMU pronunciation dictionary6 and
the LDC Chinese character-to-pinyin list7.
In the mass, the grapheme-based model is the
most robust model, since no additional resources are
needed. However, it suffers from the Chinese homo-
phonic character problem. For instance, pinyin ai
corresponds to numerous Chinese characters which
are applicable to personal names. The phoneme-
based model is the most suitable model that reflects
the essence of transliteration, while restricted by ad-
ditional grapheme to phoneme dictionaries. In or-
der to eliminate the confusion of Chinese homo-
phonic characters and alleviate the dependency on
additional resources, we implement a hybrid model
that accepts English syllables and Chinese pinyins
as formats of the training data. This model is called
hybrid, since English syllables are graphemes and
Chinese pinyins are phonemes.
5The tones of Chinese pinyins are ignored in our translitera-
tion models for simplicity.
6http://www.speech.cs.cmu.edu/cgi-bin/cmudict
7http://projects.ldc.upenn.edu/Chinese/docs/char2pinyin.txt
428
 grapheme-based
0.0
0.2
0.4
0.6
0.8
1.0
1 2 3 4 5 6 7 8max_phrase_length
BLEU WER PER EMatch
 phoneme-based
0.0
0.2
0.4
0.6
0.8
1.0
1 2 3 4 5 6 7 8max_phrase_length
BLEU WER PER EMatch
 Comparison on EMatch
0.0
0.1
0.2
0.3
0.4
0.5
1 2 3 4 5 6 7 8max_phrase_length
grapheme phoneme hybrid
 hybrid-based
0.0
0.2
0.4
0.6
0.8
1.0
1 2 3 4 5 6 7 8max_phrase_length
BLEU WER PER EMatch
Figure 2: The performances of the transliteration models
and their comparison on EMatch.
5.2 Experimental model selection
Similar to (Jiang et al, 2007), the transliteration
models were trained and tested on the LDC Chinese-
English Named Entity Lists Version 1.08. The origi-
nal list contains 572,213 English people names with
Chinese transliterations. We extracted 74,725 en-
tries in which the English names also appeared in
the CMU pronunciation dictionary. We randomly
selected 3,736 entries as an open testing set and the
remaining entries as a training set9. The results were
evaluated using the character/pinyin-based 4-gram
BLEU score (Papineni et al, 2002), word error rate
(WER), position independent word error rate (PER),
and exact match (EMatch).
Figure 2 reports the performances of the three
models and the comparison based on EMatch. From
the results, we can easily draw the conclusion that
the hybrid model performs the best under the maxi-
mal phrase length (mpl, the maximal phrase length
allowed in Moses) from 1 to 8. The performances
of the models converge at or right after mpl =
4. The pinyin-based WER of the hybrid model is
39.13%, comparable to the pinyin error rate 39.6%,
reported in (Jiang et al, 2007)10. Thus, our further
8Linguistic Data Consortium catalog number:
LDC2005T34 (former catalog number: LDC2003E01)
9Jiang et al (2007) selected 25,718 personal name pairs
from LDC2003E01 as the experiment data: 200 as development
set, 200 as test set, and the remaining entries as training set.
10It should be notified that we achieved this result by using
larger training set (70,989 vs. 25,718) and larger test set (3,736
vs. 200) comparing with (Jiang et al, 2007), and we did not use
% 0t 1t 2t 3t 4t 5t Strategy
5 .3879 .3937 .3971 .3958 .3972 .3971 top1 em
.3911 .3979 .3954 .3974 .3965 top1 am
.4062 .4182 .4208 .4218 .4201 top5 em
.3987 .4177 .4190 .4192 .4189 top5 am
10 .4092 .4282 .4258 .4202 .4203 .4205 top1 em
.4121 .4190 .4180 .4174 .4200 top1 am
.4305 .4386 .4399 .4438 .4403 top5 em
.4289 .4263 .4292 .4291 .4288 top5 am
20 .4561 .4538 .4562 .4550 .4543 .4551 top1 em
.4532 .4578 .4544 .4545 .4541 top1 am
.4624 .4762 .4754 .4748 .4746 top5 em
.4605 .4677 .4677 .4674 .4679 top5 am
40 .4779 .4791 .4793 .4799 .4794 .4808 top1 em
.4774 .4794 .4779 .4789 .4784 top1 am
.4808 .4811 .4791 .4795 .4790 top5 em
.4775 .4778 .4781 .4785 .4779 top5 am
60 .5032 .4939 .5004 .5012 .5012 .5016 top1 em
.4919 .4988 .4990 .4994 .4990 top1 am
.5013 .5063 .5059 .5066 .5065 top5 em
.4919 .4960 .4970 .4977 .4962 top5 am
80 .5038 .4984 .4984 .5004 .5006 .4995 top1 em
.4916 .4916 .4914 .4915 .4916 top1 am
.5039 .5037 .5053 .5054 .5042 top5 em
.4950 .5028 .5027 .5032 .5032 top5 am
100 .5045 .5077 .5053 .5067 .5063 .5066 top1 em
.5045 .5054 .5046 .5050 .5055 top1 am
.5108 .5102 .5111 .5108 .5115 top5 em
.5105 .5106 .5100 .5094 .5109 top5 am
Table 3: The BLEU score of self-trained h4 translitera-
tion models under four selection strategies. nt (n=1..5)
stands for the n-th iteration.
self-training experiments are pursued on the hybrid
model taking mpl to be 4 (short for h4, hereafter).
5.3 Experiments on the self-trained hybrid
model
As former mentioned, we investigate the scalability
of the self-trained h4 model by respectively using 5,
10, 20, 40, 60, 80, and 100 percent of initial training
data, and the performances of using exact matching
(em) or approximate matching (am, line 8 in Algo-
rithm 1) on the top-1 and top-5 outputs (line 7 in Al-
gorithm 1) for selecting new training samples. We
used edit distance (ed) to measure the em and am
similarities:
ed(c, C ?) = 0 or < syllable number(C ?)/2. (5)
When applying Algorithm 1 for transliteration lexi-
con mining, we decode each word in eI1 respectively.
The algorithm terminated in five iterations when we
set the terminal threshold ? (Line 13 in Algorithm 1)
to be 100.
For simplicity, Table 3 only illustrates the BLEU
score of h4 models under four selection strategies.
From this table, we can draw the following conclu-
sions. First, with fewer initial training data, the im-
provement is better. The best relative improvements
additional Web resources as Jiang et al (2007) did.
429
are 8.74%, 8.46%, 4.41%, 0.67%, 0.68%, 0.32%,
and 1.39%, respectively. Second, using top-5 and
em for new training data selection performs the best
among the four strategies. Compared under each it-
eration, using top-5 is better than using top-1; em
is better than am; and top-5 with am is a little bet-
ter than top-1 with em. We mined 39,424, 42,466,
46,116, 47,057, 49,551, 49,622, and 50,313 distinct
entries under the six types of initial data with top-5
plus em strategy. The 50,313 entries are taken as the
final transliteration lexicon for further comparison.
6 Self-Training for a Cascaded Translation
Model
We classify the parenthetical translation candidates
by employing a translation model. In contrast to
(Lin et al, 2008), wherein the lengthes of prefixes
and suffixes of English words were assumed to be
three bytes, we segment words into morphemes (se-
quences of prefixes, stems, and suffixes) by Morfes-
sor 0.9.211, an unsupervised language-independent
morphological analyzer (Creutz and Lagus, 2007).
We use the morpheme-level translation similarity
explicitly in our cascaded translation model (Wu et
al., 2008), which makes use of morpheme, word,
and phrase level translation units. We train Moses
to gain a phrase-level translation table. To gain a
morpheme-level translation table, we run GIZA++
(Och and Ney, 2003) on both directions between En-
glish morphemes and Chinese characters, and take
the intersection of Viterbi alignments. The English-
to-Chinese translation probabilities computed by
GIZA++ are attached to each morpheme-character
element in the intersection set.
6.1 Experiment
The Wanfang Chinese-English technical term dictio-
nary12, which contains 525,259 entries in total, was
used for training and testing. 10,000 entries were
randomly selected as the test set and the remaining
as the training set. Again, we investigated the scala-
bility of the self-trained cascaded translation model
by respectively using 20, 40, 60, 80, and 100 per-
cent of initial training data. An aggressive similar-
11http://www.cis.hut.fi/projects/morpho/
12http://www.wanfangdata.com.cn/Search/ResourceBrowse
.aspx
% 0t 1t 2t 3t 4t 5t
20 .1406 .1196 .1243 .1239 .1176 .1179
40 .1091 .1224 .1386 .1345 .1479 .1466
60 .1630 .1624 .1429 .1714 .1309 .1398
80 .1944 .1783 .1886 .1870 .1884 .1873
100 .1810 .1814 .1539 .1981 .1542 .1944
Table 4: The BLEU score of self-trained cascaded trans-
lation model under five initial training sets.
ity measurement was used for selecting new training
samples:
first char(c) = first char(C ?) ? min{ed(c, C ?)}.
(6)
Here, we judge if the first characters of c and C ?
are similar or not. c was gained by deleting zero
or more characters from the left side of fJ1 . When
more than one c satisfied this condition, the c that
had the smallest edit distance with C ? was selected.
When applying Algorithm 1 for translation lexicon
mining, we took eI1 as one input for decoding instead
of decoding each word respectively. Only the top-1
output (C ?) was used for comparing. The algorithm
stopped in five iterations when we set the terminal
threshold ? to be 2000.
For simplicity, Table 4 only illustrates the BLEU
score of the cascaded translation model under five
initial training sets. For the reason that there are fi-
nite phonemes in English and Chinese while the se-
mantic correspondences between the two languages
tend to be infinite, Table 4 is harder to be analyzed
than Table 3. When initially using 40%, 60%, and
100% training data for self-training, the results tend
to be better at some iterations. We gain 35.6%,
5.2%, and 9.4% relative improvements, respectively.
However, the results tend to be worse when 20% and
80% training data were used initially, with 11.6%
and 3.0% minimal relative loss. The best BLEU
scores tend to be better when more initial training
data are available. We mined 1,038,617, 1,025,606,
1,048,761, 1,056,311, and 1,060,936 distinct entries
under the five types of initial training data. The
1,060,936 entries are taken as the final translation
lexicon for further comparison.
7 Wikipedia Evaluation
We have mined three kinds of lexicons till now,
an abbreviation lexicon containing 107,856 dis-
430
En. to Ch. Ch. to En.
Cov EMatch Cov EMatch
Our Lexicon 22.8% 5.2% 23.2% 5.5%
Unsupervised 23.5% 5.4% 24.0% 5.4%
Table 5: The results of our lexicon and an unsupervised-
mined lexicon (Lin et al, 2008) evaluated under
Wikipedia title dictionary. Cov is short for coverage.
similar English acronyms with 2,020,012 Chinese
long-form candidates; a transliteration lexicon with
50,313 distinct entries; and a translation lexicon
with 1,060,936 distinct entries. The three lexicons
are combined together as our final lexicon.
Similar with (Lin et al, 2008), we compare our
final mined lexicon with a dictionary extracted from
Wikipedia, the biggest multilingual free-content en-
cyclopedia on the Web. We extracted the titles of
Chinese and English Wikipedia articles13 that are
linked to each other. Since most titles contain less
than five words, we take a linked title pair as a trans-
lation entry without considering the word alignment
relation between the words inside the titles. The re-
sult lexicon contains 105,320 translation pairs be-
tween 103,823 Chinese titles and 103,227 English
titles. Obviously, only a small percentage of titles
have more than one translation. Whenever there is
more than one translation, we take the candidate en-
try as correct if and only if it matches one of the
translations.
Moreover, we compare our semi-supervised ap-
proach with an unsupervised approach (Lin et al,
2008). Lin et al (2008) took ?2(fj , ei) score
14(Gale and Church, 1991) with threshold 0.001 as
the word alignment probability in a word alignment
algorithm, Competitive Link. Competitive Link tries
to align an unlinked ei with an unlinked fj by the
condition that ?2(fj , ei) is the biggest. Lin et al
(2008) relaxed the unlinked constraints to allow con-
secutive sequence of words on one side to be linked
to the same word on the other side15. The left
13English and Chinese Wikipedia pages due to 2008.09.23
are used here.
14?2(fj , ei) = (ad?bc)
2
(a+b)(a+c)(b+d)(c+d) , where a is the number
of fJ1 (eI1) containing both ei and fj ; (a + b) is the number of
fJ1 (eI1) containing ei; (a+ c) is the number of fJ1 (eI1) contain-
ing fj ; and d is the number of fJ1 (eI1) containing neither ei nor
fj .
15Instead of requiring both ei and fj to have no previous link-
boundary inside fJ1 is determined when each ei in
eI1 is aligned. After applying the modified Compet-
itive Link on the partially parallel corpus which in-
cludes 12,444,264 entries (Section 4.2), we obtained
2,628,366 distinct pairs.
Table 5 shows the results of the two lexicons eval-
uated under Wikipedia title dictionary. The coverage
is measured by the percentage of titles which ap-
pears in the mined lexicon. We then check whether
the translation in the mined lexicon is an exact match
of one of the translations in the Wikipedia lexicon.
Through comparing the results, our mined lexicon is
comparable with the lexicon mined in an unsuper-
vised way. Since the selection is based on phone-
mic and semantic clues instead of frequency, a par-
enthetical translation candidate will not be selected
if the in-parenthetical English text is failed to be
transliterated or translated. This is one reason that
explains why we earned a little lower coverage. An-
other reason comes from the low coverage rate of
seed lexicons used for self-training, only 8.65% En-
glish words in the partially parallel corpus are cov-
ered by the Wanfang dictionary.
8 Conclusion
We have proposed a semi-supervised learning
framework for mining bilingual lexicons from par-
enthetical expressions in monolingual Web pages.
We classified the parenthesis expressions into three
categories: abbreviation, transliteration, and transla-
tion. A set of heuristic rules, a self-trained hybrid
transliteration model, and a self-trained cascaded
translation model were proposed for each category,
respectively.
We investigated the scalability of the self-trained
transliteration and translation models by training
them with different amount of data. The results shew
the stability (transliteration) and feasibility (transla-
tion) of our proposals. Through employing the par-
allel Wikipedia article titles as a gold standard lex-
icon, we gained the comparable results comparing
our semi-supervised framework with our implemen-
tation of Lin et al (2008)?s unsupervised mining
approach.
ages, they only require that at least one of them be unlinked and
that (suppose ei is unlinked and fj is linked to ek) none of the
words between ei and ek be linked to any word other than fj .
431
Acknowledgments
This work was partially supported by Grant-in-Aid
for Specially Promoted Research (MEXT, Japan)
and Japanese/Chinese Machine Translation Project
in Special Coordination Funds for Promoting Sci-
ence and Technology (MEXT, Japan). We thank
the anonymous reviewers for their constructive com-
ments.
References
Cao, Guihong, Jianfeng Gao, and Jian-Yun Nie. 2007.
A system to Mine Large-Scale Bilingual Dictionar-
ies from Monolingual Web Pages. In MT Summit XI.
pages 57?64, Copenhagen, Denmark.
Cheng, Pu-Jen, Yi-Cheng Pan, Wen-Hsiang Lu, and Lee-
Feng Chien. 2004. Creating Multilingual Translation
Lexicons with Regional Variations Using Web Cor-
pora. In ACL 2004, pages 534?541, Barcelona,
Spain.
Creutz, Mathias and Krista Lagus. 2007. Unsupervised
Models for Morpheme Segmentation and Morphology
Learning. ACM Transactions on Speech and Lan-
guage Processing, 4(1):Article 3.
Dempster, A. P., N. M. Laird and D. B. Rubin. 1977.
Maximum Likelihood from Incomplete Data via the
EM Algorithm. Journal of the Royal Statistical Soci-
ety, 39:1?38.
Gale, W. and K. Church. 1991. Identifying word corre-
spondence in parallel text. In DARPA NLP Workshop.
Gao, Jianfeng, Mu Li, Andi Wu, and Chang-Ning Huang.
2006. Chinese Word Segmentation and Named Entity
Recognition: A Pragmatic Approach. Computational
Linguistics, 31(4):531?574.
Haghighi, Aria, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein 2008. Learning Bilingual Lexicons
from Monolingual Corpora. In ACL-08:HLT. pages
771?779, Columbus, Ohio.
Jiang, Long, Ming Zhou, Lee-Feng Chien, and Cheng
Niu. 2007. Named Entity Translation with Web Min-
ing and Transliteration. In IJCAI 2007. pages 1629?
1634, Hyderabad, India.
Koehn, Philipp, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondr?ej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open Source
Toolkit for Statistical Machine Translation. In ACL
2007 Poster Session, pages 177?180.
Koehn, Philipp and Kevin Knight. 2002. Learning
a translation lexicon from monolingual corpora. In
SIGLEX 2002, pages 9?16.
Kuo, Jin-Shea, Haizhou Li, and Ying-Kuei Yang. 2006.
Learning Transliteration Lexicons from the Web. In
COLING-ACL 2006. pages 1129?1136.
Lin, Dekang, Shaojun Zhao, Benjamin Van Durme, and
Marius Pas?ca. 2008. Mining Parenthetical Transla-
tions from the Web by Word Alignment. In ACL-
08:HLT, pages 994?1002, Columbus, Ohio.
Matthews, David. 2007. Machine Transliteration of
Proper Names. A Thesis of Master. University of Ed-
inburgh.
McClosky, David, Eugene Charniak, and Mark Johnson
2008. When is Self-Training Effective for Parsing? In
Proceedings of the 22nd International Conference on
Computational Linguistics (Coling 2008), pages 561?
568, manchester, UK.
Melamed, I. Dan. 2000. Models of Translational Equiv-
alence among Words. Computational Linguistics,
26(2):221?249.
Och, Franz Josef and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Models.
Computational Linguistics, 29(1):19?51.
Oh, Jong-Hoon, Key-Sun Choi, and Hitoshi Isahara.
2006. A Comparison of Different Machine Translit-
eration Models. Journal of Artifical Intelligence Re-
search, 27:119?151.
Okazaki, Naoaki and Sophia Ananiadou. 2006. Building
an Abbreviation Dictionary Using a Term Recognition
Approach. Bioinformatics, 22(22):3089?3095.
Papineni, Kishore, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic Eval-
uation of Machine Translation. In Proceedings of the
40th Annual Meeting of the Association for Computa-
tional Linguistics (ACL). pages 311?318, Philadel-
phia.
Shao, Li and Hwee Tou Ng. 2004. Mining New Word
Translations from Comparable Corpora. In Proceed-
ings of the 20th International Conference on Com-
putational Linguistics (COLING), pages 618?624,
Geneva, Switzerland.
Wu, Xianchao, Naoaki Okazaki, Takashi Tsunakawa, and
Jun?ichi Tsujii. 2008. Improving English-to-Chinese
Translation for Technical Terms Using Morphological
Information. In Proceedings of the 8th Conference of
the Association for Machine Translation in the Ameri-
cas (AMTA), pages 202?211, Waikiki, Hawai?i.
Yarowsky, David. 1995. Unsupervised Word Sense Dis-
ambiguation Rivaling Supervised Methods. In Pro-
ceedings of the 33rd annual meeting on Association
for Computational Linguistics, pages 189?196, Cam-
bridge, Massachusetts.
Zhu, Xiaojin. 2007. Semi-Supervised Learning Litera-
ture Survery. University of Wisconsin - Madison.
432
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 325?334,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Fine-grained Tree-to-String Translation Rule Extraction
Xianchao Wu? Takuya Matsuzaki? Jun?ichi Tsujii???
?Department of Computer Science, The University of Tokyo
7-3-1 Hongo, Bunkyo-ku, Tokyo 113-0033, Japan
?School of Computer Science, University of Manchester
?National Centre for Text Mining (NaCTeM)
Manchester Interdisciplinary Biocentre, 131 Princess Street, Manchester M1 7DN, UK
{wxc, matuzaki, tsujii}@is.s.u-tokyo.ac.jp
Abstract
Tree-to-string translation rules are widely
used in linguistically syntax-based statis-
tical machine translation systems. In this
paper, we propose to use deep syntac-
tic information for obtaining fine-grained
translation rules. A head-driven phrase
structure grammar (HPSG) parser is used
to obtain the deep syntactic information,
which includes a fine-grained description
of the syntactic property and a semantic
representation of a sentence. We extract
fine-grained rules from aligned HPSG
tree/forest-string pairs and use them in
our tree-to-string and string-to-tree sys-
tems. Extensive experiments on large-
scale bidirectional Japanese-English trans-
lations testified the effectiveness of our ap-
proach.
1 Introduction
Tree-to-string translation rules are generic and ap-
plicable to numerous linguistically syntax-based
Statistical Machine Translation (SMT) systems,
such as string-to-tree translation (Galley et al,
2004; Galley et al, 2006; Chiang et al, 2009),
tree-to-string translation (Liu et al, 2006; Huang
et al, 2006), and forest-to-string translation (Mi et
al., 2008; Mi and Huang, 2008). The algorithms
proposed by Galley et al (2004; 2006) are fre-
quently used for extracting minimal and composed
rules from aligned 1-best tree-string pairs. Deal-
ing with the parse error problem and rule sparse-
ness problem, Mi and Huang (2008) replaced the
1-best parse tree with a packed forest which com-
pactly encodes exponentially many parses for tree-
to-string rule extraction.
However, current tree-to-string rules only make
use of Probabilistic Context-Free Grammar tree
fragments, in which part-of-speech (POS) or
koroshita korosareta
(active) (passive)
VBN(killed) 6 (6/10,6/6) 4 (4/10,4/4)
VBN(killed:active) 5 (5/6,5/6) 1 (1/6,1/4)
VBN(killed:passive) 1 (1/4,1/6) 3 (3/4,3/4)
Table 1: Bidirectional translation probabilities of
rules, denoted in the brackets, change when voice
is attached to ?killed?.
phrasal tags are used as the tree node labels. As
will be testified by our experiments, we argue that
the simple POS/phrasal tags are too coarse to re-
flect the accurate translation probabilities of the
translation rules.
For example, as shown in Table 1, sup-
pose a simple tree fragment ?VBN(killed)? ap-
pears 6 times with ?koroshita?, which is a
Japanese translation of an active form of ?killed?,
and 4 times with ?korosareta?, which is a
Japanese translation of a passive form of ?killed?.
Then, without larger tree fragments, we will
more frequently translate ?VBN(killed)? into ?ko-
roshita? (with a probability of 0.6). But,
?VBN(killed)? is indeed separable into two fine-
grained tree fragments of ?VBN(killed:active)?
and ?VBN(killed:passive)?1. Consequently,
?VBN(killed:active)? appears 5 times with ?ko-
roshita? and 1 time with ?korosareta?; and
?VBN(killed:passive)? appears 1 time with ?ko-
roshita? and 3 times with ?korosareta?. Now, by
attaching the voice information to ?killed?, we are
gaining a rule set that is more appropriate to reflect
the real translation situations.
This motivates our proposal of using deep syn-
tactic information to obtain a fine-grained trans-
lation rule set. We name the information such as
the voice of a verb in a tree fragment as deep syn-
tactic information. We use a head-driven phrase
structure grammar (HPSG) parser to obtain the
1For example, ?John has killed Mary.? versus ?John was
killed by Mary.?
325
deep syntactic information of an English sentence,
which includes a fine-grained description of the
syntactic property and a semantic representation
of the sentence. We extract fine-grained trans-
lation rules from aligned HPSG tree/forest-string
pairs. We localize an HPSG tree/forest to make
it segmentable at any nodes to fit the extraction
algorithms described in (Galley et al, 2006; Mi
and Huang, 2008). We also propose a linear-time
algorithm for extracting composed rules guided
by predicate-argument structures. The effective-
ness of the rules are testified in our tree-to-string
and string-to-tree systems, taking bidirectional
Japanese-English translations as our test cases.
This paper is organized as follows. In Section 2,
we briefly review the tree-to-string and string-to-
tree translation frameworks, tree-to-string rule ex-
traction algorithms, and rich syntactic information
previously used for SMT. The HPSG grammar and
our proposal of fine-grained rule extraction algo-
rithms are described in Section 3. Section 4 gives
the experiments for applying fine-grained transla-
tion rules to large-scale Japanese-English transla-
tion tasks. Finally, we conclude in Section 5.
2 Related Work
2.1 Tree-to-string and string-to-tree
translations
Tree-to-string translation (Liu et al, 2006; Huang
et al, 2006) first uses a parser to parse a source
sentence into a 1-best tree and then searches for
the best derivation that segments and converts the
tree into a target string. In contrast, string-to-tree
translation (Galley et al, 2004; Galley et al, 2006;
Chiang et al, 2009) is like bilingual parsing. That
is, giving a (bilingual) translation grammar and a
source sentence, we are trying to construct a parse
forest in the target language. Consequently, the
translation results can be collected from the leaves
of the parse forest.
Figure 1 illustrates the training and decoding
processes of bidirectional Japanese-English trans-
lations. The English sentence is ?John killed
Mary? and the Japanese sentence is ?jyon ha mari
wo koroshita?, in which the function words ?ha?
and ?wo? are not aligned with any English word.
2.2 Tree/forest-based rule extraction
Galley et al (2004) proposed the GHKM algo-
rithm for extracting (minimal) tree-to-string trans-
lation rules from a tuple of ?F,Et, A?, where F =
 
x0 ? x1 
x0 x1 
x1 ? x0 
NP 
John 
??? 
V 
killed ??? 
NP 
Mary ??? 
NP 
V NP 
VP 
S 
John killed Mary 
??? ? ??? ? ??? 
NP VP 
S 
V NP 
VP 
x0 x1 
Training 
Aligned tree-string pair: 
Extract 
rules  
John killed Mary 
??? ? ??? ? ??? 
CKY decoding 
Testing 
NP V NP 
VP 
S 
John killed Mary 
NP 
VP 
V NP 
Apply  
rules  
?? 
 
jyon   ha    mari  wo  koroshita 
parsing 
Bottom-up 
decoding 
tree-to-string string-to-tree 
Figure 1: Illustration of the training and decod-
ing processes for tree-to-string and string-to-tree
translations.
fJ1 is a sentence of a foreign language other than
English,Et is a 1-best parse tree of an English sen-
tence E = eI1, and A = {(j, i)} is an alignment
between the words in F and E.
The basic idea of GHKM algorithm is to de-
compose Et into a series of tree fragments, each
of which will form a rule with its corresponding
translation in the foreign language. A is used as a
constraint to guide the segmentation procedure, so
that the root node of every tree fragment of Et ex-
actly corresponds to a contiguous span on the for-
eign language side. Based on this consideration, a
frontier set (fs) is defined to be a set of nodes n in
Et that satisfies the following constraint:
fs = {n|span(n) ? comp span(n) = ?}. (1)
Here, span(n) is defined by the indices of the first
and last word in F that are reachable from a node
n, and comp span(n) is defined to be the comple-
ment set of span(n), i.e., the union of the spans
of all nodes n? in Et that are neither descendants
nor ancestors of n. span(n) and comp span(n)
of each n can be computed by first a bottom-up
exploration and then a top-down traversal of Et.
By restricting each fragment so that it only takes
326
John 
CAT       N 
POS      NNP 
BASE    john 
LEXENTRY [D< 
N.3sg>]_lxm 
PRED  noun_arg0  
t0 
HEAD           t0 
SEM_HEAD t0   
CAT           NX   
XCAT  
 
c2 
killed 
CAT     V 
POS     VBD 
BASE   kill 
LEXENTRY [NP.nom  
<V.bse> NP.acc] 
_lxm-past_verb_rule 
PRED  verb_arg12   
TENSE     past 
ASPECT   none 
VOICE      active 
AUX          minus 
ARG1       c1 
ARG2       c5 
t1 
HEAD           t1 
SEM_HEAD t1  
CAT           VX  
XCAT    
 
c4 
HEAD           c6 
SEM_HEAD c6   
CAT              NP   
XCAT    
SCHEMA empty_spec_head  
 
c5 
HEAD           t2 
SEM_HEAD t2  
CAT          NX  
XCAT    
 
c6 
HEAD           c3 
SEM_HEAD c3   
CAT              S   
XCAT    
SCHEMA subj_head  
 
c0 
HEAD           c2 
SEM_HEAD c2   
CAT              NP   
XCAT    
SCHEMA empty_spec_head  
 
c1 
HEAD           c4 
SEM_HEAD c4   
CAT              VP   
XCAT    
SCHEMA  head_comp  
c3 
Mary 
CAT       N 
POS      NNP 
BASE    mary 
LEXENTRY  
[D<N.3sg>]_lxm 
PRED    noun_arg0  
 
t2 
??? ? ??? ? ??? 
1. c0(x0:c1, x1:c3)  x0 ? x1 
2. c1(x0:c2)  x0 
3. c2(t0)  ??? 
4. c3(x0:c4, x1:c5)  x1 ? x0 
5. c4(t1)  ??? 
6. c5(x0:c6)  x0 
7. c6(t2)  ??? 
c0 
c1 c3 
c4 c5 
t1 
minimum  
covering tree 
x0 ? x1 ? ??? 
An HPSG-tree based minimal rule set A PAS-based composed rule  
John killed Mary 
HEAD           c8 
SEM_HEAD c8   
CAT              S   
XCAT    
SCHEMA head_mod  
 
c7 
HEAD           c9 
SEM_HEAD c9   
CAT              S   
XCAT    
SCHEMA  subj_head  
 
c8 
killed 
CAT     V 
POS     VBD 
BASE   kill 
LEXENTRY [NP.nom 
<V.bse>]_lxm-
past_verb_rule 
PRED  verb_arg1   
TENSE     past 
ASPECT   none 
VOICE      active 
AUX          minus 
ARG1       c1 
t3 
HEAD           t3 
SEM_HEAD t3  
CAT           VP  
XCAT    
 
c9 
HEAD           c11 
SEM_HEAD c11   
CAT              NP   
XCAT    
SCHEMA empty_spec_head  
 
c10 
HEAD           t4 
SEM_HEAD t4  
CAT          NX  
XCAT    
 
c11 
Mary 
CAT       N 
POS      NNP 
BASE    mary 
LEXENTRY  
V[D<N.3sg>] 
PRED    noun_arg0  
 
t4 
 
2.77 4.52 
0.81 2.25 
0 
0.00 
-3.47 -0.03 
0 
-2.82 
-0.07 -0.001 
Figure 2: Illustration of an aligned HPSG forest-string pair. The forest includes two parse trees by taking
?Mary? as a modifier (t3, t4) or an argument (t1, t2) of ?killed?. Arrows with broken lines denote the PAS
dependencies from the terminal node t1 to its argument nodes (c1 and c5). The scores of the hyperedges
are attached to the forest as well.
the nodes in fs as the root and leaf nodes, a well-
formed fragmentation of Et is generated. With
fs computed, rules are extracted through a depth-
first traversal of Et: we cut Et at all nodes in fs
to form tree fragments and extract a rule for each
fragment. These extracted rules are calledminimal
rules (Galley et al, 2004). For example, the 1-
best tree (with gray nodes) in Figure 2 is cut into 7
pieces, each of which corresponds to the tree frag-
ment in a rule (bottom-left corner of the figure).
In order to include richer context information
and account for multiple interpretations of un-
aligned words of foreign language, minimal rules
which share adjacent tree fragments are connected
together to form composed rules (Galley et al,
2006). For each aligned tree-string pair, Gal-
ley et al (2006) constructed a derivation-forest,
in which composed rules were generated, un-
aligned words of foreign language were consis-
tently attached, and the translation probabilities
of rules were estimated by using Expectation-
Maximization (EM) (Dempster et al, 1977) train-
ing. For example, by combining the minimal rules
of 1, 4, and 5, we obtain a composed rule, as
shown in the bottom-right corner of Figure 2.
Considering the parse error problem in the
1-best or k-best parse trees, Mi and Huang
(2008) extracted tree-to-string translation rules
from aligned packed forest-string pairs. A for-
est compactly encodes exponentially many trees
327
rather than the 1-best tree used by Galley et al
(2004; 2006). Two problems were managed to
be tackled during extracting rules from an aligned
forest-string pair: where to cut and how to cut.
Equation 1 was used again to compute a frontier
node set to determine where to cut the packed
forest into a number of tree-fragments. The dif-
ference with tree-based rule extraction is that the
nodes in a packed forest (which is a hypergraph)
now are hypernodes, which can take a set of in-
coming hyperedges. Then, by limiting each frag-
ment to be a tree and whose root/leaf hypernodes
all appearing in the frontier set, the packed forest
can be segmented properly into a set of tree frag-
ments, each of which can be used to generate a
tree-to-string translation rule.
2.3 Rich syntactic information for SMT
Before describing our approaches of applying
deep syntactic information yielded by an HPSG
parser for fine-grained rule extraction, we would
like to briefly review what kinds of deep syntactic
information have been employed for SMT.
Two kinds of supertags, from Lexicalized Tree-
Adjoining Grammar and Combinatory Categorial
Grammar (CCG), have been used as lexical syn-
tactic descriptions (Hassan et al, 2007) for phrase-
based SMT (Koehn et al, 2007). By introduc-
ing supertags into the target language side, i.e.,
the target language model and the target side
of the phrase table, significant improvement was
achieved for Arabic-to-English translation. Birch
et al (2007) also reported a significant improve-
ment for Dutch-English translation by applying
CCG supertags at a word level to a factorized SMT
system (Koehn et al, 2007).
In this paper, we also make use of supertags
on the English language side. In an HPSG
parse tree, these lexical syntactic descriptions
are included in the LEXENTRY feature (re-
fer to Table 2) of a lexical node (Matsuzaki
et al, 2007). For example, the LEXEN-
TRY feature of ?t1:killed? takes the value of
[NP.nom<V.bse>NP.acc]_lxm-past
_verb_rule in Figure 2. In which,
[NP.nom<V.bse>NP.acc] is an HPSG
style supertag, which tells us that the base form
of ?killed? needs a nominative NP in the left hand
side and an accessorial NP in the right hand side.
The major differences are that, we use a larger
feature set (Table 2) including the supertags for
fine-grained tree-to-string rule extraction, rather
than string-to-string translation (Hassan et al,
2007; Birch et al, 2007).
The Logon project2 (Oepen et al, 2007) for
Norwegian-English translation integrates in-depth
grammatical analysis of Norwegian (using lexi-
cal functional grammar, similar to (Riezler and
Maxwell, 2006)) with semantic representations in
the minimal recursion semantics framework, and
fully grammar-based generation for English using
HPSG. A hybrid (of rule-based and data-driven)
architecture with a semantic transfer backbone is
taken as the vantage point of this project. In
contrast, the fine-grained tree-to-string translation
rule extraction approaches in this paper are to-
tally data-driven, and easily applicable to numer-
ous language pairs by taking English as the source
or target language.
3 Fine-grained rule extraction
We now introduce the deep syntactic informa-
tion generated by an HPSG parser and then de-
scribe our approaches for fine-grained tree-to-
string rule extraction. Especially, we localize an
HPSG tree/forest to fit the extraction algorithms
described in (Galley et al, 2006; Mi and Huang,
2008). Also, we propose a linear-time com-
posed rule extraction algorithm by making use of
predicate-argument structures.
3.1 Deep syntactic information by HPSG
parsing
Head-driven phrase structure grammar (HPSG) is
a lexicalist grammar framework. In HPSG, lin-
guistic entities such as words and phrases are rep-
resented by a data structure called a sign. A sign
gives a factored representation of the syntactic fea-
tures of a word/phrase, as well as a representation
of their semantic content. Phrases and words rep-
resented by signs are composed into larger phrases
by applications of schemata. The semantic rep-
resentation of the new phrase is calculated at the
same time. As such, an HPSG parse tree/forest
can be considered as a tree/forest of signs (c.f. the
HPSG forest in Figure 2).
An HPSG parse tree/forest has two attractive
properties as a representation of an English sen-
tence in syntax-based SMT. First, we can carefully
control the condition of the application of a trans-
lation rule by exploiting the fine-grained syntactic
2http://www.emmtee.net/
328
Feature Description
CAT phrasal category
XCAT fine-grained phrasal category
SCHEMA name of the schema applied in the node
HEAD pointer to the head daughter
SEM HEAD pointer to the semantic head daughter
CAT syntactic category
POS Penn Treebank-style part-of-speech tag
BASE base form
TENSE tense of a verb (past, present, untensed)
ASPECT aspect of a verb (none, perfect,
progressive, perfect-progressive)
VOICE voice of a verb (passive, active)
AUX auxiliary verb or not (minus, modal,
have, be, do, to, copular)
LEXENTRY lexical entry, with supertags embedded
PRED type of a predicate
ARG?x? pointer to semantic arguments, x = 1..4
Table 2: Syntactic/semantic features extracted
from HPSG signs that are included in the output
of Enju. Features in phrasal nodes (top) and lexi-
cal nodes (bottom) are listed separately.
description in the English parse tree/forest, as well
as those in the translation rules. Second, we can
identify sub-trees in a parse tree/forest that cor-
respond to basic units of the semantics, namely
sub-trees covering a predicate and its arguments,
by using the semantic representation given in the
signs. We expect that extraction of translation
rules based on such semantically-connected sub-
trees will give a compact and effective set of trans-
lation rules.
A sign in the HPSG tree/forest is represented by
a typed feature structure (TFS) (Carpenter, 1992).
A TFS is a directed-acyclic graph (DAG) wherein
the edges are labeled with feature names and the
nodes (feature values) are typed. In the original
HPSG formalism, the types are defined in a hierar-
chy and the DAG can have arbitrary shape (e.g., it
can be of any depth). We however use a simplified
form of TFS, for simplicity of the algorithms. In
the simplified form, a TFS is converted to a (flat)
set of pairs of feature names and their values. Ta-
ble 2 lists the features used in this paper, which
are a subset of those in the original output from an
HPSG parser, Enju3. The HPSG forest shown in
Figure 2 is in this simplified format. An impor-
tant detail is that we allow a feature value to be a
pointer to another (simplified) TFS. Such pointer-
valued features are necessary for denoting the se-
mantics, as explained shortly.
In the Enju English HPSG grammar (Miyao et
3http://www-tsujii.is.s.u-tokyo.ac.jp/enju/index.html
 
 
She 
ignore 
 fact 
want 
I 
dispute 
ARG1 
ARG2 
ARG1 ARG1 
ARG2 
ARG2 
John 
kill 
 Mary ARG2 
ARG1 
Figure 3: Predicate argument structures for the
sentences of ?John killed Mary? and ?She ignored
the fact that I wanted to dispute?.
al., 2003) used in this paper, the semantic content
of a sentence/phrase is represented by a predicate-
argument structure (PAS). Figure 3 shows the PAS
of the example sentence in Figure 2, ?John killed
Mary?, and a more complex PAS for another sen-
tence, ?She ignored the fact that I wanted to dis-
pute?, which is adopted from (Miyao et al, 2003).
In an HPSG tree/forest, each leaf node generally
introduces a predicate, which is represented by
the pair of LEXENTRY (lexical entry) feature and
PRED (predicate type) feature. The arguments of
a predicate are designated by the pointers from the
ARG?x? features in a leaf node to non-terminal
nodes.
3.2 Localize HPSG forest
Our fine-grained translation rule extraction algo-
rithm is sketched in Algorithm 1. Considering that
a parse tree is a trivial packed forest, we only use
the term forest to expand our discussion, hereafter.
Recall that there are pointer-valued features in the
TFSs (Table 2) which prevent arbitrary segmenta-
tion of a packed forest. Hence, we have to localize
an HPSG forest.
For example, there are ARG pointers from t1 to
c1 and c5 in the HPSG forest of Figure 2. How-
ever, the three nodes are not included in one (min-
imal) translation rule. This problem is caused
by not considering the predicate argument depen-
dency among t1, c1, and c5 while performing the
GHKM algorithm. We can combine several min-
imal rules (Galley et al, 2006) together to ad-
dress this dependency. Yet we have a faster way
to tackle PASs, as will be described in the next
subsection.
Even if we omit ARG, there are still two kinds
of pointer-valued features in TFSs, HEAD and
SEM HEAD. Localizing these pointer-valued fea-
tures is straightforward, since during parsing, the
HEAD and SEM HEAD of a node are automati-
cally transferred to its mother node. That is, the
syntactic and semantic head of a node only take
329
Algorithm 1 Fine-grained rule extraction
Input: HPSG tree/forest Ef , foreign sentence F , and align-
ment A
Output: a PAS-based rule set R1 and/or a tree-rule set R2
1: if Ef is an HPSG tree then
2: E
?
f = localize Tree(Ef )
3: R1 = PASR extraction(E
?
f , F , A) ? Algorithm 2
4: E
??
f = ignore PAS(E
?
f )
5: R2 = TR extraction(E
??
f , F , A) ? composed rule ex-
traction algorithm in (Galley et al, 2006)
6: else if Ef is an HPSG forest then
7: E
?
f = localize Forest(Ef );
8: R2 = forest based rule extraction(E
?
f , F , A) ? Algo-
rithm 1 in (Mi and Huang, 2008)
9: end if
the identifier of the daughter node as the values.
For example, HEAD and SEM HEAD of node c0
take the identical value to be c3 in Figure 2.
To extract tree-to-string rules from the tree
structures of an HPSG forest, our solution is to
pre-process an HPSG forest in the following way:
? for a phrasal hypernode, replace its HEAD
and SEM HEAD value with L, R, or S,
which respectively represent left daughter,
right daughter, or single daughter (line 2 and
7); and,
? for a lexical node, ARG?x? and PRED fea-
tures are ignored (line 4).
A pure syntactic-based HPSG forest without any
pointer-valued features can be yielded through this
pre-processing for the consequent execution of the
extraction algorithms (Galley et al, 2006; Mi and
Huang, 2008).
3.3 Predicate-argument structures
In order to extract translation rules from PASs,
we want to localize a predicate word and its ar-
guments into one tree fragment. For example, in
Figure 2, we can use a tree fragment which takes
c0 as its root node and c1, t1, and c5 on its yield (=
leaf nodes of a tree fragment) to cover ?killed? and
its subject and direct object arguments. We define
this kind of tree fragment to be a minimum cov-
ering tree. For example, the minimum covering
tree of {t1, c1, c5} is shown in the bottom-right
corner of Figure 2. The definition supplies us a
linear-time algorithm to directly find the tree frag-
ment that covers a PAS during both rule extracting
and rule matching when decoding an HPSG tree.
Algorithm 2 PASR extraction
Input: HPSG tree Et, foreign sentence F , and alignment A
Output: a PAS-based rule set R
1: R = {}
2: for node n ? Leaves(Et) do
3: if Open(n.ARG) then
4: Tc = MinimumCoveringTree(Et, n, n.ARGs)
5: if root and leaf nodes of Tc are in fs then
6: generate a rule r using fragment Tc
7: R.append(r)
8: end if
9: end if
10: end for
See (Wu, 2010) for more examples of minimum
covering trees.
Taking a minimum covering tree as the tree
fragment, we can easily build a tree-to-string
translation rule that reflects the semantic depen-
dency of a PAS. The algorithm of PAS-based
rule (PASR) extraction is sketched in Algorithm
2. Suppose we are given a tuple of ?F,Et, A?.
Et is pre-processed by replacing HEAD and
SEM HEAD to be L, R, or S, and computing the
span and comp span of each node.
We extract PAS-based rules through one-time
traversal of the leaf nodes in Et (line 2). For each
leaf node n, we extract a minimum covering tree
Tc if n contains at least one argument. That is, at
least one ARG?x? takes the value of some node
identifier, where x ranges 1 over 4 (line 3). Then,
we require the root and yield nodes of Tc being in
the frontier set of Et (line 5). Based on Tc, we can
easily build a tree-to-string translation rule by fur-
ther completing the right-hand-side string by sort-
ing the spans of Tc?s leaf nodes, lexicalizing the
terminal node?s span(s), and assigning a variable
to each non-terminal node?s span. Maximum like-
lihood estimation is used to calculate the transla-
tion probabilities of each rule.
An example of PAS-based rule is shown in the
bottom-right corner of Figure 2. In the rule, the
subject and direct-object of ?killed? are general-
ized into two variables, x0 and x1.
4 Experiments
4.1 Translation models
We use a tree-to-string model and a string-to-tree
model for bidirectional Japanese-English transla-
tions. Both models use a phrase translation table
(PTT), an HPSG tree-based rule set (TRS), and
a PAS-based rule set (PRS). Since the three rule
sets are independently extracted and estimated, we
330
use Minimum Error Rate Training (MERT) (Och,
2003) to tune the weights of the features from the
three rule sets on the development set.
Given a 1-best (localized) HPSG tree Et, the
tree-to-string decoder searches for the optimal
derivation d? that transforms Et into a Japanese
string among the set of all possible derivations D:
d? =argmax
d?D
{?1 log pLM (?(d)) + ?2|?(d)|
+ log s(d|Et)}. (2)
Here, the first item is the language model (LM)
probability where ?(d) is the target string of
derivation d; the second item is the translation
length penalty; and the third item is the transla-
tion score, which is decomposed into a product of
feature values of rules:
s(d|Et) =
?
r?d
f(r?PTT )f(r?TRS)f(r?PRS).
This equation reflects that the translation rules in
one d come from three sets. Inspired by (Liu et
al., 2009b), it is appealing to combine these rule
sets together in one decoder because PTT provides
excellent rule coverages while TRS and PRS offer
linguistically motivated phrase selections and non-
local reorderings. Each f(r) is in turn a product of
five features:
f(r) = p(s|t)?3 ? p(t|s)?4 ? l(s|t)?5 ? l(t|s)?6 ? e?7 .
Here, s/t represent the source/target part of a rule
in PTT, TRS, or PRS; p(?|?) and l(?|?) are transla-
tion probabilities and lexical weights of rules from
PTT, TRS, and PRS. The derivation length penalty
is controlled by ?7.
In our string-to-tree model, for efficient decod-
ing with integrated n-gram LM, we follow (Zhang
et al, 2006) and inversely binarize all translation
rules into Chomsky Normal Forms that contain
at most two variables and can be incrementally
scored by LM. In order to make use of the bina-
rized rules in the CKY decoding, we add two kinds
of glues rules:
S ? Xm(1), Xm(1);
S ? S(1)Xm(2), S(1)Xm(2).
Here Xm ranges over the nonterminals appearing
in a binarized rule set. These glue rules can be
seen as an extension from X to {Xm}of the two
glue rules described in (Chiang, 2007).
The string-to-tree decoder searches for the op-
timal derivation d? that parses a Japanese string
F into a packed forest of the set of all possible
derivations D:
d? =argmax
d?D
{?1 log pLM (?(d)) + ?2|?(d)|
+ ?3g(d) + log s(d|F )}. (3)
This formula differs from Equation 2 by replacing
Et with F in s(d|?) and adding g(d), which is the
number of glue rules used in d. Further definitions
of s(d|F ) and f(r) are identical with those used
in Equation 2.
4.2 Decoding algorithms
In our translation models, we have made use
of three kinds of translation rule sets which are
trained separately. We perform derivation-level
combination as described in (Liu et al, 2009b) for
mixing different types of translation rules within
one derivation.
For tree-to-string translation, we use a bottom-
up beam search algorithm (Liu et al, 2006) for
decoding an HPSG tree Et. We keep at most 10
best derivations with distinct ?(d)s at each node.
Recall the definition of minimum covering tree,
which supports a faster way to retrieve available
rules from PRS without generating all the sub-
trees. That is, when node n fortunately to be the
root of someminimum covering tree(s), we use the
tree(s) to seek available PAS-based rules in PRS.
We keep a hash-table with the key to be the node
identifier of n and the value to be a priority queue
of available PAS-based rules. The hash-table is
easy to be filled by one-time traversal of the termi-
nal nodes in Et. At each terminal node, we seek
its minimum covering tree, retrieve PRS, and up-
date the hash-table. For example, suppose we are
decoding an HPSG tree (with gray nodes) shown
in Figure 2. At t1, we can extract its minimum
covering tree with the root node to be c0, then take
this tree fragment as the key to retrieve PRS, and
consequently put c0 and the available rules in the
hash-table. When decoding at c0, we can directly
access the hash-table looking for available PAS-
based rules.
In contrast, we use a CKY-style algorithm with
beam-pruning and cube-pruning (Chiang, 2007)
to decode Japanese sentences. For each Japanese
sentence F , the output of the chart-parsing algo-
rithm is expressed as a hypergraph representing a
set of derivations. Given such a hypergraph, we
331
Train Dev. Test
# of sentences 994K 2K 2K
# of Jp words 28.2M 57.4K 57.1K
# of En words 24.7M 50.3K 49.9K
Table 3: Statistics of the JST corpus.
use the Algorithm 3 described in (Huang and Chi-
ang, 2005) to extract its k-best (k = 500 in our
experiments) derivations. Since different deriva-
tions may lead to the same target language string,
we further adopt Algorithm 3?s modification, i.e.,
keep a hash-table to maintain the unique target
sentences (Huang et al, 2006), to efficiently gen-
erate the unique k-best translations.
4.3 Setups
The JST Japanese-English paper abstract corpus4,
which consists of one million parallel sentences,
was used for training and testing. This corpus
was constructed from a Japanese-English paper
abstract corpus by using the method of Utiyama
and Isahara (2007). Table 3 shows the statistics
of this corpus. Making use of Enju 2.3.1, we suc-
cessfully parsed 987,401 English sentences in the
training set, with a parse rate of 99.3%. We mod-
ified this parser to output a packed forest for each
English sentence.
We executed GIZA++ (Och and Ney, 2003) and
grow-diag-final-and balancing strategy (Koehn et
al., 2007) on the training set to obtain a phrase-
aligned parallel corpus, from which bidirectional
phrase translation tables were estimated. SRI Lan-
guage Modeling Toolkit (Stolcke, 2002) was em-
ployed to train 5-gram English and Japanese LMs
on the training set. We evaluated the translation
quality using the case-insensitive BLEU-4 metric
(Papineni et al, 2002). The MERT toolkit we used
is Z-mert5 (Zaidan, 2009).
The baseline system for comparison is Joshua
(Li et al, 2009), a freely available decoder for hi-
erarchical phrase-based SMT (Chiang, 2005). We
respectively extracted 4.5M and 5.3M translation
rules from the training set for the 4K English and
Japanese sentences in the development and test
sets. We used the default configuration of Joshua,
expect setting the maximum number of items/rules
and the k of k-best outputs to be the identical
4http://www.jst.go.jp. The corpus can be conditionally
obtained from NTCIR-7 patent translation workshop home-
page: http://research.nii.ac.jp/ntcir/permission/ntcir-7/perm-
en-PATMT.html.
5http://www.cs.jhu.edu/ ozaidan/zmert/
PRS CS3 C3 FS F
tree nodes TFS POS TFS POS TFS
# rules 0.9 62.1 83.9 92.5 103.7
# tree types 0.4 23.5 34.7 40.6 45.2
extract time 3.5 - 98.6 - 121.2
Table 4: Statistics of several kinds of tree-to-string
rules. Here, the number is in million level and the
time is in hour.
200 for English-to-Japanese translation and 500
for Japanese-to-English translation.
We used four dual core Xeon machines
(4?3.0GHz?2CPU, 4?64GB memory) to run all
the experiments.
4.4 Results
Table 4 illustrates the statistics of several transla-
tion rule sets, which are classified by:
? using TFSs or simple POS/phrasal tags (an-
notated by a superscript S) to represent tree
nodes;
? composed rules (PRS) extracted from the
PAS of 1-best HPSG trees;
? composed rules (C3), extracted from the tree
structures of 1-best HPSG trees, and 3 is the
maximum number of internal nodes in the
tree fragments; and
? forest-based rules (F ), where the packed
forests are pre-pruned by the marginal
probability-based inside-outside algorithm
used in (Mi and Huang, 2008).
Table 5 reports the BLEU-4 scores achieved by
decoding the test set making use of Joshua and our
systems (t2s = tree-to-string and s2t = string-to-
tree) under numerous rule sets. We analyze this
table in terms of several aspects to prove the effec-
tiveness of deep syntactic information for SMT.
Let?s first look at the performance of TFSs. We
take CS3 and FS as approximations of CFG-based
translation rules. Comparing the BLEU-4 scores
of PTT+CS3 and PTT+C3, we gained 0.56 (t2s)
and 0.57 (s2t) BLEU-4 points which are signifi-
cant improvements (p < 0.05). Furthermore, we
gained 0.50 (t2s) and 0.62 (s2t) BLEU-4 points
from PTT+FS to PTT+F , which are also signif-
icant improvements (p < 0.05). The rich fea-
tures included in TFSs contribute to these im-
provements.
332
Systems BLEU-t2s Decoding BLEU-s2t
Joshua 21.79 0.486 19.73
PTT 18.40 0.013 17.21
PTT+PRS 22.12 0.031 19.33
PTT+CS3 23.56 2.686 20.59
PTT+C3 24.12 2.753 21.16
PTT+C3+PRS 24.13 2.930 21.20
PTT+FS 24.25 3.241 22.05
PTT+F 24.75 3.470 22.67
Table 5: BLEU-4 scores (%) achieved by Joshua
and our systems under numerous rule configura-
tions. The decoding time (seconds per sentence)
of tree-to-string translation is listed as well.
Also, BLEU-4 scores were inspiringly in-
creased 3.72 (t2s) and 2.12 (s2t) points by append-
ing PRS to PTT, comparing PTT with PTT+PRS.
Furthermore, in Table 5, the decoding time (sec-
onds per sentence) of tree-to-string translation by
using PTT+PRS is more than 86 times faster than
using the other tree-to-string rule sets. This sug-
gests that the direct generation of minimum cover-
ing trees for rule matching is extremely faster than
generating all subtrees of a tree node. Note that
PTT performed extremely bad compared with all
other systems or tree-based rule sets. The major
reason is that we did not perform any reordering
or distorting during decoding with PTT.
However, in both t2s and s2t systems, the
BLEU-4 score benefits of PRS were covered by
the composed rules: both PTT+CS3 and PTT+C3
performed significant better (p < 0.01) than
PTT+PRS, and there are no significant differences
when appending PRS to PTT+C3. The reason is
obvious: PRS is only a small subset of the com-
posed rules, and the probabilities of rules in PRS
were estimated by maximum likelihood, which is
fast but biased compared with EM based estima-
tion (Galley et al, 2006).
Finally, by using PTT+F , our systems achieved
the best BLEU-4 scores of 24.75% (t2s) and
22.67% (s2t), both are significantly better (p <
0.01) than that achieved by Joshua.
5 Conclusion
We have proposed approaches of using deep syn-
tactic information for extracting fine-grained tree-
to-string translation rules from aligned HPSG
forest-string pairs. The main contributions are the
applications of GHKM-related algorithms (Galley
et al, 2006; Mi and Huang, 2008) to HPSG forests
and a linear-time algorithm for extracting com-
posed rules from predicate-argument structures.
We applied our fine-grained translation rules to a
tree-to-string system and an Hiero-style string-to-
tree system. Extensive experiments on large-scale
bidirectional Japanese-English translations testi-
fied the significant improvements on BLEU score.
We argue the fine-grained translation rules are
generic and applicable to many syntax-based SMT
frameworks such as the forest-to-string model (Mi
et al, 2008). Furthermore, it will be interesting
to extract fine-grained tree-to-tree translation rules
by integrating deep syntactic information in the
source and/or target language side(s). These tree-
to-tree rules are applicable for forest-to-tree trans-
lation models (Liu et al, 2009a).
Acknowledgments
This work was partially supported by Grant-in-
Aid for Specially Promoted Research (MEXT,
Japan) and Japanese/Chinese Machine Translation
Project in Special Coordination Funds for Pro-
moting Science and Technology (MEXT, Japan),
and Microsoft Research Asia Machine Translation
Theme. The first author thanks Naoaki Okazaki
and Yusuke Miyao for their help and the anony-
mous reviewers for improving the earlier version.
References
Alexandra Birch, Miles Osborne, and Philipp Koehn.
2007. Ccg supertags in factored statistical machine
translation. In Proceedings of the Second Work-
shop on Statistical Machine Translation, pages 9?
16, June.
Bob Carpenter. 1992. The Logic of Typed Feature
Structures. Cambridge University Press.
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine transla-
tion. In Proceedings of HLT-NAACL, pages 218?
226, June.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of ACL, pages 263?270, Ann Arbor, MI.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Lingustics, 33(2):201?228.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the
em algorithm. Journal of the Royal Statistical Soci-
ety, 39:1?38.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation rule?
In Proceedings of HLT-NAACL.
333
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Pro-
ceedings of COLING-ACL, pages 961?968, Sydney.
Hany Hassan, Khalil Sima?an, and Andy Way. 2007.
Supertagged phrase-based statistical machine trans-
lation. In Proceedings of ACL, pages 288?295, June.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proceedings of IWPT.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proceedings of 7th AMTA.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondr?ej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the ACL 2007 Demo and Poster Ses-
sions, pages 177?180.
Zhifei Li, Chris Callison-Burch, Chris Dyery, Juri
Ganitkevitch, Sanjeev Khudanpur, Lane Schwartz,
Wren N. G. Thornton, Jonathan Weese, and Omar F.
Zaidan. 2009. Demonstration of joshua: An open
source toolkit for parsing-based machine translation.
In Proceedings of the ACL-IJCNLP 2009 Software
Demonstrations, pages 25?28, August.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment templates for statistical machine
transaltion. In Proceedings of COLING-ACL, pages
609?616, Sydney, Australia.
Yang Liu, Yajuan Lu?, and Qun Liu. 2009a. Improving
tree-to-tree translation with packed forests. In Pro-
ceedings of ACL-IJCNLP, pages 558?566, August.
Yang Liu, Haitao Mi, Yang Feng, and Qun Liu. 2009b.
Joint decoding with multiple translation models. In
Proceedings of ACL-IJCNLP, pages 576?584, Au-
gust.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.
2007. Efficient hpsg parsing with supertagging and
cfg-filtering. In Proceedings of IJCAI, pages 1671?
1676, January.
Haitao Mi and Liang Huang. 2008. Forest-based trans-
lation rule extraction. In Proceedings of the 2008
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 206?214, October.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proceedings of ACL-08:HLT,
pages 192?199, Columbus, Ohio.
Yusuke Miyao, Takashi Ninomiya, and Jun?ichi Tsu-
jii. 2003. Probabilistic modeling of argument struc-
tures including non-local dependencies. In Proceed-
ings of the International Conference on Recent Ad-
vances in Natural Language Processing, pages 285?
291, Borovets.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
ACL, pages 160?167.
Stephan Oepen, Erik Velldal, Jan Tore L?nning, Paul
Meurer, and Victoria Rose?n. 2007. Towards hy-
brid quality-oriented machine translation - on lin-
guistics and probabilities in mt. In Proceedings
of the 11th International Conference on Theoretical
and Methodological Issues in Machine Translation
(TMI-07), September.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings
of ACL, pages 311?318.
Stefan Riezler and John T. Maxwell, III. 2006. Gram-
matical machine translation. In Proceedings of HLT-
NAACL, pages 248?255.
Andreas Stolcke. 2002. Srilm-an extensible language
modeling toolkit. In Proceedings of International
Conference on Spoken Language Processing, pages
901?904.
Masao Utiyama and Hitoshi Isahara. 2007. A
japanese-english patent parallel corpus. In Proceed-
ings of MT Summit XI, pages 475?482, Copenhagen.
Xianchao Wu. 2010. Statistical Machine Transla-
tion Using Large-Scale Lexicon and Deep Syntactic
Structures. Ph.D. dissertation. Department of Com-
puter Science, The University of Tokyo.
Omar F. Zaidan. 2009. Z-MERT: A fully configurable
open source tool for minimum error rate training of
machine translation systems. The Prague Bulletin of
Mathematical Linguistics, 91:79?88.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin
Knight. 2006. Synchronous binarization for ma-
chine translation. In Proceedings of HLT-NAACL,
pages 256?263, June.
334
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 22?31,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Effective Use of Function Words for Rule Generalization
in Forest-Based Translation
Xianchao Wu? Takuya Matsuzaki? Jun?ichi Tsujii???
?Department of Computer Science, The University of Tokyo
?School of Computer Science, University of Manchester
?National Centre for Text Mining (NaCTeM)
{wxc, matuzaki, tsujii}@is.s.u-tokyo.ac.jp
Abstract
In the present paper, we propose the ef-
fective usage of function words to generate
generalized translation rules for forest-based
translation. Given aligned forest-string pairs,
we extract composed tree-to-string translation
rules that account for multiple interpretations
of both aligned and unaligned target func-
tion words. In order to constrain the ex-
haustive attachments of function words, we
limit to bind them to the nearby syntactic
chunks yielded by a target dependency parser.
Therefore, the proposed approach can not
only capture source-tree-to-target-chunk cor-
respondences but can also use forest structures
that compactly encode an exponential num-
ber of parse trees to properly generate target
function words during decoding. Extensive
experiments involving large-scale English-to-
Japanese translation revealed a significant im-
provement of 1.8 points in BLEU score, as
compared with a strong forest-to-string base-
line system.
1 Introduction
Rule generalization remains a key challenge for
current syntax-based statistical machine translation
(SMT) systems. On the one hand, there is a ten-
dency to integrate richer syntactic information into
a translation rule in order to better express the trans-
lation phenomena. Thus, flat phrases (Koehn et al,
2003), hierarchical phrases (Chiang, 2005), and syn-
tactic tree fragments (Galley et al, 2006; Mi and
Huang, 2008; Wu et al, 2010) are gradually used in
SMT. On the other hand, the use of syntactic phrases
continues due to the requirement for phrase cover-
age in most syntax-based systems. For example,
Mi et al (2008) achieved a 3.1-point improvement
in BLEU score (Papineni et al, 2002) by including
bilingual syntactic phrases in their forest-based sys-
tem. Compared with flat phrases, syntactic rules are
good at capturing global reordering, which has been
reported to be essential for translating between lan-
guages with substantial structural differences, such
as English and Japanese, which is a subject-object-
verb language (Xu et al, 2009).
Forest-based translation frameworks, which make
use of packed parse forests on the source and/or tar-
get language side(s), are an increasingly promising
approach to syntax-based SMT, being both algorith-
mically appealing (Mi et al, 2008) and empirically
successful (Mi and Huang, 2008; Liu et al, 2009).
However, forest-based translation systems, and, in
general, most linguistically syntax-based SMT sys-
tems (Galley et al, 2004; Galley et al, 2006; Liu
et al, 2006; Zhang et al, 2007; Mi et al, 2008;
Liu et al, 2009; Chiang, 2010), are built upon word
aligned parallel sentences and thus share a critical
dependence on word alignments. For example, even
a single spurious word alignment can invalidate a
large number of otherwise extractable rules, and un-
aligned words can result in an exponentially large
set of extractable rules for the interpretation of these
unaligned words (Galley et al, 2006).
What makes word alignment so fragile? In or-
der to investigate this problem, we manually ana-
lyzed the alignments of the first 100 parallel sen-
tences in our English-Japanese training data (to be
shown in Table 2). The alignments were generated
by running GIZA++ (Och and Ney, 2003) and the
grow-diag-final-and symmetrizing strategy (Koehn
et al, 2007) on the training set. Of the 1,324 word
alignment pairs, there were 309 error pairs, among
22
which there were 237 target function words, which
account for 76.7% of the error pairs1. This indicates
that the alignments of the function words are more
easily to be mistaken than content words. More-
over, we found that most Japanese function words
tend to align to a few English words such as ?of?
and ?the?, which may appear anywhere in an English
sentence. Following these problematic alignments,
we are forced to make use of relatively large English
tree fragments to construct translation rules that tend
to be ill-formed and less generalized.
This is the motivation of the present approach of
re-aligning the target function words to source tree
fragments, so that the influence of incorrect align-
ments is reduced and the function words can be gen-
erated by tree fragments on the fly. However, the
current dominant research only uses 1-best trees for
syntactic realignment (Galley et al, 2006; May and
Knight, 2007; Wang et al, 2010), which adversely
affects the rule set quality due to parsing errors.
Therefore, we realign target function words to a
packed forest that compactly encodes exponentially
many parses. Given aligned forest-string pairs, we
extract composed tree-to-string translation rules that
account for multiple interpretations of both aligned
and unaligned target function words. In order to con-
strain the exhaustive attachments of function words,
we further limit the function words to bind to their
surrounding chunks yielded by a dependency parser.
Using the composed rules of the present study in
a baseline forest-to-string translation system results
in a 1.8-point improvement in the BLEU score for
large-scale English-to-Japanese translation.
2 Backgrounds
2.1 Japanese function words
In the present paper, we limit our discussion
on Japanese particles and auxiliary verbs (Martin,
1975). Particles are suffixes or tokens in Japanese
grammar that immediately follow modified con-
tent words or sentences. There are eight types of
Japanese function words, which are classified de-
pending on what function they serve: case markers,
parallel markers, sentence ending particles, interjec-
1These numbers are language/corpus-dependent and are not
necessarily to be taken as a general reflection of the overall qual-
ity of the word alignments for arbitrary language pairs.
tory particles, adverbial particles, binding particles,
conjunctive particles, and phrasal particles.
Japanese grammar also uses auxiliary verbs to
give further semantic or syntactic information about
the preceding main or full verb. Alike English, the
extra meaning provided by a Japanese auxiliary verb
alters the basic meaning of the main verb so that the
main verb has one or more of the following func-
tions: passive voice, progressive aspect, perfect as-
pect, modality, dummy, or emphasis.
2.2 HPSG forests
Following our precious work (Wu et al, 2010), we
use head-drive phrase structure grammar (HPSG)
forests generated by Enju2 (Miyao and Tsujii, 2008),
which is a state-of-the-art HPSG parser for English.
HPSG (Pollard and Sag, 1994; Sag et al, 2003) is a
lexicalist grammar framework. In HPSG, linguistic
entities such as words and phrases are represented
by a data structure called a sign. A sign gives a
factored representation of the syntactic features of
a word/phrase, as well as a representation of their
semantic content. Phrases and words represented by
signs are collected into larger phrases by the appli-
cations of schemata. The semantic representation of
the new phrase is calculated at the same time. As
such, an HPSG parse forest can be considered to
be a forest of signs. Making use of these signs in-
stead of part-of-speech (POS)/phrasal tags in PCFG
results in a fine-grained rule set integrated with deep
syntactic information.
For example, an aligned HPSG forest3-string pair
is shown in Figure 1. For simplicity, we only draw
the identifiers for the signs of the nodes in the HPSG
forest. Note that the identifiers that start with ?c? de-
note non-terminal nodes (e.g., c0, c1), and the iden-
tifiers that start with ?t? denote terminal nodes (e.g.,
t3, t1). In a complete HPSG forest given in (Wu et
al., 2010), the terminal signs include features such
as the POS tag, the tense, the auxiliary, the voice of
a verb, etc.. The non-terminal signs include features
such as the phrasal category, the name of the schema
2http://www-tsujii.is.s.u-tokyo.ac.jp/enju/index.html
3The forest includes three parse trees rooted at c0, c1, and
c2. In the 1-best tree, ?by? modifies the passive verb ?verified?.
Yet in the 2- and 3-best tree, ?by? modifies ?this result was ver-
ified?. Furthermore, ?verified? is an adjective in the 2-best tree
and a passive verb in the 3-best tree.
23
 
 
jikken niyotte kono kekka ga sa re ta kensyou 
Realign target function words 
?? 0 
 
???? 1 
 
?? 2 
 
?? 3 
 
? 4 ? 6 
 
? 7 
 
? 8 
 
?? 5 
 
this 
 
result 
 
was 
 
verified 
 
by 
 
the 
 
experiments 
 
t3 t1 t4 t8 t10 t7 t0 t6 t5 t2 t9 
c9 c10 c16 c22 c4 c21 c12 c18 c19 c14 c15
c23 
c8 
c13 
c5 c17 
c3 
c6 
c2 
c7 
c11 
c0 
c20 
c1 
1-best tree 2-best tree 3-best tree 
experiments 
 
by 
 
this 
 
result 
 
 
 
verified 
 
c1 
?? 0 
 
???? 1 
 
?? 2 
 
?? 3 
 
? 4 
 
? 6 
 
? 7 
 
? 8 
 
?? 5 
 C1 C2 C3 C4 
this 
 
result 
 
was 
 
verified 
 
by 
 
the 
 
experiments 
 
t3 t1 t4 t8 t10 t7 t0 t6 t5 t2 t9 
c9 c16 c22 c45-7 | 5-8 c125-7 | 5-8  c18 c19 c14 c15
c2 c0 
c215-7 | 5-8 
c23 c8 
c13 
c5 c17 
c3 
c6 
c7 
c11 c20 
c103 | 3-4 
Figure 1: Illustration of an aligned HPSG forest-string pair for English-to-Japanese translation. The chunk-level
dependency tree for the Japanese sentence is shown as well.
applied in the node, etc..
3 Composed Rule Extraction
In this section, we first describe an algorithm that
attaches function words to a packed forest guided
by target chunk information. That is, given a triple
?FS , T, A?, namely an aligned (A) source forest
(FS) to target sentence (T ) pair, we 1) tailor the
alignment A by removing the alignments for tar-
get function words, 2) seek attachable nodes in the
source forest FS for each function word, and 3) con-
struct a derivation forest by topologically travers-
ing FS . Then, we identify minimal and composed
rules from the derivation forest and estimate the
probabilities of rules and scores of derivations us-
ing the expectation-maximization (EM) (Dempster
et al, 1977) algorithm.
3.1 Definitions
In the proposed algorithm, we make use of the fol-
lowing definitions, which are similar to those de-
scribed in (Galley et al, 2004; Mi and Huang, 2008):
? s(?): the span of a (source) node v or a (target)
chunk C, which is an index set of the words that
24
v or C covers;
? t(v): the corresponding span of v, which is an
index set of aligned words on another side;
? c(v): the complement span of v, which is the
union of corresponding spans of nodes v? that
share an identical parse tree with v but are nei-
ther antecedents nor descendants of v;
? PA: the frontier set of FS , which contains
nodes that are consistent with an alignment A
(gray nodes in Figure 1), i.e., t(v) ?= ? and
closure(t(v)) ? c(v) = ?.
The function closure covers the gap(s) that may
appear in the interval parameter. For example,
closure(t(c3)) = closure({0-1, 4-7}) = {0-7}.
Examples of the applications of these functions can
be found in Table 1. Following (Galley et al,
2006), we distinguish between minimal and com-
posed rules. The composed rules are generated by
combining a sequence of minimal rules.
3.2 Free attachment of target function words
3.2.1 Motivation
We explain the motivation for the present research
using an example that was extracted from our train-
ing data, as shown in Figure 1. In the alignment of
this example, three lines (in dot lines) are used to
align was and the with ga (subject particle), and was
with ta (past tense auxiliary verb). Under this align-
ment, we are forced to extract rules with relatively
large tree fragments. For example, by applying the
GHKM algorithm (Galley et al, 2004), a rule rooted
at c0 will take c7, t4, c4, c19, t2, and c15 as the
leaves. The final tree fragment, with a height of 7,
contains 13 nodes. In order to ensure that this rule
is used during decoding, we must generate subtrees
with a height of 7 for c0. Suppose that the input for-
est is binarized and that |E| is the average number
of hyperedges of each node, then we must generate
O(|E|26?1) subtrees4 for c0 in the worst case. Thus,
4For one (binarized) hyperedge e of a node, suppose there
are x subtrees in the left tail node and y subtrees in the right tail
node. Then the number of subtrees guided by e is (x + 1) ?
(y+1). Thus, the recursive formula is Nh = |E|(Nh?1 +1)2,
where h is the height of the hypergraph and Nh is the number
of subtrees. When h = 1, we let Nh = 0.
the existence of these rules prevents the generaliza-
tion ability of the final rule set that is extracted.
In order to address this problem, we tailor the
alignment by ignoring these three alignment pairs in
dot lines. For example, by ignoring the ambiguous
alignments on the Japanese function words, we en-
large the frontier set to include from 12 to 19 of the
24 non-terminal nodes. Consequently, the number
of extractable minimal rules increases from 12 (with
three reordering rules rooted at c0, c1, and c2) to
19 (with five reordering rules rooted at c0, c1, c2,
c5, and c17). With more nodes included in the fron-
tier set, we can extract more minimal and composed
monotonic/reordering rules and avoid extracting the
less generalized rules with extremely large tree frag-
ments.
3.2.2 Why chunking?
In the proposed algorithm, we use a target chunk
set to constrain the attachment explosion problem
because we use a packed parse forest instead of a 1-
best tree, as in the case of (Galley et al, 2006). Mul-
tiple interpretations of unaligned function words for
an aligned tree-string pair result in a derivation for-
est. Now, we have a packed parse forest in which
each tree corresponds to a derivation forest. Thus,
pruning free attachments of function words is prac-
tically important in order to extract composed rules
from this ?(derivation) forest of (parse) forest?.
In the English-to-Japanese translation test case of
the present study, the target chunk set is yielded
by a state-of-the-art Japanese dependency parser,
Cabocha v0.535 (Kudo and Matsumoto, 2002). The
output of Cabocha is a list of chunks. A chunk con-
tains roughly one content word (usually the head)
and affixed function words, such as case markers
(e.g., ga) and verbal morphemes (e.g., sa re ta,
which indicate past tense and passive voice). For
example, the Japanese sentence in Figure 1 is sepa-
rated into four chunks, and the dependencies among
these chunks are identified by arrows. These arrows
point out the head chunk that the current chunk mod-
ifies. Moreover, we also hope to gain a fine-grained
alignment among these syntactic chunks and source
tree fragments. Thereby, during decoding, we are
binding the generation of function words with the
generation of target chunks.
5http://chasen.org/?taku/software/cabocha/
25
Algorithm 1 Aligning function words to the forest
Input: HPSG forest FS , target sentence T , word alignment
A = {(i, j)}, target function word set {fw} appeared in
T , and target chunk set {C}
Output: a derivation forest DF
1: A? ? A \ {(i, s(fw))} ? fw ? {fw}
2: for each node v ? PA? in topological order do
3: Tv ? ? ? store the corresponding spans of v
4: for each function word fw ? {fw} do
5: if fw ? C and t(v)?(C) ?= ? and fw are not attached
to descendants of v then
6: append t(v) ? {s(fw)} to Tv
7: end if
8: end for
9: for each corresponding span t(v) ? Tv do
10: R ? IDENTIFYMINRULES(v, t(v), T ) ? range
over the hyperedges of v, and discount the factional
count of each rule r ? R by 1/|Tv|
11: create a node n in DF for each rule r ? R
12: create a shared parent node ? when |R| > 1
13: end for
14: end for
3.2.3 The algorithm
Algorithm 1 outlines the proposed approach to
constructing a derivation forest to include multiple
interpretations of target function words. The deriva-
tion forest is a hypergraph as previously used in
(Galley et al, 2006), to maintain the constraint that
one unaligned target word be attached to some node
v exactly once in one derivation tree. Starting from
a triple ?FS , T, A?, we first tailor the alignment A
to A? by removing the alignments for target function
words. Then, we traverse the nodes v ? PA? in topo-
logical order. During the traversal, a function word
fw will be attached to v if 1) t(v) overlaps with the
span of the chunk to which fw belongs, and 2) fw
has not been attached to the descendants of v.
We identify translation rules that take v as the root
of their tree fragments. Each tree fragment is a fron-
tier tree that takes a node in the frontier set PA?
of FS as the root node and non-lexicalized frontier
nodes or lexicalized non-frontier nodes as the leaves.
Also, a minimal frontier tree used in a minimal rule
is limited to be a frontier tree such that all nodes
other than the root and leaves are non-frontier nodes.
We use Algorithm 1 described in (Mi and Huang,
2008) to collect minimal frontier trees rooted at v in
FS . That is, we range over each hyperedges headed
at v and continue to expand downward until the cur-
A ? (A?)
node s(?) t(?) c(?) consistent
c0 0-6 0-8(0-3,5-7) ? 1
c1 0-6 0-8(0-3,5-7) ? 1
c2 0-6 0-8(0-3,5-7) ? 1
c3 3-6 0-1,4-7(0-1, 5-7) 2,8 0
c4 3 5-7 0,8(0-3) 1
c5* 4-6 0,4(0-1) 2-8(2-3,5-7) 0(1)
c6* 0-3 2-8(2-3,5-7) 0,4(0-1) 0(1)
c7 0-1 2-3 0-1,4-8(0-1,5-7) 1
c8* 2-3 4-8(5-7) 0-4(0-3) 0(1)
c9 0 2 0-1,3-8(0-1,3,5-7) 1
c10 1 3 0-2,4-8(0-2,5-7) 1
c11 2-6 0-1,4-8(0-1,5-7) 2-3 0
c12 3 5-7 0,8(0-3) 1
c13* 5-6 0,4(0) 1-8(1-3,5-7) 0(1)
c14 5 4(?) 0-8(0-3,5-7) 0
c15 6 0 1-8(1-3,5-7) 1
c16 2 4,8(?) 0-7(0-3,5-7) 0
c17* 4-6 0,4(0-1) 2-8(2-3,5-7) 0(1)
c18 4 1 0,2-8(0,2-3,5-7) 1
c19 4 1 0,2-8(0,2-3,5-7) 1
c20* 0-3 2-8(2-3,5-7) 0,4(0-1) 0(1)
c21 3 5-7 0,8(0-3) 1
c22 2 4,8(?) 0-7(0-3,5-7) 0
c23* 2-3 4-8(5-7) 0-4(0-3) 0(1)
Table 1: Change of node attributes after alignment modi-
fication from A to A? of the example in Figure 1. Nodes
with * superscripts are consistent with A? but not consis-
tent with A.
rent set of hyperedges forms a minimal frontier tree.
In the derivation forest, we use ? nodes to man-
age minimal/composed rules that share the same
node and the same corresponding span. Figure 2
shows some minimal rule and ? nodes derived from
the example in Figure 1.
Even though we bind function words to their
nearby chunks, these function words may still be at-
tached to relative large tree fragments, so that richer
syntactic information can be used to predict the
function words. For example, in Figure 2, the tree
fragments rooted at node c0?80 can predict ga and/or
ta. The syntactic foundation behind is that, whether
to use ga as a subject particle or to use wo as an ob-
ject particle depends on both the left-hand-side noun
phrase (kekka) and the right-hand-side verb (kensyou
sa re ta). This type of node v? (such as c0?80 ) should
satisfy the following two heuristic conditions:
? v? is included in the frontier set PA? of FS , and
? t(v?) covers the function word, or v? is the root
node ofFS if the function word is the beginning
or ending word in the target sentence T .
Starting from this derivation forest with minimal
26
 c103-4 
t13: result 
kekka ga  
 
* c103 
t13: result 
kekka  
 
c92 
t32: the 
kono  
 
c72-3 
c103 c92 
x0 x1 
 x0 
 x1  
c72-4 
c103-4 c92 
x0 x1 
 x0  x1  *
c62-7 
c85-7 c72-3 
x0 ga x1  
 x0 
 x1  
* c62-7 
c85-7 c72-4 
x0 x1  
 x0 
 x1  * 
c00-8 
c16 
c45-7 c50-1 
c3 
c72-4 c11 x2 x0 x1 ta 
 
x0 
 
x1 
 
x2 
 
+ 
* 
c00-8 
c16 
c45-8 c50-1 
c3 
c72-4 c11 x2 x0 x1  
 
x0 
 
x1 
 
x2 
 + 
* 
c00-8 
c16 
c45-7 c50-1 
c3 
c72-3 c11 x2 x0 ga x1 ta 
 x0  
x1 
 
x2 
 
* + c00-8 
c16 
c45-8 c50-1 
c3 
c72-3 c11 x2 x0 ga x1 
 x0  
x1 
 
x2 
 
* 
+ 
t4{}:was t4{}:was t4{}:was t4{}:was 
Figure 2: Illustration of a (partial) derivation forest. Gray nodes include some unaligned target function word(s).
Nodes annotated by ?*? include ga, and nodes annotated by ?+? include ta.
rules as nodes, we can further combine two or more
minimal rules to form composed rules nodes and can
append these nodes to the derivation forest.
3.3 Estimating rule probabilities
We use the EM algorithm to jointly estimate 1)
the translation probabilities and fractional counts of
rules and 2) the scores of derivations in the deriva-
tion forests. As reported in (May and Knight, 2007),
EM, as has been used in (Galley et al, 2006) to es-
timate rule probabilities in derivation forests, is an
iterative procedure and prefers shorter derivations
containing large rules over longer derivations con-
taining small rules. In order to overcome this bias
problem, we discount the fractional count of a rule
by the product of the probabilities of parse hyper-
edges that are included in the tree fragment of the
rule.
4 Experiments
4.1 Setup
We implemented the forest-to-string decoder de-
scribed in (Mi et al, 2008) that makes use of forest-
based translation rules (Mi and Huang, 2008) as
the baseline system for translating English HPSG
forests into Japanese sentences. We analyzed the
performance of the proposed translation rule sets by
Train Dev. Test
# sentence pairs 994K 2K 2K
# En 1-best trees 987,401 1,982 1,984
# En forests 984,731 1,979 1,983
# En words 24.7M 50.3K 49.9K
# Jp words 28.2M 57.4K 57.1K
# Jp function words 8.0M 16.1K 16.1K
Table 2: Statistics of the JST corpus. Here, En = English
and Jp = Japanese.
using the same decoder.
The JST Japanese-English paper abstract corpus6
(Utiyama and Isahara, 2007), which consists of one
million parallel sentences, was used for training,
tuning, and testing. Table 2 shows the statistics of
this corpus. Note that Japanese function words oc-
cupy more than a quarter of the Japanese words.
Making use of Enju 2.3.1, we generated 987,401
1-best trees and 984,731 parse forests for the En-
glish sentences in the training set, with successful
parse rates of 99.3% and 99.1%, respectively. Us-
ing the pruning criteria expressed in (Mi and Huang,
2008), we continue to prune a parse forest by set-
ting pe to be 8, 5, and 2, until there are no more than
e10 = 22, 026 trees in a forest. After pruning, there
are an average of 82.3 trees in a parse forest.
6http://www.jst.go.jp
27
C3-T M&H-F Min-F C3-F
free fw Y N Y Y
alignment A? A A? A?
English side tree forest forest forest
# rule 86.30 96.52 144.91 228.59
# reorder rule 58.50 91.36 92.98 162.71
# tree types 21.62 93.55 72.98 120.08
# nodes/tree 14.2 42.1 26.3 18.6
extract time 30.2 52.2 58.6 130.7
EM time 9.4 - 11.2 29.0
# rules in dev. 0.77 1.22 1.37 2.18
# rules in test 0.77 1.23 1.37 2.15
DT(sec./sent.) 2.8 15.7 22.4 35.4
BLEU (%) 26.15 27.07 27.93 28.89
Table 3: Statistics and translation results for four types of
tree-to-string rules. With the exception of ?# nodes/tree?,
the numbers in the table are in millions and the time is in
hours. Here, fw denotes function word, and DT denotes
the decoding time, and the BLEU scores were computed
on the test set.
We performed GIZA++ (Och and Ney, 2003)
and the grow-diag-final-and symmetrizing strategy
(Koehn et al, 2007) on the training set to obtain
alignments. The SRI Language Modeling Toolkit
(Stolcke, 2002) was employed to train a five-gram
Japanese LM on the training set. We evaluated the
translation quality using the BLEU-4 metric (Pap-
ineni et al, 2002).
Joshua v1.3 (Li et al, 2009), which is a
freely available decoder for hierarchical phrase-
based SMT (Chiang, 2005), is used as an external
baseline system for comparison. We extracted 4.5M
translation rules from the training set for the 4K En-
glish sentences in the development and test sets. We
used the default configuration of Joshua, with the ex-
ception of the maximum number of items/rules, and
the value of k (of the k-best outputs) is set to be 200.
4.2 Results
Table 3 lists the statistics of the following translation
rule sets:
? C3-T: a composed rule set extracted from the
derivation forests of 1-best HPSG trees that
were constructed using the approach described
in (Galley et al, 2006). The maximum number
of internal nodes is set to be three when gen-
erating a composed rule. We free attach target
function words to derivation forests;
0
5
10
15
20
25
2 12 22 32 42 52 62 72 82 92
# o
f ru
les 
(M
)
# of tree nodes in rule
M&H-F
Min-F
C3-T
C3-F
Figure 3: Distributions of the number of tree nodes in the
translation rule sets. Note that the curves of Min-F and
C3-F are duplicated when the number of tree nodes being
larger than 9.
? M&H-F: a minimal rule set extracted from
HPSG forests using the extracting algorithm of
(Mi and Huang, 2008). Here, we make use of
the original alignments. We use the two heuris-
tic conditions described in Section 3.2.3 to at-
tach unaligned words to some node(s) in the
forest;
? Min-F: a minimal rule set extracted from the
derivation forests of HPSG forests that were
constructed using Algorithm 1 (Section 3).
? C3-F: a composed rule set extracted from the
derivation forests of HPSG forests. Similar to
C3-T, the maximum number of internal nodes
during combination is three.
We investigate the generalization ability of these
rule sets through the following aspects:
1. the number of rules, the number of reordering
rules, and the distributions of the number of
tree nodes (Figure 3), i.e., more rules with rel-
atively small tree fragments are preferred;
2. the number of rules that are applicable to the
development and test sets (Table 3); and
3. the final translation accuracies.
Table 3 and Figure 3 reflect that the generalization
abilities of these four rule sets increase in the or-
der of C3-T < M&H-F < Min-F < C3-F. The ad-
vantage of using a packed forest for re-alignment is
verified by comparing the statistics of the rules and
28
0
10
20
30
40
50
0.0
0.5
1.0
1.5
2.0
2.5
C3-T M&H-F Min-F C3-F
Dec
odi
ng t
ime
 (se
c./s
ent
.)
# o
f ru
les 
(M)
# rules (M)
DT
Figure 4: Comparison of decoding time and the number
of rules used for translating the test set.
the final BLEU scores of C3-T with Min-F and C3-
F. Using the composed rule set C3-F in our forest-
based decoder, we achieved an optimal BLEU score
of 28.89 (%). Taking M&H-F as the baseline trans-
lation rule set, we achieved a significant improve-
ment (p < 0.01) of 1.81 points.
In terms of decoding time, even though we used
Algorithm 3 described in (Huang and Chiang, 2005),
which lazily generated the N-best translation can-
didates, the decoding time tended to be increased
because more rules were available during cube-
pruning. Figure 4 shows a comparison of decoding
time (seconds per sentence) and the number of rules
used for translating the test set. Easy to observe that,
decoding time increases in a nearly linear way fol-
lowing the increase of the number of rules used dur-
ing decoding.
Finally, compared with Joshua, which achieved
a BLEU score of 24.79 (%) on the test set with
a decoding speed of 8.8 seconds per sentence, our
forest-based decoder achieved a significantly better
(p < 0.01) BLEU score by using either of the four
types of translation rules.
5 Related Research
Galley et al (2006) first used derivation forests of
aligned tree-string pairs to express multiple inter-
pretations of unaligned target words. The EM al-
gorithm was used to jointly estimate 1) the trans-
lation probabilities and fractional counts of rules
and 2) the scores of derivations in the derivation
forests. By dealing with the ambiguous word align-
ment instead of unaligned target words, syntax-
based re-alignment models were proposed by (May
and Knight, 2007; Wang et al, 2010) for tree-based
translations.
Free attachment of the unaligned target word
problem was ignored in (Mi and Huang, 2008),
which was the first study on extracting tree-to-string
rules from aligned forest-string pairs. This inspired
the idea to re-align a packed forest and a target sen-
tence. Specially, we observed that most incorrect or
ambiguous word alignments are caused by function
words rather than content words. Thus, we focus on
the realignment of target function words to source
tree fragments and use a dependency parser to limit
the attachments of unaligned target words.
6 Conclusion
We have proposed an effective use of target function
words for extracting generalized transducer rules for
forest-based translation. We extend the unaligned
word approach described in (Galley et al, 2006)
from the 1-best tree to the packed parse forest. A
simple yet effective modification is that, during rule
extraction, we account for multiple interpretations
of both aligned and unaligned target function words.
That is, we chose to loose the ambiguous alignments
for all of the target function words. The consider-
ation behind is in order to generate target function
words in a robust manner. In order to avoid gener-
ating too large a derivation forest for a packed for-
est, we further used chunk-level information yielded
by a target dependency parser. Extensive experi-
ments on large-scale English-to-Japanese translation
resulted in a significant improvement in BLEU score
of 1.8 points (p < 0.01), as compared with our
implementation of a strong forest-to-string baseline
system (Mi et al, 2008; Mi and Huang, 2008).
The present work only re-aligns target function
words to source tree fragments. It will be valuable
to investigate the feasibility to re-align all the tar-
get words to source tree fragments. Also, it is in-
teresting to automatically learn a word set for re-
aligning7. Given source parse forests and a target
word set for re-aligning beforehand, we argue our
approach is generic and applicable to any language
pairs. Finally, we intend to extend the proposed
approach to tree-to-tree translation frameworks by
7This idea comes from one reviewer, we express our thank-
fulness here.
29
re-aligning subtree pairs (Liu et al, 2009; Chiang,
2010) and consistency-to-dependency frameworks
by re-aligning consistency-tree-to-dependency-tree
pairs (Mi and Liu, 2010) in order to tackle the rule-
sparseness problem.
Acknowledgments
The present study was supported in part by a Grant-
in-Aid for Specially Promoted Research (MEXT,
Japan), by the Japanese/Chinese Machine Transla-
tion Project through Special Coordination Funds for
Promoting Science and Technology (MEXT, Japan),
and by Microsoft Research Asia Machine Transla-
tion Theme.
Wu (wu.xianchao@lab.ntt.co.jp) has
moved to NTT Communication Science Laborato-
ries and Tsujii (junichi.tsujii@live.com)
has moved to Microsoft Research Asia.
References
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
ACL, pages 263?270, Ann Arbor, MI.
David Chiang. 2010. Learning to translate with source
and target syntax. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 1443?1452, Uppsala, Sweden, July. Asso-
ciation for Computational Linguistics.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the em
algorithm. Journal of the Royal Statistical Society,
39:1?38.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In Pro-
ceedings of HLT-NAACL.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proceed-
ings of COLING-ACL, pages 961?968, Sydney.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proceedings of IWPT.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of the Human Language Technology and North
American Association for Computational Linguistics
Conference (HLT/NAACL), Edomonton, Canada, May
27-June 1.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondr?ej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of the ACL 2007 Demo and Poster Sessions, pages
177?180.
Taku Kudo and Yuji Matsumoto. 2002. Japanese depen-
dency analysis using cascaded chunking. In Proceed-
ings of CoNLL-2002, pages 63?69. Taipei, Taiwan.
Zhifei Li, Chris Callison-Burch, Chris Dyery, Juri Gan-
itkevitch, Sanjeev Khudanpur, Lane Schwartz, Wren
N. G. Thornton, Jonathan Weese, and Omar F. Zaidan.
2009. Demonstration of joshua: An open source
toolkit for parsing-based machine translation. In Pro-
ceedings of the ACL-IJCNLP 2009 Software Demon-
strations, pages 25?28, August.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment templates for statistical machine
transaltion. In Proceedings of COLING-ACL, pages
609?616, Sydney, Australia.
Yang Liu, Yajuan Lu?, and Qun Liu. 2009. Improving
tree-to-tree translation with packed forests. In Pro-
ceedings of ACL-IJCNLP, pages 558?566, August.
Samuel E. Martin. 1975. A Reference Grammar of
Japanese. New Haven, Conn.: Yale University Press.
Jonathan May and Kevin Knight. 2007. Syntactic re-
alignment models for machine translation. In Pro-
ceedings of the 2007 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-
CoNLL), pages 360?368, Prague, Czech Republic,
June. Association for Computational Linguistics.
Haitao Mi and Liang Huang. 2008. Forest-based transla-
tion rule extraction. In Proceedings of EMNLP, pages
206?214, October.
Haitao Mi and Qun Liu. 2010. Constituency to depen-
dency translation with forests. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 1433?1442, Uppsala, Swe-
den, July. Association for Computational Linguistics.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proceedings of ACL-08:HLT,
pages 192?199, Columbus, Ohio.
Yusuke Miyao and Jun?ichi Tsujii. 2008. Feature forest
models for probabilistic hpsg parsing. Computational
Lingustics, 34(1):35?80.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalu-
ation of machine translation. In Proceedings of ACL,
pages 311?318.
30
Carl Pollard and Ivan A. Sag. 1994. Head-Driven Phrase
Structure Grammar. University of Chicago Press.
Ivan A. Sag, Thomas Wasow, and Emily M. Bender.
2003. Syntactic Theory: A Formal Introduction.
Number 152 in CSLI Lecture Notes. CSLI Publica-
tions.
Andreas Stolcke. 2002. Srilm-an extensible language
modeling toolkit. In Proceedings of International
Conference on Spoken Language Processing, pages
901?904.
Masao Utiyama and Hitoshi Isahara. 2007. A japanese-
english patent parallel corpus. In Proceedings of MT
Summit XI, pages 475?482, Copenhagen.
Wei Wang, Jonathan May, Kevin Knight, and Daniel
Marcu. 2010. Re-structuring, re-labeling, and re-
aligning for syntax-based machine translation. Com-
putational Linguistics, 36(2):247?277.
Xianchao Wu, Takuya Matsuzaki, and Jun?ichi Tsujii.
2010. Fine-grained tree-to-string translation rule ex-
traction. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguistics,
pages 325?334, Uppsala, Sweden, July. Association
for Computational Linguistics.
Peng Xu, Jaeho Kang, Michael Ringgaard, and Franz
Och. 2009. Using a dependency parser to improve
smt for subject-object-verb languages. In Proceedings
of HLT-NAACL, pages 245?253.
Min Zhang, Hongfei Jiang, Ai Ti Aw, Jun Sun, Sheng Li,
and Chew Lim Tan. 2007. A tree-to-tree alignment-
based model for statistical machine translation. In
Proceedings of MT Summit XI, pages 535?542, Copen-
hagen, Denmark, September.
31
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 1?10,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Learning to Translate with Multiple Objectives
Kevin Duh? Katsuhito Sudoh Xianchao Wu Hajime Tsukada Masaaki Nagata
NTT Communication Science Laboratories
2-4 Hikari-dai, Seika-cho, Kyoto 619-0237, JAPAN
kevinduh@is.naist.jp, lastname.firstname@lab.ntt.co.jp
Abstract
We introduce an approach to optimize a ma-
chine translation (MT) system on multiple
metrics simultaneously. Different metrics
(e.g. BLEU, TER) focus on different aspects
of translation quality; our multi-objective ap-
proach leverages these diverse aspects to im-
prove overall quality.
Our approach is based on the theory of Pareto
Optimality. It is simple to implement on top of
existing single-objective optimization meth-
ods (e.g. MERT, PRO) and outperforms ad
hoc alternatives based on linear-combination
of metrics. We also discuss the issue of metric
tunability and show that our Pareto approach
is more effective in incorporating new metrics
from MT evaluation for MT optimization.
1 Introduction
Weight optimization is an important step in build-
ing machine translation (MT) systems. Discrimi-
native optimization methods such as MERT (Och,
2003), MIRA (Crammer et al, 2006), PRO (Hop-
kins and May, 2011), and Downhill-Simplex (Nelder
and Mead, 1965) have been influential in improving
MT systems in recent years. These methods are ef-
fective because they tune the system to maximize an
automatic evaluation metric such as BLEU, which
serve as surrogate objective for translation quality.
However, we know that a single metric such as
BLEU is not enough. Ideally, we want to tune to-
wards an automatic metric that has perfect corre-
lation with human judgments of translation quality.
?*Now at Nara Institute of Science & Technology (NAIST)
While many alternatives have been proposed, such a
perfect evaluation metric remains elusive.
As a result, many MT evaluation campaigns now
report multiple evaluation metrics (Callison-Burch
et al, 2011; Paul, 2010). Different evaluation met-
rics focus on different aspects of translation quality.
For example, while BLEU (Papineni et al, 2002)
focuses on word-based n-gram precision, METEOR
(Lavie and Agarwal, 2007) allows for stem/synonym
matching and incorporates recall. TER (Snover
et al, 2006) allows arbitrary chunk movements,
while permutation metrics like RIBES (Isozaki et
al., 2010; Birch et al, 2010) measure deviation in
word order. Syntax (Owczarzak et al, 2007) and se-
mantics (Pado et al, 2009) also help. Arguably, all
these metrics correspond to our intuitions on what is
a good translation.
The current approach of optimizing MT towards
a single metric runs the risk of sacrificing other met-
rics. Can we really claim that a system is good if
it has high BLEU, but very low METEOR? Simi-
larly, is a high-METEOR low-BLEU system desir-
able? Our goal is to propose a multi-objective op-
timization method that avoids ?overfitting to a sin-
gle metric?. We want to build a MT system that
does well with respect to many aspects of transla-
tion quality.
In general, we cannot expect to improve multi-
ple metrics jointly if there are some inherent trade-
offs. We therefore need to define the notion of Pareto
Optimality (Pareto, 1906), which characterizes this
tradeoff in a rigorous way and distinguishes the set
of equally good solutions. We will describe Pareto
Optimality in detail later, but roughly speaking, a
1
hypothesis is pareto-optimal if there exist no other
hypothesis better in all metrics. The contribution of
this paper is two-fold:
? We introduce PMO (Pareto-based Multi-
objective Optimization), a general approach for
learning with multiple metrics. Existing single-
objective methods can be easily extended to
multi-objective using PMO.
? We show that PMO outperforms the alterna-
tive (single-objective optimization of linearly-
combined metrics) in multi-objective space,
and especially obtains stronger results for met-
rics that may be difficult to tune individually.
In the following, we first explain the theory of
Pareto Optimality (Section 2), and then use it to
build up our proposed PMO approach (Section 3).
Experiments on NIST Chinese-English and PubMed
English-Japanese translation using BLEU, TER, and
RIBES are presented in Section 4. We conclude by
discussing related work (Section 5) and opportuni-
ties/limitations (Section 6).
2 Theory of Pareto Optimality
2.1 Definitions and Concepts
The idea of Pareto optimality comes originally from
economics (Pareto, 1906), where the goal is to char-
acterize situations when a change in allocation of
goods does not make anybody worse off. Here, we
will explain it in terms of MT:
Let h ? L be a hypothesis from an N-best list L.
We have a total of K different metrics Mk(h) for
evaluating the quality of h. Without loss of gen-
erality, we assume metric scores are bounded be-
tween 0 and 1, with 1 being perfect. Each hypoth-
esis h can be mapped to a K-dimensional vector
M(h) = [M1(h);M2(h); ...;MK(h)]. For exam-
ple, suppose K = 2, M1(h) computes the BLEU
score, and M2(h) gives the METEOR score of h.
Figure 1 illustrates the set of vectors {M(h)} in a
10-best list.
For two hypotheses h1, h2, we write M(h1) >
M(h2) if h1 is better than h2 in all metrics, and
M(h1) ? M(h2) if h1 is better than or equal
to h2 in all metrics. When M(h1) ? M(h2) and
Mk(h1) > Mk(h2) for at least one metric k, we say
that h1 dominates h2 and write M(h1) . M(h2).
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 10
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
metric1
me
tric2
Figure 1: Illustration of Pareto Frontier. Ten hypotheses
are plotted by their scores in two metrics. Hypotheses
indicated by a circle (o) are pareto-optimal, while those
indicated by a plus (+) are not. The line shows the convex
hull, which attains only a subset of pareto-optimal points.
The triangle (4) is a point that is weakly pareto-optimal
but not pareto-optimal.
Definition 1. Pareto Optimal: A hypothesis h? ?
L is pareto-optimal iff there does not exist another
hypothesis h ? L such that M(h) . M(h?).
In Figure 1, the hypotheses indicated by circle
(o) are pareto-optimal, while those with plus (+) are
not. To visualize this, take for instance the pareto-
optimal point (0.4,0.7). There is no other point with
either (metric1 > 0.4 and metric2 ? 0.7), or (met-
ric1 ? 0.4 and metric2 > 0.7). On the other hand,
the non-pareto point (0.6,0.4) is ?dominated? by an-
other point (0.7,0.6), because for metric1: 0.7 > 0.6
and for metric2: 0.6 > 0.4.
There is another definition of optimality, which
disregards ties and may be easier to visualize:
Definition 2. Weakly Pareto Optimal: A hypothesis
h? ? L is weakly pareto-optimal iff there is no other
hypothesis h ? L such that M(h) > M(h?).
Weakly pareto-optimal points are a superset of
pareto-optimal points. A hypothesis is weakly
pareto-optimal if there is no other hypothesis that
improves all the metrics; a hypothesis is pareto-
optimal if there is no other hypothesis that improves
at least one metric without detriment to other met-
rics. In Figure 1, point (0.1,0.8) is weakly pareto-
optimal but not pareto-optimal, because of the com-
peting point (0.3,0.8). Here we focus on pareto-
optimality, but note our algorithms can be easily
2
modified for weakly pareto-optimality. Finally, we
can introduce the key concept used in our proposed
PMO approach:
Definition 3. Pareto Frontier: Given an N-best list
L, the set of all pareto-optimal hypotheses h ? L is
called the Pareto Frontier.
The Pareto Frontier has two desirable properties
from the multi-objective optimization perspective:
1. Hypotheses on the Frontier are equivalently
good in the Pareto sense.
2. For each hypothesis not on the Frontier, there
is always a better (pareto-optimal) hypothesis.
This provides a principled approach to optimiza-
tion: i.e. optimizing towards points on the Frontier
and away from those that are not, and giving no pref-
erence to different pareto-optimal hypotheses.
2.2 Reduction to Linear Combination
Multi-objective problems can be formulated as:
arg max
w
[M1(h);M2(h); . . . ;Mk(h)] (1)
where h = Decode(w, f)
Here, the MT system?s Decode function, parame-
terized by weight vector w, takes in a foreign sen-
tence f and returns a translated hypothesis h. The
argmax operates in vector space and our goal is to
find w leading to hypotheses on the Pareto Frontier.
In the study of Pareto Optimality, one central
question is: To what extent can multi-objective prob-
lems be solved by single-objective methods? Equa-
tion 1 can be reduced to a single-objective problem
by scalarizing the vector [M1(h); . . . ;Mk(h)] with
a linear combination:
arg max
w
K?
k=1
pkMk(h) (2)
where h = Decode(w, f)
Here, pk are positive real numbers indicating the rel-
ative importance of each metric (without loss of gen-
erality, assume
?
k pk = 1). Are the solutions to
Eq. 2 also solutions to Eq. 1 (i.e. pareto-optimal)
and vice-versa? The theory says:
Theorem 1. Sufficient Condition: If w? is solution
to Eq. 2, then it is weakly pareto-optimal. Further,
if w? is unique, then it is pareto-optimal.
Theorem 2. No Necessary Condition: There may
exist solutions to Eq. 1 that cannot be achieved by
Eq. 2, irregardless of any setting of {pk}.
Theorem 1 is a positive result asserting that lin-
ear combination can give pareto-optimal solutions.
However, Theorem 2 states the limits: in partic-
ular, Eq. 2 attains only pareto-optimal points that
are on the convex hull. This is illustrated in Fig-
ure 1: imagine sweeping all values of p1 = [0, 1]
and p2 = 1? p1 and recording the set of hypotheses
that maximizes
?
k pkMk(h). For 0.6 < p1 ? 1 we
get h = (0.9, 0.1), for p1 = 0.6 we get (0.7, 0.6),
and for 0 < p1 < 0.6 we get (0.4, 0.8). At no
setting of p1 do we attain h = (0.4, 0.7) which
is also pareto-optimal but not on the convex hull.1
This may have ramifications for issues like metric
tunability and local optima. To summarize, linear-
combination is reasonable but has limitations. Our
proposed approach will instead directly solve Eq. 1.
Pareto Optimality and multi-objective optimiza-
tion is a deep field with active inquiry in engineer-
ing, operations research, economics, etc. For the in-
terested reader, we recommend the survey by Mar-
ler and Arora (2004) and books by (Sawaragi et al,
1985; Miettinen, 1998).
3 Multi-objective Algorithms
3.1 Computing the Pareto Frontier
Our PMO approach will need to compute the Pareto
Frontier for potentially large sets of points, so we
first describe how this can be done efficiently. Given
a set of N vectors {M(h)} from an N-best list L,
our goal is extract the subset that are pareto-optimal.
Here we present an algorithm based on iterative
filtering, in our opinion the simplest algorithm to
understand and implement. The strategy is to loop
through the list L, keeping track of any dominant
points. Given a dominant point, it is easy to filter
out many points that are dominated by it. After suc-
cessive rounds, any remaining points that are not fil-
1We note that scalarization by exponentiated-combination
?
k pkMk(h)
q , for a suitable q > 0, does satisfy necessary
conditions for pareto optimality. However the proper tuning of q
is not known a priori. See (Miettinen, 1998) for theorem proofs.
3
Algorithm 1 FindParetoFrontier
Input: {M(h)}, h ? L
Output: All pareto-optimal points of {M(h)}
1: F = ?
2: while L is not empty do
3: h? = shift(L)
4: for each h in L do
5: if (M(h?) . M(h)): remove h from L
6: else if (M(h) . M(h?)): remove h from L; set
h? = h
7: end for
8: Add h? to Frontier Set F
9: for each h in L do
10: if (M(h?) . M(h)): remove h from L
11: end for
12: end while
13: Return F
tered are necessarily pareto-optimal. Algorithm 1
shows the pseudocode. In line 3, we take a point h?
and check if it is dominating or dominated in the for-
loop (lines 4-8). At least one pareto-optimal point
will be found by line 8. The second loop (lines 9-11)
further filters the list for points that are dominated by
h? but iterated before h? in the first for-loop.
The outer while-loop stops exactly after P iter-
ations, where P is the actual number of pareto-
optimal points in L. Each inner loop costs O(KN)
so the total complexity is O(PKN). Since P ? N
with the actual value depending on the probability
distribution of {M(h)}, the worst-case run-time is
O(KN2). For a survey of various Pareto algorithms,
refer to (Godfrey et al, 2007). The algorithm we de-
scribed here is borrowed from the database literature
in what is known as skyline operators.2
3.2 PMO-PRO Algorithm
We are now ready to present an algorithm for multi-
objective optimization. As we will see, it can be seen
as a generalization of the pairwise ranking optimiza-
tion (PRO) of (Hopkins and May, 2011), so we call
it PMO-PRO. PMO-PRO approach works by itera-
tively decoding-and-optimizing on the devset, sim-
2The inquisitive reader may wonder how is Pareto related
to databases. The motivation is to incorporate preferences into
relational queries(Bo?rzso?nyi et al, 2001). For K = 2 metrics,
they also present an alternative faster O(N logN) algorithm by
first topologically sorting along the 2 dimensions. All domi-
nated points can be filtered by one-pass by comparing with the
most-recent dominating point.
ilar to many MT optimization methods. The main
difference is that rather than trying to maximize a
single metric, we maximize the number of pareto
points, in order to expand the Pareto Frontier
We will explain PMO-PRO in terms of the
pseudo-code shown in Algorithm 2. For each sen-
tence pair (f, e) in the devset, we first generate an
N-best list L ? {h} using the current weight vector
w (line 5). In line 6, we evaluate each hypothesis
h with respect to the K metrics, giving a set of K-
dimensional vectors {M(h)}.
Lines 7-8 is the critical part: it gives a ?la-
bel? to each hypothesis, based on whether it is
in the Pareto Frontier. In particular, first we call
FindParetoFrontier (Algorithm 1), which re-
turns a set of pareto hypotheses; pareto-optimal hy-
potheses will get label 1 while non-optimal hypothe-
ses will get label 0. This information is added to
the training set T (line 8), which is then optimized
by any conventional subroutine in line 10. We will
follow PRO in using a pairwise classifier in line 10,
which finds w? that separates hypotheses with labels
1 vs. 0. In essence, this is the trick we employ to
directly optimize on the Pareto Frontier. If we had
used BLEU scores rather than the {0, 1} labels in
line 8, the entire PMO-PRO algorithm would revert
to single-objective PRO.
By definition, there is no single ?best? result
for multi-objective optimization, so we collect all
weights and return the Pareto-optimal set. In line 13
we evaluate each weight w on K metrics across the
entire corpus and call FindParetoFrontier
in line 14.3 This choice highlights an interesting
change of philosophy: While setting {pk} in linear-
combination forces the designer to make an a priori
preference among metrics prior to optimization, the
PMO strategy is to optimize first agnostically and
a posteriori let the designer choose among a set of
weights. Arguably it is easier to choose among so-
lutions based on their evaluation scores rather than
devising exact values for {pk}.
3.3 Discussion
Variants: In practice we find that a slight modifi-
cation of line 8 in Algorithm 2 leads to more sta-
3Note this is the same FindParetoFrontier algorithm as used
in line 7. Both operate on sets of points in K-dimensional
space, induced from either weights {w} or hypotheses {h}.
4
Algorithm 2 Proposed PMO-PRO algorithm
Input: Devset, max number of iterations I
Output: A set of (pareto-optimal) weight vectors
1: Initialize w. LetW = ?.
2: for i = 1 to I do
3: Let T = ?.
4: for each (f, e) in devset do
5: {h} =DecodeNbest(w,f )
6: {M(h)}=EvalMetricsOnSentence({h}, e)
7: {f} =FindParetoFrontier({M(h)})
8: foreach h ? {h}:
if h ? {f}, set l=1, else l=0; Add (l, h) to T
9: end for
10: w?=OptimizationSubroutine(T , w)
11: Add w? toW; Set w = w?.
12: end for
13: M(w) =EvalMetricsOnCorpus(w,devset) ?w ? W
14: Return FindParetoFrontier({M(w)})
ble results for PMO-PRO: for non-pareto hypothe-
ses h /? {f}, we set label l =
?
kMk(h)/K in-
stead of l= 0, so the method not only learns to dis-
criminate pareto vs. non-pareto but also also learns
to discriminate among competing non-pareto points.
Also, like other MT works, in line 5 the N-best list is
concatenated to N-best lists from previous iterations,
so {h} is a set with i ?N elements.
General PMO Approach: The strategy we out-
lined in Section 3.2 can be easily applied to other
MT optimization techniques. For example, by re-
placing the optimization subroutine (line 10, Algo-
rithm 2) with a Powell search (Och, 2003), one can
get PMO-MERT4. Alternatively, by using the large-
margin optimizer in (Chiang et al, 2009) and mov-
ing it into the for-each loop (lines 4-9), one can
get an online algorithm such PMO-MIRA. Virtually
all MT optimization algorithms have a place where
metric scores feedback into the optimization proce-
dure; the idea of PMO is to replace these raw scores
with labels derived from Pareto optimality.
4 Experiments
4.1 Evaluation Methodology
We experiment with two datasets: (1) The PubMed
task is English-to-Japanese translation of scientific
4A difference with traditional MERT is the necessity of
sentence-BLEU (Liang et al, 2006) in line 6. We use sentence-
BLEU for optimization but corpus-BLEU for evaluation here.
abstracts. As metrics we use BLEU and RIBES
(which demonstrated good human correlation in
this language pair (Goto et al, 2011)). (2) The
NIST task is Chinese-to-English translation with
OpenMT08 training data and MT06 as devset. As
metrics we use BLEU and NTER.
? BLEU = BP ? (?precn)1/4. BP is brevity
penality. precn is precision of n-gram matches.
? RIBES = (? + 1)/2 ? prec1/41 , with Kendall?s
? computed by measuring permutation between
matching words in reference and hypothesis5.
? NTER=max(1?TER, 0), which normalizes
Translation Edit Rate6 so that NTER=1 is best.
We compare two multi-objective approaches:
1. Linear-Combination of metrics (Eq. 2),
optimized with PRO. We search a range
of combination settings: (p1, p2) =
{(0, 1), (0.3, 0.7), (0.5, 0.5), (0.7, 0.3), (1, 0)}.
Note (1, 0) reduces to standard single-metric
optimization of e.g. BLEU.
2. Proposed Pareto approach (PMO-PRO).
Evaluation of multi-objective problems can be
tricky because there is no single figure-of-merit.
We thus adopted the following methodology: We
run both methods 5 times (i.e. using the 5 differ-
ent (p1, p2) setting each time) and I = 20 iterations
each. For each method, this generates 5x20=100 re-
sults, and we plot the Pareto Frontier of these points
in a 2-dimensional metric space (e.g. see Figure 2).
A method is deemed better if its final Pareto Fron-
tier curve is strictly dominating the other. We report
devset results here; testset trends are similar but not
included due to space constraints.7
5from www.kecl.ntt.co.jp/icl/lirg/ribes
6from www.umd.edu/?snover/tercom
7An aside: For comparing optimization methods, we believe
devset comparison is preferable to testset since data mismatch
may confound results. If one worries about generalization, we
advocate to re-decode the devset with final weights and evaluate
its 1-best output (which is done here). This is preferable to sim-
ply reporting the achieved scores on devset N-best (as done in
some open-source scripts) since the learned weight may pick
out good hypotheses in the N-best but perform poorly when
re-decoding the same devset. The re-decode devset approach
avoids being overly optimistic while accurately measuring op-
timization performance.
5
Train Devset #Feat Metrics
PubMed 0.2M 2k 14 BLEU, RIBES
NIST 7M 1.6k 8 BLEU, NTER
Table 1: Task characteristics: #sentences in Train/Dev, #
of features, and metrics used. Our MT models are trained
with standard phrase-based Moses software (Koehn and
others, 2007), with IBM M4 alignments, 4gram SRILM,
lexical ordering for PubMed and distance ordering for the
NIST system. The decoder generates 50-best lists each
iteration. We use SVMRank (Joachims, 2006) as opti-
mization subroutine for PRO, which efficiently handle all
pairwise samples without the need for sampling.
4.2 Results
Figures 2 and 3 show the results for PubMed and
NIST, respectively. A method is better if its Pareto
Frontier lies more towards the upper-right hand cor-
ner of the graph. Our observations are:
1. PMO-PRO generally outperforms Linear-
Combination with any setting of (p1, p2).
The Pareto Frontier of PMO-PRO dominates
that of Linear-Combination. This implies
PMO is effective in optimizing towards Pareto
hypotheses.
2. For both methods, trading-off between met-
rics is necessary. For example in PubMed,
the designer would need to make a choice be-
tween picking the best weight according to
BLEU (BLEU=.265,RIBES=.665) vs. another
weight with higher RIBES but poorer BLEU,
e.g. (.255,.675). Nevertheless, both the PMO
and Linear-Combination with various (p1, p2)
samples this joint-objective space broadly.
3. Interestingly, a multi-objective approach can
sometimes outperform a single-objective opti-
mizer in its own metric. In Figure 2, single-
objective PRO focusing on optimizing RIBES
only achieves 0.68, but PMO-PRO using both
BLEU and RIBES outperforms with 0.685.
The third observation relates to the issue of metric
tunability (Liu et al, 2011). We found that RIBES
can be difficult to tune directly. It is an extremely
non-smooth objective with many local optima?slight
changes in word ordering causes large changes in
RIBES. So the best way to improve RIBES is to
0.2 0.21 0.22 0.23 0.24 0.25 0.26 0.270.665
0.67
0.675
0.68
0.685
0.69
0.695
bleu
ribe
s
 
 Linear CombinationPareto (PMO?PRO)
Figure 2: PubMed Results. The curve represents the
Pareto Frontier of all results collected after multiple runs.
0.146 0.148 0.15 0.152 0.154 0.156 0.158 0.16 0.162 0.1640.694
0.695
0.696
0.697
0.698
0.699
0.7
0.701
0.702
0.703
0.704
bleu
nte
r
 
 Linear CombinationPareto (PMO?PRO)
Figure 3: NIST Results
not to optimize it directly, but jointly with a more
tunable metric BLEU. The learning curve in Fig-
ure 4 show that single-objective optimization of
RIBES quickly falls into local optimum (at iteration
3) whereas PMO can zigzag and sacrifice RIBES in
intermediate iterations (e.g. iteration 2, 15) leading
to a stronger result ultimately. The reason is the
diversity of solutions provided by the Pareto Fron-
tier. This finding suggests that multi-objective ap-
proaches may be preferred, especially when dealing
with new metrics that may be difficult to tune.
4.3 Additional Analysis and Discussions
What is the training time? The Pareto approach
does not add much overhead to PMO-PRO. While
FindParetoFrontier scales quadratically by size of
N-best list, Figure 5 shows that the runtime is triv-
6
0 2 4 6 8 10 12 14 16 18 200.63
0.64
0.65
0.66
0.67
0.68
0.69
iteration
ribe
s
 
 
Single?Objective RIBES
Pareto (PMO?PRO)
Figure 4: Learning Curve on RIBES: comparing single-
objective optimization and PMO.
0 100 200 300 400 500 600 700 800 900 10000
0.05
0.1
0.15
0.2
0.25
0.3
0.35
Set size |L|
Run
time
 (sec
onds
)
 
 
Algorithm 1
TopologicalSort (footnote 2)
Figure 5: Avg. runtime per sentence of FindPareto
ial (0.3 seconds for 1000-best). Table 2 shows
the time usage breakdown in different iterations for
PubMed. We see it is mostly dominated by decod-
ing time (constant per iteration at 40 minutes on
single 3.33GHz processor). At later iterations, Opt
takes more time due to larger file I/O in SVMRank.
Note Decode and Pareto can be ?embarrasingly par-
allelized.?
Iter Time Decode Pareto Opt Misc.
(line 5) (line 7) (line 10) (line 6,8)
1 47m 85% 1% 1% 13%
10 62m 67% 6% 8% 19%
20 91m 47% 15% 22% 16%
Table 2: Training time usage in PMO-PRO (Algo 2).
How many Pareto points? The number of pareto
0 2 4 6 8 10 12 14 16 185
10
15
20
25
30
35
Iterations
Num
ber 
of P
aret
o P
oint
s
 
 
NIST
PubMed
Figure 6: Average number of Pareto points
hypotheses gives a rough indication of the diversity
of hypotheses that can be exploited by PMO. Fig-
ure 6 shows that this number increases gradually per
iteration. This perhaps gives PMO-PRO more direc-
tions for optimizing around potential local optimal.
Nevertheless, we note that tens of Pareto points is far
few compared to the large size of N-best lists used
at later iterations of PMO-PRO. This may explain
why the differences between methods in Figure 3
are not more substantial. Theoretically, the num-
ber will eventually level off as it gets increasingly
harder to generate new Pareto points in a crowded
space (Bentley et al, 1978).
Practical recommendation: We present the
Pareto approach as a way to agnostically optimize
multiple metrics jointly. However, in practice, one
may have intuitions about metric tradeoffs even if
one cannot specify {pk}. For example, we might
believe that approximately 1-point BLEU degra-
dation is acceptable only if RIBES improves by
at least 3-points. In this case, we recommend
the following trick: Set up a multi-objective prob-
lem where one metric is BLEU and the other is
3/4BLEU+1/4RIBES. This encourages PMO to ex-
plore the joint metric space but avoid solutions that
sacrifice too much BLEU, and should also outper-
form Linear Combination that searches only on the
(3/4,1/4) direction.
5 Related Work
Multi-objective optimization for MT is a relatively
new area. Linear-combination of BLEU/TER is
7
the most common technique (Zaidan, 2009), some-
times achieving good results in evaluation cam-
paigns (Dyer et al, 2009). As far as we known, the
only work that directly proposes a multi-objective
technique is (He and Way, 2009), which modifies
MERT to optimize a single metric subject to the
constraint that it does not degrade others. These
approaches all require some setting of constraint
strength or combination weights {pk}. Recent work
in MT evaluation has examined combining metrics
using machine learning for better correlation with
human judgments (Liu and Gildea, 2007; Albrecht
and Hwa, 2007; Gimnez and Ma`rquez, 2008) and
may give insights for setting {pk}. We view our
Pareto-based approach as orthogonal to these efforts.
The tunability of metrics is a problem that is gain-
ing recognition (Liu et al, 2011). If a good evalu-
ation metric could not be used for tuning, it would
be a pity. The Tunable Metrics task at WMT2011
concluded that BLEU is still the easiest to tune
(Callison-Burch et al, 2011). (Mauser et al, 2008;
Cer et al, 2010) report similar observations, in ad-
dition citing WER being difficult and BLEU-TER
being amenable. One unsolved question is whether
metric tunability is a problem inherent to the metric
only, or depends also on the underlying optimization
algorithm. Our positive results with PMO suggest
that the choice of optimization algorithm can help.
Multi-objective ideas are being explored in other
NLP areas. (Spitkovsky et al, 2011) describe a tech-
nique that alternates between hard and soft EM ob-
jectives in order to achieve better local optimum in
grammar induction. (Hall et al, 2011) investigates
joint optimization of a supervised parsing objective
and some extrinsic objectives based on downstream
applications. (Agarwal et al, 2011) considers us-
ing multiple signals (of varying quality) from online
users to train recommendation models. (Eisner and
Daume? III, 2011) trades off speed and accuracy of
a parser with reinforcement learning. None of the
techniques in NLP use Pareto concepts, however.
6 Opportunities and Limitations
We introduce a new approach (PMO) for training
MT systems on multiple metrics. Leveraging the
diverse perspectives of different evaluation metrics
has the potential to improve overall quality. Based
on Pareto Optimality, PMO is easy to implement
and achieves better solutions compared to linear-
combination baselines, for any setting of combi-
nation weights. Further we observe that multi-
objective approaches can be helpful for optimiz-
ing difficult-to-tune metrics; this is beneficial for
quickly introducing new metrics developed in MT
evaluation into MT optimization, especially when
good {pk} are not yet known. We conclude by draw-
ing attention to some limitations and opportunities
raised by this work:
Limitations: (1) The performance of PMO is
limited by the size of the Pareto set. Small N-best
lists lead to sparsely-sampled Pareto Frontiers, and
a much better approach would be to enlarge the hy-
pothesis space using lattices (Macherey et al, 2008).
How to compute Pareto points directly from lattices
is an interesting open research question. (2) The
binary distinction between pareto vs. non-pareto
points ignores the fact that 2nd-place non-pareto
points may also lead to good practical solutions. A
better approach may be to adopt a graded definition
of Pareto optimality as done in some multi-objective
works (Deb et al, 2002). (3) A robust evaluation
methodology that enables significance testing for
multi-objective problems is sorely needed. This will
make it possible to compare multi-objective meth-
ods on more than 2 metrics. We also need to follow
up with human evaluation.
Opportunities: (1) There is still much we do
not understand about metric tunability; we can learn
much by looking at joint metric-spaces and exam-
ining how new metrics correlate with established
ones. (2) Pareto is just one approach among many
in multi-objective optimization. A wealth of meth-
ods are available (Marler and Arora, 2004) and more
experimentation in this space will definitely lead to
new insights. (3) Finally, it would be interesting to
explore other creative uses of multiple-objectives in
MT beyond multiple metrics. For example: Can we
learn to translate faster while sacrificing little on ac-
curacy? Can we learn to jointly optimize cascaded
systems, such as as speech translation or pivot trans-
lation? Life is full of multiple competing objectives.
Acknowledgments
We thank the reviewers for insightful feedback.
8
References
Deepak Agarwal, Bee-Chung Chen, Pradheep Elango,
and Xuanhui Wang. 2011. Click shaping to optimize
multiple objectives. In Proceedings of the 17th ACM
SIGKDD international conference on Knowledge dis-
covery and data mining, KDD ?11, pages 132?140,
New York, NY, USA. ACM.
J. Albrecht and R. Hwa. 2007. A re-examination of ma-
chine learning approaches for sentence-level mt evalu-
ation. In ACL.
J. L. Bentley, H. T. Kung, M. Schkolnick, and C. D.
Thompson. 1978. On the average number of max-
ima in a set of vectors and applications. Journal of the
Association for Computing Machinery (JACM), 25(4).
Alexandra Birch, Phil Blunsom, and Miles Osborne.
2010. Metrics for MT evaluation: Evaluating reorder-
ing. Machine Translation, 24(1).
S. Bo?rzso?nyi, D. Kossmann, and K. Stocker. 2001. The
skyline operator. In Proceedings of the 17th Interna-
tional Conference on Data Engineering (ICDE).
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Omar Zaidan. 2011. Findings of the 2011 work-
shop on statistical machine translation. In Proceedings
of the Sixth Workshop on Statistical Machine Transla-
tion, pages 22?64, Edinburgh, Scotland, July. Associ-
ation for Computational Linguistics.
Daniel Cer, Christopher Manning, and Daniel Jurafsky.
2010. The best lexical metric for phrase-based statis-
tical MT system optimization. In NAACL HLT.
David Chiang, Wei Wang, and Kevin Knight. 2009.
11,001 new features for statistical machine translation.
In NAACL.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passiveag-
gressive algorithms. Journal of Machine Learning Re-
search, 7.
Kalyanmoy Deb, Amrit Pratap, Sammer Agarwal, and
T. Meyarivan. 2002. A fast and elitist multiobjective
genetic algorithm: NSGA-II. IEEE Transactions on
Evolutionary Computation, 6(2).
Chris Dyer, Hendra Setiawan, Yuval Marton, and Philip
Resnik. 2009. The university of maryland statistical
machine translation system for the fourth workshop on
machine translation. In Proc. of the Fourth Workshop
on Machine Translation.
Jason Eisner and Hal Daume? III. 2011. Learning speed-
accuracy tradeoffs in nondeterministic inference algo-
rithms. In COST: NIPS 2011 Workshop on Computa-
tional Trade-offs in Statistical Learning.
Jesu?s Gimnez and Llu??s Ma`rquez. 2008. Heterogeneous
automatic mt evaluation through non-parametric met-
ric combinations. In ICJNLP.
Parke Godfrey, Ryan Shipley, and Jarek Gyrz. 2007. Al-
gorithms and analyses for maximal vector computa-
tion. VLDB Journal, 16.
Isao Goto, Bin Lu, Ka Po Chow, Eiichiro Sumita, and
Benjamin K. Tsou. 2011. Overview of the patent ma-
chine translation task at the ntcir-9 workshop. In Pro-
ceedings of the NTCIR-9 Workshop Meeting.
Keith Hall, Ryan McDonald, Jason Katz-Brown, and
Michael Ringgaard. 2011. Training dependency
parsers by jointly optimizing multiple objectives.
In Proceedings of the 2011 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1489?1499, Edinburgh, Scotland, UK., July. Associa-
tion for Computational Linguistics.
Yifan He and Andy Way. 2009. Improving the objec-
tive function in minimum error rate training. In MT
Summit.
Mark Hopkins and Jonathan May. 2011. Tuning as rank-
ing. In Proceedings of the 2011 Conference on Empir-
ical Methods in Natural Language Processing, pages
1352?1362, Edinburgh, Scotland, UK., July. Associa-
tion for Computational Linguistics.
H. Isozaki, T. Hirao, K. Duh, K. Sudoh, and H. Tsukada.
2010. Automatic evaluation of translation quality for
distant language pairs. In EMNLP.
T. Joachims. 2006. Training linear SVMs in linear time.
In KDD.
P. Koehn et al 2007. Moses: open source toolkit for
statistical machine translation. In ACL.
A. Lavie and A. Agarwal. 2007. METEOR: An auto-
matic metric for mt evaluation with high levels of cor-
relation with human judgments. In Workshop on Sta-
tistical Machine Translation.
P. Liang, A. Bouchard-Cote, D. Klein, and B. Taskar.
2006. An end-to-end discriminative approach to ma-
chine translation. In ACL.
Ding Liu and Daniel Gildea. 2007. Source-language fea-
tures and maximum correlation training for machine
translation evaluation. In NAACL.
Chang Liu, Daniel Dahlmeier, and Hwee Tou Ng. 2011.
Better evaluation metrics lead to better machine trans-
lation. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing.
Wolfgang Macherey, Franz Och, Ignacio Thayer, and
Jakob Uszkoreit. 2008. Lattice-based minimum er-
ror rate training for statistical machine translation. In
EMNLP.
R. T. Marler and J. S. Arora. 2004. Survey of
multi-objective optimization methods for engineering.
Structural and Multidisciplinary Optimization, 26.
Arne Mauser, Sas?a Hasan, and Hermann Ney. 2008.
Automatic evaluation measures for statistical machine
9
translation system optimization. In International Con-
ference on Language Resources and Evaluation, Mar-
rakech, Morocco, May.
Kaisa Miettinen. 1998. Nonlinear Multiobjective Opti-
mization. Springer.
J.A. Nelder and R. Mead. 1965. The downhill simplex
method. Computer Journal, 7(308).
Franz Och. 2003. Minimum error rate training in statis-
tical machine translation. In ACL.
Karolina Owczarzak, Josef van Genabith, and Andy Way.
2007. Labelled dependencies in machine translation
evaluation. In Proceedings of the Second Workshop
on Statistical Machine Translation.
Sebastian Pado, Daniel Cer, Michel Galley, Dan Jurafsky,
and Christopher D. Manning. 2009. Measuring ma-
chine translation quality as semantic equivalence: A
metric based on entailment features. Machine Trans-
lation, 23(2-3).
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic eval-
uation of machine translation. In ACL.
Vilfredo Pareto. 1906. Manuale di Economica Politica,
(Translated into English by A.S. Schwier as Manual of
Political Economy, 1971). Societa Editrice Libraria,
Milan.
Michael Paul. 2010. Overview of the iwslt 2010 evalua-
tion campaign. In IWSLT.
Yoshikazu Sawaragi, Hirotaka Nakayama, and Tetsuzo
Tanino, editors. 1985. Theory of Multiobjective Opti-
mization. Academic Press.
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A study of translation edit rate
with targeted human annotation. In AMTA.
Valentin I. Spitkovsky, Hiyan Alshawi, and Daniel Juraf-
sky. 2011. Lateen em: Unsupervised training with
multiple objectives, applied to dependency grammar
induction. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Processing,
pages 1269?1280, Edinburgh, Scotland, UK., July. As-
sociation for Computational Linguistics.
Omar Zaidan. 2009. Z-MERT: A fully configurable open
source tool for minimum error rate training of machine
translation systems. In The Prague Bulletin of Mathe-
matical Linguistics.
10
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 100?104,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
A Comparative Study of Target Dependency Structures
for Statistical Machine Translation
Xianchao Wu?, Katsuhito Sudoh, Kevin Duh?, Hajime Tsukada, Masaaki Nagata
NTT Communication Science Laboratories, NTT Corporation
2-4 Hikaridai Seika-cho, Soraku-gun Kyoto 619-0237 Japan
wuxianchao@gmail.com,sudoh.katsuhito@lab.ntt.co.jp,
kevinduh@is.naist.jp,{tsukada.hajime,nagata.masaaki}@lab.ntt.co.jp
Abstract
This paper presents a comparative study of
target dependency structures yielded by sev-
eral state-of-the-art linguistic parsers. Our ap-
proach is to measure the impact of these non-
isomorphic dependency structures to be used
for string-to-dependency translation. Besides
using traditional dependency parsers, we also
use the dependency structures transformed
from PCFG trees and predicate-argument
structures (PASs) which are generated by an
HPSG parser and a CCG parser. The experi-
ments on Chinese-to-English translation show
that the HPSG parser?s PASs achieved the best
dependency and translation accuracies.
1 Introduction
Target language side dependency structures have
been successfully used in statistical machine trans-
lation (SMT) by Shen et al (2008) and achieved
state-of-the-art results as reported in the NIST 2008
Open MT Evaluation workshop and the NTCIR-9
Chinese-to-English patent translation task (Goto et
al., 2011; Ma and Matsoukas, 2011). A primary ad-
vantage of dependency representations is that they
have a natural mechanism for representing discon-
tinuous constructions, which arise due to long-
distance dependencies or in languages where gram-
matical relations are often signaled by morphology
instead of word order (McDonald and Nivre, 2011).
It is known that dependency-style structures can
be transformed from a number of linguistic struc-
?Now at Baidu Inc.
?Now at Nara Institute of Science & Technology (NAIST)
tures. For example, using the constituent-to-
dependency conversion approach proposed by Jo-
hansson and Nugues (2007), we can easily yield de-
pendency trees from PCFG style trees. A seman-
tic dependency representation of a whole sentence,
predicate-argument structures (PASs), are also in-
cluded in the output trees of (1) a state-of-the-art
head-driven phrase structure grammar (HPSG) (Pol-
lard and Sag, 1994; Sag et al, 2003) parser, Enju1
(Miyao and Tsujii, 2008) and (2) a state-of-the-art
CCG parser2 (Clark and Curran, 2007). The moti-
vation of this paper is to investigate the impact of
these non-isomorphic dependency structures to be
used for SMT. That is, we would like to provide a
comparative evaluation of these dependencies in a
string-to-dependency decoder (Shen et al, 2008).
2 Gaining Dependency Structures
2.1 Dependency tree
We follow the definition of dependency graph and
dependency tree as given in (McDonald and Nivre,
2011). A dependency graph G for sentence s is
called a dependency tree when it satisfies, (1) the
nodes cover all the words in s besides the ROOT;
(2) one node can have one and only one head (word)
with a determined syntactic role; and (3) the ROOT
of the graph is reachable from all other nodes.
For extracting string-to-dependency transfer
rules, we use well-formed dependency structures,
either fixed or floating, as defined in (Shen et al,
2008). Similarly, we ignore the syntactic roles
1http://www-tsujii.is.s.u-tokyo.ac.jp/enju/index.html
2http://groups.inf.ed.ac.uk/ccg/software.html
100
 when the fluid pressure cylinder 31 is used , fluid is gradually applied . 
t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 
c2 c5 c7 c9 c11 c12 c14 c15 c17 c20 c22 c24 c25 
c3 
c4 
c6 
c8 
c10 c13 
c18 
c19 
c21 
c23 
c16 
c1 
c0 
conj_ 
arg12 
det_ 
arg1 
adj_ 
arg1 
noun_ 
arg1 
noun_ 
arg0 
adj_ 
arg1 
aux_ 
arg12 
verb_ 
arg12 
punct_ 
arg1 
noun_ 
arg0 
aux_ 
arg12 
adj_ 
arg1 
verb_ 
arg12 
* + 
* + 
* + 
* 
+ 
* + 
* + 
* + 
*  
* + 
* + 
* + 
* + 
* + 
+ 
Figure 1: HPSG tree of an example sentence. ?*?/
?+?=syntactic/semantic heads. Arrows in red (upper)=
PASs, orange (bottom)=word-level dependencies gener-
ated from PASs, blue=newly appended dependencies.
both during rule extracting and target dependency
language model (LM) training.
2.2 Dependency parsing
Graph-based and transition-based are two predom-
inant paradigms for data-driven dependency pars-
ing. The MST parser (McDonald et al, 2005) and
the Malt parser (Nivre, 2003) stand for two typical
parsers, respectively. Parsing accuracy comparison
and error analysis under the CoNLL-X dependency
shared task data (Buchholz and Marsi, 2006) have
been performed by McDonald and Nivre (2011).
Here, we compare them on the SMT tasks through
parsing the real-world SMT data.
2.3 PCFG parsing
For PCFG parsing, we select the Berkeley parser
(Petrov and Klein, 2007). In order to generate word-
level dependency trees from the PCFG tree, we use
the LTH constituent-to-dependency conversion tool3
written by Johansson and Nugues (2007). The head
finding rules4 are according to Magerman (1995)
and Collins (1997). Similar approach has been orig-
inally used by Shen et al (2008).
2.4 HPSG parsing
In the Enju English HPSG grammar (Miyao et al,
2003) used in this paper, the semantic content of
3http://nlp.cs.lth.se/software/treebank converter/
4http://www.cs.columbia.edu/ mcollins/papers/heads
a sentence/phrase is represented by a PAS. In an
HPSG tree, each leaf node generally introduces a
predicate, which is represented by the pair made up
of the lexical entry feature and predicate type fea-
ture. The arguments of a predicate are designated by
the arrows from the argument features in a leaf node
to non-terminal nodes (e.g., t0?c3, t0?c16).
Since the PASs use the non-terminal nodes in the
HPSG tree (Figure 1), this prevents their direct us-
age in a string-to-dependency decoder. We thus need
an algorithm to transform these phrasal predicate-
argument dependencies into a word-to-word depen-
dency tree. Our algorithm (refer to Figure 1 for an
example) for changing PASs into word-based depen-
dency trees is as follows:
1. finding, i.e., find the syntactic/semantic head
word of each argument node through a bottom-
up traversal of the tree;
2. mapping, i.e., determine the arc directions
(among a predicate word and the syntac-
tic/semantic head words of the argument nodes)
for each predicate type according to Table 1.
Then, a dependency graph will be generated;
3. checking, i.e., post modifying the dependency
graph according to the definition of dependency
tree (Section 2.1).
Table 1 lists the mapping from HPSG?s PAS types
to word-level dependency arcs. Since a non-terminal
node in an HPSG tree has two kinds of heads, syn-
tactic or semantic, we will generate two dependency
graphs after mapping. We use ?PAS+syn? to repre-
sent the dependency trees generated from the HPSG
PASs guided by the syntactic heads. For semantic
heads, we use ?PAS+sem?.
For example, refer to t0 = when in Figure 1.
Its arg1 = c16 (with syntactic head t10), arg2
= c3 (with syntactic head t6), and PAS type =
conj arg12. In Table 1, this PAS type corresponds
to arg2?pred?arg1, then the result word-level de-
pendency is t6(is)?t0(when)?t10(is).
We need to post modify the dependency graph af-
ter applying the mapping, since it is not guaranteed
to be a dependency tree. Referring to the definition
of dependency tree (Section 2.1), we need the strat-
egy for (1) selecting only one head from multiple
101
PAS Type Dependency Relation
adj arg1[2] [arg2 ?] pred ? arg1
adj mod arg1[2] [arg2 ?] pred ? arg1 ? mod
aux[ mod] arg12 arg1/pred ? arg2 [? mod]
conj arg1[2[3]] [arg2[/arg3]] ? pred ? arg1
comp arg1[2] pred ? arg1 [? arg2]
comp mod arg1 arg1 ? pred ? mod
noun arg1 pred ? arg1
noun arg[1]2 arg2 ? pred [? arg1]
poss arg[1]2 pred ? arg2 [? arg1]
prep arg12[3] arg2[/arg3] ? pred ? arg1
prep mod arg12[3] arg2[/arg3] ? pred ? arg1 ? mod
quote arg[1]2 [arg1 ?] pred ? arg2
quote arg[1]23 [arg1/]arg3 ? pred ? arg2
lparen arg123 pred/arg2 ? arg3 ? arg1
relative arg1[2] [arg2 ?] pred ? arg1
verb arg1[2[3[4]]] arg1[/arg2[/arg3[/arg4]]] ? pred
verb mod arg1[2[3[4]]] arg1[/arg2[/arg3[/arg4]]]?pred?mod
app arg12,coord arg12 arg2/pred ? arg1
det arg1,it arg1,punct arg1 pred ? arg1
dtv arg2 pred ? arg2
lgs arg2 arg2 ? pred
Table 1: Mapping fromHPSG?s PAS types to dependency
relations. Dependent(s)? head(s), / = and, [] = optional.
heads and (2) appending dependency relations for
those words/punctuation that do not have any head.
When one word has multiple heads, we only keep
one. The selection strategy is that, if this arc was
deleted, it will cause the biggest number of words
that can not reach to the root word anymore. In case
of a tie, we greedily pack the arc that connect two
words wi and wj where |i? j| is the biggest. For all
the words and punctuation that do not have a head,
we greedily take the root word of the sentence as
their heads. In order to fully use the training data,
if there are directed cycles in the result dependency
graph, we still use the graph in our experiments,
where only partial dependency arcs, i.e., those target
flat/hierarchical phrases attached with well-formed
dependency structures, can be used during transla-
tion rule extraction.
2.5 CCG parsing
We also use the predicate-argument dependencies
generated by the CCG parser developed by Clark
and Curran (2007). The algorithm for generating
word-level dependency tree is easier than processing
the PASs included in the HPSG trees, since the word
level predicate-argument relations have already been
included in the output of CCG parser. The mapping
from predicate types to the gold-standard grammat-
ical relations can be found in Table 13 in (Clark and
Curran, 2007). The post-processing is like that de-
scribed for HPSG parsing, except we greedily use
the MST?s sentence root when we can not determine
it based on the CCG parser?s PASs.
3 Experiments
3.1 Setup
We re-implemented the string-to-dependency de-
coder described in (Shen et al, 2008). Dependency
structures from non-isomorphic syntactic/semantic
parsers are separately used to train the transfer
rules as well as target dependency LMs. For intu-
itive comparison, an outside SMT system is Moses
(Koehn et al, 2007).
For Chinese-to-English translation, we use the
parallel data from NIST Open Machine Translation
Evaluation tasks. The training data contains 353,796
sentence pairs, 8.7M Chinese words and 10.4M En-
glish words. The NIST 2003 and 2005 test data
are respectively taken as the development and test
set. We performed GIZA++ (Och and Ney, 2003)
and the grow-diag-final-and symmetrizing strategy
(Koehn et al, 2007) to obtain word alignments. The
Berkeley Language Modeling Toolkit, berkeleylm-
1.0b35 (Pauls and Klein, 2011), was employed to
train (1) a five-gram LM on the Xinhua portion of
LDC English Gigaword corpus v3 (LDC2007T07)
and (2) a tri-gram dependency LM on the English
dependency structures of the training data. We re-
port the translation quality using the case-insensitive
BLEU-4 metric (Papineni et al, 2002).
3.2 Statistics of dependencies
We compare the similarity of the dependencies with
each other, as shown in Table 2. Basically, we in-
vestigate (1) if two dependency graphs of one sen-
tence share the same root word and (2) if the head of
one word in one sentence are identical in two depen-
dency graphs. In terms of root word comparison, we
observe that MST and CCG share 87.3% of iden-
tical root words, caused by borrowing roots from
MST to CCG. Then, it is interesting that Berkeley
and PAS+syn share 74.8% of identical root words.
Note that the Berkeley parser is trained on the Penn
treebank (Marcus et al, 1994) yet the HPSG parser
is trained on the HPSG treebank (Miyao and Tsujii,
5http://code.google.com/p/berkeleylm/
102
Dependency Precision Recall BLEU-Dev BLEU-Test # phrases # hier rules # illegal dep trees # directed cycles
Moses-1 - - 0.3349 0.3207 5.4M - - -
Moses-2 - - 0.3445 0.3262 0.7M 4.5M - -
MST 0.744 0.750 0.3520 0.3291 2.4M 2.1M 251 0
Malt 0.732 0.738 0.3423 0.3203 1.5M 1.3M 130,960 0
Berkeley 0.800 0.806 0.3475 0.3312 2.4M 2.2M 282 0
PAS+syn 0.818 0.824 0.3499 0.3376 2.2M 1.9M 10,411 5,853
PAS+sem 0.777 0.782 0.3484 0.3343 2.1M 1.6M 14,271 9,747
CCG 0.701 0.705 0.3442 0.3283 1.7M 1.3M 61,015 49,955
Table 3: Comparison of dependency and translation accuracies. Moses-1 = phrasal, Moses-2 = hierarchical.
Malt Berkeley PAS PAS CCG
+syn +sem
MST 70.5 62.5 69.2 53.3 87.3
(77.3) (64.6) (58.5) (58.1) (61.7)
Malt 66.2 73.0 46.8 62.9
(63.2) (57.7) (56.6) (58.1)
Berkeley 74.8 44.2 56.5
(64.3) (56.0) (59.2)
PAS+ 59.3 62.9
syn (79.1) (61.0)
PAS+ 60.0
sem (58.8)
Table 2: Comparison of the dependencies of the English
sentences in the training data. Without () = % of similar
root words; with () = % of similar head words.
2008). In terms of head word comparison, PAS+syn
and PAS+sem share 79.1% of identical head words.
This is basically due to that we used the similar
PASs of the HPSG trees. Interestingly, there are only
59.3% identical root words shared by PAS+syn and
PAS+sem. This reflects the significant difference be-
tween syntactic and semantic heads.
We also manually created the golden dependency
trees for the first 200 English sentences in the train-
ing data. The precision/recall (P/R) are shown in
Table 3. We observe that (1) the translation accura-
cies approximately follow the P/R scores yet are not
that sensitive to their large variances, and (2) it is
still tough for domain-adapting from the treebank-
trained parsers to parse the real-world SMT data.
PAS+syn performed the best by avoiding the errors
of missing of arguments for a predicate, wrongly
identified head words for a linguistic phrase, and in-
consistency dependencies inside relatively long co-
ordinate structures. These errors significantly influ-
ence the number of extractable translation rules and
the final translation accuracies.
Note that, these P/R scores on the first 200 sen-
tences (all from less than 20 newswire documents)
shall only be taken as an approximation of the total
training data and not necessarily exactly follow the
tendency of the final BLEU scores. For example,
CCG is worse than Malt in terms of P/R yet with a
higher BLEU score. We argue this is mainly due to
that the number of illegal dependency trees gener-
ated by Malt is the highest. Consequently, the num-
ber of flat/hierarchical rules generated by using Malt
trees is the lowest. Also, PAS+sem has a lower P/R
than Berkeley, yet their final BLEU scores are not
statistically different.
3.3 Results
Table 3 also shows the BLEU scores, the number of
flat phrases and hierarchical rules (both integrated
with target dependency structures), and the num-
ber of illegal dependency trees generated by each
parser. From the table, we have the following ob-
servations: (1) all the dependency structures (except
Malt) achieved a significant better BLEU score than
the phrasal Moses; (2) PAS+syn performed the best
in the test set (0.3376), and it is significantly better
than phrasal/hierarchical Moses (p < 0.01), MST
(p < 0.05), Malt (p < 0.01), Berkeley (p < 0.05),
and CCG (p < 0.05); and (3) CCG performed as
well as MST and Berkeley. These results lead us to
argue that the robustness of deep syntactic parsers
can be advantageous in SMT compared with tradi-
tional dependency parsers.
4 Conclusion
We have constructed a string-to-dependency trans-
lation platform for comparing non-isomorphic tar-
get dependency structures. Specially, we proposed
an algorithm for generating word-based dependency
trees from PASs which are generated by a state-of-
the-art HPSG parser. We found that dependency
trees transformed from these HPSG PASs achieved
the best dependency/translation accuracies.
103
Acknowledgments
We thank the anonymous reviewers for their con-
structive comments and suggestions.
References
Sabine Buchholz and Erwin Marsi. 2006. Conll-x shared
task on multilingual dependency parsing. In Proceed-
ings of the Tenth Conference on Computational Nat-
ural Language Learning (CoNLL-X), pages 149?164,
New York City, June. Association for Computational
Linguistics.
Stephen Clark and James R. Curran. 2007. Wide-
coverage efficient statistical parsing with ccg and log-
linear models. Computational Linguistics, 33(4):493?
552.
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In Proceedings of the
35th Annual Meeting of the Association for Computa-
tional Linguistics, pages 16?23, Madrid, Spain, July.
Association for Computational Linguistics.
Isao Goto, Bin Lu, Ka Po Chow, Eiichiro Sumita, and
Benjamin K. Tsou. 2011. Overview of the patent ma-
chine translation task at the ntcir-9 workshop. In Pro-
ceedings of NTCIR-9, pages 559?578.
Richard Johansson and Pierre Nugues. 2007. Extended
constituent-to-dependency conversion for english. In
In Proceedings of NODALIDA, Tartu, Estonia, April.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondr?ej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of the ACL 2007 Demo and Poster Sessions, pages
177?180.
Jeff Ma and Spyros Matsoukas. 2011. Bbn?s systems
for the chinese-english sub-task of the ntcir-9 patentmt
evaluation. In Proceedings of NTCIR-9, pages 579?
584.
David Magerman. 1995. Statistical decision-tree models
for parsing. In In Proceedings of of the 33rd Annual
Meeting of the Association for Computational Linguis-
tics, pages 276?283.
Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz,
Robert MacIntyre, Ann Bies, Mark Ferguson, Karen
Katz, and Britta Schasberger. 1994. The penn tree-
bank: Annotating predicate argument structure. In
Proceedings of the Workshop on HLT, pages 114?119,
Plainsboro.
Ryan McDonald and Joakim Nivre. 2011. Analyzing
and integrating dependency parsers. Computational
Linguistics, 37(1):197?230.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL?05), pages 91?98, Ann Arbor, Michigan, June.
Association for Computational Linguistics.
Yusuke Miyao and Jun?ichi Tsujii. 2008. Feature forest
models for probabilistic hpsg parsing. Computational
Lingustics, 34(1):35?80.
Yusuke Miyao, Takashi Ninomiya, and Jun?ichi Tsu-
jii. 2003. Probabilistic modeling of argument struc-
tures including non-local dependencies. In Proceed-
ings of the International Conference on Recent Ad-
vances in Natural Language Processing, pages 285?
291, Borovets.
Joakim Nivre. 2003. An efficient algorithm for projec-
tive dependency parsing. In Proceedings of the 8th In-
ternational Workshop on Parsing Technologies (IWPT,
pages 149?160.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalu-
ation of machine translation. In Proceedings of ACL,
pages 311?318.
Adam Pauls and Dan Klein. 2011. Faster and smaller
n-gram language models. In Proceedings of the 49th
Annual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
258?267, Portland, Oregon, USA, June. Association
for Computational Linguistics.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Human Language Tech-
nologies 2007: The Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics; Proceedings of the Main Conference, pages
404?411, Rochester, New York, April. Association for
Computational Linguistics.
Carl Pollard and Ivan A. Sag. 1994. Head-Driven Phrase
Structure Grammar. University of Chicago Press.
Ivan A. Sag, Thomas Wasow, and Emily M. Bender.
2003. Syntactic Theory: A Formal Introduction.
Number 152 in CSLI Lecture Notes. CSLI Publica-
tions.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proceedings of ACL-08:HLT, pages 577?585, Colum-
bus, Ohio.
104
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 127?132,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Akamon: An Open Source Toolkit
for Tree/Forest-Based Statistical Machine Translation?
Xianchao Wu?, Takuya Matsuzaki?, Jun?ichi Tsujii?
? Baidu Inc.
?National Institute of Informatics
? Microsoft Research Asia
wuxianchao@gmail.com,takuya-matsuzaki@nii.ac.jp,jtsujii@microsoft.com
Abstract
We describe Akamon, an open source toolkit
for tree and forest-based statistical machine
translation (Liu et al, 2006; Mi et al, 2008;
Mi and Huang, 2008). Akamon implements
all of the algorithms required for tree/forest-
to-string decoding using tree-to-string trans-
lation rules: multiple-thread forest-based de-
coding, n-gram language model integration,
beam- and cube-pruning, k-best hypotheses
extraction, and minimum error rate training.
In terms of tree-to-string translation rule ex-
traction, the toolkit implements the tradi-
tional maximum likelihood algorithm using
PCFG trees (Galley et al, 2004) and HPSG
trees/forests (Wu et al, 2010).
1 Introduction
Syntax-based statistical machine translation (SMT)
systems have achieved promising improvements in
recent years. Depending on the type of input, the
systems are divided into two categories: string-
based systems whose input is a string to be simul-
taneously parsed and translated by a synchronous
grammar (Wu, 1997; Chiang, 2005; Galley et al,
2006; Shen et al, 2008), and tree/forest-based sys-
tems whose input is already a parse tree or a packed
forest to be directly converted into a target tree or
string (Ding and Palmer, 2005; Quirk et al, 2005;
Liu et al, 2006; Huang et al, 2006; Mi et al, 2008;
Mi and Huang, 2008; Zhang et al, 2009; Wu et al,
2010; Wu et al, 2011a).
?Work done when all the authors were in The University of
Tokyo.
Depending on whether or not parsers are explic-
itly used for obtaining linguistically annotated data
during training, the systems are also divided into two
categories: formally syntax-based systems that do
not use additional parsers (Wu, 1997; Chiang, 2005;
Xiong et al, 2006), and linguistically syntax-based
systems that use PCFG parsers (Liu et al, 2006;
Huang et al, 2006; Galley et al, 2006; Mi et al,
2008; Mi and Huang, 2008; Zhang et al, 2009),
HPSG parsers (Wu et al, 2010; Wu et al, 2011a), or
dependency parsers (Ding and Palmer, 2005; Quirk
et al, 2005; Shen et al, 2008). A classification1 of
syntax-based SMT systems is shown in Table 1.
Translation rules can be extracted from aligned
string-string (Chiang, 2005), tree-tree (Ding and
Palmer, 2005) and tree/forest-string (Galley et al,
2004; Mi and Huang, 2008; Wu et al, 2011a)
data structures. Leveraging structural and linguis-
tic information from parse trees/forests, the latter
two structures are believed to be better than their
string-string counterparts in handling non-local re-
ordering, and have achieved promising translation
results. Moreover, the tree/forest-string structure is
more widely used than the tree-tree structure, pre-
sumably because using two parsers on the source
and target languages is subject to more problems
than making use of a parser on one language, such
as the shortage of high precision/recall parsers for
languages other than English, compound parse error
rates, and inconsistency of errors. In Table 1, note
that tree-to-string rules are generic and applicable
to many syntax-based models such as tree/forest-to-
1This classification is inspired by and extends the Table 1 in
(Mi and Huang, 2008).
127
Source-to-target Examples (partial) Decoding Rules Parser
tree-to-tree (Ding and Palmer, 2005) ? dep.-to-dep. DG
forest-to-tree (Liu et al, 2009a) ? ?? tree-to-tree PCFG
tree-to-string (Liu et al, 2006) ? tree-to-string PCFG
(Quirk et al, 2005) ? dep.-to-string DG
forest-to-string (Mi et al, 2008) ? ?? tree-to-string PCFG
(Wu et al, 2011a) ? ?? tree-to-string HPSG
string-to-tree (Galley et al, 2006) CKY tree-to-string PCFG
(Shen et al, 2008) CKY string-to-dep. DG
string-to-string (Chiang, 2005) CKY string-to-string none
(Xiong et al, 2006) CKY string-to-string none
Table 1: A classification of syntax-based SMT systems. Tree/forest-based and string-based systems are split by a line.
All the systems listed here are linguistically syntax-based except the last two (Chiang, 2005) and (Xiong et al, 2006),
which are formally syntax-based. DG stands for dependency (abbreviated as dep.) grammar. ? and ? denote top-down
and bottom-up traversals of a source tree/forest.
string models and string-to-tree model.
However, few tree/forest-to-string systems have
been made open source and this makes it diffi-
cult and time-consuming to testify and follow exist-
ing proposals involved in recently published papers.
The Akamon system2, written in Java and follow-
ing the tree/forest-to-string research direction, im-
plements all of the algorithms for both tree-to-string
translation rule extraction (Galley et al, 2004; Mi
and Huang, 2008; Wu et al, 2010; Wu et al, 2011a)
and tree/forest-based decoding (Liu et al, 2006; Mi
et al, 2008). We hope this system will help re-
lated researchers to catch up with the achievements
of tree/forest-based translations in the past several
years without re-implementing the systems or gen-
eral algorithms from scratch.
2 Akamon Toolkit Features
Limited by the successful parsing rate and coverage
of linguistic phrases, Akamon currently achieves
comparable translation accuracies compared with
the most frequently used SMT baseline system,
Moses (Koehn et al, 2007). Table 2 shows the auto-
matic translation accuracies (case-sensitive) of Aka-
mon and Moses. Besides BLEU and NIST score, we
further list RIBES score3, , i.e., the software imple-
mentation of Normalized Kendall?s ? as proposed by
(Isozaki et al, 2010a) to automatically evaluate the
translation between distant language pairs based on
rank correlation coefficients and significantly penal-
2Code available at https://sites.google.com/site/xianchaowu2012
3Code available at http://www.kecl.ntt.co.jp/icl/lirg/ribes
izes word order mistakes.
In this table, Akamon-Forest differs from
Akamon-Comb by using different configurations:
Akamon-Forest used only 2/3 of the total training
data (limited by the experiment environments and
time). Akamon-Comb represents the system com-
bination result by combining Akamon-Forest and
other phrase-based SMT systems, which made use
of pre-ordering methods of head finalization as de-
scribed in (Isozaki et al, 2010b) and used the total 3
million training data. The detail of the pre-ordering
approach and the combination method can be found
in (Sudoh et al, 2011) and (Duh et al, 2011).
Also, Moses (hierarchical) stands for the hi-
erarchical phrase-based SMT system and Moses
(phrase) stands for the flat phrase-based SMT sys-
tem. For intuitive comparison (note that the result
achieved by Google is only for reference and not a
comparison, since it uses a different and unknown
training data) and following (Goto et al, 2011), the
scores achieved by using the Google online transla-
tion system4 are also listed in this table.
Here is a brief description of Akamon?s main fea-
tures:
? multiple-thread forest-based decoding: Aka-
mon first loads the development (with source
and reference sentences) or test (with source
sentences only) file into memory and then per-
form parameter tuning or decoding in a paral-
lel way. The forest-based decoding algorithm
is alike that described in (Mi et al, 2008),
4http://translate.google.com/
128
Systems BLEU NIST RIBES
Google online 0.2546 6.830 0.6991
Moses (hierarchical) 0.3166 7.795 0.7200
Moses (phrase) 0.3190 7.881 0.7068
Moses (phrase)* 0.2773 6.905 0.6619
Akamon-Forest* 0.2799 7.258 0.6861
Akamon-Comb 0.3948 8.713 0.7813
Table 2: Translation accuracies of Akamon and the base-
line systems on the NTCIR-9 English-to-Japanese trans-
lation task (Wu et al, 2011b). * stands for only using
2 million parallel sentences of the total 3 million data.
Here, HPSG forests were used in Akamon.
i.e., first construct a translation forest by ap-
plying the tree-to-string translation rules to the
original parsing forest of the source sentence,
and then collect k-best hypotheses for the root
node(s) of the translation forest using Algo-
rithm 2 or Algorithm 3 as described in (Huang
and Chiang, 2005). Later, the k-best hypothe-
ses are used both for parameter tuning on addi-
tional development set(s) and for final optimal
translation result extracting.
? language models: Akamon can make use of
one or many n-gram language models trained
by using SRILM5 (Stolcke, 2002) or the Berke-
ley language model toolkit, berkeleylm-1.0b36
(Pauls and Klein, 2011). The weights of multi-
ple language models are tuned under minimum
error rate training (MERT) (Och, 2003).
? pruning: traditional beam-pruning and cube-
pruning (Chiang, 2007) techniques are incor-
porated in Akamon to make decoding feasi-
ble for large-scale rule sets. Before decoding,
we also perform the marginal probability-based
inside-outside algorithm based pruning (Mi et
al., 2008) on the original parsing forest to con-
trol the decoding time.
? MERT: Akamon has its own MERT module
which optimizes weights of the features so as
to maximize some automatic evaluation metric,
such as BLEU (Papineni et al, 2002), on a de-
velopment set.
5http://www.speech.sri.com/projects/srilm/
6http://code.google.com/p/berkeleylm/
 
 
 
e.tok 
corpus 
f.seg 
tokenize word segment 
e.tok.lw f.seg.lw 
lowercase lowercase 
clean 
e.clean f.clean 
GIZA++ 
alignment 
Rule set 
rule extraction 
SRILM 
Akamon Decoder (MERT) 
N-gram LM 
e.tok 
dev.e 
tokenize 
e.tok.lw 
lowercase 
e.forests 
Enju 
e.forests 
Enju 
dev 
f.seg 
dev.f 
word 
segmentation 
f.seg.lw 
lowercase 
pre-processing 
Figure 1: Training and tuning process of the Akamon sys-
tem. Here, e = source English language, f = target foreign
language.
? translation rule extraction: as former men-
tioned, we extract tree-to-string translation
rules for Akamon. In particular, we imple-
mented the GHKM algorithm as proposed by
Galley et al (2004) from word-aligned tree-
string pairs. In addition, we also implemented
the algorithms proposed by Mi and Huang
(2008) and Wu et al (2010) for extracting rules
from word-aligned PCFG/HPSG forest-string
pairs.
3 Training and Decoding Frameworks
Figure 1 shows the training and tuning progress of
the Akamon system. Given original bilingual par-
allel corpora, we first tokenize and lowercase the
source and target sentences (e.g., word segmentation
of Chinese and Japanese, punctuation segmentation
of English).
The pre-processed monolingual sentences will be
used by SRILM (Stolcke, 2002) or BerkeleyLM
(Pauls and Klein, 2011) to train a n-gram language
model. In addition, we filter out too long sentences
129
here, i.e., only relatively short sentence pairs will be
used to train word alignments. Then, we can use
GIZA++ (Och and Ney, 2003) and symmetric strate-
gies, such as grow-diag-final (Koehn et al, 2007),
on the tokenized parallel corpus to obtain a word-
aligned parallel corpus.
The source sentence and its packed forest, the tar-
get sentence, and the word alignment are used for
tree-to-string translation rule extraction. Since a 1-
best tree is a special case of a packed forest, we will
focus on using the term ?forest? in the continuing
discussion. Then, taking the target language model,
the rule set, and the preprocessed development set
as inputs, we perform MERT on the decoder to tune
the weights of the features.
The Akamon forest-to-string system includes the
decoding algorithm and the rule extraction algorithm
described in (Mi et al, 2008; Mi and Huang, 2008).
4 Using Deep Syntactic Structures
In Akamon, we support the usage of deep syn-
tactic structures for obtaining fine-grained transla-
tion rules as described in our former work (Wu et
al., 2010)7. Similarly, Enju8, a state-of-the-art and
freely available HPSG parser for English, can be
used to generate packed parse forests for source
sentences9. Deep syntactic structures are included
in the HPSG trees/forests, which includes a fine-
grained description of the syntactic property and a
semantic representation of the sentence. We extract
fine-grained rules from aligned HPSG forest-string
pairs and use them in the forest-to-string decoder.
The detailed algorithms can be found in (Wu et al,
2010; Wu et al, 2011a). Note that, in Akamon, we
also provide the codes for generating HPSG forests
from Enju.
Head-driven phrase structure grammar (HPSG) is
a lexicalist grammar framework. In HPSG, linguis-
tic entities such as words and phrases are represented
by a data structure called a sign. A sign gives a
7However, Akamon still support PCFG tree/forest based
translation. A special case is to yield PCFG style trees/forests
by ignoring the rich features included in the nodes of HPSG
trees/forests and only keep the POS tag and the phrasal cate-
gories.
8http://www-tsujii.is.s.u-tokyo.ac.jp/enju/index.html
9Until the date this paper was submitted, Enju supports gen-
erating English and Chinese forests.
Feature Description
CAT phrasal category
XCAT fine-grained phrasal category
SCHEMA name of the schema applied in the node
HEAD pointer to the head daughter
SEM HEAD pointer to the semantic head daughter
CAT syntactic category
POS Penn Treebank-style part-of-speech tag
BASE base form
TENSE tense of a verb (past, present, untensed)
ASPECT aspect of a verb (none, perfect,
progressive, perfect-progressive)
VOICE voice of a verb (passive, active)
AUX auxiliary verb or not (minus, modal,
have, be, do, to, copular)
LEXENTRY lexical entry, with supertags embedded
PRED type of a predicate
ARG?x? pointer to semantic arguments, x = 1..4
Table 3: Syntactic/semantic features extracted from
HPSG signs that are included in the output of Enju. Fea-
tures in phrasal nodes (top) and lexical nodes (bottom)
are listed separately.
factored representation of the syntactic features of
a word/phrase, as well as a representation of their
semantic content. Phrases and words represented by
signs are composed into larger phrases by applica-
tions of schemata. The semantic representation of
the new phrase is calculated at the same time. As
such, an HPSG parse tree/forest can be considered
as a tree/forest of signs (c.f. the HPSG forest in Fig-
ure 2 in (Wu et al, 2010)).
An HPSG parse tree/forest has two attractive
properties as a representation of a source sentence
in syntax-based SMT. First, we can carefully control
the condition of the application of a translation rule
by exploiting the fine-grained syntactic description
in the source parse tree/forest, as well as those in the
translation rules. Second, we can identify sub-trees
in a parse tree/forest that correspond to basic units
of the semantics, namely sub-trees covering a pred-
icate and its arguments, by using the semantic rep-
resentation given in the signs. Extraction of trans-
lation rules based on such semantically-connected
sub-trees is expected to give a compact and effective
set of translation rules.
A sign in the HPSG tree/forest is represented by a
typed feature structure (TFS) (Carpenter, 1992). A
TFS is a directed-acyclic graph (DAG) wherein the
edges are labeled with feature names and the nodes
130
  
She 
ignore 
 fact 
want 
I 
dispute 
ARG1 
ARG2 
ARG1 ARG1 
ARG2 
ARG2 
John 
kill 
 Mary ARG2 
ARG1 
Figure 2: Predicate argument structures for the sentences
of ?John killed Mary? and ?She ignored the fact that I
wanted to dispute?.
(feature values) are typed. In the original HPSG for-
malism, the types are defined in a hierarchy and the
DAG can have arbitrary shape (e.g., it can be of any
depth). We however use a simplified form of TFS,
for simplicity of the algorithms. In the simplified
form, a TFS is converted to a (flat) set of pairs of
feature names and their values. Table 3 lists the fea-
tures used in our system, which are a subset of those
in the original output from Enju.
In the Enju English HPSG grammar (Miyao et
al., 2003) used in our system, the semantic content
of a sentence/phrase is represented by a predicate-
argument structure (PAS). Figure 2 shows the PAS
of a simple sentence, ?John killed Mary?, and a more
complex PAS for another sentence, ?She ignored the
fact that I wanted to dispute?, which is adopted from
(Miyao et al, 2003). In an HPSG tree/forest, each
leaf node generally introduces a predicate, which
is represented by the pair of LEXENTRY (lexical
entry) feature and PRED (predicate type) feature.
The arguments of a predicate are designated by the
pointers from the ARG?x? features in a leaf node
to non-terminal nodes. Consequently, Akamon in-
cludes the algorithm for extracting compact com-
posed rules from these PASs which further lead to
a significant fast tree-to-string decoder. This is be-
cause it is not necessary to exhaustively generate the
subtrees for all the tree nodes for rule matching any
more. Limited by space, we suggest the readers to
refer to our former work (Wu et al, 2010; Wu et al,
2011a) for the experimental results, including the
training and decoding time using standard English-
to-Japanese corpora, by using deep syntactic struc-
tures.
5 Content of the Demonstration
In the demonstration, we would like to provide a
brief tutorial on:
? describing the format of the packed forest for a
source sentence,
? the training script on translation rule extraction,
? the MERT script on feature weight tuning on a
development set, and,
? the decoding script on a test set.
Based on Akamon, there are a lot of interesting
directions left to be updated in a relatively fast way
in the near future, such as:
? integrate target dependency structures, espe-
cially target dependency language models, as
proposed by Mi and Liu (2010),
? better pruning strategies for the input packed
forest before decoding,
? derivation-based combination of using other
types of translation rules in one decoder, as pro-
posed by Liu et al (2009b), and
? taking other evaluation metrics as the opti-
mal objective for MERT, such as NIST score,
RIBES score (Isozaki et al, 2010a).
Acknowledgments
We thank Yusuke Miyao and Naoaki Okazaki for
their invaluable help and the anonymous reviewers
for their comments and suggestions.
References
Bob Carpenter. 1992. The Logic of Typed Feature Struc-
tures. Cambridge University Press.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
ACL, pages 263?270, Ann Arbor, MI.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Lingustics, 33(2):201?228.
Yuan Ding and Martha Palmer. 2005. Machine trans-
lation using probabilistic synchronous dependency in-
sertion grammers. In Proceedings of ACL, pages 541?
548, Ann Arbor.
Kevin Duh, Katsuhito Sudoh, Xianchao Wu, Hajime
Tsukada, and Masaaki Nagata. 2011. Generalized
minimum bayes risk system combination. In Proceed-
ings of IJCNLP, pages 1356?1360, November.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In Pro-
ceedings of HLT-NAACL.
131
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proceed-
ings of COLING-ACL, pages 961?968, Sydney.
Isao Goto, Bin Lu, Ka Po Chow, Eiichiro Sumita, and
Benjamin K. Tsou. 2011. Overview of the patent ma-
chine translation task at the ntcir-9 workshop. In Pro-
ceedings of NTCIR-9, pages 559?578.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proceedings of IWPT.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proceedings of 7th AMTA.
Hideki Isozaki, Tsutomu Hirao, Kevin Duh, Katsuhito
Sudoh, and Hajime Tsukada. 2010a. Automatic eval-
uation of translation quality for distant language pairs.
In Proc.of EMNLP, pages 944?952.
Hideki Isozaki, Katsuhito Sudoh, Hajime Tsukada, and
Kevin Duh. 2010b. Head finalization: A simple re-
ordering rule for sov languages. In Proceedings of
WMT-MetricsMATR, pages 244?251, July.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondr?ej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of the ACL 2007 Demo and Poster Sessions, pages
177?180.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment templates for statistical machine
transaltion. In Proceedings of COLING-ACL, pages
609?616, Sydney, Australia.
Yang Liu, Yajuan Lu?, and Qun Liu. 2009a. Improving
tree-to-tree translation with packed forests. In Pro-
ceedings of ACL-IJCNLP, pages 558?566, August.
Yang Liu, Haitao Mi, Yang Feng, and Qun Liu. 2009b.
Joint decoding with multiple translation models. In
Proceedings of ACL-IJCNLP, pages 576?584, August.
Haitao Mi and Liang Huang. 2008. Forest-based transla-
tion rule extraction. In Proceedings of EMNLP, pages
206?214, October.
Haitao Mi and Qun Liu. 2010. Constituency to depen-
dency translation with forests. In Proceedings of ACL,
pages 1433?1442, July.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proceedings of ACL-08:HLT,
pages 192?199, Columbus, Ohio.
Yusuke Miyao, Takashi Ninomiya, and Jun?ichi Tsujii.
2003. Probabilistic modeling of argument structures
including non-local dependencies. In Proceedings of
RANLP, pages 285?291, Borovets.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of ACL,
pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalu-
ation of machine translation. In Proceedings of ACL,
pages 311?318.
Adam Pauls and Dan Klein. 2011. Faster and smaller n-
gram language models. In Proceedings of ACL-HLT,
pages 258?267, June.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. De-
pendency treelet translation: Syntactically informed
phrasal smt. In Proceedings of ACL, pages 271?279.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proceedings of ACL-08:HLT, pages 577?585.
Andreas Stolcke. 2002. Srilm-an extensible language
modeling toolkit. In Proceedings of International
Conference on Spoken Language Processing, pages
901?904.
Katsuhito Sudoh, Kevin Duh, Hajime Tsukada, Masaaki
Nagata, Xianchao Wu, Takuya Matsuzaki, and
Jun?ichi Tsujii. 2011. Ntt-ut statistical machine trans-
lation in ntcir-9 patentmt. In Proceedings of NTCIR-9
Workshop Meeting, pages 585?592, December.
Xianchao Wu, Takuya Matsuzaki, and Jun?ichi Tsujii.
2010. Fine-grained tree-to-string translation rule ex-
traction. In Proceedings of ACL, pages 325?334, July.
Xianchao Wu, Takuya Matsuzaki, and Jun?ichi Tsujii.
2011a. Effective use of function words for rule gen-
eralization in forest-based translation. In Proceedings
of ACL-HLT, pages 22?31, June.
Xianchao Wu, Takuya Matsuzaki, and Jun?ichi Tsujii.
2011b. Smt systems in the university of tokyo for
ntcir-9 patentmt. In Proceedings of NTCIR-9 Work-
shop Meeting, pages 666?672, December.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403.
Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maxi-
mum entropy based phrase reordering model for statis-
tical machine translation. In Proceedings of COLING-
ACL, pages 521?528, July.
Hui Zhang, Min Zhang, Haizhou Li, Aiti Aw, and
Chew Lim Tan. 2009. Forest-based tree sequence
to string translation model. In Proceedings of ACL-
IJCNLP, pages 172?180, Suntec, Singapore, August.
132
Proceedings of SSST-6, Sixth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 57?66,
Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational Linguistics
Head Finalization Reordering for Chinese-to-Japanese
Machine Translation
Han Dan+ Katsuhito Sudoh? Xianchao Wu??
Kevin Duh?? Hajime Tsukada? Masaaki Nagata?
+The Graduate University For Advanced Studies, Tokyo, Japan
?NTT Communication Science Laboratories, NTT Corporation
+handan@nii.ac.jp, ?wuxianchao@baidu.com, ?kevinduh@is.naist.jp
?{sudoh.katsuhito, tsukada.hajime, nagata.masaaki}@lab.ntt.co.jp
Abstract
In Statistical Machine Translation, reorder-
ing rules have proved useful in extracting
bilingual phrases and in decoding during
translation between languages that are struc-
turally different. Linguistically motivated
rules have been incorporated into Chinese-
to-English (Wang et al, 2007) and English-
to-Japanese (Isozaki et al, 2010b) transla-
tion with significant gains to the statistical
translation system. Here, we carry out a lin-
guistic analysis of the Chinese-to-Japanese
translation problem and propose one of the
first reordering rules for this language pair.
Experimental results show substantially im-
provements (from 20.70 to 23.17 BLEU)
when head-finalization rules based on HPSG
parses are used, and further gains (to 24.14
BLEU) were obtained using more refined
rules.
1 Introduction
In state-of-the-art Statistical Machine Translation
(SMT) systems, bilingual phrases are the main
building blocks for constructing a translation given
a sentence from a source language. To extract
those bilingual phrases from a parallel corpus,
the first step is to discover the implicit word-
to-word correspondences between bilingual sen-
tences (Brown et al, 1993). Then, a symmetriza-
tion matrix is built (Och and Ney, 2004) by us-
ing word-to-word alignments, and a wide variety
?Now at Baidu Japan Inc.
? Now at Nara Institute of Science and Technology
(NAIST)
of heuristics can be used to extract the bilingual
phrases (Zens et al, 2002; Koehn et al, 2003).
This method performs relatively well when the
source and the target languages have similar word
order, as in the case of French, Spanish, and En-
glish. However, when translating between lan-
guages with very different structures, as in the case
of English and Japanese, or Japanese and Chinese,
the quality of extracted bilingual phrases and the
overall translation quality diminishes.
In the latter scenario, a simple but effective strat-
egy to cope with this problem is to reorder the
words of sentences in one language so that it re-
sembles the word order of another language (Wu
et al, 2011; Isozaki et al, 2010b). The advan-
tages of this strategy are two fold. The first ad-
vantage is at the decoding stage, since it enables
the translation to be constructed almost monoton-
ically. The second advantage is at the training
stage, since automatically estimated word-to-word
alignments are likely to be more accurate and sym-
metrization matrices reveal more evident bilingual
phrases, leading to the extraction of better quality
bilingual phrases and cleaner phrase tables.
In this work, we focus on Chinese-to-Japanese
translation, motivated by the increasing interaction
between these two countries and the need to im-
prove direct machine translation without using a
pivot language. Despite the countries? close cul-
tural relationship, their languages significantly dif-
fer in terms of syntax, which poses a severe diffi-
culty in statistical machine translation. The syntac-
tic relationship of this language pair has not been
carefully studied before in the machine translation
57
field, and our work aims to contribute in this direc-
tion as follows:
? We present a detailed syntactic analysis of
several reordering issues in Chinese-Japanese
translation using the information provided by
an HPSG-based deep parser.
? We introduce novel reordering rules based on
head-finalization and linguistically inspired
refinements to make words in Chinese sen-
tences resemble Japanese word order. We em-
pirically show its effectiveness (e.g. 20.70 to
24.23 BLEU improvement).
The paper is structured as follows. Section 2 in-
troduces the background and gives an overview of
similar techniques related to this work. Section 3
describes the proposed method in detail. Exper-
imental evaluation of the performance of the pro-
posed method is described in section 4. There is an
error analysis on the obtained results in section 5.
Conclusions and a short description on future work
derived from this research are given in the final
section.
2 Background
2.1 Head Finalization
The structure of languages can be characterized
by phrase structures. The head of a phrase is the
word that determines the syntactic category of the
phrase, and its modifiers (also called dependents)
are the rest of the words within the phrase. In En-
glish, the head of a phrase can be usually found
before its modifiers. For that reason, English is
called a head-initial language (Cook and Newson,
1988). Japanese, on the other hand, is head-final
language (Fukui, 1992), since the head of a phrase
always appears after its modifiers.
In certain applications, as in the case of ma-
chine translation, word reordering can be a promis-
ing strategy to ease the task when working with
languages with different phrase structures like En-
glish and Japanese. Head Finalization is a success-
ful syntax-based reordering method designed to re-
order sentences from a head-initial language to re-
semble the word order in sentences from a head-
final language (Isozaki et al, 2010b). The essence
of this rule is to move the syntactic heads to the
end of its dependency by swapping child nodes in
a phrase structure tree when the head child appears
before the dependent child.
Isozaki et al (2010b) proposed a simple method
of Head Finalization, by using an HPSG-based
deep parser for English (Miyao and Tsujii, 2008)
to obtain phrase structures and head information.
The score results from several mainstream evalua-
tion methods indicated that the translation quality
had been improved; the scores of Word Error Rate
(WER) and Translation Edit Rate (TER) (Snover
et al, 2006) had especially been greatly reduced.
2.2 Chinese Deep Parsing
Syntax-based reordering methods need parsed sen-
tences as input. Isozaki et al (2010b) used Enju,
an HPSG-based deep parser for English, but they
also discussed using other types of parsers, such
as word dependency parsers and Penn Treebank-
style parsers. However, to use word dependency
parsers, they needed an additional heuristic rule to
recover phrase structures, and Penn Treebank-style
parsers are problematic because they output flat
phrase structures (i.e. a phrase may have multiple
dependents, which causes a problem of reorder-
ing within a phrase). Consequently, compared to
different types of parsers, Head-Final English per-
forms the best on the basis of English Enju?s pars-
ing result.
In this paper, we follow their observation, and
use the HPSG-based parser for Chinese (Chinese
Enju) (Yu et al, 2011) for Chinese syntactic pars-
ing. Since Chinese Enju is based on the same pars-
ing model as English Enju, it provides rich syn-
tactic information including phrase structures and
syntactic/semantic heads.
Figure 1 shows an example of an XML output
from Chinese Enju for the sentence ?wo (I) qu (go
to) dongjing (Tokyo) he (and) jingdu (Ky-
oto).? The label <cons> and <tok> represent
the non-terminal nodes and terminal nodes, respec-
tively. Each node is identified by a unique ?id?
and has several attributes. The attribute ?head?
indicates which child node is the syntactic head.
In this figure, <head=?c4? id=?c3?> means that
the node that has id=?c4? is the syntactic head of
the node that has id=?c3?.
58
Figure 1: An XML output for a Chinese sentence from
Chinese Enju. For clarity, we only draw information
related to the phrase structure and the heads.
2.3 Related Work
Reordering is a popular strategy for improving
machine translation quality when source and tar-
get languages are structurally very different. Re-
searchers have approached the reordering problem
in multiple ways. The most basic idea is pre-
ordering (Xia and McCord, 2004; Collins et al,
2005), that is, to do reordering during preprocess-
ing time, where the source side of the training and
development data and sentences from a source lan-
guage that have to be translated are first reordered
to ease the training and the translation, respec-
tively. In (Xu et al, 2009), authors used a depen-
dency parser to introduce manually created pre-
ordering rules to reorder English sentences when
translating into five different SOV(Subject-Object-
Verb) languages. Other authors (Genzel, 2010; Wu
et al, 2011) use automatically generated rules in-
duced from parallel data. Tillmann (2004) used a
lexical reordering model, and Galley et al (2004)
followed a syntactic-based model.
In this work, however, we are centered in the
design of manual rules inspired by the Head Final-
ization (HF) reordering (Isozaki et al, 2010b). HF
reordering is one of the simplest methods for pre-
ordering that significantly improves word align-
ments and leads to a better translation quality. Al-
though the method is limited to translation where
the target language is head-final, it requires neither
training data nor fine-tuning. To our knowledge,
HF is the best method to reorder languages when
translating into head-final languages like Japanese.
The implementation of HF method for English-
to-Japanese translation appears to work well. A
reasonable explanation for this is the close match
between the concept of ?head? in this language
pair. However, for Chinese-to-Japanese, there are
differences in the definitions of numbers of impor-
tant syntactic concepts, including the definition of
the syntactic head. We concluded that the diffi-
culties we encountered in using HF to Chinese-to-
Japanese translation were the result of these differ-
ences in the definition of ?head?. As we believe
that such differences are also likely to be observed
in other language pairs, the present work is gener-
ally important for head-initial to head-final trans-
lation as it shows a systematic linguistic analysis
that consistently improves the effectivity of the HF
method.
3 Syntax-based Reordering Rules
This section describes our method for syntax-
based reordering for Chinese-to-Japanese transla-
tion. We start by introducing Head Finalization
for Chinese (HFC), which is a simple adaptation
of Isozaki et al (2010b)?s method for English-to-
Japanese translation. However, we found that this
simple method has problems when applied to Chi-
nese, due to peculiarities in Chinese syntax. In
Section 3.2, we analyze several distinctive cases of
the problem in detail. And following this analysis,
Section 3.3 proposes a refinement of the original
HFC, with a couple of exception rules for reorder-
ing.
3.1 Head Finalization for Chinese (HFC)
Since Chinese and English are both known to be
head-initial languages1, the reordering rule intro-
duced in (Isozaki et al, 2010b) ideally would re-
order Chinese sentences to follow the word order
1As Gao (2008) summarized, whether Chinese is a head-
initial or a head-final language is open for debate. Neverthe-
less, we take the view that most Chinese sentence structures
are head-initial since the written form of Chinese mainly be-
haves as an head-initial language.
59
Figure 2: Simple example for Head-Final Chinese. The left figure shows the parsing tree of the original sentence
and its English translation. The right figure shows the reordered sentence along with its Japanese translation.
( ?*? indicate the syntactic head).
of their Japanese counterparts.
Figure 2 shows an example of a head finalized
Chinese sentence based on the output from Chi-
nese Enju shown in Figure 1. Notice that the
coordination exception rule described in (Isozaki
et al, 2010b) also applies to Chinese reordering.
This exception rule says that child nodes are not
swapped if the node is a coordination2. Another
exception rule is for punctuation symbols, which
are also preserved in their original order. In this
case, as can be seen in the example in Figure 2, the
nodes of c3, c6, and c8 had not been swapped with
their dependency. In this account, only the verb
?qu? had been moved to the end of the sentence,
following the same word order as its Japanese
translation.
3.2 Discrepancies in Head Definition
Head Finalization relies on the idea that head-
dependent relations are largely consistent among
different languages while word orders are differ-
ent. However, in Chinese, there has been much
debate on the definition of head3, possibly because
Chinese has fewer surface syntactic features than
other languages like English and Japanese. This
causes some discrepancies between the definitions
2Coordination is easily detected in the output of
Enju; it is marked by the attributes xcat="COOD" or
schema="coord-left/right" as shown in Figure 1.
3In this paper, we only consider the syntactic head.
of the head in Chinese and Japanese, which leads
to undesirable reordering of Chinese sentences.
Specifically, in preliminary experiments we ob-
served unexpected reorderings that are caused by
the differences in the head definitions, which we
describe below.
3.2.1 Aspect Particle
Although Chinese has no syntactic tense marker,
three aspect particles following verbs can be used
to identify the tense semantically. They are ?le0?
(did), ?zhe0? (doing), and ?guo4? (done), and
their counterparts in Japanese are ?ta?, ?teiru?,
and ?ta?, respectively. Both the first word and
third word can represent the past tense, but the
third one is more often used in the past perfect.
The Chinese parser4 treated aspect particles as
dependents of verbs, whereas their Japanese coun-
terparts are identified as the head. For exam-
ple in Table 15, ?qu? (go) and ?guo? (done)
aligned with ?i? and ?tta?, respectively. How-
ever, since ?guo? is treated as a dependent of
?qu?, by directly implementing the Head Final
Chinese (HFC), the sentence will be reordered like
4The discussions in this section presuppose the syntactic
analysis done by Chinese Enju, but most of the analysis is
consistent with the common explanation for Chinese syntax.
5English translation (En); Chinese original sentence
(Ch); reordered Chinese by Head-Final Chinese (HFC); re-
ordered Chinese by Refined Head-Final Chinese (R-HFC)
and Japanese translation (Ja).
60
HFC in Table 1, which does not follow the word
order of the Japanese (Ja) translation. In contrast,
the reordered sentence from refined-HFC (R-HFC)
can be translated monotonically.
En I have been to Tokyo.
Ch wo qu guo dongjing.
HFC wo dongjing guo qu.
R-HFC wo dongjing qu guo.
Ja watashi (wa) Tokyo (ni) i tta.
Table 1: An example for Aspect Particle. Best word
alignment Ja-Ch (En): ?watashi? ? ?wo?(I); ?Tokyo? ?
?dongjing? (Tokyo); ?i? ? ?qu? (been); ?tta? ? ?guo?
(have).
3.2.2 Adverbial Modifier ?bu4?
Both in Chinese and Japanese, verb phrase mod-
ifiers typically occur in pre-verbal positions, espe-
cially when the modifiers are adverbs. Since ad-
verbial modifiers are dependents in both Chinese
and Japanese, head finalization works perfectly for
them. However, there is an exceptional adverb,
?bu4?, which means negation and is usually trans-
lated into ?nai?, which is always at the end of the
sentence in Japanese and thus is the head. For ex-
ample in Table 2, the word ?kan? (watch) will be
identified as the head and the word ?bu? is its de-
pendent; on the contrary, in the Japanese transla-
tion (Ja), the word ?nai?, which is aligned with
?bu?, will be identified as the head. Therefore,
the Head Final Chinese is not in the same order,
but the reordered sentence by R-HFC obtained the
same order with the Japanese translation.
En I do not watch TV.
Ch wo bu kan dianshi.
HFC wo dianshi bu kan.
R-HFC wo dianshi kan bu.
Ja watashi (wa) terebi (wo) mi nai.
Table 2: An example for Adverbial Modifier bu4.
Best word alignment Ja-Ch (En): ?watashi? ? ?wo? (I);
?terebi? ? ?dianshi? (TV); ?mi? ? ?kan? (watch); ?nai?
? ?bu? (do not).
3.2.3 Sentence-final Particle
Sentence-final particles often appear at the end
of a sentence to express a speaker?s attitude:
e.g. ?ba0, a0? in Chinese, and ?naa, nee? in
Japanese. Although they appear in the same posi-
tion in both Chinese and Japanese, in accordance
with the differences of head definition, they are
identified as the dependent in Chinese while they
are the head in Japanese. For example in Table 3,
since ?a0? was identified as the dependent, it had
been reordered to the beginning of the sentence
while its Japanese translation ?nee? is at the end
of the sentence as the head. Likewise, by refining
the HFC, we can improve the word alignment.
En It is good weather.
Ch tianqi zhenhao a.
HFC a tianqi zhenhao.
R-HFC tianqi zhenhao a.
Ja ii tennki desu nee.
Table 3: An example for Sentence-final Particle.
Best word alignment Ja-Ch (En): ?tennki? ? ?tianqi?
(weather); ?ii? ? ?zhenhao? (good); ?nee? ? ?a? (None).
3.2.4 Et cetera
In Chinese, there are two expressions for rep-
resenting the meaning of ?and other things? with
one Chinese character: ?deng3? and ?deng3
deng3?, which are both identified as dependent
of a noun. In contrast, in Japanese, ?nado? is al-
ways the head because it appears as the right-most
word in a noun phrase. Table 4 shows an example.
En Fruits include apples, etc.
Ch shuiguo baokuo pingguo deng.
HFC shuiguo deng pingguo baokuo.
R-HFC shuiguo pingguo deng baokuo.
Ja kudamono (wa) ringo nado (wo)
fukunde iru.
Table 4: An example for Et cetera. Best word alignment
Ja-Ch (En): ?kudamono? ? ?shuiguo? (Fruits); ?ringo?
? ?pingguo? (apples); ?nado? ? ?deng? (etc.); ?fukunde
iru? ? ?baokuo? (include).
61
AS Aspect particle
SP Sentence-final particle
ETC et cetera (i.e. deng3 and deng3 deng3)
IJ Interjection
PU Punctuation
CC Coordinating conjunction
Table 5: The list of POSs for exception reordering rules
3.3 Refinement of HFC
In the preceding sections, we have discussed syn-
tactic constructions that cause wrong application
of Head Finalization to Chinese sentences. Fol-
lowing the observations, we propose a method to
improve the original Head Finalization reordering
rule to obtain better alignment with Japanese.
The idea is simple: we define a list of POSs,
and when we find one of them as a dependent
child of the node, we do not apply reordering. Ta-
ble 5 shows the list of POSs we define in the cur-
rent implementation6. While interjections are not
discussed in detail, we should obviously not re-
order to interjections because they are position-
independent. The rules for PU and CC are ba-
sically equivalent to the exception rules proposed
by (Isozaki et al, 2010b).
4 Experiments
The corpus we used as training data comes
from the China Workshop on Machine Transla-
tion (CWMT) (Zhao et al, 2011). This is a
Japanese-Chinese parallel corpus in the news do-
main, containing 281, 322 sentence pairs. We also
collected another Japanese-Chinese parallel cor-
pus from news containing 529, 769 sentences and
merged it with the CWMT corpus to create an ex-
tended version of the CWMT corpus. We will re-
fer to this corpus as ?CWMT ext.? We split an in-
verted multi-reference set into a development and a
test set containing 1, 000 sentences each. In these
two sets, the Chinese input was different, but the
Japanese reference was identical. We think that
this split does not pose any severe problem to the
comparison fairness of the experiment, since no
new phrases are added during tuning and the ex-
perimental conditions remain equal for all tested
6The POSs are from Penn Chinese Treebank.
Ch Ja
CWMT
Sentences 282K
Run. words 2.5M 3.2M
Avg. sent. leng. 8.8 11.5
Vocabulary 102K 42K
CWMT ext.
Sentences 811K
Run. words 14.7M 17M
Avg. sent. leng. 18.1 20.9
Vocabulary 249K 95K
Dev.
Sentences 1000
Run. words 29.9K 35.7K
Avg. sent. leng. 29.9 35.7
OoV w.r.t. CWMT 485 106
OoV w.r.t. CWMT ext. 244 53
Test
Sentences 1000
Run. words 25.8K 35.7K
Avg. sent. leng. 25.8 35.7
OoV w.r.t. CWMT 456 106
OoV w.r.t. CWMT ext. 228 53
Table 6: Characteristics of CWMT and extended
CWMT Chinese-Japanese corpus. Dev. stands for De-
velopment, OoV for ?Out of Vocabulary? words, K for
thousands of elements, and M for millions of elements.
Data statistics were collected after tokenizing.
methods. Detailed Corpus statistics can be found
in Table 6.
To parse Chinese sentences, we used Chinese
Enju (Yu et al, 2010), an HPSG-based parser
trained with the Chinese HPSG treebank converted
from Penn Chinese Treebank. Chinese Enju re-
quires segmented and POS-tagged sentences to
do parsing. We used the Stanford Chinese seg-
menter (Chang et al, 2008) and Stanford POS-
tagger (Toutanova et al, 2003) to obtain the seg-
mentation and POS-tagging of the Chinese side of
the training, development, and test sets.
The baseline system was trained following
the instructions of recent SMT evaluation cam-
paigns (Callison-Burch et al, 2010) by using the
MT toolkit Moses (Koehn et al, 2007) in its de-
fault configuration. Phrase pairs were extracted
from symmetrized word alignments and distor-
tions generated by GIZA++ (Och and Ney, 2003)
using the combination of heuristics ?grow-diag-
final-and? and ?msd-bidirectional-fe?. The lan-
guage model was a 5-gram language model es-
timated on the target side of the parallel cor-
pora by using the modified Kneser-Ney smooth-
ing (Chen and Goodman, 1999) implemented in
62
the SRILM (Stolcke, 2002) toolkit. The weights
of the log-linear combination of feature functions
were estimated by using MERT (Och, 2003) on the
development set described in Table 6.
The effectiveness of the reorderings proposed
in Section 3.3 was assessed by using two preci-
sion metrics and two error metrics on translation
quality. The first evaluation metric is BLEU (Pap-
ineni et al, 2002), a very common accuracy metric
in SMT that measures N -gram precision, with a
penalty for too short sentences. The second eval-
uation metric was RIBES (Isozaki et al, 2010a), a
recent precision metric used to evaluate translation
quality between structurally different languages. It
uses notions on rank correlation coefficients and
precision measures. The third evaluation metric is
TER (Snover et al, 2006), another error metric that
computes the minimum number of edits required
to convert translated sentences into its correspond-
ing references. Possible edits include insertion,
deletion, substitution of single words, and shifts of
word sequences. The fourth evaluation metric is
WER, an error metric inspired in the Levenshtein
distance at word level. BLEU, WER, and TER
were used to provide a sense of comparison but
they do not significantly penalize long-range word
order errors. For this reason, RIBES was used to
account for this aspect of translation quality.
The baseline system was trained and tuned us-
ing the same configuration setup described in this
section, but no reordering rule was implemented at
the preprocessing stage.
Three systems have been run to translate the test
set for comparison when the systems were trained
using the two training data sets. They are the
baseline system, the system consisting in the na??ve
implementation of HF reordering, and the system
with refined HFC reordering rules. Assessment of
translation quality can be found in Table 7.
As can be observed in Table 7, the translation
quality, as measured by precision and error met-
rics, was consistently and significantly increased
when the HFC reordering rule was used and was
significantly improved further when the refinement
proposed in this work was used. Specifically, the
BLEU score increased from 19.94 to 20.79 when
the CWMT corpus was used, and from 23.17 to
24.14 when the extended CWMT corpus was used.
AS SP ETC IJ PU COOD
3.8% 0.8% 1.3% 0.0%* 21.0% 38.3%
Table 8: Weighted recall of each exception rule during
reordering on CWMT ext. training data, dev data, and
test data. (* actual value 0.0016%.)
Table 8 shows the recall of each exception rule
listed in Section 3, and was computed by counting
the times an exception rule was triggered divided
by the number of times the head finalization rule
applied. Data was collected for CWMT ext. train-
ing, dev and test sets. Although the exception rules
related to aspect particles, Et cetera, sentence-final
particles and interjections have a comparatively
lower frequency of application than punctuation
or coordination exception rules, the improvements
they led to are significant.
5 Error Analysis
In Section 3 we have analyzed syntactic differ-
ences between Chinese and Japanese that led to
the design of an effective refinement. A manual
error analysis of the results of our refined reorder-
ing rules showed that some more reordering issues
remain and, although they are not side effects of
our proposed rule, they are worth mentioning in
this separate section.
5.1 Serial Verb Construction
Serial verb construction is a phenomenon occur-
ring in Chinese, where several verbs are put to-
gether as one unit without any conjunction be-
tween them. The relationship between these
verbs can be progressive or parallel. Apparently,
Japanese has a largely corresponding construc-
tion, which indicates that no reordering should
be applied. An example to illustrate this fact in
Chinese is ?weishi (maintain) shenhua (deepen)
zhongriguanxi (Japan-China relations) de
(of) gaishan (improvement) jidiao (basic
tone).?7 The two verbs ?weishi? (in Japanese,
iji) and ?shenhua? (in Japanese, shinka) are
used together, and they follow the same order as
in Japanese: ?nicchukankei (Japan-China re-
7English translation: Maintain and deepen the improved
basic tone of Japan-China relations.
63
CWMT CWMT ext.
BLEU RIBES TER WER BLEU RIBES TER WER
baseline 16.74 71.24 70.86 77.45 20.70 74.21 66.10 72.36
HFC 19.94 73.49 65.19 71.39 23.17 75.35 61.38 67.74
refined HFC 20.79 75.09 64.91 70.39 24.14 77.17 59.67 65.31
Table 7: Evaluation of translation quality of a test set when CWMT and CWMT extended corpus were used for
training. Results are given in terms of BLEU, RIBES, TER, and WER for baseline, head finalization, and proposed
refinement of head finalization reordering rules.
lations) no (of) kaizan (improvement) kityo
(basic tone) wo iji (maintain) shinka (deepen)
suru (do).?
5.2 Complementizer
A ?complementizer? is a particle used to intro-
duce a complement. In English, a very common
complementizer is the word ?that? when making a
clausal complement, while in Chinese it can de-
note other types of word, such as verbs, adjec-
tives or quantifiers. The complementizer is iden-
tified as the dependent of the verb that it modi-
fies. For instance, a Chinese sentence: ?wo (I)
mang wan le (have finished the work).? This
can be translated into Japanese: ?watashi (I) wa
shigoto (work) wo owa tta (have finished).? In
Chinese, the verb ?mang? is the head while ?wan?
is the complementizer, and its Japanese counter-
part ?owa tta? has the same word order.
However, during the reordering, ?mang? will be
placed at the end of the sentence and ?wan? in the
beginning, leading to an inconsistency with respect
to the Japanese translation where the complemen-
tizer ?tta? is the head.
5.3 Verbal Nominalization and Nounal
Verbalization
As discussed by Guo (2009), compared to English
and Japanese, Chinese has little inflectional mor-
phology, that is, no inflection to denote tense, case,
etc. Thus, words are extremely flexible, making
verb nominalization and noun verbalization appear
frequently and commonly without any conjugation
or declension. As a result, it is difficult to do dis-
ambiguation during POS tagging and parsing. For
example, the Chinese word ?kaifa? may have
two syntactic functions: verb (develop) and noun
(development). Thus, it is difficult to reliably tag
without considering the context. In contrast, in
Japanese, ?suru? can be used to identify verbs.
For example, ?kaihatu suru? (develop) is a
verb and ?kaihatu? (development) is a noun.
This ambiguity is prone to not only POS tagging
error but also parsing error, and thus affects the
identification of heads, which may lead to incor-
rect reordering.
5.4 Adverbial Modifier
Unlike the adverb ?bu4? we discussed in Sec-
tion 3.2, the ordinary adverbial modifier comes
directly before the verb it modifies both in Chi-
nese and Japanese, but not in English. Nev-
ertheless, in accordance with the principle of
identifying the head for Chinese, the adverb
will be treated as the dependent and it will
not be reordered following the verb it modi-
fied. As a result, the alignment between adverbs
and verbs is non-monotonic. This can be ob-
served in the Chinese sentence ?guojia (coun-
try) yanli (severely) chufa (penalize) jiage
(price) weifa (violation) xingwei (behavior)?8,
and its Japanese translation: ?kuni (country) wa
kakaku (price) no ihou (violation) koui (be-
havior) wo kibisiku (severely) syobatu (penal-
ize).? Both in Chinese and Japanese, the adverbial
modifier ?yanli? and ?kibisiku? are directly
in front of the verb ?chufa? and ?syobatu?, re-
spectively. However, the verb in Chinese is identi-
fied as the head and will be reordered to the end of
the sentence without the adverb.
8English translation: The country severely penalizes vio-
lations of price restrictions.
64
5.5 POS tagging and Parsing Errors
There were word reordering issues not caused
solely by differences in syntactic structures. Here
we summarize two that are difficult to remedy dur-
ing reordering and that are hard to avoid since re-
ordering rules are highly dependent on the tagger
and parser.
? POS tagging errors
In Chinese, for example, the word ?Iran?
was tagged as ?VV? or ?JJ? instead of ?NR?.
This led to identifying ?Iran? as a head in
accordance with the head definition in Chi-
nese, and it was reordered undesirably.
? Parsing errors
For example, in the Chinese verb phrase
?touzi (invest) 20 yi (200 million)
meiyuan (dollars)?, ?20? and ?yi? were
identified as dependent of ?touzi? and
?meiyuan?, respectively, which led to an
unsuitable reordering for posterior word
alignment.
6 Conclusion and Future Work
In the present work, we have proposed novel
Chinese-to-Japanese reordering rules inspired
in (Isozaki et al, 2010b) based on linguistic analy-
sis on Chinese HPSG and differences among Chi-
nese and Japanese. Although a simple implemen-
tation of HF to reorder Chinese sentences per-
forms well, translation quality was substantially
improved further by including linguistic knowl-
edge into the refinement of the reordering rules.
In Section 5, we found more patterns on reorder-
ing issues when reordering Chinese sentences to
resemble Japanese word order. The extraction of
those patterns and their effective implementation
may lead to further improvements in translation
quality, so we are planning to explore this possi-
bility.
In this work, syntactic information from a deep
parser has been used to reorder words better. We
believe that using semantic information can fur-
ther increase the expressive power of reordering
rules. With that objective, Chinese Enju can be
used since it provides the semantic head of nodes
and can interpret sentences by using their semantic
dependency.
Acknowledgments
This work was mainly developed during an intern-
ship at NTT Communication Science Laborato-
ries. We would like to thank Prof. Yusuke Miyao
for his invaluable support on this work.
References
P.F. Brown, S.A. Della Pietra, V.J. Della Pietra, and
R.L. Mercer. 1993. The mathematics of ma-
chine translation. In Computational Linguistics, vol-
ume 19, pages 263?311, June.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, and Omar Zaidan, editors. 2010. Pro-
ceedings of the joint 5th workshop on Statistical Ma-
chine Translation and MetricsMATR. Association
for Computational Linguistics, July.
Pi-Chuan Chang, Michel Galley, and Christopher D.
Manning. 2008. Optimizing Chinese word seg-
mentation for machine translation performance. In
Proceedings of the 3rd Workshop on SMT, pages
224?232, Columbus, Ohio. Association for Compu-
tational Linguistics.
Stanley F. Chen and Joshua Goodman. 1999. An
empirical study of smoothing techniques for lan-
guage modeling. Computer Speech and Language,
4(13):359?393.
Michael Collins, Philipp Koehn, and Ivona Kuc?erova?.
2005. Clause restructuring for statistical machine
translation. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics,
ACL ?05, pages 531?540, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Vivian James Cook and Mark Newson. 1988. Chom-
sky?s Universal Grammar: An introduction. Oxford:
Basil Blackwell.
Naoki Fukui. 1992. Theory of Projection in Syntax.
CSLI Publisher and Kuroshio Publisher.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. Whats in a translation rule?
In Proceedings of HLT-NAACL.
Qian Gao. 2008. Word order in mandarin: Reading and
speaking. In Proceedings of the 20th North Ameri-
can Conference on Chinese Linguistics (NACCL-20),
volume 2, pages 611?626.
Dmitriy Genzel. 2010. Automatically learning source-
side reordering rules for large scale machine transla-
tion. In Proceedings of the 23rd International Con-
ference on Computational Linguistics, COLING ?10,
65
pages 376?384, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Yuqing Guo. 2009. Treebank-based acquisition of
Chinese LFG resources for parsing and generation.
Ph.D. thesis, Dublin City University.
Hideki Isozaki, Tsutomu Hirao, Kevin Duh, Katsuhito
Sudoh, and Hajime Tsukada. 2010a. Automatic
evaluation of translation quality for distant language
pairs. In Proceedings of Empirical Methods on Nat-
ural Language Processing (EMNLP).
Hideki Isozaki, Katsuhito Sudoh, Hajime Tsukada, and
Kevin Duh. 2010b. Head finalization: A simple re-
ordering rule for sov languages. In Proceedings of
WMTMetricsMATR, pages 244?251.
P. Koehn, F. J. Och, and D. Marcu. 2003. Sta-
tistical phrase-based translation. In Proceedings
HLT/NAACL?03, pages 48?54.
Philipp Koehn et al 2007. Moses: Open source toolkit
for statistical machine translation. In Proceedings
of the ACL Demo and Poster Sessions, 2007, pages
177?180, June 25?27.
Yusuke Miyao and Jun?ichi Tsujii. 2008. Feature for-
est models for probabilistic hpsg parsing. Computa-
tional Linguistics, 34:35?80, March.
F. J. Och and H. Ney. 2003. A systematic comparison
of various statistical alignment models. Computa-
tional Linguistics, 29(1):19?51.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine trans-
lation. Computational Linguistics.
Franz J. Och. 2003. Minimum error rate training for
statistical machine translation. In Proceedings of the
41st annual conference of the Association for Com-
putational Linguistics, 2003, pages 160?167, July 7?
12.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: A method for automatic eval-
uation of machine translation. In Proceedings of the
40th annual conference of the Association for Com-
putational Linguistics, 2002, pages 311?318, July 6?
12.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of Association for Machine Transla-
tion in the Americas, pages 223?231.
Andreas Stolcke. 2002. SRILM ? an extensible lan-
guage modeling toolkit. In Proceedings of the 7th
international conference on Spoken Language Pro-
cessing, 2002, pages 901?904, September 16?20.
Christoph Tillmann. 2004. A unigram orientation
model for statistical machine translation. In Pro-
ceedings of HLT-NAACL 2004: Short Papers, HLT-
NAACL-Short ?04, pages 101?104, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings OF HLT-NAACL, pages 252?259.
Chao Wang, Michael Collins, and Philipp Koehn.
2007. Chinese syntactic reordering for statistical
machine translation. In Proceedings of the 2007
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL), pages 737?
745, Prague, Czech Republic, June. Association for
Computational Linguistics.
Xianchao Wu, Katsuhito Sudoh, Kevin Duh, Hajime
Tsukada, and Masaaki Nagata. 2011. Extracting
pre-ordering rules from predicate-argument struc-
tures. In Proceedings of 5th International Joint Con-
ference on Natural Language Processing, pages 29?
37, Chiang Mai, Thailand, November. Asian Feder-
ation of Natural Language Processing.
Fei Xia and Michael McCord. 2004. Improving a sta-
tistical mt system with automatically learned rewrite
patterns. In Proceedings of the 20th international
conference on Computational Linguistics, COLING
?04, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Peng Xu, Jaeho Kang, Michael Ringgaard, and Franz
Och. 2009. Using a dependency parser to improve
smt for subject-object-verb languages. In Proceed-
ings of Human Language Technologies: The 2009
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
NAACL ?09, pages 245?253, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Kun Yu, Yusuke Miyao, Xiangli Wang, Takuya Mat-
suzaki, and Jun ichi Tsujii. 2010. Semi-
automatically developing chinese hpsg grammar
from the penn chinese treebank for deep parsing. In
COLING (Posters)?10, pages 1417?1425.
Kun Yu, Yusuke Miyao, Takuya Matsuzaki, Xiangli
Wang, and Junichi Tsujii. 2011. Analysis of the dif-
ficulties in chinese deep parsing. In Proceedings of
the 12th International Conference on Parsing Tech-
nologies, pages 48?57.
R. Zens, F.J. Och, and H. Ney. 2002. Phrase-based
statistical machine translation. In Proceedings of
KI?02, pages 18?32.
Hong-Mei Zhao, Ya-Juan Lv, Guo-Sheng Ben, Yun
Huang, and Qun Liu. 2011. Evaluation report
for the 7th china workshop on machine translation
(cwmt2011). The 7th China Workshop on Machine
Translation (CWMT2011).
66
