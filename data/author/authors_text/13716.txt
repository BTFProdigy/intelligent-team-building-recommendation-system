Coling 2010: Poster Volume, pages 1507?1514,
Beijing, August 2010
Predicting Discourse Connectives for Implicit Discourse Relation
Recognition
Zhi-Min Zhou and Yu Xu
East China Normal University
51091201052@ecnu.cn
Zheng-Yu Niu
Toshiba China R&D Center
zhengyu.niu@gmail.com
Man Lan and Jian Su
Institute for Infocomm Research
sujian@i2r.a-star.edu.sg
Chew Lim Tan
National University of Singapore
tancl@comp.nus.edu.sg
Abstract
Existing works indicate that the absence
of explicit discourse connectives makes
it difficult to recognize implicit discourse
relations. In this paper we attempt to
overcome this difficulty for implicit rela-
tion recognition by automatically insert-
ing discourse connectives between argu-
ments with the use of a language model.
Then we propose two algorithms to lever-
age the information of these predicted
connectives. One is to use these pre-
dicted implicit connectives as additional
features in a supervised model. The other
is to perform implicit relation recognition
based only on these predicted connectives.
Results on Penn Discourse Treebank 2.0
show that predicted discourse connectives
help implicit relation recognition and the
first algorithm can achieve an absolute av-
erage f-score improvement of 3% over a
state of the art baseline system.
1 Introduction
Discourse relation analysis is to automatically
identify discourse relations (e.g., explanation re-
lation) that hold between arbitrary spans of text.
This analysis may be a part of many natural lan-
guage processing systems, e.g., text summariza-
tion system, question answering system. If there
are discourse connectives between textual units
to explicitly mark their relations, the recognition
task on these texts is defined as explicit discourse
relation recognition. Otherwise it is defined as im-
plicit discourse relation recognition.
Previous study indicates that the presence of
discourse connectives between textual units can
greatly help relation recognition. In Penn Dis-
course Treebank (PDTB) corpus (Prasad et al,
2008), the most general senses, i.e., Comparison
(Comp.), Contingency (Cont.), Temporal (Temp.)
and Expansion (Exp.), can be disambiguated in
explicit relations with more than 90% f-scores
based only on the discourse connectives explicitly
used to signal the relation (Pitler and Nenkova.,
2009b).
However, for implicit relations, there are no
connectives to explicitly mark the relations, which
makes the recognition task quite difficult. Some of
existing works attempt to perform relation recog-
nition without hand-annotated corpora (Marcu
and Echihabi, 2002), (Sporleder and Lascarides,
2008) and (Blair-Goldensohn, 2007). They use
unambiguous patterns such as [Arg1, but Arg2]
to create synthetic examples of implicit relations
and then use [Arg1, Arg2] as an training example
of an implicit relation. Another research line is
to exploit various linguistically informed features
under the framework of supervised models, (Pitler
et al, 2009a) and (Lin et al, 2009), e.g., polarity
features, semantic classes, tense, production rules
of parse trees of arguments, etc.
Our study on PDTB test data shows that the av-
erage f-score for the most general 4 senses can
reach 91.8% when we simply mapped the ground
truth implicit connective of each test instance to
its most frequent sense. It indicates the impor-
tance of connective information for implicit rela-
tion recognition. However, so far there is no previ-
ous study attempting to use such kind of connec-
tive information for implicit relation. One possi-
1507
ble reason is that implicit connectives do not ex-
ist in unannotated real texts. Another evidence
of the importance of connectives for implicit re-
lations is shown in PDTB annotation. The PDTB
annotation consists of inserting a connective ex-
pression that best conveys the inferred relation by
the readers. Connectives inserted in this way to
express inferred relations are called implicit con-
nectives, which do not exist in real texts. These
evidences inspire us to consider two interesting re-
search questions:
(1) Can we automatically predict implicit connec-
tives between arguments?
(2) How to use the predicted implicit connectives
to build an automatic discourse relation analysis
system?
In this paper we address these two questions as
follows: (1) We insert appropriate discourse con-
nectives between two textual units with the use of
a language model. Here we train the language
model on large amount of raw corpora without
the use of any hand-annotated data. (2) Then we
present two algorithms to use these predicted con-
nectives for implicit relation recognition. One is
to use these connectives as additional features in a
supervised model. The other is to perform relation
recognition based only on these connectives.
We performed evaluation of the two algorithms
and a baseline system on PDTB 2.0 corpus. Ex-
perimental results showed that using predicted
discourse connectives as additional features can
significantly improve the performance of implicit
discourse relation recognition. Specifically, the
first algorithm achieved an absolute average f-
score improvement of 3% over a state of the art
baseline system.
The rest of this paper is organized as follows.
Section 2 describes the two algorithms for implicit
discourse relation recognition. Section 3 presents
experiments and results on PDTB data. Section
4 reviews related work. Section 5 concludes this
work.
2 Our Algorithms for Implicit Discourse
Relation Recognition
2.1 Prediction of implicit connectives
Explicit discourse relations are easily identifiable
due to the presence of discourse connectives be-
tween arguments. (Pitler and Nenkova., 2009b)
showed that in PDTB corpus, the most general
senses, i.e., Comparison (Comp.), Contingency
(Cont.), Temporal (Temp.) and Expansion (Exp.),
can be disambiguated in explicit relations with
more than 90% f-scores based only on discourse
connectives.
But for implicit relations, there are no connec-
tives to explicitly mark the relations, which makes
the recognition task quite difficult. PDTB data
provides implicit connectives that are inserted be-
tween paragraph-internal adjacent sentence pairs
not marked by any of explicit connectives. The
availability of ground-truth implicit connectives
makes it possible to evaluate the contribution of
these connectives for implicit relation recognition.
Our initial study on PDTB data show that the av-
erage f-score for the most general 4 senses can
reach 91.8% when we obtained the sense of each
test example by mapping each ground truth im-
plicit connective to its most frequent sense. We
see that connective information is an important
knowledge source for implicit relation recogni-
tion. However these implicit connectives do not
exist in real texts. In this paper we overcome this
difficulty by inserting a connective between two
arguments with the use of a language model.
Following the annotation scheme of PDTB, we
assume that each implicit connective takes two ar-
guments, denoted as Arg1 and Arg2. Typically,
there are two possible positions for most of im-
plicit connectives1, i.e., the position before Arg1
and the position between Arg1 and Arg2. Given a
set of possible implicit connectives {ci}, we gen-
erate two synthetic sentences, ci+Arg1+Arg2 and
Arg1+ci+Arg2 for each ci, denoted as Sci,1 and
Sci,2. Then we calculate the perplexity (an intrin-
sic score) of these sentences with the use of a lan-
guage model, denoted as PPL(Sci,j). According
1For parallel connectives, e.g., if . . . then. . . , the two con-
nectives will take the two arguments together, so there is only
one possible combination for connectives and arguments.
1508
to the value of PPL(Sci,j) (the lower the better),
we can rank these sentences and select the con-
nectives in top N sentences as implicit connec-
tives for this argument pair. The language model
may be trained on large amount of unannotated
corpora that can be cheaply acquired, e.g., North
American News corpus.
2.2 Using predicted implicit connectives as
additional features
We predict implicit connectives on both training
set and test set. Then we can use the predicted
implicit connectives as additional features for su-
pervised implicit relation recognition. Previous
works exploited various linguistically informed
features under the framework of supervised mod-
els. In this paper, we include 9 types of features
in our system due to their superior performance
in previous studies, e.g., polarity features, seman-
tic classes of verbs, contextual sense, modality,
inquirer tags of words, first-last words of argu-
ments, cross-argument word pairs, ever used in
(Pitler et al, 2009a), production rules of parse
trees of arguments used in (Lin et al, 2009), and
intra-argument word pairs inspired by the work of
(Saito et al, 2006).
Here we provide the details of the 9 features,
shown as follows:
Verbs: Similar to the work in (Pitler et al,
2009a), the verb features consist of the number of
pairs of verbs in Arg1 and Arg2 if they are from
the same class based on their highest Levin verb
class level (Dorr, 2001). In addition, the average
length of verb phrase and the part of speech tags
of main verb are also included as verb features.
Context: If the immediately preceding (or fol-
lowing) relation is an explicit, its relation and
sense are used as features. Moreover, we use an-
other feature to indicate if Arg1 leads a paragraph.
Polarity: We use the number of positive,
negated positive, negative and neutral words in ar-
guments and their cross product as features. For
negated positives, we locate the negated words in
text span and then define the closely behind posi-
tive word as negated positive.
Modality: We look for modal words including
their various tenses or abbreviation forms in both
arguments. Then we generate a feature to indicate
the presence or absence of modal words in both
arguments and their cross product.
Inquirer Tags: Inquirer Tags extracted from
General Inquirer lexicon (Stone et al, 1966) con-
tains positive or negative classification of words.
In fact, its fine-grained categories, such as Fall
versus Rise, or Pleasure versus Pain, can indi-
cate the relation between two words, especially
for verbs. So we choose the presence or absence
of 21 pair categories with complementary relation
in Inquirer Tags as features. We also include their
cross production as features.
FirstLastFirst3: We choose the first and last
words of each argument as features, as well as the
pair of first words, the pair of last words, and the
first 3 words in each argument. In addition, we ap-
ply Porter?s Stemmer (Porter, 1980) to each word
before preparation of these features.
Production Rule: According to (Lin et al,
2009), we extract all the possible production rules
from arguments, and check whether the rules ap-
pear in Arg1, Arg2 and both arguments. We re-
move the rules occurring less than 5 times in train-
ing data.
Cross-argument Word Pairs: We perform the
Porter?s stemming (Porter, 1980), and then group
all words from Arg1 and Arg2 into two sets W1
and W2 respectively. Then we generate any possi-
ble word pair (wi, wj) (wi ? W1, wj ? W2). We
remove the word pairs with less than 5 times.
Intra-argument Word Pairs: Let
Q1 = (q1, q2, . . . , qn) be the word se-
quence of Arg1. The intra-argument word
pairs for Arg1 is defined as WP1 =
((q1, q2), (q1, q3), . . . , (q1, qn), (q2, q3), . . . ,
(qn?1, qn)). We extract all the intra-argument
word pairs from Arg1 and Arg2 and remove word
pairs appearing less than 5 times in training data.
2.3 Relation recognition based only on
predicted implicit connectives
After the prediction of implicit connectives, we
can address the implicit relation recognition task
with the methods for explicit relation recogni-
tion due to the presence of implicit connectives,
e.g., sense classification based only on connec-
tives (Pitler and Nenkova., 2009b). The work of
(Pitler and Nenkova., 2009b) showed that most
1509
of connectives are unambiguous and it is possible
to obtain high performance in prediction of dis-
course sense due to the simple mapping relation
between connectives and senses. Given two ex-
amples:
(E1) She paid less on her dress, but it is very nice.
(E2) We have to harry up because the raining is
getting heavier and heavier.
The two connectives, i.e., but in E1 and because
in E2, convey Comparison and Contingency sense
respectively. In most cases, we can easily recog-
nize the relation sense by the appearance of dis-
course connective since it can be interpreted in
only one way. That means, the ambiguity of the
mapping between sense and connective is quite
few.
We count the frequency of sense tags for each
possible connective on PDTB training data for im-
plicit relation. Then we build a sense recognition
model by simply mapping each connective to its
most frequent sense. Here we do not perform con-
nective prediction on training data. During test-
ing, we use the language model to insert implicit
connectives into each test argument pair. Then we
perform relation recognition by mapping each im-
plicit connective to its most frequent sense.
3 Experiments and Results
3.1 Experiments
3.1.1 Data sets
In this work we used the PDTB 2.0 corpus for
evaluation of our algorithms. Following the work
of (Pitler et al, 2009a), we used sections 2-20 as
training set, sections 21-22 as test set, and sec-
tions 0-1 as development set for parameter opti-
mization. For comparison with the work of (Pitler
et al, 2009a), we ran four binary classification
tasks to identify each of the main relations (Cont.,
Comp., Exp., and Temp.) from the rest. For each
relation, we used equal numbers of positive and
negative examples as training data2. The negative
examples were chosen at random from sections 2-
20. We used all the instances in sections 21 and
22 as test set, so the test set is representative of
2Here the numbers of training and test instances for Ex-
pansion relation are different from those in (Pitler et al,
2009a). The reason is that we do not include instances of
EntRel as positive examples.
the natural distribution. The numbers of positive
and negative instances for each sense in different
data sets are listed in Table 1.
Table 1: Statistics of positive and negative sam-
ples in training, development and test sets for each
relation.
Relation Train Dev Test
Pos/Neg Pos/Neg Pos/Neg
Comp. 1927/1927 191/997 146/912
Cont. 3375/3375 292/896 276/782
Exp. 6052/6052 651/537 556/502
Temp. 730/730 54/1134 67/991
In this work we used LibSVM toolkit to con-
struct four linear SVM models for a baseline sys-
tem and the system in Section 2.2.
3.1.2 A baseline system
We first built a baseline system, which used 9
types of features listed in Section 2.2.
We tuned the numbers of firstLastFirst3, cross-
argument word pair, intra-argument word pair on
development set. Finally we set the frequency
threshold at 3, 5 and 5 respectively.
3.1.3 Prediction of implicit connectives
To predict implicit connectives, we adopt the
following two steps:(1) train a language model;
(2) select top N implicit connectives.
Step 1: We used SRILM toolkit to train the lan-
guage models on three benchmark news corpora,
i.e., New York part in the BLLIP North Ameri-
can News, Xin and Ltw parts of English Gigaword
(4th Edition). We also tried different values for
n in n-gram model. The parameters were tuned
on the development set to optimize the accuracy
of prediction. In this work we chose 3-gram lan-
guage model trained on NY corpus.
Step 2: We combined each instance?s Arg1 and
Arg2 with connectives extract from PDTB2 (100
in all). There are two types of connectives, sin-
gle connective (e.g. because and but) and paral-
lel connective (such as ?not only . . . , but also?).
Since discourse connectives may appear not only
ahead of the Arg1, but also between Arg1 and
Arg2, we considered this case. Given a set of pos-
sible implicit connectives {ci}, for single connec-
tive {ci}, we constructed two synthetic sentences,
ci+Arg1+Arg2 and Arg1+ci+Arg2. In case of
1510
parallel connective, we constructed one synthetic
sentence like ci1+Arg1+ci2+Arg2.
As a result, we can get 198 synthetic sentences
for each argument pair. Then we converted all
words to lower cases and used the language model
trained in the above step to calculate perplexity
on sentence level. The perplexity scores were
ranked from low to high. For example, we got the
perplexity (ppl) for two sentences as follows:
(1) but this is an old story, we?re talking about
years ago before anyone heard of asbestos having
any questionable properties.
ppl= 652.837
(2) this is an old story, but we?re talking about
years ago before anyone heard of asbestos having
any questionable properties.
ppl= 583.514
We considered the combination of connectives
and their position as final features like mid but,
first but, where the features are binary, that is, the
presence and absence of the specific connective.
According to the value of PPL(Sci,j) (the
lower the better), we selected the connectives in
top N sentences as implicit connectives for this
argument pair. In order to get the optimal N value,
we tried various values of N on development set
and selected the minimum value of N so that the
ground-truth connectives appeared in top N con-
nectives. The final N value is set to 60 based on
the trade-off between performance and efficiency.
3.1.4 Using predicted connectives as
additional features
This system combines the predicted implicit
connectives as additional features and the 9 types
of features in an supervised framework. The 9
types of features are listed as shown in Section 2.2
and tuned on development set.
We combined predicted connectives with the
best subset features from the development data set
with respect to f-score. In our experiment of se-
lecting best subset features, single features rather
than the combination of several features achieved
much higher scores. So we combine single fea-
tures with predicted connectives as final features.
3.1.5 Using only predicted connectives for
implicit relation recognition
We built two variants for the algorithm in Sec-
tion 2.3. One is to use the data for explicit re-
lations in PDTB sections 2-20 as training data.
The other is to use the data for implicit relations
in PDTB sections 2-20 as training data. Given
training data, we obtained the most frequent sense
for each connective appearing in the training data.
Then given test data, we recognized the sense of
each argument pair by mapping each predicted
connective to its most frequent sense. In this
work we conducted another experiment to see the
upper-bound performance of this algorithm. Here
we performed recognition based on ground-truth
implicit connectives and used the data for implicit
relations as training data.
3.2 Results
3.2.1 Result of baseline system
Table 2 summarizes the best performance
achieved by the baseline system in compari-
son with previous state-of-the-art performance
achieved in (Pitler et al, 2009a). The first two
lines in the table show their best results using sin-
gle feature and using combined feature subset. It
indicates that the performance of using combined
feature subset is higher than that using single fea-
ture alone.
From this table, we can find that our base-
line system has a comparable result on Contin-
gency and Temporal. On Comparison, our system
achieved a better performance around 9% f-score
higher than their best result. However, for Expan-
sion, they expanded both training and testing sets
by including EntRel relation as positive examples,
which makes it impossible to perform direct com-
parison. Generally, our baseline system is reason-
able and thus the consequent experiments on it are
reliable.
3.2.2 Result of algorithm 1: using predicted
connectives as additional features
Table 3 summarizes the best performance
achieved by the baseline system and the first al-
gorithm (i.e., baseline + Language Model) on test
set. The second and third column show the best
performance achieved by the baseline system and
1511
Table 2: Performance comparison of the baseline system with the system of (Pitler et al, 2009a) on test
set.
System Comp. vs. Not Cont. vs. Other Exp. vs. Other Temp. vs. Other
F1 (Acc) F1 (Acc) F1 (Acc) F1 (Acc)
Using the best single feature (Pitler et al, 2009a) 21.01(52.59) 36.75(62.44) 71.29(59.23) 15.93(61.20)
Using the best feature subset (Pitler et al, 2009a) 21.96(56.59) 47.13(67.30) 76.42(63.62) 16.76(63.49)
The baseline system 30.72(78.26) 45.38(40.17) 65.95(57.94) 16.46(29.96)
the first algorithm using predicted connectives as
additional features.
Table 3: Performance comparison of the algo-
rithm in Section 2.2 with the baseline system on
test set.
Rela- Features Baseline Baseline+LM
tion F1 (Acc) F1 (Acc)
Comp. Production Rule 30.72(78.26) 31.08(68.15)
Context 24.66(42.25) 27.64(53.97)
InquirerTags 23.31(73.25) 27.87(55.48)
Polarity 21.11(40.64) 23.64(52.36)
Modality 17.25(80.06) 26.17(55.20)
Verbs 25.00(53.50) 31.79(58.22)
Cont. Prodcution Rule 45.38(40.17) 47.16(48.96)
Context 37.61(44.70) 34.74(48.87)
Polarity 35.57(50.00) 43.33(33.74)
InquirerTags 38.04(41.49) 42.22(36.11)
Modality 32.18(66.54) 35.26(55.58)
Verbs 40.44(54.06) 42.04(32.23)
Exp. Context 48.34(54.54) 68.32(53.02)
FirstLastFirst3 65.95(57.94) 68.94(53.59)
InquirerTags 61.29(52.84) 68.49(53.21)
Modality 64.36(56.14) 68.9(52.55)
Polarity 49.95(50.38) 68.62(53.40)
Verbs 52.95(53.31) 70.11(54.54)
Temp. Context 13.52(64.93) 16.99(79.68)
FirstLastFirst3 15.75(66.64) 19.70(64.56)
InquirerTags 8.51(83.74) 19.20(56.24)
Modality 16.46(29.96) 19.97(54.54)
Polarity 16.29(51.42) 20.30(55.48)
Verbs 13.88(54.25) 13.53(61.34)
From this table, we found that this additional
feature obtained from language model showed
significant improvements in almost four relations.
Specifically, the top two improvements are on Ex-
pansion and Temporal relations, which improved
4.16% and 3.84% in f-score respectively. Al-
though on Comparison relation there is only a
slight improvement (+1.07%), our two best sys-
tems both got around 10% improvements of f-
score over a state-of-the-art system in (Pitler et al,
2009a). As a whole, the first algorithm achieved
3% improvement of f-score over a state of the art
baseline system. All these results indicate that
predicted implicit connectives can help improve
the performance.
3.2.3 Result of algorithm 2: using only
predicted connectives for implicit
relation recognition
Table 4 summarizes the best performance
achieved by the second algorithm in comparison
with the baseline system on test set.
The experiment showed that the baseline sys-
tem using just gold-truth implicit connectives can
achieve an f-score of 91.8% for implicit relation
recognition. It once again proved that implicit
connectives make significant contributions for im-
plicit relation recognition. This also encourages
our future work on finding the most suitable con-
nectives for implicit relation recognition.
From this table, we found that, using only pre-
dicted implicit connectives achieved an compara-
ble performance to (Pitler et al, 2009a), although
it was still a bit lower than our best baseline. But
we should bear in mind that this algorithm only
uses 4 features for implicit relation recognition
and these 4 features are easy computable and fast
run, which makes the system more practical in ap-
plication. Furthermore, compared with other al-
gorithms which require hand-annotated data for
training, the performance of this second algorithm
is acceptable if we take into account that no la-
beled data is used for model training.
3.3 Analysis
Experimental results on PDTB showed that using
the predicted implicit connectives significantly
improves the performance of implicit discourse
relation recognition. Our first algorithm achieves
an average f-score improvement of 3% over a
state of the art baseline system. Specifically, for
the relations: Comp., Cont., Exp., Temp., our
first algorithm can achieve 1.07%, 1.78%, 4.16%,
3.84% f-score improvements over a state of the
art baseline system. Since (Pitler et al, 2009a)
1512
Table 4: Performance comparison of the algorithm in Section 2.3 with the baseline system on test set.
System Comp. vs. Other Cont. vs. Other Exp. vs. Other Temp. vs. Other
F1 (Acc) F1 (Acc) F1 (Acc) F1 (Acc)
The baseline system 30.72(78.26) 45.38(40.17) 65.95(57.94) 16.46(29.96)
Our algorithm with training data for explicit relation 26.02(52.17) 35.72(51.70) 64.94(53.97) 13.76(41.97)
Our algorithm with training data for implicit relation 24.55(63.99) 16.26(70.79) 60.70(53.50) 14.75(70.51)
Sense recognition using gold-truth implicit connectives 94.08(98.30) 98.19(99.05) 97.79(97.64) 77.04(97.07)
used different selection of instances for Expan-
sion sense3, we cannot make a direct compari-
son. However, we achieve the best f-score around
70%, which provide 5% improvements over our
baseline system. On the other hand, the second
proposed algorithm using only predicted connec-
tives still achieves promising results for each rela-
tion. Specifically, the model for the Comparison
relation achieves an f-score of 26.02% (5% over
the previous work in (Pitler et al, 2009a)). Fur-
thermore, the models for Contingency and Tem-
poral relation achieve 35.72% and 13.76% f-score
respectively, which are comparable to the previ-
ous work in (Pitler et al, 2009a). The model for
Expansion relation obtains an f-score of 64.95%,
which is only 1% less than our baseline system
which consists of ten thousands of features.
4 Related Work
Existing works on automatic recognition of dis-
course relations can be grouped into two cat-
egories according to whether they used hand-
annotated corpora.
One research line is to perform relation recog-
nition without hand-annotated corpora.
(Marcu and Echihabi, 2002) used a pattern-
based approach to extract instances of discourse
relations such as Contrast and Elaboration from
unlabeled corpora. Then they used word-pairs be-
tween two arguments as features for building clas-
sification models and tested their model on artifi-
cial data for implicit relations.
There are other efforts that attempt to extend the
work of (Marcu and Echihabi, 2002). (Saito et al,
2006) followed the method of (Marcu and Echi-
habi, 2002) and conducted experiments with com-
bination of cross-argument word pairs and phrasal
3They expanded the Expansion data set by adding ran-
domly selected EntRel instances by 50%, which is consid-
ered to significantly change data distribution.
patterns as features to recognize implicit relations
between adjacent sentences in a Japanese corpus.
They showed that phrasal patterns extracted from
a text span pair provide useful evidence in the re-
lation classification. (Sporleder and Lascarides,
2008) discovered that Marcu and Echihabi?s mod-
els do not perform as well on implicit relations as
one might expect from the test accuracies on syn-
thetic data. (Blair-Goldensohn, 2007) extended
the work of (Marcu and Echihabi, 2002) by re-
fining the training and classification process using
parameter optimization, topic segmentation and
syntactic parsing.
(Lapata and Lascarides, 2004) dealt with tem-
poral links between main and subordinate clauses
by inferring the temporal markers linking them.
They extracted clause pairs with explicit temporal
markers from BLLIP corpus as training data.
Another research line is to use human-
annotated corpora as training data, e.g., the RST
Bank (Carlson et al, 2001) used by (Soricut and
Marcu, 2003), adhoc annotations used by (?),
(Baldridge and Lascarides, 2005), and the Graph-
Bank (Wolf et al, 2005) used by (Wellner et al,
2006).
Recently the release of the Penn Discourse
TreeBank (PDTB) (Prasad et al, 2008) bene-
fits the researchers with a large discourse anno-
tated corpora, using a comprehensive scheme for
both implicit and explicit relations. (Pitler et al,
2009a) performed implicit relation classification
on the second version of the PDTB. They used
several linguistically informed features, such as
word polarity, verb classes, and word pairs, show-
ing performance increases over a random classi-
fication baseline. (Lin et al, 2009) presented an
implicit discourse relation classifier in PDTB with
the use of contextual relations, constituent Parse
Features, dependency parse features and cross-
argument word pairs.
1513
In comparison with existing works, we investi-
gated a new knowledge source, implicit connec-
tives, for implicit relation recognition. Moreover,
our two models can exploit both labeled and un-
labeled data by training a language model on un-
labeled data and then using this language model
to generate implicit connectives for recognition
models trained on labeled data.
5 Conclusions
In this paper we use a language model to auto-
matically generate implicit connectives and then
present two methods to use these connectives for
recognition of implicit relations. One method is to
use these predicted implicit connectives as addi-
tional features in a supervised model and the other
is to perform implicit relation recognition based
only on these predicted connectives. Results on
Penn Discourse Treebank 2.0 show that predicted
discourse connectives help implicit relation recog-
nition and the first algorithm achieves an absolute
average f-score improvement of 3% over a state of
the art baseline system.
Acknowledgments
This work is supported by grants from Na-
tional Natural Science Foundation of China
(No.60903093), Shanghai Pujiang Talent Program
(No.09PJ1404500) and Doctoral Fund of Ministry
of Education of China (No.20090076120029).
References
J. Baldridge and A. Lascarides. 2005. Probabilistic
head-driven parsing for discourse structure. Pro-
ceedings of the Ninth Conference on Computational
Natural Language Learning.
L. Carlson, D. Marcu, and Ma. E. Okurowski. 2001.
Building a discourse-tagged corpus in the frame-
work of rhetorical structure theory. Proceedings of
the Second SIG dial Workshop on Discourse and Di-
alogue.
B. Dorr. LCS Verb Database. Technical Report Online
Software Database, University of Maryland, Col-
lege Park, MD,2001.
R. Girju. 2003. Automatic detection of causal rela-
tions for question answering. In ACL 2003 Work-
shops.
S. Blair-Goldensohn. 2007. Long-Answer Ques-
tion Answering and Rhetorical-Semantic Relations.
Ph.D. thesis, Columbia Unviersity.
M. Lapata and A. Lascarides. 2004. Inferring
Sentence-internal Temporal Relations. Proceedings
of the North American Chapter of the Assocation of
Computational Linguistics.
Z.H. Lin, M.Y. Kan and H.T. Ng. 2009. Recognizing
Implicit Discourse Relations in the Penn Discourse
Treebank. Proceedings of the 2009 Conference on
EMNLP.
D. Marcu and A. Echihabi. 2002. An Unsupervised
Approach to Recognizing Discourse Relations. Pro-
ceedings of the 40th ACL.
E. Pitler, A. Louis, A. Nenkova. 2009. Automatic
sense prediction for implicit discourse relations in
text. Proceedings of the 47th ACL.
E. Pitler and A. Nenkova. 2009. Using Syntax to Dis-
ambiguate Explicit Discourse Connectives in Text.
Proceedings of the ACL-IJCNLP 2009 Conference
Short Papers.
M. Porter. 1980. An algorithm for suffix stripping. In
Program, vol. 14, no. 3, pp.130-137.
R. Prasad, N. Dinesh, A. Lee, E. Miltsakaki, L.
Robaldo, A. Joshi, B. Webber. 2008. The Penn Dis-
course TreeBank 2.0. Proceedings of LREC?08.
M. Saito, K.Yamamoto, S.Sekine. 2006. Using
Phrasal Patterns to Identify Discourse Relations.
Proceeding of the HLTCNA Chapter of the ACL.
R. Soricut and D. Marcu. Sentence Level Discourse
Parsing using Syntactic and Lexical Information.
Proceedings of HLT/NAACL 2003.
C. Sporleder and A. Lascarides. 2008. Using automat-
ically labelled examples to classify rhetorical rela-
tions: an assessment. Natural Language Engineer-
ing, Volume 14, Issue 03.
P.J. Stone, J. Kirsh, and Cambridge Computer Asso-
ciates. 1966. The General Inquirer: A Computer
Approach to Content Analysis. MIT Press.
B. Wellner , J. Pustejovsky, C. H. R. S., A. Rumshisky.
2006. Classification of discourse coherence rela-
tions: An exploratory study using multiple knowl-
edge sources. Proceedings of the 7th SIGDIAL
Workshop on Discourse and Dialogue.
F. Wolf, E. Gibson, A. Fisher, M. Knight. 2005.
The Discourse GraphBank: A database of texts an-
notated with coherence relations. Linguistic Data
Consortium.
1514
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 476?485,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Leveraging Synthetic Discourse Data via Multi-task Learning for Implicit
Discourse Relation Recognition
Man Lan and Yu Xu
Department of Computer Science and Technology
East China Normal University
Shanghai, P.R.China
mlan@cs.ecnu.edu.cn
51101201049@ecnu.cn
Zheng-Yu Niu
Baidu Inc.
Beijing, P.R.China
niuzhengyu@baidu.com
Abstract
To overcome the shortage of labeled data
for implicit discourse relation recogni-
tion, previous works attempted to auto-
matically generate training data by remov-
ing explicit discourse connectives from
sentences and then built models on these
synthetic implicit examples. However, a
previous study (Sporleder and Lascarides,
2008) showed that models trained on these
synthetic data do not generalize very well
to natural (i.e. genuine) implicit discourse
data. In this work we revisit this issue and
present a multi-task learning based system
which can effectively use synthetic data
for implicit discourse relation recognition.
Results on PDTB data show that under the
multi-task learning framework our models
with the use of the prediction of explicit
discourse connectives as auxiliary learn-
ing tasks, can achieve an averaged F1 im-
provement of 5.86% over baseline models.
1 Introduction
The task of implicit discourse relation recognition
is to identify the type of discourse relation (a.k.a.
rhetorical relation) hold between two spans of
text, where there is no discourse connective (a.k.a.
discourse marker, e.g., but, and) in context to ex-
plicitly mark their discourse relation (e.g., Con-
trast or Explanation). It can be of great benefit
to many downstream NLP applications, such as
question answering (QA) (Verberne et al, 2007),
information extraction (IE) (Cimiano et al, 2005),
and machine translation (MT), etc. This task is
quite challenging due to two reasons. First, with-
out discourse connective in text, the task is quite
difficult in itself. Second, implicit discourse rela-
tion is quite frequent in text. For example, almost
half the sentences in the British National Corpus
held implicit discourse relations (Sporleder and
Lascarides, 2008). Therefore, the task of implicit
discourse relation recognition is the key to im-
proving end-to-end discourse parser performance.
To overcome the shortage of manually anno-
tated training data, (Marcu and Echihabi, 2002)
proposed a pattern-based approach to automat-
ically generate training data from raw corpora.
This line of research was followed by (Sporleder
and Lascarides, 2008) and (Blair-Goldensohn,
2007). In these works, sentences containing cer-
tain words or phrases (e.g. but, although) were
selected out from raw corpora using a pattern-
based approach and then these words or phrases
were removed from these sentences. Thus the
resulting sentences were used as synthetic train-
ing examples for implicit discourse relation recog-
nition. Since there is ambiguity of a word or
phrase serving for discourse connective (i.e., the
ambiguity between discourse and non-discourse
usage or the ambiguity between two or more dis-
course relations if the word or phrase is used as a
discourse connective), the synthetic implicit data
would contain a lot of noises. Later, with the re-
lease of manually annotated corpus, such as Penn
Discourse Treebank 2.0 (PDTB) (Prasad et al,
2008), recent studies performed implicit discourse
relation recognition on natural (i.e., genuine) im-
plicit discourse data (Pitler et al, 2009) (Lin et al,
2009) (Wang et al, 2010) with the use of linguis-
tically informed features and machine learning al-
gorithms.
(Sporleder and Lascarides, 2008) conducted a
study of the pattern-based approach presented by
(Marcu and Echihabi, 2002) and showed that the
model built on synthetical implicit data has not
generalize well on natural implicit data. They
found some evidence that this behavior is largely
independent of the classifiers used and seems to
lie in the data itself (e.g., marked and unmarked
examples may be too dissimilar linguistically and
476
removing unambiguous markers in the automatic
labelling process may lead to a meaning shift in
the examples). We state that in some cases it is
true while in other cases it may not always be so.
A simple example is given here:
(E1) a. We can?t win.
b. [but] We must keep trying.
We may find that in this example whether the in-
sertion or the removal of connective but would
not lead to a redundant or missing information be-
tween the above two sentences. That is, discourse
connectives can be inserted between or removed
from two sentences without changing the seman-
tic relations between them in some cases. An-
other similar observation is in the annotation pro-
cedure of PDTB. To label implicit discourse re-
lation, annotators inserted connective which can
best express the relation between sentences with-
out any redundancy1. We see that there should
be some linguistical similarities between explicit
and implicit discourse examples. Therefore, the
first question arises: can we exploit this kind of
linguistic similarity between explicit and implicit
discourse examples to improve implicit discourse
relation recognition?
In this paper, we propose a multi-task learning
based method to improve the performance of im-
plicit discourse relation recognition (as main task)
with the help of relevant auxiliary tasks. Specif-
ically, the main task is to recognize the implicit
discourse relations based on genuine implicit dis-
course data and the auxiliary task is to recognize
the implicit discourse relations based on synthetic
implicit discourse data. According to the princi-
ple of multi-task learning, the learning model can
be optimized by the shared part of the main task
and the auxiliary tasks without bring unnecessary
noise. That means, the model can learn from syn-
thetic implicit data while it would not bring unnec-
essary noise from synthetic implicit data.
Although (Sporleder and Lascarides, 2008) did
not mention, we speculate that another possible
reason for the reported worse performance may
result from noises in synthetic implicit discourse
data. These synthetic data can be generated from
two sources: (1) raw corpora with the use of
pattern-based approach in (Marcu and Echihabi,
1According to the PDTB Annotation Manual (PDTB-
Group, 2008), if the insertion of connective leads to ?redun-
dancy?, the relation is annotated as Alternative lexicalizations
(AltLex), not implicit.
2002) and (Sporleder and Lascarides, 2008), and
(2) manually annotated explicit data with the re-
moval of explicit discourse connectives. Obvi-
ously, the data generated from the second source
is cleaner and more reliable than that from the
first source. Therefore, the second question to ad-
dress in this work is: whether synthetic implicit
discourse data generated from explicit discourse
data source (i.e., the second source) can lead to
a better performance than that from raw corpora
(i.e., the first source)? To answer this question,
we will make a comparison of synthetic discourse
data generated from two corpora, i.e., the BILLIP
corpus and the explicit discourse data annotated in
PDTB.
The rest of this paper is organized as follows.
Section 2 reviews related work on implicit dis-
course relation classification and multi-task learn-
ing. Section 3 presents our proposed multi-task
learning method for implicit discourse relation
classification. Section 4 provides the implemen-
tation technique details of the proposed multi-task
method. Section 5 presents experiments and dis-
cusses results. Section 6 concludes this work.
2 Related Work
2.1 Implicit discourse relation classification
2.1.1 Unsupervised approaches
Due to the lack of benchmark data for implicit
discourse relation analysis, earlier work used un-
labeled data to generate synthetic implicit dis-
course data. For example, (Marcu and Echi-
habi, 2002) proposed an unsupervised method
to recognize four discourse relations, i.e., Con-
trast, Explanation-evidence, Condition and Elab-
oration. They first used unambiguous pattern to
extract explicit discourse examples from raw cor-
pus. Then they generated synthetic implicit dis-
course data by removing explicit discourse con-
nectives from sentences extracted. In their work,
they collected word pairs from synthetic data set
as features and used machine learning method to
classify implicit discourse relation. Based on this
work, several researchers have extended the work
to improve the performance of relation classifica-
tion. For example, (Saito et al, 2006) showed that
the use of phrasal patterns as additional features
can help a word-pair based system for discourse
relation prediction on a Japanese corpus. Further-
more, (Blair-Goldensohn, 2007) improved previ-
ous work with the use of parameter optimization,
477
topic segmentation and syntactic parsing. How-
ever, (Sporleder and Lascarides, 2008) showed
that the training model built on a synthetic data
set, like the work of (Marcu and Echihabi, 2002),
may not be a good strategy since the linguistic dis-
similarity between explicit and implicit data may
hurt the performance of a model on natural data
when being trained on synthetic data.
2.1.2 Supervised approaches
This line of research work approaches this relation
prediction problem by recasting it as a classifica-
tion problem. (Soricut and Marcu, 2003) parsed
the discourse structures of sentences on RST Bank
data set (Carlson et al, 2001) which is annotated
based on Rhetorical Structure Theory (Mann and
Thompson, 1988). (Wellner et al, 2006) pre-
sented a study of discourse relation disambigua-
tion on GraphBank (Wolf et al, 2005). Recently,
(Pitler et al, 2009) (Lin et al, 2009) and (Wang
et al, 2010) conducted discourse relation study on
PDTB (Prasad et al, 2008) which has been widely
used in this field.
2.1.3 Semi-supervised approaches
Research work in this category exploited both la-
beled and unlabeled data for discourse relation
prediction. (Hernault et al, 2010) presented a
semi-supervised method based on the analysis of
co-occurring features in labeled and unlabeled
data. Very recently, (Hernault et al, 2011) in-
troduced a semi-supervised work using structure
learning method for discourse relation classifica-
tion, which is quite relevant to our work. However,
they performed discourse relation classification on
both explicit and implicit data. And their work is
different from our work in many aspects, such as,
feature sets, auxiliary task, auxiliary data, class la-
bels, learning framework, and so on. Furthermore,
there is no explicit conclusion or evidence in their
work to address the two questions raised in Sec-
tion 1.
Unlike their previous work, our previous work
(Zhou et al, 2010) presented a method to predict
the missing connective based on a language model
trained on an unannotated corpus. The predicted
connective was then used as a feature to classify
the implicit relation.
2.2 Multi-task learning
Multi-task learning is a kind of machine learning
method, which learns a main task together with
other related auxiliary tasks at the same time, us-
ing a shared representation. This often leads to
a better model for the main task, because it al-
lows the learner to use the commonality among
the tasks. Many multi-task learning methods have
been proposed in recent years, (Ando and Zhang,
2005a), (Argyriou et al, 2008), (Jebara, 2004),
(Bonilla et al, 2008), (Evgeniou and Pontil, 2004),
(Baxter, 2000), (Caruana, 1997), (Thrun, 1996).
One group uses task relations as regularization
terms in the objective function to be optimized.
For example, in (Evgeniou and Pontil, 2004) the
regularization terms make the parameters of mod-
els closer for similar tasks. Another group is pro-
posed to find the common structure from data and
then utilize the learned structure for multi-task
learning (Argyriou et al, 2008) (Ando and Zhang,
2005b).
3 Multi-task Learning for Discourse
Relation Prediction
3.1 Motivation
The idea of using multi-task learning for implicit
discourse relation classification is motivated by
the observations that we have made on implicit
discourse relation.
On one hand, since building a hand-annotated
implicit discourse relation corpus is costly and
time consuming, most previous work attempted to
use synthetic implicit discourse examples as train-
ing data. However, (Sporleder and Lascarides,
2008) found that the model trained on synthetic
implicit data has not performed as well as expected
in natural implicit data. They stated that the reason
is linguistic dissimilarity between explicit and im-
plicit discourse data. This indicates that straightly
using synthetic implicit data as training data may
not be helpful.
On the other hand, as shown in Section 1, we
observe that in some cases explicit discourse rela-
tion and implicit discourse relation can express the
same meaning with or without a discourse connec-
tive. This indicates that in certain degree they must
be similar to each other. If it is true, the synthetic
implicit relations are expected to be helpful for im-
plicit discourse relation classification. Therefore,
what we have to do is to find a way to train a model
which has the capabilities to learn from their sim-
ilarity and to ignore their dissimilarity as well.
To solve it, we propose a multi-task learn-
ing method for implicit discourse relation classi-
478
fication, where the classification model seeks the
shared part through jointly learning main task and
multiple auxiliary tasks. As a result, the model can
be optimized by the similar shared part without
bringing noise in the dissimilar part. Specifically,
in this work, we use alternating structure optimiza-
tion (ASO) (Ando and Zhang, 2005a) to construct
the multi-task learning framework. ASO has been
shown to be useful in a semi-supervised learning
configuration for several NLP applications, such
as, text chunking (Ando and Zhang, 2005b) and
text classification (Ando and Zhang, 2005a).
3.2 Multi-task learning and ASO
Generally, multi-task learning(MTL) considers m
prediction problems indexed by ? ? {1, ...,m},
each with n? samples (X?i , Y ?i ) for i ? {1, ...n?}
(Xi are input feature vectors and Yi are corre-
sponding classification labels) and assumes that
there exists a common predictive structure shared
by these m problems. Generally, the joint linear
model for MTL is to predict problem ? in the fol-
lowing form:
f?(?, X) = wT? X + vT? ?X,??T = I, (1)
where I is the identity matrix,w? and v? are weight
vectors specific to each problem ?, and ? is the
structure matrix shared by all the m predictors.
The main goal of MTL is to learn a common good
feature map ?X for all the m problems. Several
MTL methods have been presented to learn ?X
for all the m problems. In this work, we adopt the
ASO method.
Specifically, the ASO method adopted singu-
lar value decomposition (SVD) to obtain ? and
m predictors that minimize the empirical risk
summed over all the m problems. Thus, the prob-
lem of optimization becomes the minimization of
the joint empirical risk written as:
m?
?=1
( n??
i=1
L(f?(?, X?i ), Yi)
n?
+ ?||W?||2
)
(2)
where loss function L(.) quantifies the difference
between the prediction f(Xi) and the true out-
put Yi for each predictor, and ? is a regulariza-
tion parameter for square regularization to control
the model complexity. To minimize the empirical
risk, ASO repeats the following alternating opti-
mization procedure until a convergence criterion
is met:
1) Fix (?, V?), and find m predictors f? that
minimize the above joint empirical risk.
2) Fix m predictors f?, and find (?, V?) that
minimizes the above joint empirical risk.
3.3 Auxiliary tasks
There are two main principles to create auxiliary
tasks. First, the auxiliary tasks should be auto-
matically labeled in order to reduce the cost of
manual labeling. Second, since the MTL model
learns from the shared part of main task and aux-
iliary tasks, the auxiliary tasks should be quite rel-
evant/similar to the main task. It is generally be-
lieved that the more the auxiliary tasks are relevant
to the main task, the more the main task can ben-
efit from the auxiliary tasks. Following these two
principles, we create the auxiliary tasks by gener-
ating automatically labeled data as follows.
Previous work (Marcu and Echihabi, 2002) and
(Sporleder and Lascarides, 2008) adopted prede-
fined pattern-based approach to generate synthetic
labeled data, where each predefined pattern has
one discourse relation label. In contrast, we adopt
an automatic approach to generate synthetic la-
beled data, where each discourse connective be-
tween two texts serves as their relation label. The
reason lies in the very strong connection between
discourse connectives and discourse relations. For
example, the connective but always indicates a
contrast relation between two texts. And (Pitler et
al., 2008) proved that using only connective itself,
the accuracy of explicit discourse relation classifi-
cation is over 93%.
To build the mapping between discourse con-
nective and discourse relation, for each connec-
tive, we count the times it appears in each relation
and regard the relation in which it appears most
frequently as its most relevant relation. Based on
this mapping between connective and relation, we
extract the synthetic labeled data containing the
connective as training data for auxiliary tasks.
For example, and appears 3, 000 times in PDTB
as a discourse connective. Among them, it is man-
ually annotated as an Expansion relation for 2, 938
times. So we regard the Expansion relation as its
most relevant relation and generate a mapping pat-
tern like: ?and ? Expansion?. Then we extract
all sentences which contain discourse ?and? and
remove this connective ?and? from sentences to
generate synthetic implicit data. The resulting sen-
tences are used in auxiliary task and automatically
479
marked as Expansion relation.
4 Implementation Details of Multi-task
Learning Method
4.1 Data sets for main and auxiliary tasks
To examine whether there is a difference in syn-
thetic implicit data generated from unannotated
and annotated corpus, we use two corpora. One
is a hand-annotated explicit discourse corpus, i.e.,
the explicit discourse relations in PDTB, denoted
as exp. Another is an unannotated corpus, i.e.,
BLLIP (David McClosky and Johnson., 2008).
4.1.1 Penn Discourse Treebank
PDTB (Prasad et al, 2008) is the largest hand-
annotated corpus of discourse relation so far. It
contains 2, 312 Wall Street Journal (WSJ) articles.
The sense label of discourse relations is hierarchi-
cally with three levels, i.e., class, type and sub-
type. The top level contains four major seman-
tic classes: Comparison (denoted as Comp.), Con-
tingency (Cont.), Expansion (Exp.) and Temporal
(Temp.). For each class, a set of types is used to
refine relation sense. The set of subtypes is to fur-
ther specify the semantic contribution of each ar-
gument. In this paper, we focus on the top level
(class) and the second level (type) relations be-
cause the subtype relations are too fine-grained
and only appear in some relations.
Both explicit and implicit discourse relations
are labeled in PDTB. In our experiment, the im-
plicit discourse relations are used in the main task
and for evaluation. While the explicit discourse
relations are used in the auxiliary task. A detailed
description of the data sources for different tasks
is given below.
Data set for main task Following previous
work in (Pitler et al, 2009) and (Zhou et al, 2010),
the implicit relations in sections 2-20 are used as
training data for the main task (denoted as imp)
and the implicit relations in sections 21-22 are
for evaluation. Table 1 shows the distribution of
implicit relations. There are too few training in-
stances for six second level relations (indicated by
* in Table 1), so we removed these six relations in
our experiments.
Data set for auxiliary task All explicit in-
stances in sections 00-24 in PDTB, i.e., 18, 459
instances, are used for auxiliary task (denoted as
exp). Following the method described in Section
3.3, we build the mapping patterns between con-
Top level Second level train test
Temp 736 83
Synchrony 203 28
Asynchronous 532 55
Cont 3333 279
Cause 3270 272
Pragmatic Cause* 64 7
Condition* 1 0
Pragmatic condition* 1 0
Comp 1939 152
Contrast 1607 134
Pragmatic contrast* 4 0
Concession 183 17
Pragmatic concession* 1 0
Exp 6316 567
Conjunction 2872 208
Instantiation 1063 119
Restatement 2405 213
Alternative 147 9
Exception* 0 0
List 338 12
Table 1: Distribution of implicit discourse rela-
tions in the top and second level of PDTB
nectives and relations in PDTB and generate syn-
thetic labeled data by removing the connectives.
According to the most relevant relation sense of
connective removed, the resulting instances are
grouped into different data sets.
4.1.2 BLLIP
BLLIP North American News Text (Complete) is
used as unlabeled data source to generate syn-
thetic labeled data. In comparison with the syn-
thetic labeled data generated from the explicit re-
lations in PDTB, the synthetic labeled data from
BLLIP contains more noise. This is because the
former data is manually annotated whether a word
serves as discourse connective or not, while the
latter does not manually disambiguate two types
of ambiguity, i.e., whether a word serves as dis-
course connective or not, and the type of discourse
relation if it is a discourse connective. Finally, we
extract 26, 412 instances from BLLIP (denoted as
BLLIP) and use them for auxiliary task.
4.2 Feature representation
For both main task and auxiliary tasks, we adopt
the following three feature types. These features
are chosen due to their superior performance in
previous work (Pitler et al, 2009) and our previ-
ous work (Zhou et al, 2010).
Verbs: Following (Pitler et al, 2009), we ex-
tract the pairs of verbs from both text spans. The
number of verb pairs which have the same highest
480
Levin verb class levels (Levin, 1993) is counted
as a feature. Besides, the average length of verb
phrases in each argument is included as a feature.
In addition, the part of speech tags of the main
verbs (e.g., base form, past tense, 3rd person sin-
gular present, etc.) in each argument, i.e., MD,
VB, VBD, VBG, VBN, VBP, VBZ, are recorded
as features, where we simply use the first verb in
each argument as the main verb.
Polarity: This feature records the number of
positive, negated positive, negative and neutral
words in both arguments and their cross product
as well. For negated positives, we first locate the
negated words in text span and then define the
closely behind positive word as negated positive.
The polarity of each word in arguments is de-
rived from Multi-perspective Question Answering
Opinion Corpus (MPQA) (Wilson et al, 2009).
Modality: We examine six modal words (i.e.,
can, may, must, need, shall, will) including their
various tenses or abbreviation forms in both argu-
ments. This feature records the presence or ab-
sence of modal words in both arguments and their
cross product.
4.3 Classifiers used multi-task learning
We extract the above linguistically informed fea-
tures from two synthetic implicit data sets (i.e.,
BLLIP and exp) to learn the auxiliary classifier and
from the natural implicit data set (i.e., imp) to learn
the main classifier. Under the ASO-based multi-
task learning framework, the model of main task
learns from the shared part of main task and aux-
iliary tasks. Specifically, we adopt multiple binary
classification to build model for main task. That
is, for each discourse relation, we build a binary
classifier.
5 Experiments and Results
5.1 Experiments
Although previous work has been done on PDTB
(Pitler et al, 2009) and (Lin et al, 2009), we can-
not make a direct comparison with them because
various experimental conditions, such as, differ-
ent classification strategies (multi-class classifica-
tion, multiple binary classification), different data
preparation (feature extraction and selection), dif-
ferent benchmark data collections (different sec-
tions for training and test, different levels of dis-
course relations), different classifiers with various
parameters (MaxEnt, Na??ve Bayes, SVM, etc) and
even different evaluation methods (F1, accuracy)
have been adopted by different researchers.
Therefore, to address the two questions raised in
Section 1 and to make the comparison reliable and
reasonable, we performed experiments on the top
and second level of PDTB using single task learn-
ing and multi-task learning, respectively. The sys-
tems using single task learning serve as baseline
systems. Under the single task learning, various
combinations of exp and BLLIP data are incorpo-
rated with imp data for the implicit discourse rela-
tion classification task.
We hypothesize that synthetical implicit data
would contribute to the main task, i.e., the implicit
discourse relation classification. Specifically, the
natural implicit data (i.e., imp) are used to create
main task and the synthetical implicit data (exp or
BLLIP) are used to create auxiliary tasks for the
purpose of optimizing the objective functions of
main task. If the hypothesis is correct, the perfor-
mance of main task would be improved by auxil-
iary tasks created from synthetical implicit data.
Thus in the experiments of multi-task learning,
only natural implicit examples (i.e., imp) data are
used for main task training while different combi-
nations of synthetical implicit examples (exp and
BLLIP) are used for auxiliary task training.
We adopt precision, recall and their combina-
tion F1 for performance evaluation. We also per-
form one-tailed t-test to validate if there is signif-
icant difference between two methods in terms of
F1 performance analysis.
5.2 Results
Table 2 summarizes the experimental results under
single and multi-task learning on the top level of
four PDTB relations with respect to different com-
binations of synthetic implicit data. For each rela-
tion, the first three rows indicate the results of us-
ing different single training data under single task
learning and the last three rows indicate the results
using different combinations of training data un-
der single task and multi-task learning. The best
F1 for every relation is shown in bold font. From
this table, we can find that on four relations, our
multi-task learning systems achieved the best per-
formance using the combination of exp and BLLIP
synthetic data.
Table 3 summarizes the best single task and the
best multi-task learning results on the second level
of PDTB. For four relations, i.e., Synchrony, Con-
481
Single-task Multi-task
Level 1 class Data P R F1 Data Data P R F1
(main) (aux)
Comp. imp 21.43 37.50 27.27 - - - - -
BLLIP 12.68 53.29 20.48 - - - - -
exp 15.25 50.66 23.44 - - - - -
imp + exp 16.94 40.13 23.83 imp exp 22.94 49.34 30.90
imp + BLLIP 13.56 44.08 20.74 imp BLLIP 20.47 63.16 30.92
imp + exp + BLLIP 14.54 38.16 21.05 imp exp + BLLIP 23.47 48.03 31.53
Cont. imp 37.65 43.73 40.46 - - - - -
BLLIP 33.72 31.18 32.40 - - - - -
exp 35.24 26.52 30.27 - - - - -
imp + exp 39.00 13.98 20.58 imp exp 39.94 45.52 42.55
imp + BLLIP 37.30 24.73 29.74 imp BLLIP 37.80 63.80 47.47
imp + exp + BLLIP 39.37 31.18 34.80 imp exp + BLLIP 35.90 70.25 47.52
Exp. imp 56.59 66.67 61.21 - - - - -
BLLIP 53.29 40.04 45.72 - - - - -
exp 57.97 58.38 58.17 - - - - -
imp + exp 57.32 65.61 61.18 imp exp 59.14 67.90 63.22
imp + BLLIP 56.28 65.61 60.59 imp BLLIP 53.80 99.82 69.92
imp + exp + BLLIP 55.81 65.26 60.16 imp exp + BLLIP 53.90 99.82 70.01
Temp. imp 16.46 63.86 26.17 - - - - -
BLLIP 17.31 43.37 24.74 - - - - -
exp 15.46 36.14 21.66 - - - - -
imp + exp 15.35 39.76 22.15 imp exp 18.60 63.86 28.80
imp + BLLIP 14.74 33.73 20.51 imp BLLIP 18.12 67.47 28.57
imp + exp + BLLIP 15.94 39.76 22.76 imp exp + BLLIP 19.08 65.06 29.51
Table 2: Performance of precision, recall and F1 for 4 Level 1 relation classes. ?-? indicates N.A.
Single-task Multi-task
Level 2 type Data P R F1 Data Data P R F1
(main) (aux)
Asynchronous imp 11.36 74.55 19.71 imp exp + BLLIP 23.08 21.82 22.43
Synchrony imp - - - imp exp + BLLIP - - -
Cause imp 36.38 64.34 46.48 imp exp + BLLIP 36.01 67.65 47.00
Contrast imp 20.07 42.54 27.27 imp exp + BLLIP 20.70 52.99 29.77
Concession imp - - - imp exp + BLLIP - - -
Conjunction imp 26.35 63.46 37.24 imp exp + BLLIP 26.29 73.56 38.73
Instantiation imp 22.78 53.78 32.00 imp exp + BLLIP 22.55 57.98 32.47
Restatement imp 23.11 67.61 34.45 imp exp + BLLIP 26.93 53.99 35.94
Alternative imp - - - imp exp + BLLIP - - -
List imp - - - imp exp + BLLIP - - -
Table 3: Performance of precision, recall and F1 for 10 Level 2 relation types. ?-? indicates 0.00.
cession, Alternative and List, the classifier labels
no instances due to the small percentages for these
four types.
Table 4 summarizes the one-tailed t-test results
on the top level of PDTB between the best single
task learning system (i.e., imp) and three multi-
task learning systems (imp:exp+BLLIP indicates
that imp is used for main task and the combi-
nation of exp and BLLIP are for auxiliary task).
The systems with insignificant performance differ-
ences are grouped into one set and ?>? and ?>>?
denote better than at significance level 0.01 and
0.001 respectively.
5.3 Discussion
From Table 2 to Table 4, several findings can be
found as follows.
We can see that the multi-task learning sys-
tems perform consistently better than the single
task learning systems for the prediction of implicit
discourse relations. Our best multi-task learning
system achieves an averaged F1 improvement of
5.86% over the best single task learning system on
the top level of PDTB relations. Specifically, for
482
Class One-tailed t-test results
Comp. (imp:exp+BLLIP, imp:exp, imp:BLLIP) >> (imp)
Cont. (imp:exp+BLLIP, imp:BLLIP) >> (imp:exp) > (imp)
Exp. (imp:exp+BLLIP, imp:BLLIP) >> (imp:exp) > (imp)
Temp. (imp:exp+BLLIP, imp:exp, imp:BLLIP) >> (imp)
Table 4: Statistical significance tests results.
the relations Comp., Cont., Exp., Temp., our best
multi-task learning system achieve 4.26%, 7.06%,
8.8% and 3.34% F1 improvements over the best
single task learning system. It indicates that using
synthetic implicit data as auxiliary task greatly im-
proves the performance of the main task. This is
confirmed by the following t-tests in Table 4.
In contrast to the performance of multi-task
learning, the performance of the best single task
learning system has been achieved on natural im-
plicit discourse data alone. This finding is con-
sistent with (Sporleder and Lascarides, 2008). It
indicates that under single task learning, directly
adding synthetic implicit data to increase the num-
ber of training data cannot be helpful to implicit
discourse relation classification. The possible rea-
sons result from (1) the different nature of implicit
and explicit discourse data in linguistics and (2)
the noise brought from synthetic implicit data.
Based on the above analysis, we state that it is
the way of utilizing synthetic implicit data that is
important for implicit discourse relation classifica-
tion.
Although all three multi-task learning systems
outperformed single task learning systems, we
find that the two synthetic implicit data sets have
not been shown a universally consistent perfor-
mance on four top level PDTB relations. On one
hand, for the relations Comp. and Temp., the per-
formance of the two synthetic implicit data sets
alone and their combination are comparable to
each other and there is no significant difference
between them. On the other hand, for the rela-
tions Cont. and Exp., the performance of exp data
is inferior to that of BLLIP and their combination.
This is contrary to our original expectation that exp
data which has been manually annotated for dis-
course connective disambiguation should outper-
form BLLIP which contains a lot of noise. This
finding indicates that under the multi-task learn-
ing, it may not be worthy of using manually anno-
tated corpus to generate auxiliary data. It is quite
promising since it can provide benefits to reducing
the cost of human efforts on corpus annotation.
5.4 Ambiguity Analysis
Although our experiments show that synthetic im-
plicit data can help implicit discourse relation clas-
sification under multi-task learning framework,
the overall performance is still quite low (44.64%
in F1). Therefore, we analyze the types of ambi-
guity in relations and connectives in order to mo-
tivate possible future work.
5.4.1 Ambiguity of implicit relation
Without explicit discourse connective, the implicit
discourse relation instance can be understood in
two or more different ways. Given the example
E2 in PDTB, the PDTB annotators explain it as
Contingency or Expansion relation and manually
insert corresponding implicit connective for one
thing or because to express its relation.
(E2) Arg1:Now the stage is set for the battle to
play out
Arg2:The anti-programmers are getting
some helpful thunder from Congress
Connective1:because
Sense1:Contingency.Cause.Reason
Connective2:for one thing
Sense2:Expansion.Instantiation
(wsj 0118)
Thus the ambiguity of implicit discourse rela-
tions makes this task difficult in itself.
5.4.2 Ambiguity of discourse connectives
As we mentioned before, even given an explicit
discourse connective in text, its discourse rela-
tion still can be explained in two or more differ-
ent ways. And for different connectives, the am-
biguity of relation senses is quite different. That
is, the most frequent sense is not always the only
sense that a connective expresses. In example E3,
?since? is explained by annotators to express Tem-
poral or Contingency relation.
(E3) Arg1:MiniScribe has been on the rocks
Arg2:since it disclosed early this year that
its earnings reports for 1988 weren?t accu-
rate.
483
Sense1:Temporal.Asynchronous.Succession
Sense2:Contingency.Cause.Reason
(wsj 0003)
In PDTB, ?since? appears 184 times in explicit
discourse relations. It expresses Temporal relation
for 80 times, Contingency relation for 94 times
and both Temporal and Contingency for 10 time
(like example E3). Therefore, although we use its
most frequent sense, i.e., Contingency, to automat-
ically extract sentences and label them, almost less
than half of them actually express Temporal rela-
tion. Thus the ambiguity of discourse connectives
is another source which has brought noise to data
when we generate synthetical implicit discourse
relation.
6 Conclusions
In this paper, we present a multi-task learning
method to improve implicit discourse relation
classification by leveraging synthetic implicit dis-
course data. Results on PDTB show that under
the framework of multi-task learning, using syn-
thetic discourse data as auxiliary task significantly
improves the performance of main task. Our best
multi-task learning system achieves an averaged
F1 improvement of 5.86% over the best single task
learning system on the top level of PDTB rela-
tions. Specifically, for the relations Comp., Cont.,
Exp., Temp., our best multi-task learning system
achieves 4.26%, 7.06%, 8.8%, and 3.34% F1 im-
provements over a state of the art baseline system.
This indicates that it is the way of utilizing syn-
thetic discourse examples that is important for im-
plicit discourse relation classification.
Acknowledgements
This research is supported by grants from Na-
tional Natural Science Foundation of China
(No.60903093), Shanghai Pujiang Talent Program
(No.09PJ1404500), Doctoral Fund of Ministry of
Education of China (No. 20090076120029) and
Shanghai Knowledge Service Platform Project
(No. ZF1213).
References
R.K. Ando and T. Zhang. 2005a. A framework for
learning predictive structures from multiple tasks
and unlabeled data. The Journal of Machine Learn-
ing Research, 6:1817?1853.
R.K. Ando and T. Zhang. 2005b. A high-performance
semi-supervised learning method for text chunking.
pages 1?9. Association for Computational Linguis-
tics. Proceedings of the 43rd Annual Meeting on
Association for Computational Linguistics.
A. Argyriou, C.A. Micchelli, M. Pontil, and Y. Ying.
2008. A spectral regularization framework for
multi-task structure learning. Advances in Neural
Information Processing Systems, 20:2532.
J. Baxter. 2000. A model of inductive bias learning. J.
Artif. Intell. Res. (JAIR), 12:149?198.
S.J. Blair-Goldensohn. 2007. Long-answer question
answering and rhetorical-semantic relations. Ph.D.
thesis.
E. Bonilla, K.M. Chai, and C. Williams. 2008. Multi-
task gaussian process prediction. Advances in Neu-
ral Information Processing Systems, 20(October).
L. Carlson, D. Marcu, and M.E. Okurowski. 2001.
Building a discourse-tagged corpus in the frame-
work of rhetorical structure theory. pages 1?10. As-
sociation for Computational Linguistics. Proceed-
ings of the Second SIGdial Workshop on Discourse
and Dialogue-Volume 16.
R. Caruana. 1997. Multitask learning. Machine
Learning, 28(1):41?75.
P. Cimiano, U. Reyle, and J. Saric. 2005. Ontology-
driven discourse analysis for information extraction.
Data and Knowledge Engineering, 55(1):59?83.
Eugene Charniak David McClosky and Mark Johnson.
2008. Bllip north american news text, complete.
T. Evgeniou and M. Pontil. 2004. Regularized multi?
task learning. pages 109?117. ACM. Proceedings
of the tenth ACM SIGKDD international conference
on Knowledge discovery and data mining.
H. Hernault, D. Bollegala, and M. Ishizuka. 2010. A
semi-supervised approach to improve classification
of infrequent discourse relations using feature vector
extension. pages 399?409. Association for Compu-
tational Linguistics. Proceedings of the 2010 Con-
ference on Empirical Methods in Natural Language
Processing.
H. Hernault, D. Bollegala, and M. Ishizuka. 2011.
Semi-supervised discourse relation classification
with structural learning. In Proceedings of the 12th
international conference on Computational linguis-
tics and intelligent text processing - Volume Part
I, CICLing?11, pages 340?352, Berlin, Heidelberg.
Springer-Verlag.
T. Jebara. 2004. Multi-task feature and kernel se-
lection for svms. page 55. ACM. Proceedings of
the twenty-first international conference on Machine
learning.
B. Levin. 1993. English verb classes and alternations:
A preliminary investigation, volume 348. University
of Chicago press Chicago, IL:.
484
Z. Lin, M.Y. Kan, and H.T. Ng. 2009. Recogniz-
ing implicit discourse relations in the penn discourse
treebank. pages 343?351. Association for Compu-
tational Linguistics. Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing: Volume 1-Volume 1.
W.C. Mann and S.A. Thompson. 1988. Rhetorical
structure theory: Toward a functional theory of text
organization. Text-Interdisciplinary Journal for the
Study of Discourse, 8(3):243?281.
D. Marcu and A. Echihabi. 2002. An unsupervised
approach to recognizing discourse relations. pages
368?375. Association for Computational Linguis-
tics. Proceedings of the 40th Annual Meeting on
Association for Computational Linguistics.
PDTB-Group. 2008. The penn discourse treebank 2.0
annotation manual. Technical report, Institute for
Research in Cognitive Science, University of Penn-
sylvania.
E. Pitler, M. Raghupathy, H. Mehta, A. Nenkova,
A. Lee, and A. Joshi. 2008. Easily identifiable dis-
course relations. Citeseer. Proceedings of the 22nd
International Conference on Computational Linguis-
tics (COLING 2008), Manchester, UK, August.
E. Pitler, A. Louis, and A. Nenkova. 2009. Automatic
sense prediction for implicit discourse relations in
text. pages 683?691. Association for Computational
Linguistics. Proceedings of the Joint Conference of
the 47th Annual Meeting of the ACL and the 4th
International Joint Conference on Natural Language
Processing of the AFNLP: Volume 2-Volume 2.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, and Bonnie
Webber. 2008. The penn discourse treebank 2.0. In
In Proceedings of LREC.
M. Saito, K. Yamamoto, and S. Sekine. 2006. Us-
ing phrasal patterns to identify discourse relations.
pages 133?136. Association for Computational Lin-
guistics. Proceedings of the Human Language Tech-
nology Conference of the NAACL, Companion Vol-
ume: Short Papers on XX.
R. Soricut and D. Marcu. 2003. Sentence level dis-
course parsing using syntactic and lexical informa-
tion. pages 149?156. Association for Computational
Linguistics. Proceedings of the 2003 Conference
of the North American Chapter of the Association
for Computational Linguistics on Human Language
Technology-Volume 1.
C. Sporleder and A. Lascarides. 2008. Using automat-
ically labelled examples to classify rhetorical rela-
tions: An assessment. Natural Language Engineer-
ing, 14(03):369?416.
S. Thrun. 1996. Is learning the n-th thing any easier
than learning the first? Advances in Neural Infor-
mation Processing Systems, pages 640?646.
S. Verberne, L. Boves, N. Oostdijk, and P.A. Coppen.
2007. Evaluating discourse-based answer extraction
for why-question answering. pages 735?736. ACM.
Proceedings of the 30th annual international ACM
SIGIR conference on Research and development in
information retrieval.
W.T. Wang, J. Su, and C.L. Tan. 2010. Kernel based
discourse relation recognition with temporal order-
ing information. pages 710?719. Association for
Computational Linguistics. Proceedings of the 48th
Annual Meeting of the Association for Computa-
tional Linguistics.
B. Wellner, J. Pustejovsky, C. Havasi, A. Rumshisky,
and R. Sauri. 2006. Classification of discourse co-
herence relations: An exploratory study using multi-
ple knowledge sources. pages 117?125. Association
for Computational Linguistics. Proceedings of the
7th SIGdial Workshop on Discourse and Dialogue.
T. Wilson, J. Wiebe, and P. Hoffmann. 2009. Rec-
ognizing contextual polarity: An exploration of fea-
tures for phrase-level sentiment analysis. Computa-
tional Linguistics, 35(3):399?433.
F. Wolf, E. Gibson, A. Fisher, and M. Knight. 2005.
The discourse graphbank: A database of texts an-
notated with coherence relations. Linguistic Data
Consortium.
Z.M. Zhou, Y. Xu, Z.Y. Niu, M. Lan, J. Su, and C.L.
Tan. 2010. Predicting discourse connectives for im-
plicit discourse relation recognition. pages 1507?
1514. Association for Computational Linguistics.
Proceedings of the 23rd International Conference on
Computational Linguistics: Posters.
485
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 983?992,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Probabilistic Sense Sentiment Similarity through Hidden Emotions 
 
 
Mitra Mohtarami1, Man Lan2, and Chew Lim Tan1 
1Department of Computer Science, National University of Singapore; 
2Department of Computer Science, East China Normal University 
{mitra,tancl}@comp.nus.edu.sg;mlan@cs.ecnu.edu.cn 
 
  
 
Abstract 
Sentiment Similarity of word pairs reflects the 
distance between the words regarding their 
underlying sentiments. This paper aims to in-
fer the sentiment similarity between word 
pairs with respect to their senses. To achieve 
this aim, we propose a probabilistic emotion-
based approach that is built on a hidden emo-
tional model. The model aims to predict a vec-
tor of basic human emotions for each sense of 
the words. The resultant emotional vectors are 
then employed to infer the sentiment similarity 
of word pairs. We apply the proposed ap-
proach to address two main NLP tasks, name-
ly, Indirect yes/no Question Answer Pairs in-
ference and Sentiment Orientation prediction. 
Extensive experiments demonstrate the effec-
tiveness of the proposed approach. 
1 Introduction 
Sentiment similarity reflects the distance be-
tween words based on their underlying senti-
ments. Semantic similarity measures such as La-
tent Semantic Analysis (LSA) (Landauer et al, 
1998) can effectively capture the similarity be-
tween semantically related words like "car" and 
"automobile", but they are less effective in relat-
ing words with similar sentiment orientation like 
"excellent" and "superior". For example, the fol-
lowing relations show the semantic similarity 
between some sentiment words computed by 
LSA: 
 :			
		, 	 = 0.40		 < 		
		,  = 0.46	 < 		,   = 0.65 
Clearly, the sentiment similarity between the 
above words should be in the reversed order. In 
fact, the sentiment intensity in "excellent" is 
closer to "superior" than "good". Furthermore, 
sentiment similarity between "good" and "bad" 
should be 0. 
In this paper, we propose a probabilistic ap-
proach to detect the sentiment similarity of 
words regarding their senses and underlying sen-
timents. For this purpose, we propose to model 
the hidden emotions of word senses. We show 
that our approach effectively outperforms the 
semantic similarity measures in two NLP tasks: 
Indirect yes/no Question Answer Pairs (IQAPs) 
Inference and Sentiment Orientation (SO) pre-
diction that are described as follows: 
In IQAPs, answers do not explicitly contain 
the yes or no keywords, but rather provide con-
text information to infer the yes or no answer 
(e.g. Q: Was she the best one on that old show? 
A: She was simply funny). Clearly, the sentiment 
words in IQAPs are the pivots to infer the yes or 
no answers. We show that sentiment similarity 
between such words (e.g., here the adjectives 
best and Funny) can be used effectively to infer 
the answers. 
The second application (SO prediction) aims to 
determine the sentiment orientation of individual 
words. Previous research utilized the semantic 
relations between words obtained from WordNet 
(Hassan and Radev, 2010) and semantic similari-
ty measures (e.g. Turney and Littman, 2003) for 
this purpose. In this paper, we show that senti-
ment similarity between word pairs can be effec-
tively utilized to compute SO of words.  
The contributions of this paper are follows: 
? We propose an effective approach to predict 
the sentiment similarity between word pairs 
through hidden emotions at the sense level,  
? We show the utility of sentiment similarity 
prediction in IQAP inference and SO predic-
tion tasks, and 
? Our hidden emotional model can infer the type 
and number of hidden emotions in a corpus. 
983
2 Sentiment Similarity through Hidden 
Emotions 
As we discussed above, semantic similarity 
measures are less effective to infer sentiment 
similarity between word pairs. In addition, dif-
ferent senses of sentiment words carry different 
human emotions. In fact, a sentiment word can 
be represented as a vector of emotions with in-
tensity values from "very weak" to "very strong". 
For example, Table 1 shows several sentiment 
words and their corresponding emotion vectors 
based the following set of emotions: e = [anger, 
disgust, sadness, fear, guilt, interest, joy, shame, 
surprise]. For example, "deceive" has 0.4 and 0.5 
intensity values with respect to the emotions 
"disgust" and "sadness" with an overall -0.9 (i.e. 
-0.4-0.5) value for sentiment orientation 
(Neviarouskaya et al, 2007; Neviarouskaya et 
al., 2009).  
Word Emotional Vector SO 
e = [anger, disgust, sadness, fear, guilt, interest, joy, shame, surprise] 
Rude ['0.2', '0.4',0,0,0,0,0,0,0] -0.6 
doleful [0, 0, '0.4',0,0,0,0,0,0] -0.4 
smashed [0,0, '0.8', '0.6',0,0,0,0,0] -1.4 
shamefully [0,0,0,0,0,0,0, '0.7',0] -0.7 
deceive [0, '0.4', '0.5',0,0,0,0,0,0] -0.9 
Table  1. Sample of emotional vectors  
 
The difficulty of the sentiment similarity predic-
tion task is evident when terms carry different 
types of emotions. For instance, all the words in 
Table 1 have negative sentiment orientation, but, 
they carry different emotions with different emo-
tion vectors. For example, "rude" reflects the 
emotions "anger" and "disgust", while the word 
"doleful" only reflects the emotion "sadness". As 
such, the word "doleful" is closer to the words 
"smashed" and "deceive" involving the emotion 
"sadness" than others. We show that emotion 
vectors of the words can be effectively utilized to 
predict the sentiment similarity between them. 
Previous research shows little agreement about 
the number and types of the basic emotions 
(Ortony and Turner 1990; Izard 1971). Thus, we 
assume that the number and types of basic emo-
tions are hidden and not pre-defined and propose 
a Probabilistic Sense Sentiment Similarity 
(PSSS) approach to extract the hidden emotions 
of word senses to infer their sentiment similarity.  
3 Hidden Emotional Model  
Online review portals provide rating mechanisms 
(in terms of stars, e.g. 5- or 10-star rating) to al- 
 
Figure 1.The structure of PSSS model 
 
low users to attach ratings to their reviews. A 
rating indicates the summarized opinion of a user 
who ranks a product or service based on his feel-
ings. There are various feelings and emotions 
behind such ratings with respect to the content of 
the reviews.  
Figure 1 shows the intermediate layer of hid-
den emotions behind the ratings (sentiments) 
assigned to the documents (reviews) containing 
the words. This Figure indicates the general 
structure of our PSSS model. It shows that hid-
den emotions (ei) link the rating (rj) and the doc-
uments (dk). In this Section, we aim to employ 
ratings and the relations among ratings, docu-
ments, and words to extract the hidden emotions.  
Figure 2 illustrates a simple graphical model 
using plate representation of Figure 1. As Figures 
2 shows, the rating r from a set of ratings R= 
{r1,?,rp} is assigned to a hidden emotion set 
E={e1,?,ek}. A document d from a set of docu-
ments D= {d1,?,dN} with vocabulary set W= 
{w1,?,wM} is associated with the hidden emotion 
set.  
 
 
 
 
 
 
 
 
 
 
 
 
The model presented in Figure 2(a) has been 
explored in (Mohtarami et al, 2013) and is called 
Series Hidden Emotional Model (SHEM). This 
representation assumes that the word w is de-
pendent to d and independent to e (we refer to 
this Assumption as A1). However, in reality, a 
word w can inherit properties (e.g., emotions) 
(b): Bridged model 
Figure 1. he structure of PSSS odel 
(a): Series model 
Figure 2. Hidden emotional model 
984
from the document d that contains w. Thus, we 
can assume that w is implicitly dependant on e. 
To account for this, we present Bridged Hidden 
Emotional Model (BHEM) shown in Figure 2(b). 
Our assumption, A2, in the BHEM model is as 
follows: w is dependent to both d and e.  
Considering Figure 1, we represent the entire 
text collection as a set of (w,d,r) in which each 
observation (w,d,r) is associated with a set of 
unobserved emotions. If we assume that the ob-
served tuples are independently generated, the 
whole data set is generated based on the joint 
probability of the observation tuples (w,d,r) as 
the follows (Mohtarami et al, 2013): 
" =	###$%, , &',(,)																																						
'()
 
=	###$%, , &',(&(,) 									1
'()
 
where, P(w,d,r) is the joint probability of the tu-
ple (w,d,r), and n(w,d,r) is the frequency of w in 
document d of rating r (note that n(w,d) is the 
term frequency of w in d and n(d,r) is one if r is 
assigned to d, and 0 otherwise). The joint proba-
bility for the BHEM is defined as follows con-
sidering hidden emotion e: 
- regarding class probability of the hidden emotion e 
to be assigned to the observation (w,d,r): 
	$%, ,  = 	+$%, , |	$	
-
= 
	=	+$%, |	$|	$	
-
 
- regarding assumption A2 and Bayes' Rule: 
=	+$%|, 	$, 	$|	
-
 
- using Bayes' Rule: 
=	+$, 	|%$%$|	
-
 
- regarding A2 and conditional independency: 
		=	+$|%$	|%$%$|	
-
 
		= $|%+$%|	$	$|																																							2
-
 
In the bridged model, the joint probability does 
not depend on the probability P(d|e) and the 
probabilities P(w|e), P(e) and P(r|e) are un-
known, while in the SHEM model explained in 
(Mohtarami et al, 2013), the joint probability 
does not depend on P(w|e), and probabilities 
P(d|e), P(e), and P(r|e) are unknown.  
We employ Maximum Likelihood approach to 
learn the probabilities and infer the possible hid-
den emotions. The log-likelihood of the whole 
data set D in Equation (1) can be defined as fol-
lows: 
 
 = 	+++%, , log$%, , 														3
'()
 
Replacing P(w,d,r) by the values computed us-
ing the bridged model in Equation (2) results in: 

= 	+++%, , log[$|%+$%|	$	$|	
-
]
'()
 
										4 
The above optimization problems are hard to 
compute due to the log of sum. Thus, Expecta-
tion-maximization (EM) is usually employed. 
EM consists of two following steps: 
1. E-step: Calculates posterior probabilities for 
hidden emotions given the words, documents 
and ratings, and 
2. M-step: Updates unknown probabilities (such 
as P(w|e) etc) using the posterior probabilities 
in the E-step. 
The steps of EM can be computed for BHEM 
model. EM of the model employs assumptions 
A2 and Bayes Rule and is defined as follows: 
E-step: 
$	|%, ,  = $|	$	$%|	? $|	$	$%|	- 																												5 
M-step: 
$|	 = ? ? %, , $e|%, , '(? ? ? %, ,  $e|%, , '()  
														=	 ? %, $e|%, , '? ? %, $e|%, , ') 																														6 
$%|	 = ? ? %, , $e|%, , ()? ? ? %, , $e|%, , ()' 	 
															=	 ? %, $e|%, , )? ? %, $e|%, , )' 																													7 
$	 = ? ? ? %, , $e|%, , '()? ? ? ? %,, $e|%, , ')(8  
									= 	 ? ? %,  $e|%, , ')? ? ? %,  $e|%, , ')8 																								8 
Note that in Equation (5), the probability 
P(e|w,d,r) does not depend on the document d. 
Also, in Equations (6)-(8) we remove the de-
pendency on document d using the following 
Equation: 
+%, ,  =%, 
(
																					9 
where n(w,r) is the occurrence of w in all the 
documents in the rating r. 
The EM steps computed by the bridged model 
do not depend on the variable document d, and 
discard d from the model. The reason is that w 
bypasses d to directly associate with the hidden 
emotion e in Figure 2(b). 
985
  Similar to BHEM, the EM steps for SHEM can 
be computed by considering assumptions A1 and 
Bayes Rule as follows (Mohtarami et al, 2013): 
E-step: 
$	|%, ,  = $|	$	$|	? $|	$	$|	- 																											10 
M-step: 
$|	 = ? ? %, , $e|%, , '(? ? ? %, ,  $e|%, , '() 										11 
$|	 = ? ? %, , $e|%, , ')? ? ? %, ,  $e|%, , ')( 										12 
$	 = ? ? ? %, ,  $e|%, , '()? ? ? ? %, , $e|%, , ')(8 							13 
 
Finally, we construct the emotional vectors us-
ing the algorithm presented in Table 2. The algo-
rithm employs document-rating, term-document 
and term-rating matrices to infer the unknown 
probabilities. This algorithm can be used with 
both bridged or series models. Our goal is to in-
fer the emotional vector for each word w that can 
be obtained by the probability P(w|e). Note that, 
this probability can be simply computed for the 
SHEM model using P(d|e) as follows: 
$%|	 =+$%|$|	
(
																						14 
3.1 Enriching Hidden Emotional Models 
We enrich our emotional model by employing 
the requirement that the emotional vectors of two 
synonym words w1 and w2 should be similar. For 
this purpose, we utilize the semantic similarity 
between each two words and create an enriched 
matrix. Equation (15) shows how we compute 
this matrix. To compute the semantic similarity 
between word senses, we utilize their synsets as 
follows: 
 
%;%< = $=>%;|>%<? 
	= 1|>%;|	 +
1
|>%<| + $=%;|%<?
|@A&'B|
C
|@A&'D|
E
				15 
where, syn(w) is the synset of w. Let count(wi, 
wj) be the co-occurrence of the wi and wj, and let 
count(wj) be the total word count. The probabil-
ity of wi given wj will then be P(wi|wj) = 
count(wi, wj)/ count(wj). In addition, note that 
employing the synset of the words help to obtain 
different emotional vectors for each sense of a 
word.  
The resultant enriched matrix W?W is multi-
plied to the inputs of our hidden model (matrices 
W?D	or	W?R. Note that this takes into account  
Input: 
Series Model: Document-Rate D?R, Term-Document 
W?D 
Bridged Model: Term-Rate W?R 
Output: Emotional vectors {e1, e2, ?,ek} for w 
Algorithm: 
1. Enriching hidden emotional model: 
Series Model: Update Term-Document W?D 
Bridged Model: Update Term-Rate W?R 
2. Initialize unknown probabilities:  
Series Model: Initialize P(d|e), P(r|e), and P(e), ran-
domly 
Bridged Model: Initialize P(w|e), P(r|e), and P(e) 
3. while L  has not converged to a pre-specified value do 
4. E-step;  
Series Model: estimate the value of P(e|w,d,r) in 
Equation 10  
Bridged Model: estimate the value of P(e|w,d,r) in 
Equation 5 
5. M-step;  
Series Model: estimate the values of P(r|e), P(d|e), 
and P(e) in Equations 11-13, respectively 
Bridged Model: estimate the values of P(r|e), P(w|e), 
and P(e) in Equations 6-8, respectively 
6. end while 
7. If series hidden emotional model is used then 
8.  Infer word emotional vector: estimate P(w|e) in 
Equation 14.  
9. End if 
Table  2. Constructing emotional vectors via P(w|e)  
the senses of the words as well. The learning step 
of EM is done using the updated inputs. In this 
case, the correlated words can inherit the proper-
ties of each other. For example, if wi does not 
occur in a document or rating involving another 
word (i.e., wj), the word wi can still be indirectly 
associated with the document or rating through 
the word wj. However, the distribution of the 
opinion words in documents and ratings is not 
uniform. This may decrease the effectiveness of 
the enriched matrix.  
The nonuniform distribution of opinion words 
has been also reported by Amiri et al (2012) 
who showed that the positive words are frequent-
ly used in negative reviews. We also observed 
the same pattern in the development dataset. Fig-
ure 3 shows the overall occurrence of some posi-
tive and negative seeds in various ratings. As 
shown, in spite of the negative words, the posi-
tive words may frequently occur in both positive 
and negative documents. Such distribution of  
986
 Figure 3. Nonuniform distribution of opinion words 
positive words can mislead the enriched model. 
To address this issue, we measure the confi-
dence of an opinion word in the enriched matrix 
as follows.  
KL		' = M[NO'
P ?"O'P ? NO'R ? "O'R]NO'P ?"O'P + NO'R ?"O'R  16 
where, NO'P (NO'R) is the frequency of w in the 
ratings 1 to 4 (7 to 10), and "O'P ("O'R) is the 
total number of documents with rating 1 to 4 (7 
to 10) that contain w. The confidence value of w 
varies from 0 to 1, and it increases if: 
? There is a large difference between the occur-
rences of w in positive and negative ratings. 
? There is a large number of reviews involving 
w in the relative ratings. 
   To improve the efficiency of enriched matrix, 
the columns corresponding to each word in the 
matrix are multiplied by its confidence value.        
4 Predicting Sentiment Similarity 
We utilize the approach proposed in (Mohtarami 
et al, 2013) to compute the sentiment similarity 
between two words. This approach compares the 
emotional vector of the given words. Let X and Y 
be the emotional vectors of two words. Equation 
(17) computes their correlation: 
V, W = ? V; ? VXW; ? WX&;YZ? 1[\ 																																17 
where,  is number of emotional categories, V,] WX 
and [ , \  are the mean and standard deviation 
values of ^  and _  respectively. V, W = ?1 
indicates that the two vectors are completely dis-
similar, and V, W = 1 indicates that the vec-
tors have perfect similarity.  
The approach makes use of a thresholding 
mechanism to estimate the proper correlation 
value to find sentimentally similar words. For 
this, as in Mohtarami et al (2013) we utilized the 
antonyms of the words. We consider two words,  
Input: 
`: The adjective in the question of given IQAP. : The adjective in the answer of given IQAP. 
Output: answer ? {>	, , 	 } 
Algorithm: 
1. if ` or  are missing from our corpus then 
2.       answer=Uncertain; 
3. else if  `,  < 0 then 
4.             answer=No;  
5.        else if `,  > 0 then 
6.                   answer=yes; 
Figure 4. Sentiment similarity for IQAP inference 
%; and %< as similar in sentiment iff they satisfy 
both of the following conditions: 
1. =%; ,%<? > =%;,~%<?,  2. =%; ,%<? > =~%;,%<? 
where, ~%;  is antonym of %; , and =%; , %<? 
is obtained from Equation (17). Finally, we com-
pute the sentiment similarity (SS) as follows: 
=%; ,%<? = 
=%; ,%<? ?f 
g=%; ,~%<?, =~%;,%<?h			18 
Equation (18) enforces two sentimentally simi-
lar words to have weak correlation to the anto-
nym of each others. A positive value of SS(.,.) 
indicates the words are sentimentally similar and 
a negative value shows that they are dissimilar.  
5 Applications 
We explain our approach in utilizing sentiment 
similarity between words to perform IQAP infer-
ence and SO prediction tasks respectively.  
In IQAPs, we employ the sentiment similarity 
between the adjectives in questions and answers 
to interpret the indirect answers. Figure 4 shows 
the algorithm for this purpose. SS(.,.) indicates 
sentiment similarity computed by Equation (18). 
A positive SS means the words are sentimentally 
similar and thus the answer is yes. However, 
negative SS leads to a no response. 
In SO-prediction task, we aim to compute 
more accurate SO using our sentiment similarity 
method. Turney and Littman (2003) proposed a 
method in which the SO of a word is calculated 
based on its semantic similarity with seven posi-
tive words minus its similarity with seven nega-
tive words as shown in Figure 5. As the similari-
ty function, A(.,.), they employed point-wise mu-
tual information (PMI) to compute the similarity 
between the words. Here, we utilize the same 
approach, but instead of PMI we use our SS(.,.) 
measure as the similarity function. 
987
Input: $%: seven words with positive SO i%: seven words with negative SO . , . : similarity function, and %: a given word with 
unknown SO 
Output: sentiment orientation of w  
Algorithm: 
1. $ = j_% = 
+ %, %?	 + %, %
&'l)(m	n'l)(@o'l)(m	p'l)(@
 
Figure 5. SO based on the similarity function A(.,.) 
6 Evaluation and Results 
6.1 Data and Settings 
We used the review dataset employed by Maas et 
al. (2011) as the development dataset that con-
tains movie reviews with star rating from one 
star (most negative) to 10 stars (most positive). 
We exclude the ratings 5 and 6 that are more 
neutral. We used this dataset to compute all the 
input matrices in Table 2 as well as the enriched 
matrix. The development dataset contains 50k 
movie reviews and 90k vocabulary.  
We also used two datasets for the evaluation 
purpose: the MPQA (Wilson et al, 2005) and 
IQAPs (Marneffe et al, 2010) datasets. The 
MPQA dataset is used for SO prediction experi-
ments, while the IQAP dataset is used for the 
IQAP experiments. We ignored the neutral 
words in MPQA dataset and used the remaining 
4k opinion words. Also, the IQAPs dataset 
(Marneffe et al, 2010) contains 125 IQAPs and 
their corresponding yes or no labels as the 
ground truth. 
6.2 Experimental Results 
To evaluate our PSSS model, we perform exper-
iments on the SO prediction and IQAPs infer-
ence tasks. Here, we consider six emotions for 
both bridged and series models. We study the 
effect of emotion numbers in Section 7.1. Also, 
we set a threshold of 0.3 for the confidence value 
in Equation (16), i.e. we set the confidence val-
ues smaller than the threshold to 0. We explain 
the effect of this parameter in Section 7.3. 
Evaluation of SO Prediction 
We evaluate the performance of our PSSS mod-
els in the SO prediction task using the algorithm 
explained in Figure 5 by setting our PSSS as 
similarity function (A). The results on SO predic-
tion are presented in Table 3. The first and se- 
Method Precision Recall F1 
PMI 56.20 56.36 55.01 
ER 65.68 65.68 63.27 
PSSS-SHEM 68.51 69.19 67.96 
PSSS-BHEM 69.39 70.07 68.68 
Table 3. Performance on SO prediction task 
cond rows present the results of our baselines, 
PMI (Turney and Littman, 2003) and Expected 
Rating (ER) (Potts, 2011) of words respectively.  
PMI extracts the semantic similarity between 
words using their co-occurrences. As Table 3 
shows, it leads to poor performance. This is 
mainly due to the relatively small size of the de-
velopment dataset which affects the quality of 
the co-occurrence information used by the PMI.  
ER computes the expected rating of a word 
based on the distribution of the word across rat-
ing categories. The value of ER indicates the SO 
of the word. As shown in the two last rows of the 
table, the results of PSSS approach are higher 
than PMI and ER. The reason is that PSSS is 
based on the combination between sentiment 
space (through using ratings, and matrices W?R 
in BHEM, D?R in SHEM) and semantic space 
(through the input W?D in SHEM and enriched 
matrix W?W in both hidden models). However, 
the PMI employs only the semantic space (i.e., 
the co-occurrence of the words) and ER uses oc-
currence of the words in rating categories. 
Furthermore, the PSSS model achieves higher 
performance with BHEM rather than SHEM. 
This is because the emotional vectors of the 
words are directly computed from the EM steps 
of BHEM. However, the emotional vectors of 
SHEM are computed after finishing the EM steps 
using Equation (14). This causes the SHEM 
model to estimate the number and type of the 
hidden emotions with a lower performance as 
compared to BHEM, although the performances 
of SHEM and BHEM are comparable as ex-
plained in Section 7.1.  
Evaluation of IQAPs Inference  
To apply our PSSS on IQAPs inference task, we 
use it as the sentiment similarity measure in the 
algorithm explained in Figure 4. The results are 
presented in Table 4. The first and second rows 
are baselines. The first row is the result obtained 
by Marneffe et al (2010) approach. This ap-
proach is based on the similarity between the SO 
of the adjectives in question and answer. The 
second row of Table 4 show the results of using a 
popular semantic similarity measure, PMI, as the 
sentiment similarity (SS) measure in Figure 4.  
988
Method Prec. Rec. F1 
Marneffe et al (2010) 60.00 60.00 60.00 
PMI 60.61 58.70 59.64 
PSSS-SHEM  62.55 61.75 61.71 
PSSS-BHEM (w/o WSD) 65.90 66.11 63.74 
SS-BHEM (with WSD) 66.95 67.15 65.66 
Table 4. Performance on IQAP inference task 
The result shows that PMI is less effective to 
capture the sentiment similarity. 
Our PSSS approach directly infers yes or no 
responses using SS between the adjectives and 
does not require computing SO of the adjectives. 
In Table 4, PSSS-SHEM and PSSS-BHEM indi-
cate the results when we use our PSSS with 
SHEM and BHEM respectively. Table 4 shows 
the effectiveness of our sentiment similarity 
measure. Both models improve the performance 
over the baselines, while the bridged model leads 
to higher performance than the series model. 
Furthermore, we employ Word Sense Disam-
biguation (WSD) to disambiguate the adjectives 
in the question and its corresponding answer. For 
example, Q: ? Is that true? A: This is extraor-
dinary and preposterous. In the answer, the cor-
rect sense of the extraordinary is unusual and as 
such answer no can be correctly inferred. In the 
table, (w/o WSD) is based on the first sense (most 
common sense) of the words, whereas (with 
WSD) utilizes the real sense of the words. As 
Table 4 shows, WSD increases the performance. 
WSD could have higher effect, if more IQAPs 
contain adjectives with senses different from the 
first sense. 
7 Analysis and Discussions 
7.1 Number and Types of Emotions   
In our PSSS approach, there is no limitation on 
the number and types of emotions as we assumed 
emotions are hidden. In this Section, we perform 
experiments to predict the number and type of 
hidden emotions.  
Figure 6 and 7 show the results of the hidden 
models (SHEM and BHEM) on SO prediction 
and IQAPs inference tasks respectively with dif-
ferent number of emotions. As the Figures show, 
in both tasks, SHEM achieved high performanc-
es with 11 emotions. However, BHEM achieved 
high performances with six emotions. Now, the 
question is which emotion number should be 
considered? To answer this question, we further 
study the results as follows.  
First, for SHEM, there is no significant differ-
ence between the performances with six and 11 
emotions in the SO prediction task. This is the  
 
Figure 6. Performance of BHEM and SHEM on SO 
prediction through different #of emotions 
 
 
Figure 7. Performance of BHEM and SHEM on 
IQAPs inference through different #of emotions 
same for BHEM. Also, the performances of 
SHEM on the IQAP inference task with six and 
11 emotions are comparable. However, there is a 
significant difference between the performances 
of BHEM in six and 11 emotions. So, we consid-
er the dimension in which both hidden emotional 
models present a reasonable performance over 
both tasks. This dimension is six here. 
Second, as shown in the Figures 6 and 7, in 
contrast to BHEM, the performance of SHEM 
does not considerably change with different 
number of emotions over both tasks. This is be-
cause, in SHEM, the emotional vectors of the 
words are derived from the emotional vectors of 
the documents after the EM steps, see Equation 
(14). However, in BHEM, the emotional vectors 
are directly obtained from the EM steps. Thus, 
the bridged model is more sensitive than series 
model to the number of emotions. This could 
indicate that the bridged model is more accurate 
than the series model to estimate the number of 
emotions. 
Therefore, based on the above discussion, the 
estimated number of emotions is six in our de-
velopment dataset. This number may vary using 
different development datasets. 
In addition to the number of emotions, their 
types can also be interpreted using our approach. 
To achieve this aim, we sort the words based on 
their probability values, P(w|e), with respect to  
989
 Figure 8. Effect of synonyms & antonyms in SO pre-
diction task with different emotion numbers in BHEM 
Emotion#1 Emotion#2 Emotion#3 
excellent (1) 
magnificently(1) 
blessed (1) 
sublime (1) 
affirmation (1) 
tremendous (2) 
unimpressive (1) 
humorlessly (1) 
paltry (1) 
humiliating (1) 
uncreative (1) 
lackluster (1) 
disreputable (1) 
villian (1) 
onslaught (1) 
ugly (1) 
old (1) 
disrupt (1) 
Table 5. Sample words in three emotions 
each emotion. Then, the type of the emotions can 
be interpreted by observing the top k words in 
each emotion. For example, Table 5 shows the 
top 6 words for three out of six emotions ob-
tained for BHEM. The numbers in parentheses 
show the sense of the words. The corresponding 
emotions for these categories can be interpreted 
as "wonderful", "boring" and "disreputable", re-
spectively.  
We also observed that, in SHEM with eleven 
emotion numbers, some of the emotion catego-
ries have similar top k words such that they can 
be merged to represent the same emotion. Thus, 
it indicates that the BHEM is better than SHEM 
to estimates the number of emotions than SHEM. 
7.2 Effect of Synsets and Antonyms  
We show the important effect of synsets and an-
tonyms in computing the sentiment similarity of 
words. For this purpose, we repeat the experi-
ment for SO prediction by computing sentiment 
similarity of word pairs with and without using 
synonyms and antonyms. Figure 8 shows the 
results of obtained from BHEM. As the Figure 
shown, the highest performance can be achieved 
when synonyms and antonyms are used, while 
the lowest performance is obtained without using 
them. Note that, when the synonyms are not 
used, the entries of enriched matrix are computed 
using P(wi|wj) instead of P(syn(wi)|syn(wj)) in the 
Equation (15). Also, when the antonyms are not 
used, the Max(,) in Equation (18) is 0 and SS is 
computed using only correlation between words.  
The results show that synonyms can improve 
the performance. As Figure 8 shows, the two  
 
Figure 9. Effect of confidence values in SO prediction 
with different emotion numbers in BHEM 
highest performances are obtained when we use 
synonyms and the two lowest performances are 
achieved when we don't use synonyms. This is 
indicates that the synsets of the words can im-
prove the quality of the enriched matrix. The re-
sults also show that the antonyms can improve 
the result (compare WOSynWAnt with 
WOSynWOAnt). However, synonyms lead to 
greater improvement than antonyms (compare 
WSynWOAnt with WOSynWAnt). 
7.3 Effect of Confidence Value 
In Section 3.1, we defined a confidence value for 
each word to improve the quality of the enriched 
matrix. To illustrate the utility of the confidence 
value, we repeat the experiment for SO predic-
tion by BHEM using all the words appears in 
enriched matrix with different confidence 
thresholds. The results are shown in Figure 9, 
"w/o confidence" shows the results when we 
don?t use the confidence values, while "with con-
fidence" shows the results when use the confi-
dence values. Also, "confidence>x" indicates the 
results when we set al the confidence value 
smaller than x to 0. The thresholding helps to 
eliminate the effect of low confident words.  
As Figure 9 shows, "w/o confidence" leads to 
the lowest performance, while "with confidence" 
improves the performance with different number 
of emotions. The thresholding is also effective. 
For example, a threshold like 0.3 or 0.4 improves 
the performance. However, if a large value (e.g., 
0.6) is selected as threshold, the performance 
decreases. This is because a large threshold fil-
ters a large number of words from enriched mod-
el that decreases the effect of the enriched ma-
trix.        
7.4 Convergence Analysis 
The PSSS approach is based on the EM algo-
rithm for the BHEM (or SHEM) presented in 
Table 2. This algorithm performs a predefined 
990
number of iterations or until convergence. To 
study the convergence of the algorithm, we re-
peat our experiments for SO prediction and 
IQAPs inference tasks using BHEM with differ-
ent number of iterations. Figure 10 shows that 
after the first 15 iterations the performance does 
not change dramatically and is nearly constant 
when more than 30 iterations are performed. This 
shows that our algorithm will converge in less 
than 30 iterations for BHEM. We observed the 
same pattern in SHEM. 
7.5 Bridged Vs. Series Model  
The bridged and series models are both based on 
the hidden emotions that were developed to pre-
dict the sense sentiment similarity. Although 
their best results on the SO prediction and IQAPs 
inference tasks are comparable, they have some 
significant differences as follows: 
? BHEM is considerably faster than SHEM. The 
reason is that, the input matrix of BHEM (i.e., 
W?R) is significantly smaller than the input 
matrix of SHEM (i.e., W?D). 
?  In BHEM, the emotional vectors are directly 
computed from the EM steps. However, the 
emotional vector of a word in SHEM is com-
puted using the emotional vectors of the doc-
uments containing the word. This adds noises 
to the emotional vectors of the words.  
? BHEM gives more accurate estimation over 
type and number of emotions versus SHEM. 
The reason is explained in Section 7.1. 
8 Related Works 
Sentiment similarity has not received enough 
attention to date. Most previous works employed 
semantic similarity of word pairs to address SO 
prediction and IQAP inference tasks. Turney and 
Littman (2003) proposed to compute pair-wised 
mutual information (PMI) between a target word 
and a set of seed positive and negative words to 
infer the SO of the target word. They also uti-
lized Latent Semantic Analysis (LSA) (Landauer 
et al, 1998) as another semantic similarity meas-
ure. However, both PMI and LSA are semantic 
similarity measure. Similarly, Hassan and Radev 
(2010) presented a graph-based method for pre-
dicting SO of words. They constructed a lexical 
graph where nodes are words and edges connect 
two words with semantic similarity obtained 
from Wordnet (Fellbaum 1998). They propagat-
ed the SO of a set of seeds through this graph. 
However, such approaches did not take into ac-
count the sentiment similarity between words.  
 
Figure 10. Convergence of BHEM 
In IQAPs, Marneffe et al (2010) inferred the 
yes/no answers using SO of the adjectives. If SO 
of the adjectives have different signs, then the 
answer conveys no, and Otherwise, if the abso-
lute value of SO for the adjective in question is 
smaller than the absolute value of the adjective in 
answer, then the answer conveys yes, and other-
wise no. In Mohtarami et al (2012), we used two 
semantic similarity measures (PMI and LSA) for 
the IQAP inference task. We showed that meas-
uring the sentiment similarities between the ad-
jectives in question and answer leads to higher 
performance as compared to semantic similarity 
measures. 
In Mohtarami et al (2012), we proposed an 
approach to predict the sentiment similarity of 
words using their emotional vectors. We as-
sumed that the type and number of emotions are 
pre-defined and our approach was based on this 
assumption. However, in previous research, there 
is little agreement about the number and types of 
basic emotions. Furthermore, the emotions in 
different dataset can be varied. We relaxed this 
assumption in Mohtarami et al, (2013) by con-
sidering the emotions as hidden and presented a 
hidden emotional model called SHEM. This pa-
per also consider the emotions as hidden and pre-
sents another hidden emotional model called 
BHEM that gives more accurate estimation of 
the numbers and types of the hidden emotions.   
9 Conclusion 
We propose a probabilistic approach to infer the 
sentiment similarity between word senses with 
respect to automatically learned hidden emo-
tions. We propose to utilize the correlations be-
tween reviews, ratings, and words to learn the 
hidden emotions. We show the effectiveness of 
our method in two NLP tasks. Experiments show 
that our sentiment similarity models lead to ef-
fective emotional vector construction and signif-
icantly outperform semantic similarity measures 
for the two NLP task. 
991
References  
Hadi Amiri and Tat S. Chua. 2012. Mining Slang 
and Urban Opinion Words and Phrases from 
cQA Services: An Optimization Approach. 
Proceedings of the fifth ACM international confer-
ence on Web search and data mining (WSDM). Pp. 
193-202. 
Christiane Fellbaum. 1998. WordNet: An Electron-
ic Lexical Database. Cambridge, MA: MIT 
Press. 
Ahmed Hassan and Dragomir Radev. 2010. Identify-
ing Text Polarity Using Random Walks. Pro-
ceeding in the Association for Computational Lin-
guistics (ACL). Pp: 395?403. 
Aminul Islam and Diana Inkpen. 2008. Semantic text 
similarity using corpus-based word similarity 
and string similarity. ACM Transactions on 
Knowledge Discovery from Data (TKDD). 
Carroll E. Izard. 1971. The face of emotion. New 
York: Appleton-Century-Crofts. 
Soo M. Kim and Eduard Hovy. 2004. Determining 
the sentiment of opinions. Proceeding of the 
Conference on Computational Linguistics 
(COLING). Pp: 1367?1373. 
Thomas K. Landauer, Peter W. Foltz, and Darrell 
Laham. 1998. Introduction to Latent Semantic 
Analysis. Discourse Processes. Pp: 259-284. 
Andrew L. Maas, Raymond E. Daly, Peter T. Pham, 
Dan Huang, Andrew Y. Ng, and Christopher Potts. 
2011. Learning Word Vectors for Sentiment 
Analysis. Proceeding in the Association for Com-
putational Linguistics (ACL). Pp:142-150. 
Marie-Catherine D. Marneffe, Christopher D. Man-
ning, and Christopher Potts. 2010. "Was it good? 
It was provocative." Learning the meaning of 
scalar adjectives. Proceeding in the Association 
for Computational Linguistics (ACL). Pp: 167?
176. 
Mitra Mohtarami, Hadi Amiri, Man Lan, Thanh P. 
Tran, and Chew L. Tan. 2012. Sense Sentiment 
Similarity: An Analysis. Proceeding of the Con-
ference on Artificial Intelligence (AAAI). 
Mitra Mohtarami, Man Lan, and Chew L. Tan. 2013. 
From Semantic to Emotional Space in Proba-
bilistic Sense Sentiment Analysis. Proceeding of 
the Conference on Artificial Intelligence (AAAI). 
Alena Neviarouskaya, Helmut Prendinger, and 
Mitsuru Ishizuka. 2007. Textual Affect Sensing 
for Sociable and Expressive Online Communi-
cation. Proceedings of the conference on Affective 
Computing and Intelligent Interaction (ACII). Pp: 
218-229. 
Alena Neviarouskaya, Helmut Prendinger, and 
Mitsuru Ishizuka. 2009. SentiFul: Generating a 
Reliable Lexicon for Sentiment Analysis. Pro-
ceeding of the conference on Affective Computing 
and Intelligent Interaction (ACII). Pp: 363-368. 
Andrew Ortony and Terence J. Turner. 1990. What's 
Basic About Basic Emotions. American Psycho-
logical Association. 97(3), 315-331. 
Christopher Potts, C. 2011. On the negativity of 
negation. In Nan Li and David Lutz, eds., Pro-
ceedings of Semantics and Linguistic Theory 20, 
636-659. 
Peter D. Turney and Michael L. Littman. 2003. 
Measuring Praise and Criticism: Inference of 
Semantic Orientation from Association. ACM 
Transactions on Information Systems, 21(4), 315?
346. 
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 
2005. Recognizing contextual polarity in 
phrase-level sentiment analysis. Proceeding in 
HLT-EMNLP. Pp: 347?354. 
 
992
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 226?229,
Uppsala, Sweden, 15-16 July 2010.
c?2010 Association for Computational Linguistics
ECNU: Effective Semantic Relations Classification without Complicated
Features or Multiple External Corpora
Yuan Chen
?
, Man Lan
?,?
, Jian Su
?
, Zhi Min Zhou
?
, Yu Xu
?
?
East China Normal University, Shanghai, PRC.
?
Institute for Infocomm Research, Singapore.
lanman.sg@gmail.com
Abstract
This paper describes our approach to the
automatic identification of semantic rela-
tions between nominals in English sen-
tences. The basic idea of our strategy
is to develop machine-learning classifiers
which: (1) make use of class-independent
features and classifier; (2) make use of
a simple and effective feature set without
high computational cost; (3) make no use
of external annotated or unannotated cor-
pus at all. At SemEval 2010 Task 8 our
system achieved an F-measure of 75.43%
and a accuracy of 70.22%.
1 Introduction
Knowledge extraction of semantic relations be-
tween pairs of nominals from English text is one
important application both as an end in itself and
as an intermediate step in various downstream
NLP applications, such as information extraction,
summarization, machine translation, QA etc. It is
also useful for many auxiliary tasks such as word
sense disambiguation, language modeling, para-
phrasing and discourse relation processing.
In the past decade, semantic relation classifica-
tion has attracted a lot of interest from researchers
and a wide variety of relation classification
schemes exist in the literature. However, most
research work is quite different in definition of
relations and granularities of various applications.
That is, there is little agreement on relation
inventories. SemEval 2010 Task 8 (Hendrickx
et al, 2008) provides a new standard benchmark
for semantic relation classification to a wider
community, where it defines 9 relations includ-
ing CAUSE-EFFECT, COMPONENT-WHOLE,
CONTENT-CONTAINER, ENTITY-DESTINATION,
ENTITY-ORIGIN, INSTRUMENT-AGENCY,
MEMBER-COLLECTION, MESSAGE-TOPIC,
PRODUCT-PRODUCER, and a tenth pseudo-
relation OTHER (where relation is not one of the
9 annotated relations).
Unlike the previous semantic relation task in
SemEval 2007 Task 4, the current evaluation pro-
vides neither query pattern for each sentence nor
manually annotated word sense (in WordNet se-
mantic) for each nominals. Since its initiative is
to provide a more realistic real-world application
design that is practical, any classification system
must be usable without too much effort. It needs
to be easily computable. So we need to take into
account the following special considerations.
1. The extracted features for relation are ex-
pected to be easily computable. That is, the
steps in the feature extraction process are to
be simple and direct for the purpose of reduc-
ing errors possibly introduced by many NLP
tools. Furthermore, a unified (global) feature
set is set up for all relations rather than for
each relation.
2. Most previous work at SemEval 2007 Task
4 leveraged on external theauri or corpora
(whether unannotated or annotated) (Davi-
dov and Rappoport, 2008), (Costello, 2007),
(Beamer et al, 2007) and (Nakov and Hearst,
2008) that make the task adaption to different
domains and languages more difficult, since
they would not have such manually classified
or annotated corpus available. From a practi-
cal point of view, our system would make use
of less resources.
3. Most previous work at Semeval 2007 Task
4 constructed several local classifiers on dif-
ferent algorithms or different feature subsets,
one for each relation (Hendrickx et al, 2007)
and (Davidov and Rappoport, 2008). Our ap-
proach is to build a global classifier for all
relations in practical NLP settings.
226
Based on the above considerations, the idea of
our system is to make use of external resources as
less as possible. The purpose of this work is two-
fold. First, it provides an overview of our simple
and effective process for this task. Second, it com-
pares different features and classification strate-
gies for semantic relation.
Section 2 presents the system description. Sec-
tion 3 describes the results and discussions. Sec-
tion 4 concludes this work.
2 System Description
2.1 Features Extraction
For each training and test sentence, we reduce the
annotated target entities e1 and e2 to single nouns
noun1 and noun2, by keeping their last nouns only,
which we assume to be heads.
We create a global feature set for all relations.
The features extracted are of three types, i.e., lex-
ical, morpho-syntactic and semantic. The feature
set consists of the following 6 types of features.
Feature set 1: Lemma of target entities e1
and e2. The lemma of the entities annotated in
the given sentence.
Feature set 2: Stem and POS of words be-
tween e1 and e2. The stem and POS tag of the
words between two nominals. First all the words
between two nominals were extracted and then the
Porter?s stemming was performed to reduce words
to their base forms (Porter, 1980). Meanwhile,
OpenNLP postag tool was used to return part-of-
speech tagging for each word.
Feature set 3: syntactic pattern derived from
syntactic parser between e1 and e2. Typically,
the verb phrase or preposition phrase which con-
tain the nominals are important for relation clas-
sification. Therefore, OpenNLP Parser was per-
formed to do full syntactic parsing for each sen-
tence. Then for each nominal, we look for its par-
ent node in the syntactic tree until the parent node
is a verb phrase or preposition phrase. Then the
label of this phrase and the verb or preposition of
this phrase were extracted as the syntactic features.
Besides, we also extracted other 3 feature types
with the aid of WordNet.
Feature set 4: WordNet semantic class of e1
and e2. The WordNet semantic class of each an-
notated entity in the relation. If the nominal has
two and more words, then we examine the seman-
tic class of ?w1 w2? in WordNet. If no result re-
turned from WordNet, we examine the semantic
class of head in the nominal. Since the cost of
manually WSD is expensive, the system simply
used the first (most frequent) noun senses for those
words.
Feature set 5: meronym-holonym relation
between e1 and e2. The meronym-holonym
relation between nominals. These information
are quite important for COMPONENT-WHOLE and
MEMBER-COLLECTION relations. WordNet3.0
provides meronym and holonym information for
some nouns. The features are extracted in the fol-
lowing steps. First, for nominal e1, we extract its
holonym from WN and for nominal e2, we extract
its Synonyms/Hypernyms. Then, the system will
check if there is same word between e1?s holonym
and e2?s synonym & hypernym. The yes or no
result will be a binary feature. If yes, we also ex-
amine the type of this match is ?part of ? or ?mem-
ber of ? in holonym result. Then this type is also
a binary feature. After that, we exchange the posi-
tion of e1 and e2 and perform the same process-
ing. By creating these features, the system can
also take the direction of relations into account.
Feature set 6: hyponym-hypernym rela-
tion between nominal and the word of ?con-
tainer?. This feature is designed for CONTENT-
CONTAINER relation. For each nominal, WordNet
returns its hypernym set. Then the system examine
if the hypernym set contains the word ?container?.
The result leads to a binary feature.
2.2 Classifier Construction
Our system is to build up a global classifier based
on global feature set for all 9 non-Other relations.
Generally, for this multi-class task, there are two
strategies for building classifier, which both con-
struct classifier on a global feature set. The first
scheme is to treat this multi-class task as an multi-
way classification. Since each pair of nominals
corresponds to one relation, i.e., single label clas-
sification, we build up a 10-way SVM classifier for
all 10 relations. Here, we call it multi-way clas-
sification. That is, the system will construct one
single global classifier which can classify 10 rela-
tions simultaneously in a run. The second scheme
is to split this multi-class task into multiple binary
classification tasks. Thus, we build 9 binary SVM
classifiers, one for each non-Other relation. Noted
that in both strategies the classifiers are built on
global feature set for all relations. For the sec-
ond multiple binary classification, we also exper-
227
imented on different prob. thresholds, i.e., 0.25
and 0.5. Furthermore, in order to reduce errors
and boost performance, we also adopt the major-
ity voting strategy to combine different classifiers.
3 Results and Discussion
3.1 System Configurations and Results
The classifiers for all relations were optimized
independently in a number of 10-fold cross-
validation (CV) experiments on the provided train-
ing sets. The feature sets and learning algorithms
which were found to obtain the highest accuracies
for each relation were then used when applying the
classifiers to the unseen test data.
Table 1 summaries the 7 system configurations
we submitted and their performance on the test
data.
Among the above 7 system, SR5 system shows
the best macro-averaged F1 measure. Table 2 de-
scribes the statistics and performance obtained per
relation on the SR5 system.
Table 3 shows the performance of these 7 sys-
tems on the test data as a function of training set
size.
3.2 Discussion
The first three systems are based on three feature
sets, i.e.,F1-F3, with different classification strat-
egy. The next three systems are based on all six
feature sets with different classification strategy.
The last system adopts majority voting scheme on
the results of four systems, i.e., SR1, SR2, SR4
and SR5. Based on the above series of exper-
iments and results shown in the above 3 tables,
some interesting observations can be found as fol-
lows.
Obviously, although we did not perform WSD
on each nominal and only took the first noun sense
as semantic class, WordNet significantly improved
the performance. This result is consistent with
many previous work on Semeval 2007 Task 4 and
once again it shows that WordNet is important
for semantic relation classification. Specifically,
whether for multi-way classification or multiple
binary classification, the systems involved features
extracted from WordNet performed better than the
others not involved WN, for example, SR4 better
than SR1 (74.82% vs 60.08%), SR5 better than
SR2 (75.43% vs 72.59%), SR6 better than SR3
(72.19% vs 68.50%).
Generally, the performance of multiple binary
classifier is better than multi-way classifier. That
means, given a global feature set for 9 relations,
the performance of 9 binary classifiers is better
than a 10-way classifier. Specifically, when F1-F3
are involved, SR2 (72.59%) and SR3 (68.50%) are
both better than SR1 (60.08%). However, when
F1-F6 feature sets are involved, the performance
of SR4 is between that of SR5 and SR6 in terms of
macro-averaged F
1
measure. With respect to ac-
curacy measure (Acc), SR4 system performs the
best.
Moreover, for multiple binary classification, the
threshold of probability has impact on the perfor-
mance. Generally, the system with prob. threshold
0.25 is better than that with 0.5, for example, SR2
better than SR3 (72.59% vs 68.50%), SR5 better
than SR6 (75.43% vs 72.19%).
As an ensemble system, SR7 combines the re-
sults of SR1, SR2, SR4 and SR5. However, this
majority voting strategy has not shown significant
improvements. The possible reason may be that
these classifiers come from a family of SVM clas-
sifiers and thus the random errors are not signifi-
cantly different.
Besides, one interesting observation is that SR4
system achieved the top 2 performance on TD1
data amongst all participating systems. This
shows that, even with less training data, SR4 sys-
tem achieves good performance.
Acknowledgments
This work is supported by grants from Na-
tional Natural Science Foundation of China
(No.60903093), Shanghai Pujiang Talent Program
(No.09PJ1404500) and Doctoral Fund of Ministry
of Education of China (No.20090076120029).
References
I. Hendrickx, S. N. Kim, Z. Kozareva, P. Nakov, D.
?
O S?eaghdha, S. Pad?o, M. Pennacchiotti, L. Ro-
mano and S. Szpakowicz. SemEval-2010 Task 8:
Multi-Way Classification of Semantic Relations Be-
tween Pairs of Nominals. In Proceedings of the 5th
SIGLEX Workshop on Semantic Evaluation, pp.94-
99, 2010, Uppsala, Sweden.
D. Davidov and A. Rappoport. Classification of
Semantic Relationships between Nominals Using
Pattern Clusters. Proceedings of ACL-08: HLT,
pp.227-235, 2008.
F. J. Costello. UCD-FC: Deducing semantic rela-
tions using WordNet senses that occur frequently
228
Run Feature Set Classifier P (%) R (%) F
1
(%) Acc (%)
SR1 F1-F3 multi-way classification 70.69 58.05 60.08 57.05
SR2 F1-F3 multiple binary (prob. threshold =0.25) 74.02 71.61 72.59 67.10
SR3 F1-F3 multiple binary (prob. threshold =0.5) 80.25 60.92 68.50 62.02
SR4 F1-F6 multi-way classification 75.72 74.16 74.82 70.52
SR5 F1-F6 multiple binary (prob. threshold =0.25) 75.88 75.29 75.43 70.22
SR6 F1-F6 multiple binary (prob. threshold =0.5) 83.08 64.72 72.19 65.81
SR7 F1-F6 majority voting based on SR1, SR2, SR4 and SR5 74.83 75.97 75.21 70.15
Table 1: Summary of 7 system configurations and performance on the test data. Precision, Recall, F1
are macro-averaged for system?s performance on 9 non-Other relations and evaluated with directionality
taken into account.
Run Total # P (%) R (%) F
1
(%) Acc (%)
Cause-Effect 328 83.33 86.89 85.07 86.89
Component-Whole 312 74.82 65.71 69.97 65.71
Content-Container 192 79.19 81.25 80.21 81.25
Entity-Destination 292 79.38 86.99 83.01 86.99
Entity-Origin 258 81.01 81.01 81.01 81.01
Instrument-Agency 156 63.19 58.33 60.67 58.33
Member-Collection 233 73.76 83.26 78.23 83.26
Message-Topic 261 75.2 73.18 74.17 73.18
Product-Producer 231 73.06 61.04 66.51 61.04
Other 454 38.56 40.09 39.31 40.09
Micro-Average 76.88 76.27 76.57 70.22
Macro-Average 75.88 75.29 75.43 70.22
Table 2: Performance obtained per relation on SR5 system. Precision, Recall, F1 are macro-averaged for
system?s performance on 9 non-Other relations and evaluated with directionality taken into account.
Run TD1 TD2 TD3 TD4
F
1
(%) Acc (%) F
1
(%) Acc (%) F
1
(%) Acc (%) F
1
(%) Acc (%)
SR1 52.13 49.50 56.58 54.84 58.16 56.16 60.08 57.05
SR2 46.24 38.90 47.99 40.45 69.83 64.67 72.59 67.10
SR3 39.89 34.56 42.29 36.66 65.47 59.59 68.50 62.02
SR4 67.95 63.45 70.58 66.14 72.99 68.94 74.82 70.52
SR5 49.32 41.59 50.70 42.77 72.63 67.72 75.43 70.22
SR6 42.88 36.99 45.54 39.57 69.87 64.00 72.19 65.81
SR7 58.67 52.71 58.87 53.18 72.79 68.09 75.21 70.15
Table 3: Performance of these 7 systems on the test data as a function of training set size. The four
training subsets, TD1, TD2, TD3 and TD4, have 1000, 2000, 4000 and 8000 (complete) training samples
respectively. F1 is macro-averaged for system?s performance on 9 non-Other relations and evaluated
with directionality taken into account.
in a database of noun-noun compounds. ACL Se-
mEval?07 Workshop, pp.370C373, 2007.
B. Beamer, S. Bhat, B. Chee, A. Fister, A. Rozovskaya
and R.Girju. UIUC: A knowledge-rich approach
to identifying semantic relations between nominals.
ACL SemEval?07 Workshop, pp.386-389, 2007.
I. Hendrickx, R. Morante, C. Sporleder and A. Bosch.
ILK: machine learning of semantic relations with
shallow features and almost no data. ACL Se-
mEval?07 Workshop, pp.187C190, 2007.
P. Nakov and M. A. Hearst. Solving Relational Simi-
larity Problems Using the Web as a Corpus. In Pro-
ceedings of ACL, pp.452-460, 2008.
M. Porter. An algorithm for suffix stripping. In Pro-
gram, vol. 14, no. 3, pp.130-137, 1980.
229
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 575?578,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Tiantianzhu7:System Description of Semantic Textual Similarity (STS) in
the SemEval-2012 (Task 6)
Tiantian Zhu
Department of Computer Science and
Technology
East China Normal University
51111201046@student.ecnu.edu.cn
Man Lan
Department of Computer Science and
Technology
East China Normal University
mlan@cs.ecnu.edu.cn
Abstract
This paper briefly reports our submissions to
the Semantic Textual Similarity (STS) task
in the SemEval 2012 (Task 6). We first use
knowledge-based methods to compute word
semantic similarity as well as Word Sense Dis-
ambiguation (WSD). We also consider word
order similarity from the structure of the sen-
tence. Finally we sum up several aspects of
similarity with different coefficients and get
the sentence similarity score.
1 Introduction
The task of semantic textual similarity (STS) is to
measure the degree of semantic equivalence between
two sentences. It plays an increasingly important
role in several text-related research and applications,
such as text mining, Web page retrieval, automatic
question-answering, text summarization, and ma-
chine translation. The goal of the Semeval 2012 STS
task (task 6) is to build a unified framework for the
evaluation of semantic textual similarity modules for
different systems and to characterize their impact on
NLP applications.
Generally, there are two ways to measure sim-
ilarity of two sentences, i.e, corpus-based meth-
ods and knowledge-based methods. The corpus-
based method typically computes sentence similar-
ity based on the frequency of word occurrence or the
co-occurrence between collocated words. For ex-
ample, in (Islam and Inkpen, 2008) they proposed a
corpus-based sentence similarity measure as a func-
tion of string similarity, word similarity and com-
mon word order similarity (CWO). The knowledge-
based method computes sentence similarity based
on the semantic information collected from knowl-
edge bases. With the aid of a number of success-
ful computational linguistic projects, many seman-
tic knowledge bases are readily available, for ex-
ample, WordNet, Spatial Date Transfer Standard,
Gene Ontology, etc. Among them, the most widely
used one is WordNet, which is organized by mean-
ings and developed at Princeton University. Sev-
eral methods computed word similarity by using
WordNet, such as the Lesk method in (Banerjee and
Pedersen, 2003), the lch method in (Leacock and
Chodorow, 1998)and the wup method in (Wu and
Palmer, 1994). Generally, although the knowledge-
based methods heavily depend on the knowledge
bases, they performed much better than the corpus-
based methods in most cases. Therefore, in our STS
system, we use a knowledge-based method to com-
pute word similarity.
The rest of this paper is organized as follows. Sec-
tion 2 describes our system. Section 3 presents the
results of our system.
2 System Description
Usually, a sentence is composed of some nouns,
verbs, adjectives, adverbs and/or some stop words.
We found that these words carry a lot of informa-
tion, especially the nouns and verbs. Although the
adjectives and adverbs also make contribution to the
semantic meaning of the sentence, they are much
weaker than the nouns and verbs. So we consider
to measure the sentence semantic similarities from
three aspects. We define the following three types of
similarity from two compared sentences to measure
575
the semantic similarity: (1) Noun Similarity to mea-
sure the similarity between the nouns from the two
compared sentences, (2) Verb Similarity to measure
the similarity between Verbs, (3) ADJ-ADV Simi-
larity to measure the similarity between the adjec-
tives and adverbs from each sentence. Besides the
semantic information similarity, we also found that
the structure of the sentences carry some informa-
tion which cannot be ignored. Therefore, we define
the last aspect of the sentence similarity as Word Or-
der Similarity. In the following we will introduce the
different components of our system.
2.1 POS
As a basic natural language processing technique,
part of speech tagging is to identify the part of
speech of individual words in the sentence. In or-
der to compute the three above semantic similari-
ties, we first identify the nouns, verbs, adjectives,
and adverbs in the sentence. Then we can calculate
the Noun Similarity, Verb Similarity and ADJ-ADV
Similarity from two sentences.
2.2 Semantic similarity between words
The word similarity measurement have important
impact on the performance of sentence similarity.
Currently, many lexical resources based approaches
perform comparatively well to compute semantic
word similarities. However, the exact resources they
are based are quite different. For example, some are
based on dictionary and/or thesaurus, and others are
based on WordNet.
WordNet is a machine-readable lexical database.
The words in Wordnet are classified into four cat-
egories, i.e., nouns, verbs, adjectives and adverbs.
WordNet groups these words into sets of syn-
onyms called synsets, provides short definitions, and
records the various semantic relations between these
synsets. The synsets are interlinked by means of
conceptual-semantic and lexical relations. Word-
Net alo provides the most common relationships
include Hyponym/Hypernym (i.e., is-a relationships)
and Meronym/Holonym (i.e., part-of relationships).
Nouns and verbs are organized into hierarchies
based on the hyponymy/hypernym relation between
synsets while adjectives and adverbs are not.
In this paper, we adopt the wup method in (Wu
and Palmer, 1994) to estimate the semantic similar-
ity between two words, which estimates the seman-
tic similarity between two words based on the depth
of the two words in WordNet and the depth of their
least common subsumer (LCS), where LCS is de-
fined as the common ancestor deepest in the taxon-
omy.
For example, given two words, w1 and w2, the
semantic similarity s(w1,w2) is the function of their
depth in the taxonomy and the depth of their least
common subsumer. If d1 and d2 are the depth of
w1 and w2 in WordNet, and h is the depth of their
least common subsumer in WordNet, the semantic
similarity can be written as:
s(w1, w2) =
2.0 ? h
d1 + d2
(1)
2.3 Word Sense Disambiguation
Word Sense Disambiguation (WSD) is to identify
the actual meaning of a word according to the con-
text. In our word similarity method, we take the
nearest meaning of two words into consideration
rather than their actual meaning. More impor-
tantly, the nearest meaning does not always repre-
sent the actual meaning. In our system, we used
a WSD algorithm proposed by (Ted Pedersen et
al.,2005), which computes semantic relatedness of
word senses using extended gloss overlaps of their
dictionary definitions. We utilize this WSD algo-
rithm for each sentence to get the actual meaning of
each word before computing the word semantic sim-
ilarity.
2.4 Semantic Similarity
We adopt a similar way to compute the three types of
semantic similarities. Here we take Noun Similarity
as an example.
Suppose sentence s1 and s2 are the two sentences
to be compared, s1 has a nouns while s2 has b nouns.
Then we get a ? b noun pairs and use the word sim-
ilarity method mentioned in section 2.2 to compute
the Noun Similarity of each noun pair. After that,
for each noun, we choose its highest score in noun
pairs as its similarity score. Then we use the formula
below to compute the Noun Similarity.
SimNoun =
(
?c
i=1 ni) ? (a + b)
2ab
(2)
576
where c represents the number of noun words in
sequence a and sequence b, c = min(a, b); ni rep-
resents the highest matching similarity score of i-th
word in the shorter sequence with respect to one of
the words in the longer sequence; and
?c
i=1 ni rep-
resents the sum of the highest matching similarity
score between the words in sequence a and sequence
b. Similarly, we can get SimV erb. Since there is no
Hyponym/Hypernym relation for adjectives and ad-
verbs in WordNet, we just compute ADJ-ADV Sim-
ilarity based on the frequency of overlap of simple
words.
2.5 Word Order Similarity
We believe that word order information also make
contributions to sentence similarity. In most cases,
the longer common sequence (LCS) the two sen-
tences have, the higher similarity score the sentences
get. For example the pair of sentences s1 and s2, we
remove all the punctuation from the sentences:
? s1: But other sources close to the sale said
Vivendi was keeping the door open to further
bids and hoped to see bidders interested in in-
dividual assets team up
? s2: But other sources close to the sale said
Vivendi was keeping the door open for further
bids in the next day or two
Since the length of the longest common sequence
is 14, we use the following formula to compute the
word order similarity.
SimWordOrder =
lengthofLCS
shorterlength
(3)
where the shorter length means the length of the
shorter sentence.
2.6 Overall Similarity
After we have the Noun Similarity, Verb Similar-
ity, ADJ-ADV Similarity and Word Order Similar-
ity, we calculate the Overall Similarity of two com-
pared sentences based on these four scores of simi-
larity. We combine them in the following way:
Simsent = aSimNoun + bSimV erb+
cSimADJ?ADV + dSimWordOrder
(4)
Where a, b, c and d are the coefficients which
denote the contribution of each aspect to the over-
all sentence similarity, For different data collections,
we empirically set different coefficients, for exam-
ple, for the MSR Paraphrase data, the four coeffi-
cients are set as 0.5, 0.3, 0.1, 0.1, because it is hard
to get the highest score 5 even when the two sen-
tences are almost the same meaning, We empirically
set a threshold, if the score exceeds the threshold we
set the score 5.
3 Experiment and Results on STS
Firstly, Stanford parser1 is used to parse each
sentence and to tag each word with a part of
speech(POS). Secondly, WordNet SenseRelate All-
Words2, a WSD tool from CPAN is used to disam-
biguate and to assign a sense for each word based on
the assigned POS.
We submitted three runs: run 1 with WSD, run 2
without WSD, run 3 removing stop words and with-
out WSD. The stoplist is available online3. Table 1
lists the performance of these three systems as well
as the baseline and the rank 1 results on STS task in
SemEval 2012.
We can see that run1 gets the best result, which
means WSD has improved the accuracy of sentence
similarity. Run3 gets better result than run2, which
proves that stop words do disturb the computation of
sentence similarity, removing them is a better choice
in our system.
4 Conclusion
In our work, we adopt a knowledge-based word sim-
ilarity method with WSD to measure the seman-
tic similarity between two sentences from four as-
pects: Noun Similarity, Verb Similarity, ADJ-ADV
Similarity and Word Order Similarity. The results
show that WSD improves the pearson coefficient at
some degree. However, our system did not get a
good rank. It indicates there still exists many prob-
lems such as wrong POS tag and wrong WSD which
might lead to wrong meaning of one word in a sen-
tence.
1http://nlp.stanford.edu/software/lex-parser.shtml
2http://search.cpan.org/Tedpederse/WordNet-SenseRelate-
AllWords-0.19
3http://jmlr.csail.mit.edu/papers/volume5/lewis04a/a11-
smart-stop-list/english.stop
577
Table 1: STS system configuration and results on STS task.
Run ALL ALLnrm Mean MSRpar MSRvid SMTeur OnWN SMTnews
rank 1 .7790 .8579 .6773 .6830 .8739 .5280 .6641 .4937
baseline .3110 .6732 .4356 .4334 .2996 .4542 .5864 .3908
1 .4533 .7134 .4192 .4184 .5630 .2083 .4822 .2745
2 .4157 .7099 .3960 .4260 .5628 .1546 .4552 .1923
3 .4446 .7097 .3740 .3411 .5946 .1868 .4029 .1823
Acknowledgments
The authors would like to thank the organizers for
their invaluable support making STS a first-rank and
interesting international event.
References
Chukfong Ho, Masrah Azrifah Azmi Murad, Rabiah Ab-
dul Kadir, Shyamala C. Doraisamy. 2010. Word Sense
Disambiguation-based Sentence Similarity. In Proc.
COLING-ACL, Beijing.
Jin Feng, Yiming Zhou, Trevor Martin. 2008. Sen-
tence Similarity based on Relevance. Proceedings of
IPMU?08, Torremolinos.
Yuhua Li, David McLean, Zuhair A. Bandar, James D.
O?Shea, and Keeley Crockett. 2009. Sentence Simi-
larity Based on Semantic Nets and Corpus Statistics.
LIN LI, XIA HU, BI-YUN HU, JUN WANG, YI-MING
ZHOU. 2009. MEASURING SENTENCE SIMILAR-
ITY FROM DIFFERENT ASPECTS.
Islam Aminul and Diana Inkpen. 2008. Semantic Text
Similarity Using Corpus-Based Word Similarity and
String Similarity. ACM Transactions on Knowledge
Discovery from Data.
Banerjee and Pedersen. 2003. Extended gloss overlaps
as a measure of semantic relatedness. In Proceed-
ings of the Eighteenth International Joint Conference
on Artificial Intelligence (IJCAI-03), pages805C810,
Acapulco, Mexico.
Leacock and Chodorow. 1998. Combining local con-
text and WordNet similarity for word sense identifica-
tion. In Christiane Fellbaum, editor, WordNet: An
Electronic Lexical Database. The MIT Press, Cam-
bridge,MA.
Z.Wu and M.Palmer. 1994. Verbs semantics and
lexical selection. In Proceedings of the 32nd an-
nual meeting on Association for Computional Linguis-
tics,Morristown, NJ, USA.
Ted Pedersen, Satanjeev Banerjee, Siddharth Patward-
han. 2005. Maximizing Semantic Relatedness to Per-
form Word Sense Disambiguation.
578
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 124?131, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics
ECNUCS: Measuring Short Text Semantic Equivalence Using Multiple
Similarity Measurements
Tian Tian ZHU
Department of Computer Science and
Technology
East China Normal University
51111201046@student.ecnu.edu.cn
Man LAN?
Department of Computer Science and
Technology
East China Normal University
mlan@cs.ecnu.edu.cn
Abstract
This paper reports our submissions to the
Semantic Textual Similarity (STS) task in
?SEM Shared Task 2013. We submitted three
Support Vector Regression (SVR) systems in
core task, using 6 types of similarity mea-
sures, i.e., string similarity, number similar-
ity, knowledge-based similarity, corpus-based
similarity, syntactic dependency similarity and
machine translation similarity. Our third sys-
tem with different training data and different
feature sets for each test data set performs the
best and ranks 35 out of 90 runs. We also sub-
mitted two systems in typed task using string
based measure and Named Entity based mea-
sure. Our best system ranks 5 out of 15 runs.
1 Introduction
The task of semantic textual similarity (STS) is to
measure the degree of semantic equivalence between
two sentences, which plays an increasingly impor-
tant role in natural language processing (NLP) ap-
plications. For example, in text categorization (Yang
and Wen, 2007), two documents which are more
similar are more likely to be grouped in the same
class. In information retrieval (Sahami and Heil-
man, 2006), text similarity improves the effective-
ness of a semantic search engine by providing in-
formation which holds high similarity with the input
query. In machine translation (Kauchak and Barzi-
lay, 2006), sentence similarity can be applied for
automatic evaluation of the output translation and
the reference translations. In question answering
(Mohler and Mihalcea, 2009), once the question and
the candidate answers are treated as two texts, the
answer text which has a higher relevance with the
question text may have higher probability to be the
right one.
The STS task in ?SEM Shared Task 2013 consists
of two subtasks, i.e., core task and typed task, and
we participate in both of them. The core task aims
to measure the semantic similarity of two sentences,
resulting in a similarity score which ranges from 5
(semantic equivalence) to 0 (no relation). The typed
task is a pilot task on typed-similarity between semi-
structured records. The types of similarity to be
measured include location, author, people involved,
time, events or actions, subject and description as
well as the general similarity of two texts (Agirre et
al., 2013).
In this work we present a Support Vector Re-
gression (SVR) system to measure sentence seman-
tic similarity by integrating multiple measurements,
i.e., string similarity, knowledge based similarity,
corpus based similarity, number similarity and ma-
chine translation metrics. Most of these similari-
ties are borrowed from previous work, e.g., (Ba?r et
al., 2012), (S?aric et al, 2012) and (de Souza et al,
2012). We also propose a novel syntactic depen-
dency similarity. Our best system ranks 35 out of
90 runs in core task and ranks 5 out of 15 runs in
typed task.
The rest of this paper is organized as follows. Sec-
tion 2 describes the similarity measurements used in
this work in detail. Section 3 presents experiments
and the results of two tasks. Conclusions and future
work are given in Section 4.
124
2 Text Similarity Measurements
To compute semantic textual similarity, previous
work has adopted multiple semantic similarity mea-
surements. In this work, we adopt 6 types of
measures, i.e., string similarity, number similarity,
knowledge-based similarity, corpus-based similar-
ity, syntactic dependency similarity and machine
translation similarity. Most of them are borrowed
from previous work due to their superior perfor-
mance reported. Besides, we also propose two syn-
tactic dependency similarity measures. Totally we
get 33 similarity measures. Generally, these simi-
larity measures are represented as numerical values
and combined using regression model.
2.1 Preprocessing
Generally, we perform text preprocessing before we
compute each text similarity measurement. Firstly,
Stanford parser1 is used for sentence tokenization
and parsing. Specifically, the tokens n?t and ?m are
replaced with not and am. Secondly, Stanford POS
Tagger2 is used for POS tagging. Thirdly, Natu-
ral Language Toolkit3 is used for WordNet based
Lemmatization, which lemmatizes the word to its
nearest base form that appears in WordNet, for ex-
ample, was is lemmatized as is, not be.
Given two short texts or sentences s1 and s2, we
denote the word set of s1 and s2 as S1 and S2, the
length (i.e., number of words) of s1 and s2 as |S1|
and |S2|.
2.2 String Similarity
Intuitively, if two sentences share more strings, they
are considered to have higher semantic similarity.
Therefore, we create 12 string based features in con-
sideration of the common sequence shared by two
texts.
Longest Common sequence (LCS). The widely
used LCS is proposed by (Allison and Dix, 1986),
which is to find the maximum length of a com-
mon subsequence of two strings and here the sub-
sequence need to be contiguous. In consideration of
the different length of two texts, we compute LCS
1http://nlp.stanford.edu/software/lex-parser.shtml
2http://nlp.stanford.edu/software/tagger.shtml
3http://nltk.org/
similarity using Formula (1) as follows:
SimLCS =
Length of LCS
min(|S1|, |S2|)
(1)
In order to eliminate the impacts of various forms
of word, we also compute a Lemma LCS similarity
score after sentences being lemmatized.
word n-grams. Following (Lyon et al, 2001), we
calculate the word n-grams similarity using the Jac-
card coefficient as shown in Formula (2), where p is
the number of n-grams shared by s1 and s2, q and r
are the number of n-grams not shared by s1 and s2,
respectively.
Jacc = pp + q + r (2)
Since we focus on short texts, here only n=1,2,3,4
is used in this work. Similar with LCS, we also com-
pute a Lemma n-grams similarity score.
Weighted Word Overlap (WWO). (S?aric et al,
2012) pointed out that when measuring sentence
similarity, different words may convey different con-
tent information. Therefore, we consider to assign
more importance to those words bearing more con-
tent information. To measure the importance of each
word, we use Formula (3) to calculate the informa-
tion content for each word w:
ic(w) = ln
?
w??C freq(w?)
freq(w) (3)
where C is the set of words in the corpus and
freq(w) is the frequency of the word w in the cor-
pus. To compute ic(w), we use the Web 1T 5-gram
Corpus4, which is generated from approximately
one trillion word tokens of text from Web pages.
Obviously, the WWO scores between two sen-
tences is non-symmetric. The WWO of s2 by s1 is
given by Formula (4):
Simwwo(s1, s2) =
?
w?S1?S2 ic(w)
?
w??S2 ic(w?)
(4)
Likewise, we can get Simwwo(s2, s1) score.
Then the final WWO score is the harmonic mean of
Simwwo(s1, s2) and Simwwo(s2, s1). Similarly, we
get a Lemma WWO score as well.
4http://www.ldc.upenn.edu/Catalog/docs/LDC2006T13
125
2.3 Knowledge Based Similarity
Knowledge based similarity approaches rely on
a semantic network of words. In this work
all knowledge-based word similarity measures are
computed based on WordNet. For word similarity,
we employ four WordNet-based similarity metrics:
the Path similarity (Banea et al, 2012); the WUP
similarity (Wu and Palmer, 1994); the LCH similar-
ity (Leacock and Chodorow, 1998); the Lin similar-
ity (Lin, 1998). We adopt the NLTK library (Bird,
2006) to compute all these word similarities.
In order to determine the similarity of sentences,
we employ two strategies to convert the word simi-
larity into sentence similarity, i.e., (1) the best align-
ment strategy (align) (Banea et al, 2012) and (2) the
aggregation strategy (agg) (Mihalcea et al, 2006).
The best alignment strategy is computed as below:
Simalign(s1, s2) =
(? +
?|?|
i=1 ?i) ? (2|S1||S2|)
|S1| + |S2|
(5)
where ? is the number of shared terms between s1
and s2, list ? contains the similarities of non-shared
words in shorter text, ?i is the highest similarity
score of the ith word among all words of the longer
text. The aggregation strategy is calculated as be-
low:
Simagg(s1, s2) =
?
w?S1(maxSim(w, S2) ? ic(w))
?
w?{S1} ic(w)
(6)
where maxSim(w,S2) is the highest WordNet-
based score between word w and all words of sen-
tence S2. To compute ic(w), we use the same cor-
pus as WWO, i.e., the Web 1T 5-gram Corpus. The
final score of the aggregation strategy is the mean of
Simagg(s1, s2) and Simagg(s2, s1). Finally we get
8 knowledge based features.
2.4 Corpus Based Similarity
Latent Semantic Analysis (LSA) (Landauer et al,
1997). In LSA, term-context associations are cap-
tured by means of a dimensionality reduction op-
eration performing singular value decomposition
(SVD) on the term-by-context matrix T , where T
is induced from a large corpus. We use the TASA
corpus5 to obtain the matrix and compute the word
5http://lsa.colorado.edu/
similarity using cosine similarity of the two vectors
of the words. After that we transform word similar-
ity to sentence similarity based on Formula (5).
Co-occurrence Retrieval Model (CRM) (Weeds,
2003). CRM is based on a notion of substitutabil-
ity. That is, the more appropriate it is to substitute
word w1 in place of word w2 in a suitable natural
language task, the more semantically similar they
are. The degree of substitutability of w2 with w1
is dependent on the proportion of co-occurrences of
w1 that are also the co-occurrences of w2, and the
proportion of co-occurrences of w2 that are also the
co-occurrences of w1. Following (Weeds, 2003), the
CRM word similarity is computed using Formula
(7):
SimCRM (w1, w2) =
2 ? |c(w1) ? c(w2)|
|c(w1)| + |c(w2)|
(7)
where c(w) is the set of words that co-occur with
w. We use the 5-gram part of the Web 1T 5-gram
Corpus to obtain c(w). If two words appear in one
5-gram, we will treat one word as the co-occurring
word of each other. To obtain c(w), we propose two
methods. In the first CRM similarity, we only con-
sider the word w with |c(w)| > 200, and then take
the top 200 co-occurring words ranked by the co-
occurrence frequency as its c(w). To relax restric-
tions, we also present an extended CRM (denoted
by ExCRM), which extends the CRM list that all w
with |c(w)| > 50 are taken into consideration, but
the maximum of |c(w)| is still set to 200. Finally,
these two CRM word similarity measures are trans-
formed to sentence similarity using Formula (5).
2.5 Syntactic Dependency Similarity
As (S?aric et al, 2012) pointed out that dependency
relations of sentences often contain semantic infor-
mation, in this work we propose two novel syntactic
dependency similarity features to capture their pos-
sible semantic similarity.
Simple Dependency Overlap. First we measure the
simple dependency overlap between two sentences
based on matching dependency relations. Stanford
Parser provides 53 dependency relations, for exam-
ple:
nsubj(remain ? 16, leader ? 4)
dobj(return ? 10, home ? 11)
126
where nsubj (nominal subject) and dobj (direct ob-
ject) are two dependency types, remain is the gov-
erning lemma and leader is the dependent lemma.
Two syntactic dependencies are considered equal
when they have the same dependency type, govern-
ing lemma, and dependent lemma.
Let R1 and R2 be the set of all dependency rela-
tions in s1 and s2, we compute Simple Dependency
Overlap using Formula (8):
SimSimDep(s1, s2) =
2 ? |R1 ? R2| ? |R1||R2|
|R1| + |R2|
(8)
Special Dependency Overlap. Several types of de-
pendency relations are believed to contain the pri-
mary content of a sentence. So we extract three roles
from those special dependency relations, i.e., pred-
icate, subject and object. For example, from above
dependency relation dobj, we can extract the object
of the sentence, i.e., home. For each of these three
roles, we get a similarity score. For example, to cal-
culate Simpredicate, we denote the sets of predicates
of two sentences as Sp1 and Sp2. We first use LCH to
compute word similarity and then compute sentence
similarity using Formula (5). Similarly, the Simsubj
and Simobj are obtained in the same way. In the end
we average the similarity scores of the three roles as
the final Special Dependency Overlap score.
2.6 Number Similarity
Numbers in the sentence occasionally carry similar-
ity information. If two sentences contain different
sets of numbers even though their sentence structure
is quite similar, they may be given a low similarity
score. Here we adopt two features following (S?aric
et al, 2012), which are computed as follow:
log(1 + |N1| + |N2|) (9)
2 ? |N1 ? N2|/(|N1| + |N2|) (10)
where N1 and N2 are the sets of all numbers in s1
and s2. We extract the number information from
sentences by checking if the POS tag is CD (cardinal
number).
2.7 Machine Translation Similarity
Machine translation (MT) evaluation metrics are de-
signed to assess whether the output of a MT sys-
tem is semantically equivalent to a set of reference
translations. The two given sentences can be viewed
as one input and one output of a MT system, then
the MT measures can be used to measure their se-
mantic similarity. We use the following 6 lexical
level metrics (de Souza et al, 2012): WER, TER,
PER, NIST, ROUGE-L, GTM-1. All these measures
are obtained using the Asiya Open Toolkit for Auto-
matic Machine Translation (Meta-) Evaluation6.
3 Experiment and Results
3.1 Regression Model
We adopt LIBSVM7 to build Support Vector Regres-
sion (SVR) model for regression. To obtain the op-
timal SVR parameters C, g, and p, we employ grid
search with 10-fold cross validation on training data.
Specifically, if the score returned by the regression
model is bigger than 5 or less than 0, we normalize
it as 5 or 0, respectively.
3.2 Core Task
The organizers provided four different test sets to
evaluate the performance of the submitted systems.
We have submitted three systems for core task, i.e.,
Run 1, Run 2 and Run 3. Run 1 is trained on all
training data sets with all features except the num-
ber based features, because most of the test data do
not contain number. Run 2 uses the same feature sets
as Run 1 but different training data sets for different
test data as listed in Table 1, where different training
data sets are combined together as they have simi-
lar structures with the test data. Run 3 uses different
feature sets as well as different training data sets for
each test data. Table 2 shows the best feature sets
used for each test data set, where ?+? means the fea-
ture is selected and ?-? means not selected. We did
not use the whole feature set because in our prelimi-
nary experiments, some features performed not well
on some training data sets, and they even reduced
the performance of our system. To select features,
we trained two SVR models for each feature, one
with all features and another with all features except
this feature. If the first model outperforms the sec-
ond model, this feature is chosen.
Table 3 lists the performance of these three sys-
tems as well as the baseline and the best results on
6http://nlp.lsi.upc.edu/asiya/
7http://www.csie.ntu.edu.tw/ cjlin/libsvm/
127
Test Training
Headline MSRpar
OnWN+FNWN MSRpar+OnWN
SMT SMTnews+SMTeuroparl
Table 1: Different training data sets used for each test data set
type Features Headline OnWN and FNWN SMT
LCS + + -
Lemma LCS + + -
String N-gram + 1+2gram 1gram
Based Lemma N-gram + 1+2gram 1gram
WWO + + +
Lemma WWO + + +
Path,WUP,LCH,Lin + + +
Knowledge +aligh
Based Path,WUP,LCH,Lin + + +
+ic-weighted
Corpus LSA + + +
Based CRM,ExCRM + + +
Simple Dependency + + +
Syntactic Overlap
Dependency Special Dependency + - +
Overlap
Number Number + - -
WER - + +
TER - + +
PER + + +
MT NIST + + -
ROUGE-L + + +
GTM-1 + + +
Table 2: Best feature combination for each data set
System Mean Headline OnWN FNWN SMT
Best 0.6181 0.7642 0.7529 0.5818 0.3804
Baseline 0.3639 0.5399 0.2828 0.2146 0.2861
Run 1 0.3533 0.5656 0.2083 0.1725 0.2949
Run 2 0.4720 0.7120 0.5388 0.2013 0.2504
Run 3 (rank 35) 0.4967 0.6799 0.5284 0.2203 0.3595
Table 3: Final results on STS core task
STS core task in ?SEM Shared Task 2013. For the
three runs we submitted to the task organizers, Run
3 performs the best results and ranks 35 out of 90
runs. Run 2 performs much better than Run 1. It in-
dicates that using different training data sets for dif-
ferent test sets indeed improves results. Run 3 out-
performs Run 2 and Run 1. It shows that our feature
selection process for each test data set does help im-
128
prove the performance too. From this table, we find
that different features perform different on different
kinds of data sets and thus using proper feature sub-
sets for each test data set would make improvement.
Besides, results on the four test data sets are quite
different. Headline always gets the best result on
each run and OnWN follows second. And results
of FNWN and SMT are much lower than Headline
and OnWN. One reason of the poor performance of
FNWN may be the big length difference of sentence
pairs. That is, sentence from WordNet is short while
sentence from FrameNet is quite longer, and some
samples even have more than one sentence (e.g. ?do-
ing as one pleases or chooses? VS ?there exist a
number of different possible events that may happen
in the future in most cases, there is an agent involved
who has to consider which of the possible events will
or should occur a salient entity which is deeply in-
volved in the event may also be mentioned?). As
a result, even though the two sentences are similar
in meaning, most of our measures would give low
scores due to quite different sentence length.
In order to understand the contributions of each
similarity measurement, we trained 6 SVR regres-
sion models based on 6 types on MSRpar data set.
Table 4 presents the Pearson?s correlation scores
of the 6 types of measurements on MSRpar. We
can see that the corpus-based measure achieves the
best, then the knowledge-based measure and the MT
measure follow. Number similarity performs sur-
prisingly well, which benefits from the property of
data set that MSRpar contains many numbers in sen-
tences and the sentence similarity depends a lot on
those numbers as well. The string similarity is not
as good as the knowledge-based, the corpus-based
and the MT similarity because of its disability of ex-
tracting semantic characteristics of sentence. Sur-
prisingly, the Syntactic dependency similarity per-
forms the worst. Since we only extract two features
based on sentence dependency, they may not enough
to capture the key semantic similarity information
from the sentences.
3.3 Typed Task
For typed task, we also adopt a SVR model for
each type. Since several previous similarity mea-
sures used for core task are not suitable for evalu-
ation of the similarity of people involved, time pe-
Features results
string 0.4757
knowledge-based 0.5640
corpus-based 0.5842
syntactic dependency 0.3528
number 0.5278
MT metrics 0.5595
Table 4: Pearson correlation of features of the six aspects
on MSRpar
riod, location and event or action involved, we add
two Named Entity Recognition (NER) based fea-
tures. Firstly we use Stanford NER8 to obtain per-
son, location and date information from the whole
text with NER tags of ?PERSON?, ?LOCATION?
and ?DATE?. Then for each list of entity, we get two
feature values using the following two formulas:
SimNER Num(L1NER, L2NER) =
min(|L1NER|, |L2NER|)
max(|L1NER|, |L2NER|)
(11)
SimNER(L1NER, L2NER) =
Num(equalpairs)
|L1NER| ? |L2NER|
(12)
where LNER is the list of one entity type from
the text, and for two lists of NERs L1NER and
L2NER, there are |L1NER| ? |L2NER| NER pairs.
Num(equalpairs) is the number of equal pairs.
Here we expand the condition of equivalence: two
NERs are considered equal if one is part of another
(e.g. ?John Warson? VS ?Warson?). Features and
content we used for each similarity are presented in
Table 5. For the three similarities: people involved,
time period, location, we compute the two NER
based features for each similarity with NER type of
?PERSON?, ?LOCATION? and ?DATE?. And for
event or action involved, we add the above 6 NER
feature scores as its feature set. The NER based sim-
ilarity used in description is the same as event or ac-
tion involved but only based on ?dcDescription? part
of text. Besides, we add a length feature in descrip-
tion, which is the ratio of shorter length and longer
length of descriptions.
8http://nlp.stanford.edu/software/CRF-NER.shtml
129
Type Features Content used
author string based (+ knowledge based for Run2) dcCreator
people involved NER based whole text
time period NER based whole text
location NER based whole text
event or action involved NER based whole text
subject string based (+ knowledge based for Run2) dcSubject
description string based, NER based,length dcDescription
General the 7 similarities above
Table 5: Feature sets and content used of 8 type similarities of Typed data
We have submitted two runs. Run 1 uses only
string based and NER based features. Besides fea-
tures used in Run 1, Run 2 also adds knowledge
based features. Table 6 shows the performance of
our two runs as well as the baseline and the best re-
sults on STS typed task in ?SEM Shared Task 2013.
Our Run 1 ranks 5 and Run 2 ranks 7 out of 15 runs.
Run 2 performed worse than Run 1 and the possible
reason may be the knowledge based method is not
suitable for this kind of data. Furthermore, since we
only use NER based features which involves three
entities for these similarities, they are not enough to
capture the relevant information for other types.
4 Conclusion
In this paper we described our submissions to the
Semantic Textual Similarity Task in ?SEM Shared
Task 2013. For core task, we collect 6 types of simi-
larity measures, i.e., string similarity, number sim-
ilarity, knowledge-based similarity, corpus-based
similarity, syntactic dependency similarity and ma-
chine translation similarity. And our Run 3 with dif-
ferent training data and different feature sets for each
test data set ranks 35 out of 90 runs. For typed task,
we adopt string based measure, NER based mea-
sure and knowledge based measure, our best system
ranks 5 out of 15 runs. Clearly, these similarity mea-
sures are not quite enough. For the core task, in our
future work we will consider the measures to eval-
uate the sentence difference as well. For the typed
task, with the help of more advanced IE tools to ex-
tract more information regarding different types, we
need to propose more methods to evaluate the simi-
larity.
Acknowledgments
The authors would like to thank the organizers and
reviewers for this interesting task and their helpful
suggestions and comments, which improved the fi-
nal version of this paper. This research is supported
by grants from National Natural Science Foundation
of China (No.60903093), Shanghai Pujiang Talent
Program (No.09PJ1404500), Doctoral Fund of Min-
istry of Education of China (No.20090076120029)
and Shanghai Knowledge Service Platform Project
(No.ZF1213).
References
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. 2013. *sem 2013 shared
task: Semantic textual similarity, including a pilot on
typed-similarity. In *SEM 2013: The Second Joint
Conference on Lexical and Computational Semantics.
Association for Computational Linguistics.
Lloyd Allison and Trevor I Dix. 1986. A bit-string
longest-common-subsequence algorithm. Information
Processing Letters, 23(5):305?310.
Carmen Banea, Samer Hassan, Michael Mohler, and
Rada Mihalcea. 2012. Unt: A supervised synergistic
approach to semantic text similarity. pages 635?642.
First Joint Conference on Lexical and Computational
Semantics (*SEM).
Daniel Ba?r, Chris Biemann, Iryna Gurevych, and Torsten
Zesch. 2012. Ukp: Computing semantic textual sim-
ilarity by combining multiple content similarity mea-
sures. pages 435?440. First Joint Conference on Lex-
ical and Computational Semantics (*SEM).
Steven Bird. 2006. Nltk: the natural language toolkit. In
Proceedings of the COLING/ACL on Interactive pre-
sentation sessions, pages 69?72. Association for Com-
putational Linguistics.
130
System general author people time location event subject description mean
Best 0.7981 0.8158 0.6922 0.7471 0.7723 0.6835 0.7875 0.7996 0.7620
Baseline 0.6691 0.4278 0.4460 0.5002 0.4835 0.3062 0.5015 0.5810 0.4894
Run 1 0.6040 0.7362 0.3663 0.4685 0.3844 0.4057 0.5229 0.6027 0.5113
Run 2 0.6064 0.5684 0.3663 0.4685 0.3844 0.4057 0.5563 0.6027 0.4948
Table 6: Final results on STS typed task
Jose? Guilherme C de Souza, Matteo Negri, Trento Povo,
and Yashar Mehdad. 2012. Fbk: Machine trans-
lation evaluation and word similarity metrics for se-
mantic textual similarity. pages 624?630. First Joint
Conference on Lexical and Computational Semantics
(*SEM).
David Kauchak and Regina Barzilay. 2006. Para-
phrasing for automatic evaluation. In Proceedings of
the main conference on Human Language Technol-
ogy Conference of the North American Chapter of the
Association of Computational Linguistics, pages 455?
462. Association for Computational Linguistics.
Thomas K Landauer, Darrell Laham, Bob Rehder, and
Missy E Schreiner. 1997. How well can passage
meaning be derived without using word order? a com-
parison of latent semantic analysis and humans. In
Proceedings of the 19th annual meeting of the Cog-
nitive Science Society, pages 412?417.
Claudia Leacock and Martin Chodorow. 1998. Com-
bining local context and wordnet similarity for word
sense identification. WordNet: An electronic lexical
database, 49(2):265?283.
Dekang Lin. 1998. An information-theoretic defini-
tion of similarity. In Proceedings of the 15th inter-
national conference on Machine Learning, volume 1,
pages 296?304. San Francisco.
Caroline Lyon, James Malcolm, and Bob Dickerson.
2001. Detecting short passages of similar text in large
document collections. In Proceedings of the 2001
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 118?125.
Rada Mihalcea, Courtney Corley, and Carlo Strapparava.
2006. Corpus-based and knowledge-based measures
of text semantic similarity. In Proceedings of the na-
tional conference on artificial intelligence, volume 21,
page 775. Menlo Park, CA; Cambridge, MA; London;
AAAI Press; MIT Press; 1999.
Michael Mohler and Rada Mihalcea. 2009. Text-to-text
semantic similarity for automatic short answer grad-
ing. In Proceedings of the 12th Conference of the Eu-
ropean Chapter of the Association for Computational
Linguistics, pages 567?575. Association for Computa-
tional Linguistics.
Mehran Sahami and Timothy D Heilman. 2006. A web-
based kernel function for measuring the similarity of
short text snippets. In Proceedings of the 15th interna-
tional conference on World Wide Web, pages 377?386.
ACM.
Frane S?aric, Goran Glavas?, Mladen Karan, Jan S?najder,
and Bojana Dalbelo Bas?ic. 2012. Takelab: Systems
for measuring semantic text similarity. pages 441?
448. First Joint Conference on Lexical and Compu-
tational Semantics (*SEM).
Julie Elizabeth Weeds. 2003. Measures and applications
of lexical distributional similarity. Ph.D. thesis, Cite-
seer.
Zhibiao Wu and Martha Palmer. 1994. Verbs semantics
and lexical selection. In Proceedings of the 32nd an-
nual meeting on Association for Computational Lin-
guistics, pages 133?138. Association for Computa-
tional Linguistics.
Cha Yang and Jun Wen. 2007. Text categorization based
on similarity approach. In Proceedings of Interna-
tional Conference on Intelligence Systems and Knowl-
edge Engineering (ISKE).
131
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 118?123, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
ECNUCS: Recognizing Cross-lingual Textual Entailment Using Multiple
Text Similarity and Text Difference Measures
Jiang ZHAO
Department of Computer
Science and Technology
East China Normal University
Shanghai, P.R.China
51121201042@ecnu.cn
Man LAN?
Department of Computer
Science and Technology
East China Normal University
Shanghai, P.R.China
mlan@cs.ecnu.edu.cn
Zheng-Yu NIU
Baidu Inc.
Beijing, P.R.China
niuzhengyu@baidu.com
Abstract
This paper presents our approach used for
cross-lingual textual entailment task (task 8)
organized within SemEval 2013. Cross-
lingual textual entailment (CLTE) tries to de-
tect the entailment relationship between two
text fragments in different languages. We
solved this problem in three steps. Firstly,
we use a off-the-shelf machine translation
(MT) tool to convert the two input texts into
the same language. Then after performing a
text preprocessing, we extract multiple feature
types with respect to surface text and gram-
mar. We also propose novel feature types
regarding to sentence difference and seman-
tic similarity based on our observations in the
preliminary experiments. Finally, we adopt a
multiclass SVM algorithm for classification.
The results on the cross-lingual data collec-
tions provided by SemEval 2013 show that (1)
we can build portable and effective systems
across languages using MT and multiple ef-
fective features; (2) our systems achieve the
best results among the participants on two test
datasets, i.e., FRA-ENG and DEU-ENG.
1 Introduction
The Cross-lingual Textual Entailment (CLTE) task
in SemEval 2013 consists in detecting the entail-
ment relationship between two topic-related text
fragments (usually called T(ext) and H(ypothesis))
in different languages, which is a cross-lingual ex-
tension of TE task in (Dagan and Glickman, 2004).
We say T entails H if the meaning of H can be in-
ferred from the meaning of T. Mehdad et al (2010b)
firstly proposed this problem within a new challeng-
ing application scenario, i.e., content synchroniza-
tion. In consideration of the directionality, the task
needs to assign one of the following entailment judg-
ments to a pair of sentences (1) forward: unidirec-
tional entailment from T to H; (2) backward: unidi-
rectional entailment from H to T; (3) bidirectional:
the two fragments entail each other (i.e., semantic
equivalence); (4) non-entailment: there is no entail-
ment between T and H.
During the last decades, many researchers and
communities have paid a lot of attention to resolve
the TE detection (e.g., seven times of the Rec-
ognizing Textual Entailment Challenge, i.e., from
RTE1 to RET7, have been held) since identifying
the relationship between two sentences is at the core
of many NLP applications, such as text summa-
rization (Lloret et al, 2008) or question answer-
ing (Harabagiu and Hickl, 2006). For example,
in text summarization, a redundant sentence should
be omitted from the summary if this sentence can
be entailed from other expressions in the summary.
CLTE extends those tasks with lingual dimension-
ality, where more than one language is involved.
Although it is a relatively new task, a basic solu-
tion has been provided in (Mehdad et al, 2010b),
which brings the problem back to monolingual sce-
nario using MT to translate H into the language of
T. The promising performance indicates the poten-
tialities of such a simple approach which integrates
MT and monolingual TE algorithms (Castillo, 2011;
Jimenez et al, 2012; Mehdad et al, 2010a).
In this work, we regard CLTE as a multiclass clas-
sification problem, in which multiple feature types
are used in conjunction with a multiclass SVM clas-
sifier. Specifically, our approach can be divided
into three steps. Firstly, following (Espla`-Gomis
et al, 2012; Meng et al, 2012), we use MT to
118
bridge the gap of language differences between T
and H. Secondly, we perform a preprocessing pro-
cedure to maximize the similarity of the two text
fragments so as to make a more accurate calcula-
tion of surface text similarity measures. Besides sev-
eral features described in previous work (Malakasi-
otis, 2009; Espla`-Gomis et al, 2012), we also pro-
pose several novel features regarding to sentence dif-
ference and semantic similarity. Finally, all these
features are combined together and serves as input
of a multiclass SVM classifier. After analyzing of
the results obtained in preliminary experiments, we
also cast this problem as a hierarchical classification
problem.
The remainder of the paper is organized as fol-
lows. Section 2 describes different features used in
our systems. Section 3 presents the system settings
including the datasets and preprocessing. Section 4
shows the results of different systems on different
language pairs. Finally, we conclude this paper with
future work in Section 5.
2 Features
In this section, we will describe a variety of feature
types used in our experiments.
2.1 Basic features
The BC feature set consists of length measures on
variety sets including |A|, |B|, |A?B|, |B?A|, |A?
B|, |A ? B|, |A|/|B| and |B|/|A|, where A and B
represent two texts, and the length of set is the num-
ber of non-repeated elements in this set. Once we
view the text as a set of words, A?B means the set
of words found in A but not in B, A ? B means the
set of words found in either A or B and A?B means
the set of shared words found in both A and B.
Given a pair of texts, i.e., <T,H>, which are in
different languages, we use MT to translate one of
them to make them in the same language. Thus,
we can get two pairs of texts, i.e., <Tt,H> and
<T,H t>. We apply the above eight length measures
to the two pairs, resulting in a total of 16 features.
2.2 Surface Text Similarity features
Following (Malakasiotis and Androutsopoulos,
2007), the surface text similarity (STS) feature set
contains nine similarity measures:
Jaccard coefficient: It is defined as |A?B|
|A?B| , where
|A ?B| and |A ?B| are as in the BC.
Dice coefficient: Defined as 2?|A?B|
|A|+|B| .
Overlap coefficient: This is the following quantity,
Overlap(A,B) = |A?B|
|A| .
Weighted overlap coefficient: We assign the tf*idf
value to each word in the sentence to distinguish
the importance of different words. The weighted
overlap coefficient is defined as follows:
WOverlap(A,B) =
?
w
i
?A?B Wwi
?
w
i
?AWwi
,
where Ww
i
is the weight of word wi.
Cosine similarity: cos(??x ,??y ) =
??x ???y
?
??x ?????y ? , where
??x and ??y are vectorial representations of texts (i.e.
A and B) in tf ? idf schema.
Manhattan distance: Defined as M(??x ,??y ) =
n
?
i=1
|xi ? yi|.
Euclidean distance: Defined as E(??x ,??y ) =
?
n
?
i=1
(xi ? yi)2.
Edit distance: This is the minimum number of op-
erations needed to transform A to B. We define an
operation as an insertion, deletion or substitution of
a word.
Jaro-Winker distance: Following (Winkler and
others, 1999), the Jaro-Winkler distance is a mea-
sure of similarity between two strings at the word
level.
In total, we can get 11 features in this feature set.
2.3 Sematic Similarity features
Almost every previous work used the surface texts
or exploited the meanings of words in the dictio-
nary to calculate the similarity of two sentences
rather than the actual meaning in the sentence. In
this feature set (SS), we introduce a latent model
to model the semantic representations of sentences
since latent models are capable of capturing the
contextual meaning of words in sentences. We
used weighted textual matrix factorization (WTMF)
(Guo and Diab, 2012) to model the semantics of
the sentences. The model factorizes the original
term-sentence matrix X into two matrices such that
Xi,j ? P T
?,iQ?,j , where P?,i is a latent semantics
119
vector profile for word wi and Q?,j is the vector pro-
file that represents the sentence sj . The weight ma-
trix W is introduced in the optimization process in
order to model the missing words at the right level
of emphasis. We propose three similarity measures
according to different strategies:
wtw: word-to-word based similarity defined as
sim(A,B) = lg
?
w
i
?A
W
w
i
?max
w
j
?B
(P
?,i
,P
?,j
)
?
w
i
?A
W
w
i
.
wts: word-to-sentence based similarity defined as
sim(A,B) = lg
?
w
i
?A
W
w
i
?P
?,i
?Q
?,k
?
w
i
?A
W
w
i
.
sts: sentence-to-sentence based similarity defined as
sim(A,B) = lg (Q
?,i ?Q?,j).
Also we calculate the cosine similarity, Euclidean
and Manhattan distance, weighted overlap coeffi-
cient using those semantics vectors, resulting in 10
features.
2.4 Sentence Difference features
Most of those above measures are symmetric and
only a few are asymmetric, which means they may
not be very suitable for the task that requires dealing
with directional problems. We solve this problem by
introducing sentence difference measures.
We observed that many entailment relationships
between two sentences are determined by only tiny
parts of the sentences. As a result, the similarity of
such two sentences by using above measures will be
close to 1, which may mislead the classifier. Fur-
thermore, almost all similarity measures in STS are
symmetric, which means the same similarity has no
help to distinguish the different directions. Based on
the above considerations, we propose a novel sen-
tence difference (SD) feature set to discover the dif-
ferences between two sentences and tell the classi-
fier the possibility the entailment should not hold.
The sentence difference features are extracted as
follows. Firstly, a word in one sentence is consid-
ered as matched if we can find the same word in the
other sentence. Then we find all matched words and
count the number of unmatched words in each sen-
tence, resulting in 2 features. If one sentence has
no unmatched words, we say that this sentence can
be entailed by the other sentence. That is, we can
infer the entailment class through the number of un-
matched words. We regard this label as our third
feature type. Secondly, different POS types of un-
matched words may have different impacts on the
classification, therefore we count the number of un-
matched words in each sentence that belong to a
small set of POS tags (here consider only NN, JJ,
RB, VB and CD tags), which produces 10 features,
resulting in a total of 13 sentence difference features.
2.5 Grammatical Relationship features
The grammatical relationship feature type (GR) is
designed to capture the grammatical relationship be-
tween two sentences. We first replace the words in a
sentence with their part-of-speech (POS) tags, then
apply the STS measures on this new ?sentence?.
In addition, we use the Stanford Parser to get the
dependency information represented in a form of re-
lation units (e.g. nsubj(example, this)). We calculate
the BC measures on those units and the overlap co-
efficients together with the harmonic mean of them.
Finally, we get 22 features.
2.6 Bias features
The bias features (BS) are to check the differences
between two sentences in certain special aspects,
such as polarity and named entity. We use a method
based on subjectivity of lexicons (Loughran and Mc-
Donald, 2011) to get the polarity of a sentence by
simply comparing the numbers of positive and neg-
ative words. If the numbers are the same, then we
set the feature to 1, otherwise -1. Also, we check
whether one sentence entails the other using only
the named entity information. We consider four cat-
egories of named entities, i.e., person, organization,
location, number, which are recognized by using the
Stanford NER toolkit. We set the feature to 1 if the
named entities in one sentence are found in the other
sentence, otherwise -1. As a result, this feature set
contains 9 features.
3 Experimental Setting
We evaluated our approach using the data sets
provided in the task 8 of SemEval 2013 (Ne-
gri et al, 2013). The data sets consist of a
collection of 1500 text fragment pairs (1000 for
training consisting of training and test set in Se-
mEval 2012 and 500 for test) in each language
pair. Four different language pairs are provided:
German-English, French-English, Italian-English
and Spanish-English. See (Negri et al, 2013) for
more detailed description.
120
3.1 Preprocess
We performed the following text preprocessing.
Firstly, we employed the state-of-the-art Statistical
Machine Translator, i.e., Google translator, to trans-
late each pair of texts <T,H> into <Tt,H> and
<T,H t>, thus they were in the same language. Then
we extracted all above described feature sets from
the pair <T t,H> (note that <T,Ht> are also used
in BC), so the below steps were mainly operated on
this pair. After that, all sentences were tokenized
and lemmatized using the Stanford Lemmatizer and
all stop words were removed, followed by the equiv-
alent replacement procedure. The replacement pro-
cedure consists of the following 3 steps:
Abbreviative replacement. Many phrases or orga-
nizations can be abbreviated to a set of capitalized
letters, e.g. ?New Jersey? is usually wrote as ?NJ?
for short. In this step, we checked every word whose
length is 2 or 3 and if it is the same as the ?word?
consisting of the first letters of the successive words
in another sentence, then we replaced it by them.
Semantic replacement. We observed that although
some lemmas in H and T were in the different forms,
they actually shared the same meaning, e.g. ?hap-
pen? and ?occur?. Here, we focused on replacing a
lemma in one sentence with another lemma in the
other sentence if they were: 1) in the same syn-
onymy set; or 2) gloss-related. Two lemmas were
gloss-related if a lemma appeared in the gloss of the
other. For example, the gloss of ?trip? is ?a jour-
ney for some purpose? (WordNet 2.1 was used for
looking up the synonymy and gloss of a lemma), so
the lemma ?journey? is gloss-related with ?trip?. No
word sense disambiguation was performed and all
synsets for a particular lemma were considered.
Context replacement. The context of a lemma
is defined as the non-stopword lemmas around it.
Given two text fragments, i.e., T. ...be erroneously
label as a ?register sex offender.? and H. ...be mis-
takenly inscribe as a ?register sex offender?., af-
ter the semantic replacement, we can recognize the
lemma ?erroneously? was replaceable by ?mistak-
enly?. However, WordNet 2.1 cannot recognize the
lemmas ?label? and ?inscribe? which can also be
replaceable. To address this problem, we simply as-
sumed that two lemmas surrounded by the same con-
text can be replaceable as well. In the experiments,
we set the window size of context replacement as 3.
This step is the foundation of the extraction of
the sentence different features and can also allevi-
ate the imprecise similarity measure problem exist-
ing in STS caused by the possibility of the lemmas
in totally different forms sharing the same sense.
3.2 System Configuration
We selected 500 samples from the training data as
development set (i.e. test set in SemEval 2012) and
performed a series of preliminary experiments to
evaluate the effectiveness of different feature types
in isolation and also in different combinations. Ac-
cording to the results on the development set, we
configured five different systems on each language
pair as our final submissions with different feature
types and classification strategies. Table1 shows the
five configurations of those systems.
System Feature Set Description
1 all flat, SVM
2 best feature sets flat, SVM
3 best feature sets flat, Majority Voting
4 best feature sets flat, only 500 instancesfor train, SVM
5 best feature sets hierarchical, SVM
Table 1: System configurations using different strategies
based on the results of preliminary experiments.
Among them, System 1 serves as a baseline that
used all features and was trained using a flat SVM
while System 2 used only the best feature combi-
nations. In our preliminary experiments, different
language pairs had different best feature combina-
tions (showed in Table 2). In System 3 we per-
formed a majority voting strategy to combine the
results of different algorithm (i.e. MaxEnt, SVM,
liblinear) to further improve performance. System
4 is a backup system that used only the training set
in SemEval 2012 to explore the influence of the dif-
ferent size of train set. Based on the analysis of the
preliminary results on development set, we also find
that the misclassification mainly occur between the
class of backward and others. So in System 5, we
adopted hierarchical classification technique to filter
out backward class in the first level using a binary
classifier and then conducted multi-class classifica-
tion among the remaining three classes.
121
We used a linear SVM with the trade-off parame-
ter C=1000 (also in liblinear). The parameters in SS
are set as below: the dimension of sematic space is
100, the weight of missing words is 100 and the reg-
ularization factor is 0.01. In the hierarchical classifi-
cation, we use the liblinear (Fan et al, 2008) to train
a binary classifier and SVM for a multi-class classi-
fier with the same parameters in other Systems.
4 Results and discussion
Table 2 lists the final results of our five systems on
the test samples in terms of four language pairs. The
best feature set combinations for different language
pairs are also shown. The last two rows list the re-
sults of the best and runner-up team among six par-
ticipants, which is released by the organizers.
From this table, we have some interesting find-
ings.
Firstly, the feature types BC and SD appear in all
best feature combinations. This indicates that the
length and sentence difference information are good
and effective label indicators.
Secondly, based on the comparison between Sys-
tem 1 and System 2, we find that the behavior of the
best feature sets of different language pairs on test
and development datasets is quite different. Specif-
ically, the best feature set performs better on FRA-
ENG and DEU-ENG data sets than the full feature
set. However, the full feature set performs the best
on SPA-ENG and ITA-ENG data sets. The reason
may be the different distribution properties of test
and development data sets.
Thirdly, although the only difference between
System 2 and System 4 is the size of training sam-
ples, System 4 trained on a small number of training
instances even makes a 1.6% improvement in accu-
racy over System 2 on DEU-ENG data set. This
is beyond our expectation and it indicates that the
CLTE may not be sensitive to the size of data set.
Fourthly, by adopting a majority voting scheme,
System 3 achieves the best results on two data sets
among five systems and obtains 45.8% accuracy on
FRA-ENG which is the best result among all partic-
ipants. This indicates the majority voting strategy is
a effective way to boost the performance.
Fifthly, System 5 which adopts hierarchical clas-
sification technique fails to make further improve-
ment. But it still outperforms the runner-up system
in this task on FRA-ENG and DEU-ENG. We spec-
ulate that the failure of System 5 may be caused by
the errors sensitive to hierarchical structure in hier-
archical classification.
In general, our approaches obtained very good
results on all the language pairs. On FRA-ENG
and DEU-ENG, we achieved the best results among
the 16 systems with the accuracy 45.8% and 45.3%
respectively and largely outperformed the runner-
up. The results on SPA-ENG and ITA-ENG were
also promising, achieving the second and third place
among the 16 systems.
5 Conclusion
We have proposed several effectively features con-
sisting of sentence semantic similarity and sentence
difference, which work together with other features
presented by the previous work to solve the cross-
lingual textual entailment problem. With the aid
of machine translation, we can handle the cross-
linguality. We submitted five systems on each lan-
guage pair and obtained the best result on two data
sets, i.e., FRA-ENG and DEU-ENG, and ranked the
2nd and the 3rd on other two language pairs respec-
tively. Interestingly, we find some simple feature
types like BC and SD are good class indicators and
can be easily acquired. In future work, we will in-
vestigate the discriminating power of different fea-
ture types in the CLTE task on different languages.
Acknowledgements
The authors would like to thank the organizers and
reviewers for this interesting task and their helpful
suggestions and comments, which improves the fi-
nal version of this paper. This research is supported
by grants from National Natural Science Foundation
of China (No.60903093), Shanghai Pujiang Talent
Program (No.09PJ1404500), Doctoral Fund of Min-
istry of Education of China (No. 20090076120029)
and Shanghai Knowledge Service Platform Project
(No. ZF1213).
References
Julio Javier Castillo. 2011. A wordnet-based seman-
tic approach to textual entailment and cross-lingual
122
System SPA-ENG ITA-ENG FRA-ENG DEU-ENG
1 0.428 0.426 0.438 0.422
2 0.404 0.420 0.450 0.436
3 0.408 0.426 0.458 0.432
4 0.422 0.416 0.436 0.452
5 0.392 0.402 0.442 0.426
Best
feature set
BC+STS+SS
+GR+SD
BC+SD+SS
+GR+BS SD+BC+STS
BC+STS+SS
+BS+SD
Best 0.434 0.454 0.458 0.452
runner-up 0.428 0.432 0.426 0.414
Table 2: The accuracy results of our systems on different language pairs released by the organizer.
textual entailment. International Journal of Machine
Learning and Cybernetics, 2(3):177?189.
Ido Dagan and Oren Glickman. 2004. Probabilistic tex-
tual entailment: Generic applied modeling of language
variability. In Proceedings of the PASCAL Workshop
on LearningMethods for Text Understanding andMin-
ing.
Miquel Espla`-Gomis, Felipe Sa?nchez-Mart??nez, and
Mikel L. Forcada. 2012. Ualacant: Using online ma-
chine translation for cross-lingual textual entailment.
In Proceedings of the Sixth International Workshop on
Semantic Evaluation (SemEval 2012), pages 472?476,
Montre?al, Canada, 7-8 June.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. Liblinear: A library
for large linear classification. The Journal of Machine
Learning Research, 9:1871?1874.
Weiwei Guo and Mona Diab. 2012. Modeling sentences
in the latent space. In Proceedings of the 50th Annual
Meeting of the Association for Computational Linguis-
tics.
Sanda Harabagiu and Andrew Hickl. 2006. Methods for
using textual entailment in open-domain question an-
swering. InProceedings of the 21st InternationalCon-
ference on Computational Linguistics and 44th Annual
Meeting of the Association for Computational Linguis-
tics, pages 905?912, Sydney, Australia, July.
Sergio Jimenez, Claudia Becerra, and Alexander Gel-
bukh. 2012. Soft cardinality+ ml: Learning adaptive
similarity functions for cross-lingual textual entail-
ment. In Proceedings of the 6th International Work-
shop on Semantic Evaluation (SemEval 2012).
Elena Lloret, Oscar Ferra?ndez, Rafael Munoz, and
Manuel Palomar. 2008. A text summarization ap-
proach under the influence of textual entailment. In
Proceedings of the 5th International Workshop on
Natural Language Processing and Cognitive Science
(NLPCS 2008), pages 22?31.
Tim Loughran and Bill McDonald. 2011. When is a
liability not a liability? textual analysis, dictionaries,
and 10-ks. The Journal of Finance, 66(1):35?65.
Prodromos Malakasiotis and Ion Androutsopoulos.
2007. Learning textual entailment using svms and
string similarity measures. In Proceedings of the ACL-
PASCAL Workshop on Textual Entailment and Para-
phrasing, pages 42?47.
Prodromos Malakasiotis. 2009. Paraphrase recognition
using machine learning to combine similarity mea-
sures. In Proceedings of the ACL-IJCNLP 2009 Stu-
dent Research Workshop, pages 27?35.
Yashar Mehdad, Alessandro Moschitti, and Fabio Mas-
simo Zanzotto. 2010a. Syntactic/semantic structures
for textual entailment recognition. In Human Lan-
guage Technologies: The 2010 Annual Conference of
the North American Chapter of the Association for
Computational Linguistics, pages 1020?1028.
Yashar Mehdad, Matteo Negri, and Marcello Federico.
2010b. Towards cross-lingual textual entailment. In
Human Language Technologies: The 2010 Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 321?
324, Los Angeles, California, June.
Fandong Meng, Hao Xiong, and Qun Liu. 2012. Ict:
A translation based method for cross-lingual textual
entailment. In Proceedings of the Sixth International
Workshop on Semantic Evaluation (SemEval 2012),
pages 715?720, Montre?al, Canada, 7-8 June.
M. Negri, A. Marchetti, Y. Mehdad, L. Bentivogli, and
D. Giampiccolo. 2013. Semeval-2013 Task 8: Cross-
lingual Textual Entailment for Content Synchroniza-
tion. In Proceedings of the 7th InternationalWorkshop
on Semantic Evaluation (SemEval 2013).
William E Winkler et al 1999. The state of record link-
age and current research problems.
123
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 408?413, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
ECNUCS: A Surface Information Based System Description of Sentiment
Analysis in Twitter in the SemEval-2013 (Task 2)
Tian Tian ZHU and Fang Xi ZHANG and Man LAN?
Department of Computer Science and Technology
East China Normal University
51111201046,51111201041@ecnu.edu.cn; mlan@cs.ecnu.edu.cn
Abstract
This paper briefly reports our submissions
to the two subtasks of Semantic Analysis in
Twitter task in SemEval 2013 (Task 2), i.e.,
the Contextual Polarity Disambiguation task
(an expression-level task) and the Message
Polarity Classification task (a message-level
task). We extract features from surface infor-
mation of tweets, i.e., content features, Micro-
blogging features, emoticons, punctuation and
sentiment lexicon, and adopt SVM to build
classifier. For subtask A, our system on twit-
ter data ranks 2 on unconstrained rank and on
SMS data ranks 1 on unconstrained rank.
1 Introduction
Micro-blogging today has become a very popular
communication tool among Internet users. Millions
of messages are appearing daily in popular web sites
that provide services for Micro-blogging and one
popularly known is Twitter1. Through the twitter
platform, users share either information or opin-
ions about personalities, politicians, products, com-
panies, events (Prentice and Huffman, 2008) etc. As
a result of the rapidly increasing number of tweets,
mining sentiments expressed in tweets has attracted
more and more attention, which is also one of the
basic analysis utility functions needed by various ap-
plications.
The task of Sentiment Analysis in Twitter is
to identify the sentiment of tweets and get a bet-
ter understanding of how sentiment is conveyed in
1http://www.twitter.com
tweets and texts, which consists of two sub-tasks,
i.e., the Contextual Polarity Disambiguation task
(an expression-level task) and the Message Polarity
Classification task (a message-level task). The con-
textual polarity disambiguation task (subtask A) is
to determine whether a given message containing a
marked instance of a word or a phrase is positive,
negative or neutral in that context. The message
polarity classification task (subtask B) is to decide
whether a given message is of positive, negative, or
neutral sentiment and for messages conveying both
a positive and negative sentiment, whichever is the
stronger sentiment should be chosen (Wilson et al,
2013). We participate in these two tasks.
In recent years, many researchers have proposed
methods to analyze sentiment in twitter. For exam-
ple, (Pak and Paroubek, 2010) used a Part of Speech
(POS) tagger on the tweets and found that some POS
taggers can help identify the sentiment of tweets.
They found that objective tweets often contain more
nouns than subjective tweets. However, subjective
tweets may carry more adjectives and adverbs than
objective tweets. Besides, (Davidov et al, 2010)
proved that emoticon and punctuation like excla-
mation mark are good features when distinguishing
the sentiment of tweets. In addition, some senti-
ment lexicons like SentiWordNet (Baccianella et al,
2010) and MPQA Subjectivity Lexicon (Wilson et
al., 2009) have been adopted to calculate the senti-
ment score of tweets (Zirn et al, 2011).
The rest of this paper is organized as follows. Sec-
tion 2 describes our approach for subtask 1, i.e.,
the Contextual Polarity Disambiguation task. Sec-
tion 3 describes our approach for subtask 2, i.e., the
408
message polarity classification task. Concluding re-
marks is in Section 4.
2 System Description of Contextual
Polarity Disambiguation
For the Contextual Polarity Disambiguation task,
we first extract features from multiple aspects, i.e.,
punctuation, emoticons, POS tags, instance length
and sentiment lexicon features. Then we adopt poly-
nomial SVM to build classification models. Accord-
ing to the definition of this task, the given instance
has been marked by a start position and an end posi-
tion rather than a whole tweet. So we first record the
frequency of the first three kinds of features in this
given instance. To avoid interference from the num-
ber of words in given instance, we then normalize
the feature values by the length of instance.
2.1 Preprocessing
Typically, most tweets contain informal language
expressions, with creative spelling and punctuation,
misspellings, slang, new words, URLs, and genre-
specific terminology and abbreviations, such as,
?RT? for ?re-tweet? and #hashtags, which are a type
of tagging for Twitter messages. Therefore, working
with these informal text genres presents challenges
for natural language processing beyond those typ-
ically encountered when working with more tradi-
tional text genres, such as newswire data. So we
perform text preprocessing in order to remedy as
many informal texts as possible. Firstly, we per-
form normalization to convert creative spelling and
misspelling into its right spelling. For example, any
repetition of more than 3 continuous letters are re-
duced back to 1 letter (e.g. ?noooo? is reduced to
?no?). In addition, according to the Internet slang
dictionary2, we convert each slang to its complete
form, for example, ?aka? is rewritten as ?also known
as?. After that, we use the Stanford parser3 for to-
kenization and the Stanford POS Tagger4 for POS
tagging. Finally, Natural Language Toolkit5 is used
for WordNet based Lemmatization.
2http://www.noslang.com
3http://nlp.stanford.edu/software/lex-parser.shtml
4http://nlp.stanford.edu/software/tagger.shtml
5http://nltk.org/
2.2 Features
2.2.1 Punctuation
Typically, punctuation may express user?s senti-
ment to a certain extent. For example, many excla-
mation marks (!) in tweet may indicate strong feel-
ings or high volume (shouting). Therefore, given
a marked instance, we record the frequency of the
following four types of punctuation: (1) exclama-
tion mark (!), (2) question mark (?), (3) double or
single quotation marks( ? and ??), (4) sum of the
above three punctuation. Then the punctuation fea-
ture value is normalized by the length of instance.
2.2.2 Emoticons
We create two features that capture the number of
positive and negative emoticons. Table 1 lists the
two types of emoticons. We also use the union of
the two emoticon sets as a feature. In total, we have
three emoticon features.
Positive Emoticons Negative Emoticons
:-) : ) :D :-D =) ;) :( :-( : ( ;(
;-) ; ) ;D ;-D (; :) ;-( ; ( ):
:-P ;-P XD (-: (-; :o) ;o) -/ :/ ;-/ ;/
:0) ;0) ? ? T T T0T ToT
Table 1: List of emoticons
2.2.3 POS
According to the finding of (Pak and Paroubek,
2010), POS taggers help to identify the sentiment
of tweets. Therefore, we record the frequency of
the following four POS features, i.e., noun (?NN?,
?NNP?, ?NNS? and ?NNPS? POS tags are grouped
into noun feature), verb (?VB?, ?VBD?, ?VBG?,
?VBN?, ?VBP? and ?VBZ? POS tags are grouped
into verb feature), adjective (?JJ?, ?JJR? and ?JJS?
POS tags are grouped into adjective feature) and
adverb (?RB?, ?RBR? and ?RBS? POS tags are
grouped into adverb feature). Then we normalize
them by the length of given instance.
2.2.4 Sentiment lexicon Features
For each word in a given instance, we use three
sentiment lexicons to identify its sentiment polarity
and calculate its sentiment weight, i.e., SentiWord-
Net (Baccianella et al, 2010), MPQA Subjectivity
Lexicon (Wilson et al, 2009) and an Unigram Lex-
icon made from the Large Movie Review Dataset
409
v1.0 (Maas et al, 2011). To calculate the sentiment
score for this instance, we use the following formula
to sum up the sentiment score of each word:
Senti(I) =
?
w?I
Num(w) ? Senti weight
Length(I)
(1)
where I represents the given instance and w repre-
sents each word in I . The Senti weight is calcu-
lated based on the word in the instance and the cho-
sen sentiment lexicon. That is, for each word in the
instance, we have different Senti weight values for
it since we use different sentiment lexicons. Below
we describe the calculation of Senti weight values
for a word in three sentiment lexicons. Note that
Num(w) is always 1 since most words appear one
time in a instance.
SentiWordNet. SentiWordNet is a lexical resource
for sentiment analysis, which assigns each synset of
WordNet (Stark and Riesenfeld, 1998) three senti-
ment scores: positivity, negativity, objectivity (e.g.
living#a#3, positivity: 0.5, negativity: 0.125, ob-
jectivity: 0.375), where sum of these three scores
is always 1. For one concept, if its positive score
and negative score are all 0, we treat it as objective
concept; otherwise, we treat it as subjective concept.
And we take the first sense as the concept of each
word.
We extract three features from SentiWordNet, i.e.,
SUBWordNet, POSWordNet and NEGWordNet.
The Senti weight of SUBWordNet records
whether a word is subjective. If it is subjective,
we set Senti weight as 1, otherwise 0. Similarly,
the Senti weight values of POSWordNet and
NEGWordNet indicate the positive score and the
negative score of the given word. Considering
some negation terms may reverse the sentiment
orientation of instance, we manually generate a
negation term list (e.g. ?not?, ?never?, etc.,) and if a
negation term appears in the instance, we switch the
POSWordNet to NEGWordNet and vice versa. Be-
sides, we adopt another feature to record the ratio of
POSWordNet/NEGWordNet. If the denominator is
0, i.e., NEGWordNet = 0, that means, the word has
the strongest positive sentiment orientation, then we
set 10*POSWordNet as its feature value.
MPQA. The MPQA Subjectivity Lexicon contains
about 8, 000 subjective words. Each word in the
lexicon has two types of sentiment strength: strong
subjective and weak subjective, and four kinds of
sentiment polarity: positive, negative, both (positive
and negative) and neutral. Therefore we calculate
three features from this lexicon, i.e., SUBMPQA,
POSMPQA and NEGMPQA. For the SUBMPQA
feature, if the word has strong or weak subjective,
we set its Senti weight as 1 or 0.5 accordingly.
For the POSMPQA (NEGMPQA) feature, we set
Senti weight as 1, or 0.5 or 0 if the word has strong
positive (negative), or weak positive (negative) or
neutral. We also reverse the sentiment orientation
of POSMPQA and NEGMPQA if a negation term
appears.
Unigram Lexicon. Unlike the above two lexicons
in themselves which provide sentiment polarity and
sentiment strength for each word, we also utilize the
third lexicon to calculate the sentiment information
statistically. Therefore we generate an unigram lex-
icon by ourselves from a large Movie Review data
set(Maas et al, 2011) which contains 25, 000 posi-
tive and 25, 000 negative movie reviews. We calcu-
late the Senti weight of each word appears in the
data set as the ratio of the frequency of this word
in positive reviews to that in negative reviews and
record this feature as SentiUL.
Clearly, since we use additional data set to de-
velop a sentiment lexicon which is used to generate
this SentiUL feature, this feature is worked with all
other features to train the unconstrained system.
2.2.5 Other features
In addition, we collect three other features: (1)
length of instance, (2) uppercase word (e.g. ?WTO?
or ?Machine Learning?), (3) URL. For the uppercase
word and URL features, we record the frequency of
them and then normalize them by the instance length
as well.
2.3 Experiment and Results
2.3.1 Classification Algorithm
We adopt LibSVM6 to build polynomial kernel-
based SVM classifiers. We have also tried linear ker-
nel but get no improvement. To obtain the optimal
parameters for SVM, such as c and g, we perform
grid search with 10-fold cross validation on training
6http://www.csie.ntu.edu.tw/ cjlin/libsvm/
410
data.
2.3.2 Results and Discussion
In section 2, we obtained 22 features in total. To
train the constrained model, we used the above de-
scribed 21 features (except SentiUL) and used all
above 22 features to train the unconstrained model.
We combined the provided training and develop-
ment data by the organizers as our final training
data. And we should apologize for our misunder-
standing of the definitions of the constrained and
unconstrained condition. As the official definition
of unconstrained model, participates are allowed
to add other data to expand the training data sets,
but our unconstrained model only adds one fea-
ture (SentiUL) which is got from other data set.
Therefore, we actually submitted two results of con-
strained model. But we still refer this model trained
on all features as unconstrained model for it ap-
peared in the unconstrained list of official results.
There are two kinds of test data: 4, 435 twitter in-
stances and 2, 334 SMS message instances. Table
2 list the F-score and averaged F-score of positive,
negative and neutral class of each test data set.
On one hand, from the table we can see that
whether on constrained or unconstrained model, the
results on twitter data are slightly better than those
of SMS data. However, this difference is not signifi-
cant. This indicates that the model trained on twitter
data performs well on SMS data. And it also shows
that twitter data and SMS data are linguistically sim-
ilar with each other in nature. On the other hand, we
find that on each test data set, there is little differ-
ence between the constrained model and the uncon-
strained model, which indicates the SentiUL feature
does not have discriminating power by itself. How-
ever, since we had not used other labeled or unla-
beled data to extend the training data set, we cannot
draw a conclusion on this. Besides, our results con-
tain no neutral items even though the classifier we
used is multivariate. One reason may be the neutral
instances in training data is too sparse for the classi-
fier to learn.
On twitter data, our system ranks 2 under un-
constrained model and ranks 10 under constrained
model. On SMS data, our system ranks first under
unconstrained model and ranks 7 under constrained
model.
3 System Description of Message Polarity
Classification
Unlike the previous subtask, the Message Polarity
classification task focuses on the whole tweet rather
than a marked sequence of given instance. Firstly,
we perform text preprocessing as Task A. Besides
the previous described features, we also extract fol-
lowing features.
3.1 Features
3.1.1 Micro-blogging features
We adopted three tweet domain-specific features,
i.e., #hashtags, @USERS, URLs. We calculate the
frequency of the three features and normalize them
by the length of instance.
3.1.2 n-gram features
We used unigrams to capture the content of
tweets.
3.2 Classification Algorithm
We adopted two different classifiers in preliminary
experiments, i.e., maximum entropy and SVM. We
used the Mallet tool (McCallum, 2002) to perform
Maximum Entropy classification and LibSVM7 with
a linear kernel, where the default setting is adopted
in all experiments.
3.3 Results on Training Data
In the first experiment, we used only content fea-
tures and LibSVM classifier to do our experiments.
The results were listed in Table 3. From Table 3,
we found that the system with unigram without re-
moving stop words performs the best. The possible
reason was that Microblogs are always short (con-
strained in 140 words) and removing stop words
would cause information missing in such a short
text. In addition, although bigrams improved the
performance to some extern, they added the feature
space many more and might affect other features. So
in our final systems, we used only unigram feature
and did not remove stop words.
In the second experiment, we compared all fea-
tures described before with two learning algorithms.
The results were shown in Table 4, where 1 indi-
cates unigram, 2 indicates micro-blog, 3 indicates
7http://www.csie.ntu.edu.tw/ cjlin/libsvm/
411
System F-pos F-neg F-neu average F(pos and neg)
twitter-constrained 0.8506 0.7390 0.0 0.7948
twitter-unconstrained 0.8561 0.7468 0.0 0.8015
SMS-constrained 0.7727 0.7611 0.0 0.7669
SMS-unconstrained 0.7645 0.7824 0.0 0.7734
Table 2: Results of our systems on subtask A test data
features F-pos F-neg F-neu average F(pos and neg) acc(%)
unigrams 0.6356 0.3381 0.7122 0.4869 63.75
unigrams(remove stop words) 0.6046 0.3453 0.6988 0.4750 62.13
bigrams 0.5186 0.0196 0.6625 0.2691 55.85
unigrams+bigrams 0.6234 0.3724 0.7043 0.4979 63.18
Table 3: Results of our systems on on subtask B training data using content features
features F-pos F-neg F-neu average F(pos and neg) acc(%)
MaxEnt SVM MaxEnt SVM MaxEnt SVM MaxEnt SVM MaxEnt SVM
1 0.6178 0.6356 0.3696 0.3381 0.6848 0.7122 0.4937 0.4869 61.56 63.75
1+2 0.6403 0.6339 0.4207 0.4310 0.6990 0.7184 0.5305 0.5324 63.75 64.89
1+2+3 0.6328 0.6512 0.4051 0.4371 0.6975 0.7232 0.5190 0.5442 63.18 65.75
1+2+3+4 0.6488 0.6593 0.4587 0.4481 0.7083 0.7288 0.5538 0.5537 64.89 66.41
2+3+4 0.5290 0.5201 0.2897 0.2643 0.6503 0.6411 0.4093 0.3922 55.85 54.80
Table 4: Results of our systems on subtask B training data using all features and two learning algorithms
punctuation, 4 indicates sentiment lexicon features.
From Table 4, the best performance was obtained
by using all these features. Since the performance
of Maximum Entropy and SVM in terms of F-score
was comparable to each other, we finally chose SVM
since it achieved a better accuracy than MaxEnt.
3.4 Results on Test Data
We combined the provided training and develop-
ment data by the organizers as our final training data.
There were two kinds of test data: 3, 813 tweets and
2, 094 SMS messages . Table 5 listed the results of
our final systems on the tweet and SMS data sets by
using all above described features and SVM algo-
rithm.
From Table 5, on one hand, we can see that the
overall performance of SMS test data is inferior to
twitter data, for the reason may be that the domain
of features are all based on twitter data, and maybe
not quite suitable for SMS data. However, this dif-
ferent is not significant. On the other hand, we also
can find that there is no obvious distinction between
the constrained and the unconstrained model on each
test data.Also from Table 5, the F-score for positive
instances is higher than negative instances, and it
is interesting that most of other participants?systems
results show the same consequence. One of the rea-
son may be the positive instance in training data are
more than negative instances both in training data
and test data.
Our result on twitter message is 0.5842 , while
on SMS is 0.5477. Compared with the highest av-
erage F-score 0.6902 in twitter data and 0.6848 in
SMS data, our system does not perform very well.
On the one hand , pre-processing was roughly , then
features extracted were not suited in classification
stage. On the other hand, in classification stage all
parameters were default when used LibSVM. These
might cause low performance. In future, we may
overcome the insufficient described above and take
hashtags? sentiment inclination and the source files
of URLs into consideration to enhance the perfor-
mance.
412
System F-pos F-neg F-neu average F(pos and neg)
twitter-constrained 0.6671 0.4338 0.7124 0.5505
twitter-unconstrained 0.6775 0.4908 0.7204 0.5842
SMS-constrained 0.5796 0.4846 0.7801 0.5321
SMS-unconstrained 0.5818 0.5137 0.7612 0.5477
Table 5: Results of our systems on subtask B test data
4 Conclusion
In this work we extracted features from four aspects,
including surface information of twitters and senti-
ment lexicons like SentiWordNet and MPQA Lexi-
con. On the contextual polarity disambiguation task,
our system ranks 2 on twitter (unconstrained) rank
and ranks 1 on SMS (unconstrained) rank.
Acknowledgements
The authors would like to thank the organizers and
reviewers for this interesting task and their helpful
suggestions and comments, which improves the fi-
nal version of this paper. This research is supported
by grants from National Natural Science Foundation
of China (No.60903093), Shanghai Pujiang Talent
Program (No.09PJ1404500), Doctoral Fund of Min-
istry of Education of China (No. 20090076120029)
and Shanghai Knowledge Service Platform Project
(No. ZF1213).
References
Stefano Baccianella, Andrea Esuli, and Fabrizio Se-
bastiani. 2010. Sentiwordnet 3.0: An enhanced
lexical resource for sentiment analysis and opinion
mining. In Nicoletta Calzolari (Conference Chair),
Khalid Choukri, Bente Maegaard, Joseph Mariani,
Jan Odijk, Stelios Piperidis, Mike Rosner, and Daniel
Tapias, editors, Proceedings of the Seventh Interna-
tional Conference on Language Resources and Evalu-
ation (LREC?10), Valletta, Malta, may. European Lan-
guage Resources Association (ELRA).
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Enhanced sentiment learning using twitter hashtags
and smileys. In Proceedings of the 23rd International
Conference on Computational Linguistics: Posters,
pages 241?249. Association for Computational Lin-
guistics.
Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan
Huang, Andrew Y. Ng, and Christopher Potts. 2011.
Learning word vectors for sentiment analysis. In Pro-
ceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies, pages 142?150, Portland, Oregon, USA,
June. Association for Computational Linguistics.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
Alexander Pak and Patrick Paroubek. 2010. Twitter as a
corpus for sentiment analysis and opinion mining. In
Proceedings of LREC, volume 2010.
Sara Prentice and Ethan Huffman. 2008. Social medias
new role in emergency management. Idaho National
Laboratory, pages 1?5.
Michael M Stark and Richard F Riesenfeld. 1998. Word-
net: An electronic lexical database. In Proceedings of
11th Eurographics Workshop on Rendering. Citeseer.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2009. Recognizing contextual polarity: An explo-
ration of features for phrase-level sentiment analysis.
Computational linguistics, 35(3):399?433.
Theresa Wilson, Zornitsa Kozareva, Preslav Nakov, Alan
Ritter, Sara Rosenthal, and Veselin Stoyanov. 2013.
Semeval-2013 task 2: Sentiment analysis in twitter.
In Proceedings of the 7th International Workshop on
Semantic Evaluation. Association for Computational
Linguistics.
Ca?cilia Zirn, Mathias Niepert, Heiner Stuckenschmidt,
and Michael Strube. 2011. Fine-grained sentiment
analysis with structural features. In Proceedings of
the 5th international Joint conference on natural Lan-
guage Processing (iJcnLP-2011), volume 167.
413
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 252?258,
Dublin, Ireland, August 23-24, 2014.
ECNU: A Combination Method and Multiple Features for Aspect
Extraction and Sentiment Polarity Classification
Fangxi Zhang, Zhihua Zhang, Man Lan
?
Department of Computer Science and Technology
East China Normal University
51111201041,51131201039@ecnu.cn; mlan@cs.ecnu.edu.cn
?
Abstract
This paper reports our submissions to the
four subtasks of Aspect Based Sentimen-
t Analysis (ABSA) task (i.e., task 4) in
SemEval 2014 including aspect term ex-
traction and aspect sentiment polarity clas-
sification (Aspect-level tasks), aspect cat-
egory detection and aspect category sen-
timent polarity classification (Category-
level tasks). For aspect term extraction, we
present three methods, i.e., noun phrase
(NP) extraction, Named Entity Recogni-
tion (NER) and a combination of NP and
NER method. For aspect sentiment classi-
fication, we extracted several features, i.e.,
topic features, sentiment lexicon features,
and adopted a Maximum Entropy classifi-
er. Our submissions rank above average.
1 Introduction
Recently, sentiment analysis has attracted a lot of
attention from researchers. Most previous work
attempted to detect overall sentiment polarity on a
text span, such as document, paragraph and sen-
tence. Since sentiments expressed in text always
adhere to objects, it is much meaningful to iden-
tify the sentiment target and its orientation, which
helps user gain precise sentiment insights on spe-
cific sentiment target.
The aspect based sentiment analysis (ABSA)
task (Task 4) (Pontiki et al., 2014) in SemEval
2014 is to extract aspect terms, determine its se-
mantic category, and then to detect the sentimen-
t orientation of the extracted aspect terms and its
category. Specifically, it consists of 4 subtasks.
The aspect term extraction (ATE) aims to extrac-
t the aspect terms from the sentences in two giv-
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
en domains (laptop and restaurant). The aspec-
t category detection (ACD) is to identify the se-
mantic category of aspects in a predefined set of
aspect categories (e.g., food, price). The aspect
term polarity (ATP) classification is to determine
whether the sentiment polarity of each aspect is
positive, negative, neutral or conflict (i.e., both
positive and negative). The aspect category po-
larity (ACP) classification is to determine the sen-
timent polarity of each aspect category. We partic-
ipated in these four subtasks.
Generally, there are three methods to extract as-
pect terms: unsupervised learning method based
on word frequency ((Ku et al., 2006), (Long et
al., 2010)), supervised machine learning method
(Kovelamudi et al., 2011) and semi-supervised
learning method (Mukherjee and Liu, 2012) where
only several user interested category seeds are
given and used to extract more categorize aspect
terms. Since sentiments always adhere to entities,
several researchers worked on polarity classifica-
tion of entity. For example, (Godbole et al., 2007)
proposed a system that assigned scores represent-
ing positive or negative opinion to each distinc-
t entity in the corpus. (Kim et al., 2013) presented
a hierarchical aspect sentiment model to classify
the polarity of aspect terms from unlabeled online
reviews. Moreover, some sentiment lexicons, such
as SentiWordNet (Baccianella et al., 2010) and M-
PQA Subjectivity Lexicon (Wilson et al., 2009),
have been used to generate sentiment score fea-
tures (Zhu et al., 2013).
The rest of this paper is organized as follows.
From Section 2 to Section 5, we describe our ap-
proaches to the Aspect Term Extraction task, the
Aspect Category detection task, the Aspect Term
Polarity task and the Aspect Category Polarity task
respectively. Section 6 provides the conclusion.
252
2 Aspect Term Extraction System
For aspect terms extraction task, we first adopted
two methods: a noun phrase (NP) based method
and a Named Entity Recognition (NER) based
method. In our preliminary experiments, we found
that the NP-based method generates many noisy
terms resulting in high recall and low precision,
and the NER-based method performs inverse re-
sults. In order to overcome their drawbacks and
make use of their advantages, we proposed a third
method which combines the two methods by using
the results of NP-based method as an additional
name list feature to the NER system.
2.1 Preprocessing
We used Stanford Parser Tools
1
for POS tagging
and for parsing while the Natural Language Toolk-
it
2
was used for removing stop words and lemma-
tization.
2.2 NP-based Method
(Liu, 2012) showed that the majority of aspec-
t terms are noun phrases. Moreover, through the
observation of the training set, we found that more
than half of the aspects are pure noun phrases or
nested noun phrases. So we considered these two
types of noun phrases as aspect terms and adopt-
ed a rule-based noun phrases extraction system to
perform aspect term extraction. This extraction
is performed on parsed sentences. For example,
from parsed sentence:
?(CC but)
(S
(NP (NN iwork))
(VP (VBZ is)
(ADJP (JJ cheap))
(PP (VBN compared)
(PP (TO to)
(NP (NN office))))))?
iwork and office with NN tag are extracted as as-
pect terms. However, to make a more precise ex-
traction, we first removed white lines from parsed
sentences. Then we performed extraction only us-
ing three continuous lines. Since the NPs we ex-
tracted contain much noise which only appear in
NPs rather than in gold aspect terms list, we built
a stopwords list containing these noisy terms espe-
cially the numeric expressions. Table 1 shows the
set of manually built rules used for NP extraction.
1
http://nlp.stanford.edu/software/lex-parser.shtml
2
http://www.nltk.org/
Based on the experimental results on training
data, we found the NP-based method achieves
high recall and low precision as shown in Table
2. This indicates that we extracted plenty of NPs
which consist of a large proportion of aspect terms
and much noise such as irrelevant NPs and over-
lapping phrases. Thus the NP-based method alone
has not produced good results.
2.3 NER-based Method
We also cast aspect term extraction task as a tradi-
tional NER task (Liu, 2012). We adopted the com-
monly used BIO tag format to represent the aspect
terms in the given annotated training data (Toh et
al., 2012), where B indicates the beginning of an
aspect term, I indicates the inside of an aspect ter-
m and O indicates the outside of an aspect term.
For example, given ?the battery life is excellent?,
where battery life is annotated as aspect term, we
tagged the three words the, is and excellent as O,
battery as B and life as I.
We adopted several widely used features for the
NER-based aspect term extraction system.
Word features: current word (word 0), previ-
ous word (word -1) and next word (word 1) are
used as word features.
POS feature: the POS tag of current word
(POS 0), the POS tags of two words around cur-
rent word (POS -2, POS -1, POS 1, POS 2), and
the combinations of contextual POS tags (POS -
1/POS 0, POS 0/POS 1, POS -1/POS 0/POS 1)
are included as POS features.
Word shape: a tag sequence of characters in
current word is recorded, i.e., the lowercase letter
tagged as a, and the uppercase letter tagged as A.
Chunk: We extracted this feature from the POS
tag sequence, which is defined as follows: the
shortest phrase based on POS taggers, i.e., ?(VP
(VBD took) (NP (NN way)) (ADVP (RB too) (RB
long))?, took labeled as O, way labeled as B-NP,
too labeled as B-ADVP, long labeled as I-ADVP.
We implemented a CRF++
3
based NER system
with the above feature types.
2.4 Combination of NP and NER Method
Based on our preliminary experiments, we con-
sidered to combine the above two methods. To
do so, we adopted the results of the NP system
as additional name lists feature for the NER sys-
tem. Through the observation on the results of the
3
http://crfpp.googlecode.com/svn/trunk/doc/index.html
253
if (NP in line 1) then select line 1 as candidate
if (NP in line 1 and PP in line 2 and NP in line 3) then select line 1 + line 2 + line 3 as candidate
else if (VB in line 1 and NN in line 2) then select line 1 + line 2 as candidate
else if (NP in line 1 and NP in line 2) then select line 1 + line 2 as candidate
else if (NP in line 1 and CC in line 2 and NN in line 3) then select line 3 as candidate
else if (JJ in line 1 and NN in line 2) then select line 2 as candidate
if (current term in candidate existing in stopwords) then remove current term
if (CD start candidate) then remove CD
if (DT or PRP start candidate) then remove DT or PRP
if (JJR in candidate) then remove JJR
if (Punctuation in candidate) then remove Punctuation
Table 1: The rules in NP-based method.
method
Laptop Restaurant
Precision(%) Recall(%) F-score(%) Precision(%) Recall(%) F-score(%)
NP-based 44.35 74.43 55.59 45.99 70.50 56.17
NER-based 70.46 48.27 57.29 80.87 68.24 74.02
Combination 72.79 55.11 62.73 82.31 70.62 76.02
Table 2: The F-scores of three methods on training data.
NP-based method and the NER-based method, we
built two types of name lists for our combination
method as follows:
Gold Namelist: containing the gold aspec-
t terms and the intersection between the results of
the NP-based method and the NER-based method.
Stop Namelist: the words in original sentences
but not in gold aspect terms set or not in NPs set
we extracted before.
Table 3 shows the results of feature selection
for the combination method on training data. The
best performance was obtained by using all fea-
tures. Thus, our final submission system adopted
the combination method with all features.
Feature
Dataset
Laptop Restaurant
word:
+word 0 40.35 58.58
+word 1 54.78 72.23
POS:
+POS 0 55.81 71.11
+POS 1 57.07 74.02
+POS 2 57.18 73.24
+POS 0/POS 1 51.85 70.58
chunk:
+chunk 0 56.74 73.45
word shape:
+word shape 0 57.29 74.02
name list:
+Gold Namelist 62.66 75.39
+Stop Namelist 62.73 76.02
Table 3: The F-scores of combination method
of subtask 1 on training data based on 2 cross-
validation
Table 2 shows the results of the above three
systems on training data. Comparing with oth-
er two methods, we easily find that the combina-
tion method outperforms the other two systems in
terms of precision, recall and F values on both do-
mains.
2.5 Result and Discussion
In constrained run, we submitted the results us-
ing the method in combination of NP and NER.
Specifically, we adopted all features and the name
lists listed in Table 3 and the CRF++ tool for the
NER system. Table 4 lists the results of our fi-
nal system and the top two systems officially re-
leased by organizers. On both domains, our sys-
tem ranks above the average under constrained
model, which proves the effectiveness of the com-
bination method by using NP extraction and NER.
From Table 2 and Table 4 we find that the re-
sults on restaurant data are much better than those
on laptop data. Based on our further observation
on training data, the possible reason is that the
number of numeric descriptions in laptop dataset
is much larger than those in restaurant dataset and
the aspect terms containing numeric description
are quite difficult to be extracted.
Dataset DLIREC NRC-Canada Our result
laptop 70.41 68.57 65.88
restaurant 78.34 80.19 78.24
Table 4: The F-scores (%) of our system and the
top two systems of subtask 1 on test dataset.
254
3 Aspect Category Classification System
Aspect category classification task tries to assign
each aspect one or more semantic category labels.
Thus, we regarded this task as a multi-class clas-
sification problem. Following (Rennie, 2001), we
built a binary model for each category, where bag-
of-words is used as features.
3.1 Features
We adopted the bag-of-words schema to represent
features as follows. Since not all training instances
have annotated aspect terms, we extracted only an-
notated aspect terms from sentence if the sentence
contains annotated aspect terms, or extracted all
words from sentence which does not contain any
annotated aspect terms as features, which results
in 5200 word features in total.
3.2 Classification Algorithm
We adopted the maximum entropy algorithm im-
plemented in Mallet toolkit (McCallum, 2002) to
build a binary classifier for each category. All pa-
rameters are set as defaults. This subtask only pro-
vides restaurant data and there are five predefined
categories (i.e., food, price, service, ambience and
anecdotes/miscellaneous), thus we build five bina-
ry classifiers in total.
3.3 Results and Discussions
Table 5 lists the precision, recall and F-score of
our final system along with the top two systems
released by the organizers.
Precision(%) Recall(%) F-score(%)
our system 65.26 69.46 67.30
rank 1 system 91.04 86.24 88.58
rank 2 system 83.23 81.37 82.29
Table 5: The results of our system and the top two
systems of subtask 3 on the test data.
From Table 5, we find that there are quite a large
room to improve our system. One main reason
is that our system only uses simple features (i.e.,
bag-of-words) and these simple features may have
poor discriminating power. Another possible rea-
son may be that in training data there are at least
half sentences without annotated aspect terms. In
this case, when we used all words in the sentences
as features, it may bring much noise. In future
work, we consider to generate more effective fea-
tures from external resources to indicate the re-
lationships between aspects and categories to im-
prove our system.
4 Aspect Term Sentiment Polarity
Classification System
Once we extract aspect terms, this task aims at
classifying the sentiment orientation of the anno-
tated aspect terms. To address this task, we firstly
extracted three types of features: sentiment lexi-
con based features, topic model based features and
other features. Then two machine learning algo-
rithms, i.e., SVM and MaxEnt, were used to con-
duct classification models.
4.1 Features
4.1.1 Sentiment Lexicon (SL) Features
We observed that the sentiment orientation of an
aspect term is usually revealed by the surrounding
terms. So in this feature we took four words before
and four words after the current aspect term and
then calculated their respective positive,negative
and neutral scores. During the calculation we re-
versed the sentiment orientation of the term if a
negation occurs before it. We manually built a
negative list: {no, nor, not, neither, none, no-
body, nothing, hardly, seldom}. Eight sentimen-
t lexicons are used: Bing Liu opinion lexicon
4
,
General Inquirer lexicon
5
, IMDB
6
, MPQA
7
, Sen-
tiWordNet
8
, NRC emotion lexicon
9
, NRC Hash-
tag Sentiment Lexicon
10
and NRC Sentiment140
Lexicon
11
. With regard to the synonym selection
of SentiWordNet, we selected the first term in the
synset as our lexicon. If the eight words surround-
ing the aspect term do not exist in the eight cor-
responding sentiment lexicons, we set their three
sentiment scores as 0. Then we got 24 sentimen-
t values for each word (3 polarities * 8 lexicons)
and summed up the values of eight words for each
sentiment polarity (i.e., positive, negative and neu-
ral). Finally we got 24 sentiment lexicon features
for each aspect.
4
http://www.cs.uic.edu/
?
liub/FBS/sentiment-
analysis.html#lexicon
5
http://www.wjh.harvard.edu/
?
inquirer/homecat.htm
6
http://anthology.aclweb.org//S/S13/S13-
2.pdf#page=444
7
http://mpqa.cs.pitt.edu/
8
http://sentiwordnet.isti.cnr.it/
9
http://mailman.uib.no/public/corpora/2012-
June/015643.html
10
http://www.umiacs.umd.edu/?saif/WebDocs/NRC-
Hashtag-Sentiment-Lexicon-v0.1.zip
11
http://sentiwordnet.isti.cnr.it/
255
feature F-pos(%) F-neg(%) F-neu(%) Acc(%)
MaxEnt SVM MaxEnt SVM MaxEnt SVM MaxEnt SVM
SL 72.50? 1.91 70.99? 5.91 65.10? 1.99 65.66? 3.48 25.54? 5.68 24.02? 9.28 62.28? 2.59 61.61? 4.68
+Other 72.92? 2.12 72.70? 1.44 65.93? 3.89 65.09? 3.67 31.14? 5.77 34.00? 7.31 62.88? 3.22 62.54? 3.17
+Topic 73.14? 1.02 72.21? 1.44 65.55? 5.43 65.58? 3.45 34.34? 10.55 12.16? 4.96 63.00? 4.34 61.74? 3.10
Table 6: The results of our system in subtask 2 on laptop training data based on 5-fold cross validation.
features F-pos(%) F-neg(%) F-neu(%) Acc(%)
MaxEnt SVM MaxEnt SVM MaxEnt SVM MaxEnt SVM
SL 79.78? 1.37 79.85? 1.35 49.37? 3.54 47.96? 4.52 26.02? 3.62 31.67? 2.84 65.61? 2.59 65.45? 1.98
+Other 80.48? 2.18 79.09? 1.42 53.17? 2.70 50.51? 3.34 29.25? 3.60 33.13? 6.89 66.80? 2.33 65.21? 2.35
+Topic 80.71? 1.71 77.94? 1.34 52.61? 2.52 46.65? 3.17 34.51? 3.35 3.40? 2.79 67.18? 2.52 64.72? 1.48
Table 7: The results of our system in subtask 2 on restaurant training data based on 5-fold cross valida-
tion.
4.1.2 Topic Features
In this section we considered to use the bag-of-
topics feature to replace the traditional bag-of-
words feature since the bag-of-words feature are
very sparse in the data set. To construct the cluster-
s of topics, we used the LDA
12
based topic model
to estimate the K topics (in our experiment, we
set K to 50) from training data. Then we inferred
the topic distribution from training and test data
respectively as topic features.
4.1.3 Other Features
Besides, we also proposed the following other fea-
tures in order to capture more useful information
from the short texts.
Aspect distance This feature records the num-
ber of words from the current aspect to the next
aspect in the same sentence. If the current aspect
term is the last term in the sentence, this value is
calculated as the negative number of words from
the current aspect to the former aspect. If only one
aspect term exists in a sentence, then the value is
set to zero.
Number of aspects This feature describes the
number of aspect terms in the current sentence.
Negation flag feature We set this feature as 1
if a negation word occurs in the current sentence,
otherwise -1.
Number of negations This feature is the num-
ber of negation words in the current sentence.
4.2 Classification Algorithms
The maximum entropy and SVMwhich are imple-
mented in Mallet toolkit (McCallum, 2002) and
LibSVM (Chang and Lin, 2011) respectively are
12
http://www.cs.princeton.edu/ blei/lda-c/
used to construct the classification model from
training data. Due to the limit of time, all parame-
ters are set as defaults.
4.3 Results and Discussions
4.3.1 Results on Training Data
To compare the performance of different features
and different algorithms, we performed a 5-fold
cross validation on training data of two domain-
s. Table 6 and Table 7 show the results of two
domains in terms of F-scores and accuracy with
mean and standard deviation. The best results are
shown in bold.
From above two tables, we found that (1) Max-
Ent performed better than SVM on both dataset-
s and all feature types, and (2) using all features
achieved the best results. Moreover, the F-pos re-
sult was the highest in both datasets and the pos-
sible reason is that the majority of training in-
stances are positive sentiment. We also found that
in restaurant dataset, F-neg (52.61%) was much
smaller than F-pos (80.17%). However, in lap-
top dataset, they performed comparable results.
The possible reason is that the number of neg-
ative instances (805) is much smaller than the
number of positive instances (2164) in restauran-
t dataset, while the distribution is nearly even in
laptop dataset. So for restaurant data, we also con-
ducted another controlled experiment which dou-
bled the amount of negative instances of restaurant
dataset. Table 8 shows the preliminary experimen-
tal results on the doubled negative training data. It
illustrates that the F-neg increases a little but the
overall accuracy without any improvement even
slightly decreases after doubling the negative in-
stances. This result is beyond our expectation but
256
no further deep analysis has been done so far.
Strategy F-pos(%) F-neg(%) F-neu(%) Acc(%)
Double 80.28 55.11 19.22 65.48
No double 80.71 52.61 34.51 67.18
Table 8: The results of controlled experiment on
restaurant dataset (MaxEnt).
4.3.2 Results on Test Data
Based on above results on training data, our final
system used all provided training data for both do-
mains. The MaxEnt algorithm is used for our final
system. Table 9 shows our results alone with the
top two systems results released by organizers.
Our final results ranked the 12th on the lap-
top dataset and the 14th on the restaurant dataset.
On one hand, the accuracy in restaurant dataset is
higher than laptop dataset for the possible reason
that the data size of restaurant dataset is much big-
ger than that of laptop dataset. On the other hand,
our results ranked middle in both datasets. Since
we utilized eight contextual words around aspect
to extract features and it may bring some noise.
Dataset laptop restaurant
our system 61.16 70.72
rank 1 system 70.49 80.95
rank 2 system 66.97 80.16
Table 9: The Accuracy (%) of our system and the
top two systems on test dataset in subtask 2.
5 Aspect Category Sentiment Polarity
System
The aspect category sentiment polarity classifi-
cation task is also only applicable to restauran-
t domain. For this task, we adopted the bag-
of-sentiment words representation, extracted sen-
timent features and used the supervised machine
learning algorithms to determine the sentimen-
t orientation of each category.
5.1 Features
To extract features, we firstly used eight sentiment
lexicons mentioned in Section 4.1.1 to build a big
sentiment words dictionary. Then we extracted al-
l aspect words and all sentiment words in train-
ing set as features. In the training and test data,
we used the sentiment polarity score of sentiment
word and the presence or absence of each aspect
term as their feature values.
5.2 Classification Algorithms
TheMaxEnt algorithm implemented inMallet (M-
cCallum, 2002) with default parameters is used to
build a polarity classifier.
5.3 Experiment and Results
We used all features and the maximum entropy al-
gorithm to conduct our final system. Table 10 list-
s the final results of our submitted system along
with top two systems.
As shown in Table 10, the accuracy of our sys-
tem is 0.63 while the best result is 0.83. The main
reason is that the features we used are quite sim-
ple. For the future work, more sufficient features
are examined to help classification.
6 Conclusion
In this work we proposed a combination of NP
and NER method and multiple features for aspec-
t extraction. And we also used multiple features
including eight sentiment lexicons for aspect and
category sentiment classification. Our final sys-
tems rank above average in the four subtasks. In
future work, we would expect to improve the re-
call of aspect terms extraction by extending name
lists using external data and seek other effective
features such as discourse relation, syntactic struc-
ture to improve the classification accuracy.
Systems our system rank 1 system rank 2 system
Acc(%) 63.41 82.93 78.15
Table 10: The accuracy of our system and the top
two systems of subtask 4 on test dataset
Acknowledgements
The authors would like to thank the organizers and
reviewers for this interesting task and their helpful
suggestions and comments. This research is sup-
ported by grants from National Natural Science
Foundation of China (No.60903093) and Shang-
hai Knowledge Service Platform Project (No.
ZF1213).
References
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. Sentiwordnet 3.0: An enhanced lexical
resource for sentiment analysis and opinion mining.
In LREC, volume 10, pages 2200?2204.
257
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: A library for support vector machines. ACM
Transactions on Intelligent Systems and Technolo-
gy, 2:27:1?27:27. Software available at http://
www.csie.ntu.edu.tw/
?
cjlin/libsvm.
Namrata Godbole, Manja Srinivasaiah, and Steven
Skiena. 2007. Large-scale sentiment analysis for
news and blogs. ICWSM, 7.
Suin Kim, Jianwen Zhang, Zheng Chen, Alice Oh, and
Shixia Liu. 2013. A hierarchical aspect-sentiment
model for online reviews. In Proceedings of AAAI.
Sudheer Kovelamudi, Sethu Ramalingam, Arpit Sood,
and Vasudeva Varma. 2011. Domain independen-
t model for product attribute extraction from user
reviews using wikipedia. In IJCNLP, pages 1408?
1412.
Lun-Wei Ku, Yu-Ting Liang, and Hsin-Hsi Chen.
2006. Opinion extraction, summarization and track-
ing in news and blog corpora. In AAAI Spring Sym-
posium: Computational Approaches to Analyzing
Weblogs, volume 100107.
Bing Liu. 2012. Sentiment analysis and opinion min-
ing. Synthesis Lectures on Human Language Tech-
nologies, 5(1):1?167.
Chong Long, Jie Zhang, and Xiaoyan Zhut. 2010. A
review selection approach for accurate feature rating
estimation. In Proceedings of the 23rd International
Conference on Computational Linguistics: Posters,
pages 766?774. Association for Computational Lin-
guistics.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
Arjun Mukherjee and Bing Liu. 2012. Aspect ex-
traction through semi-supervised modeling. In Pro-
ceedings of the 50th Annual Meeting of the Associ-
ation for Computational Linguistics: Long Papers-
Volume 1, pages 339?348. Association for Compu-
tational Linguistics.
Maria Pontiki, Dimitrios Galanis, John Pavlopou-
los, Haris Papageorgiou, Ion Androutsopoulos, and
Suresh Manandhar. 2014. Semeval-2014 task 4:
Aspect based sentiment analysis. in proceedings of
the 8th international workshop on semantic evalua-
tion (semeval 2014). Dublin, Ireland.
Jason DM Rennie. 2001. Improving multi-class text
classification with naive Bayes. Ph.D. thesis, Mas-
sachusetts Institute of Technology.
Zhiqiang Toh, Wenting Wang, Man Lan, and Xi-
aoli Li. 2012. An ner-based product identifica-
tion and lucene-based product linking approach to
cprod1 challenge: Description of submission sys-
tem to cprod1 challenge. In Data Mining Workshop-
s (ICDMW), 2012 IEEE 12th International Confer-
ence on, pages 869?871. IEEE.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2009. Recognizing contextual polarity: An explo-
ration of features for phrase-level sentiment analy-
sis. Computational linguistics, 35(3):399?433.
Tian Tian Zhu, Fang Xi Zhang, and Man Lan. 2013.
Ecnucs: A surface information based system de-
scription of sentiment analysis in twitter in the
semeval-2013 (task 2). Atlanta, Georgia, USA, page
408.
258
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 259?264,
Dublin, Ireland, August 23-24, 2014.
ECNU: Expression- and Message-level Sentiment Orientation
Classification in Twitter Using Multiple Effective Features
Jiang Zhao
?
, Man Lan
?
, Tian Tian Zhu
?
Department of Computer Science and Technology
East China Normal University
?
51121201042,51111201046@ecnu.cn;
?
mlan@cs.ecnu.edu.cn
Abstract
Microblogging websites (such as Twitter,
Facebook) are rich sources of data for
opinion mining and sentiment analysis. In
this paper, we describe our approaches
used for sentiment analysis in twitter (task
9) organized in SemEval 2014. This task
tries to determine whether the sentiment
orientations conveyed by the whole tweets
or pieces of tweets are positive, negative
or neutral. To solve this problem, we ex-
tracted several simple and basic features
considering the following aspects: surface
text, syntax, sentiment score and twitter
characteristic. Then we exploited these
features to build a classifier using SVM
algorithm. Despite the simplicity of fea-
tures, our systems rank above the average.
1 Introduction
Microblogging services such as Twitter
1
, Face-
book
2
today play an important role in expressing
opinions on a variety of topics, discussing current
issues or sharing one?s feelings about different ob-
jects in our daily life (Agarwal and Sabharwal,
2012). Therefore, Twitter (and other platforms)
has become a valuable source of users? sentiments
and opinions and with the continuous and rapid
growth of the number of tweets, analyzing the sen-
timents expressed in twitter has attracted more and
more researchers and communities, for example,
the sentiment analysis task in twitter was held in
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
1
http://twitter.com
2
http://facebook.com/
SemEval 2013 (Nakov et al., 2013). It will bene-
fit lots of real applications such as simultaneously
businesses, media outlets, and help investors to
discover product trends, identify customer pref-
erences and categorize users by analyzing these
tweets (Becker et al., 2013).
The task of sentiment analysis in twitter in Se-
mEval 2014 (Sara et al., 2014) aims to classify
whether a tweet?s sentiment is positive, negative or
neutral at expression level or message level. The
expression-level subtask (i.e., subtask A) is to de-
termine the sentiment of a marked instance of a
word or phrase in the context of a given message,
while the message-level subtask (i.e., subtask B)
aims to determine the sentiment of a whole mes-
sage. Previous work (Nakov et al., 2013) showed
that message-level sentiment classification is more
difficult than that of expression-level (i.e., 0.690 vs
0.889 in terms of F-measure) since a message may
be composed of inconsistent sentiments.
To date, lots of approaches have been proposed
for conventional blogging sentiment analysis and
a very broad overview is presented in (Pang and
Lee, 2008). Inspired by that, many features used
in microblogging mining are adopted from tradi-
tional blogging sentiment analysis task. For ex-
ample, n-grams at the character or word level,
part-of-speech tags, negations, sentiment lexicons
were used in most of current work (Agarwal et
al., 2011; Barbosa and Feng, 2010; Zhu et al.,
2013; Mohammad et al., 2013; K?okciyan et al.,
2013). They found that n-grams are still effective
in spite of the short length nature of microblog-
ging and the distributions of different POS tags
in tweets with different polarities are highly dif-
ferent (Pak and Paroubek, 2010). Compared with
formal blog texts, tweets often contain many in-
formal writings including slangs, emoticons, cre-
259
ative spellings, abbreviations and special marks
(i.e., mentions @ and hashtags #), and thus many
twitter-specific features are proposed to character-
ize this phenomena. For example, features record
the number of emoticons, elongated words and
hashtags were used in (Mohammad et al., 2013;
Zhu et al., 2013; K?okciyan et al., 2013). In this
work, we adopted many features from previous
work and then these features were fed to SVM to
perform classification.
The remainder of this paper is organized as fol-
lows. Section 2 describes our systems including
preprocessing, feature representations, data sets,
etc. Results of two subtasks and discussions are
reported in Section 3. Finally, we conclude this
paper in Section 4.
2 Our Systems
We extracted eight types of features and the first
six types were used in subtask A and all features
were used in subtask B. Then, several classifica-
tion algorithms were examined on the develop-
ment data set and the algorithm with the best per-
formance was chosen in our final submitted sys-
tems.
2.1 Preprocessing
In order to remedy as many informal texts as
possible, we recovered the elongated words to
their normal forms, e.g., ?goooooood? to ?good?
and collected about five thousand slangs or ab-
breviations from Internet to convert each slang
to its complete form, e.g., ?1dering? to ?won-
dering?, ?2g2b4g? to ?to good to be forgotten?.
Then these preprocessed texts were used to extract
non twitter-specific features (i.e., POS, lexicon, n-
grams, word cluster and indicator feature).
2.2 Feature Representations
2.2.1 POS Features
(Pak and Paroubek, 2010) found that POS tags
help to identify the sentiments of tweets and they
pointed out that objective tweets often contain
more nouns than subjective tweets and subjec-
tive tweets may carry more adjectives and adverbs
than objective tweets. Therefore, we used Stan-
ford POS Tagger
3
and recorded the number of
four different tags for each tweet: noun (the cor-
responding POS tags are ?NN?, ?NNP?, ?NNS?
and ?NNPS?), verb (the corresponding POS tags
3
http://nlp.stanford.edu/software/tagger.shtml
are ?VB?, ?VBD?, ?VBG?, ?VBN?, ?VBP? and
?VBZ?), adjective (the corresponding POS tags
are ?JJ?, ?JJR? and ?JJS?) and adverb (the corre-
sponding POS tags are ?RB?, ?RBR? and ?RBS?).
Then we normalized them by the length of given
instance or message.
2.2.2 Sentiment Lexicon-based Features
Sentiment lexicons are widely used to calculate
the sentiment scores of phrases or messages in pre-
vious work (Nakov et al., 2013; Mohammad et al.,
2013) and they are proved to be very helpful to
detect the sentiment orientation. Given a phrase
or message, we calculated the following six fea-
ture values: (1) the ratio of positive words to all
words, i.e., the number of positive words divided
by the number of total words; (2) the ratio of neg-
ative words to all words; (3) the ratio of objective
words to all words; (4) the ratio of positive senti-
ment score to the total score (i.e., the sum of the
positive and negative score); (5) the ratio of nega-
tive sentiment score to the total score; (6) the ratio
of positive score to negative score, if the negative
score is zero, which means this phrase or message
has a very strong positive sentiment orientation,
we set ten times of positive score as its value.
During the calculation, we also considered the
effects of negation words since they may reverse
the sentiment orientation in most cases. To do so,
we defined the negation context as a snippet of a
tweet that starts with a negation word and ends
with punctuation marks. If a non-negation word
is in a negation context and also in the sentiment
lexicon, we reverse its polarity. For example, the
word ?bad? in phrase ?not bad? originally has a
negative score of 0.625, after reversal, this phrase
has a positive score of 0.625. A manually made
list containing 29 negation words (e.g., no, hardly,
never, etc) was used in our experiment.
Four sentiment lexicons were used to decide
whether a word is subjective or objective and ob-
tain its sentiment score.
MPQA (Wilson et al., 2009). This subjectiv-
ity lexicon contains about 8000 subjective words
and each word has two types of sentiment strength:
strong subjective and weak subjective, and four
kinds of sentiment polarities: positive, negative,
both (positive and negative) and neutral. We used
this lexicon to determine whether a word is posi-
tive, negative or objective and assign a value of 0.5
or 1 if it is weak or strong subjective (i.e., positive
or negative) respectively.
260
SentiWordNet(SWN) (Baccianella et al.,
2010). This sentiment lexicon contains about
117 thousand items and each item corresponds
to a synset of WordNet. Three sentiment scores:
positivity, negativity, objectivity are provided and
the sum of these three scores is always 1, for
example, living#a#3, positivity: 0.5, negativity:
0.125, objectivity: 0.375. In experiment we used
the most common sense of a word.
NRC (Mohammad et al., 2013). Mohammad et
al. collected two sets of tweets and each tweet con-
tains the seed hashtags or emoticons and then they
labeled the sentiment orientation for each tweet
according to its hashtags or emoticons. They used
pointwise mutual information (PMI) to calculate
the sentiment score for each word and obtained
two sentiment lexicons (i.e., hashtag lexicon and
emoticon lexicon).
IMDB. We generated an unigram lexicon by
ourselves from a large movie review data set from
IMDB website (Maas et al., 2011) which con-
tains 25,000 positive and 25,000 negative movie
reviews by calculating their PMI scores.
2.2.3 Word n-Gram
Words in themselves in tweets usually carry out
the original sentiment orientation, so we con-
sider word n-grams as one feature. We removed
URLs, mentions, hashtags, stopwords from tweet
and then all words were stemmed using the nltk
4
toolkit. For subtask A, only unigram was used and
we used word frequency as feature values. For
subtask B, both unigram and bigram were used.
Besides, weighted unigram was also used where
we replaced word frequency with their sentiment
scores using the hashtag lexicon and emoticon lex-
icon in NRC.
2.2.4 Twitter-specific Features
Punctuation Generally, punctuation may express
users? sentiment in a certain extent. Therefore we
recorded the frequency of the following four types
of punctuation: exclamation (!), question (?), dou-
ble (?) and single marks (?). In addition, we also
recorded the number of contiguous sequences of
exclamation marks, question marks, and both of
themwhich appeared at the end of a phrase or mes-
sage.
Emoticon Emoticons are widely used to directly
express the sentiment of users and thus we counted
4
http://nltk.org/
the number of positive emoticons, negative emoti-
cons and the sum of positive and negative emoti-
cons. To identify the polarities of emoticons, we
collected 36 positive emoticons and 33 negative
emoticons from the Internet.
Hashtag A hashtag is a short phrase that con-
catenates more than one words together without
white spaces and users usually use hashtags to
label the subject topic of a tweet, e.g., #toobad,
#ihateschool, #NewGlee. Since a hashtag may
contain a strong sentiment orientation, we first
used the Viterbi algorithm (Berardi et al., 2011)
to split hashtags and then calculated the sentiment
scores of hashtags using the hashtag and emoticon
lexicon in NRC.
2.2.5 Word Cluster
Apart from n-gram, we presented another word
representations based on word clusters to explore
shallow semantic meanings and reduced the spar-
sity of the word space. 1000 word clusters pro-
vided by CMU pos-tagging tool
5
were used to rep-
resent tweet contents. For each tweet we recorded
the number of words from each cluster, resulting
in 1000 features.
2.2.6 Indicator Features
We observed that the polarity of a message some-
times is revealed by some special individual posi-
tive or negative words in a certain degree. How-
ever the sentiment lexicon based features where
a synthetical sentiment score of a message is cal-
culated may hide this information. Therefore, we
directly used several individual sentiment scores
as features. Specifically, we created the following
sixteen features for each message where the hash-
tag and emoticon lexicons were used to obtain sen-
timent scores: the sentiment scores of the first and
last sentiment-bearing words, the three highest and
lowest sentiment scores.
2.3 Data sets and Evaluation Metric
The organizers provide tweet ids and a script for
all participants to collect data. Table 1 shows the
statistics of the data set used in our experiments.
To examine the generalization of models trained
on tweets, the test data provided by the organiz-
ers consists of instances from different domains
for both subtasks. Specifically, five corpora are in-
cluded: LiveJournal(2014) is a collection of com-
ments from LiveJournal blogs, SMS2013 is a SMS
5
http://www.ark.cs.cmu.edu/TweetNLP/
261
data set directly from last year, Twitter2013 is a
twitter data set directly from last year, Twitter2014
is a new twitter data set and Twitter2014Sarcasm
is a collection of tweets that contain sarcasm. No-
tice that the data set SMS2013 and Twitter2013
were also used as our development set. Form Ta-
ble 1, we find that (1) the class distributions of test
data sets almost agree with training data sets for
both subtasks, (2) the percentages of class neutral
in two subtasks are significantly different (4.7%
vs 45.5%), which reflects that a sentence which is
composed of different sentiment expressions may
act neutrality, (3) Twitter2014Sarcasm data set is
very small. According to the guideline, we did not
use any development data for training in the eval-
uation period.
data set Positive Negative Neutral Total
subtask A:
train 3,609(61%) 2,023(34%) 265(5%) 5,897
dev 2,734(62%) 1,541(35%) 160(3%) 4,435
test
LiveJournal 660(50%) 511(39%) 144(11%) 1,315
SMS2013 1,071(46%) 1,104(47%) 159( 7%) 2,334
Twitter2013 2,734(62%) 1,541(35%) 160(3%) 4,435
Twitter2014 1,807(73%) 578(23%) 88( 4%) 2,473
Twitter2014S 82(66%) 37(30%) 5(4%) 124
all 6,354(59%) 3,771(35%) 556(6%) 10,681
subtask B:
train 3,069(36%) 1,313(15%) 4,089(49%) 8,471
dev 1,572(41%) 601(16%) 1,640(43%) 3,813
test
LiveJournal 427(37%) 304(27%) 411(36%) 1,142
SMS2013 492(24%) 394(19%) 1,207(57%) 2,093
Twitter2013 1,572(41%) 601(16%) 1,640(43%) 3,813
Twitter2014 982(53%) 202(11%) 669(36%) 1,853
Twitter2014S 33(38%) 40(47%) 13(15%) 86
all 3,506(39%) 1,541(17%) 3,940(44%) 8,987
Table 1: Statistics of data sets in training (train),
development (dev), test (test) set. Twitter2014S
stands for Twitter2014Sarcasm.
We used macro-averaged F-measure of positive
and negative classes (without neutral since it is
margin in training data) to evaluate the perfor-
mance of our systems and the averaged F-measure
of five corpora was used to rank the final results.
2.4 Submitted System Configurations
For each subtask, each team can submit two runs:
(1) constrained: only the provided data set can be
used for training and no additional annotated data
is allowed for training, however other resources
such as lexicons are allowed; (2) unconstrained:
any additional data can be used for training. We
explored several classification algorithms on the
development set and configured our final systems
as follows. For constrained system, we used SVM
and logistic regression algorithm implemented in
scikit-learn toolkit (Pedregosa et al., 2011) to ad-
dress two subtasks respectively and used self-
training strategy to conduct unconstrained system.
Self-training is a semi-supervised learning method
where a classifier is first trained with a small
amount of labeled data and then we repeat the fol-
lowing procedure: the most confident predictions
by the current classifier are added to training pool
and then the classifier is retrained(Zhu, 2005). The
parameters in constrained models and the growth
size k and iteration number T in self-training are
listed in Table 2 according to the results of prelim-
inary experiments.
task constrained unconstrained
subtask A SVM, kernel=rbf, c=500 k=100, T=40
subtask B LogisticRegression, c=1 k=90, T=40
Table 2: System configurations for the constrained
and unconstrained runs in two subtasks.
3 Results and Discussion
3.1 Results
We submitted four systems as described above and
their final results are shown in Table 3, as well as
the top-ranked systems released by the organizers.
From the table, we observe the following findings.
Firstly, we find that the results of message-level
polarity classification are much worse than the re-
sults of expression-level polarity disambiguation
(82.93 vs 61.22) on both constrained and uncon-
strained systems, which is consistent with the pre-
vious work (Nakov et al., 2013). The low per-
formance of message-level task may result from
two possible reasons: (1) a message may con-
tain mixed sentiments and (2) the strength of
sentiments is different. In contrast, the texts in
expression-level task are usually short and contain
a single sentiment orientation, which leads to bet-
ter performance.
Secondly, whether on constrained or uncon-
strained systems, the performance on Twit-
ter2014Sarcasm data set is much worse than the
performance on the other four data sets. This is
because that sarcasm often expresses the opposite
meaning of what it seems to say, that means the
actual sentiment orientation of a word is opposite
to its original orientation. Moreover, even for our
human it is a challenge to identify whether it is a
sarcasm or not.
Thirdly, the results on LiveJournal and SMS
are comparable to the results on Twitter2013 and
Twitter2014 in both subtasks, which indicates that
262
online comments and SMS share some common
characteristics with tweets (e.g., emoticons and
punctuation). Therefore, in case of lack of labeled
online comments or SMS data, we can use the ex-
isting tweets as training data instead.
Fourthly, our unconstrained systems exploit the
test data of year 2014 in training stage and perform
a worse result in subtask B. We speculate that the
failure of using self-training on message-level data
set is because that the performance of initial clas-
sifier was low and thus in the following iterations
more and more noisy instances were selected to
add the training pool, which eventually resulted in
a final weak classifier.
In summary, we adopted some simple and ba-
sic features to classify the polarities of expressions
and messages and they were promising. For sub-
task A, our systems rank 5th out of 19 submissions
under the constrained setting and rank 2nd out of 6
submissions under the unconstrained setting. For
subtask B, our systems rank 16th out of 42 submis-
sions under the constrained setting and rank 5th
out of 8 submissions under the unconstrained set-
ting.
3.2 Feature Combination Experiments
To explore the effectiveness of different feature
types, we conducted a series of feature combina-
tion experiments using the constrained setting as
shown in Table 2 for both subtasks. For each time
we repeated to add one feature type to current fea-
ture set and then selected the best one until all the
feature types were processed. Table 4 shows the
results of different feature combinations and the
best results are shown in bold font.
From Table 4, we find that (1) MPQA, n-gram
and Word cluster are the most effective feature
types to identify the polarities; (2) The POS tags
make margin contribution to improve the perfor-
mance since Stanford parser is designed for for-
mal texts and in the future we may use specific
parser instead; (3) The lexicon IMDB extracted
from movie reviews has negative effects to clas-
sify twitter data, which indicates that there exist
differences in the way of expressing sentiments
between these two domains; (4) Twitter-specific
features, i.e., hashtag and emoticon, are not as ef-
fective as expected. This is because they are sparse
in the data sets. In subtask Awith 16578 instances,
only 292 instances (1.76%) have hashtags and 419
instances (2.52%) have emoticons. In subtask B
with 17458 messages, more instances have hash-
tags (16.72%) and emoticons (26.70%). (5) For
subtask A MPQA, n-gram, NRC and punctuation
features achieve the best performance and for sub-
task B the best performance is achieved by using
almost all features.
In summary, we find that n-gram and some lex-
icons such as MPQA are the most effective while
twitter-specific features (i.e., hashtag and emoti-
con) are not as discriminating as expected and the
main reason for this is that they are sparse in the
data sets.
Feature Subtask A Feature Subtask B
MPQA 77.49 Word cluster 53.50
.+n-gram 80.08(2.59) .+MPQA 58.35(4.85)
.+NRC 82.42(2.34) .+W1Gram 60.22(1.87)
.+Pun. 83.83(1.41) .+Pun. 60.99(0.77)
.+POS 83.83(0) .+Indicator 61.38(0.39)
.+Emoticon 83.49(-0.34) .+SWN 61.51(0.13)
.+Hashtag 83.54(0.05) .+Hashtag 61.54(0.03)
.+IMDB 83.51(-0.03) .+n-gram 61.56(0.02)
.+SWN 82.92(-0.59) .+Emoticon 61.69(0.13)
- - .+POS 61.71(0.02)
- - .+IMDB 61.11(-0.6)
- - .+NRC 61.23(0.12)
Table 4: The results of feature combination exper-
iments. The numbers in the brackets are the per-
formance increments compared with the previous
results. ?.+? means to add current feature to the
previous feature set.
4 Conclusion
In this paper we used several basic feature types to
identify the sentiment polarity at expression level
or message level and these feature types include
n-gram, sentiment lexicon and twitter-specific fea-
tures, etc. Although they are simple, our systems
are still promising and rank above average (e.g.,
rank 5th out of 19 and 16th out of 42 in subtask A
and B respectively under the constrained setting).
For the future work, we would like to analyze the
distributions of different sentiments in sentences.
Acknowledgments
This research is supported by grants from Na-
tional Natural Science Foundation of China
(No.60903093) and Shanghai Knowledge Service
Platform Project (No. ZF1213).
References
Apoorv Agarwal and Jasneet Sabharwal. 2012. End-
to-end sentiment analysis of twitter data. In Pro-
263
Systems LiveJournal SMS2013 Twitter2013 Twitter2014 Twitter2014S Average
A-constrained (expression-level) 81.67 89.31 87.28 82.67 73.71 82.93
A-unconstrained 81.69 89.26 87.29 82.93 73.71 82.98
NRC-Canada-A-constrained
?
85.49 88.03 90.14 86.63 77.13 85.48
Think Positive-A-unconstrained
?
80.90 87.65 88.06 82.05 76.74 83.08
B-constrained(message-level) 69.44 59.75 62.31 63.17 51.43 61.22
B-unconstrained 64.08 56.73 63.72 63.04 49.33 59.38
NRC-Canada-B-constrained
?
74.84 70.28 70.75 69.85 58.16 68.78
Think Positive-B-unconstrained
?
66.96 63.20 68.15 67.04 47.85 62.64
Table 3: Performance of our systems and the top-ranked systems (marked with asterisk).
ceedings of the Workshop on Information Extraction
and Entity Analytics on Social Media Data, pages
39?44, Mumbai, India, December. The COLING
2012 Organizing Committee.
Apoorv Agarwal, Boyi Xie, Ilia Vovsha, Owen Ram-
bow, and Rebecca Passonneau. 2011. Sentiment
analysis of twitter data. In Proceedings of the Work-
shop on Languages in Social Media, LSM ?11, pages
30?38. Association for Computational Linguistics.
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. Sentiwordnet 3.0: An enhanced lexical
resource for sentiment analysis and opinion mining.
In LREC, volume 10, pages 2200?2204.
Luciano Barbosa and Junlan Feng. 2010. Robust sen-
timent detection on twitter from biased and noisy
data. In Proceedings of the 23rd International
Conference on Computational Linguistics: Posters,
pages 36?44. Association for Computational Lin-
guistics.
Lee Becker, George Erhart, David Skiba, and Valen-
tine Matula. 2013. Avaya: Sentiment analysis on
twitter with self-training and polarity lexicon expan-
sion. In Proceedings of the Seventh International
Workshop on Semantic Evaluation (SemEval 2013),
pages 333?340. Association for Computational Lin-
guistics, June.
Giacomo Berardi, Andrea Esuli, Diego Marcheggiani,
and Fabrizio Sebastiani. 2011. Isti@ trec microblog
track 2011: Exploring the use of hashtag segmenta-
tion and text quality ranking. In TREC.
Nadin K?okciyan, Arda C?elebi, Arzucan
?
Ozg?ur, and
Suzan
?
Usk?udarli. 2013. Bounce: Sentiment classifi-
cation in twitter using rich feature sets. In Proceed-
ings of the Seventh International Workshop on Se-
mantic Evaluation (SemEval 2013), pages 554?561.
Association for Computational Linguistics, June.
Andrew L Maas, Raymond E Daly, Peter T Pham, Dan
Huang, Andrew Y Ng, and Christopher Potts. 2011.
Learning word vectors for sentiment analysis. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies-Volume 1, pages 142?150. As-
sociation for Computational Linguistics.
Saif Mohammad, Svetlana Kiritchenko, and Xiaodan
Zhu. 2013. Nrc-canada: Building the state-of-the-
art in sentiment analysis of tweets. In Proceedings
of the Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 321?327. Asso-
ciation for Computational Linguistics, June.
Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva,
Veselin Stoyanov, Alan Ritter, and Theresa Wilson.
2013. Semeval-2013 task 2: Sentiment analysis in
twitter. In Proceedings of the Seventh International
Workshop on Semantic Evaluation (SemEval 2013),
pages 312?320. Association for Computational Lin-
guistics, June.
Alexander Pak and Patrick Paroubek. 2010. Twitter as
a corpus for sentiment analysis and opinion mining.
In Proceedings of the International Conference on
Language Resources and Evaluation, LREC.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and trends in infor-
mation retrieval, 2(1-2):1?135.
Fabian Pedregosa, Ga?el. Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
Weiss, Vincent Dubourg, Jake Vanderplas, Alexan-
dre Passos, David Cournapeau, Matthieu Brucher,
Matthieu Perrot, and
?
Edouard Duchesnay. 2011.
Scikit-learn: Machine learning in Python. Journal
of Machine Learning Research, 12:2825?2830.
Rosenthal Sara, Ritter Alan, Veselin Stoyanov, and
Nakov Preslav. 2014. Semeval-2014 task 9: Sen-
timent analysis in twitter. In Proceedings of the
Eighth International Workshop on Semantic Evalu-
ation (SemEval?14). Association for Computational
Linguistics, August.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2009. Recognizing contextual polarity: An explo-
ration of features for phrase-level sentiment analy-
sis. Computational linguistics, pages 399?433.
Tian Tian Zhu, Fang Xi Zhang, and Man Lan. 2013.
Ecnucs: A surface information based system de-
scription of sentiment analysis in twitter in the
semeval-2013 (task 2). In Proceedings of the Sev-
enth International Workshop on Semantic Evalua-
tion (SemEval 2013), page 408.
Xiaojin Zhu. 2005. Semi-supervised learning litera-
ture survey. Technical Report 1530, Computer Sci-
ences, University of Wisconsin-Madison.
264
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 265?270,
Dublin, Ireland, August 23-24, 2014.
ECNU: Leveraging on Ensemble of Heterogeneous Features and
Information Enrichment for Cross Level Semantic Similarity Estimation
Tian Tian Zhu
Department of Computer Science and
Technology
East China Normal University
51111201046@ecnu.cn
Man Lan
?
Department of Computer Science and
Technology
East China Normal University
mlan@cs.ecnu.edu.cn
?
Abstract
This paper reports our submissions to the
Cross Level Semantic Similarity (CLSS)
task in SemEval 2014. We submitted
one Random Forest regression system on
each cross level text pair, i.e., Paragraph
to Sentence (P-S), Sentence to Phrase (S-
Ph), Phrase to Word (Ph-W) and Word
to Sense (W-Se). For text pairs on P-S
level and S-Ph level, we consider them as
sentences and extract heterogeneous types
of similarity features, i.e., string features,
knowledge based features, corpus based
features, syntactic features, machine trans-
lation based features, multi-level text fea-
tures, etc. For text pairs on Ph-W level
and W-Se level, due to lack of informa-
tion, most of these features are not ap-
plicable or available. To overcome this
problem, we propose several information
enrichment methods using WordNet syn-
onym and definition. Our systems rank the
2nd out of 18 teams both on Pearson cor-
relation (official rank) and Spearman rank
correlation. Specifically, our systems take
the second place on P-S level, S-Ph level
and Ph-W level and the 4th place on W-Se
level in terms of Pearson correlation.
1 Introduction
Semantic similarity is an essential component of
many applications in Natural Language Process-
ing (NLP). Previous works often focus on text se-
mantic similarity on the same level, i.e., paragraph
to paragraph or sentence to sentence, and many ef-
fective text semantic measurements have been pro-
posed (Islam and Inkpen, 2008), (B?ar et al., 2012),
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
(Heilman and Madnani, 2012). However, in many
real world cases, the two texts may not always
be on the same level. The Cross Level Semantic
Similarity (CLSS) task in SemEval 2014 provides
a universal platform to measure the degree of se-
mantic equivalence between two texts across dif-
ferent levels. For each text pair on four cross lev-
els, i.e., Paragraph to Sentence (P-S), Sentence to
Phrase (S-Ph), Phrase to Word (Ph-W) and Word
to Sense (W-Se), participants are required to re-
turn a similarity score which ranges from 0 (no
relation) to 4 (semantic equivalence). We partici-
pate in all the four cross levels and take the second
place out of all 18 teams both on Pearson correla-
tion (official) and Spearman correlation ranks.
In this work, we present a supervised regres-
sion system for each cross level separately. For
P-S level and S-Ph level, we regard the paragraph
of P-S as a long sentence, and the phrase of S-
Ph as a short sentence. Then we use various types
of text similarity features including string features,
knowledge based features, corpus based features,
syntactic features, machine translation based fea-
tures, multi-level text features and so on, to cap-
ture the semantic similarity between two texts.
Some of these features are borrowed from our pre-
vious system in the Semantic Textual Similarity
(STS) task in
?
SEM Shared Task 2013 (Zhu and
Lan, 2013). Others followed the previous work
in (
?
Saric et al., 2012) and (Pilehvar et al., 2013).
For Ph-W level and W-Se level, since the text pairs
lack contextual information, for example, word or
sense alone no longer shares the property of sen-
tence, most features used in P-S level and S-Ph
level are not applicable or available. To overcome
the problem of insufficient information in word
and sense level, we propose several information
enrichment methods to extend information with
the aid of WordNet (Miller, 1995), which signif-
icantly improved the system performance.
The rest of this paper is organized as follows.
265
Section 2 describes the similarity features used on
four cross levels in detail. Section 3 presents ex-
periments and the results of four cross levels on
training data and test data. Conclusions and future
work are given in Section 4.
2 Text Similarity Measurements
To estimate the semantic similarity on P-S level
and S-Ph level, we treat the text pairs on both lev-
els as traditional semantic similarity computation
on sentence level and adopt 7 types of features,
i.e., string features, knowledge based features, cor-
pus based features, syntactic features, machine
translation based features, multi-level text features
and other features. All of them are borrowed
from previous work due to their superior perfor-
mance reported. For Ph-W level and W-Se level,
since word and sense alone cannot be treated as
sentence, we propose an information enrichment
method to extend original text with the help of
WordNet. Once the word or sense is enriched with
its synonym and its definition description, we can
thus adopt the previous features as well.
2.1 Preprocessing
For P-S level and S-Ph level, we perform text pre-
processing before we extract semantic similarity
features. Firstly, the Stanford parser
1
is used for
sentence tokenization and parsing. Specifically,
the tokens n?t and ?m are replaced with not and
am. Secondly, the Stanford POS Tagger
2
is used
for POS tagging. Thirdly, we use Natural Lan-
guage Toolkit
3
for WordNet based Lemmatiza-
tion, which lemmatizes the word to its nearest base
form that appears in WordNet, for example, was
is lemmatized as is rather than be.
2.2 Features on P-S Level and S-Ph Level
We treat all text pairs of P-S level and S-Ph level
as sentences and then extract 7 types of similar-
ity features as below. Totally we get 52 similarity
features. Generally, these similarity features are
represented as numerical values.
String features. Intuitively, if two texts share
more strings, they are considered to be more se-
mantic similar. We extract 13 string based features
in consideration of the common sequence shared
1
http://nlp.stanford.edu/software/lex-parser.shtml
2
http://nlp.stanford.edu/software/tagger.shtml
3
http://nltk.org/
by two texts. We chose the Longest Common Se-
quence (LCS) feature (Zhu and Lan, 2013), the N-
gram Overlap feature (n=1,2,3) and the Weighted
Word Overlap feature (
?
Saric et al., 2012). All
these features are computed from original text
and from the processed text after lemmatization
as well. Besides, we also computed the N-gram
Overlap on character level, named Character N-
gram (n=2,3,4).
Knowledge based features. Knowledge based
similarity estimation relies on the semantic net-
work of words. In this work we used the knowl-
edge based features in our previous work (Zhu and
Lan, 2013), which include four word similarity
metrics based onWordNet: Path similarity (Banea
et al., 2012), WUP similarity (Wu and Palmer,
1994), LCH similarity (Leacock and Chodorow,
1998) and Lin similarity (Lin, 1998). Then two
strategies, i.e., the best alignment strategy and the
aggregation strategy, are employed to propagate
the word similarity to the text similarity. Totally
we get 8 knowledge based features.
Corpus based features. Latent Semantic Analy-
sis (LSA) (Landauer et al., 1997) is a widely used
corpus based measure when evaluating text simi-
larity. In this work we use the Vector Space Sen-
tence Similarity proposed by (
?
Saric et al., 2012),
which represents each sentence as a single distri-
butional vector by summing up the LSA vector of
each word in the sentence. Two corpora are used
to compute the LSA vector of words: New York
Times Annotated Corpus (NYT) and Wikipedia.
Besides, in consideration of different weights for
different words, they also calculated the weighted
LSA vector for each word. In addition, we use
the Co-occurrence Retrieval Model (CRM) feature
from our previous work (Zhu and Lan, 2013) as
another corpus-based feature. The CRM is calcu-
lated based on a notion of substitutability, that is,
the more appropriate it is to substitute word w
1
in place of word w
2
in a suitable natural language
task, the more semantically similar they are. At
last, 6 corpus based features are extracted.
Syntactic features. Dependency relations of sen-
tences often contain semantic information. In this
work we follow two syntactic dependency similar-
ity features presented in our previous work (Zhu
and Lan, 2013), i.e., Simple Dependency Overlap
and Special Dependency Overlap. The Simple De-
pendency Overlap measures all dependency rela-
tions while the Special Dependency Overlap fea-
266
ture only focuses on the primary roles extracted
from several special dependency relations, i.e.,
subject, object and predict.
Machine Translation based features. Machine
translation (MT) evaluation metrics are designed
to assess whether the output of a MT system is
semantically equivalent to a set of reference trans-
lations. This type of feature has been proved to
be effective in our previous work (Zhu and Lan,
2013). As a result, we extend the original 6 lexical
level MT metrics to 10 metrics, i.e., WER, TER,
PER, BLEU, NIST, ROUGE-L, GTM-1,GTM-2,
GTM-3 and METEOR-ex. All these metrics are
calculated using the Asiya Open Toolkit for Auto-
matic Machine Translation (Meta-) Evaluation
4
.
Multi-level text Features. (Pilehvar et al., 2013)
presented a unified approach to semantic similar-
ity at multiple levels from word senses to text
documents through the semantic signature repre-
sentation of texts (e.g., sense, word or sentence).
Given initial nodes (senses), they performed ran-
dom walks on semantic network like WordNet,
then the resulting frequency distribution over all
nodes in WordNet served as semantic signature of
the text. By doing so the similarity of two texts
can be computed as the similarity of two seman-
tic signatures. In this work, we borrowed their
semantic signature method and adopted 3 similar-
ity measures to estimate two semantic signatures,
i.e., Cosine similarity, Weighted Overlap and Top-
k Jaccard (k=250, 500).
Other Features. Besides, other simple surface
features from texts, such as numbers, symbols and
length of texts, are extracted. Following (
?
Saric et
al., 2012) we adopt relative length difference, rela-
tive information content difference, numbers over-
lap, case match and stocks match.
2.3 Features on Ph-W Level
For Ph-W level, since word and phrase no longer
share the property of sentence, most features used
for sentence similarity estimation are not applica-
ble for this level. Therefore, we adopt the follow-
ing features as the basic feature set for Ph-W level.
String features. This type contains two fea-
tures. The first is a boolean feature which records
whether the word appears in the phrase. The sec-
ond is the Weighted Word Overlap feature men-
tioned in Section 2.2.
Knowledge based features. As described in Sec-
4
http://nlp.lsi.upc.edu/asiya/
tion 2.2, we compute the averaged score and the
maximal score between word and phrase using the
four word similarity measures based on WordNet,
i.e., Path, WUP, LCH and Lin.
Corpus based features. We adopt the Vector
Space Similarity described in Section 2.2. Specif-
ically, for word the single distributional vector is
the LSA vector of itself.
Multi-level text Features. As described in Sec-
tion 2.2, since the semantic signatures are pro-
posed for various kinds of texts (e.g., sense, word
or sentence), they serve as one basic feature.
Obviously, the above features extracted from
the phrase-word pair is significantly less than the
features used in P-S level and S-Ph level. This is
because the information contained in phrase-word
pair is much less than that in sentences and para-
graphs. To overcome this information insufficient
problem, we propose an information enrichment
method based on WordNet to extend the initial
word in Ph-W level as below.
Word Expansion with Definition. For the word
part in Ph-W level, we extract its definition in
terms of its most common concept inWordNet and
then replace the initial word with this definition.
This gives a much richer set of initial single word.
Since a word may have many senses, not all of
this word definition expansion are correct. But we
show below empirically that using this expanded
set improves performance. By doing so we treat
the phrase and the definition of the original word
as two sentences, and thus, all features described
in Section 2.2 are calculated.
2.4 Features on W-Se Level
For W-Se level, the information that a word and
a sense carry is less than other levels. Hence, the
basic features that can be extracted from the origi-
nal word-sense pair are even less than Ph-W level.
Therefore the basic features we use for W-Se level
are as follows.
String features. Two boolean string features
are used. One records whether the word-sense
pair shares the same POS tag and another records
whether the word-sense pair share the same word.
Knowledge based features. As described in Sec-
tion 2.2, four knowledge-based word similarity
measures based on WordNet are calculated.
Multi-level text Features. The multi-level text
features are the same as Ph-W level.
In consideration of the lack of contextual infor-
267
mation between word-sense pair, we also propose
three information enrichment methods in order to
generate more effective information for word and
sense with the aid of WordNet.
Word Expansion with Synonyms. For the word
part in W-Se level, we extract its synonyms with
the help of WordNet, then update the values
of above basic features if its synonyms achieve
higher feature value than the original word itself.
Sense Expansion with Definition. For the sense
in W-Se level, we directly use its definition in
WordNet to enrich its information. By doing so
the similarity estimation of W-Se level can be con-
verted to that of word-phrase level, therefore we
use all basic features for Ph-W level described in
Section2.3.
Word-Sense Expansion with Definition. Un-
like the above two expansion methods which focus
only on one part of W-Se level, the third method is
to enrich information for word and sense together
by using their definitions in WordNet. As before
we extract the word definition in terms of its most
common concept in WordNet and then replace the
initial word with this definition. Then we use all
features in Section 2.2.
3 Experiment and Results
We adopt supervised regression model for each
cross level. In order to compare the performance
of different regression algorithms, we perform 5-
fold cross validation on training data for each cross
level. We used several regression algorithms in-
cluding Support Vector Regression (SVR) with
3 different kernels (i.e., linear, polynomial and
rbf), Random Forest, Stochastic Gradient Descent
(SGD) and Decision Tree implemented in the
scikit-learn toolkit (Pedregosa et al., 2011). The
system performance is evaluated in Pearson corre-
lation (r) (official measure) and Spearman?s rank
correlation (?).
3.1 Results on Training Data
Table 1 and Table 2 show the averaged perfor-
mance of different regression algorithms in terms
of Pearson correlation (r) and Spearman?s rank
correlation (?) on the training data of P-S level and
S-Ph level using 5-fold cross validation, where the
standard deviation is given in brackets. The re-
sults show that Random Forest performs the best
both on P-S level and S-Ph level whether in (r) or
(?). We also find that the results of P-S level are
better than that of S-Ph level, and the reason may
be that paragraph and sentence pair contain more
information than the sentence and phrase pair.
Regression Algorithm r (%) ? (%)
SVR, ker=rbf 80.70 (?1.47) 79.90 (?1.66)
SVR, ker=poly 73.78 (?1.57) 74.41 (?1.89)
SVR, ker=linear 80.43 (?1.13) 79.46 (?1.51)
Random Forest 80.92 (?1.40) 80.20 (?2.00)
SGD 77.61 (?0.76) 77.14 (?1.49)
Decision Tree 73.23 (?2.14) 71.84 (?2.55)
Table 1: Results of different algorithms using 5-
fold cross validation on training data of P-S level
Regression Algorithm r (%) ? (%)
SVR, ker=rbf 66.14 (?5.14) 65.76 (?5.93)
SVR, ker=poly 58.93 (?2.29) 63.62 (?4.15)
SVR, ker=linear 66.78 (?4.51) 66.34 (?4.90)
Random Forest 73.18 (?5.23) 70.30 (?5.51)
SGD 63.18 (?3.61) 64.80 (?4.21)
Decision Tree 67.66 (?6.76) 66.03 (?6.64)
Table 2: Results of different algorithms using 5-
fold cross validation on training data of S-Ph level
Table 3 shows the results of different regression
algorithms and different feature sets in terms of
r and ? on the training data of Ph-W level us-
ing 5-fold cross validation, where the basic fea-
tures are denoted as Feature Set A and their com-
bination with word definition expansion features
are denoted as Feature Set B. The results show
that almost all algorithms performance have been
improved by using word definition expansion fea-
ture except Decision Tree. This proves the effec-
tiveness of the information enrichment method we
proposed in this level. Besides, Random Forest
achieves the best performance again with r=44%
and ?=41%. However, in comparison with P-S
level and S-Ph level, all scores in Table 3 drop a
lot even with information enrichment method. The
possible reason may be two: the reduction of in-
formation on Ph-W level and our information en-
richment method brings in a certain noise as well.
For W-Se level, in order to examine the perfor-
mance of different information enrichment meth-
ods, we perform experiments on 4 different fea-
ture sets from A to D, where feature set A con-
tains the basic features, feature set B, C and D
add one information enrichment method based on
former feature set. Table 4 and 5 present the r
and ? results of 4 feature sets using different re-
gression algorithms. From Table 4 and 5 we see
that most correlation scores are below 40% and
268
Regression Algorithm r (%) ? (%)
Feature Set A
1
Feature Set B
2
Feature Set A Feature Set B
SVR, ker=rbf 34.67 (?4.34) 42.62 (?6.36) 33.26 (?4.24) 40.87 (?6.24)
SVR, ker=poly 19.00 (?4.26) 24.06 (?5.55) 21.13 (?4.86) 28.35 (?6.11)
SVR, ker=linear 34.87 (?4.65) 41.91 (?2.05) 35.42 (?5.05) 42.69 (?0.55)
Random Forest 43.17 (?7.72) 44.00 (?6.88) 40.34 (?5.71) 41.80 (?6.76)
SGD 26.20 (?3.37) 38.69 (?4.60) 23.55 (?5.01) 38.00 (?2.64)
Decision Tree 39.22 (?7.54) 32.22 (?12.74) 38.90 (?6.03) 31.64 (?10.47)
1
Feature Set A = basic feature set
2
Feature Set B = Feature Set A + Word Definition Expansion Features
Table 3: Results of different algorithms using 5-fold cross validation on training data of Ph-W level
the performance of W-Se level is the worst among
all these four levels. This illustrates that the less
information the texts contain, the worse perfor-
mance the model achieves. Again the Random
Forest algorithm performs the best among all algo-
rithms. Again almost all information enrichment
features perform better than Feature set A. This il-
lustrates that these information enrichment meth-
ods do help to improve performance. When we ob-
serve the three information enrichment methods,
we find that feature set C performs the best. In
comparison with feature set C, feature set B only
used word synonyms to expand information and
this expansion is quite limited. Feature set D per-
forms better than B but still worse than C. The rea-
son may be that when we extend sense with its def-
inition, the definition is accurate and exactly repre-
sents the meaning of sense. However since a word
often contains more than one concepts, and when
we use the definition of the most common concept
to extend word, such extension may not be correct
and the generated information may contain more
noise and/or change the original meaning of word.
3.2 Results on Test Data
According to the experiments on training data, we
select Random Forest as the final regression algo-
rithm. The number of trees in Random Forest n is
optimized to 50 and the rest parameters are set to
be default. All features in Section 2.2 are used on
P-S level, S-Ph level and Ph-W level. For W-Se
level, we take all features except word-sense def-
inition expansion feature which has been shown
to impair the system performance. For each level,
all training examples are used to learn the corre-
sponding regression model. According to the offi-
cial results released by organizers, Table 6 and Ta-
ble 7 list the top 3 systems in terms of r (official)
and ?. Our final systems rank the second both in
terms of r and ? and also achieve the second place
on P-S level, S-Ph level and Ph-W level, as well
as the 4th place on W-Se level in terms of official
Pearson correlation.
Team P-S S-Ph Ph-W W-Se r Rank
SimCompass 0.811 0.742 0.415 0.356 1
ECNU 0.834 0.771 0.315 0.269 2
UNAL-NLP 0.837 0.738 0.274 0.256 3
Table 6: Pearson Correlation (official) on test data
Team P-S S-Ph Ph-W W-Se ? Rank
SimCompass 0.801 0.728 0.424 0.344 1
ECNU 0.821 0.757 0.306 0.263 2
UNAL-NLP 0.820 0.710 0.249 0.236 6
Table 7: Spearman Correlation on test data
4 Conclusion
We build a supervised Random Forest regression
model for each cross level. For P-S and S-Ph level,
we adopt the ensemble of heterogeneous similar-
ity features, i.e., string features, knowledge based
features, corpus based features, syntactic features,
machine translation based features, multi-level
text features and other features to capture the se-
mantic similarity between two texts with distinc-
tively different lengths. For Ph-W and W-Se level,
we propose information enrichment methods to
lengthen original texts in order to generate more
semantic features, which has been proved to be ef-
fective. Our submitted final systems rank the 2nd
out of 18 teams both on Pearson Rank (official
rank) and Spearman Rank, and also rank the sec-
ond place on P-S level, S-Ph level and Ph-W level,
as well as the 4th place on W-Se level in terms of
Pearson correlation. In future work we will focus
on information enrichment methods which bring
in more accurate information and less noises.
Acknowledgments
This research is supported by grants from Na-
tional Natural Science Foundation of China
269
Regression Algorithm Feature Set A
1
Feature Set B
2
Feature Set C
3
Feature Set D
4
SVR, ker=rbf 29.85 (?7.29) 34.49 (?5.55) 36.80 (?6.46) 22.19 (?6.49)
SVR, ker=poly 24.62 (?3.63) 29.27 (?3.53) 26.55 (?1.27) 25.89 (?5.63)
SVR, ker=linear 29.58 (?5.88) 34.87 (?3.97) 35.96 (?1.75) 34.57 (?3.75)
Random Forest 22.87 (?5.59) 33.97 (?1.78) 40.43 (?3.00) 37.54 (?3.20)
SGD 26.32 (?7.31) 27.36 (?6.44) 32.50 (?6.02) 18.00 (?6.13)
Decision Tree 23.40 (?5.65) 26.33 (?3.86) 33.64 (?6.97) 31.86 (?3.95)
1
Feature Set A = basic feature set
2
Feature Set B = Feature Set A + Synonym Expansion
3
Feature Set C = Feature Set B + Sense Definition Expansion Features
4
Feature Set D = Feature Set C + Word-Sense Definition Expansion Features
Table 4: Results of different algorithms using 5-fold CV on training data of W-Se level (r (%))
Regression Algorithm Feature Set A Feature Set B Feature Set C Feature Set D
SVR, ker=rbf 28.41 (?8.99) 29.61 (?6.23) 34.18 (?6.36) 22.90 (?6.78)
SVR, ker=poly 23.05 (?7.53) 22.47 (?4.47) 21.63 (?4.37) 25.37 (?7.25)
SVR, ker=linear 27.29 (?7.02) 31.79 (?4.00) 34.75 (?3.55) 34.19 (?3.06)
Random Forest 19.66 (?6.75) 31.98 (?3.21) 38.57 (?3.60) 37.56 (?3.15)
SGD 24.12 (?7.98) 24.62 (?6.36) 29.27 (?5.86) 23.05 (?11.23)
Decision Tree 22.30 (?5.25) 25.09 (?3.64) 31.99 (?7.81) 30.51 (?5.27)
Table 5: Results of different algorithms using 5-fold CV on training data of W-Se level (? (%))
(No.60903093) and Shanghai Knowledge Service
Platform Project (No. ZF1213).
References
Carmen Banea, Samer Hassan, Michael Mohler, and
Rada Mihalcea. 2012. Unt: A supervised synergis-
tic approach to semantic text similarity. pages 635?
642. First Joint Conference on Lexical and Compu-
tational Semantics (*SEM).
Daniel B?ar, Chris Biemann, Iryna Gurevych, and
Torsten Zesch. 2012. Ukp: Computing seman-
tic textual similarity by combining multiple content
similarity measures. pages 435?440. First Joint
Conference on Lexical and Computational Seman-
tics (*SEM).
Michael Heilman and Nitin Madnani. 2012. Ets:
Discriminative edit models for paraphrase scoring.
pages 529?535. First Joint Conference on Lexical
and Computational Semantics (*SEM).
Aminul Islam and Diana Inkpen. 2008. Semantic text
similarity using corpus-based word similarity and
string similarity. ACM Transactions on Knowledge
Discovery from Data (TKDD), 2(2):10.
Thomas K Landauer, Darrell Laham, Bob Rehder, and
Missy E Schreiner. 1997. How well can passage
meaning be derived without using word order? a
comparison of latent semantic analysis and humans.
In Proceedings of the 19th annual meeting of the
Cognitive Science Society, pages 412?417.
Claudia Leacock and Martin Chodorow. 1998. Com-
bining local context and wordnet similarity for word
sense identification. WordNet: An electronic lexical
database, 49(2):265?283.
Dekang Lin. 1998. An information-theoretic defini-
tion of similarity. In Proceedings of the 15th in-
ternational conference on Machine Learning, vol-
ume 1, pages 296?304. San Francisco.
George A Miller. 1995. Wordnet: a lexical
database for english. Communications of the ACM,
38(11):39?41.
Fabian Pedregosa, Ga?el. Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
Weiss, Vincent Dubourg, Jake Vanderplas, Alexan-
dre Passos, David Cournapeau, Matthieu Brucher,
Matthieu Perrot, and
?
Edouard Duchesnay. 2011.
Scikit-learn: Machine learning in Python. Journal
of Machine Learning Research, 12:2825?2830.
Mohammad Taher Pilehvar, David Jurgens, and
Roberto Navigli. 2013. Align, disambiguate and
walk: A unified approach for measuring semantic
similarity. In Proceedings of the 51st Annual Meet-
ing of the Association for Computational Linguistics
(ACL 2013).
Frane
?
Saric, Goran Glava?s, Mladen Karan, Jan
?
Snajder,
and Bojana Dalbelo Ba?sic. 2012. Takelab: Systems
for measuring semantic text similarity. pages 441?
448. First Joint Conference on Lexical and Compu-
tational Semantics (*SEM).
Zhibiao Wu and Martha Palmer. 1994. Verbs seman-
tics and lexical selection. In Proceedings of the 32nd
annual meeting on Association for Computational
Linguistics, pages 133?138. Association for Com-
putational Linguistics.
Tian Tian Zhu and Man Lan. 2013. Ecnucs: Measur-
ing short text semantic equivalence using multiple
similarity measurements. Atlanta, Georgia, USA,
page 124.
270
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 271?277,
Dublin, Ireland, August 23-24, 2014.
ECNU: One Stone Two Birds: Ensemble of Heterogenous Measures for
Semantic Relatedness and Textual Entailment
Jiang Zhao, Tian Tian Zhu, Man Lan
?
Department of Computer Science and Technology
East China Normal University
51121201042,51111201046@ecnu.cn; mlan@cs.ecnu.edu.cn
?
Abstract
This paper presents our approach to se-
mantic relatedness and textual entailment
subtasks organized as task 1 in SemEval
2014. Specifically, we address two ques-
tions: (1) Can we solve these two sub-
tasks together? (2) Are features proposed
for textual entailment task still effective
for semantic relatedness task? To address
them, we extracted seven types of features
including text difference measures pro-
posed in entailment judgement subtask, as
well as common text similarity measures
used in both subtasks. Then we exploited
the same feature set to solve the both sub-
tasks by considering them as a regression
and a classification task respectively and
performed a study of influence of differ-
ent features. We achieved the first and the
second rank for relatedness and entailment
task respectively.
1 Introduction
Distributional Semantic Models (DSMs)(surveyed
in (Turney et al., 2010)) exploit the co-occurrences
of other words with the word being modeled to
compute the semantic meaning of the word un-
der the distributional hypothesis: ?similar words
share similar contexts? (Harris, 1954). Despite
their success, DSMs are severely limited to model
the semantic of long phrases or sentences since
they ignore grammatical structures and logical
words. Compositional Distributional Semantic
Models (CDSMs)(Zanzotto et al., 2010; Socher et
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
al., 2012) extend DSMs to sentence level to cap-
ture the compositionality in the semantic vector
space, which has seen a rapidly growing interest
in recent years. Although several CDSMs have
been proposed, benchmarks are lagging behind.
Previous work (Grefenstette and Sadrzadeh, 2011;
Socher et al., 2012) performed experiments on
their own datasets or on the same datasets which
are limited to a few hundred instances of very short
sentences with a fixed structure.
To provide a benchmark so as to compare dif-
ferent CDSMs, the sentences involving composi-
tional knowledge task in SemEval 2014 (Marelli et
al., 2014) develops a large dataset which is full of
lexical, syntactic and semantic phenomena. It con-
sists of two subtasks: semantic relatedness task,
which measures the degree of semantic relatedness
of a sentence pair by assigning a relatedness score
ranging from 1 (completely unrelated) to 5 (very
related); and textual entailment (TE) task, which
determines whether one of the following three re-
lationships holds between two given sentences A
and B: (1) entailment: the meaning of B can be
inferred from A; (2) contradiction: A contradicts
B; (3) neutral: the truth of B cannot be inferred on
the basis of A.
Semantic textual similarity (STS) (Lintean and
Rus, 2012) and semantic relatedness are closely
related and interchangeably used in many liter-
atures except that the concept of semantic simi-
larity is more specific than semantic relatedness
and the latter includes concepts as antonymy and
meronymy. In this paper we regard the semantic
relatedness task as a STS task. Besides, regardless
of the original intention of this task, we adopted
the mainstream machine learning methods instead
of CDSMs to solve these two tasks by extracting
heterogenous features.
271
Like semantic relatedness, TE task (surveyed
in (Androutsopoulos and Malakasiotis, 2009)) is
also closely related to STS task since in TE task
lots of similarity measures at different levels are
exploited to boost classification. For example,
(Malakasiotis and Androutsopoulos, 2007) used
ten string similarity measures such as cosine sim-
ilarity at the word and the character level. There-
fore, the first fundamental question arises, i.e.,
?Can we solve both of these two tasks together??
At the same time, since high similarity does not
mean entailment holds, the TE task also utilizes
other features besides similarity measures. For ex-
ample, in our previous work (Zhao et al., 2014)
text difference features were proposed and proved
to be effective. Therefore, the second question sur-
faces here, i.e., ?Are features proposed for TE task
still effective for STS task?? To answer the first
question, we extracted seven types of features in-
cluding text similarity and text difference and then
fed them to classifiers and regressors to solve TE
and STS task respectively. Regarding the second
question, we conducted a series of experiments
to study the performance of different features for
these two tasks.
The rest of the paper is organized as follows.
Section 2 briefly describes the related work on
STS and TE tasks. Section 3 presents our systems
including features, learning methods, etc. Section
4 shows the experimental results on training data
and Section 5 reports the results of our submitted
systems on test data and gives a detailed analysis.
Finally, Section 6 concludes this paper with future
work.
2 Related Work
Existing work on STS can be divided into 4
categories according to the similarity measures
used (Gomaa and Fahmy, 2013): (1) string-based
method (B?ar et al., 2012; Malakasiotis and An-
droutsopoulos, 2007) which calculates similarities
using surface strings at either character level or
word level; (2) corpus-based method (Li et al.,
2006) which measures word or sentence similar-
ities using the information gained from large cor-
pora, including Latent Semantic Analysis (LSA),
pointwise mutual information (PMI), etc. (3)
knowledge-based method (Mihalcea et al., 2006)
which estimates similarities with the aid of ex-
ternal resources, such as WordNet
1
; (4) hybrid
1
http://wordnet.princeton.edu/
method (Zhu and Lan, 2013; Croce et al., 2013)
which integrates multiple similarity measures and
adopts supervised machine learning algorithms to
learn the different contributions of different fea-
tures.
The approaches to the task of TE can be roughly
divided into two groups: (1) logic inference
method (Bos and Markert, 2005) where automatic
reasoning tools are used to check the logical repre-
sentations derived from sentences and (2) machine
learning method (Zhao et al., 2013; Gomaa and
Fahmy, 2013) where a supervised model is built
using a variety of similarity scores.
Unlike previous work which separately ad-
dressed these two closely related tasks by using
simple feature types, in this paper we endeavor to
simultaneously solve these two tasks by using het-
erogenous features.
3 Our Systems
We consider the two tasks as one by exploiting the
same set of features but using different learning
methods, i.e., classification and regression. Seven
types of features are extracted and most of them
are based on our previous work on TE (Zhao et
al., 2014) and STS (Zhu and Lan, 2013). Many
learning algorithms and parameters are examined
and the final submitted systems are configured ac-
cording to the preliminary results on training data.
3.1 Preprocessing
Three text preprocessing operations were per-
formed before we extracted features, which in-
cluded: (1) we converted the contractions to their
formal writings, for example, doesn?t is rewrit-
ten as does not. (2) the WordNet-based Lemma-
tizer implemented in Natural Language Toolkit
2
was used to lemmatize all words to their nearest
base forms in WordNet, for example, was is lem-
matized to be. (3) we replaced a word from one
sentence with another word from the other sen-
tence if the two words share the same meaning,
where WordNet was used to look up synonyms.
No word sense disambiguation was performed and
all synsets for a particular lemma were considered.
3.2 Feature Representations
3.2.1 Length Features (len)
Given two sentences A and B, this feature type
records the length information using the follow-
2
http://nltk.org/
272
ing eight measure functions:
|A|, |B|, |A?B|, |B ?A|, |A ?B|, |A ?B|,
(|A|?|B|)
|B|
,
(|B|?|A|)
|A|
where |A| stands for the number of non-repeated
words in sentence A , |A?B| means the number of
unmatched words found in A but not in B , |A ?B|
stands for the set size of non-repeated words found
in either A or B and |A ? B| means the set size of
shared words found in both A and B .
Moreover, in consideration of different types of
words make different contributions to text similar-
ity, we also recorded the number of words in set
A?B and B ?A whose POS tags are noun, verb,
adjective and adverb respectively. We used Stan-
ford POS Tagger
3
for POS tagging. Finally, we
collected a total of sixteen features.
3.2.2 Surface Text Similarity (st)
As shown in Table 1, we adopted six commonly
used functions to calculate the similarity between
sentence A and B based on their surface forms,
where
??
x and
??
y are vectorial representations of
sentences A and B in tf ? idf schema.
Measure Definition
Jaccard S
jacc
= |A ? B|/|A ? B|
Dice S
dice
= 2 ? |A ? B|/(|A|+ |B|)
Overlap S
over
= |A ? B|/|A| and |A ? B|/|B|
Cosine S
cos
=
??
x ?
??
y /(?
??
x ? ? ?
??
y ?)
Manhattan M(
??
x ,
??
y ) =
n
?
i=1
|x
i
? y
i
|
Euclidean E(
??
x ,
??
y ) =
?
n
?
i=1
(x
i
? y
i
)
2
Table 1: Surface text similarity measures and their
definitions used in our experiments.
We also used three statistical correlation coef-
ficients (i.e., Pearson, Spearmanr, Kendalltau) to
measure similarity by regarding the vectorial rep-
resentations as different variables. Thus we got ten
features at last.
3.2.3 Semantic Similarity (ss)
The above surface text similarity features only
consider the surface words rather than their ac-
tual meanings in sentences. In order to build the
semantic representations of sentences, we used a
latent model to capture the contextual meanings
of words. Specifically, we adopted the weighted
textual matrix factorization (WTMF) (Guo and
Diab, 2012) to model the semantics of sentences
due to its reported good ability to model short
texts. This model first factorizes the original term-
sentence matrix X into two matrices such that
3
http://nlp.stanford.edu/software/tagger.shtml
X
i,j
? P
T
?,i
.Q
?,j
, where P
?,i
is a latent seman-
tic vector profile for word w
i
and Q
?,j
is a vector
profile that represents the sentence s
j
. Then we
employed the new representations of sentences,
i.e., Q, to calculate the semantic similarity be-
tween sentences using Cosine, Manhattan, Eu-
clidean, Pearson, Spearmanr, Kendalltau measures
respectively, which results in six features.
3.2.4 Grammatical Relationship (gr)
The grammatical relationship feature measures
the semantic similarity between two sentences
at the grammar level and this feature type was
also explored in our previous work (Zhao et al.,
2013; Zhu and Lan, 2013). We used Stanford
Parser
4
to acquire the dependency information
from sentences and the grammatical information
are represented in the form of relation unit, e.g.
nsubj(example, this), where nsubj stands for a de-
pendency relationship between example and this.
We obtained a sequence of relation units for each
sentence and then used them to estimate similarity
by adopting eight measure functions described in
Section 3.2.1, resulting in eight features.
3.2.5 Text Difference Measures (td)
There are two types of text difference measures.
The first feature type is specially designed for
the contradiction entailment relationship, which
is based on the following observation: there ex-
ist antonyms between two sentences or the nega-
tion status is not consistent (i.e., one sentence has
a negation word while the other does not have) if
contradiction holds. Therefore we examined each
sentence pair and set this feature as 1 if at least one
of these conditions is met, otherwise -1. WordNet
was used to look up antonyms and a negation list
with 28 words was used.
The second feature type is extracted from two
word sets A?B and B?A as follows: we first cal-
culated the similarities between every word from
A ? B and every word from B ? A , then took the
maximum, minimum and average value of them as
features. In our experiments, four WordNet-based
similarity measures (i.e., path, lch, wup, jcn (Go-
maa and Fahmy, 2013)) were used to calculate the
similarity between two words.
Totally, we got 13 text difference features.
4
http://nlp.stanford.edu/software/lex-parser.shtml
273
3.2.6 String Features (str)
This set of features is taken from our previous
work (Zhu and Lan, 2013) due to its superior per-
formance.
Longest common sequence (LCS) We computed
the LCS similarity on the original and lemmatized
sentences. It was calculated by finding the maxi-
mum length of a common contiguous subsequence
of two strings and then dividing it by the smaller
length of two strings to eliminate the impacts of
length imbalance.
Jaccard similarity using n-grams We obtained
n-grams at three different levels, i.e., the origi-
nal word level, the lemmatized word level and the
character level. Then these n-grams were used for
calculating Jaccard similarity defined in Table 1.
In our experiments, n = {1, 2, 3} were used for
the word level and n = {2, 3, 4} were used for the
character level.
Weighted word overlap (WWO) Since not all
words are equally important, the traditional Over-
lap similarity may not be always reasonable. Thus
we used the information content of word w to es-
timate the importance of word w as follows:
ic(w) = ln
?
w
?
?C
freq(w
?
)
freq(w)
where C is the set of words in the corpus and
freq(w) is the frequency of the word w in the
corpus. To compute ic(w), we used the Web 1T
5-gram Corpus
5
. Then the WWO similarity of
two sentence s
1
and s
2
was calculated as follows:
Sim
wwo
(s
1
, s
2
) =
?
w?s
1
?s
2
ic(w)
?
w
?
?s
2
ic(w
?
)
Due to its asymmetry, we used the harmonic mean
of Sim
wwo
(s
1
, s
2
) and Sim
wwo
(s
2
, s
1
) as the fi-
nal WWO similarity. The WWO similarity is cal-
culated on the original and lemmatized strings re-
spectively.
Finally, we got two LCS features, nine Jaccard
n-gram features and two WWO features.
3.2.7 Corpus-based Features (cps)
Two types of corpus-based feature are also bor-
rowed from our previous work (Zhu and Lan,
2013), i.e., vector space sentence similarity and
co-occurrence retrieval model (CRM), which re-
sults in six features.
5
https://catalog.ldc.upenn.edu/LDC2006T13
Co-occurrence retrieval model (CRM) The
CRM word similarity is calculated as follows:
Sim
CRM
(w
1
, w
2
) =
2 ? |c(w
1
) ? c(w
2
)|
|c(w
1
)|+ |c(w
2
)|
where c(w) is the set of words that co-occur with
word w. We used the 5-gram part of the Web 1T
5-gram Corpus to obtain c(w). We only consid-
ered the word w with |c(w)| > T and then took
the top 200 co-occurring words ranked by the co-
occurrence frequency as its c(w). In our experi-
ment, we set T = {50, 200}. To propagate the
similarity from words to sentences, we adopted
the best alignment strategy used in (Banea et al.,
2012) to align two sentences.
Vector space sentence similarity This feature set
is taken from (
?
Sari?c et al., 2012), which is based
on distributional vectors of words. First we per-
formed latent semantic analysis (LSA) over two
corpora, i.e., the New York Times Annotated Cor-
pus (NYT) (Sandhaus, 2008) andWikipedia, to es-
timate the distributions of words. Then we used
two strategies to convert the distributional mean-
ings of words to sentence level: (i) simply sum-
ming up the distributional vector of each word w
in the sentence, (ii) using the information content
ic(w) to weigh the LSA vector of each wordw and
summing them up. Then we used cosine similarity
to measure the similarity of two sentences.
3.3 Learning Algorithms
We explored several classification algorithms to
classify entailment relationships and regression
algorithms to predict similarity scores using the
above 72 features after performing max-min stan-
dardization procedure by scaling them to [-1,1].
Five supervised learning methods were explored:
Support Vector Machine (SVM) which makes the
decisions according to the hyperplanes, Random
Forest (RF) which constructs a multitude of de-
cision trees at training time and selects the mode
of the classes output by individual trees, Gradient
Boosting (GB) that produces a prediction model
in the form of an ensemble of weak prediction
models, k-nearest neighbors (kNN) that decides
the class labels with the aid of the classes of k
nearest neighbors, and Stochastic Gradient De-
scent (SGD) which uses SGD technique to min-
imize loss functions. These supervised learning
methods are implemented in scikit-learn toolkit
(Pedregosa et al., 2011). Besides, we also used
a semi-supervised learning strategy for both tasks
274
in order to make full use of unlabeled test data.
Specifically, the co-training algorithm was used to
address TE task according to (Zhao et al., 2014).
Its strategy is to train two classifiers with two data
views and to add the top confident predicted in-
stances by one classifier to expand the training set
of another classifier and then to re-train the two
classifiers on the expanded training sets. For STS
task, we utilized CoReg algorithm (Zhou and Li,
2005) which uses two kNN regressors to perform
co-training paradigm.
3.4 Evaluation Measures
In order to evaluate the performance of differ-
ent algorithms, we adopted the official evaluation
measures, i.e., Pearson correlation coefficient for
STS task and accuracy for TE task.
4 Experiments on Training Data
To make a reasonable comparison between differ-
ent algorithms, we performed 5-fold cross valida-
tion on training data with 5000 sentence pairs. The
parameters tuned in different algorithms are listed
below: the trade-off parameter c in SVM, the num-
ber of trees n in RF, the number of boosting stages
n in GB, the number of nearest neighbors k in kNN
and the number of passes over the training data n
in SGD. The rest parameters are set to be default.
Algorithm
STS task TE task
Pearson para. Accuracy para.
SVM .807?.058 c=10 83.46?2.09 c=100
RF .805?.052 n=40 83.16?2.64 n=30
GB .806?.055 n=210 83.22?2.48 n=140
kNN .797?.062 k=25 82.54?2.45 k=17
SGD .765?.064 n=29 78.88?1.99 n=15
Table 2: The 5-fold cross validation results on
training data with mean and standard deviation for
each algorithm.
Table 2 reports the experimental results of 5-
fold cross validation with mean and standard devi-
ation and the optimal parameters on training data.
The results of semi-supervised learning methods
are not listed because only a few parameters are
tried due to the limit of time. From this table we
see that SVM, RF and GB perform comparable re-
sults to each other.
5 Results on Test Data
5.1 Submitted System Configurations
According to the above preliminary experimental
results, we configured five final systems for each
task. Table 3 presents the classification and regres-
sion algorithms with their parameters used in the
five systems for each task.
System STS task TE task
1 SVR, c=10 SVC, c=100
2 GB, n=210 GB, n=140
3 RF, n=40 RF, n=30
4 CoReg, k=13 co-training, k=40
5 majority voting majority voting
Table 3: Five system configurations for test data
for two tasks.
Among them, System 1 acts as our primary
and baseline system that employs SVM algorithm
and as comparison System 2 and System 3 exploit
GB and RF algorithm respectively. Unlike super-
vised settings in the aforementioned systems, Sys-
tem 4 employs a semi-supervised learning strategy
to make use of unlabeled test data. For CoReg,
the number of iteration and the number of near-
est neighbors are set as 100 and 13 respectively,
and for each iteration in co-training, the number
of confident predictions is set as 40. To further
improve performance, System 5 combines the re-
sults of 5 different algorithms (i.e. MaxEnt, SVM,
kNN, GB, RF) through majority voting. We used
the averaged values of the outputs from different
regressors as final similarity scores for semantic
similarity measurement task and chose the major
class label for entailment judgement task.
5.2 Results and Discussion
Table 4 lists the final results officially released by
the organizers in terms of Pearson and accuracy.
The best performance among these five systems is
shown in bold font. All participants can submit a
maximum of five runs for each task and only one
primary system is involved in official ranking. The
lower part of Table 4 presents the top 3 results and
the results with ? are achieved by our systems.
System STS task TE task(%)
1 0.8279 83.641
2 0.8389 84.128
3 0.8414 83.945
4 0.8210 81.165
5 0.8349 83.986
rank 1st 0.8279* 84.575
rank 2nd 0.8272 83.641*
rank 3rd 0.8268 83.053
Table 4: The results of our five systems for two
tasks and the officially top-ranked systems.
From this table, we found that (1) System 3 (us-
275
ing GB algorithm) and System 2 (using RF algo-
rithm) achieve the best performance among three
supervised systems in STS and TE task respec-
tively. However, there is no significant difference
among these systems. (2) Surprisingly, the semi-
supervised system (i.e., System 4) that employs
the co-training strategy to make use of test data
performs the worst, which is beyond our expecta-
tion. Based on our further observation in TE task,
the possible reason is that a lot of misclassified ex-
amples are added into the training pool in the ini-
tial iteration, which results in worse models built
in the subsequent iterations. And we speculate that
the weak learner kNN employed in CoReg may
lead to poor performance as well. (3) The major-
ity voting strategy fails to boost the performance
since GB and RF algorithm obtain the best perfor-
mance among these algorithms. (4) Our systems
obtain very good results on both STS and TE task,
i.e., we rank 1st out of 17 participants in STS task
and rank 2nd out of 18 participants in TE task ac-
cording to the results of primary systems and as
shown in Table 4 our primary system (i.e., System
1) do not achieve the best performance.
In a nutshell, our systems rank first and second
in STS and TE task respectively. Therefore the
answer to the first question raised in Section 1 is
yes. For two tasks, i.e., STS and TE, which are
very closely related but slightly different, we can
use the same features to solve them together.
5.3 Feature Combination Experiments
To answer the second question and explore the in-
fluences of different feature types, we performed
a series of experiments under the best system set-
ting. Table 5 shows the results of different feature
combinations where for each time we selected and
added one best feature type. From this table, we
find that for STS the most effective feature is cps
and for TE task is td. Almost all feature types have
positive effects on performance. Specifically, td
alone achieves 81.063% in TE task which is quite
close to the best performance (84.128%) and cps
alone achieves 0.7544 in STS task. Moreover, the
td feature proposed for TE task is quite effective
in STS task as well, which suggests that text se-
mantic difference measures are also crucial when
measuring sentence similarity.
Therefore the answer to the second question is
yes. It is clear that the features proposed for TE are
also effective for STS and heterogenous features
yield better performance than a single feature type.
len st ss gr td str cps result
+ 0.7544 (STS)
+ + 0.8057(+5.13)
+ + + 0.8280(+2.23)
+ + + + 0.8365(+0.85)
+ + + + + 0.8426(+0.61)
+ + + + + + 0.8432(+0.06)
+ + + + + + + 0.8429(-0.03)
+ 81.063 (TE)
+ + 82.484(+1.421)
+ + + 82.992(+0.508)
+ + + + 83.844(+0.852)
+ + + + + 83.925(+0.081)
+ + + + + + 84.067(+0.142)
+ + + + + + + 84.128(+0.061)
Table 5: Results of feature combinations, the num-
bers in the brackets are the performance incre-
ments compared with the previous results.
6 Conclusion
We set up five state-of-the-art systems and each
system employs different classifiers or regressors
using the same feature set. Our submitted systems
rank the 1st out of 17 teams in STS task with the
best performance of 0.8414 in terms of Pearson
coefficient and rank the 2nd out of 18 teams in
TE task with 84.128% in terms of accuracy. This
result indicates that (1) we can use the same fea-
ture set to solve these two tasks together, (2) the
features proposed for TE task are also effective
for STS task and (3) heterogenous features out-
perform a single feature. For future work, we may
explore the underlying relationships between these
two tasks to boost their performance by each other.
Acknowledgments
This research is supported by grants from Na-
tional Natural Science Foundation of China
(No.60903093) and Shanghai Knowledge Service
Platform Project (No. ZF1213).
References
Ion Androutsopoulos and Prodromos Malakasiotis.
2009. A survey of paraphrasing and textual entail-
ment methods. arXiv preprint arXiv:0912.3747.
Carmen Banea, Samer Hassan, Michael Mohler, and
Rada Mihalcea. 2012. Unt:a supervised synergistic
approach to semantictext similarity. In First Joint
Conference on Lexical and Computational Seman-
tics (*SEM.
276
Daniel B?ar, Chris Biemann, Iryna Gurevych, and
Torsten Zesch. 2012. Ukp: Computing seman-
tic textual similarity by combining multiple content
similarity measures. In Proceedings of the First
Joint Conference on Lexical and Computational Se-
mantics, pages 435?440. Association for Computa-
tional Linguistics.
Johan Bos and Katja Markert. 2005. Recognising tex-
tual entailment with logical inference. In Proceed-
ings of the conference on Human Language Tech-
nology and Empirical Methods in Natural Language
Processing, pages 628?635. Association for Compu-
tational Linguistics.
Danilo Croce, Valerio Storch, and Roberto Basili.
2013. Unitor-core typed: Combining text similarity
and semantic filters through sv regression. In Pro-
ceedings of the 2nd Joint Conference on Lexical and
Computational Semantics, page 59.
Wael H Gomaa and Aly A Fahmy. 2013. A survey of
text similarity approaches. International Journal of
Computer Applications, 68(13):13?18.
Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011.
Experimental support for a categorical composi-
tional distributional model of meaning. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing, pages 1394?1404. Asso-
ciation for Computational Linguistics.
Weiwei Guo and Mona Diab. 2012. Modeling sen-
tences in the latent space. In Proceedings of the
50th Annual Meeting of the Association for Compu-
tational Linguistics.
Zellig S Harris. 1954. Distributional structure. The
Philosophy of Linguistics,.
Yuhua Li, David McLean, Zuhair A Bandar, James D
O?shea, and Keeley Crockett. 2006. Sentence sim-
ilarity based on semantic nets and corpus statistics.
Knowledge and Data Engineering, IEEE Transac-
tions on, 18(8):1138?1150.
Mihai C. Lintean and Vasile Rus. 2012. Measuring se-
mantic similarity in short texts through greedy pair-
ing and word semantics. In FLAIRS Conference.
AAAI Press.
Prodromos Malakasiotis and Ion Androutsopoulos.
2007. Learning textual entailment using svms and
string similarity measures. In Proceedings of the
ACL-PASCAL Workshop on Textual Entailment and
Paraphrasing, pages 42?47. Association for Com-
putational Linguistics.
M. Marelli, L. Bentivogli, M. Baroni, R. Bernardi,
S. Menini, and R. Zamparelli. 2014. Semeval-2014
task 1: Evaluation of compositional distributional
semantic models on full sentences through seman-
tic relatedness and textual entailment. In Proceed-
ings of SemEval 2014: International Workshop on
Semantic Evaluation.
Rada Mihalcea, Courtney Corley, and Carlo Strappa-
rava. 2006. Corpus-based and knowledge-based
measures of text semantic similarity. In AAAI, vol-
ume 6, pages 775?780.
Fabian Pedregosa, Ga?el. Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
Weiss, Vincent Dubourg, Jake Vanderplas, Alexan-
dre Passos, David Cournapeau, Matthieu Brucher,
Matthieu Perrot, and
?
Edouard Duchesnay. 2011.
Scikit-learn: Machine learning in Python. Journal
of Machine Learning Research, 12:2825?2830.
Evan Sandhaus. 2008. The new york times annotated
corpus ldc2008t19. Philadelphia: Linguistic Data
Consortium.
Socher, Richard, Huval Brody, Manning Christopher,
and Ng Andrew. 2012. Semantic compositionality
through recursive matrix-vector spaces. In Proceed-
ings of EMNLP, Jeju Island, Korea.
Peter D Turney, Patrick Pantel, et al. 2010. From
frequency to meaning: Vector space models of se-
mantics. Journal of artificial intelligence research,
37(1):141?188.
Frane
?
Sari?c, Goran Glava?s, Mladen Karan, Jan
?
Snajder,
and Bojana Dalbelo Ba?si?c. 2012. Takelab: Systems
for measuring semantic text similarity. In Proceed-
ings of the First Joint Conference on Lexical and
Computational Semantics, pages 441?448, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Fabio Massimo Zanzotto, Ioannis Korkontzelos,
Francesca Fallucchi, and Suresh Manandhar. 2010.
Estimating linear models for compositional distri-
butional semantics. In Proceedings of the 23rd In-
ternational Conference on Computational Linguis-
tics, pages 1263?1271. Association for Computa-
tional Linguistics.
Jiang Zhao, Man Lan, and Zheng-Yu Niu. 2013. Ec-
nucs: Recognizing cross-lingual textual entailment
using multiple text similarity and text difference
measures. In Proceedings of the Seventh Interna-
tional Workshop on Semantic Evaluation (SemEval
2013), pages 118?123, Atlanta, Georgia, USA, June.
Association for Computational Linguistics.
Jiang Zhao, Man Lan, Zheng-Yu Niu, and Donghong
Ji. 2014. Recognizing cross-lingual textual entail-
ment with co-training using similarity and difference
views. In The 2014 International Joint Conference
on Neural Networks (IJCNN2014). IEEE.
Zhi-Hua Zhou and Ming Li. 2005. Semi-supervised
regression with co-training. In IJCAI, pages 908?
916.
Tian Tian Zhu and Man Lan. 2013. Ecnucs: Measur-
ing short text semantic equivalence using multiple
similarity measurements. In Proceedings of the 2nd
Joint Conference on Lexical and Computational Se-
mantics, page 124.
277
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 139?146,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
The Effects of Discourse Connectives Prediction on Implicit Discourse
Relation Recognition
Zhi Min Zhou?, Man Lan?,?, Zheng Yu Niu?, Yu Xu?, Jian Su?
?East China Normal University, Shanghai, PRC.
?Baidu.com Inc., Beijing, PRC.
?Institute for Infocomm Research, Singapore.
51091201052@ecnu.cn, lanman.sg@gmail.com
Abstract
Implicit discourse relation recognition is
difficult due to the absence of explicit
discourse connectives between arbitrary
spans of text. In this paper, we use lan-
guage models to predict the discourse con-
nectives between the arguments pair. We
present two methods to apply the pre-
dicted connectives to implicit discourse
relation recognition. One is to use the
sense frequency of the specific connec-
tives in a supervised framework. The
other is to directly use the presence of the
predicted connectives in an unsupervised
way. Results on PDTB2 show that using
language model to predict the connectives
can achieve comparable F-scores to the
previous state-of-art method. Our method
is quite promising in that not only it has
a very small number of features but also
once a language model based on other re-
sources is trained it can be more adaptive
to other languages and domains.
1 Introduction
Discourse relation analysis involves identifying
the discourse relations (e.g., the comparison re-
lation) between arbitrary spans of text, where
the discourse connectives (e.g., ?however?, ?be-
cause?) may or may not explicitly exist in the text.
This analysis is one important application both as
an end in itself and as an intermediate step in var-
ious downstream NLP applications, such as text
summarization, question answering etc.
As discussed in (Pitler and Nenkova., 2009b),
although explicit discourse connectives may have
two types of ambiguity, i.e., one is discourse or
non-discourse usage (?once? can be either a tem-
poral connective or a word meaning ?formerly?),
the other is discourse relation sense ambiguity
(?since? can serve as either a temporal or causal
connective), their study shows that for explicit
discourse relations in Penn Discourse Treebank
(PDTB) corpus, the most general 4 senses, i.e.,
Comparison (Comp.), Contingency (Cont.), Tem-
poral (Temp.) and Expansion (Exp.), can be eas-
ily addressed by the presence of discourse con-
nectives and a simple method only considering the
sense frequency of connectives can achieve more
than 93% accuracy. This indicates the importance
of connectives for discourse relation recognition.
However, with implicit discourse relation
recognition, there is no connective between the
textual arguments, which results in a very difficult
task. In recent years, a multitude of efforts have
been employed to solve this task. One approach
is to exploit various linguistically informed fea-
tures extracted from human-annotated corpora in
a supervised framework (Pitler et al, 2009a) and
(Lin et al, 2009). Another approach is to perform
recognition without human-annotated corpora by
creating synthetic examples of implicit relations in
an unsupervised way (Marcu and Echihabi, 2002).
Moreover, our initial study on PDTB implicit
relation data shows that the averaged F-score for
the most general 4 senses can reach 91.8% when
we obtain the sense of test examples by map-
ping each implicit connective to its most frequent
sense (i.e., sense recognition using gold-truth im-
plicit connectives). This high F-score performance
again proves that the connectives are very crucial
source for implicit relation recognition.
In this paper, we present a new method to ad-
dress the problem of recognizing implicit dis-
course relation. This method is inspired by the
above observations, especially the two gold-truth
results, which reveals that discourse connectives
are very important signals for discourse relation
recognition. Our basic idea is to recover the im-
plicit connectives (not present in real text) be-
tween two spans of text with the use of a language
139
model trained on large amount of raw data without
any human-annotation. Then we use these pre-
dicted connectives to generate feature vectors in
two ways for implicit discourse relation recogni-
tion. One is to use the sense frequency of the spe-
cific connectives in a supervised framework. The
other is to directly use the presence of the pre-
dicted connectives in an unsupervised way.
We performed evaluation on explicit and im-
plicit relation data sets in the PDTB 2 corpus. Ex-
perimental results showed that the two methods
achieved comparable F-scores to the state-of-art
methods. It indicates that the method using lan-
guage model to predict connectives is very useful
in solving this task.
The rest of this paper is organized as follows.
Section 2 reviews related work. Section 3 de-
scribes our methods for implicit discourse relation
recognition. Section 4 presents experiments and
results. Section 5 offers some conclusions.
2 Related Work
Existing works on automatic recognition of im-
plicit discourse relations fall into two categories
according to whether the method is supervised or
unsupervised.
Some works perform relation recognition with
supervised methods on human-annotated corpora,
for example, the RST Bank (Carlson et al, 2001)
used by (Soricut and Marcu, 2003), adhoc anno-
tations used by (Girju, 2003) and (Baldridge and
Lascarides, 2005), and the GraphBank (Wolf et al,
2005) used by (Wellner et al, 2006).
Recently the release of the Penn Discourse
TreeBank (PDTB) (Prasad et al, 2006) has sig-
nificantly expanded the discourse-annotated cor-
pora available to researchers, using a comprehen-
sive scheme for both implicit and explicit rela-
tions. (Pitler et al, 2009a) performed implicit re-
lation classification on the second version of the
PDTB. They used several linguistically informed
features, such as word polarity, verb classes, and
word pairs, showing performance increases over a
random classification baseline. (Lin et al, 2009)
presented an implicit discourse relation classifier
in PDTB with the use of contextual relations, con-
stituent Parse Features, dependency parse features
and cross-argument word pairs. Although both of
two methods achieved the state of the art perfor-
mance for automatical recognition of implicit dis-
course relations, due to lack of human-annotated
corpora, their approaches are not very useful in the
real word.
Another line of research is to use the unsuper-
vised methods on unhuman-annotated corpus.
(Marcu and Echihabi, 2002) used several pat-
terns to extract instances of discourse relations
such as contrast and elaboration from unlabeled
corpora. Then they used word-pairs between argu-
ments as features for building classification mod-
els and tested their model on artificial data for im-
plicit relations.
Subsequently other studies attempt to ex-
tend the work of (Marcu and Echihabi, 2002).
(Sporleder and Lascarides, 2008) discovered that
Marcu and Echihabi?s models do not perform as
well on implicit relations as one might expect
from the test accuracy on synthetic data. (Gold-
ensohn, 2007) extended the work of (Marcu and
Echihabi, 2002) by refining the training and clas-
sification process using parameter optimization,
topic segmentation and syntactic parsing. (Saito
et al, 2006) followed the method of (Marcu and
Echihabi, 2002) and conducted experiments with
a combination of cross-argument word pairs and
phrasal patterns as features to recognize implicit
relations between adjacent sentences in a Japanese
corpus.
Previous work showed that with the use of some
patterns, structures, or the pairs of words, rela-
tion classification can be performed using unsu-
pervised methods.
In contrast to existing work, we investigated a
new knowledge source, i.e., implicit connectives
predicted using a language model, for implicit re-
lation recognition. Moreover, this method can
be applied in both supervised and unsupervised
ways by generating features on labeled and unla-
beled training data and then performing implicit
discourse connectives recognition.
3 Methodology
3.1 Predicting implicit connectives via a
language model
Previous work (Pitler and Nenkova., 2009b)
showed that with the presence of discourse con-
nectives, explicit discourse relations in PDTB can
be easily identified with more than 90% F-score.
Our initial study on PDTB human-annotated im-
plicit relation data shows that the averaged F-score
for the most general 4 senses can reach 91.8%
when we simply map each implicit connective to
140
its most frequent sense. These high F-scores indi-
cate that the connectives are very crucial source of
information for both explicit and implicit relation
recognition. However, for implicit relations, there
are no explicitly discourse connectives in real text.
This built-in absence makes the implicit relation
recognition task quite difficult. In this work we
overcome this difficulty by inserting connectives
into the two arguments with the use of a language
model.
Following the annotation scheme of PDTB, we
assume that each implicit connective takes two
arguments, denoted as Arg1 and Arg2. Typi-
cally, there are two possible positions for most
of implicit connectives, i.e., the position before
Arg1 and the position between Arg1 and Arg2.
Given a set of implicit connectives {ci}, we gen-
erate two synthetic sentences, ci+Arg1+Arg2 and
Arg1+ci+Arg2 for each ci, denoted as Sci,1 and
Sci,2. Then we calculate the perplexity (an intrin-
sic score) of these sentences with the use of a lan-
guage model, denoted as Ppl(Sci,j). According to
the value of Ppl(Sci,j) (the lower the better), we
can rank these sentences and select the connec-
tives in top N sentences as implicit connectives
for this argument pair. Here the language model
may be trained on any large amount of unanno-
tated corpora that can be cheaply acquired. Typi-
cally, a large corpora with the same domain as the
test data will be used for training language model.
Therefore, we chose news corpora, such as North
American News Corpora.
After that, we use the top N predicted connec-
tives to generate different feature vectors and per-
form the classification in two ways. One is to use
the sense frequency of predicted connectives in a
supervised framework. The other is to directly use
the presence of the predicted connectives in an un-
supervised way. The two approaches are described
as follows.
3.2 Using sense frequency of predicted
discourse connectives as features
After the above procedure, we get a sorted set of
predicted discourse connectives. Due to the pres-
ence of an implicit connective, the implicit dis-
course relation recognition task can be addressed
with the methods for explicit relation recognition,
e.g., sense classification based only on connectives
(Pitler et al, 2009a). Inspired by their work, the
first approach is to use sense frequency of pre-
dicted discourse connectives as features. We take
the connective with the lowest perplexity value
(i.e., top 1 connective) as the real connective for
the arguments pair. Then we count the sense
frequency of this connective on the training set.
Figure 1 illustrates the procedure of generating
predicted discourse connective from a language
model and calculating its sense frequency from
training data. Here the calculation of sense fre-
quency of connective is based on the annotated
training data which has labeled discourse rela-
tions, thus this method is a supervised one.
Figure 1: Procedure of generating a predicted dis-
course connective and its sense frequency from the
training set and a language model.
Then we can directly use the sense frequency
to generate a 4-feature vector to perform the clas-
sification. For example, the sense frequency of
the connective but in the most general 4 senses
can be counted from training set as 691, 6, 49,
2, respectively. For a given pair of arguments,
if but is predicted as the top 1 connective based
on a language model, a 4-dimension feature vec-
tor (691, 6, 49, 2) is generated for this pair and
used for training and test procedure. Figure 2
and 3 show the training and test procedure for this
method.
Figure 2: Training procedure for the first ap-
proach.
141
Figure 3: Test procedure for the first approach.
3.3 Using presence or absence of predicted
discourse connective as features
(Pitler et al, 2008) showed that most connectives
are unambiguous and it is possible to obtain high-
accuracy in prediction of discourse senses due to
the simple mapping relation between connectives
and senses. Given two examples:
(E1) She paid less on her dress, but it is very nice.
(E2) We have to harry up because the raining is
getting heavier and heavier.
The two connectives, i.e., but in E1 and because
in E2, convey the Comparison and Contingency
senses respectively. In most cases, we can easily
recognize the relation sense by the appearance of
a discourse connective since it can be interpreted
in only one way. That means the ambiguity of
the mapping between sense and connective is quite
low. Therefore, the second approach is to use only
the presence of the top N predicted discourse con-
nectives to generate a feature vector for a given
pair of arguments.
4 Experiment
4.1 Data sets
We used PDTB as our data set to perform the eval-
uation of our methods. The corpus contains anno-
tations of explicit and implicit discourse relations.
The first evaluation is performed on the annotated
implicit data set. Following the work of (Pitler et
al., 2009a), we used sections 2-20 as the training
set, sections 21-22 as the test set and sections 0-
1 as the development set for parameter optimiza-
tion (e.g., N value). The second evaluation is per-
formed on the annotated explicit data set. We fol-
low the method used in (Sporleder and Lascarides,
2008) to remove the discourse connective from the
explicit instances and consider these processed in-
stances as implicit ones.
We constructed four binary classifiers to recog-
nize each main senses (i.e., Cont., Cont., Exp.,
Temp.) from the rest. For each sense we used
equal numbers of positive and negative instances
in training set. The negative instances were cho-
sen at random from the rest of training set. For
both evaluations all instances in sections 21-22
were used as test set. Table 1 lists the numbers
of positive and negative instances for each sense
in training, development and test sets of implicit
and explicit relation data sets.
4.2 Evaluation and classifier
To evaluate the performance of above systems, we
used two widely-used measures, F-score ( i.e., F1)
and accuracy. In addition, in this work we used
the LIBSVM toolkit to construct four linear SVM
classifiers for each sense.
4.3 Preprocessing
We used the SRILM toolkit to build a language
model and calculated the perplexity value for each
training and test sample. The steps are described
as follows. First, since perplexity is an intrin-
sic score to measure the similarity between train-
ing and test samples, in order to fit the restric-
tion of perplexity we chose 3 widely-used cor-
pora in the Newswire domain to train the language
model, i.e., (1) the New York part of BLLIP North
American News Text (Complete), (2) the Xin and
(3) the Ltw parts of the English Gigaword Fourth
Edition. For the BLLIP corpus with 1,796,386
automatically parsed English sentences, we con-
verted the parsed sentences into original textual
data. Some punctuation marks such as commas,
periods, minuses, right/left parentheses are con-
verted into their original form. For the Xin and
Ltw parts, we only used the Sentence Detector
toolkit in OpenNLP to split each sentence. Finally
we constructed 3-, 4- and 5-grams language mod-
els from these three corpora. Table 2 lists statis-
tics of different n-grams in the different language
models and different corpora.
Next, for each instance we combined its Arg1
and Arg2 with connectives obtained from PDTB.
There are two types of connectives, single con-
nectives (e.g. ?because? and ?but?) and paral-
142
Table 1: Statistics of positive and negative instances for each sense in training, development and test sets
of implicit and explicit relation data sets.
Implicit Explicit
Comp. Cont. Exp. Temp. Comp. Cont. Exp. Temp.
Train(Pos/Neg) 1927/1927 3375/3375 6052/6052 730/730 4080/4080 2732/2732 4609/4609 2663/2663
Dev(Pos/Neg) 191/997 292/896 651/537 54/1134 438/1071 295/1214 514/995 262/1247
Test(Pos/Neg) 146/912 276/782 556/502 67/991 388/1025 235/1178 501/912 289/1124
Table 2: Statistics of different n-grams in the dif-
ferent language models and different corpora.
n-gram BLLIP - Gigaword- Gigaword-
New York Xin Ltw
1-gram 1638156 2068538 2276491
2-grams 26156851 23961796 33504873
3-grams 80876435 77799100 101855639
4-grams 127142452 134410879 159791916
5-grams 146454530 168166195 183794771
lel connectives (such as ?not only . . . , but also?).
Since discourse connectives may appear not only
ahead of the Arg1, but also between Arg1 and
Arg2, we considered this case. Given a set of
possible implicit connectives {ci}, for a single
connective ci, we constructed two synthetic sen-
tences, ci+Arg1+Arg2 and Arg1+ci+Arg2. In case
of parallel connectives, we constructed one syn-
thetic sentence like ci,1+Arg1+ci,2+Arg2.
As a result, we obtain 198 synthetic sentences
(|ci| ? 2 for single connective or |ci| for parallel
connective) for each pair of arguments. Then we
converted all words to lower cases and used the
language model trained in the above step to calcu-
late its perplexity (the lower the better) value on
sentence level. The sentences were ranked from
low to high according to their perplexity scores.
For example, given a sentence with arguments pair
as follows:
Arg1: it increased its loan-loss reserves by $93
million after reviewing its loan portfolio,
Arg2: before the loan-loss addition it had operat-
ing profit of $10 million for the quarter.
we got the perplexity (Ppl) values for this argu-
ments pair in combination with two connectives
(but and by comparison) in two positions as fol-
lows:
1. but + Arg1 + Arg2: Ppl= 349.622
2. Arg1 + but + Arg2: Ppl= 399.339
3. by comparison + Arg1 + Arg2: Ppl= 472.206
4. Arg1 + by comparison + Arg2: Ppl= 543.051
In our second approach described in Section
3.3, we considered the combination of connectives
and their position as final features like mid but,
first but, where the features are binary, that is,
the presence or absence of the specific connective.
According to the value of Ppl(Sci,j), we tried var-
ious N values on development set to get the opti-
mal N value.
4.4 Results
Table 3 summarizes the best performance
achieved using gold-truth implicit connectives,
the previous state-of-art performance achieved
by (Pitler et al, 2009a) and our approaches.
The first line shows the result by mapping the
gold-truth implicit connectives directly to the
relation?s sense. The second line presents the best
result of (Pitler et al, 2009a). One thing worth
mentioning here is that for the Expansion relation,
(Pitler et al, 2009a) expanded both training and
test sets by including EntRel relation as positive
examples, which makes it impossible to perform
direct comparison. The third and fourth lines
show the best results using our first approach,
where the sense frequency is counted on explicit
and implicit training set respectively. The last line
shows the best result of our second approach only
considering the presence of top N connectives.
Table 4 summarizes the best performance using
gold-truth explicit connectives reported in (Pitler
and Nenkova., 2009b) and our two approaches.
Figure 4 shows the curves of averaged F-scores
on implicit connective classification with differ-
ent n-gram language models. From this figure we
can see that all 4-grams language models achieved
around 0.5% better averaged F-score than 3-grams
models. And except for Ltw corpus, other 5-grams
models achieved lower averaged F-score than 4-
grams models. Specially the 5-grams result of
New York corpus is much lower than its 3-grams
result.
Figure 5 shows the averaged F-scores of dif-
ferent top N on the New York corpus with 3-,
4- and 5-grams language models. The essential
143
Table 3: Best result of implicit relations compared with state-of-art methods.
System Comp. vs. Not Cont. vs. Other Exp. vs. Other Temp. vs. Other Averaged
F1 (Acc) F1 (Acc) F1 (Acc) F1 (Acc) F1 (Acc)
Sense recognition using
gold-truth implicit connectives 94.08(98.30) 98.19(99.05) 97.79(97.64) 77.04(97.07) 91.78(98.02)
Best result in (Pitler et al, 2009a) 21.96(56.59) 47.13(67.30) 76.42(63.62) 16.76(63.49) 40.57(62.75)
Use sense frequency in explicit training set 26.02(52.17) 35.72(51.70) 64.94(53.97) 13.76(41.97) 35.10(49.95)
Use sense frequency in implicit training set 24.55(63.99) 16.26(70.79) 60.70(53.50) 14.75(70.51) 29.07(64.70)
Use presence of top N connectives only 21.91(52.84) 39.53(50.85) 68.84(52.93) 11.91(6.33) 35.55(40.74)
Table 4: Best result of explicit relation conversion to implicit relation compared with results using the
same method.
System Comp. vs. Not Cont. vs. Other Exp. vs. Other Temp. vs. Other Average
F1 (Acc) F1 (Acc) F1 (Acc) F1 (Acc) F1 (Acc)
Sense recognition using gold-truth
explicit connectives in (Pitler et al, 2009a) N/A N/A N/A N/A N/A(93.67)
Use sense frequency in explicit training set 41.62(50.96) 27.46(59.24) 48.44(50.88) 35.14(54.28) 38.17(53.84)
Use presence of top N connectives only 42.92(55.77) 31.83(56.05) 47.26(55.77) 37.89(58.24) 39.98(56.46)
0 10 20 30 40 50 60 70 80 90 100110120130140150160170180190200
30.0
30.5
31.0
31.5
32.0
32.5
33.0
33.5
34.0
34.5
 
 NY 3-gram
 NY 4-gram
 NY 5-gram
Top N value
A
v
e
r
a
g
e
d
 
F
-
S
c
o
r
e
Figure 5: Curves of averages F-score on New York 3-, 4- and 5-grams language models with different
top N values.
trend of these curves cannot be summarized in
one sentence. But we can see that the best aver-
aged F-scores mostly appeared in the range from
100 ? 160. For 4-grams and 5-grams models, the
system achieved the top averaged F-scores when
N = 20 as well.
4.5 Discussion
Experimental results on PDTB showed that using
predicted connectives achieved the comparable F-
scores of the state-of-art method.
From Table 3 we can find that our results are
closely to the best performance of previous state-
of-art methods in terms of averaged F-score. On
the Comparison sense, our first approach has an
improvement of more than 4% F-score on the pre-
vious state-of-art method (Pitler et al, 2009a). As
we mentioned before, for the Expansion sense,
they included EntRel relation to expand the train-
ing set and test set, which makes it impossible to
perform a direct comparison. Since the positive in-
stances size has been increased by 50%, they may
achieve a higher F-score than our approach. For
other relations, our best performance is slightly
lower than theirs. While bearing in mind that our
approach only uses a very small amount of fea-
tures for implicit relation recognition. Compared
144
3-gram 4-gram 5-gram
31.0
31.2
31.4
31.6
31.8
32.0
32.2
32.4
32.6
 
 New York
 Xin
 Ltw
n-gram
A
v
e
r
a
g
e
d
 
F
-
s
c
o
r
e
Figure 4: Curves of averaged F-score on implicit
connective classification with n-Gram language
model.
with other approaches involving thousands of fea-
tures, our method is quite promising.
From Table 4 we observe comparable averaged
F-score (39.98% F-score) on explicit relation data
set to that on implicit relation data set. Previ-
ously, (Sporleder and Lascarides, 2008) also used
the same conversion method to perform implicit
relation recognition on different corpora and their
best result is around 33.69% F-score. Although
the two results cannot be compared directly due to
different data sets, the magnitude of performance
quantities is comparable and reliable.
By comparing with the above different systems,
we find several useful observations. First, our
method using predicted implicit connectives via a
language model can help the task of implicit dis-
course relation recognition. The results are com-
parable to the previous state-of-art studies. Sec-
ond, our method has a lot of advantages, i.e., a
very small amount of features (several or no more
than 200 vs. ten thousand), easy computation
(only based on the trained language model vs. us-
ing a lot of NLP tools to extract a large amount of
linguistically informed features) and fast running,
which makes it more practical in real world appli-
cation. Furthermore, since the language model can
be trained on many corpora whether annotated or
unannotated, this method is more adaptive to other
languages and domains.
5 Conclusions
In this paper we have presented an approach to
implicit discourse relation recognition using pre-
dicted implicit connectives via a language model.
The predicted connectives have been used for im-
plicit relation recognition in two ways, i.e., super-
vised and unsupervised framework. Results on the
Penn Discourse Treebank 2.0 show that the pre-
dicted discourse connectives can help implicit re-
lation recognition and the two algorithms achieve
comparable F-scores with the state-of-art method.
In addition, this method is quite promising due to
its simple, easy to retrieve, fast run and increased
adaptivity to other languages and domains.
Acknowledgments
We thank the reviewers for their helpful com-
ments and Jonathan Ginzburg for his mentor-
ing. This work is supported by grants from
National Natural Science Foundation of China
(No.60903093), Shanghai Pujiang Talent Program
(No.09PJ1404500) and Doctoral Fund of Ministry
of Education of China (No.20090076120029).
References
J. Baldridge and A. Lascarides. 2005. Probabilistic
head-driven parsing for discourse structure. Pro-
ceedings of the Ninth Conference on Computational
Natural Language Learning.
L. Carlson, D. Marcu, and Ma. E. Okurowski. 2001.
Building a discourse-tagged corpus in the frame-
work of rhetorical structure theory. Proceedings of
the Second SIG dial Workshop on Discourse and Di-
alogue.
B. Dorr. LCS Verb Database. Technical Report Online
Software Database, University of Maryland, College
Park, MD,2001.
R. Girju. 2003. Automatic detection of causal relations
for question answering. In ACL 2003 Workshops.
S. Blair-Goldensohn. 2007. Long-Answer Ques-
tion Answering and Rhetorical-Semantic Relations.
Ph.D. thesis, Columbia Unviersity.
M. Lapata and A. Lascarides. 2004. Inferring
Sentence-internal Temporal Relations. Proceedings
of the North American Chapter of the Assocation of
Computational Linguistics.
Z.H. Lin, M.Y. Kan and H.T. Ng. 2009. Recognizing
Implicit Discourse Relations in the Penn Discourse
Treebank. Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing.
D. Marcu and A. Echihabi. 2002. An Unsupervised
Approach to Recognizing Discourse Relations. Pro-
ceedings of the 40th Annual Meeting of the Associ-
ation for Computational Linguistics.
145
E. Pitler, M. Raghupathy, H. Mehta, A. Nenkova, A.
Lee, A. Joshi. 2008. Easily Identifiable Dis-
course Relations. Coling 2008: Companion vol-
ume: Posters.
E. Pitler, A. Louis, A. Nenkova. 2009. Automatic
sense prediction for implicit discourse relations in
text. Proceedings of the 47th Annual Meeting of the
Association for Computational Linguistics.
E. Pitler and A. Nenkova. 2009. Using Syntax to Dis-
ambiguate Explicit Discourse Connectives in Text.
Proceedings of the ACL-IJCNLP 2009 Conference
Short Papers.
M. Porter. An algorithm for suffix stripping. In Pro-
gram, vol. 14, no. 3, pp.130-137, 1980.
R. Prasad, N. Dinesh, A. Lee, A. Joshi, B. Webber.
2006. Annotating attribution in the Penn Discourse
TreeBank. Proceedings of the COLING/ACL Work-
shop on Sentiment and Subjectivity in Text.
R. Prasad, N. Dinesh, A. Lee, E. Miltsakaki, L.
Robaldo, A. Joshi, B. Webber. 2008. The Penn Dis-
course TreeBank 2.0. Proceedings of LREC?08.
M. Saito, K.Yamamoto, S.Sekine. 2006. Using
Phrasal Patterns to Identify Discourse Relations.
Proceeding of the HLTCNA Chapter of the ACL.
R. Soricut and D. Marcu. 2003. Sentence Level Dis-
course Parsing using Syntactic and Lexical Informa-
tion. Proceedings of the Human Language Technol-
ogy and North American Association for Computa-
tional Linguistics Conference.
C. Sporleder and A. Lascarides. 2008. Using automat-
ically labelled examples to classify rhetorical rela-
tions: an assessment. Natural Language Engineer-
ing, Volume 14, Issue 03.
B. Wellner , J. Pustejovsky, C. H. R. S., A. Rumshisky.
2006. Classification of discourse coherence rela-
tions: An exploratory study using multiple knowl-
edge sources. Proceedings of the 7th SIGDIAL
Workshop on Discourse and Dialogue.
F. Wolf, E. Gibson, A. Fisher, M. Knight. 2005.
The Discourse GraphBank: A database of texts an-
notated with coherence relations. Linguistic Data
Consortium.
146
