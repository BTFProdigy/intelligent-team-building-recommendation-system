Proceedings of the 12th Conference of the European Chapter of the ACL, pages 487?495,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Improvements in Analogical Learning:
Application to Translating multi-Terms of the Medical Domain
Philippe Langlais
DIRO
Univ. of Montreal, Canada
felipe@iro.umontreal.ca
Franc?ois Yvon and Pierre Zweigenbaum
LIMSI-CNRS
Univ. Paris-Sud XI, France
{yvon,pz}@limsi.fr
Abstract
Handling terminology is an important
matter in a translation workflow. However,
current Machine Translation (MT) sys-
tems do not yet propose anything proactive
upon tools which assist in managing termi-
nological databases. In this work, we in-
vestigate several enhancements to analog-
ical learning and test our implementation
on translating medical terms. We show
that the analogical engine works equally
well when translating from and into a mor-
phologically rich language, or when deal-
ing with language pairs written in differ-
ent scripts. Combining it with a phrase-
based statistical engine leads to significant
improvements.
1 Introduction
If machine translation is to meet commercial
needs, it must offer a sensible approach to trans-
lating terms. Currently, MT systems offer at best
database management tools which allow a human
(typically a translator, a terminologist or even the
vendor of the system) to specify bilingual ter-
minological entries. More advanced tools are
meant to identify inconsistencies in terminological
translations and might prove useful in controlled-
language situations (Itagaki et al, 2007).
One approach to translate terms consists in us-
ing a domain-specific parallel corpus with stan-
dard alignment techniques (Brown et al, 1993) to
mine new translations. Massive amounts of par-
allel data are certainly available in several pairs
of languages for domains such as parliament de-
bates or the like. However, having at our disposal
a domain-specific (e.g. computer science) bitext
with an adequate coverage is another issue. One
might argue that domain-specific comparable (or
perhaps unrelated) corpora are easier to acquire,
in which case context-vector techniques (Rapp,
1995; Fung and McKeown, 1997) can be used
to identify the translation of terms. We certainly
agree with that point of view to a certain extent,
but as discussed by Morin et al (2007), for many
specific domains and pairs of languages, such re-
sources simply do not exist. Furthermore, the task
of translation identification is more difficult and
error-prone.
Analogical learning has recently regained some
interest in the NLP community. Lepage and De-
noual (2005) proposed a machine translation sys-
tem entirely based on the concept of formal anal-
ogy, that is, analogy on forms. Stroppa and
Yvon (2005) applied analogical learning to sev-
eral morphological tasks also involving analogies
on words. Langlais and Patry (2007) applied it to
the task of translating unknown words in several
European languages, an idea investigated as well
by Denoual (2007) for a Japanese to English trans-
lation task.
In this study, we improve the state-of-the-art of
analogical learning by (i) proposing a simple yet
effective implementation of an analogical solver;
(ii) proposing an efficient solution to the search is-
sue embedded in analogical learning, (iii) investi-
gating whether a classifier can be trained to recog-
nize bad candidates produced by analogical learn-
ing. We evaluate our analogical engine on the task
of translating terms of the medical domain; a do-
main well-known for its tendency to create new
words, many of which being complex lexical con-
structions. Our experiments involve five language
pairs, including languages with very different mor-
phological systems.
487
In the remainder of this paper, we first present
in Section 2 the principle of analogical learn-
ing. Practical issues in analogical learning are
discussed in Section 3 along with our solutions.
In Section 4, we report on experiments we con-
ducted with our analogical device. We conclude
this study and discuss future work in Section 5.
2 Analogical Learning
2.1 Definitions
A proportional analogy, or analogy for short, is a
relation between four items noted [x : y = z : t ]
which reads as ?x is to y as z is to t?. Among pro-
portional analogies, we distinguish formal analo-
gies, that is, those we can identify at a graphemic
level, such as [adrenergic beta-agonists, adren-
ergic beta-antagonists, adrenergic alpha-agonists,
adrenergic alpha-antagonists].
Formal analogies can be defined in terms of
factorizations1. Let x be a string over an alpha-
bet ?, a factorization of x, noted fx, is a se-
quence of n factors fx = (f1x, . . . , f
n
x ), such that
x = f1x  f
2
x  . . .  f
n
x , where  denotes the
concatenation operator. After (Stroppa and Yvon,
2005) we thus define a formal analogy as:
Definition 1 ?(x, y, z, t) ? ??
4
, [x : y = z : t] iff
there exist factorizations (fx, fy, fz, ft) ? (?
?d)4
of (x, y, z, t) such that, ?i ? [1, d], (f iy, f
i
z) ?{
(f ix, f
i
t ), (f
i
t , f
i
x)
}
. The smallest d for which this
definition holds is called the degree of the analogy.
Intuitively, this definition states that (x, y, z, t)
are made up of a common set of alternating sub-
strings. It is routine to check that it captures the
exemplar analogy introduced above, based on the
following set of factorizations:
fx ? (adrenergic bet, a-agonists)
fy ? (adrenergic bet, a-antagonists)
fz ? (adrenergic alph, a-agonists)
ft ? (adrenergic alph, a-antagonists)
As no smaller factorization can be found, the de-
gree of this analogy is 2. In the sequel, we call
an analogical equation an analogy where one item
(usually the fourth) is missing and we note it [x :
y = z : ? ].
1Factorizations of strings correspond to segmentations.
We keep the former term, to emphasize the genericity of the
definition, which remains valid for other algebraic structures,
for which factorization and segmentation are no longer syno-
mymous.
2.2 Analogical Inference
Let L = {(i, o) | i ? I, o ? O} be a learning set
of observations, where I (O) is the set of possible
forms of the input (output) linguistic system under
study. We denote I(u) (O(u)) the projection of u
into the input (output) space; that is, if u = (i, o),
then I(u) ? i and O(u) ? o. For an incomplete
observation u = (i, ?), the inference procedure is:
1. building EI(u) = {(x, y, z) ? L3 | [I(x) :
I(y) = I(z) : I(u) ]}, the set of input triplets
that define an analogy with I(u) .
2. building EO(u) = {o ? O | ?(x, y, z) ?
EI(u) s.t. [O(x) : O(y) = O(z) : o]} the set
of solutions to the equations obtained by pro-
jecting the triplets of EI(u) into the output
space.
3. selecting candidates among EO(u).
In the sequel, we distinguish the generator
which implements the first two steps, from the se-
lector which implements step 3.
To give an example, assume L contains
the following entries: (beeta-agonistit, adren-
ergic beta-agonists), (beetasalpaajat, adrenergic
beta-antagonists) and (alfa-agonistit, adrener-
gic alpha-agonists). We might translate the
Finnish term alfasalpaajat into the English term
adrenergic alpha-antagonists by 1) identifying
the input triplet: (beeta-agonistit, beetasalpaa-
jat, alfa-agonistit) ; 2) projecting it into the equa-
tion [adrenergic beta-agonists : adrenergic beta-
antagonists = adrenergic alpha-agonists : ? ]; and
solving it: adrenergic alpha-antagonists is one of
its solutions.
During inference, analogies are recognized in-
dependently in the input and the output space, and
nothing pre-establishes which subpart of one in-
put form corresponds to which subpart of the out-
put one. This ?knowledge? is passively captured
thanks to the inductive bias of the learning strat-
egy (an analogy in the input space corresponds to
one in the output space). Also worth mentioning,
this procedure does not rely on any pre-defined no-
tion of word. This might come at an advantage for
languages that are hard to segment (Lepage and
Lardilleux, 2007).
3 Practical issues
Each step of analogical learning, that is, search-
ing for input triplets, solving output equations and
488
selecting good candidates involves some practical
issues. Since searching for input triplets might in-
volve the need for solving (input) equations, we
discuss the solver first.
3.1 The solver
Lepage (1998) proposed an algorithm for solving
an analogical equation [x : y = z : ? ]. An
alignment between x and y and between x and z
is first computed (by edit-distance) as illustrated
in Figure 1. Then, the three strings are synchro-
nized using x as a backbone of the synchroniza-
tion. The algorithm can be seen as a deterministic
finite-state machine where a state is defined by the
two edit-operations being visited in the two tables.
This is schematized by the two cursors in the fig-
ure. Two actions are allowed: copy one symbol
from y or z into the solution and move one or both
cursors.
x: r e a d e r x: r e a d e r
y: r e a d a b l e z: d o e r
4 4
Figure 1: Illustration of the synchronization done
by the solver described in (Lepage, 1998).
There are two things to realize with this algo-
rithm. First, since several (minimal-cost) align-
ments can be found between two strings, several
synchronizations are typically carried out while
solving an equation, leading to (possibly many)
different solutions. Indeed, in adverse situations,
an exponential number of synchronizations will
have to be computed. Second, the algorithm fails
to deliver an expected form in a rather frequent
situation where two identical symbols align fortu-
itously in two strings. This is for instance the case
in our running example where the symbol d in
doer aligns to the one in reader, which puzzles the
synchronization. Indeed, dabloe is the only form
proposed to [reader : readable = doer : ? ], while
the expected one is doable. The algorithm would
have no problem, however, to produce the form
writable out of the equation [reader : readable =
writer : ? ].
Yvon et al (2004) proposed an analogical
solver which is not exposed to the latter prob-
lem. It consists in building a finite state transducer
which generates the solutions to [x : y = z : ? ]
while recognizing the form x.
Theorem 1 t is a solution to [x : y = z : ?] iff
t belongs to {y ? z}\x.
shuffle and complement are two rational op-
erations. The shuffle of two strings w and
v, noted w ? v, is the regular language con-
taining the strings obtained by selecting (with-
out replacement) alternatively in w and v, se-
quences of characters in a left-to-right man-
ner. For instance, spondyondontilalgiatis and
ondspondonylaltitisgia are two strings belong-
ing to spondylalgia ? ondontitis). The comple-
mentary set of w with respect to v, noted w\v, is
the set of strings formed by removing from w, in
a left-to-right manner, the symbols in v. For in-
stance, spondylitis and spydoniltis are belong-
ing to spondyondontilalgiatis \ ondontalgia.
Our implementation of the two rational operations
are sketched in Algorithm 1.
Because the shuffle of two strings may con-
tain an exponential number of elements with re-
spect to the length of those strings, building such
an automaton may face combinatorial problems.
Our solution simply consists in randomly sam-
pling strings in the shuffle set. Our solver, depicted
in Algorithm 2, is thus controlled by a sampling
size s, the impact of which is illustrated in Ta-
ble 1. By increasing s, the solver generates more
(mostly spurious) solutions, but also increases the
relative frequency with which the expected output
is generated. In practice, provided a large enough
sampling size,2 the expected form very often ap-
pears among the most frequent ones.
s nb (solution,frequency)
10 11 (doable,7) (dabloe,3) (adbloe,3)
102 22 (doable,28) (dabloe,21) (abldoe,21)
103 29 (doable,333) (dabloe,196) (abldoe,164)
Table 1: The 3-most frequent solutions generated
by our solver, for different sampling sizes s, for
the equation [reader : readable = doer : ? ]. nb
indicates the number of (different) solutions gen-
erated. According to our definition, there are 32
distinct solutions to this equation. Note that our
solver has no problem producing doable.
3.2 Searching for input triplets
A brute-force approach to identifying the input
triplets that define an analogy with the incom-
plete observation u = (t, ?) consists in enumerat-
ing triplets in the input space and checking for an
2We used s = 2 000 in this study.
489
function shuffle(y,z)
Input: ?y, z? two forms
Output: a random word in y ? z
if y =  then
return z
else
n? rand(1,|y|)
return y[1:n] . shuffle(z,y[n+1:])
function complementary(m,x,r,s)
Input: m ? y ? z, x
Output: the set m \ x
if (m = ) then
if (x = ) then
s? s ? r
else
complementary(m[2:],x,r.m[1],s)
if m[1] = x[1] then
complementary(m[2:],x[2:],r,s)
Algorithm 1: Simulation of the two rational op-
erations required by the solver. x[a:b] denotes the
sequence of symbols x starting from index a to
index b inclusive. x[a:] denotes the suffix of x
starting at index a.
analogical relation with t. This amounts to check
o(|I|3) analogies, which is manageable for toy
problems only. Instead, Langlais and Patry (2007)
proposed to solve analogical equations [y : x = t :
? ] for some pairs ?x, y? belonging to the neighbor-
hood3 of I(u), denotedN (t). Those solutions that
belong to the input space are the z-forms retained;
EI(u) = { ?x, y, z? : x ? N (t) , y ? N (x),
z ? [y : x = t : ? ] ? I }
This strategy (hereafter named LP) directly fol-
lows from a symmetrical property of an analogy
([x : y = z : t ] ? [y : x = t : z]), and reduces
the search procedure to the resolution of a number
of analogical equations which is quadratic with the
number of pairs ?x, y? sampled.
We found this strategy to be of little use for
input spaces larger than a few tens of thousands
forms. To solve this problem, we exploit a prop-
erty on symbol counts that an analogical relation
must fulfill (Lepage, 1998):
[x : y = z : t ]? |x|c + |t|c = |y|c + |z|c ?c ? A
3The authors proposed to sample x and y among the clos-
est forms in terms of edit-distance to I(u).
function solver(?x, y, z?, s)
Input: ?x, y, z?, a triplet, s the sampling size
Output: a set of solutions to [x : y = z : ? ]
sol? ?
for i? 1 to s do
?a, b? ? odd(rand(0, 1))? ?z, y? : ?y, z?
m ? shuffle(a,b )
c? complementary(m,x,,{})
sol? sol ? c
return sol
Algorithm 2: A Stroppa&Yvon flavored solver.
rand(a, b) returns a random integer between a
and b (included). The ternary operator ?: is to
be understood as in the C language.
where A is the alphabet on which the forms are
built, and |x|c stands for the number of occur-
rences of symbol c in x.
Our search strategy (named TC) begins by se-
lecting an x-form in the input space. This en-
forces a set of necessary constraints on the counts
of characters that any two forms y and z must sat-
isfy for [x : y = z : t ] to be true. By considering
all forms x in turn,4 we collect a set of candidate
triplets for t. A verification of those that define
with t an analogy must then be carried out. For-
mally, we built:
EI(u) = { ?x, y, z? : x ? I,
?y, z? ? C(?x, t?),
[x : y = z : t ] }
where C(?x, t?) denotes the set of pairs ?y, z?
which satisfy the count property.
This strategy will only work if (i) the number
of quadruplets to check is much smaller than the
number of triplets we can form in the input space
(which happens to be the case in practice), and
if (ii) we can efficiently identify the pairs ?y, z?
that satisfy a set of constraints on character counts.
To this end, we proposed in (Langlais and Yvon,
2008) to organize the input space into a data struc-
ture which supports efficient runtime retrieval.
3.3 The selector
Step 3 of analogical learning consists in selecting
one or several solutions from the set of candidate
forms produced by the generator. We trained in
a supervised manner a binary classifier to distin-
guish good translation candidates (as defined by
4Anagram forms do not have to be considered separately.
490
a reference) from spurious ones. We applied to
this end the voted-perceptron algorithm described
by Freund and Schapire (1999). Online voted-
perceptrons have been reported to work well in a
number of NLP tasks (Collins, 2002; Liang et al,
2006). Training such a classifier is mainly a matter
of feature engineering. An example e is a pair of
source-target analogical relations (r, r?) identified
by the generator, and which elects t? as a transla-
tion for the term t:
e ? (r, r?) ? ([x : y = z : t], [x? : y? = z? : t?])
where x?, y?, and z? are respectively the projections
of the source terms x, y and z. We investigated
many features including (i) the degree of r and r?,
(ii) the frequency with which a form is generated,5
(iii) length ratios between t and t?, (iv) likelihoods
scores (min, max, avg.) computed by a character-
based n-gram model trained on a large general cor-
pus (without overlap to DEV or TRAIN), etc.
4 Experiments
4.1 Calibrating the engine
We compared the two aforementioned searching
strategies on a task of identifying triplets in an
input space of French words for 1 000 randomly
selected test words. We considered input spaces
of various sizes. The results are reported in Ta-
ble 2. TC clearly outperforms LP by systemati-
cally identifying more triplets in much less time.
For the largest input space of 84 000 forms, TC
could identify an average of 746 triplets for 946
test words in 1.2 seconds, while the best compro-
mise we could settle with LP allows the identifi-
cation of 56 triplets on average for 889 words in
6.3 seconds on average. Note that in this exper-
iment, LP was calibrated for each input space so
that the best compromise between recall (%s) and
speed could be found. Reducing the size of the
neighborhood in LP improves computation time,
but significantly affects recall. In the following,
we only consider the TC search strategy.
4.2 Experimental Protocol
Datasets The data we used in this study comes
from the Medical Subject Headings (MeSH) the-
saurus. This thesaurus is used by the US National
Library of Medicine to index the biomedical sci-
5A form t? may be generated thanks to many examples.
s %s (s) s %s (s) s %s (s)
TC 34 83.1 0.2 261 94.1 0.5 746 96.4 1.2
LP 17 71.7 7.4 46 85.0 7.6 56 88.9 6.3
|I| 20 000 50 000 84 076
Table 2: Average number s of input analogies
found over 1 000 test words as a function of the
size of the input space. %s stands for the percent-
age of source forms for which (at least) one source
triplet is found; and (s) indicates the average time
(counted in seconds) to treat one form.
entific literature in the MEDLINE database.6 Its
preferred terms are called ?Main Headings?. We
collected pairs of source and target Main Head-
ings (TTY = ?MH?) with the same MeSH identi-
fiers (SDUI).
We considered five language pairs with three
relatively close European languages (English-
French, English-Spanish and English-Swedish), a
more distant one (English-Finnish) and one pair
involving different scripts (English-Russian).7
The material was split in three randomly se-
lected parts, so that the development and test ma-
terial contain exactly 1 000 terms each. The char-
acteristics of this material are reported in Table 3.
For the Finnish-English and Swedish-English lan-
guage pairs, the ratio of uni-terms in the Foreign
language (uf%) is twice the ratio of uni-terms in
the English counterpart. This is simply due to
the agglutinative nature of these two languages.
For instance, according to MeSH, the English
multi-term speech articulation tests corresponds
to the Finnish uni-term a?a?nta?miskokeet and to the
Swedish one artikulationstester. The ratio of out-
of-vocabulary forms (space-separated words un-
seen in TRAIN) in the TEST material is rather
high: between 36% and 68% for all Foreign-
to-English translation directions, but Finnish-to-
English, where surprisingly, only 6% of the word
forms are unknown.
Evaluation metrics For each experimental con-
dition, we compute the following measures:
Coverage the fraction of input words for which
the system can generate translations. If Nt words
receive translations among N , coverage is Nt/N .
6The MeSH thesaurus and its translations are included in
the UMLS Metathesaurus.
7Russian MeSH is normally written in Cyrillic, but some
terms are simply English terms written in uppercase Latin
script (e.g., ACHROMOBACTER for English Achromobac-
ter). We removed those terms.
491
TRAIN TEST DEV TEST
f nb uf% ue% nb uf% uf% oov%
FI 19 787 63.7 33.7 1 000 64.2 64.0 5.7
FR 17 230 29.8 29.3 1 000 30.8 28.3 36.3
RU 21 407 38.6 38.6 1 000 38.5 40.2 44.4
SP 19 021 31.1 31.1 1 000 31.7 33.3 36.6
SW 17 090 67.9 32.5 1 000 67.4 67.9 68.4
Table 3: Main characteristics of our datasets. nb
indicates the number of pairs of terms in a bi-
text, uf% (ue%) stands for the percentage of uni-
terms in the Foreign (English) part. oov% indi-
cates the percentage of out-of-vocabulary forms
(space-separated forms of TEST unseen in TRAIN).
Precision among the Nt words for which the
system proposes an answer, precision is the pro-
portion of those for which a correct translation is
output. Depending on the number of output trans-
lations k that one is willing to examine, a correct
translation will be output for Nk input words. Pre-
cision at rank k is thus defined as Pk = Nk/Nt.
Recall is the proportion of the N input words
for which a correct translation is output. Recall at
rank k is defined as Rk = Nk/N .
In all our experiments, candidate translations
are sorted in decreasing order of frequency with
which they were generated.
4.3 The generator
The performances of the generator on the 10
translation sessions are reported in Table 4.
The coverage of the generator varies between
38.5% (French-to-English) and 47.1% (English-
to-Finnish), which is rather low. In most cases, the
silence of the generator is due to a failure to iden-
tify analogies in the input space (step 1). The last
column of Table 4 reports the maximum recall we
can obtain if we consider all the candidates output
by the generator. The relative accuracy of the gen-
erator, expressed by the ratio ofR? to cov, ranges
from 64.3% (English-French) to 79.1% (Spanish-
to-English), for an average value of 73.8% over
all translation directions. This roughly means that
one fourth of the test terms with at least one solu-
tion do not contain the reference.
Overall, we conclude that analogical learning
offers comparable performances for all transla-
tion directions, although some fluctuations are ob-
served. We do not observe that the approach is
affected by language pairs which do not share the
Cov P1 R1 P100 R100 R?
? FI 47.1 31.6 14.9 57.7 27.2 31.9
FR 41.2 35.4 14.6 60.4 24.9 26.5
RU 46.2 40.5 18.7 69.9 32.3 34.8
SP 47.0 41.5 19.5 69.1 32.5 35.9
SW 42.8 36.0 15.4 66.8 28.6 31.9
? FI 44.8 36.6 16.4 66.7 29.9 33.2
FR 38.5 47.0 18.1 69.9 26.9 29.4
RU 42.1 49.4 20.8 70.3 29.6 32.3
SP 42.6 47.7 20.3 75.1 32.0 33.7
SW 44.6 40.8 18.2 69.5 31.0 32.9
Table 4: Main characteristics of the generator, as a
function of the translation directions (TEST).
same script (Russian/English). The best (worse)
case (as far as R? is concerned) corresponds to
translating into Spanish (French).
Admittedly, the largest recall andR? values re-
ported in Table 4 are disappointing. Clearly, for
analogical learning to work efficiently, enough lin-
guistic phenomena must be attested in the TRAIN
material. To illustrate this, we collected for the
Spanish-English language pair a set of medical
terms from the Medical Drug Regulatory Activi-
ties thesaurus (MedDRA) which contains roughly
three times more terms than the Spanish-English
material used in this study. This extra material al-
lows to raise the coverage to 73.4% (Spanish to
English) and 79.7% (English to Spanish), an abso-
lute improvement of more than 30%.
4.4 The selector
We trained our classifiers on the several millions
of examples generated while translating the devel-
opment material. Since we considered numerous
feature representations in this study, this implies
saving many huge datafiles on disk. In order to
save some space, we decided to remove forms that
were generated less than 3 times.8 Each classifier
was trained using 20 epochs.
It is important to note that we face a very unbal-
anced task. For instance, for the English to Finnish
task, the generator produces no less than 2.7 mil-
lions of examples, among which only 4 150 are
positive ones. Clearly, classifying all the examples
as negative will achieve a very high classification
accuracy, but will be of no practical use. There-
fore, we measure the ability of a classifier to iden-
8Averaged over all translation directions, this incurs an
absolute reduction of the coverage of 3.4%.
492
FI?EN FR?EN RU?EN SP?EN SW?EN
p r p r p r p r p r
argmax-f1 41.3 56.7 46.7 63.9 48.1 65.6 49.2 63.4 43.2 61.0
s-best 53.6 61.3 57.5 68.4 61.9 66.7 64.3 70.0 53.1 64.4
Table 5: Precision (p) and recall (r) of some classifiers on the TEST material.
tify the few positive forms among the set of candi-
dates. We measure precision as the percentage of
forms selected by the classifier that are sanctioned
by the reference lexicon, and recall as the percent-
age of forms selected by the classifier over the to-
tal number of sanctioned forms that the classifier
could possibly select. (Recall that the generator
often fails to produce oracle forms.)
The performance measured on the TEST mate-
rial of the best classifier we monitored on DEV
are reported in Table 5 for the Foreign-to-English
translation directions (we made consistent obser-
vations on the reverse directions). For compari-
son purposes, we implemented a baseline classi-
fier (lines argmax-f1) which selects the most-
frequent candidate form. This is the selector
used as a default in several studies on analogi-
cal learning (Lepage and Denoual, 2005; Stroppa
and Yvon, 2005). The baseline identifies between
56.7% to 65.6% of the sanctioned forms, at pre-
cision rates ranging from 41.3% to 49.2%. We
observe for all translation directions that the best
classifier we trained systematically outperforms
this baseline, both in terms of precision and recall.
4.4.1 The overall system
Table 6 shows the overall performance of the ana-
logical translation device in terms of precision, re-
call and coverage rates as defined in Section 4.2.
Overall, our best configuration (the one embed-
ding the s-best classifier) translates between
19.3% and 22.5% of the test material, with a preci-
sion ranging from 50.4% to 63.2%. This is better
than the variant which always proposes the most
frequent generated form (argmax-f1). Allowing
more answers increases both precision and recall.
If we allow up to 10 candidates per source term,
the analogical translator translates one fourth of
the terms (26.1%) with a precision of 70.9%, aver-
aged over all translation directions. The oracle
variant, which looks at the reference for select-
ing the good candidates produced by the genera-
tor, gives an upper bound of the performance that
could be obtained with our approach: less than
a third of the source terms can be translated cor-
rectly. Recall however that increasing the TRAIN
material leads to drastic improvements in cover-
age.
4.5 Comparison with a PB-SMT engine
To put these figures in perspective, we mea-
sured the performance of a phrase-based statisti-
cal MT (PB-SMT) engine trained to handle the
same translation task. We trained a phrase table
on TRAIN, using the standard approach.9 How-
ever, because of the small training size, and the
rather huge OOV rate of the translation tasks we
address, we did not train translation models on
word-tokens, but at the character level. There-
fore a phrase is indeed a sequence of charac-
ters. This idea has been successively investigated
in a Catalan-to-Spanish translation task by Vi-
lar et al (2007). We tuned the 8 coefficients of
the so-called log-linear combination maximized
at decoding time on the first 200 pairs of terms
of the DEV corpora. On the DEV set, BLEU
scores10 range from 67.2 (English-to-Finnish) to
77.0 (Russian-to-English).
Table 7 reports the precision and recall of both
translation engines. Note that because the SMT
engine always propose a translation, its precision
equals its recall. First, we observe that the preci-
sion of the SMT engine is not high (between 17%
and 31%), which demonstrates the difficulty of
the task. The analogical device does better for all
translation directions (see Table 6), but at a much
lower recall, remaining silent more than half of
the time. This suggests that combining both sys-
tems could be advantageous. To verify this, we
ran a straightforward combination: whenever the
analogical device produces a translation, we pick
it; otherwise, the statistical output is considered.
The gains of the resulting system over the SMT
alone are reported in column ?B. Averaged over
9We used the scripts distributed by Philipp Koehn to train
the phrase-table, and Pharaoh (Koehn, 2004) for producing
the translations.
10We computed BLEU scores at the character level.
493
FI?EN FR?EN RU?EN SP?EN SW?EN
k Pk Rk Pk Rk Pk Rk Pk Rk Pk Rk
argmax-f 1 41.3 17.3 46.7 16.8 47.8 18.6 48.7 19.2 43.4 18.1
10 61.6 25.8 62.8 22.6 61.7 24.0 69.3 27.3 62.1 25.9
s-best 1 53.5 20.8 56.9 19.3 58.5 20.3 63.2 22.5 50.4 21
10 69.4 27.0 69.0 23.4 71.8 24.9 78.4 27.9 65.7 27.4
oracle 1 100 30.5 100 26.3 100 28.5 100 30.6 100 29.5
Table 6: Precision and recall at rank 1 and 10 for the Foreign-to-English translation tasks (TEST).
all translation directions, BLEU scores increase on
TEST from 66.2 to 71.5, that is, an absolute im-
provement of 5.3 points.
? EN ? EN
Psmt ?B Psmt ?B
FI 20.2 +7.4 21.6 +6.4
FR 19.9 +5.3 17.0 +6.0
RU 24.1 +3.1 28.0 +6.4
SP 22.1 +4.9 26.4 +5.5
SW 25.9 +4.2 31.6 +3.2
Table 7: Translation performances on TEST. Psmt
stands for the precision and recall of the SMT en-
gine. ?B indicates the absolute gain in BLEU
score of the combined system.
We noticed a tendency of the statistical engine
to produce literal translations; a default the ana-
logical device does not show. For instance, the
Spanish term instituciones de atencio?n ambulato-
ria is translated word for word by Pharaoh into
institutions, atention ambulatory while analogical
learning produces ambulatory care facilities. We
also noticed that analogical learning sometimes
produces wrong translations based on morpholog-
ical regularities that are applied blindly. This is,
for instance, the case in a Russian/English exam-
ple where mouthal manifestations is produced, in-
stead of oral manifestations.
5 Discussion and future work
In this study, we proposed solutions to practical is-
sues involved in analogical learning. A simple yet
effective implementation of a solver is described.
A search strategy is proposed which outperforms
the one described in (Langlais and Patry, 2007).
Also, we showed that a classifier trained to se-
lect good candidate translations outperforms the
most-frequently-generated heuristic used in sev-
eral works on analogical learning.
Our analogical device was used to translate
medical terms in different language pairs. The
approach rates comparably across the 10 transla-
tion directions we considered. In particular, we
do not see a drop in performance when trans-
lating into a morphology rich language (such as
Finnish), or when translating into languages with
different scripts. Averaged over all translation di-
rections, the best variant could translate in first po-
sition 21% of the terms with a precision of 57%,
while at best, one could translate 30% of the terms
with a perfect precision. We show that the ana-
logical translations are of better quality than those
produced by a phrase-based engine trained at the
character level, albeit with much lower recall. A
straightforward combination of both approaches
led an improvement of 5.3 BLEU points over the
SMT alone. Better SMT performance could be
obtained with a system based on morphemes, see
for instance (Toutanova et al, 2008). However,
since lists of morphemes specific to the medical
domain do not exist for all the languages pairs we
considered here, unsupervised methods for acquir-
ing morphemes would be necessary, which is left
as a future work. In any case, this comparison is
meaningful, since both the SMT and the analogi-
cal device work at the character level.
This work opens up several avenues. First, we
will test our approach on terminologies from dif-
ferent domains, varying the size of the training
material. Second, analyzing the segmentation in-
duced by analogical learning would be interesting.
Third, we need to address the problem of com-
bining the translations produced by analogy into a
front-end statistical translation engine. Last, there
is no reason to constrain ourselves to translating
terminology only. We targeted this task in the first
place, because terminology typically plugs trans-
lation systems, but we think that analogical learn-
ing could be useful for translating infrequent enti-
ties.
494
References
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Compu-
tational Linguistics, 19(2):263?311.
M. Collins. 2002. Discriminative training methods for
hidden markov models: theory and experiments with
perceptron algorithms. In EMNLP, pages 1?8, Mor-
ristown, NJ, USA.
E. Denoual. 2007. Analogical translation of unknown
words in a statistical machine translation framework.
In MT Summit, XI, pages 10?14, Copenhagen.
Y. Freund and R. E. Schapire. 1999. Large margin
classification using the perceptron algorithm. Mach.
Learn., 37(3):277?296.
P. Fung and K. McKeown. 1997. Finding terminology
translations from non-parallel corpora. In 5th An-
nual Workshop on Very Large Corpora, pages 192?
202, Hong Kong.
M. Itagaki, T. Aikawa, and X. He. 2007. Auto-
matic validation of terminology translation consis-
tency with statistical method. In MT Summit XI,
pages 269?274, Copenhagen, Denmark.
P. Koehn. 2004. Pharaoh: A beam search decoder for
phrase-based statistical machine translation models.
In AMTA, pages 115?124, Washington, DC, USA.
P. Langlais and A. Patry. 2007. Translating unknown
words by analogical learning. In EMNLP-CoNLL,
pages 877?886, Prague, Czech Republic.
P. Langlais and F. Yvon. 2008. Scaling up analogi-
cal learning. In 22nd International Conference on
Computational Linguistics (COLING 2008), pages
51?54, Manchester, United Kingdom.
Y. Lepage and E. Denoual. 2005. ALEPH: an EBMT
system based on the preservation of proportion-
nal analogies between sentences across languages.
In International Workshop on Statistical Language
Translation (IWSLT), Pittsburgh, PA, October.
Y. Lepage and A. Lardilleux. 2007. The GREYC Ma-
chine Translation System for the IWSLT 2007 Eval-
uation Campaign. In IWLST, pages 49?53, Trento,
Italy.
Y. Lepage. 1998. Solving analogies on words: an algo-
rithm. In COLING-ACL, pages 728?734, Montreal,
Canada.
P. Liang, A. Bouchard-Co?te?, D. Klein, and B. Taskar.
2006. An end-to-end discriminative approach to ma-
chine translation. In 21st COLING and 44th ACL,
pages 761?768, Sydney, Australia.
E. Morin, B. Daille, K. Takeuchi, and K. Kageura.
2007. Bilingual terminology mining - using brain,
not brawn comparable corpora. In 45th ACL, pages
664?671, Prague, Czech Republic.
R. Rapp. 1995. Identifying word translation in non-
parallel texts. In 33rd ACL, pages 320?322, Cam-
bridge,Massachusetts, USA.
N. Stroppa and F. Yvon. 2005. An analogical learner
for morphological analysis. In 9th CoNLL, pages
120?127, Ann Arbor, MI.
K Toutanova, H. Suzuki, and A. Ruopp. 2008. Ap-
plying morphology generation models to machine
translation. In ACL-8 HLT, pages 514?522, Colom-
bus, Ohio, USA.
D. Vilar, J. Peter, and H. Ney. 2007. Can we trans-
late letters? In Proceedings of the Second Work-
shop on Statistical Machine Translation, pages 33?
39, Prague, Czech Republic, June.
F. Yvon, N. Stroppa, A. Delhay, and L. Miclet. 2004.
Solving analogical equations on words. Techni-
cal Report D005, E?cole Nationale Supe?rieure des
Te?le?communications, Paris, France, July.
495
Proceedings of the 2009 Workshop on Knowledge and Reasoning for Answering Questions, ACL-IJCNLP 2009, pages 1?2,
Suntec, Singapore, 6 August 2009.
c?2009 ACL and AFNLP
Knowledge and Reasoning for Medical Question-Answering
Pierre Zweigenbaum
CNRS, LIMSI
Orsay, F-91403 France
pz@limsi.fr
Abstract
Restricted domains such as medicine set
a context where question-answering is
more likely expected to be associated
with knowledge and reasoning (Moll? and
Vicedo, 2007; Ferret and Zweigenbaum,
2007). On the one hand, knowledge and
reasoning may be more necessary than
in open-domain question-answering be-
cause of more specific or more difficult
questions. On the other hand, it may
also be more manageable, since by def-
inition restricted-domain QA should not
have to face the same breadth of ques-
tions as open-domain QA. It is therefore
interesting to study the role of knowl-
edge and reasoning in restricted-domain
question-answering systems. We shall do
so in the case of the (bio-)medical domain,
which has a long tradition of investigat-
ing knowledge representation and reason-
ing and, more generally, artificial intel-
ligence methods (Shortliffe et al, 1975),
and which has seen a growing interest
in question-answering systems (Zweigen-
baum, 2003; Yu et al, 2005; Demner-
Fushman and Lin, 2007; Zweigenbaum et
al., 2007).
1 Knowledge and Reasoning for
Processing Medical Questions
Medical question-answering has to address ques-
tions other than the usual factual questions of most
QA evaluations. This calls for different question
classifications (Ely et al, 2000; Yu et al, 2005),
especially to determine whether a given ques-
tion can be answered using medical knowledge
backed with a sufficient level of evidence (Lin and
Demner-Fushman, 2005; Kilicoglu et al, 2009).
This can also lead to a different representation of
questions, for instance using a structured represen-
tation such as PICO (Niu et al, 2003; Huang et al,
2006; Demner-Fushman and Lin, 2007) or simple
concepts and relations (Lin, 2001; Jacquemart and
Zweigenbaum, 2003).
2 Knowledge and Reasoning for Finding
Medical Answers
Answers to medical questions should be searched
in the most reliable data available. When data exist
in structured knowledge bases (e.g. a drug com-
pendium), it may be more appropriate to query
such knowledge bases directly. Therefore an ap-
proach akin to that of Start/Omnibase (Lin and
Katz, 2003) may be indicated. When answers are
to be found in a collection of documents, as is the
case in traditional question-answering systems,
a representation of the information contained in
these documents can be built, offline (Fleischman
et al, 2003; Sang et al, 2005; Delbecque et al,
2005) or dynamically.
In medical QA systems, both document anal-
ysis and question analysis nearly always rely on
extensive knowledge of domain concepts and re-
lations, e.g. as provided by the UMLS knowl-
edge sources (McCray and Nelson, 1995). More
than named entities, systems need to detect men-
tions of concepts (Aronson, 2001) and their rela-
tions (Rindflesch et al, 2005). Besides, taking into
account the structure of documents such as sci-
entific articles or encyclopedia entries may help
focus on more relevant sections (Niu and Hirst,
2004; Sang et al, 2005). Finally, answers to com-
plex medical questions often need to span more
than one sentence. Extractive summarization is
performed both from single documents (Demner-
Fushman and Lin, 2007) and from multiple docu-
ments (Fiszman et al, 2008).
1
References
Alan R. Aronson. 2001. Effective mapping of biomed-
ical text to the UMLS Metathesaurus: The MetaMap
program. Journal of the American Medical Infor-
matics Association, 8(suppl):17?21.
Thierry Delbecque, Pierre Jacquemart, and Pierre
Zweigenbaum. 2005. Indexing UMLS semantic
types for medical question-answering. In Rolf En-
gelbrecht, Antoine Geissbuhler, Christian Lovis, and
G. Mihalas, editors, Proceedings Medical Informat-
ics Europe, volume 116 of Studies in Health Tech-
nology and Informatics, pages 805?810, Amster-
dam. IOS Press.
Dina Demner-Fushman and Jimmy Lin. 2007. An-
swering clinical questions with knowledge-based
and statistical techniques. Computational Linguis-
tics, 33(1):63?103.
John W. Ely, Jerome A. Osheroff, Paul N. Gor-
man, Mark H. Ebell, M. Lee Chambliss, Eric A.
Pifer, and P. Zoe Stavri. 2000. A taxonomy
of generic clinical questions: classification study.
BMJ, 321:429?432. Available at http://bmj.
com/cgi/content/full/321/7258/429.
Olivier Ferret and Pierre Zweigenbaum. 2007.
Repr?sentation des connaissances pour les sys-
t?mes de question-r?ponse. In Brigitte Grau
and Jean-Pierre Chevallet, editors, La recherche
d?informations pr?cises : traitement automatique de
la langue, apprentissage et connaissances pour les
syst?mes de question-r?ponse, chapter 4, pages 133?
169. Herm?s-Lavoisier, Paris.
Marcelo Fiszman, Dina Demner-Fushman, Halil Kil-
icoglu, and Thomas C Rindflesch. 2008. Automatic
summarization of MEDLINE citations for evidence-
based medical treatment: A topic-oriented evalua-
tion. J Biomed Inform, November.
Michael Fleischman, Abdessamad Echihabi, and Ed-
uard Hovy. 2003. Offline strategies for online ques-
tion answering: Answering questions before they
are asked. In Proceedings of the ACL Conference,
pages 1?7, Sapporo, Japan.
Xiaoli Huang, Jimmy Lin, and Dina Demner-Fushman.
2006. Evaluation of PICO as a knowledge represen-
tation for clinical questions. In AMIA Annu Symp
Proc, page 359?63.
Pierre Jacquemart and Pierre Zweigenbaum. 2003. To-
wards a medical question-answering system: a feasi-
bility study. In Robert Baud, Marius Fieschi, Pierre
Le Beux, and Patrick Ruch, editors, Proceedings
Medical Informatics Europe, volume 95 of Studies
in Health Technology and Informatics, pages 463?
468, Amsterdam. IOS Press.
Halil Kilicoglu, Dina Demner-Fushman, Thomas C
Rindflesch, Nancy L Wilczynski, and R Brian
Haynes. 2009. Towards automatic recognition of
scientifically rigorous clinical research evidence. J
Am Med Inform Assoc, 16(1):25?31.
Jimmy Lin and Dina Demner-Fushman. 2005. ?Bag of
words? is not enough for strength of evidence clas-
sification. In AMIA Annu Symp Proc, page 1031.
Jimmy Lin and Boris Katz. 2003. Question answering
techniques for the World Wide Web. In Tutorial at
EACL 2003, Budapest. ACL.
Jimmy Lin. 2001. Indexing and retrieving natural lan-
guage using ternary expressions. Master?s thesis,
Massachusetts Institute of Technology.
Alexa T. McCray and Stuart J. Nelson. 1995. The se-
mantics of the UMLS knowledge sources. Methods
of Information in Medicine, 34(1/2).
Diego Moll? and Jos? Luis Vicedo. 2007. Question an-
swering in restricted domains: An overview. Com-
putational Linguistics, 33(1):41?61.
Yun Niu and Graeme Hirst. 2004. Analysis of seman-
tic classes in medical text for question answering. In
Proceedings ACL 2004 Workshop on Question An-
swering in Restricted Domains. ACL.
Yun Niu, Graeme Hirst, Gregory McArthur, and Patri-
cia Rodriguez-Gianolli. 2003. Answering clinical
questions with role identification. In ACL Workshop
Natural Language Processing in Biomedicine, pages
73?80. ACL.
Thomas C. Rindflesch, Marcelo Fiszman, and B. Lib-
bus. 2005. Semantic interpretation for the biomed-
ical literature. In H Chen, S Fuller, WR Hersh,
and C Friedman, editors, Medical informatics: Ad-
vances in knowledge management and data mining
in biomedicine, pages 399?422, Berlin / Heidelberg.
Springer.
Erik Tjong Kim Sang, Gosse Bouma, and Maarten
de Rijke. 2005. Developing offline strategies for an-
swering medical questions. In Proceedings AAAI-05
workshop on Question Answering in restricted do-
mains, pages 41?45. AAAI.
E H Shortliffe, R Davis, S G Axline, B G Buchanan,
C C Green, and S N Cohen. 1975. Computer-based
consultations in clinical therapeutics: explanation
and rule acquisition capabilities of the MYCIN sys-
tem. Comput Biomed Res, 8(4):303?20, August.
Hong Yu, Carl Sable, and Hai Ran Zhu. 2005. Classi-
fying medical questions based on an evidence taxon-
omy. In Proceedings AAAI 2005 Workshop on Ques-
tion Answering in Restricted Domains. AAAI.
Pierre Zweigenbaum, Dina Demner-Fushman, Hong
Yu, and K. Bretonnel Cohen. 2007. Fron-
tiers of biomedical text mining: current progress.
Briefings in Bioinformatics, 8:358?375, October.
doi:10.1093/bib/bbm045.
Pierre Zweigenbaum. 2003. Question answering in
biomedicine. In Maarten de Rijke and Bonnie Web-
ber, editors, Proceedings Workshop on Natural Lan-
guage Processing for Question Answering, EACL
2003, pages 1?4, Budapest. ACL. Keynote speech.
2
Proceedings of the 2nd Workshop on Building and Using Comparable Corpora, ACL-IJCNLP 2009, pages 2?10,
Suntec, Singapore, 6 August 2009. c?2009 ACL and AFNLP
Extracting Lay Paraphrases of Specialized Expressions
from Monolingual Comparable Medical Corpora
Louise Del?ger
INSERM U872 Eq.20
Paris, F-75006 France
louise.deleger@spim.jussieu.fr
Pierre Zweigenbaum
CNRS, LIMSI
Orsay, F-91403 France
pz@limsi.fr
Abstract
Whereas multilingual comparable corpora
have been used to identify translations of
words or terms, monolingual corpora can
help identify paraphrases. The present
work addresses paraphrases found be-
tween two different discourse types: spe-
cialized and lay texts. We therefore built
comparable corpora of specialized and lay
texts in order to detect equivalent lay and
specialized expressions. We identified two
devices used in such paraphrases: nomi-
nalizations and neo-classical compounds.
The results showed that the paraphrases
had a good precision and that nominaliza-
tions were indeed relevant in the context of
studying the differences between special-
ized and lay language. Neo-classical com-
pounds were less conclusive. This study
also demonstrates that simple paraphrase
acquisition methods can also work on texts
with a rather small degree of similarity,
once similar text segments are detected.
1 Introduction
Comparable corpora refer to collections of texts
sharing common characteristics. Very often com-
parable corpora consist of texts in two (or more)
languages that address the same topic without be-
ing translations of each other. But this notion
also applies to monolingual texts. In a mono-
lingual context, comparable corpora can be texts
from different sources (such as articles from var-
ious newspapers) or from different genres (such
as specialized and lay texts) but dealing with the
same general topic. Comparable corpora have
been used to perform several Natural Language
Processing tasks, such as extraction of word trans-
lations (Rapp, 1995; Chiao and Zweigenbaum,
2002) in a multilingual context or acquisition of
paraphrases (Barzilay and Lee, 2003; Shinyama
and Sekine, 2003) in a monolingual context. In
this work1, we are interested in using comparable
corpora to extract paraphrases.
Paraphrases are useful to various applications,
including information retrieval (Ibrahim et al,
2003), information extraction (Shinyama and
Sekine, 2003), document summarization (Barzi-
lay, 2003) and text simplification (Elhadad and Su-
taria, 2007). Several methods have been designed
to extract paraphrases, many of them dealing with
comparable text corpora. A few paraphrase acqui-
sition approaches used plain monolingual corpora
to detect paraphrases, such as (Jacquemin, 1999)
who detects term variants or (Pasca and Dienes,
2005) who extract paraphrases from random Web
documents. This type of corpus does not insure
the actual existence of paraphrases and a majority
of methods have relied on corpora with a stronger
similarity between the documents, thus likely to
provide a greater amount of paraphrases. Some
paraphrase approaches used monolingual paral-
lel corpora, i.e. different translations or versions
of the same texts. For instance (Barzilay and
McKeown, 2001) detected paraphrases in a corpus
of English translations of literary novels. How-
ever such corpora are not easily available and ap-
proaches which rely instead on other types of cor-
pora are actively investigated.
Bilingual parallel corpora have been exploited
for acquiring paraphrases in English (Bannard and
Callison-Burch, 2005) and French (Max, 2008).
Comparable corpora are another useful source of
paraphrases. In this regard, only closely related
corpora have been used, especially and almost ex-
clusively corpora of news sources reporting the
1This paper is an extension of the work presented
in (Del?ger and Zweigenbaum, 2008a) and (Del?ger and
Zweigenbaum, 2008b), more specifically, a new corpus is
added, an additional type of paraphrase (based on neo-
classical compounds) is extracted and the evaluation is more
relevant.
2
same events. (Barzilay and Lee, 2003) gener-
ated paraphrase sentences from news articles us-
ing finite state automata. (Shinyama and Sekine,
2003) extracted paraphrases through the detection
of named entities anchors in a corpus of Japanese
news articles. In the medical domain, (Elhadad
and Sutaria, 2007) worked with a comparable, al-
most parallel, corpus of medical scientific articles
and their lay versions to extract paraphrases be-
tween specialized and lay languages.
We aim at detecting paraphrases in medical cor-
pora in the same line as (Elhadad and Sutaria,
2007) but for French. This type of paraphrases
would be a useful resource for text simplification
or to help authoring medical documents dedicated
to the general public. However, in a French medi-
cal context, it is difficult to obtain comparable cor-
pora of documents with a high level of similarity,
such as pairs of English scientific articles and their
translations in lay language, or news articles re-
porting the same events used in general language
(Barzilay and Lee, 2003; Shinyama and Sekine,
2003). Therefore, in addition to using this type
of comparable corpora, we also tried to rely on
corpora with less similarity but more easily avail-
able documents: lay and specialized documents
from various sources dealing with the same overall
medical topic.
We describe our experiment in building and ex-
ploiting these corpora to find paraphrases between
specialized and lay language. Issues at stake in-
volve: (i) how to collect corpora as relevant as
possible (Section 2.1); (ii) how to identify pas-
sages which potentially convey comparable in-
formation (Section 2.2); and (iii) what sorts of
paraphrases can be collected between these two
types of discourse, which is addressed in Sec-
tion 2.3, through the identification of two kinds
of paraphrases: nominalization paraphrases and
paraphrases of neo-classical compounds. An eval-
uation of the method (Section 2.4) is conducted
and results are presented (Section 3) and discussed
(Section 4).
2 Material and Methods
2.1 Building comparable corpora of lay and
specialized texts
Today, a popular way of acquiring a corpus is col-
lecting it from the Web (Kilgarriff and Grefen-
stette, 2003), as it provides easy access to an un-
limited amount of documents. Here we focus
on monolingual comparable corpora of special-
ized and lay medical French documents, with the
objective of identifying correspondences between
the two varieties of languages in these documents.
We collected three corpora from the Web dealing
with the following three topics: nicotine addiction,
diabetes and cancer.
When dealing with a Web corpus several is-
sues arise. The first one is the relevance of
the documents retrieved to the domain targeted
and is highly dependant on the method used to
gather the documents. Possible methods include
querying a general-purpose search engine (such
as Google) with selected key words, querying a
domain-specific search engine (in domains where
they exist) indexing potentially more relevant and
trustworthy documents, or directly downloading
documents from known relevant websites. An-
other important issue specific to our type of cor-
pus is the relevance to the genre targeted, i.e. lay
vs. specialized. Hence the need to classify each
collected document as belonging to one genre or
the other. This can be done by automatic cate-
gorisation of texts or by direct knowledge of the
sources of documents. In order to obtain a corpus
as relevant as possible to the domain and to the
genres, we used direct knowledge and restricted
search for selecting the documents. In the case of
the cancer topic, we had knowledge of a website
containing comparable lay and specialized docu-
ments: the Standards, Options: Recommandations
website2 which gives access to guidelines on can-
cer for the medical specialists on the one hand and
guides for the general public on the same topics on
the other hand. This case was immediate: we only
had to download the documents from the website.
This corpus is therefore constituted of quite sim-
ilar documents (professional guidelines and their
lay versions). The other two corpora (on nico-
tine addiction and diabetes), however, were built
from heterogeneous sources through a restricted
search and are less similar. We first queried two
health search engines (the health Web portals CIS-
MeF3 and HON4) with key words. Both allow
the user to search for documents targeted to a
population (e.g., patient-oriented documents). We
also queried known relevant websites for docu-
ments dealing with our chosen topics. Those were
2http://www.sor-cancer.fr/
3http://www.cismef.org/
4http://www.hon.ch/
3
French governmental websites, including that of
the HAS5 which issues guidelines for health pro-
fessionals, and that of the INPES6 which provides
educational material for the general public; as well
as health websites dedicated to the general pub-
lic, including Doctissimo7, Tabac Info Service8,
Stoptabac9 and Diab?te Qu?bec10.
The corpus dealing with the topic of diabetes
served as our development corpus for the first type
of paraphrases we extracted, the other two corpora
were used as test corpora.
Once collected, a corpus needs to be cleaned
and converted into an appropriate format to allow
further processing, i.e. extracting the textual con-
tent of the documents. HTML documents typi-
cally contain irrelevant information such as nav-
igation bars, footers and advertisements?referred
to as ?boilerplate??which can generate noise.
Boilerplate removal methods can rely on HTML
structure, visual features (placement and size of
blocks) and plain text features. We used HTML
structure (such as meta-information and density of
HTML tags) and plain text (such as spotting phone
and fax numbers and e-mails, as often appear at
the end of documents) to get rid of boilerplate.
2.2 Aligning similar text segments
We hypothesize that paraphrases will be found
more reliably in text passages taken from both
sides of our comparable corpora which address
similar topics. So, as a first step, we tried to re-
late such passages. We proceeded in three steps:
1. as multiple topics are usually addressed in
a single text, we performed topic segmenta-
tion on each text using the TextTiling (Hearst,
1997) segmentation tool. A segment may
consist of one or several paragraphs;
2. we then tried to identify pairs of text seg-
ments addressing similar topics and likely to
contain paraphrases. For this we used a com-
mon, vector-based measure of text similarity:
the cosine similarity measure which we com-
puted for each pair of topic segments in the
cross-product of both corpus sides (each seg-
ment was represented as a bag of words);
5http://www.has-sante.fr/
6http://www.inpes.sante.fr/
7http://www.doctissimo.fr/
8http://www.tabac-info-service.fr/
9http://www.stop-tabac.ch/
10http://www.diabete.qc.ca/
3. we selected the best text segment pairs, that
is the pairs with a similarity score equal or
superior to 0.33, a threshold we determined
based on the results of a preliminary study
(Del?ger and Zweigenbaum, 2008a).
2.3 Extracting paraphrases
We are looking for paraphrases between two vari-
eties of language (specialized and lay), as opposed
to any kind of possible paraphrases. We there-
fore endeavoured to determine what kind of para-
phrases may be relevant in this regard. A com-
mon hypothesis (Fang, 2005) is that specialized
language uses more nominal constructions where
lay language uses more verbs instead. We test this
hypothesis and build on it to detect specialized-lay
paraphrases around noun-to-verb mappings (a first
version of this work was published in (Del?ger and
Zweigenbaum, 2008b)). A second hypothesis is
that medical language contains a fair proportion of
words from Latin and Greek origins, which are re-
ferred to as neo-classical compounds. The mean-
ing of these words may be quite obscure to non-
experts readers. So one would expect to find less
of these words in lay texts and instead some sort
of paraphrases in common language. We therefore
tried to detect these paraphrases as a second type
of specialized vs. lay correspondences.
2.3.1 Paraphrases of nominalizations
A first type of paraphrases we tried to extract
was paraphrases between nominal constructions
in the specialized side (such as treatment of the
disease) and verbal constructions in the lay side
(such as the disease is treated). This type of para-
phrases involves nominalizations of verbal phrases
and is built around the relation between a dever-
bal noun (e.g. treatment) and its base verb (e.g.
treat). Therefore, we relied on a lexicon of French
deverbal nouns paired with corresponding verbs
(Hathout et al, 2002) to detect such pairs in the
corpus segments. These noun-verb pairs served as
anchors for the detection of paraphrases. In order
to design paraphrasing patterns we extracted all
pairs of deverbal noun and verb with their contexts
from the development corpus. The study of such
pairs with their contexts allowed us to establish a
set of lexico-syntactic paraphrasing patterns11. An
example of such patterns can be seen in Table 1.
11Texts were first tagged with Treetagger (http://www.
ims.uni-stuttgart.de/projekte/corplex/TreeTagger/).
4
Specialized Lay
N1 PREP (DET) N2 V1 (DET) N2
N1 PREP (DET) N2A3 V1(DET) N2A3
N1 A2 V1(DET) N2
Table 1: Example paraphrasing patterns (a shared
index indicates equality or synonymy. N=noun,
V=verb, A=adjective, PREP=preposition,
DET=determiner, 1 in index = pair of dever-
bal noun and verb)
The general method was to look for correspond-
ing content words (mainly noun and adjective) in
the contexts. We defined corresponding words as
either equal or synonymous (we used lexicons of
synonyms as resources12). Equals may have ei-
ther the same part-of-speech, or different parts-of-
speech, in which case stemming13 is performed to
take care of derivational variation (e.g., medicine
and medical). We then applied the patterns to both
development and test corpora.
The patterns thus designed are close to the
transformation rules of (Jacquemin, 1999) who
detects morpho-syntactico-semantic variants of
terms in plain monolingual corpora. One dif-
ference is that our patterns are built around one
specific type of morphological variation (noun to
verb variation) that seemed relevant in the context
of the specialized/lay opposition, as opposed to
any possible variation. We also identify the para-
phrases by comparing the two sides of a compara-
ble corpus while (Jacquemin, 1999) starts from a
given list of terms and searches for their variants
in a plain monolingual corpus. Finally, we do not
apply our method on terms specifically but on any
expression corresponding to the patterns.
2.3.2 Paraphrases of neo-classical
compounds
We then extracted paraphrases of neo-classical
compounds as a second type of paraphrases that
seemed relevant to the opposition between lay
and specialized languages. This means that we
looked for neo-classical compounds on one side
of the corpora and equivalents in modern lan-
guage on the other side. To do this we relied
on the morphosemantic parser D?riF (Namer and
12The lexicons used came from the Masson and Robert dic-
tionaries.
13Stemming was performed using the Lingua::Stem
perl package (http://search.cpan.org/~snowhare/
Lingua-Stem-0.83) which is similar to the Snowball
stemmers (http://snowball.tartarus.org)
Zweigenbaum, 2004). D?riF analyzes morpholog-
ically complex words and outputs a decomposi-
tion of those words into their components and a
definition-like gloss of the words according to the
meaning of the components in modern language
when they are from Greek or Latin origins. For
instance the French word gastrite (gastritis) is de-
composed into gastr+ite and its gloss is inflamma-
tion de l?estomac (inflammation of stomach).
We first ran the analyzer on the specialized
side of the corpora to detect neo-classical com-
pounds. Then we searched for paraphrases of
those compounds based on the output of D?riF,
that is we looked for the modern-language equiva-
lents of the word components (in the case of gas-
tritis this means searching for inflammation and
stomach) close to each other within a syntactic
phrase (we empirically set a threshold of 4 words
as the maximum distance between the modern-
language translations of the components). A pat-
tern used to search those paraphrases is for in-
stance:
C ? ((DET)? N PREP)? (DET)? C1 W0?4 C2
where C is a neo-classical compounds in a spe-
cialized text segment, C1 and C2 are the modern-
language components of C, N is a noun, PREP a
preposition, DET a determiner and W an arbitrary
word.
2.4 Evaluation
We first evaluated the quality of the extracted para-
phrases by measuring their precision, that is, the
percentage of correct results over the entire re-
sults. We computed precision for each type of
paraphrases.
We then estimated recall for the first type
of paraphrases (nominalization paraphrases): the
percentage of correct extracted paraphrases over
the total number of paraphrases that should have
been extracted. We used as gold standard a ran-
dom sample of 10 segment pairs from which we
manually extracted paraphrases.
Finally, since we aim at detecting paraphrases
between lay and specialized languages, we also
looked at the relevance of the two types we chose
to extract. That is, we evaluated the coherence of
the results with our two initial hypotheses, which
are expected to apply when both a specialized text
segment and a lay text segment convey similar
information: (1) nominalizations are more often
used in specialized texts while lay texts tend to
5
Specialized Lay
(a) Ns ...the benefits of smoking cessation... Nl ...withdrawal symptoms of smoking cessation...
(b) Ns ...regular use of tobacco concerned... Nl ...tobacco use is the first cause...
(c) Ns ...which goes with smoking cessation... Vl ...who wants to stop smoking...
Table 2: Sample cases used to compute the conditional probability for nominalizations; (a) and (b)
represent cases where a paraphrase was expected but did not occur and (c) a case where a paraphrase was
indeed used. N = nominalization; V = verbal form.
Specialized Lay
(a) Cs ...glycemia is lower... Cl ...a drop of glycemia...
(b) Cs ...the starting point of thrombosis... Cl ...the risk of thrombosis...
(c) Cs ...especially cardiopathies and... Ml ...25% of heart diseases...
Table 3: Sample cases used to compute the conditional probability for neo-classical compounds; (a) and
(b) represent cases where a paraphrase was expected but did not occur and (c) a case where a paraphrase
was indeed used. C = compound; M = modern.
replace them with verbs; (2) specialized texts use
more neoclassical compounds while lay texts give
a paraphrase in modern language.
To evaluate (1) we measured the conditional
probability P (Vl|Ns) that a nominalization pat-
tern Ns in a specialized segment be replaced by
a matching verbal pattern Vl in a corresponding
lay segment. These patterns are the paraphras-
ing patterns defined in Section 2.3.1 and exempli-
fied in Table 1. Table 2 gives examples of cases
taken into account when computing this probabil-
ity, i.e. cases where both text segments convey the
same information, as a nominalization in the spe-
cialized side and as a nominalization or a verbal
paraphrase in the lay side. Formally, the proba-
bility can be estimated by |ParNs?Vl ||ExpParNs?Vl | , where
|ParNs?Vl | is the number of correct extracted
paraphrases involving a nominalization in a spe-
cialized segment and a verbal construction in the
corresponding lay segment (case (c) of Table 2),
and |ExpParNs?Vl | the expected number of para-
phrases. The expected number of paraphrases cor-
responds to the total number of instances where
a specialized text segment contains a nominal-
ization and the corresponding lay segment con-
veys the same information, expressed either as a
nominalization or as a paraphrasing verbal con-
struction (cases (a), (b) and (c) of Table 2). It
is therefore computed as the sum of |ParNs?Vl |
and |ParNs?Nl |, the latter referring to the number
of occurrences where both the specialized and lay
segments match the same nominalization pattern,
i.e., instances where a paraphrase was expected
but did not occur (cases (a) and (b) of Table 2). For
instance use of tobacco on one side and tobacco
use on the other side, as in (b), is a case where
one would have expected a paraphrase such as to-
bacco is used. Note that matching allows the same
flexibility as described in Section 2.3.1 in terms
of synonyms and morphological variants. To test
whether this tendency of using verbal construc-
tions instead of nominalizations is indeed stronger
in lay texts we also measured the reverse, i.e. the
conditional probability P (Vs|Nl), given a nomi-
nalization pattern Nl in a lay segment, that it be
replaced with a matching verbal pattern Vs in the
corresponding specialized segment, computed as
|ParNl?Vs |
|ExpParNl?Vs |
. If our hypothesis is verified, this
reverse probability should be lower then the direct
probability.
In the same way, to evaluate (2) we measured
the conditional probability P (Ml|Cs) that a neo-
classical compound Cs in a specialized segment
be replaced by a modern-language equivalent Ml
in a corresponding lay segment. Table 3 gives ex-
amples of cases taken into account when comput-
ing this probability, that is cases where both text
segments convey the same information, as a neo-
classical compound in the specialized side and as
a neo-classical compound or a modern-language
paraphrase in the lay side. Formally, it can be
estimated by |ParCs?Ml ||ExpParCs?Ml | , where |ParCs?Ml |is the number of correct extracted paraphrases in-
volving a neo-classical compound in a specialized
6
Diabetes Nicotine addiction Cancer
S L S L S L
docs 135 600 62 620 22 16
words 580,712 461,066 595,733 603,257 641,584 228,742
segment pairs 183 547 438
Table 4: Sizes of the corpora (Number of documents, words and segment pairs; S=specialized, L=lay)
Diabetes Nicotine add. Cancer
total
paraph.
42 79 93
correct
paraph.
30 62 62
precision 71.4% 78.5% 75.8%
Table 5: Precision for nominalization paraphrases
(at the type level, not token level)
segment and a modern-language equivalent in the
corresponding lay segment (case (c) of Table 3)
, and |ExpParCs?Ml | is the expected number
of paraphrases (case (a), (b) and (c) of Table 3).
The expected number of paraphrases is the sum of
|ParCs?Ml | and |ParCs?Cl |, the latter referring
to the number of occurrences where both the spe-
cialized and lay segments contains the same neo-
classical compound (instances where a paraphrase
was expected but did not occur, for instance cases
(a) and (b) of Table 3). We then measured the re-
verse, i.e. the conditional probability P (Ms|Cl),
given a neo-classical compound Cl in a lay seg-
ment, that it be replaced with a modern-language
equivalent Ms in the corresponding specialized
segment, computed as |ParCl?Ms ||ExpParCl?Ms | .
3 Results
Table 4 gives size figures for each side (lay and
specialized) of the three corpora in terms of docu-
ments, words and segment pairs.
Evaluation of the quality of the extracted para-
phrases shows that precision is rather good for
both type of paraphrases (see Tables 5 and 6), al-
though the figures cannot be considered signica-
tive for paraphrases of compounds extracted in the
tobacco and cancer corpora given the small num-
ber of paraphrases (only 3 paraphrases in both
cases).
Examples of nominalization paraphrases and
paraphrases of neo-classical compounds are given
in Tables 7 and 8. The last line of Table 7 shows
Diabetes Nicotine add. Cancer
total
paraph.
39 3 3
correct
paraph.
24 3 3
precision 61.5% 100% 100%
Table 6: Precision for paraphrases of neo-classical
compounds (at the type level, not token level)
an example of incorrect paraphrase, which is due
to the synonymy link established between French
words charge and poids which is not valid in
that particular context. The last line of Table 8
also gives an incorrect example, which is caused
by the imprecision of the modern-language para-
phrase which is only partially equivalent to the
neo-classical compound.
Specialized Lay
consommation
r?guli?re
consommer de fa?on
r?guli?re
regular use to use in a regular
fashion
g?ne ? la lecture emp?che de lire
reading difficulty prevents from reading
?volution de l?affection la maladie ?volue
evolution of the
condition
the disease is evolving
*prise en charge prendre du poids
the taking care of to take on weight
Table 7: Examples of extracted nominalization
paraphrases (* indicates an incorrect example)
With regard to the quantitative evaluation of the
nominalization paraphrases, we measured a 30%
recall on our sample of segment pairs, meaning
that out of the 10 manually extracted paraphrases
only 3 were automatically detected by our method.
Cases of non-detected paraphrases were due to the
restrained scope of the paraphrasing patterns, as
well as to the presence of synonyms not contained
7
Specialized Lay
leucospermie Augmentation du nombre de
globules blancs dans le sperme
leucospermia Increase in the number of white
cells in the sperm
glyc?mie la quantit? de sucre dans le sang
glycemia amount of sugar in the blood
prostatectomie l?ablation de la prostate
prostatectomy ablation of the prostate
*hyperglyc?mie le taux de sucre dans le sang
hyperglycemia proportion of sugar in the blood
Table 8: Examples of extracted paraphrases of
neo-classical compounds (* indicates an incorrect
example)
in our lists.
Table 9 displays results for the investigation on
the coherence of our first initial hypothesis that
specialized texts use nominalizations where lay
texts use verbal constructions. The conditional
probability that a nominalization be replaced with
a verbal construction is higher for nominalizations
in specialized texts than for the reverse direction,
which means that nominalizations in specialized
texts are indeed more likely to be replaced by
verbal constructions in lay texts than nominaliza-
tions in lay texts by verbal constructions in spe-
cialized texts. Results for the second hypothe-
sis (neo-classical compounds in specialized texts
tend to be replaced by modern-language equiva-
lents in lay texts) are given in Table 10. As for the
first hypothesis, the conditional probability for the
neo-classical compounds in the specialized texts is
higher, which seems to be coherent with the ini-
tial hypothesis. However, given the very small
number of paraphrases, we cannot draw a signi-
ficative conclusion as regards this second type of
paraphrases.
4 Discussion
In this work we built comparable corpora of spe-
cialized and lay texts on which we implemented
simple paraphrase acquisition methods to extract
certain types of paraphrases that seemed rele-
vant in the context of specialized and lay lan-
guage: paraphrases based on nominalization vs.
verbal constructions and paraphrases based on
neo-classical compounds vs. modern-language ex-
pressions. The precision measured on the set of
detected paraphrases is rather good, which indi-
cates good quality of the paraphrases (hence of the
paraphrasing patterns and extracted segments).
An originality of this work lies in the fact
that, in contrast to approaches working with more
closely related comparable corpora (Barzilay and
Lee, 2003; Shinyama and Sekine, 2003; Elhadad
and Sutaria, 2007), we also gathered comparable
corpora of documents which, although addressing
the same general topics (nicotine addiction, dia-
betes), were a priori rather different since coming
from various sources and targeted to different pop-
ulations. We showed that simple paraphrase ac-
quisition methods could also work on documents
with a lesser degree of similarity, once similar seg-
ments were detected. Indeed the precision of the
extracted paraphrases is within the same range for
the three corpora we built, despite the fact that one
corpus (the cancer corpus) was composed of more
similar documents than the other two.
We extracted a type of paraphrases much less
exploited in existing work, with the exception of
(Elhadad and Sutaria, 2007), that is paraphrases
between specialized and lay language. This meant
that we had to take into account what kind of
paraphrases might be relevant, therefore the meth-
ods used to extract them were more constrained
and supervised than approaches aiming at detect-
ing any type of paraphrases. We based a part of
our work on the hypothesis that among relevant
types were paraphrases involving nominalizations
of verbal contructions, meaning that lay texts tend
to use verb phrases where specialized texts use
deverbal noun contructions. Our results seem to
support this hypothesis. Such paraphrases there-
fore seem to be interesting advice to give to au-
thors of lay texts. Future work includes testing
our method on English and comparing the results
for the two languages. We would expect them to
be fairly similar since the tendency to use nominal
constructions in scientific literature has also been
observed for English (Fang, 2005). The second
part of our work exploited the hypothesis that lay
texts use modern-language expressions where spe-
cialized texts use neo-classical compound words.
In this case, the paraphrases were too few to en-
able us to draw a significative conclusion. Testing
this method on different and larger corpora might
give more insight into the relevance of extracting
this type of paraphrases. As it is, this work is still
experimental and needs to be further investigated.
8
Diabetes Nicotine addiction Cancer
S?L L?S S?L L?S S?L L?S
# paraphrases 44 37 140 76 73 57
(|ParNs?Vl | or |ParNl?Vs |)
# expected paraphrases 712 695 1675 1626 770 772
(|ExpParNs?Vl | or |ExpParNl?Vs |)
Conditional Probability 0.062 0.053 0.084 0.047 0.095 0.074
(P (Vl|Ns) or P (Vs|Nl))
Table 9: Conditional probability for nominalization paraphrases in both directions, specialized-lay
(S?L) and lay-specialized (L?S)
Diabetes Nicotine addiction Cancer
S?L L?S S?L L?S S?L L?S
# paraphrases 53 40 18 0 3 0
(|ParCs?Ml | or |ParCl?Ms |)
# expected paraphrases 686 675 196 178 1482 1479
(|ExpParCs?Ml | or |ExpParCl?Ms |)
Conditional Probability 0.074 0.059 0.092 0 0.002 0
(P (Ml|Cs) or P (Ms|Cl))
Table 10: Conditional probability for paraphrases of neo-classical compounds in both directions
Its major drawback is the low number of para-
phrases, in particular for the paraphrases of neo-
classical compounds which brought inconclusive
results. In order to gain insight on the low quan-
tity of paraphrases of neo-classical compounds,
we manually looked at sample text segments from
the nicotine addiction and cancer corpora (the
two corpora where very few paraphrases were ex-
tracted) and could not find any paraphrase of neo-
classical compounds. This would seem to indicate
that the low quantity of this type of paraphrases
is due to the characteristics of the corpora rather
than to defects of our extraction technique. As
for the nominalization paraphrase, even though the
method brought more paraphrases and gave en-
couraging results, their quantity is still quite small.
The recall computed on a sample of segment pairs
is low. This is mainly due to the fact that we set up
rather rectricted paraphrasing patterns. This was
done to ensure a high precision but caused the re-
call to fall. A future step would be to improve re-
call by modifying some aspects of the paraphras-
ing patterns while trying to keep a good precision.
Regardless of recall, the number of nominaliza-
tion paraphrases in itself is also small. This can
be due to the fact that we restrict ourselves to one
specific type of paraphrases, but also to the facts
that we first align and select similar text segments,
that the coverage of our corpora might not be suffi-
cient, and that we work on comparable corpora of
lesser similarity than other methods. Future work
to increase the number of paraphrases involves us-
ing clusters of text segments instead of pairs, in-
creasing the corpus sizes and developing methods
to detect other types of paraphrases besides the
two kinds investigated here.
5 Conclusion
We presented a method based on comparable med-
ical corpora to extract paraphrases between spe-
cialized and lay languages. We identified two
kinds of paraphrases, nominalization paraphrases
and paraphrases of neo-classical compounds, the
first type seeming to indeed reflect some of the
systematic differences between specialized and
lay texts while the second type brought too few
results to draw a signicative conclusion.
References
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Pro-
ceedings of the 43rd Annual Meeting on Association
for Computational Linguistics, pages 597?604.
Regina Barzilay and Lillian Lee. 2003. Learn-
ing to paraphrase: An unsupervised approach us-
9
ing multiple-sequence alignment. In HLT-NAACL,
pages 16?23, Edmonton, Canada.
Regina Barzilay and Kathleen McKeown. 2001. Ex-
tracting paraphrases from a parallel corpus. In
ACL/EACL, pages 50?57.
Regina Barzilay. 2003. Information Fusion for Mul-
tidocument Summarization: Paraphrasing and Gen-
eration. Ph.D. thesis, Columbia University.
Yun-Chuang Chiao and Pierre Zweigenbaum. 2002.
Looking for French-English translations in compa-
rable medical corpora. In Proc AMIA Symp, pages
150?4.
Louise Del?ger and Pierre Zweigenbaum. 2008a.
Aligning lay and specialized passages in compara-
ble medical corpora. In Stud Health Technol Inform,
volume 136, pages 89?94.
Louise Del?ger and Pierre Zweigenbaum. 2008b.
Paraphrase acquisition from comparable medical
corpora of specialized and lay texts. In Proceedings
of the AMIA Annual Fall Symposium, pages 146?
150, Washington, DC.
Noemie Elhadad and Komal Sutaria. 2007. Min-
ing a lexicon of technical terms and lay equivalents.
In ACL BioNLP Workshop, pages 49?56, Prague,
Czech Republic.
Zhihui Fang. 2005. Scientific literacy: A systemic
functional linguistics perspective. Science Educa-
tion, 89(2):335?347.
Nabil Hathout, Fiammetta Namer, and Georgette Dal.
2002. An Experimental Constructional Database:
The MorTAL Project. In Many Morphologies, pages
178?209.
Marti A. Hearst. 1997. Texttiling: Segmenting text
into multi-paragraph subtopic passages. Computa-
tional Linguistics, 23(1):33?64.
Ali Ibrahim, Boris Katz, and Jimmy Lin. 2003. Ex-
tracting structural paraphrases from aligned mono-
lingual corpora. In Proceedings of the second inter-
national workshop on Paraphrasing, pages 57?64,
Sapporo, Japan. Association for Computational Lin-
guistics.
Christian Jacquemin. 1999. Syntagmatic and paradig-
matic representations of term variation. In Pro-
ceedings of the 37th annual meeting of the Asso-
ciation for Computational Linguistics on Compu-
tational Linguistics, pages 341?348, College Park,
Maryland.
Adam Kilgarriff and Gregory Grefenstette. 2003. In-
troduction to the special issue on the web as corpus.
Computational Linguistics, 29(3):333?47.
Aur?lien Max. 2008. Local rephrasing suggestions for
supporting the work of writers. In Proceedings of
GoTAL, Gothenburg, Sweden.
Fiammetta Namer and Pierre Zweigenbaum. 2004.
Acquiring meaning for French medical terminology:
contribution of morphosemantics. In Marius Fi-
eschi, Enrico Coiera, and Yu-Chuan Jack Li, editors,
MEDINFO, pages 535?539, San Francisco.
Marius Pasca and Peter Dienes. 2005. Aligning nee-
dles in a haystack: Paraphrase acquisition across the
web. In Proceedings of IJCNLP, pages 119?130.
Reinhard Rapp. 1995. Identifying word translations
in non-parallel texts. In Proceedings of the 33rd an-
nual meeting on Association for Computational Lin-
guistics, pages 320?322.
Yusuke Shinyama and Satoshi Sekine. 2003. Para-
phrase acquisition for information extraction. In
Proceedings of the second international workshop
on Paraphrasing (IWP), pages 65?71, Sapporo,
Japan.
10
Looking for candidate translational equivalents in specialized, comparable
corpora
Yun-Chuang Chiao and Pierre Zweigenbaum
STIM/DSI, Assistance Publique ? H?pitaux de Paris &
D?partement de Biomath?matiques, Universit? Paris 6
Abstract
Previous attempts at identifying translational equiv-
alents in comparable corpora have dealt with very
large ?general language? corpora and words. We ad-
dress this task in a specialized domain, medicine,
starting from smaller non-parallel, comparable cor-
pora and an initial bilingual medical lexicon. We
compare the distributional contexts of source and
target words, testing several weighting factors and
similarity measures. On a test set of frequently oc-
curring words, for the best combination (the Jaccard
similarity measure with or without tf:idf weight-
ing), the correct translation is ranked first for 20% of
our test words, and is found in the top 10 candidates
for 50% of them. An additional reverse-translation
filtering step improves the precision of the top can-
didate translation up to 74%, with a 33% recall.
1 Introduction
One of the issues that have to be addressed
in cross-language information retrieval (CLIR,
Grefenstette (1998b)) is that of query transla-
tion, which relies on some form of bilingual
lexicon. Methods have been proposed to ac-
quire a lexicon from corpora when such a lex-
icon does not exist or is not complete enough
(Fung and McKeown, 1997; Fung and Yee, 1998;
Picchi and Peters, 1998; Rapp, 1999). The present
work addresses this issue in a specialized domain:
medicine. We aim at identifying French-English
translation candidates from comparable medical
corpora, extending an existing specialized bilingual
lexicon. These translational equivalents may then
be used, e.g., for query expansion and translation.
We first recall previous work on this topic, then
present the corpora and initial bilingual lexicon we
start with, and the method we use to build, trans-
fer and compare context vectors. We finally pro-
vide and discuss experimental results on a test set
of French medical words.
2 Background
Salton (1970) first demonstrated that with carefully
constructed thesauri, cross-language retrieval can
perform as well as monolingual retrieval. In many
experiments, parallel corpora have been used for
training statistical models for bilingual lexicon com-
pilation and disambiguation of query translation
(Hiemstra et al, 1997; Littman et al, 1998). A lim-
iting factor in these experiments was an expensive
investment of human effort for collecting large-size
parallel corpora, although Chen and Nie (2000)?s
experiments show a potential solution by automati-
cally collecting parallel Web pages.
Comparable corpora are ?texts which, though
composed independently in the respective lan-
guage communities, have the same communica-
tive function? (Laffling, 1992). Such non-parallel
texts can become prevalent in the development
of bilingual lexicons and in cross-language infor-
mation research as they may be easier to col-
lect than parallel corpora (Fung and Yee, 1998;
Rapp, 1999; Picchi and Peters, 1998). Among
these, Rapp (1999) proposed that in any language
there is a correlation between the cooccurrences
of words which are translations of each other.
Fung and Yee (1998) demonstrated that the asso-
ciations between a word and its context seed
words are preserved in comparable texts of dif-
ferent languages. By designing procedures to
retrieve crosslingual lexical equivalents together,
Picchi and Peters (1998) proposed that their system
could have applications such as retrieving docu-
ments containing terms or contexts which are se-
mantically equivalent in more than one language.
3 Collecting comparable medical corpora
The material for the present experiments con-
sists of comparable medical corpora in French
and English and a French-English medical lexicon
(Fung and Yee (1998) call its words ?seed words?).
3.1 ?Signs and Symptoms? Corpora
We selected two medical corpora from Inter-
net catalogs of medical web sites. Some of
these catalogs index web pages with controlled
vocabulary keywords taken from the MeSH
thesaurus (www.nlm.nih.gov/mesh/meshhome),
among which CISMeF (French language med-
ical web sites, www.chu-rouen.fr/cismef) and
CliniWeb (English language medical web sites,
www.ohsu.edu/cliniweb). The MeSH thesaurus
is hierarchically structured, so that it is easy to
select a subfield of medicine. We chose the subtree
under the MeSH concept ?Pathological Conditions,
Signs and Symptoms? (?C23?), which is the best
represented in CISMeF.
We compiled the 2,338 URLs indexed by CIS-
MeF under that concept, and downloaded the cor-
responding pages, plus the pages directly linked to
them, so that framesets or tables of contents be ex-
panded. 9,787 pages were converted into plain text
from HTML or PDF, yielding a 602,484-word cor-
pus (41,295 unique words). The initial pages should
all be in French; the additional pages sometimes
happen to be foreign language versions of the ini-
tial ones. In the same line, we collected 2,019
pages under 921 URLs indexed by CliniWeb, and
obtained a 608,320-word English medical corpus
(32,919 unique words).
3.2 Base bilingual medical lexicon
A base French-English lexicon of simple words
was compiled from several sources. On the one
hand, an online French medical dictionary (Diction-
naire M?dical Masson, www.atmedica.com) which
includes English translations of most of its en-
tries. On the other hand, some international medical
terminologies which are available in both English
and French. We obtained these from the UMLS
metathesaurus, which includes French versions of
MeSH, WHOART, ICPC and their English coun-
terparts (www.nlm.nih.gov/research/umls). The re-
sulting lexicon (see excerpt in table 1) contains
18,437 entries, mainly specialized medical terms.
When several translations of the same term are
available, they are all listed.
4 Methods
The basis of the method is to find the target words
that have the most similar distributions with a given
source word. We explain how distributional behav-
ior is approximated through context vectors, how
abarognosie abarognosis
abarthrose abarthrosis
abarticulaire abarticular
abasie abasia
abattement prostration
abaxial abaxial
abc?d? abscessed
abc?s abscess
abdomen abdomen, belly
abdominal abdominal
abdomino-g?nital abdominogenital
abdomino-thoracique abdominothoracic
abdomino-v?sical abdominovesical
abducteur abducens, abducent
Table 1: Lexicon excerpt
context vectors are transferred into target context
vectors, and how context vectors are compared.
4.1 Computing context vectors
Each input corpus is segmented at non-
alphanumeric characters. Stop words are then
removed, and a simple lemmatization is per-
formed. For English, we used a list of stop
words that we had from a former project. For
French, we merged Savoy?s online stop words list
(www.unine.ch/info/clef) with a list of our own.
The S-stemmer algorithm (Harman, 1991) was
applied to the English words. Another simple
stemmer was used for French; it handles some -s
and -x endings.
The context of occurrence of each word is then
approximated by the bag of words that occur within
a window of N words around any occurrence of that
?pivot? word. In the experiments reported here, N
was set to 3 (i.e., a seven-word window) to approxi-
mate syntactic dependencies. The context vector of
a pivot word j is the vector of all words in the cor-
pus,1 where each word i is represented by its num-
ber of occurrences occj
i
in that bag of words.
A context vector is similar to a document (the
document that would be produced by concatenating
the windows around all the occurrences of the given
pivot word). Therefore, weights that are used for
words in documents can be tested here in order to
eliminate word-frequency effects and to emphasize
significant word pairs. Besides simple context fre-
quency occj
i
, two additional, alternative weights are
computed: tf:idf and log likelihood.
1We shall see below that actually, only a subset of the corpus
words will be kept in each vector.
The formulas we used to compute tf:idf are the
following: the normalized frequency of a word i
in a context j is tf j
i
=
occ
j
i
max occ
where occj
i
is the
number of occurrences of word i in the context of j
and max
occ
= max
ij
occ
j
i
is the maximum number
of cooccurrences of any two words in the corpus;
idf
i
= 1 + log
max
occ
occ
i
(Sparck Jones, 1979) where
occ
i
is the total number of contexts in which i oc-
curs in the corpus.
For the computation of the log likelihood ratio,
we used the following formula from Dunning:2
loglike(a; b) =
P
ij
log
k
ij
N
C
i
R
j
= k
11
log
k
11
N
C
1
R
1
+
k
12
log
k
12
N
C
1
R
2
+ k
21
log
k
21
N
C
2
R
1
+ k
22
log
k
22
N
C
2
R
2
;
C
1
= k
11
+k
12
, C
2
= k
21
+k
22
, R
1
= k
11
+k
21
,
R
2
= k
12
+ k
22
, N = k
11
+ k
12
+ k
21
+ k
22
;
k
11
= # cooccurrences of word a and word b,
k
12
= occ
a
  k
11
, k
21
= occ
b
  k
11
,
k
22
= corpus size ? k
12
? k
21
+ k
11
.
At the end of this step, each non-stop word in
both corpora has a weighted context vector.
4.2 Transferring context vectors
When a translation is sought for a source word, its
context vector is transferred into a target language
context vector, relying on the existing bilingual lex-
icon. Only the words in the bilingual lexicon can
be used in the transfer. When several translations
are listed, only the first one is added to the target
context vector. The result is a target-language con-
text vector which is comparable to ?native? context
vectors directly obtained from the target corpus.
Let us now be more precise about the context-
word space. Since we want to compare context
vectors obtained through transfer with native con-
text vectors, these two sorts of vectors should be-
long to the same space, i.e., range over the same
set of context words. A (target) word belongs to
this set iff (i) it occurs in the target corpus, (ii) it
is listed in the bilingual lexicon, and (iii) (one of)
its source counterpart(s) occurs in the source cor-
pus. This set corresponds to the ?seed words? of
Fung and Yee (1998). Therefore, the dimension of
the target context vectors is reduced to this set of
?cross-language pivot words?. In our experimental
setting, 4,963 pivot words are used.
4.3 Computing vector similarity
Given a transferred context vector, for each native
target vector, a similarity score is computed; a rank-
2Posted on the ?corpora? mailing list on 22/7/1997
(helmer.hit.uib.no/corpora/1997-2/0148.html).
ing list is built according to this score. The tar-
get words that ?own? the best-ranked target vectors
are the words in the target corpus whose distribu-
tions with respect to the bilingual pivot words are
the most similar to that of the source word; they are
considered candidate translational equivalents.
We used several similarity metrics for compar-
ing pairs of vectors V and W (of length n): Jac-
card (Romesburg, 1990) and cosine (Losee, 1998),
each combined with the three different weighting
schemes. With k; l;m ranging from 1 to n:
Jaccard(V;W ) =
P
k
v
k
w
k
P
k
v
2
k
+
P
l
w
2
l
 
P
m
v
m
w
m
cos(V;W ) =
P
k
v
k
w
k
p
P
k
v
2
k
p
P
l
w
2
l
4.4 Experiments
The present work performs a first evaluation of this
method in a favorable, controlled setting. It tests, in
a ?leave-one-out? style, whether the correct transla-
tion of one of the source (French) words in the bilin-
gual lexicon can be found among the target (En-
glish) words of this lexicon, based on context vector
similarity. To make similarity measures more re-
liable, we selected the most frequent words in the
English corpus (N
occ
> 100) whose French trans-
lations were known in our lexicon. Among these,
we chose the most frequent ones (N
occ
> 60) in
the French corpus. This provides us with a test set
of 95 French words (i) which are frequent in the
French corpus, (ii) of which we know the correct
translation, and (iii) such that this translation oc-
curs often in the English corpus. For each of the
French test words, we computed a weighted con-
text vector for each of the different weighting mea-
sures (occj
i
, tf:idf , log likelihood). Then, using the
above-mentioned similarity measures (cosine, Jac-
card), we compared this weighted vector with the
set of cross-language pivot words?s context vectors
computed from the English corpus. We then pro-
duced a ranked list of the top translational equiv-
alents and tested whether the expected translation
can be differentiated from other well-known domain
words. For the evaluation, we computed the rank of
the expected translation of each test word and syn-
thesized them as a percentile rank distribution.
5 Initial Results
Table 2 shows example results for the French words
anxi?t? and infection with different weightings and
similarity measures. For reasons of space, we only
Meas. Weight Fr word En word R Top 5 ranked candidate translations
Cos. occj
i
anxi?t? anxiety 1 anxiety .55, depression .45, medication .36, insomnia .36, memory .34
Cos. tf:idf anxi?t? anxiety 1 anxiety .54, depression .41, eclipse .33, medication .29, psychiatrist .29
Cos. loglike anxi?t? anxiety 1 anxiety .56, depression .43, eclipse .37, psychiatrist .36, dysthymia .33
Jac. occj
i
anxi?t? anxiety 2 memory .21, anxiety .21, insomnia .19, confusion .19, psychiatrist .18
Jac. tf:idf anxi?t? anxiety 1 anxiety .21, psychiatrist .17, confusion .15, memory .14, phobia .14
Jac. loglike anxi?t? anxiety 1 anxiety .26, psychiatrist .19, memory .15, phobia .14, depressed .14
Cos. occj
i
infection infection 2 infected .55, infection .52, neurotropic .47, homosexual .43
Cos. tf:idf infection infection 3 infected .56, neurotropic .49, infection .48, aids .45, homosexual .41
Cos. loglike infection infection 2 infected .67, infection .55, neurotropic .53, aids .48, homosexual .48
Jac. occj
i
infection infection 1 infection .33, aids .21, tract .17, positive .16, prevention .15
Jac. tf:idf infection infection 1 infection .27, aids .24, positive .17, hiv .15, virus .15
Jac. loglike infection infection 1 infection .38, aids .27, tract .18, infected .18, positive .17
Table 2: Example results; R = rank of expected target English word for source French word
print out the top 5 ranked words. Rank refers to the
performance of our program, with a 1 meaning that
the correct translation of the input French word was
found as the first candidate.
0
10
20
30
40
50
60
0 5 10 15 20
cosine/tf.idf
Jaccard/tf.idf
cosine/loglike
Jaccard/loglike
cosine/occ
Jaccard/occ
Figure 1: Percentile rank of the measures.
A percentile rank (figure 1) showed that using the
combination of occj
i
and Jaccard, about 20% of the
French test words have their correct translation as
the first ranked word. If we look at the best ranked
words, we find that they have a strong thematic rela-
tion: e.g., anxiety, depression, psychiatrist, phobia,
or infection, infected, aids, homosexual.
6 Discussion and Improvement Directions
As the percentile rank figure showed, the combi-
nation of context frequency weighting (occj
i
) and
Jaccard gives an accuracy of about 20% for cor-
rect translation which is followed by tf:idf /Jaccard
measures. However, if we look among the top 20
ranked words, we can find that the tf:idf /Jaccard
and tf:idf /cosine have better performance: more
than 60% of the words find their correct transla-
tions within the top 20 words, which is much better
than occj
i
/Jaccard and occj
i
/cosine. It seems that the
loglike weighting factor did not help to improve the
translation performance; this is true when we com-
bined it with the cosine measure, but with Jaccard,
we can see an improvement at the 20th percentile.
In some cases where the correct translation was
badly ranked, the French test words have different
usages, which induces an important context diver-
sity. For instance, for the French word chirurgie
whose expected translation is surgery, we have as
top ranked words pain, breast, desmoplasia, pro-
cedure, metastatic..., and for m?decine (medicine),
we have information, clinician, article, medical....
For common words like, e.g., analyse/analysis and
sang/blood, we have girdle, sample, statistic... for
analysis and output, collection, calorimetry... for
blood as best ranked translations.
As an attempt to improve the precision of the
French-English translation method, the same model
was applied in the reverse direction to find the
French counterparts of the 10 top-scoring English
candidates. We then kept only those English candi-
dates that had the initial French source word among
their top 10 reverse translation candidates. In the
present settings, only 42 of the 95 French source
words remained, 38 of which kept exactly one En-
glish candidate; among these, 27 are the expected
translation, and 1 is an adjective derived from the
expected translation (estomac/gastric). The other
4 words still have multiple translation candidates,
which can be ordered according to their combined
similarity scores: for 2 of them, the top ranked can-
didate is then correct, and 1 is a derived adjective
(th?rapie/therapeutic).
Altogether, if we propose the top ranked re-
maining candidate according to this scheme, re-
call/precision reach .31/.69, or .33/.74 if derived ad-
jectives are considered acceptable. This result is re-
ally encouraging as it shows that the reverse appli-
cation of the translation method to the English can-
didate words improves its effectiveness.
As a comparison, on a ?general language? cor-
pus, Rapp (1999) reports an accuracy of 65% at
the first percentile by using loglike weighting and
city-block metric.3 This difference in accuracy
may be accounted for by the larger size of the
corpora (135 and 163 Mwords), the use of a
general English-German lexicon (16,380 entries),
and the consideration of word order within con-
texts. In Fung and McKeown (1997), a transla-
tion model applied to a pair of unrelated languages
(English/Japanese) with a random selection of test
words, many of them multi-word terms, gives a pre-
cision around 30% when only the top candidate is
proposed.
Our bilingual lexicon does not include general
French and English words. This implies that some
contexts are ignored: all cooccurrences of a special-
ized word with a general word are lost in our case.
We therefore plan to explore the effectiveness of in-
corporating a general lexicon, as well as applying
POS-tagging to the corpus. An additional differ-
ence with Fung and Yee (1998) is that they look for
translational equivalents only among words that are
unknown in both corpora. This additional condition
might also help to improve our current results.
7 Acknowledgements
We thank Jean-David Sta, Julien Quint and Beno?t
Habert for their help during this work.
References
Jiang Chen and J-Y. Nie. 2000. Parallel web text
mining for cross-language IR. In Proceedings
of RIAO 2000: Content-Based Multimedia In-
formation Access, volume 1, pages 62?78, Paris,
France, April. C.I.D.
3The city-block metric is computed as the sum of the abso-
lute differences of corresponding vectors positions.
Pascale Fung and Kathleen McKeown. 1997. Find-
ing terminology translations from non-paralle
corpora. In Proceedings of the 5th Annual Work-
shop on Very Large Copora, volume 1, pages
192?202, Hong Kong.
Pascale Fung and L. Y. Yee. 1998. An IR ap-
proach for translating new words from non-
parallel, comparable texts. In Proceedings of the
36th ACL, pages 414?420, Montr?al, August.
Gregory Grefenstette. 1998a. Cross-Language In-
formation Retrieval. Kluwer Academic Publish-
ers, London.
Gregory Grefenstette. 1998b. The problem
of cross-language information retrieval.
In Cross-Language Information Retrieval
(Grefenstette, 1998a), pages 1?9.
D. Harman. 1991. How effective is suffixing. Jour-
nal of the American Society for Information Sci-
ence, 42:7?15.
D. Hiemstra, F. de Jong, and W. Kraaij. 1997. A
domain specific lexicon acquisition tool for cross-
linguage information retrieval. In Proceedings of
RIAO97, pages 217?232, Montreal, Canada.
J. Laffling. 1992. On constructiong a transfer dic-
tionary for man and machine. Target, 4(1):17?31.
M.L. Littman, S.T. Dumais, and T.K. Landauer.
1998. Automatic cross-language information re-
trieval using latent semantic indexing. In Grefen-
stette (Grefenstette, 1998a), chapter 5, pages 51?
62.
Robert M. Losee. 1998. Text Retrieval and Filter-
ing: Analytic Models of Performance, volume 3
of Information Retrieval. Kluwer Academic Pub-
lishers, Dordrecht & Boston.
E. Picchi and C. Peters. 1998. Cross-language
information retrieval: A system for com-
parable corpus querying. In Grefenstette
(Grefenstette, 1998a), chapter 7, pages 81?90.
Reinhard Rapp. 1999. Automatic identification
of word translations from unrelated English and
German corpora. In Proceedings of the 37th
ACL, College Park, Maryland, June.
H. Charles Romesburg. 1990. Cluster Analysis for
Researchers. Krieger, Malabar, FL.
Gerald Salton. 1970. Automatic processing of for-
eign language documents. Journal of the Ameri-
can Society for Information Science, 21(3):187?
194.
Karen Sparck Jones. 1979. Experiments in rel-
evance weighting of search terms. Information
Processing and Management, 15:133?144.
Accenting unknown words in a specialized language
Pierre Zweigenbaum and Natalia Grabar
DIAM ? STIM/DSI, Assistance Publique ? H?pitaux de Paris
& D?partement de Biomath?matiques, Universit? Paris 6
{ngr,pz}@biomath.jussieu.fr
Abstract
We propose two internal methods for ac-
centing unknown words, which both learn
on a reference set of accented words the
contexts of occurrence of the various ac-
cented forms of a given letter. One method
is adapted from POS tagging, the other is
based on finite state transducers.
We show experimental results for letter
e on the French version of the Medical
Subject Headings thesaurus. With the
best training set, the tagging method ob-
tains a precision-recall breakeven point
of 84.24.4% and the transducer method
83.84.5% (with a baseline at 64%) for
the unknown words that contain this let-
ter. A consensus combination of both in-
creases precision to 92.03.7% with a re-
call of 75%. We perform an error analysis
and discuss further steps that might help
improve over the current performance.
1 Introduction
The ISO-latin family, Unicode or the Universal
Character Set have been around for some time now.
They cater, among other things, for letters which can
bear different diacritic marks. For instance, French
uses four accented es (????) besides the unaccented
form e. Some of these accented forms correspond to
phonemic differences. The correct handling of such
accented letters, beyond US ASCII, has not been
immediate and general. Although suitable charac-
ter encodings are widely available and used, some
texts or terminologies are still, for historical rea-
sons, written with unaccented letters. For instance,
in the French version of the US National Library
of Medicine?s Medical Subject Headings thesaurus
(MeSH, (INS, 2000)), all the terms are written in
unaccented uppercase letters. This causes difficul-
ties when these terms are used in Natural Language
interfaces or for automatically indexing textual doc-
uments: a given unaccented word may match several
words, giving rise to spurious ambiguities such as,
e.g., marche matching both the unaccented marche
(walking) and the accented march? (market).
Removing all diacritics would simplify match-
ing, but would increase ambiguity, which is al-
ready pervasive enough in natural language pro-
cessing systems. Another of our aims, besides,
is to build language resources (lexicons, morpho-
logical knowledge bases, etc.) for the medi-
cal domain (Zweigenbaum, 2001) and to learn lin-
guistic knowledge from terminologies and cor-
pora (Grabar and Zweigenbaum, 2000), including
the MeSH. We would rather work, then, with lin-
guistically sound data in the first place.
We therefore endeavored to produce an accented
version of the French MeSH. This thesaurus in-
cludes 19,971 terms and 9,151 synonyms, with
21,475 different word forms. Human reaccentua-
tion of the full thesaurus is a time-consuming, error-
prone task. As in other instances of preparation of
linguistic resources, e.g., part-of-speech-tagged cor-
pora or treebanks, it is generally more efficient for a
human to correct a first annotation than to produce
it from scratch. This can also help obtain better con-
sistency over volumes of data. The issue is then to
find a method for (semi-)automatic accentuation.
The CISMeF team of the Rouen University Hos-
                                            Association for Computational Linguistics.
                            the Biomedical Domain, Philadelphia, July 2002, pp. 21-28.
                         Proceedings of the Workshop on Natural Language Processing in
pital already accented some 5,500 MeSH terms
that are used as index terms in the CISMeF online
catalog of French-language medical Internet sites
(Darmoni et al, 2000) (www.chu-rouen.fr/cismef).
This first means that less material has to be reac-
cented. Second, this accented portion of the MeSH
might be usable as training material for a learning
procedure.
However, the methods we found in the literature
do not address the case of ?unknown? words, i.e.,
words that are not found in the lexicon used by the
accenting system. Despite the recourse to both gen-
eral and specialized lexicons, a large number of the
MeSH words are in this case, for instance those in
table 1. One can argue indeed that the compila-
cryomicroscopie dactylolyse
decarboxylases decoquinate
denitrificans deoxyribonuclease
desmodonte desoxyadrenaline
dextranase dichlorobenzidine
dicrocoeliose diiodotyrosine
dimethylamino dimethylcysteine
dioctophymatoidea diosgenine
Table 1: Unaccented words not in lexicon.
tion of a larger lexicon should reduce the propor-
tion of unknown words. But these are for the most
part specialized, rare words, some of which we did
not find even in a large reference medical dictionary
(Garnier and Delamare, 1992). It is then reasonable
to try to accentuate automatically these unknown
words to help human domain experts perform faster
post-editing. Moreover, an automatic accentuation
method will be reusable for other unaccented textual
resources. For instance, the ADM (Medical Diagno-
sis Aid) knowledge base online at Rennes University
(Seka et al, 1997) is another large resource which is
still in unaccented uppercase format.
We first review existing methods (section 2). We
then present two trainable accenting methods (sec-
tion 3), one adapted from part-of-speech tagging, the
other based on finite-state transducers. We show ex-
perimental results for letter e on the French MeSH
(section 4) with both methods and their combina-
tion. We finally discuss these results (section 5) and
conclude on further research directions.
2 Background
Previous work has addressed text accentuation, with
an emphasis on the cases where all possible words
are assumed to be known (listed in a lexicon). The
issue in that case is to disambiguate unaccented
words when they match several possible accented
word forms in the lexicon ? the marche/march? ex-
amples in the introduction.
Yarowsky (1999) addresses accent restoration in
Spanish and in French, and notes that they can be
linked to part-of-speech ambiguities and to seman-
tic ambiguities which context can help to resolve.
He proposes three methods to handle these: N-gram
tagging, Bayesian classification and decision lists,
which obtain the best results. These methods rely
either on full words, on word suffixes or on parts-
of-speech. They are tested on ?the most problem-
atic cases of each ambiguity type?, extracted from
the Spanish AP Newswire. The agreement with hu-
man accented words reaches 78.4?98.4% depending
on ambiguity type.
Spriet and El-B?ze (1997) use an N-gram model
on parts-of-speech. They evaluate this method on a
19,000 word test corpus consisting of news articles
and obtain a 99.31% accuracy. In this corpus, only
2.6% of the words were unknown, among which
89.5% did not need accents. The resulting error rate
(0.3%) accounts for nearly one half of the total er-
ror rate, but is so small that it is not worth trying to
guess accentuation for unknown words.
The same kind of approach is used in project
R?ACC (Simard, 1998). Here again, unknown
words are left untouched, and account for one fourth
of the errors. We typed the words in table 1
through the demonstration interface of R?ACC on-
line at www-rali.iro.umontreal.ca/Reacc/: none of
these words was accented by the system (7 out of
16 do need accentuation).
When the unaccented words are in the lexicon,
the problem can also be addressed as a spelling cor-
rection task, using methods such as string edit dis-
tance (Levenshtein, 1966), possibly combined with
the previous approach (Ruch et al, 2001).
However, these methods have limited power when
a word is not in the lexicon. At best, they might say
something about accented letters in grammatical af-
fixes which mark contextual, syntactic constraints.
We found no specific reference about the accentua-
tion of such ?unknown? words: a method that, when
a word is not listed in the lexicon, proposes an ac-
cented version of that word. Indeed, in the above
works, the proportion of unknown words is too small
for specific steps to be taken to handle them. The sit-
uation is quite different in our case, where about one
fourth of the words are ?unknown?. Moreover, con-
textual clues are scarce in our short, often ungram-
matical terms.
We took obvious measures to reduce the number
of unknown words: we filtered out the words that
can be found in accented lexicons and corpora. But
this technique is limited by the size of the corpus that
would be necessary for such ?rare? words to occur,
and by the lack of availability of specialized French
lexicons for the medical domain.
We then designed two methods that can learn ac-
centing rules for the remaining unknown words: (i)
adapting a POS-tagging method (Brill, 1995) (sec-
tion 3.3); (ii) adapting a method designed for learn-
ing morphological rules (Theron and Cloete, 1997)
(section 3.4).
3 Accenting unknown words
3.1 Filtering out know words
The French MeSH was briefly presented in the in-
troduction; we work with the 2001 version. The part
which was accented and converted into mixed case
by the CISMeF team is that of November 2001. As
more resources are added to CISMeF on a regular
basis, a larger number of these accented terms must
now be available. The list of word forms that oc-
cur in these accented terms serves as our base lex-
icon (4861 word forms). We removed from this
list the ?words? that contain numbers, those that are
shorter than 3 characters (abbreviations), and con-
verted them in lower case. The resulting lexicon in-
cludes 4054 words (4047 once unaccented). This
lexicon deals with single words. It does not try to
register complex terms such as myocardial infarc-
tion, but instead breaks them into the two words my-
ocardial and infarction.
A word is considered unknown when it is not
listed in our lexicon. A first concern is to filter out
from subsequent processing words that can be found
in larger lexicons. The question is then to find suit-
able sources of additional words.
We used various specialized word lists found on
the Web (lexicon on cancer, general medical lex-
icon) and the ABU lexicon (abu.cnam.fr/DICO),
which contains some 300,000 entries for ?gen-
eral? French. Several corpora provided accented
sources for extending this lexicon with some med-
ical words (cardiology, haematology, intensive care,
drawn from the current state of the CLEF corpus
(Habert et al, 2001), and drug monographs). We
also used a word list extracted from the French ver-
sions of two other medical terminologies: the In-
ternational Classification of Diseases (ICD-10) and
the Microglossary for Pathology of the Systematized
Nomenclature of Medicine (SNOMED). This word
list contains 8874 different word forms. The total
number of word forms of the final word list was
276 445.
After application of this list to the MeSH, 7407
words were still not recognized. We converted these
words to lower case, removed those that did not in-
clude the letter e, were shorter than 3 letters (mainly
acronyms) or contained numbers. The remaining
5188 words, among which those listed in table 1,
were submitted to the following procedure.
3.2 Representing the context of a letter
The underlying hypotheses of this method are that
sufficiently regular rules determine, for most words,
which letters are accented, and that the context of
occurrence of a letter (its neighboring letters) is a
good basis for making accentuation decisions. We
attempted to compile these rules by observing the
occurrences of e???? in a reference list of words
(the training set, for instance, the part of the French
MeSH accented by the CISMeF team). In the fol-
lowing, we shall call pivot letter a letter that is part
of the confusion set e???? (set of letters to discrimi-
nate).
An issue is then to find a suitable description of
the context of a pivot letter in a word, for instance
the letter ? in excis?e. We explored and compared
two different representation schemes, which under-
lie two accentuation methods.
3.3 Accentuation as contextual tagging
This first method is based on the use of a part-of-
speech tagger: Brill?s (1995) tagger. We consider
each word as a ?string of letters?: each letter makes
one word, and the sequence of letters of a word
makes a sentence. The ?tag? of a letter is the ex-
pected accented form of this letter (or the same letter
if it is not accented). For instance, for the word en-
dometre (endometer), to be accented as endom?tre,
the ?tagged sentence? is e/e n/n d/d o/o m/m e/? t/t
r/r e/e (in the format of Brill?s tagger). The regular
procedure of the tagger then learns contextual accen-
tuation rules, the first of which are shown on table 2.
Brill Format Gloss
(1) e ? NEXT2TAG i e.i) e! ?
(2) e ? NEXT1OR2TAG o e.?o) e! ?
(3) e ? NEXT1OR2TAG a e.?a) e! ?
(4) e ? NEXT1OR2WD e e.?e) e! ?
(5) e ? NEXT2TAG h e.h) e! ?
(6) ? ? NEXTBIGRAM n e ?ne) ?! ?
(7) ? e NEXTBIGRAM m e ?me) ?! e
(8) e ? NEXTBIGRAM t r etr ) e! ?
(9) ? e NEXT1OR2OR3TAG x ?.?.?x) ?! e
(10) e ? NEXT1OR2TAG y e.?y) e! ?
(11) e ? NEXT2TAG u e.u) e! ?
(12) e ? SURROUNDTAG t i tei) e! ?
(13) ? ? NEXTBIGRAM s e ?se) ?! ?
Table 2: Accentuation correction rules, of the form
?change t
1
to t
2
if test true on x [y]?. NEXT2TAG =
second next tag, NEXT1OR2TAG = one of next 2 tags,
NEXTBIGRAM = next 2 words, NEXT1OR2OR3TAG = one
of next 3 tags, SURROUNDTAG = previous and next
tags,
Given a new ?sentence?, Brill?s tagger first assigns
each ?word? its mots frequent tag: this consists in
accenting no e. The contextual rules are then ap-
plied and successively correct the current accentu-
ation. For instance, when accenting the word flex-
ion, rule (1) first applies (if e with second next tag
= i, change to ?) and accentuates the e to yield fl?x-
ion (as in ...?mie). Rule (9) applies next (if ? with
one of next three tags = x, change to e) to correct
this accentuation before an x, which finally results
in flexion. These rules correspond to representations
of the contexts of occurrence of a letter. This rep-
resentation is mixed (left and right contexts can be
combined, e.g., in SURROUNDTAG, where both imme-
diate left and right tags are examined), and can ex-
tend to a distance of three letters left and right, but
in restricted combinations.
3.4 Mixed context representation
The ?mixed context? representation used by
Theron and Cloete (1997) folds the letters of a word
around a pivot letter: it enumerates alternately
the next letter on the right then on the left, until it
reaches the word boundaries, which are marked with
special symbols (here, ^ for start of word, and $ for
end of word). Theron & Cloete additionally repeat
an out-of-bounds symbol outside the word, whereas
we dispense with these marks. For instance, the
first e in excis?e (excised) is represented as the
mixed context in the right column of the first row of
table 3. The left column shows the order in which
the letters of the word are enumerated. The next two
rows explain the mixed context representations for
the two other es in the word. This representation
Word Mixed Context=Output
^ e x c i s ? e $
2 . 1 3 4 5 6 7 8 x
^ c i s e e $=e
^ e x c i s ? e $
8 7 6 5 4 2 . 1 3 e s $ i c x e
^
=?
^ e x c i s ? e $
8 7 6 5 4 3 2 . 1 $ e s i c x e
^
=e
Table 3: Mixed context representations.
caters for contexts of different sizes and facilitates
their comparison.
Each of these contexts is unaccented (it is meant
to be matched with representations of unaccented
words) and the original form of the pivot letter is
associated to the context as an output (we use the
symbol ?=? to mark this output). Each context is
thus converted into a transducer: the input tape is the
mixed context of a pivot letter, and the output tape is
the appropriate letter in the confusion set e????.
The next step is to determine minimal discrimi-
nating contexts (figure 1). To obtain them, we join
all these transducers (OR operator) by factoring their
common prefixes as a trie structure, i.e., a determin-
istic transducer that exactly represents the training
set. We then compute, for each state of this trans-
ducer and for each possible output (letter in the con-
fusion set) reachable from this state, the number of
paths starting from this state that lead to this output.
^allergie$, ^chirurgie$i
^r?fugi?$
^cytologie$
^?chographie$
^lipoatrophie$
r
h
u
o
e
?
e
e
$
g3 ?505 e,
65 e
6 e
1 ?
63 e
1 ?
86 e,
Figure 1: Trie of mixed contexts, each state showing
the frequency of each possible output.
We call a state unambiguous if all the paths from
this state lead to the same output. In that case, for
our needs, these paths may be replaced with a short-
cut to an exit to the common output (see figure 1).
This amounts to generalizing the set of contexts by
replacing them with a set of minimal discriminating
contexts.
Given a word that needs to be accented, the first
step consists in representing the context of each of
its pivot letters. For instance, the word biologie:
$igoloib^ . Each context is matched with the trans-
ducer in order to find the longest path from the start
state that corresponds to a prefix of the context string
(here, $igo). If this path leads to an output state, this
output provides the proposed accented form of the
pivot letter (here, e). If the match terminates earlier,
we have an ambiguity: several possible outputs can
be reached (e.g., h?morragie matches $ig).
We can take absolute frequencies into account to
obtain a measure of the support (confidence level)
for a given output O from the current state S: how
much evidence there is to support this decision. It
is computed as the number of contexts of the train-
ing set that go through S to an output state labelled
with O (see figure 1). The accenting procedure can
choose to make a decision only when the support
for that decision is above a given threshold. Table 4
Context Support Gloss Examples
$igo=e 65 ?ogie cytologie
$ih=e 63 ?hie lipoatrophie
$uqit=e 77 ?tique am?lanotique
u=e 247 -eu- activateur, calleux
x=e 68 -ex- excis?e
Table 4: Some minimal discriminating contexts.
shows some minimal discriminating contexts learnt
from the accented part of the French MeSH with a
high support threshold. However, in previous exper-
iments (Zweigenbaum and Grabar, 2002), we tested
a range of support thresholds and observed that the
gain in precision obtained by raising the support
threshold was minor, and counterbalanced by a large
loss in recall. We therefore do not use this device
here and accept any level of support.
Instead, we take into account the relative frequen-
cies of occurrence of the paths that lead to the dif-
ferent outputs, as marked in the trie. A probabilistic,
majority decision is made on that basis: if one of the
competing outputs has a relative frequency above a
given threshold, this output is chosen. In the present
experiments, we tested two thresholds: 0.9 (90% or
more of the examples must support this case; this
makes the correct decision for h?morragie) and 1
(only non-ambiguous states lead to a decision: no
decision for the first e in hemorragie, which we
leave unaccented).
Simpler context representations of the same fam-
ily can also be used. We examined right contexts
(a variable-length string of letters on the right of the
pivot letter) and left contexts (idem, on the left).
3.5 Evaluating the rules
We trained both methods, Brill and contexts (mixed,
left and right), on three training sets: the 4054 words
of the accented part of the MeSH, the 54,291 lem-
mas of the ABU lexicon and the 8874 words in the
ICD-SNOMED word list. To check the validity of
the rules, we applied them to the accented part of
the MeSH. The context method knows when it can
make a decision, so that we can separate the words
that are fully processed (f , all es have lead to deci-
sions) from those that are partially (p) processed or
not (n) processed at all. Let f
c
the number of correct
accentuations in f . If we decide to only propose an
accented form for the words that get fully accented,
we can compute recall R
f
and precision P
f
figures
as follows: R
f
=
f
c
f+p+n
and P
f
=
f
c
f
. Similar
measures can be computed for p and n, as well as
for the total set of words.
We then applied the accentuation rules to the 5188
accentable ?unknown? words of the MeSH. No gold
standard is available for these words: human vali-
dation was necessary. We drew from that set a ran-
dom sample containing 260 words (5% of the total)
which were reviewed by the CISMeF team. Because
of sampling, precision measures must include a con-
fidence interval.
We also tested whether the results of several meth-
ods can be combined to increase precision. We sim-
ply applied a consensus rule (intersection): a word
is accepted only if all the methods considered agree
on its accentuation.
The programs were developed in the Perl5 lan-
guage. They include a trie manipulation package
which we wrote by extending the Tree::Trie pack-
age, online on the Comprehensive Perl Archive Net-
work (www.cpan.org).
4 Results
The baseline of this task consists in accenting no e.
On the accented part of the MeSH, it obtains an ac-
curacy of 0.623, and on the test sample, 0.642. The
Brill tagger learns 80 contextual rules with MeSH
training (208 on ABU and 47 on CIM-SNOMED).
The context method learns 1,832 rules on the MeSH
training set (16,591 on ABU and 3,050 on CIM-
SNOMED).
Tables 5, 6 and 7 summarize the validation results
obtained on the accented part of the MeSH. Set de-
notes the subset of words as explained in section 3.5.
Cor. stands for the number of correctly accented
words.
Not surprizingly, the best global precision is ob-
tained with MeSH training (table 6). The mixed
context method obtains a perfect precision, whereas
Brill reaches 0.901 (table 5). ABU and CIM-
SNOMED training also obtain good results (table 7),
again better with the mixed context method (0.912?
0.931) than with Brill (0.871?0.895). We performed
the same tests with right and left contexts (table 6):
precision can be as good for fully processed words
(set f ) as that of mixed contexts, but recall is always
lower. The results of these two context variants are
therefore not kept in the following tables. Both pre-
cision and recall are generally slightly better with
the majority decision variant. If we concentrate on
the fully processed words (f ), precision is always
higher than the global result and than that of words
with no decision (n). The n class, whose words
are left unaccented, generally obtain a precision well
over the baseline. Partially processed words (p) are
always those with the worst precision.
training set cor. recall precisionci
MeSH 3646 0.899 0.9010.009
ABU 3524 0.869 0.8710.010
CIM-SNOMED 3621 0.893 0.8950.009
Table 5: Validation: Brill, 4054 words of accented
MeSH.
context set cor. recall precisionci
right n 1906 0.470 0.7470.017
p 943 0.233 0.8040.023
f 324 0.080 1.0000.000
tot 3173 0.783 0.7840.013
left n 743 0.183 0.6490.028
p 500 0.123 0.4280.028
f 1734 0.428 1.0000.000
tot 2977 0.734 0.7360.014
mixed n 7 0.002 1.0000.000
p 0 0.000 0.0000.000
f 4040 0.997 1.0000.000
tot 4047 0.998 1.0000.000
majority decision (0.9)
mixed n 2 0.000 1.0000.000
p 0 0.000 0.0000.000
f 4045 0.998 1.0000.000
tot 4047 0.998 1.0000.000
Table 6: Validation: different context methods,
MeSH training, 4054 words of accented MeSH.
Precision and recall for the unaccented part of
the MeSH are showed on tables 8 and 9. The
global results with the different training sets at
breakeven point, with their confidence intervals, are
not really distinguishable. They are clustered from
0.8190.047 to 0.8420.044, except the unambigu-
ous decision method trained on MeSH which stands
a bit lower at 0.8000.049 and the Brill tagger
trained on ABU (0.785). If we only consider fully
processed words, precision can reach 0.8840.043
(ICD-SNOMED training, majority decision), with a
recall of 0.731 (or 0.8760.043 / 0.758 with MeSH
training, majority decision).
Consensus combination of several methods (ta-
ble 8) does increase precision, at the expense of
recall. A precision/recall of 0.9200.037/0.750 is
ABU training (strict)
set cor. recall precisionci
n 368 0.091 0.8640.033
p 227 0.056 0.6680.050
f 3164 0.780 0.9640.006
tot 3759 0.927 0.9290.008
majority decision (0.9)
cor. recall precisionci
111 0.027 0.8600.060
77 0.019 0.5240.081
3585 0.884 0.9510.007
3773 0.931 0.9320.008
CIM-SNOMED training
n 176 0.043 0.7520.055
p 114 0.028 0.4250.059
f 3400 0.839 0.9590.007
tot 3690 0.910 0.9120.009
majority decision (0.9)
57 0.014 0.8030.093
51 0.013 0.3000.069
3607 0.890 0.9480.007
3715 0.916 0.9180.008
Table 7: Validation: mixed contexts, strict (thresh-
old = 1) and majority (threshold = 0.9) decisions,
4054 words of accented MeSH.
training set cor. recall precisionci
MeSH 219 0.842 0.8420.044
ABU 204 0.785 0.7850.050
CIM-SNOMED 218 0.838 0.8380.045
Combined methods
mesh/Brill + mesh/majority 195 0.750 0.9200.037
mesh/Brill + mesh/majority
f
185 0.712 0.9300.036
mesh+abu+cim-snomed/Brill 178 0.685 0.9270.037
+ mesh/majority
Table 8: Evaluation on the rest of the MeSH: Brill,
estimate on 5% sample (260 words).
obtained by combining Brill and the mixed context
method (majority decision), with MeSH training on
both sides. The same level of precision is obtained
with other combinations, but with lower recalls.
5 Discussion and Conclusion
We showed that a higher precision, which should
make human post-editing easier, can be obtained in
two ways. First, within the mixed context method,
three sets of words are separated: if only the ?fully
processed? words f are considered (table 9), preci-
sion/recall can reach 0.884/0.731 (CIM-SNOMED,
majority) or 0.876/0.758 (MeSH, majority). Second,
the results of several methods can be combined with
a consensus rule: a word is accepted only if all these
methods agree on its accentuation. The combination
of Brill mixed contexts (majority decision), for in-
stance with MeSH training on both sides, increases
precision to 0.9200.037 with a recall still at 0.750
(table 8).
The results obtained show that the methods pre-
sented here obtain not only good performance on
their training set, but also useful results on the tar-
MeSH training (strict)
set cor. recall precisionci
n 19 0.073 0.7310.170
p 15 0.058 0.4290.164
f 174 0.669 0.8740.046
tot 208 0.800 0.8000.049
majority decision
cor. recall precisionci
8 0.031 0.7270.263
11 0.042 0.4580.199
197 0.758 0.8760.043
216 0.831 0.8310.046
ABU training (strict)
n 30 0.115 0.8820.108
p 32 0.123 0.7110.132
f 153 0.588 0.8450.053
tot 215 0.827 0.8270.046
majority decision
13 0.050 0.9290.135
11 0.042 0.7860.215
194 0.746 0.8360.048
218 0.838 0.8380.045
CIM-SNOMED training
n 27 0.104 0.8180.132
p 19 0.073 0.4870.157
f 168 0.646 0.8940.044
tot 214 0.823 0.8230.046
majority decision
14 0.054 0.8240.181
9 0.035 0.3210.173
190 0.731 0.8840.043
213 0.819 0.8190.047
Table 9: Evaluation on the rest of the MeSH: mixed
contexts, estimate on same 5% sample.
get data. We believe these methods will allow us to
reduce dramatically the final human time needed to
accentuate useful resources such as the MeSH the-
saurus and ADM knowledge base.
It is interesting that a general-language lexicon
such as ABU can be a good training set for accent-
ing specialized-language unknown words, although
this is true with the mixed context method and the
reverse with the Brill tagger.
A study of the 44 errors made by the mixed con-
text method (table 9, MeSH training, majority deci-
sion: 216 correct out of 260) revealed the follow-
ing errors classes. MeSH terms contain some En-
glish words (academy, cleavage) and many Latin
words (arenaria, chrysantemi, denitrificans), some
of which built over proper names (edwardsiella).
These loan words should not bear accents; some of
their patterns are correctly processed by the meth-
ods presented here (i.e., unaccented eae$, ella$), but
others are not distinguishable from normal French
words and get erroneously accented (rena of are-
naria is erroneously processed as in r?nal; acad?my
as in acad?mie). A first-stage classifier might help
handle this issue by categorizing Latin (and English)
words and excluding them from processing. Our
first such experiments are not conclusive and add as
many errors as are removed.
Another class of errors are related with mor-
pheme boundaries: some accentuation rules which
depend on the start-of-word boundary would need
to apply to morpheme boundaries. For in-
stance, pilo/erection fails to receive the ? of r^e=?
(^?rection), apic/ectomie erroneously receives an ?
as in cc=? (c?cit?). An accurate morpheme seg-
menter would be needed to provide suitable input
to this process without again adding noise to it.
In some instances, no accentuation decision could
be made because no example had been learnt for a
specific context (e.g., accentuation of c?falo in ce-
faloglycine).
We also uncovered accentuation inconsistencies
in both the already accented MeSH words and the
validated sample (e.g., bacterium or bact?rium in
different compounds). Cross-checking on the Web
confirmed the variability in the accentuation of rare
words. This shows the difficulty to obtain consistent
human accentuation across large sets of complex
words. One potential development of the present au-
tomated accentuation methods could be to check the
consistency of word lists. In addition, we discovered
spelling errors in some MeSH terms (e.g., bethane-
chol instead of betanechol prevents the proper ac-
centuation of beta).
Finally, further testing is necessary to check the
relevance of these methods to other accented letters
in French and in other languages.
Acknowledgements
We wish to thank Magaly Douy?re, Beno?t Thirion
and St?fan Darmoni, of the CISMeF team, for pro-
viding us with accented MeSH terms and patiently
reviewing the automatically accented word samples.
References
[Brill1995] Eric Brill. 1995. Transformation-based error-
driven learning and natural language processing: A
case study in part-of-speech tagging. Computational
Linguistics, 21(4):543?565.
[Darmoni et al2000] St?fan J. Darmoni, J.-P. Leroy,
Beno?t Thirion, F. Baudic, Magali Douyere, and
J. Piot. 2000. CISMeF: a structured health resource
guide. Methods Inf Med, 39(1):30?35.
[Garnier and Delamare1992] M. Garnier and V. Dela-
mare. 1992. Dictionnaire des Termes de M?decine.
Maloine, Paris.
[Grabar and Zweigenbaum2000] Natalia Grabar and
Pierre Zweigenbaum. 2000. Automatic acquisition
of domain-specific morphological resources from the-
sauri. In Proceedings of RIAO 2000: Content-Based
Multimedia Information Access, pages 765?784,
Paris, France, April. C.I.D.
[Habert et al2001] Beno?t Habert, Natalia Grabar, Pierre
Jacquemart, and Pierre Zweigenbaum. 2001. Build-
ing a text corpus for representing the variety of medi-
cal language. In Corpus Linguistics 2001, Lancaster.
[INS2000] Institut National de la Sant? et de la Recherche
M?dicale, Paris, 2000. Th?saurus Biom?dical
Fran?ais/Anglais.
[Levenshtein1966] V. I. Levenshtein. 1966. Binary codes
capable of correcting deletions, insertions, and rever-
sals. Soviet Physics-Doklandy, pages 707?710.
[Ruch et al2001] Patrick Ruch, Robert H. Baud, Antoine
Geissbuhler, Christian Lovis, Anne-Marie Rassinoux,
and A. Rivi?re. 2001. Looking back or looking all
around: comparing two spell checking strategies for
documents edition in an electronic patient record. J
Am Med Inform Assoc, 8(suppl):568?572.
[Seka et al1997] LP Seka, C Courtin, and P Le Beux.
1997. ADM-INDEX: an automated system for index-
ing and retrieval of medical texts. In Stud Health Tech-
nol Inform, volume 43 Pt A, pages 406?410. Reidel.
[Simard1998] Michel Simard. 1998. Automatic inser-
tion of accents in French text. In Proceedings of the
Third Conference on Empirical Methods in Natural
Language Processing, Grenade.
[Spriet and El-B?ze1997] Thierry Spriet and Marc El-
B?ze. 1997. R?accentuation automatique de textes.
In FRACTAL 97, Besan?on.
[Theron and Cloete1997] Pieter Theron and Ian Cloete.
1997. Automatic acquisition of two-level morpholog-
ical rules. In Ralph Grishman, editor, Proceedings
of the Fifth Conference on Applied Natural Language
Processing, pages 103?110, Washington, DC, March-
April. ACL.
[Yarowsky1999] David Yarowsky. 1999. Corpus-based
techniques for restoring accents in Spanish and French
text. In Natural Language Processing Using Very
Large Corpora, pages 99?120. Kluwer Academic Pub-
lishers.
[Zweigenbaum and Grabar2002] Pierre Zweigenbaum
and Natalia Grabar. 2002. Accenting unknown words:
application to the French version of the MeSH. In
Workshop NLP in Biomedical Applications, pages
69?74, Cyprus, March. EFMI.
[Zweigenbaum2001] Pierre Zweigenbaum. 2001. Re-
sources for the medical domain: medical terminolo-
gies, lexicons and corpora. ELRA Newsletter, 6(4):8?
11.
Lexically-Based Terminology Structuring: Some Inherent Limits
Natalia Grabar and Pierre Zweigenbaum
STIM/DSI, Assistance Publique ? H?pitaux de Paris
& D?partement de Biomath?matiques, Universit? Paris 6
{ngr,pz}@biomath.jussieu.fr
http://www.biomath.jussieu.fr/?{ngr,pz}
Abstract
Terminology structuring has been the subject of
much work in the context of terms extracted from
corpora: given a set of terms, obtained from an ex-
isting resource or extracted from a corpus, identi-
fying hierarchical (or other types of) relations be-
tween these terms. The present paper focusses on
terminology structuring by lexical methods, which
match terms on the basis on their content words,
taking morphological variants into account. Exper-
iments are done on a ?flat? list of terms obtained
from an originally hierarchically-structured termi-
nology: the French version of the US National
Library of Medicine MeSH thesaurus. We com-
pare the lexically-induced relations with the original
MeSH relations: after a quantitative evaluation of
their congruence through recall and precision met-
rics, we perform a qualitative, human analysis of the
?new? relations not present in the MeSH. This anal-
ysis shows, on the one hand, the limits of the lex-
ical structuring method. On the other hand, it also
reveals some specific structuring choices and nam-
ing conventions made by the MeSH designers, and
emphasizes ontological commitments that cannot be
left to automatic structuring.
1 Background
Terminology structuring, i.e., organizing a set of
terms through semantic relations, is one of the dif-
ficult issues that have to be addressed when build-
ing terminological resources. These relations in-
clude subsumption or hyperonymy (the is-a re-
lation), meronymy (part-of and its variants), as
well as other, diverse relations, sometimes called
?transversal? (e.g., cause, or the general see also).
Various methods have been proposed
to discover relations between terms (see
Jacquemin and Bourigault (2002) for a review).
We divide them into internal and external meth-
ods, in the same way as McDonald (1993)
for proper names. Internal methods look
at the constituency of terms, and compare
terms based on the words they contain. Term
matching can rely directly on raw word forms
(Bodenreider et al, 2001), on morphological
variants (Jacquemin and Tzoukermann, 1999),
on syntactic structure (Bourigault, 1994;
Jacquemin and Tzoukermann, 1999) or on se-
mantic variants (synonyms, hyperonyms, etc.)
(Hamon et al, 1998). External methods take
advantage of the context in which terms occur:
they examine the behavior of terms in corpora.
Distributional methods group terms that occur
in similar contexts (Grefenstette, 1994). The
detection of appropriate syntactic patterns of
cooccurrence is another method to uncover re-
lations between terms in corpora (Hearst, 1992;
S?gu?la and Aussenac, 1999).
In previous work we applied lexical methods to
identify relations between terms on the basis on
their content words, taking morphological variants
into account. Our goal was then to assess the feasi-
bility of such structuring by studying it on an exist-
ing, hierarchically structured terminology. Ignoring
this existing structure and starting from the set of its
terms, we attempt to discover hierarchical term-to-
term links and compare them with the preexisting
relations.
Our goal in the present paper is to analyze ?new?
relations. ?New? means that these induced relations
are not present in the original hierarchical structure
of the MeSH thesaurus; they might nevertheless re-
flect useful links. Performing this analysis allows us
to propose a more precise evaluation of the methods
and their results and to point out some inherent lim-
its.
After the exposition of the data we used in our
experiments (section 2), we present methods (sec-
tion 3) for generating hierarchical links between
terms through the study of lexical inclusion and for
evaluating their quality with appropriate recall and
precision metrics. We then present the analysis of
some ?new? induced relations and attempt to pro-
pose a typology of term dependency in these rela-
tions (section 4). We finally discuss the limits of
lexical methods for the structuring task (section 5).
2 The MeSH biomedical thesaurus, and
associated morphological knowledge
We first present the existing hierarchically struc-
tured thesaurus, a ?stop word? list and morpholog-
ical knowledge involved in the present work.
2.1 The MeSH biomedical thesaurus
The Medical Subject Headings (MeSH,
NLM (2001a)) is one of the main international
medical terminologies (see, e.g., Cimino (1996)
for a presentation of medical terminologies). It is
a thesaurus specifically designed for information
retrieval in the biomedical domain. The MeSH is
used to index the international biomedical literature
in the Medline bibliographic database. The French
version of the MeSH (INSERM, 2000) contains
a translation of these terms (19,638 terms) plus
synonyms. It happens to be written in unaccented,
uppercase letters. Both the American and French
MeSH can be found in the UMLS Metathesaurus
(NLM, 2001b), which can be obtained through a
convention with the National Library of Medicine.
The concept names (main headings) which the
MeSH contains have been designed to reflect their
broad meanings and to facilitate their use by hu-
man indexers and librarians. In that, they follow a
tradition in information sciences, and are not nec-
essarily the expressions used in naturally occurring
biomedical documents. The MeSH can be consid-
ered as a fine-grained thesaurus: concepts are cho-
sen to insure a good coverage of the biomedical do-
main (Zweigenbaum, 1999).
As many other medical terminologies, the MeSH
has a hierarchical structure: ?narrower? concepts
(children) are related to ?broader? concepts (par-
ents). This both covers the usual is-a relation and
partitive relations (part-of, conceptual-part-of and
process-of ). The MeSH also includes see-also re-
lations, which we do not take into account in the
present experiments. This structure has also been
designed in the aim to be intellectually accessi-
ble to users: an indexer must be able to assign a
given concept to an article and a clinician must be
able to find a given concept in the tree hierarchy
(Nelson et al, 2001). To conclude, the MeSH team
aims to organize it in a clear and intuitive manner,
both for concept naming and concept placement.
The version of the French MeSH we used in these
experiments contains 19,638 terms, 26,094 direct
child-to-parent links and (under transitive closure)
95,815 direct or indirect child-to-ancestor links.
2.2 Stop word list
The aim of using a ?stop word? list is to remove from
term comparison very frequent words which are
considered not to be content-bearing, hence ?non-
significant? for terminology structuring. We used
in this experiment a short stop word list (15 word
forms). It contains the few frequent grammatical
words, such as articles and prepositions, that occur
in MeSH terms.
2.3 Morphological knowledge
The morphological knowledge involved consists of
lemma/derived-word or lemma/inflected form pairs
where the first is the ?normalized? form and the sec-
ond a ?variant? form.
Inflection produces the various forms of a given
word such as plural, feminine or the multiple forms
of a verb according to person, tense, etc.: inter-
vention ? interventions, acid ? acids. We per-
form the reverse process (lemmatization), reducing
an inflected form to its lemma (canonical form).
We worked with two alternate lexicons. The first
one is based on a general French lexicon (ABU,
abu.cnam.fr/DICO) which we have augmented with
pairs obtained from medical corpora processed
through a tagger/lemmatizer (in cardiology, hema-
tology, intensive care, and drug monographs): it to-
tals 219,759 pairs (where the inflected form is dif-
ferent from the lemma). The second lexicon, more
specialized and tuned to the vocabulary in medi-
cal terminologies, is the result of applying rules ac-
quired in previous work from two other medical ter-
minologies (ICD-10 and SNOMED) to the vocab-
ulary in the MeSH, ICD-10 and SNOMED (total:
2,889 pairs).
Derivation produces, e.g., the adjectival form of
a noun (noun aorta   adjective aortic), the nom-
inal form of a verb (verb intervene   noun inter-
vention), or the adverbial form of an adjective (ad-
jective human   adverb humanely). We perform
linguistically-motivated stemming to reduce a de-
rived word to its base word. For derivation, we also
used resources acquired in previous work which,
once combined with inflection pairs, results in 4,517
pairs.
Compounding, which combines several radicals,
often of Greek or Latin origin, to obtain complex
words (e.g., aorta + coronary yields aortocoro-
nary), has not been used because we do not have
a reliable procedure to segment a compound into its
component morphemes.
3 Acquiring links through lexical
inclusion of terms
The present work induces hierarchical relations be-
tween terms when the constituent words of one term
lexically include those of the second term (sec-
tion 3.1). When comparing these relations with
those that preexist in the MeSH, precision can reach
29.3% and recall 13.7% (section 3.2). We focus
here on the analysis of the relations that are not
found in the MeSH (section 3.3), which we develop
in the next section (section 4).
3.1 Lexical inclusion
The method we use here for inducing hierarchical
relations between terms is basically a test of lexical
inclusion: we check whether a term   (parent) is
?included? in another term  (child), i.e., whether
all words in   occur in  . We assume that this type
of inclusion is a clue of a hierarchical relation be-
tween terms, as in acides gras / acides gras indis-
pensables (fatty acids / fatty acids, essential).
To detect this type of relation, we test whether
all the content words of   occur in  . We do this
on segmented terms with a gradually increasing nor-
malization on word forms. Basic normalizations are
performed first: conversion to lower case, removal
of punctuation, of numbers and of ?stop words?.
Subsequent normalizations rely on morphological
ressources: lemmatization (with the two alternate
inflectional lexicons) and stemming with a deriva-
tional lexicon. Terms are indexed by their words to
speed up the computation of term inclusion over all
term pairs of the whole MeSH thesaurus.
3.2 Application to MeSH and quantification
This structuring method has been applied to the flat
list of 19,638 terms of the MeSH thesaurus. As ex-
pected, the number of links induced between terms
increases when applying inflectional normalization
and again with derivational normalization.
We evaluated the quality of the links obtained
with this approach by comparing them automati-
cally with the original structure of the MeSH and
computing recall and precision metrics. We sum-
marize here the main results; a detailed evaluation
can be found in (Grabar and Zweigenbaum, 2002).
Depending on the normalization, up to 29.3% of
the links found are correct (precision), and up to
13.7% of the direct MeSH links are found by lex-
ical inclusion (recall). We also examined whether
each term was correctly placed under one of its an-
cestors: this was true for up to 26% of the terms
(recall); and the placement advices were correct in
up to 58% of the cases (precision). The recall of
links increases when applying more complete mor-
phological knowledge (inflection then derivation).
The evolution of precision is opposite: injection of
more extensive morphological knowledge (deriva-
tion vs inflection) leads to taking more ?chances? for
generating links between terms: the precision with
no normalization (raw results) is 29.3% vs 22.5%
when using all normalizations (lem-stem-med). De-
pending on the type of normalization, the best pre-
cision obtained for links is 43%.
3.3 Human analysis of ?new? relations
The evaluations presented in the previous section
quantify the match between the induced relations
and existing MeSH relations. However, they give
no explanation for the fact that 70% of the induced
relations are not considered relevant by the MeSH.
This is what we study in the remainder of this paper:
why these terms are not hierarchically related in the
MeSH, and what kinds of relations exist between
them.
According to the position of the words of the
?parent? term in the ?child? term, we divide the
extra-MeSH relations into three sets:  the par-
ent concept is at the head position in the child con-
cept: absorption/absorption intestinale; 	 the par-
ent concept is at the tail (expansion) position in the
child concept: abdomen/tumeur abdomen; 
	 other
types of positions. Each set of relations is sam-
pled by randomly selecting a 20% subset, both with-
out normalization (raw) and with inflectional and
derivational normalizations (med-lem-stem). Ta-
ble 1 presents the number of analyzed relations (to-
tal = 194).
Normalizations Head Expan. Other
raw 22 31 14
lem-stem-med 37 57 33
Table 1: Relations to analyze: sample sizes.
4 An analysis of new, lexically-induced
relations
We first examine the issues encountered when try-
ing to identify the head of each term (section 4.1),
then review in turn each analyzed subset: head (sec-
tion 4.2), expansion (section 4.3) and other relations
(section 4.4).
4.1 Finding the head
In French, the semantic head of a noun phrase is
usually located at the beginning of this phrase (this
contrasts with English, where the semantic head is
generally at the end of NPs). Moreover, as is often
the case with terms, MeSH terms do not include de-
terminers, so that the semantic head is usually the
first word here. We therefore rely on a heuristic
for determining ?head? and ?expansion? subsets: the
head is the first word of the term, and the expansion
is the last word. This is correct most of the time, but
in some cases, the semantic head is positioned at the
end of the term, generally separated with a comma,
a tradition sometimes followed in thesauri:
filoviridae/filoviridae, infections,
leishmania/leishmania tropica, infection,
quinones/quinone reductases,
neurone/neurone moteur, maladie,
syndrome/bouche main pied, syndrome.
These cases must be hand-corrected and distributed
into the following classes.
We also encountered another kind of error, due to
overzealous derivational knowledge:
contracture/contraction musculaire,
biologie/testament biologique,
where contracture (a muscle disease) and con-
traction (normal muscle function) have both been
stemmed to the same base word; the expansion ad-
jective biologique is derived from the noun biologie,
but its sense is generally more specific than biolo-
gie.
4.2 ?Head? subset
Let us first discard a case where it seems that we
encountered a translation error. An examination of
the structure of the English MeSH and a search on
Web pages show that in the French MeSH, acide
linoleique alpha should read acide linolenique al-
pha, which is a kind of acide linolenique (and not a
kind of acide linoleique). The induced relation:
acide linoleique/acide linoleique alpha
is therefore incorrect; with the correct spelling, the
lexical inclusion:
acide linolenique/acide linolenique alpha
would reveal a correct hierarchical relation.
4.2.1 The head is not the ?genus? of the term
We encountered cases where the whole term did not
have an is-a relation with the head as defined above.
This happens in two types of situations.
The first situation is due to syntactic reasons. In
the following induced relation,
acides amines / acides amines, peptides et pro-
teines,
the larger term is an enumeration, with the sense
of a logical OR. It is therefore the genus term, of
which each of its components (e.g., acides amines)
is a sub-type.
The second situation is due to semantic reasons.
Lexical induction of hierarchical relations assumes
inheritance of the defining features of the genus
term (e.g., a fatty acid, essential is a kind of fatty
acid). However, it is well known that this is not al-
ways true: a plaster cat is not a cat (i.e., a mammal,
etc.). This is sometimes modeled as a type coercion
phenomenon. We found quite a few ?plaster cats? in
our terms:
personnalite/personnalite compulsive,
voix/voix oesophagienne.
For instance, personnalite here describes ?behavior-
response patterns that characterize the individual?,
whereas personnalite compulsive (compulsive per-
sonality disorder) describes a mental disorder. Dis-
orders (or diseases) are different objects than behav-
iors in the MeSH.
4.2.2 The head is ambiguous
This depends on the choice of term names in the ter-
minology (here, the MeSH). Terms like absorption,
investissement, etc., have specific senses that make
them polysemous. To determine a precise sense,
these terms have to be specialized by their contexts:
investissement/investissement (psychanalyse),
absorption/absorption cutanee,
goitre/goitre ovarien
Here, investissement alone (investment) has the fi-
nancial sense, whereas in investissement (psych-
analyse), it has its more generic sense. In a simi-
lar way, absorption has a specific meaning in chem-
istry, and goitre alone is a disorder of the thyroid
gland. These cases are often non-ambiguous in the
original English version of the same terms: for in-
stance, investissement (psychanalyse) (fr) is a trans-
lation of cathexis (en).
A related case occurs when the name of a parent
term is underspecified:
acides/acides pentanoiques,
acne/acne rosacee.
In these examples, acides means inorganic acids1
and acne means acne vulgaris, but the convention
adopted is to use these single words to name the cor-
responding concepts.
4.2.3 Ontological commitment
Finally, some induced links, although absent from
the MeSH, are potentially correct is-a links, but the
designers of the MeSH have made a different mod-
eling choice:
amyotrophies/amyotrophies spinales enfance,
hyperplasie/hyperplasie epitheliale focale,
centre public sante/centre public sante men-
tale,
rectocolite/rectocolite hemorragique,
penicillines/penicilline g.
A general representational choice in the MeSH,
as in some other medical terminologies (e.g.,
SNOMED), is to differentiate on the one hand signs
or symptoms and on the other hand diseases (a
more fully characterized pathological state). This
is the case for amyotrophies and hyperplasie (signs
or symptoms) vs amyotrophies spinales enfance and
hyperplasie epitheliale focale (disease of the ner-
vous system, of the mouth).
For some reason, a centre public sante mentale
(public mental health center) is considered not to
share all the attributes of a general centre public
sante (public health center), which prevents them
from being in a parent-child relationship: they are
only siblings in the MeSH thesaurus.
Penicillines, in the MeSH, have been chosen to
refer to a therapeutic class of drugs (under antibi-
otics, under chemical actions), whereas penicilline
g is considered as a chemical substance.
The structuring involved in these instances re-
flects the ontological commitments of the terminol-
1Note, though, that if inorganic acids was named this way,
it would be impossible to link it by lexical induction to other,
more specific types of inorganic acids.
ogy designers, and cannot be recovered by lexical
inclusion.2
4.3 ?Expansion? subset
When a ?parent? term is in ?expansion? position (end
position) in a ?child? term, we assume that the se-
mantic head of the child term is modified; the in-
duced relation is indeed expected not to be is-a.
Some of the main cases found are close to those for
the ?head? subset. Among others, we find again enu-
merations (see subsection 4.2.1):
immunodepresseurs / antineoplasiques et im-
munodepresseurs
and syntactic ambiguity (subsection 4.2.2):
oncogene/antigene viral oncogene,
where the word oncogene is a noun in the first term
and an adjective in the second one.
Many of the relations found in the ?expansion?
subset are partitive:
abdomen/muscle droit abdomen,
amerique centrale/indien amerique centrale,
argent/nitrate argent.
(human body parts, a continent and its peoples, and
chemical substances).
In some instances, a general type of link between
terms can be detected:
caused-by: myxome/virus myxome,
but in most other cases, we have what looks like a
specific thematic relation between a predicate and
its argument:
comportement alimentaire/troubles comporte-
ment alimentaire,
bovin/pneumonie interstitielle atypique bovin,
hopital/capacite lits hopital,
services sante/fermeture service sante,
macrophage/activation macrophage.
Note that some of these expansion relations involve
adjectival derivations of nouns:
cubitus/nerf cubital,
genes/epreuve complementation genetique.
2They might be amenable to distributional methods if their
contexts of occurrence are different enough.
4.4 ?Other? subset
In this last subset, the ?parent? term can be at any
position in the ?child? term other than head or ex-
pansion. It can also be non-contiguous, accepting
modifiers or some other intervening elements. All
these cases are actually similar to those of the ?ex-
pansion? subset except those of the form:
bacterie aerobie/bacterie gram-negatif aerobie
where bacterie remains the head of the term.
The following examples reproduce the general
cases of the ?expansion? subset with additional mod-
ifiers:
arteres/anevrysme artere iliaque,
hepatite b/virus hepatite b canard,
encephalite/virus encephalite equine ouest,
sommeil/troubles sommeil extrinseques,
irrigation/liquide irrigation endocanalaire,
maladie/assurance maladie personne agee.
In some of them, adjectival derivation is involved:
cellules/molecule-1 adhesion cellulaire vascu-
laire,
chimie/produits chimiques inorganiques,
dent/implantation dentaire sous-periostee.
Some relations are characteristic of the language
of chemical compounds:
cytochrome c/ubiquinol-cytochrome c reduc-
tase,
diphosphate/uridine diphosphate acide glu-
curonique,
lysine/histone-lysine n-methyltransferase.
The ?other? subset alo hosted the following mor-
phosyntactic ambiguity:
cilie/cellule ciliee externe
where the words cilie (noun, an invertebrate organ-
ism) and ciliee (inflected form of adjective cilie,
which characterizes a type of cell) are conflated by
lemmatization. This error is mainly due to the fact
that the MeSH is written with unaccented uppercase
letters: the adjective is actually spelled cili?, which
would be unambiguous here.
5 Synthesis
We presented in this paper a human analysis of auto-
matically, lexically-induced term relations that were
not found in the terminology from which the terms
were obtained (the MeSH thesaurus). This lexical
method considers that a term   is probably a par-
ent of a term  iff all the words of   occur in  .
This inclusion test is helped by morphological nor-
malization.
Morphological normalization was found to be
useful not only in identifying the already ex-
isting relations (section 3.2), but also for the
?new? relations. This confirms previous work by
Jacquemin and Tzoukermann (1999).
The occurrences of syntactic ambiguity
suggest that morphosyntactic tagging could
be useful. The methods specifically de-
signed for detection of syntactic and morpho-
syntactic term variants (Bourigault, 1994;
Jacquemin and Tzoukermann, 1999) might then be
more efficient and less error-prone. We must be
warned however that this may not be an easy task,
since most of the MeSH terms are not syntactically
well-formed (few determiners and prepositions,
inverted heads) and contain rare, technical words
that are likely to be absent from most electronic
lexicons.
Spurious relations may come from several
sources. A few cases are due to abusive morpho-
logical normalization; errors in term names (trans-
lation errors) were also uncovered. We made a dis-
tinction between ?head? and ?expansion? positions
of the ?parent? term in its ?child?. One would expect
that relations where the parent is in head position
would be correct; however, this is not always true.
The putative head of a term is sometimes not cor-
rectly identified because of specific thesaural con-
structs (the ?comma? form) and chemical constructs
(quinone reductases are a kind of reductases) which
display head inversion, and because of enumera-
tions. An additional situation is that of a term whose
actual syntactic head does not entertain an is-a re-
lation with it (the ?plaster cat?). Furthermore, the
head word may not have a stable meaning: it may
be syntactically ambiguous (cilie), polysemous (in-
vestissement) or underspecified (acne).
The remaining ?head? cases reveal specific mod-
eling options, or ?ontological commitments?, of the
terminology designers: the relations induced might
be considered semantically valid, but were dis-
carded in the MeSH because of overall structuring
choices. These choices cannot be predicted with the
lexical methods used here, and seem to be the most
resistant to attempts at automatic derivation. They
also show that what is correct is not necessarily use-
ful for a given terminology.
The ?expansion? cases may be useful to propose
other relations than is-a: we displayed partitive re-
lations, but left to further work a classification of the
remaining ones. The UMLS semantic network rela-
tions (NLM, 2001b) might be a relevant direction to
look into to represent such links.
References
Olivier Bodenreider, Anita Burgun, and Thomas C.
Rindflesch. 2001. Lexically-suggested hy-
ponymic relations among medical terms and their
representation in the UMLS. In URI INIST
CNRS, editor, TIA?2001 Terminologie et Intelli-
gence artificielle, pages 11?21, Nancy.
Didier Bourigault. 1994. Extraction et structura-
tion automatiques de terminologie pour l?aide ?
l?acquisition de connaissances ? partir de textes.
In Proceedings of the 9    Conference RFIA-
AFCET, pages 1123?1132, Paris, France, Jan-
uary. AFCET.
James J Cimino. 1996. Coding systems in health
care. In Jan H. van Bemmel and Alexa T. Mc-
Cray, editors, Yearbook of Medical Informat-
ics ?95 ? The Computer-based Patient Record,
pages 71?85. Schattauer, Stuttgart.
Natalia Grabar and Pierre Zweigenbaum. 2002.
Lexically-based terminology structuring: a fea-
sibility study. In LREC Workshop on Using Se-
mantics for Information Retrieval and Filtering,
pages 73?77, Las Palmas, Canaries, May. ELRA.
Gregory Grefenstette. 1994. Explorations in Auto-
matic Thesaurus Discovery. Natural Language
Processing and Machine Translation. Kluwer
Academic Publishers, London.
Thierry Hamon, Adeline Nazarenko, and C?cile
Gros. 1998. A step towards the detection of
semantic variants of terms in technical docu-
ments. In Christian Boitet, editor, Proceedings
of the 17    COLING, pages 498?504, Montr?al,
Canada, 10?14 August.
Marti A. Hearst. 1992. Automatic acquisition of
hyponyms from large text corpora. In Antonio
Zampolli, editor, Proceedings of the 14    COL-
ING, pages 539?545, Nantes, France, 23?28 July.
INSERM, 2000. Th?saurus Biom?dical
Fran?ais/Anglais. Institut National de la
Sant? et de la Recherche M?dicale, Paris.
Christian Jacquemin and Didier Bourigault. 2002.
Term extraction and automatic indexing. In Rus-
lan Mitkov, editor, Handbook of Computational
Linguistics. Oxford University Press, Oxford. To
appear.
Christian Jacquemin and ?velyne Tzoukermann.
1999. NLP for term variant extraction: A syn-
ergy of morphology, lexicon, and syntax. In
Tomek Strzalkowski, editor, Natural language
information retrieval, volume 7 of Text, speech
and language technology, chapter 2, pages 25?
74. Kluwer Academic Publishers, Dordrecht &
Boston.
David D. McDonald. 1993. Internal and external
evidence in the identification and semantic cate-
gorization of proper names. In Branimir Bogu-
raev and James Pustejovsky, editors, Corpus Pro-
cessing for Lexical Acquisition, pages 61?76.
MIT Press, Cambridge, MA.
Stuart J Nelson, Douglas Johnston, and Betsy L
Humphreys. 2001. Relationships in medical sub-
ject headings. In Carol A Bean and Rebecca
Green, editors, Relationships in the organization
of knowledge, New York. Kluwer Academic Pub-
lishers.
National Library of Medicine, Bethesda, Mary-
land, 2001a. Medical Subject Headings.
www.nlm.nih.gov/mesh/meshhome.html.
National Library of Medicine, Bethesda, Mary-
land, 2001b. UMLS Knowledge Sources Manual.
www.nlm.nih.gov/research/umls/.
Patrick S?gu?la and Nathalie Aussenac. 1999. Ex-
traction de relations s?mantiques entre termes
et enrichissement de mod?les du domaine. In
R?gine Teulier, editor, Actes de IC?99, June.
Pierre Zweigenbaum. 1999. Encoder l?information
m?dicale : des terminologies aux syst?mes de
repr?sentation des connaissances. Innovation
Strat?gique en Information de Sant?, (2?3):27?
47.
Detecting semantic relations between terms in definitions
V?ronique MALAIS? 1,2
1 STIM/AP-HP, ERM 202 INSERM
& CRIM-INaLCO
91, boulevard de l?H?pital
75013 Paris,
France,
{vma,pz}@biomath.jussieu.fr
Pierre ZWEIGENBAUM1 Bruno BACHIMONT 2
2DRE de l?INA
Institut National de l?Audiovisuel
4, avenue de l?Europe
94366 Bry-sur-Marne Cedex,
France,
{vmalaise,bbachimont}@ina.fr
Abstract
Terminology structuring aims to elicit semantic re-
lations between the terms of a domain. We propose
here to exploit definitions found in corpora to ob-
tain such semantic relations. Definition typologies
show that definitions can be introduced by differ-
ent semantic relations, some of these relations be-
ing likely to structure terminologies. Our aim is
therefore to mine ?defining expressions? in domain-
specific corpora, and to detect the semantic rela-
tions they involve between their main terms. We
use lexico-syntactic markers and patterns to detect
at the same time both a definition and its main se-
mantic relation. 46 markers and 74 patterns have
been designed and tuned on a first corpus in the field
of anthropology. We report on their evaluation on a
second corpus in the field of dietetics, where they
obtained 4% to 36% recall and from 61 to 66% pre-
cision, and discuss the relative accuracy of different
subclasses of markers for this task.
1 Introduction
A terminology is an artifact structuring terms ac-
cording to some semantic relations. Grabar and
Hamon (2004) present the different semantic rela-
tions likely to be found in terminologies. These
can be divided into lexical (synonymy), vertical
(hypernymy, meronymy) and transversal relations
(domain-specific relations). A study of definition
typologies, like the one of (Auger, 1997), shows
that these different relations are also present in def-
initions. We can then hypothesise that mining defi-
nitions along with the detection of their inherent se-
mantic relation can help to organise terms according
to the relations used in structured terminologies. We
focus in this paper on the detection of terms related
by hypernymy and synonymy in definitions.
The automatic detection of definitions can rely
on different types of existing works. We can, first,
consider the studies describing what definition is,
and more particularly what definition in corpus is
like. In this respect, we can cite the work of Trim-
ble (1985), Flowerdew (1992), Sager (2001) and
Meyer (2001). Another type of interesting exist-
ing work is about typologies of definitions: Mar-
tin (1983), Chukwu and Thoiron (1989) and Auger
(1997), amongst others, provide, in their classifica-
tions of definitions, linguistic clues to find defin-
ing statements in corpus. We propose to integrate
the typologies that we mention in section 2.2, along
with the linguistic clues they give: the definition
markers. And, at last, some works have already fo-
cused on mining definitions from corpora, including
Cartier (1997), Pearson (1996), Rebeyrolle (2000)
and Muresan and Klavans (2002), mostly through
the use of lexical definition markers. These works
provide us with methodological guidelines and an-
other set of lexical markers for our own experiment.
As (Pearson (1996); Rebeyrolle (2000)), our
method is based on lexico-syntactic patterns, so that
we can build on the work on French language by
Rebeyrolle (2000). We extended her work in two
respects: an analysis of the parenthesis as low-level
linguistic clue for definitions, and the concomitant
extraction of the semantic relation involved in a
?defining expression?, along with the extraction of
the definition itself. Previous works have, for in-
stance, mined definitions to find terms specific to a
particular domain of knowledge (Chukwu and Tho-
iron (1989)), and to describe their meaning (Rebey-
rolle, 2000); we focus on the detection of the seman-
tic relations between the main terms of a definition
in order to help a terminologist to build a structured
terminology following these relations.
We implemented an interface to visualise these
definitions and semantic relations extractions. We
tuned markers and patterns for extracting defini-
tions and semantic relations on a first corpus about
anthropology; we then tested the validity of these
markers and patterns on another corpus focused on
dietetics. The purpose of this test was, on the one
hand, to observe whether definitions were still cor-
rectly extracted on the basis of patterns trained on
a corpus differing in the domain of knowledge and
in the genre of documents involved, and, on the
CompuTerm 2004  -  3rd International Workshop on Computational Terminology 55
other hand, to detect if the semantic relation associ-
ated with each pattern was the same as the one ob-
served in the first corpus. The markers and patterns
showed to be comparable to the other experiments
mentioned in terms of definition extraction: the pre-
cision reached from 61 to 66%. As for the seman-
tic relation associated with the patterns, it obtained
different scores, depending on the marker. But, in
most cases, one main semantic relation is associ-
ated with a pattern in the scope of a single domain,
event though a few patterns convey the same rela-
tion across our two corpora.
The remainder of this paper is organised as fol-
lows: we first present previous work (section 2), de-
scribe our method and experiment (section 3), then
present and discuss results (section 4) and conclude
with directions for future work (section 5).
2 Previous work
2.1 Description of definitions in corpus
As a first approach for detecting and extracting
defining statements in corpora, we have to... de-
fine this object. In the literature (Trimble (1985);
Flowerdew (1992),. . . ), three categories of defini-
tions are often mentioned: the formal definition, the
semi-formal and the ?non-formal? one. The formal
definition follows the Aristotelian schema: X = Y +
specific characteristics, where X is the defined term
(the ?definiendum?), ?=? means an equivalence re-
lation, Y stands for the generic class to which X
belongs (the ?Genus?), and specific characteris-
tics detail in which respect X is different from the
other items composing the same generic class. A
semi-formal definition relates the definiendum only
with specific characteristics, or with its attribute(s)
(Meyer, 2001). Formal and semi-formal definitions
can be of simple type (expressed in one sentence),
or complex (expressed in two, or more sentences).
A non-formal definition aims ?to define in a gen-
eral sense so that a reader can see the familiar el-
ement in whatever the new term may be? (Trimble,
1985). It can be an association with a synonym, a
paraphrase or grammatical derivation.
The common point between all these points of
views on the same linguistic object, or between all
these different objects sharing the same appellation
?definition?, is that they all follow the same didac-
tic purpose of disambiguating the meaning of a lex-
ical item, that is to distinguish it from the others in
the general language, or inside a specific vocabu-
lary. These definition descriptions present them as
the association between a term and its hypernym (its
?genus?), or between a term and its specific charac-
teristics. But there are yet other ways to express
definitions, as the works on their typology shows.
2.2 Typology of definitions
Existing definitions typologies are all dedicated to
a specific purpose. We are particularly interested in
those which aim at eliciting linguistic clues that can
be used to mine defining contexts from corpora. We
work on French, for which Martin (1983) has classi-
fied dictionary definitions in order to give guidelines
for a consistent (electronic) dictionary. In the con-
text of corpus-based research, Chukwu and Thoiron
(1989) gave another classification, aiming at finding
domain-specific terms in corpora. A unified typol-
ogy is provided by Auger (1997), compiling both
cited typologies along with three others, and from
which we draw the following three categories:
? Definitions expressed by ?low level? linguistic
markers: punctuation clues such as parenthe-
sis, quote, dash, colon;
? Definitions expressed by lexical markers: lin-
guistic or metalinguistic lexical items;
? Definitions expressed by ?high level? linguistic
markers: syntactic patterns such as anaphora or
apposition.
The definitions introduced by lexical means are di-
vided in two branches, characterised by the lexi-
cal markers in table 1. We added elements from
other studies ((Rebeyrolle, 2000) and (Fuchs, 1994)
amongst others), and augmented this typology with
Definitions introduced by linguistic markers
Copulative ?a X is a Y that?
Equivalence ?equivalent to?
Characterisation ?attribute of?, ?qual-
ity?,. . .
Analysis ?composed of?, ?equipped
with?, ?made of?,. . .
Function ?to have the function?, ?the
role of?, ?to use X to do
Y?,. . .
Causality ?to cause X by Y?, ?to ob-
tain X by?,. . .
Definitions introduced by metalinguistic mark-
ers
Designation ?to designate?, ?to
mean?,. . .
Denomination ?to name?
Systemic ?to write?, ?to spell?, ?the
noun?,. . .
Table 1: Lexical markers (English translation)
CompuTerm 2004  -  3rd International Workshop on Computational Terminology56
new markers, including some items introducing re-
formulation contexts (?that is?, ?to say?, ?for in-
stance?, . . . ).
The Aristotelian definition type is presented here
as a ?copulative? definition, as it is linguistically
marked by the copula ??tre? (to be). It involves a
hypernymic relation (and specific differences) to de-
scribe the meaning of a term, so we consider it as a
?hypernymic definition?. But we can see in table 1
that other semantic relations can also be used to de-
fine a term: synonymy (definition of ?equivalence?
type), meronymy (?analysis? type), causality and
other domain-specific transversal relations (?func-
tion?, ?characterisation? types). Mining a defini-
tion of ?synonymic type? provides different denom-
inations for the same concept; one of ?hypernymy
type? can help modelling the vertical structure be-
tween the ?definiendum? and the first term of the
?definiens? (conceptual ?father? and ?son? associa-
tion); and definitions following transversal relations
allow the expression of specific knowledge. We fo-
cus in this paper on the extraction of definitions in-
volving hypernymy and synonymy, which are the
most generally considered relations in terminology
building.
2.3 Automatic definition mining
Automatic definition mining from corpora can
be divided in different groups, according to the
methodologies followed. We will illustrate them by
describing three recent families of works: (i) Cartier
(1997), (ii) Pearson (1996) and Rebeyrolle (2000),
(iii) Muresan and Klavans (2002). They have
used respectively ?contextual exploration?, lexico-
syntactic patterns and linguistic analysis and rules.
The former one extracts defining statements on
the basis of the match of linguistic clues, when they
are relayed in the sentence by some linguistic rules.
These rules are developped by the author, withing
the schema defined in the ?contextual exploration?
methodology (Descl?s, 1996).
Pearson (1996) and Rebeyrolle (2000) have fol-
lowed the methodology described by Hearst (1992),
up to now mainly applied to discover hyponymous
terms. It consists in describing the lexico-syntactic
context of an occurrence of a pair of terms known to
share a semantic relation. Modelling the context in
which they occur provides a ?pattern? to apply to the
corpus, in order to extract other pairs of terms con-
nected by the same relation. Pearson and Rebeyrolle
have modelled lexico-syntactic contexts around lex-
ical clues interpreted as ?definition markers?. Re-
beyrolle, working on French, evaluated the different
pattern types she modelled, across different corpora:
she obtained a precision range of 17.95 ? 79.19%,
and a recall of 94.75 ? 100%. The difference be-
tween the two numeric boundaries of the precision
range is due to the kind of markers involved in
the lexico-syntactic pattern evaluated: metalinguis-
tic markers obtained a high precision rate, but not
linguistic lexical markers.
The latter pair of authors have based their system
DEFINDER (http://www1.cs.columbia.
edu/~smara/DEFINDER/) on the lexical and
syntactic analysis of a medical corpus, with semi-
automatic definition acquisition. Their evaluation
is focused on the usefulness of the system, as
compared with existing specialised medical dictio-
naries. They reach a 86.95% precision and 75.47%
recall, following their evaluation methodology.
We chose to follow the first methodology in our
experiment (see section 3), in which we additionally
explore definition mining in some cases where the
definition is not introduced by lexical items. Fol-
lowing this methodology enables us to build on ex-
isting work dedicated to French, which showed to
be interesting and efficient. The lexico-syntactic
pattern methodology also enables us to access the
different linguistic elements we were interested in
mining: the definition itself, the main terms of the
definition and the semantic relation between them.
We focus this experimentation more particularly
on identifying the semantic relations of synonymy
and hypernymy involved in the different definitions
likely to be found in corpora. We aim at testing
whether a stable link can be established between the
definition extraction pattern and a specific semantic
relation.
3 Detecting Semantic Relations
Our goal is to automatically detect some of the se-
mantic relations that might be found in definitions
and to propose them to a human validator in charge
of structuring a terminology. We focus on hyper-
nymy and synonymy, which are the most classical
relations found in terminology. If the relation is
hypernymy, the terms are to be modelled in a hi-
erarchical way, if it is synonymy, both terms can
be used to express the same concept. The rela-
tions and the definitions are extracted together from
corpora, by the same lexico-syntactic patterns. We
present in the next subsections our two corpora (sec-
tion 3.1), then the lexico-syntactic patterns we used
(section 3.2) and their experimental evaluation (sec-
tion 3.3): we analyse whether a relation found in
connection with a lexico-syntactic pattern in the
training corpus can be unchanged in the context of
the same lexico-syntactic pattern, when applied to a
CompuTerm 2004  -  3rd International Workshop on Computational Terminology 57
different corpus.
3.1 Description and preparation of the corpora
Our training corpus (76 Kwords) is focused on
childhood, from the point of view of anthropolo-
gists. It is composed of different genres of docu-
ments (documentary descriptions, thesis report ex-
tracts, Web documents). Documentary descriptions
were humanly collected, whereas electronic doc-
uments were automatically collected from Inter-
net via the tools of (Grabar and Berland, 2001).
Our evaluation corpus (480 Kwords), in the do-
main of dietetics, is composed of Web documents
indexed by the CISMeF quality-controlled cata-
log of French medical Web sites (http://www.
chu-rouen.fr/cismef/) in the subtrees ?Di-
etetics? and ?Nutrition? of the MeSH thesaurus.
It is mainly composed of medical courses and
Web pages presenting information about nutri-
tion in different medical contexts. Both cor-
pora were morpho-syntactically analysed by Cor-
dial Analyser (Synapse Developpement, http://
www.synapse-fr.com/). Cordial tags, lem-
matises and parses a corpus, yielding grammatical
functions (subject, object, . . . ) between chunks.
3.2 Lexico-syntactic patterns
A given linguistic marker (see, e.g., table 1) can oc-
cur in different contexts, some of which are defini-
tions, and can be a clue for different semantic rela-
tions. Lexico-syntactic patterns aim at reducing this
ambiguity by specifying more restricted contexts in
which a definition is found, and, furthermore, in
which one specific semantic relation is involved.
Unlike (Hearst, 1992), we started the pattern de-
sign by analysing marker occurrences in our train-
ing corpus. We designed and tuned our lexico-
syntactic patterns on this corpus, patterns dedicated
to the extraction of definitions and specific relations:
hypernymy and synonymy. Our patterns use the in-
formation output by the parser, including lemma,
morpho-syntactic category and grammatical func-
tion. For instance: ?N (N)? specifies that the marker
?(? has to be preceded by a noun, and immediately
followed by a single common noun, followed by a
closing parenthesis. In this specific case, ?(? intro-
duces a hypernymic definition.
Each pattern drives different kinds of processing:
? extraction of the defining sentence on the basis
of the whole pattern;
? selection of one ?preferred? relation associated
with the specific pattern, among the set of pos-
sible relations associated with the marker; this
relation stands between the interdefined terms
of the definition;
? extraction of the interdefined terms following
two strategies (contextual or based on depen-
dencies around the marker), depending on the
morphosyntactic category of the marker. When
the marker is a punctuation or a noun, we usu-
ally extract its left and right syntactic contexts1
(roughly the first chunk before the marker, and
the first chunk after the marker in the sen-
tence). When the marker is a verb, we extract
its subject and object if they exist in the sen-
tence, otherwise we extract its left and right
chunks, as in the previous case.
Our patterns are implemented in XSLT and the re-
sulting extractions are shown to a human validator
through a Web interface (figure 1): an HTML form
allowing the validator to complete and correct the
extractions. It is possible for the validator to correct
the terms extracted from the definition, in particu-
lar because the chunk often includes punctuation,
which is usually not considered as part of the term,
and it is possible to select a different semantic rela-
tion than the one proposed when it happens not to
be the correct one. A combo box shows all the pos-
sible relations related to the marker involved in the
lexico-syntactic pattern which provided the extrac-
tion of the defining sentence.
3.3 Experimental setup
We tuned our lexico-syntactic patterns to extract
definitions from the test corpus. We associated
with each pattern a ?preferential? semantic relation,
which human corpus analysis showed to be the more
likely to be connected to the definitions extracted by
the means of this pattern. The aim of the experiment
is to test the stability of this connection, by applying
the patterns to the evaluation corpus.
A random sample of the test corpus (13 texts
among 132) was manually processed to tag its def-
initions, in order to have a standard measure for
the evaluation of recall. Table 2 shows the number
of definitions of synonymic and hypernymic types
found in that sample, and provides the percentages
of these definitions among all the different kinds of
tagged definitions (?% definitions?) in that sample.
Some definitions involved more than one semantic
relation, so we also present the percentage of hyper-
nymic and synonymic relations among all the se-
mantic relations (?% relations?).
1Depending on the position of the marker in the sentence, it
might be the two following or two preceding chunks.
CompuTerm 2004  -  3rd International Workshop on Computational Terminology58
Figure 1: Human validation interface for definitions extracted with the parenthesis marker
Hypernymy Synonymy
# definitions 90 22
% definitions 44, 5% 10, 8%
% relations 39, 1% 9, 5%
Table 2: Number and percentages of hypernymic
and synonymic definitions in a random sample of
the test corpus, according to the human evaluator
In our experiment, we evaluate in turn the quality
of the extracted definitions, then that of semantic
relations (hypernymy and synonymy).
4 Results and discussion
Table 3 shows the number of markers and patterns
prepared and tuned on the training corpus to extract
definitions based on hypernymy or synonymy. Note
that a given marker can be used in different patterns
to extract different semantic relations. Some mark-
ers were also associated in one pattern: the met-
alinguistic nouns and verbs. We combined them
because their individual recall was not lowered by
this association and their precision score was im-
proved. The sentences below are examples of sen-
Hypernymy Synonymy
# markers 3 43
# patterns 4 70
Table 3: Number of markers and patterns
tences extracted by our system; the underlined part
is the marker:
? Hypernymic relation:
?Les acides gras de la s?rie omega-3 ( MAX-
epa ) peuvent ?galement ?tre prescrits .?,
?[. . . ]les fromages ? p?te cuite ( tels que
par exemple le fromage de Hollande ).?
? Synonymic relation:
?L? activit? physique est d?finie comme tout
mouvement corporel produit par la contraction
des muscles squelettiques ,[. . . ]?,
?une relation inverse entre l? activit? physique
et l? insulin?mie ou la sensibilit? ? l? insuline
est habituellement observ?e .?
Table 4 presents the evaluation results: we divide
them according to the semantic relation extracted.
It shows the number of definitions retrieved, and the
associated precision and recall. Precision is divided
in two measures.
Hypernymy Synonymy
# extracted
sentences
270 585
Precision (def) 61% 66%
Precision (rel) 26% 15%
Recall (rel) 4% 36%
Table 4: Evaluation of precision (test corpus) and
recall (random sample of test corpus)
CompuTerm 2004  -  3rd International Workshop on Computational Terminology 59
? the proportion of extracted sentences that cor-
responded to definitions (def ), and
? the proportion of correct semantic relations
found in retrieved definitions (rel).
Recall is the proportion of retrieved definitions
which correctly display the semantic relation iden-
tified in the sample corpus among all the definitions
present in this sample which were tagged as having
this semantic relation by the human evaluator.2
The precision of extracted definitions is compara-
ble to Rebeyrolle?s results. The precision of seman-
tic relations is much lower, but a global evaluation
does not show the particular behavior of some of the
markers. We list below the markers which were ac-
tually involved in the extraction of definitions in the
test corpus.
? Markers implied in hypernymic definition re-
trieval: ?parenth?se? (parenthesis), ?par ex-
emple? (for instance), ?sorte de? (a kind of);
? Markers implied in synonymic definition re-
trieval: ?parenth?se? (parenthesis), ?il s?agit
de? (as for), ?indiquer? (to indicate), ?soit?
(that is), ?expliquer? (to explain), ?pr?ciser?
(to specify), ?marquer? (to mark), ?enfin?
(say), ?ou? (or), ?comme? (as), ?? savoir?
(that is), ?autrement dit? (in other words), ?au
sens de? (meaning), ??quivaloir? (to be equiv-
alent), ?c?est-?-dire? (that is), ?d?finir? (to
define), ?d?signer? (to designate), ?nommer?
(to name), ?d?nommer? (to name), ?r?f?rer?
(to refer), ?expression? (expression), ?terme?
(term).
Table 5 presents the different semantic relations
found in the definitions retrieved by each marker.
The first column references the markers involved in
the extraction of the definition, the second (?Ex-
pected?) presents the number of definitions, ex-
tracted by each marker, following the expected re-
lation. ?Other? gives the number of retrieved def-
initions following another semantic relation, ?Un-
decidable? represents the number of definitions for
which we could not determine the semantic rela-
tion,3 and ?Non definition? presents the number of
retrieved sentences that were not definitions.4
2The percentage of definitions of hypernymic and syn-
onymic type among all definitions in the sample of the test cor-
pus is given in table 2.
3Because our system extracts only one sentence, and a
larger context was necessary to understand the semantic rela-
tion involved, or because of a problem in the conversion of
some HTML documents to texts for the evaluation corpus.
4Except sentences presenting terms in a paradigm context,
which is also interesting for terminology structuring. We in-
Definitions retrieved with the hypernymy patterns
involved very generic markers, and they introduced
a number of other semantic relations. The pattern
around ?for instance?, for which 16 extracted sen-
tences out of 95 were not definitions, can still be
specified to discriminate defining contexts from oth-
ers. We can notice, though, that it is one of the
most productive patterns (95 extractions) and that it
reaches a 47, 3% precision. But the patterns around
the parenthesis show that the same syntactic con-
text can introduce different kinds of relations: in
this case, the lexico-syntactic pattern cannot disam-
biguate the relation any further. The pattern ?N
(N)? introduced ?hypernymic definitions?, as well
as ?synonymic? or ?meronymic? ones, the same
syntactic context being even likely to be interpreted
as a transversal relation between a treatment and a
disease, for instance. It is the sentence as a whole
that has to be interpreted in order to be able to define
the relevant semantic relation between the terms in
that syntactic context.
Some linguistic markers (as ?comme?) are re-
liable for detecting a semantic relation: 9 sen-
tences out of 13 were ?synonymic definitions?.
But surprisingly enough, some metalinguistic verbs
(?d?finir?, for instance) were not as effective as
them in that purpose. ?D?finir? introduced only
22 ?synonymic definitions? out of 68 sentences re-
trieved. One could think that a verb with metalin-
guistic function could be less polysemic than an-
other of more ?generic purpose?. This naive hope
happens to be wrong: ?D?finir? means ?to fix
(a limit)? as often as ?to define?. Some markers
steadily introduced a semantic relation, but not the
one they were supposed to: this variation is prob-
ably due to the change in domains across our two
corpora. And some patterns obviously introduced a
definition, but the defined element was in the previ-
ous sentence (this is the case of 92 extractions with
patterns involving the marker ?Il s?agit de?). As our
system, up to now, extracts only one sentence, we
could not determine whether the semantic relation
was the one expected. We must address this prob-
lem, and we can hope that the precision rate will
then be better than the one presented here: some
sentences for which we could not interpret the se-
mantic relation might convey the one we expected.
The best precision score is reached by patterns in-
volving two markers: a metalinguistic noun associ-
ated with a metalinguistic verb. In a more general
way, analysing the defining sentences extracted, we
could see that sentences that were the ?best? defi-
nitions (the closest to dictionary definitions) often
cluded this paradigm context in the ?Other? column.
CompuTerm 2004  -  3rd International Workshop on Computational Terminology60
Marker Expected Other Undecidable Non definition Total
Parenthesis (Parenth?se) Hyp: 25 Meronymy: 1,
Syn-
onymy: 38 (+3),
Transversal: 7
4 84 163
For instance (Par exemple) Hyp: 45 Transversal: 2 32 16 95
A kind of (Une sorte de) Hyp: 1 Transversal: 2 5 5 13
Parenthesis (Parenth?se) Syn: 10 Paradigm: 9 2 4 25
As for (Il s?agit de) Syn: 10 Transversal: 4,
Hypernymy: 1
92 9 115
To indicate (Indiquer) Syn: 5 Transversal: 12 6 77 100
That is (Soit) Syn: 7 Paradigm: 31,
Transversal: 13
15 1 66
To explain (Expliquer) Syn: 1 Transversal: 21 15 28 65
To specify (Sp?cifier) Syn:1 Transversal: 5 9 26 41
To mark (Marquer) Syn: 1 Transversal: 7 6 12 26
Say (Enfin) Syn: 0 Paradigm: 3 2 1 6
Or (Ou) Syn: 3 Paradigm: 23 1 0 27
As (Comme) Syn: 9 Paradigm: 1 1 2 13
That is (A savoir) Syn: 4 Hypernymy: 3 5 0 12
In other words (Autrement dit) Syn: 1 0 2 0 3
Equivalent to (?quivaloir) Syn: 0 0 4 0 4
To define (D?finir) Syn: 22 Transversal: 8 19 19 68
To designate (D?signer) Syn: 3 Hypernymy: 0 0 0 3
Term (Terme) Syn: 1 0 1 0 2
Meaning (Au sens de) Syn: 0 0 1 0 1
That is (C?est-?-dire) Syn: 1 0 0 0 1
To name (Nommer) Syn: 0 0 2 1 3
To name (D?nommer) Syn: 1 0 0 0 1
To refer to (R?f?rer) Syn: 0 0 0 2 2
Expression (Expression) Syn: 0 1 1 0 2
Table 5: Semantic relations in retrieved definitions
involved two or even three markers. This underlines
the interest of introducing a relevance measure that
takes into account the number of markers present in
the sentence.
5 Conclusions
Our experiment tried to link the semantic relation
inherent to different kinds of definitions with the
marker (the heart of our lexico-syntactic patterns)
and more specifically with the lexico-syntactic pat-
terns at the origin of the extraction of the definition
itself. Having a close look at some of the mark-
ers, we can observe that some linguistic items can
be very reliable markers for definition extraction as-
sociated with a semantic relation. We can also find
out that the polysemy of some markers is related
to the domain of the corpus. In that respect, the
reusability of the lexico-syntactic patterns is limited
to a set of markers which were found to be reli-
able across our two corpora. What is more prob-
lematic is the fact that it is sometimes not possible
to make a specific distinction between different se-
mantic relations detected with the same marker in
the context of definitions sharing most of their syn-
tactic contexts. But most of the patterns retrieve a
good rate of defining sentences, some patterns be-
ing more reliable than others; and the more numer-
ous the markers involved, the more likely it is that
we have a definition. And usually these patterns re-
trieve definitions following one main semantic rela-
tion (this is not the case however for parenthesis and
the patterns involving the marker ?? savoir?). This
leads to the hypothesis that if lexico-syntactic pat-
terns may not be used to propose semantic relations
that are valid across different domains, they remain
a good clue for mining definitions, especially defini-
CompuTerm 2004  -  3rd International Workshop on Computational Terminology 61
tions of one type of semantic relation inside a given
domain. Moreover, given a new corpus, applying
the existing patterns to a sub-corpus could lead to
the elicitation of the associated semantic relations
for that corpus, which could be a relevant method-
ology to discover pairs of terms following these as-
sociated relations.
References
A. Auger. 1997. Rep?rage des ?nonc?s d?int?r?t
d?finitoire dans les bases de donn?es textuelles.
Th?se de doctorat, Universit? de Neuch?tel.
E. Cartier. 1997. La d?finition dans les textes
scientifiques et techniques : pr?sentation d?un
outil d?extraction automatique de relations d?fini-
toires. 2e Rencontres "Terminologie et In-
telligence Artificielle" (TIA?97), Equipe de
Recherche en Syntaxe et S?mantique. Toulouse,
3-4 avril 1997:127?140.
U. Chukwu and P. Thoiron. 1989. Reformulation
et rep?rage des termes. La Banque des Mots,
Num?ro sp?cial CTN - INaLF - CNRS:23?53.
J.-P. Descl?s. 1996. Syst?mes d?exploration con-
textuelle. Table ronde sur le Contexte, avril 1996,
Caen.
J. Flowerdew. 1992. Definitions in science lectures.
Linguistics, vol.13 (2):202?221.
C. Fuchs. 1994. Paraphrase et ?nonciation. Paris,
Ophrys.
N. Grabar and S. Berland. 2001. Construire un
corpus web pour l?acquisition terminologique.
4e rencontres Terminologie et Intelligence Arti-
ficielle (TIA 2001), Nancy:44?54.
N. Grabar and T. Hamon. 2004. Les relations dans
les terminologies structur?es : de la th?orie ? la
pratique. Revue d?Intelligence Artificielle (RIA),
18-1:57?85.
M. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. 15th Interna-
tional Conference on Computational Linguistics
(COLING 1992), Nantes:539?545.
R. Martin. 1983. Pour une logique du sens. Paris,
PUF.
I. Meyer. 2001. Extracting knowledge-rich con-
texts for terminography. In D. Bourigault, edi-
tor, Recent advances in Computational Terminol-
ogy, pages 279?302. John Benjamins Publishing
Company, Philadelphia, PA.
S. Muresan and J. L. Klavans. 2002. A method
for automatically building and evaluating dic-
tionary resources. the language Resources and
Evaluation Conference (LREC 2002), Las Pal-
mas, Spain:231?234.
J. Pearson. 1996. The expression of definitions
in specialised texts: a corpus-based analysis.
In M. Gellerstam, J. J?rborg, S. G. Malmgren,
K. Nor?n, L.Rogstr?m, and C. Papmehl, edi-
tors, 7th International Congress on Lexicography
(EURALEX?96), pages 817?824. G?teborg Uni-
versity, G?teborg, Sweden.
J. Rebeyrolle. 2000. Forme et fonction de la d?fi-
nition en discours. Th?se de doctorat, Universit?
de Toulouse II - Le Mirail.
J. C. Sager. 2001. Essays on Definition. John Ben-
jamins, Amsterdam.
L. Trimble. 1985. English for Science and Technol-
ogy: A Discourse Approach. Cambridge Univer-
sity Press, Cambridge.
CompuTerm 2004  -  3rd International Workshop on Computational Terminology62
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 479?489,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Building Specialized Bilingual Lexicons Using Large-Scale Background
Knowledge
Dhouha Bouamor1, Adrian Popescu1, Nasredine Semmar1, Pierre Zweigenbaum2
1 CEA, LIST, Vision and Content Engineering Laboratory, 91191
Gif-sur-Yvette CEDEX, France; firstname.lastname@cea.fr
2LIMSI-CNRS, F-91403 Orsay CEDEX, France; pz@limsi.fr
Abstract
Bilingual lexicons are central components of
machine translation and cross-lingual infor-
mation retrieval systems. Their manual con-
struction requires strong expertise in both lan-
guages involved and is a costly process. Sev-
eral automatic methods were proposed as an
alternative but they often rely on resources
available in a limited number of languages
and their performances are still far behind
the quality of manual translations. We intro-
duce a novel approach to the creation of spe-
cific domain bilingual lexicon that relies on
Wikipedia. This massively multilingual en-
cyclopedia makes it possible to create lexi-
cons for a large number of language pairs.
Wikipedia is used to extract domains in each
language, to link domains between languages
and to create generic translation dictionaries.
The approach is tested on four specialized do-
mains and is compared to three state of the art
approaches using two language pairs: French-
English and Romanian-English. The newly in-
troduced method compares favorably to exist-
ing methods in all configurations tested.
1 Introduction
The plethora of textual information shared on the
Web is strongly multilingual and users? information
needs often go well beyond their knowledge of for-
eign languages. In such cases, efficient machine
translation and cross-lingual information retrieval
systems are needed. Machine translation already has
a decades long history and an array of commercial
systems were already deployed, including Google
Translate 1 and Systran 2. However, due to the intrin-
sic difficulty of the task, a number of related prob-
lems remain open, including: the gap between text
semantics and statistically derived translations, the
scarcity of resources in a large majority of languages
and the quality of automatically obtained resources
and translations. While the first challenge is general
and inherent to any automatic approach, the second
and the third can be at least partially addressed by
an appropriate exploitation of multilingual resources
that are increasingly available on the Web.
In this paper we focus on the automatic creation of
domain-specific bilingual lexicons. Such resources
play a vital role in Natural Language Processing
(NLP) applications that involve different languages.
At first, research on lexical extraction has relied on
the use of parallel corpora (Och and Ney, 2003).
The scarcity of such corpora, in particular for spe-
cialized domains and for language pairs not involv-
ing English, pushed researchers to investigate the
use of comparable corpora (Fung, 1998; Chiao and
Zweigenbaum, 2003). These corpora include texts
which are not exact translation of each other but
share common features such as domain, genre, sam-
pling period, etc.
The basic intuition that underlies bilingual lexi-
con creation is the distributional hypothesis (Harris,
1954) which puts that words with similar meanings
occur in similar contexts. In a multilingual formu-
lation, this hypothesis states that the translations of
a word are likely to appear in similar lexical envi-
ronments across languages (Rapp, 1995). The stan-
dard approach to bilingual lexicon extraction builds
1http://translate.google.com/
2http://www.systransoft.com/
479
on the distributional hypothesis and compares con-
text vectors for each word of the source and tar-
get languages. In this approach, the comparison of
context vectors is conditioned by the existence of a
seed bilingual dictionary. A weakness of the method
is that poor results are obtained for language pairs
that are not closely related (Ismail and Manandhar,
2010). Another important problem occurs whenever
the size of the seed dictionary is small due to ignor-
ing many context words. Conversely, when dictio-
naries are detailed, ambiguity becomes an important
drawback.
We introduce a bilingual lexicon extraction ap-
proach that exploits Wikipedia in an innovative
manner in order to tackle some of the problems
mentioned above. Important advantages of using
Wikipedia are:
? The resource is available in hundreds of lan-
guages and it is structured as unambiguous con-
cepts (i.e. articles).
? The languages are explicitly linked through
concept translations proposed by Wikipedia
contributors.
? It covers a large number of domains and is thus
potentially useful in order to mine a wide array
of specialized lexicons.
Mirroring the advantages, there are a number of
challenges associated with the use of Wikipedia:
? The comparability of concept descriptions in
different languages is highly variable.
? The translation graph is partial since, when
considering any language pair, only a part of
the concepts are available in both languages
and explicitly connected.
? Domains are unequally covered in Wikipedia
(Halavais and Lackaff, 2008) and efficient do-
main targeting is needed.
The approach introduced in this paper aims to
draw on Wikipedia?s advantages while appropri-
ately addressing associated challenges. Among
the techniques devised to mine Wikipedia content,
we hypothesize that an adequate adaptation of Ex-
plicit Semantic Analysis (ESA) (Gabrilovich and
Markovitch, 2007) is fitted to our application con-
text. ESA was already successfully tested in differ-
ent NLP tasks, such as word relatedness estimation
or text classification, and we modify it to mine spe-
cialized domains, to characterize these domains and
to link them across languages.
The evaluation of the newly introduced approach
is realized on four diversified specialized domains
(Breast Cancer, Corporate Finance, Wind Energy
and Mobile Technology) and for two pairs of lan-
guages: French-English and Romanian-English.
This choice allows us to study the behavior of dif-
ferent approaches for a pair of languages that are
richly represented and for a pair that includes Roma-
nian, a language that has fewer associated resources
than French and English. Experimental results show
that the newly introduced approach outperforms the
three state of the art methods that were implemented
for comparison.
2 Related Work
In this section, we first give a review of the stan-
dard approach and then introduce methods that build
upon it. Finally, we discuss works that rely on Ex-
plicit Semantic Analysis to solve other NLP tasks.
2.1 Standard Approach (SA)
Most previous approaches that address bilingual lex-
icon extraction from comparable corpora are based
on the standard approach (Fung, 1998; Chiao and
Zweigenbaum, 2002; Laroche and Langlais, 2010).
This approach is composed of three main steps:
1. Building context vectors: Vectors are first
extracted by identifying the words that ap-
pear around the term to be translated Wcand
in a window of n words. Generally, asso-
ciation measures such as the mutual infor-
mation (Morin and Daille, 2006), the log-
likelihood (Morin and Prochasson, 2011) or the
Discounted Odds-Ratio (Laroche and Langlais,
2010) are employed to shape the context vec-
tors.
2. Translation of context vectors: To enable the
comparison of source and target vectors, source
vectors are translated intoto the target language
by using a seed bilingual dictionary. When-
ever several translations of a context word exist,
480
all translation variants are taken into account.
Words not included in the seed dictionary are
simply ignored.
3. Comparison of source and target vectors:
Given Wcand, its automatically translated con-
text vector is compared to the context vectors
of all possible translations from the target lan-
guage. Most often, the cosine similarity is
used to rank translation candidates but alterna-
tive metrics, including the weighted Jaccard in-
dex (Prochasson et al, 2009) and the city-block
distance (Rapp, 1999), were studied.
2.2 Improvements of the Standard Approach
Most of the improvements of the standard approach
are based on the observation that the more repre-
sentative the context vectors of a candidate word
are, the better the bilingual lexicon extraction is. At
first, additional linguistic resources, such as special-
ized dictionaries (Chiao and Zweigenbaum, 2002) or
transliterated words (Prochasson et al, 2009), were
combined with the seed dictionary to translate con-
text vectors.
The ambiguities that appear in the seed bilingual
dictionary were taken into account more recently.
(Morin and Prochasson, 2011) modify the standard
approach by weighting the different translations ac-
cording to their frequency in the target corpus. In
(Bouamor et al, 2013), we proposed a method that
adds a word sense disambiguation process relying
on semantic similarity measurement from WordNet
to the standard approach. Given a context vector in
the source language, the most probable translation of
polysemous words is identified and used for build-
ing the corresponding vector in the target language.
The most probable translation is identified using the
monosemic words that appear in the same lexical en-
vironment.
On specialized French-English comparable cor-
pora, this approach outperforms the one proposed in
(Morin and Prochasson, 2011), which is itself bet-
ter than the standard approach. The main weakness
of (Bouamor et al, 2013) is that the approach relies
on WordNet and its application depends on the ex-
istence of this resource in the target language. Also,
the method is highly dependent on the coverage of
the seed bilingual dictionary.
2.3 Explicit Semantic Analysis
Explicit Semantic Analysis (ESA) (Gabrilovich and
Markovitch, 2007) is a method that maps textual
documents onto a structured semantic space using
classical text indexing schemes such as TF-IDF. Ex-
amples of semantic spaces used include Wikipedia
or the Open Directory Project but, due to superior
performances, Wikipedia is most frequently used.
In the original evaluation, ESA outperformed state
of the art methods in a word relatedness estimation
task.
Subsequently, ESA was successfully exploited in
other NLP tasks and in information retrieval. Radin-
sky and al. (2011) added a temporal dimension to
word vectors and showed that this addition improves
the results of word relatedness estimation. (Hassan
and Mihalcea, 2011) introduced Salient Semantic
Analysis (SSA), a development of ESA that relies
on the detection of salient concepts prior to map-
ping words to concepts. SSA and the original ESA
implementation were tested on several word related-
ness datasets and results were mixed. Improvements
were obtained for text classification when compar-
ing SSA with the authors? in-house representation
of the method. ESA has weak language depen-
dence and was already deployed in multilingual con-
texts. (Sorg and Cimiano, 2012) extended ESA to
other languages and showed that it is useful in cross-
lingual and multilingual retrieval task. Their focus
was on creating a language independent conceptual
space in which documents would be mapped and
then retrieved.
Some open ESA topics related to bilingual lex-
icon creation include: (1) the document represen-
tation which is simply done by summing individ-
ual contributions of words, (2) the adaptation of the
method to specific domains and (3) the coverage of
the underlying resource in different language.
3 ESA for Bilingual Lexicon Extraction
The main objective of our approach is to devise lex-
icon translation methods that are easily applicable
to a large number of language pairs, while preserv-
ing the overall quality of results. A subordinated
objective is to exploit large scale background mul-
tilingual knowledge, such as the encyclopedic con-
tent available in Wikipedia. As we mentioned, ESA
481
Figure 1: Overview of the Explicit Semantic
Analysis enabled bilingual lexicon extraction.
(Gabrilovich and Markovitch, 2007) was exploited
in a number of NLP tasks but not in bilingual lexi-
con extraction.
Figure 1 shows the overall architecture of the lex-
ical extraction process we propose. The process is
completed in the following three steps:
1. Given a word to be translated and its con-
text vector in the source language, we derive
a ranked list of similar Wikipedia concepts (i.e.
articles) using the ESA inverted index.
2. Then, a translation graph is used to retrieve the
corresponding concepts in the target language.
3. Candidate translations are found through a sta-
tistical processing of concept descriptions from
the ESA direct index in the target language.
In this section, we first introduce the elements of
the original formulation of ESA necessary in our ap-
proach. Then, we detail the three steps that com-
pose the main bilingual lexicon extraction method
illustrated in Figure 1. Finally, as a complement to
the main method we introduce a measure for domain
word specificity and present a method for extracting
generic translation lexicons.
3.1 ESA Word and Concept Representation
Given a semantic space structured using a set of M
concepts and including a dictionary of N words,
a mapping between words and concepts can be
expressed as the following matrix:
w(W1, C1) w(W2, C1) ... w(WN , C1)
w(W1, C2) w(W2, C2) ... w(WN , C2)
... ... ...
w(W1, CM ) w(W2, CM ) ... w(WN , CM )
When Wikipedia is exploited concepts are
equated to Wikipedia articles and the texts of the ar-
ticles are processed in order to obtain the weights
that link words and concepts. In (Gabrilovich and
Markovitch, 2007), the weights w that link words
and concepts were obtained through a classical TF-
IDF weighting of Wikipedia articles. A series of
tweaks destined to improve the method?s perfor-
mance were used and disclosed later3. For instance,
administration articles, lists, articles that are too
short or have too few links are discarded. Higher
weight is given to words in the article title and
more longer articles are favored over shorter ones.
We implemented a part of these tweaks and tested
our own version of ESA with the Wikipedia ver-
sion used in the original implementation. The cor-
relation with human judgments of word relatedness
was 0.72 against 0.75 reported by (Gabrilovich and
Markovitch, 2007). The ESA matrix is sparse since
the N size of the dictionary, is usually in the range
of hundreds of thousands and each concept is usu-
ally described by hundreds of distinct words. The
direct ESA index from Figure 1 is obtained by read-
ing the matrix horizontally while the inverted ESA
index is obtained by reading the matrix vertically.
3https://github.com/faraday/
wikiprep-esa/wiki/roadmap
482
Terme Concepts
action e?valuation d?action, communisme, actionnaire activiste, socialisme,
de?velopement durable . . .
de?ficit crise de la dette dans la zone euro, dette publique, re`gle d?or budge?taire,
de?ficit, trouble du de?ficit de l?attention . . .
cisaillement taux de cisaillement, zone de cisaillement, cisaillement, contrainte de cisaille-
ment, viscoanalyseur . . .
turbine ffc turbine potsdam, turbine a` gaz, turbine, urbine hydraulique, coge?ne?ration
. . .
cryptage TEMPEST, chiffrement, liaison 16, Windows Vista, transfert de fichiers . . .
protocole Ad-hoc On-demand Distance Vector, protocole de Kyoto, optimized link state
routing protocol, liaison 16, IPv6 . . .
biopsie biopsie, maladie de Horton, cancer du sein, cancer du poumon, imagerie par
re?sonance magne?tique . . .
palpation cancer du sein, cellulite, examen clinique, appendicite, te?nosynovite . . .
Table 1: The five most similar Wikipedia concepts to the French terms action[share], de?ficit[deficit], ci-
saillement[shear], turbine[turbine], cryptage[encryption], biopsie[biopsie] and palpation[palpation] and
their context vectors.
3.2 Source Language Processing
The objective of the source language processing is
to obtain a ranked list of similar Wikipedia concepts
for each candidate word (Wcand) in a specialized do-
main. To do this, a context vector is first built for
each Wcand from a specialized monolingual corpus.
The association measure between Wcand and context
words is obtained using the Odds-Ratio (defined in
equation 5). Wikipedia concepts in the source lan-
guage Cs that are similar to Wcand and to a part of its
context words are extracted and ranked using equa-
tion 1.
Rank(Cs) = (10 ?max(Odds
Wcand
Wsi
)
?w(Wcand, Cs)) +
n?
i=1
OddsWcandWsi
?w(Wsi , Cs)
(1)
where max(OddsWcandWsi
) is the highest Odds-Ratio
association between Wcand and any of its context
words Wsi ; the factor 10 was empirically set to
give more importance to Wcand over context words;
w(Wcand, Cs) is the weight of the association be-
tween Wcand and Cs from the ESA matrix; n is the
total number of words Wsi in the context vector of
Wcand; Odds
Wcand
Wsi
is the association value between
Wcand and Wsi and w(Wsi , Cs) are the weights of
the associations between each context word Wsi and
Cs from the ESA matrix. The use of contextual in-
formation in equation 1 serves to characterize the
candidate word in the target domain.
In table 1, we present the five most similar
Wikipedia concepts to the French terms action,
de?ficit, cisaillement, turbine, cryptage, biopsie and
palpation and their context vectors. These terms are
part of the four specialized domains we are studying
here. From observing these examples, we note that
despite the difference between the specialized do-
mains and word ambiguity (words action and proto-
cole), our method has the advantage of successfully
representing each word to be translated by relevant
conceptual spaces.
3.3 Translation Graph Construction
To bridge the gap between the source and target lan-
guages, a concept translation graph that enables the
multilingual extension of ESA is used. This con-
cept translation graph is extracted from the explicit
translation links available in Wikipedia articles and
is exploited in order to connect a word?s conceptual
space in the source language with the correspond-
ing conceptual space in the target language. Only a
part of the articles have translations and the size of
483
the conceptual space in the target language is usu-
ally smaller than the space in the source language.
For instance, the French-English translation graph
contains 940,215 pairs of concepts while the French
and English Wikipedias contain approximately 1.4
million articles, respectively 4.25 million articles.
3.4 Target Language Processing
The third step of the approach takes place in the tar-
get language. Using the translation graph, we select
the 100 most similar concept translations (thresh-
old determined empirically after preliminary exper-
iments) from the target language and use their di-
rect ESA representations in order to retrieve poten-
tial translations for the candidate word Wcand from
source language. These candidate translations Wt
are ranked using equation 2.
Rank(Wt) = (
n?
i=1
w(Wt, Cti)
avg(Cti)
)
? log(count(Wt,S)) (2)
with w(Wt, Cti) is the weight of the translation can-
didate WT for concept Cti from the ESA matrix
in the target language; avg(Cti) is the average TF-
IDF score of words that appear in Cti ; S is the set
of similar concepts Cti in the target language and
count(Wt,S) accounts for the number of different
concepts from S in which the candidate translation
WT appears.
The accumulation of weights w(Wt, Cti) fol-
lows the way original ESA text representations
are calculated (Gabrilovich and Markovitch, 2007)
and avg(Cti) is used in order to correct the
bias of the TF-IDF scheme towards short articles.
log(count(Wt,S)) is used to favor words that are
associated with a larger number of concepts. log
weighting was chosen after preliminary experiments
with a wide range of functions.
3.5 Domain Specificity
In previous works, ESA was usually exploited
in generic tasks that did not require any domain
adaptation. Here we process information from
specific domains and we need to measure the
specificity of words in those domains. The domain
extraction is seeded by using Wikipedia concepts
(noted Cseed) that best describes the domain in
the target language. For instance, in English,
the Corporate Finance domain is seeded with
https://en.wikipedia.org/wiki/Corporate finance.
We extract a set of 10 words with the highest
TF-IDF score from this article (noted SW ) and use
them to retrieve a domain ranking of concepts in the
target language Rankdom(Ct) with equation 3.
Rankdom(Ct) = (
n?
i=1
w(Wti , Ct)
? w(Cseed,Wti)) ? count(SW,Ct) (3)
where n is size of the seed list of words (i.e. 10
items), w(Wti , Ct) is the weight of the domain
words in the concept Ct ; w(Cseed,Wti) is the
weight of Wti in Cseed, the seed concept of the do-
main, and count(SW,Ct) is the number of distinct
seed words from SW that appear in Ct.
The first part of equation 3 sums up the contribu-
tions of different words from SW that appear in Ct
while the second part is meant to further reinforce
articles that contain a larger number of domain key-
words from SW .
Domain delimitation is performed by retaining
articles whose Rankdom(Ct) is at least 1% or the
score of the top Rankdom(Ct) score. This threshold
was set up during preliminary experiments. Given
the delimitation obtained with equation 3, we calcu-
late a domain specificity score (specifdom(Wt)) for
each word that occurs in the domain ( equation 4).
specifdom(Wt) estimates how much of a word?s use
in an underlying corpus is related to a target domain.
specifdom(Wt) =
DFdom(Wt)
DFgen(Wt)
(4)
where DFdom and DFgen stand for the domain and
the generic document frequency of the word Wt.
specifdom(Wt) will be used to favor words with
greater domain specificity over more general ones
when several translations are available in a seed
generic translation lexicon. For instance, the French
word action is ambiguous and has English transla-
tions such as action, stock, share etc. In a general
case, the most frequent translation is action whereas
in a corporate finance context, share or stock are
more relevant. The specificity of the three transla-
tions, from highest to lowest, is: share, stock and ac-
tion and is used to rank these potential translations.
484
3.6 Generic Dictionaries
Generic translation dictionaries, already used by ex-
isting bilingual lexicon extraction approaches, can
also be integrated in the newly proposed approach.
The Wikipedia translation graph is transformed into
a translation dictionary by removing the disam-
biguation marks from ambiguous concept titles, as
well as lists, categories and other administration
pages. Moreover, since the approach does not han-
dle multiword units, we retain only translation pairs
that are composed of unigrams in both languages.
When existing, unigram redirections are also added
in each language.
The obtained dictionaries are incomplete since:
(1) Wikipedia focuses on concepts that are most of-
ten nouns, (2) specialized domain terms often do not
have an associated Wikipedia entry and (3) the trans-
lation graph covers only a fraction of the concepts
available in a language. For instance, the result-
ing translation dictionaries have 193,543 entries for
French-English and 136,681 entries for Romanian-
English. They can be used in addition to or instead
of other resources available and are especially useful
when there are only few other resources that link the
pair of languages processed.
4 Evaluation
The performances of our approach are evaluated
against the standard approach and its developments
proposed by (Morin and Prochasson, 2011) and
(Bouamor et al, 2013). In this section, we first
describe the data and resources we used in our ex-
periments. We then present differents parameters
needed in the implementation of the different meth-
ods tested. Finally, we discuss the obtained results.
4.1 Data and Resources
Comparable corpora
We conducted our experiments on four French-
English and Romanian-English specialized compa-
rable corpora: Corporate Finance, Breast Can-
cer, Wind Energy and Mobile Technology. For
the Romanian-English language pair, we used
Wikipedia to collect comparable corpora for all do-
mains since they were not already available. The
Wikipedia corpora are harvested using a category-
based selection. We consider the topic in the source
Domain FR EN
Corporate Finance 402,486 756,840
Breast Cancer 396,524 524,805
Wind Energy 145,019 345,607
Mobile Technology 197,689 144,168
Domain RO EN
Corporate Finance 206,169 524,805
Breast Cancer 22,539 322,507
Wind Energy 121,118 298,165
Mobile Technology 200,670 124,149
Table 2: Number of content words in the
comparable corpora.
language (for instance Cancer Mamar [Breast Can-
cer]) as a query to Wikipedia and extract all its sub-
topics (i.e., sub-categories) to construct a domain-
specific category tree. Then, based on the con-
structed tree, we collect all Wikipedia articles be-
longing to at least one of these categories and use
inter-language links to build the comparable cor-
pora.
Concerning the French-English pair, we followed
the strategy described above to extract the compa-
rable corpora related to the Corporate Finance and
Breast Cancer domains since they were otherwise
unavailable. For the two other domains, we used
the corpora released in the TTC project4. All cor-
pora were normalized through the following linguis-
tic preprocessing steps: tokenization, part-of-speech
tagging, lemmatization, and function word removal.
The resulting corpora5 sizes are presented in Table
2. The size of the domain corpora vary within and
across languages, with the corporate finance domain
being the richest in both languages. In Romanian,
Breast Cancer is particularly small, with approxi-
mately 22,000 tokens included. This variability will
allow us to test if there is a correlation between cor-
pus size and quality of results.
Bilingual dictionary
The seed generic French-English dictionary used
to translate French context vectors consists of an
in-house manually built resource which contains
approximately 120,000 entries. For Romanian-
4http://www.ttc-project.eu/index.php/
releases-publications
5Comparable corpora will be shared publicly
485
Domain FR-EN RO-EN
Corporate Finance 125 69
Breast Cancer 96 38
Wind Energy 89 38
Mobile Technology 142 94
Table 3: Sizes of the evaluation lists.
English, we used the generic dictionary extracted
following the procedure described in Subsection 3.6.
Gold standard
In bilingual terminology extraction from compara-
ble corpora, a reference list is required to evaluate
the performance of the alignment. Such lists are usu-
ally composed of around 100 single terms (Hazem
and Morin, 2012; Chiao and Zweigenbaum, 2002).
Reference lists6 were created for the four specialized
domains and the two pairs of languages. For the
French-English, reference words from the Corpo-
rate Finance domain were extracted from the glos-
sary of bilingual micro-finance terms7. For Breast
Cancer, the list is derived from the MESH and the
UMLS thesauri8. Concerning Wind Energy and Mo-
bile Technology, lists were extracted from special-
ized glossaries found on the Web. The Romanian-
English gold standard was manually created by a na-
tive speaker starting from the French-English lists.
Table 3 displays the sizes of the obtained lists. Ref-
erence terms pairs were retained if each word com-
posing them appeared at least five times in the com-
parable domain corpora.
4.2 Experimental setup
Aside from those already mentioned, three param-
eters need to be set up: (1) the window size that
defines contexts, (2) the association measure that
measures the strength of the association between
words and the (3) similarity measure that ranks can-
didate translations for state of the art methods. Con-
text vectors are defined using a seven-word window
which approximates syntactic dependencies. The
association and the similarity measures (Discounted
Log-Odds ratio (equation 5) and the cosine simi-
6Reference lists will be shared publicly
7http://www.microfinance.lu/en/
8http://www.nlm.nih.gov/
larity) were set following Laroche and Langlais
(2010), a comprehensive study of the influence of
these parameters on the bilingual alignment.
Odds-Ratiodisc = log
(O11 + 12 )(O22 +
1
2 )
(O12 + 12 )(O21 +
1
2 )
(5)
where Oij are the cells of the 2? 2 contingency ma-
trix of a token s co-occurring with the term S within
a given window size.
The F-measure of the Top 20 results (F-
Measure@20), which measures the harmonic mean
of precision and recall, is used as evaluation metric.
Precision is the total number of correct translations
divided by the number of terms for which the system
returned at least one answer. Recall is equal to the
ratio between the number of correct translation and
the total number of words to translate (Wcand).
4.3 Results and discussion
In addition to the basic approach based on ESA
(denoted ESA), we evaluate the performances of
a method so-called DicoSpec in which the transla-
tions are extracted from a generic dictionary and
a method we called ESASpec which combine ESA
and DicoSpec. DICOSpec is based on the generic
dictionary we presented in subsection 3.6 and pro-
ceeds as follows: we extract a list of translations for
each word to be translated from the generic dictio-
nary. The domain specificity introduced in subsec-
tion 3.5 is then used to rank these translations. For
instance, the french term port referring in the Mobile
Technology domain, to the system that allows com-
puters to receive and transmit information is trans-
lated into port and seaport. According to domain
specificity values, the following ranking is obtained:
the English term port obtain the highest specificity
value (0.48). seaport comes next with a specificity
value of 0.01. In ESASpec, the translations set out in
the translations lists proposed by both ESA and the
generic dictionary are weighted according to their
domain specificity values. The main intuition be-
hind this method is that by adding the information
about the domain specificity, we obtain a new rank-
ing of the bilingual extraction results.
The obtained results are displayed in table 4. The
comparison of state of the art method shows that
BA13 performs better than STAPP and MP11 for
French-English and has comparable performances
486
a)
F
R
-E
N
Method
F-Measure@20
Breast Cancer Corporate Finance Wind Eenrgy Mobile Technology
STAPP 0.49 0.17 0.08 0.06
MP11 0.55 0.33 0.24 0.05
BA13 0.61 0.37 0.30 0.24
Dicospec 0.50 0.20 0.36 0.25
ESA 0.74 0.50 0.83 0.72
ESAspec 0.81 0.56 0.86 0.75
b)
R
O
-E
N
Method
F-Measure@20
Breast Cancer Corporate Finance Wind Eenrgy Mobile Technology
STAPP 0.21 0.13 0.08 0.16
MP11 0.21 0.13 0.08 0.16
BA13 0.21 0.14 0.08 0.17
Dicospec 0.44 0.11 0.21 0.16
ESA 0.76 0.17 0.58 0.53
ESAspec 0.78 0.24 0.58 0.55
Table 4: Results of the specialized dictionary creation on four specific domains, two pairs of languages.Three
state of the art methods were used for comparison: STAPP is the standard approach, MP11 is the improve-
ment of the standard approach introduced in (Morin and Prochasson, 2011), BA13 is a recent method that
we developed (Bouamor et al, 2013). Dicospec exploits a generic dictionary, combined with the use of do-
main specificity (see Subsection 3.5). ESA stands for the ESA based approach introduced in this paper (see
Figure 1). ESAspec combines the results of Dicospec and ESA.
for RO-EN. Consequently, we will use BA13 as the
main baseline for discussing the newly introduced
approach. The results presented in Table 4 show
that ESAspec clearly outperforms the three base-
lines for the four domains and the two pairs of lan-
guages tested. When comparing ESAspec to BA13
for French-English, improvements range between
0.19 for Corporate Finance and 0.56 for Wind En-
ergy. For RO-EN, the improvements vary from 0.1
for Corporate Finance to 0.5 for Wind Energy. Also,
except for the Corporate Finance domain in Roma-
nian, the performance variation across domains is
much smaller for ESAspec than for the three state
of the art methods. This shows that ESAspec is more
robust to domain change and thus more generic.
The results obtained with ESA are signifi-
cantly better than those obtained with Dicospec and
ESAspec, their combination, further improves the
results. The main contribution to ESAspec perfor-
mances comes from ESA, a finding that validates
our assumption that the adequate use of a rich multi-
lingual resource such as Wikipedia is appropriate for
specialized lexicon translation. Dicospec is a sim-
ple method that ranks the different meanings of a
candidate word available in a generic dictionary but
its average performances are comparable to those
of BA13 for FR-EN and higher for RO-EN. This
finding advocates for the importance of good qual-
ity generic dictionaries in specialized lexicon trans-
lation approaches. However, it is clear that such
dictionaries are far from being sufficient in order
to cover all possible domains. There is no clear
correlation between domain size and quality of re-
sults. Although richer than the other three domains,
Corporate Finance has the lowest associated per-
formances. This finding is probably explained by
the intrinsic difficulty of each domain. When pass-
ing from FR-EN to RO-EN the average performance
drop is more significant for BA13 than for the ESA
based methods. The result indicates that our ap-
proach is more robust to language change.
5 Conclusion
We have presented a new approach to the creation
of specialized bilingual lexicons, one of the central
487
building blocks of machine translation systems. The
proposed approach directly tackles two of the ma-
jor challenges identified in the Introduction. The
scarcity of resources is addressed by an adequate
exploitation of Wikipedia, a resource that is avail-
able in hundreds of languages. The quality of auto-
matic translations was improved by appropriate do-
main delimitation and linking across languages, as
well as by an adequate statistical processing of con-
cepts similar to a word in a given context.
The main advantages of our approach compared
to state of the art methods come from: the increased
number of languages that can be processed, from
the smaller sensitivity to structured resources and
the appropriate domain delimitation. Experimental
validation is obtained through evaluation with four
different domains and two pairs of languages which
shows consistent performance improvement. For
French-English, two languages that have rich asso-
ciated Wikipedia representations, performances are
very interesting and are starting to approach those of
manual translations for three domains out of four (F-
Measure@20 around 0.8). For Romanian-English, a
pair involving a language with a sparser Wikipedia
representation, the performances of our method drop
compared to French-English . However, they do not
decrease to the same extent as those of the best state
of the art method tested. This finding indicates that
our approach is more general and, given its low lan-
guage dependence, it can be easily extended to a
large number of language pairs.
The results presented here are very encouraging
and we will to pursue work in several directions.
First, we will pursue the integration of our method,
notably through comparable corpora creation using
the data driven domain delimitation technique de-
scribed in Subsection 3.5. Equally important, the
size of the domain can be adapted so as to find
enough context for all the words in domain reference
lists. Second, given a word in a context, we currently
exploit all similar concepts from the target language.
Given that comparability of article versions in the
source and the target language varies, we will eval-
uate algorithms for filtering out concepts from the
target language that have low alignment with their
source language versions. A final line of work is
constituted by the use of distributional properties of
texts in order to automatically rank parts of concept
descriptions (i.e. articles) by their relatedness to the
candidate word. Similar to the second direction, this
process involves finding comparable text blocks but
rather at a paragraph or sentence level than at the
article level.
References
Dhouha Bouamor, Nasredine Semmar, and Pierre
Zweigenbaum. 2013. Context vector disambiguation
for bilingual lexicon extraction. In Proceedings of the
51st Association for Computational Linguistics (ACL-
HLT), Sofia, Bulgaria, August.
Yun-Chuang Chiao and Pierre Zweigenbaum. 2002.
Looking for candidate translational equivalents in spe-
cialized, comparable corpora. In Proceedings of the
19th international conference on Computational lin-
guistics - Volume 2, COLING ?02, pages 1?5. Associ-
ation for Computational Linguistics.
Yun-Chuang Chiao and Pierre Zweigenbaum. 2003. The
effect of a general lexicon in corpus-based identifi-
cation of french-english medical word translations.
In Proceedings Medical Informatics Europe, volume
95 of Studies in Health Technology and Informatics,
pages 397?402, Amsterdam.
Pascale Fung. 1998. A statistical view on bilingual lexi-
con extraction: From parallel corpora to non-parallel
corpora. In Parallel Text Processing, pages 1?17.
Springer.
Evgeniy Gabrilovich and Shaul Markovitch. 2007. Com-
puting semantic relatedness using wikipedia-based
explicit semantic analysis. In Proceedings of the
20th international joint conference on Artifical intel-
ligence, IJCAI?07, pages 1606?1611, San Francisco,
CA, USA. Morgan Kaufmann Publishers Inc.
Alexander Halavais and Derek Lackaff. 2008. An Anal-
ysis of Topical Coverage of Wikipedia. Journal of
Computer-Mediated Communication, 13(2):429?440.
Z.S. Harris. 1954. Distributional structure. Word.
Samer Hassan and Rada Mihalcea. 2011. Semantic re-
latedness using salient semantic analysis. In AAAI.
Amir Hazem and Emmanuel Morin. 2012. Adaptive dic-
tionary for bilingual lexicon extraction from compara-
ble corpora. In Proceedings, 8th international confer-
ence on Language Resources and Evaluation (LREC),
Istanbul, Turkey, May.
Azniah Ismail and Suresh Manandhar. 2010. Bilin-
gual lexicon extraction from comparable corpora us-
ing in-domain terms. In Proceedings of the 23rd In-
ternational Conference on Computational Linguistics:
Posters, COLING ?10, pages 481?489. Association for
Computational Linguistics.
488
Audrey Laroche and Philippe Langlais. 2010. Revisiting
context-based projection methods for term-translation
spotting in comparable corpora. In 23rd Interna-
tional Conference on Computational Linguistics (Col-
ing 2010), pages 617?625, Beijing, China, Aug.
Emmanuel Morin and Be?atrice Daille. 2006. Compara-
bilite? de corpus et fouille terminologique multilingue.
In Traitement Automatique des Langues (TAL).
Emmanuel Morin and Emmanuel Prochasson. 2011.
Bilingual lexicon extraction from comparable corpora
enhanced with parallel corpora. In Proceedings, 4th
Workshop on Building and Using Comparable Cor-
pora (BUCC), page 27?34, Portland, Oregon, USA.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Comput. Linguist., 29(1):19?51, March.
Emmanuel Prochasson, Emmanuel Morin, and Kyo
Kageura. 2009. Anchor points for bilingual lexi-
con extraction from small comparable corpora. In
Proceedings, 12th Conference on Machine Translation
Summit (MT Summit XII), page 284?291, Ottawa, On-
tario, Canada.
Kira Radinsky, Eugene Agichtein, Evgeniy Gabrilovich,
and Shaul Markovitch. 2011. A word at a time: com-
puting word relatedness using temporal semantic anal-
ysis. In Proceedings of the 20th international confer-
ence on World wide web, WWW ?11, pages 337?346,
New York, NY, USA. ACM.
Reinhard Rapp. 1995. Identifying word translations in
non-parallel texts. In Proceedings of the 33rd annual
meeting on Association for Computational Linguistics,
ACL ?95, pages 320?322. Association for Computa-
tional Linguistics.
Reinhard Rapp. 1999. Automatic identification of word
translations from unrelated english and german cor-
pora. In Proceedings of the 37th annual meeting of the
Association for Computational Linguistics on Compu-
tational Linguistics, ACL ?99, pages 519?526. Asso-
ciation for Computational Linguistics.
P. Sorg and P. Cimiano. 2012. Exploiting wikipedia for
cross-lingual and multilingual information retrieval.
Data Knowl. Eng., 74:26?45, April.
489
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 759?764,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Context Vector Disambiguation for Bilingual Lexicon Extraction from
Comparable Corpora
Dhouha Bouamor
CEA, LIST, Vision and
Content Engineering Laboratory,
91191 Gif-sur-Yvette CEDEX
France
dhouha.bouamor@cea.fr
Nasredine Semmar
CEA, LIST, Vision and Content
Engineering Laboratory,
91191 Gif-sur-Yvette
CEDEX France
nasredine.semmar@cea.fr
Pierre Zweigenbaum
LIMSI-CNRS,
F-91403 Orsay CEDEX
France
pz@limsi.fr
Abstract
This paper presents an approach that ex-
tends the standard approach used for bilin-
gual lexicon extraction from comparable
corpora. We focus on the unresolved prob-
lem of polysemous words revealed by the
bilingual dictionary and introduce a use of
a Word Sense Disambiguation process that
aims at improving the adequacy of con-
text vectors. On two specialized French-
English comparable corpora, empirical ex-
perimental results show that our method
improves the results obtained by two state-
of-the-art approaches.
1 Introduction
Over the years, bilingual lexicon extraction from
comparable corpora has attracted a wealth of re-
search works (Fung, 1998; Rapp, 1995; Chiao
and Zweigenbaum, 2003). The basic assumption
behind most studies is a distributional hypothe-
sis (Harris, 1954), which states that words with a
similar meaning are likely to appear in similar con-
texts across languages. The so-called standard ap-
proach to bilingual lexicon extraction from com-
parable corpora is based on the characterization
and comparison of context vectors of source and
target words. Each element in the context vector
of a source or target word represents its associa-
tion with a word which occurs within a window
of N words. To enable the comparison of source
and target vectors, words in the source vectors are
translated into the target language using an exist-
ing bilingual dictionary.
The core of the standard approach is the bilin-
gual dictionary. Its use is problematic when a word
has several translations, whether they are synony-
mous or polysemous. For instance, the French
word action can be translated into English as
share, stock, lawsuit or deed. In such cases, it
is difficult to identify in flat resources like bilin-
gual dictionaries which translations are most rel-
evant. The standard approach considers all avail-
able translations and gives them the same impor-
tance in the resulting translated context vectors in-
dependently of the domain of interest and word
ambiguity. Thus, in the financial domain, trans-
lating action into deed or lawsuit would introduce
noise in context vectors.
In this paper, we present a novel approach that
addresses the word polysemy problem neglected
in the standard approach. We introduce a Word
Sense Disambiguation (WSD) process that iden-
tifies the translations of polysemous words that
are more likely to give the best representation of
context vectors in the target language. For this
purpose, we employ five WordNet-based semantic
similarity and relatedness measures and use a data
fusion method that merges the results obtained by
each measure. We test our approach on two spe-
cialized French-English comparable corpora (fi-
nancial and medical) and report improved results
compared to two state-of-the-art approaches.
2 Related Work
Most previous works addressing the task of bilin-
gual lexicon extraction from comparable corpora
are based on the standard approach. In order to
improve the results of this approach, recent re-
searches based on the assumption that more the
context vectors are representative, better is the
bilingual lexicon extraction were conducted. In
these works, additional linguistic resources such
as specialized dictionaries (Chiao and Zweigen-
baum, 2002) or transliterated words (Prochasson
et al, 2009) were combined with the bilingual dic-
759
tionary to translate context vectors. Few works
have however focused on the ambiguity problem
revealed by the seed bilingual dictionary. (Hazem
and Morin, 2012) propose a method that filters the
entries of the bilingual dictionary on the base of
a POS-Tagging and a domain relevance measure
criteria but no improvements have been demon-
strated. Gaussier et al (2004) attempted to solve
the problem of word ambiguities in the source and
target languages. They investigated a number of
techniques including canonical correlation analy-
sis and multilingual probabilistic latent semantic
analysis. The best results, with an improvement of
the F-Measure (+0.02 at Top20) were reported for
a mixed method. Recently, (Morin and Prochas-
son, 2011) proceed as the standard approach but
weigh the different translations according to their
frequency in the target corpus. Here, we propose a
method that differs from Gaussier et al (2004) in
this way: If they focus on words ambiguities on
source and target languages, we thought that it
would be sufficient to disambiguate only trans-
lated source context vectors.
3 Context Vector Disambiguation
3.1 Semantic similarity measures
A large number of WSD techniques were pro-
posed in the literature. The most widely used ones
are those that compute semantic similarity1 with
the help of WordNet. WordNet has been used in
many tasks relying on word-based similarity, in-
cluding document (Hwang et al, 2011) and im-
age (Cho et al, 2007; Choi et al, 2012) retrieval
systems. In this work, we use it to derive a se-
mantic similarity between lexical units within the
same context vector. To the best of our knowledge,
this is the first application of WordNet to bilingual
lexicon extraction from comparable corpora.
Among semantic similarity measures using
WordNet, we distinguish: (1) measures based on
path length which simply counts the distance be-
tween two words in the WordNet taxonomy, (2)
measures relying on information content in which
a semantically annotated corpus is needed to com-
pute frequencies of words to be compared and (3)
the ones using gloss overlap which are designed
to compute semantic relatedness. In this work,
we use five similarity measures and compare
their performances. These measures include three
1For consiseness, we often use ?semantic similarity? to
refer collectively to both similarity and relatedness.
path-based semantic similarity measures denoted
PATH,WUP (Wu and Palmer, 1994) and LEA-
COCK (Leacock and Chodorow, 1998). PATH is
a baseline that is equal to the inverse of the short-
est path between two words. WUP finds the depth
of the least common subsumer of the words, and
scales that by the sum of the depths of individual
words. The depth of a word is its distance to the
root node. LEACOCK finds the shortest path be-
tween two words, and scales that by the maximum
path length found in the is?a hierarchy in which
they occur. Path length measures have the advan-
tage of being independent of corpus statistics, and
therefor uninfluenced by sparse data.
Since semantic relatedness is considered to be
more general than semantic similarity, we also
use two relatedness measures: LESK (Banerjee
and Pedersen, 2002) and VECTOR (Patwardhan,
2003). LESK finds overlaps between the glosses
of word pairs, as well as words? hyponyms. VEC-
TOR creates a co-occurrence matrix for each gloss
token. Each gloss is then represented as a vector
that averages token co-occurrences.
3.2 Disambiguation process
Once translated into the target language, the con-
text vectors disambiguation process intervenes.
This process operates locally on each context vec-
tor and aims at finding the most prominent trans-
lations of polysemous words. For this purpose,
we use monosemic words as a seed set of dis-
ambiguated words to infer the polysemous word?s
translations senses. We hypothesize that a word is
monosemic if it is associated to only one entry in
the bilingual dictionary. We checked this assump-
tion by probing monosemic entries of the bilingual
dictionary against WordNet and found that 95% of
the entries are monosemic in both resources. Ac-
cording to the above-described semantic similarity
measures, a similarity value SimV alue is derived
between all the translations provided for each pol-
ysemous word by the bilingual dictionary and all
monosemic words appearing within the same con-
text vector. In practice, since a word can belong to
more than one synset2 in WordNet, the semantic
similarity between two words w1 and w2 is defined
as the maximum of SimV alue between the synset
or the synsets that include the synsets(w1) and
2a group of a synonymous words in WordNet
760
synsets(w2) according to the following equation:
SemSim(w1, w2) = max{SimV alue(s1, s2);
(s1, s2) ? synsets(w1)? synsets(w2)} (1)
Then, to identify the most prominent transla-
tions of each polysemous unit wp, an average sim-
ilarity is computed for each translation wjp of wp:
Ave Sim(wjp) =
1
N
NX
i=1
SemSim(wi, wjp) (2)
where N is the total number of monosemic words
in each context vector and SemSim is the simi-
larity value of wjp and the ith monosemic word.
Hence, according to average similarity values
Ave Sim(wjp), we obtain for each polysemous
word wp an ordered list of translations w1p . . . wnp .
4 Experiments and Results
4.1 Resources and Experimental Setup
We conducted our experiments on two French-
English comparable corpora specialized on the
corporate finance and the breast cancer sub-
domains. Both corpora were extracted from
Wikipedia3. We consider the domain topic in
the source language (for instance cancer du sein
[breast cancer]) as a query to Wikipedia and
extract all its sub-topics (i.e., sub-categories in
Wikipedia) to construct a domain-specific cate-
gories tree. Then we collected all articles belong-
ing to one of these categories and used inter-
language links to build the comparable corpus.
Both corpora have been normalized through the
following linguistic preprocessing steps: tokeni-
sation, part-of-speech tagging, lemmatisation and
function words removal. The resulting corpora4
sizes as well as their polysemy rate PR are given
in Table 1. The polysemy rate indicates how much
words in the comparable corpora are associated
to more than one translation in the seed bilingual
dictionary. The dictionary consists of an in-house
bilingual dictionary which contains about 120,000
entries belonging to the general language with an
average of 7 translations per entry.
In bilingual terminology extraction from com-
parable corpora, a reference list is required to
evaluate the performance of the alignment. Such
lists are often composed of about 100 single
3http://dumps.wikimedia.org/
4Comparable corpora will be shared publicly
Corpus French English PR
Corporate finance 402.486 756.840 41%
Breast cancer 396.524 524.805 47%
Table 1: Comparable corpora sizes in term of
words and polysemy rates (PR) associated to each
corpus
terms (Hazem and Morin, 2012; Chiao and
Zweigenbaum, 2002). Here, we created two ref-
erence lists5 for the corporate finance and the
breast cancer sub-domains. The first list is com-
posed of 125 single terms extracted from the glos-
sary of bilingual micro-finance terms6. The second
list contains 79 terms extracted from the French-
English MESH and the UMLS thesauri7. Note
that reference terms pairs appear more than five
times in each part of both comparable corpora.
Three other parameters need to be set up,
namely the window size, the association measure
and the similarity measure. We followed (Laroche
and Langlais, 2010) to define these parame-
ters. They carried out a complete study of the
influence of these parameters on the bilingual
alignment. The context vectors were defined by
computing the Discounted Log-Odds Ratio (equa-
tion 3) between words occurring in the same con-
text window of size 7.
Odds-Ratiodisc = log (O11 +
1
2 )(O22 + 12 )
(O12 + 12 )(O21 + 12 )
(3)
where Oij are the cells of the 2 ? 2 contingency
matrix of a token s co-occurring with the term S
within a given window size. As similarity mea-
sure, we chose to use the cosine measure.
4.2 Results of bilingual lexicon extraction
To evaluate the performance of our approach, we
used both the standard approach (SA) and the ap-
proach proposed by (Morin and Prochasson, 2011)
(henceforth MP11) as baselines. The experiments
were performed with respect to the five semantic
similarity measures described in section 3.1. Each
measure provides, for each polysemous word, a
ranked list of translations. A question that arises
here is whether we should introduce only the top-
ranked translation into the context vector or con-
sider a larger number of translations, mainly when
a translation list contains synonyms. For this
5Reference lists will be shared publicly
6http://www.microfinance.lu/en/
7http://www.nlm.nih.gov/
761
a)
Co
rpo
rat
eF
ina
nc
e Method WN-T1 WN-T2 WN-T3 WN-T4 WN-T5 WN-T6 WN-T7Standard Approach (SA) 0.172
MP11 0.336
Sin
gle
me
asu
re
WUP 0.241 0.284 0.301 0.275 0.258 0.215 0.224
PATH 0.250 0.284 0.301 0.284 0.258 0.215 0.215
LEACOCK 0.250 0.293 0.301 0.275 0.275 0.241 0.232
LESK 0.272 0.293 0.293 0.275 0.258 0.250 0.215
VECTOR 0.267 0.310 0.284 0.284 0.232 0.232 0.232
CONDORCETMerge 0.362 0.379 0.353 0.362 0.336 0.275 0.267
b)
Br
eas
tC
an
cer
Method WN-T1 WN-T2 WN-T3 WN-T4 WN-T5 WN-T6 WN-T7
Standard Approach (SA) 0.493
MP11 0.553
Sin
gle
me
asu
re
WUP 0.481 0.566 0.566 0.542 0.554 0.542 0.554
PATH 0.542 0.542 0.554 0.566 0.578 0.554 0.554
LEACOCK 0.506 0.578 0.554 0.566 0.542 0.554 0.542
LESK 0.469 0.542 0.542 0.590 0.554 0.554 0.542
VECTOR 0.518 0.566 0.530 0.566 0.542 0.566 0.554
CONDORCETMerge 0.566 0.614 0.600 0.590 0.600 0.578 0.578
Table 2: F-Measure at Top20 for the two domains; MP11 = (Morin and Prochasson, 2011). In each
column, italics shows best single similarity measure, bold shows best result. Underline shows best result
overall.
reason, we take into account in our experiments
different numbers of translations, noted WN-Ti,
ranging from the pivot translation (i = 1) to the
seventh word in the translation list. This choice is
motivated by the fact that words in both corpora
have on average 7 translations in the bilingual dic-
tionary. Both baseline systems use all translations
associated to each entry in the bilingual dictionary.
The only difference is that in MP11 translations
are weighted according to their frequency in the
target corpus.
The results of different works focusing on bilin-
gual lexicon extraction from comparable corpora
are evaluated on the number of correct candidates
found in the first N first candidates output by the
alignment process (the TopN ). Here, we use the
Top20 F-measure as evaluation metric. The results
obtained for the corporate finance corpus are pre-
sented in Table 2a. The first notable observation is
that disambiguating context vectors using seman-
tic similarity measures outperforms the SA. The
highest F-measure is reported by VECTOR. Us-
ing the top two words (WN-T2) in context vec-
tors increases the F-measure from 0.172 to 0.310.
However, compared to MP11, no improvement
is achieved. Concerning the breast cancer cor-
pus, Table 2b shows improvements in most cases
over both the SA and MP11. The maximum F-
measure was obtained by LESK when for each
polysemous word up to four translations (WN-T4)
are considered in context vectors. This method
achieves an improvement of respectively +0.097
and +0.037% over SA and MP11.
Each of the tested 5 semantic similarity mea-
sures provides a different view of how to rank
the translations of a given test word. Combining
the obtained ranked lists should reinforce the con-
fidence in consensus translations, while decreas-
ing the confidence in non-consensus translations.
We have therefore tested their combination. For
this, we used a voting method, and chose one in
the Condorcet family the Condorcet data fusion
method. This method was widely used to combine
document retrieval results from information re-
trieval systems (Montague and Aslam, 2002; Nu-
ray and Can, 2006). It is a single-winner election
method that ranks the candidates in order of pref-
erence. It is a pairwise voting, i.e. it compares ev-
ery possible pair of candidates to decide the pref-
erence of them. A matrix can be used to present
the competition process. Every candidate appears
in the matrix as a row and a column as well. If
there are m candidates, then we need m2 elements
in the matrix in total. Initially 0 is written to all the
elements. If di is preferred to dj , then we add 1 to
the element at row i and column j (aij). The pro-
762
cess is repeated until all the ballots are processed.
For every element aij , if aij > m/2 , then di
beats dj ; if aij < m/2, then dj beats di; other-
wise (aij = m/2), there is a draw between di and
dj . The total score of each candidate is quantified
by summing the raw scores it obtains in all pair-
wise competitions. Finally the ranking is achiev-
able based on the total scores calculated.
Here, we view the ranking of the extraction re-
sults from different similarity measures as a spe-
cial instance of the voting problem where the
Top20 extraction results correspond to candidates
and different semantic similarity measures are the
voters. The combination method referred to as
CONDORCETMerge outperformed all the others
(see Tables 2a and 2b): (1) individual measures,
(2) SA, and (3) MP11. Even though the two cor-
pora are fairly different (subject and polysemy
rate), the optimal results are obtained when con-
sidering up to two most similar translations in con-
text vectors. This behavior shows that the fusion
method is robust to domain change. The addition
of supplementary translations, which are probably
noisy in the given domain, degrades the overall re-
sults. The F-measure gains with respect to SA are
+0.207 for corporate finance and +0.121 for the
breast cancer corpus. More interestingly, our ap-
proach outperforms MP11, showing that the role
of disambiguation is more important than that of
feature weighting.
5 Conclusion
We presented in this paper a novel method that
extends the standard approach used for bilingual
lexicon extraction. This method disambiguates
polysemous words in context vectors by selecting
only the most relevant translations. Five seman-
tic similarity and relatedness measures were used
for this purpose. Experiments conducted on two
specialized comparable corpora indicate that the
combination of similarity metrics leads to a better
performance than two state-of-the-art approaches.
This shows that the ambiguity present in special-
ized comparable corpora hampers bilingual lexi-
con extraction, and that methods such as the one
introduced here are needed. The obtained results
are very encouraging and can be improved in a
number of ways. First, we plan to mine much
larger specialized comparable corpora and focus
on their quality (Li and Gaussier, 2010). We also
plan to test our method on bilingual lexicon extrac-
tion from general-domain corpora, where ambigu-
ity is generally higher and disambiguation meth-
ods should be all the more needed.
References
Satanjeev Banerjee and Ted Pedersen. 2002. An
adapted lesk algorithm for word sense disambigua-
tion using wordnet. In Proceedings of the Third In-
ternational Conference on Computational Linguis-
tics and Intelligent Text Processing, CICLing ?02,
pages 136?145, London, UK, UK. Springer-Verlag.
Yun-Chuang Chiao and Pierre Zweigenbaum. 2002.
Looking for candidate translational equivalents in
specialized, comparable corpora. In Proceedings of
the 19th international conference on Computational
linguistics - Volume 2, COLING ?02, pages 1?5. As-
sociation for Computational Linguistics.
Yun-Chuang Chiao and Pierre Zweigenbaum. 2003.
The effect of a general lexicon in corpus-based iden-
tification of french-english medical word transla-
tions. In Proceedings Medical Informatics Europe,
volume 95 of Studies in Health Technology and In-
formatics, pages 397?402, Amsterdam.
Miyoung Cho, Chang Choi, Hanil Kim, Jungpil Shin,
and PanKoo Kim. 2007. Efficient image retrieval
using conceptualization of annotated images. Lec-
ture Notes in Computer Science, pages 426?433.
Springer.
Dongjin Choi, Jungin Kim, Hayoung Kim, Myungg-
won Hwang, and Pankoo Kim. 2012. A method for
enhancing image retrieval based on annotation using
modified wup similarity in wordnet. In Proceed-
ings of the 11th WSEAS international conference
on Artificial Intelligence, Knowledge Engineering
and Data Bases, AIKED?12, pages 83?87, Stevens
Point, Wisconsin, USA. World Scientific and Engi-
neering Academy and Society (WSEAS).
Pascale Fung. 1998. A statistical view on bilingual
lexicon extraction: From parallel corpora to non-
parallel corpora. In Parallel Text Processing, pages
1?17. Springer.
E?ric Gaussier, Jean-Michel Renders, Irina Matveeva,
Cyril Goutte, and Herve? De?jean. 2004. A geometric
view on bilingual lexicon extraction from compara-
ble corpora. In ACL, pages 526?533.
Z.S. Harris. 1954. Distributional structure. Word.
Amir Hazem and Emmanuel Morin. 2012. Adap-
tive dictionary for bilingual lexicon extraction from
comparable corpora. In Proceedings, 8th interna-
tional conference on Language Resources and Eval-
uation (LREC), Istanbul, Turkey, May.
Myunggwon Hwang, Chang Choi, and Pankoo Kim.
2011. Automatic enrichment of semantic relation
763
network and its application to word sense disam-
biguation. IEEE Transactions on Knowledge and
Data Engineering, 23:845?858.
Audrey Laroche and Philippe Langlais. 2010. Re-
visiting context-based projection methods for term-
translation spotting in comparable corpora. In 23rd
International Conference on Computational Lin-
guistics (Coling 2010), pages 617?625, Beijing,
China, Aug.
Claudia Leacock and Martin Chodorow, 1998. Com-
bining local context and WordNet similarity for word
sense identification, pages 305?332. In C. Fellbaum
(Ed.), MIT Press.
Bo Li and E?ric Gaussier. 2010. Improving corpus
comparability for bilingual lexicon extraction from
comparable corpora. In 23rd International Confer-
ence on Computational Linguistics (Coling 2010),
Beijing, China, Aug.
Mark Montague and Javed A. Aslam. 2002. Con-
dorcet fusion for improved retrieval. In Proceedings
of the eleventh international conference on Informa-
tion and knowledge management, CIKM ?02, pages
538?548, New York, NY, USA. ACM.
Emmanuel Morin and Emmanuel Prochasson. 2011.
Bilingual lexicon extraction from comparable cor-
pora enhanced with parallel corpora. In Proceed-
ings, 4th Workshop on Building and Using Compa-
rable Corpora (BUCC), page 27?34, Portland, Ore-
gon, USA.
Rabia Nuray and Fazli Can. 2006. Automatic ranking
of information retrieval systems using data fusion.
Inf. Process. Manage., 42(3):595?614, May.
Siddharth Patwardhan. 2003. Incorporating Dictio-
nary and Corpus Information into a Context Vector
Measure of Semantic Relatedness. Master?s thesis,
University of Minnesota, Duluth, August.
Emmanuel Prochasson, Emmanuel Morin, and Kyo
Kageura. 2009. Anchor points for bilingual lexi-
con extraction from small comparable corpora. In
Proceedings, 12th Conference on Machine Transla-
tion Summit (MT Summit XII), page 284?291, Ot-
tawa, Ontario, Canada.
Reinhard Rapp. 1995. Identifying word translations in
non-parallel texts. In Proceedings of the 33rd an-
nual meeting on Association for Computational Lin-
guistics, ACL ?95, pages 320?322. Association for
Computational Linguistics.
Zhibiao Wu and Martha Palmer. 1994. Verbs seman-
tics and lexical selection. In Proceedings of the 32nd
annual meeting on Association for Computational
Linguistics, ACL ?94, pages 133?138. Association
for Computational Linguistics.
764
Proceedings of the 2011 Workshop on Biomedical Natural Language Processing, ACL-HLT 2011, pages 56?64,
Portland, Oregon, USA, June 23-24, 2011. c?2011 Association for Computational Linguistics
Medical Entity Recognition:
A Comparison of Semantic and Statistical Methods
Asma Ben Abacha
LIMSI-CNRS
BP 133, 91403 Orsay Cedex, France
asma.benabacha@limsi.fr
Pierre Zweigenbaum
LIMSI-CNRS
BP 133, 91403 Orsay Cedex, France
pz@limsi.fr
Abstract
Medical Entity Recognition is a crucial step
towards efficient medical texts analysis. In
this paper we present and compare three
methods based on domain-knowledge and
machine-learning techniques. We study two
research directions through these approaches:
(i) a first direction where noun phrases are
extracted in a first step with a chunker be-
fore the final classification step and (ii) a sec-
ond direction where machine learning tech-
niques are used to identify simultaneously en-
tities boundaries and categories. Each of the
presented approaches is tested on a standard
corpus of clinical texts. The obtained results
show that the hybrid approach based on both
machine learning and domain knowledge ob-
tains the best performance.
1 Introduction
Medical Entity Recognition (MER) consists in two
main steps: (i) detection and delimitation of phrasal
information referring to medical entities in textual
corpora (e.g. pyogenic liver abscess, infection of bil-
iary system) and (ii) identification of the semantic
category of located entities (e.g. Medical Problem,
Test). Example 1 shows the result of MER on a sen-
tence where the located entity and its category are
marked with treatment and problem tags.
(1) <treatment> Adrenal-sparing surgery
</treatment> is safe and effective , and may
become the treatment of choice in patients
with <problem> hereditary
phaeochromocytoma </problem>.
This task is very important for many applications
such as Question-Answering where MER is used in
the question analysis step (to determine the expected
answers? type, the question focus, etc.) and in the
offline text tagging or annotation.
One of the most important obstacles to identify-
ing medical entities is the high terminological vari-
ation in the medical domain (e.g. Diabetes melli-
tus type 1, Type 1 diabetes, IDDM, or juvenile di-
abetes all express the same concept). Other aspects
also have incidence on MER processes such as the
evolution of entity naming (e.g. new abbreviations,
names for new drugs or diseases). These obstacles
limit the scalability of methods relying on dictionar-
ies and/or gazetteers. Thus, it is often the case that
other types of approaches are developed by exploit-
ing not only domain knowledge but also domain-
independent techniques such as machine learning
and natural language processing tools.
In this paper, we study MER with three dif-
ferent methods: (i) a semantic method relying on
MetaMap (Aronson, 2001) (a state-of-the-art tool
for MER) (ii) chunker-based noun phrase extraction
and SVM classification and (iii) a last method us-
ing supervised learning with Conditional Random
Fields (CRF), which is then combined with the se-
mantic method. With these methods we particularly
study two processing directions: (i) pre-extraction
of noun phrases with specialized tools, followed by
a medical classification step and (ii) exploitation
of machine-learning techniques to detect simultane-
ously entity boundaries and their categories.
We also present a comparative study of the perfor-
mance of different noun phrase chunkers on medical
56
texts: Treetagger-chunker, OpenNLP and MetaMap.
The best chunker was then used to feed some of
the proposed MER approaches. All three methods
were experimented on the i2b2/VA 2010 challenge
corpus of clinical texts (Uzuner, 2010). Our study
shows that hybrid methods achieve the best perfor-
mance w.r.t machine learning approaches or domain
knowledge-based approaches if applied separately.
After a review of related work (Section 2), we de-
scribe the chunker comparison and the three MER
methods (Section 3). We present experiments on
clinical texts (Section 4), followed by a discussion
and variant experiments on literature abstracts (Sec-
tion 5), then conclude and draw some perspectives
for further work (Section 6).
2 Related Work
Several teams have tackled named entity recognition
in the medical domain. (Rindflesch et al, 2000) pre-
sented the EDGAR system which extracts informa-
tion about drugs and genes related to a given can-
cer from biomedical texts. The system exploits the
MEDLINE database and the UMLS. Protein name
extraction has also been studied through several ap-
proaches (e.g. (Liang and Shih, 2005; Wang, 2007)).
(Embarek and Ferret, 2008) proposed an approach
relying on linguistic patterns and canonical entities
for the extraction of medical entities belonging to
five categories: Disease, Treatment, Drug, Test, and
Symptom. Another kind of approach uses domain-
specific tools such as MetaMap (Aronson, 2001).
MetaMap recognizes and categorizes medical terms
by associating them to concepts and semantic types
of the UMLS Metathesaurus and Semantic Network.
(Shadow and MacDonald, 2003) presented an ap-
proach based on MetaMap for the extraction of med-
ical entities of 20 medical classes from pathologist
reports. (Meystre and Haug, 2005) obtained 89.9%
recall and 75.5% precision for the extraction of med-
ical problems with an approach based on MetaMap
Transfer (MMTx) and the NegEx negation detection
algorithm.
In contrast with semantic approaches which re-
quire rich domain-knowledge for rule or pattern con-
struction, statistical approaches are more scalable.
Several approaches used classifiers such as decision
trees or SVMs (Isozaki and Kazawa, 2002). Markov
models-based methods are also frequently used (e.g.
Hidden Markov Models, or CRFs (He and Kayaalp,
2008)). However, the performance achieved by such
supervised algorithms depends on the availability of
a well-annotated training corpus and on the selection
of a relevant feature set.
Hybrid approaches aim to combine the advan-
tages of semantic and statistical approaches and to
bypass some of their weaknesses (e.g. scalability
of rule-based approaches, performance of statistical
methods with small training corpora). (Proux et al,
1998) proposed a hybrid approach for the extraction
of gene symbols and names. The presented system
processed unknown words with lexical rules in order
to obtain candidate categories which were then dis-
ambiguated with Markov models. (Liang and Shih,
2005) developed a similar approach using empiri-
cal rules and a statistical method for protein-name
recognition.
3 Medical Entity Recognition Approaches
Named entity recognition from medical texts in-
volves two main tasks: (i) identification of entity
boundaries in the sentences and (ii) entity catego-
rization. We address these tasks through three main
approaches which are listed in Table 1.
3.1 Noun Phrase Chunking
Although noun phrase segmentation is an important
task for MER, few comparative studies on available
tools have been published. A recent study (Kang et
al., 2010), which claims to be the first to do such
comparative experiments, tested six state-of-the-art
chunkers on a biomedical corpus: GATE chunker,
Genia Tagger, Lingpipe, MetaMap, OpenNLP, and
Yamcha. This study encompassed sentence split-
ting, tokenization and part-of-speech tagging and
showed that for both noun-phrase chunking and
verb-phrase chunking, OpenNLP performed best (F-
scores 89.7% and 95.7%, respectively), but differ-
ences with Genia Tagger and Yamcha were small.
With a similar objective, we compared the perfor-
mance of three different noun-phrase chunkers in the
medical domain: (i) Treetagger-chunker1, a state-of-
the-art open-domain tool, (ii) OpenNLP2 and (iii)
1http://www.ims.uni-stuttgart.de/
projekte/corplex/TreeTagger
2http://incubator.apache.org/opennlp
57
Medical Entity Recognition
1. Boundary
identification
2. Type categorization
(with n medical entity categories)
Method 1
(MetaMap+)
Noun phrase
segmentation
- Rule-based method,
- Noun phrase classification,
- Number of classes = n + 1
Method 2
(TT-SVM)
Noun phrase
segmentation
- Statistical method with a SVM classifier,
- Noun phrase classification,
- Number of classes = n + 1
Method 3
(BIO-CRF)
- Statistical method with a CRF classifier,
- and the BIO format,
- word-level classification,
- Number of classes = 2n + 1
Table 1: Proposed MER methods
Corpus of clinical texts (i2b2) Corpus of scientific abstracts (Berkeley)
MetaMap TreeTagger OpenNLP MetaMap TreeTagger OpenNLP
Reference entities 58115 58115 58115 3371 3371 3371
Correct entities 6532 35314 26862 151 2106 1874
Found entities 212227 129912 122131 22334 19796 18850
Recall 11.14% 60.06% 46.62% 4.48% 62.27% 55.59%
Table 2: NP Segmentation Results
MetaMap. Regardless of the differences in corpora
with (Kang et al, 2010) we chose these particu-
lar tools to compare medical-domain specific tools
with open domain tools and to highlight the lower
performance of MetaMap for noun-phrase chunk-
ing compared to other tools. This last point led
us to introduce the MetaMap+ approach for MER
(Ben Abacha and Zweigenbaum, 2011) in order to
take advantage of MetaMap?s domain-knowledge
approach while increasing performance by relying
on external tools for noun-phrase chunking.
We evaluate these tools on the subset of noun
phrases referring to medical entities in our corpora
(cf. Section 4.1 for a description of the i2b2 cor-
pus and Section 5 for the Berkeley corpus). We
consider that a noun phrase is correctly extracted if
it corresponds exactly to an annotated medical en-
tity from the reference corpora. Also, as our cor-
pora are not fully annotated (only entities of the tar-
geted types are annotated), we do not evaluate ?extra
noun-phrases? corresponding to non-annotated enti-
ties. The retrieved noun phrases are heterogeneous:
many of them are not all relevant to the medical field
and therefore not relevant to the MER task. Our
goal is to obtain the maximal number of correct noun
phrases and leave it to the next step to filter out those
that are irrelevant. We therefore wish to maximize
recall at this stage.
Table 2 shows that in this framework, Treetagger-
chunker obtains the best recall. We thus used it
for noun-phrase segmentation in the experimented
MER approaches (cf. Sections 3.2 and 3.3).
3.2 Semantic and Rule-Based Method: MM+
MetaMap is a reference tool which uses the UMLS
to map noun phrases in raw texts to the best match-
ing UMLS concepts according to matching scores.
MetaMap leads however to some residual problems,
which we can arrange into three classes: (i) noun
phrase chunking is not at the same level of per-
formance as some specialized NLP tools, (ii) med-
ical entity detection often retrieves general words
and verbs which are not medical entities and (iii)
some ambiguity is left in entity categorization since
MetaMap can provide several concepts for the same
term as well as several semantic types for the same
concept. Several ?term/concept/type? combinations
58
are then possible.
To improve MetaMap output, we therefore
use an external noun phrase chunker (cf. Sec-
tion 3.1) and stop-list based filtering to recover fre-
quent/noticeable errors. MetaMap can propose dif-
ferent UMLS semantic types for the same noun
phrase, thus leading to different categories for the
same entity. In such cases we apply a voting pro-
cedure. For instance, if the process retrieves three
UMLS semantic types for one noun phrase where
two are associated to the target category ?Problem?
and one is associated to ?Treatment?, the ?Problem?
category is chosen as the entity?s category. In case
of a tie, we rely on the order output by MetaMap and
take the first returned type.
More precisely, our rule-based method, which we
call MetaMap+ (MM+), can be decomposed into the
following steps:
1. Chunker-based noun phrase extraction. We
use Treetagger-chunker according to the above-
mentioned test (cf. Table 2).
2. Noun phrase filtering with a stop-word list.
3. Search for candidate terms in specialized lists
of medical problems, treatments and tests
gathered from the training corpus, Wikipedia,
Health on the Net and Biomedical Entity Net-
work.
4. Use MetaMap to annotate medical entities
(which were not retrieved in the specialized
lists) with UMLS concepts and semantic types.
5. Finally, filter MetaMap results with (i) a list
of common/noticeable errors and (ii) the selec-
tion of only a subset of semantic types to look
for (e.g. Quantitative Concept, Functional Con-
cept, Qualitative Concept are too general se-
mantic types and produce noise in the extrac-
tion process).
3.3 Statistical Method: TT-SVM
The second presented approach uses Treetagger-
chunker to extract noun phrases followed by a ma-
chine learning step to categorize medical entities
(e.g. Treatment, Problem, Test). The problem is
then modeled as a supervised classification task with
n + 1 categories (n is the number of entity cate-
gories). We chose an SVM classifier.
As noted by (Ekbal and Bandyopadhyay, 2010),
SVMs (Support Vector Machines) have advantages
over conventional statistical learning algorithms,
such as Decision Trees or Hidden Markov Mod-
els, in the following two aspects: (1) SVMs have
high generalization performance independent of the
dimension of feature vectors, and (2) SVMs allow
learning with all feature combinations without in-
creasing computational complexity, by introducing
kernel functions.
In our experiments we use the libSVM (Chang
and Lin, 2001) implementation of the SVM classi-
fier. We chose the following feature set to describe
each noun phrase (NP):
1. Word features:
? words of the NP
? number of the NP words
? lemmas of the NP words
? 3 words and their lemmas before the NP
? 3 words and their lemmas after the NP
2. Orthographic features (some examples):
? first letter capitalized for the first word,
one word or all words
? all letters uppercase for the first word, one
word or all words
? all letters lowercase for the first word, one
word or all words
? NP is or contains an abbreviation
? word of NP contains a single upper-
case, digits, hyphen, plus sign, amper-
sand, slash, etc.
3. Part-of-speech tags: POS tags of the NP words,
of the 3 previous and 3 next words.
3.4 Statistical Method: BIO-CRF
We conducted MER with a CRF in one single step
by determining medical categories and entity bound-
aries at the same time. We used the BIO format: B
(beginning), I (inside), O (outside) which represents
entity tagging by individual word-level tagging. For
instance, a problem-tagged entity is represented as
a first word tagged B-P (begin problem) and other
59
(following) words tagged I-P (inside a problem). A
problem entity comprising one single word will be
tagged B-P. Words outside entities are tagged with
the letter ?O?.
If we have n categories (e.g. Problem, Treatment,
Test), we then have n classes of type B-, n classes
of type I- (e.g. P-B and P-I classes associated to the
problem category) and one class of type ?O?. Fig-
ure 1 shows an example sentence tagged with the
BIO format. As a result, the classification task con-
sists in a word classification task (instead of a noun-
phrase classification task) into 2n+1 target classes,
where n is the number of categories. As a conse-
quence, relying on a chunker is no longer necessary.
Figure 1: BIO Format (T = Test, P = Problem)
Words in a sentence form a sequence, and the de-
cision on a word?s category can be influenced by
the decision on the category of the preceding word.
This dependency is taken into account in sequential
models such as Hidden Markov Models (HMMs) or
Conditional Random Fields (CRF). In contrast with
HMMs, CRF learning maximizes the conditional
probability of classes w.r.t. observations rather than
their joint probability. This makes it possible to use
any number of features which may be related to all
aspects of the input sequence of words. These prop-
erties are assets of CRFs for several natural language
processing tasks, such as POS tagging, noun phrase
chunking, or named entity recognition (see (Tellier
and Tommasi, 2010) for a survey).
In our experiments we used the CRF++3 imple-
mentation of CRFs. CRF++ eases feature descrip-
tion through feature templates. We list hereafter
some of our main features. We instructed CRF++ to
model the dependency of successive categories (in-
struction B in feature template).
For each word we use the following features:
3http://crfpp.sourceforge.net/
1. Word features: The word itself, two words be-
fore and three words after, with their lemmas.
2. Morphosyntactic features: POS tag of the word
itself, two words before and three words after.
3. Orthographic features (some examples):
? The word contains hyphen, plus sign, am-
persand, slash, etc.
? The word is a number, a letter, a punctua-
tion sign or a symbol.
? The word is in uppercase, capitalized, in
lowercase (AA, Aa, aa)
? Prefixes of different lengths (from 1 to 4)
? Suffixes of different lengths (from 1 to 4)
4. Semantic features: semantic category of the
word (provided by MetaMap+)
5. Other features: next verb, next noun, word
length over a threshold, etc.
Additionally, we tested semantic features con-
structed from MM+ results. More detail on these
last features is given in Section 5.3.
4 Experiments on Clinical Texts
We performed MER experiments on English clinical
texts.
4.1 Corpus
The i2b2 corpus was built for the i2b2/VA 2010
challenge4 in Natural Language Processing for Clin-
ical Data (Uzuner, 2010). The data for this challenge
includes discharge summaries from Partners Health-
Care and from Beth Israel Deaconess Medical Cen-
ter (MIMIC II Database), as well as discharge sum-
maries and progress notes from University of Pitts-
burgh Medical Center. All records have been fully
de-identified and manually annotated for concept,
assertion, and relation information. The corpus con-
tains entities of three different categories: Problem,
Treatment and Test, 76,665 sentences and 663,476
words with a mean of 8.7 words per sentence. Ex-
ample 2 shows an annotated sentence from the i2b2
corpus.
4http://www.i2b2.org/NLP/Relations/
60
(2) <problem>CAD</problem> s/p
<treatment>3v-CABG </treatment> 2003
and subsequent <treatment>stenting
</treatment> of
<treatment>SVG</treatment> and LIMA.
Table 3 presents the number of training and test sen-
tences.
i2b2 Corpus Sentences Words
Training Corpus 31 238 267 304
Test Corpus 44 927 396 172
Table 3: Number of training and test sentences
4.2 Experimental Settings
We tested the above-described five configurations
(see Table 1):
1. MM: MetaMap is applied as a baseline method
2. MM+: MetaMap Plus (semantic and rule-based
method)
3. TT-SVM: Statistical method, chunking with
Treetagger and Categorization with a SVM
classifier
4. BIO-CRF: Statistical method, BIO format with
a CRF classifier
5. BIO-CRF-H: Hybrid method combining se-
mantic and statistical methods (BIO-CRF with
semantic features constructed from MM+ re-
sults)
We evaluate the usual metrics of Recall (proportion
of correctly detected entities among the reference
entities), Precision (proportion of correctly detected
entities among those output by the system), and F-
measure (harmonic means of Recall and Precision).
4.3 Results
Table 4 presents the results obtained by each con-
figuration. BIO-CRF and BIO-CRF-H obtained the
best precision, recall and F-measures. MM+ comes
next, followed by TT-SVM and MetaMap alone.
Table 5 presents the obtained results per each
medical category (i.e. Treatment, Problem and Test)
for three configurations. Again, BIO-CRF-H obtains
the best results for all metrics and all categories.
Setting P R F
MM 15.52 16.10 15.80
MM+ 48.68 56.46 52.28
TT-SVM 43.65 47.16 45.33
BIO-CRF 70.15 83.31 76.17
BIO-CRF-H 72.18 83.78 77.55
Table 4: Results per setting on the i2b2 corpus. R = recall,
P = precision, F = F-measure
Setting Category P R F
MM+
Problem 60.84 53.04 56.67
Treatment 51.99 61.93 56.53
Test 56.67 28.48 37.91
TT-SVM
Problem 48.25 43.16 45.56
Treatment 42.45 50.86 46.28
Test 57.37 35.76 44.06
BIO-CRF-H
Problem 82.05 73.14 77.45
Treatment 83.18 73.33 78.12
Test 87.50 68.69 77.07
Table 5: Results per setting and per category on the i2b2
corpus
5 Discussion and Further Experiments
We presented three different methods for MER:
MM+, TT-SVM, and BIO-CRF (with variant BIO-
CRF-H). In this section we quickly present supple-
mentary results obtained on a second corpus with
the same methods, and discuss differences in results
when corpora and methods vary.
5.1 Corpora
Different kinds of corpora exist in the biomedical
domain (Zweigenbaum et al, 2001). Among the
most recurring ones we may cite (i) clinical texts and
(ii) scientific literature (Friedman et al, 2002). Clin-
ical texts have motivated a long stream of research
(e.g. (Sager et al, 1995), (Meystre et al, 2008)), and
more recently international challenges such as i2b2
2010 (Uzuner, 2010). The scientific literature has
also been the subject of much research (e.g. (Rind-
flesch et al, 2000)), especially in genomics for more
than a decade, e.g. through the BioCreative chal-
lenge (Yeh et al, 2005).
61
Section 4 presented experiments in MER on En-
glish clinical texts. To have a complementary
view on the performance of our methods, we per-
formed additional experiments on the Berkeley cor-
pus (Rosario and Hearst, 2004) of scientific litera-
ture abstracts and titles extracted from MEDLINE.
The original aim of this corpus was to study the ex-
traction of semantic relationships between problems
and treatments (e.g. cures, prevents, and side effect).
In our context, we only use its annotation of med-
ical entities. The corpus contains two categories of
medical entities: problems (1,660 entities) and treat-
ments (1,179 entities) in 3,654 sentences (74,754
words) with a mean of 20.05 words per sentence. We
divided the corpus into 1,462 sentences for training
and 2,193 for testing.
We tested the MetaMap (MM), MetaMap+
(MM+) and BIO-CRF methods on the Berkeley cor-
pus. Table 6 presents the results. BIO-CRF again
obtain the best results, but it is not much better than
MM+ in this case.
P R F
MM
Problem 5.32 7.63 6.27
Treatment 6.37 18.84 9.52
Total 5.35 12.34 7.46
MM+
Problem 34.47 44.97 39.02
Treatment 18.11 39.36 24.81
Total 23.43 42.47 30.20
BIO-CRF
Problem 41.88 38.88 40.32
Treatment 29.85 23.86 26.52
Total 36.94 32.13 34.37
Table 6: Results on the Berkeley Corpus
We constructed three different models for the
BIO-CRF method: a first model constructed from
the Berkeley training corpus, a second model con-
structed from the i2b2 corpus and a third model
constructed from a combination of the former
two. We obtained the best results with the last
model: F=34.37% (F=22.97% for the first model
and F=30.08% for the second model). These re-
sults were obtained with a feature set with which we
obtained 76.17% F-measure on the i2b2 corpus (i.e.
words, lemmas, morphosyntactic categories, ortho-
graphic features, suffixes and prefixes, cf. set A4 in
Table 7).
The results obtained on the two corpora are not
on the same scale of performance. This is mainly
due to the characteristics of each corpus. For in-
stance, the i2b2 2010 corpus has an average words-
per-sentence ratio of 8.7 while the Berkeley corpus
has a ratio of 20.45 words per sentence. Besides,
the i2b2 corpus uses a quite specific vocabulary such
as conventional abbreviations of medical terms (e.g.
k/p for kidney pancreas and d&c for dilation and
curettage) and abbreviations of domain-independent
words (e.g. w/o for without and y/o for year old).
However, according to our observations, the most
important characteristic which may explain these re-
sults may be the quality of annotation. The i2b2 cor-
pus was annotated according to well-specified crite-
ria to be relevant for the challenge, while the Berke-
ley corpus was annotated with different rules and
less control measures. We evaluated a random sam-
ple of 200 annotated medical entities in the Berkeley
corpus, using the i2b2 annotation criteria, and found
that 20% did not adhere to these criteria.
5.2 Semantic Methods
The semantic methods have the advantage of being
reproducible on all types of corpora without a pre-
processing or learning step. However, their depen-
dency to knowledge reduces their performance w.r.t.
machine learning approaches. Also the development
of their knowledge bases is a relatively slow process
if we compare it with the time which is necessary for
machine learning approaches to build new extraction
and categorization models.
On the other hand, a clear advantage of semantic
approaches is that they facilitate semantic access to
the extracted information through conventional se-
mantics (e.g. the UMLS Semantic Network).
In our experiments we did not obtain good results
when applying MetaMap alone. This is mainly due
to the detection of entity boundaries (e.g. ?no peri-
cardial effusion.? instead of ?pericardial effusion?
and ?( Warfarin? instead of ?Warfarin?).
We were able to enhance the overall performance
of MetaMap for this task by applying several input
and output filtering primitives, among which the use
of an external chunker to obtain the noun phrases.
Our observation is that the final results are limited by
chunker performance. Nevertheless, the approach
provided the correct categories for 52.28% correctly
62
extracted entities while the total ratio of the retrieved
entities with correct boundaries is 60.76%.
5.3 Machine Learning Methods
We performed several tests with semantic features
with the BIO-CRF method. For instance, applying
MM+ on each word and using the obtained medical
category as an input feature for CRF decreased per-
formance from 76.17% F-measure to 76.01%. The
same effect was observed by using the UMLS se-
mantic type instead of the final category for each
word, with an F-measure decrease from 76.17% to
73.55%. This can be explained by a reduction in
feature value space size (22 UMLS types instead of
3 final categories) but also by the reduced perfor-
mance of MetaMap if it is applied at the word level.
The best solution was obtained by transforming
the output of the MM+ approach into BIO format
tags and feeding them to the learning process as
features for each word. Thus, each word in an en-
tity tagged by MM+ has an input feature value cor-
responding to one of the following: B-problem, I-
problem, B-treatment, I-treatment, B-test and I-test.
Words outside entities tagged by MM+ received an
?O? feature value.
With these semantic features we were able to in-
crease the F-measure from 76.19% to 77.55%. Ta-
ble 7 presents the contribution of each feature cate-
gory to the BIO-CRF method on the i2b2 corpus.
Features P R F
A1: Words/Lemmas/POS 62.81 82.25 71.23
A2: A1 + orthographic features 63.72 82.19 71.78
A3: A2 + suffixes 67.91 82.89 74.65
A4: A3 + prefixes 70.15 83.31 76.17
A5: A4 + other features 70.22 83.28 76.19
A6: A5 + semantic features 72.18 83.78 77.55
Table 7: Contribution of each feature category (BIO-CRF
method) on the i2b2 corpus
6 Conclusion
We presented and compared three different ap-
proaches to MER. Our experiments show that per-
forming the identification of entity boundaries with
a chunker in a first step limits the overall perfor-
mance, even though categorization can be performed
efficiently in a second step. Using machine learning
methods for joint boundary and category identifica-
tion allowed us to bypass such limits. We obtained
the best results with a hybrid approach combining
machine learning and domain knowledge. More pre-
cisely, the best performance was obtained with a
CRF classifier using the BIO format with lexical and
morphosyntactic features combined with semantic
features obtained from a domain-knowledge based
method using MetaMap.
Future work will tackle French corpora with both
a semantic method and the BIO-CRF approach. We
also plan to exploit these techniques to build a cross-
language question answering system. Finally, it
would be interesting to try ensemble methods to
combine the set of MER methods tested in this pa-
per.
Acknowledgments
This work has been partially supported by OSEO un-
der the Quaero program.
References
Alan R. Aronson. 2001. Effective mapping of biomed-
ical text to the UMLS metathesaurus: the MetaMap
program. In AMIA Annu Symp Proc, pages 17?21.
Asma Ben Abacha and Pierre Zweigenbaum. 2011. Au-
tomatic extraction of semantic relations between medi-
cal entities: a rule based approach. Journal of Biomed-
ical Semantics. In Press.
Chih-Chung Chang and Chih-Jen Lin, 2001. LIB-
SVM: a library for support vector machines. Soft-
ware available at http://www.csie.ntu.edu.
tw/?cjlin/libsvm.
Asif Ekbal and Sivaji Bandyopadhyay. 2010. Named en-
tity recognition using support vector machine: A lan-
guage independent approach. International Journal of
Electrical and Electronics Engineering, 4(2):155?170.
Mehdi Embarek and Olivier Ferret. 2008. Learning pat-
terns for building resources about semantic relations in
the medical domain. In LREC?08, May.
Carol Friedman, Pauline Kra, and Andrey Rzhetsky.
2002. Two biomedical sublanguages: a description
based on the theories of Zellig Harris. Journal of
Biomedical Informatics, 35:222?235.
Ying He and Mehmet Kayaalp. 2008. Biological en-
tity recognition with Conditional Random Fields. In
AMIA Annu Symp Proc, pages 293?297.
63
Hideki Isozaki and Hideto Kazawa. 2002. Efficient sup-
port vector classifiers for named entity recognition. In
Proceedings of COLING-2002, pages 390?396.
N Kang, EM van Mulligen, and JA Kors. 2010. Com-
paring and combining chunkers of biomedical text. J
Biomed Inform, 44(2):354?360, nov.
Tyne Liang and Ping-Ke Shih. 2005. Empirical textual
mining to protein entities recognition from PubMed
corpus. In NLDB?05, pages 56?66.
Ste?phane M. Meystre and Peter J. Haug. 2005. Compar-
ing natural language processing tools to extract medi-
cal problems from narrative text. In AMIA Annu Symp
Proc, pages 525?529.
S M Meystre, G K Savova, K C Kipper-Schuler, and J F
Hurdle. 2008. Extracting information from textual
documents in the electronic health record: a review of
recent research. Yearb Med Inform, 35:128?44.
Denys Proux, Franc?ois Rechenmann, Laurent Julliard,
Violaine Pillet, and Bernard Jacq. 1998. Detecting
gene symbols and names in biological texts : A first
step toward pertinent information extraction. In Pro-
ceedings of Genome Informatics, pages 72?80, Tokyo,
Japan : Universal Academy Press.
Thomas C. Rindflesch, Lorraine Tanabe, John N. Wein-
stein, and Lawrence Hunter. 2000. Edgar: Extraction
of drugs, genes and relations from the biomedical lit-
erature. In Proceedings of Pacific Symposium on Bio-
computing, pages 517?528.
Barbara Rosario and Marti A. Hearst. 2004. Classify-
ing semantic relations in bioscience text. In Proceed-
ings of the 42nd Annual Meeting of the Association
for Computational Linguistics (ACL 2004), Barcelona,
July.
N Sager, M Lyman, N T Nha`n, and L J Tick. 1995. Med-
ical language processing: applications to patient data
representation and automatic encoding. Meth Inform
Med, 34(1?2):140?6.
G Shadow and C MacDonald. 2003. Extracting struc-
tured information from free text pathology reports. In
AMIA Annu Symp Proc, Washington, DC.
Isabelle Tellier and Marc Tommasi. 2010. Champs
Markoviens Conditionnels pour l?extraction
d?information. In E?ric Gaussier and Franc?ois
Yvon, editors, Mode`les probabilistes pour l?acce`s a`
l?information textuelle. Herme`s, Paris.
O?zlem Uzuner, editor. 2010. Working papers of i2b2
Medication Extraction Challenge Workshop. i2b2.
Xinglong Wang. 2007. Rule-based protein term identi-
fication with help from automatic species tagging. In
Proceedings of CICLING 2007, pages 288?298.
Alexander Yeh, Alexander Morgan, Marc Colosimo, and
Lynette Hirschman. 2005. BioCreAtIvE task 1A:
gene mention finding evaluation. BMC Bioinformat-
ics, 6 Suppl 1.
Pierre Zweigenbaum, Pierre Jacquemart, Natalia Grabar,
and Beno??t Habert. 2001. Building a text corpus for
representing the variety of medical language. In V. L.
Patel, R. Rogers, and R. Haux, editors, Proceedings of
Medinfo 2001, pages 290?294, Londres.
64
Proceedings of the Fifth Law Workshop (LAW V), pages 92?100,
Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational Linguistics
Proposal for an Extension of Traditional Named Entities:
From Guidelines to Evaluation, an Overview
Cyril Grouin?, Sophie Rosset?, Pierre Zweigenbaum?
Kar?n Fort?,? , Olivier Galibert?, Ludovic Quintard?
?LIMSI?CNRS, France ?INIST?CNRS, France ?LIPN, France ?LNE, France
{cyril.grouin,sophie.rosset,pierre.zweigenbaum}@limsi.fr
karen.fort@inist.fr, {olivier.galibert,ludovic.quintard}@lne.fr
Abstract
Within the framework of the construction of a
fact database, we defined guidelines to extract
named entities, using a taxonomy based on an
extension of the usual named entities defini-
tion. We thus defined new types of entities
with broader coverage including substantive-
based expressions. These extended named en-
tities are hierarchical (with types and compo-
nents) and compositional (with recursive type
inclusion and metonymy annotation). Human
annotators used these guidelines to annotate a
1.3M word broadcast news corpus in French.
This article presents the definition and novelty
of extended named entity annotation guide-
lines, the human annotation of a global corpus
and of a mini reference corpus, and the evalu-
ation of annotations through the computation
of inter-annotator agreements. Finally, we dis-
cuss our approach and the computed results,
and outline further work.
1 Introduction
Within the framework of the Quaero project?a mul-
timedia indexing project?we organized an evalu-
ation campaign on named entity extraction aiming
at building a fact database in the news domain, the
first step being to define what kind of entities are
needed. This campaign focused on broadcast news
corpora in French. While traditional named enti-
ties include three major classes (persons, locations
and organizations), we decided to extend the cov-
erage of our campaign to new types of entities and
to broaden their main parts-of-speech from proper
names to substantives, this extension being neces-
sary for ever-increasing knowledge extraction from
documents. We thus produced guidelines to specify
the way corpora had to be annotated, and launched
the annotation process.
In this paper, after covering related work (Sec-
tion 2), we describe the taxonomy we created (Sec-
tion 3) and the annotation process and results (Sec-
tion 4), including the corpora we gathered and the
tools we developed to facilitate annotation. We then
present inter-annotator agreement measures (Sec-
tion 5), outline limitations (Section 6) and conclude
on perspectives for further work (Section 7).
2 Related work
2.1 Named entity definitions
Named Entity recognition was first defined as recog-
nizing proper names (Coates-Stephens, 1992). Since
MUC-6 (Grishman and Sundheim, 1996; SAIC,
1998), named entities have been proper names
falling into three major classes: persons, locations
and organizations.
Proposals were made to sub-divide these entities
into finer-grained classes. The ?politicians? sub-
class was proposed for the ?person? class by (Fleis-
chman and Hovy, 2002) while the ?cities? subclass
was added to the ?location? class by (Fleischman,
2001; Lee and Lee, 2005).
The CONLL conference added a miscellaneous
type that includes proper names falling outside the
previous classes. Some classes have thus sometimes
been added, e.g. the ?product? class by (Bick, 2004;
Galliano et al, 2009).
92
Specific entities are proposed and handled in
some tasks: ?language? or ?shape? for question-
answering systems in specific domains (Rosset et
al., 2007), ?email address? or ?phone number? to
process electronic messages (Maynard et al, 2001).
Numeric types are also often described and used.
They include ?date?, ?time?, and ?amount? types
(?amount? generally covers money and percentage).
In specific domains, entities such as gene, protein,
are also handled (Ohta, 2002), and campaigns are or-
ganized for gene detection (Yeh et al, 2005). At the
same time, extensions of named entities have been
proposed: (Sekine, 2004) defined a complete hierar-
chy of named entities containing about 200 types.
2.2 Named Entities and Annotation
As for any other kind of annotation, some aspects are
known to lead to difficulties in obtaining coherence
in the manual annotation process (Ehrmann, 2008;
Fort et al, 2009). Three different classes of prob-
lems are distinguished: (1) selecting the correct cat-
egory in cases of ambiguity, where one entity can
fall into several classes, depending on the context
(?Paris? can be a town or a person name); (2) detect-
ing the boundaries (in a person designation, is only
the proper name to be annotated or the trigger ?Mr?
too?) and (3) annotating metonymies (?France? can
be a sports team, a country, etc.).
In the ACE Named Entity task (Doddington et al,
2004), a complex task, the obtained inter-annotator
agreement was 0.86 in 2002 and 0.88 in 2003. Some
tasks obtain better agreement. Desmet and Hoste
(2010) described the Named Entity annotation real-
ized within the Sonar project, where Named Entity
are clearly simpler. They follow the MUC Named
Entity definition with the subtypes as proposed
by ACE. The agreement computed over the Sonar
Dutch corpus ranges from 0.91 to 0.97 (kappa val-
ues) depending of the emphasized elements (span,
main type, subtype, etc.).
3 Taxonomy
3.1 Guidelines production
Having in mind the objective of building a fact
database through the extraction of named entities
from texts, we defined a richer taxonomy than those
used in other information extraction works.
Following (Bonneau-Maynard et al, 2005; Alex
et al, 2010), the annotation guidelines were first
written from December 2009 to May 2010 by three
researchers managing the manual annotation cam-
paign. During guidelines production, we evaluated
the feasibility of this specific annotation task and the
usefulness of the guidelines by annotating a small
part of the target corpus. Then, these guidelines
were delivered to the annotators. They consist of a
description of the objects to annotate, general anno-
tation rules and principles, and more than 250 pro-
totypical and real examples extracted from the cor-
pus (Rosset et al, 2010). Rules are important to set
the general way annotations must be produced. Ad-
ditionally, examples are essential for human annota-
tors to grasp the annotation rationale more easily.
Indeed, while producing the guidelines, we knew
that the given examples would never cover all possi-
ble cases because of the specificity of language and
of the ambiguity of formulations and situations de-
scribed in corpora, as shown in (Fort et al, 2009).
Nevertheless, guidelines examples must be consid-
ered as a way to understand the final objective of
the annotation work. Thanks to numerous meetings
from May to November 2010, we gathered feedback
from the annotators (four annotators plus one anno-
tation manager). This feedback allowed us to clarify
and extend the guidelines in several directions. The
guidelines are 72 pages long and consist of 3 major
parts: general description of the task and the prin-
ciples (25% of the overall document), presentation
of each type of named entity (57%), and a simpler
?cheat sheet? (18%).
3.2 Definition
We decided to use the three general types of
named entities: name (person, location, organi-
zation) as described in (Grishman and Sundheim,
1996; SAIC, 1998), time (date and duration), and
quantity (amount). We then included named entities
extensions proposed by (Sekine, 2004; Galliano et
al., 2009) (respectively products and functions) and
we extended the definition of named entities to ex-
pressions which are not composed of proper names
(e.g., phrases built around substantives). The ex-
tended named entities we defined are both hierar-
chical and compositional. For example, type pers
(person) is split into two subtypes, pers.ind (indi-
93
Person Function
pers.ind (individual
person)
pers.coll (group of
persons)
func.ind (individual
function)
func.coll (collectivity
of functions)
Location Product
administrative
(loc.adm.town,
loc.adm.reg,
loc.adm.nat,
loc.adm.sup)
physical
(loc.phys.geo,
loc.phys.hydro,
loc.phys.astro)
facilities
(loc.fac),
oronyms
(loc.oro),
address
(loc.add.phys,
loc.add.elec)
prod.object
(manufac-
tured object)
prod.serv
(transporta-
tion route)
prod.fin
(financial
products)
prod.doctr
(doctrine)
prod.rule
(law)
prod.soft
(software)
prod.art prod.media prod.award
Organization Time
org.adm (administra-
tion)
org.ent (services)
Amount
amount (with unit or general object), includ-
ing duration
time.date.abs
(absolute date),
time.date.rel (relative
date)
time.hour.abs
(absolute hour),
time.hour.rel (relative
hour)
Table 1: Types (in bold) and subtypes (in italic)
vidual person) and pers.coll (collective person), and
pers entities are composed of several components,
among which are name.first and name.last.
3.3 Hierarchy
We used two kinds of elements: types and compo-
nents. The types with their subtypes categorize a
named entity. While types and subtypes were used
before (ACE, 2000; Sekine, 2004; ACE, 2005; Gal-
liano et al, 2009), we consider that structuring the
contents of an entity (its components) is important
too. Components categorize the elements inside a
named entity.
Our taxonomy is composed of 7 main types
(person, function, location, product, organization,
amount and time) and 32 subtypes (Table 1). Types
and subtypes refer to the general category of a
named entity. They give general information about
the annotated expression. Almost each type is then
specified using subtypes that either mark an opposi-
tion between two major subtypes (individual person
vs. collective person), or add precisions (for exam-
ple for locations: administrative location, physical
location, etc.).
This two-level representation of named entities,
with types and components, constitutes a novel ap-
proach.
Types and subtypes To deal with the intrinsic am-
biguity of named entities, we defined two specific
transverse subtypes: 1. other for entities with a dif-
ferent subtype than those proposed in the taxon-
omy (for example, prod.other for games), and 2. un-
known when the annotator does not know which sub-
type to use.
Types and subtypes constitute the first level of an-
notation. They refer to a general segmentation of
the world into major categories. Within these cate-
gories, we defined a second level of annotation we
call components.
Components Components can be considered as
clues that help the annotator to produce an anno-
tation: either to determine the named entity type
(e.g. a first name is a clue for the pers.ind named
entity subtype), or to set the named entity bound-
aries (e.g. a given token is a clue for the named en-
tity, and is within its scope, while the next token is
not a clue and is outside its scope). Components are
second-level elements, and can never be used out-
side the scope of a type or subtype element. An en-
tity is thus composed of components that are of two
kinds: transverse components and specific compo-
nents (Table 2). Transverse components can be used
in several types of entities, whereas specific compo-
nents can only be used in one type of entity.
94
Transverse components
name (name of the entity), kind (hyperonym of the entity), qualifier (qualifying adjective), demonym
(inhabitant or ethnic group name), demonym.nickname (inhabitant or ethnic group nickname), val
(a number), unit (a unit), extractor (an element in a series), range-mark (range between two values),
time-modifier (a time modifier).
pers.ind loc.add.phys time.date.abs/rel amount
name.last, name.first,
name.middle, pseudonym,
name.nickname, title
address-number, po-box,
zip-code,
other-address-component
week, day, month, year,
century, millennium,
reference-era
object
prod.award
award-cat
Table 2: Transverse and specific components
3.4 Composition
Another original point in this work is the compo-
sitional nature of the annotations. Entities can be
compositional for three reasons: (i) a type contains a
component; (ii) a type includes another type, used as
a component; and (iii) in cases of metonymy. Dur-
ing the Ester II evaluation campaign, there was an
attempt to use compositionality in named entities for
two categories: persons and functions, where a per-
son entity could contain a function entity.
<pers.hum> <func.pol> pr?sident </func.pol>
<pers.hum> Chirac </pers.hum> </pers.hum>
Nevertheless, the Ester II evaluation did not take
this inclusion into account and only focused on
the encompassing annotation (<pers.hum> pr?sident
Chirac </pers.hum>). We drew our inspiration from
this experience, and allowed the annotators and the
systems to use compositionality in the annotations.
Cases of inclusion can be found in the function
type (Figure 1), where type func.ind, which spans
the whole expression, includes type org.adm, which
spans the single word ?budget?. In this case, we con-
sider that the designation of this function (?ministre
du budget?) includes both the kind (?ministre?) and
nouveau
qualifier
ministre
kind
du Budget
name
org.adm
func.ind
, Fran?ois
name.first
Baroin
name.last
pers.ind
Figure 1: Multi-level annotation of entity types (red tags)
and components (blue tags): new minister of budget ,
Fran?ois Baroin.
the name (?budget?) of the ministry, which itself is
typed as is relevant (org.adm). Recursive cases of
embedding can be found when a subtype includes
another named entity annotated with the same sub-
type (org.ent in Figure 2).
le collectif
kind
des associations
kind
des droits de l' Homme
name
prod.rule
au Sahara
name
loc.phys.geo
loc.adm.sup
org.ent
org.ent
Figure 2: Recursive embedding of the same subtype:
Collective of the Human Rights Organizations in Sahara.
Cases of metonymy include strict metonymy (a
term is substituted with another one in a relation
of contiguity) and antonomasia (a proper name is
used as a substantive or vice versa). In such cases,
the entity must be annotated with both types, first
(inside) with the intrinsic type of the entity, then
(outside) with the type that corresponds to the re-
sult of the metonymy. Basically, country names
correspond to ?national administrative? locations
(loc.adm.nat) but they can also designate the admin-
istration (org.adm) of the country (Figure 3).
depuis
time-modifier
plusieurs
val
mois
unit
amount
time.date.rel
, la Russie
name
loc.adm.nat
org.adm
Figure 3: Annotation with a metonymic use of country
?Russia? as its government: for several months , Russia...
95
3.5 Boundaries
Our definition of the scope of entities excludes rel-
ative clauses, subordinate clauses, and interpolated
clauses: the annotation of an entity must end before
these clauses. If an interpolated clause occurs inside
an entity, its annotation must be split. Moreover, two
distinct persons sharing the same last name must be
annotated as two separate entities (Figure 4); we in-
tend to use relations between entities to gather these
segments in the next step of the project.
depuis
utmi-oefrl
vifr-eua
il n.s,etui
utmi-oefrl
Rprveu
utmi-strl
vifr-eua
Figure 4: Separate (coordinated) named entities.
4 Annotation process
4.1 Corpus
We managed the annotation of a corpus of about one
hundred hours of transcribed speech from several
French-speaking radio stations in France and Mo-
rocco. Both news and entertainment shows were
transcribed, including dialogs, with speaker turns.1
Once annotated, the corpus was split into a de-
velopment corpus: one file from a French radio sta-
tion;2 a training corpus: 188 files from five French
stations3 and one Moroccan station;4 and a test cor-
pus: 18 files from two French stations already stud-
ied in the training corpus5 and from unseen sources,
both radio6 and television,7 in order to evaluate the
robustness of systems. These data have been used in
the 2011 Quaero named entity evaluation campaign.
1Potential named entities may be split across several seg-
ments or turns.
2News from France Culture.
3News from France Culture (refined language), France Info
(news with short news headlines), France Inter (generalist radio
station), Radio Classique (classical music and economic news),
RFI (international radio broadcast out of France).
4News from RTM (generalist French speaking radio).
5News from France Culture, news and entertainment from
France Inter.
6A popular entertainment show from Europe 1.
7News from Arte (public channel with art and culture),
France 2 (public generalist channel), and TF1 (private gener-
alist popular channel).
This corpus allows us to perform different evalua-
tions, depending of the knowledge the systems have
of the source (source seen in the training corpus vs.
unseen source), the kind of show (news vs. enter-
tainment), the language style (popular vs. refined),
and the type of media (radio vs. television).
4.2 Tools for annotators
To perform our test annotations (see Section 2.2),
we developed a very simple annotation tool as an in-
terface based on XEmacs. We provided the human
annotators with this tool and they decided to use it
for the campaign, despite the fact that it is very sim-
ple and that we told them about other, more generic,
annotation tools such as GATE8 or Glozz.9 This is
probably due to the fact that apart from being very
simple to install and use, it has interesting features.
The first feature is the insertion of annotations
using combinations of keyboard shortcuts based on
the initial of each type, subtype and component
name. For example, combination F2 key + initial
keys is used to annotate a subtype (pers.ind, etc.),
F3 + keys for a transverse component (name, kind,
etc.), F4 + keys for a specific component (name.first,
etc.), and F5 to delete the annotation selected with
the cursor (both opening and closing tags).
The second feature is boundary management: if
the annotator puts the cursor over the token to anno-
tate, the annotation tool will handle the boundaries
of this token; opening and closing tags will be in-
serted around the token.
However, it presents some limitations: tags are
inserted in the text (which makes visualization more
complex, especially for long sentences or in cases
of multiple annotations on the same entity), no per-
sonalization is offered (tags are of only one color),
and there is no function to express annotator uncer-
tainty (the user must choose among several possible
tags the one that fits the best;10 while producing the
guidelines, we did not consider it could be of inter-
est: as a consequence, no uncertainty management
was implemented). Therefore, this tool allows users
to insert tags rapidly into a text, but it offers no exter-
nal resources, as real annotation tools (e.g. GATE)
often do.
8http://gate.ac.uk/
9http://www.glozz.org/
10Uncertainty can be found in cases of lack of context.
96
These simplistic characteristics combined with a
fast learning curve allow the annotators to rapidly
annotate the corpora. Annotators were allowed not
to annotate the transverse component name (only if
it was the only component in the annotated phrase,
e.g. ?Russia? in Figure 3, blue tag) and to annotate
events, even though we do not focus on this type
of entity as of yet. We therefore also provided a
normalization tool which adds the transverse com-
ponent name in these instances, and which removes
event annotations.
4.3 Corpus annotation
Global annotation It took four human annotators
two months and a half to annotate the entire corpus
(10 man-month). These annotators were hired grad-
uate students (MS in linguistics). The overall corpus
was annotated in duplicate. Regular comparisons of
annotations were performed and allowed the anno-
tators to develop a methodology, which was subse-
quently used to annotate the remaining documents.
Mini reference corpus To evaluate the global an-
notation, we built a mini reference corpus by ran-
domly selecting 400 sentences from the training cor-
pus and distributing them into four files. These files
were annotated by four graduate human annotators
from two research institutes (Figure 5) with two hu-
mans per institute, in about 10 hours per annotator.
	








Figure 5: Creation of mini reference corpus and compu-
tation of inter-annotator agreement. Institute 1 = LIMSI?
CNRS, Institute 2 = INIST?CNRS
First, we merged the annotations of each file
within a given institute (1.5h per pair of annotators),
then merged the results across the two institutes
(2h). Finally, we merged the results with the anno-
tations of the hired annotators (8h). We thus spent
about 90 hours to annotate and merge annotations in
this mini reference corpus (0.75 man-month).
4.4 Annotation results
Our broadcast news corpus includes 1,291,225
tokens, among which there are 954,049 non-
punctuation tokens. Its annotation contains 113,885
named entities and 146,405 components (Table 3),
i.e. one entity per 8.4 non-punctuation tokens, and
one component per 6.5 non-punctuation tokens.
There is an average of 6 annotations per line.
PPPPPPPPInf.
Data
Training Test
# shows 188 18
# lines 43,289 5,637
# words 1,291,225 108,010
# entity types 113,885 5,523
# distinct types 41 32
# components 146,405 8,902
# distinct comp. 29 22
Table 3: Statistics on annotated corpora.
5 Inter-Annotator Agreement
5.1 Procedure
During the annotation campaign, we measured sev-
eral criteria on a regular basis: inter-annotator agree-
ment and disagreement. We used them to correct er-
roneous annotations, and mapped these corrections
to the original annotations. We also used these mea-
sures to give the annotators feedback on the en-
countered problems, discrepancies, and residual er-
rors. Whereas we performed these measurements all
along the annotation campaign, this paper focuses
on the final evaluation on the mini reference corpus.
5.2 Metrics
Because human annotation is an interpretation pro-
cess (Leech, 1997), there is no ?truth? to rely on. It
is therefore impossible to really evaluate the validity
of an annotation. All we can and should do is to eval-
uate its reliability, i.e. the consistency of the anno-
tation across annotators, which is achieved through
computation of the inter-annotator agreement (IAA).
97
The best way to compute it is to use one of
the Kappa family coefficients, namely Cohen?s
Kappa (Cohen, 1960) or Scott?s Pi (Scott, 1955),
also known as Carletta?s Kappa (Carletta, 1996),11
as they take chance into account (Artstein and Poe-
sio, 2008). However, these coefficients imply a
comparison with a ?random baseline? to establish
whether the correlation between annotations is sta-
tistically significant. This baseline depends on the
number of ?markables?, i.e. all the units that could
be annotated.
In the case of named entities, as in many others,
this ?random baseline? is known to be difficult?if
not impossible?to identify (Alex et al, 2010). We
wish to analyze this in more detail, to see how we
could actually compute these coefficients and what
information it would give us about the annotation.
Markables Annotators Both institutes
F = 0.84522 F = 0.91123
U1: n-grams
? = 0.84522 ? = 0.91123
pi = 0.81687 pi = 0.90258
U2: n-grams ? 6
? = 0.84519 ? = 0.91121
pi = 0.81685 pi = 0.90257
U3: NPs
? = 0.84458 ? = 0.91084
pi = 0.81628 pi = 0.90219
U4: Ester entities
? = 0.71300 ? = 0.82607
pi = 0.71210 pi = 0.82598
U5: Pooling
? = 0.71300 ? = 0.82607
pi = 0.71210 pi = 0.82598
Table 4: Inter-Annotator Agreements (? stands for Co-
hen?s Kappa, pi for Scott?s Pi, and F for F-measure). IAA
values were computed by taking as the reference the hired
annotators? annotation or that obtained by merging from
both institutes (see Figure 5).
In the present case, we could consider that, poten-
tially, all the noun phrases can be annotated (row U3
in Table 4, based on the PASSAGE campaign (Vil-
nat et al, 2010)). Of course, this is a wrong approx-
imation as named entities are not necessarily noun
phrases (e.g., ?? partir de l?automne prochain?, from
next autumn).
We could also consider all n-grams of tokens in
the corpus (row U1). However, it would be more
11For more details on terminology issues, we refer to the in-
troduction of (Artstein and Poesio, 2008).
relevant to limit their size. For a maximum size of
six, we get the results shown in row U2. All this, of
course, is artificial, as the named entity annotation
process is not random.
To obtain results that are closer to reality, we
could use numbers of named entities from previous
named entity annotation campaigns (row U4 based
on the Ester II campaign (Galliano et al, 2009)), but
as we consider here a largely extended version of
those, the results would again be far from reality.
Another solution is to consider as ?markables? all
the units annotated by at least one of the annotators
(row U5). In this particular case, units not annotated
by any of the annotators (i.e. silence) are overlooked.
The lowest IAA will be the one computed with
this last solution, while the highest IAA will be
equal to the F-measure (i.e. the measure computed
with all the markables as shown in row U1 in Ta-
ble 4). We notice that the first two solutions (U1
and U2 with n-grams) are not acceptable because
they are far from reality; even extended named en-
tities are sparse annotations, and just considering
all tokens as ?markables? is not suitable. The last
three ones seem to be more relevant because they
are based on an observed segmentation on similar
data. Still, the U3 solution (NPs) overrates the num-
ber of markables because not all noun phrases are
extended named entities. Although the U4 solution
(Ester entities) is based on the same corpus used for
a related task, it underrates the number of markables
because that task produced 16.3 times less annota-
tions. Finally the U5 solution (pooling) gives the
lower bound for the ? estimation which is an in-
teresting information but may easily undervalue the
quality of the annotation.
As (Hripcsak and Rothschild, 2005) showed, in
our case ? tends towards the F-measure when the
number of negative cases tends towards infinity. Our
results show that it is hard to build a justifiable hy-
pothesis on the number of markables which is larger
than the number of actually annotated entities while
keeping ? significantly under the F-measure. But
building no hypothesis leads to underestimating the
? value.
This reinforces the idea of using the F-measure
as the main inter-annotator agreement measure for
named entity annotation tasks.
98
6 Limitations
We used syntax to define some components (e.g. a
qualifier is an adjective) and to set the scope of en-
tities (e.g. stop at relative clauses). Nevertheless,
this syntactic definition cannot fit all named enti-
ties, which are mainly defined according to seman-
tics: the phrase ?dans les mois qui viennent? (?in
the coming months?) expresses an entity of type
time.date.rel where the relative clause ?qui vien-
nent? is part of the entity and contributes the time-
modifier component.
The distinction between some types of entities
may be fuzzy, especially for the organizations (is
the Social Security an administrative organization or
a company?) and for context-dependent annotations
(is lemonde.fr a URL, a media, or a company?). As a
consequence, some entity types might be converted
into specific components in a future revision, e.g. the
func type could become a component of the pers
type, where it would become a description of the
function itself instead of the person who performs
this function (Figure 6).
depuisptm
-its
oftrlits
vaienr
tn.pl,num
Rpeulits
depuisptm
oftr
vaienr
tn.pl,num
Rpeulits
Figure 6: Possible revision: current annotation (left),
transformation of func from entity to component (right).
7 Conclusion and perspectives
In this paper, we presented an extension of the tra-
ditional named entity categories to new types (func-
tions, civilizations) and new coverage (expressions
built over a substantive). We created guidelines
that were used by graduate annotators to annotate
a broadcast news corpus.
The organizers also annotated a small part of the
corpus to build a mini reference corpus. We evalu-
ated the human annotations with our mini-reference
corpus: the actual computed ? is between 0.71 et
0.85 which, given the complexity of the task, seems
to indicate a good annotation quality. Our results are
consistent with other studies (Dandapat et al, 2009)
in demonstrating that human annotators? training is
a key asset to produce quality annotations.
We also saw that guidelines are never fixed, but
evolve all along the annotation process due to feed-
back between annotators and organizers; the rela-
tionship between guidelines producers and human
annotators evolved from ?parent? to ?peer? (Akrich
and Boullier, 1991). This evolution was observed
during the annotation development, beyond our ex-
pectations. These data have been used for the 2011
Quaero Named Entity evaluation campaign.
Extensions and revisions are planned. Our first
goal is to add a new type of named entity for all
kinds of events; guidelines are being written and hu-
man annotation tests are ongoing. We noticed that
some subtypes are more difficult to disambiguate
than others, especially org.adm and org.ent (defi-
nition and examples in the guidelines are not clear
enough). We shall make decisions about this kind
of ambiguity, either by merging these subtypes or by
reorganizing the distinctions within the organization
type. We also plan to link the annotated entities us-
ing relations; further work is needed to define more
precisely the way we will perform these annotations.
Moreover, the taxonomy we defined was applied to
a broadcast news corpus, but we intend to use it in
other corpora. The annotation of an old press corpus
was performed according to the same process. Its
evaluation will start in the coming months.
Acknowledgments
We thank all the annotators who did such a great
work on this project, as well as Sabine Barreaux
(INIST?CNRS) for her work on the reference cor-
pus.
This work was partly realized as part of the
Quaero Programme, funded by Oseo, French State
agency for innovation and by the French ANR Etape
project.
References
ACE. 2000. Entity detection and tracking,
phase 1, ACE pilot study. Task definition.
http://www.nist.gov/speech/tests/ace/phase1/doc/summary-
v01.htm.
ACE. 2005. ACE (Automatic Con-
tent Extraction) English annotation guide-
lines for entities version 5.6.1 2005.05.23.
http://www.ldc.upenn.edu/Projects/ACE/docs/English-
Entities-Guidelines_v5.6.1.pdf.
99
Madeleine Akrich and Dominique Boullier. 1991. Le
mode d?emploi, gen?se, forme et usage. In Denis
Chevallier, editor, Savoir faire et pouvoir transmettre,
pages 113?131. ?d. de la MSH (collection Ethnologie
de la France, Cahier 6).
Beatrice Alex, Claire Grover, Rongzhou Shen, and Mijail
Kabadjov. 2010. Agile Corpus Annotation in Prac-
tice: An Overview of Manual and Automatic Annota-
tion of CVs. In Proc. of the Fourth Linguistic Annota-
tion Workshop, pages 29?37, Uppsala, Sweden. ACL.
Ron Artstein and Massimo Poesio. 2008. Inter-Coder
Agreement for Computational Linguistics. Computa-
tional Linguistics, 34(4):555?596.
Eckhard Bick. 2004. A named entity recognizer for dan-
ish. In LREC?04.
H?l?ne Bonneau-Maynard, Sophie Rosset, Christelle Ay-
ache, Anne Kuhn, and Djamel Mostefa. 2005. Seman-
tic Annotation of the French Media Dialog Corpus. In
InterSpeech, Lisbon.
Jean Carletta. 1996. Assessing Agreement on Classifi-
cation Tasks: the Kappa Statistic. Computational Lin-
guistics, 22:249?254.
Sam Coates-Stephens. 1992. The analysis and acquisi-
tion of proper names for the understanding of free text.
Computers and the Humanities, 26:441?456.
Jacob Cohen. 1960. A Coefficient of Agreement for
Nominal Scales. Educational and Psychological Mea-
surement, 20(1):37?46.
Sandipan Dandapat, Priyanka Biswas, Monojit Choud-
hury, and Kalika Bali. 2009. Complex Linguistic An-
notation - No Easy Way Out! A Case from Bangla
and Hindi POS Labeling Tasks. In Proc. of the Third
Linguistic Annotation Workshop, Singapour. ACL.
Bart Desmet and V?ronique Hoste. 2010. Towards a
balanced named entity corpus for dutch. In LREC.
George Doddington, Alexis Mitchell, Mark Przybocki,
Lance Ramshaw, Stephanie Strassel, and Ralph
Weischedel. 2004. The automatic content extraction
(ACE) program tasks, data, and evaluation. In Proc. of
LREC.
Maud Ehrmann. 2008. Les entit?s nomm?es, de la lin-
guistique au TAL : statut th?orique et m?thodes de
d?sambigu?sation. Ph.D. thesis, Univ. Paris 7 Diderot.
Michael Fleischman and Eduard Hovy. 2002. Fine
grained classification of named entities. In Proc. of
COLING, volume 1, pages 1?7. ACL.
Michael Fleischman. 2001. Automated subcategoriza-
tion of named entities. In Proc. of the ACL 2001 Stu-
dent Research Workshop, pages 25?30.
Kar?n Fort, Maud Ehrmann, and Adeline Nazarenko.
2009. Towards a Methodology for Named Entities An-
notation. In Proceeding of the 3rd ACL Linguistic An-
notation Workshop (LAW III), Singapore.
Sylvain Galliano, Guillaume Gravier, and Laura
Chaubard. 2009. The ESTER 2 evaluation campaign
for the rich transcription of French radio broadcasts.
In Proc of Interspeech 2009.
Ralph Grishman and Beth Sundheim. 1996. Message
Understanding Conference - 6: A brief history. In
Proc. of COLING, pages 466?471.
George Hripcsak and Adam S. Rothschild. 2005. Tech-
nical brief: Agreement, the f-measure, and reliability
in information retrieval. JAMIA, 12(3):296?298.
Seungwoo Lee and Gary Geunbae Lee. 2005. Heuris-
tic methods for reducing errors of geographic named
entities learned by bootstrapping. In IJCNLP, pages
658?669.
Geoffrey Leech. 1997. Introducing corpus annotation.
In Geoffrey Leech Roger Garside and Tony McEnery,
editors, Corpus annotation: Linguistic information
from computer text corpora, pages 1?18. Longman,
London.
Diana Maynard, Valentin Tablan, Cristian Ursu, Hamish
Cunningham, and Yorick Wilks. 2001. Named en-
tity recognition from diverse text types. In Recent Ad-
vances in NLP 2001 Conference, Tzigov Chark.
Tomoko Ohta. 2002. The genia corpus: An annotated
research abstract corpus in molecular biology domain.
In Proc. of HLTC, pages 73?77.
Sophie Rosset, Olivier Galibert, Gilles Adda, and Eric
Bilinski. 2007. The LIMSI participation to the QAst
track. In Working Notes for the CLEF 2007 Workshop,
Budapest, Hungary.
Sophie Rosset, Cyril Grouin, and Pierre Zweigenbaum.
2010. Entit?s nomm?es : guide d?annotation Quaero,
November. T3.2, presse ?crite et orale.
SAIC. 1998. Proceedings of the seventh message under-
standing conference (MUC-7).
William A Scott. 1955. Reliability of Content Analysis:
The Case of Nominal Scale Coding. Public Opinion
Quaterly, 19(3):321?325.
Satoshi Sekine. 2004. Definition, dictionaries and tagger
of extended named entity hierarchy. In Proc. of LREC.
Anne Vilnat, Patrick Paroubek, Eric Villemonte de la
Clergerie, Gil Francopoulo, and Marie-Laure Gu?not.
2010. Passage syntactic representation: a minimal
common ground for evaluation. In Proc. of LREC.
Alex Yeh, Alex Morgan, Marc Colosimo, and Lynette
Hirschman. 2005. BioCreAtIvE task 1A: gene men-
tion finding evaluation. BMC Bioinformatics, 6(1).
100
Proceedings of the BioNLP Shared Task 2013 Workshop, pages 1?7,
Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational Linguistics
Overview of BioNLP Shared Task 2013 
 Claire N?dellec MIG INRA UR1077  F-78352 Jouy-en-Josas cedex claire.nedellec@jouy.inra.fr  
Robert Bossy MIG INRA UR1077  F-78352 Jouy-en-Josas cedex robert.bossy@jouy.inra.fr   Jin-Dong Kim Database Center for Life Science  2-11-16 Yayoi, Bunkyo-ku, Tokyo  jdkim@dbcls.rois.ac.jp  
Jung-jae Kim Nanyang Technological University Singapore  jungjae.kim@ntu.edu.sg  Tomoko Ohta National Centre for Text Mining and School of Computer Science University of Manchester tomoko.ohta@manchester.ac.uk  
Sampo Pyysalo National Centre for Text Mining and School of Computer Science University of Manchester sampo.pyysalo@gmail.com    Pierre Zweigenbaum LIMSI-CNRS F-91403 Orsay  pz@limsi.fr 
 
    Abstract The BioNLP Shared Task 2013 is the third edition of the BioNLP Shared Task series that is a community-wide effort to address fine-grained, structural information extraction from biomedical literature. The BioNLP Shared Task 2013 was held from January to April 2013. Six main tasks were proposed. 38 final submissions were received, from 22 teams. The results show advances in the state of the art and demonstrate that extraction methods can be successfully generalized in various aspects. 1 Introduction The BioNLP Shared Task (BioNLP-ST hereafter) series is a community-wide effort toward fine-grained biomolecular event extraction, from scientific documents. BioNLP-ST 2013 follows the general outline and goals of the previous tasks, namely BioNLP-ST?09 (Kim  et al, 2009) and BioNLP-ST?11 (Kim et al, 
2011). BioNLP-ST aims to provide a common framework for the comparative evaluation of information extraction (IE) methods in the biomedical domain. It shares this common goal with other tasks, namely BioCreative (Critical Assessment of Information Extraction in Biology) (Arighi  et al, 2011), DDIExtraction (Extraction of Drug-Drug Interactions from biomedical texts) (Segura-Bedmar et al, 2011) and i2b2 (Informatics for Integrating Biology and the Bedside) Shared-Tasks (Sun et al, 2013).  The biological questions addressed by the BioNLP-ST series belong to the molecular biology domain and its related fields. With the three editions, the series gathers several groups that prepared various tasks and resources, which represent diverse themes in biology. As the two previous editions, this one measures the progress accomplished by the community on complex text-bound event extraction. Compared to the other initiatives, the BioNLP-ST series proposes a linguistically motivated approach to event representation that enables the evaluation of the participating methods in a unifying computer science framework. Each edition has attracted an 
1
increasing number of teams with 22 teams submitting 38 final results this year. The task setup and the data serve as a basis for numerous further studies, released event extraction systems, and published datasets.  The first event in 2009 triggered active research in the community on a specific fine-grained IE task called Genia event extraction  task. Expanding on this, the second BioNLP-ST was organized under the theme Generalization, where the participants introduced numerous systems that could be straightforwardly applied to different tasks. This time, the BioNLP-ST goes a step further and pursues the grand theme of Knowledge base construction. There were five tasks in 2011, and this year there are 6.  - [GE] Genia Event Extraction for NFkB knowledge base  - [CG] Cancer Genetics  - [PC] Pathway Curation  - [GRO] Corpus Annotation with Gene Regulation Ontology  - [GRN] Gene Regulation Network in Bacteria  - [BB] Bacteria Biotopes  The grand theme of Knowledge base construction is addressed in various ways: semantic web (GE, GRO), pathway (PC), molecular mechanism of cancer (CG), regulation network (GRN) and ontology population (GRO, BB).  In the biology domain, BioNLP-ST 2013 covers many new hot topics that reflect the evolving needs of biologists. BioNLP-ST 2013 broadens the scope of the text-mining application domains in biology by introducing new issues on cancer genetics and pathway curation. It also builds on the well-known previous datasets GENIA, LLL/BI and BB to propose tasks closer to the actual needs of biological data integration.  As in previous events, manually annotated data are provided to the participants for training, development and evaluation of the information extraction methods. According to their relevance for biological studies, the annotations are either bound to specific expressions in the text or represented as structured knowledge. Linguistic processing support was provided to the participants in the form of analyses of the dataset texts produced by state-of-the art tools. This paper summarizes the BioNLP-ST 2013 organization, the task characteristics and their relationships. It gives synthetic figures on the participants and discusses the participating system advances. 
2 Tasks The BioNLP-ST?13 includes six tasks from four groups: DBCLS, NaCTeM, NTU and INRA. As opposed to the last edition, all tasks were main extraction tasks. There were no supporting tasks designed to assist the extraction tasks. All tasks share the same event-based representation and file format, which is similar to the previous editions. This makes it easier to reuse the systems across tasks. Five kinds of annotation types are defined: ? T: text-bound annotation (entity/event trigger) ? Equiv: entity aliases ? E: event ? M: event modification ? R: relation ? N: normalization (external reference) The normalization type has been introduced this year to represent the references to external resources such as dictionaries for GRN or ontologies for GRO and BB. The annotations are stand-off: the texts of the documents are kept separate from the annotations that refer to specific spans of texts through character offsets. More detail and examples can be found on the BioNLP-ST?13 web site. 2.1     Genia Event Extraction (GE) Originally the design and implementation of the GE task was based on the Genia event corpus (Kim et al, 2008) that represents domain knowledge of NF?B proteins. It was first organized as the sole task of the initial 2009 edition of BioNLP-ST (Kim et al, 2009). While in 2009 the data sets consisted only of Medline abstracts, in its second edition in 2011 (Kim et al, 2011b), it was extended to include full text articles to measure the generalization of the technology to full text papers. For its third edition this year, the GE task is organized with the goal of making it a more ?real? task useful for knowledge base construction. The first design choice is to construct the data sets with recent full papers only, so that the extracted pieces of information could represent up-to-date knowledge of the domain. Second, the co-reference annotations are integrated into the event annotations, to encourage the use of these co-reference features in the solution of the event extraction. 
2
2.2     Cancer Genetics (CG) The CG task concerns the extraction of events relevant to cancer, covering molecular foundations, cellular, tissue, and organ-level effects, and organism-level outcomes. In addition to the domain, the task is novel in particular in extending event extraction to upper levels of biological organization. The CG task involves the extraction of 40 event types involving 18 types of entities, defined with respect to community-standard ontologies (Pyysalo et al, 2011a; Ohta et al, 2012). The newly introduced CG task corpus, prepared as an extension of a previously introduced corpus of 250 abstracts (Pyysalo et al, 2012), consists of 600 PubMed abstracts annotated for over 17,000 events. 2.3     Pathway Curation (PC) The PC task focuses on the automatic extraction of biomolecular reactions from text with the aim of supporting the development, evaluation and maintenance of biomolecular pathway models. The PC task setting and its document selection protocol account for both signaling and metabolic pathways. The 23 event types, including chemical modifications (Pyysalo et al, 2011b), are defined primarily with respect to the Systems Biology Ontology (SBO) (Ohta et al, 2011b; Ohta et al, 2011c), involving 4 SBO entity types. The PC task corpus was newly annotated for the task and consists of 525 PubMed abstracts, chosen for the relevance to specific pathway reactions selected from SBML models registered in BioModels and PANTHER DB repositories (Mi and Thomas, 2009). The corpus was manually annotated for over 12,000 events on top of close to 16,000 entities.  2.4     Gene Regulation Ontology (GRO) The GRO task aims to populate the Gene Regulation Ontology (GRO) (Beisswanger et al, 2008) with events and relations identified from text. The large size and the complex semantic representation of the underlying ontology are the main challenges of the task. Those issues, to a greater extent, should be addressed to support full-fledged semantic search over the biomedical literature, which is the ultimate goal of this work.   The corpus consists of 300 MEDLINE abstracts, prepared as an extension of (Kim et al, 2011c). The analysis of the inter-annotator agreement between the two annotators shows 
Kappa values of 43%-56%, which might indicate the difficulty of the task.  2.5     Gene Regulation Network in Bacteria           (GRN) The Gene Regulation Network task consists of the extraction of the regulatory network of a set of genes involved in the sporulation phenomenon of the model organism Bacillus subtilis. Participant system predictions are evaluated with respect to the target regulation network, rather than the text-bound relations. The aim is to assess the IE methods with regards to the needs of systems biology and predictive biology studies. The GRN corpus is a set of sentences from PubMed abstracts that extends the BioNLP-ST 2011 BI (Jourde et al, 2011) and LLL (Nedellec, 2005) corpora. The additional sentences cover a wider range of publication dates and complement the regulation network of the sporulation phenomenon. It has been thoroughly annotated with different levels of biological abstraction: entities, biochemical events, genic interactions and the corresponding regulation network. The network prediction submissions have been evaluated against the reference network using an original metric, the Slot Error Rate (Makhoul et al, 1999) that is more adapted to graph comparison than the usual Recall, Precision and F-score measures.  2.6     Bacteria Biotopes (BB) The Bacteria Biotope (BB) task concerns the extraction of locations in which bacteria live and the categorization of these habitats with concepts from OntoBiotope,1 a large ontology of 1,700 concepts and 2,000 synonyms. The association between bacteria and their habitats is essential information for environmental biology studies, metagenomics and phylogeny. In the previous edition of the BB task, participants had to recognize bacteria and habitat entities, to categorize habitat entities among eight broad types and to extract localization relations between bacteria and their habitats (Bossy et al, 2011). The BioNLP-ST 2013 edition has been split into 3 sub-tasks in order to better assess the performance of the predictive systems for each step. The novelty of this task is mainly the more comprehensive and fine-grained categorization. It addresses the critical problem of habitat normalization necessary for the                                                            1 http://bibliome.jouy.inra.fr/MEM-OntoBiotope 
3
automatic exploitation of bacteria-habitat databases.  2.7     Task characteristics Task features are given in Table 1. Three different types of text were considered: the abstracts of scientific papers taken from PubMed (CG, PC, GRO and GRN), full-text scientific papers (GE) and scientific web pages (BB).   Task Documents # types # events GE 34 Full papers  2 13 CG 600 Abstracts 18 40 PC 525 Abstracts 4 23 GRO 300 Abstracts 174  126 GRN 201 Abstracts 6 12 BB 124 Web pages 563 2 Table 1. Characteristics of the BioNLP-ST 2013 tasks. The number of relations or events targeted greatly varies with the tasks as shown in column 3. The high number of types and events reflect the increasing complexity of the biological knowledge to be extracted. The grand theme of Knowledge base construction in this edition has been translated into rich knowledge representations with the goal of integrating textual data with data from sources other than text. These figures illustrate the shared ambition of the organizers to promote fine-grained information extraction together with an increasing biological plausibility. Beyond gene and protein interactions, they include many complex biological phenomena and environmental factors. 3 BioNLP-ST?13 organization  BioNLP-ST?13 was split in three main periods. During thirteen weeks from mid-January to the first week of April, the participants prepared their systems with the training data. Supporting resources were delivered to participants during this period. Supporting resources were provided by the organizers and by three external providers after a public call for contribution. They range from tokenizers to entity detection tools, mostly focusing on syntactic parsing (Enju (Miyao and Tsujii, 2008), Stanford (Klein and Manning, 2002), McCCJ (Charniak and Johnson, 2005)). The test data were made available for 10 days before the participants had to submit their final results using on-line services. The evaluation results were 
communicated shortly after and published on the ST site. The descriptions of the tasks and representative sample data have been available since October 2012 so that the participants could become acquainted with the task goals and data formats in advance. Table 2 shows the task schedule.  Date Event 23 Oct. 2012 Release of sample data sets 17 Jan 2013 Release of the training data sets 06 Apr. 2013 Release of the test data sets 16 Apr. 2013 Result submission 17 Apr. 2013 Notification of the evaluation results Table 2: Schedule of BioNLP-ST 2013. The BioNLP-ST?13 web site and a dedicated mailing-list have kept the participant informed about the whole process.  4 Participation  GE 1-2-3 CG PC GRO GRN BB 1 - 2-3 EVEX ? ? ?    ?    TEES-2.1 ? ? ? ? ? ? ?  ? ? BioSEM ?          NCBI ?          DlutNLP ?          HDS 4NLP ?          NICTA  ?  ?        USheff ?          UZH  ?          HCMUS ?          NaCTeM     ? ?      NCBI     ?       RelAgent     ?       UET-NII     ?       ISI    ?       OSEE      ?     U. of Ljubljana       ?    K.U. Leuven       ?    IRISA-TexMex       ? ? ?  Boun        ? ?  LIPN        ?   LIMSI        ? ? ? Table 3: Participating teams per task. BioNLP-ST 2013 received 38 submissions from 22 teams (Table 3). One third, or seven teams, participated in multiple tasks. Only one team, UTurku, submitted final results with TEES-2.1 to 
4
all the tasks except one ? entity categorization. This broad participation resulted from the growing capability of the systems to be applied to various tasks without manual tuning. The remaining 15 teams participated in one single task. 5 Results  Table 4 summarizes the best results and the participating systems for each task and sub-task. They are all measured using F-scores, except when it is not relevant, in which case SER is used instead. It is noticeable that the TEES-2.1 system that participated in 9 of the 10 tasks and sub-tasks achieved the best result in 6 cases. Most of the participating systems applied a combination of machine learning algorithms and linguistic features, mainly syntactic parses, with some noticeable exceptions.   Tasks Evaluation results  GE Core event extraction TEES-2.1, EVEX, BioSEM:  0.51 GE 2 Event enrichment TEES2.1:  0.32 GE 3 Negation/Speculation TEES-2.1, EVEX:   0.25 CG TEES-2.1:  0.55 PC NaCTeM:  0.53 
GRO TEES-2.1: 0.22 (events),   0.63 (relations) 
GRN U. of Ljubljana:   0.73 (SER) BB 1 Entity detection and categorization IRISA: 0.46 (SER) BB 2 Relation extraction IRISA:  0.40 BB 3 Full event extraction TEES-2.1:  0.14 Table 4. Best results and team per task  (F-score, except when SER). Twelve teams submitted final results to the GE task. The performance of highly ranked systems shows that the event extraction technology is applicable to the most recent full papers without drop of performance. Six teams submitted final results to the CG task. The highest-performing systems achieved 
results comparable to those for established molecular level extraction tasks (Kim et al, 2011). The results indicate that event extraction methods generalize well to higher levels of biological organization and are applicable to the construction of knowledge bases on cancer. Two teams successfully completed the PC task, and the highest F-score reached 52.8%, indicating that event extraction is a promising approach to support pathway curation efforts. The GRN task attracted five participants. The best SER score was 0,73 (the higher, the worse), which shows their capability of designing regulatory network, but handling modalities remains an issue. Five teams participated to the 3 BB subtasks with 10 final submissions. Not surprisingly, the systems achieved better results in relation extraction than habitat categorization, which remains a major challenge in IE. One team participated in the GRO task, and their results were compared with those of a preliminary system prepared by the task organizers. An analysis of the evaluation results leads us to study issues such as the need to consider the ontology structure and the need for semantic analysis, which are not seriously dealt with by current approaches to event extraction. 6 Organization of the workshop The BioNLP Shared Task 2013 (BioNLP-ST) workshop was organized as part of the ACL BioNLP 2013 workshop. After submission of their system results, participants were invited to submit a paper on their systems to the workshop.  Task organizers were also invited to present overviews of each task, with analyses of the participant system features and results. The workshop was held in August 2013 in Sofia (Bulgaria). It included overview presentations on tasks, as well as oral and poster presentations by Shared Task participants.  7 Discussion and Conclusion This year, the tasks has significantly gained in complexity to face the increasing need for Systems Biology knowledge from various textual sources. The high level of participation and the quality of the results show that the maturity of the field is such that it can meet this challenge. The innovative and various solutions applied this year will without doubt be extended in the future. As for previous editions of BioNLP-ST, all tasks maintain an online evaluation service that is 
5
publicly available. This on-going challenge will contribute to the assessment of the evolving information extraction field in the biomedical domain. References  Auhors. 2013. Title. In Proceedings of the BioNLP 2013 Workshop Companion Volume for Shared Task, Sofia, Bulgaria. Association for Computational Linguistics. Arighi, C., Lu, Z., Krallinger, M., Cohen, K., Wilbur, W., Valencia, A., Hirschman, L. and Wu, C. 2011. Overview of the BioCreative III Workshop. BMC Bioinformatics, 12, S1. E Beisswanger, V Lee, JJ Kim, D Rebholz-Schuhmann, A Splendiani, O Dameron, S Schulz, U Hahn. Gene Regulation Ontology (GRO): Design principles and use cases. Studies in Health Technology and Informatics, 136:9-14, 2008. BioNLP-ST?13 web site: https://2013.bionlp-st.org Robert Bossy, Julien Jourde, Philippe Bessi?res, Maarten van de Guchte, Claire N?dellec. 2011. BioNLP shared Tasks 2011 - Bacteria Biotope. In Proceedings of BioNLP 2011 Workshop, pages 65-73. Association for Computational Linguistics, Portland, USA, 2011. Eugene Charniak and Mark Johnson. 2005. Coarse-to-fine n-best parsing and maxent discriminative reranking. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 173?180. Association for Computational Linguistics. Julien Jourde, Alain-Pierre Manine, Philippe Veber, Karen Fort, Robert Bossy, Erick Alphonse, Philippe Bessi?res. 2011. BioNLP Shared Task 2011 - Bacteria Gene Interactions and Renaming. In Proceedings of BioNLP 2011 Workshop, pages 65-73. Association for Computational Linguistics, Portland. Jin-Dong Kim, Tomoko Ohta and Jun'ichi Tsujii, 2008, Corpus annotation for mining biomedical events from literature, BMC Bioinformatics, 9(1): 10. Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshinobu Kano and Jun'ichi Tsujii. 2009. Overview of BioNLP'09 Shared Task on Event Extraction. In Proceedings of the BioNLP 2009 Workshop Companion Volume for Shared Task, pages 1-9. Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, Robert Bossy, Ngan Nguyen and Jun?ichi Tsujii. 2011. Overview of BioNLP Shared Task 2011. In Proceedings of BioNLP 2011 Workshop, pages 1-6. Association for Computational Linguistics. 
Jin-Dong Kim, Yue Wang, Toshihisa Takagi, and Akinori Yonezawa. 2011b. Overview of the Genia Event task in BioNLP Shared Task 2011. In Proceedings of the BioNLP 2011 Workshop Companion Volume for Shared Task, Portland, Oregon, June. Association for Computational Linguistics. Jung-Jae Kim, Xu Han and Watson Wei Khong Chua. 2011c. Annotation of biomedical text with Gene Regulation Ontology: Towards Semantic Web for biomedical literature. Proceedings of LBM 2011, pp. 63-70. Dan Klein and Christopher D Manning. 2002. Fast ex act inference with a factored model for natural language parsing. Advances in neural information processing systems, 15(2003):3?10. John Makhoul, Francis Kubala, Richard Schwartz and Ralph Weischedel. 1999.  Performance measures for information extraction. In Proceedings of DARPA Broadcast News Workshop, Herndon, VA, February. Yusuke Miyao and Jun?ichi Tsujii. 2008. Feature forest models for probabilistic HPSG parsing. Computational Linguistics, 34(1):35?80. Huaiyu Mi and Paul Thomas. 2009. PANTHER path- way: an ontology-based pathway database coupled with data analysis tools. In Protein Networks and Pathway Analysis, pages 123?140. Springer. Claire N?dellec. 2005. Learning Language in Logic - Genic Interaction Extraction Challenge. In Proceedings of the Learning Language in Logic (LLL05) workshop joint to ICML'05. Cussens J. and Nedellec C. (eds). Bonn, August. Tomoko Ohta, Sampo Pyysalo, Sophia Ananiadou, and Jun'ichi Tsujii. 2011b. Pathway curation support as an information extraction task. Proceedings of LBM 2011. Tomoko Ohta, Sampo Pyysalo, and Jun?ichi Tsujii. 2011c. From pathways to biomolecular events: opportunities and challenges. In Proceedings of BioNLP 2011 Workshop, pages 105?113. Association for Computational Linguistics. Tomoko Ohta, Sampo Pyysalo, Jun?ichi Tsujii, and Sophia Ananiadou. 2012. Open-domain anatomical entity mention detection. In Proceedings of DSSD 2012, pages 27?36. Sampo Pyysalo, Tomoko Ohta, Makoto Miwa, Han-Cheol Cho, Jun?ichi Tsujii, and Sophia Ananiadou. 2012. Event extraction across multiple levels of biological organization. Bioinformatics, 28(18):i575-i581. Sampo Pyysalo, Tomoko Ohta, Makoto Miwa, and Jun'ichi Tsujii. 2011b. Towards exhaustive event extraction for protein modifications. In Proceedings of the BioNLP 2011 Workshop, 
6
pp.114-123, Association for Computational Linguistics. Sampo Pyysalo, Tomoko Ohta, Jun'ichi Tsujii and Sophia Ananiadou. 2011a. Anatomical Entity Recognition with Open Biomedical Ontologies. In proceedings of LBM 2011. Isabel Segura-Bedmar, Paloma Martinez, and Daniel Sanchez-Cisneros. 2011. The 1st DDIExtraction-2011 challenge task: Extraction of Drug-Drug Interactions from biomedical texts. In Proceedings of the 1st Challenge Task on Drug-Drug Interaction Extraction 2011, SEPLN 2011 satellite workshop. Huelva, Spain, September 7. Weiyi Sun, Anna Rumshisky, Ozlem Uzuner. 2013. Evaluating temporal relations in clinical text: 2012 i2b2 Challenge. J Am Med Inform Assoc.
7
Proceedings of the 7th Linguistic Annotation Workshop & Interoperability with Discourse, pages 168?177,
Sofia, Bulgaria, August 8-9, 2013. c?2013 Association for Computational Linguistics
Automatic Named Entity Pre-Annotation
for Out-of-Domain Human Annotation
Sophie Rosset?, Cyril Grouin?, Thomas Lavergne?,? , Mohamed Ben Jannet?,?,?,?
Je?re?my Leixa, Olivier Galibert? , Pierre Zweigenbaum?.
?LIMSI?CNRS ?Universite? Paris-Sud ?LNE
?LPP, Universite? Sorbonne Nouvelle ELDA
{rosset,grouin,lavergne,ben-jannet,pz}@limsi.fr
leixa@elda.org, olivier.galibert@lne.fr
Abstract
Automatic pre-annotation is often used to
improve human annotation speed and ac-
curacy. We address here out-of-domain
named entity annotation, and examine
whether automatic pre-annotation is still
beneficial in this setting. Our study de-
sign includes two different corpora, three
pre-annotation schemes linked to two an-
notation levels, both expert and novice an-
notators, a questionnaire-based subjective
assessment and a corpus-based quantita-
tive assessment. We observe that pre-
annotation helps in all cases, both for
speed and for accuracy, and that the sub-
jective assessment of the annotators does
not always match the actual benefits mea-
sured in the annotation outcome.
1 Introduction
Human corpus annotation is a difficult, time-
consuming, and hence costly process. This mo-
tivates research into methods which reduce this
cost (Leech, 1997). One such method consists of
automatically pre-annotating the corpus (Marcus
et al, 1993; Dandapat et al, 2009) using an ex-
isting system, e.g., a POS tagger, syntactic parser,
named entity recognizer, according to the task for
which the annotations aim to provide a gold stan-
dard. The pre-annotations are then corrected by
the human annotators. The underlying hypothe-
sis is that this should reduce annotation time while
possibly at the same time increasing annotation
completeness and consistency.
We study here corpus pre-annotation in a spe-
cific setting, out-of-domain named entity annota-
tion, in which we examine specific questions that
we present below. We produced corpora and an-
notation guidelines for named entities which are
both hierarchical and compositional (Grouin et al,
2011),1 and which we used in contrastive stud-
ies of news texts in French (Rosset et al, 2012).
We want to rely on the same named entity def-
initions for studies on two types of data we did
not cover: parliament debates (Europarl corpus)
and regional, contemporary written news (L?Est
Re?publicain), both in French. To help the annota-
tion process we could reuse our system (Dinarelli
and Rosset, 2011), but needed first to examine
whether a system trained on one type of text (our
first Broadcast News data) could be used to pro-
duce a useful pre-annotation for different types of
text (our two corpora).
We therefore set up the present study in which
we aim to answer the following questions linked
to this point and to related annotation issues:
? can a system trained on data from one spe-
cific domain be useful on data from another
domain in a pre-annotation task?
? does this pre-annotation help human annota-
tors or bias them?
? what importance can we give to the annota-
tors? subjective assessment of the usefulness
of the pre-annotation?
? can we observe differences in the use of pre-
annotation depending on the level of exper-
tise of human annotators?
Moreover, as the aforementioned annotation
scheme is based on two annotation levels (entities
and components), we want to answer these ques-
tions taking into account these two levels.
We first examine related work on pre-annotation
(Section 2), then present our corpora and annota-
tion task (Section 3). We describe and discuss ex-
periments in Section 4, and make subjective and
1Corpora, guidelines and tools are available through
ELRA under references ELRA-S0349 and ELRA-W0073.
168
quantitative observations in Sections 5 and 6. Fi-
nally, we conclude and present some perspectives
in Section 7.
2 Related Work
Facilitating human annotations has been the topic
of a large amount of research. Two different
approaches can be distinguished: active learn-
ing (Ringger et al, 2007; Settles et al, 2008) and
pre-annotation (Marcus et al, 1993; Dandapat et
al., 2009). Our work falls into the latter type.
Pre-annotation can be used in several ways. The
first is to provide annotations to be corrected by
human annotators (Fort and Sagot, 2010). A vari-
ant consists of merging multiple automatic anno-
tations before having them corrected by human
curators to produce a gold-standard (Rebholz-
Schuhmann et al, 2011). The second type con-
sists of providing clues to help human annotators
perform the annotation task (Mihaila et al, 2013).
This work addresses the first type, a single-
system pre-annotation with human correction. An
objective is to examine whether a system trained
on one type of text can be useful to pre-annotate
texts of a different type. Most previous studies
have been performed on well-behaved tasks such
as part-of-speech tagging on in-domain data, i.e.,
the model used for pre-annotating the target data
had been trained on similar data. For instance, Fort
and Sagot (2010) provide a precise evaluation of
the usefulness of pre-annotation and compare the
impact of different quality levels in POS taggers
on the Penn TreeBank corpus. They first trained
different models on the training part of the cor-
pus and applied them to the test corpus. The pre-
annotated test corpus was then corrected by hu-
mans. They reported gains in accuracy and inter-
annotator agreement. The study focused on the
minimal quality (accuracy threshold) of automatic
annotation that would prove useful for human an-
notation. They reported a gain for human annota-
tion when accuracy ranged from 66.5% to 81.6%.
On the contrary, for a semantic-frame annotation
task, Rehbein et al (2009) observed no significant
gain in quality and speed of annotation even when
using a state-of-the-art system.
Generally speaking, annotators find the pre-
annotation stage useful (Rehbein et al, 2009;
South et al, 2011; Huang et al, 2011). Anno-
tation managers consider that a bias may occur
depending on how much human annotators trust
the pre-annotation (Rehbein et al, 2009; Fort and
Sagot, 2010; South et al, 2011). In their frame-
semantic argument structure annotation, Rehbein
et al (2009) addressed a specific question consid-
ering a two-level annotation scheme: is the pre-
annotation of frame assignment (low-level anno-
tation) useful for annotating semantic roles (high-
level annotation)? Although for the low-level an-
notation task they observed a significant difference
in quality of final annotation, for the high-level
task they found no difference.
Most of these studies used a pre-annotation sys-
tem trained on the same kind of data as those
which were to be annotated manually. Neverthe-
less some system-oriented studies have focused on
the results obtained by systems trained on one type
of corpus and applied to another type of corpus,
e.g., for a Latin POS tagger (Poudat and Longre?e,
2009; Skj?rholt, 2011) or for a CoNLL named en-
tity tagger for German (Faruqui and Pado?, 2010)
for which the authors noticed noticed a reduc-
tion of the F-measure when going from in-domain
(newswire data, F=0.782 for their best system) to
out-of-domain (Europarl data, F=0.656).
One of our objectives is then to examine
whether a system trained on one type of text can
be useful to pre-annotate texts of a different type.
We set up experiments to study precisely the pos-
sible induced bias and whether the level of experi-
ence of the annotators would make a difference in
such a context. In this study, we used two different
kinds of corpora, which were both different from
the corpus used to train the pre-annotation system.
3 Task and corpus description
3.1 Task
In this work, we used the structured named entity
definition we proposed in a previous study (Grouin
et al, 2011): entities are both hierarchical (types
have subtypes) and compositional (types and com-
ponents are included in entities) as in Figure 1.
func.coll
org.ent
name
BEIde la
kind
analystes financiersles
Figure 1: Multi-level annotation of entity sub-
types (red tags) and components (blue tags): the
financial analysts of the EBI
169
This taxonomy of entity types is composed of
7 types (person, location, organization, amount,
time, production and function) and 32 sub-types
(individual person pers.ind vs. group of persons
pers.coll; administrative organization org.adm vs.
services org.ent; etc.). Types and subtypes consti-
tute the first level of annotation.
Within these categories, components are
second-level elements (kind, name, first.name,
etc.), and can never be used outside the scope of a
type or subtype element.
3.2 Corpora
Two French corpora were sampled from larger
ones:
Europarl: Prepared speech (Parliament
Debates?Europarl): 15,306 word extract;
Press: Local, contemporary written news (L?Est
Re?publicain): 11,146 word extract.
These corpora were automatically annotated us-
ing the system described in (Dinarelli and Rosset,
2011). This system relies on a Conditional Ran-
dom Field (CRF) model for the detection of com-
ponents and on a probabilistic context-free gram-
mar (PCFG) model for types and sub-types. These
models have been trained on Broadcast News data.
This system achieved a Slot Error Rate (Makhoul
et al, 1999) of 37.0% on Broadcast conversation
and 29.7% on Broadcast news, and ranked first in
the Quaero evaluation campaign (Galibert et al,
2011).
4 Experiments
In this section we present the protocol we designed
to study the usefulness of pre-annotation under
different conditions, and its overall results.
4.1 Protocol
We defined the following protocol, similar to the
one used in Rehbein et al (2009).
Corpora. Four versions of our two corpora were
prepared: (i) raw text, (ii) pre-annotation of
types, (iii) pre-annotation of components, and
(iv) full pre-annotation of both types and compo-
nents. Each of these four versions was split into
four quarters.
Annotators. Eight human annotators were in-
volved in this task. Among them, four are con-
sidered as expert annotators (they annotated cor-
pora in the previous years) while the four re-
maining ones are novice annotators (this was the
first time they annotated such corpora; they were
given training sessions before starting actual anno-
tation). We defined four pairs of annotators, where
each pair was composed of an expert and a novice
annotator.
Quarter allocation. We allocated each corpus
quarter in such a way that each pair of annotators
processed, in each corpus, material from each one
of the four pre-annotated versions (see Table 3).
The same allocation was made in both corpora.
4.2 Results
For each corpus part, a reference was built based
on a majority vote by confronting all annotations.
The resulting reference corpus is presented in Ta-
ble 1.
Corpus # comp. # types # entities # words
P
re
ss
Q1 481 310 791 3047
Q2 367 246 673 2628
Q3 495 327 822 2971
Q4 413 282 695 2600
E
ur
op
ar
l Q1 362 259 621 3926
Q2 309 221 530 3809
Q3 378 247 625 3604
Q4 413 299 712 3967
Table 1: General description of the reference an-
notations: number of components, types, entities
(the sum of components and types), and words
Table 2 presents the performance of the au-
tomatic pre-annotation system against the refer-
ence corpus. We used the well known F-measure
and in addition the Slot Error Rate as it allows
to weight different error classes (deletions, inser-
tions, type or frontier errors). Fort and Sagot
(2010) reported a gain in human annotation when
pre-annotation accuracy ranged from 66.5% to
81.6%. Given their results we can hope for a gain
in both accuracy and annotation time when using
pre-annotation.
Table 3 presents all results obtained by each an-
notators given each pre-annotation condition (raw,
components, types and full) in terms of precision,
recall and F-measure.
170
Corpus #
Raw text Components Types Full
R P F R P F R P F R P F
Press
Q1
0.874 0.777 0.823 0.876 0.741 0.803 0.824 0.870 0.846 0.852 0.800 0.825
0.810 0.766 0.788 0.815 0.777 0.796 0.645 0.724 0.683 0.844 0.785 0.813
Q2
0.765 0.796 0.780 0.870 0.773 0.819 0.822 0.801 0.812 0.917 0.773 0.839
0.558 0.654 0.602 0.826 0.775 0.800 0.815 0.777 0.795 0.816 0.752 0.783
Q3
0.835 0.715 0.771 0.888 0.809 0.847 0.884 0.796 0.837 0.887 0.859 0.873
0.792 0.689 0.736 0.904 0.780 0.837 0.876 0.771 0.820 0.780 0.827 0.803
Q4
0.802 0.757 0.779 0.845 0.876 0.860 0.900 0.702 0.789 0.914 0.840 0.876
0.794 0.727 0.759 0.696 0.715 0.705 0.812 0.701 0.752 0.802 0.757 0.779
Europarl
Q1
0.809 0.728 0.766 0.800 0.568 0.665 0.776 0.862 0.817 0.754 0.720 0.736
0.754 0.720 0.736 0.720 0.609 0.660 0.687 0.607 0.644 0.736 0.638 0.683
Q2
0.776 0.792 0.784 0.782 0.617 0.690 0.797 0.645 0.713 0.821 0.526 0.641
0.563 0.498 0.529 0.802 0.619 0.699 0.698 0.553 0.617 0.769 0.566 0.652
Q3
0.747 0.459 0.569 0.749 0.624 0.681 0.805 0.800 0.803 0.735 0.744 0.739
0.732 0.598 0.658 0.736 0.717 0.726 0.822 0.738 0.777 0.808 0.734 0.769
Q4
0.742 0.624 0.678 0.874 0.760 0.813 0.732 0.480 0.580 0.743 0.608 0.669
0.721 0.566 0.634 0.695 0.652 0.672 0.707 0.600 0.649 0.738 0.603 0.664
Table 3: Overall recall, precision and F-measure for each pair of annotators (blue: pair #1, ocre: pair
#2, green: pair #3, white: pair #4) on each corpus quarter (Q1, Q2, Q3, Q4), depending on the kind of
pre-annotation (raw text, only components, only types, full pre-annotation). Expert annotator is on the
upper line of each quarter, novice annotator is on the lower line. Boldface indicates the best F-measure
for each novice and expert annotator among all pre-annotation tasks in a given corpus quarter
Corpus
Components Types Full
F SER F SER F SER
P
re
ss
Q1 72.4 37.9 63.5 46.3 68.9 41.0
Q2 77.2 32.2 66.8 43.5 73.1 36.6
Q3 76.1 34.1 68.3 41.7 73.1 36.9
Q4 76.1 33.3 63.3 45.7 71.0 38.2
E
ur
op
ar
l Q1 61.9 49.9 57.5 55.4 60.1 52.2
Q2 61.2 51.3 54.6 54.3 58.5 52.5
Q3 61.6 50.1 53.3 55.7 58.2 52.2
Q4 57.1 57.0 48.1 59.7 53.3 58.1
Broad. 88.3 29.1 73.1 39.1 73.2 33.1
Table 2: F-measure and Slot Error Rate achieved
by the automatic system on each kind of annota-
tion and on in-domain broadcast data
We also computed inter-annotator agreement
(IAA) for each corpus considering two groups of
annotators, experts and novices. We consider that
the inter-annotator agreement is somewhere be-
tween the F-measure and the standard IAA con-
sidering as markables all the units annotated by at
least one of the annotators (Grouin et al, 2011).
We computed Scott?s Pi (Scott, 1955), and Co-
hen?s Kappa (Cohen, 1960). The former considers
one model for all annotators while the latter con-
siders one model per annotator. In our case, these
two values are almost the same, which means that
the proportions and kinds of annotations are very
similar across experts and novices. Figure 2 shows
the IAA (Cohen?s Kappa and F-measure) obtained
on the two corpora given the four pre-annotation
conditions (no pre-annotation, components, types,
and full pre-annotation). As we can see, IAA is
systematically higher for the Press corpus than
for the Europarl corpus, which can be linked
to the higher performance of the automatic pre-
annotation system on this corpus. We also can see
that pre-annotation always improves agreement
and that full pre-annotation yields the best result.
We observe that, as expected, pre-annotation leads
human annotators to obtain higher consistency.
5 Subjective assessment
An important piece of information in any anno-
tation campaign is the feelings of the annotators
about the task. This can give interesting clues
about the expected quality of their work and on the
usefulness of the pre-annotation step. We asked
the annotators a few questions concerning sev-
eral features of this project, such as the annotation
171
 0.5
 0.6
 0.7
 0.8
 0.9
 1
raw comp types full
Press: Cohen's kappaPress: F-measureEuroparl: Cohen's kappaEuroparl: F-measure
Figure 2: Cohen?s Kappa (red and blue) and F-
measure (green and pink) measuring agreement of
experts and novices on Press and Europarl corpora
in four pre-annotation conditions. Each measure
compares the concatenated annotations of the four
experts with the four novices.
manual, or how they assessed the benefits of pre-
annotation in the different corpora (Section 5.1).
Another important point is the experience of the
annotators, which we also examine in the light of
theirs answers to the questionnaire (Section 5.2).
5.1 Questionnaire
The questionnaire submitted to the annotators con-
tained 4 questions, dealing with their feedback on
the annotation process:
1. According to you, which level of pre-
annotation has been the most helpful during
the annotation process? Types, components,
or both?
2. To what extent would you say that pre-
annotation helped you in terms of precision
and speed? Did it produce many errors you
had to correct?
3. If you had to choose between the Europarl
corpus and the Press corpus, could you say
that one has been easier to annotate than the
other?
4. Concerning the annotation manual, are there
topics that you would like to change, or cor-
rect? In the same way, which named entities
caused you the most difficulties to deal with?
All 8 annotators answered these questions. We
summarize below what we found in their answers.
5.1.1 Level of pre-annotation
Most of the annotators preferred the corpora that
were pre-annotated with types only. The reason,
for the most part, is that a pre-annotation of types
allows the annotator to work faster on their files,
because guessing the components from the types
is easier than guessing types from components.2
Indeed, the different types of entities defined in
the manual always imply the same components,
be they specific (to one entity type) or transverse
(common to several entity types). On the contrary,
a transverse component, such as <kind>, can be
part of any type of named entity. The other rea-
son for this choice of pre-annotation concerns the
readability brought to the corpora. An annotation
with types only is easier to read than an annotation
with components, and less exhausting after many
hours of work on the texts.
5.1.2 Gain in precision and speed
What motivated the answers to the second ques-
tion mainly concerns the accuracy of the different
pre-annotation methods. While all of them pre-
sented errors that needed to be corrected, the pre-
annotation of types was the one that they felt pre-
sented the smaller number of errors. Thus, annota-
tors spent less time reviewing the corpora in search
of errors, compared to the other pre-annotated cor-
pora (with components, and with both types and
components), where more errors had to be spot-
ted and corrected. This search for incorrect pre-
annotations impacted the time spent on each cor-
pus. Indeed, most annotators declared that pre-
annotation with types was quicker to deal with
than other pre-annotation schemes.
5.1.3 Corpus differences
About one half of the annotators agreed that the
Europarl corpus had been more difficult to anno-
tate. Despite obvious differences in register, sen-
tence structure and vocabulary between the two
corpora, Europarl seemed more redundant and
complex than the other corpus. For instance, one
of the annotators declared:
The Europarl corpus is more difficult
to annotate in the sense that the exist-
ing types and components do not al-
ways match the realities found in the
corpus, either because their definitions
2This feeling is supported by results about ambiguity pre-
sented in Fort et al (2012).
172
cannot apply exactly, or because the re-
quired types and components are miss-
ing (mainly for frequencies: ?five times
per year?).
The other half of the annotators did not feel any
specific difficulties in annotating one corpus or the
other. According to them, both corpora are the
same in terms of register and sentence structure.
5.1.4 Improvements in guidelines
All of the annotators were unanimous in think-
ing that two points need to be modified in the
manual. First of all, the distinction between the
<org.adm> and <org.ent> subtypes is too diffi-
cult to apprehend, above all in the Europarl corpus
where these entities are too ambiguous to be anno-
tated correctly. Secondly, the distinction between
the <pers> and <func> types has also been diffi-
cult to deal with. The other remarks about poten-
tial changes mainly concerned the introduction of
explicit rules for frequencies, which are recurrent
in the Europarl corpus.
5.2 Experience
As mentioned earlier in Section 4.1, we will now
see if the differences in experience between an-
notators impacted their difficulty in annotating the
corpora. First of all, when we look at the answers
given to question 3, we notice that both novice and
expert annotators consider the Europarl corpus the
most difficult to annotate. Most of their answers
deal with the redundancy and the formal register
of the data. Moreover, as everyone answered in
question 4, both <func> and <org> entities have
to be modified to be easier to understand and to
use. This unanimous opinion about what needs
to be reviewed in the manual allows us to think
that the annotators? level of experience has a low
impact on their apprehension of the corpora, both
Europarl and Press. To confirm this, we can look
at the answers given to questions 1 and 2, as indi-
cated in the previous paragraph. As has been ex-
plained, every annotator correctly pointed at the
many errors found in pre-annotation, regardless
of their experience. Besides, the assessment of
the benefits of pre-annotation is the same for al-
most everyone, regardless of their experience too:
both novice and expert annotators agree that pre-
annotation with type adds efficiency and speed to
annotation.
To conclude, according to our observations
based on the questionnaire, we cannot assert that
there has been a difference between novice and ex-
pert annotators. Both groups agreed on the same
difficulties, pointed at the same errors, and crit-
icized the same entities, saying that their defini-
tions needed to be clarified.
6 Quantitative observations
In this section we provide results of quantitative
observations in order to support, or not, the anno-
tators? subjective assessment.
6.1 Corpus statistics
The annotators reported different feelings depend-
ing on the corpora. Some of them reported that
the Europarl corpus was more difficult to annotate,
with more complex sentence structures, or usage
of fewer proper nouns.
To explore these differences, we computed
some statistics over the two original, un-annotated
corpora (which are much larger than the samples
annotated in this experiment) as well as over the
original broadcast news corpus used to train the
pre-annotation system. Each of these corpora con-
tains several million words.
Table 4 reports simple statistics about sentences
in the three corpora. Based on these statistics,
while the Europarl (Euro) corpus is very similar to
the original Broadcast News (BN), the Press cor-
pus shows differences: sentences are 20% shorter,
with fewer but larger chunks, confirming the im-
pression of simpler, less convoluted sentences.
BN Press Euro
Mean sentence length 30.2 23.9 29.7
Mean chunk count 10.9 6.7 10.4
Mean chunk length 2.7 3.6 2.8
Table 4: Sentence summary of the three corpora
Looking more closely at the contents of these
sentences, Figure 3 summarizes the proportions of
grammatical word classes. The sentiment of ex-
tensive naming of entities in the Press corpus is
confirmed by the four times higher rate of proper
nouns. On the other hand, entities are more often
referred to using nouns with an optional adjective
in the Europarl corpus, leading to a more frequent
usage of the latter.
173
Figure 3: Frequency of word classes in the three
corpora (BN = Broadcast News, Est = Press, Euro
= Europarl). TOOL = grammatical words, PCT/NB
= punctuation and numbers, ADJ/ADV = adjectives
and adverbs, NAM = proper name, NOM = noun,
VER = verb.
6.2 Influence of pre-annotation on the
behaviour of annotators
As already mentioned, it is often reported that a
bias may occur depending on human confidence
in the pre-annotation (Fort and Sagot, 2010; Re-
hbein et al, 2009; South et al, 2011). An im-
portant unknown is always the influence of pre-
annotation on the behaviour of annotators, and at
which point pre-annotation induces more errors
than it helps. This may obviously depend on pre-
annotation quality. Table 5 summarizes the er-
ror rates of the automatic annotator in the stud-
ied data (Press + Europarl) and in comparison to
in-domain data. Insertions (Ins) are extra anno-
tations, deletions (Del) missing annotations, and
substitutions (Subs) are annotations that are incor-
rect in type, boundaries, or both. We can see that
Domain Pre-annotation Ins Del Subs
Components 4.4% 33.6% 7.8%
Out Types 7.0% 36.2% 12.7%
Full 5.5% 34.6% 9.7%
In Full 3.7% 23.4% 10.6%
Table 5: Pre-annotation errors and comparison
with in-domain (Broadcast News) data
going out-of-domain increased deletions, proba-
bly through a lack of knowledge of domain vo-
cabulary. But it did not influence the other error
rates significantly. It is also noticeable that dele-
tion is the type of error most produced by the sys-
tem, with every third entity missed. Automatic,
full pre-annotation of Press + Europarl obtains a
precision of 0.79 and a recall of 0.56.
Human annotator performance can then be mea-
sured over the same three error types (Table 6). We
Pre-annotation Ins Del Subs
Raw 8.9% 18.9% 12.8%
Components 5.9% 16.7% 11.3%
Types 7.1% 16.5% 12.0%
Full 7.1% 16.5% 10.1%
Table 6: Mean human annotation error levels for
each pre-annotation scheme
can see that annotation quality was systematically
improved by pre-annotation, with the best global
result obtained by full pre-annotation. In addition
there was no increase in deletions (had the human
stopped looking at the unannotated text) or inser-
tions (had the human always trusted the system) as
might have been feared. This may be a side effect
of the high deletion rate, making it obvious to the
human that the system was missing things. In any
case, the annotation was clearly beneficial in our
experiment with no ill effects seen in error rates
compared to the gold standard.
6.3 Is pre-annotation useful and to whom?
All annotators asserted that pre-annotation is use-
ful, specifically with types. In this section, we pro-
vide observations concerning variations in annota-
tion both in terms of accuracy (F-measure is used)
and duration.
Raw Comp. Types Full
Experts 0.748 0.786 0.778 0.791
Novices 0.682 0.737 0.721 0.742
Table 7: Mean F-measure of experts and novices,
for each pre-annotation scheme
Raw Comp. Types Full
Experts 109.0 52.5 64.0 39.13
Novices 151.7 135.5 117.9 103.88
Table 8: Mean duration (in minutes) of annotation
for experts and novices, for each pre-annotation
scheme (two corpus quarters)
Tables 7 and 8 confirm the hypothesis that auto-
matic pre-annotation helps annotators to annotate
174
faster and to be more efficient. All pre-annotation
levels (components, types and both) seem to be
helpful for both experts and novices. Experts
reached a higher accuracy (F=0.791) and they
were more than twice faster with components or
full pre-annotation. Similarly, novices performed
better when working on a full pre-annotation
(F=0.742) and reached a faster working time
(48mn less than with no pre-annotation). This last
observation contradicts the annotators? reported
experience: the annotators felt more comfortable
and faster with a types-only pre-annotation than
with full pre-annotation (see Section 5.1.2). The
results show that full pre-annotation was the best
choice for both quality and speed.
These results confirm that pre-annotation is use-
ful, even with a moderate level of performance of
the system. Does it help to annotate components
and types equally? To answer this question, we
computed the F-measure of novices and experts
for both components and types separately (see Fig-
ure 4).
 60
 65
 70
 75
 80
 85
 90
raw comp types full
types/novicescomponents/novicestypes/expertscomponents/experts
Figure 4: Mean F-measure on each pre-annotation
level for expert and novice annotators
For experts we can see that all pre-annotation
levels allow them to improve their performance on
both types and components. However for novices,
pre-annotation with types does not improve their
performance in labeling components. We also no-
tice that pre-annotation in both types and compo-
nents allows experts and novices to reach their best
performance for both types and components.
7 Conclusion and Perspectives
Conclusion. In this paper, we studied the inter-
est of a pre-annotation process for a complex an-
notation task with only an out-of-domain annota-
tion system available. We also designed our exper-
iments to check whether the level of experience of
the annotators made a difference in such a context.
The experiment produced in the end a high-quality
gold standard (8-way merge including 2 versions
without pre-annotation) which enabled us to mea-
sure quantitatively the performance of every pre-
annotation scheme.
We noticed that the pre-annotation system
proved relatively precise for such a complex task,
with 79% correct pre-annotations, but with a poor
recall at 56%. This may be a good operating point
for a pre-annotation system to reduce bias though.
In our quantitative experiments we found that
the fullest pre-annotation helped most, both in
terms of quality and annotation speed, even though
the quality of the pre-annotation system varied de-
pending on the annotation layer. This contradicted
the feelings of the annotators who thought that a
type-only pre-annotation was the most efficient.
This shows that in such a setting self-evaluation
cannot be trusted. On the other hand their remarks
about the problems in the annotation guide itself
seemed rather pertinent.
When it comes to experts vs. novices, we noted
that their behaviour and remarks were essentially
identical. Experts were both better and faster
at annotating, but had similar reactions to pre-
annotation and essentially the same feelings.
In conclusion, even with an out-of-domain sys-
tem, a pre-annotation step proves extremely useful
in both annotation speed and annotation quality,
and at least in our setting, with a reasonably pre-
cise system (at the expense of recall) no bias was
detectable. In addition, no matter what the anno-
tators feel, as long as precision is good enough,
the more pre-annotations the better. Pre-filtering
either of our two levels did not help.
Perspectives. Based upon this conclusion, we
plan to use automatic pre-annotation in further an-
notation work, beginning with the present corpora.
As a first use, we plan to propose a few changes
to the annotation principles in the guidelines we
used. To annotate existing corpora with these
changes, automatic pre-annotation will be useful.
As a second piece of future work, we plan to
annotate new corpora with the existing annotation
framework. We also plan to add new types of
named entities (e.g., events) to extend the anno-
tation of existing annotated corpora, using the pre-
annotation process to reduce the overall workload.
175
Acknowledgments
This work has been partially funded by OSEO un-
der the Quaero program and by the French ANR
VERA project.
References
Jacob Cohen. 1960. A coefficient of agreement
for nominal scales. Educational and Psychological
Measurement, 20(1):37?46.
Sandipan Dandapat, Priyanka Biswas, Monojit Choud-
hury, and Kalika Bali. 2009. Complex linguistic an-
notation ? no easy way out! A case from Bangla and
Hindi POS labeling tasks. In Proc of 3rd Linguistic
Annotation Workshop (LAW-III), pages 10?18, Sun-
tec, Singapore, August. ACL.
Marco Dinarelli and Sophie Rosset. 2011. Models
cascade for tree-structured named entity detection.
In Proc of IJCNLP, pages 1269?1278, Chiang Mai,
Thailand.
Manaal Faruqui and Sebastian Pado?. 2010. Training
and evaluating a German named entity recognizer
with semantic generalization. In Proc of Konvens,
Saarbru?cken, Germany.
Kare?n Fort and Beno??t Sagot. 2010. Influence of pre-
annotation on POS-tagged corpus development. In
Proc of 4th Linguistic Annotation Workshop (LAW-
IV), pages 56?63, Uppsala, Sweden. ACL.
Kare?n Fort, Adeline Nazarenko, and Sophie Rosset.
2012. Modeling the complexity of manual anno-
tation tasks: a grid of analysis. In Proceedings of
COLING 2012, pages 895?910, Mumbai, India, De-
cember. The COLING 2012 Organizing Committee.
Olivier Galibert, Sophie Rosset, Cyril Grouin, Pierre
Zweigenbaum, and Ludovic Quintard. 2011. Struc-
tured and extended named entity evaluation in au-
tomatic speech transcriptions. In Proc of IJCNLP,
Chiang Mai, Thailand.
Cyril Grouin, Sophie Rosset, Pierre Zweigenbaum,
Kare?n Fort, Olivier Galibert, and Ludovic Quin-
tard. 2011. Proposal for an extension of tradi-
tional named entities: From guidelines to evalua-
tion, an overview. In Proc of 5th Linguistic Anno-
tation Workshop (LAW-V), pages 92?100, Portland,
OR. ACL.
Minlie Huang, Aure?lie Ne?ve?ol, and Zhiyong Lu.
2011. Recommending MeSH terms for annotating
biomedical articles. Journal of the American Medi-
cal Informatics Association, 18(5):660?7.
Geoffrey Leech. 1997. Introducing corpus annota-
tion. In Roger Garside, Geoffrey Leech, and Tony
McEnery, editors, Corpus annotation: Linguistic in-
formation from computer text corpora, pages 1?18.
Longman, London.
John Makhoul, Francis Kubala, Richard Schwartz, and
Ralph Weischedel. 1999. Performance measures for
information extraction. In Proc. of DARPA Broad-
cast News Workshop, pages 249?252.
Mitchell P Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of English: the Penn TreeBank. Com-
putational Linguistics, 19(2):313?330.
Claudiu Mihaila, Tomoko Ohta, Sampo Pyysalo, and
Sophia Ananiadou. 2013. Biocause: Annotating
and analysing causality in the biomedical domain.
BMC Bioinformatics, 14:2.
Ce?line Poudat and Doninique Longre?e. 2009. Vari-
ations langagie`res et annotation morphosyntaxique
du latin classique. Traitement Automatique des
Langues, 50(2):129?148.
Dietrich Rebholz-Schuhmann, Antonio Jimeno, Chen
Li, Senay Kafkas, Ian Lewin, Ning Kang, Peter Cor-
bett, David Milward, Ekaterina Buyko, Elena Beiss-
wanger, Kerstin Hornbostel, Alexandre Kouznetsov,
Rene? Witte, Jonas B Laurila, Christopher JO Baker,
Cheng-Ju Kuo, Simone Clematide, Fabio Rinaldi,
Richa?rd Farkas, Gyo?rgy Mo?ra, Kazuo Hara, Laura I
Furlong, Michael Rautschka, Mariana Lara Neves,
Alberto Pascual-Montano, Qi Wei, Nigel Collier,
Md Faisal Mahbub Chowdhury, Alberto Lavelli,
Rafael Berlanga, Roser Morante, Vincent Van Asch,
Walter Daelemans, Jose? L Marina, Erik van Mulli-
gen, Jan Kors, and Udo Hahn. 2011. Assessment of
NER solutions against the first and second CALBC
silver standard corpus. J Biomed Semantics, 2.
Ines Rehbein, Josef Ruppenhofer, and Caroline
Sporleder. 2009. Assessing the benefits of partial
automatic pre-labeling for frame-semantic annota-
tion. In Proc of 3rd Linguistic Annotation Workshop
(LAW-III), pages 19?26, Suntec, Singapore. ACL.
Eric Ringger, Peter McClanahan, Robbie Haertel,
George Busby, Marc Carmen, James Carroll, Kevin
Seppi, and Deryle Lonsdale. 2007. Active learning
for part-of-speech tagging: Accelerating corpus an-
notation. In Proc of Linguistic Annotation Workshop
(LAW), pages 101?108. ACL.
Sophie Rosset, Cyril Grouin, Kare?n Fort, Olivier Gal-
ibert, Juliette Kahn, and Pierre Zweigenbaum. 2012.
Structured named entities in two distinct press cor-
pora: Contemporary broadcast news and old news-
papers. In Proc of 6th Linguistic Annotation Work-
shop (LAW-VI), pages 40?48, Jeju, South Korea.
ACL.
William A Scott. 1955. Reliability of content analysis:
The case of nominal scale coding. Public Opinion
Quaterly, 19(3):321?325.
Burr Settles, Mark Craven, and Lewis Friedland. 2008.
Active learning with real annotation costs. In Proc
of the NIPS Workshop on Cost-Sensitive Learning.
176
Arne Skj?rholt. 2011. More, faster: Accelerated cor-
pus annotation with statistical taggers. Journal for
Language Technology and Computational Linguis-
tics, 26(2):151?163.
Brett R South, Shuying Shen, Robyn Barrus, Scott L
DuVall, O?zlem Uzuner, and Charlene Weir. 2011.
Qualitative analysis of workflow modifications used
to generate the reference standard for the 2010
i2b2/VA challenge. In Proc of AMIA.
177
Proceedings of the 6th Workshop on Building and Using Comparable Corpora, pages 16?23,
Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational Linguistics
Using WordNet and Semantic Similarity for Bilingual Terminology
Mining from Comparable Corpora
Dhouha Bouamor
CEA, LIST, Vision and
Content Engineering Laboratory,
91191 Gif-sur-Yvette CEDEX
France
dhouha.bouamor@cea.fr
Nasredine Semmar
CEA, LIST, Vision and Content
Engineering Laboratory,
91191 Gif-sur-Yvette
CEDEX France
nasredine.semmar@cea.fr
Pierre Zweigenbaum
LIMSI-CNRS,
F-91403 Orsay CEDEX
France
pz@limsi.fr
Abstract
This paper presents an extension of the
standard approach used for bilingual lex-
icon extraction from comparable corpora.
We study of the ambiguity problem re-
vealed by the seed bilingual dictionary
used to translate context vectors. For
this purpose, we augment the standard ap-
proach by a Word Sense Disambiguation
process relying on a WordNet-based se-
mantic similarity measure. The aim of
this process is to identify the translations
that are more likely to give the best rep-
resentation of words in the target lan-
guage. On two specialized French-English
comparable corpora, empirical experimen-
tal results show that the proposed method
consistently outperforms the standard ap-
proach.
1 Introduction
Bilingual lexicons play a vital role in many Natu-
ral Language Processing applications such as Ma-
chine Translation (Och and Ney, 2003) or Cross-
Language Information Retrieval (Shi, 2009). Re-
search on lexical extraction from multilingual cor-
pora have largely focused on parallel corpora. The
scarcity of such corpora in particular for special-
ized domains and for language pairs not involv-
ing English pushed researchers to investigate the
use of comparable corpora (Fung, 1998; Chiao
and Zweigenbaum, 2003). These corpora are com-
prised of texts which are not exact translation of
each other but share common features such as do-
main, genre, sampling period, etc.
The main work in this research area could be
seen as an extension of Harris?s distributional hy-
pothesis (Harris, 1954). It is based on the sim-
ple observation that a word and its translation are
likely to appear in similar contexts across lan-
guages (Rapp, 1995). Based on this assumption,
the alignment method, known as the standard ap-
proach builds and compares context vectors for
each word of the source and target languages.
A particularity of this approach is that, to enable
the comparison of context vectors, it requires the
existence of a seed bilingual dictionary to translate
source context vectors. The use of the bilingual
dictionary is problematic when a word has sev-
eral translations, whether they are synonymous or
polysemous. For instance, the French word action
can be translated into English as share, stock, law-
suit or deed. In such cases, it is difficult to iden-
tify in flat resources like bilingual dictionaries,
wherein entries are usually unweighted and un-
ordered, which translations are most relevant. The
standard approach considers all available trans-
lations and gives them the same importance in
the resulting translated context vectors indepen-
dently of the domain of interest and word ambigu-
ity. Thus, in the financial domain, translating ac-
tion into deed or lawsuit would probably introduce
noise in context vectors.
In this paper, we present a novel approach
which addresses the word ambiguity problem ne-
glected in the standard approach. We introduce a
use of a WordNet-based semantic similarity mea-
sure permitting the disambiguation of translated
context vectors. The basic intuition behind this
method is that instead of taking all translations
of each seed word to translate a context vector,
we only use the translations that are more likely
to give the best representation of the context vec-
tor in the target language. We test the method on
two specialized French-English comparable cor-
16
pora (financial and medical) and report improved
results, especially when many of the words in the
corpus are ambiguous.
The remainder of the paper is organized as fol-
lows: Section 2 presents the standard approach
and recalls in some details previous work address-
ing the task of bilingual lexicon extraction from
comparable corpora. In section 3 we present our
context disambiguation process. Before conclud-
ing and presenting directions for future work, we
describe in section 4 the experimental protocol we
followed and discuss the obtained results.
2 Bilingual lexicon extraction
2.1 Standard Approach
Most previous works addressing the task of bilin-
gual lexicon extraction from comparable corpora
are based on the standard approach (Fung, 1998;
Chiao and Zweigenbaum, 2002; Laroche and
Langlais, 2010). Formally, this approach is com-
posed of the following three steps:
1. Building context vectors: Vectors are first
extracted by identifying the words that appear
around the term to be translated S in a win-
dow of N words. Generally, an association
measure like the mutual information (Morin
and Daille, 2006), the log-likelihood (Morin
and Prochasson, 2011) or the Discounted
Odds-Ratio (Laroche and Langlais, 2010) are
employed to shape the context vectors.
2. Translation of context vectors: To enable
the comparison of source and target vectors,
source terms vectors are translated in the tar-
get language by using a seed bilingual dic-
tionary. Whenever it provides several trans-
lations for an element, all proposed transla-
tions are considered. Words not included in
the bilingual dictionary are simply ignored.
3. Comparison of source and target vectors:
Translated vectors are compared to target
ones using a similarity measure. The most
widely used is the cosine similarity, but
many authors have studied alternative metrics
such as the Weighted Jaccard index (Prochas-
son et al, 2009) or the City-Block dis-
tance (Rapp, 1999). According to similarity
values, a ranked list of translations for S is
obtained.
2.2 Related Work
Recent improvements of the standard approach are
based on the assumption that the more the con-
text vectors are representative, the better the bilin-
gual lexicon extraction is. Prochasson et al (2009)
used transliterated words and scientific compound
words as ?anchor points?. Giving these words
higher priority when comparing target vectors im-
proved bilingual lexicon extraction. In addition to
transliteration, Rubino and Linare`s (2011) com-
bined the contextual representation within a the-
matic one. The basic intuition of their work is that
a term and its translation share thematic similari-
ties. Hazem and Morin (2012) recently proposed a
method that filters the entries of the bilingual dic-
tionary based upon POS-tagging and domain rel-
evance criteria, but no improvements was demon-
strated.
Gaussier et al (2004) attempted to solve the
problem of different word ambiguities in the
source and target languages. They investigated a
number of techniques including canonical corre-
lation analysis and multilingual probabilistic la-
tent semantic analysis. The best results, with a
very small improvement were reported for a mixed
method. One important difference with Gaussier
et al (2004) is that they focus on words ambigu-
ities on source and target languages, whereas we
consider that it is sufficient to disambiguate only
translated source context vectors.
A large number of Word Sense Disambigua-
tion WSD techniques were previously proposed
in the literature. The most popular ones are those
that compute semantic similarity with the help
of existing thesauri such as WordNet (Fellbaum,
1998). This resource groups English words into
sets of synonyms called synsets, provides short,
general definitions and records various semantic
relations (hypernymy, meronymy, etc.) between
these synonym sets. This thesaurus has been ap-
plied to many tasks relying on word-based sim-
ilarity, including document (Hwang et al, 2011)
and image (Cho et al, 2007; Choi et al, 2012)
retrieval systems. In this work, we use this re-
source to derive a semantic similarity between lex-
ical units within the same context vector. To the
best of our knowledge, this is the first application
of WordNet to the task of bilingual lexicon extrac-
tion from comparable corpora.
17
  
Word?to?be?translated?(source?language)
Building?Context?Vector
Context?vector?  Translated?Context?vector
Bilingual?Dictionary WordNet
Disambiguated??Context?vector
Context?Vectors?(Target?language)
Figure 1: Overall architecture of the lexical extraction approach
3 Context Vector Disambiguation
The approach we propose includes the three steps
of the standard approach. As it was mentioned in
section 1, when lexical extraction applies to a spe-
cific domain, not all translations in the bilingual
dictionary are relevant for the target context vec-
tor representation. For this reason, we introduce
a WordNet-based WSD process that aims at im-
proving the adequacy of context vectors and there-
fore improve the results of the standard approach.
Figure 1 shows the overall architecture of the lexi-
cal extraction process. Once translated into the tar-
get language, the context vectors disambiguation
process intervenes. This process operates locally
on each context vector and aims at finding the
most prominent translations of polysemous words.
For this purpose, we use monosemic words as a
seed set of disambiguated words to infer the pol-
ysemous word?s translations senses. We hypoth-
esize that a word is monosemic if it is associated
to only one entry in the bilingual dictionary. We
checked this assumption by probing monosemic
entries of the bilingual dictionary against WordNet
and found that 95% of the entries are monosemic
in both resources.
Formally, we derive a semantic similarity value
between all the translations provided for each pol-
ysemous word by the bilingual dictionary and
all monosemic words appearing whithin the same
context vector. There is a relatively large number
of word-to-word similarity metrics that were pre-
viously proposed in the literature, ranging from
path-length measures computed on semantic net-
works, to metrics based on models of distribu-
tional similarity learned from large text collec-
tions. For simplicity, we use in this work, the Wu
and Palmer (1994) (WUP) path-length-based se-
mantic similarity measure. It was demonstrated by
(Lin, 1998) that this metric achieves good perfor-
mances among other measures. WUP computes a
score (equation 1) denoting how similar two word
senses are, based on the depth of the two synsets
(s1 and s2) in the WordNet taxonomy and that of
their Least Common Subsumer (LCS), i.e., the
most specific word that they share as an ancestor.
WupSim(s1, s2) =
2? depth(LCS)
depth(s1) + depth(s2)
(1)
In practice, since a word can belong to more
than one synset in WordNet, we determine the
semantic similarity between two words w1 and
w2 as the maximum WupSim between the synset
or the synsets that include the synsets(w1) and
synsets(w2) according to the following equation:
SemSim(w1, w2) = max{WupSim(s1, s2);
(s1, s2) ? synsets(w1)? synsets(w2)} (2)
18
Context Vector Translations Comparison Ave Sim
liquidite? liquidity ? ?
action
act SemSim(act,liquidity), SemSim(act,dividend) 0.2139
action SemSim(action,liquidity), SemSim(action,dividend) 0.4256
stock SemSim(stock,liquidity), SemSim(stock,dividend) 0.5236
deed SemSim(deed,liquidity), SemSim(deed,dividend) 0.1594
lawsuit SemSim(lawsuit,liquidity), SemSim(lawsuit,dividend) 0.1212
fact SemSim(fact,liquidity), SemSim(fact,dividend) 0.1934
operation SemSim(operation,liquidity), SemSim(operation,dividend) 0.2045
share SemSim(share,liquidity), SemSim(share,dividend) 0.5236
plot SemSim(plot,liquidity), SemSim(plot,dividend) 0.2011
dividende dividend ? ?
Table 1: Disambiguation of the context vector of the French term be?ne?fice [income] in the corporate
finance domain. liquidite? and dividende are monosemic and are used to infer the most similar translations
of the term action.
Then, to identify the most prominent translations
of each polysemous unit wp, an average similarity
is computed for each translation wjp of wp:
Ave Sim(wjp) =
?N
i=1 SemSim(wi, w
j
p)
N
(3)
where N is the total number of monosemic words
and SemSim is the similarity value of w
j
p and the
ith monosemic word. Hence, according to average
relatedness values Ave Sim(wjp), we obtain for
each polysemous word wp an ordered list of trans-
lations w1p . . . w
n
p . This allows us to select trans-
lations of words which are more salient than the
others to represent the word to be translated.
In Table 1, we present the results of the dis-
ambiguation process for the context vector of the
French term be?ne?fice in the corporate finance cor-
pus. This vector contains the words action, div-
idende, liquidite? and others. The bilingual dic-
tionary provides the following translations {act,
stock, action, deed, lawsuit, fact, operation, plot,
share} for the French polysemous word action.
We use the monosemic words dividende and liq-
uidite? to disambiguate the word action. From ob-
serving average similariy values (Ave Sim), we
notice that the words share and stock are on the
top of the list and therefore are most likely to rep-
resent the source word action in this context.
Corpus French English
Corporate finance 402, 486 756, 840
Breast cancer 396, 524 524, 805
Table 2: Comparable corpora sizes in term of
words.
4 Experiments and Results
4.1 Resources
4.1.1 Comparable corpora
We conducted our experiments on two French-
English comparable corpora specialized on
the corporate finance and the breast cancer
domains. Both corpora were extracted from
Wikipedia1. We consider the topic in the source
language (for instance finance des entreprises
[corporate finance]) as a query to Wikipedia
and extract all its sub-topics (i.e., sub-categories
in Wikipedia) to construct a domain-specific
category tree. A sample of the corporate fi-
nance sub-domain?s category tree is shown in
Figure 2. Then, based on the constructed tree,
we collect all Wikipedia pages belonging to one
of these categories and use inter-language links
to build the comparable corpus. Both corpora
were normalized through the following linguistic
preprocessing steps: tokenisation, part-of-speech
tagging, lemmatisation, and function word re-
moval. The resulting corpora2 sizes are given in
Table 2.
1http://dumps.wikimedia.org/
2Comparable corpora will be shared publicly
19
  
Finance?des?entreprise?[Corporate?Finance]
Analyse?Financi?re?[Financial?Analysis] Comptabilit??g?n?rale[Financial?accountancy] Indicateur?Financier[Financial?ratios]
Risque[Risk] Cr?dit[Credit]Actifs[Asset] Bilan[Balance?sheet] Salaire[Salary]Solde[Balance]
B?n?fice[profit] Revenu[Income]
...?...?
Figure 2: Wikipedia categories tree of the corporate finance sub-domain.
4.1.2 Bilingual dictionary
The bilingual dictionary used to translate context
vectors consists of an in-house manually revised
bilingual dictionary which contains about 120,000
entries belonging to the general domain. It is im-
portant to note that words on both corpora has on
average, 7 translations in the bilingual dictionary.
4.1.3 Evaluation list
In bilingual terminology extraction from compa-
rable corpora, a reference list is required to eval-
uate the performance of the alignment. Such
lists are usually composed of about 100 sin-
gle terms (Hazem and Morin, 2012; Chiao and
Zweigenbaum, 2002). Here, we created two refer-
ence lists3 for the corporate finance and the breast
cancer domains. The first list is composed of 125
single terms extracted from the glossary of bilin-
gual micro-finance terms4. The second list con-
tains 96 terms extracted from the French-English
MESH and the UMLS thesauri5. Note that refer-
ence terms pairs appear at least five times in each
part of both comparable corpora.
4.2 Experimental setup
Three other parameters need to be set up: (1) the
window size, (2) the association measure and the
(3) similarity measure. To define context vectors,
we use a seven-word window as it approximates
syntactic dependencies. Concerning the rest of the
3Reference lists will be shared publicly
4http://www.microfinance.lu/en/
5http://www.nlm.nih.gov/
parameters, we followed Laroche and Langlais
(2010) for their definition. The authors carried out
a complete study of the influence of these param-
eters on the bilingual alignment and showed that
the most effective configuration is to combine the
Discounted Log-Odds ratio (equation 4) with the
cosine similarity. The Discounted Log-Odds ratio
is defined as follows:
Odds-Ratiodisc = log
(O11 + 12)(O22 +
1
2)
(O12 + 12)(O21 +
1
2)
(4)
where Oij are the cells of the 2 ? 2 contingency
matrix of a token s co-occurring with the term S
within a given window size.
4.3 Results and discussion
It is difficult to compare results between different
studies published on bilingual lexicon extraction
from comparable corpora, because of difference
between (1) used corpora (in particular their con-
struction constraints and volume), (2) target do-
mains, and also (3) the coverage and relevance of
linguistic resources used for translation. To the
best of our knowledge, there is no common bench-
mark that can serve as a reference. For this reason,
we use the results of the standard approach (SA)
described in section 2.1 as a reference. We evalu-
ate the performance of both the SA and ours with
respect to TopN precision (PN ), recall (RN ) and
Mean Reciprocal Rank (MRR) (Voorhees, 1999).
Precision is the total number of correct translations
divided by the number of terms for which the sys-
tem gave at least one answer. Recall is equal to
20
a)
C
or
po
ra
te
F
in
an
ce
Method P1 P10 P20 R1 R10 R20 MRR
Standard Approach (SA) 0.046 0.140 0.186 0.040 0.120 0.160 0.064
WN-T1 0.065 0.196 0.261 0.056 0.168 0.224 0.089
WN-T2 0.102 0.252 0.308 0.080 0.216 0.264 0.122
WN-T3 0.102 0.242 0.327 0.088 0.208 0.280 0.122
WN-T4 0.112 0.224 0.299 0.090 0.190 0.250 0.124
WN-T5 0.093 0.205 0.280 0.080 0.176 0.240 0.110
WN-T6 0.084 0.205 0.233 0.072 0.176 0.200 0.094
WN-T7 0.074 0.177 0.242 0.064 0.152 0.208 0.090
b)
B
re
as
tC
an
ce
r
Method P1 P10 P20 R1 R10 R20 MRR
Standard Approach (SA) 0.342 0.542 0.585 0.250 0.395 0.427 0.314
WN-T1 0.257 0.500 0.571 0.187 0.364 0.416 0.257
WN-T2 0.314 0.614 0.671 0.229 0.447 0.489 0.313
WN-T3 0.342 0.628 0.671 0.250 0.458 0.489 0.342
WN-T4 0.342 0.571 0.642 0.250 0.416 0.468 0.332
WN-T5 0.357 0.571 0.657 0.260 0.416 0.479 0.348
WN-T6 0.357 0.571 0.652 0.260 0.416 0.468 0.347
WN-T7 0.357 0.585 0.657 0.260 0.427 0.479 0.339
Table 3: Precision, Recall at TopN (N=1,10,20) and MRR at Top20 for the two domains. In each column,
bold show best results. Underline show best results overall.
the ratio of correct translation to the total number
of terms. The MRR takes into account the rank
of the first good translation found for each entry.
Formally, it is defined as:
MRR =
1
Q
i=1?
|Q|
1
ranki
(5)
where Q is the total number of terms to be trans-
lated and ranki is the position of the first correct
translation in the translations candidates.
Our method provides a ranked list of transla-
tions for each polysemous word. A question that
arises here is whether we should introduce only
the best ranked translation in the context vector
or consider a larger number of words, especially
when a translations list contain synonyms (share
and stock in Table 1). For this reason, we take
into account in our experiments different number
of translations, noted WN-Ti, ranging from the
pivot translation (i = 1) to the seventh word in the
translations list. This choice is motivated by the
fact that words in both corpora have on average 7
translations in the bilingual dictionary. The base-
line (SA) uses all translations associated to each
entry in the bilingual dictionary. Table 3a displays
the results obtained for the corporate finance cor-
pus. The first substantial observation is that our
method which consists in disambiguating polyse-
mous words within context vectors consistently
outperforms the standard approach (SA) for all
configurations. The best MRR is reported when
for each polysemous word, we keep the most simi-
lar four translations (WN-T4) in the context vector
of the term to be translated. However, the highest
Top20 precision and recall are obtained by WN-
T3. Using the top three word translations in the
vector boosts the Top20 precision from 0.186 to
0.327 and the Top20 recall from 0.160 to 0.280.
Concerning the Breast Cancer corpus, slightly dif-
ferent results were obtained. As Table 3b show,
when the context vectors are totally disambiguated
(i.e. each source unit is translated by at most one
word in context vectors), all TopN precision, re-
call and MRR decrease. However, we report im-
provements against the SA in most other cases.
For WN-T5, we obtain the maximum MRR score
with an improvement of +0.034 over the SA. But,
as for the corporate finance corpus, the best Top20
precision and recall are reached by the WN-T3
method, with a gain of +0.082 in both Top10 and
Top20 precision and of about +0.06 in Top10 and
Top20 recall.
From observing result tables of both corporate
finance and breast cancer domains, we notice that
our approach performs better than the SA but with
different degrees. The improvements achieved in
21
Corpus Corpus PR Vectors PR
Corporate finance 41% 91, 6%
Breast cancer 47% 85, 1%
Table 4: Comparable corpora?s and context vec-
tor?s Polysemy Rates PR.
the corporate finance domain are higher than those
reported in the breast cancer domain. The reason
being that the vocabulary used in the breast cancer
corpus is more specific and therefore less ambigu-
ous than that used in corporate finance texts. The
results given in table 4 validate this assumption. In
this table, we give the polysemy rates of the com-
parable corpora (Corpus PR) and that of context
vectors (Vectors PR). PR indicates the percent-
age of words that are associated to more than one
translation in the bilingual dictionary. The results
show that breast cancer corpus is more polysemic
than that of the corporate finance. Nevertheless,
even if in both corpora, the candidates? context
vectors are highly polysemous, breast cancer?s
context vectors are less polysemous than those of
the corporate finance texts. In this corpus, 91, 6%
of the words used as entries to define context vec-
tors are polysemous. This shows that the ambi-
guity present in specialized comparable corpora
hampers bilingual lexicon extraction, and that dis-
ambiguation positively affects the overall results.
Even though the two corpora are fairly different
(subject and polysemy rate), the optimal Top20
precision and recall results are obtained when con-
sidering up to three most similar translations in
context vectors. This behavior shows that the dis-
ambiguation method is relatively robust to domain
change. We notice also that the addition of supple-
mentary translations, which are probably noisy in
the given domain, degrades the overall results but
remains greater than the SA.
5 Conclusion
We presented in this paper a novel method that
extends the standard approach used for bilin-
gual lexicon extraction from comparable corpora.
The proposed method disambiguates polysemous
words in context vectors and selects only the trans-
lations that are most relevant to the general con-
text of the corpus. Conducted experiments on two
highly polysemous specialized comparable cor-
pora show that integrating such process leads to
a better performance than the standard approach.
Although our initial experiments are positive, we
believe that they could be improved in a number
of ways. In addition to the metric defined by (Wu
and Palmer, 1994), we plan to apply other seman-
tic similarity and relatedness measures and com-
pare their performance. It would also be interest-
ing to mine much more larger comparable corpora
and focus on their quality as presented in (Li and
Gaussier, 2010). We want also to test our method
on bilingual lexicon extraction for a larger panel of
specialized corpora, where disambiguation meth-
ods are needed to prune translations that are irrel-
evant to the domain.
References
Yun-Chuang Chiao and Pierre Zweigenbaum. 2002.
Looking for candidate translational equivalents in
specialized, comparable corpora. In Proceedings of
the 19th international conference on Computational
linguistics - Volume 2, COLING ?02, pages 1?5. As-
sociation for Computational Linguistics.
Yun-Chuang Chiao and Pierre Zweigenbaum. 2003.
The effect of a general lexicon in corpus-based iden-
tification of french-english medical word transla-
tions. In Proceedings Medical Informatics Europe,
volume 95 of Studies in Health Technology and In-
formatics, pages 397?402, Amsterdam.
Miyoung Cho, Chang Choi, Hanil Kim, Jungpil Shin,
and PanKoo Kim. 2007. Efficient image retrieval
using conceptualization of annotated images. Lec-
ture Notes in Computer Science, pages 426?433.
Springer.
Dongjin Choi, Jungin Kim, Hayoung Kim, Myungg-
won Hwang, and Pankoo Kim. 2012. A method for
enhancing image retrieval based on annotation using
modified wup similarity in wordnet. In Proceed-
ings of the 11th WSEAS international conference
on Artificial Intelligence, Knowledge Engineering
and Data Bases, AIKED?12, pages 83?87, Stevens
Point, Wisconsin, USA. World Scientific and Engi-
neering Academy and Society (WSEAS).
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. Bradford Books.
Pascale Fung. 1998. A statistical view on bilingual
lexicon extraction: From parallel corpora to non-
parallel corpora. In Parallel Text Processing, pages
1?17. Springer.
E?ric Gaussier, Jean-Michel Renders, Irina Matveeva,
Cyril Goutte, and Herve? De?jean. 2004. A geometric
view on bilingual lexicon extraction from compara-
ble corpora. In ACL, pages 526?533.
Z.S. Harris. 1954. Distributional structure. Word.
22
Amir Hazem and Emmanuel Morin. 2012. Adap-
tive dictionary for bilingual lexicon extraction from
comparable corpora. In Proceedings, 8th interna-
tional conference on Language Resources and Eval-
uation (LREC), Istanbul, Turkey, May.
Myunggwon Hwang, Chang Choi, and Pankoo Kim.
2011. Automatic enrichment of semantic relation
network and its application to word sense disam-
biguation. IEEE Transactions on Knowledge and
Data Engineering, 23:845?858.
Audrey Laroche and Philippe Langlais. 2010. Re-
visiting context-based projection methods for term-
translation spotting in comparable corpora. In 23rd
International Conference on Computational Lin-
guistics (Coling 2010), pages 617?625, Beijing,
China, Aug.
Bo Li and E?ric Gaussier. 2010. Improving corpus
comparability for bilingual lexicon extraction from
comparable corpora. In 23rd International Confer-
ence on Computational Linguistics (Coling 2010),
Beijing, China, Aug.
Dekang Lin. 1998. An information-theoretic def-
inition of similarity. In Proceedings of the Fif-
teenth International Conference on Machine Learn-
ing, ICML ?98, pages 296?304, San Francisco, CA,
USA. Morgan Kaufmann Publishers Inc.
Emmanuel Morin and Be?atrice Daille. 2006. Com-
parabilite? de corpus et fouille terminologique mul-
tilingue. In Traitement Automatique des Langues
(TAL).
Emmanuel Morin and Emmanuel Prochasson. 2011.
Bilingual lexicon extraction from comparable cor-
pora enhanced with parallel corpora. In Proceed-
ings, 4th Workshop on Building and Using Compa-
rable Corpora (BUCC), page 27?34, Portland, Ore-
gon, USA.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Comput. Linguist., 29(1):19?51, March.
Emmanuel Prochasson, Emmanuel Morin, and Kyo
Kageura. 2009. Anchor points for bilingual lexi-
con extraction from small comparable corpora. In
Proceedings, 12th Conference on Machine Transla-
tion Summit (MT Summit XII), page 284?291, Ot-
tawa, Ontario, Canada.
Reinhard Rapp. 1995. Identifying word translations in
non-parallel texts. In Proceedings of the 33rd an-
nual meeting on Association for Computational Lin-
guistics, ACL ?95, pages 320?322. Association for
Computational Linguistics.
Reinhard Rapp. 1999. Automatic identification of
word translations from unrelated english and german
corpora. In Proceedings of the 37th annual meet-
ing of the Association for Computational Linguistics
on Computational Linguistics, ACL ?99, pages 519?
526. Association for Computational Linguistics.
Raphae?l Rubino and Georges Linare`s. 2011. A multi-
view approach for term translation spotting. In
Computational Linguistics and Intelligent Text Pro-
cessing, Lecture Notes in Computer Science, pages
29?40.
Lei Shi. 2009. Adaptive web mining of bilingual
lexicons for cross language information retrieval.
In Proceedings of the 18th ACM conference on In-
formation and knowledge management, CIKM ?09,
pages 1561?1564, New York, NY, USA. ACM.
Ellen M. Voorhees. 1999. The trec-8 question an-
swering track report. In In Proceedings of TREC-8,
pages 77?82.
Zhibiao Wu and Martha Palmer. 1994. Verbs seman-
tics and lexical selection. In Proceedings of the 32nd
annual meeting on Association for Computational
Linguistics, ACL ?94, pages 133?138. Association
for Computational Linguistics.
23
