Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 790?799, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
A Sequence Labelling Approach to Quote Attribution
Tim O?Keefe? Silvia Pareti James R. Curran? Irena Koprinska? Matthew Honnibal?
? e-lab, School of IT School of Informatics ?Centre for Language Technology
University of Sydney University of Edinburgh Macquarie University
NSW 2006, Australia United Kingdom NSW 2109, Australia
{tokeefe,james,irena}@it.usyd.edu.au S.Pareti@sms.ed.ac.uk matthew.honnibal@mq.edu.au
Abstract
Quote extraction and attribution is the task of
automatically extracting quotes from text and
attributing each quote to its correct speaker.
The present state-of-the-art system uses gold
standard information from previous decisions
in its features, which, when removed, results
in a large drop in performance. We treat the
problem as a sequence labelling task, which
allows us to incorporate sequence features
without using gold standard information. We
present results on two new corpora and an aug-
mented version of a third, achieving a new
state-of-the-art for systems using only realis-
tic features.
1 Introduction
News stories are often driven by the quotes made
by politicians, sports stars, musicians, and celebri-
ties. When these stories exit the news cycle, the
quotes they contain are often forgotten by both read-
ers and journalists. A system that automatically ex-
tracts quotes and attributes those quotes to the cor-
rect speaker would enable readers and journalists to
place news in the context of all comments made by
a person on a given topic.
Though quote attribution may appear to be a
straightforward task, the simple rule-based ap-
proaches proposed thus far have produced disap-
pointing results. Going beyond these to machine
learning approaches presents several problems that
make quote attribution surprisingly difficult. The
main challenge is that while a large portion of quotes
can be attributed to a speaker based on simple rules,
the remainder have few or no contextual clues as
to who the correct speaker is. Additionally, many
quote sequences, such as dialogues, rely on the
reader understanding that there is an alternating se-
quence of speakers, which creates dependencies be-
tween attribution decisions made by a classifier.
Elson and McKeown (2010) is the only study that
directly uses machine learning in quote attribution,
treating the task as a classification task, where each
quote is attributed independently of other quotes. To
handle conversations and similar constructs they use
gold standard information about speakers of previ-
ous quotes as features for their model. This is an
unrealistic assumption, since gold standard informa-
tion is not available in practice.
The primary contribution of this paper is that we
reformulate quote attribution as a sequence labelling
task. This allows us to use sequence features with-
out having to use the unrealistic gold standard fea-
tures that were used in Elson and McKeown (2010).
We experiment with three sequence decoding mod-
els including greedy, Viterbi and a linear chain Con-
ditional Random Field (CRF).
Furthermore we present results on two new cor-
pora and an augmented version of a third. The two
new corpora are from news articles from the Wall
Street Journal and the Sydney Morning Herald re-
spectively, while the third corpus is an extension to
the classic literature corpus from Elson and McK-
eown (2010). Our results show that a quote attri-
bution system using only realistic features is highly
feasible for the news domain, with accuracies of
92.4% on the SMH corpus and 84.1% on the WSJ
corpus.
790
2 Background
Early work into quote attribution by Zhang et al
(2003) focused on identifying when different char-
acters were talking in children?s stories, so that a
speech synthesis system could read the quoted parts
in different voices. While they were able to ex-
tract quotes with high precision and recall, their at-
tribution accuracy was highly dependent on the doc-
ument in question, ranging from 47.6% to 86.7%.
Mamede and Chaleira (2004) conducted similar re-
search on children?s stories written in Portuguese.
Their system proved to be very good at extracting
quotes through simple rules, but when using a hand-
crafted decision tree to attribute those quotes to a
speaker, they achieved an accuracy of only 65.7%.
In the news domain, both Pouliquen et al2007)
and Sarmento and Nunes (2009) proposed rule-
based systems that work over large volumes of text.
Both systems aimed for high precision at the ex-
pense of low recall, as their data contained many re-
dundant quotes. More recently, SAPIENS, a French-
language quote extraction and attribution system,
was developed by de La Clergerie et al2011). It
conducts a full parse of the text, which allows it to
use patterns to extract direct and indirect quotes, as
well as the speaker of each quote. Their evaluation
found that 19 out of 40 quotes (47.5%) had a correct
span and author, while a further 19 had an incorrect
author, and 4 had an incorrect span. In related work,
Sagot et al2010) built a lexicon of French reported
speech verbs, and conducted some analysis of dif-
ferent types of quotes.
Glass and Bangay (2007) approached the task
with a three stage method. For each quote they
first find the nearest speech verb, they then find the
grammatical actor of that speech verb, and finally
they select the appropriate speaker for that actor. To
achieve each of these subtasks they built a model
with several manually weighted features that good
candidates should possess. For each subtask they
then choose the candidate with the largest weighted
sum of features. Their full approach yields an ac-
curacy of 79.4% on a corpus of manually annotated
fiction books.
Schneider et al2010) describe PICTOR, which
is principally a quote visualisation tool. Their task
was to find direct and indirect quotes, which they
attribute to a text span representing the speaker.
To do this they constructed a specialised grammar,
which was built with reference to a small develop-
ment corpus. With a permissive evaluation metric
their grammar-based approach yielded 86% recall
and 75% precision, however this dropped to 52% re-
call and 56% precision when measured in terms of
completely correct quote-speaker pairs.
The work most similar to ours is the work by El-
son and McKeown (2010). Their aim was to au-
tomatically identify both quotes and speakers, and
then to attribute each quote to a speaker, in a corpus
of classic literature that they compiled themselves.
To identify potential speakers they used the Stanford
NER tagger (Finkel et al2005) and a method out-
lined in Davis et al2003) that allowed them to find
nominal character references. They then grouped
name variants and pronominal mentions into a coref-
erence chain.
To attribute a quote to a speaker they first classi-
fied the quotes into categories. Several of the cat-
egories have a speaker explicit in their structure,
so they attribute quotes to those speakers with no
further processing. For the remaining categories,
they cast the attribution problem as a binary clas-
sification task, where each quote-speaker pair has
a ?speaker? or ?not speaker? label predicted by the
classifier. They then reconciled these independent
decisions using various techniques to produce a sin-
gle speaker prediction for each quote. For the sim-
ple category predictions they achieved 93-99% ac-
curacy, while for the more complicated categories
they achieved 63-64%, with an overall result of 83%
accuracy. This compares favourably with their rule-
based baseline, which achieved an accuracy of 52%.
While the results of Elson and McKeown (2010)
appear encouraging, they are misleading for two rea-
sons. First their corpus does not include quotes
where all three annotators chose different speakers.
While these quotes include some cases where the
annotators chose coreferent spans, it also includes
cases of legitimate disagreement about the speaker.
An automated system would likely find these cases
challenging. Second both their category predictions
and machine learning predictions rely on gold stan-
dard information from previous quotes, which is not
available in practice. In our study we address both
these issues.
791
Proportion (%) Accuracy (%)
LIT WSJ SMH LIT WSJ SMH
Quote-Said-Person 17.9 20.2 3.1 98.9 99.8 99.1
Quote-Person-Said 2.8 6.1 16.6 97.7 97.0 98.5
Other Trigram 0.1 2.3 0.3 66.7 56.2 54.5
Quote-Said-Pronoun 1.9 0.1 0.0 38.6 100.0 0.0
Quote-Pronoun-Said 5.9 8.8 13.5 36.5 92.2 93.9
Other Anaphors 0.1 0.1 0.2 0.0 100.0 62.5
Added* 24.6 28.3 23.9 89.7 76.3 97.5
Backoff 11.0 33.9 32.3 - - -
Alone 18.0 0.2 9.7 - - -
Conversation* 17.7 0.2 0.3 85.2 0.0 8.3
Total 100.0 100.0 100.0 60.5 57.2 55.8
Table 1: The proportion of quotes in each category and the accuracy of the speaker prediction based on the category.
The two categories marked with an asterisk (*) depend on previous decisions.
3 Corpora
We evaluate our methods on two new corpora com-
ing from the news domain, and an augmented ver-
sion of an existing corpus, which covers classic lit-
erature. They are described below.
3.1 Columbia Quoted Speech Attribution
Corpus (LIT)
The first corpus we use was originally created by
Elson and McKeown (2010). It is a set of excerpts
from 11 fictional 19th century works by six well-
known authors, split into 18 documents. In total it
contains 3,126 quotes annotated with their speakers.
Elson and McKeown used an automated system
to find named entity spans and nominal mentions in
the text, with the named entities being linked to form
a coreference chain (they did not link nominal men-
tions). The corpus was built using Amazon?s Me-
chanical Turk, with three annotations per quote. To
ensure quality, all annotations from poorly perform-
ing annotators were removed, as were quotes where
each annotator chose a different speaker. Though
excluding some quotes ensures quality annotations,
it causes gaps in the quote chains, which is a prob-
lem for sequence labelling. Furthermore, the cases
where annotators disagreed are likely to be challeng-
ing, so removing them from the corpus could make
results appear better than they would be in practice.
To rectify this, we conducted additional annota-
tion of the quotes that were excluded by the origi-
nal authors. Two postgraduates annotated 654 addi-
tional quotes, with a raw agreement of 79% over 48
double-annotated quotes. Our annotators reported
seeing some errors in existing annotations, so we
had one annotator check 400 existing annotations for
correctness. This additional check found that 92.5%
of the quotes were correctly annotated.
3.2 PDTB Attribution Corpus Extension (WSJ)
Our next corpus is an extension to the attribution
annotations found in the Penn Discourse TreeBank
(PDTB). The original PDTB contains several forms
of discourse, including assertions, beliefs, facts, and
eventualities. These can be attributed to named enti-
ties or to unnamed, pronominal, or implicit sources.
Recent work by Pareti (2012) conducted further an-
notation of this corpus, including reconstructing at-
tributions that were only partially annotated, and in-
troducing additional information. From this corpus
we use only direct quotes and the directly quoted
portions of mixed quotes, giving us 4,923 quotes.
For the set of potential speakers we use the
BBN pronoun coreference and entity type cor-
pus (Weischedel and Brunstein, 2005), with auto-
matically coreferred pronouns. We automatically
matched BBN entities to PDTB extension speakers,
and included the PDTB speaker where no matching
BBN entity could be found. This means an automatic
system has an opportunity to find the correct speaker
for all quotes in the corpus.
792
3.3 Sydney Morning Herald Corpus (SMH)
We compiled the final corpus from a set of news
documents taken from the Sydney Morning Her-
ald website1. We randomly selected 965 documents
published in 2009 that were not obituaries, opin-
ion pages, advertisements or other non-news sto-
ries. To conduct the annotation we employed 11
non-expert annotators via the outsourcing site Free-
lancer2, as well as five expert annotators from our
research group. A total of 400 news stories were
double-annotated, with at least 33 double-annotated
stories per annotator. Raw agreement on the speaker
of each quote was high at 98.3%. These documents
had already been annotated with named entities as
part of a separate research project (Hachey et al
2012), which includes manually constructed coref-
erence chains. The resulting corpus contains 965
documents, with 3,535 quotes.
3.4 Corpus Comparisons
In order to compare the corpora we categorise the
quotes into the categories defined by Elson and
McKeown (2010), as shown in Table 1. We assigned
quotes to these categories by testing (after text pre-
processing) whether the quote belonged to each cat-
egory, in the order shown below:
1. Trigram ? the quote appears consecutively with
a mention of an entity, and a reported speech
verb, in any order;
2. Anaphors ? same as above, except that the men-
tion is a pronoun;
3. Added ? the quote is in the same paragraph as
another quote that precedes it;
4. Conversation ? the quote appears in a para-
graph on its own, and the two paragraphs pre-
ceding the current paragraph each contain a sin-
gle quote, with alternating speakers;
5. Alone ? the quote is in a paragraph on its own;
6. Miscellaneous ? the quote matches none of the
preceding categories. This category is called
?Backoff? in Elson and McKeown (2010).
1http://www.smh.com.au
2http://www.freelancer.com
Unsurprisingly, the two corpora from the news do-
main share similar proportions of quotes in each
category. The main differences are that the SMH
uses a larger number of pronouns compared to the
WSJ, which tends to use explicit attribution more fre-
quently. The SMH also has a significant proportion
of quotes that appear alone in a paragraph, while
the WSJ has almost none. Finally, when attribut-
ing a quote using a trigram pattern, the SMH mostly
uses the Quote-Person-Said pattern, while the WSJ
mostly uses the Quote-Said-Person pattern. These
differences probably reflect the editorial guidelines
of the two newspapers.
The differences between the news corpora and
the literature corpus are more substantial. Most no-
tably the LIT corpus has a much higher proportion
of quotes that fall into the Conversation and Alone
categories. This is unsurprising as both monologues
and dialogues are common in fiction, but are rare in
newswire. The two news corpora have more quotes
in the Trigram and Backoff categories.
4 Quote Extraction
Quote extraction is the task of finding the spans that
represent quotes within a document. There are three
types of quotes that can appear:
1. Direct quotes appear entirely between quota-
tion marks, and are used to indicate that the
speaker said precisely what is written;
2. Indirect quotes do not appear between or con-
tain quotation marks, and are used to get the
speaker?s point across without implying that
the speaker used the exact words of the quote;
3. Mixed quotes are indirect quotes that contain a
directly quoted portion.
In this work, we limit ourselves to detecting direct
quotes and the direct portions of mixed quotes.
To extract quotes we use a regular expression that
searches for text between quotation marks. We also
deal with the special case of multi-paragraph quotes
where one quotation mark opens the quote and every
new paragraph that forms part of the quote, with a fi-
nal quotation mark only at the very end of the quote.
This straightforward approach yields over 99% ac-
curacy on all three corpora.
793
5 Quote Attribution
Given a document with a set of quotes and a set
of entities, quote attribution is the task of finding
the entity that represents the speaker of each quote,
based on the context provided by the document.
Identifying the correct entity can involve choosing
either an entire coreference chain representing an
entity, or identifying a specific span of text that rep-
resents the entity.
In practice, most applications only need to know
which coreference chain represents the speaker, not
which particular span in the text. Despite this, the
best evidence about which chain is the speaker is
found in the context of the individual text spans, and
most existing systems aim to get the particular entity
span correct. This presents a problem for evaluation,
as an incorrect entity span may be identified, but it
might still be part of the correct coreference chain.
We chose to count attributions as correct if they at-
tributed the quote to the correct coreference chain
for both the LIT and SMH corpora, while for the WSJ
corpus, where the full coreference chains do not ex-
ist, we evaluated an attribution as correct if it was to
the correct entity span in the text.
5.1 Rule-based Baseline
To establish the effectiveness of our method we built
a rule-based baseline system. For each quote it pro-
ceeds with the following steps:
1. Search backwards in the text from the end of
the sentence the quote appears in for a reported
speech verb
2. If the verb is found return the entity mention
nearest the verb (ignoring mentions in quotes),
in the current sentence or any sentence preced-
ing it
3. If not, return the mention of an entity near-
est the end of the quote (ignoring mentions in
quotes), in the current sentence or any sentence
preceding it
This forms a reasonable baseline as it is able to pick
up the quotes that fall into the more simple cate-
gories, such as the Trigram category and the Added
category. It is also able to make a guess at the more
complicated categories, without using gold standard
information as the category predictions do.
6 Experimental Setup
We use two classifiers: a logistic regression imple-
mentation available in LIBLINEAR (Fan et al2008),
and a Conditional Random Field (CRF) from CRF-
Suite (Okazaki, 2007). Both packages use maxi-
mum likelihood estimation with L2 regularisation.
We experimented with several values for the coef-
ficient on a development set, but found that it had
little impact, so stuck with the default value. All of
our machine learning experiments use the same text
encoding, which is explained below, and all use the
category predictions when they are available.
6.1 Text Encoding
We encode our text similarly to Elson and McKeown
(2010). The major steps are:
1. Replace all quotes and speakers with special
symbols;
2. Replace all reported speech verbs with a sym-
bol. Elson and McKeown (2010) provided us
with their list of reported speech verbs;
3. Part-of-Speech (POS) tag the text and remove
adjectives, adverbs, and other parts of speech
that do not contribute useful information. We
used the POS tagger from Curran and Clark
(2003);
4. Remove any paragraphs or sentences where no
quotes, pronouns or names occur.
All features that will be discussed are calculated
with respect to this encoding (e.g. word distance
would be the number of words in the encoded text,
rather than the number of words in the original text).
6.2 Features
In our experiments we use the feature set from Elson
and McKeown (2010). The features for a particu-
lar pair of target quote (q) and target speaker (s) are
summarised below.
Distance features including number of words be-
tween q and s, number of paragraphs between
q and s, number of quotes between q and s, and
number of entity mentions between q and s
794
Corpus
Sequence Features
Gold Pred None
LIT 74.7 49.0 49.6
WSJ 87.3 74.1 82.9
SMH 95.0 85.6 92.4
Table 2: Accuracy results comparing the E&M approach
with gold standard, predicted or no sequence features.
Paragraph features derived from the 10 para-
graphs preceding the quote (including the para-
graph the quote is in), includes number of men-
tions of s, number of mentions of other speak-
ers, number of words in each paragraph, and
number of quotes in each paragraph
Nearby features relating to the two tokens either
side of q and s, includes binary features for
each position indicating whether the position is
punctuation, s, q, a different speaker, a differ-
ent quote, or a reported speech verb
Quote features about q itself, including whether s
is mentioned within it, whether other speakers
are mentioned within it, how far the quote is
from the start of its paragraph and the length in
words of q
Sequence features that depend on the speakers
chosen for the previous quotes, includes num-
ber of quotes in the 10 paragraphs preceding
and including the paragraph where q appears
that were attributed to s, and the number that
were attributed to other speakers
6.3 Elson and McKeown Reimplementation
As part of our study we reproduce the core results
of Elson and McKeown (2010) (E&M ), as we be-
lieve it is a state-of-the-art system. This allows us
to determine the effectiveness of our approach when
compared to a state-of-the-art approach, and it also
allows us to determine how well the E&M approach
performs on other corpora. In this section we will
briefly summarise the key elements needed to repro-
duce their work.
The E&M approach makes a binary classification
between ?speaker? and ?not speaker? for up to 15
candidate speakers for each quote. They then recon-
cile these 15 classifications into one speaker predic-
tion for the quote. While E&M experimented with
several different reconciliation methods, we simply
chose the speaker with the highest probability at-
tached to its ?speaker? label.
We conducted an experiment using our imple-
mentation of the E&M method on the original,
unaugmented E&M corpus, to see how our result
compared with E&M ?s 83%. On our test set we
achieved 78.2%, however this rose to 82.3% when
performing 10-fold cross validation across the whole
corpus. Though this is a large difference, it is not
necessarily that surprising, as our test set contains
documents by authors which are unseen, whereas
both the original E&M test set and all the cross val-
idation test sets contain documents by authors that
the learner has seen before.
In their work, E&M make a simplifying assump-
tion that all previous attribution decisions were cor-
rect. Due to this, their sequence features use gold
standard labels from previous quotes, which makes
their results unrealistic. In Table 2 we show the ef-
fect of replacing the gold standard sequence features
with features based on the predicted labels, or with
no sequence features at all. All three corpora show a
significant drop in accuracy, with the LIT corpus in
particular suffering a drop of more than 25%. This
motivates our study into including sequence infor-
mation without using gold standard labels.
7 Class Models
We consider two class models for our experiments,
which are described in detail below. The binary
model is able to take advantage of more data but has
less competition between decisions, while the n-way
model has more competition with less data. Both
models are used with all the decoding methods, with
the exception that the binary model is unsuitable for
the CRF experiments.
7.1 Binary
When working with n previous speakers, a binary
class model works by predicting n independent
?speaker? versus ?not speaker? labels, one for each
quote-speaker pair. As the classifications are inde-
pendent the n decisions need to be reconciled, as
more than one speaker might be predicted. We rec-
oncile the n decisions by attributing the quote to the
795
speaker with the highest ?speaker? probability. Us-
ing a binary class with reconciliation in a greedy
decoding model is equivalent to the method in El-
son and McKeown (2010), except that the gold stan-
dard sequence features are replaced with predicted
sequence features.
7.2 n-way
A key advantage of the binary class model is that
when predicting ?speaker? versus ?not speaker? the
classifier only needs to predict one probability, and
thus can take into account the evidence of all other
quote-speaker pairs. The drawback to the binary
model is that the probabilities assigned to the can-
didate speakers do not need to directly compete
against each other. In other words when assigning
a binary probability to a candidate speaker, the clas-
sifier does not take into account how good the other
candidate speakers are.
To rectify these issues we experiment with a sin-
gle classification for each quote, where the classifier
directly decides between up to n candidate speakers
per quote. As speaker-specific evidence is far too
sparse, we encode the speakers with their ordinal po-
sition backwards from the quote. In other words, the
candidate speaker immediately preceding the quote
would be labelled ?speaker1?, the speaker preced-
ing it would be ?speaker2? and so on. The classifier
then directly predicts these labels. This representa-
tion means that candidate speakers need to directly
compete for probability mass, although it has the
drawback that the evidence for the higher-numbered
speakers is quite sparse.
The features we use for this representation are
similar to the features used in the E&M binary
model. The key difference is that where there were
individual features that were calculated with respect
to the speaker, there are now n features, one for each
of the speaker candidates. This allows the model to
account for the strength of other candidates when as-
signing a speaker label.
8 Sequence Decoding
We noted in the previous section that the E&M re-
sults are based on the unrealistic assumption that
all previous quotes were attributed correctly. In
this section we outline three sequence decoding ap-
proaches that remove this unrealistic assumption,
without removing all of the transition information
that it provides. We believe the transition infor-
mation is important as many quotes have no ex-
plicit attribution in the text, and instead rely on the
reader understanding something about the sequence
of speakers.
For these experiments we regard the set of speaker
attributions in a document as the sequence that we
want to decode. Each individual state therefore rep-
resents a sequence of w previous attribution deci-
sions, and a decision for the current quote. Obtain-
ing a probability for this state can be done in one
of two ways. Either the transition probabilities from
state to state can be learned explicitly, or the w pre-
vious attribution decisions can be used to build the
sequence features for the current state, which im-
plicitly encodes the transition probabilities.
8.1 Greedy Decoding
In sequence decoding the greedy algorithm calcu-
lates the probability of each label at a decision point
based on the predictions it has already made for pre-
vious decisions. More concretely this means we ap-
ply a standard classifier at each step, with the se-
quence features being calculated from the predic-
tions made in previous steps. Greedy decoding is
efficient in that it only considers one possible history
at each decision point, but it is consequently unable
to make trade-offs between good previous choices
and good current choices, which means that in gen-
eral it will not return the optimum sequence of la-
bels. As greedy decoding is an efficient algorithm
we do not restrict w, the number of previous deci-
sions, beyond the 10 paragraph restriction that is al-
ready in place.
8.2 Viterbi Decoding
Viterbi decoding finds the most probable path
through a sequence of decisions. It does this by de-
termining the probabilities of each of the labels at
the current decision point, with each of the possi-
ble histories of decisions within a given window w.
These probabilities can be multiplied together with
the previous decisions to retrieve a joint probability
for the entire sequence. The final decision for each
quote is then just the speaker which is predicted by
the sequence with the largest joint probability.
796
Although they do not come with probabilities,
we chose to include the category predictions in our
Viterbi model. As we already know that they are
accurate indicators of the speaker we assign them
a probability of 100%, which effectively forces the
Viterbi decoder to choose the category predictions
when they are available. It is worth noting that
quotes are only assigned to the Conversation cate-
gory if the two prior quotes had alternating speakers.
As such, during the Viterbi decoding the categori-
sation of the quote actually needs to be recalculated
with regard to the two previous attribution decisions.
By forcing the Viterbi decoder to choose category
predictions when they are available, we get the ad-
vantage that quote sequences with no intervening
text may be forced into the Conversation category,
which is typically under-represented otherwise.
Both the sequences using the binary class and
the n-way class can be decoded using the Viterbi
algorithm, so we experiment with both class mod-
els. We also experiment with varying window sizes
(w), in order to gain insight into how many previous
decisions impact the current decision. Though the
Viterbi algorithm is able to find the best sequence
of probabilities without the need for an exhaustive
search, it can still take an impractical amount of time
to run. As such we ignore all but the 10 most promis-
ing sequences at each decision point.
8.3 Conditional Random Field (CRF) Decoding
The key drawback with the logistic regression ex-
periments described thus far is that the sequence
features are trained with gold standard information.
This means that during the training phase the se-
quence features have perfect information about pre-
vious speakers and are thus unrealistically good pre-
dictors of the final outcome. When the resulting
model is used with the less accurate predicted se-
quence features, it is overconfident about the infor-
mation those features provide.
We account for this by using a first-order linear
chain CRF model, which learns the probabilities of
progressing from speaker to speaker more directly.
During training the CRF is able to learn the asso-
ciation between features and labels, as well as the
chance of transitioning from one label to the next.
It also has the advantage of avoiding the label bias
problem that would be present in the equivalent Hid-
den Markov Model (Lafferty et al2001).
Though the n-way class model can be used di-
rectly in a CRF, the binary class model is more chal-
lenging. The main problem is that the ?speaker?
versus ?not speaker? output of the binary classifier
does not directly form a meaningful sequence that
the CRF can learn over. If the reconciliation step is
included it effectively adds an extra layer to the lin-
ear chain, making learning more difficult. Due to
these difficulties we only use the n-way class model
in our CRF experiments.
9 Results
The main result of our experiments with the E&M
method is the large drop in accuracy that occurs
when the gold standard sequence features are re-
moved, which can be seen in Table 3. When using
the binary class model this results in a drop of 25.1%
for the LIT corpus, while for the WSJ and SMH cor-
pora the drop is less substantial at 4.4% and 2.6%,
respectively. For the LIT corpus the drop is so severe
that it actually performs worse than the simple rule-
based system. Even more surprisingly, when the
predictions from previous decisions are used with a
simple greedy decoder, the accuracy drops even fur-
ther for all three corpora. This indicates that the clas-
sifier is putting too much weight on the gold stan-
dard sequence features during training, and is mis-
led into making poor decisions when the predicted
features are used during test time.
Table 4 shows the results for the n-way class
model. Compared to the binary model, the n-way
class model generally produced lower results, al-
though the results were more stable to changes in
parameters and decoders. The only corpus that pro-
duced better results with the n-way class model was
the WSJ corpus, which does not have full entity
coreference information. This indicates that the n-
way model may be helpful when there is more vari-
ety in the choice of entities.
The final results we would like to discuss here are
the CRF results. On all three corpora the CRF results
are underwhelming. The major issue that we can
see when applying a CRF model to this task is that
the sequences that it needs to learn over are entire
documents. This means that for the LIT corpus the
training set consisted of only 12 sequences, while
797
Corpus E&M Rule No seq. Greedy Viterbi
w = 1 w = 2 w = 5
LIT 74.7 53.3 49.6 49.0 46.0 49.8 45.9
WSJ 87.3 77.9 82.9 74.1 82.3 83.1 83.1
SMH 95.0 91.2 92.4 85.6 91.7 90.5 84.1
Table 3: Accuracy on test set with the binary class model. Italicised results indicate gold standard information is used.
Bold results show the best realistic result for each corpus.
Corpus Gold seq. Rule No seq. Greedy Viterbi CRF
w = 1 w = 2 w = 5
LIT 68.6 53.3 47.1 46.7 42.5 46.5 44.4 48.6
WSJ 88.9 77.9 83.6 77.0 84.1 83.7 83.3 79.6
SMH 94.4 91.2 90.0 89.6 89.5 90.1 90.4 91.0
Table 4: Accuracy on test set with the n-way class model. Italicised results indicate gold standard information is used.
Bold results show the best realistic result for each corpus.
the test set consisted of 6 sequences. With so few
sequences it is unsurprising that the CRF model did
not perform well. The limited range of the first order
linear chain model could also have played a part in
the poor performance of the CRF models. However,
moving to a higher-order model is problematic as
the number of transition probabilities that need to be
calculated increases exponentially with the order of
the model.
10 Conclusion
In this paper, we present the first large-scale evalua-
tion of a quote attribution system on newswire from
the 1989 Wall Street Journal (WSJ) and the 2009
Sydney Morning Herald (SMH), as well as compar-
ing against previous work (Elson and McKeown,
2010) on 19th-century literature.
We show that when Elson and McKeown?s unre-
alistic use of gold-standard history information is re-
moved, accuracy on all three corpora drops substan-
tially. We demonstrate that by treating quote attribu-
tion as a sequence labelling task, we can achieve re-
sults that are very close to their results on newswire,
though not for literature.
In future work, we intend to further explore the
sequence features that have a large impact on accu-
racy, and to find similar features or proxies for the
sequence features that would be beneficial. We will
also explore other approaches to representing quote
attribution with a CRF. For the task more broadly,
it would be beneficial to compare methods of find-
ing indirect and mixed quotes, and to evaluate how
well quote attribution performs on those quotes as
opposed to just direct quotes.
Our newswire results, 92.4% for the SMH and
84.1% for the WSJ corpus, demonstrate it is possible
to develop an accurate and practical quote extraction
system. On the LIT corpus our best result was from
the simple rule-based system, which yielded 53.3%.
It is clear that literature poses an ongoing research
challenge.
Acknowledgements
We would like to thank David Elson for helping
us to reimplement his method and Bonnie Webber
for her feedback and assistance. O?Keefe has been
supported by a University of Sydney Merit schol-
arship and a Capital Markets CRC top-up scholar-
ship; Pareti has been supported by a Scottish Infor-
matics and Computer Science Alliance (SICSA) stu-
dentship. This work has been supported by ARC
Discovery grant DP1097291 and the Capital Mar-
kets CRC Computable News project.
References
James R. Curran and Stephen Clark. 2003. Investi-
gating GIS and smoothing for maximum entropy
taggers. In Proceedings of the tenth conference on
798
European chapter of the Association for Compu-
tational Linguistics, pages 91?98.
Peter T. Davis, David K. Elson, and Judith L. Kla-
vans. 2003. Methods for precise named entity
matching in digital collections. In Proceedings of
the 3rd ACM/IEEE-CS Joint Conference on Digi-
tal libraries, pages 125?127.
Eric de La Clergerie, Benoit Sagot, Rosa Stern, Pas-
cal Denis, Gaelle Recource, and Victor Mignot.
2011. Extracting and visualizing quotations from
news wires. Human Language Technology. Chal-
lenges for Computer Science and Linguistics,
pages 522?532.
David. K Elson and Kathleen. R McKeown. 2010.
Automatic attribution of quoted speech in literary
narrative. In Proceedings of AAAI, pages 1013?
1019.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh,
Xiang-Rui Wang, and Chih-Jen Lin. 2008. LIB-
LINEAR: A library for large linear classification.
Journal of Machine Learning Research, 9:1871?
1874.
Jenny Rose Finkel, Trond Grenager, and Christo-
pher Manning. 2005. Incorporating non-local in-
formation into information extraction systems by
gibbs sampling. In Proceedings of the 43rd An-
nual Meeting on Association for Computational
Linguistics, pages 363?370.
Kevin Glass and Shaun Bangay. 2007. A naive
salience-based method for speaker identification
in fiction books. In Proceedings of the 18th An-
nual Symposium of the Pattern Recognition Asso-
ciation of South Africa (PRASA07), pages 1?6.
Ben Hachey, Will Radford, Joel Nothman, Matthew
Honnibal, and James R. Curran. 2012. Evaluating
entity linking with Wikipedia. Artificial Intelli-
gence. (in press).
John Lafferty, Andrew McCallum, and Fernando
C.N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling
sequence data. International Conference on Ma-
chine Learning, pages 282?289.
Nuno Mamede and Pedro Chaleira. 2004. Charac-
ter identification in children stories. Advances in
Natural Language Processing, pages 82?90.
Naoaki Okazaki. 2007. CRFsuite: a fast im-
plementation of Conditional Random Fields
(CRFs). URL http://www.chokkan.org/
software/crfsuite/.
Silvia Pareti. 2012. A database of attribution rela-
tions. In Proceedings of the Eight International
Conference on Language Resources and Evalua-
tion (LREC?12), pages 3213?3217.
Bruno Pouliquen, Ralf Steinberger, and Clive Best.
2007. Automatic detection of quotations in multi-
lingual news. In Proceedings of Recent Advances
in Natural Language Processing, pages 487?492.
Beno??t Sagot, Laurence Danlos, and Rosa Stern.
2010. A lexicon of french quotation verbs for au-
tomatic quotation extraction. In 7th international
conference on Language Resources and Evalua-
tion - LREC 2010.
Luis Sarmento and Sergio Nunes. 2009. Automatic
extraction of quotes and topics from news feeds.
In 4th Doctoral Symposium on Informatics Engi-
neering.
Nathan Schneider, Rebecca Hwa, Philip Gianfor-
toni, Dipanjan Das, Michael Heilman, Alan W.
Black, Frederik L. Crabbe, and Noah A. Smith.
2010. Visualizing topical quotations over time
to understand news discourse. Technical Report
CMU-LTI-01-013, Carnegie Mellon University.
Ralph Weischedel and Ada Brunstein. 2005. BBN
pronoun coreference and entity type corpus. Lin-
guistic Data Consortium, Philadelphia.
Jason Zhang, Alan Black, and Richard Sproat.
2003. Identifying speakers in children?s stories
for speech synthesis. In Proceedings of EU-
ROSPEECH, pages 2041?2044.
799
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 989?999,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Automatically Detecting and Attributing Indirect Quotations
Silvia Pareti? Tim O?Keefe?? Ioannis Konstas James R. Curran? Irena Koprinska?
ILCC, School of Informatics ? e-lab, School of IT
University of Edinburgh University of Sydney
United Kingdom NSW 2006, Australia
{s.pareti,i.konstas}@sms.ed.ac.uk {tokeefe,james,irena}@it.usyd.edu.au
Abstract
Direct quotations are used for opinion min-
ing and information extraction as they have an
easy to extract span and they can be attributed
to a speaker with high accuracy. However,
simply focusing on direct quotations ignores
around half of all reported speech, which is
in the form of indirect or mixed speech. This
work presents the first large-scale experiments
in indirect and mixed quotation extraction and
attribution. We propose two methods of ex-
tracting all quote types from news articles and
evaluate them on two large annotated corpora,
one of which is a contribution of this work.
We further show that direct quotation attribu-
tion methods can be successfully applied to in-
direct and mixed quotation attribution.
1 Introduction
Quotations are crucial carriers of information, par-
ticularly in news texts, with up to 90% of sentences
in some articles being reported speech (Bergler
et al, 2004). Reported speech is a carrier of evi-
dence and factuality (Bergler, 1992; Saur?? and Puste-
jovsky, 2009), and as such, text mining applications
use quotations to summarise, organise and validate
information. Extraction of quotations is also rele-
vant to researchers interested in media monitoring.
Most quotation attribution studies (Pouliquen
et al, 2007; Glass and Bangay, 2007; Elson and
McKeown, 2010) thus far have limited their scope
to direct quotations (Ex.1a), as they are delimited
?*These authors contributed equally to this work.
by quotation marks, which makes them easy to ex-
tract. However, annotated resources suggest that di-
rect quotations represent only a limited portion of all
quotations, i.e., around 30% in the Penn Attribution
Relation Corpus (PARC), which covers Wall Street
Journal articles, and 52% in the Sydney Morning
Herald Corpus (SMHC), with the remainder being in-
direct (Ex.1c) or mixed (Ex.1b) quotations. Retriev-
ing only direct quotations can miss key content that
can change the interpretation of the quotation (Ex.
1b) and will entirely miss indirect quotations.
(1) a. ?For 10 million, you can move $100 mil-
lion of stocks,? a specialist on the Big Board
gripes. ?That gives futures traders a lot
more power.?
b. Police would only apply for the restrictions
when ?we have a lot of evidence that late-
night noise. . . is disturbing the residents of
that neighbourhood?, Superintendent Tony
Cooke said.
c. Mr Walsh said Rio was continuing to hold
discussions with its customers to arrive at a
mutually agreed price.
Previous work on extracting indirect and mixed
quotations has suffered from a lack of large-scale
data, and has instead used hand-crafted lexica of re-
porting verbs with rule-based approaches. The lack
of data has also made comparing the relative merit
of these approaches difficult, as existing evaluations
are small-scale and do not compare multiple meth-
ods on the same data.
In this work we address this lack of clear, com-
parable results by evaluating two baseline meth-
989
Method Language Test Size Results
(quotations) P R
Krestel et al (2008) hand-built grammar English 133 74% 99%
Sarmento and Nunes (2009) patterns over text Portuguese 570 88% 5%1
Fernandes et al (2011) ML and regex Portuguese 205 64%2 67%2
de La Clergerie et al (2011) patterns over parse French 40 87% 70%
Schneider et al (2010) hand-built grammar English N/D 56%2 52%2
Table 1: Related work on direct, indirect and mixed quotation extraction. Note that they are not directly comparable
as they apply to different languages and greatly differ in evaluation style and size of test set. 1 Figure estimated by the
authors for extracting 570 quotations from 26k articles. 2 Results are for quotation extraction and attribution jointly.
ods against both a token-based approach that uses a
Conditional Random Field (CRF) to predict IOB la-
bels, and a maximum entropy classifier that predicts
whether parse nodes are quotations or not. We eval-
uate these approaches on two large-scale corpora
from the news domain that together include over
18,000 quotations. One of these corpora (SMHC) is a
a contribution of this work, while our results are the
first presented on the other corpus (PARC). Instead
of relying on a lexicon of reporting verbs, we de-
velop a classifier to detect verbs introducing a quo-
tation. To inform future research we present results
for direct, indirect, and mixed quotations, as well as
overall results.
Finally, we use the direct quotation attribution
methods described in O?Keefe et al (2012) and
show that they can be successfully applied to indi-
rect and mixed quotations, albeit with lower accu-
racy. This leads us to conclude that attributing indi-
rect and mixed quotations to speakers is harder than
attributing direct quotations.
With this work, we set a new state of the art in
quotation extraction. We expect that the main con-
tribution of this work will be that future methods can
be evaluated in a comparable way, so that the relative
merit of various approaches can be determined.
2 Background
Pareti (2012) defines an attribution as having a
source span, a cue span, and a content span:
Source is the span of text that indicates who the
content is attributed to, e.g. ?president Obama?,
?analysts?, ?China?, ?she?.
Cue is the lexical anchor of the attribution relation,
usually a verb, e.g. ?say?, ?add?, ?quip?.
Content is the span of text that is attributed.
Based on the type of attitude the source expresses
towards a proposition or eventuality, attributions are
subcategorised (Prasad et al, 2006) into assertions
(Ex.2a) and beliefs (Ex.2b), which imply different
degrees of commitment, facts (Ex.2c), expressing
evaluation or knowledge, and eventualities (Ex.2d),
expressing intention or attitude.
(2) a. Mr Abbott said that he will win the election.
b. Mr Abbott thinks he will win the election.
c. Mr Abbott knew that Gillard was in Sydney.
d. Mr Abbott agreed to the public sector cuts.
Only assertion attributions necessarily imply a
speech act. Their content corresponds to a quotation
span and their source is generally referred to in the
literature as the speaker. Direct, indirect and mixed
quotations differ in the degree of factuality they en-
tail, since the former are by convention interpreted
as a verbatim transcription of an utterance whereas
indirect and the non-quoted portion of mixed quota-
tions can be paraphrased forms of the original word-
ing, and are thus filtered by the writer?s perspective.
The first speaker attribution systems (Zhang et al,
2003; Mamede and Chaleira, 2004; Glass and Ban-
gay, 2007) originate from the narrative domain and
were concerned with the identification of different
characters for speech synthesis applications. Direct
quotation attribution, with direct quotations being
given or extracted heuristically, has been the focus
of further studies in both the narrative (Elson and
McKeown, 2010) and news (Pouliquen et al, 2007;
Liang et al, 2010) domains. The few studies that
990
have addressed the extraction and attribution of in-
direct and mixed quotations are discussed below.
Krestel et al (2008) developed a quotation ex-
traction and attribution system that combines a lexi-
con of 53 common reporting verbs and a hand-built
grammar to detect constructions that match 6 gen-
eral lexical patterns. They evaluate their work on 7
articles from the Wall Street Journal, which contain
133 quotations, achieving macro-averaged Precision
(P ) of 99% and Recall (R) of 74% for quotation
span detection. PICTOR (Schneider et al, 2010) re-
lies instead on a context-free grammar for the extrac-
tion and attribution of quotations. PICTOR yielded
75% P and 86% R in terms of words correctly as-
cribed to a quotation or speaker, while it achieved
56% P and 52% R when measured in terms of com-
pletely correct quotation-speaker pairs.
SAPIENS (de La Clergerie et al, 2011) extracts
quotations from French news, by using a lexicon
of reporting verbs and syntactic patterns to extract
the complement of a reporting verb as the quota-
tion span and its subject as the source. They eval-
uated 40 randomly sampled quotations and found
that their system made 32 predictions and correctly
identified the span in 28 of the 40 cases. Verba-
tim (Sarmento and Nunes, 2009) extracts quotations
from Portuguese news feeds by first finding one of
35 speech verbs and then matching the sentence to
one of 19 patterns. Their manual evaluation shows
that 11.9% of the quotations Verbatim finds are er-
rors and that the system identifies approximately one
distinct quotation for every 46 news articles.
The system presented by Fernandes et al (2011)
also works over Portuguese news. Their work is the
closest to ours as they partially apply supervised ma-
chine learning to quotation extraction. Their work
introduces GloboQuotes, a corpus of 685 news items
containing 1,007 quotations of which 802 were used
to train an Entropy Guided Transformation Learn-
ing (ETL) algorithm (dos Santos and Milidiu?, 2009).
They treat quotation extraction as an IOB labelling
task, where they use ETL with POS and NE features
to identify the beginning of a quotation, while the
inside and outside labels are found using regular ex-
pressions. Finally they use ETL to attribute quota-
tions to their source. The overall system achieves
64% P and 67% R.
We have summarised these approaches in Table 1,
SMHC PARC
Corpus Doc Corpus Doc
Docs 965 - 2,280 -
Tokens 601k 623.3 1,139k 499.9
Quotations 7,991 8.3 10,526 4.6
Direct 4,204 4.4 3,262 1.4
Indirect 2,930 3.0 5,715 2.5
Mixed 857 0.9 1,549 0.6
Table 2: Comparison of the SMHC and PARC corpora, re-
porting their document and token size and per-type occur-
rence of quotations overall and per document (average).
which shows that the majority of evaluations thus far
have been small-scale. Furthermore, the published
results do not include any comparisons with previ-
ous work, which prevents a quantitative comparison
of the approaches, and they do not include results
broken down by whether the quotation is direct, in-
direct, or mixed. It is these issues that motivate our
work.
3 Corpora
We perform our experiments over two large corpora
from the news domain.
3.1 Penn Attribution Relations Corpus (PARC)
Our first corpus (Pareti, 2012), which we will re-
fer to as PARC, is a semi-automatically built ex-
tension to the attribution annotations included in
the PDTB (Prasad et al, 2008). The corpus covers
2,280 Wall Street Journal articles and contains an-
notations of assertions, beliefs, facts, and eventual-
ities, which are altogether referred to as attribution
relations (ARs). For this work we use only the asser-
tions, as they correspond to quotations (direct, indi-
rect and mixed). The drawback of this corpus is that
it is not yet fully annotated, i.e., it comprises positive
and unlabelled data.
The corpus includes a test set of 14 articles that
are fully annotated, which enables us to properly
evaluate our work and estimate that a proportion of
30-50% of ARs are unlabelled in the rest of the cor-
pus. The test set was manually annotated by two ex-
pert annotators. The annotators identified 491 ARs,
of which 22% were nested within another AR, with
991
an agreement score of 87%1. The agreement for the
selection of the content and source spans of com-
monly annotated ARs was 95% and 94% respec-
tively. In this work we address only non-embedded
assertions, so the final test-set includes 267 quotes,
totalling 321 non-discontinuous gold spans.
3.2 Sydney Morning Herald Corpus (SMHC)
We based our second corpus on the existing anno-
tations of direct quotations within Sydney Morning
Herald articles presented in O?Keefe et al (2012).
In that work we defined direct quotations as any
text between quotation marks, which included the
directly-quoted portion of mixed quotations, as well
as scare quotes. Under that definition direct quo-
tations could be automatically extracted with very
high accuracy, so annotations in that work were
over the automatically extracted direct quotations.
As part of this work one annotator removed scare
quotes, updated mixed quotations to include both
the directly and indirectly quoted portions, and
added whole new indirect quotations. The anno-
tation scheme was developed to be comparable to
the scheme used in the PARC corpus (Pareti, 2012),
although the SMHC corpus only includes assertions
and does not annotate the lexical cue.
The resulting corpus contains 7,991 quotations
taken from 965 articles from the 2009 Sydney Morn-
ing Herald (we refer to this corpus as SMHC). The
annotations in this corpus also include the speakers
of the quotations, as well as gold standard Named
Entities (NEs). We use 60% of this corpus as train-
ing data (4,872 quotations), 10% as development
data (759 quotations), and 30% as test data (2,360
quotations). Early experiments were conducted over
the development data, while the final results were
trained on both the training and development sets
and were tested on the unseen test data.
3.3 Comparison
Table 2 shows a comparison of the two corpora and
the quotations annotated within them. SMHC has a
higher density of quotations per document, 8.3 vs.
4.6 in PARC, since articles are fully annotated and
1The agreement was calculated using the agr metric de-
scribed in Wiebe and Riloff (2005) as the proportion of com-
monly annotated ARs with respect to the ARs identified overall
by Annotator A and Annotator B respectively
P R F
Bsay 94.4 43.5 59.5
Blist 75.4 71.1 73.2
k-NN 88.9 72.6 79.9
Table 3: Results for the k-NN verb-cue classifier. Bsay
classifies as verb-cue all instances of say while Blist
marks as verb-cues all verbs from a pre-compiled list in
Krestel et al (2008).
were selected to contain at least one quotation. PARC
is instead only partially annotated and comprises ar-
ticles with no quotations. Excluding null-quotation
articles from PARC, the average incidence of anno-
tated quotations per article raises to 7.1. The corpora
also differ in quotation type distribution, with di-
rect quotations being largely predominant in SMHC
while indirect are more common in PARC.
4 Experimental Setup
4.1 Quotation Extraction
Quotation extraction is the task of extracting the
content span of all of the direct, indirect, and mixed
quotations within a given document. More pre-
cisely, we consider quotations to be acts of com-
munication, which correspond to assertions in Pareti
(2012). Some quotations have content spans that are
split into separate, non-adjacent spans, as in exam-
ple (1a). Ideally the latter span should be marked as
a continuation of a quotation, however we consider
this to be out of scope for this work, so we treat each
span as a separate quotation.
4.2 Preprocessing
As a pre-processing step, both corpora were to-
kenised and POS tagged, and the potential speak-
ers anonymised to prevent over-fitting. We used the
Stanford factored parser (Klein and Manning, 2002)
to retrieve both the Stanford dependencies and the
phrase structure parse. Quotation marks were nor-
malised to a single character, as the quotation di-
rection is often incorrect for multi-paragraph quo-
tations.
4.3 Verb-cue Classifier
Verbs are by far the most common introducer of a
quotation. In PARC verbs account for 96% of all
992
cues, the prepositional phrase according to for 3%,
with the remaining 1% being nouns, adverbials and
prepositional groups. Attributional verbs are not a
closed set, they can vary across styles and genres,
and their attributional use is highly dependent on the
context in which they occur. It is therefore not possi-
ble to simply rely on a pre-compiled list of common
speech verbs. Quotations in PARC are introduced by
232 verb types, 87 of which are unique occurrences.
Not all of the verbs are speech verbs, for example
add, which is the second most frequent after say, or
the manner verb gripe (Ex.1a).
We used the attributional cues in the PARC cor-
pus to develop a separate component of our system
to identify attribution verb-cues. The classifier pre-
dicts whether the head of each verb group is a verb-
cue using the k-nearest neighbour (k-NN) algorithm,
with k equal to 3. The classifier uses 20 feature
types, including:
? Lexical (e.g. token, lemma, adjacent tokens)
? VerbNet classes membership
? Syntactic (e.g. node-depth in the sentence, par-
ent and sibling nodes)
? Sentence features (e.g. distance from sentence
start/end, within quotation markers).
We compared the system to one baseline, Bsay,
that marks every instance of say as a verb-cue, and
another, Blist, that marks every instance of a verb
that is on the list of 53 verbs presented in Krestel
et al (2008). We tested the system on the test set for
PARC, which contains 1809 potential verb-cues, of
which 354 are positive and 1455 are negative.
The results in Table 3 show that the verb-cue
classifier can outperform expert-derived knowledge.
The classifier was able to identify verb-cues with P
of 88.9% and R of 72.6%. While frequently oc-
curring verbs are highly predictive, the inclusion of
VerbNet classes (Schuler, 2005) and contextual fea-
tures allows for a more accurate classification of pol-
ysemous and unseen verbs.
Since PARC contains labelled and unlabelled attri-
butions, which is detrimental for training, we used
the verb-cue classifier to identify in the corpus sen-
tences that we suspected contained an unlabelled at-
tribution. Sentences containing a verb classified as a
cue that do not contain a quotation were removed
from the training set for the quotation extraction
model.
4.4 Evaluation
We use two metrics, listed below, for evaluating the
quotation spans predicted by our model against the
gold spans from the annotation.
Strict The first is a strict metric where a predicted
span is only considered to be correct if it exactly
matches a span from the gold standard. The stan-
dard precision, recall, and F -score can be calculated
using this definition of correctness. The drawback of
this strict score is that if a prediction is incorrect by
as little as one token it will be considered completely
incorrect.
Partial We also consider an overlap metric
(Hollingsworth and Teufel, 2005), which allows
partially correct predictions to be proportionally
counted. Precision (P ), recall (R), and F -score for
this method are:
P =
?
g?gold
?
p?pred overlap(g, p)
|pred|
(1)
R =
?
g?gold
?
p?pred overlap(p, g)
|gold|
(2)
F =
2PR
(P + R)
(3)
Where overlap(x, y) returns the proportion of to-
kens of y that are overlapped by x. For each of these
metrics we report the micro-average, as the number
of quotations in each document varies significantly.
When reporting P for the typewise results we re-
strict the set of predicted quotations to only those
with the requisite type, while still considering the
full set of gold quotations. Similarly, when calculat-
ing R we restrict the set of gold quotations to only
those with the required type.
4.5 Baselines
We have developed two baselines inspired by the
current lexical/syntactic pattern-based approaches
in the literature, which combine speech verbs and
hand-crafted rules.
993
Blex Lexical: cue verb + the longest of the spans be-
fore or after it until the sentence boundary.
Bsyn Syntactic: cue verb + verb syntactic object.
Bsyn is close to the model in de La Clergerie
et al (2011).
Instead of relying on a lexicon of verbs, our base-
lines use those identified by the verb-cue classifier.
As direct quotations are not always explicitly intro-
duced by a cue-verb, we defined a separate baseline
with a rule-based approach (Brule) that returns text
between quotation marks that has at least 3 tokens,
and where the non-stopword and non-proper noun
tokens are not all title cased. In our full results we
apply each method along with Brule and greedily
take the longest predicted spans that do not overlap.
5 Supervised Approaches
We present two supervised approaches to quotation
extraction, which operate over the tokens and the
phrase-structure parse nodes respectively. Despite
the difference in the item being classified, these ap-
proaches have some common features:
Lexical: unigram and bigram versions of the token,
lemma, and POS tags within a window of 5 to-
kens either side of the target, all indexed by po-
sition.
Sentence: features indicating whether the sentence
contains a quotation mark, a NE, a verb-cue, a
pronoun, or any combination of these. There is
also a sentence length feature.
Dependency: relation with parent, relations with
any dependants, as well as versions of these
that include the head and dependent tokens.
External knowledge: position-indexed features for
whether any of the tokens in the sentence match
a known role, organisation, or title. The titles
come from a small hand-built list, while the
role and organisation lists were built by recur-
sively following the WordNet (Fellbaum, 1998)
hyponyms of person and organization respec-
tively.
Other: features for whether the target is within quo-
tation marks, and whether there is a verb-cue
near the end of the sentence.
Strict Partial
P R F P R F
PARC Brule 75 94 83 96 94 95
Token 97 91 94 98 97 97
SMHC Brule 87 93 90 98 94 96
Token 94 90 92 99 97 98
Table 4: PARC and SMHC results on direct quotations.
The token based approach is trained and tested on all quo-
tations.
5.1 Token-based Approach
The token-based approach treats quotation extrac-
tion as analogous to NE tagging, where there are a
sequence of tokens that need to be individually la-
belled. Each token is given either an I, an O, or a B
label, where B denotes the first token in a quotation,
I denotes the token is inside a quotation, and O indi-
cates that the token is not part of a quotation. For NE
tagging it is common to use a sentence as a single
sequence, as NEs do not cross sentence boundaries.
This does not work for quotations, as they can cross
sentence and even paragraph boundaries. As such,
we treat the entire document as a single sequence,
which allows the predicted quotations to span both
sentence and paragraph bounds.
We use a linear chain Conditional Random Field
(CRF)2 as the learning algorithm, with the common
features listed above, as well as the following fea-
tures:
Verb: features indicating whether the current token
is a (possibly indirect) dependent of a verb-cue,
and another for whether the token is at the start
of a constituent that is a dependent of a verb-
cue.
Ancestor: the labels of all constituents that contain
the current token in their span, indexed by their
depth in the parse tree.
Syntactic: the label, depth, and token span size of
the highest constituent where the current token
is the left-most token in the constituent, as well
as its parent, and whether either of those con-
tains a verb-cue.
2http://www.chokkan.org/software/crfsuite/
994
Indirect Mixed All1
Strict P R F P R F P R F
Blex 34 32 33 17 26 20 46 44 45
Bsyn 78 46 58 61 40 49 80 63 70
Token 66 54 59 55 58 56 76 70 73
Constituent 61 50 55 50 38 43 70 64 67
ConstituentG 66 42 51 68 49 57 76 62 68
Partial P R F P R F P R F
Blex 56 66 61 78 79 78 73 79 76
Bsyn 89 58 70 88 75 81 92 74 82
Token 79 74 76 85 90 87 87 86 87
Constituent 78 67 72 84 82 83 86 80 83
ConstituentG 80 54 65 90 80 85 90 74 81
Table 5: Results on PARC. 1All reports the results over all quotations (direct, indirect and mixed). For the baselines,
this is a combination of the strategy in Blex or Bsyn with the rules for direct quotations. ConstituentG shows the
results for the constituent model using the gold parse.
5.2 Constituent-based Approach
The constituent approach classifies whole phrase
structure nodes as either quotation or not a quota-
tion. Ideally each quotation would match exactly
one constituent, however this is not always the case
in our data. In cases without an exact match we la-
bel every constituent that is a subspan of the quo-
tation as a quotation as long as it has a parent that
is not a subspan of the quotation. In these cases
multiple nodes will be labelled quotation, so a post-
processing step is introduced that rebuilds quota-
tions by merging predicted spans that are adjacent or
overlapping within a sentence. Restricting the merg-
ing process this way loses the ability to predict quo-
tations that cover more than a sentence, but without
this restriction too many predicted quotations are er-
roneously merged.
This approach uses a maximum entropy classi-
fier3 with L1 regularisation. In early experiments
we found that the constituent-based approach per-
formed poorly when trained on all quotations, so for
these experiments the constituent classifier is trained
only on indirect and mixed quotations. The classifier
uses the common features listed above as well as the
following features:
Span: length of the span, features for whether there
is a verb or a NE.
3http://scikit-learn.org/
Node: the label, number of descendants, number of
ancestors, and number of children of the target.
Context: dependency, node, and span features for
the parent and siblings of the target.
In addition the lexical features described earlier
are applied to both the start and end tokens of the
node?s span, as well as the highest token in the de-
pendency parse that is within the span.
6 Results
6.1 Direct Quotations
Table 4 shows the results for predicting direct quota-
tions on PARC and SMHC. In both corpora and with
both metrics the token-based approach outperforms
Brule. Although direct quotations should be trivial
to extract, and a simple system that returns the con-
tent between quotation marks should be hard to beat,
there are two main factors that confound the rule-
based system.
The first is the presence of mixed quotations,
which is most clearly demonstrated in the difference
between the strict precision scores and the partial
precision scores for Brule. Brule will find all of
the directly-quoted portions of mixed quotes, which
do not exactly match a quotation, and so will re-
ceive a low precision score with the strict metric.
However the partial overlap score will reward these
995
Indirect Mixed All1
Strict P R F P R F P R F
Blex 37 42 40 15 36 21 50 50 50
Bsyn 63 49 55 67 36 47 82 72 76
Token 69 53 60 80 91 85 82 75 78
Constituent 54 49 51 64 42 51 77 72 75
Partial P R F P R F P R F
Blex 52 68 59 87 77 82 77 84 81
Bsyn 75 59 66 89 66 76 91 80 85
Token 82 67 74 88 84 86 92 86 89
Constituent 77 63 69 91 75 82 91 82 86
Table 6: Results on SMHC. 1All reports the results over all quotations (direct, indirect and mixed). For the baselines,
this is a combination of the strategy in Blex or Bsyn with the rules for direct quotations.
predictions, as they do partially match a quote, so
there is a large difference in those scores. Note that
the reduced strict score does not occur for the token
method, which correctly identifies mixed quotations.
The other main issue is the presence of quotation
marks around items such as book titles and scare
quotes (i.e. text that is in quotation marks to distance
the author from a particular wording or claim). In
Section 4.5 we described the methods that we use to
avoid scare quotes and titles, which are rule-based
and imperfect. While these methods increase the
overall F -score of Brule, they do have a negative
impact on recall, which is why the recall is lower
than might be expected. These results demonstrate
that although direct quotations can be accurately ex-
tracted with rules, the accuracy will be lower than
might be anticipated and the returned spans will in-
clude a number of mixed quotations, which will be
missing some content.
6.2 Indirect and Mixed Quotations
The token approach was also the most effective
method for extracting indirect and mixed quotations
as Tables 5 and 6 show. Indirect quotations were
extracted with strict F -scores of 59% and 60% and
partial F -scores of 76% and 74% in PARC and SMHC
respectively, while mixed quotes were found with
strict F -scores of 56% and 85% and partial F -scores
of 87% and 86%.
Although there is a strong interconnection be-
tween syntax and attribution, results for Bsyn show
that merely considering attribution as a syntactic re-
lation (Skadhauge and Hardt, 2005) has a large im-
pact on recall: only a subset of inter-sentential quo-
tations can be effectively matched by verb comple-
ment boundaries.
The constituent model yielded lower results than
the token one, and in particular it greatly lowered
the recall of mixed quotations in both corpora. Since
the model heavily relies on syntax, it is particularly
affected by errors made by the parser. The conjunc-
tion and in Example 3 is incorrectly attached by the
parser to the cue said, leading the classifier to iden-
tify two separate spans. In order to verify the impact
of incorrect parsing on the model, we ran the con-
stituent model using gold standard parses for PARC.
This resulted in an increase in strict P and increased
the F -score for mixed quotations to 57%, similarly
to the score achieved by the token model. However,
it surprisingly negatively affected R for indirect quo-
tations.
(3) Graeme Hugo, said strong links between Aus-
tralia?s 700,000 ethnic Chinese and China
could benefit both countries and were unlikely
to pose a threat.
The tables also report results for the extraction of
all quotations, irrespective of their type. For this
score, the baseline models for indirect and mixed
quotations are combined with Brule for direct quo-
tations.
6.3 Model Comparison
We designed the features for the token and con-
stituent models to be largely similar. This al-
996
lows us to conclude that the difference in perfor-
mance between the token and constituent models
is largely driven by the class labelling and learn-
ing method. Overall, the token-based approach out-
performed both the baselines and the constituent
method. Qualitatively we found that the token-based
approach was making reasonable predictions most
of the time, but would often fail when a quotation
was attributed to a speaker through a parenthetical
clause, as in Example 4.
(4) Finding lunar ice, said Tidbinbilla?s
spokesman, Glen Nagle, would give a
major boost to NASA?s hopes of returning
humans to the moon by 2020.
The token-based approach has a reasonable bal-
ance of the various label types, and benefits from a
decoding step that allows it to make trade-offs be-
tween good local decisions and a good overall so-
lution. By comparison, the constituent-based ap-
proach has a large class imbalance, as there are many
more negative (i.e. not quotation) parse nodes than
there are positive, which makes finding a good deci-
sion boundary difficult. We experimented with re-
ducing the number of negative nodes to consider,
but found that the overall F -score was equivalent or
worse, largely driven by a drop in recall. We also
found that in many cases the constituent-approach
predicted quotes that were too short, or that were
only the second half of a conjunction, without the
first half being labelled. We expect that these issues
would be corrected with the addition of a decoding
step, that forces the classifier to make a good global
decision.
7 Speaker Attribution
While the focus of this paper is on extracting quota-
tions, we also present results on finding the speaker
of each quotation. As discussed in Section 2, quo-
tation attribution has been addressed in the litera-
ture before, including some work that includes large-
scale data (Elson and McKeown, 2010). However,
the large-scale evaluations that exist cover only di-
rect quotations, whereas we present results for di-
rect, indirect, and mixed quotations.
For this evaluation we use four of the methods that
were introduced in O?Keefe et al (2012). The first
is a simple rule-based approach (Rule) that returns
the entity closest to the speech verb nearest the quo-
tation, or if there is no such speech verb then the
entity nearest the end of the quotation. The second
method uses a CRF which is able to choose between
up to 15 entities that are in the paragraph containing
the quotation or any preceding it. The third method
(No seq.) is a binary MaxEnt classifier that predicts
whether each entity is the speaker or not the speaker,
with the entity achieving the highest speaker proba-
bility predicted. In O?Keefe et al (2012) this model
achieved the best results on the direct quotations in
SMHC, despite not using the sequence features or de-
coding methods that were available to other models.
The final method that we evaluate (Gold) is the ap-
proach that uses sequence features that use the gold-
standard labels from previous decisions. As noted
by O?Keefe et al, this method is not realisable in
practise, however we include these results so that
we can reassess the claims of O?Keefe et al when
direct, indirect, and mixed quotations are included.
For our results to be comparable we use the list of
speech verbs that was presented in Elson and McK-
eown (2010) and used in O?Keefe et al (2012).
Table 7 shows the accuracy of the two meth-
ods on both PARC and SMHC, broken down by the
type of the quotation. The first observation that
we make about these results in comparison to the
O?Keefe et al results, is that the accuracy is gener-
ally lower, even for direct quotations. This discrep-
ancy is caused by differences in our data compared
to theirs, notably that the sequence of quotations is
altered in ours by the introduction of indirect quota-
tions, and that some of the direct quotations that they
evaluated would be considered mixed quotations in
our corpora. The rule based method performs par-
ticularly poorly on PARC, which is likely caused by
the relative scarcity of direct quotations and the fact
that it was designed for direct quotations only. Di-
rect quotations are much more frequent in SMHC, so
the rules that rely on the sequence of speakers would
likely perform relatively better than on PARC.
While the approach using gold-standard sequence
features unsurprisingly performed the best, the most
straightforward learned model (No seq.), trained
without any sequence information, equalled or out-
performed the two other non-gold approaches for all
quotation types on both corpora. This indicates that
the CRF model evaluated here was not able to effec-
997
Corpus Method Dir. Ind. Mix. All
PARC Rule 70 60 47 62
CRF 82 68 65 73
No seq. 85 74 65 77
Gold 88 79 74 82
SMHC Rule 89 76 78 84
CRF 83 72 71 78
No seq. 91 79 81 87
Gold 93 81 83 89
Table 7: Speaker attribution accuracy results for both cor-
pora over gold standard quotations.
tively use the sequence information that is present.
8 Conclusion
In this work we have presented the first large-scale
experiments on the entire quotation extraction and
attribution task: evaluating the extraction and at-
tribution of direct, indirect and mixed quotations
over two large news corpora. One of these corpora
(SMHC) is a novel contribution of this work, while
our results are the first presented for the other cor-
pus (PARC). This work has shown that while rule-
based approaches that return the object of a speech
verb are indeed effective, they are outperformed by
supervised systems that can take advantage of addi-
tional evidence. We also show that state-of-the-art
quotation attribution methods are less accurate on
indirect and mixed quotations than they are on di-
rect quotations.
Future work will include extending these methods
to extract all attributions, i.e. beliefs, eventualities,
and facts, as well as the source spans. We will also
evaluate the effect of adding a decoding step to the
constituent approach. This work provides an accu-
rate and complete quotation extraction and attribu-
tion system that can be used for a wide range of tasks
in information extraction and opinion mining.
Acknowledgements
We would like to thank Bonnie Webber for her feed-
back and assistance. Pareti has been supported by a
Scottish Informatics & Computer Science Alliance
(SICSA) studentship; O?Keefe has been supported
by a University of Sydney Merit scholarship and
a Capital Markets CRC top-up scholarship. This
work has been supported by ARC Discovery grant
DP1097291 and the Capital Markets CRC Com-
putable News project.
References
Sabine Bergler. 1992. Evidential analysis of re-
ported speech. Ph.D. thesis, Brandeis University.
Sabine Bergler, Monia Doandes, Christine Gerard,
and Rene? Witte. 2004. Attributions. In Exploring
Attitude and Affect in Text: Theories and Applica-
tions, Technical Report SS-04-07, pages 16?19.
Papers from the 2004 AAAI Spring Symposium.
Eric de La Clergerie, Benoit Sagot, Rosa Stern, Pas-
cal Denis, Gaelle Recource, and Victor Mignot.
2011. Extracting and visualizing quotations from
news wires. Human Language Technology. Chal-
lenges for Computer Science and Linguistics,
pages 522?532.
C??cero Nogueira dos Santos and Ruy Luiz Milidiu?.
2009. Entropy guided transformation learning. In
Foundations of Computational, Intelligence Vol-
ume 1, Studies in Computational Intelligence,
pages 159?184. Springer.
David K. Elson and Kathleen R. McKeown. 2010.
Automatic attribution of quoted speech in literary
narrative. In Proceedings of the Twenty-Fourth
Conference of the Association for the Advance-
ment of Artificial Intelligence, pages 1013?1019.
Christine Fellbaum. 1998. WordNet: An electronic
lexical database. MIT press Cambridge, MA.
William Paulo Ducca Fernandes, Eduardo Motta,
and Ruy Luiz Milidiu?. 2011. Quotation extraction
for portuguese. In Proceedings of the 8th Brazil-
ian Symposium in Information and Human Lan-
guage Technology (STIL 2011), pages 204?208.
Kevin Glass and Shaun Bangay. 2007. A naive
salience-based method for speaker identification
in fiction books. In Proceedings of the 18th An-
nual Symposium of the Pattern Recognition Asso-
ciation of South Africa (PRASA07), pages 1?6.
Bill Hollingsworth and Simone Teufel. 2005. Hu-
man annotation of lexical chains: Coverage and
agreement measures. In ELECTRA Workshop on
Methodologies and Evaluation of Lexical Cohe-
sion Techniques in Real-world Applications (Be-
yond Bag of Words), page 26.
998
Dan Klein and Christopher D Manning. 2002. Fast
exact inference with a factored model for natural
language parsing. In Advances in neural informa-
tion processing systems, pages 3?10.
Ralf Krestel, Sabine Bergler, and Rene? Witte. 2008.
Minding the source: Automatic tagging of re-
ported speech in newspaper articles. In Pro-
ceedings of the Sixth International Language Re-
sources and Evaluation (LREC?08).
Jisheng Liang, Navdeep Dhillon, and Krzysztof
Koperski. 2010. A large-scale system for an-
notating and querying quotations in news feeds.
In Proceedings of the 3rd International Semantic
Search Workshop, pages 1?5.
Nuno Mamede and Pedro Chaleira. 2004. Charac-
ter identification in children stories. Advances in
Natural Language Processing, pages 82?90.
Tim O?Keefe, Silvia Pareti, James R. Curran, Irena
Koprinska, and Matthew Honnibal. 2012. A se-
quence labelling approach to quote attribution. In
Proceedings of the 2012 Joint Conference on Em-
pirical Methods in Natural Language Processing
and Computational Natural Language Learning,
pages 790?799.
Silvia Pareti. 2012. A database of attribution rela-
tions. In Proceedings of the Eight International
Conference on Language Resources and Evalua-
tion, pages 3213?3217.
Bruno Pouliquen, Ralf Steinberger, and Clive Best.
2007. Automatic detection of quotations in multi-
lingual news. In Proceedings of Recent Advances
in Natural Language Processing, pages 487?492.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Aravind
Joshi, and Bonnie Webber. 2006. Annotating at-
tribution in the Penn Discourse TreeBank. In Pro-
ceedings of the Workshop on Sentiment and Sub-
jectivity in Text, pages 31?38.
Rashmi Prasad, Eleni Miltsakaki, Nikhil Dinesh,
Alan Lee, Aravind Joshi, Livio Robaldo, and
Bonnie Webber. 2008. The Penn Discourse Tree-
Bank 2.0 annotation manual. In Technical report,
University of Pennsylvania: Institute for Research
in Cognitive Science.
Luis Sarmento and Sergio Nunes. 2009. Automatic
extraction of quotes and topics from news feeds.
In 4th Doctoral Symposium on Informatics Engi-
neering.
Roser Saur?? and James Pustejovsky. 2009. Factbank:
A corpus annotated with event factuality. In Lan-
guage Resources and Evaluation, pages 227?268.
Nathan Schneider, Rebecca Hwa, Philip Gianfor-
toni, Dipanjan Das, Michael Heilman, Alan W.
Black, Frederik L. Crabbe, and Noah A. Smith.
2010. Visualizing topical quotations over time
to understand news discourse. Technical report,
Carnegie Mellon University.
Karin K. Schuler. 2005. Verbnet: A Broad-
Coverage, Comprehensive Verb Lexicon. Ph.D.
thesis, Faculties of Computer and Information
Science of the University of Pennsylvania.
Peter R. Skadhauge and Daniel Hardt. 2005. Syn-
tactic identification of attribution in the RST tree-
bank. In Proceedings of the Sixth International
Workshop on Linguistically Interpreted Corpora.
Janyce Wiebe and Ellen Riloff. 2005. Creating
subjective and objective sentence classifiers from
unannotated texts. In Computational Linguistics
and Intelligent Text Processing, pages 486?497.
Springer.
Jason Zhang, Alan Black, and Richard Sproat.
2003. Identifying speakers in children?s stories
for speech synthesis. In Proceedings of EU-
ROSPEECH, pages 2041?2044.
999
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 516?520,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
An Annotated Corpus of Quoted Opinions in News Articles
Tim O?Keefe James R. Curran Peter Ashwell Irena Koprinska
e-lab, School of Information Technologies
University of Sydney
NSW 2006, Australia
{tokeefe,james,pash4408,irena}@it.usyd.edu.au
Abstract
Quotes are used in news articles as evi-
dence of a person?s opinion, and thus are
a useful target for opinion mining. How-
ever, labelling each quote with a polarity
score directed at a textually-anchored tar-
get can ignore the broader issue that the
speaker is commenting on. We address
this by instead labelling quotes as support-
ing or opposing a clear expression of a
point of view on a topic, called a position
statement. Using this we construct a cor-
pus covering 7 topics with 2,228 quotes.
1 Introduction
News articles are a useful target for opinion min-
ing as they discuss salient opinions by newswor-
thy people. Rather than asserting what a person?s
opinion is, journalists typically provide evidence
by using reported speech, and in particular, direct
quotes. We focus on direct quotes as expressions
of opinion, as they can be accurately extracted and
attributed to a speaker (O?Keefe et al, 2012).
Characterising the opinions in quotes remains
challenging. In sentiment analysis over product
reviews, polarity labels are commonly used be-
cause the target, the product, is clearly identified.
However, for quotes on topics of debate, the target
and meaning of polarity labels is less clear. For ex-
ample, labelling a quote about abortion as simply
positive or negative is uninformative, as a speaker
can use either positive or negative language to sup-
port or oppose either side of the debate.
Previous work (Wiebe et al, 2005; Balahur
et al, 2010) has addressed this by giving each
expression of opinion a textually-anchored target.
While this makes sense for named entities, it does
not apply as obviously for topics, such as abortion,
that may not be directly mentioned. Our solution
is to instead define position statements, which are
Abortion: Women should have the right to choose an abortion.
Carbon tax: Australia should introduce a tax on carbon or an
emissions trading scheme to combat global warming.
Immigration: Immigration into Australia should be maintained
or increased because its benefits outweigh any negatives.
Reconciliation: The Australian government should formally
apologise to the Aboriginal people for past injustices.
Republic: Australia should cease to be a monarchy with the
Queen as head of state and become a republic with an Australian
head of state.
Same-sex marriage: Same-sex couples should have the right to
attain the legal state of marriage as it is for heterosexual couples.
Work choices: Australia should introduce WorkChoices to give
employers more control over wages and conditions.
Table 1: Topics and their position statements.
clear statements of a viewpoint or position on a
particular topic. Quotes related to this topic can
then be labelled as supporting, neutral, or oppos-
ing the position statement. This disambiguates the
meaning of the polarity labels, and allows us to
determine the side of the debate that the speaker
is on. Table 1 shows the topics and position state-
ments used in this work, and some example quotes
from the republic topic are given below. Note that
the first example includes no explicit mention of
the monarchy or the republic.
Positive: ?I now believe that the time has come. . . for us to
have a truly Australian constitutional head of state.?
Neutral: ?The establishment of an Australian republic is es-
sentially a symbolic change, with the main arguments, for
and against, turning on national identity. . . ?
Negative: ?I personally think that the monarchy is a tradition
which we want to keep.?
With this formulation we define an annotation
scheme and build a corpus covering 7 topics, with
100 documents per topic. This corpus includes
3,428 quotes, of which 1,183 were marked in-
valid, leaving 2,228 that were marked as support-
ing, neutral, or opposing the relevant topic state-
ment. All quotes in our corpus were annotated by
three annotators, with Fleiss? ? values of between
0.43 and 0.45, which is moderate.
516
2 Background
Early work in sentiment analysis (Turney, 2002;
Pang et al, 2002; Dave et al, 2003; Blitzer et al,
2007) focused on product and movie reviews,
where the text under analysis discusses a single
product or movie. In these cases, labels like posi-
tive and negative are appropriate as they align well
with the overall communicative goal of the text.
Later work established aspect-oriented opinion
mining (Hu and Liu, 2004), where the aim is to
find features or aspects of products that are dis-
cussed in a review. The reviewer?s position on
each aspect can then be classified as positive or
negative, which results in a more fine-grained clas-
sification that can be combined to form an opin-
ion summary. These approaches assume that each
document has a single source (the document?s au-
thor), whose communicative goal is to evaluate a
well-defined target, such as a product or a movie.
However this does not hold in news articles, where
the goal of the journalist is to present the view-
points of potentially many people.
Several studies (Wiebe et al, 2005; Wilson
et al, 2005; Kim and Hovy, 2006; Godbole et al,
2007) have looked at sentiment in news text, with
some (Balahur and Steinberger, 2009; Balahur
et al, 2009, 2010) focusing on quotes. In all of
these studies the authors have textually-anchored
the target of the sentiment. While this makes sense
for targets that can be resolved back to named enti-
ties, it does not apply as obviously when the quote
is arguing for a particular viewpoint in a debate,
as the topic may not be mentioned explicitly and
polarity labels may not align to sides of the debate.
Work on debate summarisation and subgroup
detection (Somasundaran and Wiebe, 2010; Abu-
Jbara et al, 2012; Hassan et al, 2012) has of-
ten used data from online debate forums, partic-
ularly those forums where users are asked to se-
lect whether they support or oppose a given propo-
sition before they can participate. This is simi-
lar to our aim with news text, where instead of a
textually-anchored target, we have a proposition,
against which we can evaluate quotes.
3 Position Statements
Our goal in this study is to determine which side of
a debate a given quote supports. Assigning polar-
ity labels to a textually-anchored target does not
work here for several reasons. Quotes may not
mention the debate topic, there may be many rel-
No cont. Context
Topic Quotes AA ? AA ?
Abortion 343 .77 .57 .73 .53
Carbon tax 278 .71 .42 .57 .34
Immigration 249 .58 .18 .58 .25
Reconcil. 513 .66 .37 .68 .44
Republic 347 .68 .51 .71 .58
Same-sex m. 246 .72 .51 .71 .55
Work choices 269 .72 .45 .65 .44
Total 2,245 .69 .43 .66 .45
Table 2: Average Agreement (AA) and Fleiss? ?
over the valid quotes
evant textually-anchored targets for a single topic,
and polarity labels do not necessarily align with
sides of a debate.
We instead define position statements, which
clearly state the position that one side of the debate
is arguing for. We can then characterise opinions
as supporting, neutral towards, or opposing this
particular position. Position statements should not
argue for a particular position, rather they should
simply state what the position is. Table 1 shows
the position statements that we use in this work.
4 Annotation
For our task we expect a set of news articles on
a given topic as input, where the direct quotes in
the articles have been extracted and attributed to
speakers. A position statement will have been de-
fined, that states a point of view on the topic, and
a small subset of quotes will have been labelled as
supporting, neutral, or opposing the given state-
ment. A system performing this task would then
label the remaining quotes as supporting, neutral,
or opposing, and return them to the user.
A major contribution of this work is that we
construct a fully labelled corpus, which can be
used to evaluate systems that perform the task de-
scribed above. To build this corpus we employed
three annotators, one of whom is an author, while
the other two were hired using the outsourcing
website Freelancer1. Our data is drawn from the
Sydney Morning Herald2 archive, which ranges
from 1986 until 2009, and it covers seven topics
that were subject to debate within Australian news
media during that time. For each topic we used
1http://www.freelancer.com
2http://www.smh.com.au
517
No cont. Context
Topic Quotes AA ? AA ?
Abortion 343 .78 .52 .74 .46
Carbon tax 278 .72 .39 .59 .19
Immigration 249 .58 .08 .58 .14
Reconcil. 513 .66 .31 .69 .36
Republic 347 .69 .39 .72 .41
Same-sex m. 246 .73 .43 .73 .40
Work choices 269 .73 .40 .67 .32
Total 2,245 .70 .36 .68 .32
Table 3: Average Agreement (AA) and Fleiss? ?
when the labels are neutral versus non-neutral
Apache Solr3 to find the top 100 documents that
matched a manually-constructed search query. All
documents were tokenised and POS-tagged and the
named entities were found using the system from
Hachey et al (2013). Finally, the quotes were ex-
tracted and attributed to speakers using the system
from O?Keefe et al (2012).
For the first part of the task, annotators were
asked to label each quote without considering any
context. In other words they were asked to only
use the text of the quote itself as evidence for an
opinion, not the speaker?s prior opinions or the
text of the document. They were then asked to la-
bel the quote a second time, while considering the
text surrounding the quote, although they were still
asked to ignore the prior opinions of the speaker.
For each of these choices annotators were given a
five-point scale ranging from strong or clear op-
position to strong or clear support, where support
or opposition is relative to the position statement.
Annotators were also asked to mark instances
where either the speaker or quote span was incor-
rectly identified, although they were asked to con-
tinue annotating the quote as though it were cor-
rect. They were also asked to mark quotes that
were invalid due to either the quote being off-
topic, or the item not being a quote (e.g. book ti-
tles, scare quotes, etc.).
5 Corpus results
In order to achieve the least amount of noise in
our corpus, we opted to discard quotes that any an-
notator had marked as invalid. From the original
set of 3,428 quotes, 1,183 (35%) were removed,
which leaves 2,245 (65%). From the original cor-
pus, 23% were marked off-topic, which shows that
3http://lucene.apache.org/solr/
in order to label opinions in news, a system would
first have to identify the topic-relevant parts of the
text. The annotators further indicated that 16%
were not quotes, and there were a small number of
cases (<1%) where the quote span was incorrect.
Annotators were able to select multiple reasons for
a quote being invalid.
Table 2 shows both Fleiss? ? and the raw agree-
ment averaged between annotators for each topic.
We collapsed the two supporting labels together,
as well as the two opposing labels, such that we
end up with a classification of opposes vs. neu-
tral vs. supports. The no context and context cases
scored 0.69 and 0.66 in raw agreement, while the
? values were 0.43 and 0.45, which is moderate.
Intuitively we expect that the confusion is
largely between neutral and the two polar labels.
To examine this we merged all the non-neutral la-
bels into one group and calculated the agreement
between the non-neutral group and the neutral la-
bel, as shown in Table 3. For the non-neutral vs.
neutral agreement we find that despite stability in
raw agreement, Fleiss? ? drops substantially, to
0.36 (no context) and 0.32 (context).
For comparison we remove all neutral annota-
tions and focus on disagreement between the po-
lar labels. For this we cannot use Fleiss? ?, as it
requires a fixed number of annotations per quote,
however we can average the pairwise ? values be-
tween annotators, which results in values of 0.93
(no context) and 0.92 (context). Though they are
not directly comparable, the magnitude of the dif-
ference between the numbers (0.36 and 0.32 vs.
0.93 and 0.92) indicates that deciding when an
opinion provides sufficient evidence of support or
opposition is the main challenge facing annotators.
To adjudicate the decisions annotators made, we
opted to take a majority vote for cases of two
or three-way agreement, while discarding cases
where annotators did not agree (1% of quotes).
The final distribution of labels in the corpus is
shown in Table 4. For both the no context and
context cases the largest class was neutral with
61% and 46% of the corpus respectively. The drop
in neutrality between the no context and context
cases shows that the interpretation of a quote can
change based on the context it is placed in.
6 Discussion
In refining our annotation scheme we noted several
factors that make annotation difficult.
518
No context Context
Topic Quotes Opp. Neut. Supp. Quotes Opp. Neut. Supp.
Abortion 343 .13 .63 .25 340 .16 .52 .32
Carbon tax 273 .09 .70 .21 273 .14 .44 .42
Immigration 247 .09 .72 .19 245 .12 .64 .23
Reconciliation 509 .05 .57 .38 503 .07 .42 .50
Republic 345 .24 .48 .28 342 .32 .37 .32
Same-sex marriage 246 .16 .55 .28 243 .25 .38 .37
Work choices 265 .14 .72 .14 266 .26 .50 .24
Total 2,228 .12 .61 .26 2,212 .18 .46 .36
Table 4: Label distribution for the final corpus.
Opinion relevance When discussing a topic,
journalists will often delve into the related aspects
and opinions that people hold. This introduces a
challenge as annotators need to decide whether a
particular quote is on-topic enough to be labelled.
For instance, these quotes by the same speaker
were in an article on the carbon tax:
1) ?Whether it?s a stealth tax, the emissions trading scheme,
whether it?s an upfront. . . tax like a carbon tax, there will not
be any new taxes as part of the Coalition?s policy?
2) ?I don?t think it?s something that we should rush into. But
certainly I?m happy to see a debate about the nuclear option.?
In the first quote the speaker is voicing opposi-
tion to a tax on carbon, which is easy to annotate
with our scheme. However in the second quote,
the speaker is discussing nuclear power in relation
to a carbon tax, which is much more difficult, as it
is unclear whether is is off-topic or neutral.
Obfuscation and self-contradiction While
journalists usually quote someone to provide
evidence of the person?s opinion, there are some
cases where they include quotes to show that
the person is inconsistent. The following quotes
by the same speaker were included in an arti-
cle to illustrate that the speaker?s position was
inconsistent:
1) ?My point is that. . . the most potent argument in favour of
the republic, is that why should we have a Briton as the Queen
? who, of course, in reality is also the Queen of Australia ?
but a Briton as the head of State of Australia?
2) ?The Coalition supports the Constitution not because we
support the. . . notion of the monarchy, but because we support
the way our present Constitution works?
The above example also indicates a level of ob-
fuscation that is reasonably common for politi-
cians. Neither of the quotes actually expresses a
clear statement of how the speaker feels about a
potential republic. The first quote is an opinion
about the strongest argument in favour of a re-
public, without necessarily making that argument,
while the second quote states a party line, with a
caveat that might indicate personal disagreement.
Annotator bias This task is prone to be influ-
enced by an annotator?s biases, including their po-
litical or cultural background, their opinion about
the topic or speaker, or their level of knowledge
about the topic.
7 Conclusion
In this work we examined the problem of anno-
tating opinions in news articles. We proposed to
exploit quotes, as they are used by journalists to
provide evidence of an opinion, and are easy to
extract and attribute to speakers. Our key con-
tribution is that rather than requiring a textually-
anchored target for each quote, we instead label
quotes as supporting, neutral, or opposing a posi-
tion statement, which states a particular viewpoint
on a topic. This allowed us to resolve ambigu-
ities that arise when considering a polarity label
towards a topic. We next defined an annotation
scheme and built a corpus, which covers 7 top-
ics, with 100 documents per topic, and a total of
2,228 annotated quotes. Future work will include
building a system able to perform the task we have
defined, as well as extending this work to include
indirect quotes.
Acknowledgements
O?Keefe was supported by a University of Sydney
Merit scholarship and a Capital Markets CRC top-
up scholarship. This work was supported by ARC
Discovery grant DP1097291 and the Capital Mar-
kets CRC Computable News project.
519
References
Amjad Abu-Jbara, Mona Diab, Pradeep Dasigi,
and Dragomir Radev. 2012. Subgroup detec-
tion in ideological discussions. In Proceedings
of the 50th Annual Meeting of the Association
for Computational Linguistics, pages 399?409.
Alexandra Balahur and Ralf Steinberger. 2009.
Rethinking sentiment analysis in the news:
From theory to practice and back. Proceedings
of the First Workshop on Opinion Mining and
Sentiment Analysis.
Alexandra Balahur, Ralf Steinberger, Mijail
Kabadjov, Vanni Zavarella, Erik Van Der Goot,
Matina Halkia, Bruno Pouliquen, and Jenya
Belyaeva. 2010. Sentiment analysis in the news.
In Proceedings of the 7th International Confer-
ence on Language Resources and Evaluation,
pages 2216?2220.
Alexandra Balahur, Ralf Steinberger, Erik Van
Der Goot, Bruno Pouliquen, and Mijail Kabad-
jov. 2009. Opinion mining on newspa-
per quotations. In Proceedings of the 2009
IEEE/WIC/ACM International Joint Confer-
ence on Web Intelligence and Intelligent Agent
Technology, pages 523?526.
John Blitzer, Mark Dredze, and Fernando Pereira.
2007. Biographies, bollywood, boom-boxes
and blenders: Domain adaptation for sentiment
classification. In Proceedings of the 45th An-
nual Meeting of the Association of Computa-
tional Linguistics, pages 440?447.
Kushal Dave, Steve Lawrence, and David Pen-
nock. 2003. Mining the peanut gallery: Opinion
extraction and semantic classification of prod-
uct reviews. In Proceedings of the 12th inter-
national conference on World Wide Web, pages
519?528.
Namrata Godbole, Manjunath Srinivasaiah, and
Steven Skiena. 2007. Large-scale sentiment
analysis for news and blogs. In Proceedings
of the International Conference on Weblogs and
Social Media.
Ben Hachey, Will Radford, Joel Nothman,
Matthew Honnibal, and James R. Curran. 2013.
Evaluating entity linking with Wikipedia. Arti-
ficial Intelligence.
Ahmed Hassan, Amjad Abu-Jbara, and Dragomir
Radev. 2012. Detecting subgroups in online dis-
cussions by modeling positive and negative re-
lations among participants. In Proceedings of
the 2012 Joint Conference on Empirical Meth-
ods in Natural Language Processing and Com-
putational Natural Language Learning, pages
59?70.
Minqing Hu and Bing Liu. 2004. Mining and sum-
marizing customer reviews. In Proceedings of
the tenth ACM SIGKDD International Confer-
ence on Knowledge Discovery and Data Min-
ing, pages 168?177.
Soo-Min Kim and Eduard Hovy. 2006. Extracting
opinions, opinion holders, and topics expressed
in online news media text. In Proceedings of the
Workshop on Sentiment and Subjectivity in Text,
pages 1?8.
Tim O?Keefe, Silvia Pareti, James R. Curran,
Irena Koprinska, and Matthew Honnibal. 2012.
A sequence labelling approach to quote attri-
bution. In Proceedings of the 2012 Joint Con-
ference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural
Language Learning, pages 790?799.
Bo Pang, Lillian Lee, and Shivakumar
Vaithyanathan. 2002. Thumbs up?: Senti-
ment classification using machine learning
techniques. In Proceedings of the 2002
Conference on Empirical Methods in Natural
Language Processing, pages 79?86.
Swapna Somasundaran and Janyce Wiebe. 2010.
Recognizing stances in ideological on-line de-
bates. In Proceedings of the NAACL HLT
2010 Workshop on Computational Approaches
to Analysis and Generation of Emotion in Text,
pages 116?124.
Peter Turney. 2002. Thumbs up or thumbs down?:
Semantic orientation applied to unsupervised
classification of reviews. In Proceedings of
the 40th Annual Meeting of the Association for
Computational Linguistics, pages 417?424.
Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005. Annotating expressions of opinions and
emotions in language. Language Resources and
Evaluation, 39(2/3):165?210.
Theresa Wilson, Paul Hoffmann, Swapna Soma-
sundaran, Jason Kessler, Janyce Wiebe, Yejin
Choi, Claire Cardie, Ellen Riloff, and Siddharth
Patwardhan. 2005. OpinionFinder: a system
for subjectivity analysis. In Proceedings of
HLT/EMNLP Interactive Demonstrations.
520
