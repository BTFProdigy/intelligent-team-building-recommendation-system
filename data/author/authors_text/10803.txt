Proceedings of the EACL 2009 Student Research Workshop, pages 46?53,
Athens, Greece, 2 April 2009. c?2009 Association for Computational Linguistics
A Chain-starting Classifier of Definite NPs in Spanish
Marta Recasens
CLiC - Centre de Llenguatge i Computacio?
Department of Linguistics
University of Barcelona
08007 Barcelona, Spain
mrecasens@ub.edu
Abstract
Given the great amount of definite noun
phrases that introduce an entity into the
text for the first time, this paper presents a
set of linguistic features that can be used
to detect this type of definites in Span-
ish. The efficiency of the different fea-
tures is tested by building a rule-based and
a learning-based chain-starting classifier.
Results suggest that the classifier, which
achieves high precision at the cost of re-
call, can be incorporated as either a filter
or an additional feature within a corefer-
ence resolution system to boost its perfor-
mance.
1 Introduction
Although often treated together, anaphoric pro-
noun resolution differs from coreference resolu-
tion (van Deemter and Kibble, 2000). Whereas
the former attempts to find an antecedent for each
anaphoric pronoun in a discourse, the latter aims
to build full coreference chains, namely linking
all noun phrases (NPs) ? whether pronominal or
with a nominal head ? that point to the same en-
tity. The output of anaphora resolution1 are noun-
pronoun pairs (or pairs of a discourse segment and
a pronoun in some cases), whereas the output of
coreference resolution are chains containing a va-
riety of items: pronouns, full NPs, discourse seg-
ments... Thus, coreference resolution requires a
wider range of strategies in order to build the full
chains of coreferent mentions.2
1A different matter is the resolution of anaphoric full NPs,
i.e. those semantically dependent on a previous mention.
2We follow the ACE terminology (NIST, 2003) but in-
stead of talking of objects in the world we talk of objects in
the discourse model: we use entity for an object or set of ob-
jects in the discourse model, and mention for a reference to
an entity.
One of the problems specific to coreference res-
olution is determining, once a mention is encoun-
tered by the system, whether it refers to an entity
previously mentioned or it introduces a new entity
into the text. Many algorithms (Aone and Ben-
nett, 1996; Soon et al, 2001; Yang et al, 2003)
do not address this issue specifically, but implic-
itly assume all mentions to be potentially corefer-
ent and examine all possible combinations; only
if the system fails to link a mention with an al-
ready existing entity, it is considered to be chain
starting.3 However, such an approach is computa-
tionally expensive and prone to errors, since nat-
ural language is populated with a huge number of
entities that appear just once in the text. Even def-
inite NPs, which are traditionally believed to refer
to old entities, have been demonstrated to start a
coreference chain over 50% of the times (Fraurud,
1990; Poesio and Vieira, 1998).
An alternative line of research has considered
applying a filter prior to coreference resolution
that classifies mentions as either chain starting or
coreferent. Ng and Cardie (2002) and Poesio et al
(2005) have tested the impact of such a detector
on the overall coreference resolution performance
with encouraging results. Our chain-starting clas-
sifier is comparable ? despite some differences4
? to the detectors suggested by Ng and Cardie
(2002), Uryupina (2003), and Poesio et al (2005)
for English, but not identical to strictly anaphoric
ones5 (Bean and Riloff, 1999; Uryupina, 2003),
since a non-anaphoric NP can corefer with a pre-
vious mention.
This paper presents a corpus-based study of def-
3By chain starting we refer to those mentions that are the
first element ? and might be the only one ? in a coreference
chain.
4Ng and Cardie (2002) and Uryupina (2003) do not limit
to definite NPs but deal with all types of NPs.
5Notice the confusing use of the term anaphoric in (Ng
and Cardie, 2002) for describing their chain-starting filtering
module.
46
inite NPs in Spanish that results in a set of eight
features that can be used to identify chain-starting
definite NPs. The heuristics are tested by building
two different chain-starting classifiers for Spanish,
a rule-based and a learning-based one. The evalu-
ation gives priority to precision over recall in view
of the classifier?s efficiency as a filtering module.
The paper proceeds as follows. Section 2 pro-
vides a qualitative comparison with related work.
The corpus study and the empirically driven set of
heuristics for recognizing chain-starting definites
are described in Section 3. The chain-starting clas-
sifiers are built in Section 4. Section 5 reports on
the evaluation and discusses its implications. Fi-
nally, Section 6 summarizes the conclusions and
outlines future work.
2 Related Work
Some of the corpus-driven features here presented
have a precedent in earlier classifiers of this kind
for English while others are our own contribution.
In any case, they have been adapted and tested for
Spanish for the first time.
We build a list of storage units, which is in-
spired by research in the field of cognitive linguis-
tics. Bean and Riloff (1999) and Uryupina (2003)
have already employed a definite probability mea-
sure in a similar way, although the way the ratio
is computed is slightly different. The former use
it to make a ?definite-only list? by ranking those
definites extracted from a corpus that were ob-
served at least five times and never in an indefi-
nite construction. In contrast, the latter computes
four definite probabilities ? which are included
as features within a machine-learning classifier ?
from the Web in an attempt to overcome Bean and
Riloff?s (1999) data sparseness problem. The defi-
nite probabilities in our approach are checked with
confidence intervals in order to guarantee the reli-
ability of the results, avoiding to draw any gener-
alization when the corpus does not contain a large
enough sample.
The heuristics concerning named entities and
storage-unit variants find an equivalent in the fea-
tures used in Ng and Cardie?s (2002) supervised
classifier that represent whether the mention is a
proper name (determined based on capitalization,
whereas our corpus includes both weak and strong
named entities) and whether a previous NP is an
alias of the current mention (on the basis of a rule-
based alias module that tries out different transfor-
mations). Uryupina (2003) and Vieira and Poesio
(2000) also take capital and low case letters into
account.
All four approaches exploit syntactic structural
cues of pre- and post- modification to detect com-
plex NPs, as they are considered to be unlikely to
have been previously mentioned in the discourse.
A more fine-grained distinction is made by Bean
and Riloff (1999) and Vieira and Poesio (2000)
to distinguish restrictive from non-restrictive post-
modification by ommitting those modifiers that
occur between commas, which should not be clas-
sified as chain starting. The latter also list a series
of ?special predicates? including nouns like fact
or result, and adjectives such as first, best, only,
etc. A subset of the feature vectors used by Ng
and Cardie (2002) and Uryupina (2003) is meant
to code whether the NP is or not modified. In
this respect, our contribution lies in adapting these
ideas for the way modification occurs in Spanish
? where premodifiers are rare ? and in introducing
a distinction between PP and AP modifiers, which
we correlate in turn with the heads of simple defi-
nites.
We borrow the idea of classifying definites oc-
curring in the first sentence as chain starting from
Bean and Riloff (1999).
The precision and recall results obtained by
these classifiers ? tested on MUC corpora ? are
around the eighties, and around the seventies in
the case of Vieira and Poesio (2000), who use the
Penn Treebank.
Luo et al (2004) make use of both a linking
and a starting probability in their Bell tree algo-
rithm for coreference resolution, but the starting
probability happens to be the complementary of
the linking one. The chain-starting classifier we
build can be used to fine-tune the starting probabil-
ity used in the construction of coreference chains
in Luo et al?s (2004) style.
3 Corpus-based Study
As fully documented by Lyons (1999), definite-
ness varies cross-linguistically. In contrast with
English, for instance, Spanish adds the article be-
fore generic NPs (1), within some fixed phrases
(2), and in postmodifiers where English makes use
of bare nominal premodification (3). Altogether
results in a larger number of definite NPs in Span-
ish and, by extension, a larger number of chain-
starting definites (Recasens et al, 2009).
47
(1) Tard??a
Late
incorporacio?n
incorporation
de
of
la
the
mujer
woman
al
to the
trabajo.
work.
?Late incorporation of  women into  work.?
(2) Villalobos
Villalobos
dio
gave
las
the
gracias
thanks
a
to
los
the
militantes.
militants.
?Villalobos gave  thanks to the militants.?
(3) El
The
mercado
market
internacional
international
del
of the
cafe?.
coffee.
?The international  coffee market.?
Long-held claims that equate the definite arti-
cle with a specific category of meaning cannot be
hold. The present-day definite article is a cate-
gory that, although it did originally have a seman-
tic meaning of ?identifiability?, has increased its
range of contexts so that it is often a grammati-
cal rather than a semantic category (Lyons, 1999).
Definite NPs cannot be considered anaphoric by
default, but strategies need to be introduced in or-
der to classify a definite as either a chain-starting
or a coreferent mention. Given that the extent
of grammaticization6 varies from language to lan-
guage, we considered it appropriate to conduct a
corpus study oriented to Spanish: (i) to check the
extent to which strategies used in previous work
can be extended to Spanish, and (ii) to explore ad-
ditional linguistic cues.
3.1 The corpus
The empirical data used in our corpus study come
from AnCora-Es, the Spanish corpus of AnCora
? Annotated Corpora for Spanish and Catalan
(Taule et al, 2008), developed at the University
of Barcelona and freely available from http:
//clic.ub.edu/ancora. AnCora-Es is a
half-million-word multilevel corpus consisting of
newspaper articles and annotated, among other
levels of information, with PoS tags, syntactic
constituents and functions, and named entities. A
subset of 320 000 tokens (72 500 full NPs7) was
used to draw linguistic features about definiteness.
3.2 Features
As quantitatively supported by the figures in Ta-
ble 1, the split between simple (i.e. non-modified)
and complex NPs seems to be linguistically rele-
vant. We assume that the referential properties of
6Grammaticization, or grammaticalization, is a process
of linguistic change by which a content word becomes part
of the grammar by losing its lexical and phonological load.
7By full NPs we mean NPs with a nominal head, thus
omitting pronouns, NPs with an elliptical head as well as co-
ordinated NPs.
simple NPs differ from complex ones, and this dis-
tinction is kept when designing the eight heuristics
for recognizing chain-starting definites that we in-
troduce in this section.
1. Head match. Ruling out those definites that
match an earlier noun in the text has proved
to be able to filter out a considerable num-
ber of coreferent mentions (Ng and Cardie,
2002; Poesio et al, 2005). We considered
both total and partial head match, but stuck
to the first as the second brought much noise.
On its own, namely if definite NPs are all
classified as chain starting only if no mention
has previously appeared with the same lexical
head, we obtain a precision (P) not less than
84.95% together with 89.68% recall (R). Our
purpose was to increase P as much as pos-
sible with the minimum loss in R: it is pre-
ferred not to classify a chain-starting instance
? which can still be detected by the corefer-
ence resolution module at a later stage ? since
a wrong label might result in a missed coref-
erence link.
2. Storage units. A very grammaticized defi-
nite article accounts for the large number of
definite NPs attested in Spanish (column 2 in
Table 1): 46% of the total. In the light of
Bybee and Hopper?s (2001) claim that lan-
guage structure dynamically results from fre-
quency and repetition, we hypothesized that
specific simple definite NPs in which the ar-
ticle has fully grammaticized constitute what
Bybee and Hopper (2001) call storage units:
the more a specific chunk is used, the more
stored and automatized it becomes. These
article-noun storage units might well head a
coreference chain.
With a view to providing the chain-starting
classifier with a list of these article-noun
storage units, we extracted from AnCora-Es
all simple NPs preceded by a determiner8
(columns 2 and 3 in the second row of Table
1) and ranked them by their definite probabil-
ity, which we define as the number of simple
definite NPs with respect to the number of
simple determined NPs. Secondly, we set a
threshold of 0.7, considering as storage units
8Only noun types occurring a minimum of ten times were
included in this study. Singular and plural forms as well as
masculine and feminine were kept as distinct types.
48
Definite NPs Other det. NPs Bare NPs Total
Simple NPs 12 739 6 642 15 183 34 564 (48%)
Complex NPs 20 447 9 545 8 068 38 060 (52%)
Total 33 186 (46%) 16 187 (22%) 23 251 (32%) 72 624 (100%)
Table 1: Overall distribution of full NPs in AnCora-Es (subset).
those definites above the threshold. In order
to avoid biased probabilities due to a small
number of observed examples in the corpus, a
95 percent confidence interval was computed.
The final list includes 191 storage units, such
as la UE ?the EU?, el euro ?the euro?, los con-
sumidores ?the consumers?, etc.
3. Named entities (NEs). A closer look at the
list of storage units revealed that the higher
the definite probability, the more NE-like a
noun is. This led us to extrapolate that the
definite article has completely grammaticized
(i.e. lost its semantic load) before simple def-
inites which are NEs (e.g. los setenta ?the
seventies?, el Congreso de Estados Unidos
?the U.S. Congress?9), and so they are likely
to be chain-starting.
4. Storage-unit variants. The fact that some
of the extracted storage units were variants
of a same entity gave us an additional cue:
complementing the plain head_match feature
by adding a gazetteer with variants (e.g. la
Unio?n Europea ?the European Union? and la
UE ?the EU?) stops the storage_unit heuris-
tic from classifying a simple definite as chain
starting if a previous equivalent unit has ap-
peared.
5. First sentence. Given that the probability
for any definite NP occurring in the first sen-
tence of a text to be chain starting is very
high, since there has not been time to intro-
duce many entities, all definites appearing in
the first sentence can be classified as chain
starting.
6. AP-preference nouns. Complex definites
represent 62% out of all definite NPs (Table
1). In order to assess to what extent the refer-
ential properties of a noun on its own depend
on its combinatorial potential to occur with
9The underscore represents multiword expressions.
either a prepositional phrase (PP) or an ad-
jectival phrase (AP), complex definites were
grouped into those containing a PP (49%) and
those containing an AP10 (27%). Next, the
probability for each noun to be modified by a
PP or an AP was computed. The results made
it possible to draw a distinction ? and two re-
spective lists ? between PP-preference nouns
(e.g. el inicio ?the beginning?) and nouns that
prefer an AP modifier (e.g. las autoridades
?the authorities?). Given that APs are not as
informative as PPs, they are more likely to
modify storage units than PPs. Nouns with
a preference for APs turned out to be storage
units or behave similarly. Thus, simple defi-
nites headed by such nouns are unlikely to be
coreferent.
7. PP-preference nouns. Nouns that prefer to
combine with a PP are those that depend on
an extra argument to become referential. This
argument, however, might not appear as a
nominal modifier but be recoverable from the
discourse context, either explicitly or implic-
itly. Therefore, a simple definite headed by
a PP-preference noun might be anaphoric but
not necessarily a coreferent mention. Thus,
grouping PP-preference nouns offers an em-
pirical way for capturing those nouns that are
bridging anaphors when they appear in a sim-
ple definite. For instance, it is not rare that,
once a specific company has been introduced
into the text, reference is made for the first
time to its director simply as el director ?the
director?.
8. Neuter definites. Unlike English, the Span-
ish definite article is marked for grammati-
cal gender. Nouns might be either mascu-
line or feminine, but a third type of definite
article, the neuter one (lo), is used to nomi-
nalize adjectives and clauses, namely ?to cre-
ate a referential entity? out of a non-nominal
10When a noun was followed by more than one modifier,
only the syntactic type of the first one was taken into account.
49
Given a definite mention m,
1. If m is introduced by a neuter definite article, classify
as chain starting.
2. If m appears in the first sentence of the document, clas-
sify as chain starting.
3. If m shares the same lexical head with a previous men-
tion or is a storage-unit variant of it, classify as coref-
erent.
4. If the head of m is PP-preference, classify as chain
starting.
5. If m is a simple definite,
(a) and the head of m appears in the list of storage
units, classify as chain starting.
(b) and the head of m is AP-preference, classify as
chain starting.
(c) and m is an NE, classify as chain starting.
(d) Otherwise, classify as coreferent.
6. Otherwise (i.e. m is a complex definite), classify as
chain starting.
Figure 1: Rule-based algorithm.
item. Since such neuters have a low corefer-
ential capacity, the classification of these NPs
as chain starting can favour recall.
4 Chain-starting Classifier
In order to test the linguistic cues outlined above,
we build two different chain-starting classifiers: a
rule-based model and a learning-based one. Both
aim to detect those definite NPs for which there is
no need to look for a previous reference.
4.1 Rule-based approach
The first way in which the linguistic findings in
Section 3.2 are tested is by building a rule-based
classifier. The heuristics are combined and or-
dered in the most efficient way, yielding the hand-
crafted algorithm shown in Figure 1. Two main
principles underlie the algorithm: (i) simple defi-
nites tend to be coreferent mentions, and (ii) com-
plex definites tend to be chain starting (if their
head has not previously appeared). Accordingly,
Step 5 in Figure 1 finishes by classifying simple
definites as coreferent, and Step 6 complex def-
inites as chain starting. Before these last steps,
however, a series of filters are applied correspond-
ing to the different heuristics. The performance is
presented in Table 2.
4.2 Machine-learning approach
The second way in which the suggested linguistic
cues are tested is by constructing a learning-based
classifier. The Weka machine learning toolkit
(Witten and Frank, 2005) is used to train a J48
decision tree on a 10-fold cross-validation. A to-
tal of eight learning features are considered: (i)
head match, (ii) storage-unit variant, (iii) is a
neuter definite, (iv) is first sentence, (v) is a PP-
preference noun, (vi) is a storage unit, (vii) is
an AP-preference noun, (viii) is an NE. All fea-
tures are binary (either ?yes? or ?no?). We experi-
ment with different feature vectors, incrementally
adding one feature at a time. The performance is
presented in Table 3.
5 Evaluation
A subset of AnCora-CO-Es consisting of 60 Span-
ish newspaper articles (23 335 tokens, 5 747 full
NPs) is kept apart for the test corpus. AnCora-
CO-Es is the coreferentially annotated AnCora-Es
corpus, following the guidelines described in (Re-
casens et al, 2007). Coreference relations were
annotated manually with the aid of the PALinkA
(Orasan, 2003) and AnCoraPipe (Bertran et al,
2008) tools. Interestingly enough, the test corpus
contains 2 575 definite NPs, out of which 1 889 are
chain-starting (1401 chain-starting definite NPs
are actually isolated entities), namely 73% defi-
nites head a coreference chain, which implies that
a successful classifier has the potential to rule out
almost three quarters of all definite mentions.
Given that chain starting is the majority class
and following (Ng and Cardie, 2002), we took the
?one class? classification as a naive baseline: all
instances were classified as chain starting, giving
a precision of 71.95% (first row in Tables 2 and 3).
5.1 Performance
Tables 2 and 3 show the results in terms of preci-
sion (P), recall (R), and F0.5-measure (F0.5). F0.5-
measure,11 which weights P twice as much as R,
is chosen since this classifier is designed as a filter
for a coreference resolution module and hence we
want to make sure that the discarded cases can be
really discarded. P matters more than R.
Each row incrementally adds a new heuristic to
the previous ones. The score is cumulative. No-
tice that the order of the features in Table 2 does
11F0.5 is computed as 1.5PR0.5P+R .
50
Cumulative Features P (%) R (%) F0.5 (%)
Baseline 71.95 100.0 79.37
+Head match 84.95 89.68 86.47
+Storage-unit variant 85.02 89.58 86.49
+Neuter definite 85.08 90.05 86.68
+First sentence 85.12 90.32 86.79
+PP preference 85.12 90.32 86.79
+Storage unit 89.65** 71.54** 82.67
+AP preference 89.70** 71.96** 82.89
+Named entity 89.20* 78.22** 85.21
Table 2: Performance of the rule-based classifier.
Cumulative Features P (%) R (%) F0.5 (%)
Baseline 71.95 100.0 79.37
+Head match 85.00 89.70 86.51
+Storage-unit variant 85.00 89.70 86.51
+Neuter definite 85.00 90.20 86.67
+First sentence 85.10 90.40 86.80
+PP preference 85.10 90.40 86.80
+Storage unit 83.80 93.50** 86.80
+AP preference 83.90 93.60** 86.90
+Named entity 83.90 93.60** 86.90
Table 3: Performance of the learning-based classi-
fier (J48 decision tree).
not directly map the order as presented in the algo-
rithm (Figure 1): the head_match heuristic and the
storage-unit_variant need to be applied first, since
the other heuristics function as filters that are ef-
fective only if head match between the mentions
has been first checked. Table 3 presents the incre-
mental performance of the learning-based classi-
fier for the different sets of features.
Diacritics ** (p<.01) and * (p<.05) indicate
whether differences in P and R between the re-
duced classifier (head_ match) and the extended
ones are significant (using a one-way ANOVA fol-
lowed by Tukey?s post-hoc test).
5.2 Discussion
Although the central role played by the
head_match feature has been emphasized by
prior work, it is striking that such a simple heuris-
tic achieves results over 85%, raising P by 13
percentage points. All in all, these figures can only
be slightly improved by some of the additional
features. These features have a different effect
on each approach: whereas they improve P (and
decrease R) in the hand-crafted algorithm, they
improve R (and decrease P) in the decision tree.
In the first case, the highest R is achieved with
the first four features, and the last three features
obtain an increase in P statistically significant yet
accompanied by a decrease in R also statistically
significant. We expected that the second block of
features would favour P without such a significant
drop in R.
The drop in P in the decision tree is not statis-
tically significant as it is in the rule-based classi-
fier. Our goal, however, was to increase P as much
as possible, since false positive errors harm the
performance of the subsequent coreference resolu-
tion system much more than false negative errors,
which can still be detected at a later stage. The
very same attributes might prove more efficient if
used as additional learning features within the vec-
tor of a coreference resolution system rather than
as an independent pre-classifier.
From a linguistic perspective, the fact that the
linguistic heuristics increase P provides support
for the hypotheses about the grammaticized def-
inite article and the existence of storage units.
We carried out an error analysis to consider those
cases in which the features are misleading in terms
of precision errors. The first_sentence feature, for
instance, results in an error in (4), where the first
sentence includes a coreferent NP.
(4) La expansio?n de la pirater??a en el Sudeste de Asia
puede destruir las econom??as de la regio?n.
?The expansion of piracy in South-East Asia can de-
stroy the economies of the region.?
Classifying PP-preference nouns as chain starting
fails when a noun like el protagonista ?the pro-
tagonist?, which could appear as the first mention
in a film critique, happens to be previously men-
tioned with a different head. Likewise, not using
the same head in cases such as la competicio?n ?the
competition? and la Liga ?the League? accounts
for the failure of the storage_unit or named_entity
feature, which classify the second mention as
chain starting. On the other hand, some recall er-
rors are due to head_match, which might link two
NPs that despite sharing the same head point to a
different entity (e.g. el grupo Agnelli ?the Agnelli
group? and el grupo industrial Montedison ?the in-
dustrial group Montedison?).
6 Conclusions and Future Work
The paper presented a corpus-driven chain-
starting classifier of definite NPs for Spanish,
pointing out and empirically supporting a series
of linguistic features to be taken into account.
Given that definiteness is very much language de-
51
pendent, the AnCora-Es corpus was mined to in-
fer some linguistic hypotheses that could help in
the automatic identification of chain-starting def-
inites. The information from different linguistic
levels (lexical, semantic, morphological, syntac-
tic, and pragmatic) in a computationally not ex-
pensive way casts light on potential features help-
ful for resolving coreference links. Each resulting
heuristic managed to improve precision although
at the cost of a drop in recall. The highest improve-
ment in precision (89.20%) with the lowest loss
in recall (78.22%) translates into an F0.5-measure
of 85.21%. Hence, the incorporation of linguistic
knowledge manages to outperform the baseline by
17 percentage points in precision.
Priority is given to precision, since we want to
assure that the filter prior to coreference resolu-
tion module does not label as chain starting def-
inite NPs that are coreferent. The classifier was
thus designed to minimize false positives. No less
than 73% of definite NPs in the data set are chain
starting, so detecting 78% of these definites with
almost 90% precision could have substantial sav-
ings. From a linguistic perspective, the improve-
ment in precision supports the linguistic hypothe-
ses, even if at the expense of recall. However, as
this classifier is not a final but a prior module, ei-
ther a filter within a rule-based system or one ad-
ditional feature within a larger learning-based sys-
tem, the shortage of recall can be compensated
at the coreference resolution stage by considering
other more sophisticated features.
The results here presented are not comparable
with other existing classifiers of this type for sev-
eral reasons. Our approach would perform differ-
ently for English, which has a lower number of
definite NPs. Secondly, our classifier has been
evaluated on a corpus much larger than prior ones
such as Uryupina?s (2003). Thirdly, some classi-
fiers aim at detecting non-anaphoric NPs, which
are not the same as chain-starting. Fourthly, we
have empirically explored the contribution of the
set of heuristics with respect to the head_match
feature. None of the existing approaches com-
pares its final performance in relation with this
simple but extremely powerful feature. Some of
our heuristics do draw on previous work, but we
have tuned them for Spanish and we have also con-
tributed with new ideas, such as the use of storage
units and the preference of some nouns for a spe-
cific syntactic type of modifier.
As future work, we will adapt this chain-starting
classifier for Catalan, fine-tune the set of heuris-
tics, and explore to what extent the inclusion of
such a classifier improves the overall performance
of a coreference resolution system for Spanish.
Alternatively, we will consider using the sug-
gested attributes as part of a larger set of learning
features for coreference resolution.
Acknowledgments
We would like to thank the three anonymous
reviewers for their suggestions for improve-
ment. This paper has been supported by the
FPU Grant (AP2006-00994) from the Span-
ish Ministry of Education and Science, and
the Lang2World (TIN2006-15265-C06-06) and
Ancora-Nom (FFI2008-02691-E/FILO) projects.
References
Chinatsu Aone and Scott W. Bennett. 1996. Ap-
plying machine learning to anaphora resolution.
In S. Wermter, E. Riloff and G. Scheler (eds.),
Connectionist, Statistical and Symbolic Approaches
to Learning for Natural Language Processing.
Springer Verlag, Berlin, 302-314.
David L. Bean and Ellen Riloff. 1999. Corpus-based
identification of non-anaphoric noun phrases. In
Proceedings of the ACL 1999, 373-380.
Manuel Bertran, Oriol Borrega, Marta Recasens, and
Ba`rbara Soriano. 2008. AnCoraPipe: A tool for
multilevel annotation. Procesamiento del Lenguaje
Natural, 41:291-292.
Joan Bybee and Paul Hopper. 2001. Introduction to
frequency and the emergence of linguistic structure.
In J. Bybee and P. Hopper (eds.), Frequency and the
Emergence of Linguistic Structure. John Benjamins,
Amsterdam, 1-24.
Kari Fraurud. 1990. Definiteness and the processing
of NPs in natural discourse. Journal of Semantics,
7:395-433.
Xiaoqiang Luo, Abe Ittycheriah, Hongyan Jing, Nanda
Kambhatla, and Salim Roukos. 2004. A mention-
synchronous coreference resolution algorithm based
on the Bell tree. In Proceedings of ACL 2004.
Christopher Lyons. 1999. Definiteness. Cambridge
University Press, Cambridge.
Vincent Ng and Claire Cardie. 2002. Identifying
anaphoric and non-anaphoric noun phrases to im-
prove coreference resolution. In Proceedings of
COLING 2002.
NIST. 2003. ACE Entity detection and tracking.
V.2.5.1.
52
Constantin Orasan. 2003. PALinkA: A highly cus-
tomisable tool for discourse annotation. In Proceed-
ings of the 4th SIGdial Workshop on Discourse and
Dialogue.
Massimo Poesio and Renata Vieira. 1998. A corpus-
based investigation of definite description use. Com-
putational Linguistics, 24(2):183-216.
Massimo Poesio, Mijail Alexandrov-Kabadjov, Renata
Vieira, Rodrigo Goulart, and Olga Uryupina. 2005.
Does discourse-new detection help definite descrip-
tion resolution? In Proceedings of IWCS 2005.
Marta Recasens, M. Anto`nia Mart??, and Mariona Taule?.
2007. Where anaphora and coreference meet. An-
notation in the Spanish CESS-ECE corpus. In Pro-
ceedings of RANLP 2007. Borovets, Bulgaria.
Marta Recasens, M. Anto`nia Mart??, and Mariona Taule?.
2009. First-mention definites: more than excep-
tional cases. In S. Featherston and S. Winkler (eds.),
The Fruits of Empirical Linguistics. Volume 2. De
Gruyter, Berlin.
Wee M. Soon, Hwee T. Ng, and Daniel C. Y. Lim.
2001. A machine learning approach to coreference
resolution of noun phrases. Computational Linguis-
tics, 27(4):521-544.
Mariona Taule?, M. Anto`nia Mart??, and Marta Recasens.
2008. AnCora: Multilevel Annotated Corpora for
Catalan and Spanish. In Proceedings of the 6th In-
ternational Conference on Language Resources and
Evaluation (LREC 2008),
Olga Uryupina. 2003. High-precision identification
of discourse-new and unique noun phrases. In Pro-
ceedings of the ACL 2003 Student Workshop, 80-86.
Kees van Deemter and Rodger Kibble. 2000. Squibs
and Discussions: On coreferring: coreference in
MUC and related annotation schemes. Computa-
tional Linguistics, 26(4):629-637.
Renata Vieira and Massimo Poesio. 2000. An empir-
ically based system for processing definite descrip-
tions. Computational Linguistics, 26(4):539-593.
Ian Witten and Eibe Frank. 2005. Data Mining: Practi-
cal machine learning tools and techniques. Morgan
Kaufmann.
Xiaofeng Yang, Guodong Zhou, Jian Su, and Chew
L. Tan. 2003. Coreference resolution using com-
petition learning approach. In Proceedings of ACL
2003. 176-183.
53
Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 70?75,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
SemEval-2010 Task 1: Coreference Resolution in Multiple Languages 
 
 
Marta Recasens, Toni Mart?, Mariona Taul? Llu?s M?rquez, Emili Sapena 
Centre de Llenguatge i Computaci? (CLiC) TALP Research Center,  
University of Barcelona Technical University of Catalonia 
Gran Via de les Corts Catalanes 585 
08007 Barcelona 
Jordi Girona Salgado 1-3 
08034 Barcelona 
{mrecasens, amarti, mtaule} 
@ub.edu 
{lluism, esapena} 
@lsi.upc.edu 
 
 
 
Abstract 
This paper presents the task ?Coreference 
Resolution in Multiple Languages? to be run 
in SemEval-2010 (5th International Workshop 
on Semantic Evaluations). This task aims to 
evaluate and compare automatic coreference 
resolution systems for three different lan-
guages (Catalan, English, and Spanish) by 
means of two alternative evaluation metrics, 
thus providing an insight into (i) the portabil-
ity of coreference resolution systems across 
languages, and (ii) the effect of different scor-
ing metrics on ranking the output of the par-
ticipant systems. 
1 Introduction 
Coreference information has been shown to be 
beneficial in many NLP applications such as In-
formation Extraction (McCarthy and Lehnert, 
1995), Text Summarization (Steinberger et al, 
2007), Question Answering (Morton, 2000), and 
Machine Translation. In these systems, there is a 
need to identify the different pieces of information 
that refer to the same discourse entity in order to 
produce coherent and fluent summaries, disam-
biguate the references to an entity, and solve ana-
phoric pronouns.  
Coreference is an inherently complex phenome-
non. Some of the limitations of the traditional rule-
based approaches (Mitkov, 1998) could be over-
come by machine learning techniques, which allow 
automating the acquisition of knowledge from an-
notated corpora. 
 
This task will promote the development of lin-
guistic resources ?annotated corpora1? and ma-
chine-learning techniques oriented to coreference 
resolution. In particular, we aim to evaluate and 
compare coreference resolution systems in a multi-
lingual context, including Catalan, English, and 
Spanish languages, and by means of two different 
evaluation metrics.  
By setting up a multilingual scenario, we can 
explore to what extent it is possible to implement a 
general system that is portable to the three lan-
guages, how much language-specific tuning is nec-
essary, and the significant differences between 
Romance languages and English, as well as those 
between two closely related languages such as 
Spanish and Catalan. Besides, we expect to gain 
some useful insight into the development of multi-
lingual NLP applications.  
As far as the evaluation is concerned, by em-
ploying B-cubed (Bagga and Baldwin, 1998) and 
CEAF (Luo, 2005) algorithms we can consider 
both the advantages and drawbacks of using one or 
the other scoring metric. For comparison purposes, 
the MUC score will also be reported. Among oth-
ers, we are interested in the following questions: 
Which evaluation metric provides a more accurate 
picture of the accuracy of the system performance? 
Is there a strong correlation between them? Can 
                                                           
1 Corpora annotated with coreference are scarce, especially for 
languages other than English.  
70
statistical systems be optimized under both metrics 
at the same time? 
The rest of the paper is organized as follows. 
Section 2 describes the overall task. The corpora 
and the annotation scheme are presented in Section 
3. Conclusions and final remarks are given in Sec-
tion 4. 
 
2 Task description  
The SemEval-2010 task ?Coreference Resolution 
in Multiple Languages? is concerned with auto-
matic coreference resolution for three different 
languages: Catalan, English, and Spanish.  
2.1 Specific tasks  
Given the complexity of the coreference phenom-
ena, we will concentrate only in two tractable as-
pects, which lead to the two following subtasks for 
each of the languages: 
i) Detection of full coreference chains, com-
posed by named entities, pronouns, and full 
noun phrases (NPs). 
ii) Pronominal resolution, i.e. finding the antece-
dents of the pronouns in the text.  
 
 
The example in Figure 1 illustrates the two sub-
tasks.2 Given a text in which NPs are identified and 
indexed (including elliptical subjects, represented 
as ?), the goal of (i) is to extract all coreference 
chains: 1?5?6?30?36, 9?11, and 7?18; while the 
goal of (ii) is to identify the antecedents of pro-
nouns 5 and 6, which are 1 and 5 (or 1), respec-
tively. Note that (b) is a simpler subtask of (a) and 
that for a given pronoun there can be multiple an-
tecedents (e.g. both 1 and 5 are correct antecedents 
for 6).  
We restrict the task to solving ?identity? rela-
tions between NPs (coreference chains), and be-
tween pronouns and antecedents. Nominal 
predicates and appositions as well as NPs with a 
non-nominal antecedent (discourse deixis) will not 
been taken into consideration in the recognition of 
coreference chains (see Section 3.1 for more in-
formation about decisions concerning the annota-
tion scheme). 
Although we target at general systems address-
ing the full multilingual task, we will allow taking 
part on any subtask of any language in order to 
promote participation. 
 
 
Figure 1.  NPs in a sample from the Catalan training 
data (left) and the English translation (right). 
                                                           
2 The example in Figure 1 is a simplified version of the anno-
tated format. See Section 2.2 for more details. 
[The beneficiaries of [[spouse?s]3 pensions]2]1 will 
be able to keep [the payment]4 even if [they]5 re-
marry provided that [they]6 fulfill [a series of [con-
ditions]8]7, according to [the royal decree approved 
yesterday by [the Council of Ministers]10]9.  
[The new rule]11 affects [the recipients of [a 
[spouse?s]13 pension]12 [that]14 get married after 
[January_1_,_2002]16]17. 
[The first of [the conditions]18]19 is being older 
[than 61 years old]20 or having [an officially rec-
ognized permanent disability [that]22 makes one 
disabled for [any [profession]24 or [job]25]23]21. 
[The second one]26 requires that [the pension]27 be 
[the main or only source of [the [pensioner?s]30 in-
come]29]28, and provided that [the annual amount 
of [the pension]32]31 represents, at least, [75% of 
[the total [yearly income of [the pen-
sioner]36]35]34]33. 
[Els beneficiaris de [pensions de [viudetat]3]2]1 po-
dran conservar [la paga]4 encara_que [?]5 es tornin 
a casar si [?]6 compleixen [una s?rie de [condi-
cions]8]7 , segons [el reial decret aprovat ahir pel 
[Consell_de_Ministres]10]9 .  
[La nova norma]11 afecta [els perceptors d' [una 
pensi? de [viudetat]13]12 [que]14 contreguin [matri-
moni]15 a_partir_de [l' 1_de_gener_del_2002]16]17 .  
[La primera de [les condicions]18]19 ?s tenir [m?s 
de 61 anys]20 o tenir reconeguda [una incapacitat 
permanent [que]22 inhabiliti per a [tota [professi?]24 
o [ofici]25]23]21. 
[La segona]26 ?s que [la pensi?]27 sigui [la principal 
o ?nica font d' [ingressos del [pensionista]30]29]28 , i 
sempre_que [l' import anual de [la mateixa pen-
si?]32]31 representi , com_a_m?nim , [el 75% del 
[total dels [ingressos anuals del [pensionis-
ta]36]35]34]33.  
 
71
2.2 Evaluation  
2.1.1 Input information 
The input information for the task will consist of: 
word forms, lemmas, POS, full syntax, and seman-
tic role labeling. Two different scenarios will be 
considered regarding the source of the input infor-
mation: 
 
i) In the first one, gold standard annotation will 
be provided to participants. This input annota-
tion will correctly identify all NPs that are part 
of coreference chains. This scenario will be 
only available for Catalan and Spanish. 
ii) In the second, state-of-the-art automatic lin-
guistic analyzers for the three languages will 
be used to generate the input annotation of the 
data. The matching between the automatically 
generated structure and the real NPs interven-
ing in the chains does not need to be perfect in 
this setting. 
  
By defining these two experimental settings, we 
will be able to check the performance of corefer-
ence systems when working with perfect linguistic 
(syntactic/semantic) information, and the degrada-
tion in performance when moving to a more realis-
tic scenario with noisy input annotation.  
2.1.2 Closed/open challenges 
In parallel, we will also consider the possibility of 
differentiating between closed and open chal-
lenges, that is, when participants are allowed to use 
strictly the information contained in the training 
data (closed) and when they make use of some ex-
ternal resources/tools (open). 
2.1.3 Scoring measures 
Regarding evaluation measures, we will have spe-
cific metrics for each of the subtasks, which will be 
computed by language and overall.  
Several metrics have been proposed for the task 
of coreference resolution, and each of them pre-
sents advantages and drawbacks. For the purpose 
of the current task, we have selected two of them ? 
B-cubed and CEAF ? as the most appropriate ones. 
In what follows we justify our choice.  
The MUC scoring algorithm (Vilain et al, 1995) 
has been the most widely used for at least two rea-
sons. Firstly, the MUC corpora and the MUC 
scorer were the first available systems. Secondly, 
the MUC scorer is easy to understand and imple-
ment. However, this metric has two major weak-
nesses: (i) it does not give any credit to the correct 
identification of singleton entities (chains consist-
ing of one single mention), and (ii) it intrinsically 
favors systems that produce fewer coreference 
chains, which may result in higher F-measures for 
worse systems. 
A second well-known scoring algorithm, the 
ACE value (NIST, 2003), owes its popularity to 
the ACE evaluation campaign. Each error (a miss-
ing element, a misclassification of a coreference 
chain, a mention in the response not included in the 
key) made by the response has an associated cost, 
which depends on the type of entity (e.g. person, 
location, organization) and on the kind of mention 
(e.g. name, nominal, pronoun). The fact that this 
metric is entity-type and mention-type dependent, 
and that it relies on ACE-type entities makes this 
measure inappropriate for the current task. 
The two measures that we are interested in com-
paring are B-cubed (Bagga and Baldwin, 1998) 
and CEAF (Luo, 2005). The former does not look 
at the links produced by a system as the MUC al-
gorithm does, but looks at the presence/absence of 
mentions for each entity in the system output. Pre-
cision and recall numbers are computed for each 
mention, and the average gives the final precision 
and recall numbers.  
CEAF (Luo, 2005) is a novel metric for evaluat-
ing coreference resolution that has already been 
used in some published papers (Ng, 2008; Denis 
and Baldridge, 2008). It mainly differs from B-
cubed in that it finds the best one-to-one entity 
alignment between the gold and system responses 
before computing precision and recall. The best 
mapping is that which maximizes the similarity 
over pairs of chains. The CEAF measure has two 
variants: a mention-based, and an entity-based one. 
While the former scores the similarity of two 
chains as the absolute number of common men-
tions between them, the latter scores the relative 
number of common mentions. 
Luo (2005) criticizes the fact that a response 
with all mentions in the same chain obtains 100% 
B-cubed recall, whereas a response with each men-
tion in a different chain obtains 100% B-cubed 
72
precision. However, precision will be penalized in 
the first case, and recall in the second case, each 
captured by the corresponding F-measure. Luo?s 
entity alignment might cause that a correctly iden-
tified link between two mentions is ignored by the 
scoring metric if that entity is not aligned. Finally, 
as far as the two CEAF metrics are concerned, the 
entity-based measure rewards alike a correctly 
identified one-mention entity and a correctly iden-
tified five-mention entity, while the mention-based 
measure takes into account the size of the entity. 
Given this series of advantages and drawbacks, 
we opted for including both B-cubed and CEAF 
measures in the final evaluation of the systems. In 
this way we will be able to perform a meta-
evaluation study, i.e. to evaluate and compare the 
performance of metrics with respect to the task 
objectives and system rankings. It might be inter-
esting to break B-cubed and CEAF into partial re-
sults across different kinds of mentions in order to 
get a better understanding of the sources of errors 
made by each system. Additionally, the MUC met-
ric will also be included for comparison purposes 
with previous results.  
Finally, for the setting with automatically gener-
ated input information (second scenario in Section 
2.1.1), it might be desirable to devise metric vari-
ants accounting for partial matches of NPs. In this 
case, capturing the correct NP head would give 
most of the credit. We plan to work in this research 
line in the near future.  
Official scorers will be developed in advance 
and made available to participants when posting 
the trial datasets. The period in between the release 
of trial datasets and the start of the full evaluation 
will serve as a test for the evaluation metrics. De-
pending on the feedback obtained from the partici-
pants we might consider introducing some 
improvements in the evaluation setting.  
3 AnCora-CO corpora  
The corpora used in the task are AnCora-CO, 
which are the result of enriching the AnCora cor-
pora (Taul? et al, 2008) with coreference informa-
tion. AnCora-CO is a multilingual corpus 
annotated at different linguistic levels consisting of 
400K words in Catalan3, 400K words in Spanish2, 
                                                           
3 Freely available for research purposes from the following 
URL: http://clic.ub.edu/ancora 
and 120K words in English. For the purpose of the 
task, the corpora are split into a training (85%) and 
test (15%) set. Each file corresponds to one news-
paper text.  
AnCora-CO consists mainly of newspaper and 
newswire articles: 200K words from the Spanish 
and Catalan versions of El Peri?dico newspaper, 
and 200K words from the EFE newswire agency in 
the Spanish corpus, and from the ACN newswire 
agency in the Catalan corpus. The source corpora 
for Spanish and Catalan are the AnCora corpora, 
which were annotated by hand with full syntax 
(constituents and functions) as well as with seman-
tic information (argument structure with thematic 
roles, semantic verb classes, named entities, and 
WordNet nominal senses). The annotation of 
coreference constitutes an additional layer on top 
of the previous syntactic-semantic information. 
The English part of AnCora-CO consists of a se-
ries of documents of the Reuters newswire corpus 
(RCV1 version).4 The RCV1 corpus does not come 
with any syntactic nor semantic annotation. This is 
why we only count with automatic linguistic anno-
tation produced by statistical taggers and parsers 
on this corpus. 
Although the Catalan, English, and Spanish cor-
pora used in the task all belong to the domain of 
newspaper texts, they do not form a three-way par-
allel corpus. 
3.1 Coreference annotation 
The annotation of a corpus with coreference in-
formation is highly complex due to (i) the lack of 
information in descriptive grammars about this 
topic, and (ii) the difficulty in generalizing the in-
sights from one language to another. Regarding (i), 
a wide range of units and relations occur for which 
it is not straightforward to determine whether they 
are or not coreferent. Although there are theoretical 
studies for English, they cannot always be ex-
tended to Spanish or Catalan since coreference is a 
very language-specific phenomenon, which ac-
counts for (ii). 
In the following we present some of the linguis-
tic issues more problematic in relation to corefer-
ence annotation, and how we decided to deal with 
them in AnCora-CO (Recasens, 2008). Some of 
them are language dependent (1); others concern 
                                                           
4 Reuters Corpus RCV1 is distributed by NIST at the follow-
ing URL: http://trec.nist.gov/data/reuters/reuters.html 
73
the internal structure of the mentions (2), or the 
type of coreference link (3). Finally, we present 
those NPs that were left out from the annotation 
for not being referential (4). 
 
1. Language-specific issues 
- Since Spanish and Catalan are pro-drop 
languages, elliptical subjects were intro-
duced in the syntactic annotation, and they 
are also annotated with coreference.  
- Expletive it pronouns, which are frequent 
in English and to a lesser extent in Spanish 
and Catalan are not referential, and so they 
do not participate in coreference links. 
- In Spanish, clitic forms for pronouns can 
merge into a single word with the verb; in 
these cases the whole verbal node is anno-
tated for coreference. 
2. Issues concerning the mention structure  
- In possessive NPs, only the reference of 
the thing possessed (not the possessor) is 
taken into account. For instance, su libro 
?his book? is linked with a previous refer-
ence of the same book; the possessive de-
terminer su ?his? does not constitute an NP 
on its own. 
- In the case of conjoined NPs, three (or 
more) links can be encoded: one between 
the entire NPs, and additional ones for 
each of the constituent NPs. AnCora-CO 
captures links at these different levels. 
3. Issues concerning types of coreference links 
- Plural NPs can refer to two or more ante-
cedents that appear separately in the text. 
In these cases an entity resulting from the 
addition of two or more entities is created.  
- Discourse deixis is kept under a specific 
link tag because not all coreference resolu-
tion systems can handle such relations. 
- Metonymy is annotated as a case of iden-
tity because both mentions pragmatically 
corefer. 
4. Non-referential NPs 
- In order to be linguistically accurate (van 
Deemter and Kibble, 2000), we distinguish 
between referring and attributive NPs: 
while the first point to an entity, the latter 
express some of its properties. Thus, at-
tributive NPs like apposition and predica-
tive phrases are not treated as identity 
coreference in AnCora-CO (they are kept 
distinct under the ?predicative link? tag).  
- Bound anaphora and bridging reference go 
beyond coreference and so are left out 
from consideration. 
The annotation process of the corpora is outlined in 
the next section. 
3.2 Annotation process 
The Ancora coreference annotation process in-
volves: (a) marking of mentions, and (b) marking 
of coreference chains (entities). 
(a) Referential full NPs (including proper nouns) 
and pronouns (including elliptical and clitic pro-
nouns) are the potential mentions of a coreference 
chain.  
(b) In the current task only identity relations 
(coreftype=?ident?) will be considered, which link 
referential NPs that point to the same discourse 
entity. Coreferent mentions are annotated with the 
attribute entity. Mentions that point to the same 
entity share the same entity number. In Figure 1, 
for instance, el reial decret aprovat ahir pel Con-
sell_de_Ministres ?the royal decree approved yes-
terday by the Council of Ministers? is 
entity=?entity9? and la nova norma ?the new rule? 
is also entity=?entity9? because they corefer. 
Hence, mentions referring to the same discourse 
entity all share the same entity number.  
The corpora were annotated by a total of seven 
annotators (qualified linguists) using the An-
CoraPipe annotation tool (Bertran et al, 2008), 
which allows different linguistic levels to be anno-
tated simultaneously and efficiently. AnCoraPipe 
supports XML in-line annotations.  
An initial reliability study was performed on a 
small portion of the Spanish AnCora-CO corpus. 
In that study, eight linguists annotated the corpus 
material in parallel. Inter-annotator agreement was 
computed with Krippendorff?s alpha, achieving a 
result above 0.8. Most of the problems detected 
were attributed either to a lack of training of the 
coders or to ambiguities that are left unresolved in 
the discourse itself. After carrying out this reliabil-
ity study, we opted for annotating the corpora in a 
two-stage process: a first pass in which all mention 
attributes and coreference links were coded, and a 
second pass in which the already annotated files 
were revised. 
 
74
4 Conclusions 
The SemEval-2010 multilingual coreference reso-
lution task has been presented for discussion.  
Firstly, we aim to promote research on coreference 
resolution from a learning-based perspective in a 
multilingual scenario in order to: (a) explore port-
ability issues; (b) analyze language-specific tuning 
requirements; (c) facilitate cross-linguistic com-
parisons between two Romance languages and be-
tween Romance languages and English; and (d) 
encourage researchers to develop linguistic re-
sources ? annotated corpora ? oriented to corefer-
ence resolution for other languages. 
Secondly, given the complexity of the corefer-
ence phenomena we split the coreference resolu-
tion task into two (full coreference chains and 
pronominal resolution), and we propose two dif-
ferent scenarios (gold standard vs. automatically 
generated input information) in order to evaluate to 
what extent the performance of a coreference reso-
lution system varies depending on the quality of 
the other levels of information. 
Finally, given that the evaluation of coreference 
resolution systems is still an open issue, we are 
interested in comparing different coreference reso-
lution metrics: B-cubed and CEAF measures. In 
this way we will be able to evaluate and compare 
the performance of these metrics with respect to 
the task objectives and system rankings. 
Acknowledgments 
This research has been supported by the projects 
Lang2World (TIN2006-15265-C06), TEXT-MESS 
(TIN2006-15265-C04), OpenMT (TIN2006-
15307-C03-02), AnCora-Nom (FFI2008-02691-E), 
and the FPU grant (AP2006-00994) from the Span-
ish Ministry of Education and Science, and the 
funding given by the government of the Generalitat 
de Catalunya.  
References  
Bagga, Amit and Breck Baldwin. 1998. Algorithms for 
scoring coreference chains. In Proceedings of Lan-
guage Resources and Evaluation Conference. 
Bertran, Manuel, Oriol Borrega, Marta Recasens, and 
B?rbara Soriano. 2008. AnCoraPipe: A tool for mul-
tilevel annotation, Procesamiento del Lenguaje Natu-
ral, n. 41: 291-292, SEPLN. 
Denis, Pascal and Jason Baldridge. 2008. Specialized 
models and ranking for coreference resolution. Pro-
ceedings of the Empirical Methods in Natural Lan-
guage Processing (EMNLP 2008). 
Luo, Xiaoqiang. 2005. On coreference resolution per-
formance metrics. Proceedings of HLT/NAACL 2005. 
McCarthy Joseph and Wendy Lehnert.  1995. Using 
decision trees for coreference resolution. Proceed-
ings of the Fourteenth International Joint Conference 
on Artificial Intelligence. 
Mitkov, Ruslan. 1998. Robust pronoun resolution with 
limited knowledge. Proceedings of the 36th Annual 
Meeting of the Association for Computational Lin-
guistics, and 17th International Conference on Com-
putational Linguistics (COLING-ACL98). 
Morton, Thomas. 2000. Using coreference for question 
answering. Proceedings of the 8th Text REtrieval 
Conference (TREC-8). 
Ng, Vincent. 2008. Unsupervised models for corefer-
ence resolution. Proceedings of the Empirical Meth-
ods in Natural Language Processing (EMNLP 2008). 
NIST. 2003. In Proceedings of ACE 2003 workshop. 
Booklet, Alexandria, VA. 
Recasens, Marta. 2008. Towards Coreference Resolu-
tion for Catalan and Spanish. Master Thesis. Univer-
sity of Barcelona.  
Steinberger, Josef, Massimo Poesio, Mijail Kabadjov, 
and Karel Jezek. 2007. Two uses of anaphora resolu-
tion in summarization. Information Processing and 
Management, 43:1663?1680.  
Taul?, Mariona, Ant?nia Mart?, and Marta Recasens. 
2008. AnCora: Multilevel corpora with coreference 
information for Spanish and Catalan. In Proceedings 
of the Language Resources and Evaluation Confer-
ence (LREC 2008). 
van Deemter, Kees and Rodger Kibble. 2000. Squibs 
and Discussions: On coreferring: coreference in 
MUC and related annotation schemes. Computa-
tional Linguistics, 26(4):629-637. 
Vilain, Marc, John Burger, John Aberdeen, Dennis 
Connolly, and Lynette Hirschman. 1995. A model-
theoretic coreference scoring scheme. In Proceedings 
of MUC-6. 
 
75
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 489?500, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Joint Entity and Event Coreference Resolution across Documents
Heeyoung Lee, Marta Recasens, Angel Chang, Mihai Surdeanu, Dan Jurafsky
Stanford University, Stanford, CA 94305
{heeyoung,recasens,angelx,mihais,jurafsky}@stanford.edu
Abstract
We introduce a novel coreference resolution
system that models entities and events jointly.
Our iterative method cautiously constructs
clusters of entity and event mentions using lin-
ear regression to model cluster merge opera-
tions. As clusters are built, information flows
between entity and event clusters through fea-
tures that model semantic role dependencies.
Our system handles nominal and verbal events
as well as entities, and our joint formulation
allows information from event coreference to
help entity coreference, and vice versa. In a
cross-document domain with comparable doc-
uments, joint coreference resolution performs
significantly better (over 3 CoNLL F1 points)
than two strong baselines that resolve entities
and events separately.
1 Introduction
Most coreference resolution systems focus on enti-
ties and tacitly assume a correspondence between
entities and noun phrases (NPs). Focusing on NPs
is a way to restrict the challenging problem of coref-
erence resolution, but misses coreference relations
like the one between hanged and his suicide in (1),
and between placed and put in (2).
1. (a) One of the key suspected Mafia bosses ar-
rested yesterday has hanged himself.
(b) Police said Lo Presti had hanged himself.
(c) His suicide appeared to be related to clan feuds.
2. (a) The New Orleans Saints placed Reggie Bush
on the injured list on Wednesday.
(b) Saints put Bush on I.R.
As (1c) shows, NPs can also refer to events, and
so corefer with phrases other than NPs (Webber,
1988). By being anchored in spatio-temporal dimen-
sions, events represent the most frequent referent of
verbal elements. In addition to time and location,
events are characterized by their participants or ar-
guments, which often correspond with discourse en-
tities. This two-way feedback between events and
their arguments (or entities) is the core of our ap-
proach. Since arguments play a key role in describ-
ing an event, knowing that two arguments corefer
is useful for finding coreference relations between
events, and knowing that two events corefer is use-
ful for finding coreference relations between enti-
ties. In (1), the coreference relation between One
of the key suspected Mafia bosses arrested yesterday
and Lo Presti can be found by knowing that their
predicates (i.e., has hanged and had hanged) core-
fer. On the other hand, the coreference relations be-
tween the arguments Saints and Bush in (2) helps
to determine the coreference relation between their
predicates placed and put.
In this paper, we take a holistic approach to coref-
erence. We annotate a corpus with cross-document
coreference relations for nominal and verbal men-
tions. We focus on both intra and inter-document
coreference because this scenario is at the same time
more challenging and more relevant to real-world
applications such as news aggregation. We use this
corpus to train a model that jointly addresses refer-
ences to both entities and events across documents.
The contributions of this work are the following:
? We introduce a novel approach for entity and
event coreference resolution. At the core of
489
our approach is an iterative algorithm that cau-
tiously constructs clusters of entity and event
mentions using linear regression to model clus-
ter merge operations. Importantly, our model
allows information to flow between clusters of
both types through features that model context
using semantic role dependencies.
? We annotate and release a new corpus with
coreference relations between both entities and
events across documents. The relations anno-
tated are both intra and inter-document, which
more accurately models real-world scenarios.
? We evaluate our cross-document coreference
resolution system on this corpus and show that
our joint approach significantly outperforms
two strong baselines that resolve entities and
events separately.
2 Related Work
Entity coreference resolution is a well studied prob-
lem with many successful techniques for identify-
ing mention clusters (Ponzetto and Strube, 2006;
Haghighi and Klein, 2009; Stoyanov et al2009;
Haghighi and Klein, 2010; Raghunathan et al2010;
Rahman and Ng, 2011, inter alia). Most of these
techniques focus on matching compatible noun pairs
using various syntactic and semantic features, with
efforts targeted toward improving features and clus-
tering models.
Prior work showed that models that jointly resolve
mentions across multiple entities result in better per-
formance than simply resolving mentions in a pair-
wise fashion (Denis and Baldridge, 2007; Poon and
Domingos, 2008; Wick et al2008; Lee et al2011,
inter alia). A natural extension is to perform coref-
erence jointly across both entities and events. Yet
there has been little attempt in this direction.
We know of only limited work that incorporates
event-related information in entity coreference, typ-
ically by incorporating the verbs in context as fea-
tures. For instance, Haghighi and Klein (2010) in-
clude the governor of the head of nominal mentions
as features in their model. Rahman and Ng (2011)
also used event-related information by looking at
which semantic role the entity mentions can have
and the verb pairs of their predicates. We confirm
that such features are useful but also show that the
complementary features for verbal mentions lead to
even better performance, especially when event and
entity clusters are jointly modeled.
Compared to the extensive work on entity coref-
erence, the related problem of event coreference re-
mains relatively under-explored, with minimal work
on how entity and event coreference can be con-
sidered jointly on an open domain. Early work on
event coreference for MUC (Humphreys et al1997;
Bagga and Baldwin, 1999) focused on scenario-
specific events. More recently, there have been
approaches that looked at event coreference for
wider domains. Chen and Ji (2009) proposed us-
ing spectral graph clustering to cluster events. Be-
jan and Harabagiu (2010) proposed a nonparamet-
ric Bayesian model for open-domain event resolu-
tion. However, most of this prior work focused only
on event coreference, whereas we address both en-
tities and events with a single model. Humphreys
et al1997) considered entities as well as events,
but due to the lack of a corpus annotated with event
coreference, their approach was only evaluated im-
plicitly in the MUC-6 template filling task. To our
knowledge, the only previous work that considered
entity and event coreference resolution jointly is
He (2007), but limited to the medical domain and
focused on just five semantic categories.
3 Architecture
Following the intuition introduced in Section 1, our
approach iteratively builds clusters of event and en-
tity mentions jointly. As more information becomes
available (e.g., finding out that two verbal mentions
have arguments that belong to the same entity clus-
ter), the features of both entity and event mentions
are re-generated, which prompts future clustering
operations. Our model follows a cautious (or ?baby
steps?) approach, which we previously showed to be
successful for entity coreference resolution (Raghu-
nathan et al2010; Lee et al2011). However,
unlike our previous work, which used deterministic
rules, in this paper we learn a coreference resolution
model using linear regression. Algorithm 1 summa-
rizes the flow of the proposed algorithm. We detail
its steps next. We describe the training procedure in
Section 4 and the features used in Section 5.
490
Algorithm 1: Joint Coreference Resolution
input : set of documents D
input : coreference model ?
// clusters of mentions:
E= {}1
// clusters of documents:
C = clusterDocuments(D)2
foreach document cluster c in C do3
// all mentions in one doc cluster:
M = extractMentions(c)4
// singleton mention clusters:
E ? = buildSingletonClusters(M)5
// high-precision deterministic sieves:
E ? = applyHighPrecisionSieves(E ?)6
// iterative event/entity coreference:
while ? e1, e2 ? E ?s.t. score(e1, e2,?) > 0.5 do7
(e1, e2) = arg max e1,e2?E? score(e1, e2,?)8
E ? = merge(e1, e2, E ?)9
// pronoun sieve:
E ? = applyPronounSieve(E ?)10
// append to global output:
E = E + E ?11
output : E
3.1 Document Clustering
Our approach starts with several steps that reduce
the search space for the actual coreference resolution
task. The first is document clustering, which clusters
the set of input documents (D) into a set of docu-
ment clusters (C). In the subsequent steps we only
cluster mentions that appear in the same document
cluster. We found this to be very useful in practice
because, in addition to reducing the search space, it
provides a word sense disambiguation mechanism
based on corpus-wide topics. For example, with-
out document clustering, our algorithm may decide
to cluster two mentions of the verb hit, but know-
ing that one belongs to a cluster containing earth-
quake reports and the other to a cluster with reports
on criminal activities, this decision can be avoided.1
Any non-parametric clustering algorithm can be
used in this step. In this paper, we used the algo-
rithm proposed by Surdeanu et al2005). This algo-
rithm is an Expectation Maximization (EM) variant
where the initial points (and the number of clusters)
are selected from the clusters generated by a hierar-
chical agglomerative clustering algorithm using ge-
1Since different mentions of the verb say in the same topic
might refer to different events, they are only merged if they have
coreferent arguments.
ometric heuristics. This algorithm performs well on
our data. For example, in the training dataset, only
two topics (handling different earthquake events) are
incorrectly merged into the same cluster.
3.2 Mention Extraction
In this step (4 in Algorithm 1) we extract nominal,
pronominal, and verbal mentions. We extract nom-
inal and pronominal mentions using the mention
identification component in the publicly download-
able Stanford coreference resolution system (Raghu-
nathan et al2010; Lee et al2011). We consider
as verbal mentions all words whose part of speech
starts with VB, with the exception of some auxil-
iary/copulative verbs (have, be and seem). For each
of the identified mentions we build a singleton clus-
ter (step 5 in Algorithm 1).
Crucially, we do not make a formal distinction be-
tween entity and event mentions. This distinction is
not trivial to implement (e.g., is the noun earthquake
an entity or an event mention?) and an imperfect
classification would negatively affect the following
coreference resolution. Instead, we simply classify
mentions into verbal or nominal, and use this dis-
tinction later during feature generation (Section 5).
To compare event nouns (e.g., development) with
verbal mentions, the ?derivationally related form?
relation in WordNet is used.
3.3 High-precision Entity Resolution Sieves
To further reduce the problem?s search space, in
step 6 of Algorithm 1 we apply a set of high-
precision filters from the Stanford coreference res-
olution system. This system is a collection of deter-
ministic models (or ?sieves?) for entity coreference
resolution that incorporate lexical, syntactic, seman-
tic, and discourse information. These sieves are ap-
plied from higher to lower precision. As clusters are
built, information such as mention gender and num-
ber is propagated across mentions in the same clus-
ter, which helps subsequent decisions. The Stanford
system obtained the highest score at the CoNLL-
2011 shared task on English coreference resolution.
For this step, we selected all the sieves from the
Stanford system with the exception of the pronoun
resolution sieve. All the remaining sieves (listed
in Table 1) have high precision because they em-
ploy linguistic heuristics with little ambiguity, e.g.,
491
High-precision sieves
Discourse processing sieve
Exact string match sieve
Relaxed string match sieve
Precise constructs sieve (e.g., appositives)
Strict head match sieves
Proper head noun match sieve
Relaxed head matching sieve
Table 1: Deterministic sieves in step 6 of Algorithm 1.
one sieve clusters together two entity mentions only
when they have the same head word. Note that all
these heuristics were designed for within-document
coreference. They work well in our context be-
cause we apply them in individual document clus-
ters, where the one-sense-per-discourse principle
still holds (Yarowsky, 1995).
Importantly, these sieves do not address verbal
mentions. That is, all verbal mentions are still in sin-
gleton clusters after this step. Furthermore, none of
these sieves use features that facilitate the joint reso-
lution of nominal and verbal mentions (e.g., features
from semantic role frames). All these limitations are
addressed next.
3.4 Iterative Entity/Event Resolution
In this stage (steps 7 ? 9 in Algorithm 1), we con-
struct entity and event clusters using a cautious or
?baby steps? approach. We use a single linear re-
gressor (?) to model cluster merge operations be-
tween both verbal and nominal clusters. Intuitively,
the linear regressor models the quality of the merge
operation, i.e., a score larger than 0.5 indicates that
more than half of the mention pairs introduced by
this merge are correct. We discuss the training pro-
cedure that yields this scoring function in Section 4.
In each iteration, we perform the merge operation
that has the highest score. Once two clusters are
merged (step 9) we regenerate all the mention fea-
tures to reflect the current clusters. We stop when no
merging operation with an overall benefit is found.
This iterative procedure is the core of our joint
coreference resolution approach. This algorithm
transparently merges both entity and event men-
tions and, importantly, allows information to flow
between clusters of both types as merge operations
take place. For example, assume that during iter-
ation i we merge the two hanged verbs in the first
example in Section 1 (because they have the same
lemma). Because of this merge, in iteration i+ 1 the
nominal mentions Lo Presti and One of the key sus-
pected Mafia bosses have the same semantic role for
verbs assigned to the same cluster. This is a strong
hint that these two nominal mentions belong to the
same cluster. Indeed, the feature that models this
structure received one of the highest weights in our
linear regression model (see Section 7).
3.5 Pronoun Sieve
Our approach concludes with the pronominal coref-
erence resolution sieve from the Stanford system.
This sieve is necessary because our current reso-
lution algorithm ignores mention ordering and dis-
tance (i.e., in step 7 we compare all clusters regard-
less of where their mentions appear in the text). As
previous work has proved, the structure of the text is
crucial for pronominal coreference (Hobbs, 1978).
For this reason, we handle pronouns outside of the
main algorithm block.
4 Training the Cluster Merging Model
Two observations drove our choice of model and
training algorithm. First, modeling the merge op-
eration as a classification task is not ideal, because
only a few of the resulting clusters are entirely cor-
rect or incorrect. In practice, most of the clusters
will contain some mention pairs that are correct and
some that are not. Second, generating training data
for the merging model is not trivial: a brute force
approach that looks at all the possible combinations
is exponential in the number of mentions. This is
both impractical and unnecessary, as some of these
combinations are unlikely to be seen in practice.
We address these observations with Algorithm 2.
The algorithm uses gold coreference labels to train a
linear regressor that models the quality of the clus-
ters produced by merge operations. We define the
quality score q of a new cluster as the percentage of
new mention pairs (i.e., not present in either one of
the clusters to be merged) that are correct:
q =
linkscorrect
linkscorrect + linksincorrect
(1)
where links(in)correct is the number of newly intro-
duced (in)correct pairwise mention links when two
clusters are merged.
492
Algorithm 2: Training Procedure
input : set of documents D
input : correct mention clusters G
C = clusterDocuments(D)1
// linear regression coreference model:
? = assignInitialWeights(C,G)2
// repeat for T epochs:
for t = 1 to T do3
// training data for linear regressor:
? = {}4
foreach document cluster c in C do5
M = extractMentions(c)6
E = buildSingletonClusters(M)7
E = applyHighPrecisionSieves(E)8
// gather training examples
// as clusters are built:
while ? e1, e2 ? Es.t. sco(e1, e2,?) > 0.5 do9
forall e?1, e
?
2 ? E do10
q = qualityOfMerge(e?1, e
?
2,G)11
? = append(e?1, e
?
2, q,?)12
(e1, e2) = arg max e1,e2?E sco(e1, e2,?)13
E = merge(e1, e2, E)14
// train using data from last epoch:
?? = trainLinearRegressor(?)15
// interpolate with older model:
? = ?? + (1? ?)??16
output : ?
We address the potential explosion in training data
size by considering only merge operations that are
likely to be inspected by the algorithm as it runs.
To achieve this, Algorithm 2 repeatedly runs the ac-
tual clustering algorithm (as given by the current
model ?) over the training dataset (steps 5 ? 14).2
When the algorithm iteratively constructs its clus-
ters (steps 9 ? 14), we generate training data from
all possible cluster pairs available during a particular
iteration (steps 10 ? 12). For each pair, we compute
its score using Equation 1 (step 11) and add it to the
training corpus ? (step 12). Note that this avoids in-
specting many of the possible cluster combinations:
once a cluster is built (e.g., during the previous iter-
ations or by the deterministic sieves in step 8), we
do not generate training data from its members, but
rather treat it as an atomic unit. On the other hand,
our approach generates more training data than on-
line learning, which trains using only the actual de-
cisions taken during inference in each iteration (i.e.,
2We skip the pronoun sieve here because it does not affect
the decisions taken during the iterative resolution steps.
the pair (e1, e2) in step 13).
After each epoch we have a new training cor-
pus ?, which we use to train the new linear regres-
sion model ?? (step 15), which is then interpolated
with the old one (step 16).
Our training procedure is similar in spirit to trans-
formation based learning (TBL) (Brill, 1995). Sim-
ilarly to TBL, our approach repeatedly applies the
model over the training data and attempts to mini-
mize the error rate of the current model. However,
while TBL learns rules that directly minimize the
current error rate, our approach achieves this indi-
rectly, by incorporating the reduction in error rate in
the score of the generated datums. This allows us
to fit a linear regression to this task, which, as dis-
cussed before, is a better model for this task.
Just like any hill-climbing algorithm, our ap-
proach has the risk of converging to a local max-
imum. To mitigate this risk, we do not initialize
our model ? with random weights, but rather use
hints from the deterministic sieves. This procedure
(listed in step 2) runs the high-precision sieves in-
troduced in Section 3.3 and, just like the data gen-
eration loop in Algorithm 2, creates training exam-
ples from the clusters available after every merge
operation. Since these deterministic models address
only nominal clusters, at the end we generate train-
ing data for events by inspecting all the pairs of sin-
gleton verbal clusters. Using this data, we train the
initial linear regression model.
We trained our model using L2 regularized linear
regression with a regularization coefficient of 1.0.
We did not tune the regularization coefficient. We
ran the training algorithm for 10 epochs, although
we observed minimal changes after three epochs.
We tuned the interpolation weight (?) to a value
of 0.7 using our development corpus.
5 Features
We list in Table 2 the features used by the lin-
ear regression model. As the table indicates, our
feature set relies heavily on semantic roles, which
were extracted using the SwiRL semantic role la-
beling (SRL) system (Surdeanu et al2007).3 Be-
cause SwiRL addresses only verbal predicates, we
extended it to handle nominal predicates. In this
3http://www.surdeanu.name/mihai/swirl/
493
Feature Name
Applies to
Entities (E)
or Events (V)
Description and Example
Entity Heads E
Cosine similarity of the head-word vectors of two clusters. The head-word vector
stores the head words of all mentions in a cluster and their frequencies. For example,
the vector for the three-mention cluster {Barack Obama, President Obama, US
president}, is {Obama:2, president:1}.
Event Lemmas V
Cosine similarity of the lemma vectors of two clusters. For example, the lemma
vector for the cluster {murdered, murders, hitting} is {murder:2, hit:1}.
Links between
Synonyms
E, V
The percentage of newly-introduced mention links after the merge that are WordNet
synonyms (Fellbaum, 1998). For example, when merging the following two clus-
ters, {hit, strike} and {strike, join, say}, two out of the six new links are between
words that belong to the same WordNet synset: (hit ? strike) and (strike ? strike).
Number of Coreferent
Arguments or
Predicates
E, V
The total number of shared arguments and predicates between mentions in the
two clusters. We use the cluster IDs of the corresponding arguments/predicates
to check for identity. For example, when comparing the event clusters {bought}
and {acquired}, extracted from the sentences [AMD]Arg0 bought [ATI]Arg1 and
[AMD]Arg0 acquired [ATI]Arg1, the value of this feature is 2 because the two men-
tions share one Arg0 and one Arg1 argument (assuming that the clusters {AMD,
AMD} and {ATI, ATI} were previously created). For entity clusters, this feature
counts the number of coreferent predicates. In addition to PropBank-style roles, for
event mentions we also include the closest left and right entity mentions in order to
capture any arguments missed by the SRL system.
Coreferent Arguments
in a Specific Role?
E, V
Indicator feature set to 1 if the two clusters have at least one coreferent argument in
a given role. We generate one variant of this feature for each argument label, e.g.,
Arg0, Arg1, etc. For example, the value of this feature for Arg0 for the clusters
{bought} and {acquired} in the above example is 1.
Coreferent Predicate in
a Specific Role?
E
Indicator feature set to 1 if the two clusters have at least one coreferent predicate for
a given role. For example, for the clusters {the man} and {the person}, extracted
from the sentences helped [the man]Arg1 and helped [the person]Arg1, the value of
this feature is 1 if the two helped verbs were previously clustered together.
2nd Order Similarity of
Mention Words
E
Cosine similarity of vectors containing words that are distributionally similar to
words in the cluster mentions. We built these vectors by extracting the top-ten
most-similar words in Dekang Lin?s similarity thesaurus (Lin, 1998) for all the
nouns/adjectives/verbs in a cluster. For example, for the singleton cluster {a new
home}, we construct this vector by expanding new and home to: {new:1, original:1,
old:1, existing:1, current:1, unique:1, modern:1, different:1, special:1, major:1,
small:1, home:1, house:1, apartment:1, building:1, hotel:1, residence:1, office:1,
mansion:1, school:1, restaurant:1, hospital:1 }.
Number; Animacy;
Gender; NE Label
E
Cosine similarity of number, gender, animacy, and NE label vectors. For example,
the number and gender vectors for the two-mention cluster {systems, a pen} are
Number = {singular:1, plural:1}, Gender = {neutral:2}.
Table 2: List of features used when comparing two clusters. If any of the two clusters contains a verbal mention we
consider the merge an operation between event (V) clusters; otherwise it is a merge between entity (E) clusters. We
append to all entity features the suffix Proper or Common based on the type of the head word of the first mention in
each of the two clusters. We use the suffix Proper only if both head words are proper nouns.
paper we used a single heuristic: the possessor of
a nominal event?s predicate is marked as its Arg0,
e.g., Logan is the Arg0 to run in Logan?s run.4
4A principled solution to this problem is to use an SRL sys-
tem for nominal predicates trained using NomBank (Meyers et
al., 2004). We will address this in future work.
494
We extracted named entity labels using the named
entity recognizer from the Stanford CoreNLP suite.
6 Evaluation
6.1 Corpus
The training and test data sets were derived from
the EventCorefBank (ECB) corpus5 created by Be-
jan and Harabagiu (2010) to study event coreference
since standard corpora such as OntoNotes (Pradhan
et al2007) contain a small number of annotated
event clusters. The ECB corpus consists of 482 doc-
uments from Google News clustered into 43 topics,
where a topic is described as a seminal event. The
reason for including comparable documents was to
increase the number of cross-document coreference
relations. Bejan and Harabagiu (2010) only anno-
tated a selection of events.
For the purpose of our study, we extended the
original corpus in two directions: (i) fully anno-
tated sentences, and (ii) entity coreference relations.
In addition, we removed relations other than coref-
erence (e.g., subevent, purpose, related, etc.) that
had been originally annotated. We revised and com-
pleted the original annotation by annotating every
entity and event in the sentences that were (partially)
annotated. The annotation was performed by four
experts, using the Callisto annotation tool.6 The
annotation guidelines and the generated corpus are
available here.7
Our annotation of the ECB corpus followed the
OntoNotes (Pradhan et al2007) standard for coref-
erence annotation, with a few extensions to handle
events. For nouns, we annotated full NPs (with all
modifiers), excluding appositive phrases and nomi-
nal predicates. Only premodifiers that were proper
nouns or possessive phrases were annotated. For
events, we annotated the semantic head of the verb
phrase. We extended the OntoNotes guidelines by
also annotating singletons (but we do not score
them; see below), and by including all events men-
tions (not only those mentioned at least once with an
NP). This required us to be specific with respect to:
5http://faculty.washington.edu/bejan/
data/ECB1.0.tar.gz
6http://callisto.mitre.org
7http://nlp.stanford.edu/pubs/
jcoref-corpus.zip
Training Dev Test Total
# Topics 12 3 28 43
# Documents 112 39 331 482
# Entities 459 46 563 1068
# Entity Mentions 1723 259 3465 5447
# Events 300 30 444 774
# Event Mentions 751 140 1642 2533
Table 3: Corpus statistics.
?ENTITY COREFID=?26?? A publicist ?/ENTITY? ?EVENT
COREFID=?4?? says ?/EVENT? ?ENTITY COREFID=?23??
Tara Reid ?/ENTITY? has ?EVENT COREFID=?3?? checked
?/EVENT? ?ENTITY COREFID=?23?? herself ?/ENTITY? ?EVENT
COREFID=?3*?? into ?/EVENT? ?ENTITY COREFID=?28?? rehab
?/ENTITY?.
Figure 1: Annotation example.
Light verbs Verbs such as give and make followed
by a noun (e.g., make an offer) were not anno-
tated, but the noun was.
Phrasal verbs We annotated the verb together with
the preposition or adverb (e.g., check in).
Idioms They were annotated with all their elements
(e.g., booze it up).
The first topic was annotated by all four anno-
tators as burn-in. Afterwards, annotation disagree-
ments were resolved between all annotators and the
next three topics were annotated again by all four an-
notators to measure agreement. Following Passon-
neau (2004), we computed an inter-annotator agree-
ment of ? = 0.55 (Krippendorff, 2004) on these
three topics, indicating moderate agreement among
the annotators. Given the complexity of the task, we
consider this to be a good score. For example, the
average of the CoNLL F1 between any two annota-
tors is 73.58, which is much higher than the system
scores reported in the literature.
After annotating the four topics, disagreements
were resolved again and all the documents in the
four topics were corrected to match the consensus.
The rest of the corpus was split between the four an-
notators, and each document was annotated by a sin-
gle annotator. Figure 1 shows an example. Table 3
shows the corpus statistics, including the training,
development (dev) and test set splits. The dev topics
were used for tuning the interpolation parameter ?
from Section 4.
495
MUC B3 CEAF-?4 BLANC
System R P F1 R P F1 R P F1 R P F1 CoNLL F1
Baseline 1
Wo/ SRL
Entity 47.4 72.3 57.2 44.1 82.7 57.5 42.5 21.9 28.9 60.1 78.3 64.8 47.9
Event 56.0 56.8 56.4 59.8 71.9 65.3 32.2 31.6 31.9 63.5 68.8 65.7 51.2
Both 49.9 75.4 60.0 44.9 83.9 58.5 46.2 23.3 31.0 60.9 81.2 66.1 49.8
Baseline 2
With SRL
Entity 52.7 73.0 61.2 48.6 80.8 60.7 41.8 24.1 30.6 63.4 78.4 68.2 50.8
Event 59.2 57.0 58.1 62.3 70.8 66.3 31.5 33.2 32.3 65.4 68.0 66.6 52.2
Both 54.5 76.4 63.7 48.7 82.6 61.3 46.3 25.5 32.9 63.9 81.1 69.2 52.6
This paper
Entity 60.7 70.6 65.2 55.5 74.9 63.7 39.3 29.5 33.7 66.9 79.6 71.5 54.2
Event 62.7 62.8 62.7 62.5 73.9 67.7 34.0 33.9 33.9 67.6 78.5 71.7 54.8
Both 61.2 75.9 67.8 53.9 79.0 64.1 45.2 30.0 35.8 67.1 82.2 72.3 55.9
Table 4: Performance of the two baselines and our model. We report scores for entity clusters, event clusters and the
complete task using five metrics.
6.2 Evaluation
We use five coreference evaluation metrics widely
used in the literature:
MUC (Vilain et al1995) Link-based metric which
measures how many predicted and gold clus-
ters need to be merged to cover the gold and
predicted clusters, respectively.
B3 (Bagga and Baldwin, 1998) Mention-based
metric which measures the proportion of over-
lap between predicted and gold clusters for a
given mention.
CEAF (Luo, 2005) Entity-based metric that, unlike
B3, enforces a one-to-one alignment between
gold and predicted clusters. We employ the
entity-based version of CEAF.
BLANC (Recasens and Hovy, 2011) Metric based
on the Rand index (Rand, 1971) that consid-
ers both coreference and non-coreference links
to address the imbalance between singleton and
coreferent mentions.
CoNLL F1 Average of MUC, B3, and CEAF-?4.
This was the official metric in the CoNLL-2011
shared task (Pradhan et al2011).
We followed the CoNLL-2011 evaluation methodol-
ogy, that is, we removed all singleton clusters, and
apposition/copular relations before scoring.
We evaluated the systems on three different set-
tings: only on entity clusters, only on event clus-
ters, and on the complete task, i.e., both entities and
events. Note that the gold corpus separates clusters
into entity and event clusters (see Table 3), but our
system does not make this distinction at runtime.
In order to compute the entity-only and event-only
scores in Table 4, we implemented the following
procedure: (a) when scoring entity clusters, we re-
moved all mentions that were found to be coreferent
with at least one gold event mention and not coref-
erent with any gold entity mentions; and (b) we per-
formed the opposite action when scoring event clus-
ters. This procedure is necessary because our men-
tion identification component is not perfect, i.e., it
generates mentions that do not exist in the gold an-
notation. Furthermore, this procedure is conserva-
tive with respect to the clustering errors of our sys-
tem, e.g., all spurious mentions that our system in-
cludes in a cluster with a gold entity mention are
considered for the entity score, regardless of their
gold type (event or entity).
6.3 Results
Table 4 compares the performance of our system
against two strong baselines that resolve entities and
events separately. Baseline 1 uses a modified Stan-
ford coreference resolution system after our doc-
ument clustering and mention identification steps.
Because the original Stanford system implements
only entity coreference, we extended it with an extra
sieve that implements lemma matching for events.
This additional sieve merges two verbal clusters
(i.e., clusters that contain at least one verbal men-
tion) or a verbal and a nominal cluster when at least
two lemmas of mention head words are the same be-
tween clusters, e.g., helped and the help.
The second baseline adds two more sieves to
Baseline 1. Both these sieves model entity and event
496
contextual information using semantic roles. The
first sieve merges two nominal clusters when two
mentions in the respective clusters have the same
head words and two mentions (possibly with dif-
ferent heads) modify with the same role label two
predicates that have the same lemma. For exam-
ple, this sieve merges the clusters {Obama, the pres-
ident} (seen in the text [Obama]Arg0 attended and
[the president]Arg1 was elected) and {Obama} (seen
in the text [Obama]Arg1 was elected), because they
share a mention with the same head word (Obama)
and two mentions modify with the same role (Arg1)
predicates with the same lemma (elect). The sec-
ond sieve implements the complementary action for
event clusters. That is, it merges two verbal clusters
when at least two mentions have the same lemma
and at least two mentions have semantic arguments
with the same role label and the same lemma.
7 Discussion
The first block in Table 4 indicates that lemma
matching is a strong baseline for event resolution.
Most of the event scores for Baseline 1 are actually
higher than the corresponding entity scores, which
were obtained using the highest ranked system at the
CoNLL-2011 shared task (Lee et al2011). Adding
contextual information using semantic roles (Base-
line 2) helps both entities and events. The CoNLL
F1 for Baseline 2 increases almost 3 points for enti-
ties and 1 point for events. This demonstrates that
local syntactico-semantic context is important for
coreference resolution even in a cross-document set-
ting and that the current state-of-the-art in SRL can
model this context accurately.
The best scores (almost unanimously) are ob-
tained by the model proposed in this paper, which
scores 3.4 CoNLL F1 points higher than Baseline 2
for entities, and 2.6 points higher for events. For the
complete task, our approach scores 3.3 CoNLL F1
points higher than Baseline 2, and 6.1 points higher
than Baseline 1. This demonstrates that a holistic
approach to coreference resolution improves the res-
olution of both entities and events more than models
that address aspects of the task separately. To fur-
ther understand our experiments, we listed the top
five entity/event features with the highest weights in
our model in Table 5. The table indicates that six out
of the ten features serve the purpose of passing infor-
Entity Feature Weight
Entity Heads ? Proper 1.10
Coreferent Predicate for ArgM-LOC ? Common 0.45
Entity Heads ? Common 0.36
Coreferent Predicate for Arg0 ? Proper 0.29
Coreferent Predicate for Arg2 ? Common 0.28
Event Feature Weight
Event Lemmas 0.45
Coreferent Argument for Arg1 0.19
Links between Synonym 0.16
Coreferent Argument for Arg2 0.13
Number of Coreferent Arguments 0.07
Table 5: Top five features with the highest weights.
mation between entity and event clusters. For exam-
ple, the ?Coreferent Argument for Arg1? feature is
triggered when two event clusters have Arg1 argu-
ments that already belong to the same entity cluster.
This allows information from previous entity coref-
erence operations to impact future merges of event
clusters. This is the crux of our iterative approach to
joint coreference resolution.
Finally, we performed an error analysis by man-
ually evaluating 100 errors. We distinguished nine
major types of errors. Their ratios together with a
description and an example are given in Table 6.
This work demonstrates that an approach that
jointly models entities and events is better for cross-
document coreference resolution. However, our
model can be improved. For example, document
clustering and coreference resolution can be solved
jointly, which we expect would improve both tasks.
Furthermore, our iterative coreference resolution
procedure (Algorithm 1) could be modified to ac-
count for mention ordering and distance, which
would allow us to include pronominal resolution in
our joint model, rather than addressing it with a sep-
arate deterministic sieve.
8 Conclusion
We have presented a holistic model for cross-
document coreference resolution that jointly solves
references to events and entities by handling both
nominal and verbal mentions. Our joint resolution
algorithm allows event coreference to help improve
entity coreference, and vice versa. In addition, our
iterative procedure, based on a linear regressor that
models the quality of cluster merges, allows each
497
Error Type (Ratio)
Description
Example
Pronoun resolution
(36%)
The pronoun is incorrectly resolved by the pronominal sieve of the Stanford deterministic entity
system. These errors include (only a small number of) event pronouns.
He said Timmons aimed and missed his target.
Semantics beyond
role frames
(20%)
The semantics of the coreference relation cannot be captured by role frames or WordNet.
Israeli forces on Tuesday killed at least 40 people . . . The Israeli army said the UN school in the
Jabaliya refugee camp was hit . . . and that the dead included a number of Hamas militants.
Arguments of
nominal events
(17%)
The arguments of two nominal events are not detected and thus not coreferred.
The attack on the school has caused widespread shock across Israel . . . while Israeli forces on
Tuesday killed at least 40 people during an attack on a United Nations-run school in Gaza.
Cascaded errors
(7%)
Entities or events are not coreferred due to errors in a previous merge iteration in the same
semantic frame. In the example below, we failed to link the two die verbs, which leads to the
listed entity error.
An Australian climber who survived two nights stuck on Mount Cook after seeing his brother
die . . . Dr Mark Vinar, 43, is presumed dead . . .
Initial high-precision
sieves
(6%)
An error made by the initial high-precision entity resolution sieves is propagated to our model.
Timmons told police he fired when he thought he saw someone in the other group reach for
a gun . . . 15-year-old Timmons was at the scene of the shooting and had a gun.
Phrasal verbs
(6%)
The meaning of a phrasal verb is not captured.
A relative unknown will take over the title role of Doctor Who . . . But the casting of Smith is
a stroke of genius.
Linear regression
(4%)
Recall error made by the regression model when the features are otherwise correct.
The Interior Department on Thursday issued ?revised? regulations . . . Interior Secretary Dirk
Kempthorne announced major changes . . .
Mention detection
(3%)
The mention detection module detects a spurious mention.
Police have arrested a man . . . in the parking lot crosswalk at Sam?s Club in Bloomington.
SRL
(1%)
The SRL system fails to label the semantic role. In this example, jail is detected as the ArgM-
MNR of hanged instead of ArgM-LOC.
A Mafia boss in Palermo hanged himself in jail.
Table 6: Error analysis. Mentions to be resolved are in bold face, correct antecedents are in italics, and our system?s
predictions are underlined.
merging state to benefit from the previous merged
entity and event mentions. This approach allows us
to start with a set of high-precision coreference rela-
tions and gradually add new ones to increase recall.
The experimental evaluation shows that our coref-
erence algorithm gives markedly better F1 for both
entities and events, outperforming two strong base-
lines that handle entities and events separately, mea-
sured by all the standard measures: MUC, B3,
CEAF-?4, BLANC and the official CoNLL-2011
metric. This is noteworthy since each measure has
been shown to place primary emphasis in evaluating
a different aspect of the coreference resolution task.
Our system is tailored for cross-document coref-
erence resolution on a corpus that contains news ar-
ticles that repeatedly report on a smaller number of
topics. This makes it particularly suitable for real-
world applications such as multi-document summa-
rization and cross-document information extraction.
We also release our labeled corpus to facilitate ex-
tensions and comparisons to our work.
Acknowledgements
We acknowledge the support of Defense Advanced Re-
search Projects Agency (DARPA) Machine Reading Pro-
gram under Air Force Research Laboratory (AFRL)
prime contract no. FA8750-09-C-0181. Any opinions,
findings, and conclusion or recommendations expressed
in this material are those of the author(s) and do not nec-
essarily reflect the view of the DARPA, AFRL, or the US
government. MR is supported by a Beatriu de Pino?s post-
doctoral scholarship (2010 BP-A 00149) from Generali-
tat de Catalunya. AC is supported by a SAP Stanford
Graduate Fellowship. We also gratefully thank Cosmin
Bejan for sharing his code and the useful discussions.
498
References
Amit Bagga and Breck Baldwin. 1998. Algorithms
for scoring coreference chains. In Proceedings of
the LREC 1998 Workshop on Linguistic Coreference,
pages 563?566.
Amit Bagga and Breck Baldwin. 1999. Cross-document
event coreference: Annotations, experiments, and ob-
servations. In Proceedings of the ACL 1999 Workshop
on Coreference and Its Applications, pages 1?8.
Cosmin Bejan and Sanda Harabagiu. 2010. Unsuper-
vised Event Coreference Resolution with Rich Lin-
guistic Features. In Proceedings of ACL 2010, pages
1412?1422.
Eric Brill. 1995. Transformation-based error-driven
learning and natural language processing: a case study
in part of speech tagging. Computational Linguistics,
21(4):543?565.
Zheng Chen and Heng Ji. 2009. Graph-based event
coreference resolution. In Proceedings of the ACL-
IJCNLP 2009 Workshop on Graph-based Methods for
Natural Language Processing, pages 54?57.
Pascal Denis and Jason Baldridge. 2007. Joint determi-
nation of anaphoricity and coreference resolution us-
ing integer programming. In Proceedings of NAACL-
HLT 2007.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press, Cambridge.
Aria Haghighi and Dan Klein. 2009. Simple coreference
resolution with rich syntactic and semantic features. In
Proceedings of EMNLP 2009, pages 1152?1161.
Aria Haghighi and Dan Klein. 2010. Coreference resolu-
tion in a modular, entity-centered model. In Proceed-
ings of HLT-NAACL 2010, pages 385?393.
Tian He. 2007. Coreference Resolution on Entities and
Events for Hospital Discharge Summaries. Thesis,
Massachusetts Institute of Technology.
Jerry R. Hobbs. 1978. Resolving pronoun references.
Lingua, 44(4):311?338.
Kevin Humphreys, Robert Gaizauskas, and Saliha Az-
zam. 1997. Event coreference for information extrac-
tion. In Proceedings of the Workshop On Operational
Factors In Practical Robust Anaphora Resolution For
Unrestricted Texts, pages 75?81.
Klaus Krippendorff. 2004. Content Analysis: An In-
troduction to its Methodology. Sage, Thousand Oaks,
CA, second edition.
Heeyoung Lee, Yves Peirsman, Angel Chang, Nathanael
Chambers, Mihai Surdeanu, and Dan Jurafsky. 2011.
Stanford?s multi-pass sieve coreference resolution sys-
tem at the CoNLL-2011 shared task. In Proceedings
of CoNLL 2011: Shared Task, pages 28?34.
Dekang Lin. 1998. Automatic retrieval and clustering of
similar words. In Proceedings of COLING-ACL 1998,
pages 768?774.
Xiaoqiang Luo. 2005. On coreference resolution perfor-
mance metrics. In Proceedings of HLT-EMNLP 2005,
pages 25?32.
A. Meyers, R. Reeves, C. Macleod, R. Szekely, V. Zielin-
ska, B. Young, and R. Grishman. 2004. The Nom-
Bank project: an interim report. In Proceedings of the
HLT-NAACL 2004 Workshop on Frontiers in Corpus
Annotation, pages 24?31.
Rebecca Passonneau. 2004. Computing reliability for
coreference annotation. In Proceedings of LREC
2004, pages 1503?1506.
Simone Paolo Ponzetto and Michael Strube. 2006.
Exploiting semantic role labeling, WordNet and
Wikipedia for coreference resolution. In Proceedings
of HLT-NAACL 2006, pages 192?199.
Hoifung Poon and Pedro Domingos. 2008. Joint unsu-
pervised coreference resolution with Markov logic. In
Proceedings of EMNLP 2008, pages 650?659.
Sameer S. Pradhan, Lance Ramshaw, Ralph Weischedel,
Jessica MacBride, and Linnea Micciulla. 2007. Un-
restricted coreference: Identifying entities and events
in OntoNotes. In Proceedings of ICSC 2007, pages
446?453.
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Nianwen Xue.
2011. CoNLL-2011 shared task: Modeling unre-
stricted coreference in OntoNotes. In Proceedings of
CoNLL 2011: Shared Task, pages 1?27.
Karthik Raghunathan, Heeyoung Lee, Sudarshan Ran-
garajan, Nathanael Chambers, Mihai Surdeanu, Dan
Jurafsky, and Chris Manning. 2010. A multi-pass
sieve for coreference resolution. In Proceedings of
EMNLP 2010, pages 492?501.
Altaf Rahman and Vincent Ng. 2011. Coreference
resolution with world knowledge. In Proceedings of
ACL 2011, pages 814?824.
William M. Rand. 1971. Objective criteria for the eval-
uation of clustering methods. Journal of the American
Statistical Association, 66(336):846?850.
Marta Recasens and Eduard Hovy. 2011. BLANC: Im-
plementing the Rand index for coreference evaluation.
Natural Language Engineering, 17(4):485?510.
Veselin Stoyanov, Nathan Gilbert, Claire Cardie, and
Ellen Riloff. 2009. Conundrums in noun phrase coref-
erence resolution: Making sense of the state-of-the-art.
In Proceedings of ACL-IJCNLP 2009, pages 656?664.
Mihai Surdeanu, Jordi Turmo, and Alicia Ageno. 2005.
A hybrid unsupervised approach for document cluster-
ing. In Proceedings of KDD 2005, pages 685?690.
499
Mihai Surdeanu, Llu??s Ma`rquez, Xavier Carreras, and
Pere R. Comas. 2007. Combination strategies for se-
mantic role labeling. Journal of Artificial Intelligence
Research, 29:105?151.
Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A model-
theoretic coreference scoring scheme. In Proceedings
of MUC-6, pages 45?52.
Bonnie Lynn Webber. 1988. Discourse deixis: reference
to discourse segments. In Proceedings of ACL 1988,
pages 113?122.
Michael L. Wick, Khashayar Rohanimanesh, Karl
Schultz, and Andrew McCallum. 2008. A unified ap-
proach for schema matching, coreference and canoni-
calization. In Proceedings of KDD 2008, pages 722?
730.
David Yarowsky. 1995. Unsupervised word sense dis-
ambiguation rivaling supervised methods. In Proceed-
ings of ACL 1995, pages 189?196.
500
Squibs
On Paraphrase and Coreference
Marta Recasens?
University of Barcelona
Marta Vila??
University of Barcelona
By providing a better understanding of paraphrase and coreference in terms of similarities and
differences in their linguistic nature, this article delimits what the focus of paraphrase extraction
and coreference resolution tasks should be, and to what extent they can help each other. We argue
for the relevance of this discussion to Natural Language Processing.
1. Introduction
Paraphrase extraction1 and coreference resolution have applications in Question An-
swering, Information Extraction, Machine Translation, and so forth. Paraphrase pairs
might be coreferential, and coreference relations are sometimes paraphrases. The two
overlap considerably (Hirst 1981), but their definitions make them significantly different
in essence: Paraphrasing concerns meaning, whereas coreference is about discourse
referents. Thus, they do not always coincide. In the following example, b and d are both
coreferent and paraphrastic, whereas a, c, e, f, and h are coreferent but not paraphrastic,
and g and i are paraphrastic but not coreferent.
(1) [Tony]a went to see [the ophthalmologist]b and got [his]c eyes checked. [The eye
doctor]d told [him]e that [his]f [cataracts]g were getting worse. [His]h mother also
suffered from [cloudy vision]i.
The discourse model built for Example (1) contains six entities (i.e., Tony, the eye doctor,
Tony?s eyes, Tony?s cataracts, Tony?s mother, cataracts). Because a, c, e, f, and h all point
to Tony, we say that they are coreferent. In contrast, in paraphrasing, we do not need to
build a discourse entity to state that g and i are paraphrase pairs; we restrict ourselves to
semantic content and this is why we check for sameness of meaning between cataracts
and cloudy vision alone, regardless of whether they are a referential unit in a discourse.
Despite the differences, it is possible for paraphrasing and coreference to co-occur, as in
the case of b and d.
NLP components dealing with paraphrasing and coreference seem to have great
potential to improve understanding and generation systems. As a result, they have been
the focus of a large amount of work in the past couple of decades (see the surveys by
? CLiC, Department of Linguistics, Gran Via 585, 08007 Barcelona, Spain. E-mail: mrecasens@ub.edu.
?? CLiC, Department of Linguistics, Gran Via 585, 08007 Barcelona, Spain. E-mail: marta.vila@ub.edu.
1 Recognition, extraction, and generation are all paraphrase-related tasks. We will center ourselves on
paraphrase extraction, as this is the task in which paraphrase and coreference resolution mainly overlap.
Submission received: 3 March 2010; accepted for publication: 1 June 2010.
? 2010 Association for Computational Linguistics
Computational Linguistics Volume 36, Number 4
Androutsopoulos and Malakasiotis [2010], Madnani and Dorr [2010], Ng [2010], and
Poesio and Versley [2009]). Before computational linguistics, coreference had not been
studied on its own from a purely linguistic perspective but was indirectly mentioned in
the study of pronouns. Although there have been some linguistic works that consider
paraphrasing, they do not fully respond to the needs of paraphrasing from a computa-
tional perspective.
This article discusses the similarities between paraphrase and coreference in order
to point out the distinguishing factors that make paraphrase extraction and coref-
erence resolution two separate yet related tasks. This is illustrated with examples
extracted/adapted from different sources (Dras 1999; Doddington et al 2004; Dolan,
Brockett, and Quirk 2005; Recasens and Mart?? 2010; Vila et al 2010) and our own. Apart
from providing a better understanding of these tasks, we point out ways in which they
can mutually benefit, which can shed light on future research.
2. Converging and Diverging Points
This section explores the overlapping relationship between paraphrase and coreference,
highlighting the most relevant aspects that they have in common as well as those that
distinguish them. They are both sameness relations (Section 2.2), but one is between
meanings and the other between referents (Section 2.1). In terms of linguistic units,
coreference is mainly restricted to noun phrases (NPs), whereas paraphrasing goes
beyond and includes word-, phrase- and sentence-level expressions (Section 2.3). One
final diverging point is the role they (might) play in discourse (Section 2.4).
2.1 Meaning and Reference
The two dimensions that are the focus of paraphrasing and coreference are meaning
and reference, respectively. Traditionally, paraphrase is defined as the relation between
two expressions that have the same meaning (i.e., they evoke the same mental concept),
whereas coreference is defined as the relation between two expressions that have the
same referent in the discourse (i.e., they point to the same entity). We follow Karttunen
(1976) and talk of ?discourse referents? instead of ?real-world referents.?
In Table 1, the italicized pairs in cells (1,1) and (2,1) are both paraphrastic but
they only corefer in (1,1). We cannot decide on (non-)coreference in (2,1) as we need a
discourse to first assign a referent. In contrast, we can make paraphrasing judgments
Table 1
Paraphrase?coreference matrix.
Paraphrase
" %
Coreference
"
(1,1)
Tony went to see the
ophthalmologist and got his
eyes checked. The eye doctor
told him . . .
(1,2)
Tony went to see the
ophthalmologist and got
his eyes checked.
%
(2,1)
ophthalmologist
eye doctor
(2,2)
His cataracts were getting
worse. His mother also
suffered from cloudy vision.
640
Recasens and Vila On Paraphrase and Coreference
without taking discourse into consideration. Pairs like the one in cell (1,2) are only
coreferent but not paraphrases because the proper noun Tony and the pronoun his have
reference but no meaning. Lastly, neither phenomenon is observed in cell (2,2).
2.2 Sameness
Paraphrasing and coreference are usually defined as sameness relations: Two expres-
sions that have the same meaning are paraphrastic, and two expressions that refer to
the same entity in a discourse are coreferent. The concept of sameness is usually taken
for granted and left unexplained, but establishing sameness is not straightforward.
A strict interpretation of the concept makes sameness relations only possible in logic
and mathematics, whereas a sloppy interpretation makes the definition too vague. In
paraphrasing, if the loss of at the city in Example (2b) is not considered to be relevant, Ex-
amples (2a) and (2b) are paraphrases; but if it is considered to be relevant, then they are
not. It depends on where we draw the boundaries of what is accepted as the ?same?
meaning.
(2) a. The waterlogged conditions that ruled out play yesterday still prevailed at
the city this morning.
b. The waterlogged conditions that ruled out play yesterday still prevailed this
morning.
(3) On homecoming night Postville feels like Hometown, USA . . . For those who
prefer the old Postville, Mayor John Hyman has a simple answer.
Similarly, with respect to coreference (3), whether Postville and the old Postville in Ex-
ample 3 are or are not the same entity depends on the granularity of the discourse.
On a sloppy reading, one can assume that because Postville refers to the same spatial
coordinates, it is the same town. On a strict reading, in contrast, drawing a distinction
between the town as it was at two different moments in time results in two different
entities: the old Postville versus the present-day Postville. They are not the same in that
features have changed from the former to the latter.
The concept of sameness in paraphrasing has been questioned on many occasions.
If we understood ?same meaning? in the strictest sense, a large number of paraphrases
would be ruled out. Thus, some authors argue for a looser definition of paraphrasing.
Bhagat (2009), for instance, talks about ?quasi-paraphrases? as ?sentences or phrases
that convey approximately the same meaning.? Milic?evic? (2007) draws a distinction
between ?exact? and ?approximate? paraphrases. Finally, Fuchs (1994) prefers to use
the notion of ?equivalence? to ?identity? on the grounds that the former allows for the
existence of some semantic differences between the paraphrase pairs. The concept of
identity in coreference, however, has hardly been questioned, as prototypical examples
appear to be straightforward (e.g., Barack Obama andObama and he). Only recently have
Recasens, Hovy, and Mart?? (2010) pointed out the need for talking about ?near-identity?
relations in order to account for cases such as Example (3), proposing a typology of such
relations.
2.3 Linguistic Units
Another axis of comparison between paraphrase and coreference concerns the types
of linguistic units involved in each relation. Paraphrase can hold between different
641
Computational Linguistics Volume 36, Number 4
linguistic units, from morphemes to full texts, although the most attention has been paid
to word-level paraphrase (kid and child in Example (4)), phrase-level paraphrase (cried
and burst into tears in Example (4)), and sentence-level paraphrase (the two sentences
in Example (4)).
(4) a. The kid cried.
b. The child burst into tears.
In contrast, coreference is more restricted in that the majority of relations occur at the
phrasal level, especially between NPs. This explains why this has been the largest focus
so far, although prepositional and adverbial phrases are also possible yet less frequent,
as well as clauses or sentences. Coreference relations occur indistinctively between
pronouns, proper nouns, and full NPs that are referential, namely, that have discourse
referents. For this reason, pleonastic pronouns, nominal predicates, and appositives
cannot enter into coreference relations. The first do not refer to any entity but are
syntactically required; the last two express properties of an entity rather than introduce
a new one. But this is an issue ignored by the corpora annotated for the MUC and ACE
programs (Hirschman and Chinchor 1997; Doddington et al 2004), hence the criticism
by van Deemter and Kibble (2000).
In the case of paraphrasing, it is linguistic expressions that lack meaning (i.e.,
pronouns and proper nouns) that should not be treated as members of a paraphrase pair
on their own (Example (5a)) because paraphrase is only possible between meaningful
units. This issue, however, takes on another dimension when seen at the sentence level.
The sentences in Example (5b) can be said to be paraphrases because they themselves
contain the antecedent of the pronouns I and he.
(5) a. (i) A. Jime?nez
(ii) I
b. (i) The Atle?tico de Madrid goalkeeper, A. Jime?nez, yesterday realized one
of his dreams by defeating Barcelona: ?I had never beaten Barcelona.?
(ii) The Atle?tico de Madrid goalkeeper, A. Jime?nez, yesterday realized one
of his dreams by defeating Barcelona, and said that he had never beaten
Barcelona.
In Example (5b), A. Jime?nez and I/he continue not being paraphrastic. Polysemic, un-
derspecified, and metaphoric words show a slightly different behavior. It is not possible
to establish paraphrase between them when they are deprived of context (Callison-
Burch 2007, Chapter 4). In Example (6a), police officers could be patrol police officers,
and investigators could be university researchers. However, once they are embedded in a
disambiguating context that fills them semantically, as in Example (6b), then paraphrase
can be established between police officers and investigators.
(6) a. (i) Police officers
(ii) Investigators
b. (i) Police officers searched 11 stores in Barcelona.
(ii) The investigators conducted numerous interviews with the victim.
As a final remark, and in accordance with the approach by Fuchs (1994), we consider
Example (7)?like paraphrases that Fujita (2005) and Milic?evic? (2007) call, respectively,
642
Recasens and Vila On Paraphrase and Coreference
?referential? and ?cognitive? to be best treated as coreference rather than paraphrase,
because they only rely on referential identity in a discourse.
(7) a. They got married last year.
b. They got married in 2004.
2.4 Discourse Function
A further difference between paraphrasing and coreference concerns their degree of de-
pendency on discourse. Given that coreference establishes sameness relations between
the entities that populate a discourse (i.e., discourse referents), it is a linguistic phe-
nomenon whose dependency on discourse is much stronger than paraphrasing. Thus,
the latter can be approached from a discursive or a non-discursive perspective, which
in turn allows for a distinction between reformulative paraphrasing (Example (8)) and
non-reformulative paraphrasing (Example (9)).
(8) Speaker 1: Then they also diagnosed a hemolytic?uremic syndrome.
Speaker 2: What?s that?
Speaker 1: Renal insufficiency, in the kidneys.
(9) a. X wrote Y.
b. X is the author of Y.
Reformulative paraphrasing occurs in a reformulation context when a rewording of
a previously expressed content is added for discursive reasons, such as emphasis,
correction, or clarification. Non-reformulative paraphrasing does not consider the
role that paraphrasing plays in discourse. Reformulative paraphrase pairs have to be
extracted from a single piece of discourse; non-reformulative paraphrase pairs can
be extracted?each member of the pair on its own?from different discourse pieces. The
reformulation in the third utterance in Example (8) gives an explanation in a language
less technical than that in the first utterance; whereas Examples (9a) and (9b) are simply
two alternative ways of expressing an authorship relation.
The strong discourse dependency of coreference explains the major role it plays
in terms of cohesion. Being such a cohesive device, it follows that intra-document
coreference, which takes place within a single discourse unit (or across a collection of
documents linked by topic), is the most primary. Cross-document coreference, on the
other hand, constitutes a task on its own in NLP but falls beyond the scope of linguistic
coreference due to the lack of a common universe of discourse. The assumption behind
cross-document coreference is that there is an underlying global discourse that enables
various documents to be treated as a single macro-document.
Despite the differences, the discourse function of reformulative paraphrasing brings
it close to coreference in the sense that they both contribute to the cohesion and devel-
opment of discourse.
3. Mutual Benefits
Both paraphrase extraction and coreference resolution are complex tasks far from being
solved at present, and we believe that there could be improvements in performance
643
Computational Linguistics Volume 36, Number 4
if researchers on each side paid attention to the others. The similarities (i.e., relations
of sameness, relations between NPs) allow for mutual collaboration, whereas the differ-
ences (i.e., focus on either meaning or reference) allow for resorting to either paraphrase
or coreference to solve the other. In general, the greatest benefits come for cases in which
either paraphrase or coreference are especially difficult to detect automatically. More
specifically, we see direct mutual benefits when both phenomena occur either in the
same expression or in neighboring expressions.
For pairs of linguistic expressions that show both relations, we can hypothesize
paraphrasing relationships between NPs for which coreference is easier to detect. For
instance, coreference between the two NPs in Example (10) is very likely given that they
have the same head, head match being one of the most successful features in coreference
resolution (Haghighi and Klein 2009). In contrast, deciding on paraphrase would be
hard due to the difficulty of matching the modifiers of the two NPs.
(10) a. The director of a multinational with huge profits.
b. The director of a solvent company with headquarters in many countries.
In the opposite direction, we can hypothesize coreference links between NPs for which
paraphrasing can be recognized with considerable ease (Example (11)). Light elements
(e.g., fact), for instance, are normally taken into account in paraphrasing?but not in
coreference resolution?as their addition or deletion does not involve a significant
change in meaning.
(11) a. The creation of a company.
b. The fact of creating a company.
By neighboring expressions, we mean two parallel structures each containing a coref-
erent mention of the same entity next to a member of the same paraphrase pair. Note
that the coreferent expressions in the following examples are printed in italics and the
paraphrase units are printed in bold. If a resolution module identifies the coreferent
pairs in Example (12), then these can function as two anchor points,X andY, to infer that
the text between them is paraphrastic: X complained today before Y, and X is formulating
the corresponding complaint to Y.
(12) a. ArgentinaX complained today before the British GovernmentY about the
violation of the air space of this South American country.
b. This ChancellorshipX is formulating the corresponding complaint to the
British GovernmentY for this violation of the Argentinian air space.
Some authors have already used coreference resolution in their paraphrasing systems
in a similar way to the examples herein. Shinyama and Sekine (2003) benefit from the
fact that a single event can be reported in more than one newspaper article in different
ways, keeping certain kinds of NPs such as names, dates, and numbers unchanged.
Thus, these can behave as anchor points for paraphrase extraction. Their system uses
coreference resolution to find anchors which refer to the same entity.
Conversely, knowing that a stretch of text next to an NP paraphrases another stretch
of text next to another NP helps to identify a coreference link between the two NPs,
as shown by Example (13), where two diction verbs are easily detected as a para-
phrase and thus their subjects can be hypothesized to corefer. If the paraphrase system
644
Recasens and Vila On Paraphrase and Coreference
identifies the mapping between the indirect speech in Example (13a) and the direct
speech in Example (13b), the coreference relation between the subjects is corroborated.
Another difficult coreference link that can be detected with the help of paraphrasing
is Example (14): If the predicates are recognized as paraphrases, then the subjects are
likely to corefer.
(13) a. The trainer of the Cuban athlete Sotomayor said that the world record holder
is in a fit state to win the Games in Sydney.
b. ?The record holder is in a fit state to win the Olympic Games,? explained
De la Torre.
(14) a. Police officers searched 11 stores in Barcelona.
b. The investigators carried out 11 searches in stores in the center of
Barcelona.
Taking this idea one step further, new coreference resolution strategies can be developed
with the aid of shallow paraphrasing techniques. A two-step process for coreference
resolution might consist of hypothesizing first sentence-level paraphrases via n-gram
or named-entity overlapping, aligning phrases that are (possible) paraphrases, and
hypothesizing that they corefer. Second, a coreference module can act as a filter and
provide a second classification. Such a procedure could be successful for the cases
exemplified in Examples (12) to (14).
This strategy reverses the tacit assumption that coreference is solved before
sentence-level paraphrasing. Meaning alone does not make it possible to state that the
two pairs in Example (5b), repeated in Example (15), or the two pairs in Example (16)
are paraphrases without first solving the coreference relations.
(15) a. The Atle?tico de Madrid goalkeeper, A. Jime?nez, yesterday realized one of his
dreams by defeating Barcelona: ?I had never beaten Barcelona.?
b. The Atle?tico de Madrid goalkeeper, A. Jime?nez, yesterday realized one of
his dreams by defeating Barcelona, and said that he had never beaten
Barcelona.
(16) a. Secretary of State Colin Powell last week ruled out a non-aggression treaty.
b. But Secretary of State Colin Powell brushed off this possibility.
However, cooperative work between paraphrasing and coreference is not always pos-
sible, and it is harder if neither of the two can be detected by means of widely used
strategies. In other cases, cooperation can even be misleading. In Example (17), the two
bold phrases are paraphrases, but their subjects do not corefer. The detection of words
like another (Example (17b)) gives a key to help to prevent this kind of error.
(17) a. A total of 26 Cuban citizens remain in the police station of the airport of
Barajas after requesting political asylum.
b. Another three Cubans requested political asylum.
On the basis of these various examples, we claim that a full understanding of both
the similarities and disparities will enable fruitful collaboration between researchers
working on paraphrasing and those working on coreference. Even more importantly,
645
Computational Linguistics Volume 36, Number 4
our main claim is that such an understanding about the fundamental linguistic issues is
a prerequisite for building paraphrase and coreference systems not lacking in linguistic
rigor. In brief, we call for the return of linguistics to paraphrasing and coreference
automatic applications, as well as to NLP in general, adhering to the call by Wintner
(2009: 643), who cites examples that demonstrate ?what computational linguistics can
achieve when it is backed up and informed by linguistic theory? (page 643).
Acknowledgments
We are grateful to Eduard Hovy, M. Anto`nia
Mart??, Horacio Rodr??guez, and Mariona
Taule? for their helpful advice as experienced
researchers. We would also like to express
our gratitude to the three anonymous
reviewers for their suggestions to improve
this article.
This work was partly supported by FPU
Grants AP2006-00994 and AP2008-02185
from the Spanish Ministry of Education,
and Project TEXT-MESS 2.0 (TIN2009-
13391-C04-04).
References
Androutsopoulos, Ion and Prodromos
Malakasiotis. 2010. A survey of
paraphrasing and textual entailment
methods. Journal of Artificial Intelligence
Research, 38:135?187.
Bhagat, Rahul. 2009. Learning
Paraphrases from Text. Ph.D. thesis,
University of Southern California,
Los Angeles, CA.
Callison-Burch, Chris. 2007. Paraphrasing and
Translation. Ph.D. thesis, University of
Edinburgh, Edinburgh.
Doddington, George, Alexis Mitchell, Mark
Przybocki, Lance Ramshaw, Stephanie
Strassel, and Ralph Weischedel. 2004.
The Automatic Content Extraction (ACE)
program?Tasks, data, and evaluation.
In Proceedings of the 4th International
Conference on Language Resources and
Evaluation (LREC 2004), pages 837?840,
Lisbon.
Dolan, Bill, Chris Brockett, and Chris Quirk.
2005. README file included in the
Microsoft Research Paraphrase Corpus,
March, Redmond, WA.
Dras, Mark. 1999. Tree Adjoining Grammar
and the Reluctant Paraphrasing of Text. Ph.D.
thesis, Macquarie University, Sydney.
Fuchs, Catherine. 1994. Paraphrase et
e?nonciation. Mode?lisation de la paraphrase
langagie`re. Ophrys, Paris.
Fujita, Atsushi. 2005. Automatic Generation of
Syntactically Well-formed and Semantically
Appropriate Paraphrases. Ph.D. thesis, Nara
Institute of Science and Technology,
Ikoma, Nara.
Haghighi, Aria and Dan Klein. 2009.
Simple coreference resolution with rich
syntactic and semantic features. In
Proceedings of the Conference on Empirical
Methods in Natural Language Processing
(EMNLP 2009), pages 1152?1161,
Singapore.
Hirschman, Lynette and Nancy Chinchor.
1997. MUC-7 Coreference task definition?
Version 3.0. In Proceedings of the Message
Understanding Conference-7 (MUC-7),
Washington, DC.
Hirst, Graeme J. 1981. Anaphora in Natural
Language Understanding: A Survey.
Springer-Verlag, Berlin.
Karttunen, Lauri. 1976. Discourse referents.
In J. McCawley, editor, Syntax and
Semantics, volume 7. Academic Press,
New York, pages 363?385.
Madnani, Nitin and Bonnie J. Dorr. 2010.
Generating phrasal and sentential
paraphrases: A survey of data-driven
methods. Computational Linguistics, 36,
pages 341?387.
Milic?evic?, Jasmina. 2007. La paraphrase.
Peter Lang, Berne.
Ng, Vincent. 2010. Supervised noun phrase
coreference research: The first fifteen
years. In Proceedings of the 48th Meeting
of the Association for Computational
Linguistics (ACL 2010), Uppsala,
pages 1396?1411.
Poesio, Massimo and Yannick Versley. 2009.
Computational models for the
interpretation of anaphora: A survey.
Notes from the ACL-2009 Tutorial on
State-of-the-art NLP Approaches to
Coreference Resolution, Singapore.
Recasens, Marta, Eduard Hovy, and
M. Anto`nia Mart??. 2010. A typology of
near-identity relations for coreference
(NIDENT). In Proceedings of the 7th
International Conference on Language
Resources and Evaluation (LREC 2010),
pages 149?156, Valletta.
Recasens, Marta and M. Anto`nia Mart??. 2010.
AnCora-CO: Coreferentially annotated
646
Recasens and Vila On Paraphrase and Coreference
corpora for Spanish and Catalan. Language
Resources and Evaluation, 44(4):315?345.
doi: 10.1007/s10579-009-9108-x.
Shinyama, Yusuke and Satoshi Sekine.
2003. Paraphrase acquisition for
information extraction. In Proceedings of
the ACL 2nd International Workshop on
Paraphrasing (IWP 2003), pages 65?71,
Sapporo.
van Deemter, Kees and Rodger Kibble. 2000.
On coreferring: Coreference in MUC and
related annotation schemes. Computational
Linguistics, 26(4):629?637.
Vila, Marta, Santiago Gonza?lez, M. Anto`nia
Mart??, Joaquim Llisterri, and M. Jesu?s
Machuca. 2010. ClInt: A bilingual
Spanish-Catalan spoken corpus of
clinical interviews. Procesamiento del
Lenguaje Natural, 45, 105?111.
Wintner, Shuly. 2009. What science
underlies Natural Language Engineering?
Computational Linguistics, 35(4):641?644.
647

Proceedings of NAACL-HLT 2013, pages 627?633,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
The Life and Death of Discourse Entities: Identifying Singleton Mentions
Marta Recasens
Linguistics Department
Stanford University
Stanford, CA 94305
recasens@google.com
Marie-Catherine de Marneffe
Linguistics Department
The Ohio State University
Columbus, OH 43210
mcdm@ling.osu.edu
Christopher Potts
Linguistics Department
Stanford University
Stanford, CA 94305
cgpotts@stanford.edu
Abstract
A discourse typically involves numerous en-
tities, but few are mentioned more than once.
Distinguishing discourse entities that die out
after just one mention (singletons) from those
that lead longer lives (coreferent) would ben-
efit NLP applications such as coreference res-
olution, protagonist identification, topic mod-
eling, and discourse coherence. We build a lo-
gistic regression model for predicting the sin-
gleton/coreferent distinction, drawing on lin-
guistic insights about how discourse entity
lifespans are affected by syntactic and seman-
tic features. The model is effective in its own
right (78% accuracy), and incorporating it into
a state-of-the-art coreference resolution sys-
tem yields a significant improvement.
1 Introduction
Not all discourse entities are created equal. Some
lead long lives and appear in a variety of discourse
contexts (coreferent), whereas others never escape
their birthplaces, dying out after just one mention
(singletons). The ability to make this distinction
based on properties of the NPs used to identify these
referents (mentions) would benefit not only corefer-
ence resolution, but also topic analysis, textual en-
tailment, and discourse coherence.
The existing literature provides numerous gen-
eralizations relevant to answering the question of
whether a given discourse entity will be singleton
or coreferent. These involve the internal syntax and
morphology of the target NP (Prince, 1981a; Prince,
1981b; Wang et al, 2006), the grammatical function
and discourse role of that NP (Chafe, 1976; Hobbs,
1979; Walker et al, 1997; Beaver, 2004), and the in-
teraction of all of those features with semantic oper-
ators like negation, modals, and attitude predicates
(Karttunen, 1973; Karttunen, 1976; Kamp, 1981;
Heim, 1982; Heim, 1992; Roberts, 1990; Groe-
nendijk and Stokhof, 1991; Bittner, 2001).
The first step in our analysis is to bring these
insights together into a single logistic regression
model ? the lifespan model ? and assess their
predictive power on real data. We show that the
features generally behave as the existing literature
leads us to expect, and that the model itself is highly
effective at predicting whether a given mention is
singleton or coreferent. We then provide an initial
assessment of the engineering value of making the
singleton/coreferent distinction by incorporating our
lifespan model into the Stanford coreference resolu-
tion system (Lee et al, 2011). This addition results
in a significant improvement on the CoNLL-2012
Shared Task data, across the MUC, B3, CEAF, and
CoNLL scoring algorithms.
2 Data
All the data used throughout the paper come from
the CoNLL-2012 Shared Task (Pradhan et al,
2012), which included the 1.6M English words from
OntoNotes v5.0 (Hovy et al, 2006) that have been
annotated with different layers of annotation (coref-
erence, parse trees, etc.). We used the training, de-
velopment (dev), and test splits as defined in the
shared task (Table 1). Since the OntoNotes corefer-
ence annotations do not contain singleton mentions,
we automatically marked as singletons all the NPs
627
MENTIONS
Dataset Docs Tokens Coreferent Singletons
Training 2,802 1.3M 152,828 192,248
Dev 343 160K 18,815 24,170
Test 348 170K 19,392 24,921
Table 1: CoNLL-2012 Shared Task data statistics. We
added singletons (NPs not annotated as coreferent).
not annotated as coreferent. Thus, our singletons in-
clude non-referential NPs but not verbal mentions.
3 Predicting lifespans
Our lifespan model makes a binary distinction be-
tween discourse referents that are not part of a coref-
erence chain (singletons) and items that are part of
one (coreferent). The distribution of lifespans in our
data (Figure 1) suggests that this is a natural divi-
sion. The propensity of singletons also highlights
the relevance of detecting singletons for a coref-
erence system. We fit a binary logistic regression
model in R (R Core Team, 2012) on the training
data, coding singletons as ?0? and coreferent men-
tions as ?1?. Throughout the following tables of co-
efficient estimates, positive values favor coreferents
and negative ones favor singletons. We turn now to
describing and motivating the features of this model.
Singleton 2 3 4 5 6-10 11-15 16-20 >20
0
5K
15
K
25
K
Figure 1: Distribution of lifespans in the dev set. Single-
tons account for 56% of the data.
Internal morphosyntax of the mention Table 2
summarizes the features from our model that con-
cern the internal morphology and syntactic structure
of the mention. Many are common in coreference
systems (Recasens and Hovy, 2009), but our model
highlights their influence on lifespans. The picture
is expected on the taxonomy of given and new de-
fined by Prince (1981b) and assumed throughout dy-
namic semantics (Kamp, 1981; Heim, 1982): pro-
nouns depend on anaphoric connections to previous
mentions for disambiguation and thus are very likely
to be coreferent. This is corroborated by the pos-
itive coefficient estimate for ?Type = pronoun? in
Table 2. Few quantified phrases easily participate
in discourse anaphora (Partee, 1987; Wang et al,
2006), accounting for the association between quan-
tifiers and singletons (negative coefficient estimate
for ?Quantifier = quantified? in Table 2). The one
surprise is the negative coefficient for indefinites. In
theories stretching back to Karttunen (1976), indef-
inites function primarily to establish new discourse
entities, and should be able to participate in coref-
erence chains, but here the association with such
chains is negative. However, interactions explain
this fact (see Table 4 and our discussion of it).
The person, number, and animacy values suggest
that singular animates are excellent coreferent NPs,
a previous finding of Centering Theory (Grosz et al,
1995; Walker et al, 1998) and of cross-linguistic
work on obviative case-marking (Aissen, 1997).
Our model also includes named-entity features for
all of the eighteen OntoNotes entity-types (omitted
from Table 2 for space and clarity reasons). As a
rule, they behave like ?Type = proper noun? in asso-
ciating with coreferents. The exceptions are ORDI-
NAL, PERCENT, and QUANTITY, which seem intu-
itively unlikely to participate in coreference chains.
Estimate P-value
Type = pronoun 1.21 < 0.001
Type = proper noun 1.88 < 0.001
Animacy = inanimate ?1.36 < 0.001
Animacy = unknown ?0.38 < 0.001
Person = 1 1.05 < 0.001
Person = 2 0.13 < 0.001
Person = 3 1.62 < 0.001
Number = singular 0.61 < 0.001
Number = unknown 0.17 < 0.001
Quantifier = indefinite ?1.49 < 0.001
Quantifier = quantified ?1.23 < 0.001
Number of modifiers ?0.39 < 0.001
Table 2: Internal morphosyntactic features.
Grammatical role of the mention Synthesizing
much work in Centering Theory and information
structuring, we conclude that coreferent mentions
are likely to appear as core verbal arguments and
will favor sentence-initial (topic-tracking) positions
(Ward and Birner, 2004). The coefficient estimates
628
Estimate P-value
Sentence Position = end ?0.22 < 0.001
Sentence Position = first 0.04 0.07
Sentence Position = last ?0.31 < 0.001
Sentence Position = middle ?0.11 < 0.001
Relation = noun argument 0.56 < 0.001
Relation = other ?0.67 < 0.001
Relation = root ?0.61 < 0.001
Relation = subject 0.65 < 0.001
Relation = verb argument 0.32 < 0.001
In coordination ?0.48 < 0.001
Table 3: Grammatical role features.
in Table 3 corroborate these conclusions. To de-
fine the ?Relation? and ?In coordination? features, we
used the Stanford dependencies (de Marneffe et al,
2006) on the gold constituents.
Semantic environment of the mention Table 4
highlights the complex interactions between dis-
course anaphora and semantic operators. These
interactions have been a focus of logical seman-
tics since Karttunen (1976), whose guiding obser-
vation is semantic: an indefinite interpreted inside
the scope of a negation, modal, or attitude predicate
is generally unavailable for anaphoric reference out-
side of the scope of that operator, as in Kim didn?t
understand [an exam question]i. #Iti was too hard.
Of course, such discourses cohere if the indefinite
is interpreted as taking wide scope (?there is a ques-
tion Kim didn?t understand?). Such readings are of-
ten disfavored, but they become more salient when
modifiers like certain are included (Schwarzschild,
2002) or when the determiner is sensitive to the po-
larity or intensionality of its environment (Baker,
1970; Ladusaw, 1980; van der Wouden, 1997; Is-
rael, 1996; Israel, 2001; Giannakidou, 1999). Sub-
sequent research identified many other factors that
further extend or restrict the anaphoric potential of
an indefinite (Roberts, 1996).
We do not have direct access to semantic scope,
but we expect syntactic scope to correlate strongly
with semantic scope, so we used dependency rep-
resentations to define features capturing syntactic
scope for negation, modal auxiliaries, and a broad
range of attitude predicates. These features tend to
bias in favor of singletons because they so radically
restrict the possibilities for intersentential anaphora.
Interacting these features with those for the inter-
nal syntax of mentions is also informative. Since
proper names and pronouns are not scope-taking,
they are largely unaffected by the environment fea-
tures, whereas indefinites emerge as even more re-
stricted, just as Karttunen and others would predict.
Attitude predicates seem initially anomalous,
though. They share the relevant semantic proper-
ties with negation and modals, and yet they seem
to facilitate coreference. Here, the findings of de
Marneffe et al (2012) seem informative. Those au-
thors find that, in texts of the sort we are studying,
attitude predicates are used predominantly to mark
the source of information that is effectively asserted
despite being embedded (Rooryck, 2001; Simons,
2007). That is, though X said p does not semanti-
cally entail p, it is often interpreted as a commitment
to p, which correspondingly elevates mentions in p
to main-clause status (Harris and Potts, 2009).
Estimate P-value
Presence of negation ?0.18 < 0.001
Presence of modality ?0.22 < 0.001
Under an attitude verb 0.03 0.01
AttitudeVerb * (Type = pronoun) 0.29 < 0.001
AttitudeVerb * (Type = proper noun) 0.14 < 0.001
Modal * (Type = pronoun) 0.12 0.04
Modal * (Type = proper noun) 0.35 < 0.001
Negation * (Type = pronoun) 1.07 < 0.001
Negation * (Type = proper noun) 0.30 < 0.001
Negation * (Quantifier = indefinite) ?0.37 < 0.001
Negation * (Quantifier = quantified) ?0.36 0.23
Negation * (Number of modifiers) 0.11 < 0.001
Table 4: Semantic environment features and interactions.
Results The model successfully learns to tease
singletons and coreferent mentions apart. Table 5
summarizes its performance on the dev set. The
STANDARD model uses 0.5 as the decision bound-
ary, with 78% accuracy. The CONFIDENT model
predicts singleton if Pr < .2 and coreferent if Pr > .8,
which increases precision (P) at a cost to recall (R).
STANDARD CONFIDENT
Prediction R P F1 R P F1
Singleton 82.3 79.2 80.7 50.5 89.6 64.6
Coreferent 72.2 76.1 74.1 41.3 86.8 55.9
Table 5: Recall, precision, and F1 for the lifespan model.
629
MUC B3 CEAF-?3 CEAF-?4 CoNLL
System R P F1 R P F1 R / P / F1 R P F1 F1
Baseline 66.64* 64.72 65.67 68.05* 71.58 69.77* 58.31 45.49 47.55* 46.50 60.65
w/ Lifespan 66.08 67.33* 66.70* 66.40 73.14* 69.61 58.83* 47.77* 46.38 47.07* 61.13*
Table 6: Performance on the test set according to the official CoNLL-2012 scorer. Scores are on automatically pre-
dicted mentions. Stars indicate a statistically significant difference (paired Mann-Whitney U-test, p < 0.05).
B3 CEAF-?3 CoNLL
System R P F1 R P F1 F1
Baseline 58.53* 71.58 64.40 63.71* 58.31 60.89 58.86
w/ Lifespan 58.14 73.14* 64.78* 63.38 58.83* 61.02 59.52*
Table 7: B3, CEAF-?3 and CoNLL measures on the test set according to a modified CoNLL-2012 scorer that follows
Cai and Strube (2010). Scores are on automatically predicted mentions.
4 Application to coreference resolution
To assess the usefulness of the lifespan model in an
NLP application, we incorporate it into the Stanford
coreference resolution system (Lee et al, 2011),
which we take as our baseline. This was the highest-
scoring system in the CoNLL-2011 Shared Task,
and was also part of the highest-scoring system in
the CoNLL-2012 Shared Task (Fernandes et al,
2012). It is a rule-based system that includes a to-
tal of ten rules (or ?sieves?) for entity coreference,
such as exact string match and pronominal resolu-
tion. The sieves are applied from highest to lowest
precision, each rule adding coreference links.
Incorporating the lifespan model The lifespan
model can improve coreference resolution in two
different ways: (i) mentions classified as singletons
should not be considered as either antecedents or
coreferent, and (ii) mentions classified as coreferent
should be linked with another mention(s). By suc-
cessfully predicting singletons (i), we can enhance
the system?s precision; by successfully predicting
coreferent mentions (ii), we can improve the sys-
tem?s recall. Here we focus on (i) and use the lifes-
pan model for detecting singletons. This decision
is motivated by two factors. First, given the large
number of singletons (Figure 1), we are more likely
to see a gain in performance from discarding sin-
gletons. Second, the multi-sieve nature of the Stan-
ford coreference system does not make it straightfor-
ward to decide which antecedent a mention should
be linked to even if we know that it is coreferent.
We leave the incorporation of coreferent predictions
for future work.
To integrate the singleton model into the Stanford
coreference system, we let a sieve consider whether
a pair of mentions is coreferent only if neither of
the two mentions are classified as singletons by our
CONFIDENT model. Experiments on the dev set
showed that the model often made wrong predic-
tions for NEs. We do not trust the model for NE
mentions. Performance on coreference (on the dev
set) was higher with the CONFIDENT model than
with the STANDARD model.
Results and discussion To evaluate the corefer-
ence system with and without the lifespan model, we
used the English dev and test sets from the CoNLL-
2012 Shared Task, presented in Section 2. Although
the CoNLL shared task evaluated systems on only
multi-mention (i.e., non-singleton) entities, by stop-
ping singletons from being linked to multi-mention
entities, we expected the lifespan model to increase
the system?s precision. Our evaluation uses five
of the measures given by the CoNLL-2012 scorer:
MUC (Vilain et al, 1995), B3 (Bagga and Baldwin,
1998), CEAF-?3 and CEAF-?4 (Luo, 2005), and the
CoNLL official score (Denis and Baldridge, 2009).
We do not include BLANC (Recasens and Hovy,
2011) because it assumes gold mentions and so is
not suited for the scenario considered in this paper,
which uses automatically predicted mentions.
Table 6 summarizes the test set performance. All
the scores are on automatically predicted mentions.
We use gold POS, parse trees, and NEs. The base-
630
line is the Stanford system, and ?w/ Lifespan? is the
same system extended with our lifespan model to
discard singletons, as explained above.
As expected, the lifespan model increases preci-
sion but decreases recall. Overall, however, we ob-
tain a significant improvement of 0.5?1 points in the
F1 score of MUC, CEAF-?3, CEAF-?4 and CoNLL.
The drop in B3 traces to a bug in the CoNLL scorer?s
implementation of Cai and Strube (2010)?s algo-
rithm for aligning gold and automatically predicted
mentions, which affects the computation of B3 and
CEAF-?3.1 Table 7 presents the results after mod-
ifying the CoNLL-2012 scorer to compute B3 and
CEAF-?3 according to Cai and Strube (2010).2 We
do see an improvement in the precision and F1
scores of B3, and the overall CoNLL score remains
significant. The CEAF-?3 F1 score is no longer sig-
nificant, but is still in the expected direction.
5 Conclusion
We built a model to predict the lifespan of discourse
referents, teasing apart singletons from coreferent
mentions. The model validates existing linguistic
insights and performs well in its own right. This
alone has ramifications for tracking topics, identify-
ing protagonists, and modeling coreference and dis-
course coherence. We applied the lifespan model to
coreference resolution, showing how to incorporate
it effectively into a state-of-the-art rule-based coref-
erence system. We expect similar improvements
with machine-learning-based coreference systems,
where incorporating all the power of the lifespan
model would be easier.
Our lifespan model has been integrated into the
latest version of the Stanford coreference resolution
system.3
1At present, if the system links two mentions that do not
exist in the gold standard, the scorer adds two singletons to the
gold standard. This results in a higher B3 F1 score (when it
should be lower) because recall increases instead of staying the
same (precision goes up).
2In the modified scorer, twinless predicted mentions are
added to the gold standard to compute precision but not to com-
pute recall.
3http://nlp.stanford.edu/software/
dcoref.shtml
Acknowledgments
We thank Emili Sapena for modifying the CoNLL-
2012 scorer to follow Cai and Strube (2010).
This research was supported in part by ONR
grant No. N00014-10-1-0109 and ARO grant
No. W911NF-07-1-0216. The first author was sup-
ported by a Beatriu de Pino?s postdoctoral schol-
arship (2010 BP-A 00149) from Generalitat de
Catalunya.
References
Judith Aissen. 1997. On the syntax of obviation. Lan-
guage, 73(4):705?750.
Amit Bagga and Breck Baldwin. 1998. Algorithms
for scoring coreference chains. In Proceedings of
the LREC 1998 Workshop on Linguistic Coreference,
pages 563?566.
C. L. Baker. 1970. Double negatives. Linguistic Inquiry,
1(2):169?186.
David Beaver. 2004. The optimization of discourse
anaphora. Linguistics and Philosophy, 27(1):3?56.
Maria Bittner. 2001. Surface composition as bridging.
Journal of Semantics, 18(2):127?177.
Jie Cai and Michael Strube. 2010. Evaluation metrics
for end-to-end coreference resolution systems. In Pro-
ceedings of SIGDIAL 2010, pages 28?36.
Wallace L. Chafe. 1976. Givenness, Contrastiveness,
Definiteness, Subjects, Topics, and Point of View. In
Charles N. Li, editor, Subject and Topic, pages 25?55.
Academic Press, New York.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed de-
pendency parses from phrase structure parses. In Pro-
ceedings of LREC 2006.
Marie-Catherine de Marneffe, Christopher D. Manning,
and Christopher Potts. 2012. Did it happen?
The pragmatic complexity of veridicality assessment.
Computational Linguistics, 38(2):301?333.
Pascal Denis and Jason Baldridge. 2009. Global joint
models for coreference resolution and named entity
classification. Procesamiento del Lenguaje Natural,
42:87?96.
Eraldo Fernandes, C??cero dos Santos, and Ruy Milidiu?.
2012. Latent structure perceptron with feature induc-
tion for unrestricted coreference resolution. In Pro-
ceedings of CoNLL-2012: Shared Task, pages 41?48.
Anastasia Giannakidou. 1999. Affective dependencies.
Linguistics and Philosophy, 22(4):367?421.
Jeroen Groenendijk and Martin Stokhof. 1991. Dynamic
predicate logic. Linguistics and Philosophy, 14(1):39?
100.
631
Barbara J. Grosz, Aravind K. Joshi, and Scott Weinstein.
1995. Centering: A framework for modeling the lo-
cal coherence of discourse. Computational Linguis-
tics, 21(2):203?225.
Jesse A. Harris and Christopher Potts. 2009.
Perspective-shifting with appositives and expressives.
Linguistics and Philosophy, 32(6):523?552.
Irene Heim. 1982. The Semantics of Definite and Indefi-
nite Noun Phrases. Ph.D. thesis, UMass Amherst.
Irene Heim. 1992. Presupposition projection and the
semantics of attitude verbs. Journal of Semantics,
9(2):183?221.
Jerry R. Hobbs. 1979. Coherence and coreference. Cog-
nitive Science, 3(1):67?90.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. OntoNotes:
The 90% solution. In Proceedings of HLT-
NAACL 2006, pages 57?60.
Michael Israel. 1996. Polarity sensitivity as lexical se-
mantics. Linguistics and Philosophy, 19(6):619?666.
Michael Israel. 2001. Minimizers, maximizers, and the
rhetoric of scalar reasoning. Journal of Semantics,
18(4):297?331.
Hans Kamp. 1981. A theory of truth and discourse
representation. In Jeroen Groenendijk, Theo M. V.
Janssen, and Martin Stockhof, editors, Formal Meth-
ods in the Study of Language, pages 277?322. Mathe-
matical Centre, Amsterdam.
Lauri Karttunen. 1973. Presuppositions and compound
sentences. Linguistic Inquiry, 4(2):169?193.
Lauri Karttunen. 1976. Discourse referents. In James D.
McCawley, editor, Syntax and Semantics, volume 7:
Notes from the Linguistic Underground, pages 363?
385. Academic Press, New York.
William A. Ladusaw. 1980. On the notion ?affective?
in the analysis of negative polarity items. Journal of
Linguistic Research, 1(1):1?16.
Heeyoung Lee, Yves Peirsman, Angel Chang, Nathanael
Chambers, Mihai Surdeanu, and Dan Jurafsky. 2011.
Stanford?s multi-pass sieve coreference resolution sys-
tem at the CoNLL-2011 Shared Task. In Proceedings
of CoNLL-2011: Shared Task, pages 28?34.
Xiaoqiang Luo. 2005. On coreference resolution perfor-
mance metrics. In Proceedings of HLT-EMNLP 2005,
pages 25?32.
Barbara H. Partee. 1987. Noun phrase interpretation
and type-shifting principles. In Jeroen Groenendijk,
Dick de Jong, and Martin Stokhof, editors, Studies in
Discourse Representation Theory and the Theory of
Generalized Quantifiers, pages 115?143. Foris Publi-
cations, Dordrecht.
Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,
Olga Uryupina, and Yuchen Zhang. 2012. CoNLL-
2012 Shared Task: Modeling multilingual unrestricted
coreference in OntoNotes. In Proceedings of EMNLP
and CoNLL-2012: Shared Task, pages 1?40.
Ellen Prince. 1981a. On the inferencing of indefi-
nite ?this? NPs. In Bonnie Lynn Webber, Ivan Sag,
and Aravind Joshi, editors, Elements of Discourse Un-
derstanding, pages 231?250. Cambridge University
Press, Cambridge.
Ellen F. Prince. 1981b. Toward a taxonomy of given?
new information. In Peter Cole, editor, Radical Prag-
matics, pages 223?255. Academic Press, New York.
R Core Team, 2012. R: A Language and Environment
for Statistical Computing. R Foundation for Statistical
Computing, Vienna, Austria.
Marta Recasens and Eduard Hovy. 2009. A deeper
look into features for coreference resolution. In Sobha
Lalitha Devi, Anto?nio Branco, and Ruslan Mitkov, ed-
itors, Anaphora Processing and Applications, volume
5847 of Lecture Notes in Computer Science, pages 29?
42. Springer.
Marta Recasens and Eduard Hovy. 2011. BLANC: Im-
plementing the Rand index for coreference evaluation.
Natural Language Engineering, 17(4):485?510.
Craige Roberts. 1990. Modal Subordination, Anaphora,
and Distributivity. Garland, New York.
Craige Roberts. 1996. Anaphora in intensional contexts.
In Shalom Lappin, editor, The Handbook of Contem-
porary Semantic Theory, pages 215?246. Blackwell
Publishers, Oxford.
Johan Rooryck. 2001. Evidentiality, Part II. Glot Inter-
national, 5(5):161?168.
Roger Schwarzschild. 2002. Singleton indefinites. Jour-
nal of Semantics, 19(3):289?314.
Mandy Simons. 2007. Observations on embedding
verbs, evidentiality, and presupposition. Lingua,
117(6):1034?1056.
Ton van der Wouden. 1997. Negative Contexts: Col-
location, Polarity and Multiple Negation. Routledge,
London and New York.
Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A model-
theoretic coreference scoring scheme. In Proceedings
of MUC-6, pages 45?52.
Marilyn A. Walker, Aravind K. Joshi, and Ellen F. Prince,
editors. 1997. Centering in Discourse. Oxford Uni-
versity Press.
Marilyn A. Walker, Aravind K. Joshi, and Ellen F. Prince.
1998. Centering in naturally-occurring discourse: An
overview. In Marilyn A. Walker, Aravind K. Joshi,
and Ellen F. Prince, editors, Centering Theory in Dis-
course, pages 1?28, Oxford. Clarendon Press.
Linton Wang, Eric McCready, and Nicholas Asher. 2006.
Information dependency in quantificational subordina-
tion. In Klaus von Heusinger and Ken Turner, editors,
632
Where Semantics Meets Pragmatics, pages 267?304.
Elsevier Science, Amsterdam.
Gregory Ward and Betty Birner. 2004. Information
structure and non-canonical syntax. In Laurence R.
Horn and Gregory Ward, editors, The Handbook of
Pragmatics, pages 153?174. Blackwell, Oxford.
633
Proceedings of NAACL-HLT 2013, pages 897?906,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Same Referent, Different Words:
Unsupervised Mining of Opaque Coreferent Mentions
Marta Recasens*, Matthew Can?, and Dan Jurafsky*
*Linguistics Department, Stanford University, Stanford, CA 94305
?Computer Science Department, Stanford University, Stanford, CA 94305
recasens@google.com, {mattcan,jurafsky}@stanford.edu
Abstract
Coreference resolution systems rely heav-
ily on string overlap (e.g., Google Inc. and
Google), performing badly on mentions with
very different words (opaque mentions) like
Google and the search giant. Yet prior at-
tempts to resolve opaque pairs using ontolo-
gies or distributional semantics hurt precision
more than improved recall. We present a new
unsupervised method for mining opaque pairs.
Our intuition is to restrict distributional se-
mantics to articles about the same event, thus
promoting referential match. Using an En-
glish comparable corpus of tech news, we built
a dictionary of opaque coreferent mentions
(only 3% are in WordNet). Our dictionary can
be integrated into any coreference system (it
increases the performance of a state-of-the-art
system by 1% F1 on all measures) and is eas-
ily extendable by using news aggregators.
1 Introduction
Repetition is one of the most common coreferential
devices in written text, making string-match features
important to all coreference resolution systems. In
fact, the scores achieved by just head match and a
rudimentary form of pronominal resolution1 are not
far from that of state-of-the-art systems (Recasens
and Hovy, 2010). This suggests that opaque men-
tions (i.e., lexically different) such as iPad and the
Cupertino slate are a serious problem for modern
systems: they comprise 65% of the non-pronominal
1Closest NP with the same gender and number.
errors made by the Stanford system on the CoNLL-
2011 data. Solving this problem is critical for over-
coming the recall gap of state-of-the-art systems
(Haghighi and Klein, 2010; Stoyanov et al, 2009).
Previous systems have turned either to ontologies
(Ponzetto and Strube, 2006; Uryupina et al, 2011;
Rahman and Ng, 2011) or distributional semantics
(Yang and Su, 2007; Kobdani et al, 2011; Bansal
and Klein, 2012) to help solve these errors. But nei-
ther semantic similarity nor hypernymy are the same
as coreference: Microsoft and Google are distribu-
tionally similar but not coreferent; people is a hy-
pernym of both voters and scientists, but the peo-
ple can corefer with the voters, but is less likely
to corefer with the scientists. Thus ontologies lead
to precision problems, and to recall problems like
missing NE descriptions (e.g., Apple and the iPhone
maker) and metonymies (e.g., agreement and word-
ing), while distributional systems lead to precision
problems like coreferring Microsoft and the Moun-
tain View giant because of their similar vector rep-
resentation (release, software, update).
We increase precision by drawing on the intuition
that referents that are both similar and participate in
the same event are likely to corefer. We restrict dis-
tributional similarity to collections of articles that
discuss the same event. In the following two doc-
uments on the Nexus One from different sources,
we take the subjects of the identical verb release?
Google and the Mountain View giant?as coreferent.
Document 1: Google has released a software update.
Document 2: The Mountain View giant released an update.
Based on this idea, we introduce a new unsuper-
vised method that uses verbs in comparable corpora
897
as pivots for extracting the hard cases of corefer-
ence resolution, and build a dictionary of opaque
coreferent mentions (i.e., the dictionary entries are
pairs of mentions). This dictionary is then inte-
grated into the Stanford coreference system (Lee et
al., 2011), resulting in an average 1% improvement
in the F1 score of all the evaluation measures.
Our work points out the importance of context to
decide whether a specific mention pair is coreferent.
On the one hand, we need to know what semantic
relations are potentially coreferent (e.g., content and
video). On the other, we need to distinguish contexts
that are compatible for coreference?(1) and (2-a)?
from those that are not?(1) and (2-b).
(1) Elemental helps those big media entities process
content across a full slate of mobile devices.
(2) a. Elemental provides the picks and shovels to
make video work across multiple devices.
b. Elemental is powering the video for HBO Go.
Our dictionary of opaque coreferent pairs is our so-
lution to the first problem, and we report on some
preliminary work on context compatibility to ad-
dress the second problem.
2 Building a Dictionary for Coreference
To build a dictionary of semantic relations that are
appropriate for coreference we will use a cluster of
documents about the same news event, which we
call a story. Consider as an example the story Sprint
blocks out vacation days for employees. We deter-
mine using tf-idf the representative verbs for this
story, the main actions and events of the story (e.g.,
block out). Since these verbs are representative of
the story, different instances across documents in the
cluster are likely to refer to the same events (Sprint
blocks out. . . and the carrier blocks out. . . ). By the
same logic, the subjects and objects of the verbs are
also likely to be coreferent (Sprint and the carrier).
2.1 Comparable corpus
To build our dictionary, we require a monolingual
comparable corpus, containing clusters of docu-
ments from different sources that discuss the same
story. To ensure likely coreference, the story must
be the very same; documents that are merely clus-
tered by (general) topic do not suffice. The corpus
does not need to be parallel in the sense that docu-
ments in the same cluster do not need to be sentence
aligned.
We used Techmeme,2 a news aggregator for tech-
nology news, to construct a comparable corpus. Its
website lists the major tech stories, each with links
to several articles from different sources. We used
the Readability API3 to download and extract the ar-
ticle text for each document. We scraped two years
worth of data from Techmeme and only took stories
containing at least 5 documents. Our corpus con-
tains approximately 160 million words, 25k stories,
and 375k documents. Using a corpus from Tech-
meme means that our current coreference dictionary
is focused on the technological domain. Our method
can be easily extended to other domains, however,
since getting comparable corpora is relatively sim-
ple from the many similar news aggregator sites.
2.2 Extraction
After building our corpus, we used Stanford?s
CoreNLP tools4 to tokenize the text and annotate it
with POS tags and named entity types. We parsed
the text using the MaltParser 1.7, a linear time de-
pendency parser (Nivre et al, 2004).5
We then extracted the representative verbs of each
story by ranking the verbs in each story according
to their tf-idf scores. We took the top ten to be the
representative set. For each of these verbs, we clus-
tered together its subjects and objects (separately)
across instances of the verb in the document clus-
ter, excluding pronouns and NPs headed by the same
noun. For example, suppose that crawl is a represen-
tative verb and that in one document we have Google
crawls web pages and The search giant crawls sites
in another document. We will create the clusters
{Google, the search giant} and {web pages, sites}.
When detecting representative verbs, we kept
phrasal verbs as a unit (e.g., give up) and excluded
auxiliary and copular verbs,6 light verbs,7 and report
2http://www.techmeme.com
3http://www.readability.com/developers/api
4http://nlp.stanford.edu/software/corenlp.shtml
5http://www.maltparser.org
6Auxiliary and copular verbs include appear, be, become,
do, have, seem.
7Light verbs include do, get, give, go, have, keep, make, put,
set, take.
898
verbs,8 as they are rarely representative of a story
and tend to add noise to our dictionary. To increase
recall, we also considered the synonyms from Word-
Net and nominalizations from NomBank of the rep-
resentative verbs, thus clustering together the sub-
jects and objects of any synonym as well as the ar-
guments of nominalizations.9 We used syntactic re-
lations instead of semantic roles because the Malt-
Parser is faster than any SRL system, but we checked
for frequent syntactic structures in which the agent
and patient are inverted, such as passive and ergative
constructions.10
From each cluster of subject or object mentions,
we generated all pairs of mentions. This forms the
initial version of our dictionary. The next sections
describe how we filter and generalize these pairs.
2.3 Filtering
We manually analyzed 200 random pairs and clas-
sified them into coreference and spurious relations.
The spurious relations were caused by errors due to
the parser, the text extraction, and violations of our
algorithm assumption (i.e., the representative verb
does not refer to a unique event). We employed a fil-
tering strategy to improve the precision of the dictio-
nary. We used a total of thirteen simple rules, which
are shown in Table 1. For instance, we sometimes
get the same verb with non-coreferent arguments,
especially in tech news that compare companies or
products. In these cases, NEs are often used, and so
we can get rid of a large number of errors by auto-
matically removing pairs in which both mentions are
NEs (e.g., Google and Samsung).
Before filtering, 53% of all relations were good
coreference relations versus 47% spurious ones. Of
the relations that remained after filtering, 74% were
8Report verbs include argue, claim, say, suggest, tell, etc.
9As a general rule, we extract possessive phrases as subjects
(e.g. Samsung?s plan) and of -phrases as objects (e.g. develop-
ment of the new logo).
10We can easily detect passive subjects (i-b) as they have their
own dependency label, and ergative subjects (ii-b) using a list
of ergative verbs extracted from Levin (1993).
(i) a. Developers hacked the device.
b. The device was hacked.
(ii) a. Police scattered the crowds.
b. The crowds scattered.
Both mentions are NEs
Both mentions appear in the same document
Object of a negated verb
Enumeration or list environment
Sentence is ill-formed
Number NE
Temporal NE
Quantifying noun
Coordinated
Verb is preceded by a determiner or an adjective
Head is not nominal
Sentence length ? 100
Mention length ? 70% of sentence length
Table 1: Filters to improve the dictionary precision. Un-
less otherwise noted, the filter was applied if either men-
tion in the relation satisfied the condition.
coreferent and only 26% were spurious. In total,
about half of the dictionary relations were removed
in the filtering process, resulting in a total of 128,492
coreferent pairs.
2.4 Generalization
The final step of generating our dictionary is to pro-
cess the opaque mention pairs so that they gener-
alize better. We strip mentions of any determiners,
relative clauses, and -ing and -ed clauses. However,
we retain adjectives and prepositional modifiers be-
cause they are sometimes necessary for corefer-
ence to hold (e.g., online piracy and distribution
of pirated material). We also generalize NEs to
their types so that our dictionary entries can func-
tion as templates (e.g., Cook?s departure becomes
<person>?s departure), but we keep NE tokens that
are in the head position as these are pairs containing
world knowledge (e.g., iPad and slate). Finally, we
replace all tokens with their lemmas. Table 2 shows
a snapshot of the dictionary.
2.5 Semantics of coreference
From manually classifying a sample of 200 dictio-
nary pairs (e.g., Table 2), we find that our dictio-
nary includes many synonymy (e.g., IPO and offer-
ing) and hypernymy relations (e.g., phone and de-
vice), which are the relations that are typically ex-
tracted from ontologies for coreference resolution.
However, not all synonyms and hypernyms are valid
for coreference (recall the voters-people vs. scien-
tists-people example in the introduction), so our dic-
899
Mention 1 Mention 2
offering IPO
user consumer
phone device
Apple company
hardware key digital lock
iPad slate
content photo
bug issue
password login information
Google search giant
site company
filing complaint
company government
TouchPad tablet
medical record medical file
version handset
information credit card
government chairman
app software
Android platform
the leadership change <person>?s departure
change update
Table 2: Coreference relations in our dictionary.
tionary only includes the ones that are relevant for
coreference (e.g., update and change). Furthermore,
only 3% of our 128,492 opaque pairs are related in
WordNet, confirming that our method is introducing
a large number of new semantic relations.
We also discover other semantic relations that are
relevant for coreference, such as various metonymy
relations like mentioning the part for the whole.
Again though, we can use some part-whole rela-
tions coreferentially (e.g., car and engine) but not
others (e.g., car and window). Our dictionary in-
cludes part-whole relations that have been observed
as coreferent at least once (e.g., company and site).
We also extract world-knowledge descriptions for
NEs (e.g., Google and the Internet giant).
3 Integration into a Coreference System
We next integrated our dictionary into an existing
coreference resolution system to see if it improves
resolution.
3.1 Stanford coreference resolution system
Our baseline is the Stanford coreference resolution
system (Lee et al, 2011) which was the highest-
scoring system in the CoNLL-2011 Shared Task,
Sieve number Sieve name
1 Discourse processing
2 Exact string match
3 Relaxed string match
4 Precise constructs
5?7 Strict head match
8 Proper head noun match
9 Relaxed head match
10 Pronoun match
Table 3: Rules of the baseline system.
and was also part of the highest-scoring system in
the CoNLL-2012 Shared Task (Fernandes et al,
2012). It is a rule-based system that includes a to-
tal of ten rules (or ?sieves?) for entity coreference,
shown in Table 3. The sieves are applied from high-
est to lowest precision, each rule extending entities
(i.e., mention clusters) built by the previous tiers, but
never modifying links previously made. The major-
ity of the sieves rely on string overlap.11
The highly modular architecture made it easy for
us to integrate additional sieves using our dictionary
to increase recall.
3.2 Dictionary sieves
We propose four new sieves, each one using a differ-
ent granularity level from our dictionary, with each
consecutive sieve using higher precision relations
than the previous one. The Dict 1 sieve uses only
the heads of mentions in each relation (e.g., devices).
Dict 2 uses the heads and one premodifier, if it ex-
ists (e.g., iOS devices). Dict 3 uses the heads and up
to two premodifiers (e.g., new iOS devices). Dict 4
uses the full mentions, including any postmodifiers
(e.g., new iOS devices for businesses).
We take advantage of frequency counts to get rid
of low-precision coreference pairs and only keep
(i) pairs that have been seen more than 75 times
(Dict 1) or 15 times (Dict 2, Dict 3, Dict 4);
and (ii) pairs with a frequency count larger than 8
(Dict 1) or 2 (Dict 2, Dict 3, Dict 4) and a normal-
ized PMI score larger than 0.18. We use the nor-
malized PMI score (Bouma, 2009) as a measure of
association between the mentions mi and mj of a
11Exceptions: sieve 1 links first-person pronouns inside a
quotation with the speaker; sieve 4 links mention pairs that ap-
pear in an appositive, copular, acronym, etc., construction; sieve
10 implements generic pronominal coreference resolution.
900
dictionary pair, computed as
(ln p(mi,mj)p(mi)p(mj)) /? ln p(mi,mj)
These thresholds were set on the development set.
Since the different coreference rules in the Stan-
ford system are arranged in decreasing order of pre-
cision, we start by applying the sieve that uses the
highest-precision relations in the dictionary (Dict 4),
followed by Dict 3, Dict 2, and Dict 1. We add
these new sieves right before the last sieve, as the
pronominal sieve can perform better if opaque men-
tions have been successfully linked. The current
sieves only use the dictionary for linking singular
mentions, as the experiments on the dev showed that
plural mentions brought too much noise.
For any mention pair under analysis, each sieve
checks whether it is supported by the dictionary as
well as whether basic constraints are satisfied, such
as number, animacy and NE-type agreement, and
NE?common noun order (not the opposite).
4 Experiments
4.1 Data
Although our dictionary creation technology can ap-
ply across domains, our current coreference dictio-
nary is focused on the technical domain, so we cre-
ated a coreference labeled corpus in this domain for
evaluation. We extracted new data from Techmeme
(different from that used to extract the dictionary) to
create a development and a test set. It is important
to note that we do not need comparable data at this
stage. A massive comparable corpus is only needed
for mining the coreference dictionary (Section 2);
once it is built, it can be used for solving corefer-
ence within and across documents.
The annotation was performed by two experts, us-
ing the Callisto annotation tool. The development
and test sets were annotated with coreference rela-
tions following the OntoNotes guidelines (Pradhan
et al, 2007). We annotated full NPs (with all mod-
ifiers), excluding appositive phrases and predicate
nominals. Only premodifiers that were proper nouns
or possessive phrases were annotated. We extended
the OntoNotes guidelines by also annotating single-
tons. Table 4 shows the dataset statistics.
Dataset Stories Docs Tokens Entities Mentions
Dev 4 27 7837 1360 2279
Test 24 24 8547 1341 2452
Table 4: Dataset statistics: development (dev) and test.
4.2 Evaluation measures
We evaluated using six coreference measures, as
they sometimes provide different results and there is
no agreement on a standard. We used the scorer of
the CoNLL-2011 Shared Task (Pradhan et al, 2011).
? MUC (Vilain et al, 1995). Link-based metric
that measures how many links the true and sys-
tem partitions have in common.
? B3 (Bagga and Baldwin, 1998). Mention-based
metric that measures the proportion of mention
overlap between gold and predicted entities.
? CEAF-?3 (Luo, 2005). Mention-based metric
that, unlike B3, enforces a one-to-one align-
ment between gold and predicted entities.
? CEAF-?4 (Luo, 2005). The entity-based ver-
sion of the above metric.
? BLANC (Recasens and Hovy, 2011). Link-
based metric that considers both coreference
and non-coreference links.
? CoNLL (Denis and Baldridge, 2009). Average
of MUC, B3 and CEAF-?4. It was the official
metric of the CoNLL-2011 Shared Task.
4.3 Results
We always start from the baseline, which corre-
sponds to the Stanford system with the sieves listed
in Table 3. This is the set of sieves that won the
CoNLL-2011 Shared Task (Pradhan et al, 2011),
and they exclude WordNet.
Table 5 shows the incremental scores, on the de-
velopment set, for the four sieves that use the dictio-
nary, corresponding to the different granularity lev-
els, from the highest precision one (Dict 4) to the
lowest one (Dict 1). The largest improvement is
achieved by Dict 4 and Dict 3, as they improve re-
call (R) without hurting precision (P). R is equiva-
lent to P for CEAF-?4, and vice versa. The other
two sieves increase R further, especially Dict 1,
but also decrease P, although the trade-off for the
F-score (F1) is still positive. It is the best score, with
the exception of B3.
901
MUC B3 CEAF-?3 CEAF-?4 BLANC CoNLL
System R P F1 R P F1 R / P / F1 R P F1 R P B F1
Baseline 55.9 72.8 63.3 74.1 89.8 81.2 74.6 85.2 73.6 79.0 66.6 87.1 72.6 74.5
+Dict 4 57.0 72.8 63.9 75.1 89.4 81.6 75.3 85.2 74.3 79.4 68.2 87.3 74.2 75.0
+Dict 3 57.6 72.8 64.3 75.4 89.3 81.7 75.5 85.1 74.6 79.5 68.4 87.2 74.4 75.2
+Dict 2 57.6 72.5 64.2 75.4 89.1 81.7 75.4 85.0 74.6 79.5 68.4 87.0 74.3 75.1
+Dict 1 58.4 71.9 64.5 75.7 88.5 81.6 75.5 84.6 75.1 79.6 68.6 86.6 74.4 75.2
Table 5: Incremental results for the four sieves using our dictionary on the development set. Baseline is the Stanford
system without the WordNet sieves. Scores are on gold mentions.
MUC B3 CEAF-?3 CEAF-?4 BLANC CoNLL
System R P F1 R P F1 R / P / F1 R P F1 R P B F1
Baseline 62.4 78.2 69.4 73.7 89.5 80.8 75.1 86.2 73.8 79.5 71.4 88.6 77.3 76.6
w/ WN 63.5 75.3 68.9 74.2 87.5 80.3 74.1 83.7 74.1 78.6 71.8 87.3 77.3 75.9
w/ Dict 64.7* 77.6* 70.6* 75.7* 88.5* 81.6* 76.5* 85.3* 75.0* 79.9* 74.6* 88.6 79.9* 77.3*
w/ Dict +
Context
64.8* 77.8* 70.7* 75.7* 88.6* 81.7* 76.5* 85.5* 75.1* 80.0* 74.6* 88.7 79.9* 77.5*
Table 6: Performance on the test set. Scores are on gold mentions. Stars indicate a statistically significant difference
with respect to the baseline.
Table 6 reports the scores on the test set and com-
pares the scores obtained by adding the WordNet
sieves to the baseline (w/ WN) with those obtained
by adding the dictionary sieves (w/ Dict). Whereas
adding WordNet only brings a small improvement
in R that is much lower than the loss in P, the dic-
tionary sieves succeed in increasing R by a larger
amount and at a smaller cost to P, resulting in a sig-
nificant improvement in F1: 1.2 points according to
MUC, 0.8 points according to B3, 1.4 points accord-
ing to CEAF-?3, 0.4 points according to CEAF-?4,
2.6 points according to BLANC, and 0.7 points ac-
cording to CoNLL. Section 5.2 presents the last line
(w/ Dict + Context).
5 Discussion
5.1 Error analysis
Thanks to the dictionary, the coreference system im-
proves the baseline by establishing coreference links
between the bolded mentions in (3) and (4).
(3) With Groupon Inc.?s stock down by half from its IPO
price and the company heading into its first earnings
report since an accounting blowup [...] outlining op-
portunity ahead and the promise of new products for
the daily-deals company.
(4) Thompson revealed the diagnosis as evidence arose
that seemed to contradict his story about why he was
not responsible for a degree listed on his resume that
he does not have, the newspaper reports, citing anony-
mous sources familiar with the situation [...] a Yahoo
board committee appointed to investigate the matter.
The first case requires world knowledge and the sec-
ond case, semantic knowledge.
We manually analyzed 40 false positive errors
caused by the dictionary sieves. Only a small num-
ber of them were due to noise in the dictionary. The
majority of errors were due to the discourse context:
the two mentions could be coreferent, but not in the
given context. For example, Apple and company are
potentially coreferent?which is successfully cap-
tured by our dictionary?and while they are coref-
erent in (5), they are not in (6).12
(5) It will only get better as Apple will be updating it
with iOS6, an operating system that the company will
likely be showing off this summer.
(6) Since Apple reinvented the segment, Microsoft is the
latest entrant into the tablet market, banking on its
Windows 8 products to bridge the gap between PCs
and tablets. [...] The company showed off Windows 8
last September.
12Examples in this section show gold coreference relations in
bold and incorrectly predicted coreferent mentions in italics.
902
In these cases it does not suffice to check whether
the opaque mention pair is included in the corefer-
ence dictionary, but we need a method for taking the
surrounding context into account. In the next section
we present our preliminary work in this direction.
5.2 Context fit
To help the coreference system choose the right an-
tecedent in examples like (6), we exploit the fact
that the company is closely followed by Windows 8,
which is a clue for selecting Microsoft instead of Ap-
ple as the antecedent. We devise a contextual con-
straint that rules out a mention pair if the contexts are
incompatible. To check for context compatibility,
we borrow the idea of topic signatures from Lin and
Hovy (2000) and that Agirre et al (2001) used for
Word Sense Disambiguation. Instead of identifying
the keywords of a topic, we find the NEs that tend
to co-occur with another NE. For example, the sig-
nature for Apple should include terms like iPhone,
MacBook, iOS, Steve Jobs, etc. This is what we call
the NE signature for Apple.
To construct NE signatures, we first compute the
log-likelihood ratio (LLR) statistic between NEs in
our corpus (the same one used to build the dictio-
nary). Then, the signature for a NE, w, is the list of
k other NEs that have the highest LLR with w. The
LLR between two NEs, w1 and w2, is ?2 ln
L(H1)
L(H2)
,
where H1 is the hypothesis that
P (w1 ? sent|w2 ? sent) = P (w1 ? sent|w2 /? sent),
H2 is the hypothesis that
P (w1 ? sent|w2 ? sent) 6= P (w1 ? sent|w2 /? sent),
and L(?) is the likelihood. We assume a binomial
distribution for the likelihood.
Once we have NE signatures, we determine the
context fit as follows. When the system compares a
NE antecedent with a (non-NE) anaphor, we check
whether any NEs in the anaphor?s sentence are in
the antecedent?s signature. We also check whether
the antecedent is in the signature list of any NE?s in
the anaphor?s sentence. If neither of these is true,
we do not allow the system to link the antecedent
and the anaphor. In (6), Apple is not linked with the
company because it is not in Windows? signature,
and Windows is not in Apple?s signature either (but
Microsoft is in Windows? signature).
The last two lines in Table 6 compare the scores
without using this contextual feature (w/ Dict) with
those using context (w/ Dict + Context). Our feature
for context compatibility leads to a small but posi-
tive improvement, taking the final improvement of
the dictionary sieves to be about 1 percentage point
above the baseline according to all six evaluation
measures. We leave as future work to test this idea
on a larger test set and refine it further so as to ad-
dress more challenging cases where comparing NEs
is not enough, like in (7).
(7) Snapchat will notify users [...] The program is avail-
able for free in Apple?s App Store [...] While the com-
pany ?attempts to delete image data as soon as possi-
ble after the message is transmitted,? it cannot guaran-
tee messages will always be deleted.
To resolve (7), it would be helpful to know that
Snapchat is a picture messaging platform, as the
context mentions image data and messages.
6 Related Work
Existing ontologies are not optimal for solving
opaque coreferent mentions because of both a preci-
sion and a recall problem (Lee et al, 2011; Uryupina
et al, 2011). On the other hand, using data-driven
methods such as distributional semantics for coref-
erence resolution suffers especially from a precision
problem (Ng, 2007). Our work combines ideas from
distributional semantics and paraphrase acquisition
methods in order to efficiently use contextual infor-
mation to extract coreference relations.
The main idea that we borrow from paraphrase
acquisition is the use of monolingual (non-parallel)
comparable corpora, which have been exploited
to extract both sentence-level (Barzilay and McK-
eown, 2001) and sub-sentential-level paraphrases
(Shinyama and Sekine, 2003; Wang and Callison-
Burch, 2011). To ensure that the NPs are coreferent,
we limit the meaning of comparable corpora to col-
lections of documents that report on the very same
story, as opposed to collections of documents that
are about the same (general) topic. However, the
distinguishing factor is that while most paraphrasing
studies, including Lin and Pantel (2001), use NEs?
or nouns in general?as pivots to learn paraphrases
of their surrounding context, we use verbs as pivots
to learn coreference relations at the NP level.
There are many similarities between paraphrase
and coreference, and our work is most similar to
903
that by Wang and Callison-Burch (2011). However,
some paraphrases that might not be considered to
be valid (e.g., under $200 and around $200) can
be acceptable coreference relations. Unlike Wang
and Callison-Burch (2011), we do not work on doc-
ument pairs but on sets of at least five (comparable)
documents, and we do not require sentence align-
ment, but just verb alignment.
Another source of inspiration is the work by Bean
and Riloff (2004). They use contextual roles (i.e.,
the role that an NP plays in an event) for extract-
ing patterns that can be used in coreference reso-
lution, showing the relevance of verbs in deciding
on coreference between their arguments. However,
they use a very small corpus (two domains) and do
not aim to build a dictionary. The idea of creating
a repository of extracted concept-instance relations
appears in Fleischman et al (2003), but restricted
to person-role pairs, e.g. Yasser Arafat and leader.
Although it was originally designed for answering
who-is questions, Daume? III and Marcu (2005) suc-
cessfully used it for coreference resolution.
The coreference relations that we extract might
overlap but go beyond those detected by Bansal and
Klein (2012)?s Web-based features. First, they focus
on NP headwords, while we extract full NPs, includ-
ing multi-word mentions. Second, the fact that they
use the Google n-gram corpus means that the two
headwords must appear at most four words apart,
thus ruling out coreferent mentions that can only ap-
pear far from each other. Finally, while their extrac-
tion patterns focus on synonymy and hypernymy re-
lations, we discover other types of semantic relations
that are relevant for coreference (Section 2.5).
7 Conclusions
We have pointed out an important problem with cur-
rent coreference resolution systems: their heavy re-
liance on string overlap. Pronouns aside, opaque
mentions account for 65% of the errors made by
state-of-the-art systems. To improve coreference
scores beyond 60-70%, we therefore need to make
better use of semantic and world knowledge to deal
with non-identical-string coreference. But, as we
have also shown, coreference is not the same as se-
mantic similarity or hypernymy. Only certain se-
mantic relations in certain contexts are good cues for
coreference. We therefore need semantic resources
specifically targeted at coreference.
We proposed a new solution for detecting opaque
mention pairs: restricting distributional similarity to
a comparable corpus of articles about the very same
story, thus ensuring that similar mentions will also
likely be coreferent. We used this corpus to build a
dictionary focused on coreference, and successfully
extracted the specific semantic and world knowledge
relevant for coreference. The resulting dictionary
can be added on top of any coreference system to
increase recall at a minimum cost to precision. Inte-
grated into the Stanford coreference resolution sys-
tem, which won the CoNLL-2011 shared task, the
F-score increases about 1 percentage point accord-
ing to all of the six evaluation measures. The dictio-
nary and NE signatures are available on the Web.13
We showed that apart from the need for extracting
coreference-specific semantic and world knowledge,
we need to take into account the context surrounding
the mentions. The results from our preliminary work
for identifying incompatible contexts is promising.
Our unsupervised method for extracting opaque
coreference relations can be easily extended to other
domains by using online news aggregators, and
trained on more data to build a more comprehensive
dictionary that can increase recall even further. We
integrated the dictionary into a rule-based corefer-
ence system, but it remains for future work to in-
tegrate it into a learning-based architecture, where
the system can combine the dictionary features with
other features. This can also make it easier to in-
clude contextual features that take into account how
well a dictionary pair fits in a specific context.
Acknowledgments
We would like to thank the members of the Stanford
NLP Group, Valentin Spitkovsky, and Ed Hovy for
valuable comments at various stages of the project.
The first author was supported by a Beatriu de
Pino?s postdoctoral scholarship (2010 BP-A 00149)
from Generalitat de Catalunya. We also gratefully
acknowledge the support of Defense Advanced Re-
search Projects Agency (DARPA) Machine Read-
ing Program under Air Force Research Laboratory
(AFRL) prime contract no. FA8750-09-C-0181.
13http://nlp.stanford.edu/pubs/coref-dictionary.zip
904
References
Eneko Agirre, Olatz Ansa, David Martinez, and Eduard
Hovy. 2001. Enriching wordnet concepts with topic
signatures. In Proceedings of the NAACLWorkshop on
WordNet and Other Lexical Resources: Applications,
Extensions and Customizations, pages 23?28.
Amit Bagga and Breck Baldwin. 1998. Algorithms
for scoring coreference chains. In Proceedings of
the LREC 1998 Workshop on Linguistic Coreference,
pages 563?566.
Mohit Bansal and Dan Klein. 2012. Coreference seman-
tics from web features. In Proceedings of ACL, pages
389?398.
Regina Barzilay and Kathleen McKeown. 2001. Extract-
ing paraphrases from a parallel corpus. In Proceedings
of ACL, pages 50?57.
David Bean and Ellen Riloff. 2004. Unsupervised learn-
ing of contextual role knowledge for coreference reso-
lution. In Proceedings of NAACL-HTL.
Geolof Bouma. 2009. Normalized (pointwise) mutual
information in collocation extraction. In Proceedings
of the Biennial GSCL Conference, pages 31?40.
Hal Daume? III and Daniel Marcu. 2005. A large-scale
exploration of effective global features for a joint en-
tity detection and tracking model. In Proceedings of
HLT-EMNLP, pages 97?104.
Pascal Denis and Jason Baldridge. 2009. Global joint
models for coreference resolution and named entity
classification. Procesamiento del Lenguaje Natural,
42:87?96.
Eraldo Fernandes, C??cero dos Santos, and Ruy Milidiu?.
2012. Latent structure perceptron with feature induc-
tion for unrestricted coreference resolution. In Pro-
ceedings of CoNLL - Shared Task, pages 41?48.
Michael Fleischman, Eduard Hovy, and Abdessamad
Echihabi. 2003. Offline strategies for online question
answering: answering questions before they are asked.
In Proceedings of ACL, pages 1?7.
Aria Haghighi and Dan Klein. 2010. Coreference resolu-
tion in a modular, entity-centered model. In Proceed-
ings of HLT-NAACL, pages 385?393.
Hamidreza Kobdani, Hinrich Schu?tze, Michael
Schiehlen, and Hans Kamp. 2011. Bootstrap-
ping coreference resolution using word associations.
In Proceedings of ACL, pages 783?792.
Heeyoung Lee, Yves Peirsman, Angel Chang, Nathanael
Chambers, Mihai Surdeanu, and Dan Jurafsky. 2011.
Stanford?s multi-pass sieve coreference resolution sys-
tem at the CoNLL-2011 Shared Task. In Proceedings
of CoNLL - Shared Task, pages 28?34.
Beth Levin. 1993. English Verb Class and Alternations:
A Preliminary Investigation. University of Chicago
Press, Chicago.
Chin-Yew Lin and Eduard Hovy. 2000. The automated
acquisition of topic signatures for text summarization.
In Proceedings of COLING, pages 495?501.
Dekang Lin and Patrick Pantel. 2001. DIRT - Discov-
ery of inference rules from text. In Proceedings of the
ACM SIGKDD, pages 323?328.
Xiaoqiang Luo. 2005. On coreference resolution perfor-
mance metrics. In Proceedings of HLT-EMNLP, pages
25?32.
Vincent Ng. 2007. Shallow semantics for coreference
resolution. In Proceedings of IJCAI, pages 1689?
1694.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2004.
Memory-based dependency parsing. In Proceedings
of CoNLL, pages 49?56.
Simone Paolo Ponzetto and Michael Strube. 2006.
Exploiting semantic role labeling, WordNet and
Wikipedia for coreference resolution. In Proceedings
of HLT-NAACL, pages 192?199.
Sameer S. Pradhan, Lance Ramshaw, Ralph Weischedel,
Jessica MacBride, and Linnea Micciulla. 2007. Unre-
stricted coreference: Identifying entities and events in
OntoNotes. In Proceedings of ICSC, pages 446?453.
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Nianwen Xue.
2011. CoNLL-2011 Shared Task: Modeling unre-
stricted coreference in OntoNotes. In Proceedings of
CoNLL - Shared Task, pages 1?27.
Altaf Rahman and Vincent Ng. 2011. Coreference reso-
lution with world knowledge. In Proceedings of ACL,
pages 814?824.
Marta Recasens and Eduard Hovy. 2010. Corefer-
ence resolution across corpora: Languages, coding
schemes, and preprocessing information. In Proceed-
ings of ACL, pages 1423?1432.
Marta Recasens and Eduard Hovy. 2011. BLANC: Im-
plementing the Rand index for coreference evaluation.
Natural Language Engineering, 17(4):485?510.
Yusuke Shinyama and Satoshi Sekine. 2003. Paraphrase
acquisition for information extraction. In Proceedings
of ACL, pages 65?71.
Veselin Stoyanov, Nathan Gilbert, Claire Cardie, and
Ellen Riloff. 2009. Conundrums in noun phrase coref-
erence resolution: Making sense of the state-of-the-art.
In Proceedings of ACL-IJCNLP, pages 656?664.
Olga Uryupina, Massimo Poesio, Claudio Giuliano, and
Kateryna Tymoshenko. 2011. Disambiguation and
filtering methods in using web knowledge for coref-
erence resolution. In Proceedings of FLAIRS, pages
317?322.
Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A model-
theoretic coreference scoring scheme. In Proceedings
of MUC-6, pages 45?52.
905
Rui Wang and Chris Callison-Burch. 2011. Para-
phrase fragment extraction from monolingual compa-
rable corpora. In Proceedings of the 4th ACL Work-
shop on Building and Using Comparable Corpora,
pages 52?60.
Xiaofeng Yang and Jian Su. 2007. Coreference resolu-
tion using semantic relatedness information from auto-
matically discovered patterns. In Proceedings of ACL,
pages 528?535.
906
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1423?1432,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Coreference Resolution across Corpora:
Languages, Coding Schemes, and Preprocessing Information
Marta Recasens
CLiC - University of Barcelona
Gran Via 585
Barcelona, Spain
mrecasens@ub.edu
Eduard Hovy
USC Information Sciences Institute
4676 Admiralty Way
Marina del Rey CA, USA
hovy@isi.edu
Abstract
This paper explores the effect that dif-
ferent corpus configurations have on the
performance of a coreference resolution
system, as measured by MUC, B3, and
CEAF. By varying separately three param-
eters (language, annotation scheme, and
preprocessing information) and applying
the same coreference resolution system,
the strong bonds between system and cor-
pus are demonstrated. The experiments
reveal problems in coreference resolution
evaluation relating to task definition, cod-
ing schemes, and features. They also ex-
pose systematic biases in the coreference
evaluation metrics. We show that system
comparison is only possible when corpus
parameters are in exact agreement.
1 Introduction
The task of coreference resolution, which aims to
automatically identify the expressions in a text that
refer to the same discourse entity, has been an in-
creasing research topic in NLP ever since MUC-6
made available the first coreferentially annotated
corpus in 1995. Most research has centered around
the rules by which mentions are allowed to corefer,
the features characterizing mention pairs, the algo-
rithms for building coreference chains, and coref-
erence evaluation methods. The surprisingly im-
portant role played by different aspects of the cor-
pus, however, is an issue to which little attention
has been paid. We demonstrate the extent to which
a system will be evaluated as performing differ-
ently depending on parameters such as the corpus
language, the way coreference relations are de-
fined in the corresponding coding scheme, and the
nature and source of preprocessing information.
This paper unpacks these issues by running the
same system?a prototype entity-based architec-
ture called CISTELL?on different corpus config-
urations, varying three parameters. First, we show
how much language-specific issues affect perfor-
mance when trained and tested on English and
Spanish. Second, we demonstrate the extent to
which the specific annotation scheme (used on the
same corpus) makes evaluated performance vary.
Third, we compare the performance using gold-
standard preprocessing information with that us-
ing automatic preprocessing tools.
Throughout, we apply the three principal coref-
erence evaluation measures in use today: MUC,
B3, and CEAF. We highlight the systematic prefer-
ences of each measure to reward different config-
urations. This raises the difficult question of why
one should use one or another evaluation mea-
sure, and how one should interpret their differ-
ences in reporting changes of performance score
due to ?secondary? factors like preprocessing in-
formation.
To this end, we employ three corpora: ACE
(Doddington et al, 2004), OntoNotes (Pradhan
et al, 2007), and AnCora (Recasens and Mart??,
2009). In order to isolate the three parameters
as far as possible, we benefit from a 100k-word
portion (from the TDT collection) that is common
to both ACE and OntoNotes. We apply the same
coreference resolution system in all cases. The re-
sults show that a system?s score is not informative
by itself, as different corpora or corpus parameters
lead to different scores. Our goal is not to achieve
the best performance to date, but rather to ex-
pose various issues raised by the choices of corpus
preparation and evaluation measure and to shed
light on the definition, methods, evaluation, and
complexities of the coreference resolution task.
The paper is organized as follows. Section 2
sets our work in context and provides the motiva-
tions for undertaking this study. Section 3 presents
the architecture of CISTELL, the system used in
the experimental evaluation. In Sections 4, 5,
1423
and 6, we describe the experiments on three differ-
ent datasets and discuss the results. We conclude
in Section 7.
2 Background
The bulk of research on automatic coreference res-
olution to date has been done for English and used
two different types of corpus: MUC (Hirschman
and Chinchor, 1997) and ACE (Doddington et al,
2004). A variety of learning-based systems have
been trained and tested on the former (Soon et al,
2001; Uryupina, 2006), on the latter (Culotta et
al., 2007; Bengtson and Roth, 2008; Denis and
Baldridge, 2009), or on both (Finkel and Manning,
2008; Haghighi and Klein, 2009). Testing on both
is needed given that the two annotation schemes
differ in some aspects. For example, only ACE
includes singletons (mentions that do not corefer)
and ACE is restricted to seven semantic types.1
Also, despite a critical discussion in the MUC task
definition (van Deemter and Kibble, 2000), the
ACE scheme continues to treat nominal predicates
and appositive phrases as coreferential.
A third coreferentially annotated corpus?the
largest for English?is OntoNotes (Pradhan et al,
2007; Hovy et al, 2006). Unlike ACE, it is not
application-oriented, so coreference relations be-
tween all types of NPs are annotated. The identity
relation is kept apart from the attributive relation,
and it also contains gold-standard morphological,
syntactic and semantic information.
Since the MUC and ACE corpora are annotated
with only coreference information,2 existing sys-
tems first preprocess the data using automatic tools
(POS taggers, parsers, etc.) to obtain the infor-
mation needed for coreference resolution. How-
ever, given that the output from automatic tools
is far from perfect, it is hard to determine the
level of performance of a coreference module act-
ing on gold-standard preprocessing information.
OntoNotes makes it possible to separate the coref-
erence resolution problem from other tasks.
Our study adds to the previously reported evi-
dence by Stoyanov et al (2009) that differences in
corpora and in the task definitions need to be taken
into account when comparing coreference resolu-
tion systems. We provide new insights as the cur-
rent analysis differs in four ways. First, Stoyanov
1The ACE-2004/05 semantic types are person, organiza-
tion, geo-political entity, location, facility, vehicle, weapon.
2ACE also specifies entity types and relations.
et al (2009) report on differences between MUC
and ACE, while we contrast ACE and OntoNotes.
Given that ACE and OntoNotes include some of
the same texts but annotated according to their re-
spective guidelines, we can better isolate the effect
of differences as well as add the additional dimen-
sion of gold preprocessing. Second, we evaluate
not only with the MUC and B3 scoring metrics,
but also with CEAF. Third, all our experiments
use true mentions3 to avoid effects due to spuri-
ous system mentions. Finally, including different
baselines and variations of the resolution model al-
lows us to reveal biases of the metrics.
Coreference resolution systems have been
tested on languages other than English only within
the ACE program (Luo and Zitouni, 2005), prob-
ably due to the fact that coreferentially annotated
corpora for other languages are scarce. Thus there
has been no discussion of the extent to which sys-
tems are portable across languages. This paper
studies the case of English and Spanish.4
Several coreference systems have been devel-
oped in the past (Culotta et al, 2007; Finkel
and Manning, 2008; Poon and Domingos, 2008;
Haghighi and Klein, 2009; Ng, 2009). It is not our
aim to compete with them. Rather, we conduct
three experiments under a specific setup for com-
parison purposes. To this end, we use a different,
neutral, system, and a dataset that is small and dif-
ferent from official ACE test sets despite the fact
that it prevents our results from being compared
directly with other systems.
3 Experimental Setup
3.1 System Description
The system architecture used in our experiments,
CISTELL, is based on the incrementality of dis-
course. As a discourse evolves, it constructs a
model that is updated with the new information
gradually provided. A key element in this model
are the entities the discourse is about, as they form
the discourse backbone, especially those that are
mentioned multiple times. Most entities, however,
are only mentioned once. Consider the growth of
the entity Mount Popocate?petl in (1).5
3The adjective true contrasts with system and refers to the
gold standard.
4Multilinguality is one of the focuses of SemEval-2010
Task 1 (Recasens et al, 2010).
5Following the ACE terminology, we use the term men-
tion for an instance of reference to an object, and entity for a
collection of mentions referring to the same object. Entities
1424
(1) We have an update tonight on [this, the volcano in
Mexico, they call El Popo]m3 . . . As the sun rises
over [Mt. Popo]m7 tonight, the only hint of the fire
storm inside, whiffs of smoke, but just a few hours
earlier, [the volcano]m11 exploding spewing rock
and red-hot lava. [The fourth largest mountain in
North America, nearly 18,000 feet high]m15, erupt-
ing this week with [its]m20 most violent outburst in
1,200 years.
Mentions can be pronouns (m20), they can be a
(shortened) string repetition using either the name
(m7) or the type (m11), or they can add new infor-
mation about the entity: m15 provides the super-
type and informs the reader about the height of the
volcano and its ranking position.
In CISTELL,6 discourse entities are conceived
as ?baskets?: they are empty at the beginning of
the discourse, but keep growing as new attributes
(e.g., name, type, location) are predicated about
them. Baskets are filled with this information,
which can appear within a mention or elsewhere
in the sentence. The ever-growing amount of in-
formation in a basket alows richer comparisons to
new mentions encountered in the text.
CISTELL follows the learning-based corefer-
ence architecture in which the task is split into
classification and clustering (Soon et al, 2001;
Bengtson and Roth, 2008) but combines them si-
multaneously. Clustering is identified with basket-
growing, the core process, and a pairwise clas-
sifier is called every time CISTELL considers
whether a basket must be clustered into a (grow-
ing) basket, which might contain one or more
mentions. We use a memory-based learning clas-
sifier trained with TiMBL (Daelemans and Bosch,
2005). Basket-growing is done in four different
ways, explained next.
3.2 Baselines and Models
In each experiment, we compute three baselines
(1, 2, 3), and run CISTELL under four different
models (4, 5, 6, 7).
1. ALL SINGLETONS. No coreference link is
ever created. We include this baseline given
the high number of singletons in the datasets,
since some evaluation measures are affected
by large numbers of singletons.
2. HEAD MATCH. All non-pronominal NPs that
have the same head are clustered into the
same entity.
containing one single mention are referred to as singletons.
6?Cistell? is the Catalan word for ?basket.?
3. HEAD MATCH + PRON. Like HEAD MATCH,
plus allowing personal and possessive pro-
nouns to link to the closest noun with which
they agree in gender and number.
4. STRONG MATCH. Each mention (e.g., m11) is
paired with previous mentions starting from
the beginning of the document (m1?m11, m2?
m11, etc.).7 When a pair (e.g., m3?m11) is
classified as coreferent, additional pairwise
checks are performed with all the mentions
contained in the (growing) entity basket (e.g.,
m7?m11). Only if all the pairs are classified
as coreferent is the mention under consider-
ation attached to the existing growing entity.
Otherwise, the search continues.8
5. SUPER STRONG MATCH. Similar to STRONG
MATCH but with a threshold. Coreference
pairwise classifications are only accepted
when TiMBL distance is smaller than 0.09.9
6. BEST MATCH. Similar to STRONG MATCH
but following Ng and Cardie (2002)?s best
link approach. Thus, the mention under anal-
ysis is linked to the most confident men-
tion among the previous ones, using TiMBL?s
confidence score.
7. WEAK MATCH. A simplified version of
STRONG MATCH: not all mentions in the
growing entity need to be classified as coref-
erent with the mention under analysis. A sin-
gle positive pairwise decision suffices for the
mention to be clustered into that entity.10
3.3 Features
We follow Soon et al (2001), Ng and Cardie
(2002) and Luo et al (2004) to generate most
of the 29 features we use for the pairwise
model. These include features that capture in-
formation from different linguistic levels: textual
strings (head match, substring match, distance,
frequency), morphology (mention type, coordi-
nation, possessive phrase, gender match, number
match), syntax (nominal predicate, apposition, rel-
ative clause, grammatical function), and semantic
match (named-entity type, is-a type, supertype).
7The opposite search direction was also tried but gave
worse results.
8Taking the first mention classified as coreferent follows
Soon et al (2001)?s first-link approach.
9In TiMBL, being a memory-based learner, the closer the
distance to an instance, the more confident the decision. We
chose 0.09 because it appeared to offer the best results.
10STRONG and WEAK MATCH are similar to Luo et al
(2004)?s entity-mention and mention-pair models.
1425
For Spanish, we use 34 features as a few varia-
tions are needed for language-specific issues such
as zero subjects (Recasens and Hovy, 2009).
3.4 Evaluation
Since they sometimes provide quite different re-
sults, we evaluate using three coreference mea-
sures, as there is no agreement on a standard.
? MUC (Vilain et al, 1995). It computes the
number of links common between the true
and system partitions. Recall (R) and preci-
sion (P) result from dividing it by the mini-
mum number of links required to specify the
true and the system partitions, respectively.
? B3 (Bagga and Baldwin, 1998). R and P are
computed for each mention and averaged at
the end. For each mention, the number of
common mentions between the true and the
system entity is divided by the number of
mentions in the true entity or in the system
entity to obtain R and P, respectively.
? CEAF (Luo, 2005). It finds the best one-to-
one alignment between true and system en-
tities. Using true mentions and the ?3 sim-
ilarity function, R and P are the same and
correspond to the number of common men-
tions between the aligned entities divided by
the total number of mentions.
4 Parameter 1: Language
The first experiment compared the performance
of a coreference resolution system on a Germanic
and a Romance language?English and Spanish?
to explore to what extent language-specific issues
such as zero subjects11 or grammatical gender
might influence a system.
Although OntoNotes and AnCora are two dif-
ferent corpora, they are very similar in those as-
pects that matter most for the study?s purpose:
they both include a substantial amount of texts
belonging to the same genre (news) and manu-
ally annotated from the morphological to the se-
mantic levels (POS tags, syntactic constituents,
NEs, WordNet synsets, and coreference relations).
More importantly, very similar coreference anno-
tation guidelines make AnCora the ideal Spanish
counterpart to OntoNotes.
11Most Romance languages are pro-drop allowing zero
subject pronouns, which can be inferred from the verb.
Datasets Two datasets of similar size were se-
lected from AnCora and OntoNotes in order to
rule out corpus size as an explanation of any differ-
ence in performance. Corpus statistics about the
distribution of mentions and entities are shown in
Tables 1 and 2. Given that this paper is focused on
coreference between NPs, the number of mentions
only includes NPs. Both AnCora and OntoNotes
annotate only multi-mention entities (i.e., those
containing two or more coreferent mentions), so
singleton entities are assumed to correspond to
NPs with no coreference annotation.
Apart from a larger number of mentions in
Spanish (Table 1), the two datasets look very sim-
ilar in the distribution of singletons and multi-
mention entities: about 85% and 15%, respec-
tively. Multi-mention entities have an average
of 3.9 mentions per entity in AnCora and 3.5 in
OntoNotes. The distribution of mention types (Ta-
ble 2), however, differs in two important respects:
AnCora has a smaller number of personal pro-
nouns as Spanish typically uses zero subjects, and
it has a smaller number of bare NPs as the definite
article accompanies more NPs than in English.
Results and Discussion Table 3 presents CIS-
TELL?s results for each dataset. They make evi-
dent problems with the evaluation metrics, namely
the fact that the generated rankings are contradic-
tory (Denis and Baldridge, 2009). They are con-
sistent across the two corpora though: MUC re-
wards WEAK MATCH the most, B3 rewards HEAD
MATCH the most, and CEAF is divided between
SUPER STRONG MATCH and BEST MATCH.
These preferences seem to reveal weaknesses
of the scoring methods that make them biased to-
wards a type of output. The model preferred by
MUC is one that clusters many mentions together,
thus getting a large number of correct coreference
links (notice the high R for WEAK MATCH), but
AnCora OntoNotes
Pronouns 14.09 17.62
Personal pronouns 2.00 12.10
Zero subject pronouns 6.51 ?
Possessive pronouns 3.57 2.96
Demonstrative pronouns 0.39 1.83
Definite NPs 37.69 20.67
Indefinite NPs 7.17 8.44
Demonstrative NPs 1.98 3.41
Bare NPs 33.02 42.92
Misc. 6.05 6.94
Table 2: Mention types (%) in Table 1 datasets.
1426
#docs #words #mentions #entities (e) #singleton e #multi-mention e
AnCora
Training 955 299,014 91,904 64,535 54,991 9,544
Test 30 9,851 2,991 2,189 1,877 312
OntoNotes
Training 850 301,311 74,692 55,819 48,199 7,620
Test 33 9,763 2,463 1,790 1,476 314
Table 1: Corpus statistics for the large portion of OntoNotes and AnCora.
MUC B3 CEAF
P R F P R F P / R / F
AnCora - Spanish
1. ALL SINGLETONS ? ? ? 100 73.32 84.61 73.32
2. HEAD MATCH 55.03 37.72 44.76 91.12 79.88 85.13 75.96
3. HEAD MATCH + PRON 48.22 44.24 46.14 86.21 80.66 83.34 76.30
4. STRONG MATCH 45.64 51.88 48.56 80.13 82.28 81.19 75.79
5. SUPER STRONG MATCH 45.68 36.47 40.56 86.10 79.09 82.45 77.20
6. BEST MATCH 43.10 35.59 38.98 85.24 79.67 82.36 75.23
7. WEAK MATCH 45.73 65.16 53.75 68.50 87.71 76.93 69.21
OntoNotes - English
1. ALL SINGLETONS ? ? ? 100 72.68 84.18 72.68
2. HEAD MATCH 55.14 39.08 45.74 90.65 80.87 85.48 76.05
3. HEAD MATCH + PRON 47.10 53.05 49.90 82.28 83.13 82.70 75.15
4. STRONG MATCH 47.94 55.42 51.41 81.13 84.30 82.68 78.03
5. SUPER STRONG MATCH 48.27 47.55 47.90 84.00 82.27 83.13 78.24
6. BEST MATCH 50.97 46.66 48.72 86.19 82.70 84.41 78.44
7. WEAK MATCH 47.46 66.72 55.47 70.36 88.05 78.22 71.21
Table 3: CISTELL results varying the corpus language.
also many spurious links that are not duly penal-
ized. The resulting output is not very desirable.12
In contrast, B3 is more P-oriented and scores con-
servative outputs like HEAD MATCH and BEST
MATCH first, even if R is low. CEAF achieves a
better compromise between P and R, as corrobo-
rated by the quality of the output.
The baselines and the system runs perform very
similarly in the two corpora, but slightly better
for English. It seems that language-specific issues
do not result in significant differences?at least
for English and Spanish?once the feature set has
been appropriately adapted, e.g., including fea-
tures about zero subjects or removing those about
possessive phrases. Comparing the feature ranks,
we find that the features that work best for each
language largely overlap and are language inde-
pendent, like head match, is-a match, and whether
the mentions are pronominal.
5 Parameter 2: Annotation Scheme
In the second experiment, we used the 100k-word
portion (from the TDT collection) shared by the
OntoNotes and ACE corpora (330 OntoNotes doc-
12Due to space constraints, the actual output cannot be
shown here. We are happy to send it to interested requesters.
uments occurred as 22 ACE-2003 documents, 185
ACE-2004 documents, and 123 ACE-2005 docu-
ments). CISTELL was trained on the same texts
in both corpora and applied to the remainder. The
three measures were then applied to each result.
Datasets Since the two annotation schemes dif-
fer significantly, we made the results comparable
by mapping the ACE entities (the simpler scheme)
onto the information contained in OntoNotes.13
The mapping allowed us to focus exclusively on
the differences expressed on both corpora: the
types of mentions that were annotated, the defi-
nition of identity of reference, etc.
Table 4 presents the statistics for the OntoNotes
dataset merged with the ACE entities. The map-
ping was not straightforward due to several prob-
lems: there was no match for some mentions
due to syntactic or spelling reasons (e.g., El Popo
in OntoNotes vs. Ell Popo in ACE). ACE men-
tions for which there was no parse tree node in
the OntoNotes gold-standard tree were omitted, as
creating a new node could have damaged the tree.
Given that only seven entity types are annotated
in ACE, the number of OntoNotes mentions is al-
13Both ACE entities and types were mapped onto the
OntoNotes dataset.
1427
#docs #words #mentions #entities (e) #singleton e #multi-mention e
OntoNotes
Training 297 87,068 22,127 15,983 13,587 2,396
Test 33 9,763 2,463 1,790 1,476 314
ACE
Training 297 87,068 12,951 5,873 3,599 2,274
Test 33 9,763 1,464 746 459 287
Table 4: Corpus statistics for the aligned portion of ACE and OntoNotes on gold-standard data.
MUC B3 CEAF
P R F P R F P / R / F
OntoNotes scheme
1. ALL SINGLETONS ? ? ? 100 72.68 84.18 72.68
2. HEAD MATCH 55.14 39.08 45.74 90.65 80.87 85.48 76.05
3. HEAD MATCH + PRON 47.10 53.05 49.90 82.28 83.13 82.70 75.15
4. STRONG MATCH 46.81 53.34 49.86 80.47 83.54 81.97 76.78
5. SUPER STRONG MATCH 46.51 40.56 43.33 84.95 80.16 82.48 76.70
6. BEST MATCH 52.47 47.40 49.80 86.10 82.80 84.42 77.87
7. WEAK MATCH 47.91 64.64 55.03 71.73 87.46 78.82 71.74
ACE scheme
1. ALL SINGLETONS ? ? ? 100 50.96 67.51 50.96
2. HEAD MATCH 82.35 39.00 52.93 95.27 64.05 76.60 66.46
3. HEAD MATCH + PRON 70.11 53.90 60.94 86.49 68.20 76.27 68.44
4. STRONG MATCH 64.21 64.21 64.21 76.92 73.54 75.19 70.01
5. SUPER STRONG MATCH 60.51 56.55 58.46 76.71 69.19 72.76 66.87
6. BEST MATCH 67.50 56.69 61.62 82.18 71.67 76.57 69.88
7. WEAK MATCH 63.52 80.50 71.01 59.76 86.36 70.64 64.21
Table 5: CISTELL results varying the annotation scheme on gold-standard data.
most twice as large as the number of ACE men-
tions. Unlike OntoNotes, ACE mentions include
premodifiers (e.g., state in state lines), national
adjectives (e.g., Iraqi) and relative pronouns (e.g.,
who, that). Also, given that ACE entities corre-
spond to types that are usually coreferred (e.g.,
people, organizations, etc.), singletons only rep-
resent 61% of all entities, while they are 85% in
OntoNotes. The average entity size is 4 in ACE
and 3.5 in OntoNotes.
A second major difference is the definition of
coreference relations, illustrated here:
(2) [This] was [an all-white, all-Christian community
that all the sudden was taken over ... by different
groups].
(3) [ [Mayor] John Hyman] has a simple answer.
(4) [Postville] now has 22 different nationalities ... For
those who prefer [the old Postville], Mayor John
Hyman has a simple answer.
In ACE, nominal predicates corefer with their
subject (2), and appositive phrases corefer with
the noun they are modifying (3). In contrast,
they do not fall under the identity relation in
OntoNotes, which follows the linguistic under-
standing of coreference according to which nom-
inal predicates and appositives express properties
of an entity rather than refer to a second (corefer-
ent) entity (van Deemter and Kibble, 2000). Fi-
nally, the two schemes frequently disagree on bor-
derline cases in which coreference turns out to be
especially complex (4). As a result, some features
will behave differently, e.g., the appositive feature
has the opposite effect in the two datasets.
Results and Discussion From the differences
pointed out above, the results shown in Table 5
might be surprising at first. Given that OntoNotes
is not restricted to any semantic type and is based
on a more sophisticated definition of coreference,
one would not expect a system to perform better
on it than on ACE. The explanation is given by the
ALL SINGLETONS baseline, which is 73?84% for
OntoNotes and only 51?68% for ACE. The fact
that OntoNotes contains a much larger number of
singletons?as Table 4 shows?results in an ini-
tial boost of performance (except with the MUC
score, which ignores singletons). In contrast, the
score improvement achieved by HEAD MATCH is
much more noticeable on ACE than on OntoNotes,
which indicates that many of its coreferent men-
tions share the same head.
The systematic biases of the measures that were
observed in Table 3 appear again in the case of
1428
MUC and B3. CEAF is divided between BEST
MATCH and STRONG MATCH. The higher value
of the MUC score for ACE is another indication
of its tendency to reward correct links much more
than to penalize spurious ones (ACE has a larger
proportion of multi-mention entities).
The feature rankings obtained for each dataset
generally coincide as to which features are ranked
best (namely NE match, is-a match, and head
match), but differ in their particular ordering.
It is also possible to compare the OntoNotes re-
sults in Tables 3 and 5, the only difference being
that the first training set was three times larger.
Contrary to expectation, the model trained on a
larger dataset performs just slightly better. The
fact that more training data does not necessarily
lead to an increase in performance conforms to
the observation that there appear to be few general
rules (e.g., head match) that systematically gov-
ern coreference relationships; rather, coreference
appeals to individual unique phenomena appear-
ing in each context, and thus after a point adding
more training data does not add much new gener-
alizable information. Pragmatic information (dis-
course structure, world knowledge, etc.) is proba-
bly the key, if ever there is a way to encode it.
6 Parameter 3: Preprocessing
The goal of the third experiment was to determine
how much the source and nature of preprocess-
ing information matters. Since it is often stated
that coreference resolution depends on many lev-
els of analysis, we again compared the two cor-
pora, which differ in the amount and correctness
of such information. However, in this experiment,
entity mapping was applied in the opposite direc-
tion: the OntoNotes entities were mapped onto the
automatically preprocessed ACE dataset. This ex-
poses the shortcomings of automated preprocess-
ing in ACE for identifying all the mentions identi-
fied and linked in OntoNotes.
Datasets The ACE data was morphologically
annotated with a tokenizer based on manual rules
adapted from the one used in CoNLL (Tjong
Kim Sang and De Meulder, 2003), with TnT 2.2,
a trigram POS tagger based on Markov models
(Brants, 2000), and with the built-in WordNet lem-
matizer (Fellbaum, 1998). Syntactic chunks were
obtained from YamCha 1.33, an SVM-based NP-
chunker (Kudoh and Matsumoto, 2000), and parse
trees from Malt Parser 0.4, an SVM-based parser
(Hall et al, 2007).
Although the number of words in Tables 4 and 6
should in principle be the same, the latter con-
tains fewer words as it lacks the null elements
(traces, ellipsed material, etc.) manually anno-
tated in OntoNotes. Missing parse tree nodes in
the automatically parsed data account for the con-
siderably lower number of OntoNotes mentions
(approx. 5,700 fewer mentions).14 However, the
proportions of singleton:multi-mention entities as
well as the average entity size do not vary.
Results and Discussion The ACE scores for the
automatically preprocessed models in Table 7 are
about 3% lower than those based on OntoNotes
gold-standard data in Table 5, providing evidence
for the advantage offered by gold-standard prepro-
cessing information. In contrast, the similar?if
not higher?scores of OntoNotes can be attributed
to the use of the annotated ACE entity types. The
fact that these are annotated not only for proper
nouns (as predicted by an automatic NER) but also
for pronouns and full NPs is a very helpful feature
for a coreference resolution system.
Again, the scoring metrics exhibit similar bi-
ases, but note that CEAF prefers HEAD MATCH
+ PRON in the case of ACE, which is indicative of
the noise brought by automatic preprocessing.
A further insight is offered from comparing the
feature rankings with gold-standard syntax to that
with automatic preprocessing. Since we are evalu-
ating now on the ACE data, the NE match feature
is also ranked first for OntoNotes. Head and is-a
match are still ranked among the best, yet syntac-
tic features are not. Instead, features like NP type
have moved further up. This reranking probably
indicates that if there is noise in the syntactic infor-
mation due to automatic tools, then morphological
and syntactic features switch their positions.
Given that the noise brought by automatic pre-
processing can be harmful, we tried leaving out the
grammatical function feature. Indeed, the results
increased about 2?3%, STRONG MATCH scoring
the highest. This points out that conclusions drawn
from automatically preprocessed data about the
kind of knowledge relevant for coreference reso-
lution might be mistaken. Using the most success-
ful basic features can lead to the best results when
only automatic preprocessing is available.
14In order to make the set of mentions as similar as possible
to the set in Section 5, OntoNotes singletons were mapped
from the ones detected in the gold-standard treebank.
1429
#docs #words #mentions #entities (e) #singleton e #multi-mention e
OntoNotes
Training 297 80,843 16,945 12,127 10,253 1,874
Test 33 9,073 1,931 1,403 1,156 247
ACE
Training 297 80,843 13,648 6,041 3,652 2,389
Test 33 9,073 1,537 775 475 300
Table 6: Corpus statistics for the aligned portion of ACE and OntoNotes on automatically parsed data.
MUC B3 CEAF
P R F P R F P / R / F
OntoNotes scheme
1. ALL SINGLETONS ? ? ? 100 72.66 84.16 72.66
2. HEAD MATCH 56.76 35.80 43.90 92.18 80.52 85.95 76.33
3. HEAD MATCH + PRON 47.44 54.36 50.66 82.08 83.61 82.84 74.83
4. STRONG MATCH 52.66 58.14 55.27 83.11 85.05 84.07 78.30
5. SUPER STRONG MATCH 51.67 46.78 49.11 85.74 82.07 83.86 77.67
6. BEST MATCH 54.38 51.70 53.01 86.00 83.60 84.78 78.15
7. WEAK MATCH 49.78 64.58 56.22 75.63 87.79 81.26 74.62
ACE scheme
1. ALL SINGLETONS ? ? ? 100 50.42 67.04 50.42
2. HEAD MATCH 81.25 39.24 52.92 94.73 63.82 76.26 65.97
3. HEAD MATCH + PRON 69.76 53.28 60.42 86.39 67.73 75.93 68.05
4. STRONG MATCH 58.85 58.92 58.89 73.36 70.35 71.82 66.30
5. SUPER STRONG MATCH 56.19 50.66 53.28 75.54 66.47 70.72 63.96
6. BEST MATCH 63.38 49.74 55.74 80.97 68.11 73.99 65.97
7. WEAK MATCH 60.22 78.48 68.15 55.17 84.86 66.87 59.08
Table 7: CISTELL results varying the annotation scheme on automatically preprocessed data.
7 Conclusion
Regarding evaluation, the results clearly expose
the systematic tendencies of the evaluation mea-
sures. The way each measure is computed makes
it biased towards a specific model: MUC is gen-
erally too lenient with spurious links, B3 scores
too high in the presence of a large number of sin-
gletons, and CEAF does not agree with either of
them. It is a cause for concern that they provide
contradictory indications about the core of coref-
erence, namely the resolution models?for exam-
ple, the model ranked highest by B3 in Table 7 is
ranked lowest by MUC. We always assume eval-
uation measures provide a ?true? reflection of our
approximation to a gold standard in order to guide
research in system development and tuning.
Further support to our claims comes from the
results of SemEval-2010 Task 1 (Recasens et al,
2010). The performance of the six participating
systems shows similar problems with the evalua-
tion metrics, and the singleton baseline was hard
to beat even by the highest-performing systems.
Since the measures imply different conclusions
about the nature of the corpora and the preprocess-
ing information applied, should we use them now
to constrain the ways our corpora are created in
the first place, and what preprocessing we include
or omit? Doing so would seem like circular rea-
soning: it invalidates the notion of the existence of
a true and independent gold standard. But if ap-
parently incidental aspects of the corpora can have
such effects?effects rated quite differently by the
various measures?then we have no fixed ground
to stand on.
The worrisome fact that there is currently no
clearly preferred and ?correct? evaluation measure
for coreference resolution means that we cannot
draw definite conclusions about coreference reso-
lution systems at this time, unless they are com-
pared on exactly the same corpus, preprocessed
under the same conditions, and all three measures
agree in their rankings.
Acknowledgments
We thank Dr. M. Anto`nia Mart?? for her generosity
in allowing the first author to visit ISI to work with
the second. Special thanks to Edgar Gonza`lez for
his kind help with conversion issues.
This work was partially supported by the Span-
ish Ministry of Education through an FPU schol-
arship (AP2006-00994) and the TEXT-MESS 2.0
Project (TIN2009-13391-C04-04).
1430
References
Amit Bagga and Breck Baldwin. 1998. Algorithms for
scoring coreference chains. In Proceedings of the
LREC 1998 Workshop on Linguistic Coreference,
pages 563?566, Granada, Spain.
Eric Bengtson and Dan Roth. 2008. Understanding
the value of features for coreference resolution. In
Proceedings of EMNLP 2008, pages 294?303, Hon-
olulu, Hawaii.
Thorsten Brants. 2000. TnT ? A statistical part-of-
speech tagger. In Proceedings of ANLP 2000, Seat-
tle, WA.
Aron Culotta, Michael Wick, Robert Hall, and Andrew
McCallum. 2007. First-order probabilistic models
for coreference resolution. In Proceedings of HLT-
NAACL 2007, pages 81?88, Rochester, New York.
Walter Daelemans and Antal Van den Bosch. 2005.
Memory-Based Language Processing. Cambridge
University Press.
Pascal Denis and Jason Baldridge. 2009. Global joint
models for coreference resolution and named entity
classification. Procesamiento del Lenguaje Natural,
42:87?96.
George Doddington, Alexis Mitchell, Mark Przybocki,
Lance Ramshaw, Stephanie Strassel, and Ralph
Weischedel. 2004. The Automatic Content Extrac-
tion (ACE) Program - Tasks, Data, and Evaluation.
In Proceedings of LREC 2004, pages 837?840.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. The MIT Press.
Jenny Rose Finkel and Christopher D. Manning.
2008. Enforcing transitivity in coreference resolu-
tion. In Proceedings of ACL-HLT 2008, pages 45?
48, Columbus, Ohio.
Aria Haghighi and Dan Klein. 2009. Simple coref-
erence resolution with rich syntactic and semantic
features. In Proceedings of EMNLP 2009, pages
1152?1161, Singapore. Association for Computa-
tional Linguistics.
Johan Hall, Jens Nilsson, Joakim Nivre, Gu?lsen
Eryigit, Bea?ta Megyesi, Mattias Nilsson, and
Markus Saers. 2007. Single malt or blended?
A study in multilingual parser optimization. In
Proceedings of the CoNLL shared task session of
EMNLP-CoNLL 2007, pages 933?939.
Lynette Hirschman and Nancy Chinchor. 1997. MUC-
7 Coreference Task Definition ? Version 3.0. In Pro-
ceedings of MUC-7.
Eduard Hovy, Mitchell Marcus, Martha Palmer,
Lance Ramshaw, and Ralph Weischedel. 2006.
OntoNotes: the 90% solution. In Proceedings of
HLT-NAACL 2006, pages 57?60.
Taku Kudoh and Yuji Matsumoto. 2000. Use of sup-
port vector learning for chunk identification. In Pro-
ceedings of CoNLL 2000 and LLL 2000, pages 142?
144, Lisbon, Portugal.
Xiaoqiang Luo and Imed Zitouni. 2005. Multi-lingual
coreference resolution with syntactic features. In
Proceedings of HLT-EMNLP 2005, pages 660?667,
Vancouver.
Xiaoqiang Luo, Abe Ittycheriah, Hongyan Jing, Nanda
Kambhatla, and Salim Roukos. 2004. A mention-
synchronous coreference resolution algorithm based
on the Bell tree. In Proceedings of ACL 2004, pages
21?26, Barcelona.
Xiaoqiang Luo. 2005. On coreference resolution
performance metrics. In Proceedings of HLT-
EMNLP 2005, pages 25?32, Vancouver.
Vincent Ng and Claire Cardie. 2002. Improving
machine learning approaches to coreference resolu-
tion. In Proceedings of ACL 2002, pages 104?111,
Philadelphia.
Vincent Ng. 2009. Graph-cut-based anaphoricity de-
termination for coreference resolution. In Proceed-
ings of NAACL-HLT 2009, pages 575?583, Boulder,
Colorado.
Hoifung Poon and Pedro Domingos. 2008. Joint unsu-
pervised coreference resolution with Markov logic.
In Proceedings of EMNLP 2008, pages 650?659,
Honolulu, Hawaii.
Sameer S. Pradhan, Eduard Hovy, Mitch Mar-
cus, Martha Palmer, Lance Ramshaw, and Ralph
Weischedel. 2007. Ontonotes: A unified rela-
tional semantic representation. In Proceedings of
ICSC 2007, pages 517?526, Washington, DC.
Marta Recasens and Eduard Hovy. 2009. A
Deeper Look into Features for Coreference Res-
olution. In S. Lalitha Devi, A. Branco, and
R. Mitkov, editors, Anaphora Processing and Ap-
plications (DAARC 2009), volume 5847 of LNAI,
pages 29?42. Springer-Verlag.
Marta Recasens and M. Anto`nia Mart??. 2009. AnCora-
CO: Coreferentially annotated corpora for Spanish
and Catalan. Language Resources and Evaluation,
DOI 10.1007/s10579-009-9108-x.
Marta Recasens, Llu??s Ma`rquez, Emili Sapena,
M. Anto`nia Mart??, Mariona Taule?, Ve?ronique Hoste,
Massimo Poesio, and Yannick Versley. 2010.
SemEval-2010 Task 1: Coreference resolution in
multiple languages. In Proceedings of the Fifth In-
ternational Workshop on Semantic Evaluations (Se-
mEval 2010), Uppsala, Sweden.
Wee M. Soon, Hwee T. Ng, and Daniel C. Y. Lim.
2001. A machine learning approach to coreference
resolution of noun phrases. Computational Linguis-
tics, 27(4):521?544.
1431
Veselin Stoyanov, Nathan Gilbert, Claire Cardie, and
Ellen Riloff. 2009. Conundrums in noun phrase
coreference resolution: Making sense of the state-
of-the-art. In Proceedings of ACL-IJCNLP 2009,
pages 656?664, Singapore.
Erik F. Tjong Kim Sang and Fien De Meulder.
2003. Introduction to the CoNLL-2003 Shared
Task: Language-independent Named Entity Recog-
nition. In Walter Daelemans and Miles Osborne, ed-
itors, Proceedings of CoNLL 2003, pages 142?147.
Edmonton, Canada.
Olga Uryupina. 2006. Coreference resolution with
and without linguistic knowledge. In Proceedings
of LREC 2006.
Kees van Deemter and Rodger Kibble. 2000. On core-
ferring: Coreference in MUC and related annotation
schemes. Computational Linguistics, 26(4):629?
637.
Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A model-
theoretic coreference scoring scheme. In Proceed-
ings of MUC-6, pages 45?52, San Francisco.
1432
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1650?1659,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Linguistic Models for Analyzing and Detecting Biased Language
Marta Recasens
Stanford University
recasens@google.com
Cristian Danescu-Niculescu-Mizil
Stanford University
Max Planck Institute SWS
cristiand@cs.stanford.edu
Dan Jurafsky
Stanford University
jurafsky@stanford.edu
Abstract
Unbiased language is a requirement for
reference sources like encyclopedias and
scientific texts. Bias is, nonetheless, ubiq-
uitous, making it crucial to understand its
nature and linguistic realization and hence
detect bias automatically. To this end we
analyze real instances of human edits de-
signed to remove bias from Wikipedia ar-
ticles. The analysis uncovers two classes
of bias: framing bias, such as praising or
perspective-specific words, which we link
to the literature on subjectivity; and episte-
mological bias, related to whether propo-
sitions that are presupposed or entailed in
the text are uncontroversially accepted as
true. We identify common linguistic cues
for these classes, including factive verbs,
implicatives, hedges, and subjective inten-
sifiers. These insights help us develop fea-
tures for a model to solve a new prediction
task of practical importance: given a bi-
ased sentence, identify the bias-inducing
word. Our linguistically-informed model
performs almost as well as humans tested
on the same task.
1 Introduction
Writers and editors of reference works such as
encyclopedias, textbooks, and scientific articles
strive to keep their language unbiased. For ex-
ample, Wikipedia advocates a policy called neu-
tral point of view (NPOV), according to which
articles should represent ?fairly, proportionately,
and as far as possible without bias, all signifi-
cant views that have been published by reliable
sources? (Wikipedia, 2013b). Wikipedia?s style
guide asks editors to use nonjudgmental language,
to indicate the relative prominence of opposing
points of view, to avoid presenting uncontroversial
facts as mere opinion, and, conversely, to avoid
stating opinions or contested assertions as facts.
Understanding the linguistic realization of bias
is important for linguistic theory; automatically
detecting these biases is equally significant for
computational linguistics. We propose to ad-
dress both by using a powerful resource: edits in
Wikipedia that are specifically designed to remove
bias. Since Wikipedia maintains a complete revi-
sion history, the edits associated with NPOV tags
allow us to compare the text in its biased (before)
and unbiased (after) form, helping us better under-
stand the linguistic realization of bias. Our work
thus shares the intuition of prior NLP work apply-
ing Wikipedia?s revision history (Nelken and Ya-
mangil, 2008; Yatskar et al, 2010; Max and Wis-
niewski, 2010; Zanzotto and Pennacchiotti, 2010).
The analysis of Wikipedia?s edits provides valu-
able linguistic insights into the nature of biased
language. We find two major classes of bias-driven
edits. The first, framing bias, is realized by sub-
jective words or phrases linked with a particular
point of view. In (1), the term McMansion, unlike
homes, appeals to a negative attitude toward large
and pretentious houses. The second class, episte-
mological bias, is related to linguistic features that
subtly (often via presupposition) focus on the be-
lievability of a proposition. In (2), the assertive
stated removes the bias introduced by claimed,
which casts doubt on Kuypers? statement.
(1) a. Usually, smaller cottage-style houses have been de-
molished to make way for these McMansions.
b. Usually, smaller cottage-style houses have been de-
molished to make way for these homes.
(2) a. Kuypers claimed that the mainstream press in Amer-
ica tends to favor liberal viewpoints.
b. Kuypers stated that the mainstream press in America
tends to favor liberal viewpoints.
Bias is linked to the lexical and grammatical cues
identified by the literature on subjectivity (Wiebe
et al, 2004; Lin et al, 2011), sentiment (Liu et
al., 2005; Turney, 2002), and especially stance
1650
or ?arguing subjectivity? (Lin et al, 2006; So-
masundaran and Wiebe, 2010; Yano et al, 2010;
Park et al, 2011; Conrad et al, 2012). For ex-
ample, like stance, framing bias is realized when
the writer of a text takes a particular position on
a controversial topic and uses its metaphors and
vocabulary. But unlike the product reviews or de-
bate articles that overtly use subjective language,
editors in Wikipedia are actively trying to avoid
bias, and hence biases may appear more subtly,
in the form of covert framing language, or pre-
suppositions and entailments that may not play as
important a role in other genres. Our linguistic
analysis identifies common classes of these subtle
bias cues, including factive verbs, implicatives and
other entailments, hedges, and subjective intensi-
fiers.
Using these cues could help automatically de-
tect and correct instances of bias, by first finding
biased phrases, then identifying the word that in-
troduces the bias, and finally rewording to elim-
inate the bias. In this paper we propose a so-
lution for the second of these tasks, identifying
the bias-inducing word in a biased phrase. Since,
as we show below, this task is quite challenging
for humans, our system has the potential to be
very useful in improving the neutrality of refer-
ence works like Wikipedia. Tested on a subset of
non-neutral sentences from Wikipedia, our model
achieves 34% accuracy?and up to 59% if the
top three guesses are considered?on this difficult
task, outperforming four baselines and nearing hu-
mans tested on the same data.
2 Analyzing a Dataset of Biased
Language
We begin with an empirical analysis based on
Wikipedia?s bias-driven edits. This section de-
scribes the data, and summarizes our linguistic
analysis.1
2.1 The NPOV Corpus from Wikipedia
Given Wikipedia?s strict enforcement of an NPOV
policy, we decided to build the NPOV corpus,
containing Wikipedia edits that are specifically de-
signed to remove bias. Editors are encouraged to
identify and rewrite biased passages to achieve a
more neutral tone, and they can use several NPOV
1The data and bias lexicon we developed are available at
http://www.mpi-sws.org/?cristian/Biased_
language.html
Data Articles Revisions Words Edits Sents
Train 5997 2238K 11G 13807 1843
Dev 653 210K 0.9G 1261 163
Test 814 260K 1G 1751 230
Total 7464 2708K 13G 16819 2235
Table 1: Statistics of the NPOV corpus, extracted
from Wikipedia. (Edits refers to bias-driven ed-
its, i.e., with an NPOV comment. Sents refers to
sentences with a one-word bias-driven edit.)
tags to mark biased content.2 Articles tagged this
way fall into Wikipedia?s category of NPOV dis-
putes.
We constructed the NPOV corpus by retrieving
all articles that were or had been in the NPOV-
dispute category3 together with their full revision
history. We used Stanford?s CoreNLP tools4 to to-
kenize and split the text into sentences. Table 1
shows the statistics of this corpus, which we split
into training (train), development (dev), and test.
Following Wikipedia?s terminology, we call each
version of a Wikipedia article a revision, and so an
article can be viewed as a set of (chronologically
ordered) revisions.
2.2 Extracting Edits Meant to Remove Bias
Given all the revisions of a page, we extracted the
changes between pairs of revisions with the word-
mode diff function from the Diff Match and Patch
library.5 We refer to these changes between revi-
sions as edits, e.g., McMansion > large home. An
edit consists of two strings: the old string that is
being replaced (i.e., the before form), and the new
modified string (i.e., the after form).
Our assumption was that among the edits hap-
pening in NPOV disputes, we would have a high
density of edits intended to remove bias, which we
call bias-driven edits, like (1) and (2) from Sec-
tion 1. But many other edits occur even in NPOV
disputes, including edits to fix spelling or gram-
matical errors, simplify the language, make the
meaning more precise, or even vandalism (Max
2{{POV}}, {{POV-check}}, {{POV-section}}, etc.
Adding these tags displays a template such as ?The neutrality
of this article is disputed. Relevant discussion may be found
on the talk page. Please do not remove this message until the
dispute is resolved.?
3http://en.wikipedia.org/wiki/
Category:All_NPOV_disputes
4http://nlp.stanford.edu/software/
corenlp.shtml
5http://code.google.com/p/google-diff-
match-patch
1651
and Wisniewski, 2010). Therefore, in order to ex-
tract a high-precision set of bias-driven edits, we
took advantage of the comments that editors can
associate with a revision?typically short and brief
sentences describing the reason behind the revi-
sion. We considered as bias-driven edits those that
appeared in a revision whose comment mentioned
(N)POV, e.g., Attempts at presenting some claims
in more NPOV way; or merging in a passage
from the researchers article after basic NPOV-
ing. We only kept edits whose before and af-
ter forms contained five or fewer words, and dis-
carded those that only added a hyperlink or that
involved a minimal change (character-based Lev-
enshtein distance < 4). The final number of bias-
driven edits for each of the data sets is shown in
the ?Edits? column of Table 1.
2.3 Linguistic Analysis
Style guides talk about biased language in a pre-
scriptive manner, listing a few words that should
be avoided because they are flattering, vague, or
endorse a particular point of view (Wikipedia,
2013a). Our focus is on analyzing actual bi-
ased text and bias-driven edits extracted from
Wikipedia.
As we suggested above, this analysis uncovered
two major classes of bias: epistemological bias
and framing bias. Table 2 shows the distribution
(from a sample of 100 edits) of the different types
and subtypes of bias presented in this section.
(A) Epistemological bias involves propositions
that are either commonly agreed to be true or com-
monly agreed to be false and that are subtly pre-
supposed, entailed, asserted or hedged in the text.
1. Factive verbs (Kiparsky and Kiparsky, 1970)
presuppose the truth of their complement
clause. In (3-a) and (4-a), realize and re-
veal presuppose the truth of ?the oppression
of black people...? and ?the Meditation tech-
nique produces...?, whereas (3-b) and (4-b)
present the two propositions as somebody?s
stand or an experimental result.
(3) a. He realized that the oppression of black peo-
ple was more of a result of economic exploita-
tion than anything innately racist.
b. His stand was that the oppression of black
people was more of a result of economic ex-
ploitation than anything innately racist.
(4) a. The first research revealed that the Meditation
technique produces a unique state fact.
b. The first research indicated that the Medita-
tion technique produces a unique state fact.
Bias Subtype %
A. Epistemological bias 43
- Factive verbs 3
- Entailments 25
- Assertives 11
- Hedges 4
B. Framing bias 57
- Intensifiers 19
- One-sided terms 38
Table 2: Proportion of the different bias types.
2. Entailments are directional relations that
hold whenever the truth of one word or
phrase follows from another, e.g., murder en-
tails kill because there cannot be murdering
without killing (5). However, murder en-
tails killing in an unlawful, premeditated way.
This class includes implicative verbs (Kart-
tunen, 1971), which imply the truth or un-
truth of their complement, depending on the
polarity of the main predicate. In (6-a), co-
erced into accepting entails accepting in an
unwilling way.
(5) a. After he murdered three policemen, the
colony proclaimed Kelly a wanted outlaw.
b. After he killed three policemen, the colony
proclaimed Kelly a wanted outlaw.
(6) a. A computer engineer who was coerced into
accepting a plea bargain.
b. A computer engineer who accepted a plea bar-
gain.
3. Assertive verbs (Hooper, 1975) are those
whose complement clauses assert a proposi-
tion. The truth of the proposition is not pre-
supposed, but its level of certainty depends
on the asserting verb. Whereas verbs of say-
ing like say and state are usually neutral,
point out and claim cast doubt on the cer-
tainty of the proposition.
(7) a. The ?no Boeing? theory is a controversial is-
sue, even among conspiracists, many of whom
have pointed out that it is disproved by ...
b. The ?no Boeing? theory is a controversial is-
sue, even among conspiracists, many of whom
have said that it is disproved by...
(8) a. Cooper says that slavery was worse in South
America and the US than Canada, but clearly
states that it was a horrible and cruel practice.
b. Cooper says that slavery was worse in South
America and the US than Canada, but points
out that it was a horrible and cruel practice.
1652
4. Hedges are used to reduce one?s commit-
ment to the truth of a proposition, thus
avoiding any bold predictions (9-b) or state-
ments (10-a).6
(9) a. Eliminating the profit motive will decrease the
rate of medical innovation.
b. Eliminating the profit motive may have a
lower rate of medical innovation.
(10) a. The lower cost of living in more rural areas
means a possibly higher standard of living.
b. The lower cost of living in more rural areas
means a higher standard of living.
Epistemological bias is bidirectional, that is,
bias can occur because doubt is cast on a propo-
sition commonly assumed to be true, or because
a presupposition or implication is made about a
proposition commonly assumed to be false. For
example, in (7) and (8) above, point out is replaced
in the former case, but inserted in the second case.
If the truth of the proposition is uncontroversially
accepted by the community (i.e., reliable sources,
etc.), then the use of a factive is unbiased. In con-
trast, if only a specific viewpoint agrees with its
truth, then using a factive is biased.
(B) Framing bias is usually more explicit than
epistemological bias because it occurs when sub-
jective or one-sided words are used, revealing the
author?s stance in a particular debate (Entman,
2007).
1. Subjective intensifiers are adjectives or ad-
verbs that add (subjective) force to the mean-
ing of a phrase or proposition.
(11) a. Schnabel himself did the fantastic reproduc-
tions of Basquiat?s work.
b. Schnabel himself did the accurate reproduc-
tions of Basquiat?s work.
(12) a. Shwekey?s albums are arranged by many tal-
ented arrangers.
b. Shwekey?s albums are arranged by many dif-
ferent arrangers.
2. One-sided terms reflect only one of the sides
of a contentious issue. They often belong
to controversial subjects (e.g., religion, ter-
rorism, etc.) where the same event can be
seen from two or more opposing perspec-
tives, like the Israeli-Palestinian conflict (Lin
et al, 2006).
6See Choi et al (2012) for an exploration of the interface
between hedging and framing.
(13) a. Israeli forces liberated the eastern half of
Jerusalem.
b. Israeli forces captured the eastern half of
Jerusalem.
(14) a. Concerned Women for America?s major ar-
eas of political activity have consisted of op-
position to gay causes, pro-life law...
b. Concerned Women for America?s major ar-
eas of political activity have consisted of op-
position to gay causes, anti-abortion law...
(15) a. Colombian terrorist groups.
b. Colombian paramilitary groups.
Framing bias has been studied within the liter-
ature on stance recognition and arguing subjectiv-
ity. Because this literature has focused on iden-
tifying which side an article takes on a two-sided
debate such as the Israeli-Palestinian conflict (Lin
et al, 2006), most studies cast the problem as a
two-way classification of documents or sentences
into for/positive vs. against/negative (Anand et
al., 2011; Conrad et al, 2012; Somasundaran and
Wiebe, 2010), or into one of two opposing views
(Yano et al, 2010; Park et al, 2011). The fea-
tures used by these models include subjectivity
and sentiment lexicons, counts of unigrams and
bigrams, distributional similarity, discourse rela-
tionships, and so on.
The datasets used by these studies come from
genres that overtly take a specific stance (e.g.,
debates, editorials, blog posts). In contrast,
Wikipedia editors are asked not to advocate a par-
ticular point of view, but to provide a balanced ac-
count of the different available perspectives. For
this reason, overtly biased opinion statements such
as ?I believe that...? are not common in Wikipedia.
The features used by the subjectivity literature
help us detect framing bias, but we also need fea-
tures that capture epistemological bias expressed
through presuppositions and entailments.
3 Automatically Identifying Biased
Language
We now show how the bias cues identified in Sec-
tion 2.3 can help solve a new task. Given a biased
sentence (e.g., a sentence that a Wikipedia editor
has tagged as violating the NPOV policy), our goal
in this new task is to identify the word that intro-
duces bias. This is part of a potential three-step
process for detecting and correcting biased lan-
guage: (1) finding biased phrases, (2) identifying
the word that introduces the bias, (3) rewording to
eliminate the bias. As we will see below, it can be
1653
hard even for humans to track down the sources of
bias, because biases in reference works are often
subtle and implicit. An automatic bias detector
that can highlight the bias-inducing word(s) and
draw the editors? attention to words that need to
be modified could thus be important for improving
reference works like Wikipedia or even in news re-
porting.
We selected the subset of sentences that had a
single NPOV edit involving one (original) word.
(Although the before form consists of only one
word, the after form can be either one or more
words or the null string (i.e., deletion edits); we do
not use the after string in this identification task).
The number of sentences in the train, dev and test
sets is shown in the last column of Table 1.
We trained a logistic regression model on a
feature vector for every word that appears in the
NPOV sentences from the training set, with the
bias-inducing words as the positive class, and all
the other words as the negative class. The features
are described in the next section.
At test time, the model is given a set of sen-
tences and, for each of them, it ranks the words ac-
cording to their probability to be biased, and out-
puts the highest ranked word (TOP1 model), the
two highest ranked words (TOP2 model), or the
three highest ranked words (TOP3 model).
3.1 Features
The types of features used in the logistic regres-
sion model are listed in Table 3, together with
their value space. The total number of features is
36,787. The ones targeting framing bias draw on
previous work on sentiment and subjectivity de-
tection (Wiebe et al, 2004; Liu et al, 2005). Fea-
tures to capture epistemological bias are based on
the bias cues identified in Section 2.3.
A major split separates the features that de-
scribe the word under analysis (e.g., lemma, POS,
whether it is a hedge, etc.) from those that de-
scribe its surrounding context (e.g., the POS of the
word to the left, whether there is a hedge in the
context, etc.). We define context as a 5-gram win-
dow, i.e., two words to the left of the word un-
der analysis, and two to the right. Taking con-
text into account is important given that biases can
be context-dependent, especially epistemological
bias since it depends on the truth of a proposition.
To define some of the features like POS and gram-
matical relation, we used the Stanford?s CoreNLP
tagger and dependency parser (de Marneffe et al,
2006).
Features 9?10 use the list of hedges from Hy-
land (2005), features 11?14 use the factives and
assertives from Hooper (1975), features 15?16
use the implicatives from Karttunen (1971), fea-
tures 19?20 use the entailments from Berant et
al. (2012), features 21?25 employ the subjectiv-
ity lexicon from Riloff and Wiebe (2003), and fea-
tures 26?29 use the sentiment lexicon?positive
and negative words?from Liu et al (2005). If the
word (or a word in the context) is in the lexicon,
then the feature is true, otherwise it is false.
We also included a ?bias lexicon? (feature 31)
that we built based on our NPOV corpus from
Wikipedia. We used the training set to extract the
lemmas of words that were the before form of at
least two NPOV edits, and that occurred in at least
two different articles. Of the 654 words included
in this lexicon, 433 were unique to this lexicon
(i.e., recorded in neither Riloff and Wiebe?s (2003)
subjectivity lexicon nor Liu et al?s (2005) senti-
ment lexicon) and represented many one-sided or
controversial terms, e.g., abortion, same-sex, exe-
cute.
Finally, we also included a ?collaborative fea-
ture? that, based on the previous revisions of the
edit?s article, computes the ratio between the num-
ber of times that the word was NPOV-edited and
its frequency of occurrence. This feature was de-
signed to capture framing bias specific to an article
or topic.
3.2 Baselines
Previous work on subjectivity and stance recog-
nition has been evaluated on the task of classify-
ing documents as opinionated vs. factual, for vs.
against, positive vs. negative. Given that the task
of identifying the bias-inducing word of a sentence
is novel, there were no previous results to compare
directly against. We ran the following five base-
lines.
1. Random guessing. Naively returns a random
word from every sentence.
2. Role baseline. Selects the word with the
syntactic role that has the highest probabil-
ity to be biased, as computed on the train-
ing set. This is the parse tree root (proba-
bility p = .126 to be biased), followed by
verbal arguments (p = .085), and the subject
(p = .084).
1654
ID Feature Value Description
1* Word <string> Word w under analysis.
2 Lemma <string> Lemma of w.
3* POS {NNP, JJ, ...} POS of w.
4* POS ? 1 {NNP, JJ, ...} POS of one word before w.
5 POS ? 2 {NNP, JJ, ...} POS of two words before w.
6* POS + 1 {NNP, JJ, ...} POS of one word after w.
7 POS + 2 {NNP, JJ, ...} POS of two words after w.
8 Position in sentence {start, mid, end} Position of w in the sentence (split into three parts).
9 Hedge {true, false} w is in Hyland?s (2005) list of hedges (e.g., apparently).
10* Hedge in context {true, false} One/two words) around w is a hedge (Hyland, 2005).
11* Factive verb {true, false} w is in Hooper?s (1975) list of factives (e.g., realize).
12* Factive verb in context {true, false} One/two word(s) around w is a factive (Hooper, 1975).
13* Assertive verb {true, false} w is in Hooper?s (1975) list of assertives (e.g., claim).
14* Assertive verb in context {true, false} One/two word(s) around w is an assertive (Hooper, 1975).
15 Implicative verb {true, false} w is in Karttunen?s (1971) list of implicatives (e.g., manage).
16* Implicative verb in context {true, false} One/two word(s) around w is an implicative (Karttunen, 1971).
17* Report verb {true, false} w is a report verb (e.g., add).
18 Report verb in context {true, false} One/two word(s) around w is a report verb.
19* Entailment {true, false} w is in Berant et al?s (2012) list of entailments (e.g., kill).
20* Entailment in context {true, false} One/two word(s) around w is an entailment (Berant et al, 2012).
21* Strong subjective {true, false} w is in Riloff and Wiebe?s (2003) list of strong subjectives (e.g.,
absolute).
22 Strong subjective in context {true, false} One/two word(s) around w is a strong subjective (Riloff and
Wiebe, 2003).
23* Weak subjective {true, false} w is in Riloff and Wiebe?s (2003) list of weak subjectives (e.g.,
noisy).
24* Weak subjective in context {true, false} One/two word(s) around w is a weak subjective (Riloff and
Wiebe, 2003).
25 Polarity {+, ?, both, ...} The polarity of w according to Riloff and Wiebe (2003), e.g.,
praising is positive.
26* Positive word {true, false} w is in Liu et al?s (2005) list of positive words (e.g., excel).
27* Positive word in context {true, false} One/two word(s) around w is positive (Liu et al, 2005).
28* Negative word {true, false} w is in Liu et al?s (2005) list of negative words (e.g., terrible).
29* Negative word in context {true, false} One/two word(s) around w is negative (Liu et al, 2005).
30* Grammatical relation {root, subj, ...} Whether w is the subject, object, root, etc. of its sentence.
31 Bias lexicon {true, false} w has been observed in NPOV edits (e.g., nationalist).
32* Collaborative feature <numeric> Number of times that w was NPOV-edited in the article?s prior
history / frequency of w.
Table 3: Features used by the bias detector. The star (*) shows the most contributing features.
3. Sentiment baseline. Logistic regression
model that only uses the features based on
Liu et al?s (2005) lexicons of positive and
negative words (i.e., features 26?29).
4. Subjectivity baseline. Logistic regression
model that only uses the features based on
Riloff and Wiebe?s (2003) lexicon of subjec-
tive words (i.e., features 21?25).
5. Wikipedia baseline. Selects as biased the
words that appear in Wikipedia?s list of words
to avoid (Wikipedia, 2013a).
These baselines assessed the difficulty of the
task, as well as the extent to which traditional
sentiment-analysis and subjectivity features would
suffice to detect biased language.
3.3 Results and Discussion
To measure performance, we used accuracy de-
fined as:
#sentences with the correctly predicted biased word
#sentences
The results are shown in Table 4. As explained
earlier, we evaluated all the models by outputting
as biased either the highest ranked word or the
two or three highest ranked words. These corre-
spond to the TOP1, TOP2 and TOP3 columns, re-
spectively. The TOP3 score increases to 59%. A
tool that highlights up to three words to be revised
would simplify the editors? job and decrease sig-
nificantly the time required to revise.
Our model outperforms all five baselines by a
large margin, showing the importance of consid-
ering a wide range of features. Wikipedia?s list
of words to avoid falls very short on recall. Fea-
1655
System TOP1 TOP2 TOP3
Baseline 1: Random 2.18 7.83 9.13
Baseline 2: Role 15.65 20.43 25.65
Baseline 3: Sentiment 14.78 22.61 27.83
Baseline 4: Subjectivity 16.52 25.22 33.91
Baseline 5: Wikipedia 10.00 10.00 10.00
Our system 34.35 46.52 58.70
Humans (AMT) 37.39 50.00 59.13
Table 4: Accuracy (%) of the bias detector on the
test set.
tures that contribute the most to the model?s per-
formance (in a feature ablation study on the dev
set) are highlighted with a star (*) in Table 3. In
addition to showing the importance of linguistic
cues for different classes of bias, the ablation study
highlights the role of contextual features. The bias
lexicon does not seem to help much, suggesting
that it is overfit to the training data.
An error analysis shows that our system makes
acceptable errors in that words wrongly predicted
as bias-inducing may well introduce bias in a dif-
ferent context. In (16), the system picked eschew,
whereas orthodox would have been the correct
choice according to the gold edit. Note that both
the sentiment and the subjectivity lexicons list es-
chew as a negative word. The bias type that poses
the greatest challenge to the system are terms that
are one-sided or loaded in a particular topic, such
as orthodox in this example.
(16) a. Some Christians eschew orthodox theology; such
as the Unitarians, Socinian, [...]
b. Some Christians eschew mainstream trinitarian
theology; such as the Unitarians, Socinian, [...]
The last row in Table 4 lists the performance
of humans on the same task, presented in the next
section.
4 Human Perception of Biased Language
Is it difficult for humans to find the word in a
sentence that induces bias, given the subtle, of-
ten implicit biases in Wikipedia. We used Ama-
zon Mechanical Turk7 (AMT) to elicit annotations
from humans for the same 230 sentences from the
test set that we used to evaluate the bias detector
in Section 3.3. The goal of this annotation was
twofold: to compare the performance of our bias
detector against a human baseline, and to assess
the difficulty of this task for humans. While AMT
labelers are not trained Wikipedia editors, under-
7http://www.mturk.com
standing how difficult these cases are for untrained
labelers is an important baseline.
4.1 Task
Our HIT (Human Intelligence Task) was called
?Find the biased word!?. We kept the task descrip-
tion succinct. Turkers were shown Wikipedia?s
definition of a ?biased statement? and two exam-
ple sentences that illustrated the two types of bias,
framing and epistemological. In each HIT, annota-
tors saw 10 sentences, one after another, and each
one followed by a text box entitled ?Word intro-
ducing bias.? For each sentence, they were asked
to type in the text box the word that caused the
statement to be biased. They were only allowed to
enter a single word.
Before the 10 sentences, turkers were asked to
list the languages they spoke as well as their pri-
mary language in primary school. This was En-
glish in all the cases. In addition, we included a
probe question in the form of a paraphrasing task:
annotators were given a sentence and two para-
phrases (a correct and a bad one) to choose from.
The goal of this probe question was to discard
annotators who were not paying attention or did
not have a sufficient command of English. This
simple test was shown to be effective in verifying
and eliciting linguistic attentiveness (Munro et al,
2010). This was especially important in our case
as we were interested in using the human annota-
tions as an oracle. At the end of the task, partici-
pants were given the option to provide additional
feedback.
We split the 230 sentences into 23 sets of 10
sentences, and asked for 10 annotations of each
set. Each approved HIT was rewarded with $0.30.
4.2 Results and Discussion
On average, it took turkers about four minutes to
complete each HIT. The feedback that we got from
some of them confirmed our hypothesis that find-
ing the bias source is difficult: ?Some of the ?bi-
ases? seemed very slight if existent at all,? ?This
was a lot harder than I thought it would be... Inter-
esting though!?.
We postprocessed the answers ignoring case,
punctuation signs, and spelling errors. To ensure
an answer quality as high as possible, we only
kept those turkers who answered attentively by ap-
plying two filters: we only accepted answers that
matched a valid word from the sentence, and we
discarded answers from participants who did not
1656
2 3 4 5 6 7 8 9 10
Number of times the top word was selected
Nu
mb
er o
f se
nte
nce
s
0
10
20
30
40
50
Figure 1: Distribution of the number of turkers
who selected the top word (i.e., the word selected
by the majority of turkers).
pass the paraphrasing task?there were six such
cases. These filters provided us with confidence in
the turkers? answers as a fair standard of compari-
son.
Overall, humans correctly identified the biased
word 30% of the time. For each sentence, we
ranked the words according to the number of turk-
ers (out of 10) who selected them and, like we
did for the automated system, we assessed per-
formance when considering only the top word
(TOP1), the top 2 words (TOP2), and the top 3
words (TOP3). The last row of Table 4 reports the
results. Only 37.39% of the majority answers co-
incided with the gold label, slightly higher than
our system?s accuracy. The fact that the human
answers are very close to the results of our system
reflects the difficulty of the task. Biases in refer-
ence works can be very subtle and go unnoticed
by humans; automated systems could thus be ex-
tremely helpful.
As a measure of inter-rater reliability, we com-
puted pairwise agreement. The turkers agreed
40.73% of the time, compared to the 5.1% chance
agreement that would be achieved if raters had
randomly selected a word for each sentence. Fig-
ure 1 plots the number of times the top word of
each sentence was selected. The bulk of the sen-
tences only obtained between four and six answers
for the same word.
There is a good amount of overlap (?34%) be-
tween the correct answers predicted by our system
and those from humans. Much like the automated
system, humans also have the hardest time identi-
fying words that are one-sided or controversial to
a specific topic. They also picked eschew for (16)
instead of orthodox. Compared to the system, they
do better in detecting bias-inducing intensifiers,
and about the same with epistemological bias.
5 Related Work
The work in this paper builds upon prior work on
subjectivity detection (Wiebe et al, 2004; Lin et
al., 2011; Conrad et al, 2012) and stance recogni-
tion (Yano et al, 2010; Somasundaran and Wiebe,
2010; Park et al, 2011), but applied to the genre
of reference works such as Wikipedia. Unlike the
blogs, online debates and opinion pieces which
have been the major focus of previous work, bias
in reference works is undesirable. As a result,
the expression of bias is more implicit, making it
harder to detect by both computers and humans.
Of the two classes of bias that we uncover, fram-
ing bias is indeed strongly linked to subjectiv-
ity, but epistemological bias is not. In this re-
spect, our research is comparable to Greene and
Resnik?s (2009) work on identifying implicit sen-
timent or perspective in journalistic texts, based on
semantico-syntactic choices.
Given that the data that we use is not supposed
to be opinionated, our task consists in detecting
(implicit) bias instead of classifying into side A
or B documents about a controversial topic like
ObamaCare (Conrad et al, 2012) or the Israeli-
Palestinian conflict (Lin et al, 2006; Greene and
Resnik, 2009). Our model detects whether all
the relevant perspectives are fairly represented by
identifying statements that are one-sided. To this
end, the features based on subjectivity and senti-
ment lexicons turn out to be helpful, and incor-
porating more features for stance detection is an
important direction for future work.
Other aspects of Wikipedia structure have been
used for other NLP applications. The Wikipedia
revision history has been used for spelling correc-
tion, text summarization (Nelken and Yamangil,
2008), lexical simplification (Yatskar et al, 2010),
paraphrasing (Max and Wisniewski, 2010), and
textual entailment (Zanzotto and Pennacchiotti,
2010). Ganter and Strube (2009) have used
Wikipedia?s weasel-word tags to train a hedge de-
tector. Callahan and Herring (2011) have exam-
ined cultural bias based on Wikipedia?s NPOV
policy.
1657
6 Conclusions
Our study of bias in Wikipedia has implications
for linguistic theory and computational linguis-
tics. We show that bias in reference works falls
broadly into two classes, framing and epistemo-
logical. The cues to framing bias are more ex-
plicit and are linked to the literature on subjec-
tivity; cues to epistemological bias are subtle and
implicit, linked to presuppositions and entailments
in the text. Epistemological bias has not received
much attention since it does not play a major role
in overtly opinionated texts, the focus of much re-
search on stance recognition. However, our logis-
tic regression model reveals that epistemological
and other features can usefully augment the tradi-
tional sentiment and subjectivity features for ad-
dressing the difficult task of identifying the bias-
inducing word in a biased sentence.
Identifying the bias-inducing word is a chal-
lenging task even for humans. Our linguistically-
informed model performs nearly as well as hu-
mans tested on the same task. Given the sub-
tlety of some of these biases, an automated sys-
tem that highlights one or more potentially biased
words would provide a helpful tool for editors of
reference works and news reports, not only mak-
ing them aware of unnoticed biases but also sav-
ing them hours of time. Future work could in-
vestigate the incorporation of syntactic features or
further features from the stance detection litera-
ture. Features from the literature on veridicality
(de Marneffe et al, 2012) could be informative of
the writer?s commitment to the truth of the events
described, and document-level features could help
assess the extent to which the article provides a
balanced account of all the facts and points of
view.
Finally, the NPOV data and the bias lexicon that
we release as part of this research could prove use-
ful in other bias related tasks.
Acknowledgments
We greatly appreciate the support of Jean Wu and
Christopher Potts in running our task on Ama-
zon Mechanical Turk, and all the Amazon Turkers
who participated. We benefited from comments
by Valentin Spitkovsky on a previous draft and
from the helpful suggestions of the anonymous re-
viewers. The first author was supported by a Beat-
riu de Pino?s postdoctoral scholarship (2010 BP-A
00149) from Generalitat de Catalunya. The sec-
ond author was supported by NSF IIS-1016909.
The last author was supported by the Center for
Advanced Study in the Behavioral Sciences at
Stanford.
References
Pranav Anand, Marilyn Walker, Rob Abbott, Jean
E. Fox Tree, Robeson Bowmani, and Michael Mi-
nor. 2011. Cats rule and dogs drool!: Classifying
stance in online debate. In Proceedings of ACL-
HLT 2011 Workshop on Computational Approaches
to Subjectivity and Sentiment Analysis, pages 1?9.
Jonathan Berant, Ido Dagan, Meni Adler, and Jacob
Goldberger. 2012. Efficient tree-based approxima-
tion for entailment graph learning. In Proceedings
of ACL 2012, pages 117?125.
Ewa Callahan and Susan C. Herring. 2011. Cul-
tural bias in Wikipedia articles about famous per-
sons. Journal of the American Society for Informa-
tion Science and Technology, 62(10):1899?1915.
Eunsol Choi, Chenhao Tan, Lillian Lee, Cristian
Danescu-Niculescu-Mizil, and Jennifer Spindel.
2012. Hedge detection as a lens on framing in the
GMO debates: a position paper. In Proceedings
of the ACL-2012 Workshop on Extra-Propositional
Aspects of Meaning in Computational Linguistics,
pages 70?79.
Alexander Conrad, Janyce Wiebe, and Rebecca Hwa.
2012. Recognizing arguing subjectivity and argu-
ment tags. In Proceedings of ACL-2012 Workshop
on Extra-Propositional Aspects of Meaning in Com-
putational Linguistics, pages 80?88.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of LREC 2006.
Marie-Catherine de Marneffe, Christopher D. Man-
ning, and Christopher Potts. 2012. Did it hap-
pen? The pragmatic complexity of veridicality as-
sessment. Computational Linguistics, 38(2):301?
333.
Robert M. Entman. 2007. Framing bias: Media in the
distribution of power. Journal of Communication,
57(1):163?173.
Viola Ganter and Michael Strube. 2009. Finding
hedges by chasing weasels: Hedge detection using
Wikipedia tags and shallow linguistic features. In
Proceedings of ACL-IJCNLP 2009, pages 173?176.
Stephan Greene and Philip Resnik. 2009. More than
words: Syntactic packaging and implicit sentiment.
In Proceedings of NAACL-HLT 2009, pages 503?
511.
1658
Joan B. Hooper. 1975. On assertive predicates. In
J. Kimball, editor, Syntax and Semantics, volume 4,
pages 91?124. Academic Press, New York.
Ken Hyland. 2005. Metadiscourse: Exploring Interac-
tion in Writing. Continuum, London and New York.
Lauri Karttunen. 1971. Implicative verbs. Language,
47(2):340?358.
Paul Kiparsky and Carol Kiparsky. 1970. Fact. In
M. Bierwisch and K. E. Heidolph, editors, Progress
in Linguistics, pages 143?173. Mouton, The Hague.
Wei-Hao Lin, Theresa Wilson, Janyce Wiebe, and
Alexander Hauptmann. 2006. Which side are you
on? Identifying perspectives at the document and
sentence levels. In Proceedings of CoNLL 2006,
pages 109?116.
Chenghua Lin, Yulan He, and Richard Everson.
2011. Sentence subjectivity detection with
weakly-supervised learning. In Proceedings of
AFNLP 2011, pages 1153?1161.
Bing Liu, Minqing Hu, and Junsheng Cheng. 2005.
Opinion Observer: analyzing and comparing opin-
ions on the Web. In Proceedings of WWW 2005,
pages 342?351.
Aure?lien Max and Guillaume Wisniewski. 2010. Min-
ing naturally-occurring corrections and paraphrases
from Wikipedia?s revision history. In Proceedings
of LREC 2010, pages 3143?3148.
Robert Munro, Steven Bethard, Victor Kuperman,
Vicky Tzuyin Lai, Robin Melnick, Christopher
Potts, Tyler Schnoebelen, and Harry Tily. 2010.
Crowdsourcing and language studies: the new gen-
eration of linguistic data. In Proceedings of the
NAACL-HLT 2010 Workshop on Creating Speech
and Language Data With Amazons Mechanical
Turk, pages 122?130.
Rani Nelken and Elif Yamangil. 2008. Mining
Wikipedias article revision history for training Com-
putational Linguistics algorithms. In Proceedings of
the 1st AAAI Workshop on Wikipedia and Artificial
Intelligence.
Souneil Park, KyungSoon Lee, and Junehwa Song.
2011. Contrasting opposing views of news articles
on contentious issues. In Proceedings of ACL 2011,
pages 340?349.
Ellen Riloff and Janyce Wiebe. 2003. Learning extrac-
tion patterns for subjective expressions. In Proceed-
ings of EMNLP 2003, pages 105?112.
Swapna Somasundaran and Janyce Wiebe. 2010. Rec-
ognizing stances in ideological on-line debates. In
Proceedings of the NAACL-HLT 2010 Workshop on
Computational Approaches to Analysis and Genera-
tion of Emotion in Text, pages 116?124.
Peter D. Turney. 2002. Thumbs up or thumbs down?
Semantic orientation applied to unsupervised clas-
sification of reviews. In Proceedings of ACL 2002,
pages 417?424.
Janyce Wiebe, Theresa Wilson, Rebecca Bruce,
Matthew Bell, and Melanie Martin. 2004. Learn-
ing subjective language. Computational Linguistics,
30(3):277?308.
Wikipedia. 2013a. Wikipedia: Manual of style / Words
to watch. http://en.wikipedia.org/
wiki/Wikipedia:Words_to_avoid. [Re-
trieved February 5, 2013].
Wikipedia. 2013b. Wikipedia: Neutral point of view.
http:http://en.wikipedia.org/wiki/
Wikipedia:Neutral_point_of_view.
[Retrieved February 5, 2013].
Tae Yano, Philip Resnik, and Noah A. Smith. 2010.
Shedding (a thousand points of) light on biased lan-
guage. In Proceedings of the NAACL-HLT 2010
Workshop on Creating Speech and Language Data
With Amazons Mechanical Turk, pages 152?158.
Mark Yatskar, Bo Pang, Cristian Danescu-Niculescu-
Mizil, and Lillian Lee. 2010. For the sake of sim-
plicity: Unsupervised extraction of lexical simplifi-
cations from Wikipedia. In Proceedings of NAACL-
HLT 2010, pages 365?368.
Fabio M. Zanzotto and Marco Pennacchiotti. 2010.
Expanding textual entailment corpora from
Wikipedia using co-training. In Proceedings of
the 2nd Coling Workshop on The People?s Web
Meets NLP: Collaboratively Constructed Semantic
Resources, pages 28?36.
1659
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 24?29,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
An Extension of BLANC to System Mentions
Xiaoqiang Luo
Google Inc.
111 8th Ave, New York, NY 10011
xql@google.com
Sameer Pradhan
Harvard Medical School
300 Longwood Ave., Boston, MA 02115
sameer.pradhan@childrens.harvard.edu
Marta Recasens
Google Inc.
1600 Amphitheatre Pkwy,
Mountain View, CA 94043
recasens@google.com
Eduard Hovy
Carnegie Mellon University
5000 Forbes Ave.
Pittsburgh, PA 15213
hovy@cmu.edu
Abstract
BLANC is a link-based coreference eval-
uation metric for measuring the qual-
ity of coreference systems on gold men-
tions. This paper extends the original
BLANC (?BLANC-gold? henceforth) to
system mentions, removing the gold men-
tion assumption. The proposed BLANC
falls back seamlessly to the original one if
system mentions are identical to gold men-
tions, and it is shown to strongly correlate
with existing metrics on the 2011 and 2012
CoNLL data.
1 Introduction
Coreference resolution aims at identifying natu-
ral language expressions (or mentions) that refer
to the same entity. It entails partitioning (often
imperfect) mentions into equivalence classes. A
critically important problem is how to measure the
quality of a coreference resolution system. Many
evaluation metrics have been proposed in the past
two decades, including the MUC measure (Vilain
et al, 1995), B-cubed (Bagga and Baldwin, 1998),
CEAF (Luo, 2005) and, more recently, BLANC-
gold (Recasens and Hovy, 2011). B-cubed and
CEAF treat entities as sets of mentions and mea-
sure the agreement between key (or gold standard)
entities and response (or system-generated) enti-
ties, while MUC and BLANC-gold are link-based.
In particular, MUC measures the degree of
agreement between key coreference links (i.e.,
links among mentions within entities) and re-
sponse coreference links, while non-coreference
links (i.e., links formed by mentions from different
entities) are not explicitly taken into account. This
leads to a phenomenon where coreference systems
outputting large entities are scored more favorably
than those outputting small entities (Luo, 2005).
BLANC (Recasens and Hovy, 2011), on the other
hand, considers both coreference links and non-
coreference links. It calculates recall, precision
and F-measure separately on coreference and non-
coreference links in the usual way, and defines
the overall recall, precision and F-measure as the
mean of the respective measures for coreference
and non-coreference links.
The BLANC-gold metric was developed with
the assumption that response mentions and key
mentions are identical. In reality, however, men-
tions need to be detected from natural language
text and the result is, more often than not, im-
perfect: some key mentions may be missing in
the response, and some response mentions may be
spurious?so-called ?twinless? mentions by Stoy-
anov et al (2009). Therefore, the identical-
mention-set assumption limits BLANC-gold?s ap-
plicability when gold mentions are not available,
or when one wants to have a single score mea-
suring both the quality of mention detection and
coreference resolution. The goal of this paper is
to extend the BLANC-gold metric to imperfect re-
sponse mentions.
We first briefly review the original definition of
BLANC, and rewrite its definition using set nota-
tion. We then argue that the gold-mention assump-
tion in Recasens and Hovy (2011) can be lifted
without changing the original definition. In fact,
the proposed BLANC metric subsumes the origi-
nal one in that its value is identical to the original
one when response mentions are identical to key
mentions.
The rest of the paper is organized as follows.
We introduce the notions used in this paper in
Section 2. We then present the original BLANC-
gold in Section 3 using the set notation defined in
Section 2. This paves the way to generalize it to
24
imperfect system mentions, which is presented in
Section 4. The proposed BLANC is applied to the
CoNLL 2011 and 2012 shared task participants,
and the scores and its correlations with existing
metrics are shown in Section 5.
2 Notations
To facilitate the presentation, we define the nota-
tions used in the paper.
We use key to refer to gold standard mentions or
entities, and response to refer to system mentions
or entities. The collection of key entities is denoted
by K = {k
i
}
|K|
i=1
, where k
i
is the i
th
key entity;
accordingly, R = {r
j
}
|R|
j=1
is the set of response
entities, and r
j
is the j
th
response entity. We as-
sume that mentions in {k
i
} and {r
j
} are unique;
in other words, there is no duplicate mention.
Let C
k
(i) and C
r
(j) be the set of coreference
links formed by mentions in k
i
and r
j
:
C
k
(i) = {(m
1
,m
2
) : m
1
? k
i
,m
2
? k
i
,m
1
6= m
2
}
C
r
(j) = {(m
1
,m
2
) : m
1
? r
j
,m
2
? r
j
,m
1
6= m
2
}
As can be seen, a link is an undirected edge be-
tween two mentions, and it can be equivalently
represented by a pair of mentions. Note that when
an entity consists of a single mention, its corefer-
ence link set is empty.
Let N
k
(i, j) (i 6= j) be key non-coreference
links formed between mentions in k
i
and those
in k
j
, and let N
r
(i, j) (i 6= j) be response non-
coreference links formed between mentions in r
i
and those in r
j
, respectively:
N
k
(i, j) = {(m
1
,m
2
) : m
1
? k
i
,m
2
? k
j
}
N
r
(i, j) = {(m
1
,m
2
) : m
1
? r
i
,m
2
? r
j
}
Note that the non-coreference link set is empty
when all mentions are in the same entity.
We use the same letter and subscription with-
out the index in parentheses to denote the union of
sets, e.g.,
C
k
= ?
i
C
k
(i), N
k
= ?
i 6=j
N
k
(i, j)
C
r
= ?
j
C
r
(j), N
r
= ?
i6=j
N
r
(i, j)
We use T
k
= C
k
? N
k
and T
r
= C
r
? N
r
to
denote the total set of key links and total set of
response links, respectively. Clearly, C
k
and N
k
form a partition of T
k
since C
k
? N
k
= ?, T
k
=
C
k
?N
k
. Likewise, C
r
and N
r
form a partition of
T
r
.
We say that a key link l
1
? T
k
equals a response
link l
2
? T
r
if and only if the pair of mentions
from which the links are formed are identical. We
write l
1
= l
2
if two links are equal. It is easy to
see that the gold mention assumption?same set
of response mentions as the set of key mentions?
can be equivalently stated as T
k
= T
r
(this does
not necessarily mean that C
k
= C
r
or N
k
= N
r
).
We also use | ? | to denote the size of a set.
3 Original BLANC
BLANC-gold is adapted from Rand Index (Rand,
1971), a metric for clustering objects. Rand Index
is defined as the ratio between the number of cor-
rect within-cluster links plus the number of correct
cross-cluster links, and the total number of links.
When T
k
= T
r
, Rand Index can be applied di-
rectly since coreference resolution reduces to a
clustering problem where mentions are partitioned
into clusters (entities):
Rand Index =
|C
k
? C
r
|+ |N
k
?N
r
|
1
2
(
|T
k
|(|T
k
| ? 1)
)
(1)
In practice, though, the simple-minded adoption
of Rand Index is not satisfactory since the number
of non-coreference links often overwhelms that of
coreference links (Recasens and Hovy, 2011), or,
|N
k
|  |C
k
| and |N
r
|  |C
r
|. Rand Index, if
used without modification, would not be sensitive
to changes of coreference links.
BLANC-gold solves this problem by averaging
the F-measure computed over coreference links
and the F-measure over non-coreference links.
Using the notations in Section 2, the recall, pre-
cision, and F-measure on coreference links are:
R
(g)
c
=
|C
k
? C
r
|
|C
k
? C
r
|+ |C
k
?N
r
|
(2)
P
(g)
c
=
|C
k
? C
r
|
|C
r
? C
k
|+ |C
r
?N
k
|
(3)
F
(g)
c
=
2R
(g)
c
P
(g)
c
R
(g)
c
+ P
(g)
c
; (4)
Similarly, the recall, precision, and F-measure on
non-coreference links are computed as:
R
(g)
n
=
|N
k
?N
r
|
|N
k
? C
r
|+ |N
k
?N
r
|
(5)
P
(g)
n
=
|N
k
?N
r
|
|N
r
? C
k
|+ |N
r
?N
k
|
(6)
F
(g)
n
=
2R
(g)
n
P
(g)
n
R
(g)
n
+ P
(g)
n
. (7)
25
Finally, the BLANC-gold metric is the arithmetic
average of F
(g)
c
and F
(g)
n
:
BLANC
(g)
=
F
(g)
c
+ F
(g)
n
2
. (8)
Superscript
g
in these equations highlights the fact
that they are meant for coreference systems with
gold mentions.
Eqn. (8) indicates that BLANC-gold assigns
equal weight to F
(g)
c
, the F-measure from coref-
erence links, and F
(g)
n
, the F-measure from non-
coreference links. This avoids the problem that
|N
k
|  |C
k
| and |N
r
|  |C
r
|, should the original
Rand Index be used.
In Eqn. (2) - (3) and Eqn. (5) - (6), denominators
are written as a sum of disjoint subsets so they can
be related to the contingency table in (Recasens
and Hovy, 2011). Under the assumption that T
k
=
T
r
, it is clear that C
k
= (C
k
? C
r
) ? (C
k
?N
r
),
C
r
= (C
k
? C
r
) ? (N
k
? C
r
), and so on.
4 BLANC for Imperfect Response
Mentions
Under the assumption that the key and response
mention sets are identical (which implies that
T
k
= T
r
), Equations (2) to (7) make sense. For
example, R
c
is the ratio of the number of correct
coreference links over the number of key corefer-
ence links; P
c
is the ratio of the number of cor-
rect coreference links over the number of response
coreference links, and so on.
However, when response mentions are not iden-
tical to key mentions, a key coreference link may
not appear in either C
r
or N
r
, so Equations (2) to
(7) cannot be applied directly to systems with im-
perfect mentions. For instance, if the key entities
are {a,b,c} {d,e}; and the response entities
are {b,c} {e,f,g}, then the key coreference
link (a,b) is not seen on the response side; sim-
ilarly, it is possible that a response link does not
appear on the key side either: (c,f) and (f,g)
are not in the key in the above example.
To account for missing or spurious links, we ob-
serve that
? C
k
\ T
r
are key coreference links missing in
the response;
? N
k
\ T
r
are key non-coreference links miss-
ing in the response;
? C
r
\ T
k
are response coreference links miss-
ing in the key;
? N
r
\ T
k
are response non-coreference links
missing in the key,
and we propose to extend the coreference F-
measure and non-coreference F-measure as fol-
lows. Coreference recall, precision and F-measure
are changed to:
R
c
=
|C
k
? C
r
|
|C
k
? C
r
|+ |C
k
?N
r
|+ |C
k
\ T
r
|
(9)
P
c
=
|C
k
? C
r
|
|C
r
? C
k
|+ |C
r
?N
k
|+ |C
r
\ T
k
|
(10)
F
c
=
2R
c
P
c
R
c
+ P
c
(11)
Non-coreference recall, precision and F-measure
are changed to:
R
n
=
|N
k
?N
r
|
|N
k
? C
r
|+ |N
k
?N
r
|+ |N
k
\ T
r
|
(12)
P
n
=
|N
k
?N
r
|
|N
r
? C
k
|+ |N
r
?N
k
|+ |N
r
\ T
k
|
(13)
F
n
=
2R
n
P
n
R
n
+ P
n
. (14)
The proposed BLANC continues to be the arith-
metic average of F
c
and F
n
:
BLANC =
F
c
+ F
n
2
. (15)
We observe that the definition of the proposed
BLANC, Equ. (9)-(14) subsume the BLANC-
gold (2) to (7) due to the following proposition:
If T
k
= T
r
, then BLANC = BLANC
(g)
.
Proof. We only need to show that R
c
= R
(g)
c
,
P
c
= P
(g)
c
, R
n
= R
(g)
n
, and P
n
= P
(g)
n
. We prove
the first one (the other proofs are similar and elided
due to space limitations). Since T
k
= T
r
and
C
k
? T
k
, we have C
k
? T
r
; thus C
k
\T
r
= ?, and
|C
k
? T
r
| = 0. This establishes that R
c
= R
(g)
c
.
Indeed, since C
k
is a union of three disjoint sub-
sets: C
k
= (C
k
? C
r
) ? (C
k
? N
r
) ? (C
k
\ T
r
),
R
(g)
c
and R
c
can be unified as
|C
k
?C
r
|
|C
K
|
. Unification
for other component recalls and precisions can be
done similarly. So the final definition of BLANC
can be succinctly stated as:
R
c
=
|C
k
? C
r
|
|C
k
|
, P
c
=
|C
k
? C
r
|
|C
r
|
(16)
R
n
=
|N
k
?N
r
|
|N
k
|
, P
n
=
|N
k
?N
r
|
|N
r
|
(17)
F
c
=
2|C
k
? C
r
|
|C
k
|+ |C
r
|
, F
n
=
2|N
k
?N
r
|
|N
k
|+ |N
r
|
(18)
BLANC =
F
c
+ F
n
2
(19)
26
4.1 Boundary Cases
Care has to be taken when counts of the BLANC
definition are 0. This can happen when all key
(or response) mentions are in one cluster or are
all singletons: the former case will lead to N
k
= ?
(or N
r
= ?); the latter will lead to C
k
= ? (or
C
r
= ?). Observe that as long as |C
k
|+ |C
r
| > 0,
F
c
in (18) is well-defined; as long as |N
k
|+|N
r
| >
0, F
n
in (18) is well-defined. So we only need to
augment the BLANC definition for the following
cases:
(1) If C
k
= C
r
= ? and N
k
= N
r
= ?, then
BLANC = I(M
k
= M
r
), where I(?) is an in-
dicator function whose value is 1 if its argument
is true, and 0 otherwise. M
k
and M
r
are the key
and response mention set. This can happen when a
document has no more than one mention and there
is no link.
(2) If C
k
= C
r
= ? and |N
k
| + |N
r
| > 0, then
BLANC = F
n
. This is the case where the key
and response side has only entities consisting of
singleton mentions. Since there is no coreference
link, BLANC reduces to the non-coreference F-
measure F
n
.
(3) If N
k
= N
r
= ? and |C
k
| + |C
r
| > 0, then
BLANC = F
c
. This is the case where all mentions
in the key and response are in one entity. Since
there is no non-coreference link, BLANC reduces
to the coreference F-measure F
c
.
4.2 Toy Examples
We walk through a few examples and show how
BLANC is calculated in detail. In all the examples
below, each lower-case letter represents a mention;
mentions in an entity are closed in {}; two letters
in () represent a link.
Example 1. Key entities are {abc} and {d}; re-
sponse entities are {bc} and {de}. Obviously,
C
k
= {(ab), (bc), (ac)};
N
k
= {(ad), (bd), (cd)};
C
r
= {(bc), (de)};
N
r
= {(bd), (be), (cd), (ce)}.
Therefore, C
k
? C
r
= {(bc)}, N
k
? N
r
=
{(bd), (cd)}, and R
c
=
1
3
, P
c
=
1
2
, F
c
=
2
5
; R
n
=
2
3
, P
n
=
2
4
, F
n
=
4
7
. Finally, BLANC =
17
35
.
Example 2. Key entity is {a}; response entity
is {b}. This is boundary case (1): BLANC = 0.
Example 3. Key entities are {a}{b}{c}; re-
sponse entities are {a}{b}{d}. This is boundary
case (2): there are no coreference links. Since
N
k
= {(ab), (bc), (ca)},
Participant R P BLANC
lee 50.23 49.28 48.84
sapena 40.68 49.05 44.47
nugues 47.83 44.22 45.95
chang 44.71 47.48 45.49
stoyanov 49.37 29.80 34.58
santos 46.74 37.33 41.33
song 36.88 39.69 30.92
sobha 35.42 39.56 36.31
yang 47.95 29.12 36.09
charton 42.32 31.54 35.65
hao 45.41 32.75 36.98
zhou 29.93 45.58 34.95
kobdani 32.29 33.01 32.57
xinxin 36.83 34.39 35.02
kummerfeld 34.84 29.53 30.98
zhang 30.10 43.96 35.71
zhekova 26.40 15.32 15.37
irwin 3.62 28.28 6.28
Table 1: The proposed BLANC scores of the
CoNLL-2011 shared task participants.
N
r
= {(ab), (bd), (ad)},
we have
N
k
?N
r
= {(ab)}, and R
n
=
1
3
, P
n
=
1
3
.
So BLANC = F
n
=
1
3
.
Example 4. Key entity is {abc}; response entity
is {bc}. This is boundary case (3): there are no
non-coreference links. Since
C
k
= {(ab), (bc), (ca)}, and C
r
= {(bc)},
we have
C
k
? C
r
= {(bc)}, and R
c
=
1
3
, P
c
= 1,
So BLANC = F
c
=
2
4
=
1
2
.
5 Results
5.1 CoNLL-2011/12
We have updated the publicly available CoNLL
coreference scorer
1
with the proposed BLANC,
and used it to compute the proposed BLANC
scores for all the CoNLL 2011 (Pradhan et al,
2011) and 2012 (Pradhan et al, 2012) participants
in the official track, where participants had to au-
tomatically predict the mentions. Tables 1 and 2
report the updated results.
2
5.2 Correlation with Other Measures
Figure 1 shows how the proposed BLANC mea-
sure works when compared with existing met-
rics such as MUC, B-cubed and CEAF, us-
ing the BLANC and F1 scores. The proposed
BLANC is highly positively correlated with the
1
http://code.google.com/p/reference-coreference-scorers
2
The order is kept the same as in Pradhan et al (2011) and
Pradhan et al (2012) for easy comparison.
27
Participant R P BLANC
Language: Arabic
fernandes 33.43 44.66 37.99
bjorkelund 32.65 45.47 37.93
uryupina 31.62 35.26 33.02
stamborg 32.59 36.92 34.50
chen 31.81 31.52 30.82
zhekova 11.04 62.58 18.51
li 4.60 56.63 8.42
Language: English
fernandes 54.91 63.66 58.75
martschat 52.00 58.84 55.04
bjorkelund 52.01 59.55 55.42
chang 52.85 55.03 53.86
chen 50.52 56.82 52.87
chunyang 51.19 55.47 52.65
stamborg 54.39 54.88 54.42
yuan 50.58 54.29 52.11
xu 45.99 54.59 46.47
shou 49.55 52.46 50.44
uryupina 44.15 48.89 46.04
songyang 40.60 50.85 45.10
zhekova 41.46 33.13 34.80
xinxin 44.39 32.79 36.54
li 25.17 52.96 31.85
Language: Chinese
chen 48.45 62.44 54.10
yuan 53.15 40.75 43.20
bjorkelund 47.58 45.93 44.22
xu 44.11 36.45 38.45
fernandes 42.36 61.72 49.63
stamborg 39.60 55.12 45.89
uryupina 33.44 56.01 41.88
martschat 27.24 62.33 37.89
chunyang 37.43 36.18 36.77
xinxin 36.46 39.79 37.85
li 21.61 62.94 30.37
chang 18.74 40.76 25.68
zhekova 21.50 37.18 22.89
Table 2: The proposed BLANC scores of the
CoNLL-2012 shared task participants.
R P F1
MUC 0.975 0.844 0.935
B-cubed 0.981 0.942 0.966
CEAF-m 0.941 0.923 0.966
CEAF-e 0.797 0.781 0.919
Table 3: Pearson?s r correlation coefficients be-
tween the proposed BLANC and the other coref-
erence measures based on the CoNLL 2011/2012
results. All p-values are significant at < 0.001.
l
lll
l
l
l
llll lll
l
l
l
l
ll
l
l
lll
l
l l
llllll
lll
ll l
l
lll
l
l
0 10 20 30 40 50 60 70
10
20
30
40
50
60
MUC
BLA
NC
l
lll
l
l
l
lllllll
l
l
l
l
ll
l
lllll
l
ll
llllll
ll
lll
ll
l
l
l
0 10 20 30 40 50 60 70
10
20
30
40
50
60
B?cubed
BLA
NC
l
lll
l
l
l
lllllll
l
l
l
l
ll
l
lllll
l
ll
lllllll
ll
lll
ll
ll
l
l
0 10 20 30 40 50 60 70
10
20
30
40
50
60
CEAF?m
BLA
NC
l
ll
l
l
l
llll lll l
l
l
l
l
ll
l
llllll
l
ll
llllllll
ll
l
l
ll
l
l
0 10 20 30 40 50 60 70
10
20
30
40
50
60
CEAF?e
BLA
NC
Figure 1: Correlation plot between the proposed
BLANC and the other measures based on the
CoNLL 2011/2012 results. All values are F1
scores.
other measures along R, P and F1 (Table 3),
showing that BLANC is able to capture most
entity-based similarities measured by B-cubed and
CEAF. However, the CoNLL data sets come from
OntoNotes (Hovy et al, 2006), where singleton
entities are not annotated, and BLANC has a wider
dynamic range on data sets with singletons (Re-
casens and Hovy, 2011). So the correlations will
likely be lower on data sets with singleton entities.
6 Conclusion
The original BLANC-gold (Recasens and Hovy,
2011) requires that system mentions be identical
to gold mentions, which limits the metric?s utility
since detected system mentions often have missing
key mentions or spurious mentions. The proposed
BLANC is free from this assumption, and we
have shown that it subsumes the original BLANC-
gold. Since BLANC works on imperfect system
mentions, we have used it to score the CoNLL
2011 and 2012 coreference systems. The BLANC
scores show strong correlation with existing met-
rics, especially B-cubed and CEAF-m.
Acknowledgments
We would like to thank the three anonymous re-
viewers for their invaluable suggestions for im-
proving the paper. This work was partially sup-
ported by grants R01LM10090 from the National
Library of Medicine.
28
References
Amit Bagga and Breck Baldwin. 1998. Algorithms
for scoring coreference chains. In Proceedings of
the Linguistic Coreference Workshop at The First In-
ternational Conference on Language Resources and
Evaluation (LREC?98), pages 563?566.
Eduard Hovy, Mitchell Marcus, Martha Palmer,
Lance Ramshaw, and Ralph Weischedel. 2006.
OntoNotes: The 90% solution. In Proceedings of
the Human Language Technology Conference of the
NAACL, Companion Volume: Short Papers, pages
57?60, New York City, USA, June. Association for
Computational Linguistics.
Xiaoqiang Luo. 2005. On coreference resolution per-
formance metrics. In Proc. of Human Language
Technology (HLT)/Empirical Methods in Natural
Language Processing (EMNLP).
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Nianwen
Xue. 2011. CoNLL-2011 shared task: Modeling
unrestricted coreference in OntoNotes. In Proceed-
ings of the Fifteenth Conference on Computational
Natural Language Learning: Shared Task, pages 1?
27, Portland, Oregon, USA, June. Association for
Computational Linguistics.
Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,
Olga Uryupina, and Yuchen Zhang. 2012. CoNLL-
2012 shared task: Modeling multilingual unre-
stricted coreference in OntoNotes. In Joint Confer-
ence on EMNLP and CoNLL - Shared Task, pages
1?40, Jeju Island, Korea, July. Association for Com-
putational Linguistics.
W. M. Rand. 1971. Objective criteria for the evalua-
tion of clustering methods. Journal of the American
Statistical Association, 66(336):846?850.
M. Recasens and E. Hovy. 2011. BLANC: Implement-
ing the Rand index for coreference evaluation. Nat-
ural Language Engineering, 17:485?510, 10.
Veselin Stoyanov, Nathan Gilbert, Claire Cardie, and
Ellen Riloff. 2009. Conundrums in noun phrase
coreference resolution: Making sense of the state-
of-the-art. In Proceedings of the Joint Conference of
the 47th Annual Meeting of the ACL and the 4th In-
ternational Joint Conference on Natural Language
Processing of the AFNLP: Volume 2 - Volume 2,
ACL ?09, pages 656?664, Stroudsburg, PA, USA.
Association for Computational Linguistics.
M. Vilain, J. Burger, J. Aberdeen, D. Connolly, , and
L. Hirschman. 1995. A model-theoretic coreference
scoring scheme. In In Proc. of MUC6, pages 45?52.
29
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 30?35,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Scoring Coreference Partitions of Predicted Mentions:
A Reference Implementation
Sameer Pradhan
1
, Xiaoqiang Luo
2
, Marta Recasens
3
,
Eduard Hovy
4
, Vincent Ng
5
and Michael Strube
6
1
Harvard Medical School, Boston, MA,
2
Google Inc., New York, NY
3
Google Inc., Mountain View, CA,
4
Carnegie Mellon University, Pittsburgh, PA
5
HLTRI, University of Texas at Dallas, Richardson, TX,
6
HITS, Heidelberg, Germany
sameer.pradhan@childrens.harvard.edu, {xql,recasens}@google.com,
hovy@cmu.edu, vince@hlt.utdallas.edu, michael.strube@h-its.org
Abstract
The definitions of two coreference scoring
metrics?B
3
and CEAF?are underspeci-
fied with respect to predicted, as opposed
to key (or gold) mentions. Several varia-
tions have been proposed that manipulate
either, or both, the key and predicted men-
tions in order to get a one-to-one mapping.
On the other hand, the metric BLANC was,
until recently, limited to scoring partitions
of key mentions. In this paper, we (i) ar-
gue that mention manipulation for scoring
predicted mentions is unnecessary, and po-
tentially harmful as it could produce unin-
tuitive results; (ii) illustrate the application
of all these measures to scoring predicted
mentions; (iii) make available an open-
source, thoroughly-tested reference imple-
mentation of the main coreference eval-
uation measures; and (iv) rescore the re-
sults of the CoNLL-2011/2012 shared task
systems with this implementation. This
will help the community accurately mea-
sure and compare new end-to-end corefer-
ence resolution algorithms.
1 Introduction
Coreference resolution is a key task in natural
language processing (Jurafsky and Martin, 2008)
aiming to detect the referential expressions (men-
tions) in a text that point to the same entity.
Roughly over the past two decades, research in
coreference (for the English language) had been
plagued by individually crafted evaluations based
on two central corpora?MUC (Hirschman and
Chinchor, 1997; Chinchor and Sundheim, 2003;
Chinchor, 2001) and ACE (Doddington et al,
2004). Experimental parameters ranged from us-
ing perfect (gold, or key) mentions as input for
purely testing the quality of the entity linking al-
gorithm, to an end-to-end evaluation where pre-
dicted mentions are used. Given the range of
evaluation parameters and disparity between the
annotation standards for the two corpora, it was
very hard to grasp the state of the art for the
task of coreference. This has been expounded in
Stoyanov et al (2009). The activity in this sub-
field of NLP can be gauged by: (i) the contin-
ual addition of corpora manually annotated for
coreference?The OntoNotes corpus (Pradhan et
al., 2007; Weischedel et al, 2011) in the general
domain, as well as the i2b2 (Uzuner et al, 2012)
and THYME (Styler et al, 2014) corpora in the
clinical domain would be a few examples of such
emerging corpora; and (ii) ongoing proposals for
refining the existing metrics to make them more
informative (Holen, 2013; Chen and Ng, 2013).
The CoNLL-2011/2012 shared tasks on corefer-
ence resolution using the OntoNotes corpus (Prad-
han et al, 2011; Pradhan et al, 2012) were an
attempt to standardize the evaluation settings by
providing a benchmark annotated corpus, scorer,
and state-of-the-art system results that would al-
low future systems to compare against them. Fol-
lowing the timely emphasis on end-to-end evalu-
ation, the official track used predicted mentions
and measured performance using five coreference
measures: MUC (Vilain et al, 1995), B
3
(Bagga
and Baldwin, 1998), CEAF
e
(Luo, 2005), CEAF
m
(Luo, 2005), and BLANC (Recasens and Hovy,
2011). The arithmetic mean of the first three was
the task?s final score.
An unfortunate setback to these evaluations had
its root in three issues: (i) the multiple variations
of two of the scoring metrics?B
3
and CEAF?
used by the community to handle predicted men-
tions; (ii) a buggy implementation of the Cai and
Strube (2010) proposal that tried to reconcile these
variations; and (iii) the erroneous computation of
30
the BLANC metric for partitions of predicted men-
tions. Different interpretations as to how to com-
pute B
3
and CEAF scores for coreference systems
when predicted mentions do not perfectly align
with key mentions?which is usually the case?
led to variations of these metrics that manipulate
the gold standard and system output in order to
get a one-to-one mention mapping (Stoyanov et
al., 2009; Cai and Strube, 2010). Some of these
variations arguably produce rather unintuitive re-
sults, while others are not faithful to the original
measures.
In this paper, we address the issues in scor-
ing coreference partitions of predicted mentions.
Specifically, we justify our decision to go back
to the original scoring algorithms by arguing that
manipulation of key or predicted mentions is un-
necessary and could in fact produce unintuitive re-
sults. We demonstrate the use of our recent ex-
tension of BLANC that can seamlessly handle pre-
dicted mentions (Luo et al, 2014). We make avail-
able an open-source, thoroughly-tested reference
implementation of the main coreference evalua-
tion measures that do not involve mention manip-
ulation and is faithful to the original intentions of
the proposers of these metrics. We republish the
CoNLL-2011/2012 results based on this scorer, so
that future systems can use it for evaluation and
have the CoNLL results available for comparison.
The rest of the paper is organized as follows.
Section 2 provides an overview of the variations
of the existing measures. We present our newly
updated coreference scoring package in Section 3
together with the rescored CoNLL-2011/2012 out-
puts. Section 4 walks through a scoring example
for all the measures, and we conclude in Section 5.
2 Variations of Scoring Measures
Two commonly used coreference scoring metrics
?B
3
and CEAF?are underspecified in their ap-
plication for scoring predicted, as opposed to key
mentions. The examples in the papers describing
these metrics assume perfect mentions where pre-
dicted mentions are the same set of mentions as
key mentions. The lack of accompanying refer-
ence implementation for these metrics by its pro-
posers made it harder to fill the gaps in the speci-
fication. Subsequently, different interpretations of
how one can evaluate coreference systems when
predicted mentions do not perfectly align with key
mentions led to variations of these metrics that ma-
nipulate the gold and/or predicted mentions (Stoy-
anov et al, 2009; Cai and Strube, 2010). All these
variations attempted to generate a one-to-one map-
ping between the key and predicted mentions, as-
suming that the original measures cannot be ap-
plied to predicted mentions. Below we first pro-
vide an overview of these variations and then dis-
cuss the unnecessity of this assumption.
Coining the term twinless mentions for those
mentions that are either spurious or missing from
the predicted mention set, Stoyanov et al (2009)
proposed two variations to B
3
? B
3
all
and B
3
0
?to
handle them. In the first variation, all predicted
twinless mentions are retained, whereas the lat-
ter discards them and penalizes recall for twin-
less predicted mentions. Rahman and Ng (2009)
proposed another variation by removing ?all and
only those twinless system mentions that are sin-
gletons before applying B
3
and CEAF.? Follow-
ing upon this line of research, Cai and Strube
(2010) proposed a unified solution for both B
3
and
CEAF
m
, leaving the question of handling CEAF
e
as future work because ?it produces unintuitive
results.? The essence of their solution involves
manipulating twinless key and predicted mentions
by adding them either from the predicted parti-
tion to the key partition or vice versa, depend-
ing on whether one is computing precision or re-
call. The Cai and Strube (2010) variation was used
by the CoNLL-2011/2012 shared tasks on corefer-
ence resolution using the OntoNotes corpus, and
by the i2b2 2011 shared task on coreference res-
olution using an assortment of clinical notes cor-
pora (Uzuner et al, 2012).
1
It was later identified
by Recasens et al (2013) that there was a bug in
the implementation of this variation in the scorer
used for the CoNLL-2011/2012 tasks. We have
not tested the correctness of this variation in the
scoring package used for the i2b2 shared task.
However, it turns out that the CEAF metric (Luo,
2005) was always intended to work seamlessly on
predicted mentions, and so has been the case with
the B
3
metric.
2
In a latter paper, Rahman and Ng
(2011) correctly state that ?CEAF can compare par-
titions with twinless mentions without any modifi-
cation.? We will look at this further in Section 4.3.
We argue that manipulations of key and re-
sponse mentions/entities, as is done in the exist-
ing B
3
variations, not only confound the evalu-
ation process, but are also subject to abuse and
can seriously jeopardize the fidelity of the evalu-
1
Personal communication with Andreea Bodnari, and
contents of the i2b2 scorer code.
2
Personal communication with Breck Baldwin.
31
ation. Given space constraints we use an exam-
ple worked out in Cai and Strube (2010). Let
the key contain an entity with mentions {a, b, c}
and the prediction contain an entity with mentions
{a, b, d}. As detailed in Cai and Strube (2010,
p. 29-30, Tables 1?3), B
3
0
assigns a perfect pre-
cision of 1.00 which is unintuitive as the system
has wrongly predicted a mention d as belonging to
the entity. For the same prediction, B
3
all
assigns a
precision of 0.556. But, if the prediction contains
two entities {a, b, d} and {c} (i.e., the mention c
is added as a spurious singleton), then B
3
all
preci-
sion increases to 0.667 which is counter-intuitive
as it does not penalize the fact that c is erroneously
placed in its own entity. The version illustrated in
Section 4.2, which is devoid of any mention ma-
nipulations, gives a precision of 0.444 in the first
scenario and the precision drops to 0.333 in the
second scenario with the addition of a spurious
singleton entity {c}. This is a more intuitive be-
havior.
Contrary to both B
3
and CEAF, the BLANC mea-
sure (Recasens and Hovy, 2011) was never de-
signed to handle predicted mentions. However, the
implementation used for the SemEval-2010 shared
task as well as the one for the CoNLL-2011/2012
shared tasks accepted predicted mentions as input,
producing undefined results. In Luo et al (2014)
we have extended the BLANC metric to deal with
predicted mentions
3 Reference Implementation
Given the potential unintuitive outcomes of men-
tion manipulation and the misunderstanding that
the original measures could not handle twinless
predicted mentions (Section 2), we redesigned the
CoNLL scorer. The new implementation:
? is faithful to the original measures;
? removes any prior mention manipulation,
which might depend on specific annotation
guidelines among other problems;
? has been thoroughly tested to ensure that it
gives the expected results according to the
original papers, and all test cases are included
as part of the release;
? is free of the reported bugs that the CoNLL
scorer (v4) suffered (Recasens et al, 2013);
? includes the extension of BLANC to handle
predicted mentions (Luo et al, 2014).
This is the open source scoring package
3
that
we present as a reference implementation for the
3
http://code.google.com/p/reference-coreference-scorers/
SYSTEM MD MUC B
3
CEAF BLANC CONLL
m e AVERAGE
F
1
F
1
1
F
2
1
F
1
F
3
1
CoNLL-2011; English
lee 70.7 59.6 48.9 53.0 46.1 48.8 51.5
sapena 68.4 59.5 46.5 51.3 44.0 44.5 50.0
nugues 69.0 58.6 45.0 48.4 40.0 46.0 47.9
chang 64.9 57.2 46.0 50.7 40.0 45.5 47.7
stoyanov 67.8 58.4 40.1 43.3 36.9 34.6 45.1
santos 65.5 56.7 42.9 45.1 35.6 41.3 45.0
song 67.3 60.0 41.4 41.0 33.1 30.9 44.8
sobha 64.8 50.5 39.5 44.2 39.4 36.3 43.1
yang 63.9 52.3 39.4 43.2 35.5 36.1 42.4
charton 64.3 52.5 38.0 42.6 34.5 35.7 41.6
hao 64.3 54.5 37.7 41.9 31.6 37.0 41.3
zhou 62.3 49.0 37.0 40.6 35.0 35.0 40.3
kobdani 61.0 53.5 34.8 38.1 34.1 32.6 38.7
xinxin 61.9 46.6 34.9 37.7 31.7 35.0 37.7
kummerfeld 62.7 42.7 34.2 38.8 35.5 31.0 37.5
zhang 61.1 47.9 34.4 37.8 29.2 35.7 37.2
zhekova 48.3 24.1 23.7 23.4 20.5 15.4 22.8
irwin 26.7 20.0 11.7 18.5 14.7 6.3 15.5
CoNLL-2012; English
fernandes 77.7 70.5 57.6 61.4 53.9 58.8 60.7
martschat 75.2 67.0 54.6 58.8 51.5 55.0 57.7
bjorkelund 75.4 67.6 54.5 58.2 50.2 55.4 57.4
chang 74.3 66.4 53.0 57.1 48.9 53.9 56.1
chen 73.8 63.7 51.8 55.8 48.1 52.9 54.5
chunyang 73.7 63.8 51.2 55.1 47.6 52.7 54.2
stamborg 73.9 65.1 51.7 55.1 46.6 54.4 54.2
yuan 72.5 62.6 50.1 54.5 46.0 52.1 52.9
xu 72.0 66.2 50.3 51.3 41.3 46.5 52.6
shou 73.7 62.9 49.4 53.2 46.7 50.4 53.0
uryupina 70.9 60.9 46.2 49.3 42.9 46.0 50.0
songyang 68.8 59.8 45.9 49.6 42.4 45.1 49.4
zhekova 67.1 53.5 35.7 39.7 32.2 34.8 40.5
xinxin 62.8 48.3 35.7 38.0 31.9 36.5 38.6
li 59.9 50.8 32.3 36.3 25.2 31.9 36.1
CoNLL-2012; Chinese
chen 71.6 62.2 55.7 60.0 55.0 54.1 57.6
yuan 68.2 60.3 52.4 55.8 50.2 43.2 54.3
bjorkelund 66.4 58.6 51.1 54.2 47.6 44.2 52.5
xu 65.2 58.1 49.5 51.9 46.6 38.5 51.4
fernandes 66.1 60.3 49.6 54.4 44.5 49.6 51.5
stamborg 64.0 57.8 47.4 51.6 41.9 45.9 49.0
uryupina 59.0 53.0 41.7 46.9 37.6 41.9 44.1
martschat 58.6 52.4 40.8 46.0 38.2 37.9 43.8
chunyang 61.6 49.8 39.6 44.2 37.3 36.8 42.2
xinxin 55.9 48.1 38.8 42.9 34.5 37.9 40.5
li 51.5 44.7 31.5 36.7 25.3 30.4 33.8
chang 47.6 37.9 28.8 36.1 29.6 25.7 32.1
zhekova 47.3 40.6 28.1 31.4 21.2 22.9 30.0
CoNLL-2012; Arabic
fernandes 64.8 46.5 42.5 49.2 46.5 38.0 45.2
bjorkelund 60.6 47.8 41.6 46.7 41.2 37.9 43.5
uryupina 55.4 41.5 36.1 41.4 35.0 33.0 37.5
stamborg 59.5 41.2 35.9 40.0 32.9 34.5 36.7
chen 59.8 39.0 32.1 34.7 26.0 30.8 32.4
zhekova 41.0 29.9 22.7 31.1 25.9 18.5 26.2
li 29.7 18.1 13.1 21.0 17.3 8.4 16.2
Table 1: Performance on the official, closed track
in percentages using all predicted information for
the CoNLL-2011 and 2012 shared tasks.
community to use. It is written in perl and stems
from the scorer that was initially used for the
SemEval-2010 shared task (Recasens et al, 2010)
and later modified for the CoNLL-2011/2012
shared tasks.
4
Partitioning detected mentions into entities (or
equivalence classes) typically comprises two dis-
tinct tasks: (i) mention detection; and (ii) coref-
erence resolution. A typical two-step coreference
algorithm uses mentions generated by the best
4
We would like to thank Emili Sapena for writing the first
version of the scoring package.
32
a     b
c
de fg
h
a     bc
de
hi i
f    g f    g
h i
cd
a     b
Solid: KeyDashed: Response Solid: KeyDashed: partition wrt Response Solid: Partition wrt KeyDashed: Response
Figure 1: Example key and response entities along
with the partitions for computing the MUC score.
possible mention detection algorithm as input to
the coreference algorithm. Therefore, ideally one
would want to score the two steps independently
of each other. A peculiarity of the OntoNotes
corpus is that singleton referential mentions are
not annotated, thereby preventing the computation
of a mention detection score independently of the
coreference resolution score. In corpora where all
referential mentions (including singletons) are an-
notated, the mention detection score generated by
this implementation is independent of the corefer-
ence resolution score.
We used this reference implementation to
rescore the CoNLL-2011/2012 system outputs for
the official task to enable future comparisons with
these benchmarks. The new CoNLL-2011/2012
results are in Table 1. We found that the over-
all system ranking remained largely unchanged for
both shared tasks, except for some of the lower
ranking systems that changed one or two places.
However, there was a considerable drop in the
magnitude of all B
3
scores owing to the combi-
nation of two things: (i) mention manipulation, as
proposed by Cai and Strube (2010), adds single-
tons to account for twinless mentions; and (ii) the
B
3
metric allows an entity to be used more than
once as pointed out by Luo (2005). This resulted
in a drop in the CoNLL averages (B
3
is one of the
three measures that make the average).
4 An Illustrative Example
This section walks through the process of com-
puting each of the commonly used metrics for
an example where the set of predicted mentions
has some missing key mentions and some spu-
rious mentions. While the mathematical formu-
lae for these metrics can be found in the original
papers (Vilain et al, 1995; Bagga and Baldwin,
1998; Luo, 2005), many misunderstandings dis-
cussed in Section 2 are due to the fact that these
papers lack an example showing how a metric is
computed on predicted mentions. A concrete ex-
ample goes a long way to prevent similar misun-
derstandings in the future. The example is adapted
from Vilain et al (1995) with some slight modifi-
cations so that the total number of mentions in the
key is different from the number of mentions in
the prediction. The key (K) contains two entities
with mentions {a, b, c} and {d, e, f, g} and the re-
sponse (R) contains three entities with mentions
{a, b}; {c, d} and {f, g, h, i}:
K =
K
1
? ?? ?
{a, b, c}
K
2
? ?? ?
{d, e, f, g} (1)
R =
R
1
? ?? ?
{a, b}
R
2
? ?? ?
{c, d}
R
3
? ?? ?
{f, g, h, i}. (2)
Mention e is missing from the response, and men-
tions h and i are spurious in the response. The fol-
lowing sections use R to denote recall and P for
precision.
4.1 MUC
The main step in the MUC scoring is creating the
partitions with respect to the key and response re-
spectively, as shown in Figure 1. Once we have
the partitions, then we compute the MUC score by:
R =
?
N
k
i=1
(|K
i
| ? |p(K
i
)|)
?
N
k
i=1
(|K
i
| ? 1)
=
(3? 2) + (4? 3)
(3? 1) + (4? 1)
= 0.40
P =
?
N
r
i=1
(|R
i
| ? |p
?
(R
i
)|)
?
N
r
i=1
(|R
i
| ? 1)
=
(2? 1) + (2? 2) + (4? 3)
(2? 1) + (2? 1) + (4? 1)
= 0.40,
where K
i
is the i
th
key entity and p(K
i
) is the
set of partitions created by intersecting K
i
with
response entities (cf. the middle sub-figure in Fig-
ure 1); R
i
is the i
th
response entity and p
?
(R
i
) is
the set of partitions created by intersectingR
i
with
key entities (cf. the right-most sub-figure in Fig-
ure 1); and N
k
and N
r
are the number of key and
response entities, respectively.
The MUC F
1
score in this case is 0.40.
4.2 B
3
For computing B
3
recall, each key mention is as-
signed a credit equal to the ratio of the number of
correct mentions in the predicted entity contain-
ing the key mention to the size of the key entity to
which the mention belongs, and the recall is just
33
the sum of credits over all key mentions normal-
ized over the number of key mentions. B
3
preci-
sion is computed similarly, except switching the
role of key and response. Applied to the example:
R =
?
N
k
i=1
?
N
r
j=1
|K
i
?R
j
|
2
|K
i
|
?
N
k
i=1
|K
i
|
=
1
7
? (
2
2
3
+
1
2
3
+
1
2
4
+
2
2
4
) =
1
7
?
35
12
? 0.42
P =
?
N
k
i=1
?
N
r
j=1
|K
i
?R
j
|
2
|R
j
|
?
N
r
i=1
|R
j
|
=
1
8
? (
2
2
2
+
1
2
2
+
1
2
2
+
2
2
4
) =
1
8
?
4
1
= 0.50
Note that terms with 0 value are omitted. The B
3
F
1
score is 0.46.
4.3 CEAF
The first step in the CEAF computation is getting
the best scoring alignment between the key and
response entities. In this case the alignment is
straightforward. Entity R
1
aligns with K
1
and R
3
aligns with K
2
. R
2
remains unaligned.
CEAF
m
CEAF
m
recall is the number of aligned mentions
divided by the number of key mentions, and preci-
sion is the number of aligned mentions divided by
the number of response mentions:
R =
|K
1
? R
1
|+ |K
2
? R
3
|
|K
1
|+ |K
2
|
=
(2 + 2)
(3 + 4)
? 0.57
P =
|K
1
? R
1
|+ |K
2
? R
3
|
|R
1
|+ |R
2
|+ |R
3
|
=
(2 + 2)
(2 + 2 + 4)
= 0.50
The CEAF
m
F
1
score is 0.53.
CEAF
e
We use the same notation as in Luo (2005):
?
4
(K
i
, R
j
) to denote the similarity between a key
entity K
i
and a response entity R
j
. ?
4
(K
i
, R
j
) is
defined as:
?
4
(K
i
, R
j
) =
2? |K
i
? R
j
|
|K
i
|+ |R
j
|
.
CEAF
e
recall and precision, when applied to this
example, are:
R =
?
4
(K
1
, R
1
) + ?
4
(K
2
, R
3
)
N
k
=
(2?2)
(3+2)
+
(2?2)
(4+4)
2
= 0.65
P =
?
4
(K
1
, R
1
) + ?
4
(K
2
, R
3
)
N
r
=
(2?2)
(3+2)
+
(2?2)
(4+4)
3
? 0.43
The CEAF
e
F
1
score is 0.52.
4.4 BLANC
The BLANC metric illustrated here is the one in
our implementation which extends the original
BLANC (Recasens and Hovy, 2011) to predicted
mentions (Luo et al, 2014).
Let C
k
and C
r
be the set of coreference links
in the key and response respectively, and N
k
and
N
r
be the set of non-coreference links in the key
and response respectively. A link between a men-
tion pair m and n is denoted by mn; then for the
example in Figure 1, we have
C
k
= {ab, ac, bc, de, df, dg, ef, eg, fg}
N
k
= {ad, ae, af, ag, bd, be, bf, bg, cd, ce, cf, cg}
C
r
= {ab, cd, fg, fh, fi, gh, gi, hi}
N
r
= {ac, ad, af, ag, ah, ai, bc, bd, bf, bg, bh, bi,
cf, cg, ch, ci, df, dg, dh, di}.
Recall and precision for coreference links are:
R
c
=
|C
k
? C
r
|
|C
k
|
=
2
9
? 0.22
P
c
=
|C
k
? C
r
|
|C
r
|
=
2
8
= 0.25
and the coreference F-measure, F
c
? 0.23. Sim-
ilarly, recall and precision for non-coreference
links are:
R
n
=
|N
k
?N
r
|
|N
k
|
=
8
12
? 0.67
P
n
=
|N
k
?N
r
|
|N
r
|
=
8
20
= 0.40,
and the non-coreference F-measure, F
n
= 0.50.
So the BLANC score is
F
c
+F
n
2
? 0.36.
5 Conclusion
We have cleared several misunderstandings about
coreference evaluation metrics, especially when a
response contains imperfect predicted mentions,
and have argued against mention manipulations
during coreference evaluation. These misunder-
standings are caused partially by the lack of il-
lustrative examples to show how a metric is com-
puted on predicted mentions not aligned perfectly
with key mentions. Therefore, we provide detailed
steps for computing all four metrics on a represen-
tative example. Furthermore, we have a reference
implementation of these metrics that has been rig-
orously tested and has been made available to the
public as open source software. We reported new
scores on the CoNLL 2011 and 2012 data sets,
which can serve as the benchmarks for future re-
search work.
Acknowledgments
This work was partially supported by grants
R01LM10090 from the National Library of
Medicine and IIS-1219142 from the National Sci-
ence Foundation.
34
References
Amit Bagga and Breck Baldwin. 1998. Algorithms
for scoring coreference chains. In Proceedings of
LREC, pages 563?566.
Jie Cai and Michael Strube. 2010. Evaluation metrics
for end-to-end coreference resolution systems. In
Proceedings of SIGDIAL, pages 28?36.
Chen Chen and Vincent Ng. 2013. Linguistically
aware coreference evaluation metrics. In Pro-
ceedings of the Sixth IJCNLP, pages 1366?1374,
Nagoya, Japan, October.
Nancy Chinchor and Beth Sundheim. 2003. Mes-
sage understanding conference (MUC) 6. In
LDC2003T13.
Nancy Chinchor. 2001. Message understanding con-
ference (MUC) 7. In LDC2001T02.
George Doddington, Alexis Mitchell, Mark Przybocki,
Lance Ramshaw, Stephanie Strassel, and Ralph
Weischedel. 2004. The automatic content extrac-
tion (ACE) program-tasks, data, and evaluation. In
Proceedings of LREC.
Lynette Hirschman and Nancy Chinchor. 1997. Coref-
erence task definition (v3.0, 13 jul 97). In Proceed-
ings of the 7th Message Understanding Conference.
Gordana Ilic Holen. 2013. Critical reflections on
evaluation practices in coreference resolution. In
Proceedings of the NAACL-HLT Student Research
Workshop, pages 1?7, Atlanta, Georgia, June.
Daniel Jurafsky and James H. Martin. 2008. Speech
and Language Processing: An Introduction to Nat-
ural Language Processing, Computational Linguis-
tics, and Speech Recognition. Prentice Hall. Second
Edition.
Xiaoqiang Luo, Sameer Pradhan, Marta Recasens, and
Eduard Hovy. 2014. An extension of BLANC to
system mentions. In Proceedings of ACL, Balti-
more, Maryland, June.
Xiaoqiang Luo. 2005. On coreference resolution per-
formance metrics. In Proceedings of HLT-EMNLP,
pages 25?32.
Sameer Pradhan, Eduard Hovy, Mitchell Marcus,
Martha Palmer, Lance Ramshaw, and Ralph
Weischedel. 2007. OntoNotes: A Unified Rela-
tional Semantic Representation. International Jour-
nal of Semantic Computing, 1(4):405?419.
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Nianwen
Xue. 2011. CoNLL-2011 shared task: Modeling
unrestricted coreference in OntoNotes. In Proceed-
ings of CoNLL: Shared Task, pages 1?27.
Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,
Olga Uryupina, and Yuchen Zhang. 2012. CoNLL-
2012 shared task: Modeling multilingual unre-
stricted coreference in OntoNotes. In Proceedings
of CoNLL: Shared Task, pages 1?40.
Altaf Rahman and Vincent Ng. 2009. Supervised mod-
els for coreference resolution. In Proceedings of
EMNLP, pages 968?977.
Altaf Rahman and Vincent Ng. 2011. Coreference res-
olution with world knowledge. In Proceedings of
ACL, pages 814?824.
Marta Recasens and Eduard Hovy. 2011. BLANC:
Implementing the Rand Index for coreference eval-
uation. Natural Language Engineering, 17(4):485?
510.
Marta Recasens, Llu??s M`arquez, Emili Sapena,
M. Ant`onia Mart??, Mariona Taul?e, V?eronique Hoste,
Massimo Poesio, and Yannick Versley. 2010.
Semeval-2010 task 1: Coreference resolution in
multiple languages. In Proceedings of SemEval,
pages 1?8.
Marta Recasens, Marie-Catherine de Marneffe, and
Chris Potts. 2013. The life and death of discourse
entities: Identifying singleton mentions. In Pro-
ceedings of NAACL-HLT, pages 627?633.
Veselin Stoyanov, Nathan Gilbert, Claire Cardie, and
Ellen Riloff. 2009. Conundrums in noun phrase
coreference resolution: Making sense of the state-
of-the-art. In Proceedings of ACL-IJCNLP, pages
656?664.
William F. Styler, Steven Bethard an Sean Finan,
Martha Palmer, Sameer Pradhan, Piet C de Groen,
Brad Erickson, Timothy Miller, Chen Lin, Guergana
Savova, and James Pustejovsky. 2014. Temporal
annotation in the clinical domain. Transactions of
Computational Linguistics, 2(April):143?154.
Ozlem Uzuner, Andreea Bodnari, Shuying Shen, Tyler
Forbush, John Pestian, and Brett R South. 2012.
Evaluating the state of the art in coreference res-
olution for electronic medical records. Journal of
American Medical Informatics Association, 19(5),
September.
Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A model theo-
retic coreference scoring scheme. In Proceedings of
the 6th Message Understanding Conference, pages
45?52.
Ralph Weischedel, Eduard Hovy, Mitchell Marcus,
Martha Palmer, Robert Belvin, Sameer Pradhan,
Lance Ramshaw, and Nianwen Xue. 2011.
OntoNotes: A large training corpus for enhanced
processing. In Joseph Olive, Caitlin Christian-
son, and John McCary, editors, Handbook of Natu-
ral Language Processing and Machine Translation:
DARPA Global Autonomous Language Exploitation.
Springer.
35
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 1?8,
Uppsala, Sweden, 15-16 July 2010.
c
?2010 Association for Computational Linguistics
SemEval-2010 Task 1: Coreference Resolution in Multiple Languages
Marta Recasens
?
Llu??s M
`
arquez
?
Emili Sapena
?
M. Ant
`
onia Mart??
?
Mariona Taul
?
e
?
V
?
eronique Hoste
?
Massimo Poesio

Yannick Versley
??
?: CLiC, University of Barcelona, {mrecasens,amarti,mtaule}@ub.edu
?: TALP, Technical University of Catalonia, {lluism,esapena}@lsi.upc.edu
?: University College Ghent, veronique.hoste@hogent.be
: University of Essex/University of Trento, poesio@essex.ac.uk
??: University of T?ubingen, versley@sfs.uni-tuebingen.de
Abstract
This paper presents the SemEval-2010
task on Coreference Resolution in Multi-
ple Languages. The goal was to evaluate
and compare automatic coreference reso-
lution systems for six different languages
(Catalan, Dutch, English, German, Italian,
and Spanish) in four evaluation settings
and using four different metrics. Such a
rich scenario had the potential to provide
insight into key issues concerning corefer-
ence resolution: (i) the portability of sys-
tems across languages, (ii) the relevance of
different levels of linguistic information,
and (iii) the behavior of scoring metrics.
1 Introduction
The task of coreference resolution, defined as the
identification of the expressions in a text that re-
fer to the same discourse entity (1), has attracted
considerable attention within the NLP community.
(1) Major League Baseball sent its head of se-
curity to Chicago to review the second in-
cident of an on-field fan attack in the last
seven months. The league is reviewing se-
curity at all ballparks to crack down on
spectator violence.
Using coreference information has been shown to
be beneficial in a number of NLP applications
including Information Extraction (McCarthy and
Lehnert, 1995), Text Summarization (Steinberger
et al, 2007), Question Answering (Morton, 1999),
and Machine Translation. There have been a few
evaluation campaigns on coreference resolution in
the past, namely MUC (Hirschman and Chinchor,
1997), ACE (Doddington et al, 2004), and ARE
(Orasan et al, 2008), yet many questions remain
open:
? To what extent is it possible to imple-
ment a general coreference resolution system
portable to different languages? How much
language-specific tuning is necessary?
? How helpful are morphology, syntax and se-
mantics for solving coreference relations?
How much preprocessing is needed? Does its
quality (perfect linguistic input versus noisy
automatic input) really matter?
? How (dis)similar are different coreference
evaluation metrics?MUC, B-CUBED,
CEAF and BLANC? Do they all provide the
same ranking? Are they correlated?
Our goal was to address these questions in a
shared task. Given six datasets in Catalan, Dutch,
English, German, Italian, and Spanish, the task
we present involved automatically detecting full
coreference chains?composed of named entities
(NEs), pronouns, and full noun phrases?in four
different scenarios. For more information, the
reader is referred to the task website.
1
The rest of the paper is organized as follows.
Section 2 presents the corpora from which the task
datasets were extracted, and the automatic tools
used to preprocess them. In Section 3, we describe
the task by providing information about the data
format, evaluation settings, and evaluation met-
rics. Participating systems are described in Sec-
tion 4, and their results are analyzed and compared
in Section 5. Finally, Section 6 concludes.
2 Linguistic Resources
In this section, we first present the sources of the
data used in the task. We then describe the auto-
matic tools that predicted input annotations for the
coreference resolution systems.
1
http://stel.ub.edu/semeval2010-coref
1
Training Development Test
#docs #sents #tokens #docs #sents #tokens #docs #sents #tokens
Catalan 829 8,709 253,513 142 1,445 42,072 167 1,698 49,260
Dutch 145 2,544 46,894 23 496 9,165 72 2,410 48,007
English 229 3,648 79,060 39 741 17,044 85 1,141 24,206
German 900 19,233 331,614 199 4,129 73,145 136 2,736 50,287
Italian 80 2,951 81,400 17 551 16,904 46 1,494 41,586
Spanish 875 9,022 284,179 140 1,419 44,460 168 1,705 51,040
Table 1: Size of the task datasets.
2.1 Source Corpora
Catalan and Spanish The AnCora corpora (Re-
casens and Mart??, 2009) consist of a Catalan and
a Spanish treebank of 500k words each, mainly
from newspapers and news agencies (El Peri?odico,
EFE, ACN). Manual annotation exists for ar-
guments and thematic roles, predicate semantic
classes, NEs, WordNet nominal senses, and coref-
erence relations. AnCora are freely available for
research purposes.
Dutch The KNACK-2002 corpus (Hoste and De
Pauw, 2006) contains 267 documents from the
Flemish weekly magazine Knack. They were
manually annotated with coreference information
on top of semi-automatically annotated PoS tags,
phrase chunks, and NEs.
English The OntoNotes Release 2.0 corpus
(Pradhan et al, 2007) covers newswire and broad-
cast news data: 300k words from The Wall Street
Journal, and 200k words from the TDT-4 col-
lection, respectively. OntoNotes builds on the
Penn Treebank for syntactic annotation and on the
Penn PropBank for predicate argument structures.
Semantic annotations include NEs, words senses
(linked to an ontology), and coreference informa-
tion. The OntoNotes corpus is distributed by the
Linguistic Data Consortium.
2
German The T?uBa-D/Z corpus (Hinrichs et al,
2005) is a newspaper treebank based on data taken
from the daily issues of ?die tageszeitung? (taz). It
currently comprises 794k words manually anno-
tated with semantic and coreference information.
Due to licensing restrictions of the original texts, a
taz-DVD must be purchased to obtain a license.
2
Italian The LiveMemories corpus (Rodr??guez
et al, 2010) will include texts from the Italian
Wikipedia, blogs, news articles, and dialogues
2
Free user license agreements for the English and German
task datasets were issued to the task participants.
(MapTask). They are being annotated according
to the ARRAU annotation scheme with coref-
erence, agreement, and NE information on top
of automatically parsed data. The task dataset
included Wikipedia texts already annotated.
The datasets that were used in the task were ex-
tracted from the above-mentioned corpora. Ta-
ble 1 summarizes the number of documents
(docs), sentences (sents), and tokens in the train-
ing, development and test sets.
3
2.2 Preprocessing Systems
Catalan, Spanish, English Predicted lemmas
and PoS were generated using FreeLing
4
for
Catalan/Spanish and SVMTagger
5
for English.
Dependency information and predicate semantic
roles were generated with JointParser, a syntactic-
semantic parser.
6
Dutch Lemmas, PoS and NEs were automat-
ically provided by the memory-based shallow
parser for Dutch (Daelemans et al, 1999), and de-
pendency information by the Alpino parser (van
Noord et al, 2006).
German Lemmas were predicted by TreeTagger
(Schmid, 1995), PoS and morphology by RFTag-
ger (Schmid and Laws, 2008), and dependency in-
formation by MaltParser (Hall and Nivre, 2008).
Italian Lemmas and PoS were provided by
TextPro,
7
and dependency information by Malt-
Parser.
8
3
The German and Dutch training datasets were not com-
pletely stable during the competition period due to a few er-
rors. Revised versions were released on March 2 and 20, re-
spectively. As to the test datasets, the Dutch and Italian doc-
uments with formatting errors were corrected after the eval-
uation period, with no variations in the ranking order of sys-
tems.
4
http://www.lsi.upc.es/ nlp/freeling
5
http://www.lsi.upc.edu/ nlp/SVMTool
6
http://www.lsi.upc.edu// xlluis/?x=cat:5
7
http://textpro.fbk.eu
8
http://maltparser.org
2
3 Task Description
Participants were asked to develop an automatic
system capable of assigning a discourse entity to
every mention,
9
thus identifying all the NP men-
tions of every discourse entity. As there is no
standard annotation scheme for coreference and
the source corpora differed in certain aspects, the
coreference information of the task datasets was
produced according to three criteria:
? Only NP constituents and possessive deter-
miners can be mentions.
? Mentions must be referential expressions,
thus ruling out nominal predicates, appos-
itives, expletive NPs, attributive NPs, NPs
within idioms, etc.
? Singletons are also considered as entities
(i.e., entities with a single mention).
To help participants build their systems, the
task datasets also contained both gold-standard
and automatically predicted linguistic annotations
at the morphological, syntactic and semantic lev-
els. Considerable effort was devoted to provide
participants with a common and relatively simple
data representation for the six languages.
3.1 Data Format
The task datasets as well as the participants?
answers were displayed in a uniform column-
based format, similar to the style used in previous
CoNLL shared tasks on syntactic and semantic de-
pendencies (2008/2009).
10
Each dataset was pro-
vided as a single file per language. Since corefer-
ence is a linguistic relation at the discourse level,
documents constitute the basic unit, and are de-
limited by ?#begin document ID? and ?#end doc-
ument ID? comment lines. Within a document, the
information of each sentence is organized verti-
cally with one token per line, and a blank line after
the last token of each sentence. The information
associated with each token is described in several
columns (separated by ?\t? characters) represent-
ing the following layers of linguistic annotation.
ID (column 1). Token identifiers in the sentence.
Token (column 2). Word forms.
9
Following the terminology of the ACE program, a men-
tion is defined as an instance of reference to an object, and
an entity is the collection of mentions referring to the same
object in a document.
10
http://www.cnts.ua.ac.be/conll2008
ID Token Intermediate columns Coref
1 Major . . . (1
2 League . . .
3 Baseball . . . 1)
4 sent . . .
5 its . . . (1)|(2
6 head . . .
7 of . . .
8 security . . . (3)|2)
9 to . . .
. . . . . . . . . . . .
27 The . . . (1
28 league . . . 1)
29 is . . .
Table 2: Format of the coreference annotations
(corresponding to example (1) in Section 1).
Lemma (column 3). Token lemmas.
PoS (column 5). Coarse PoS.
Feat (column 7). Morphological features (PoS
type, number, gender, case, tense, aspect,
etc.) separated by a pipe character.
Head (column 9). ID of the syntactic head (?0? if
the token is the tree root).
DepRel (column 11). Dependency relations cor-
responding to the dependencies described in
the Head column (?sentence? if the token is
the tree root).
NE (column 13). NE types in open-close notation.
Pred (column 15). Predicate semantic class.
APreds (column 17 and subsequent ones). For
each predicate in the Pred column, its seman-
tic roles/dependencies.
Coref (last column). Coreference relations in
open-close notation.
The above-mentioned columns are ?gold-
standard columns,? whereas columns 4, 6, 8, 10,
12, 14, 16 and the penultimate contain the same
information as the respective previous column but
automatically predicted?using the preprocessing
systems listed in Section 2.2. Neither all layers
of linguistic annotation nor all gold-standard and
predicted columns were available for all six lan-
guages (underscore characters indicate missing in-
formation).
The coreference column follows an open-close
notation with an entity number in parentheses (see
Table 2). Every entity has an ID number, and ev-
ery mention is marked with the ID of the entity
it refers to: an opening parenthesis shows the be-
ginning of the mention (first token), while a clos-
ing parenthesis shows the end of the mention (last
3
token). For tokens belonging to more than one
mention, a pipe character is used to separate mul-
tiple entity IDs. The resulting annotation is a well-
formed nested structure (CF language).
3.2 Evaluation Settings
In order to address our goal of studying the effect
of different levels of linguistic information (pre-
processing) on solving coreference relations, the
test was divided into four evaluation settings that
differed along two dimensions.
Gold-standard versus Regular setting. Only
in the gold-standard setting were participants al-
lowed to use the gold-standard columns, includ-
ing the last one (of the test dataset) with true
mention boundaries. In the regular setting, they
were allowed to use only the automatically pre-
dicted columns. Obtaining better results in the
gold setting would provide evidence for the rel-
evance of using high-quality preprocessing infor-
mation. Since not all columns were available for
all six languages, the gold setting was only possi-
ble for Catalan, English, German, and Spanish.
Closed versus Open setting. In the closed set-
ting, systems had to be built strictly with the in-
formation provided in the task datasets. In con-
trast, there was no restriction on the resources that
participants could utilize in the open setting: sys-
tems could be developed using any external tools
and resources to predict the preprocessing infor-
mation, e.g., WordNet, Wikipedia, etc. The only
requirement was to use tools that had not been de-
veloped with the annotations of the test set. This
setting provided an open door into tools or re-
sources that improve performance.
3.3 Evaluation Metrics
Since there is no agreement at present on a stan-
dard measure for coreference resolution evalua-
tion, one of our goals was to compare the rank-
ings produced by four different measures. The
task scorer provides results in the two mention-
based metrics B
3
(Bagga and Baldwin, 1998) and
CEAF-?
3
(Luo, 2005), and the two link-based
metrics MUC (Vilain et al, 1995) and BLANC
(Recasens and Hovy, in prep). The first three mea-
sures have been widely used, while BLANC is a
proposal of a new measure interesting to test.
The mention detection subtask is measured with
recall, precision, and F
1
. Mentions are rewarded
with 1 point if their boundaries coincide with those
of the gold NP, with 0.5 points if their boundaries
are within the gold NP including its head, and
with 0 otherwise.
4 Participating Systems
A total of twenty-two participants registered for
the task and downloaded the training materials.
From these, sixteen downloaded the test set but
only six (out of which two task organizers) sub-
mitted valid results (corresponding to nine system
runs or variants). These numbers show that the
task raised considerable interest but that the final
participation rate was comparatively low (slightly
below 30%).
The participating systems differed in terms of
architecture, machine learning method, etc. Ta-
ble 3 summarizes their main properties. Systems
like BART and Corry support several machine
learners, but Table 3 indicates the one used for the
SemEval run. The last column indicates the exter-
nal resources that were employed in the open set-
ting, thus it is empty for systems that participated
only in the closed setting. For more specific details
we address the reader to the system description pa-
pers in Erk and Strapparava (2010).
5 Results and Evaluation
Table 4 shows the results obtained by two naive
baseline systems: (i) SINGLETONS considers each
mention as a separate entity, and (ii) ALL-IN-ONE
groups all the mentions in a document into a sin-
gle entity. These simple baselines reveal limita-
tions of the evaluation metrics, like the high scores
of CEAF and B
3
for SINGLETONS. Interestingly
enough, the naive baseline scores turn out to be
hard to beat by the participating systems, as Ta-
ble 5 shows. Similarly, ALL-IN-ONE obtains high
scores in terms of MUC. Table 4 also reveals dif-
ferences between the distribution of entities in the
datasets. Dutch is clearly the most divergent cor-
pus mainly due to the fact that it only contains sin-
gletons for NEs.
Table 5 displays the results of all systems for all
languages and settings in the four evaluation met-
rics (the best scores in each setting are highlighted
in bold). Results are presented sequentially by lan-
guage and setting, and participating systems are
ordered alphabetically. The participation of sys-
tems across languages and settings is rather irreg-
ular,
11
thus making it difficult to draw firm conclu-
11
Only 45 entries in Table 5 from 192 potential cases.
4
System Architecture ML Methods External Resources
BART
(Broscheit et al, 2010) Closest-first with entity-
mention model (English),
Closest-first model (German,
Italian)
MaxEnt (English, Ger-
man), Decision trees
(Italian)
GermaNet & gazetteers (Ger-
man), I-Cab gazetteers (Italian),
Berkeley parser, Stanford NER,
WordNet, Wikipedia name list,
U.S. census data (English)
Corry
(Uryupina, 2010) ILP, Pairwise model SVM Stanford parser & NER, Word-
Net, U.S. census data
RelaxCor
(Sapena et al, 2010) Graph partitioning (solved by
relaxation labeling)
Decision trees, Rules WordNet
SUCRE
(Kobdani and Sch?utze, 2010) Best-first clustering, Rela-
tional database model, Regular
feature definition language
Decision trees, Naive
Bayes, SVM, MaxEnt
?
TANL-1
(Attardi et al, 2010) Highest entity-mention simi-
larity
MaxEnt PoS tagger (Italian)
UBIU
(Zhekova and K?ubler, 2010) Pairwise model MBL ?
Table 3: Main characteristics of the participating systems.
sions about the aims initially pursued by the task.
In the following, we summarize the most relevant
outcomes of the evaluation.
Regarding languages, English concentrates the
most participants (fifteen entries), followed by
German (eight), Catalan and Spanish (seven each),
Italian (five), and Dutch (three). The number of
languages addressed by each system ranges from
one (Corry) to six (UBIU and SUCRE); BART and
RelaxCor addressed three languages, and TANL-1
five. The best overall results are obtained for En-
glish followed by German, then Catalan, Spanish
and Italian, and finally Dutch. Apart from differ-
ences between corpora, there are other factors that
might explain this ranking: (i) the fact that most of
the systems were originally developed for English,
and (ii) differences in corpus size (German having
the largest corpus, and Dutch the smallest).
Regarding systems, there are no clear ?win-
ners.? Note that no language-setting was ad-
dressed by all six systems. The BART system,
for instance, is either on its own or competing
against a single system. It emerges from par-
tial comparisons that SUCRE performs the best in
closed?regular for English, German, and Italian,
although it never outperforms the CEAF or B
3
sin-
gleton baseline. While SUCRE always obtains the
best scores according to MUC and BLANC, Re-
laxCor and TANL-1 usually win based on CEAF
and B
3
. The Corry system presents three variants
optimized for CEAF (Corry-C), MUC (Corry-M),
and BLANC (Corry-B). Their results are consis-
tent with the bias introduced in the optimization
(see English:open?gold).
Depending on the evaluation metric then, the
rankings of systems vary with considerable score
differences. There is a significant positive corre-
lation between CEAF and B
3
(Pearson?s r = 0.91,
p< 0.01), and a significant lack of correlation be-
tween CEAF and MUC in terms of recall (Pear-
son?s r = 0.44, p< 0.01). This fact stresses the
importance of defining appropriate metrics (or a
combination of them) for coreference evaluation.
Finally, regarding evaluation settings, the re-
sults in the gold setting are significantly better than
those in the regular. However, this might be a di-
rect effect of the mention recognition task. Men-
tion recognition in the regular setting falls more
than 20 F
1
points with respect to the gold setting
(where correct mention boundaries were given).
As for the open versus closed setting, there is only
one system, RelaxCor for English, that addressed
the two. As expected, results show a slight im-
provement from closed?gold to open?gold.
6 Conclusions
This paper has introduced the main features of
the SemEval-2010 task on coreference resolution.
5
CEAF MUC B
3
BLANC
R P F
1
R P F
1
R P F
1
R P Blanc
SINGLETONS: Each mention forms a separate entity.
Catalan 61.2 61.2 61.2 0.0 0.0 0.0 61.2 100 75.9 50.0 48.7 49.3
Dutch 34.5 34.5 34.5 0.0 0.0 0.0 34.5 100 51.3 50.0 46.7 48.3
English 71.2 71.2 71.2 0.0 0.0 0.0 71.2 100 83.2 50.0 49.2 49.6
German 75.5 75.5 75.5 0.0 0.0 0.0 75.5 100 86.0 50.0 49.4 49.7
Italian 71.1 71.1 71.1 0.0 0.0 0.0 71.1 100 83.1 50.0 49.2 49.6
Spanish 62.2 62.2 62.2 0.0 0.0 0.0 62.2 100 76.7 50.0 48.8 49.4
ALL-IN-ONE: All mentions are grouped into a single entity.
Catalan 11.8 11.8 11.8 100 39.3 56.4 100 4.0 7.7 50.0 1.3 2.6
Dutch 19.7 19.7 19.7 100 66.3 79.8 100 8.0 14.9 50.0 3.2 6.2
English 10.5 10.5 10.5 100 29.2 45.2 100 3.5 6.7 50.0 0.8 1.6
German 8.2 8.2 8.2 100 24.8 39.7 100 2.4 4.7 50.0 0.6 1.1
Italian 11.4 11.4 11.4 100 29.0 45.0 100 2.1 4.1 50.0 0.8 1.5
Spanish 11.9 11.9 11.9 100 38.3 55.4 100 3.9 7.6 50.0 1.2 2.4
Table 4: Baseline scores.
The goal of the task was to evaluate and compare
automatic coreference resolution systems for six
different languages in four evaluation settings and
using four different metrics. This complex sce-
nario aimed at providing insight into several as-
pects of coreference resolution, including portabil-
ity across languages, relevance of linguistic infor-
mation at different levels, and behavior of alterna-
tive scoring metrics.
The task attracted considerable attention from a
number of researchers, but only six teams submit-
ted their final results. Participating systems did not
run their systems for all the languages and evalu-
ation settings, thus making direct comparisons be-
tween them very difficult. Nonetheless, we were
able to observe some interesting aspects from the
empirical evaluation.
An important conclusion was the confirmation
that different evaluation metrics provide different
system rankings and the scores are not commen-
surate. Attention thus needs to be paid to corefer-
ence evaluation. The behavior and applicability of
the scoring metrics requires further investigation
in order to guarantee a fair evaluation when com-
paring systems in the future. We hope to have the
opportunity to thoroughly discuss this and the rest
of interesting questions raised by the task during
the SemEval workshop at ACL 2010.
An additional valuable benefit is the set of re-
sources developed throughout the task. As task
organizers, we intend to facilitate the sharing of
datasets, scorers, and documentation by keeping
them available for future research use. We believe
that these resources will help to set future bench-
marks for the research community and will con-
tribute positively to the progress of the state of the
art in coreference resolution. We will maintain and
update the task website with post-SemEval contri-
butions.
Acknowledgments
We would like to thank the following peo-
ple who contributed to the preparation of the
task datasets: Manuel Bertran (UB), Oriol
Borrega (UB), Orph?ee De Clercq (U. Ghent),
Francesca Delogu (U. Trento), Jes?us Gim?enez
(UPC), Eduard Hovy (ISI-USC), Richard Johans-
son (U. Trento), Xavier Llu??s (UPC), Montse
Nofre (UB), Llu??s Padr?o (UPC), Kepa Joseba
Rodr??guez (U. Trento), Mihai Surdeanu (Stan-
ford), Olga Uryupina (U. Trento), Lente Van Leu-
ven (UB), and Rita Zaragoza (UB). We would also
like to thank LDC and die tageszeitung for dis-
tributing freely the English and German datasets.
This work was funded in part by the Span-
ish Ministry of Science and Innovation through
the projects TEXT-MESS 2.0 (TIN2009-13391-
C04-04), OpenMT-2 (TIN2009-14675-C03), and
KNOW2 (TIN2009-14715-C04-04), and an FPU
doctoral scholarship (AP2006-00994) held by
M. Recasens. It also received financial sup-
port from the Seventh Framework Programme
of the EU (FP7/2007-2013) under GA 247762
(FAUST), from the STEVIN program of the Ned-
erlandse Taalunie through the COREA and SoNaR
projects, and from the Provincia Autonoma di
Trento through the LiveMemories project.
6
Mention detection CEAF MUC B
3
BLANC
R P F
1
R P F
1
R P F
1
R P F
1
R P Blanc
Catalan
closed?gold
RelaxCor 100 100 100 70.5 70.5 70.5 29.3 77.3 42.5 68.6 95.8 79.9 56.0 81.8 59.7
SUCRE 100 100 100 68.7 68.7 68.7 54.1 58.4 56.2 76.6 77.4 77.0 72.4 60.2 63.6
TANL-1 100 96.8 98.4 66.0 63.9 64.9 17.2 57.7 26.5 64.4 93.3 76.2 52.8 79.8 54.4
UBIU 75.1 96.3 84.4 46.6 59.6 52.3 8.8 17.1 11.7 47.8 76.3 58.8 51.6 57.9 52.2
closed?regular
SUCRE 75.9 64.5 69.7 51.3 43.6 47.2 44.1 32.3 37.3 59.6 44.7 51.1 53.9 55.2 54.2
TANL-1 83.3 82.0 82.7 57.5 56.6 57.1 15.2 46.9 22.9 55.8 76.6 64.6 51.3 76.2 51.0
UBIU 51.4 70.9 59.6 33.2 45.7 38.4 6.5 12.6 8.6 32.4 55.7 40.9 50.2 53.7 47.8
open?gold
open?regular
Dutch
closed?gold
SUCRE 100 100 100 58.8 58.8 58.8 65.7 74.4 69.8 65.0 69.2 67.0 69.5 62.9 65.3
closed?regular
SUCRE 78.0 29.0 42.3 29.4 10.9 15.9 62.0 19.5 29.7 59.1 6.5 11.7 46.9 46.9 46.9
UBIU 41.5 29.9 34.7 20.5 14.6 17.0 6.7 11.0 8.3 13.3 23.4 17.0 50.0 52.4 32.3
open?gold
open?regular
English
closed?gold
RelaxCor 100 100 100 75.6 75.6 75.6 21.9 72.4 33.7 74.8 97.0 84.5 57.0 83.4 61.3
SUCRE 100 100 100 74.3 74.3 74.3 68.1 54.9 60.8 86.7 78.5 82.4 77.3 67.0 70.8
TANL-1 99.8 81.7 89.8 75.0 61.4 67.6 23.7 24.4 24.0 74.6 72.1 73.4 51.8 68.8 52.1
UBIU 92.5 99.5 95.9 63.4 68.2 65.7 17.2 25.5 20.5 67.8 83.5 74.8 52.6 60.8 54.0
closed?regular
SUCRE 78.4 83.0 80.7 61.0 64.5 62.7 57.7 48.1 52.5 68.3 65.9 67.1 58.9 65.7 61.2
TANL-1 79.6 68.9 73.9 61.7 53.4 57.3 23.8 25.5 24.6 62.1 60.5 61.3 50.9 68.0 49.3
UBIU 66.7 83.6 74.2 48.2 60.4 53.6 11.6 18.4 14.2 50.9 69.2 58.7 50.9 56.3 51.0
open?gold
Corry-B 100 100 100 77.5 77.5 77.5 56.1 57.5 56.8 82.6 85.7 84.1 69.3 75.3 71.8
Corry-C 100 100 100 77.7 77.7 77.7 57.4 58.3 57.9 83.1 84.7 83.9 71.3 71.6 71.5
Corry-M 100 100 100 73.8 73.8 73.8 62.5 56.2 59.2 85.5 78.6 81.9 76.2 58.8 62.7
RelaxCor 100 100 100 75.8 75.8 75.8 22.6 70.5 34.2 75.2 96.7 84.6 58.0 83.8 62.7
open?regular
BART 76.1 69.8 72.8 70.1 64.3 67.1 62.8 52.4 57.1 74.9 67.7 71.1 55.3 73.2 57.7
Corry-B 79.8 76.4 78.1 70.4 67.4 68.9 55.0 54.2 54.6 73.7 74.1 73.9 57.1 75.7 60.6
Corry-C 79.8 76.4 78.1 70.9 67.9 69.4 54.7 55.5 55.1 73.8 73.1 73.5 57.4 63.8 59.4
Corry-M 79.8 76.4 78.1 66.3 63.5 64.8 61.5 53.4 57.2 76.8 66.5 71.3 58.5 56.2 57.1
German
closed?gold
SUCRE 100 100 100 72.9 72.9 72.9 74.4 48.1 58.4 90.4 73.6 81.1 78.2 61.8 66.4
TANL-1 100 100 100 77.7 77.7 77.7 16.4 60.6 25.9 77.2 96.7 85.9 54.4 75.1 57.4
UBIU 92.6 95.5 94.0 67.4 68.9 68.2 22.1 21.7 21.9 73.7 77.9 75.7 60.0 77.2 64.5
closed?regular
SUCRE 79.3 77.5 78.4 60.6 59.2 59.9 49.3 35.0 40.9 69.1 60.1 64.3 52.7 59.3 53.6
TANL-1 60.9 57.7 59.2 50.9 48.2 49.5 10.2 31.5 15.4 47.2 54.9 50.7 50.2 63.0 44.7
UBIU 50.6 66.8 57.6 39.4 51.9 44.8 9.5 11.4 10.4 41.2 53.7 46.6 50.2 54.4 48.0
open?gold
BART 94.3 93.7 94.0 67.1 66.7 66.9 70.5 40.1 51.1 85.3 64.4 73.4 65.5 61.0 62.8
open?regular
BART 82.5 82.3 82.4 61.4 61.2 61.3 61.4 36.1 45.5 75.3 58.3 65.7 55.9 60.3 57.3
Italian
closed?gold
SUCRE 98.4 98.4 98.4 66.0 66.0 66.0 48.1 42.3 45.0 76.7 76.9 76.8 54.8 63.5 56.9
closed?regular
SUCRE 84.6 98.1 90.8 57.1 66.2 61.3 50.1 50.7 50.4 63.6 79.2 70.6 55.2 68.3 57.7
UBIU 46.8 35.9 40.6 37.9 29.0 32.9 2.9 4.6 3.6 38.4 31.9 34.8 50.0 46.6 37.2
open?gold
open?regular
BART 42.8 80.7 55.9 35.0 66.1 45.8 35.3 54.0 42.7 34.6 70.6 46.4 57.1 68.1 59.6
TANL-1 90.5 73.8 81.3 62.2 50.7 55.9 37.2 28.3 32.1 66.8 56.5 61.2 50.7 69.3 48.5
Spanish
closed?gold
RelaxCor 100 100 100 66.6 66.6 66.6 14.8 73.8 24.7 65.3 97.5 78.2 53.4 81.8 55.6
SUCRE 100 100 100 69.8 69.8 69.8 52.7 58.3 55.3 75.8 79.0 77.4 67.3 62.5 64.5
TANL-1 100 96.8 98.4 66.9 64.7 65.8 16.6 56.5 25.7 65.2 93.4 76.8 52.5 79.0 54.1
UBIU 73.8 96.4 83.6 45.7 59.6 51.7 9.6 18.8 12.7 46.8 77.1 58.3 52.9 63.9 54.3
closed?regular
SUCRE 74.9 66.3 70.3 56.3 49.9 52.9 35.8 36.8 36.3 56.6 54.6 55.6 52.1 61.2 51.4
TANL-1 82.2 84.1 83.1 58.6 60.0 59.3 14.0 48.4 21.7 56.6 79.0 66.0 51.4 74.7 51.4
UBIU 51.1 72.7 60.0 33.6 47.6 39.4 7.6 14.4 10.0 32.8 57.1 41.6 50.4 54.6 48.4
open?gold
open?regular
Table 5: Official results of the participating systems for all languages, settings, and metrics.
7
References
Giuseppe Attardi, Stefano Dei Rossi, and Maria Simi.
2010. TANL-1: coreference resolution by parse
analysis and similarity clustering. In Proceedings
of SemEval-2.
Amit Bagga and Breck Baldwin. 1998. Algorithms for
scoring coreference chains. In Proceedings of the
LREC Workshop on Linguistic Coreference, pages
563?566.
Samuel Broscheit, Massimo Poesio, Simone Paolo
Ponzetto, Kepa Joseba Rodr??guez, Lorenza Ro-
mano, Olga Uryupina, Yannick Versley, and Roberto
Zanoli. 2010. BART: A multilingual anaphora res-
olution system. In Proceedings of SemEval-2.
Walter Daelemans, Sabine Buchholz, and Jorn Veen-
stra. 1999. Memory-based shallow parsing. In Pro-
ceedings of CoNLL 1999.
George Doddington, Alexis Mitchell, Mark Przybocki,
Lance Ramshaw, Stephanie Strassel, and Ralph
Weischedel. 2004. The Automatic Content Extrac-
tion (ACE) program ? Tasks, data, and evaluation.
In Proceedings of LREC 2004, pages 837?840.
Katrin Erk and Carlo Strapparava, editors. 2010. Pro-
ceedings of SemEval-2.
Johan Hall and Joakim Nivre. 2008. A dependency-
driven parser for German dependency and con-
stituency representations. In Proceedings of the ACL
Workshop on Parsing German (PaGe 2008), pages
47?54.
Erhard W. Hinrichs, Sandra K?ubler, and Karin Nau-
mann. 2005. A unified representation for morpho-
logical, syntactic, semantic, and referential annota-
tions. In Proceedings of the ACL Workshop on Fron-
tiers in Corpus Annotation II: Pie in the Sky, pages
13?20.
Lynette Hirschman and Nancy Chinchor. 1997.
MUC-7 Coreference Task Definition ? Version 3.0.
In Proceedings of MUC-7.
V?eronique Hoste and Guy De Pauw. 2006. KNACK-
2002: A richly annotated corpus of Dutch written
text. In Proceedings of LREC 2006, pages 1432?
1437.
Hamidreza Kobdani and Hinrich Sch?utze. 2010. SU-
CRE: A modular system for coreference resolution.
In Proceedings of SemEval-2.
Xiaoqiang Luo. 2005. On coreference resolution
performance metrics. In Proceedings of HLT-
EMNLP 2005, pages 25?32.
Joseph F. McCarthy and Wendy G. Lehnert. 1995. Us-
ing decision trees for coreference resolution. In Pro-
ceedings of IJCAI 1995, pages 1050?1055.
Thomas S. Morton. 1999. Using coreference in ques-
tion answering. In Proceedings of TREC-8, pages
85?89.
Constantin Orasan, Dan Cristea, Ruslan Mitkov, and
Ant?onio Branco. 2008. Anaphora Resolution Exer-
cise: An overview. In Proceedings of LREC 2008.
Sameer S. Pradhan, Eduard Hovy, Mitch Mar-
cus, Martha Palmer, Lance Ramshaw, and Ralph
Weischedel. 2007. Ontonotes: A unified rela-
tional semantic representation. In Proceedings of
the International Conference on Semantic Comput-
ing (ICSC 2007), pages 517?526.
Marta Recasens and Eduard Hovy. in prep. BLANC:
Implementing the Rand Index for Coreference Eval-
uation.
Marta Recasens and M. Ant`onia Mart??. 2009. AnCora-
CO: Coreferentially annotated corpora for Spanish
and Catalan. Language Resources and Evaluation,
DOI:10.1007/s10579-009-9108-x.
Kepa Joseba Rodr??guez, Francesca Delogu, Yannick
Versley, Egon Stemle, and Massimo Poesio. 2010.
Anaphoric annotation of Wikipedia and blogs in
the Live Memories Corpus. In Proceedings of
LREC 2010, pages 157?163.
Emili Sapena, Llu??s Padr?o, and Jordi Turmo. 2010.
RelaxCor: A global relaxation labeling approach to
coreference resolution for the SemEval-2 Corefer-
ence Task. In Proceedings of SemEval-2.
Helmut Schmid and Florian Laws. 2008. Estimation
of conditional probabilities with decision trees and
an application to fine-grained POS tagging. In Pro-
ceedings of COLING 2008, pages 777?784.
Helmut Schmid. 1995. Improvements in part-of-
speech tagging with an application to German. In
Proceedings of the ACL SIGDAT Workshop, pages
47?50.
Josef Steinberger, Massimo Poesio, Mijail A. Kabad-
jov, and Karel Jeek. 2007. Two uses of anaphora
resolution in summarization. Information Process-
ing and Management: an International Journal,
43(6):1663?1680.
Olga Uryupina. 2010. Corry: A system for corefer-
ence resolution. In Proceedings of SemEval-2.
Gertjan van Noord, Ineke Schuurman, and Vincent
Vandeghinste. 2006. Syntactic annotation of large
corpora in STEVIN. In Proceedings of LREC 2006.
Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A model-
theoretic coreference scoring scheme. In Proceed-
ings of MUC-6, pages 45?52.
Desislava Zhekova and Sandra K?ubler. 2010. UBIU:
A language-independent system for coreference res-
olution. In Proceedings of SemEval-2.
8
