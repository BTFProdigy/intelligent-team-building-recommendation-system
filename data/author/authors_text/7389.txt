On UNL as the future "html of the linguistic content" & the reuse of 
existing NLP components in UNL-related applications with the 
example of a UNL-French deconverter 
Gilles St~RASSET 
GETA, CLIPS, IMAG 
385, av. de la biblioth~que, BP 53 
F-38041 Grenoble cedex 9, France 
Gilles.Serasset@ imag,fr 
Christian BOITET 
GETA, CLIPS, IMAG 
385, av. de la biblioth~que, BP 53 
F-38041 Grenoble cedex 9, France 
Christian.Boitet @imag.fr 
Abstract 
After 3 years of specifying the UNL (Universal Networking Language) language and 
prototyping deconverters I from more than 12 languages and enconverters for about 4, the 
UNL project has opened to the community by publishing the specifcations (v2.0) of the UNL 
language, intended to encode the meaning of NL utterances as semantic hypergraphs and to be 
used as a "pivot" representation in multilingual information and communication systems. 
A UNL document is an html document with special tags to delimit the utterances and their 
rendering in UNL and in all natural languages currently handled. UNL can be viewed as the 
future "html of the linguistic content". It is only an interface format, leading as well to the reuse 
of existing NLP components as to the development of original tools in a variety of possible 
applications, from automatic rough enconversion for information retrieval and information 
gathering translation to partially interactive nconversion or deconversion for higher quality. 
We illustrate these points by describing an UNL-French deconverter organized as a specific 
"localizer" followed by a classical MT transfer and an existing generator. 
Keywords 
UNL, interlingua, pivot, deconversion, UNL~French localization, transfer, generation. 
Introduction 
The UNL project of network-oriented 
multilinguat communication has proposed a 
standard for encoding the meaning of natural 
language utterances as semantic hypergraphs 
intended to be used as pivots in multilingual 
information and communication systems. In the 
first phase (1997-1999), more than 16 partners 
representing 14 languages have worked to build 
deconverters transforming an (interlingual) 
UNL hypergraph into a natural language 
utterance. 
In this project, the strategy used to achieve this 
initial objective is free. The UNL-French 
deconverter under development first performs a 
"localization" operation within the UNL format, 
and then classical transfer and generation steps, 
using the Ariane-G5 environment and some 
UNL-specifc tools. 
The use of classical transfer and generation 
steps in the context of an interlingual project 
may sound surprising. But it reflects many 
interesting issues about the status of the UNL 
language, designed as an interlingua, but 
diversely used as a linguistic pivot (disambi- 
guated abstract English), or as a purely semantic 
pivot. 
After introducing the UNL language, we present 
the architecture of the UNL-French deconverter, 
which "generates" from the UNL interlingua by 
first "localizing" the UNL form for French, 
within UNL, and then applying slightly adapted 
but classical transfer and generation techniques, 
implemented in the Ariane-G5 environlnent, 
supplemented by some UNL-specific tools. 
Then, we discuss the use of the UNL language 
as a linguistic or semantic pivot for highly 
multilingual information systems. 
1 The UNL project and language 
1.1 The project 
UNL is a project of multilingual personal 
networking communication initiated by the 
University of United Nations based in Tokyo. 
The pivot paradigm is used: the representation 
I The terms << deconvcrsion, and <~ enconvcrsion, are specific to tile UNL proiect and are defined at paragraph 2.
768 
of an utterance in the UNL interlingua (UNL 
stands for "Universal Networking Language") is 
a hyl)ergraph where normal nodes bear UWs 
CUniversal Words", or interlingual acceptions) 
with semantic attributes, and arcs bear semantic 
relations (deep cases, such as agt, obj, goal, etc.). 
Hypernodes group a subgraph defined by a set 
of connected arcs. A UW denotes a set of 
interlingual acceptions (word senses), although 
we often loosely speak of "the" word sense 
demoted by a UW. 
Because English is known by all UNL 
developers, the syntax of a tlormal WW is: 
"<Engl ish word or compound> ( <list  
of res t r i c t ions> ) ", O. Z. "look for 
( icl>action, agt>human, obj>thing)" 
Going fronl a text to the corresponding "UNL 
text" or interactively constructing a UNL text is 
called "enconversioif', while producing a text 
fiom a sequence of UNL graphs is called 
"deconversion". 
This departure fi'om the standard terms of 
analysis and generation is used to stress that this 
is not a classical M\]: projecl, bu! that UNL is 
planned to be the source format preferred for 
representing textual inl:ormation in tile 
envisaged multilingual network environment. 
Tile schedule of tile project, beginning with 
deconversion rather than cnconvcrsion, also 
reflects that difference. 
14 hmguages have been tackled during the first 
3--year phase of the prqject (1997-1999), while 
many more arc to be added in tile second 
phase. Each group is fi-ee to reuse its own 
software lools and/or lingware resources, or to 
develop directly with tools provided by tile 
UNL Center (UNU/IAS). 
Emphasis is on a very large lexical coverage, so 
that all groups spend most of their time on tile 
UNL-NL lexicons, and develop tools and 
methods for efficient lexical development. By 
contrast, gramnmrs have been initially limited to 
those necessary for deconversion, and will then 
bc gradually expanded to allow for more 
naturalness m formulating text to be 
enconverted. 
1.2 The UNL components 
1.2.1 Universal Words 
Tile nodes of a UNL utterance are called 
Universal Words (or Uws). The syntax of a 
normal UW consists of 2 parts : 
a headword, 
a list of  restrictions 
Because English is known by all UNL 
developers, tile headword is an English word or 
compound. The restrictions are given as all 
attribute value pail" where attributes are semantic 
relation labels (as the ones used in the graphs) 
and wllues are other UWs (restricted or not). 
A UW denotes a collection of interlingual 
acceptions (word senses), although we often 
loosely speak of "the" word sense denoted by an 
UW. For example, the unrestricted UW " look  
for" denotes all the word-senses associated to 
tile English compound word "look for". Tile 
restricted UW " look for ( ic l>action, 
agt>human, obj>thing) " represents all tile 
word senses of the English word "look fo r "  
that are an action, perl%rmed by a human that 
affects a thing. In this case this leads to the word 
sense: "look fo r -  to try to find". 
1.2.2 UNL hypergraph 
A UNL expression is a hypergraph (a graph 
where a node is simple or recursively contains a 
hypergraph). Tile arcs bear semantic relation 
labels (deep cases, such as agt, obj, goal, etc.). 
score(icl>event agt>human,tld>sport) \[ I @ entry. @ past. @ complete \[ 
7 i \ 
agt ....... ' /  ins \~  
i Rona~do 1 ?b~ / \ pit head(p~ol>body) ~ "\\ 
/ ,~  corner / 
goal i~cl>thing) \[.41 obj mod \[ 
~left : 
Figm'e I. 1: A UNL graph deconvertible as "Ronaldo 
has headed the ball into the left corner of the net" 
In a UNL graph, UWs appear with attributes 
describing what is said from tile speaker's point 
of view. This includes phenomena like speech 
acts, truth wllues, time, etc. 
Hypernodes may also be used ill UNL 
expressions. 
agt ...... i 
I driver.~Pl \] 
aoj 
I reckless \] 
01.@entry 
\[ drink\] 
\ 
drive \] 
Figure 1.2: A UNL Io,pergraph that may be 
deconverted as "Reckless drivers drink and drive" 
Graphs and subgraphs nmst contain one special 
node, called the entry of tile graph. 
1.2.3 Denoting a UNL  graph 
These hypergraphs are denoted using the UNL 
language per se. In the UNL hmguagc, an 
769 
expression consists in a set of arcs, connecting 
the different nodes. As an example, the graph 
presented in figure 1.1 will be denoted as: 
agt(score(...).@entry.@past.@complete, 
Ronaldo) 
obj(score(_.).@entry.@past.@complete, 
goal(icl>thing)) 
ins(score(...) .@entry.@past.@complete, 
head(pof>body)) 
plt(score(...) .@entry.@past.@complete, 
corner) 
obj (corner, goal(icl>thing)) 
mod(corner, left) 
Hypernodes are denoted by numbers. The 
graph contained by a hypernode is denoted as a 
set of arcs colored by this number as in: 
agt (:Ol.@entry, driver. @pl) 
aoj (reckless, driver.@pl) 
and:Ol (drive, drink.@entry) 
Entries of the graph and subgraphs are denoted 
with the ".@entry" attribute. 
2 Inside the French deconverter  
2.1 Overv iew 
Deconversion is the process of transforming a 
UNL graph into one (or possibly several) 
utterance in a natural language. Any means 
may be used to achieve this task. Many UNL 
project partners use a specialized tool called 
DeCo but, like several other partners, we choose 
to use our own tools for this purpose. 
One reason is that DeCo realizes the 
deconversion in one step, as in some transfer- 
based MT systems such as METAL \[17\]. We 
prefer to use a more modular architecture and 
to split deconversion into 2 steps, transfer and 
generation, each divided into several phases, 
most of them written in Arlene-G5. 
Another reason for not using DeCo is that it is 
not well suited for the morphological gene- 
ration of inflected languages (several thousands 
rules are needed for Italian, tens of thousands 
for Russian, but only about 20 rules and 350 
affixes suffice to build an exhaustive GM for 
French in Sygmor). Last, but not least, this 
choice allows us to reuse modules already 
developed for French generation. 
This strategy is illustrated by figure 2.1. 
/~;~,; o_;.,,.~,', Transfer 
~ . . . . . .  v" Ge I~ati0n her 
/ \,4v 
French utterance 
Fig. 2.1:2 possible deconversqon strategies 
Using this approach, we segment the decon- 
version process into 7 phases, as illustrated by 
figure 2.2. 
The third phase (graph-to-tree) produces a 
decorated tree which is fed into an Ariane-G5 
TS (structural transfer). 
Valklatiolff l,exicaI l'lansl~.'r (h~,li~h 1otree 
\[ .ocalization COllversion 
,Z~ "UNL Tree" 
l'araphra~c choice 
UMA structure?N\ 
Syntactic ~gcnerali(ln 
,t 
UMC structure~ 
Morl~lml.gic\[ll generation 
't 
French utterance 
Fig. 2.2: architecture of the French deconverter 
2.2  Trans fer  
2.2.1 Validation 
When we receive a UNL Graph for decon- 
version, we first check it for correctness. A UNL 
graph has to be connected, and the different 
features handled by the nodes have to be 
defined in UNL. 
If the graph proves incorrect, an explicit error 
message is sent back. This validation has to be 
performed to ilaprove robustness of the 
deconverter, as there is no hypothesis on the 
way a graph is created. When a graph proves 
valid, it is accepted for deconversion. 
2.2.2 Loeal&ation 
In order to be correctly deconverted, tile graph 
has to be slightly modified. 
2.2.2.1 Lexical localization 
Some lexical units used in the graph may not be 
present in the French deconversion dictionary. 
This problem may appear under different 
circumstances. First, the French dictionary 
(which is still under development) may be 
incomplete. Second, the UW nmy use an 
unknown notation to represent a known French 
word sense, and third, the LAV may represent a 
non-French word sense. 
We solve these problems with the same method :
Let w be a UWin  the graph G. Let D be the 
French dictionary (a set of UWs). We substitute 
w in G by w' such that: w' e D and 
VxeD d(w, w', G) = d(w, x, G). where d is a 
pseudo-distance function. 
770 
If different French UWs are at the same pseudo- 
distance of w, w' is chosen at random among 
these UWs (default in non-interactive mode). 
2.2.2.2 "Cultural" localization 
Some crucial information may be missing, 
depending on the language of the source 
utterance (sex, modality, number, determination, 
politeness, kinship...). 
It is in general impossible to solve this problem 
fully automatically in a perfect manner, as we 
do not know anything about the document, its 
c:ontext, and its intended usage: FAHQDC 2 is no 
more possible than FAHQMT on arbitrary texts. 
We have to rely on necessarily imperfect 
heuristics. 
ttowever, we can specialize tile general French 
deconverter to produce specialized servers for 
different tasks and different (target) 
sublanguages. It is possible to assign priorities 
not only to various parts of the dictionaries 
(e.g., specialized vs. general), but also to 
equivalents of the same UW within a given 
dictionary. We can then define several user 
profiles. It is also possible to build a memory of 
deconverted and possibly postedited utterances 
for each specialized French deconversion 
server. 
2.2.3 Lexical Transfer 
After the localization phase, we have to perform 
the lexical transfer. It would seem natural to do 
ill within Ariane-G5, after converting the graph 
into a tree. But lexical transfer is context- 
sensitive, and we want to avoid the possibility of 
transferring differently two tree nodes 
corresponding to one and the same graph node. 
Each graph node is replaced by a French lcxical 
unit (LU), along with some variables. A lexical 
unit used in tile French dictionary denotes a 
derivational family (e.g. in English: destroy 
denotes destroy, destruction, destructible, 
destructive .... in French: d6truire for d6truire, 
destruction, destructible, indestructible, 
destructif, destructeur). 
There may be several possible lexical units for 
one UW. This happens when there is a real 
synonymy or when different erms are used in 
different domains to denote the same word 
sense  3. In  that case, we currently choose tile 
lexical unit at random as we do not have any 
information on tile task the deconverter is used 
for. 
Tile same problem also appears because of tile 
slrategy used to build the French dictionary. In 
order to obtain a good coverage from the 
beginning, we have underspecified tile UWs and 
linked them to dift'ercnt lexical units. This way, 
we considered a UW as tile denotation of a set 
of word senses in French. 
Hence, we were able to reuse previous 
dictionaries and we can use the dictionary even 
if it is still under development and incolnplete. 
In our first version, we also solve this problem 
by a random selection of a lexical unit. 
2.2.4 Graph to tree conversion 
The subsequent deconversion phases are 
performed in Ariane-G5. Hence, it is necessary 
to convert he UNL hypergraph into an Ariane- 
G5 decorated tree. 
The UNL graph is directed. Each arc is labelled 
by a semantic relation (agt, obj, ben, con...) and 
each node is decorated by a UW and a set of 
features, or is a hypernode. One node is 
distinguished as the "entry" of the graph. 
An ARIANE tree is a general (non binary) tree 
with decorations on its nodes. Each decoration 
is a set of wlriable-value pairs. 
The graph-to-tree conversion algorithln has to 
lnaintain the direction and labelling of the 
graph along with the decoration ot' the nodes. 
Our algorithm splits tile nodes that are the target 
of more than one arc, and reverses the direction 
of as few arcs as possible. An example of such a 
conversion is shown in figure 2.3. 
! a \] 
\[5E3 / \  
J x, 
x y 
I~!  I ce  
z t 
b (x  
I I => l 
d : z +~ 
c :Y  i 
? :1  
Fig. 2.3: example graph to tree convel:vion 
Let Z be the set of nodes of G, A the set of 
labels, T the created tree, and N is the set of 
nodes of T. 
Tile graph G={ (a,b,l) l ac  Y.,b6 Z , I~  A} is 
defined as a set of directed labelled arcs. We use 
an association list A = { (n,;,n.r) I ,,,+ ~ r,, U. r E 
N }, where we memorize the correspondence 
between nodes of the tree and nodes of the 
graph. 
2 fully autonmtic high quality dcconvcrsion. 
3 strictly speaking, tile same collection of intcrlingual 
woM senses (acccptions). 
771 
l e t  e(; e  such that e is the entry  of G 
e r 6- new tree-node (ed, entry) 
inT  +- er ( ) ;  N 6- {e,r\]; A <-- {(ec;,eT)} 
whi le G :~ O do 
if there  is (a,b,l) in  G such  that  
G ~- G \ (a ,b , l ) ;  
b r 6- new tree-node(b, i) ; 
A 6- A <J {(b,b,,)); 
l e t  a, r e N such that  (a,a, r) e A 
in  add b r to the daughters of a,r; 
else if there  is (a,b,l) in G such  that  (b,br) 6 
G e- G \ (a ,b , l ) ;  
a T ( -new tree-node(a, l  i); 
A <--- A U {(a ,a . r )} ;  
l e t  brl,e N such  that  (b,br)  e A 
i n  add a,, to the daughters of br; 
else exi t  on  er ro r  ("non connected  graph") ;  
(a ,  a. r) e A then  
A then  
2.2.5 Structural transfer 
The purpose of the structural transfer is to 
transform the tree obtained so far into a 
Generating Multilevel Abstract (GMA) structure 
\[4\]. 
In this structure, non-interlingual linguistic 
levels (syntactic functions, syntagmatic 
categories...) are underspecified, and (if 
present), are used only as a set of hints for the 
generation stage. 
2.3 Generation 
2.3.1 Paraphrase choice 
The next phase is in charge of the paraphrase 
choice. During this phase, decisions are taken 
regarding the derivation applied to each lexical 
unit in order to obtain the correct syntaglnatic 
category for each node. During this phase, the 
order of appearance and the syntactic functions 
of each parts of the utterance is also decided. 
The resulting structure is called Unique 
Multilevel Abstract (UMA) structure. 
2.3.2 Syntactic and morphological generation 
The UMA structure is still lacking the syntactic 
sugar used in French to realize the choices 
made in the previous phase by generating 
articles, auxiliaries, and non connected 
compunds uch as ne...pas, etc. 
The role of this phase is to create a Unique 
Multilevel Concrete (UMC) structure. By 
concrete, we mean that the structure ~s 
projective, hence the corresponding French text 
may be obtained by a standard left to right 
traversal of the leaves and simple morphological 
and graphemic rules. The result of these phases 
is a surface French utterance. 
3 Different uses of the UNL language 
3.1 Hypergraphs  vs colored graphs 
As presented in section 1.2.3, the syntax of the 
UNL language is based on the description of a 
graph, arc by arc. Some of these arcs are 
"coloured" by a number. This colouring is 
currently interpreted as hypernodes (nodes 
containing a graph, rather than a classical UW). 
This interpretation is arbitrary and imposes 
semantic onstraints on a UNL utterance: 
the subgraph (the set of arcs labeled with 
the same colour) is connected, 
arcs with different colours cannot be 
connected to the same node. 
However, even if one uses the UNL language 
for a particular kind of application, a different 
interpretation may be chosen. By adding new 
semantic constraints to UNL expressions, one 
may restrict o the use of trees. On the contrary, 
by loosening semantic onstraint, one may use 
colored graphs instead of the more restrictive 
hypergraphs. 
This flexibility of UNL may lead to uses that 
differ from the computer science point of view 
(different structures leading to different kinds 
of methods and applications) as well as from the 
linguistic point of view (different ways to 
represent the linguistic content of a utterance). 
This kind of structure is very useful to represent 
some utterances like "Christian pulls Gilles' 
leg". Using a colored graph, one can represent 
the utterance with the graph shown in figure 
3.1, which is not a hypergraph. 
772 
01 .@entry 
ag t.. i\[ pull.@entry i 
\[ Chns~lian \] I ~,obj 
pos  
G es \] 
Figure 3.1: this graph is not cut hypergral)h, it can 
however be represented in UNL htnguage 
When using normal hypergraphs, one could 
only represent the utterance as shown in figure 
3.2. 
agt  .... \[ make fun of i 
i Chns'~tan I , i ob j  
i i 
Figure 3.2: this graph is a valid hyperglztph 
Heuce, keeping backward compatibility with 
other UNL based systems, one may develop an 
entirely new and more powerfld kind of 
application. 
3.2 L inguist ic  vs senmnt ie  pivot 
The UNL language defines the interface 
structure to be used by applications (either a 
hypergraph or a colored graph). However, it 
does not restrict the choice of the data to be 
encoded. 
Since tile beginning, two possible and wflid 
apl~roaches has been mentioned. During the 
kickoff meeting of tile UNL prelect, Pr. Tsujii 
prolnoted the use of UNL as a linguistic pivot. 
With this approach, a UNL utterance should be 
the encoding of the deep structure of a valid 
English utterance that reflects the meaning of 
the source utterance. With this approach, the 
German sentence "Hans schwimt sehr gern"  
should be encoded as shown in figure 3.3. 
agt.. _ - like.@entry ~.  .. 
\[ Ha-'~s \[ ' "-. man 
I ob  j "-. "A, 
"~--agt ........ \[ s~wim \] i much , 
Figmv 3.3: a linguistic encoding of "ltcms schwimt 
sehr gern " 
On the opposite, Hiroshi Uchida promotes the 
use of UNL as a semantic pivot. With this 
second approach, the same sentence should be 
encoded as shown in figure 3.4. 
agt /zswim.l@entry 
/ / /  I ~ iman 
H wil~lgly 
l ined 
Figure 3.4: a semantic encoding of "ltans schwimt sehr 
gem" 
Each approach has its advantages and 
drawbacks and the choice between them can 
only be made with an application in mind. The 
linguistic approach leads to a better quality ill 
the produced results and is an answer to highly 
multilingual machine translation projects. With 
this approach, the UNL graphs can only be 
produced by people mastering English or by 
(partially) automatic enconverters. 
With the semantic approach, subtle differences 
in source utterances (indefinite, reflexivity...) 
can not be expressed, leading to a lower quality. 
However, using this approach, the UNL 
encoding is much more natural and easy to 
perform by a non English speaker (as the 
semantic relations and UWs are expressed at the 
source level). Hence, this approach is to be used 
for multilingual casual communication where 
users may express themselves by directly 
encoding UNL expressions with an appropriate 
editing tool. 
Conc lus ion  
Working oil tile French deconvel-ter has led to 
im interestiug architecture where deconversion, 
in principle a "generation from interlingua", is 
implemented as transfer + generation from all 
abstract structure (UNL hypergraph) produced 
from a NL utterance. The idea to use UNL for 
directly creating documents gets here an 
indirect and perhaps paradoxical support, 
although it is clear that considerable progress 
and innovative interface design will be needed 
to make it practical. 
However, the UNL language proves flexible 
enough to be used by very different proiects. 
Moreover, with deconverters currently 
developed for 14 languages, joining the UNL 
project is really attractive. Let's hope that this 
effort will help breaking the language barriers. 
Acknowledgements  
We would like to thank the sponsors of the UNL 
project, especially UNU/IAS (T. Della Senta) & 
ASCII (K.Nishi) and of the UNL-FR subproject, 
especially UJF (C. Feuerstein), IMAG 
(J. Voiron), CLIPS (Y. Chiaramella), and the 
773 
French Ministery of Foreign Affairs (Ph. Perez), 
as well as the members of UNL Center, 
especially project leader H. Uchida, M. L. Zhu, 
and K. Sakai. Last but not least, other members 
of GETA have contributed in many ways to the 
research reported here, in particular N. N6deau, 
E. Blanc, M. Mangeot, J. Sitko, L. Fischer, 
M. Tomokiyo, and K. Fort. 
References  
\[1\] Blanc l~. & Guillaume P. (1997) 
Developing MT lingware through hlternet : 
ARIANE and the CASH interface. Proc. Pacific 
Association for Computational Linguistics 1997 
Conference (PACLING'97), Ohme, Japon, 2-5 
September 1997, vol. 1/1, pp. 15-22. 
\[2\] Blanehon H. (1994) Persl)ectives of DBMT 
for monolingual uthors on the basis of LIDIA-I, an 
implemented mockup. Proc. 15th International 
Conference on Computational Linguistics, 
COLING-94, 5-9 Aug. 1994, vol. 1/2, pp. 
115--119. 
\[3\] Boitet C., R6d. (1982) "DSE-I"--  Le point 
sur ARIANE-78 ddbut 1982. Contrat ADI/CAP- 
Sogcti/Champollion (3 vol.), GETA, Grenoble, 
fdvrier 1982, 400 p. 
\[4\] Boitet C. (1994) Dialogue-Based MT attd 
se(f exl)lahting documents as atl alternative to 
MAHT and MT of controlled languages. Proc. 
Machine Translation 10 Years On, 11 - 14 Nov. 1994, 
Cranfield University Press, pp. 22.1--9. 
\[5\] Boitet C. (1997) GETA's MT methodology 
attd its current development towards petwonal 
networking communication attd speech translation in 
the context of the UNL and C-STAR projects. Proc. 
PACLING-97, Ohme, 2-5 September 1997, Meisei 
University, pp. 23-57. (invited communication) 
\[6\] Boitet C. & Blanehon H. (1994) 
Multilingual Dialogue-Based MT for monolingual 
authotw: the LIDIA project arm a fil:s't mockup. 
Machine Translation, 9/2, pp. 99--132. 
\[7\] Boitet C., Guillaume P. & Qu6zel- 
Ambrunaz M. (1982) ARIANE-78, an 
hltegrated environment for atttomated translation attd 
human revision. Proc. COL1NG-82, Prague, July 
1982, pp. 19--27. 
\[18\] Brown R. D. (1989) Augmentation. 
Machine Translation, 4, pp. 1299-1347. 
\[19\] Ducrot J.-M. (1982) TITUS IV. In 
"Information research in Europe. Proc. ot' tile 
EURIM 5 conf. (Versailles)", P. J. Taylor, cd., 
ASLIB, London. 
\[10\] Kay M. (1973) The MIND system. In 
"Courant Computer Science Symposium 8: Natural 
Language Processing", R. Rustin, ed., Algorithmics 
Press, Inc., New York, pp. 155-188. 
\[11\] Maruyama H., Watanabe H. & Oglno S. 
(1990) An Interactive Japanese Parser for Machine 
Translation. Proc. COLING-90, Helsinki, 20-25 
aofit 1990, ACL, vol. 2/3, pp. 257-262. 
\[12\] Melby A. K., Smith M. R. & Peterson 
J. (1980) ITS : An Interactive Translation 
System. Proc. COLING-80, Tokyo, 30/9-4/10/80, 
pp. 424---429. 
1113\] Moneinme W. (1989) (159 p. 
+annexes)7;40 vet's" l'arabe. Sp&'ification d'une 
gdudration sfatldard e l'arabe. Rdalisation d'un 
l)romO'l)e anglais'-ambe ?t partir d'un attalyseur 
existant. Nouvelle thbse, UJF. 
\[114\] Nirenburg S. & al. (1989) KBMT-89 
Project Report. Center for Machine Translation, 
Carnegie Mellon University, Pittsburg, April 1989, 
286 p. 
\[115\] Nyberg E. H. & Mitamura T. (1992) 
The KANT system: Fast, Accurate, High-Quality 
Translation in Practical Domains. Proc. COLING- 
92, Nantes, 23-28 July 92, ACL, vol. 3/4, pp. 
1069--1073. 
\[16\] Qu6zel-Ambrunaz M. (1990) Ariane-G5 
v.3 - Le moniteut: GETA, IMAG, juin 1990, 206 p. 
\[17\] Sloeum J. (1984) METAL: the LRC 
Machine Translation O,stem. In "Machine 
Translation today: the state of the art (Proc. third 
Lugano Tutorial, 2-7 April 1984)", M. King, cd., 
Edinburgh University Press (1987). 
\[18\] Wehrli E. (1992) The IPS System. Proc. 
COLING-92, Nantes, 23-28 July 1992, vol. 3/4, pp. 
870-874. 
774 
Frameworks, Implementation and Open Problems for the 
Collaborative Building of a Multilingual Lexical Database 
Mathieu MANGEOT-LEREBOURS(1), Gilles S?RASSET(2) and Fr?d?ric ANDR?S(1) 
(1) Software Research Division, NII 
Hitotsubashi, 2-1-2-1913 Chiyoda-ku 
101-8430 Tokyo, Japan 
{mangeot,andres}@nii.ac.jp 
(2) GETA-CLIPS-IMAG 
185, rue de la biblioth?que, BP 53 
F-38041 GRENOBLE CEDEX 9, France 
Gilles.Serasset@imag.fr 
 
Abstract 
Many NLP systems are based on lexical 
data. The development costs of such data 
are a major drawback in such NLP systems. 
In order to cut these costs, we adopt a 
strategy inspired from "open-source" 
projects to allow volunteers to collaborate 
in the creation of a multilingual lexical 
database. 
For this, we had to specify and develop 
tools to manage a lexical database 
containing information complete and 
detailed enough to be usable for a wide 
range of applications. 
This paper presents our project and details 
the tools, frameworks and structures used 
to manage such a database. We will also 
show some research problems still to be 
addressed in this context. 
R?sum? 
La connaissance linguistique reste une 
constituante importante de nombreux 
syst?mes de traitement automatique des 
langues (TAL). Le co?t de cr?ation d?un 
dictionnaire est l?un des freins majeurs 
dans le d?veloppement de ces syst?mes. 
Afin de r?duire les co?ts de cr?ation de 
cette connaissance lexicale, nous adoptons 
une m?thode inspir?e des projets 
"open-source" afin de cr?er une base 
lexicale multilingue. 
 
Pour cela, nous avons sp?cifi? et 
d?velopp? des outils de gestion d'une base 
lexicale contenant des informations 
suffisamment compl?tes et d?taill?es pour 
?tres utilis?es dans de nombreuses 
applications diff?rentes. 
Cet article pr?sente notre projet et d?taille 
les outils, les cadres et les structures 
utilis?es pour la gestion de cette base. 
Nous montrons aussi certains probl?mes 
de recherche ouverts qu'il nous faut 
aborder dans ce contexte. 
Introduction 
Many NLP systems are based on lexical data. 
The development costs of such data are a major 
drawback in such NLP systems. Furthermore, the 
existing lexical data have generally been 
developed for a specific purpose and can?t be 
reused easily in other applications. 
The Papillon project applies some tools and 
methods to develop multipurpose, multilingual 
lexical data collaboratively on Internet. This data 
is complete and detailed enough to be eventually 
used either by NLP systems (MT engines for 
example) or by human users (language learners, 
translators?).  
After presenting the motivations of the Papillon 
project, we will show the management of existing 
data. Then we will describe the structure of the 
Papillon dictionary, and the tools that are used to 
allow contributions from Internet volunteers. 
1 The Papillon Project 
1.1 Motivations 
The Papillon project is the result of the gathering 
of different people sharing common problems 
and solutions. 
1.1.1 A Lack of Resources 
On the Internet, a lot of free dictionaries are 
available but very few of them imply more than 2 
languages. Most of these dictionaries include 
English as one of their languages. 
Furthermore, the existing dictionaries often lack 
information essential for beginners or NLP 
systems.  
Another point contributing to this lack: the high 
costs of development of large lexical resources 
for NLP involves also a high price, dissuasive for 
the end-user. 
1.1.2 Existing Structures and Tools for 
Multilingual Dictionaries 
Some partners of the Papillon project have been 
involved in research on the definition of 
structures and tools to handle multilingual lexical 
databases. 
They were looking for an opportunity to apply 
their research results on real scale lexical data. 
1.1.3 Collaborative Development on the 
Internet 
Most partners were participating, as computer 
scientists, in the development of open source 
products. With the democratisation of Internet 
access in a lot of countries, came the opportunity 
to apply the open source principles to the 
development of a multipurpose, multilingual 
lexical database. 
Cooperation projects for bilingual dictionaries 
are already going on such as EDICT, a 
Japanese-English dictionary lead by Jim Breen 
(2001) for more than 10 years and more recently, 
SAIKAM, a Japanese-Thai dictionary (see 
Ampornaramveth (2000)).  
With the Papillon project, the dictionary is 
extended to a multilingual lexical database. 
Volunteers will find lexicons developed by 
others and some tools to complete or correct the 
Papillon multilingual dictionary. Users will also 
be able to define their own personal views of the 
database. 
1.2 Dictionary Markup Language 
Framework 
Mathieu Mangeot-Lerebours (2001) defines a 
complete framework for the consultation and the 
construction of dictionaries. The framework is 
completely generic in order to manage 
heterogeneous dictionaries with their own proper 
structures. This framework is extensively used in 
Papillon project. 
1.2.1 Dictionary Markup Language (DML) 
 The framework consists in the definition of an 
XML namespace1 called DML   (Dictionary  
Markup Language). All lexical data of a lexical 
database can be described with DML elements. 
The entire hierarchy of the XML files, elements 
and attributes is described using XML schemata 
and grouped into the DML namespace. Figure 1 
describes the organisation of the main DML 
elements.  
Database
Entry
Dictionary
Client
API
Supplier
API
Volume
User
History
Group
CDM set
?headword
?pos
?pronunciation
?translation
?example
?idiom
Basic Types
?boolean
?integer
?date
function
tree
graph
automaton
link
Figure 1. The DML Framework 
The XML schemata are available online. This 
allows users to edit and validate their files online 
with an XML schema validator. 
1.2.2 Common Dictionary Markup (CDM) 
The DML framework may be used to encode 
many different dictionary structures. Indeed, two 
dictionary structures can be radically different. In 
order to handle such heterogeneous structures 
with the same tools, we have defined a subset of 
DML element and attributes that are used to 
 
                                                     
1  http://www-clips.imag.fr/geta/services/dml 
identify which part of the different structures 
represent the same lexical information. This 
subset is called Common Dictionary Markup 
(CDM). This set is in constant evolution. If the 
same kind of information is found in several 
dictionaries then a new element representing this 
piece of information is added to the CDM set. It 
allows tools to have access to common 
information in heterogeneous dictionaries by way 
of pointers into the structures of the dictionaries. 
1.3 Three Layers for the Lexical Data 
The lexical data repository of the Papillon project 
is divided into 4 subdirectories:  
Administration contains guidelines and 
administrative files 
? 
? 
? 
? 
Hell (data in original format) 
Purgatory (data in XML & UTF-8) 
Paradise (data in Papillon format) 
The name of the files and directories is 
normalised in order to allow easy navigation into 
the repository. 
All lexical data stored in the repository is free of 
rights or protected by a GPL-like licence. 
1.3.1 Hell Directory 
This directory contains lexical data in their 
original format. When a dictionary is received, it 
is first stored there while waiting to be ?recycled?. 
For each dictionary, we create a metadata file 
containing all available information concerning 
the dictionary (name, languages covered, 
creation date, size, authors, domain, etc.). It is 
then used to evaluate the quality of the dictionary 
and to guide the recycling process. These 
dicitonaries are freely downloadable as they are. 
1.3.2 Purgatory Directory 
The Purgatory directory receives the lexical data 
once the recuperation process is over. This 
process consists in converting the lexical data 
from its original format into XML encoded in 
UTF-8. To perform this task, we use the 
RECUPDIC methodology described in 
Doan-Nguyen (1998) regular expression tools 
like Perl scripts. 
 
If a dictionary is already encoded in XML, the 
recuperation process consists in mapping the 
elements of information into CDM elements and 
storing the correspondence into the metadata file.  
Internet users access these dictionaries as 
classical online dictionaries, retrieving individual 
entries by way of requests on the Papillon web 
site. 
1.3.3 Paradise Directory 
The Paradise directory contains only one 
dictionary often called the "Papillon dictionary". 
This dictionary has a particular DML structure. 
Internet users access entries of this dictionary by 
way of requests to the Papillon web site. 
It is possible to retrieve only one entry, or any 
subset of entries in any available output format. 
The ?native? format is the Papillon textual XML 
DML format in UTF-8. Users also have ways to 
add new entries or correct existing ones online.  
Other purgatory dictionaries may be integrated 
into the Papillon dictionary with the help of the 
CDM elements. 
2 The Papillon Multilingual 
Dictionary 
2.1 Macrostructure 
The architecture of the Papillon multilingual 
dictionary is based on Gilles S?rasset (1994) and  
has been prototyped by Blanc (1999). This 
architecture uses a pivot structure based on 
multiple monolingual volumes linked to an 
interlingual acception volume.  
Each entry of a monolingual volume represents a 
word sense. In this document, we use the term of 
?lexie? as in the Explanatory and Combinatory 
Dictionary to name a monolingual entry. The 
meaning of ?lexie? is not the same as ?lexeme?. 
A lexie is a complete monolingual entry. 
Figure 2. Illustration of Papillon's macrostructure. 
 
The interlingual volume gathers all the 
interlingual acceptions. An interlingual acception 
represents the union of word-senses or ?lexies? 
considered as ?equivalent? among different 
monolingual volumes. This equivalence is 
calculated from translation links. In this 
document, we use the term of ?axie? to name an 
interlingual acception. 
Real contrastive problems in lexical equivalence 
(not to be confused with monolingual polysemy, 
homonymy or synonymy as clearly explained in 
Mel'cuk and Wanner (2001) are 
handled by way of a special kind 
of link between axies. Figure 2 
illustrates this architecture using a 
classical example involving 
"Rice" in 4 languages. In this 
example, we used the word senses 
as given by the "Petit Robert" 
dictionary for French and the 
"Longman Dictionary of 
Contemporary English" for 
English. As shown, the French and 
English dictionaries do not make 
any word sense distinction 
between cooked and uncooked 
rice seeds. However, this 
distinction is clearly made in 
Japanese and Malay. No axie may 
be used to denote the union of the 
word senses for Malay "nasi" and 
"beras" unless we want to consider 
them as true synonyms in Malay 
(which would be false). Hence, we 
have to create 3 different axies: 
one for the union of "nasi" and ?
? (gohan), the other for the union 
of "beras" and ? (kome) and one 
for the union of "rice" and "riz". A 
link (non-continuous line in Figure 
1 has to be added between the third 
axies and the others in order to 
keep the translation equivalence 
between the word-senses. 
Note that the links between axies 
do not bear any particular 
semantics and should not be 
confused with some kind of 
ontological links. 
 
Bilingual dictionaries can be obtained from the 
multilingual dictionary. 
2.2 Microstructure 
The structure of the lexies (units of the 
monolingual dictionaries) is based on Polgu?re 
(2000) and Mel'cuk?s work on the combinatorial 
and explanatory lexicography, a part of the 
meaning-text theory. An XML schema using the 
DML framework has been defined to represent 
this structure as accurately as possible. 
This structure is common to all the monolingual 
dictionaries. In order to cope with language 
<lexie xmlns="http://www-clips.imag.fr/geta/services/dml" 
       xmlns:d="http://www-clips.imag.fr/geta/services/dml" 
       xmlns:xlink="http://www.w3.org/1999/xlink"  
       xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
       basic="true" d:id="meurtre$1" frequency="0.3"  
       name="Papillon-fra" source-language="fra"  
       ...> 
<headword hn="1">meurtre</headword> 
  <pronunciation encoding="GETA">meu+rtr(e)</pronunciation> 
  <pos>n.m.</pos> 
  <semantic-for action de tuer: ~ PAR L' mula>
    <sem-label>individu</sem-label><actor>X</actor> DE L' 
    <sem-label>individu</sem-label><actor>Y</actor> 
  </semantic-formula> 
  <government-pattern> 
   <mod nb="1"> 
    <actor> 
      <sem-actant>X</sem-actant><synt-actant>I</synt-actant> 
      <surface-group> 
       <surface>de N</surface> 
       <surface>A-poss</surface></surface-group></actor> 
    <actor> 
     <sem-actant>Y</sem-actant><synt-actant>II</synt-actant> 
      <surface-group> 
       <surface>de N surface> </
       <surface>A-poss</surface></surface-group></actor> 
   </mod></government-pattern> 
  <lexical-functions> 
   <function name="Qsyn"> 
     <valgroup> 
      <value> 
       <reflexie xlink:href="#assassinat$1">assassinat 
       </reflexie></value> 
     <value> 
      <reflexie xlink:href="#homicide$2">homicide</reflexie> 
     </value><value> 
      <reflexie xlink:href="#crime$1">crime</reflexie> 
     </value></valgroup></function> 
   <function name="V0"> 
    <valgroup> 
     <value><reflexie xlink:href="#tuer$1">tuer</reflexie> 
     </value></valgroup></function> 
    ... 
  </lexical-functions> 
  <examples> 
    <example d:id="#meurtre$1-e1"> 
    C'est ici que le double meurtre a ?t? commis.</example>   
    ...</examples> 
  ... 
</lexie> 
Figure 3. XML encoding of the French entry "meurtre" (excerpt) 
 
differences, small variations are authorised for 
each monolingual lexicon. Up to now, these 
variations have been used to define the parts of 
speech for each language and to add information 
specific to each language, such as level of 
politeness and counters for Japanese. 
Figure 3 presents an excerpt of the XML 
encoding of the French entry "meurtre" (murder) 
and Figure 4 shows a DEC-like view. 
The general schema has been presented in detail 
in Gilles S?rasset & Mathieu Mangeot-Lerebours 
(2001). 
3 Implementation of the 
Collaborative Web Site 
For the external user, the Papillon project is 
viewed as a dynamic web site providing access 
the existing dictionaries and giving ways to 
contribute to the Papillon dictionary. 
3.1 General Architecture 
The Papillon web site is built with a Java based 
open source framework called Enhydra2. It is 
designed around a standard 3-tier architecture 
a presentation layer in charge of the 
interface with the user. We currently use 
classical HTML/CSS rendering, but plan to 
integrate WML access to the dictionaries 
(for mobile phones), 
? 
a business layer in charge of data 
manipulation and transformation. We 
currently use XML data (in UTF-8) and 
XSL transformations for data manipulation, 
? 
a data layer in charge of the communication 
with the database via a JDBC driver. The 
data layer should be managed by an XML 
database allowing language dependent 
sorting. For the moment, XML databases are 
still in an early stage. In order to advance in 
the project, a mapping system for DML has 
been defined in order to store the XML data 
into conventional relational databases. 
PostgreSQL is used at this point. 
? 
3.2 Particular features 
As different users may have different needs 
(translators, learners?) we define different 
views of the Papillon dictionary. Each view is 
encoded as a XSL stylesheet that is applied on the 
result of each user query. In the future, we will 
also allow users to define their own custom views 
and store them on the server. All these 
transformations are done on the server in order to 
allow users to use their preferred browser (even if 
it is not XML aware). Figure 4 shows an example 
of the French entry "MEURTRE" (murder) 
viewed as in Mel'cuk's DEC dictionary. 
                                                     
2 available at www.enhydra.org 
 
 
Figure 4. French entry "meurtre" dynamically 
displayed using Mel'cuk's classical view 
 
To avoid the unintentional pollution of the 
database by erroneous data, the contributions of a 
user are to be validated by a central group of 
trusted users. In the mean time, the contributions 
are stored as XSL stylesheets in the cntributor?s 
private space.  
Each time a user requests a corresponding entry, 
the request is performed in the main database and 
in the user space. The results from the user space 
are used to modify results from the main database. 
This way, the contribution is immediately visible 
to the user exactly as if it had been integrated into 
the main database. 
While contributions are waiting to be validated 
and integrated into the common space, The 
contributors may choose to share them with other 
users or groups of users.  
Every user can contribute at his/her level. For 
example, a linguist specialist of lexical functions  
will enter values of lexico-semantic functions, a 
phonologist pronunciations and a professional 
bilingual translator will enter new interlingual 
links or check the semi-automatically generated 
ones. For this, different interfaces will be 
 
developed to accommodate the various user 
profiles. 
3.3 Annex Tools 
As the web site hosts a rather complex 
collaborative work, we have added some tools 
that are not related to lexicography, but that have 
to work in a multilingual context. 
First, there is a tool to archive our Papillon 
mailing list. Such a tool is very common on 
Internet sites. However, as we found out, these 
tools may not be used in our multilingual context, 
where mails may contain discussion in different 
languages, written with different tools, and 
encoded using different standards. Hence we 
patched an existing tool so that it archives all 
mail in UTF-8, regardless of its original 
encoding. 
To avoid the considerable work of the webmaster 
and to facilitate the communication and the 
exchange of informations between the users of 
the database, we are developing tools to facilitate 
the use of a document repository. 
After registration and login, users can easily 
upload online a file in whatever format. It will 
immediately be stored into the document 
repository and made accessible online on the 
web. 
4 Actual Research and Development 
Directions 
The Papillon project is a extremely interesting 
experimentation platform. We are currently 
working on validation of monolingual data, 
management of axies and acquisition of new 
data. 
4.1 Validation of the Monolingual Data 
A team of trusted lexicographers validates user 
contributions before they are integrated into the 
main database. 
This validation is a time consuming process and 
implies a good level in linguistics and 
lexicography. Moreover, we may not find enough 
specialists volunteering for such a work and we 
may have to pay a core team for this. 
This task is essential and should be conducted as 
quickly as possible lest the users will be 
discouraged by the delays implied by the central 
team. 
Hence, even in this validation process, we wish to 
enroll users as much as possible. For this task, we 
plan to implement tools for indirect validation of 
information using vote mechanisms and 
generating questions answerable without any 
special knowledge in linguistics. 
As a first experiment, we will use a French 
generator in order to produce a lot of examples 
using the word to be validated and a set of known 
words (already validated). These examples will 
be presented to native speakers and they will 
simply have to accept or reject them. This 
strategy is very interesting in our context, as it 
will help validating the lexical functions. 
4.2 Management of the Interlingual Links 
The use of a pivot dictionary to represent 
translation equivalence is challenging. This 
macrostructure is very satisfying on a theoretical 
level, but introduces a high complexity of 
management. 
In S?rasset (1994), we envisaged that these 
interlingual acceptions would be created and 
managed by hand by a team of specialists, helped 
by tools that would detect inconsistencies and 
propagate decisions among the different 
languages. This appeared to be unrealistic. 
However, we now have means to manage these 
acceptions automatically. For this, we use the fact 
that the interlingual acceptions volume does not, 
in any way, represent a semantic pivot. It is not 
related to an ontology. 
In fact, the only relevant purpose of this 
interlingual volume is to factorise the bilingual 
links we find in classical bilingual dictionaries 
(or the ones that will be specified by the users). 
Hence, given a set of translation equivalences 
between monolingual acceptions of different 
languages, it is possible to compute a minimal set 
of acceptions (and their links) that conforms to a 
set of well-formedness criteria. 
One of the difficult tasks is to obtain bilingual 
translation equivalences between monolingual 
acceptions when bilingual dictionaries often 
provide bilingual links between mere lemmas. 
For this, we will use aligned corpora and 
translations memories to add contextual 
information to the translation pairs. 
 
4.3 Acquisition of new data 
To depend entirely on volunteer work is of course 
unrealistic, especially while beginning to build 
the lexical database. That is why we first reuse 
existing dictionaries in order to build the kernelof 
the database.  
Contributors will come in later, filling in missing 
informationin existing entries and creating partial 
or complete new entries as well as links. 
However, as we are using a rather complex 
structure which require some skills that are not 
shared by all Internet users, we will have to help 
them help us. 
In particular, we are beginning to use 
corpus-based techniques to extract lemmas that 
will be candidates as a value of a lexical function. 
Determining the appropriate lexical function is 
one of the jobs of our contributors, but they will 
be helped in this task by tools that will provide 
them with questions and candidate paraphrases. 
For a complement of information or to help the 
contributors in their task, the database should 
also propose the consultation of other 
dictionaries stored locally or available online on 
the web. 
Moreover, to be really useful for the reader, and 
especially to the learners, the examples found in 
the dictionaries will be translated in other 
languages literally and semantically. Some of 
these translations will be extracted from aligned 
corpora. 
Conclusion 
The theoretical frameworks for the whole 
database, the macrostructure and the 
microstructure are very well defined. It 
constitutes a solid basis for the implementation. 
A lot of open problems still have to be addressed 
for the Papillon project to be a success. In this 
respect, the Papillon project appears to be a very 
interesting experimentation platform for a lot of 
NLP research as data acquisition or human access 
to lexical data, among others. 
All this research will improve the attraction of 
such a project to the Internet users. This attraction 
is necessary for the project to go on, as it is highly 
dependent on its users motivations. 
This way, we will be able to provide a very 
interesting multilingual lexical database that we 
hope useful for a lot of persons.  
References  
Vutichai Ampornaramveth, Akiko Aizawa, Keizo 
Oyama & Tanasee Methapisit (2000) An 
Internet-Based Collaborative Dictionary 
Development Project: SAIKAM., Proc. AdInfo 2000, 
9-10 March 2000, NACSIS, Tokyo, Japan, 4 p. 
Etienne Blanc (1999) PARAX-UNL: a Large Scale 
Hypertextual Multilingual Lexical Database. Proc. 
NLPRS 1999, Tsinghua University Press, Beijing, 
1999, pp. 507-510.  
Jim W. Breen (2001) A WWW Dictionary and Word 
Translator: Threat or Aid to Language Acquisition?, 
in R Gitsaki-Taylor and P Lewis (eds), Proc. 
JALT-CALL 2001, Gunma, Japan, 26-27 May 2001, 
10 pp. 
Ha? Doan-Nguyen (1998) Accumulation of Lexical 
Sets: Acquisition of Dictionary Resources and 
Production of New Lexical Sets. COLING-ACL'98, 
Montr?al, 10-14 August 1998, pp 330-335.  
Mathieu Mangeot-Lerebours (2001) Environnements 
centralis?s et distribu?s pour lexicographes et 
lexicologues en contexte multilingue. PhD Thesis in 
Computer Sciences Universit? Joseph Fourier 
Grenoble I, 27 September 2001, 280 p.  
Igor Melc?uk & Leo Wanner (2001) Towards a 
Lexicographic Approach to Lexical Transfer in 
Machine Translation (Illustrated by the 
German?Russian Language Pair). Machine 
Translation 16: 21?87, 2001. ? 2001 Kluwer 
Academic Publishers. Printed in the Netherlands. 
Alain Polgu?re (1998) La th?orie 
Sens-Texte .Dialangue, Vol. 8-9, Universit? du 
Qu?bec ? Chicoutimi, pp 9-30. 
Alain Polgu?re (2000) Towards a 
theoretically-motivated general public dictionary of 
semantic derivations and collocations for French. 
Proc. EURALEX'2000, Stuttgart, pp 517-527. 
Gilles S?rasset (1994) Interlingual Lexical 
Organisation for Multilingual Lexical Databases in 
NADIA. In Proc. COLING-94, Kyoto, 5-9 August 
1994, M. Nagao ed. vol. 1/2 : pp. 278-282. 
Gilles S?rasset & Mathieu Mangeot-Lerebours (2001) 
Papillon Lexical Database Project: Monolingual 
Dictionaries & Interlingual Links. Proc. 
NLPRS'2001, Hitotsubashi Memorial Hall, 
National Center of Sciences, Tokyo, Japan, 27-30 
November 2001, vol 1/1, pp. 119-125. 
Mutsuko Tomokiyo et al (2000) Papillon : a Project 
of Lexical Database for English, French and 
Japanese, using Interlingual Links. Journ?es 
Science et Technologie de l'ambassade de France au 
Japon, 13 November 2000, Tokyo, Japan, 3 p. 
 
Proceedings of the Workshop on Multilingual Language Resources and Interoperability, pages 25?31,
Sydney, July 2006. c?2006 Association for Computational Linguistics
The LexALP Information System: Term Bank and Corpus for Multi-lingual Legal Terminology Consolidated 
  Verena Lyding, Elena Chiocchetti EURAC research Viale Druso 1, 39100 Bozen/Bolzano - Italy  forename.name@eurac.edu 
Gilles S?rasset, Francis Brunet-Manquat GETA-CLIPS IMAG BP 53, 38041 Grenoble cedex 9 - France forename.name@imag.fr     Abstract Standard techniques used in multilingual terminology management fail to describe legal terminologies as they are bound to different legal systems and terms do not share a common meaning. In the LexALP project, we use a technique defined for general lexical databases to achieve cross language interoperability between lan-guages of the Alpine Convention. In this paper we present the methodology and tools developed for the collection, de-scription and harmonisation of the legal terminology of spatial planning and sus-tainable development in the four lan-guages of the countries of the Alpine Space. 1 Introduction The aim of the LexALP project is to harmo-nise the terminology used by the Alpine Conven-tion, both for internal purposes and for commu-nication among the member states. The Alpine Convention is an international treaty signed by all states of the Alpine territory (France, Monaco, Switzerland Liechtenstein, Austria, Germany, Italy and Slovenia) for the protection of land-scape and sustainable development of this moun-tain area1. The member states speak four differ-ent languages, namely French, German, Italian, and Slovene and have different legal systems and traditions. Hence arises the need for a systematization and uniformation of terminology and clear trans-lation equivalence in all four languages. For this reason, the project intends to provide all                                                 1 cf. also http: www.alpenkonvention.org  
stakeholders and the wider public with an infor-mation system which combines three main com-ponents, a terminology data base, a multilingual corpus and the relative bibliographic data base. In this way the manually revised, elaborated and validated (harmonised) quadrilingual information on the legal terminology (i.e. complete termino-logical entries) will be closely interacting with a facility to dynamically search for additional con-texts in a relevant set of legal texts in all lan-guages and for all main legal systems involved. 2 Multilingual legal information system The information system for the terminology of the Alpine Convention, with a specific focus on spatial planning and sustainable development, will give the possibility to search for relevant terms and their (harmonised or rejected) translations in all 4 official languages of the Alpine Convention in the first module, the term bank. Next to retrieving synonyms and translation equivalents within each legal system, the user will be provided with a representative context and a valid definition of the concept under consideration. Source information will be provided for each text field in the terminological entry. Via a link from the terminological data base to the second module, the corpus facility, the information system will give the possibility to search the corpus for further contexts. Finally, both term bank and corpus will be in-teracting with a third module, the bibliographic database, so as to allow retrieving full informa-tion on text excepts cited in the term bank and to store important meta data on corpus documents. 
25
3 Terminological data 3.1 Data categories and motivations The data categories present in the terminology database allow entering and organising relevant information on the concept under analysis. The term bank interface allows entering of the fol-lowing terminological data categories: denomi-nation/term, definition, context, note, sources (text fields), grammatical information to the term, harmonisation status, processing status, geo-graphical usage, frequency and domain, accord-ing to the appositely elaborated domain classifi-cation structure2 (pull down menus). Again by means of pull-down menus the terminologist will be able to signal to the users which terms are al-ready processed (i.e. checked by legal experts), harmonised or rejected and - most important - to which legal system they belong (the menu geo-graphical usage allows to specify this informa-tion). Furthermore it is possible to specify syno-nyms, short forms, abbreviations etc. in the ter-minological entry and, if necessary, link them to the relative full information already present in the term bank (however, no direct access to these linked data is possible, this must be done via the search interface). Finally, the terminologist is given the possibility of writing general com-ments to the entry. At the very end of one lan-guage entry the terminologist can decide whether to release the data to the public (by clicking on the button ?finish?) or keep it for further fine-tuning (button ?update?). Each term is created in its ?language volume? and described by means of all necessary informa-tion. As soon as one or all equivalents in the other languages are available too, the single en-tries can be linked to each other with the help of an axie (see detailed description below). Searches can be done for all languages or on a user-defined selection of source and target lan-guages. Presently the database allows global searching in all text fields and filtering by source, author, date of creation, as well as by axie name and ids. Results can be displayed in full form, as a short list of terms only or in XML. Some ex-port/import functions are granted. As the term bank serves mainly the scope of diffusing harmonised terminology, the four trans-lation equivalents (validated by a group of ex-perts) are displayed together, whereas rejected synonyms are displayed separately for each search language. In this way the user may well                                                 2 See also 4.1  
look for a non validated synonym and find it in the database but be warned as to which is the preferred term and its harmonised equivalents in the other languages. Figure 1 shows such a situa-tion where the French rejected term ?transport intra-alpin? is linked to the harmonised term ?trafic intra-alpin?.  
 Figure 1: A set of Alpine Convention terms and their relations 3.2 Monolingual data The LexALP term bank consists in 5 volumes for French, German, Italian, Slovene and English (no data is being entered for this fifth language at the moment), which contain the term descrip-tions. The set of data categories is represented in an XML structure that follows a common schema.  <entry id="fra.trafic_intra-alpin.1010743.e"        lang="fra"        legalSystem="AC"        process_status="FINALISED"        status="HARMONISED">   <term>trafic intra-alpin</term>   <grammar>n.m.</grammar>   <domain>Transport</domain>   <usage frequency="common"          geographical-code="INT"          technical="false"/>   <relatedTerm      isHarmonised="false"     relationToTerm="Synonym"     termref="fra.transport_intra-alpin?"/>   <relatedTerm      isHarmonised="false"     relationToTerm="Synonym"     termref="fra.circulation_intra-?"/>   <definition>     [T]rafic constitu? de trajets ayant leur      point de d?part et/ou d'arriv?e ?      l'int?rieur de l'espace alpin.   </definition>   <source>Prot. Transp., art. 2 </source>   <context url="http://www...">     Des projets routiers ? grand d?bit pour      le trafic intra-alpin peuvent ?tre      r?alis?s, si [...].   </context> </entry> Figure 2: XML form of the term ?trafic intra-alpin? Each entry represents a unique term/meaning. Terms with the same denomination, but belong-
26
ing to different legal systems have, de facto, dif-ferent meanings. Hence, different entries are cre-ated. Terms with different denominations but conveying the same ?meaning? (concept) are also represented using different entries3. In this case, the entries are linked through a synonymy rela-tion. Figure 2 shows the XML structure of the French term ?trafic intra-alpin?, as defined in the Alpine Convention. The term entry is associated to a unique identifier used to establish relations between volume entries. The example term belongs to the Alpine Con-vention legal system4 (code AC). The entry also bears the information on its status (harmonised or rejected) and its processing status (to be proc-essed, provisionally processed or finalised). In addition, a definition (along with its source) and a context may be given. The definition and context should be extracted from a legal text, which must be identified in the source field. 3.3 Achieving language/legal system interoperability As the project deals with several different le-gal terms, standard techniques used in multilin-gual terminology management need to be adapted to the peculiarities of the specialised language of the law. Indeed, terms in different languages are (generally) defined according to different legal systems and these legal systems cannot be changed. Hence, it is not possible to define a common ?meaning? that could be used as a pivot for language interoperability5. In this respect, legal terminology is closer to general lexicography than to standard terminology. In order to achieve language/legal system interoperability we had several options that are used in general lexicography.  Using a set of bilingual dictionaries is not an option here, as we have to deal with at least 16                                                 3 Variants, acronyms, etc. are not considered as dif-ferent denominations. 4 Strictly speaking, the Alpine Convention does not constitute a legal system per se. 5 Consider for instance the difference between the Italian and the Austrian concepts of journalists? pro-fessional confidentiality. Whereas the Redaktionsge-heimnis explicitly underlines that the journalist can refuse to witness in court in order to keep the profes-sional secret, in Italy the segreto giornalistico must obligatorily be lifted on a judge?s request. The two concepts have overlapping meanings in the two states, however, they diverge greatly with respect to the be-haviour in court.  
language/legal system couples (with alpine Con-vention and EU levels, but without taking into account regional levels). Moreover, such a solu-tion will not reflect the multilingual aspect of the Alpine Convention or the Swiss legal system. Finally, building bilingual volumes between the French and Italian legal systems is far beyond the objectives of the LexALP project. Another solution would be to use an ?Eu-rowordnet like? approach (Vossen, 1998) where a specific language/legal system is used as a pivot and elements of the other systems are linked by equivalent or near-equivalent links. As such an approach artificially puts a language in the pivot position, it generally leads to an ?eth-nocentric? view of the other languages. The ad-vantage being that the architecture uses the bilin-gual competence of lexicographers to achieve multilingualism.  In this project, we chose to use ?interlingual acceptions? (a.k.a. axies) as defined in (S?rasset, 1994) to represent such complex contrastive phenomena as generally described in general lexicography work. In this approach, each ?term meaning? is associated to an interlingual accep-tion (or axie). These axies are used to achieve interoperability as a pivot linking terms of differ-ent languages bearing the same meaning. However, as we are dealing with legal terms (bound to different legal systems), it is generally not possible to find terms in different languages that bear the same meaning. In fact such terms can only be found in the Alpine Convention (which is considered as a legal system expressed in all the considered languages). Hence, we use these terms to achieve interoperability between languages. In this aspect, we are close to Eu-rowordnet?s approach as we use a specific legal system as a pivot, but in our case the pivot itself is generally a quadrilingual set of entries.  These harmonised Alpine Convention terms are linked through an interlingual acception. An axie is a place holder for relations. Each interlin-gual acception may be linked to several term en-tries in the languages volumes through termref elements and to other interlingual acceptions through axieref elements, as illustrated in Figure 3. <axie id="axi..1011424.e">  <termref    idref="ita.traffico_intraalpino.1010654.e"    lang="ita"/>  <termref    idref="fra.trafic_intra-alpin.1010743.e"    lang="fra"/>  <termref   idref="deu.inneralpiner_Verkehr.1011065.e"  
27
  lang="deu"/>  <termref    idref="slo.znotrajalpski_promet.1011132.e"    lang="slo"/>  <axieref idref=""/>  <misc></misc> </axie> Figure 3: XML form of the interlingual acception illustrated Figure 1 The termref relation establishes a direct translation relation between these harmonised equivalents. Then, national legal terms are indi-rectly linked to Alpine Convention terms through the axieref relation as illustrated in Figure 4. 
 Figure 4: An example French term, linked to a quadrilingual Alpine Convention Term. 4 Corpus 4.1 Corpus content The corpus comprises around 3000 legal documents of eight legal systems (Germany, It-aly, France, Switzerland, Austria, Slovenia, European law  and international law with the specific framework of the Alpine Convention,) (see table 1).   AT CH DE FR IT SI AC EU INT 612 119 62 613 490 213 38 791 149 Table 1: Corpus documents for each legal system Documents of the supranational level are pro-vided in up to four languages (subject to avail-ability). National legislation is generally added in the national language (monolingual documents) and in case of Switzerland (multilingual docu-ments) in the three official languages of that na-tion (French, German and Italian). The documents are selected by legal experts of the respective legal systems following prede-fined criteria: ? entire documents (no single paragraphs or excerpts etc.); ? strong relevance to the subjects ?spatial planning and sustainable development? as described in art. 9 of the relative Alpine Convention Protocol; 
? primary sources of the law for every sys-tem at national and international/EU level, i.e. normative texts only (laws, codes etc.);  ? latest amendments and versions of all legislation (at time of collection: June ? August 2005); ? terminological relevance. Each document is classified according to the following (bibliographical) categories: full title, short title, abbreviation, legal system, language, legal hierarchy, legal text type, subfield (1, 2 and 3), official date, official number, published in official journal (date, number, page), ? The bib-liographical information of all documents is stored in a database and can at any time be con-sulted by the user. The subfields have been elaborated and se-lected by a team of legal experts, taking into ac-count the classification specificities followed by the Alpine Convention and the need to classify texts from several different legal systems accord-ing to one common structure. For this reason, the legal experts have subdivided the fields spatial planning and sustainable development into 5 main areas, in accordance with the Alpine Con-vention Protocol dealing with these subjects and subsequently adopted an EU-based model for further subdividing the 5 main topics in such a way that all countries involved could classify their selected documents under a maximum of 3 main items, the first of which must be indicated obligatorily. This classification allows an easy selection of all subsets of documents according to subject field. 
 Figure 5: Example of document classification 
28
<header   lang="ita"  creator="X"  created="Fri Feb 17 10:45:15 CET 2006"> <h.title>  Legge_regionale_25974.14_87.txt </h.title>  <bibID>  17658 </bibID> </header> Figure 6: XML-header of corpus documents <text id="17658"> <body id="17658.b"> <div type="intro" id="17658.b.i"> <p id="17658.b.i.p1"> <title id="17658.b.i.p1.ti1"> LEGGE REGIONALE 15/05/1987, N. 014  Disciplina dell' esercizio [?] di fauna selvatica.  </title> </p> </div> <div type="section" id="17658.b.c0.se1"> <p id="17658.b.c0.se1.p1"> <title id="17658.b.c0.se1.p1.ti1"> Art. 1 </title> </p> <p id="17658.b.c0.se1.p2"> <s id="17658.b.c0.se1.p2.s1"> 1. Sull' intero territorio  regionale la caccia selettiva  per qualita', [?] </s> <s id="17658.b.c0.se1.p2.s2"> a) capriolo: dal 15 maggio al  15 gennaio;  </s> <s id="17658.b.c0.se1.p2.s3"> b) cinghiale: dal 15 giugno  al 15 gennaio;  </s> </p> <p id="17658.b.c0.se1.p3"> <s id="17658.b.c0.se1.p3.s1"> 2. E' ammesso l' uso [?]  </s> </p> </div> </body> </text> Figure 7: XML-structure of corpus document 4.2 Structural organization of corpus data Collected in raw text format (one file for each legal text) the documents are first transformed into XML-structured files and in a second step inserted into the database.  The XML-annotation is done in compliance with the Corpus Encoding Standard for XML (XCES) 6 . Slightly simplified, the provided schema7 serves to add structural information to the documents. Each text is segmented into sub-sections like: preamble, chapter, section, para-                                                6 http://www.cs.vassar.edu/XCES/ 7 http://www.cs.vassar.edu/XCES/schema/xcesDoc.xsd 
graph, title and sentence. Furthermore, a link to the classification data (bibliographic data base) is inserted and, in case of multilingual documents, alignment is done at sentence level. The XML-annotated documents hold all the information needed for the insertion into the cor-pus database, such as structural mark-up and bib-liographical information. The full text documents are transformed into sets of database entries, which can be imported into the database. 4.3 Technical organization of corpus data Following the bistro approach as realized for the Corpus Ladin dl?Eurac (CLE) (Streiter et al 2004) the corpus data is stored in a relational database (PostgreSQL). The information present in the XML-annotated documents is distributed among four main tables: document_info, cor-pus_words, corpus_structure, corpus_alignment.   The four tables can be described as follows: document_info: This table holds the meta-information about the documents; each category (like full title, short title, abbreviation, legal sys-tem, language, etc.) is represented by a separate column. For each legal document one entry (one row) with unique identification number is added to the table. These identification numbers are cited in the XML-header of the corpus docu-ments. corpus_words: This table holds the actual text of the collected documents. Instead of stor-ing entire paragraphs as it was done during the creation of CLE, for this corpus a different ap-proach is being tested. Every annotated text is split into an indexed sequence of words, starting with counter one. Once inserted into the database a text is stored as a set of tuples composed of word, position in text and document id (as a ref-erence to the document information).  corpus_structure: This table holds all infor-mation about the internal structure of the docu-ments. Titles, sentences, paragraphs etc. are stored by indicating starting and ending point of the section. For each segment a tuple of segment type, segment id, starting point (indicated by the index of the first word), ending point (indicated by the index of the last word) and document id is added. corpus_alignment: This table defines the alignment of multilingual documents. By provid-ing one column for each language the texts are aligned via the document ids or via the ids of single segments.  
29
The tables are interconnected by explicitly stated references. That means that the columns of one table refer to the values of a certain column of another table. As shown in figure 8 all tables hold a column document_id that refers to the document id of the table document_info. Fur-thermore, the table corpus_structure holds refer-ences to the column position of the table cor-pus_words.  
 Figure 8: Interconnection of tables 5 Searching the corpus Due to the fine-grained classification (see section 4.1) and the structural mark-up (see section 4.2) of all corpus documents, corpus searches can be restricted in the following ways: ? by specifying a subset of corpus docu-ments over which the search should be carried out (e.g. all documents of legal system CH with language French); ? by choosing the type of unit to be dis-played (whole paragraphs <p>, sentences <s>, titles <title>, ?); ? by searching for whole words only (ex-act match) or parts of words (fuzzy match); ? by restricting the number of hits to be displayed at a time. For searches in multilingual documents it will be possible to search for aligned segments, specify-ing search word as well as target translation. For example, the user could search for all alignments of German-Italian sentences that contain the word Umweltschutz translated as tutela ambien-tale (and not with protezione dell?ambiente). Figure 9 shows a simple interface for searching monolingual documents. 
 Figure 9: Example search over monolingual documents 6 Interaction term bank and corpus Term bank and corpus are independent compo-nents which together form the LexALP Informa-tion System. The interaction between corpus and term bank will concern in particular 1) corpus segments used as contexts and definitions in the termino-logical entries, 2) short source references in the term bank (and the associated sets of biblio-graphical information) and 3) legal terms. 6.1 Entering data into term bank When adding citations to a term bank entry, the relative bibliographic information will automati-cally be counterchecked with the contents of the bibliographical database. In case the information about the cited document is already present in the DB, a link to the term bank can be added. Oth-erwise the terminologist is asked to provide all information about the new source to the biblio-graphic database and later create the link. Next to static contexts and definitions present for each terminological entry, each entry will show a button for the dynamic creation of con-texts. Hitting the button will start a context search in the corpus and return all sentences con-taining the term under consideration. 6.2 Searching the corpus When searching the corpus the user will have the opportunity to highlight terms present in the term bank. In the same way standardised or rejected terms can be brought out. Via a link it will then 
30
be possible to directly access the term bank entry for the term found in the corpus. In general each corpus segment is linked to the full set of bibliographic information of the document that the segment is part of. Accessing the source information will lead the user to a de-tailed overview as shown in figure 4. 7 Conclusion In this paper, we have presented the LexALP information system, used to collect, describe and harmonise the terminology used by the Alpine Convention and to link it with national legal ter-minology of the alpine Convention?s member states. Even if we currently give a specific focus on spatial planning and sustainable development, the project is not restricted to these fields and the methodology and tools developed can be adapted to legal terminology of other fields.  In this paper we also proposed a solution to the encoding of multilingual legal terminologies in a context where standard techniques used in multilingual terminology management usually fail. The terminology developed and the corpus used for its development will be accessible on-line for the stakeholders and the wider public through the LexALP information system. 
Acknowledgements The LexALP research project started in Janu-ary 2005 thanks to the funds granted by the IN-TERREG IIIB ?Alpine Space? Programme, a Community Initiative Programme funded by the European Regional Development Fund. References Gilles S?rasset. 1994. Interlingual Lexical Organisa-tion for Multilingual Lexical Databases in NADIA.  In Makoto Nagao, editor, COLING-94, volume 1, pages 278?282, August. Streiter, O., Stuflesser, M. & Ties, I. (2004). CLE, an aligned Tri-lingual Ladin-Italian-German Corpus. Corpus Design and Interface, LREC 2004, Work-shop on "First Steps for Language Documentation of Minority Languages: Computational Linguistic Tools for Morphology, Lexicon and Corpus Com-pilation" Lisbon, May 24, 2004. Vossen, Piek. 1998. Introduction to EuroWordNet. In Nancy Ide, Daniel Greenstein, and Piek Vossen, editors, Special Issue on EuroWordNet, Computers and the Humanities, 32(2-3): 73-89. Wright, Sue Ellen 2001. Data Categories for Termi-nology Management. In Sue Ellen Wright & Gerhard Budin, editors, Handbook of Terminology Management, volume 2, pages 552-569.  
31
A Generic Collaborative Platform for Multilingual Lexical
Database Development
Gilles S?RASSET
GETA-CLIPS, IMAG, Universit? Joseph Fourier
BP 53 ? 38041 Grenoble cedex 9 ? France
Gilles.Serasset@imag.fr
Abstract
The motivation of the Papillon project is to
encourage the development of freely accessible
Multilingual Lexical Resources by way of on-
line collaborative work on the Internet. For this,
we developed a generic community website orig-
inally dedicated to the diffusion and the devel-
opment of a particular acception based multilin-
gual lexical database.
The generic aspect of our platform allows
its use for the development of other lexical
databases. Adapting it to a new lexical database
is a matter of description of its structures and
interfaces by way of XML files. In this paper, we
show how we already adapted it to other very
different lexical databases. We also show what
future developments should be done in order to
gather several lexical databases developers in a
common network.
1 Introduction
In order to cope with information available in
many languages, modern information systems
need large, high quality and multilingual lexi-
cal resources. Building such a resource is very
expensive. To reduce these costs, we chose to
use the ?collaborative? development paradigm
already used with LINUX and other open source
developments.
In order to develop such a specific multilin-
gual lexical database, we built a Web platform
to gather an Internet community around lex-
ical services (accessing many online dictionar-
ies, contributing to a rich lexical database, vali-
dating contributions from others, sharing doc-
uments, . . . ). Initially built for the Papillon
project, this platform is generic and allows for
the collaborative development of other lexical
resources (monolingual, bilingual or multilin-
gual) provided that such resources are described
to the platform.
After presenting the Papillon project and
platform, we will show how we may give access
to many existing dictionaries, using an unified
interface. Then, we will present the edition ser-
vice, and detail how it may be customised to
handle other very different dictionaries.
2 The Papillon project
2.1 Motivations
Initially launched in 2000 by a French-Japanese
consortium, the Papillon project1 (S?rasset and
Mangeot-Lerebours, 2001) rapidly extended its
original goal ? the development of a rich French
Japanese lexical database ? to its actual goal ?
the development of an Acception based Multilin-
gual Lexical Database (currently tackling Chi-
nese, English , French, German, Japanese, Lao,
Malay, Thai and Vietnamese).
This evolution was motivated in order to:
? reuse many existing lexical resources even
the ones that do not directly involve both
initial languages,
? be reusable by many people on the Internet,
hence raising the interest of others in its
development,
? allow for external people (translator, native
speakers, teachers. . . ) to contribute to its
development,
For this project, we chose to adopt as much
as possible the development paradigm of LINUX
and GNU software2, as we believe that the lack
of high level, rich and freely accessible multi-
lingual lexical data is one of the most crucial
obstacle for the development of a truly multilin-
gual information society3.
1http://www.papillon-dictionary.org/
2i.e. allowing and encouraging external users to ac-
cess and contribute to the database.
3i.e. an Information Society with no linguistic dom-
ination and where everybody will be able to access any
content in its own mother tongue.
2.2 Papillon acception based
multilingual database
The Papillon multilingual database has been de-
signed independently of its usage(s). It consists
in several monolingual volumes linked by way of
a single interlingual volume called the interlin-
gual acception dictionary.
Fran?ais
Anglais
Malais
Japonais
Axies
Riz (plante monocotyl?done)
Riz (grain)
Rice (food grain)
Rice (seeds)
??
?
?
padi (unharvested 
         grain)
nasi (cooked)
beras (uncooked)
Figure 1: Macrostructure of the Papillon
MLDB, showing the handling of contractive
problems.
Each monolingual volume consists in a set of
word senses (lexies), each lexie being described
using a structure derived from the Explanatory
and Combinatory Dictionary (Mel??uk et al,
1995; Mel??uk et al, 1984 1989 1995 1996).
The interlingual acception dictionary consists
in a set of interlingual acceptions (axies) as de-
fined in (S?rasset, 1994). An interlingual accep-
tion serves as a placeholder bearing links to lex-
ies and links between axies4. This simple mech-
anism allows for the coding of translations. As
an example, figure 1 shows how we can repre-
sent a quadrilingual database with contrastive
problems (on the well known ?rice? example).
2.3 Development methodology
The development of the Papillon multilingual
dictionary gathers voluntary contributors and
trusted language specialist involved in different
tasks (as shown in figure 2).
? First, an automatic process creates a
draft acception based multilingual lexical
database from existing monolingual and
bilingual lexical resources as shown in
(Teeraparseree, 2003; Mangeot-Lerebours
et al, 2003). This step is called the boot-
strapping process.
4Note that these links are not interpreted semanti-
cally, but only reflect the fact that translation is possible
Papillon server
L1<->L3
L2<->L3
L1
L2
L3
reused data
bootstrapping
Papillon database
L1 L2
L3
Axies
Document
Document
Document
Modifications/
additions/
Suppressions
Integration
Contributions
Validations
Figure 2: Methodology for the development of
the Papillon database.
? Then, contributions may be performed by
volunteers or trusted language specialists.
A contribution is either the modification
of an entry, its creation or its deletion.
Each contribution is stored and immedi-
ately available to others.
? Volunteers or language specialist may vali-
date these contributions by ranking them.
? Finally, trusted language specalists will in-
tegrate the contribution and apply them to
the master MLDB. Rejected contributions
won?t be available anymore.
2.4 The Papillon Platform
The Papillon platform is a community web site
specifically developed for this project. This plat-
form is entirely written in Java using the ?En-
hydra5? web development Framework. All XML
data is stored in a standard relational database
(Postgres). This community web site proposes
several services:
? a unified interface to simultaneously ac-
cess the Papillon MLDB and several other
monolingual and bilingual dictionaries;
? a specific edition interface to contribute to
the Papillon MLDB,
? an open document repository where regis-
tered users may share writings related to
the project; among these documents, one
may find all the papers presented in the
5see http://www.enhydra.org/
different Papillon workshops organized each
year by the project partners;
? a mailing list archive,
Sections 3 and 4 present the first and second
services.
3 Unified access to existing
dictionaries
3.1 Presentation
Figure 3: The unified access interface and re-
sults from three different dictionaries
To encourage volunteers, we think that it is
important to give a real service to attract as
many Internet users as possible. As a result, we
began our development with a service to allow
users to access to many dictionaries in a uni-
fied way. This service currently gives access to
twelve (12) bilingual and monolingual dictionar-
ies, totalizing a little less than 1 million entries,
as detailled in table 1.
3.2 Strong points
The unified access interface allows the user
to access simultaneously to several dictionaries
with different structures. All available dictio-
nary will be queried according to its own struc-
ture. Moreover, all results will be displayed in a
form that fits its own structure.
Any monolingual, bilingual or multilingual
dictionary may be added in this collection, pro-
vided that it is available in XML format.
With the Papillon platform, giving access to
a new, unknown, dictionary is a matter of writ-
ing 2 XML files: a dictionary description and an
Dictionary Languages Nb of Entries
Armamenta fra eng 1116
Cedictb zho eng 215424
Dingc deu eng 124413
Engdictd eng kor 214127
FeMe fra eng msa 19247
Homericaf fra 441
JMDictg jp en fr de 96264
KanjiDicth jpn eng 6355
Papillon multi 1323
ThaiDicti tha 10295
VietDictj fra vie 41029
WaDokuJiTenk jpn deu 214274
aJapanese French dictionary of armament from the
French Embassy in Japan
bChinese English from Mandel Shi (Xiamen univ.)
c(Richter, 1999)
d(Paik and Bond, 2003)
e(Gut et al, 1996)
fUniversity Stendhal, Grenoble III
g(Breen, 2004a)
h(Breen, 2004b)
iThai Dictionary of Kasetsart University
j(Duc, 1998)
k(Apel, 2004)
Table 1: Dictionaries available through the uni-
fied access interface
XSL stylesheet. For currently available dictio-
naries, this took an average of about one hour
per dictionary.
3.3 Implementation
It is possible to give access to any XML dictio-
nary, regardless of its structure. For this, you
have to identify a minimum set of information
in the dictionary?s XML structure.
The Papillon platform defines a standard
structure of an abstract dictionary contain-
ing the most frequent subset of information
found in most dictionaries. This abstract struc-
ture is called the Common Dictionary Markup
(Mangeot-Lerebours and S?rasset, 2002). To
describe a new dictionary, one has to write an
XML file that associate CDM element to point-
ers in the original dictionary structure.
As an example, the French English Malay
FeM dictionary (Gut et al, 1996) has a specific
structure, illustrated by figure 4.
Figure 5 gives the XML code associating el-
ements of the FeM dictionary with elements of
the CDM.
Along with this description, one has to de-
<HFEM xmlns:xml="http://www.w3.org/.../namespace">
<HW-FRE>montre</HW-FRE>
<HOM/>
<PRNC>mon-tr(e)</PRNC>
<AUX/>
<BODY>
<SENSE-STAR>
<SENSE>
<CAT-STAR>n.f.</CAT-STAR>
<SENSE1-STAR>
<SENSE1>
<TRANS-STAR>
<TRANS>
<ENG-STAR>watch</ENG-STAR>
<MAL-STAR>jam</MAL-STAR>
</TRANS>
</TRANS-STAR>
<EXPL-STAR/>
</SENSE1>
</SENSE1-STAR>
</SENSE>
</SENSE-STAR>
</BODY>
</HFEM>
Figure 4: A simplified example entry from the
French English Malay FeM dictionary.
<cdm-elements>
<cdm-volume element="volume"/>
<cdm-entry element="HFEM"/>
<cdm-headword element="HW-FRE"/>
<cdm-pronunciation element="PRNC"/>
<cdm-pos element="CAT-STAR"/>
<cdm-definition element="FRE"/>
<cdm-translation d:lang="eng"
element="ENG-STAR"/>
<cdm-translation d:lang="msa"
element="MAL-STAR"/>
<cdm-example d:lang="fra" element="FRE"/>
<cdm-example d:lang="eng" element="ENG"/>
<cdm-example d:lang="msa" element="MAL"/>
<cdm-key1 element="HOM"/>
</cdm-elements>
Figure 5: Associations between elements of the
FeM dictionary and elements of the CDM.
fine an XSL style sheet that will be applied on
requested dictionary elements to produce the
HTML code that defines the final form of the
result. If such a style sheet is not provided,
the Papillon platform will itself transform the
dictionary structure into a CDM structure (us-
ing the aforementioned description) and apply a
generic style sheet on this structure.
4 Editing dictionaries entries
4.1 Presentation
As the main purpose of the Papillon platform is
to gather a community around the development
of a dictionary, we also developed a service for
the edition of dictionary entries.
Figure 6: The edition interface is a standard
HTML interface
Any user, who is registered and logged in to
the Papillon web site, may contribute to the Pa-
pillon dictionary6 by creating or editing7 an en-
try. Moreover, when a user asks for an unknown
word, he is encouraged to contribute it to the
dictionary.
Contribution is made through a standard
HTML interface (see figure 6). This interface
is rather crude and raises several problems. For
instance, there is no way to copy/paste part of
an existing entry into the edition window. More-
over, editing has to be done on-line8. However,
as the interface uses only standard HTML ele-
ments with minimal javascript functionality, it
may be used with any Internet browser on any
platform (provided that the browser/platform
correctly handles unicode forms).
4.2 Strong points
From the beginning, we wanted this interface
to be fully customizable by Papillon members
6And, for now, only to this particular dictionary.
7Removal of an entry is not yet implemented.
8In fact, entries may be edited off-line and uploaded
on the server, but there is currently no specialized inter-
face for off-line edition, meaning that users will have to
use standard text/XML editor for this.
without relying on the availability of a computer
science specialist. our reasons are:
? the fact that we wanted the structure of the
Papillon dictionary to be adaptable along
with the evolution of the project, without
implying a full revisit of the web site imple-
mentation;
? the fact that each language may slightly
adapt the Papillon structure to fit its own
needs (specific set of part of speech, lan-
guage levels, etc.), hence adding a new dic-
tionary implies adding a new custom inter-
face;
Hence, we chose to develop a system capable
of generating a usable interface from a) a de-
scription of the dictionary structure (an XML
Schema) and b) a description of the mapping
between element of the XML structure and stan-
dard HTML inputs.
For this, we used the ARTStudio tool de-
scribed by (Calvary et al, 2001). Using a tool
that allows for the development of plastic user
interfaces allows us to generate not only one, but
several interfaces on different devices. Hence,
as we are now able to generate an HTML in-
terface usable with any standard web browser
supporting Unicode, we may, in the future, gen-
erate interfaces for Java applications (that can
be used offline) or interfaces for portable devices
like pocket PCs or Palm computers.
4.3 Implementation
4.3.1 Definition of the dictionary
structure
To provide an edition interface, the Papillon
platform needs to know the exact dictionary
structure. The structure has to be defined as
a standard XML schema. We chose to use XML
schema because it allows for a finer description
compared to DTDs (for instance, we may de-
fine the set of valid values of the textual content
of an XML element). Moreover XML schemata
provides a simple inheritance mechanism that
is useful for the definition of a dictionary. For
instance, we defined a general structure for the
Papillon dictionary (figure 7) and used the in-
heritance mechanism to refine this general struc-
ture for each language (as in figure 8).
4.3.2 Description of the interface
Describing the interface is currently the most
delicate required operation. The first step is to
define the set of elements that will appear in the
<element name="lexie">
<complexType>
<sequence>
<element ref="d:headword" minOccurs="1"
macOccurs="1" />
<element ref="d:writing" ... />
<element ref="d:reading" ... />
<element ref="d:pronunciation" ... />
<element ref="d:pos" ... />
<element ref="d:language-levels" ... />
<element ref="d:semantic-formula" ... />
<element ref="d:government-pattern" .../>
<element ref="d:lexical-functions" ... />
<element ref="d:examples" ... />
<element ref="d:full-idioms" ... />
<element ref="d:more-info" ... />
</sequence>
<attribute ref="d:id" use="required" />
</complexType>
</element>
...
<element name="pos" type="d:posType" />
<simpleType name="posType">
<restriction base="string" />
</simpleType>
...
Figure 7: General structure shared by all vol-
umes of the Papillon dictionary; showing the
part of speech element pos defined as a textual
element.
<simpleType name="posType">
<restriction base="d:posType">
<enumeration value="n.m." />
<enumeration value="n.m. inv." />
<enumeration value="n.m. pl." />
<enumeration value="n.m., f." />
<enumeration value="n.f." />
<enumeration value="n.f. pl." />
...
</restriction>
</simpleType>
Figure 8: Redefinition of the type of the part
of speech pos element in the Papillon French
definition.
interface and their relation with the dictionary
structure. Each such element is given a unique
ID. This step defines an abstract interface where
all elements are known, but not their layout, nor
their kind.
This step allows for the definition of several
different tasks for the edition of a single dictio-
nary.
The second step is to define the concrete re-
alization and the position of all these elements.
For instance, in this step, we specify the POS
element to be rendered as a menu. Several kind
of widgets are defined by ARTStudio. Among
them, we find simple HTML inputs like text
boxes, menus, check-boxs, radio buttons, la-
bels. . . , but we also find several high level el-
ements like generic lists of complex elements.
As an simple example, we will see how the pos
(part of speech) element is rendered in the Pa-
pillon interface. First, there will be an interface
element (called S.364) related to the pos element
(figure 9). Second, this element will be realized
in our interface as a comboBox (figure 10).
<Instance type="element" id="S.364">
<InstanceKind value="static"/>
<InstanceBuildKind value="regular"/>
<Name value="pos"/>
<ClassNameSpace value=""/>
<ClassName value="posType"/>
<TaskOwnerID value="S.360"/>
<TaskRangeID list="S.360"/>
</Instance>
Figure 9: Definition of the abstract interface el-
ement associated to the pos element. This el-
ement will display/edit value of type posType
defined in the aforementioned schema.
<Interactor type="element"
class="GraphicInteractor" id="i2008">
<Type value="presentation"/>
<TaskID value="S.363"/>
<InteractorID value="ComboBox"/>
<InstanceID value="S.364"/>
<Width value="10"/>
<Height value="20"/>
</Interactor>
Figure 10: Definition of the effective widget for
the pos element.
Using this technique is rather tricky as there
is currently no simple interface to generate these
rather complex descriptions. However, using
these separate description allows the definition
of several edition tasks (depending on the user
profile) and also allows, for a single task, to gen-
erate several concrete interfaces, depending on
the device that will be used for edition (size of
the screen, methods of interactions, etc.).
4.3.3 Interface generation
Using the describe structure of the dictionary,
we are able to generate an empty dictionary en-
try containing all mandatory elements. Then,
we walk this structure and instantiate all as-
sociated widgets (in our case HTML input ele-
ments), as defined in the interface description.
This way, we are able to generate the corre-
sponding HTML form.
When the user validates a modification, val-
ues of the HTML input elements are associated
to the corresponding parts of the edited dictio-
nary structure (this is also the case if the user
asks for the addition/suppression of an element
in the structure). Then, we are able to regener-
ate the interface for the modified structure. We
iterate this step until the user saves the modified
structure.
5 Conclusions
The Papillon platform is still under develop-
ment. However, it already proves useful for the
diffusion of a little less than 1 million entries
from 12 very different dictionaries. This is pos-
sible as, from the very beginning, we designed
the platform to be as a generic as possible.
This genericity also allows for its use for the
on-line development of the Papillon database.
It is also used for the development of the Esto-
nian French GDEF dictionary, managed by An-
toine Chalvin from INALCO, Paris. Moreover,
we developed an interface for the japanese Ger-
man WadokujiTen (Apel, 2004). This proves
that our platform may be useful in a general
context.
Our future activities will follow 3 axis:
? improving the definition of edition inter-
faces; currently, we have no tool to simplify
this definition and its complexity makes it
difficult for a linguist to use it without help
from computer science specialists;
? generating different interfaces from the
same descriptions; currently, we only gener-
ate on-line HTML interfaces, but the tools
we use allows for the development of inter-
faces in other contexts; hence with the same
approach, we will develop java applets or
java applications to be used either on-line
or off-line;
? developing network cooperation modules
between several instances of the Papillon
platform; this will allow the deployment of
the platform on several sites; we will ad-
dress two aspects of such a deployment;
first, duplication of identical instances pro-
viding access and edition services on the
same dictionaries; second the deployment
of several instances providing access and
edition services on different dictionaries
(where dictionaries edited on a site may be
accessed on another site).
6 Acknowledgements
Developments on the Papillon project could not
have taken place without support from CNRS
(France) and JSPS (Japan). We would like to
warmly thank Fran?ois Brown de Colstoun who
supports this project since its very beginning.
Developments of the platform and especially the
editing part has been mainly done by Mathieu
Mangeot and David Thevenin during their Post
Doctoral fellowship at NII (National Institue of
Informatics), Tokyo. Finally the Papillon plat-
form would not be useful without partners who
agreed to give free access to their superb dictio-
naries.
References
Ulrich Apel. 2004. WaDokuJT - A Japanese-
German Dictionary Database. In Papil-
lon 2002 Workshop on Multilingual Lexical
Databases, NII, Tokyo, Japan, 6-18 July.
Jim W. Breen. 2004a. JMdict: a Japanese-
Multilingual Dictionary. In Gilles S?rasset,
Susan Armstrong, Christian Boitet, Andrei
Pospescu-Belis, and Dan Tufis, editors, post
COLING Wokshop on Multilingual Linguis-
tic Resources, Geneva, Switzerland, 28th au-
gust. International Committee on Computa-
tional Linguistics.
Jim W. Breen. 2004b. Multiple Indexing in an
Electronic Kanji Dictionary. In Michael Zock
and Patrick St Dizier, editors, post COLING
workshop on Enhancing and Using Electronic
Dictionaries, Geneva, Switzerland, 29th au-
gust. International Committee on Computa-
tional Linguistics.
Ga?lle Calvary, Jo?lle Coutaz, and David
Thevenin. 2001. A unifying reference frame-
work for the development of plastic user in-
terfaces. In M. Reed Little and L. Nigay, ed-
itors, Engineering for Human-Computer In-
teraction: 8th IFIP International Confer-
ence, EHCI 2001, volume 2254 / 2001 of
Lecture Notes in Computer Science, page
173. Springer-Verlag Heidelberg, Toronto,
Canada, May.
Ho Ngoc Duc, 1998. Vietnamese French On-
line Dictionary. http://www.informatik.uni-
leipzig.de/~duc/Dict/.
Yvan Gut, Puteri Rashida Megat Ramli, Za-
harin Yusoff, Kim Choy Chuah, Salina A.
Samat, Christian Boitet, Nicolai Nedobejkine,
Mathieu Lafourcade, Jean Gaschler, and Do-
rian Levenbach. 1996. Kamus Perancis-
Melayu Dewan, Dictionnaire francais-malais.
Dewan Bahasa dan Pustaka, Kuala Lumpur.
Mathieu Mangeot-Lerebours and Gilles S?ras-
set. 2002. Frameworks, implementation and
open problems for the collaborative building
of a multilingual lexical database. In Grace
Ngai, Pascale Fung, and Kenneth W. Church,
editors, Proc. of SEMANET Workshop, Post
COLING 2002 Workshop, pages 9?15, Taipei,
Taiwan, 31 August.
Mathieu Mangeot-Lerebours, Gilles S?rasset,
and Mathieu Lafourcade. 2003. Construction
collaborative dune base lexicale multilingue,
le projet Papillon. TAL, 44(2):151?176.
Igor Mel??uk, Nadia Arbatchewsky-Jumarie,
Louise Dagenais, L?o Eltnisky, Lidija Iordan-
skaja, Marie-No?lle Lefebvre, Ad?le Lessard,
Alain Polgu?re, and Suzanne Mantha. 1984,
1989, 1995, 1996. Dictionnaire Explicatif
et Combinatoire du fran?ais contemporain,
recherches lexico-s?mantiques, volumes I, II,
III et IV. Presses de l?Universit? de Montr?al,
Montr?al(Quebec), Canada.
Igor Mel??uk, Andre Clas, and Alain Polgu?re.
1995. Introduction ? la lexicologie explicative
et combinatoire. Universites francophones
et champs linguistiques. AUPELF-UREF et
Duculot, Louvain-la Neuve.
Kyonghee Paik and Francis Bond. 2003. En-
hancing an English/Korean Dictionary. In
Papillon 2003 Workshop on Multilingual Lex-
ical Databases, Sapporo, Japan, 3-5 July.
Franck Richter, 1999. Ding: a Dictio-
nary Lookup Program. http://www-user.tu-
chemnitz.de/~fri/ding/.
Gilles S?rasset and Mathieu Mangeot-
Lerebours. 2001. Papillon lexical database
project: Monolingual dictionaries and in-
terlingual links. In NLPRS-2001, pages
119?125, Tokyo, 27-30 November.
Gilles S?rasset. 1994. Interlingual lexical organ-
isation for multilingual lexical databases in
nadia. In Makoto Nagao, editor, COLING-
94, volume 1, pages 278?282, August.
Aree Teeraparseree. 2003. Jeminie: A flexible
system for the automatic creation of interlin-
gual databases. In Papillon 2003 Workshop
on Multilingual Lexical Databases, Sapporo,
Japan, 3-5 July.
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 937?944,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Multilingual Legal Terminology on the Jibiki Platform:
The LexALP Project
Gilles Se?rasset, Francis Brunet-Manquat
Universite? Joseph Fourier,
Laboratoire CLIPS-IMAG, BP 53
38041 Grenoble Cedex 9 - France,
Gilles.Serasset@imag.fr
Francis.Brunet-Manquat@imag.fr
Elena Chiocchetti
EURAC Research
Viale Druso 1
39100 Bozen/Bolzano - Italy
Elena.Chiocchetti@eurac.edu
Abstract
This paper presents the particular use of
?Jibiki? (Papillon?s web server develop-
ment platform) for the LexALP1 project.
LexALP?s goal is to harmonise the ter-
minology on spatial planning and sustain-
able development used within the Alpine
Convention2, so that the member states
are able to cooperate and communicate
efficiently in the four official languages
(French, German, Italian and Slovene). To
this purpose, LexALP uses the Jibiki plat-
form to build a term bank for the con-
trastive analysis of the specialised termi-
nology used in six different national legal
systems and four different languages. In
this paper we present how a generic plat-
form like Jibiki can cope with a new kind
of dictionary.
1 Introduction
One of the most time-consuming hindrances to
supranational law drafting and convention nego-
tiation is the lack of understanding among nego-
tiators and technical writers. This is not only due
to the fact that different languages are involved,
but mainly to the inherent differences in the legal
systems. Countries that speak the same language
(like France and part of Switzerland) may use the
same word to represent different legal concepts3,
1Legal Language Harmonisation System for Environment
and Spatial Planning within the Multilingual Alps
2http://www.convenzionedellealpi.org
3E.g.: In the German-speaking province of Bolzano Italy
the Landeshauptmann is the president of the provincial coun-
cil, with much more limited competence that the Austrian
Landeshauptmann, who is head of one of the states (Bundes-
land) that are part of the Austrian federation.
as defined in their respective legal traditions. The
same concept may be referred to in different ways
according to the legal system4. Also, terms that
may superficially seem to be translations of each
other can represent different legal notions5.
In order to concretely address these problems,
several institutions representing translators, ter-
minologists, legal experts and computational lin-
guists joined in the LexALP project, co-funded by
EU?s INTERREG IIIb Alpine Space programme.
The objective of the project is to compare the spe-
cialised terminology of six different national legal
systems (Austria, France, Germany, Italy, Switzer-
land and Slovenia) and three supranational sys-
tems (EU law, international law and the particu-
lar framework of the Alpine Convention) in the
four official languages of the Al-pine Convention,
which is an international framework agreement
signed by all countries of the Alpine arc and the
EU. This contrastive analysis serves as a basis for
the work of a group of experts (the Harmonising
Group) who will determine translation equivalents
in French, Italian, German and Slovene (one-to-
one correspondence) in the fields of spatial plan-
ning and sustainable development for use within
the Convention, thus optimising the understanding
between the Alpine states at supranational level.
The tools that are to be developed for these ob-
jectives comprise a corpus bank and a term bank.
The corpus bank is developed by adapting the
bistro system (Streiter et al, 2006; Streiter et al,
2004). The term bank is based on the Jibiki plat-
4See for instance the European Union use of chien drogue
while French legislation calls them chien renifleur.
5For example, in Italy an elezione suppletiva is commonly
held whenever an elected deputy or senator either resigns or
dies. In Germany in such cases the first non-elected candidate
is called to parliament. Ersatzwahlen are a rare phenomenon,
foreseen in some very specific cases.
937
form (Mangeot et al, 2003; Se?rasset, 2004).
This paper details the way the Jibiki platform is
used in order to cope with a new dictionary struc-
ture. The platform provides dictionary access and
edition services without any new and specific de-
velopment.
After a brief overview of the Jibiki platform, we
describe the choices made by the LexALP team for
the structure and organisation of their term bank.
Then, we show how this structure is described us-
ing Jibiki metadata description languages. Finally,
we give some details on the resulting LexALP In-
formation System.
2 Jibiki, The Papillon Dictionary
Development Platform
2.1 Overview
The Jibiki platform has been designed to support
the collaborative development of multilingual dic-
tionaries. This platform is used as the basis of the
Papillon project web site6.
This platform offers several services to its users:
? access to many different dictionaries from a
single easy to use query form,
? advance search for particular dictionary en-
tries through an advanced search form,
? creation and edition of dictionary entries.
What makes the Jibiki platform quite unique is
the fact that it provides these services regardless of
the dictionary structure. In other words it may be
used by any dictionary builder to give access and
collaboratively edit any dictionary, provided that
the resulting dictionary will be freely accessible
online.
2.2 Jibiki Platform Architecture
The Jibiki platform is a framework used to set up
a web server dedicated to the collaborative devel-
opment of multilingual dictionaries. All services
provided by the platform are organised as classi-
cal 3-tier architectures with a presentation layer
(in charge of the interface with users), a business
layer (which provides the services per se) and a
data layer (in charge of the storage of persistent
data).
In order to adapt the Jibiki platform to a new
dictionary, the dictionary manager does not have
6http://www.papillon-dictionary.org/
Papillon Application (java + enhydra
presentation
layer
serveur
HTTP
(apache)
Relational database
(PostgreSQL)
XML-UTF8
HTML
CSS
javascript
+
CGI
WML
xhtml
chtml
business layer data layer
J
D
B
C
Lexie
axie
Dico
Historique
Utilisateur
...
Data
validation
Mailing list
archive
Users/Groups
Contributions
management
Volume
Information
sharing
requests
management
Information
Message
Figure 1: The Jibiki platform general architecture
to write specific java code nor specific dynamic
web pages. The only necessary information used
by the platform consists in:
? a description of the dictionary volumes and
their relations,
? a mapping between the envisaged dictionary
structure and a simple hypothetical dictionary
structure (called CDM)7,
? the definition of the XML structure of each
envisaged dictionary volume by way of XML
schemas,
? the development of a specific edition in-
terface as a standard xhtml form (that can
be adapted from an automatically generated
draft).
3 The LexALP Terminology Structure
3.1 Overview
The objective of the LexALP project is to com-
pare the specialised terminology of six different
national legal systems and three supranational sys-
tems in four different languages, and to harmonise
it, thus optimising communication between the
Alpine states at supranational level. To achieve
this objective, the terminology of the Alpine Con-
vention is described and compared to the equiva-
lent terms used in national legislation. The result-
ing terminology entries feed a specific term bank
that will support the harmonisation work.
As the project deals with legal terms, which re-
fer to concepts that are proper of the considered
national law or international convention, equiva-
lence problems are the norm, given that concepts
are not ?stable? between the different national leg-
islations. Standard terminology techniques for
other fields can not be applied to the field of law,
where the standardisation approach (Felber, 1987;
7This mapping is sufficient for simple dictionary access
938
Felber, 1994) is not applicable. For this, we chose
to use ?acceptions? as they are defined in the Pa-
pillon dictionary (Se?rasset, 1994) to represent the
equivalence links between concepts of the differ-
ent legal systems (Arntz, 1993).
Italian
Slovene
German
French
inneralpiner Verkehr
znotrajalpski promet
transport intra-alpin
circulation intra-alpine
trafic intra-alpin
traffico intraalpino
trasporto intraalpino
Figure 2: An Alpine Convention concept in four
languages
The example given in figure 2 shows a concept
defined in the Alpine Convention. This concept
has the same definition in the four languages of
the Alpine Convention but is expressed by differ-
ent denominations. The Alpine Convention also
uses the terms ?circulation intra-alpine? or ?trans-
port intra-alpin? which are identified as synonyms
by the terminologist.
This illustrates the first goal of the LexALP
project. In different texts, the same concept may
be realised by different terms in the same lan-
guage. This may lead to inefficient communica-
tion. Hence, a single term has to be determined
as part of a harmonised quadruplet of transla-
tion equivalents. The other denominations will be
represented in the term bank as non-harmonised
synonyms in order to direct drafting and translat-
ing within the Alpine Convention towards a more
clear and consistent terminology use for interlin-
gual and supranational communication.
In this example, the lexicographers and jurists
did not identify any existing concept in the differ-
ent national laws that could be considered close
enough to the concept analysed. This is coherent
with the minutes from the French National Assem-
bly which clearly states that the term ?trafic intra-
alpin? (among others) should be clarified by a dec-
laration to be added to the Alpine Convention.
Figure 3 shows an analogous quadrilingual ex-
ample where the Alpine Convention concept may
be related to a legal term defined in the French
laws. In this example the French term is distin-
guished from the Alpine Convention terms, be-
cause these concepts belong to different legal sys-
Italian
Slovene
German
French
principio di precauzione
Vorsorgeprinzip
nacelo preventive
principe de pr?caution
principe de pr?caution
Figure 3: A quadrilingual term extracted from the
Alpine Convention with reference to its equivalent
at French national level
tems (and are not identically defined in them).
Hence, the terminologists created distinct accep-
tions, one for each concept. These acceptions are
related by a translation link.
This illustrates the second goal of the project,
which is to help with the fine comprehension of the
Alpine Convention and with the detailed knowl-
edge necessary to evaluate the implementation and
implementability of the convention in the different
legal systems.
As a by-product of the project, one can see that
there is an indirect relation between concepts from
different national legal systems (by way of their
respective relation to the concepts of the Alpine
Convention). However, establishing these indi-
rect relations is not one of the main objectives of
the LexALP project and would require more direct
contrastive analysis.
3.2 Macro- and Micro- Structures
The LexALP term bank consists in 5 volumes
(for French, German, Italian, Slovene and English)
containing all term descriptions (grammatical in-
formation, definition, contexts etc.). The transla-
tion links are established through a central accep-
tion volume. Figure 2 and 3 show examples of
terms extracted from the Alpine Convention, syn-
onymy links in the French and Italian volumes,
as well as inter-lingual relations by way of accep-
tions.
All language volumes share the same mi-
crostructure. This structure is stored in XML.
Figure 4 shows the xml structure of the French
term ?trafic intra-alpin?, as defined in the Alpine
Convention. The term entry is associated to a
unique identifier used to establish relations be-
tween volume entries. Each term entry belongs
to one (and only one) legal system. The exam-
ple term belongs to the Alpine Convention legal
939
<entry id="fra.trafic_intra-alpin.1010743.e"
lang="fra"
legalSystem="AC"
process_status="FINALISED"
status="HARMONISED">
<term>trafic intra-alpin</term>
<grammar>n.m.</grammar>
<domain>Transport</domain>
<usage frequency="common"
geographical-code="INT"
technical="false"/>
<relatedTerm isHarmonised="false"
relationToTerm="Synonym"
termref="">
transport intra-alpin
</relatedTerm>
<relatedTerm isHarmonised="false"
relationToTerm="Synonym"
termref="">
circulation intra-alpine
</relatedTerm>
<definition>
[T]rafic constitue? de trajets ayant leur
point de de?part et/ou d?arrive?e a` l?inte?-
rieur de l?espace alpin.
</definition>
<source url="">Prot. Transp., art. 2</source>
<context url="http://www...">
Des projets routiers a` grand de?bit pour
le trafic intra-alpin peuvent e?tre re?alise?s,
si [...].
</context>
</entry>
Figure 4: XML form of the term ?trafic intra-
alpin?.
system8 (code AC). The set of known legal sys-
tems includes of course countries belonging to the
Alpine Space (Austria, France, Germany, Italy,
Slovenia and Switzerland9) but also international
treaties or conventions. The entry also bears the
information on its status (harmonised or rejected)
and its process status (to be processed, provision-
ally processed or finalised).
The term itself and its part of speech is also
given, with the general domain to which the term
belongs, along with some usage notes. In these us-
age notes, the attribute geographical-code
allows for discrimination between terms defined
in national (or federal) laws and terms defined in
regional laws as in some of the countries involved
legislative power is distributed at different levels.
Then the term may be related to other terms.
These relations may lead to simple strings of
texts (as in the given example) or to autonomous
term entries in the dictionary by the use of the
termref attribute. The relation itself is specified
in the relationToTerm attribute. The current
schema allows for the representation of relations
8Strictly speaking, the Alpine Convention does not con-
stitute a legal system per se.
9Also Liechtenstein and Monaco are parties to the Alpine
Convention, however, their legal systems are not terminolog-
ically processed within LexALP.
between concepts (synonymy, hyponymy and hy-
peronymy), as well as relations between graphies
(variant, abbreviation, acronym, etc.).
Then, a definition and a context may be given.
Both should be extracted from legal texts, which
must be identified in the source field.
An interlingual acception (or axie) is a place
holder for relations. Each interlingual acception
may be linked to several term entries in the lan-
guage volumes through termref elements and
to other interlingual acceptions through axieref
elements, as illustrated in figure 5.
<axie id="axi..1011424.e">
<termref
idref="ita.traffico_intraalpino.1010654.e"
lang="ita"/>
<termref
idref="fra.trafic_intra-alpin.1010743.e"
lang="fra"/>
<termref
idref="deu.inneralpiner_Verkehr.1011065.e"
lang="deu"/>
<termref
idref="slo.znotrajalpski_promet.1011132.e"
lang="slo"/>
<axieref idref=""/>
<misc></misc>
</axie>
Figure 5: XML form of the interlingual acception
illustated in figure 2.
4 LexALP Information System
4.1 Overview
Building such a term bank can only be envisaged
as a collaborative work involving terminologists,
translators and legal experts from all the involved
countries. Hence, the LexALP consortium has set
up a centralised information system that is used to
gather all textual and terminological data.
This information system is organized in two
main parts. The first one is dedicated to corpus
management. It allows the users to upload legal
texts that will serve to bootstrap the terminology
work (by way of candidate term extraction) and
to let terminologists find occurrences of the term
they are working on, in order for them to provide
definitions or contexts.
The second part is dedicated to terminology
work per se. It has been developed with the Jibiki
platform described in section 2. In this section, we
show the LexALP Information System functional-
ity, along with the metadata required to implement
it with Jibiki.
940
4.2 Dictionary Browsing
The first main service consists in browsing the cur-
rently developed dictionary. It consists in two dif-
ferent query interfaces (see figures 6 and 7) and a
unique result presentation interface (see figure 10).
Figure 6: Simple search interface present on all
pages of the LexALP Information System
<dictionary-metadata
[...]
d:category="multilingual"
d:fullname="LexALP multilingual Term Base"
d:name="LexALP"
d:owner="LexALP consortium"
d:type="pivot">
<languages>
<source-language d:lang="deu"/>
<source-language d:lang="fra"/>
<target-language d:lang="deu"/>
<target-language d:lang="fra"/>
[...]
</languages>
[...]
<volumes>
<volume-metadata-ref name="LexALP_fra"
source-language="fra"
xlink:href="LexALP_fra-metadata.xml"/>
<volume-metadata-ref name="LexALP_deu"
source-language="deu"
xlink:href="LexALP_deu-metadata.xml"/>
[...]
<volume-metadata-ref name="LexALP_axi"
source-language="axi"
xlink:href="LexALP_axi-metadata.xml"/>
</volumes>
<xsl-stylesheet name="LexALP" default="true"
xlink:href="LexALP-view.xsl"/>
<xsl-stylesheet name="short-list"
xlink:href="short-list-view.xsl"/>
</dictionary-metadata>
Figure 8: Excerpt of the dictionary descriptor
In the provided examples, the user of the sys-
tem specifies an entry (a term), or part of it, and
a language in which the search is to be done. The
expected behaviour may only be achieved if :
? the system knows in which volume the search
is to be performed,
? the system knows where, in the volume entry,
the headword is to be found,
? the system is able to produce a presentation
for the retrieved XML structures.
However, as the Jibiki platform is entirely in-
dependent of the underlying dictionary structure
<volume-metadata
[...]
dbname="lexalpfra"
dictname="LexALP"
name="LexALP_fra"
source-language="fra">
<cdm-elements>
<cdm-entry-id index="true"
xpath="/volume/entry/@id"/>
<cdm-headword d:lang="fra" index="true"
xpath="/volume/entry/term/text()"/>
<cdm-pos d:lang="fra" index="true"
xpath="/volume/entry/grammar/text()"/>
[...]
</cdm-elements>
<xmlschema-ref xlink:href="lexalp.xsd"/>
<template-entry-ref
xlink:href="lexalp_fra-template.xml"/>
<template-interface-ref
xlink:href="lexalp-interface.xhtml"/>
</volume-metadata>
Figure 9: Excerpt of a volume descriptor
(which makes it highly adaptable), the expected
result may only be achieved if additional metadata
is added to the system.
These pieces of information are to be found in
the mandatory dictionary descriptor. It consists
in a structure defined in the Dictionary Metadata
Language (DML), as set of metadata structures
and a specific XML namespace defined in (Man-
geot, 2001).
Figure 8 gives an excerpt of this descriptor. The
metadata first identify the dictionary by giving it
a name and a type. In this example the dictionary
is a pivot dictionary (DML also defines monolin-
gual and bilingual dictionary types). The descrip-
tor also defines the set of source and target lan-
guages. Finally, the dictionary is defined as a set
of volumes, each volume being described in an-
other file. As the LexALP dictionary is a pivot
dictionary, there should be a volume for the artifi-
cial language axi, which is the pivot volume.
Figure 9 shows an excerpt of the description of
the French volume of the LexALP dictionary. Af-
ter specifying the name of the dictionary, the de-
scriptor provides a set of cdm-elements. These el-
ements are used to identify standard dictionary el-
ements (that can be found in several dictionaries)
in the specific dictionary structure. For instance,
the descriptor tells the system that the headword of
the dictionary (cdm-headword) is to be found
by applying the specified xpath10 to the dictionary
structure.
With this set of metadata, the system knows
that:
10an xpath is a standard way to extract a sub-part of any
XML structure
941
Figure 7: Advanced search interface
? requests on French should be directed to the
LexALP fra volume,
? the requested headword will be found in the
text of the term element of the volume
entry element,
Hence, the system can easily perform a request
and retrieve the desired XML entries. The only
remaining step is to produce a presentation for
the user, based on the retrieved entries. This is
achieved by way of a xsl11 stylesheet. This
stylesheet is specified either on the dictionary level
(for common presentations) or on the volume level
(for volume specific presentation).
In the given example, the dictionary adminis-
trator provided two presentations called LexALP
(the default one, as shown in figure 10) and
short-list, both of them defined in the dic-
tionary descriptor.
This mechanism allows for the definition of pre-
sentation outputs in xhtml (for online browsing)
or for presentation output in pdf (for dictionary
export and print).
4.3 Dictionary Edition
The second main service provided by the Jibiki
platform is to allow terminologists to collabora-
tively develop the envisaged dictionary. In this
sense, Jibiki is quite unique as it federates, on the
very same platform the construction and diffusion
of a structured dictionary.
As before, Jibiki may be used to edit any dictio-
nary. Hence, it needs some metadata information
in order to work:
? the complete definition of the dictionary entry
structures by way of an XML schema,
? a template describing an empty entry struc-
ture,
11XSL is a standard way to transform an XML structure
into another structure (XML or not).
Current XML 
structure
Empty 
XHTML form
Instanciate Form
Instanciated 
XHTML form
Online edition
Network
CGI decoding
Figure 11: Basic flow chart of the editing service
? a xhtml form used to edit a dictionary entry
structure (which can be adapted from an au-
tomatically generated one).
When this information is known, the Jibiki plat-
form provides a specific web page to edit a dictio-
nary entry structure. As shown in figure 11, the
XML structure is projected into the given empty
XHTML form. This form is served as a standard
web page on the client browser. After manual edit-
ing, the resulting form is sent back to the Jibiki
platform as CGI12 data. The Jibiki platform de-
codes this data and modifies the edited XML struc-
ture accordingly. Then the process iterates as long
as necessary. Figure 12 shows an example of such
a dynamically created web page.
After each update, the resulting XML structure
is stored in the dictionary database. However, it
is not available to other users until it is marked as
finished by the contributor (by clicking on the
save button). If the contributor leaves the web
page without saving the entry, he will be able to
retrieve it and finish his contribution later.
12Common Gateway Interface
942
Figure 10: Query result presentation interface
Figure 12: Edition interface of a LexALP French entry
943
At each step of the contribution (after each up-
date) and at each step of dictionary editing (after
each save), the previous state is saved and the con-
tributor (or the dictionary administrator) is able to
browse the history of changes and to revert the en-
try to a previous version.
5 Conclusion
In this article we give some details on the way the
Jibiki platform allows the diffusion and the online
editing of a dictionary, regardless of his structure
(monolingual, bilingual (directed or not) or multi-
lingual (multi-bilingual or pivot based)).
Initially developed to support the editing of the
Papillon multilingual dictionary13, the Jibiki plat-
form proved useful for the development of other
very different dictionaries. It is currently used for
the development of the GDEF (Grand Dictionnaire
Estonien-Franc?ais) project14 an Estonian French
bilingual dictionary. This article also shows the
use of the platform for the development of a Eu-
ropean term bank for legal terms on spatial plan-
ning and sustainable development in the LexALP
project.
Adapting the Jibiki platform to a new dictio-
nary requires the definition of several metadata in-
formation, taking the form of several XML files.
While not trivial, this metadata definition does not
require any competence in computer development.
This adaptation may therefore also be done by ex-
perimented linguists. Moreover, when the dictio-
nary microstructure needs to evolve, this evolu-
tion does not require any programming. Hence the
Jibiki platform gives linguists great liberty in their
decisions.
Another positive aspect of Jibiki is that it inte-
grates diffusion and editing services on the same
platform. This allows for a tighter collaboration
between linguists and users and also allows for the
involvement of motivated users to the editing pro-
cess.
The Jibiki platform is freely available for use by
any willing team of lexicographer/terminologists,
provided that the resulting dictionary data will be
freely available for online browsing.
In this article, we also presented the choices
made by the LexALP consortium to structure a
term bank used for the description and harmonisa-
tion of legal terms in the domain of spacial plan-
13http://www.papillon-dictionary.org/
14http://estfra.ee/
ning and sustainable development of the Alpine
Space. In such a domain, classical techniques
used in multilingual terminology cannot be used
as the term cannot be defined by reference to a sta-
ble/shared semantic level (each country having its
own set of non-equivalent legal concepts).
References
Reiner Arntz. 1993. Terminological equivalence
and translation. In H. Sonneveld and K. Loen-
ing, editors, Terminology. Applications in Interdisci-
plinary Communication, pages 5?19. Amsterdam et
Philadelphia, John Benjamins Publishing Company.
Helmut Felber, 1987. Manuel de terminologie. UN-
ESCO, Paris.
Helmut Felber. 1994. Terminology research: Its rela-
tion to the theory of science. ALFA, 8(7):163?172.
Mathieu Mangeot, Gilles Se?rasset, and Mathieu
Lafourcade. 2003. Construction collaborative d?une
base lexicale multilingue, le projet Papillon. TAL,
44(2):151?176.
Mathieu Mangeot. 2001. Environnements centralise?s
et distribue?s pour lexicographes et lexicologues en
contexte multilingue. The`se de nouveau doctorat,
spe?cialite? informatique, Universite? Joseph Fourier
Grenoble I, Septembre.
Gilles Se?rasset. 1994. Interlingual lexical organi-
sation for multilingual lexical databases in nadia.
In Makoto Nagao, editor, COLING-94, volume 1,
pages 278?282, August.
Gilles Se?rasset. 2004. A generic collaborative plat-
form for multilingual lexical database development.
In Gilles Se?rasset, editor, COLING 2004 Multilin-
gual Linguistic Resources, pages 73?79, Geneva,
Switzerland, August 28. COLING.
Oliver Streiter, Leonhard Voltmer, Isabella Ties, and
Natascia Ralli. 2004. BISTRO, the online plat-
form for terminology management: structuring ter-
minology without entry structures. In The transla-
tion of domain specific languages and multilingual
terminology, number 3 in Linguistica Antverpien-
sia New Series. Hoger Instituut voor Vertalers en
Tolken, Hogeschool Antwerpen.
Oliver Streiter, Leonhard Voltmer, Isabella Ties, Natas-
cia Ralli, and Verena Lyding. 2006. BISTRO: Data
structure, term tools and interface. Terminology Sci-
ence and Research, 16.
944
The PAPILLON project: cooperatively building a multilingual lexical
data-base to derive open source dictionaries & lexicons
Christian BOITET(1), Mathieu MANGEOT(2) & Gilles S?RASSET(1)
 (1) GETA, CLIPS, IMAG
385, av. de la biblioth?que, BP 53
F-38041 Grenoble cedex 9, France
Christian.Boitet@imag.fr
(2) National Institute of Informatics (NII)
2-1-2-1314, Hitotsubashi
Chiyoda-ku Tokyo 101-8430, Japan
Mathieu.Mangeot@imag.fr
Abstract
The PAPILLON project aims at creating a cooperative, free, permanent, web-oriented and personalizable environment for the
development and the consultation of a multilingual lexical database. The initial motivation is the lack of dictionaries, both for
humans and machines, between French and many Asian languages. In particular, although there are large F-J paper usage
dictionaries, they are usable only by Japanese literates, as they never contain both original (kanji/kana) and romaji writing.
This applies as well to Thai, Vietnamese, Lao, etc.
Introduction
The project was initiated in 2000 and launched
with the support of the French Embassy and NII
(Tokyo) in July 2000, and took really shape in
2001, with a technical seminar in July 2001 at
Grenoble, and concrete work (data gathering,
tool building, etc.).
The macrostructure of Papillon is a set of
monolingual dictionaries (one for each language)
of word senses, called "lexies", linked through a
central set of interlingual links, called "axies".
This pivot macrostructure has been defined by
S?rasset (1994) and experimented by Blanc
(1994) in the PARAX mockup.
The microstructure of the monolingual
dictionaries is the "DiCo" structure, which is a
simplification of Mel'tchuk's (1981;1987;1995)
DEC (Explanatory and Combinatory
Dictionary) designed by Polgu?re (2000) &
Mel'tchuk !"# make it possible to construct
large, detailed and principled dictionaries in
tractable time.
1. Languages included in the project
In 2000, the initial languages of the Papillon
project were English, French, Japanese and Thai.
Thai was included because there had been a
successful project, SAIKAM (Ampornaramveth
et al 1998; 2000), supported by NII and
NECTEC, of building a Japanese-Thai lexicon by
volunteers on the web. Lao, Vietnamese, and
Malay have been added in 2001 because of active
interest of labs and individuals.
The star-like macrostructure of Papillon makes
it easy to add a new language. Also, the DiCo
microstructure of each monolingual dictionary is
defined by an XML schema, containing a large
common core and a small specialization part
(morphosyntactic categories, language usage).
2. Interlingual links
Axies, also called "interlingual acceptions", are
not concepts, but simply interlingual links
between lexies, motivated by translations found
in existing dictionaries or proposed by the
contributors.
In case of discrepancies, 1 axie may be linked
with lexies of some languages only, e.g.
FR(mur#1), EN(wall#1), RU(stena#1), and t o
other axies by refinement links:
Example:
Axie#234 --lg--> FR(mur#1),
EN(wall#1), RU(stena#1)
--rf--> Axie#235, Axie#236
Axie#235 --lg--> DE(Wand#2),
IT(muro#1), ES(muro#1)
Axie#236 --lg--> DE(Mauer#2),
IT(parete#1), ES(pared#1)
It is also possible to have 2 axies for the same
"concept" at a certain stage in the life of the
database, because the monolingual information is
not yet detailed enough.
Suppose the level of language (usual, specialized,
vulgar, familiar?) is not yet given for
FR(maladie#1), FR(affection#2), EN(disease#1),
EN(affection#3).
Then we might have, for translational reasons:
Axie#456
--lg-->
FR(maladie#1),
EN(disease#1)
Axie#457
--lg-->
FR(affection#2),
EN(affection#3)
When this information will be put in each of the
above 4 monolingual entries, we may merge the
2 axies and get:
Axie#500
--lg-->
FR(maladie#1, affection#2),
EN(disease#1, affection#3)
Axies may also be linked to "external" systems
of semantic description. Each axie contains a
(possibly empty) list for each such system, and
the list of systems is open. The following are
included at this stage; UNL UWs (universal
words), ONTOS concepts, WordNet synsets,
NTT semantic categories.
3. Building the content
3.1. Recuperating existing resources
Building the content of the data base has several
aspects. To initiate it, the project starts from
open source computerized data, called "raw
dictionaries", which may be monolingual (4,000
French DiCo entries from UdM, 10,000 Thai
entries from Kasetsart Univ.), bilingual (70,000
Japanese-English entries and 10,000 Japanese-
French entries in J.Breen's JDICT XML format,
8000 Japanese-Thai entries in SAIKAM XML
format, 120,000 English-Japanese entries in
KDD-KATE LISP format), or multilingual
(50,000 French-English-Malay entries in FEM
XML format).
3.2. Integrating the data into Papillon
In the second phase, the "raw dictionaries" are
transformed into a "lexical soup" in
M.Mangeot's (2001) intermediary DML format
(an XML schema and namespace). The
transformation into almost empty DiCo entries
and the creation of axies for the translational
equivalences is semi-automatic. A tool has been
programmed at NII for that task.
3.3. Enriching the data with contributions
After that, it is hoped that many contributors
will "fill in" the missing information. The basis
for that third and continuous phase is a server
for cooperative building of the data base, where
each contributor has his/her own space, so that
contributions can be validated and integrated
into the DB by a group experts. Users can
establish user groups with specific read and write
access rights on their spaces.
4. Consultation of the resulting data
4.1. Online consultation
Consultation is meant to be free for anybody,
and open source. Requests produce personalizable
views of the data base, the most classical of
which are fragments of bilingual dictionaries.
However, it is also possible to produce
multitarget entries, on the fly and offline. Users
(consumers) are encouraged to become
contributors. To contribute, one may propose a
new word sense, a definition, an example of use,
a translation, the translation of an example, a
correction, etc., or an annotation on any
accessible information: Every user can
contribute with his own knowledge level.
4.2. Download of entire files
Users can also retrieve files, and can contribute
to define new output formats. The files retrieved
can contain structural, content-oriented tags.
This open source orientation contrasts with the
current usage of allowing users to retrieve files
containing only presentation-oriented tags.
4.3. Coverage of the dictionary
An interesting point is that the project wants t o
cover both general terms and terminological
terms.
Another one is that it contains a translation
subproject, because definitions, examples,
citations, etc. have to be translated into all
languages of the collection. For this, the notion
of complex lexie, already present to account for
lexical collocations such as compound predicates
(e.g. "to kick the bucket"), is extended to cover
full sentences. Axies relating them are special
because they can't in general relate them t o
external semantic systems such as WordNet. An
exception is UNL: the UNL list for an axie may
contain one UNL graph, produced automatically,
manually, or semi-automatically.  This graph
may be automatically sent to available UNL
"deconverters" to get draft translations.
5. Project organisation
In the current stage, the project has no legal
implementation as a fundation, association,
company, etc., although many participants have
already established official MOUs and other
types of agreements on which to base their
cooperative work.
There is a steering committee of about 10-12
members, who represent Papillon where they
are, and not the converse. There is a set of
tasks, and for each task a working group and an
advisory committee. One of the tasks is the
management of the project. In between, there is
a coordinating group containing the heads of the
tasks and chaired by the head of the
management task.
Sponsors may not donate money to the project,
which has no bank account. Rather, they are
encouraged to donate data, to assign personal
part time to the project, and to fund
participating organizations and persons as they
see fit.
Conclusion
The theoretical frameworks for the whole
database, the macrostructure and the
microstructure are very well defined. I t
constitutes a solid basis for the implementation.
A lot of open problems still have to be addressed
for the Papillon project to be a success. In this
respect, the Papillon project appears to be a
very interesting experimentation platform for a
lot of NLP research as data acquisition or human
access to lexical data, among others.
All these research will improve the attraction of
such a project to the Internet users. This
attraction is necessary for the project to go on,
as it is highly dependent on its users
motivations.
This way, we will be able to provide a very
interesting multilingual lexical database that we
hope useful for a lot of persons.
Rerefences
Ampornaramveth V., Aizawa A. & Oyama K. (2000)
An Internet-based Collaborative Dictionary
Development Project: SAIKAM. Proc. of 7th Intl.
Workshop on Academic Information Networks and
Systems (WAINS'7), Bangkok, 7-8 December 2000,
Kasetsart University.
Blanc ?., S?rasset G. & Tch?ou F. (1994) Designing
an Acception-Based Multilingual Lexical Data Base
under HyperCard: PARAX. Research Report, GETA,
IMAG (UJF & CNRS), Aug. 1994, 10 p.
Connolly, Dan (1997) XML Principles, Tools and
Techniques World Wide Web Journal, Volume 2,
Issue 4, Fall 1997, O'REILLY & Associates, 250 p.
Ide, N. & Veronis, J. (1995) Text Encoding Initiative,
background and context. Kluwer Academic
Publishers, 242 p.
Mangeot-Lerebours M. (2000) Papillon Lexical
Database Project: Monolingual Dictionaries &
Interlingual Links. Proc. of 7th Workshop on
Advanced Information Network and System Pacific
Association for Computational Linguistics 1997
Conference (WAINS'7), Bangkok, Thailande, 7-8
d?cembre 2000, Kasetsart University, 6 p.
Mangeot-Lerebours M. (2001) Environnements
centralis?s et distribu?s pour lexicographes et
lexicologues en contexte multilingue. Nouvelle th?se,
Universit? Joseph Fourier (Grenoble I), 27 September
2001, 280 p.
Mel?tchuk I., Clas A. & Polgu?re A. (1995)
Introduction ? la lexicologie explicative et
combinatoire. AUPELF-UREF/Duculot, Louvain-la-
Neuve, 256 p.
Polgu?re, A. (2000) Towards a theoretically-motivated
general public dictionary of semantic derivations
and collocations for French. Proc. EURALEX'2000,
Stuttgart, pp 517-527.
S?rasset G. (1994a) Interlingual Lexical Organisation
for Multilingual Lexical Databases. Proc. of 15th
International Conference on Computational
Linguistics, COLING-94, 5-9 Aug. 1994, 6 p.
S?rasset G. (1994b) SUBLIM, un syst?me universel de
bases lexicales multilingues; et NADIA, sa
sp?cialisation aux bases lexicales interlingues par
acceptions. Nouvelle th?se, UJF (Grenoble 1), d?c.
1994.
S?rasset G. (1997) Le projet NADIA-DEC : vers un
dictionnaire explicatif et combinatoire informatis? ?
Proc. of La m?moire des mots, 5?me journ?es
scientifiques du r?seau LTT, Tunis, 25-27 septembre
1997, AUPELF?UREF, 7 p.
S?rasset G. & Mangeot-Lerebours M. (2001) Papillon
Lexical Database Project: Monolingual Dictionaries
& Interlingual Links. Proc. NLPRS'2001,
Hitotsubashi Memorial Hall, National Center of
Sciences, Tokyo, Japan, 27-30 November 2001, vol
1/1, pp. 119-125.
Tomokiyo M., Mangeot-Lerebours M. & Planas E.
(2000) Papillon : a Project of Lexical Database for
English, French and Japanese, using Interlingual
Links. Proc. of Journ?es des Sciences et Techniques
de l'Ambassade de France au Japon, Tokyo, Japon,
13-14 novembre 2000, Ambassade de France au
Japon, 3 p.
-o-o-o-o-o-o-o-o-o-
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 232?240, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
GETALP: Propagation of a Lesk Measure through an Ant Colony Algorithm
Didier Schwab, Andon Tchechmedjiev, J?r?me Goulian,
Mohammad Nasiruddin, Gilles S?rasset, Herv? Blanchon
LIG-GETALP
Univ. Grenoble Alpes
http://getalp.imag.fr/WSD
firstname.lastname@imag.fr
Abstract
This article presents the GETALP system for
the participation to SemEval-2013 Task 12,
based on an adaptation of the Lesk measure
propagated through an Ant Colony Algorithm,
that yielded good results on the corpus of Se-
meval 2007 Task 7 (WordNet 2.1) as well as
the trial data for Task 12 SemEval 2013 (Ba-
belNet 1.0). We approach the parameter es-
timation to our algorithm from two perspec-
tives: edogenous estimation where we max-
imised the sum the local Lesk scores; exoge-
nous estimation where we maximised the F1
score on trial data. We proposed three runs
of out system, exogenous estimation with Ba-
belNet 1.1.1 synset id annotations, endoge-
nous estimation with BabelNet 1.1.1 synset id
annotations and endogenous estimation with
WordNet 3.1 sense keys. A bug in our imple-
mentation led to incorrect results and here, we
present an amended version thereof. Our sys-
tem arrived third on this task and a more fine
grained analysis of our results reveals that the
algorithms performs best on general domain
texts with as little named entities as possible.
The presence of many named entities leads the
performance of the system to plummet greatly.
1 Introduction
Out team is mainly interested in Word Sense Disam-
biguation (WSD) based on semantic similarity mea-
sures. This approach to WSD is based on a local
algorithm and a global algorithm. The local algo-
rithm corresponds to a semantic similarity measure
(for example (Wu and Palmer, 1994), (Resnik, 1995)
or (Lesk, 1986)), while the global algorithm propa-
gates the values resulting from these measures at the
level of a text, in order to disambiguate the words
that compose it. For two years, now, our team has
focussed on researching global algorithms. The lo-
cal algorithm we use, a variant of the Lesk algo-
rithm that we have evaluated with several global al-
gorithms (Simulated Annealing (SA), Genetic Al-
gorithms (GA) and Ant Colony Algorithms (ACA))
(Schwab et al, 2012; Schwab et al, 2013), has
shown its robustness with WordNet 3.0. For the
present campaign, we chose to work with an ant
colony based global algorithms that has proven its
efficiency (Schwab et al, 2012; Tchechmedjiev et
al., 2012).
Presently, for this SemEval 2013 Task 12 (Nav-
igli et al, 2013), the objective is to disambiguate a
set of target words (nouns) in a corpus of 13 texts
in 5 Languages (English, French, German, Italian,
Spanish) by providing, for each sense the appropri-
ate sense labels. The evaluation of the answers is
performed by comparing them to a gold standard
annotation of the corpus in all 5 languages using
three possible sense inventories and thus sense tags:
BabelNet 1.1.1 Synset ids (Navigli and Pozetto,
2012), Wikipedia page names and Wordnet sense
keys (Miller, 1995).
Our ant colony algorithm is a stochastic algorithm
that has several parameters that need to be selected
and tuned. Choosing the values of the parameters
based on linguistic criteria remains an open and dif-
ficult problem, which is why we wanted to autom-
atize the parameter search process. There are two
ways to go about this process: exogenous estima-
232
tion, when the parameter values are selected so as
to maximise the F-score on a small training anno-
tated corpus and then used to disambiguate another
corpus (weakly supervised); endogenous estimation,
when the parameters are chosen so as to maximise
the global similarity score on a text or corpus (unsu-
pervised). Our first experiment and system run con-
sists in tuning the parameters on the trial corpus of
the campaign and running the system with the Ba-
belNet sense inventory. Our second and third exper-
iments consist in endogenous parameter estimation,
the first using BabelNet as a sense inventory and the
second using WordNet. Unfortunately, the presence
of an implementation issue prevented us from ob-
taining scores up to par with the potential of our sys-
tem and thus we will present indicative results of the
performance of the system after the implementation
issue was fixed.
2 The GETALP System: Propagation of a
Lesk Measure through an Ant Colony
Algorithm
In this section we will first describe the local al-
gorithm we used, followed by a quick overview of
global algorithms and our own Ant Colony Algo-
rithm.
2.1 The Local Algorithm: a Lesk Measure
Our local algorithm is a variant of the Lesk Algo-
rithm (Lesk, 1986). Proposed more than 25 years
ago, it is simple, only requires a dictionary and no
training. The score given to a sense pair is the num-
ber of common words (space separated strings) in
the definition of the senses, without taking into ac-
count neither the word order in the definitions (bag-
of-words approach), nor any syntactic or morpho-
logical information. Variants of this algorithm are
still today among the best on English-language texts
(Ponzetto and Navigli, 2010).
Our local algorithm exploits the links provided by
WordNet: it considers not only the definition of a
sense but also the definitions of the linked senses
(using all the semantic relations for WordNet, most
of them for BabelNet) following (Banerjee and Ped-
ersen, 2002), henceforth referred asExtLesk1 Con-
1All dictionaries and Java implementations of all algorithms
of our team can be found on our WSD page
trarily to Banerjee, however, we do not consider
the sum of squared sub-string overlaps, but merely
a bag-of-words overlap that allows us to generate
a dictionary from WordNet, where each word con-
tained in any of the word sense definitions is indexed
by a unique integer and where each resulting defini-
tion is sorted. Thus we are able to lower the compu-
tational complexity fromO(mn) toO(m), wherem
and n are the respective length of two definitions and
m ? n. For example for the definition: "Some kind
of evergreen tree", if we say that Some is indexed by
123, kind by 14, evergreen by 34, and tree by 90,
then the indexed representation is {14, 34, 90, 123}.
2.2 Global Algorithm : Ant Colony Algorithm
We will first review the principles pertaining to
global algorithms and then a more detailed account
of our Ant Colony algorithm.
2.2.1 Global algorithms, Global scores and
Configurations
A global algorithm is a method that allows to
propagate a local measure to a whole text in or-
der to assign a sense label to each word. In the
similarity-based WSD perspective, the algorithms
require some fitness measure to evaluate how good
a configuration is. With this in mind, the score
of the selected sense of a word can be expressed
as the sum of the local scores between that sense
and the selected senses of all the other words of a
context. Hence, in order to obtain a fitness value
(global score) for the whole configuration, it is
possible to simply sum the scores for all selected
senses of the words of the context: Score(C) =
?m
i=1
?m
j=iExtLesk(wi,C[i], wj,C[j]).
For a given text, the chosen configuration is
the one which maximizes the global score among
the evaluated ones. The simplest approach is the
exhaustive evaluation of sense combinations (BF),
used for example in (Banerjee and Pedersen, 2002),
that assigns a score to each word sense combination
in a given context (window or whole text) and se-
lects the one with the highest score. The main is-
sue with this approach is that it leads to a combi-
http://getalp.imag.fr/WSD and more specifically for
SemEval 2013 Task 12 on the following page
http://getalp.imag.fr/static/wsd/
GETALP-WSD-ACA/
233
natorial explosion in the length of the context win-
dow or text. The number of combinations is indeed
?|T |
i=1(|s(wi)|), where s(wi) is the set of possible
senses of word i of a text T . For this reason it is
very difficult to use the BF approach on an analy-
sis window larger than a few words. In our work,
we consider the whole text as context. In this per-
spective, we studied several methods to overcome
the combinatorial explosion problem.
2.2.2 Complete and Incomplete Approaches
Several approximation methods can be used in or-
der to overcome the combinatorial explosion issue.
On the one hand, complete approaches try to reduce
dimensionality using pruning techniques and sense
selection heuristics. Some examples include: (Hirst
and St-Onge, 1998), based on lexical chains that re-
strict the possible sense combinations by imposing
constraints on the succession of relations in a taxon-
omy (e.g. WordNet); or (Gelbukh et al, 2005) that
review general pruning techniques for Lesk-based
algorithms; or yet (Brody and Lapata, 2008) who
exploit distributional similarity measures extracted
from corpora (information content).
On the other hand, incomplete approaches gen-
erally use stochastic sampling techniques to reach a
local maximum by exploring as little as necessary
of the search space. Our present work focuses on
such approaches. Furthermore, we can distinguish
two possible variants:
? local neighbourhood-based approaches (new
configurations are created from existing con-
figurations) among which are some approaches
from artificial intelligence such as genetic al-
gorithms or optimization methods such as sim-
ulated annealing;
? constructive approaches (new configurations
are generated by iteratively adding new ele-
ments of solutions to the configuration under
construction), among which are for example
ant colony algorithms.
2.2.3 Principle of our Ant Colony Algorithm
In this section, we briefly describe out Ant Colony
Algorithm so as to give a general idea of how it op-
erates. However, readers are strongly encouraged
to read the detailed papers (Schwab et al, 2012;
Schwab et al, 2013) for a more detailed description
of the system, including examples of how the graph
is built, of how the algorithm operates step by step
as well all pseudo code listing.
Ant colony algorithms (ACA) are inspired from
nature through observations of ant social behavior.
Indeed, these insects have the ability to collectively
find the shortest path between their nest and a source
of food (energy). It has been demonstrated that
cooperation inside an ant colony is self-organised
and allows the colony to solve complex problems.
The environment is usually represented by a graph,
in which virtual ants exploit pheromone trails de-
posited by others, or pseudo-randomly explore the
graph. ACAs are a good alternative for the resolu-
tion of optimization problems that can be encoded
as graphs and allow for a fast and efficient explo-
ration on par with other search heuristics. The main
advantage of ACAs lies in their high adaptivity to
dynamically changing environments. Readers can
refer to (Dorigo and St?tzle, 2004) or (Monmarch?,
2010) for a state of the art.
In this article we use a simple hierarchical graph
(text, sentence, word) that matches the structure of
the text and that exploits no external linguistic infor-
mation. In this graph we distinguish two types of
nodes: nests and plain nodes. Following (Schwab et
al., 2012), each possible word sense is associated to
a nest. Nests produce ants that move in the graph in
order to find energy and bring it back to their mother
nest: the more energy is brought back by ants, the
more ants can be produced by the nest in turn. Ants
carry an odour (a vector) that contains the words of
the definition of the sense of its mother nest. From
the point of view of an ant, a node can be: (1) its
mother nest, where it was born; (2) an enemy nest
that corresponds to another sense of the same word;
(3) a potential friend nest: any other nest; (4) a plain
node: any node that is not a nest. Furthermore, to
each plain node is also associated an odour vector of
a fixed length that is initially empty.
Ant movement is function of the scores given by
the local algorithm, of the presence of energy, of the
passage of other ants (when passing on an edge ants
leave a pheromone trail that evaporates over time)
and of the nodes? odour vectors (ants deposit a part
of their odour on the nodes they go through). When
an ant arrives onto the nest of another word (that cor-
responds to a sense thereof), it can either continue its
234
exploration or, depending on the score between this
nest and its mother nest, decide to build a bridge be-
tween them and to follow it home. Bridges behave
like normal edges except that if at any given time the
concentration of pheromone reaches 0, the bridge
collapses. Depending on the lexical information
present and the structure of the graph, ants will fa-
vor following bridges between more closely related
senses. Thus, the more closely related the senses of
the nests are, the more bridges between them will
contribute to their mutual reinforcement and to the
sharing of resources between them (thus forming
meta-nests); while the bridges between more dis-
tant senses will tend to fade away. We are thus able
to build interpretative paths (possible interpretations
of the text) through emergent behaviour and to sup-
press the need to use a complete graph that includes
all the links between the senses from the start (as is
usually the case with classical graph-based optimi-
sation approaches).
Through the emergence of interpretative paths,
sense pairs that are closer semantically benefit from
an increased ant traffic and thus tend to capture most
of the energy of the system at a faster pace, thus
favouring a faster convergence over an algorithm
that uses a local neighbourhood graph (nodes are
senses interconnected so as to represent all sense
combinations in a context window) without sacrific-
ing the quality of the results.
The selected answers correspond, for each word
to the nest node with the highest energy value. The
reason for this choice over using the pheromone con-
centration is that empirically, the energy level bet-
ter correlates with the actual F1 scores. In turn, the
global Lesk score of a selected sense combination
correlates even better with the F1 score, which is
why, we keep the sense combinations resulting from
each iteration of the algorithm (highest energy nests
at each iteration) and select the one with the highest
global Lesk score as the final solution.
2.3 Parameters
This version of our ant algorithm has seven param-
eters (?, Ea, Emax, E0, ?v, ?, LV ) which have an
influence on the emergent phenomena in the system:
? The maximum amount of energy an ant can
carry, Emax and Ea the amount of energy an
ant can take on a node, influences how much
an ant explores the environment. Ants cannot
go back through an edge they just crossed and
have to make circuits to come back to their nest
(if the ant does not die before that). The size
of the circuits depend on the moment the ants
switch to return mode, hence on Emax.
? The evaporation rate of the pheromone between
cycles (?) is one of the memories of the sys-
tem. The higher the rate is, the least the trails
from previous ants are given importance and
the faster interpretative paths have to be con-
firmed (passed on) by new ants in order not to
be forgotten by the system.
? The initial amount of energy per node (E0)
and the ant life-span (?) influence the number
of ants that can be produced and therefore the
probability of reinforcing less likely paths.
? The odour vector length (Lv) and the propor-
tion of odour components deposited by an ant
on a plain node (?V ) are two dependent param-
eters that influence the global system memory.
The higher the length of the vector, the longer
the memory of the passage of an ant is kept. On
the other hand, the proportion of odour compo-
nents deposited has the opposite effect.
Given the lack of an analytical way of determin-
ing the optimal parameters of the ant colony al-
gorithm, they have to be estimated experimentally,
which is detailed in the following section.
3 Acquisition of Parameter Values
The algorithms we are interested in have a certain
number of parameters that need tuning in order to
obtain the best possible score on the evaluation cor-
pus. There are three possible approaches:
? Make an educated guess about the value ranges
based on a priori knowledge about the dynam-
ics of the algorithm;
? Test manually (or semi-manually) several com-
binations of parameters that appear promising
and determine the influence of making small
adjustments to the values ;
? Use a learning algorithm to automate acquisi-
tion of parameters values. We present that ap-
proach in the following part.
235
3.1 Automated Parameter Estimation
Two methods can be used to automatically acquire
parameters. The first one consists in maximizing
the F-score on an sense-annotated corpus (weak ap-
proach) while the second one consist in maximizing
the global Lesk score (unsupervised approach).
3.1.1 Generalities
Both approaches are based on the same principle
(Tchechmedjiev et al, 2012). We use a simulated
annealing algorithm (Laarhoven and Aarts, 1987)
combined with a non-parametric statistical (Mann-
Whitney-U test (Mann and Whitney, 1947)) test with
a p-value adapted for multiple comparisons through
False Discovery Rate control (FDR) (Benjamini and
Hochberg, 1995). The estimation algorithm oper-
ates on all the parameters of the ant colony algo-
rithm described above and attempts to maximise the
objective function (Global score, F1). The reason
why we need to use a statistical test and FDR rather
than using the standard SA algorithm, is that the
Ant Colony Algorithm is stochastic in nature and
requires tuning to be performed over the distribu-
tion of possible answers for a given set of param-
eter values. Indeed, there is no guarantee that the
value resulting from one execution is representative
at all of the distribution. The exact nature of the dis-
tribution of answers is unknown and thus we take
a sampling of the distribution as precise as can be
afforded. Thus, we require the statistical test to as-
certain the significance between the scores for two
parameter configurations.
3.1.2 Exogenous parameter tuning
If we have a sense-annotated corpus at our dis-
posal, it is possible to directly use the F1 value ob-
tained by the system on this reference to tune the
parameters of the systems so as to maximise said F1
score. The main issues that arise from such meth-
ods are the fact that gold standards are expensive to
produce and that there is no guarantee on the gen-
erality of the contents of the gold standard. Thus,
in languages with little resources we may be un-
able to obtain a gold standard and in the case one
is available, there is a potentially strong risk of over
fitting. Furthermore due to the nature of the train-
ing, taking training samples in a random order for
cross-validation becomes tricky. This is why we also
want to test another method that can tune the pa-
rameters without using labelled examples. For the
evaluation, we estimated parameters on the F1 score
on the test corpus for English and French (the only
ones available). We used the parameters estimated
for English for our English results for our first sys-
tem run GETALP-BN1 and the French parameters
for the results on French, German, Italian, Spanish.
For English we found: ? = 26, Ea =
14, Emax = 3, E0 = 34, ?v = 0.9775, ? =
0.3577, LV = 25.
For French: ? = 19, Ea = 9, Emax = 3, E0 =
32, ?v = 0.9775, ? = 0.3577, LV = 25.
3.1.3 Endogenous parameter tuning
In the context of the evaluation campaign, the ab-
sence of an example gold standard on the same ver-
sion of the resource (synset id mismatch between
BabelNet 1.0 and 1.1.1 2) made dubious the prospect
of using parameters estimated from a gold standard.
Consequently, we set out to investigate the relation
between the F1 score of the gold standard and the
Global Lesk Score of successive solutions through-
out the execution of the algorithm.
We observed that the Lesk score is highly corre-
lated to the F1 score and can be used as an estimator
thereof. The main quality criterion being the dis-
criminativeness of the Lesk score compared to the
F1 score (average ratio between the number of pos-
sible F1 score values for a single Lesk score value),
for which the correlation is a possible indicator. We
make the hypothesis based on the correlation that for
a given specific local measure, the global score will
be an adequate estimator of the F1 score. Our sec-
ond system run GETALP-WSD-BN2 is based on the
endogenous parameter estimation. We will not list
all the parameters here, as there is a different set of
parameters for each text and each language.
3.2 Voting
In previous experiment, as can be expected, we have
observed a consistent rise the F1 score when apply-
ing a majority vote method on the output of several
executions (Schwab et al, 2012). Consequently we
followed the same process here, and for all the runs
of our system we performed 100 executions and ap-
plied a majority vote (For each word, our of all se-
2http://lcl.uniroma1.it/babelnet/
236
lected senses, take the one that has been selected the
most over all the executions) on all 100 answer files.
The result of this process is a single answer file and
comes with the advantage of greatly reducing the
variability of the answers. Say this voting process
is repeated over and over again 100 times, then the
standard deviation of F1 scores around the mean is
much smaller. Thus, we also have a good solutions
to the problem of selecting the answer that yields the
highest score, without actually having access to the
gold standard.
4 Runs for SemEval 2013 task 12
In this section we will describe the various runs we
performed in the context of Task 12. We will first
present our methodologies relating to the BabelNet
tagged gold standard followed by the methodologies
relating to the WordNet tagged gold standard.
4.1 BabelNet Gold Standard Evaluation
In the context of the BabelNet gold standard evalu-
ation, we need to tag the words of the corpus with
BabelNet synset ids. Due to the slow speed of re-
trieving Babel synsets and extracting glosses, espe-
cially in the context of our extended Lesk Approach,
we pre-generate a dictionary for each language that
contains entries for each word of the corpus and then
for each possible sense (as per BabelNet). In the
short time allotted for the competition, we restrict
ourselves to building dictionaries only for the words
of the corpus, but the process described can be ap-
plied to pre-generate a dictionary for the whole of
BabelNet.
Each BabelNet synset for a word is considered as
a possible sense in the dictionary. For each synset
we retrieve the Babel senses and retain the ones that
are in the appropriate language. Then, we retrieve
the Glosses corresponding to each selected sense
and combine them in as the definition correspond-
ing to that particular BabelNet synset. Furthermore,
we also retrieve certain of the related synsets and
repeat the same process so as to add the related def-
initions to the BabelNet synset being considered. In
our experiments on the test corpus, we determined
that what worked best (i.e. English and French)
was to use only relations coming from WordNet, all
the while excluding the r, gdis, gmono relation
added by BabelNet. We observed a similar increase
in disambiguation quality with the Degree (Navigli
and Lapata, 2010) algorithm implementation that
comes with BabelNet. The r relation correspond to
the relations in BabelNet extracted from Wikipedia,
whereas gdis and gmono corresponds to relation
created using a disambiguation algorithm (respec-
tively for monosemous and polysemous words).
4.2 WordNet Gold Standard Evaluation
In the context of the WordNet gold standard evalua-
tion, we initially thought the purpose would be to an-
notate the corpus in all five languages with WordNet
sense keys through alignments extracted from Ba-
belNet. As a consequence, we exploited BabelNet
as a resource, merely obtaining WordNet sense keys
through the main senses expressed in BabelNet, that
correspond to WordNet synsets. Although we were
able to produce annotations for all languages, as it
turns out, the WordNet evaluation was merely aimed
at evaluating monolingual systems that do not sup-
port BabelNet at all. For reference, we subsequently
generated a dictionary from WordNet only, to gauge
the performance of our system on the evaluation as
intended by the organisers.
5 Results
We will first present the general results pertaining
to Task 12, followed by a more detailed analysis on
a text by text basis, as well as the comparison with
results obtained on the Semeval 2007 WSD task in
terms of specific parts of speech.
5.1 General Results for Semeval-2013 Task 12
Important: implementation issue during the
evaluation period During the evaluation period,
we had an implementation issue, where a parameter
that limited the size of definition was not disabled
properly. As a consequence, when we experimented
to determine the appropriate relations to consider
for the context expansion of the glosses, we arrived
at the experimental conclusion that using all rela-
tions worked best. However, since it was already the
case with WordNet (Schwab et al, 2011), we read-
ily accepted that our experimental conclusion was
indeed correct. The issue was indirectly resolved
as an unforeseen side effect of another hot-fix ap-
plied shortly before the start of the evaluation period.
237
Given that we were not aware of the presence of a
limitation on the definition length before the hot-fix,
we performed all the experiments under an incorrect
hypothesis which led us to an incorrect conclusion,
that itself led to the results we obtained for the cam-
paign. Indeed, with no restrictions on the size of the
definition, our official results for this task were con-
sistently inferior to the random baseline across the
board. After a thorough analysis of our runs we ob-
served that the sum of local measures (global lesk
score) that correlated inversely with the gold stan-
dard F1 score, the opposite of what it should have
been. We immediately located and corrected this
bug when we realized what had caused these bad
results that did not correspond at all with what we
obtained on the test corpus. After the fix, we strictly
ran the same experiment without exploiting the gold
standard, so as to obtain the results we would have
obtained had the bug not been present in the first
place.
Run Lang. P R F1 MFS
BN1 EN 58.3 58.3 58.3 65.6
FR 48.3 48.2 48.3 50.1
DE 52.3 52.3 52.3 68.6
ES 57.6 57.6 57.6 64.4
IT 52.6 52.5 52.6 57.2
BN2 EN 56.8 56.8 56.8 65.6
FR 48.3 48.2 48.3 50.1
DE 51.9 51.9 51.9 68.6
ES 57.8 57.8 57.8 64.4
IT 52.8 52.8 52.8 57.2
WN1 EN 51.4 51.4 51.4 63.0
Table 1: Results after fixing the implementation is-
sue for all three of our runs, compared to the Most
Frequent Sense baseline (MFS).
We can see in Table 1, that after the removal of
the implementation issues, the scores become more
competitive and meaningful compared to the other
system, although we remain third of the evalua-
tion campaign. We can observe that there is no
large difference between the exogenous results (us-
ing a small annotated corpus) and endogenous re-
sults. Except for the English corpus where there is
a 2% increase. The endogenous estimation, since it
is performed on a text by text basis is much slower
and resource consuming. Given that the exogenous
estimation offers slightly better results and that it re-
quires very little annotated data, we can conclude
that in most cases the exogenous estimation will be
much faster to obtain.
5.2 A more detailed analysis
In this section we will first make a more detailed
analysis for each text on the English corpus, by look-
ing where our algorithm performed best. We restrict
ourselves on one language for this analysis for the
sake of brevity. As we can see in Table 2, the re-
sults can vary greatly depending on the text (within
a twofold range). The system consistently performs
better on texts from the general domain (T 4, 6, 10),
often beating the first sense baseline. For more spe-
cialized texts, however, (T 2, 7, 8, 11, 12, 13) the
algorithm performs notably lower than the baseline.
The one instance where the algorithm truly fails, is
when the text in question contains many ambigu-
ous entities. Indeed for text 7, which is about foot-
ball, many of the instance words to disambiguate are
the names of players and of clubs. Intuitively, this
behaviour is understandable and can be mainly at-
tributed to the local Lesk algorithm. Since we use
glosses from the resource, that mostly remain in the
general domain, a better performance in matching
texts is likely. As for named entities, the Lesk algo-
rithm is mainly meant to capture the similarity be-
tween concepts and it is much more difficult to dif-
ferentiate two football players from a definition over
concepts (often more general).
To further outline the strength of our approach, we
need to look back further at a setting with all parts
of speech being considered, namely Task 7 from Se-
mEval 2007. As can be seen in Table 3, even though
for adjectives and adverbs the system is slightly be-
low the MFS (respectively), it has a good perfor-
mance compared to graph based WSD approaches
that would be hindered by the lack of taxonomical
relations. For verbs the performance is lower as is
consistently observed with WSD algorithms due to
the high degree of polysemy of verbs. For example,
in the case of Degree (Navigli and Pozetto, 2012),
nouns are the part of speech for which the system
performs the best, while the scores for other parts of
speech are somewhat lower. Thus, we can hypoth-
238
Text Descr. Len. F1 MFS Diff.
1 Gen. Env. 228 61.4 68.9 -7.5
2 T. Polit. 84 51.2 66.7 -15.5
3 T. Econ. 84 52.4 56.0 - 3.6
4 News. Gen. 119 58.8 58.0 0.8
5 T. Econ. 74 39.2 36.5 2.7
6 Web Gen. 210 67.1 64.3 2.8
7 T. Sport. 190 34.2 60.5 -26.3
8 Sci. 153 63.4 67.3 -3.9
9 Geo. Econ. 190 63.2 74.2 -11
10 Gen. Law. 160 61.9 61.9 0
11 T. Sport. 125 56.8 64.0 -7.2
12 T. Polit. 185 64.3 73.0 -8.7
13 T. Econ. 130 68.5 72.6 -4.1
Table 2: Text by text F1 scores compared to the
MFS baseline for the English corpus (T.= Trans-
lated, Gen.= General, Env.= Environment, Polit.=
Politics, Econ.= Economics, Web= Internet, Sport.=
Sports, Geo.= Geopolitics, Sci.= Science).
A P.O.S. F1 MFS F1 Diff
1108 Noun 79.42 77.4 +1.99
591 Verb 74.78 75.3 -0.51
362 Adj. 82.66 84.3 -1.59
208 Adv. 86.95 87.5 -0.55
2269 All 79.42 78.9 +0.53
Table 3: Detailed breakdown of F1 score per part
of speech category for Semeval-2007 Task 7, over
results resulting from a vote over 100 executions
esise that using a different local measure depending
on the part of speech may constitute an interesting
development while allowing a return to a more gen-
eral all-words WSD task where all parts of speech
are considered, even when the resource does not of-
fer taxonomical relation for the said parts of speech.
6 Conclusions & Perspectives
In this paper, we present a method based on a
Lesk inspired local algorithm and a global algorithm
based on ant colony optimisation. An endogenous
version (parameter estimation based on the maximi-
sation of the F-score on an annotated corpus) and
an exogenous version (parameter estimation based
on the maximisation of the global Lesk score on
the corpus) of the latter algorithm do not exhibit a
significant difference in terms of the F-score of the
result. After a more detailed analysis on a text by
text basis, we found that the algorithm performs best
on general domain texts with as little named enti-
ties as possible (around or above the MFS baseline).
For texts of more specialized domain the algorithm
consistently performs below the MFS baseline, and
for texts with many named entities, the performance
plummets greatly slightly above the level of a ran-
dom selection. We also show that with our Lesk
measure the system is best suited for WSD in a more
general setting with all parts of speech, however in
the context of just nouns, it is not the most suitable
local measure. As we have seen from the other sys-
tems, graph based local measures may be the appro-
priate answer to reach the level of the best systems
on this task, however it is important not to dismiss
the potential of other approaches. The quality of the
results depend on the global algorithm, however they
are also strongly bounded by the local measure con-
sidered. Our team, is headed towards investigating
local semantic similarity measures and towards ex-
ploiting multilingual features so as to improve the
disambiguation quality.
7 Acknowledgements
The work presented in this paper was conducted in
the context of the Formicae project funded by the
University Grenoble 2 (Universit? Pierre Mend?s
France) and the Videosense project, funded by the
French National Research Agency (ANR) under
its CONTINT 2009 programme (grant ANR-09-
CORD-026).
References
[Banerjee and Pedersen2002] Satanjee Banerjee and Ted
Pedersen. 2002. An adapted lesk algorithm for word
sense disambiguation using wordnet. In CICLing
2002, Mexico City, February.
[Benjamini and Hochberg1995] Yoav Benjamini and
Yosef Hochberg. 1995. Controlling the False Dis-
covery Rate: A Practical and Powerful Approach to
Multiple Testing. Journal of the Royal Statistical
Society. Series B (Methodological), 57(1):289?300.
[Brody and Lapata2008] Samuel Brody and Mirella La-
pata. 2008. Good neighbors make good senses:
Exploiting distributional similarity for unsupervised
239
WSD. In Proceedings of the 22nd International Con-
ference on Computational Linguistics (Coling 2008),
pages 65?72, Manchester, UK.
[Dorigo and St?tzle2004] Dorigo and St?tzle. 2004. Ant
Colony Optimization. MIT-Press.
[Gelbukh et al2005] Alexander Gelbukh, Grigori
Sidorov, and Sang-Yong Han. 2005. On some opti-
mization heuristics for Lesk-like WSD algorithms. In
International Conference on Applications of Natural
Language to Information Systems ? NLDB?05, pages
402?405, Alicante, Spain.
[Hirst and St-Onge1998] G. Hirst and David D. St-Onge.
1998. Lexical chains as representations of context for
the detection and correction of malapropisms. Word-
Net: An electronic Lexical Database. C. Fellbaum. Ed.
MIT Press. Cambridge. MA, pages 305?332. Ed. MIT
Press.
[Laarhoven and Aarts1987] P.J.M. Laarhoven and E.H.L.
Aarts. 1987. Simulated annealing: theory and appli-
cations. Mathematics and its applications. D. Reidel.
[Lesk1986] Michael Lesk. 1986. Automatic sense dis-
ambiguation using mrd: how to tell a pine cone from
an ice cream cone. In Proceedings of SIGDOC ?86,
pages 24?26, New York, NY, USA. ACM.
[Mann and Whitney1947] H. B. Mann and D. R. Whitney.
1947. On a Test of Whether one of Two Random Vari-
ables is Stochastically Larger than the Other. The An-
nals of Mathematical Statistics, 18(1):50?60.
[Miller1995] George A. Miller. 1995. Wordnet: A lexical
database. ACM, Vol. 38(No. 11):p. 1?41.
[Monmarch?2010] N. Monmarch?. 2010. Artificial Ants.
Iste Series. John Wiley & Sons.
[Navigli and Lapata2010] Roberto Navigli and Mirella
Lapata. 2010. An experimental study of graph con-
nectivity for unsupervised word sense disambiguation.
IEEE Trans. Pattern Anal. Mach. Intell., 32:678?692,
April.
[Navigli and Pozetto2012] Roberto Navigli and Si-
mone Paolo Pozetto. 2012. Babelnet: The
automatic construction, evaluation and applica-
tion of a wide-coverage multilingual semantic
network. Artificial Intelligence, 193:217?250.
http://dx.doi.org/10.1016/j.artint.2012.07.004.
[Navigli et al2013] Roberto Navigli, David Jurgens, and
Daniele Vannella. 2013. Semeval-2013 task 12: Mul-
tilingual word sense disambiguation. In Proceedings
of the 7th International Workshop on Semantic Eval-
uation (SemEval 2013), in conjunction with the Sec-
ond Joint Conference on Lexical and Computational
Semantics (*SEM 2013), Atlanta, Georgia, 14-15 June.
[Ponzetto and Navigli2010] Simone Paolo Ponzetto and
Roberto Navigli. 2010. Knowledge-rich word sense
disambiguation rivaling supervised systems. In Pro-
ceedings of the 48th Annual Meeting of the Association
for Computational Linguistics, pages 1522?1531.
[Resnik1995] Philip Resnik. 1995. Using information
content to evaluate semantic similarity in a taxonomy.
In Proceedings of the 14th international joint confer-
ence on Artificial intelligence - Volume 1, IJCAI?95,
pages 448?453, San Francisco, CA, USA. Morgan
Kaufmann Publishers Inc.
[Schwab et al2011] Didier Schwab, J?r?me Goulian, and
Nathan Guillaume. 2011. D?sambigu?sation lexicale
par propagation de mesures semantiques locales par al-
gorithmes a colonies de fourmis. In TALN, Montpel-
lier (France), Juillet.
[Schwab et al2012] Didier Schwab, J?r?me Goulian, An-
don Tchechmedjiev, and Herv? Blanchon. 2012. Ant
colony algorithm for the unsupervised word sense dis-
ambiguation of texts: Comparison and evaluation. In
Proceedings of COLING?2012, Mumbai (India), De-
cember. To be published.
[Schwab et al2013] Didier Schwab, Jer?me Goulian, and
Andon Tchechmedjiev. 2013. Theoretical and empir-
ical comparison of artificial intelligence methods for
unsupervised word sense disambiguation. Int. J. of
Web Engineering and Technology. In Press.
[Tchechmedjiev et al2012] Andon Tchechmedjiev,
J?r?me Goulian, Didier Schwab, and Gilles S?rasset.
2012. Parameter estimation under uncertainty with
simulated annealing applied to an ant colony based
probabilistic wsd algorithm. In Proceedings of
the First International Workshop on Optimization
Techniques for Human Language Technology, pages
109?124, Mumbai, India, December. The COLING
2012 Organizing Committee.
[Wu and Palmer1994] Zhibiao Wu and Martha Palmer.
1994. Verbs semantics and lexical selection. In Pro-
ceedings of the 32nd annual meeting of Association for
Computational Linguistics, ACL ?94, pages 133?138,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
240
