Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 244?252,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
A Non-negative Matrix Tri-factorization Approach to
Sentiment Classification with Lexical Prior Knowledge
Tao Li Yi Zhang
School of Computer Science
Florida International University
{taoli,yzhan004}@cs.fiu.edu
Vikas Sindhwani
Mathematical Sciences
IBM T.J. Watson Research Center
vsindhw@us.ibm.com
Abstract
Sentiment classification refers to the task
of automatically identifying whether a
given piece of text expresses positive or
negative opinion towards a subject at hand.
The proliferation of user-generated web
content such as blogs, discussion forums
and online review sites has made it possi-
ble to perform large-scale mining of pub-
lic opinion. Sentiment modeling is thus
becoming a critical component of market
intelligence and social media technologies
that aim to tap into the collective wis-
dom of crowds. In this paper, we consider
the problem of learning high-quality senti-
ment models with minimal manual super-
vision. We propose a novel approach to
learn from lexical prior knowledge in the
form of domain-independent sentiment-
laden terms, in conjunction with domain-
dependent unlabeled data and a few la-
beled documents. Our model is based on a
constrained non-negative tri-factorization
of the term-document matrix which can
be implemented using simple update rules.
Extensive experimental studies demon-
strate the effectiveness of our approach on
a variety of real-world sentiment predic-
tion tasks.
1 Introduction
Web 2.0 platforms such as blogs, discussion fo-
rums and other such social media have now given
a public voice to every consumer. Recent sur-
veys have estimated that a massive number of in-
ternet users turn to such forums to collect rec-
ommendations for products and services, guid-
ing their own choices and decisions by the opin-
ions that other consumers have publically ex-
pressed. Gleaning insights by monitoring and an-
alyzing large amounts of such user-generated data
is thus becoming a key competitive differentia-
tor for many companies. While tracking brand
perceptions in traditional media is hardly a new
challenge, handling the unprecedented scale of
unstructured user-generated web content requires
new methodologies. These methodologies are
likely to be rooted in natural language processing
and machine learning techniques.
Automatically classifying the sentiment ex-
pressed in a blog around selected topics of interest
is a canonical machine learning task in this dis-
cussion. A standard approach would be to manu-
ally label documents with their sentiment orienta-
tion and then apply off-the-shelf text classification
techniques. However, sentiment is often conveyed
with subtle linguistic mechanisms such as the use
of sarcasm and highly domain-specific contextual
cues. This makes manual annotation of sentiment
time consuming and error-prone, presenting a bot-
tleneck in learning high quality models. Moreover,
products and services of current focus, and asso-
ciated community of bloggers with their idiosyn-
cratic expressions, may rapidly evolve over time
causing models to potentially lose performance
and become stale. This motivates the problem of
learning robust sentiment models from minimal
supervision.
In their seminal work, (Pang et al, 2002)
demonstrated that supervised learning signifi-
cantly outperformed a competing body of work
where hand-crafted dictionaries are used to assign
sentiment labels based on relative frequencies of
positive and negative terms. As observed by (Ng et
al., 2006), most semi-automated dictionary-based
approaches yield unsatisfactory lexicons, with ei-
ther high coverage and low precision or vice versa.
However, the treatment of such dictionaries as
forms of prior knowledge that can be incorporated
in machine learning models is a relatively less ex-
plored topic; even lesser so in conjunction with
semi-supervised models that attempt to utilize un-
244
labeled data. This is the focus of the current paper.
Our models are based on a constrained non-
negative tri-factorization of the term-document
matrix, which can be implemented using simple
update rules. Treated as a set of labeled features,
the sentiment lexicon is incorporated as one set of
constraints that enforce domain-independent prior
knowledge. A second set of constraints introduce
domain-specific supervision via a few document
labels. Together these constraints enable learning
from partial supervision along both dimensions of
the term-document matrix, in what may be viewed
more broadly as a framework for incorporating
dual-supervision in matrix factorization models.
We provide empirical comparisons with several
competing methodologies on four, very different
domains ? blogs discussing enterprise software
products, political blogs discussing US presiden-
tial candidates, amazon.com product reviews and
IMDB movie reviews. Results demonstrate the ef-
fectiveness and generality of our approach.
The rest of the paper is organized as follows.
We begin by discussing related work in Section 2.
Section 3 gives a quick background on Non-
negative Matrix Tri-factorization models. In Sec-
tion 4, we present a constrained model and compu-
tational algorithm for incorporating lexical knowl-
edge in sentiment analysis. In Section 5, we en-
hance this model by introducing document labels
as additional constraints. Section 6 presents an
empirical study on four datasets. Finally, Section 7
concludes this paper.
2 Related Work
We point the reader to a recent book (Pang and
Lee, 2008) for an in-depth survey of literature on
sentiment analysis. In this section, we briskly
cover related work to position our contributions
appropriately in the sentiment analysis and ma-
chine learning literature.
Methods focussing on the use and generation of
dictionaries capturing the sentiment of words have
ranged from manual approaches of developing
domain-dependent lexicons (Das and Chen, 2001)
to semi-automated approaches (Hu and Liu, 2004;
Zhuang et al, 2006; Kim and Hovy, 2004), and
even an almost fully automated approach (Turney,
2002). Most semi-automated approaches have met
with limited success (Ng et al, 2006) and super-
vised learning models have tended to outperform
dictionary-based classification schemes (Pang et
al., 2002). A two-tier scheme (Pang and Lee,
2004) where sentences are first classified as sub-
jective versus objective, and then applying the sen-
timent classifier on only the subjective sentences
further improves performance. Results in these
papers also suggest that using more sophisticated
linguistic models, incorporating parts-of-speech
and n-gram language models, do not improve over
the simple unigram bag-of-words representation.
In keeping with these findings, we also adopt a
unigram text model. A subjectivity classification
phase before our models are applied may further
improve the results reported in this paper, but our
focus is on driving the polarity prediction stage
with minimal manual effort.
In this regard, our model brings two inter-
related but distinct themes from machine learning
to bear on this problem: semi-supervised learn-
ing and learning from labeled features. The goal
of the former theme is to learn from few labeled
examples by making use of unlabeled data, while
the goal of the latter theme is to utilize weak
prior knowledge about term-class affinities (e.g.,
the term ?awful? indicates negative sentiment and
therefore may be considered as a negatively la-
beled feature). Empirical results in this paper
demonstrate that simultaneously attempting both
these goals in a single model leads to improve-
ments over models that focus on a single goal.
(Goldberg and Zhu, 2006) adapt semi-supervised
graph-based methods for sentiment analysis but
do not incorporate lexical prior knowledge in the
form of labeled features. Most work in machine
learning literature on utilizing labeled features has
focused on using them to generate weakly labeled
examples that are then used for standard super-
vised learning: (Schapire et al, 2002) propose one
such framework for boosting logistic regression;
(Wu and Srihari, 2004) build a modified SVM
and (Liu et al, 2004) use a combination of clus-
tering and EM based methods to instantiate simi-
lar frameworks. By contrast, we incorporate lex-
ical knowledge directly as constraints on our ma-
trix factorization model. In recent work, Druck et
al. (Druck et al, 2008) constrain the predictions of
a multinomial logistic regression model on unla-
beled instances in a Generalized Expectation for-
mulation for learning from labeled features. Un-
like their approach which uses only unlabeled in-
stances, our method uses both labeled and unla-
beled documents in conjunction with labeled and
245
unlabeled words.
The matrix tri-factorization models explored in
this paper are closely related to the models pro-
posed recently in (Li et al, 2008; Sindhwani et al,
2008). Though, their techniques for proving algo-
rithm convergence and correctness can be readily
adapted for our models, (Li et al, 2008) do not
incorporate dual supervision as we do. On the
other hand, while (Sindhwani et al, 2008) do in-
corporate dual supervision in a non-linear kernel-
based setting, they do not enforce non-negativity
or orthogonality ? aspects of matrix factorization
models that have shown benefits in prior empirical
studies, see e.g., (Ding et al, 2006).
We also note the very recent work of (Sind-
hwani and Melville, 2008) which proposes a dual-
supervision model for semi-supervised sentiment
analysis. In this model, bipartite graph regulariza-
tion is used to diffuse label information along both
sides of the term-document matrix. Conceptually,
their model implements a co-clustering assump-
tion closely related to Singular Value Decomposi-
tion (see also (Dhillon, 2001; Zha et al, 2001) for
more on this perspective) while our model is based
on Non-negative Matrix Factorization. In another
recent paper (Sandler et al, 2008), standard regu-
larization models are constrained using graphs of
word co-occurences. These are very recently pro-
posed competing methodologies, and we have not
been able to address empirical comparisons with
them in this paper.
Finally, recent efforts have also looked at trans-
fer learning mechanisms for sentiment analysis,
e.g., see (Blitzer et al, 2007). While our focus
is on single-domain learning in this paper, we note
that cross-domain variants of our model can also
be orthogonally developed.
3 Background
3.1 Basic Matrix Factorization Model
Our proposed models are based on non-negative
matrix Tri-factorization (Ding et al, 2006). In
these models, an m? n term-document matrix X
is approximated by three factors that specify soft
membership of terms and documents in one of k-
classes:
X ? FSGT . (1)
where F is an m? k non-negative matrix repre-
senting knowledge in the word space, i.e., i-th row
of F represents the posterior probability of word
i belonging to the k classes, G is an n? k non-
negative matrix representing knowledge in docu-
ment space, i.e., the i-th row of G represents the
posterior probability of document i belonging to
the k classes, and S is an k? k nonnegative matrix
providing a condensed view of X .
The matrix factorization model is similar to
the probabilistic latent semantic indexing (PLSI)
model (Hofmann, 1999). In PLSI, X is treated
as the joint distribution between words and doc-
uments by the scaling X ? X? = X/?i j Xi j thus
?i j X?i j = 1). X? is factorized as
X? ?WSDT ,?
k
Wik = 1,?
k
D jk = 1,?
k
Skk = 1.
(2)
where X is the m ? n word-document seman-
tic matrix, X = WSD, W is the word class-
conditional probability, and D is the document
class-conditional probability and S is the class
probability distribution.
PLSI provides a simultaneous solution for the
word and document class conditional distribu-
tion. Our model provides simultaneous solution
for clustering the rows and the columns of X . To
avoid ambiguity, the orthogonality conditions
FT F = I, GT G = I. (3)
can be imposed to enforce each row of F and G
to possess only one nonzero entry. Approximating
the term-document matrix with a tri-factorization
while imposing non-negativity and orthogonal-
ity constraints gives a principled framework for
simultaneously clustering the rows (words) and
columns (documents) of X . In the context of co-
clustering, these models return excellent empiri-
cal performance, see e.g., (Ding et al, 2006). Our
goal now is to bias these models with constraints
incorporating (a) labels of features (coming from
a domain-independent sentiment lexicon), and (b)
labels of documents for the purposes of domain-
specific adaptation. These enhancements are ad-
dressed in Sections 4 and 5 respectively.
4 Incorporating Lexical Knowledge
We used a sentiment lexicon generated by the
IBM India Research Labs that was developed for
other text mining applications (Ramakrishnan et
al., 2003). It contains 2,968 words that have been
human-labeled as expressing positive or negative
sentiment. In total, there are 1,267 positive (e.g.
?great?) and 1,701 negative (e.g., ?bad?) unique
246
terms after stemming. We eliminated terms that
were ambiguous and dependent on context, such
as ?dear? and ?fine?. It should be noted, that this
list was constructed without a specific domain in
mind; which is further motivation for using train-
ing examples and unlabeled data to learn domain
specific connotations.
Lexical knowledge in the form of the polarity
of terms in this lexicon can be introduced in the
matrix factorization model. By partially specify-
ing term polarities via F , the lexicon influences
the sentiment predictions G over documents.
4.1 Representing Knowledge in Word Space
Let F0 represent prior knowledge about sentiment-laden words in the lexicon, i.e., if word i is a
positive word (F0)i1 = 1 while if it is negative
(F0)i2 = 1. Note that one may also use soft sen-timent polarities though our experiments are con-
ducted with hard assignments. This information
is incorporated in the tri-factorization model via a
squared loss term,
min
F,G,S
?X ?FSGT?2 +?Tr[(F?F0)TC1(F?F0)]
(4)
where the notation Tr(A) means trace of the matrix
A. Here, ? > 0 is a parameter which determines
the extent to which we enforce F ? F0, C1 is a m?
m diagonal matrix whose entry (C1)ii = 1 if thecategory of the i-th word is known (i.e., specified
by the i-th row of F0) and (C1)ii = 0 otherwise.The squared loss terms ensure that the solution for
F in the otherwise unsupervised learning problem
be close to the prior knowledge F0. Note that if
C1 = I, then we know the class orientation of allthe words and thus have a full specification of F0,Eq.(4) is then reduced to
min
F,G,S
?X?FSGT?2 +??F?F0?2 (5)
The above model is generic and it allows certain
flexibility. For example, in some cases, our prior
knowledge on F0 is not very accurate and we usesmaller ? so that the final results are not depen-
dent on F0 very much, i.e., the results are mostlyunsupervised learning results. In addition, the in-
troduction of C1 allows us to incorporate partialknowledge on word polarity information.
4.2 Computational Algorithm
The optimization problem in Eq.( 4) can be solved
using the following update rules
G jk? G jk
(XT FS) jk
(GGT XT FS) jk
, (6)
Sik ? Sik
(FT XG)ik
(FT FSGT G)ik
. (7)
Fik? Fik
(XGST +?C1F0)ik
(FFT XGST +?C1F)ik
. (8)
The algorithm consists of an iterative procedure
using the above three rules until convergence. We
call this approach Matrix Factorization with Lex-
ical Knowledge (MFLK) and outline the precise
steps in the table below.
Algorithm 1 Matrix Factorization with Lexical
Knowledge (MFLK)
begin
1. Initialization:
Initialize F = F0
G to K-means clustering results,
S = (FT F)?1FT XG(GT G)?1.
2. Iteration:
Update G: fixing F,S, updating G
Update F: fixing S,G, updating F
Update S: fixing F,G, updating S
end
4.3 Algorithm Correctness and Convergence
Updating F,G,S using the rules above leads to an
asymptotic convergence to a local minima. This
can be proved using arguments similar to (Ding
et al, 2006). We outline the proof of correctness
for updating F since the squared loss term that in-
volves F is a new component in our models.
Theorem 1 The above iterative algorithm con-
verges.
Theorem 2 At convergence, the solution satisfies
the Karuch, Kuhn, Tucker optimality condition,
i.e., the algorithm converges correctly to a local
optima.
Theorem 1 can be proved using the standard
auxiliary function approach used in (Lee and Se-
ung, 2001).
Proof of Theorem 2. Following the theory of con-
strained optimization (Nocedal and Wright, 1999),
247
we minimize the following function
L(F)= ||X?FSGT ||2 +?Tr[(F?F0)TC1(F?F0)]
Note that the gradient of L is,
?L
?F =?2XGS
T +2FSGT GST +2?C1(F?F0).
(9)
The KKT complementarity condition for the non-
negativity of Fik gives
[?2XGST +FSGT GST +2?C1(F?F0)]ikFik = 0.(10)
This is the fixed point relation that local minima
for F must satisfy. Given an initial guess of F , the
successive update of F using Eq.(8) will converge
to a local minima. At convergence, we have
Fik = Fik
(XGST +?C1F0)ik
(FFT XGST +?C1F)ik
.
which is equivalent to the KKT condition of
Eq.(10). The correctness of updating rules for G in
Eq.(6) and S in Eq.(7) have been proved in (Ding
et al, 2006). u?Note that we do not enforce exact orthogonality
in our updating rules since this often implies softer
class assignments.
5 Semi-Supervised Learning With
Lexical Knowledge
So far our models have made no demands on hu-
man effort, other than unsupervised collection of
the term-document matrix and a one-time effort in
compiling a domain-independent sentiment lexi-
con. We now assume that a few documents are
manually labeled for the purposes of capturing
some domain-specific connotations leading to a
more domain-adapted model. The partial labels
on documents can be described using G0 where
(G0)i1 = 1 if the document expresses positive sen-timent, and (G0)i2 = 1 for negative sentiment. Aswith F0, one can also use soft sentiment labelingfor documents, though our experiments are con-
ducted with hard assignments.
Therefore, the semi-supervised learning with
lexical knowledge can be described as
min
F,G,S
?X?FSGT?2 +?Tr[(F?F0)TC1(F?F0)]+
?Tr[(G?G0)TC2(G?G0)]
Where ? > 0,? > 0 are parameters which deter-
mine the extent to which we enforce F ? F0 and
G ? G0 respectively, C1 and C2 are diagonal ma-trices indicating the entries of F0 and G0 that cor-respond to labeled entities. The squared loss terms
ensure that the solution for F,G, in the otherwise
unsupervised learning problem, be close to the
prior knowledge F0 and G0.
5.1 Computational Algorithm
The optimization problem in Eq.( 4) can be solved
using the following update rules
G jk? G jk
(XT FS+?C2G0) jk
(GGT XT FS+?GGTC2G0) jk (11)
Sik ? Sik
(FT XG)ik
(FT FSGT G)ik
. (12)
Fik? Fik
(XGST +?C1F0)ik
(FFT XGST +?C1F)ik
. (13)
Thus the algorithm for semi-supervised learning
with lexical knowledge based on our matrix fac-
torization framework, referred as SSMFLK, con-
sists of an iterative procedure using the above three
rules until convergence. The correctness and con-
vergence of the algorithm can also be proved using
similar arguments as what we outlined earlier for
MFLK in Section 4.3.
A quick word about computational complexity.
The term-document matrix is typically very sparse
with z nm non-zero entries while k is typically
also much smaller than n,m. By using sparse ma-
trix multiplications and avoiding dense intermedi-
ate matrices, the updates can be very efficiently
and easily implemented. In particular, updating
F,S,G each takes O(k2(m + n) + kz) time per it-
eration which scales linearly with the dimensions
and density of the data matrix. Empirically, the
number of iterations before practical convergence
is usually very small (less than 100). Thus, com-
putationally our approach scales to large datasets
even though our experiments are run on relatively
small-sized datasets.
6 Experiments
6.1 Datasets Description
Four different datasets are used in our experi-
ments.
Movies Reviews: This is a popular dataset in
sentiment analysis literature (Pang et al, 2002).
It consists of 1000 positive and 1000 negative
movie reviews drawn from the IMDB archive of
the rec.arts.movies.reviews newsgroups.
248
Lotus blogs: The data set is targeted at detect-
ing sentiment around enterprise software, specif-
ically pertaining to the IBM Lotus brand (Sind-
hwani and Melville, 2008). An unlabeled set
of blog posts was created by randomly sampling
2000 posts from a universe of 14,258 blogs that
discuss issues relevant to Lotus software. In ad-
dition to this unlabeled set, 145 posts were cho-
sen for manual labeling. These posts came from
14 individual blogs, 4 of which are actively post-
ing negative content on the brand, with the rest
tending to write more positive or neutral posts.
The data was collected by downloading the lat-
est posts from each blogger?s RSS feeds, or ac-
cessing the blog?s archives. Manual labeling re-
sulted in 34 positive and 111 negative examples.
Political candidate blogs: For our second blog
domain, we used data gathered from 16,742 polit-
ical blogs, which contain over 500,000 posts. As
with the Lotus dataset, an unlabeled set was cre-
ated by randomly sampling 2000 posts. 107 posts
were chosen for labeling. A post was labeled as
having positive or negative sentiment about a spe-
cific candidate (Barack Obama or Hillary Clinton)
if it explicitly mentioned the candidate in posi-
tive or negative terms. This resulted in 49 posi-
tively and 58 negatively labeled posts. Amazon
Reviews: The dataset contains product reviews
taken from Amazon.com from 4 product types:
Kitchen, Books, DVDs, and Electronics (Blitzer
et al, 2007). The dataset contains about 4000 pos-
itive reviews and 4000 negative reviews and can
be obtained from http://www.cis.upenn.
edu/?mdredze/datasets/sentiment/.
For all datasets, we picked 5000 words with
highest document-frequency to generate the vo-
cabulary. Stopwords were removed and a nor-
malized term-frequency representation was used.
Genuinely unlabeled posts for Political and Lo-
tus were used for semi-supervised learning experi-
ments in section 6.3; they were not used in section
6.2 on the effect of lexical prior knowledge. In the
experiments, we set ?, the parameter determining
the extent to which to enforce the feature labels,
to be 1/2, and ?, the corresponding parameter for
enforcing document labels, to be 1.
6.2 Sentiment Analysis with Lexical
Knowledge
Of course, one can remove all burden on hu-
man effort by simply using unsupervised tech-
niques. Our interest in the first set of experi-
ments is to explore the benefits of incorporating a
sentiment lexicon over unsupervised approaches.
Does a one-time effort in compiling a domain-
independent dictionary and using it for different
sentiment tasks pay off in comparison to simply
using unsupervised methods? In our case, matrix
tri-factorization and other co-clustering methods
form the obvious unsupervised baseline for com-
parison and so we start by comparing our method
(MFLK) with the following methods:
? Four document clustering methods: K-
means, Tri-Factor Nonnegative Ma-
trix Factorization (TNMF) (Ding et al,
2006), Information-Theoretic Co-clustering
(ITCC) (Dhillon et al, 2003), and Euclidean
Co-clustering algorithm (ECC) (Cho et al,
2004). These methods do not make use of
the sentiment lexicon.
? Feature Centroid (FC): This is a simple
dictionary-based baseline method. Recall
that each word can be expressed as a ?bag-
of-documents? vector. In this approach, we
compute the centroids of these vectors, one
corresponding to positive words and another
corresponding to negative words. This yields
a two-dimensional representation for docu-
ments, on which we then perform K-means
clustering.
Performance Comparison Figure 1 shows the
experimental results on four datasets using accu-
racy as the performance measure. The results are
obtained by averaging 20 runs. It can be observed
that our MFLK method can effectively utilize the
lexical knowledge to improve the quality of senti-
ment prediction.
Movies Lotus Political Amazon
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Ac
cu
ra
cy
 
 
MFLK
FC
TNMF
ECC
ITCC
K?Means
Figure 1: Accuracy results on four datasets
249
Size of Sentiment Lexicon We also investigate
the effects of the size of the sentiment lexicon on
the performance of our model. Figure 2 shows
results with random subsets of the lexicon of in-
creasing size. We observe that generally the per-
formance increases as more and more lexical su-
pervision is provided.
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0.5
0.55
0.6
0.65
0.7
0.75
0.8
0.85
Fraction of sentiment words labeled
Ac
cu
ra
cy
 
 
Movies
Lotus
Political
Amazon
Figure 2: MFLK accuracy as size of sentiment
lexicon (i.e., number of words in the lexicon) in-
creases on the four datasets
Robustness to Vocabulary Size High dimen-
sionality and noise can have profound impact on
the comparative performance of clustering and
semi-supervised learning algorithms. We simu-
late scenarios with different vocabulary sizes by
selecting words based on information gain. It
should, however, be kept in mind that in a tru-
ely unsupervised setting document labels are un-
available and therefore information gain cannot
be practically computed. Figure 3 and Figure 4
show results for Lotus and Amazon datasets re-
spectively and are representative of performance
on other datasets. MLFK tends to retain its po-
sition as the best performing method even at dif-
ferent vocabulary sizes. ITCC performance is also
noteworthy given that it is a completely unsuper-
vised method.
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0.5
0.55
0.6
0.65
0.7
0.75
0.8
0.85
Fraction of Original Vocabulary 
Ac
cu
ra
cy
 
 
MFLK
FC
TNMF
K?Means
ITCC
ECC
Figure 3: Accuracy results on Lotus dataset with
increasing vocabulary size
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0.5
0.52
0.54
0.56
0.58
0.6
0.62
0.64
0.66
0.68
Fraction of Original Vocabulary
Ac
cu
ra
cy
 
 
MFLK
FC
TNMF
K?Means
ITCC
ECC
Figure 4: Accuracy results on Amazon dataset
with increasing vocabulary size
6.3 Sentiment Analysis with Dual
Supervision
We now assume that together with labeled features
from the sentiment lexicon, we also have access to
a few labeled documents. The natural question is
whether the presence of lexical constraints leads
to better semi-supervised models. In this section,
we compare our method (SSMFLK) with the fol-
lowing three semi-supervised approaches: (1) The
algorithm proposed in (Zhou et al, 2003) which
conducts semi-supervised learning with local and
global consistency (Consistency Method); (2) Zhu
et al?s harmonic Gaussian field method coupled
with the Class Mass Normalization (Harmonic-
CMN) (Zhu et al, 2003); and (3) Green?s function
learning algorithm (Green?s Function) proposed
in (Ding et al, 2007).
We also compare the results of SSMFLK with
those of two supervised classification methods:
Support Vector Machine (SVM) and Naive Bayes.
Both of these methods have been widely used in
sentiment analysis. In particular, the use of SVMs
in (Pang et al, 2002) initially sparked interest
in using machine learning methods for sentiment
classification. Note that none of these competing
methods utilizes lexical knowledge.
The results are presented in Figure 5, Figure 6,
Figure 7, and Figure 8. We note that our SSMFLK
method either outperforms all other methods over
the entire range of number of labeled documents
(Movies, Political), or ultimately outpaces other
methods (Lotus, Amazon) as a few document la-
bels come in.
Learning Domain-Specific Connotations In
our first set of experiments, we incorporated the
sentiment lexicon in our models and learnt the
sentiment orientation of words and documents via
F,G factors respectively. In the second set of
250
0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5
0.4
0.45
0.5
0.55
0.6
0.65
0.7
0.75
0.8
Number of documents labeled as a fraction of the original set of labeled documents
Ac
cu
ra
cy
 
 
SSMFLK
Consistency Method
Homonic?CMN
Green Function
SVM
Naive Bays
Figure 5: Accuracy results with increasing number
of labeled documents on Movies dataset
0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5
0.3
0.4
0.5
0.6
0.7
0.8
0.9
Number of documents labeled as a fraction of the original set of labeled documents
Ac
cu
ra
cy
 
 
SSMFLK
Consistency Method
Homonic?CMN
Green Function 
SVM
Naive Bayes
Figure 6: Accuracy results with increasing number
of labeled documents on Lotus dataset
experiments, we additionally introduced labeled
documents for domain-specific adjustments. Be-
tween these experiments, we can now look for
words that switch sentiment polarity. These words
are interesting because their domain-specific con-
notation differs from their lexical orientation. For
amazon reviews, the following words switched
polarity from positive to negative: fan, impor-
tant, learning, cons, fast, feature, happy, memory,
portable, simple, small, work while the following
words switched polarity from negative to positive:
address, finish, lack, mean, budget, rent, throw.
Note that words like fan, memory probably refer
to product or product components (i.e., computer
fan and memory) in the amazon review context
but have a very different connotation say in the
context of movie reviews where they probably re-
fer to movie fanfare and memorable performances.
We were surprised to see happy switch polarity!
Two examples of its negative-sentiment usage are:
I ended up buying a Samsung and I couldn?t be
more happy and BORING, not one single exciting
thing about this book. I was happy when my lunch
break ended so I could go back to work and stop
reading.
0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5
0.3
0.35
0.4
0.45
0.5
0.55
0.6
0.65
0.7
0.75
0.8
Number of documents labeled as a fraction of the original set of labeled documents
Ac
cu
ra
cy
 
 
SSMFLK
Consistency Method
Homonic?CMN
Green Function
SVM
Naive Bays
Figure 7: Accuracy results with increasing number
of labeled documents on Political dataset
0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5
0.4
0.45
0.5
0.55
0.6
0.65
0.7
0.75
0.8
Number of documents labeled as a fraction of the original set of labeled documents
Ac
cu
ra
cy
 
 
SSMFLK
Consistency Method
Homonic?CMN
Green Function
SVM
Naive Bays
Figure 8: Accuracy results with increasing number
of labeled documents on Amazon dataset
7 Conclusion
The primary contribution of this paper is to pro-
pose and benchmark new methodologies for sen-
timent analysis. Non-negative Matrix Factoriza-
tions constitute a rich body of algorithms that have
found applicability in a variety of machine learn-
ing applications: from recommender systems to
document clustering. We have shown how to build
effective sentiment models by appropriately con-
straining the factors using lexical prior knowledge
and document annotations. To more effectively
utilize unlabeled data and induce domain-specific
adaptation of our models, several extensions are
possible: facilitating learning from related do-
mains, incorporating hyperlinks between docu-
ments, incorporating synonyms or co-occurences
between words etc. As a topic of vigorous current
activity, there are several very recently proposed
competing methodologies for sentiment analysis
that we would like to benchmark against. These
are topics for future work.
Acknowledgement: The work of T. Li is par-
tially supported by NSF grants DMS-0844513 and
CCF-0830659. We would also like to thank Prem
Melville and Richard Lawrence for their support.
251
References
J. Blitzer, M. Dredze, and F. Pereira. 2007. Biogra-phies, bollywood, boom-boxes and blenders: Do-main adaptation for sentiment classification. In Pro-
ceedings of ACL, pages 440?447.
H. Cho, I. Dhillon, Y. Guan, and S. Sra. 2004. Mini-
mum sum squared residue co-clustering of gene ex-pression data. In Proceedings of The 4th SIAM Data
Mining Conference, pages 22?24, April.
S. Das and M. Chen. 2001. Yahoo! for amazon:Extracting market sentiment from stock messageboards. In Proceedings of the 8th Asia Pacific Fi-
nance Association (APFA).
I. S. Dhillon, S. Mallela, and D. S. Modha. 2003.Information-theoretical co-clustering. In Proceed-
ings of ACM SIGKDD, pages 89?98.
I. S. Dhillon. 2001. Co-clustering documents andwords using bipartite spectral graph partitioning. In
Proceedings of ACM SIGKDD.
C. Ding, T. Li, W. Peng, and H. Park. 2006. Orthogo-
nal nonnegative matrix tri-factorizations for cluster-ing. In Proceedings of ACM SIGKDD, pages 126?135.
C. Ding, R. Jin, T. Li, and H.D. Simon. 2007. Alearning framework using green?s function and ker-nel regularization with application to recommender
system. In Proceedings of ACM SIGKDD, pages260?269.
G. Druck, G. Mann, and A. McCallum. 2008. Learn-
ing from labeled features using generalized expecta-tion criteria. In SIGIR.
A. Goldberg and X. Zhu. 2006. Seeing stars
when there aren?t many stars: Graph-based semi-supervised learning for sentiment categorization. In
HLT-NAACL 2006: Workshop on Textgraphs.
T. Hofmann. 1999. Probabilistic latent semantic in-dexing. Proceeding of SIGIR, pages 50?57.
M. Hu and B. Liu. 2004. Mining and summarizingcustomer reviews. In KDD, pages 168?177.
S.-M. Kim and E. Hovy. 2004. Determining the sen-
timent of opinions. In Proceedings of International
Conference on Computational Linguistics.
D.D. Lee and H.S. Seung. 2001. Algorithms for non-negative matrix factorization. In Advances in Neural
Information Processing Systems 13.
T. Li, C. Ding, Y. Zhang, and B. Shao. 2008. Knowl-edge transformation from word space to documentspace. In Proceedings of SIGIR, pages 187?194.
B. Liu, X. Li, W.S. Lee, and P. Yu. 2004. Text classifi-cation by labeling words. In AAAI.
V. Ng, S. Dasgupta, and S. M. Niaz Arifin. 2006. Ex-amining the role of linguistic knowledge sources inthe automatic identification and classification of re-
views. In COLING & ACL.
J. Nocedal and S.J. Wright. 1999. Numerical Opti-
mization. Springer-Verlag.
B. Pang and L. Lee. 2004. A sentimental education:sentiment analysis using subjectivity summarizationbased on minimum cuts. In ACL.
B. Pang and L. Lee. 2008. Opinion mining
and sentiment analysis. Foundations and Trendsin Information Retrieval: Vol. 2: No 12, pp
1-135 http://www.cs.cornell.edu/home/llee/opinion-mining-sentiment-analysis-survey.html.
B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs
up? sentiment classification using machine learningtechniques. In EMNLP.
G. Ramakrishnan, A. Jadhav, A. Joshi, S. Chakrabarti,and P. Bhattacharyya. 2003. Question answeringvia bayesian inference on lexical relations. In ACL,pages 1?10.
T. Sandler, J. Blitzer, P. Talukdar, and L. Ungar. 2008.Regularized learning with networks of features. In
NIPS.
R.E. Schapire, M. Rochery, M.G. Rahim, andN. Gupta. 2002. Incorporating prior knowledge into
boosting. In ICML.
V. Sindhwani and P. Melville. 2008. Document-word co-regularization for semi-supervised senti-
ment analysis. In Proceedings of IEEE ICDM.
V. Sindhwani, J. Hu, and A. Mojsilovic. 2008. Regu-larized co-clustering with dual supervision. In Pro-
ceedings of NIPS.
P. Turney. 2002. Thumbs up or thumbs down? Se-mantic orientation applied to unsupervised classifi-
cation of reviews. Proceedings of the 40th Annual
Meeting of the Association for Computational Lin-
guistics, pages 417?424.
X. Wu and R. Srihari. 2004. Incorporating priorknowledge with weighted margin support vector ma-chines. In KDD.
H. Zha, X. He, C. Ding, M. Gu, and H.D. Simon.2001. Bipartite graph partitioning and data cluster-ing. Proceedings of ACM CIKM.
D. Zhou, O. Bousquet, T.N. Lal, J. Weston, andB. Scholkopf. 2003. Learning with local and globalconsistency. In Proceedings of NIPS.
X. Zhu, Z. Ghahramani, and J. Lafferty. 2003. Semi-supervised learning using gaussian fields and har-monic functions. In Proceedings of ICML.
L. Zhuang, F. Jing, and X. Zhu. 2006. Movie reviewmining and summarization. In CIKM, pages 43?50.
252
Proceedings of the NAACL HLT Workshop on Active Learning for Natural Language Processing, pages 27?35,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Data Quality from Crowdsourcing:
A Study of Annotation Selection Criteria
Pei-Yun Hsueh, Prem Melville, Vikas Sindhwani
IBM T.J. Watson Research Center
1101 Kitchawan Road, Route 134
Yorktown Heights, NY 10598, USA
Abstract
Annotation acquisition is an essential step in
training supervised classifiers. However, man-
ual annotation is often time-consuming and
expensive. The possibility of recruiting anno-
tators through Internet services (e.g., Amazon
Mechanic Turk) is an appealing option that al-
lows multiple labeling tasks to be outsourced
in bulk, typically with low overall costs and
fast completion rates. In this paper, we con-
sider the difficult problem of classifying sen-
timent in political blog snippets. Annotation
data from both expert annotators in a research
lab and non-expert annotators recruited from
the Internet are examined. Three selection cri-
teria are identified to select high-quality anno-
tations: noise level, sentiment ambiguity, and
lexical uncertainty. Analysis confirm the util-
ity of these criteria on improving data quality.
We conduct an empirical study to examine the
effect of noisy annotations on the performance
of sentiment classification models, and evalu-
ate the utility of annotation selection on clas-
sification accuracy and efficiency.
1 Introduction
Crowdsourcing (Howe, 2008) is an attractive solu-
tion to the problem of cheaply and quickly acquir-
ing annotations for the purposes of constructing all
kinds of predictive models. To sense the potential of
crowdsourcing, consider an observation in von Ahn
et al (2004): a crowd of 5,000 people playing an
appropriately designed computer game 24 hours a
day, could be made to label all images on Google
(425,000,000 images in 2005) in a matter of just 31
days. Several recent papers have studied the use
of annotations obtained from Amazon Mechanical
Turk, a marketplace for recruiting online workers
(Su et al, 2007; Kaisser et al, 2008; Kittur et al,
2008; Sheng et al, 2008; Snow et al, 2008; Sorokin
and Forsyth, 2008).
With efficiency and cost-effectiveness, online re-
cruitment of anonymous annotators brings a new set
of issues to the table. These workers are not usually
specifically trained for annotation, and might not be
highly invested in producing good-quality annota-
tions. Consequently, the obtained annotations may
be noisy by nature, and might require additional val-
idation or scrutiny. Several interesting questions im-
mediately arise in how to optimally utilize annota-
tions in this setting: How does one handle differ-
ences among workers in terms of the quality of an-
notations they provide? How useful are noisy anno-
tations for the end task of creating a model? Is it pos-
sible to identify genuinely ambiguous examples via
annotator disagreements? How should these consid-
erations be treated with respect to intrinsic informa-
tiveness of examples? These questions also hint at a
strong connection to active learning, with annotation
quality as a new dimension to the problem.
As a challenging empirical testbed for these is-
sues, we consider the problem of sentiment classi-
fication on political blogs. Given a snippet drawn
from a political blog post, the desired output is a
polarity score that indicates whether the sentiment
expressed is positive or negative. Such an analysis
provides a view of the opinion around a subject of
interest, e.g., US Presidential candidates, aggregated
across the blogsphere. Recently, sentiment analy-
27
sis is emerging as a critical methodology for social
media analytics. Previous research has focused on
classifying subjective-versus-objective expressions
(Wiebe et al, 2004), and also on accurate sentiment
polarity assignment (Turney, 2002; Yi et al, 2003;
Pang and Lee, 2004; Sindhwani and Melville, 2008;
Melville et al, 2009).
The success of most prior work relies on the qual-
ity of their knowledge bases; either lexicons defin-
ing the sentiment polarity of words around a topic
(Yi et al, 2003), or quality annotation data for sta-
tistical training. While manual intervention for com-
piling lexicons has been significantly lessened by
bootstrapping techniques (Yu and Hatzivassiloglou,
2003; Wiebe and Riloff, 2005), manual intervention
in the annotation process is harder to avoid. More-
over, the task of annotating blog-post snippets is
challenging, particularly in a charged political at-
mosphere with complex discourse spanning many
issues, use of cynicism and sarcasm, and highly
domain-specific and contextual cues. The downside
is that high-performance models are generally dif-
ficult to construct, but the upside is that annotation
and data-quality issues are more clearly exposed.
In this paper we aim to provide an empirical basis
for the use of data selection criteria in the context
of sentiment analysis in political blogs. Specifically,
we highlight the need for a set of criteria that can be
applied to screen untrustworthy annotators and se-
lect informative yet unambiguous examples for the
end goal of predictive modeling. In Section 2, we
first examine annotation data obtained by both the
expert and non-expert annotators to quantify the im-
pact of including non-experts. Then, in Section 3,
we quantify criteria that can be used to select anno-
tators and examples for selective sampling. Next, in
Section 4, we address the questions of whether the
noisy annotations are still useful for this task and
study the effect of the different selection criteria on
the performance of this task. Finally, in Section 5
we present conclusion and future work.
2 Annotating Blog Sentiment
This section introduces the Political Blog Snippet
(PBS) corpus, describes our annotation procedure
and the sources of noise, and gives an overview of
the experiments on political snippet sentiments.
2.1 The Political Blog Snippet Corpus
Our dataset comprises of a collection of snippets ex-
tracted from over 500,000 blog posts, spanning the
activity of 16,741 political bloggers in the time pe-
riod of Aug 15, 2008 to the election day Nov 4,
2008. A snippet was defined as a window of text
containing four consecutive sentences such that the
head sentence contained either the term ?Obama?
or the term ?McCain?, but both candidates were
not mentioned in the same window. The global
discourse structure of a typical political blog post
can be highly complicated with latent topics ranging
from policies (e.g., financial situation, economics,
the Iraq war) to personalities to voting preferences.
We therefore expected sentiment to be highly non-
uniform over a blog post. This snippetization proce-
dure attempts to localize the text around a presiden-
tial candidate with the objective of better estimat-
ing aggregate sentiment around them. In all, we ex-
tracted 631,224 snippets. For learning classifiers, we
passed the snippets through a stopword filter, pruned
all words that occur in less than 3 snippets and cre-
ated normalized term-frequency feature vectors over
a vocabulary of 3,812 words.
2.2 Annotation Procedure
The annotation process consists of two steps:
Sentiment-class annotation: In the first step, as
we are only interested in detecting sentiments re-
lated to the named candidate, the annotators were
first asked to mark up the snippets irrelevant to the
named candidate?s election campaign. Then, the an-
notators were instructed to tag each relevant snippet
with one of the following four sentiment polarity la-
bels: Positive, Negative, Both, or Neutral.
Alignment annotation: In the second step, the
annotators were instructed to mark up whether each
snippet was written to support or oppose the target
candidate therein named. The motivation of adding
this tag comes from our interest in building a classi-
fication system to detect positive and negative men-
tions of each candidate. For the snippets that do
not contain a clear political alignment, the annota-
tors had the freedom to mark it as neutral or simply
not alignment-revealing.
In our pilot study many bloggers were observed
to endorse a named candidate by using negative ex-
28
pressions to denounce his opponent. Therefore, in
our annotation procedure, the distinction is made
between the coding of manifest content, i.e., sen-
timents ?on the surface?, and latent political align-
ment under these surface elements.
2.3 Agreement Study
In this section, we compare the annotations obtained
from the on-site expert annotators and those from the
non-expert AMT annotators.
2.3.1 Expert (On-site) Annotation
To assess the reliability of the sentiment annota-
tion procedure, we conducted an agreement study
with three expert annotators in our site, using 36
snippets randomly chosen from the PBS Corpus.
Overall agreement among the three annotators on
the relevance of snippets is 77.8%. Overall agree-
ment on the four-class sentiment codings is 70.4%.
Analysis indicate that the annotators agreed better
on some codings than the others. For the task of
determining whether a snippet is subjective or not1,
the annotators agreed 86.1% of the time. For the
task of determining whether a snippet is positive or
negative, they agreed 94.9% of the time.
To examine which pair of codings is the most dif-
ficult to distinguish, Table 1 summarizes the confu-
sion matrix for the three pairs of annotator?s judge-
ments on sentiment codings. Each column describes
the marginal probability of a coding and the prob-
ability distribution for this coding being recognized
as another coding (including itself). As many blog-
gers use cynical expressions in their writings, the
most confusing cases occur when the annotators
have to determine whether a snippet is ?negative?
or ?neutral?. The effect of cynical expressions on
% Neu Pos Both Neg
Marginal 21.9 20.0 10.5 47.6
Neutral (Neu) 47.8 14.3 9.1 16.0
Positive (Pos) 13.0 61.9 18.2 6.0
Both (Both) 4.4 9.5 9.1 14.0
Negative (Neg) 34.8 14.3 63.6 64.0
Table 1: Summary matrix for the three on-site annotators?
sentiment codings.
1This is done by grouping the codings of Positive, Negative,
and Both into the subjective class.
sentiment analysis in the political domain is also re-
vealed in the second step of alignment annotation.
Only 42.5% of the snippets have been coded with
alignment coding in the same direction as its senti-
ment coding ? i.e., if a snippet is intended to support
(oppose) a target candidate, it will contain positive
(negative) sentiment. The alignment coding task has
been shown to be reliable, with the annotators agree-
ing 76.8% of the time overall on the three-level cod-
ings: Support/Against/Neutral.
2.3.2 Amazon Mechanical Turk Annotation
To compare the annotation reliability between
expert and non-expert annotators, we further con-
ducted an agreement study with the annotators re-
cruited from Amazon Mechanical Turk (AMT). We
have collected 1,000 snippets overnight, with the
cost of 4 cents per annotation.
In the agreement study, a subset of 100 snippets
is used, and each snippet is annotated by five AMT
annotators. These annotations were completed by
25 annotators whom were selected based on the ap-
proval rate of their previous AMT tasks (over 95%
of times).2 The AMT annotators spent on average
40 seconds per snippet, shorter than the average of
two minutes reported by the on-site annotators. The
lower overall agreement on all four-class sentiment
codings, 35.3%, conforms to the expectation that the
non-expert annotators are less reliable. The Turk an-
notators also agreed less on the three-level alignment
codings, achieving only 47.2% of agreement.
However, a finer-grained analysis reveals that they
still agree well on some codings: The overall agree-
ment on whether a snippet is relevant, whether a
snippet is subjective or not, and whether a snippet
is positive or negative remain within a reasonable
range: 81.0%, 81.8% and 61.9% respectively.
2.4 Gold Standard
We defined the gold standard (GS) label of a snip-
pet in terms of the coding that receives the major-
ity votes.3 Column 1 in Table 2 (onsite-GS predic-
2Note that we do not enforce these snippets to be annotated
by the same group of annotators. However, Kappa statistics
requires to compute the chance agreement of each annotator.
Due to the violation of this assumption, we do not measure the
intercoder agreement with Kappa in this agreement study.
3In this study, we excluded 6 snippets whose annotations
failed to reach majority vote by the three onsite annotators.
29
onsite-GS prediction onsite agreement AMT-GS prediction AMT agreement
Sentiment (4-class) 0.767 0.704 0.614 0.353
Alignment (3-level) 0.884 0.768 0.669 0.472
Relevant or not 0.889 0.778 0.893 0.810
Subjective or not 0.931 0.861 0.898 0.818
Positive or negative 0.974 0.949 0.714 0.619
Table 2: Average prediction accuracy on gold standard (GS) using one-coder strategy and inter-coder agreement.
tion) shows the ratio of the onsite expert annotations
that are consistent with the gold standard, and Col-
umn 3 (AMT-GS prediction) shows the same for the
AMT annotations. The level of consistency, i.e., the
percentage agreement with the gold standard labels,
can be viewed as a proxy of the quality of the an-
notations. Among the AMT annotations, Columns
2 (onsite agreement) and 4 (AMT agreement) show
the pair-wise intercoder agreement in the on-site ex-
pert and AMT annotations respectively.
The results suggest that it is possible to take one
single expert annotator?s coding as the gold standard
in a number of annotation tasks using binary clas-
sification. For example, there is a 97.4% chance
that one expert?s coding on the polarity of a snip-
pet, i.e., whether it is positive or negative, will be
consistent with the gold standard coding. However,
this one-annotator strategy is less reliable with the
introduction of non-expert annotators. Take the task
of polarity annotation as an example, the intercoder
agreement among the AMT workers goes down to
61.9% and the ?one-coder? strategy can only yield
71.4% accuracy. To determine reliable gold stan-
dard codings, multiple annotators are still necessary
when non-expert annotators are recruited.
3 Annotation Quality Measures
Given the noisy AMT annotations, in this section we
discuss some summary statistics that are needed to
control the quality of annotations.
3.1 Annotator-level noise
To study the question of whether there exists a group
of annotators who tend to yield more noisy annota-
tions, we evaluate the accumulated noise level intro-
duced by each of the annotators. We define the noise
level as the deviation from the gold standard labels.
Similar to the measure of individual error rates pro-
posed in (Dawid and Skene, 1979), the noise level of
a particular annotator j, i.e., noise(annoj), is then
estimated by summing up the deviation of the an-
notations received from this annotator, with a small
sampling correction for chance disagreement. Anal-
ysis results demonstrate that there does exist a subset
of annotators who yield more noisy annotations than
the others. 20% of the annotators (who exceed the
noise level 60%) result in annotations that have 70%
disagreement with the gold standard.
In addition, we also evaluate how inclusion of
noisy annotators reduces the mean agreement with
Gold Standard. The plot (left) in Figure 1 plots the
mean agreement rate with GS over the subset of an-
notators that pass a noise threshold. These results
show that the data quality decreases with the inclu-
sion of more untrustworthy annotators.
3.2 Snippet-level sentiment ambiguity
We have observed that not all snippets are equally
easy to annotate, with some containing more am-
biguous expressions. To incorporate this concern in
the selection process, a key question to be answered
is whether there exist snippets whose sentiment is
substantially less distinguishable than the others.
We address this question by quantifying ambigu-
ity measures with the two key properties shown as
important in evaluating the controversiality of anno-
tation snippets (Carenini and Cheung, 2008): (1) the
strength of the annotators? judgements and (2) the
polarity of the annotations. The measurement needs
to satisfy the constraints demonstrated in the follow-
ing snippets: (1) An example that has received three
positive codings are more ambiguous than that has
received five, and (2) an example that has received
five positive codings is more ambiguous than the one
that has received four positive and one negative cod-
ing. In addition, as some snippets were shown to
30
Annotator noise level
Predictio
n Accur
acy
0.0 0.2 0.4 0.6 0.80.
0
0.2
0.4
0.6
0.8
1.0
Annotator Noise
0.0 0.2 0.4 0.6 0.8 1.00.
0
0.2
0.4
0.6
0.8
1.0
Sentiment Ambigity
Lexical Uncertainty
Predictio
n Accur
acy
0.0 0.2 0.4 0.6 0.8 1.00.
0
0.2
0.4
0.6
0.8
1.0
Lexical Uncertainty
Figure 1: Data quality (consistency with GS) as a function of noise level (left), sentiment ambiguity (middle), and
lexical uncertainty (right).
be difficult to tell whether they contain negative or
neutral sentiment, the measure of example ambigu-
ity has to go beyond controversiality and incorporate
codings of ?neutral? and ?both?.
To satisfy these constraints, we first enumerated
through the codings of each snippet and counted
the number of neutral, positive, both, and negative
codings: We added (1) one to the positive (nega-
tive) category for each positive (negative) coding,
(2) 0.5 to the neutral category with each neutral cod-
ing, and (3) 0.5 to both the positive and negative
categories with each both coding. The strength of
codings in the three categories, i.e., str+(snipi),
strneu(snipi), and str?(snipi), were then summed
up into str(snipi). The distribution were parame-
terized with
?+(snipi) = str+(snipi)/str(snipi)
?neu(snipi) = strneu(snipi)/str(snipi)
??(snipi) = str?(snipi)/str(snipi)
We then quantify the level of ambiguity in the an-
notator?s judgement as follows:
H(?(snipi)) = ??+(snipi)log(?+(snipi))
??neu(snipi)log(?neu(snipi))
???(snipi)log(??(snipi))
Amb(snipi) = str(snipi)strmax ?H(?(snipi)),
where strmax is the maximum value of str among
all the snippets in the collection. The plot (middle)
in Figure 1 shows that with the inclusion of snip-
pets that are more ambiguous in sentiment disam-
biguation, the mean agreement with Gold Standard
decreases as expected.
3.3 Combining measures on multiple
annotations
Having established the impact of noise and senti-
ment ambiguity on annotation quality, we then set
out to explore how to integrate them for selection.
First, the ambiguity scores for each of the snippets
are reweighed with respect to the noise level.
w(snipi) =
?
j
noise(annoj)? (1e )
?(ij)
Conf(snipi) = w(snipi)?
iw(snipi)
?Amb(snipi),
where ?(ij) is an indicator function of whether a
coding of snipi from annotator j agrees with its gold
standard coding. w(expi) is thus computed as the
aggregated noise level of all the annotators who la-
beled the ith snippet.
To understand the baseline performance of the se-
lection procedure, we evaluate the the true predic-
tions versus the false alarms resulting from using
each of the quality measures separately to select an-
notations for label predictions. In this context, a true
prediction occurs when an annotation suggested by
our measure as high-quality indeed matches the GS
label, and a false alarm occurs when a high quality
annotation suggested by our measure does not match
the GS label. The ROC (receiver operating charac-
teristics) curves in Figure 2 reflect all the potential
operating points with the different measures.
We used data from 2,895 AMT annotations on
579 snippets, including 63 snippets used in the
agreement study. This dataset is obtained by filter-
ing out the snippets with their GS labels as 1 (?ir-
relevant?) and the snippets that do not receive any
coding that has more than two votes.
31
0.0 0.2 0.4 0.6 0.8 1.0
0.0
0.2
0.4
0.6
0.8
1.0
False Alarm Rate
True P
redict
ion Ra
te
lllll
l
l
l
ll
0.1.20.30.40.50.6
0.7
0.8
0.91
(a) Match Prediction Before Removing Divisive Snippets
1?confusion1?ambiguity1?noise
0.0 0.2 0.4 0.6 0.8 1.0
0.0
0.2
0.4
0.6
0.8
1.0
False Alarm Rate
True P
redict
ion Ra
te
lllll
l
l
l
l
l
0.10.20.30.40.50.6
0.7
0.8
0.9
1
(b) Match Prediction After Removing Divisive Snippets
1?confusion(all4codings)1?confusion(pos/neg)1?ambiguity(all4codings)1?ambiguity(pos/neg)1?noise
Figure 2: Modified ROC curves for quality measures: (a) before removing divisive snippets, (b) after removing divisive
snippets. The numbers shown with the ROC curve are the values of the aggregated quality measure (1-confusion).
Initially, three quality measures are tested: 1-
noise, 1-ambiguity, 1-confusion. Examination of the
snippet-level sentiment codings reveals that some
snippets (12%) result in ?divisive? codings, i.e.,
equal number of votes on two codings.
The ROC curves in Figure 2 (a) plot the base-
line performance of the different quality measures.
Results show that before removing the subset of di-
visive snippets, the only effective selection criteria
is obtained by monitoring the noise level of anno-
tators. Figure 2 (b) plots the performance after re-
moving the divisive snippets. In addition, our am-
biguity scores are computed under two settings: (1)
with only the polar codings (pos/neg), and (2) with
all the four codings (all4codings). The ROC curves
reveal that analyzing only the polar codings is not
sufficient for annotation selection.
The results also demonstrate that confusion, an in-
tegrated measure, does perform best. Confusion is
just one way of combining these measures. One may
chose alternative combinations ? the results here pri-
marily illustrate the benefit of considering these dif-
ferent dimensions in tandem. Moreover, the differ-
ence between plot (a) and (b) suggests that removing
divisive snippets is essential for the quality measures
to work well. How to automatically identify the di-
visive snippets is therefore important to the success
of the annotation selection process.
3.4 Effect of lexical uncertainty on divisive
snippet detection
In search of measures that can help identify the di-
visive snippets automatically, we consider the inher-
ent lexical uncertainty of an example. Uncertainty
Sampling (Lewis and Catlett, 1994) is one common
heuristic for the selection of informative instances,
which select instances that the current classifier is
most uncertain about. Following on these lines we
measure the uncertainty on instances, with the as-
sumption that the most uncertain snippets are likely
to be divisive.
In particular, we applied a lexical sentiment clas-
sifier (c.f. Section 4.1.1) to estimate the likelihood of
an unseen snippet being of positive or negative sen-
timent, i.e., P+(expi), P?(expi), by counting the
sentiment-indicative word occurrences in the snip-
pet. As in our dataset the negative snippets far ex-
ceed the positive ones, we also take the prior proba-
bility into account to avoid class bias. We then mea-
sure lexical uncertainty as follows.
Deviation(snipi) =
1
C ? |(log(P (+))?log(P (?)))
+(log(P+(snipi))?log(P?(snipi)))|,
Uncertainty(snipi) =1?Deviation(snipi),
where class priors, P (+) and P (?), are estimated
with the dataset used in the agreement studies, and
C is the normalization constant.
We then examine not only the utility of lexical un-
certainty in identifying high-quality annotations, but
32
Classifier Accuracy AUC
LC 49.60 0.614
NB 83.53 0.653
SVM 83.89 0.647
Pooling 84.51 0.700
Table 3: Accuracy of sentiment classification methods.
also the utility of such measure in identifying divi-
sive snippets. Figure 1 (right) shows the effect of
lexical uncertainty on filtering out low-quality anno-
tations. Figure 3 demonstrates the effect of lexical
uncertainty on divisive snippet detection, suggesting
the potential use of lexical uncertainty measures in
the selection process.
Lexical Uncertainty
Divisive 
Snippet D
etection 
Accuracy
0.0 0.2 0.4 0.6 0.8 1.00.0
0.2
0.4
0.6
0.8
1.0
Figure 3: Divisive snippet detection accuracy as a func-
tion of lexical uncertainty.
4 Empirical Evaluation
The analysis in Sec. 3 raises two important ques-
tions: (1) how useful are noisy annotations for sen-
timent analysis, and (2) what is the effect of online
annotation selection on improving sentiment polar-
ity classification?
4.1 Polarity Classifier with Noisy Annotations
To answer the first question raised above, we train
classifiers based on the noisy AMT annotations to
classify positive and negative snippets. Four dif-
ferent types of classifiers are used: SVMs, Naive
Bayes (NB), a lexical classifier (LC), and the lexi-
cal knowledge-enhanced Pooling Multinomial clas-
sifier, described below.
4.1.1 Lexical Classifier
In the absence of any labeled data in a domain,
one can build sentiment-classification models that
rely solely on background knowledge, such as a lex-
icon defining the polarity of words. Given a lexi-
con of positive and negative terms, one straightfor-
ward approach to using this information is to mea-
sure the frequency of occurrence of these terms in
each document. The probability that a test document
belongs to the positive class can then be computed
as P (+|D) = aa+b ; where a and b are the numberof occurrences of positive and negative terms in the
document respectively. A document is then classi-
fied as positive if P (+|D) > P (?|D); otherwise,
the document is classified as negative. For this study,
we used a lexicon of 1,267 positive and 1,701 nega-
tive terms, as labeled by human experts.
4.1.2 Pooling Multinomials
The Pooling Multinomials classifier was intro-
duced by the authors as an approach to incorpo-
rate prior lexical knowledge into supervised learn-
ing for better text classification. In the context
of sentiment analysis, such lexical knowledge is
available in terms of the prior sentiment-polarity of
words. Pooling Multinomials classifies unlabeled
examples just as in multinomial Na??ve Bayes clas-
sification (McCallum and Nigam, 1998), by predict-
ing the class with the maximum likelihood, given by
argmaxcjP (cj)
?
i P (wi|cj); where P (cj) is the
prior probability of class cj , and P (wi|cj) is the
probability of word wi appearing in a snippet of
class cj . In the absence of background knowledge
about the class distribution, we estimate the class
priors P (cj) solely from the training data. However,
unlike regular Na??ve Bayes, the conditional prob-
abilities P (wi|cj) are computed using both the la-
beled examples and the lexicon of labeled features.
Given two models built using labeled examples and
labeled features, the multinomial parameters of such
models can be aggregated through a convex combi-
nation, P (wi|cj) = ?Pe(wi|cj)+(1??)Pf (wi|cj);
where Pe(wi|cj) and Pf (wi|cj) represent the proba-
bility assigned by using the example labels and fea-
ture labels respectively, and ? is the weight for com-
bining these distributions. The weight indicates a
level of confidence in each source of information,
and can be computed based on the training set accu-
racy of the two components. The derivation and de-
tails of these models are not directly relevant to this
paper, but can be found in (Melville et al, 2009).
33
Q1 Q2 Q3 Q4
Accuracy AUC Accuracy AUC Accuracy AUC Accuracy AUC
Noise 84.62% 0.688 74.36% 0.588 74.36% 0.512 79.49% 0.441
Ambiguity 84.21% 0.715 78.95% 0.618 68.42% 0.624 84.21% 0.691
Confusion 82.50% 0.831 82.50% 0.762 80.00% 0.814 80.00% 0.645
Table 4: Effect of annotation selection on classification accuracy.
4.1.3 Results on Polarity Classification
We generated a data set of 504 snippets that had
3 or more labels for either the positive or negative
class. We compare the different classification ap-
proaches using 10-fold cross-validation and present
our results in Table 3. Results show that the Pool-
ing Multinomial classifier, which makes predictions
based on both the prior lexical knowledge and the
training data, can learn the most from the labeled
data to classify sentiments of the political blog snip-
pets. We observe that despite the significant level
of noise and ambiguity in the training data, using
majority-labeled data for training still results in clas-
sifiers with reasonable accuracy.
4.2 Effect of Annotation Selection
We then evaluate the utility of the quality measures
in a randomly split dataset (with 7.5% of the data in
the test set). We applied each of the measures to rank
the annotation examples and then divide them into
4 equal-sized training sets based on their rankings.
For example, Noise-Q1 contains only the least noisy
quarter of annotations and Q4 the most noisy ones.
Results in Table 4 demonstrate that the classi-
fication performance declines with the decrease of
each quality measure in general, despite exceptions
in the subset with the highest sentiment ambiguity
(Ambiguity-Q4), the most noisy subset Q4 (Noise-
Q4), and the subset yielding less overall confusion
(Confusion-Q2). The results also reveal the benefits
of annotation selection on efficiency: using the sub-
set of annotations predicted in the top quality quar-
ter achieves similar performance as using the whole
training set. These preliminary results suggest that
an active learning scheme which considers all three
quality measures may indeed be effective in improv-
ing label quality and subsequent classification accu-
racy.
5 Conclusion
In this paper, we have analyzed the difference be-
tween expert and non-expert annotators in terms of
annotation quality, and showed that having a single
non-expert annotator is detrimental for annotating
sentiment in political snippets. However, we con-
firmed that using multiple noisy annotations from
different non-experts can still be very useful for
modeling. This finding is consistent with the sim-
ulated results reported in (Sheng et al, 2008). Given
the availability of many non-expert annotators on-
demand, we studied three important dimensions to
consider when acquiring annotations: (1) the noise
level of an annotator compared to others, (2) the in-
herent ambiguity of an example?s class label, and
(3) the informativeness of an example to the current
classification model. While the first measure has
been studied with annotations obtained from experts
(Dawid and Skene, 1979; Clemen and Reilly, 1999),
the applicability of their findings on non-expert an-
notation selection has not been examined.
We showed how quality of labels can be improved
by eliminating noisy annotators and ambiguous ex-
amples. Furthermore, we demonstrated the quality
measures are useful for selecting annotations that
lead to more accurate classification models. Our re-
sults suggest that a good active learning or online
learning scheme in this setting should really con-
sider all three dimensions. The way we use to in-
tegrate the different dimensions now is still prelimi-
nary. Also, our empirical findings suggest that some
of the dimensions may have to be considered sepa-
rately. For example, due to the divisive tendency of
the most informative examples, these examples may
have to be disregarded in the initial stage of anno-
tation selection. Also, the way we use to combine
these measures is still preliminary. The design and
testing of such schemes are avenues for future work.
34
References
Giuseppe Carenini and Jackie C. K. Cheung. 2008. Ex-
tractive vs. NLG-based abstractive summarization of
evaluative text: The effect of corpus controversiality.
In Proceedings of the Fifth International Natural Lan-
guage Generation Conference.
R.T. Clemen and T. Reilly. 1999. Correlations and cop-
ulas for decision and risk analysis. Management Sci-
ence, 45:208?224.
A. P. Dawid and A. .M. Skene. 1979. Maximum likli-
hood estimation of observer error-rates using the em
algorithm. Applied Statistics, 28(1):20?28.
Jeff Howe. 2008. Crowdsourcing: Why the Power of
the Crowd Is Driving the Future of Business. Crown
Business, 1 edition, August.
Michael Kaisser, Marti Hearst, and John B. Lowe. 2008.
Evidence for varying search results summary lengths.
In Proceedings of ACL 2008.
Aniket Kittur, Ed H. Chi, and Bongwon Suh. 2008.
Crowdsourcing user studies with mechanical turk. In
Proceedings of CHI 2008.
David D. Lewis and Jason Catlett. 1994. Heterogeneous
uncertainty sampling for supervised learning. pages
148?156, San Francisco, CA, July.
Andrew McCallum and Kamal Nigam. 1998. A com-
parison of event models for naive Bayes text classifi-
cation. In AAAI Workshop on Text Categorization.
Prem Melville, Wojciech Gryc, and Richard Lawrence.
2009. Sentiment analysis of blogs by combining lexi-
cal knowledge with text classification. In KDD.
Bo Pang and Lillian Lee. 2004. A sentiment education:
Sentiment analysis using subjectivity summarization
based on minimum cuts. In Proceedings of ACL 2004.
Victor Sheng, Foster Provost, and G. Panagiotis Ipeiro-
tis. 2008. Get another label? Improving data quality
and data mining using multiple, noisy labelers. In Pro-
ceeding of KDD 2008, pages 614?622.
Vikas Sindhwani and Prem Melville. 2008. Document-
word co-regularization for semi-supervised sentiment
analysis. In Proceedings of IEEE International Con-
ference on Data Mining (ICDM), pages 1025?1030,
Los Alamitos, CA, USA. IEEE Computer Society.
R. Snow, B. O?Connor, D. Jurafsky, and A. Ng. 2008.
Cheap and fast?but is it good? evaluating non-expert
annotations for natural language tasks. In Proceedings
of EMNLP 2008.
Alexander Sorokin and David Forsyth. 2008. Utility
data annotation via amazon mechanical turk. In IEEE
Workshop on Internet Vision at CVPR 08.
Qi Su, Dmitry Pavlov, Jyh-Herng Chow, and Wendell
C.Baker. 2007. Internet-scale collection of human-
reviewed data. In Proceedings of WWW 2007.
Peter D. Turney. 2002. Thumbs up or thumbs down:
Semantic orientation applied to unsupervised classifi-
cation of reviews. In Proceedings of ACL 2002.
Luis von Ahn and Laura Dabbish. 2004. Labeling im-
ages with a computer game. In Proceedings of CHI
2004, pages 319?326.
Janyce Wiebe and E. Riloff. 2005. Creating subjec-
tive and objective sentence classifiers from unanno-
tated texts. In Proceedings of CICLing 2005.
Janyce Wiebe, Theresa Wilson, Rebecca Bruce, Matthew
Bell, and Melanie Martin. 2004. Learning subjective
language. Computational Linguistics, 30 (3).
Jeonghee Yi, Tetsuya Nasukawa, Razvan Bunescu, and
Wayne Niblack. 2003. Sentiment analyzer: Extract-
ing sentiments about a given topic using natural lan-
guage processing technique. In Proceedings of the
International Conference on Data Mining (ICDM),
pages 427?434.
Hong Yu and Vasileios Hatzivassiloglou. 2003. Towards
answering opinion questions: Separating facts from
opinions and identifying the polarity of opinion sen-
tences. In Proceedings of EMNLP 2003.
35
Proceedings of the NAACL HLT Workshop on Active Learning for Natural Language Processing, pages 49?57,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Active Dual Supervision: Reducing the Cost
of Annotating Examples and Features
Prem Melville
IBM T.J. Watson Research Center
Yorktown Heights, NY 10598
pmelvil@us.ibm.com
Vikas Sindhwani
IBM T.J. Watson Research Center
Yorktown Heights, NY 10598
vsindhw@us.ibm.com
Abstract
When faced with the task of building machine
learning or NLP models, it is often worthwhile
to turn to active learning to obtain human an-
notations at minimal costs. Traditional active
learning schemes query a human for labels of
intelligently chosen examples. However, hu-
man effort can also be expended in collecting
alternative forms of annotations. For example,
one may attempt to learn a text classifier by
labeling class-indicating words, instead of, or
in addition to, documents. Learning from two
different kinds of supervision brings a new,
unexplored dimension to the problem of ac-
tive learning. In this paper, we demonstrate
the value of such active dual supervision in
the context of sentiment analysis. We show
how interleaving queries for both documents
and words significantly reduces human effort
? more than what is possible through tradi-
tional one-dimensional active learning, or by
passive combinations of supervisory inputs.
1 Introduction
As a canonical running example for the theme of
this paper, consider the problem of sentiment anal-
ysis (Pang and Lee, 2008). Given a piece of text as
input, the desired output is a polarity score that indi-
cates whether this text expresses a positive or nega-
tive opinion towards a topic of interest. From a ma-
chine learning viewpoint, this problem may be posed
as a typical binary text classification task. Senti-
ment, however, is often conveyed with subtle lin-
guistic mechanisms such as sarcasm, negation and
the use of highly domain-specific and contextual
cues. This brings a multi-disciplinary flavor to the
problem, drawing interest from both Natural Lan-
guage Processing and Machine Learning communi-
ties.
Many methodologies proposed in these disci-
plines share a common limitation that their perfor-
mance is bounded by the amount and quality of la-
beled data. However, they differ conceptually in
the type of human effort they require. On one
hand, supervised machine learning techniques re-
quire human effort in acquiring labeled examples,
which requires reading documents and annotating
them with their aggregate sentiment. On the other
hand, dictionary-based NLP systems require human
effort in collecting labeled features: for example,
in the domain of movie reviews, words that evoke
positive sentiment (e.g., ?mesmerizing?, ?thrilling?
etc) may be labeled positive, while words that evoke
negative sentiment (e.g., ?boring?,?disappointing?)
may be labeled negative. This kind of annotation
requires a human to condense prior linguistic expe-
rience with a word into a sentiment label that reflects
the net emotion that the word evokes.
We refer to the general setting of learning from
both labels on examples and features as dual super-
vision. This setting arises more broadly in tasks
where in addition to labeled documents, it is fre-
quently possible to provide domain knowledge in the
form of words, or phrases (Zaidan and Eisner, 2008)
or even more sophisticated linguistic features, that
associate strongly with a class. Recent work (Druck
et al, 2008; Sindhwani and Melville, 2008) has
demonstrated that the presence of word supervision
can greatly reduce the number of labeled documents
49
required to build high quality text classifiers.
In general, these two sources of supervision are
not mutually redundant, and have different annota-
tion costs, human response quality, and degrees of
utility towards learning a dual supervision model.
This leads naturally to the problem of active dual
supervision, or, how to optimally query a human or-
acle to simultaneously collect document and feature
annotations, with the objective of building the high-
est quality model with the lowest cost. Much of the
machine learning literature on active learning has
focused on one-sided example-only annotation for
classification problems. Less attention has been de-
voted to simultaneously acquiring alternative forms
of supervisory domain knowledge, such as the kind
routinely encountered in NLP. Our contribution may
be viewed as a step in this direction.
2 Dual supervision
Most work in supervised learning has focused on
learning from examples, each represented by a set
of feature values and a class label. In dual super-
vision we consider an additional aspect, by way of
labels of features, which convey prior knowledge on
associations of features to particular classes. Since
we deal only with text classification in this paper, all
features represent term-frequencies of words, and as
such we use feature and word interchangeably.
The active learning schemes we explore in this pa-
per are broadly applicable to any learner that can
support dual supervision, but here we focus on ac-
tive learning for the Pooling Multinomials classi-
fier (Melville et al, 2009) described below. In con-
current related work, we propose active dual su-
pervision schemes for a class of graph-based and
kernel-based dual supervision methods (Sindhwani
et al, 2009).
2.1 Pooling Multinomials
The Pooling Multinomials classifier was introduced
by Melville et al (2009) as an approach to incorpo-
rate prior lexical knowledge into supervised learn-
ing for better sentiment detection. In the context of
sentiment analysis, lexical knowledge is available in
terms of the prior sentiment-polarity of words. From
a dual supervision point of view, this knowledge can
be seen as labeled features, since the lexicon effec-
tively provides associations of a set of words with
the positive or negative class.
Pooling Multinomials classifies unlabeled exam-
ples just as in multinomial Na??ve Bayes classifica-
tion (McCallum and Nigam, 1998), by predicting
the class with the maximum likelihood, given by
argmaxcjP (cj)
?
i P (wi|cj); where P (cj) is the
prior probability of class cj , and P (wi|cj) is the
probability of word wi appearing in a document of
class cj . In the absence of background knowledge
about the class distribution, we estimate the class
priors P (cj) solely from the training data. However,
unlike regular Na??ve Bayes, the conditional prob-
abilities P (wi|cj) are computed using both the la-
beled examples and the labeled features.
Pooling distributions is a general approach for
combining information from multiple sources or ex-
perts; where experts are typically represented in
terms of probability distributions (Clemen and Win-
kler, 1999). Here, we only consider the special case
of combining multinomial distributions from two
sources ? namely, the labeled examples and labeled
features. The multinomial parameters of such mod-
els can be easily combined using the linear opin-
ion pool (Clemen and Winkler, 1999), in which
the aggregate probability is given by P (wi|cj) =
?Pe(wi|cj) + (1 ? ?)Pf (wi|cj); where Pe(wi|cj)
and Pf (wi|cj) represent the probability assigned by
using the example labels and feature labels respec-
tively, and ? is the weight for combining these dis-
tributions. The weight indicates a level of confi-
dence in each source of information, and Melville
et al (2009) explore ways of automatically selecting
this weight. However, in order to not confound our
results with the choice of weight-selection mecha-
nism, here we make the simplifying assumption that
the two experts based on instance and feature labels
are equally valuable, and as such set ? to 0.5.
To learn a model from the labeled examples we
compute conditionals Pe(wi|cj) based on observed
term frequencies, as in standard Na??ve Bayes classi-
fication. In addition, for Pooling Multinomials we
need to construct a multinomial model represent-
ing the labeled features in the background knowl-
edge. For this, we assume that the feature-class as-
sociations provided by labeled features are implic-
itly arrived at by human experts by examining many
positive and negative sentiment documents. So we
50
attempt to select the parameters Pf (wi|cj) of the
multinomial distributions that would generate such
documents. The exact values of these condition-
als are presented below. Their derivation is not di-
rectly pertinent to the subject of this paper, but can
be found in (Melville et al, 2009).
Given:
V ? the vocabulary, i.e., set of words in our domain
P ? set of words labeled as positive
N ? set of words labeled as negative
U ? set of unknown words, i.e. V ? (N ? P)
m ? size of vocabulary, i.e. |V|
p ? number of positive words, i.e. |P|
n ? number of negative words, i.e. |N |
All words in the vocabulary can be divided into
three categories ? words with a positive label, nega-
tive label, and unknown label. We refer to the prob-
ability of any positive term appearing in a positive
document simply as Pf (w+|+). Similarly, we refer
to the probability of any negative term appearing in a
negative document as Pf (w?|?); and the probabil-
ity of an unknown word in a positive or negative con-
text as Pf (wu|+) and Pf (wu|?) respectively. The
generative model for labeled features can then be de-
fined by:
Pf (w+|+) = Pf (w?|?) = 1p + n
Pf (w+|?) = Pf (w?|+) = 1p + n ?
1
r
Pf (wu|+) = n(1? 1/r)(p + n)(m? p? n)
Pf (wu|?) = p(1? 1/r)(p + n)(m? p? n)
where, the polarity level, r, is a measure of how
much more likely it is for a positive term to occur
in a positive document compared to a negative term.
The value of r is set to 100 in our experiments, as
done in (Melville et al, 2009).
2.2 Learning from example vs. feature labels
Dual supervision makes it possible to learn from la-
beled examples and labeled features simultaneously;
and, as in most supervised learning tasks, one would
expect more labeled data of either form to lead to
more accurate models. In this section we explore the
influence of increased number of instance labels and
feature labels independently, and also in tandem.
For these, and all subsequent experiments, we
use 10-fold cross-validation on the publicly avail-
able data of movie reviews provided by Pang et
al. (2002). This data consists of 1000 positive
and 1000 negative reviews from the Internet Movie
Database; where positive labels were assigned to re-
views that had a rating above 3.5 stars and negative
labels were assigned to ratings of 2 stars and below.
We use a bag-of-words representation of reviews,
where each review is represented by the term fre-
quencies of the 5000 most frequent words across all
reviews, excluding stop-words.
In order to study the effect of increasing number
of labels we need to simulate a human oracle label-
ing data. In the case of examples this is straight-
forward, since all examples in the Movies dataset
have labels. However, in the case of features, we
do not have a gold-standard set of feature labels. So
in order to simulate human responses to queries for
feature labels, we construct a feature oracle in the
following manner. The information gain of words
with respect to the known true class labels in the
dataset is computed using binary feature represen-
tations. Next, out of the 5000 total words, the top
1000 as ranked by information gain are assigned a
label. This label is the class in which the word ap-
pears more frequently. The oracle returns a ?dont
know? response for the remaining words. Thus, this
oracle simulates a human domain expert who is able
to recognize and label the most relevant task-specific
words, and also reject a word that falls below the rel-
evance threshold. For instance, in sentiment classi-
fication, we would expect a ?don?t know? response
for non-polar words.
We ran experiments beginning with a classifier
provided with labels for 10 randomly selected in-
stances and 10 randomly selected features. We then
compare three schemes - Instances-then-features,
Features-then-instances, and Passive Interleaving.
As the name suggests, Instances-then-features, is
provided labels for randomly selected instances until
all instances have been labeled, and then switches to
labeling features. Similarly, Features-then-instances
acquires labels for randomly selected features first
and then switches to getting instance labels. In
Passive Interleaving we probabilistically switch be-
51
tween issuing queries for randomly chosen instance
and feature labels. In particular, at each step we
choose to query for an instance with probability
0.36, otherwise we query for a feature label. The
instance-query rate of 0.36 is selected based on the
ratio of available instances (1800) to available fea-
tures (5000). The results of these learning curves are
presented in Fig. 1. Note that the x-axis in the figure
corresponds to the number of queries issued. As dis-
cussed earlier, in the case of features, the oracle may
respond to a query with a class label or may issue
a ?don?t know? response, indicating that no label is
available. As such, the number of feature-queries
on the x-axis does not correspond to the number
of actual known feature labels. We would expect
that on average 1 in 5 feature-label queries prompts
a response from the feature oracle that results in a
known feature label being provided.
At the end of the learning curves, each method
has labels for all available instances and features;
and as such, the last points of all three curves are
identical. The results show that fixing the number
of labeled features, and increasing the number of la-
beled instances steadily improves classification ac-
curacy. This is what one would expect from tra-
ditional supervised learning curves. More interest-
ingly, the results also indicate that we can fix the
number of instances, and improve accuracy by la-
beling more features. Finally, results on Passive In-
terleaving show that, though both feature labels and
example labels are beneficial by themselves, dual su-
pervision which exploits the interaction of examples
and features does in fact benefit from acquiring both
types of labels concurrently.
For all results above, we are selecting instances
and/or features to be labeled uniformly at random.
Based on previous work in active learning one would
expect that we can select instances to be labeled
more efficiently, by having the learner decide which
instances it is most likely to benefit from. The results
in this section suggests that actively selecting fea-
tures to be labeled may also be beneficial. Further-
more, the Passive Interleaving results suggest that an
ideal active dual supervision scheme would actively
select both instances and features for labeling. We
begin by exploring active learning for feature labels
in the next section, and then consider the simultane-
ous selection of instances and features in Sec. 4.
 50
 55
 60
 65
 70
 75
 80
 85
 90
 0  1000  2000  3000  4000  5000  6000  7000
Ac
cu
ra
cy
Number of queries
Instances-then-features
Features-then-instances
Passive Interleaving
Figure 1: Comparing the effect of instance and feature
label acquisition in dual supervision.
3 Acquiring feature labels
Traditional active learning has primarily focused on
selecting unlabeled instances to be labeled. The
dual-supervision setting now provides us with an ad-
ditional dimension to active learning, where labels
may also be acquired for features. In this section
we look at the novel task of active learning applied
only to feature-label acquisition. In Section 4 we
study the more general task of active dual supervi-
sion, where both instance and feature labels may be
acquired concurrently.
3.1 Feature uncertainty vs. certainty
A very common approach to active learning for in-
stances is Uncertainty Sampling (Lewis and Catlett,
1994). In this approach we acquire labels for in-
stances that the current model is most uncertain
about. Uncertainty Sampling is founded on the
heuristic that uncertain instances are close to the cur-
rent classification boundary, and acquiring the cor-
rect labels for them are likely to help refine the loca-
tion of this boundary. Despite its simplicity, Uncer-
tainty Sampling is usually quite effective in practice;
which raises the question of whether one can apply
the same principle to feature-label acquisition. In
this case, we want to select unlabeled features that
the current model is most uncertain about.
Much like instance uncertainty, feature uncer-
tainty can be measured in different ways, depend-
ing on the underlying method used for dual super-
vision. For instance, if the learner produces a lin-
52
ear classifier as in (Sindhwani and Melville, 2008),
we could use the magnitude of the weights on the
features as a measure of uncertainty ? where lower
weights indicate less certainty. Since Pooling Multi-
nomials builds a multinomial Na??ve Bayes model,
we can directly use the model?s conditional proba-
bilities of each word (feature) given a class.
For ease of exposition we refer to the two classes
in binary classification as postive (+) and negative
(-), without loss of generality. Given the probabili-
ties of word f belonging to the positive and negative
class, P (f |+) and P (f |?), we can determine the
uncertainty of a feature using the absolute value of
the log-odds ratio, i.e.,
abs
(
log
(P (f |+)
P (f |?)
))
(1)
The smaller this value, the more uncertain the model
is about the feature?s class association. In every it-
eration of active learning we can select the features
with the lowest certainty scores. We refer to this ap-
proach as Feature Uncertainty.
Though Uncertainty Sampling for features seems
like an appealing notion, it may not lead to better
models. If a classifier is uncertain about a feature,
it may have insufficient information about this fea-
ture and may indeed benefit from learning its la-
bel. However, it is also quite likely that a feature
has a low certainty score because it does not carry
much discriminative information about the classes.
In the context of sentiment detection, one would ex-
pect that neutral/non-polar words will appear to be
uncertain words. For example, words such as ?the?
which are unlikely to help in discriminating between
classes, are also likely to be considered the most un-
certain. As we shortly report, on the movies dataset,
Feature Uncertainty ends up wasting queries on such
words ending up with performance inferior to ran-
dom feature queries. What works significantly bet-
ter is an alternative strategy which acquires labels
for features in the descending order of the score in
Eq 1. We refer to this approach as Feature Certainty.
3.2 Expected feature utility
The intuition underlying the feature certainty heuris-
tic is that it serves to confirm or correct the orienta-
tion of model probabilities on different words during
the active learning process. One can argue that fea-
ture certainty is also suboptimal in that queries may
be wasted simply confirming confident predictions,
which is of limited utility to the model. An alterna-
tive to using a certainty-based heuristic, is to directly
estimate the expected value of acquiring each fea-
ture label. Such Expected Utility (Estimated Risk
Minimization) approaches have been applied suc-
cessfully to traditional active learning (Roy and Mc-
Callum, 2001), and to active feature-value acquisi-
tion (Melville et al, 2005). In this section we de-
scribe how this Expected Utility framework can be
adapted for feature-label acquisition.
At every step of active learning for features, the
next best feature to label is one that will result in
the highest improvement in classifier performance.
Since the true label of the unlabeled features are
unknown prior to acquisition, it is necessary to es-
timate the potential impact of every feature query
for all possible outcomes.1 Hence, the decision-
theoretic optimal policy is to ask for feature labels
which, once incorporated into the data, will result in
the highest increase in classification performance in
expectation.
If fj is the label of the j-th feature, and qj is the
query for this feature?s label, then the Expected Util-
ity of a feature query qj can be computed as:
EU(qj) =
K?
k=1
P (fj = ck)U(fj = ck) (2)
Where P (fj = ck) is the probability that fj will be
labeled with class ck, and U(fj = ck) is the util-
ity to the model of knowing that fj has the label
ck. In practice, the true values of these two quan-
tities are unknown, and the main challenge of any
Expected Utility approach is to accurately estimate
these quantities from the data currently available.
A direct way to estimate the utility of a feature la-
bel to classification, is to measure classification ac-
curacy on the training set of a model built using this
feature label. However, small changes in the model
that may result from a acquiring a single additional
feature label may not be reflected by a change in ac-
curacy. As such, we use a more fine-grained mea-
sure of classifier performance, Log Gain, which is
1In the case of binary polarity classification, the possible
outcomes are a positive or negative label for a queried feature.
53
computed as follows. For a model induced from a
training set T , let P? (ck|xi) be the probability es-
timated by the model that instance xi belongs to
class ck; and I is an indicator function such that
I(ck, xi) = 1 if ck is the correct class for xi and
I(ck, xi) = 0, otherwise. Log Gain is then defined
as:
LG(xi) = ?
K?
k=1
I(ck)P? (ck|xi) (3)
Then the utility of a classifier, U , can be measured
by summing the Log Gain for all instances in the
training set T . A lower value of Log Gain indi-
cates a better classifier performance. For a deeper
discussion of this measure see (Saar-Tsechansky et
al., 2008).
In Eq. 2, apart from the measure of utility, we
also do not know the true probability distribution
of labels for the feature under consideration. This
too can be estimated from the training data, by see-
ing how frequently the word appears in documents
of each class. In a multinomial Na??ve Bayes model
we already collect these statistics in order to deter-
mine the conditional probability of a class given a
word, i.e. P (fj |ck). We can use these probabilities
to get an estimate of the feature label distribution,
P? (fj = ck) = P (fj |ck)?K
k=1 P (fj |ck)
.
Given the estimated values of the feature-label
distribution and the utility of a particular feature
query outcome, we can now estimate the Expected
Utility of each unknown feature, and select the fea-
tures with the highest Expected Utility to modeling.
Though theoretically appealing, this approach is
quite computationally intensive if applied to evalu-
ate all unknown features. In the worst case it re-
quires building and evaluating models for each pos-
sible outcome of each unlabeled feature query. If
you have m features and K classes, this approach
requires training O(mK) classifiers. However, the
complexity of the approach can be significantly al-
leviated by only applying Expected Utility evalua-
tion to a sub-sample of all unlabeled features. Given
the large number of features with no true class la-
bels, selecting a sample of available features uni-
formly at random may be sub-optimal. Instead we
select a sample of features based on Feature Cer-
tainty. In particular we select the top 100 unknown
features that the current model is most certain about,
and identify the features in this pool with the highest
Expected Utility. We refer to this approach as Ex-
pected Feature Utility. We use Feature Certainty to
sub-sample the available feature queries, since this
approach is more likely to select features for which
the label is known by the Oracle.
3.3 Active learning with feature labels
We ran experiments comparing the three different
active learning approaches described above. In
these, and all subsequent experiments, we begin
with a model trained on 10 labeled features and 100
labeled instances, which were randomly selected.
From our prior efforts of manually labeling such
data, we find this to be a reasonable initial setting.
The experiments in this section focus only on the
selection of features to be labeled. So, in each itera-
tion of active learning we select the next 10 feature-
label queries, based on Feature Uncertainty, Feature
Certainty, or Expected Feature Utility. As a baseline,
we also compare to the performance of a model that
selects features uniformly at random. Our results are
presented in Fig. 2.
 50
 55
 60
 65
 70
 75
 0  50  100  150  200  250  300  350  400
Ac
cu
ra
cy
Number of queries
Expected Feature Utility
Feature Certainty
Random Feature
Feature Uncertainty
Figure 2: Comparing different active learning approaches
for acquiring feature labels.
The results show that Feature Uncertainty, which
is a direct analog of Uncertainty Sampling, actu-
ally performs worse than random sampling. Many
uncertain features may actually not be very useful
in discriminating between the classes, and selecting
them can be systematically worse than selecting uni-
formly at random. However, the converse approach
54
of Feature Certainty does remarkably well. This
may be because polarized words are better for learn-
ing, but it is also likely that querying for such words
increases the likelihood of selecting one whose label
is known to the oracle.
The results on Expected Feature Utility show that
estimating the expected impact of potential labels
for features does in fact perform much better than
feature certainty. The results confirm that despite
our crude estimations in Eq. 2, Expected Feature
Utility is an effective approach to active learning of
feature labels. Furthermore, we demonstrate that by
applying the approach to only a small sub-sample of
certain features, we are able to make this method
computationally feasible to use in practice. In-
creasing the size of the sample of candidate feature
queries is likely to improve performance, at the cost
of increased time in selecting queries.
4 Active dual supervision
In the previous section we demonstrated that ac-
tively selecting informative features to be labeled is
significantly better than random selection. In this
section, we look at the complementary task of se-
lecting instances to be labeled, and combined active
learning for both forms of supervision.
Selecting unlabeled examples for learning has
been a well-studied problem, and we use Uncer-
tainty Sampling (Lewis and Catlett, 1994), which
has been shown to be a computationally efficient
and effective approach in the literature. In particular
we select unlabeled examples to be labeled in order
of decreasing uncertainty, where uncertainty is mea-
sured in terms of the margin, as done in (Melville
and Mooney, 2004). The margin on an unlabeled ex-
ample is defined as the absolute difference between
the class probabilities predicted by the classifier for
the given example, i.e., |P (+|x)? P (?|x)|. We re-
fer to the selection of instances based on this uncer-
tainty as Instance Uncertainty, in order to distinguish
it from Feature Uncertainty.
We ran experiments as before, comparing selec-
tion of instances using Instance Uncertainty and se-
lection of features using Expected Feature Utility.
In addition, we also combine these to methods by
interleaving feature and instance selection. In par-
ticular, we first order instances in decreasing order
of uncertainty, and features in terms of decreasing
Expected Feature Utility. We then probabilistically
select instances or features from the top of these
lists, where, as before, the probability of selecting
an instance is 0.36. Recall that this probability cor-
responds to the ratio of available instances (1800)
and features (5000). We refer to this approach as Ac-
tive Interleaving, in contrast to Passive Interleaving,
which we also present as a baseline. Recall that Pas-
sive Interleaving corresponds to probabilistically in-
terleaving queries for randomly chosen, not actively
chosen, examples and features. Our results are pre-
sented in Fig. 3.
We observe that, Instance Uncertainty performs
better than Passive Interleaving, which in turn is bet-
ter than random selection of only instances or fea-
tures ? as seen in Fig. 1. However, effectively se-
lecting features labels, via Expected Feature Util-
ity, does even better than actively selecting only in-
stances. Finally, selecting instance and features si-
multaneously via Active Interleaving performs bet-
ter than active learning of features or instances sep-
arately. Active Interleaving is indeed very effective,
reaching an accuracy of 77% with only 500 queries,
while Passive Interleaving requires more than 4000
queries to reach the same performance ? as evi-
denced by Fig. 1
 50
 55
 60
 65
 70
 75
 80
 0  50  100  150  200  250  300  350  400
Ac
cu
ra
cy
Number of queries
Active Interleaving
Expected Feature Utility
Instance Uncertainty
Passive Interleaving
Figure 3: Comparing Active Interleaving to alternative
label acquisition strategies.
5 Related work
Active learning in the context of dual supervision
models is a new area of research with very little prior
55
work, to the best of our knowledge. Most prior work
has focused on pooled-based active learning, where
examples from an unlabeled pool are selected for la-
beling (Cohn et al, 1994; Tong and Koller, 2000). In
contrast, active feature-value acquisition (Melville
et al, 2005) and budgeted learning (Lizotte et al,
2003) focus on estimating the value of acquiring
missing features, but do not deal with the task of
learning from feature labels. In contrast, Raghavan
and Allan (2007) and Raghavan et al (2006) study
the problem of tandem learning where they combine
uncertainty sampling for instances along with co-
occurence based interactive feature selection. God-
bole et al (2004) propose notions of feature uncer-
tainty and incorporate the acquired feature labels,
into learning by creating one-term mini-documents.
Learning from labeled examples and features via
dual supervision, is itself a new area of research.
Sindhwani et al (2008) use a kernel-based frame-
work to build dual supervision into co-clustering
models. Sindhwani and Melville (2008) apply sim-
ilar ideas for graph-based sentiment analysis. There
have also been previous attempts at using only fea-
ture supervision, mostly along with unlabeled doc-
uments. Much of this work (Schapire et al, 2002;
Wu and Srihari, 2004; Liu et al, 2004; Dayanik
et al, 2006) has focused on using labeled features
to generate pseudo-labeled examples that are then
used with well-known models. In contrast, Druck
et al (2008) constrain the outputs of a multinomial
logistic regression model to match certain reference
distributions associated with labeled features.
6 Perspectives and future work
Though Active Interleaving is a very effective ap-
proach to active dual supervision, there is still a lot
of room for improvement. Firstly, Active Interleav-
ing relies on Uncertainty Sampling for the selection
of instances. Though Uncertainty Sampling has the
advantage of being fast and effective, there exist ap-
proaches that lead to better models with fewer ex-
amples ? usually at the cost of computation time.
One such method, estimating error reduction (Roy
and McCallum, 2001), is a direct analog of Ex-
pected Feature Utility applied to instance selection.
One would expect that an improvement in instance
selection, should directly improve any method that
combines instance and feature label selection. Sec-
ondly, Active Interleaving uses the simple approach
of probabilistically choosing to select an instance or
feature for each subsequent query. However, a more
intelligent active scheme should be able to assess if
an instance or feature would be more beneficial at
each step. Furthermore, we do not currently con-
sider the cost of acquiring labels. Presumably la-
beling a feature versus labeling an instance could
incur very different costs ? which could be mone-
tary costs or time taken for each annotation. Fortu-
nately, the Expected Utility method is very flexible,
and allows us to address all these issues within a sin-
gle framework. We can specifically estimate the ex-
pected utility of different forms of annotation, per
unit cost. For instance, Provost et al (2007) use
such an approach to estimate the utility of acquir-
ing class labels and feature values (not labels) per
unit cost, within one unified framework. A similar
method can be applied for a holistic approach to ac-
tive dual supervision, where the Expected Utility of
an instance or feature label query q, can be computed
as EU(q) = ?Kk=1 P (q = ck)U(q=ck)?q ; where ?q is
cost of the query q, and utility U can be computed as
in Eq. 3. By evaluating instances and features on the
same scale, and by measuring utility per unit cost of
acquisition, such a framework should enable us to
handle the trade-off between the costs and benefits
of the different types of acquisitions. The primary
challenge in the success of this approach is to accu-
rately and efficiently estimate the different quantities
in the equation above, using only the training data
currently available. These are directions for future
exploration.
7 Conclusions
This paper is a preliminary foray into active dual su-
pervision. We have demonstrated that not only is
combining example and feature labels beneficial for
modeling, but that actively selecting the most infor-
mative examples and features for labeling can sig-
nificantly reduce the burden of annotating such data.
In future work, we would like to explore more effec-
tive solutions to the problem, and also to corroborate
our results on a larger number of datasets and under
different experimental settings.
56
References
R. T. Clemen and R. L. Winkler. 1999. Combining prob-
ability distributions from experts in risk analysis. Risk
Analysis, 19:187?203.
D. Cohn, L. Atlas, and R. Ladner. 1994. Improving gen-
eralization with active learning. Machine Learning,
15(2):201?221.
Aynur Dayanik, David D. Lewis, David Madigan,
Vladimir Menkov, and Alexander Genkin. 2006. Con-
structing informative prior distributions from domain
knowledge in text classification. In SIGIR.
G. Druck, G. Mann, and A. McCallum. 2008. Learn-
ing from labeled features using generalized expecta-
tion criteria. In SIGIR.
S. Godbole, A. Harpale, S. Sarawagi, and S. Chakrabarti.
2004. Document classification through interactive su-
pervision of document and term labels. In PKDD.
David D. Lewis and Jason Catlett. 1994. Heteroge-
neous uncertainty sampling for supervised learning. In
Proc. of 11th Intl. Conf. on Machine Learning (ICML-
94), pages 148?156, San Francisco, CA, July. Morgan
Kaufmann.
Bing Liu, Xiaoli Li, Wee Sun Lee, and Philip Yu. 2004.
Text classification by labeling words. In AAAI.
Dan Lizotte, Omid Madani, and Russell Greiner. 2003.
Budgeted learning of naive-Bayes classifiers. In UAI.
Andrew McCallum and Kamal Nigam. 1998. A com-
parison of event models for naive Bayes text classifi-
cation. In AAAI Workshop on Text Categorization.
Prem Melville and Raymond J. Mooney. 2004. Diverse
ensembles for active learning. In Proc. of 21st Intl.
Conf. on Machine Learning (ICML-2004), pages 584?
591, Banff, Canada, July.
Prem Melville, Maytal Saar-Tsechansky, Foster Provost,
and Raymond Mooney. 2005. An expected utility ap-
proach to active feature-value acquisition. In ICDM.
Prem Melville, Wojciech Gryc, and Richard Lawrence.
2009. Sentiment analysis of blogs by combining lexi-
cal knowledge with text classification. In KDD.
Bo Pang and Lilian Lee. 2008. Opinion mining and sen-
timent analysis. Foundations and Trends in Informa-
tion Retrieval: Vol. 2: No 1, pp 1-135.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? sentiment classification using ma-
chine learning techniques. In EMNLP.
Foster Provost, Prem Melville, and Maytal Saar-
Tsechansky. 2007. Data acquisition and cost-effective
predictive modeling: Targeting offers for electronic
commerce. In ICEC ?07: Proceedings of the ninth in-
ternational conference on Electronic commerce.
Hema Raghavan, Omid Madani, and Rosie Jones. 2006.
Active learning with feedback on features and in-
stances. J. Mach. Learn. Res., 7:1655?1686.
H. Raghavan, O. Madani, and R. Jones. 2007. An inter-
active algorithm for asking and incorporating feature
feedback into support vector machines. In SIGIR.
Nicholas Roy and Andrew McCallum. 2001. Toward
optimal active learning through sampling estimation of
error reduction. In ICML.
Maytal Saar-Tsechansky, Prem Melville, and Foster
Provost. 2008. Active feature-value acquisition. In
Management Science.
Robert E. Schapire, Marie Rochery, Mazin G. Rahim, and
Narendra Gupta. 2002. Incorporating prior knowl-
edge into boosting. In ICML.
Vikas Sindhwani and Prem Melville. 2008. Document-
word co-regularization for semi-supervised sentiment
analysis. In ICDM.
Vikas Sindhwani, Jianying Hu, and Alexandra Mo-
jsilovic. 2008. Regularized co-clustering with dual
supervision. In NIPS.
Vikas Sindhwani, Prem Melville, and Richard Lawrence.
2009. Uncertainty sampling and transductive experi-
mental design for active dual supervision. In ICML.
Simon Tong and Daphne Koller. 2000. Support vec-
tor machine active learning with applications to text
classification. In Proc. of 17th Intl. Conf. on Machine
Learning (ICML-2000).
Xiaoyun Wu and Rohini Srihari. 2004. Incorporating
prior knowledge with weighted margin support vector
machines. In KDD.
O. F. Zaidan and J. Eisner. 2008. Modeling annotators:
A generative approach to learning from annotator ra-
tionales. In EMNLP.
57
