Proceedings of the 43rd Annual Meeting of the ACL, pages 223?230,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Exploring and Exploiting the Limited Utility of Captions in Recognizing
Intention in Information Graphics?
Stephanie Elzer1 and Sandra Carberry2 and Daniel Chester2 and Seniz Demir2 and
Nancy Green3 and Ingrid Zukerman4 and Keith Trnka2
1Dept. of Computer Science, Millersville University, Millersville, PA 17551
2Dept. of Computer Science, University of Delaware, Newark, DE 19716
3Dept. of Mathematical Sciences, Univ. of NC at Greensboro, Greensboro, NC 27402
4School of CS & Software Engrg, Monash Univ., Clayton, Victoria 3800 Australia
Abstract
This paper presents a corpus study that ex-
plores the extent to which captions con-
tribute to recognizing the intended mes-
sage of an information graphic. It then
presents an implemented graphic interpre-
tation system that takes into account a va-
riety of communicative signals, and an
evaluation study showing that evidence
obtained from shallow processing of the
graphic?s caption has a significant impact
on the system?s success. This work is part
of a larger project whose goal is to provide
sight-impaired users with effective access
to information graphics.
1 Introduction
Language research has posited that a speaker or
writer executes a speech act whose intended mean-
ing he expects the listener to be able to deduce, and
that the listener identifies the intended meaning by
reasoning about the observed signals and the mutual
beliefs of author and interpreter (Grice, 1969; Clark,
1996). But as noted by Clark (Clark, 1996), lan-
guage is more than just words. It is any ?signal? (or
lack of signal when one is expected), where a sig-
nal is a deliberate action that is intended to convey a
message.
Although some information graphics are only in-
tended to display data values, the overwhelming ma-
jority of the graphics that we have examined (taken
?Authors can be reached via email as fol-
lows: elzer@cs.millersville.edu, nlgreen@uncg.edu,
{carberry, chester, demir, trnka}@cis.udel.edu, In-
grid.Zukerman@infotech.monash.edu.au.
1998 1999 2000 20011000
1500
2000
2500
3000
personal filingsLocal bankruptcy
Figure 1: Graphic from a 2001 Local Newspaper
from newspaper, magazine, and web articles) ap-
pear to have some underlying goal or intended mes-
sage, such as the graphic in Figure 1 whose com-
municative goal is ostensibly to convey the sharp in-
crease in local bankruptcies in the current year com-
pared with the previous decreasing trend. Applying
Clark?s view of language, it is reasonable to presume
that the author of an information graphic expects the
viewer to deduce from the graphic the message that
the graphic was intended to convey, by reasoning
about the graphic itself, the salience of entities in
the graphic, and the graphic?s caption.
This paper adopts Clark?s view of language as any
deliberate signal that is intended to convey a mes-
sage. Section 3 investigates the kinds of signals used
in information graphics. Section 4 presents a cor-
pus study that investigates the extent to which cap-
tions capture the message of the graphic, illustrates
the issues that would arise in trying to fully under-
stand such captions, and proposes shallow process-
ing of the caption to extract evidence from it. Sec-
tion 5 then describes how evidence obtained from
a variety of communicative signals, including shal-
low processing of the graphic?s caption, is used in a
probabilistic system for hypothesizing the intended
message of the graphic. Section 6 presents an eval-
223
10
 5
15
0?680+ 65?79 7?19 35?4980+65?7950?6435?49
10
 5
15
20?347?190?6 20?3450?64
(a) (b)
Figure 2: Two Alternative Graphs from the Same Data
uation showing the system?s success, with particu-
lar attention given to the impact of evidence from
shallow processing of the caption, and Section 7 dis-
cusses future work.
Although we believe that our findings are ex-
tendible to other kinds of information graphics, our
current work focuses on bar charts. This research is
part of a larger project whose goal is a natural lan-
guage system that will provide effective access to
information graphics for individuals with sight im-
pairments, by inferring the intended message under-
lying the graphic, providing an initial summary of
the graphic that includes the intended message along
with notable features of the graphic, and then re-
sponding to follow-up questions from the user.
2 Related Work
Our work is related to efforts on graph summariza-
tion. (Yu et al, 2002) used pattern recognition tech-
niques to summarize interesting features of automat-
ically generated graphs of time-series data from a
gas turbine engine. (Futrelle and Nikolakis, 1995)
developed a constraint grammar for parsing vector-
based visual displays and producing representations
of the elements comprising the display. The goal
of Futrelle?s project is to produce a graphic that
summarizes one or more graphics from a document
(Futrelle, 1999). The summary graphic might be a
simplification of a graphic or a merger of several
graphics from the document, along with an appropri-
ate summary caption. Thus the end result of summa-
rization will itself be a graphic. The long range goal
of our project, on the other hand, is to provide alter-
native access to information graphics via an initial
textual summary followed by an interactive follow-
up component for additional information. The in-
tended message of the graphic will be an important
component of the initial summary, and hypothesiz-
ing it is the goal of our current work.
3 Evidence about the Intended Message
The graphic designer has many alternative ways of
designing a graphic; different designs contain differ-
ent communicative signals and thus convey differ-
ent communicative intents. For example, consider
the two graphics in Figure 2. The graphic in Fig-
ure 2a conveys that average doctor visits per year
is U-shaped by age; it starts out high when one is
very young, decreases into middle age, and then
rises again as one ages. The graphic in Figure 2b
presents the same data; but instead of conveying a
trend, this graphic seems to convey that the elderly
and the young have the highest number of doctor vis-
its per year. These graphics illustrate how choice of
design affects the message that the graphic conveys.
Following the AutoBrief work (Kerpedjiev and
Roth, 2000) (Green et al, 2004) on generating
graphics that fulfill communicative goals, we hy-
pothesize that the designer chooses a design that best
facilitates the perceptual and cognitive tasks that
are most important to conveying his intended mes-
sage, subject to the constraints imposed by compet-
ing tasks. By perceptual tasks we mean tasks that
can be performed by simply viewing the graphic,
such as finding the top of a bar in a bar chart; by
cognitive tasks we mean tasks that are done via men-
tal computations, such as computing the difference
between two numbers.
Thus one source of evidence about the intended
message is the relative difficulty of the perceptual
tasks that the viewer would need to perform in order
to recognize the message. For example, determining
224
the entity with maximum value in a bar chart will be
easiest if the bars are arranged in ascending or de-
scending order of height. We have constructed a set
of rules, based on research by cognitive psycholo-
gists, that estimate the relative difficulty of perform-
ing different perceptual tasks; these rules have been
validated by eye-tracking experiments and are pre-
sented in (Elzer et al, 2004).
Another source of evidence is entities that have
been made salient in the graphic by some kind of fo-
cusing device, such as coloring some elements of the
graphic, annotations such as an asterisk, or an arrow
pointing to a particular location in a graphic. Enti-
ties that have been made salient suggest particular
instantiations of perceptual tasks that the viewer is
expected to perform, such as comparing the heights
of two highlighted bars in a bar chart.
And lastly, one would expect captions to help con-
vey the intended message of an information graphic.
The next section describes a corpus study that we
performed in order to explore the usefulness of cap-
tions and how we might exploit evidence from them.
4 A Corpus Study of Captions
Although one might suggest relying almost ex-
clusively on captions to interpret an information
graphic, (Corio and Lapalme, 1999) found in a cor-
pus study that captions are often very general. The
objective of their corpus study was to categorize the
kinds of information in captions so that their find-
ings could be used in forming rules for generating
graphics with captions.
Our project is instead concerned with recogniz-
ing the intended message of an information graphic.
To investigate how captions might be used in a sys-
tem for understanding information graphics, we per-
formed a corpus study in which we analyzed the
first 100 bar charts from our corpus of information
graphics; this corpus contains a variety of bar charts
from different publication venues. The following
subsections present the results of this corpus study.
4.1 Do Captions Convey the Intended
Message?
Our first investigation explored the extent to which
captions capture the intended message of an infor-
mation graphic. We extracted the first 100 graphics
Category #
Category-1: Captures intention (mostly) 34
Category-2: Captures intention (somewhat) 15
Category-3: Hints at intention 7
Category-4: No contribution to intention 44
Figure 3: Analysis of 100 Captions on Bar Charts
from our corpus of bar charts. The intended mes-
sage of each bar chart had been previously annotated
by two coders. The coders were asked to identify
1) the intended message of the graphic using a list
of 12 high-level intentions (see Section 5 for exam-
ples) and 2) the instantiation of the parameters. For
example, if the coder classified the intended mes-
sage of a graphic as Change-trend, the coder was
also asked to identify where the first trend began,
its general slope (increasing, decreasing, or stable),
where the change in trend occurred, the end of the
second trend, and the slope of the second trend. If
there was disagreement between the coders on either
the intention or the instantiation of the parameters,
we utilized consensus-based annotation (Ang et al,
2002), in which the coders discussed the graphic to
try to come to an agreement. As observed by (Ang
et al, 2002), this allowed us to include the ?harder?
or less obvious graphics in our study, thus lowering
our expected system performance. We then exam-
ined the caption of each graphic, and determined to
what extent the caption captured the graphic?s in-
tended message. Figure 3 shows the results. 44%
of the captions in our corpus did not convey to any
extent the message of the information graphic. The
following categorizes the purposes that these cap-
tions served, along with an example of each:
? general heading (8 captions): ?UGI Monthly
Gas Rates? on a graphic conveying a recent
spike in home heating bills.
? reference to dependent axis (15 captions):
?Lancaster rainfall totals for July? on a
graphic conveying that July-02 was the driest
of the previous decade.
? commentary relevant to graphic (4 captions):
?Basic performers: One look at the best per-
forming stocks in the Standard&Poor?s 500 in-
dex this year shows that companies with ba-
sic businesses are rewarding investors? on a
225
graphic conveying the relative rank of different
stocks, some of which were basic businesses
and some of which were not. This type of in-
formation was classified as deductive by (Corio
and Lapalme, 1999) since it draws a conclusion
from the data depicted in the graphic.
? commentary extending message of graphic (8
captions): ?Profits are getting squeezed? on
a graphic conveying that Southwest Airlines
net income is estimated to increase in 2003 af-
ter falling the preceding three years. Here the
commentary does not draw a conclusion from
the data in the graphic but instead supplements
the graphic?s message. However this type of
caption would probably fall into the deductive
class in (Corio and Lapalme, 1999).
? humor (7 captions): ?The Sound of Sales? on
a graphic conveying the changing trend (down-
ward after years of increase) in record album
sales. This caption has nothing to do with the
change-trend message of the graphic, but ap-
pears to be an attempt at humor.
? conclusion unwarranted by graphic (2 cap-
tions): ?Defense spending declines? on a
graphic that in fact conveys that recent defense
spending is increasing.
Slightly over half the captions (56%) contributed
to understanding the graphic?s intended message.
34% were judged to convey most of the intended
message. For example, the caption ?Tennis play-
ers top nominees? appeared on a graphic whose in-
tended message is to convey that more tennis players
were nominated for the 2003 Laureus World Sports
Award than athletes from any other sport. Since we
argue that captions alone are insufficient for inter-
preting information graphics, in the few cases where
it was unclear whether a caption should be placed
in Category-1 or Category-2, we erred on the side
of over-rating the contribution of a caption to the
graphic?s intended message. For example, consider
the caption ?Chirac is riding high in the polls?
which appeared on a graphic conveying that there
has been a steady increase in Chirac?s approval rat-
ings from 55% to about 75%. Although this caption
does not fully capture the communicative intention
of the graphic (since it does not capture the steady
increase conveyed by the graphic), we placed it in
the first category since one might argue that riding
high in the polls would suggest both high and im-
proving ratings.
15% of the captions were judged to convey only
part of the graphic?s intended message; an example
is ?Drug spending for young outpace seniors? that
appears on a graphic whose intended message ap-
pears to be that there is a downward trend by age for
increased drug spending; we classified the caption
in Category-2 since the caption fails to capture that
the graphic is talking about percent increases in drug
spending, not absolute drug spending, and that the
graphic conveys the downward trend for increases in
drug spending by age group, not just that increases
for the young were greater than for the elderly.
7% of the captions were judged to only hint at the
graphic?s message. An example is ?GM?s Money
Machine? which appeared on a graphic whose in-
tended message was a contrast of recent perfor-
mance against the previous trend ? ie., that al-
though there had been a steady decrease in the per-
centage of GM?s overall income produced by its fi-
nance unit, there was now a substantial increase in
the percentage provided by the finance unit. Since
the term money machine is a colloquialism that sug-
gests making a lot of money, the caption was judged
to hint at the graphic?s intended message.
4.2 Understanding Captions
For the 49 captions in Category 1 or 2 (where the
caption conveyed at least some of the message of
the graphic), we examined how well the caption
could be parsed and understood by a natural lan-
guage system. We found that 47% were fragments
(for example, ?A Growing Biotech Market?), or in-
volved some other kind of ill-formedness (for ex-
ample, ?Running tops in sneaker wear in 2002? or
?More seek financial aid?1). 16% would require ex-
tensive domain knowledge or analogical reasoning
to understand. One example is ?Chirac is riding
high in the polls? which would require understand-
ing the meaning of riding high in the polls. Another
example is ?Bad Moon Rising?; here the verb ris-
ing suggests that something is increasing, but the
1Here we judge the caption to be ill-formed due to the ellip-
sis since More should be More students.
226
system would need to understand that a bad moon
refers to something undesirable (in this case, delin-
quent loans).
4.3 Simple Evidence from Captions
Although our corpus analysis showed that captions
can be helpful in understanding the message con-
veyed by an information graphic, it also showed that
full understanding of a caption would be problem-
atic; moreover, once the caption was understood, we
would still need to relate it to the information ex-
tracted from the graphic itself, which appears to be
a difficult problem.
Thus we began investigating whether shallow pro-
cessing of the caption might provide evidence that
could be effectively combined with other evidence
obtained from the graphic itself. Our analysis pro-
vided the following observations:
? Verbs in a caption often suggest the kind of
message being conveyed by the graphic. An
example from our corpus is ?Boating deaths
decline?; the verb decline suggests that the
graphic conveys a decreasing trend. Another
example from our corpus is ?American Express
total billings still lag?; the verb lag suggests
that the graphic conveys that some entity (in
this case American Express) is ranked behind
some others.
? Adjectives in a caption also often suggest the
kind of message being conveyed by the graphic.
An example from our corpus is ?Air Force has
largest percentage of women?; the adjective
largest suggests that the graphic is conveying
an entity whose value is largest. Adjectives de-
rived from verbs function similarly to verbs.
An example from our corpus is ?Soaring De-
mand for Servers? which is the caption on a
graphic that conveys the rapid increase in de-
mand for servers. Here the adjective soaring is
derived from the verb soar, and suggests that
the graphic is conveying a strong increase.
? Nouns in a caption often refer to an entity that
is a label on the independent axis. When this
occurs, the caption brings the entity into focus
and suggests that it is part of the intended mes-
sage of the graphic. An example from our cor-
pus is ?Germans miss their marks? where the
graphic displays a bar chart that is intended to
convey that Germans are the least happy with
the Euro. Words that usually appear as verbs,
but are used in the caption as a noun, may func-
tion similarly to verbs. An example is ?Cable
On The Rise?; in this caption, rise is used as a
noun, but suggests that the graphic is conveying
an increase.
5 Utilizing Evidence
We developed and implemented a probabilistic
framework for utilizing evidence from a graphic and
its caption to hypothesize the graphic?s intended
message. To identify the intended message of a
new information graphic, the graphic is first given
to a Visual Extraction Module (Chester and Elzer,
2005) that is responsible for recognizing the indi-
vidual components of a graphic, identifying the re-
lationship of the components to one another and to
the graphic as a whole, and classifying the graphic
as to type (bar chart, line graph, etc.); the result is
an XML file that describes the graphic and all of its
components.
Next a Caption Processing Module analyzes the
caption. To utilize verb-related evidence from cap-
tions, we identified a set of verbs that would indicate
each category of high-level goal2, such as recover
for Change-trend and beats for Relative-difference;
we then extended the set of verbs by examining
WordNet for verbs that were closely related in mean-
ing, and constructed a verb class for each set of
closely related verbs. Adjectives such as more and
most were handled in a similar manner. The Caption
Processing Module applies a part-of-speech tagger
and a stemmer to the caption in order to identify
nouns, adjectives, and the root form of verbs and
adjectives derived from verbs. The XML represen-
tation of the graphic is augmented to indicate any
independent axis labels that match nouns in the cap-
tion, and the presence of a verb or adjective class in
the caption.
The Intention Recognition Module then analyzes
the XML file to build the appropriate Bayesian net-
work; the current system is limited to bar charts, but
2As described in the next paragraph, there are 12 categories
of high-level goals.
227
the principles underlying the system should be ex-
tendible to other kinds of information graphics. The
network is described in (Elzer et al, 2005). Very
briefly, our analysis of simple bar charts has shown
that the intended message can be classified into one
of 12 high-level goals; examples of such goals in-
clude:
? Change-trend: Viewer to believe that there
is a <slope-1> trend from <param1>
to <param2> and a significantly differ-
ent <slope-2> trend from <param3> to
<param4>
? Relative-difference: Viewer to believe that the
value of element <param1> is <comparison>
the value of element <param2> where
<comparison> is greater-than, less-than, or
equal-to.
Each category of high-level goal is represented by a
node in the network (whose parent is the top-level
goal node), and instances of these goals (ie., goals
with their parameters instantiated) appear as chil-
dren with inhibitory links (Huber et al, 1994) cap-
turing their mutual exclusivity. Each goal is broken
down further into subtasks (perceptual or cognitive)
that the viewer would need to perform in order to
accomplish the goal of the parent node. The net-
work is built dynamically when the system is pre-
sented with a new information graphic, so that nodes
are added to the network only as suggested by the
graphic. For example, low-level nodes are added for
the easiest primitive perceptual tasks and for per-
ceptual tasks in which a parameter is instantiated
with a salient entity (such as an entity colored dif-
ferently from others in the graphic or an entity that
appears as a noun in the caption), since the graphic
designer might have intended the viewer to perform
these tasks; then higher-level goals that involve these
tasks are added, until eventually a link is established
to the top-level goal node.
Next evidence nodes are added to the network to
capture the kinds of evidence noted in Sections 3
and 4.3. For example, evidence nodes are added to
the network as children of each low-level perceptual
task; these evidence nodes capture the relative dif-
ficulty (categorized as easy, medium, hard, or im-
possible) of performing the perceptual task as esti-
mated by our effort estimation rules mentioned in
Section 3, whether a parameter in the task refers to
an entity that is salient in the graphic, and whether
a parameter in the task refers to an entity that is a
noun in the caption. An evidence node, indicating
for each verb class whether that verb class appears
in the caption (either as a verb, or as an adjective de-
rived from a verb, or as a noun that can also serve as
a verb) is added as a child of the top level goal node.
Adjectives such as more and most that provide evi-
dence are handled in a similar manner.
In a Bayesian network, conditional probability ta-
bles capture the conditional probability of a child
node given the value of its parent(s). For example,
the network requires the conditional probability of
an entity appearing as a noun in the caption given
that recognizing the intended message entails per-
forming a particular perceptual task involving that
entity. Similarly, the network requires the condi-
tional probability, for each class of verb, that the
verb class appears in the caption given that the in-
tended message falls into a particular intention cat-
egory. These probabilities are learned from our cor-
pus of graphics, as described in (Elzer et al, 2005).
6 Evaluation
In this paper, we are particularly interested in
whether shallow processing of captions can con-
tribute to recognizing the intended message of an
information graphic. As mentioned earlier, the in-
tended message of each information graphic in our
corpus of bar charts had been previously annotated
by two coders. To evaluate our approach, we used
leave-one-out cross validation. We performed a se-
ries of experiments in which each graphic in the cor-
pus is selected once as the test graphic, the probabil-
ity tables in the Bayesian network are learned from
the remaining graphics, and the test graphic is pre-
sented to the system as a test case. The system was
judged to fail if either its top-rated hypothesis did
not match the intended message that was assigned
to the graphic by the coders or the probability rat-
ing of the system?s top-rated hypothesis did not ex-
ceed 50%. Overall success was then computed by
averaging together the results of the whole series of
experiments.
Each experiment consisted of two parts, one in
228
Diner?s Club
Discover
American Express
Mastercard
Visa
400 600200
Total credit card purchases per year in billions
Figure 4: A Graphic from Business Week3
which captions were not taken into account in the
Bayesian network and one in which the Bayesian
network included evidence from captions. Our
overall accuracy without the caption evidence was
64.5%, while the inclusion of caption evidence in-
creased accuracy to 79.1% for an absolute increase
in accuracy of 14.6% and a relative improvement of
22.6% over the system?s accuracy without caption
evidence. Thus we conclude that shallow process-
ing of a caption provides evidence that can be effec-
tively utilized in a Bayesian network to recognize
the intended message of an information graphic.
Our analysis of the results provides some interest-
ing insights on the role of elements of the caption.
There appear to be two primary functions of verbs.
The first is to reflect what is in the data, thereby
strengthening the message that would be recognized
without the caption. One example from our corpus
is a graphic with the caption ?Legal immigration to
the U.S. has been rising for decades?. Although
the early part of the graphic displays a change from
decreasing immigration to a steadily increasing im-
migration trend, most of the graphic focuses on the
decades of increasing immigration and the caption
strengthens increasing trend in immigration as the
intended message of the graphic. If we do not in-
clude the caption, our system hypothesizes an in-
creasing trend message with a probability of 66.4%;
other hypotheses include an intended message that
emphasizes the change in trend with a probability
of 15.3%. However, when the verb increasing from
the caption is taken into account, the probability of
increasing trend in immigration being the intended
message rises to 97.9%.
3This is a slight variation of the graphic from Business
Week. In the Business Week graphic, the labels sometimes ap-
The second function of a verb is to focus atten-
tion on some aspect of the data. For example, con-
sider the graphic in Figure 4. Without a caption, our
system hypothesizes that the graphic is intended to
convey the relative rank in billings of different credit
card issuers and assigns it a probability of 72.7%.
Other possibilities have some probability assigned
to them. For example, the intention of conveying
that Visa has the highest billings is assigned a prob-
ability of 26%. Suppose that the graphic had a cap-
tion of ?Billings still lag?; if the verb lag is taken
into account, our system hypothesizes an intended
message of conveying the credit card issuer whose
billings are lowest, namely Diner?s Club; the prob-
ability assigned to this intention is now 88.4%, and
the probability assigned to the intention of convey-
ing the relative rank of different credit card issuers
drops to 7.8%. This is because the verb class con-
taining lag appeared in our corpus as part of the cap-
tion for graphics whose message conveyed an en-
tity with a minimum value, and not with graphics
whose message conveyed the relative rank of all the
depicted entities. On the other hand, if the caption
is ?American Express total billings still lag? (which
is the caption associated with the graphic in our cor-
pus), then we have two pieces of evidence from the
caption ? the verb lag, and the noun American Ex-
press which matches a label. In this case, the proba-
bilities change dramatically; the hypothesis that the
graphic is intended to convey the rank of American
Express (namely third behind Visa and Mastercard)
is assigned a probability of 76% and the probability
drops to 24% that the graphic is intended to con-
vey that Diner?s Club has the lowest billings. This is
not surprising. The presence of the noun American
Express in the caption makes that entity salient and
is very strong evidence that the intended message
places an emphasis on American Express, thus sig-
nificantly affecting the probabilities of the different
hypotheses. On the other hand, the verb class con-
taining lag occurred both in the caption of graphics
whose message was judged to convey the entity with
the minimum value and in the caption of graphics
pear on the bars and sometimes next to them, and the heading
for the dependent axis appears in the empty white space of the
graphic instead of below the values on the horizontal axis as we
show it. Our vision system does not yet have heuristics for rec-
ognizing non-standard placement of labels and axis headings.
229
that conveyed an entity ranked behind some others.
Therefore, conveying the entity with minimum value
is still assigned a non-negligible probability.
7 Future Work
It is rare that a caption contains more than one verb
class; when it does happen, our current system by
default uses the first one that appears. We need to
examine how to handle the occurrence of multiple
verb classes in a caption. Occasionally, labels in the
graphic appear differently in the caption. An exam-
ple is DJIA (for Dow Jones Industrial Average) that
occurs in one graphic as a label but appears as Dow
in the caption. We need to investigate resolving such
coreferences.
We currently limit ourselves to recognizing what
appears to be the primary communicative intention
of an information graphic; in the future we will also
consider secondary intentions. We will also extend
our work to other kinds of information graphics such
as line graphs and pie charts, and to complex graph-
ics, such as grouped and composite bar charts.
8 Summary
To our knowledge, our project is the first to inves-
tigate the problem of understanding the intended
message of an information graphic. This paper
has focused on the communicative evidence present
in an information graphic and how it can be used
in a probabilistic framework to reason about the
graphic?s intended message. The paper has given
particular attention to evidence provided by the
graphic?s caption. Our corpus study showed that
about half of all captions contain some evidence that
contributes to understanding the graphic?s message,
but that fully understanding captions is a difficult
problem. We presented a strategy for extracting ev-
idence from a shallow analysis of the caption and
utilizing it, along with communicative signals from
the graphic itself, in a Bayesian network that hy-
pothesizes the intended message of an information
graphic, and our results demonstrate the effective-
ness of our methodology. Our research is part of a
larger project aimed at providing alternative access
to information graphics for individuals with sight
impairments.
References
J. Ang, R. Dhillon, A. Krupski, E. Shriberg, and A. Stol-
cke. 2002. Prosody-based automatic detection of an-
noyance and frustration in human-computer dialog. In
Proc. of the Int?l Conf. on Spoken Language Process-
ing (ICSLP).
D. Chester and S. Elzer. 2005. Getting computers to see
information graphics so users do not have to. To ap-
pear in Proc. of the 15th Int?l Symposium on Method-
ologies for Intelligent Systems.
H. Clark. 1996. Using Language. Cambridge University
Press.
M. Corio and G. Lapalme. 1999. Generation of texts
for information graphics. In Proc. of the 7th European
Workshop on Natural Language Generation, 49?58.
S. Elzer, S. Carberry, N. Green, and J. Hoffman. 2004.
Incorporating perceptual task effort into the recogni-
tion of intention in information graphics. In Proceed-
ings of the 3rd Int?l Conference on Diagrams, LNAI
2980, 255?270.
S. Elzer, S. Carberry, I. Zukerman, D. Chester, N. Green,
S. Demir. 2005. A probabilistic framework for recog-
nizing intention in information graphics. To appear in
Proceedings of the Int?l Joint Conf. on AI (IJCAI).
R. Futrelle and N. Nikolakis. 1995. Efficient analysis of
complex diagrams using constraint-based parsing. In
Proc. of the Third International Conference on Docu-
ment Analysis and Recognition.
R. Futrelle. 1999. Summarization of diagrams in docu-
ments. In I. Mani and M. Maybury, editors, Advances
in Automated Text Summarization. MIT Press.
Nancy Green, Giuseppe Carenini, Stephan Kerpedjiev,
Joe Mattis, Johanna Moore, and Steven Roth. Auto-
brief: an experimental system for the automatic gen-
eration of briefings in integrated text and information
graphics. International Journal of Human-Computer
Studies, 61(1):32?70, 2004.
H. P. Grice. 1969. Utterer?s Meaning and Intentions.
Philosophical Review, 68:147?177.
M. Huber, E. Durfee, and M. Wellman. 1994. The auto-
mated mapping of plans for plan recognition. In Proc.
of Uncertainty in AI, 344?351.
S. Kerpedjiev and S. Roth. 2000. Mapping communica-
tive goals into conceptual tasks to generate graphics in
discourse. In Proc. of Int. Conf. on Intelligent User
Interfaces, 60?67.
J. Yu, J. Hunter, E. Reiter, and S. Sripada. 2002.
Recognising visual patterns to communicate gas tur-
bine time-series data. In ES2002, 105?118.
230
A Natural Language Processing Infrastructure for Turkish 
A. C. Cem SAY 
Department of Computer Engineering, 
Bogazi?i University,  
Bebek,stanbul 
say@boun.edu.tr 
 
?zlem ?ETNOLU 
Faculty of Engineering and Natural Sciences, 
Sabanc? University,  
Tuzla,stanbul 
ozlemc@sabanciuniv.edu 
eniz DEMR 
Department of Computer Engineering, 
Bogazi?i University 
Bebek,stanbul 
sdemir@cse.yeditepe.edu.tr 
Fatih ??N 
Department of Computer Engineering, 
Bogazi?i University 
Bebek,stanbul 
fatih_ogun@yahoo.com 
 
Abstract 
We built an open-source software platform in-
tended to serve as a common infrastructure that 
can be of use in the development of new applica-
tions involving the processing of Turkish. The 
platform incorporates a lexicon, a morphological 
analyzer/generator, and a DCG parser/generator 
that translates Turkish sentences to predicate 
logic formulas, and a knowledge base frame-
work. Several developers have already utilized 
the platform for a variety of applications, includ-
ing conversation programs and an artificial per-
sonal assistant, tools for automatic analysis of 
rhyme and meter in Turkish folk poems, a proto-
type sentence-level translator between Albanian, 
Turkish, and English, natural language interfaces 
for generating SQL queries and JAVA code, as 
well as a text tagger used for collecting statistics 
about Turkish morpheme order for a speech rec-
ognition algorithm. The results indicate the 
adaptability of the infrastructure to different 
kinds of applications and how it facilitates im-
provements and modifications.  
Introduction 
The obvious potential of natural language processing 
technology for economic, social and cultural pro-
gress can be realized more comprehensively if NLP 
techniques applicable to a wider selection of the lan-
guages of the world are developed. Before the full-
scale treatment of a new language can start, a con-
siderable amount of effort has to be invested to 
computerize the lexical, morphological and syntactic 
specifics of that language, which would be required 
by any nontrivial application.  
We built an open-source software platform in-
tended to serve as a common infrastructure that can 
be of use in the development of new applications 
involving the processing of Turkish. The platform, 
named TOY (?etinolu 2001), is essentially a big 
set of predicates in the logic programming language 
Prolog. The choice of Prolog, which was designed 
specifically with computational linguistics applica-
tions in mind, as the implementation language for 
our software has natural consequences for the 
knowledge representation setup to be used by other 
programs built on our platform. Prolog is based on 
first-order predicate calculus, it allows knowledge 
items to be represented in terms of logic-style facts 
and rules, and a built-in theorem prover drives the 
execution of Prolog queries. 
The TOY program?s internal organization into 
source files reflects the three different levels (see 
Figure 1) on which text-based NLP applications can 
be based. In terms of that figure, processing at a 
?deeper? level necessitates all components of ?shal-
lower? levels.  
In this paper, we describe this infrastructure and 
how it was adapted to a variety of applications. Sec-
tion 2 gives a brief overview of the infrastructure. 
Section 3 presents the applications based on it. 
1 Infrastructure 
The TOY platform is formed of a lexicon that con-
tains most of the Turkish morphemes (either root or 
suffix), a Turkish morphological analyzer/generator, 
a DCG parser/generator for Turkish, and a semantic 
processor which interfaces the aforementioned sub-
units with the underlying knowledge base for 
knowledge addition and extraction. 
 
 Figure 1.  TOY?s Internal Organization. 
1.1    Lexicon 
A complete lexicon is supposed to contain entries 
for all morphemes (meaningful units that make up 
words) for the language in question. There are two 
kinds of morphemes: roots and affixes. In Turkish, 
all affixes follow the root, that is, they are suffixes. 
Our lexicon contains entries for over 29000 roots 
and 157 suffixes. A single morpheme may have 
more than one entry, corresponding to its different 
allomorphs. A morpheme definition example for the 
word ??ocuk? (?child?) is shown in Figure 2. 
 
Figure 2. Morpheme Definition. 
 
The semantic representation slot gives a descrip-
tion of the contribution of the morpheme to the 
meaning of full-size sentences in which it appears: 
Since the meanings of sentences are represented by 
predicate logic formulas in our setup, roots contrib-
ute ?partial? versions of such formulas, with ?holes? 
to be filled by the contributions of the other words of 
the sentence. For instance, the semantic representa-
tion entry of the noun ??ocuk? is ?ocuk(_). In the 
sentence ?Ali ?ocuktur? (?Ali is a child?), the value 
of the missing argument is supplied by the name Ali, 
resulting in the formula ?ocuk('Ali') for the overall 
sentence. 
For noun entries, the commonsense knowledge 
slot contains a pointer to the location of the thing 
described by this noun in the taxonomy tree (see 
Figure 4) used by the program. (The entry for 
??ocuk?, for instance, indicates that it can be 
reached by descending from the root along the ?con-
crete entity?-?animate-?human being? arcs.) For 
verbs, this slot contains a set of restrictions on the 
various argument slots of the verb. As an example, 
the agent of the verb ?ye-? (?eat?) is restricted to be 
a living thing, and its theme is restricted to be a 
solid.  
 
1.2    Morphological Analyzer/ Generator 
In the infrastructure, possible legal orderings of 
Turkish morphemes are represented by a large finite 
state diagram, a small part of which can be seen in 
Figure 3. The morphological component of the plat-
form is a finite state transducer that makes use of the 
lexicon for traversing the arcs of the diagram 
(adopted, with changes, from Kemal Oflazer?s work 
on Turkish morphology (Oflazer 1993)) to associate 
a character string with the list of meaning contribu-
tions of its morphemes. This traversal is complicated 
by the vowel harmony constraint of Turkish mor-
phology. This rule means that, when adding a suffix 
to a word, the allomorph to be added is a function of 
the last vowel of the word to be extended. For in-
stance, the plural suffix has the two allomorphs ?-
ler? and ?-lar?. The Turkish word for ?children? is 
??ocuklar?, while ?olives? is ?zeytinler?, since 
?back vowels? like ?a? and ?u? require a back vowel 
in the suffix, whereas ?front vowels? like ?e? and ?i? 
require a front vowel there. The program keeps track 
of the vowels during the transduction to enforce 
these constraints. Words of foreign origin which vio-
late vowel harmony are flagged appropriately in the 
lexicon. 
 
Figure 3. A Subgraph of the Morphological FST 
Employed by TOY. 
 
Like most Prolog predicates the morphological 
component is reversible, that is, the same piece of 
code can be used both to analyze a given word to 
obtain its underlying constituents, and to generate a 
word when given a list of such constituents. Another 
built-in feature of Prolog makes it very easy for the 
program to compute all results associated with a par-
ticular input when more than one legal output is 
possible, as in the case of the analysis of the 
morphologically ambiguous word ?yedi?, where our 
morphological parser produces, through backtrack-
ing, two alternative analyses: the third-
person/singular past tense inflection of the verb ?ye-
? (?eat?), and the Turkish number word for ?seven?. 
1.3    DCG Parser/Generator 
We encoded a subset of the syntax rules of Turkish 
in Prolog?s DCG notation. The DCG formalism al-
lows the computation of the meaning formula of a 
constituent to be performed in parallel with its syn-
tactic parsing; each DCG rule is written to indicate 
how the partial meanings of the elements on its 
right-hand side fit together to produce the semantic 
expression for the constituent on the left-hand side.  
Certain language constructs correspond naturally 
to the notion of quantification in predicate calculus. 
For instance, the sentence ?Bir ?ocuk zeytin yedi.? 
(?A child ate (an/some) olive(s)?) can be represented 
by the logical formula  
?X?Y( ?ocuk(X) ? zeytin(Y)  ? ye(Event, X,Y, Loca-
tion, Time, Goal, Source, Instrument, defi-
nite_past, none, positive) ). 
In the Prolog program, existentially quantified 
expressions like this one have the form 
some(X,Restrictor,Scope), where X is the quantified 
variable, and Restrictor and Scope are the two sides 
of the conjunction (Covington, 1994) 
some(X,?ocuk(X),some(Y,zeytin(Y),ye(EventMarker,
X,Y,Location,Time,Goal,Source,Instrument, 
definite_past,none,positive))) 
The successful DCG parsing of a sentence also re-
sults in a field being instantiated to a symbol repre-
senting the sentence?s mood. Possible values for the 
mood field are ?statement,? ?yes_no_question,? and 
?wh_question.? 
This component of the program is also designed 
to be reversible, that is, it can produce the corre-
sponding sentence when given a logical formula, but 
yet another peculiarity of the language complicates 
the solution: Turkish word order is (almost) free, 
which basically means that the sentence constituents 
can be shuffled around without changing the mean-
ing. Therefore, a single semantic formula usually 
corresponds to several different sentences, even 
without taking synonymity of words into account. 
Our software has the capability of producing multi-
ple alternative sentences as output in such cases. 
1.4    Anaphora Resolver 
In general, the full-scale processing of all but very 
simple sentences necessitates information that is not 
present in the sentence itself, the most obvious ex-
amples being question sentences. This additional 
information is either pre-encoded in the knowledge 
base as part of a big store of commonsense knowl-
edge, or, when the agent is involved in a dialogue or 
a story understanding task, it is gleaned from the 
other sentences in the input. 
One example where the computation of the mean-
ing of a declarative sentence requires access to 
knowledge obtained from previous sentences is the 
process of anaphora resolution. Resolving an ana-
phor is the job of finding out which discourse 
marker (unique internal name) to use for the entity 
referred to by this phrase in the knowledge base. 
There is no ?correct? algorithm for this task because 
of the inherent ambiguity of natural language (Lenat 
1995). Our resolver selects a discourse marker for an 
anaphoric reference making use of the taxonomy 
tree (see Figure 4), semantic type information in its 
dictionary, pointers to the locations in this tree, and 
the positions of the original referents in their sen-
tences.  
Our resolver treats only definite clauses as ana-
phors and resolves direct anaphors. Gelbukh and 
Sidorov (1999) propose ways of solving indirect 
anaphors. 
Figure 4. TOY?s Taxonomy Tree. 
 
An anaphor and its antecedent can be related if 
semantic type of the anaphor contains the semantic 
type of the antecedent or vice versa or their types 
intersect. Resolution of indirect anaphors will be 
added to TOY?s anaphora resolver in the future. This 
taxonomy tree will also be used for this purpose.   
1.5    Knowledge Base Interface 
This module translates predicate logic formulas cre-
ated by the DCG parser to Prolog facts and rules. As 
an example, the sentence ?Ali ?ocuktur? (?Ali is a 
child?) is eventually translated to the Prolog fact 
?ocuk('Ali'), whose form enables it to take part in 
automatic proofs involving this knowledge item 
when necessary. In general, nouns and adjectives are 
represented as single-argument predicates standing 
for the invoked property. 
Verbs other than ?to be? have a considerably 
more complicated representation. The Prolog 
equivalent of the sentence ?Ali gitti? (?Ali left?) is 
 
Figure 5. Prolog Representation Example. 
Of course, from the point of view of the com-
puter, (or, for that matter, of anybody who cannot 
speak Turkish,) a formula like ?ocuk('Ali') is just as 
opaque as ?Ali ?ocuktur?. When we look up a 
strange word in the dictionary, we comprehend its 
meaning by mentally linking it in appropriate ways 
to the words appearing in its description. If a suffi-
ciently large subgraph of this network of concepts 
that exists in our minds is replicated in the computer, 
it would be able to give the same response to an in-
put sentence as a human utilizing the same network. 
For instance, the Prolog rule 
?ocuk(X):- insan(X), k???k(X). 
(where ?insan? means ?human? and ?k???k? means 
?small? in Turkish) relates these three concepts in a 
way similar to the picture in most people?s minds. 
The translation of a Turkish sentence to the corre-
sponding predicate logic formula by the DCG rules 
is just an intermediate step in the processing of that 
sentence. ?Understanding? a sentence necessitates a 
computation involving both this formula and the 
current contents of the knowledge base, possibly 
resulting in a change to the knowledge base, and the 
generation of an appropriate response. 
Skolemization is used in the automatic transfor-
mation of the logical formulas of declarative sen-
tences to actual Prolog code by means of replacing 
all the existentially quantified variables by special 
expressions called Skolem functions. The purpose of 
this operation is to assign a discourse marker to 
every entity which is mentioned but not named in 
the sentence. These markers are the atomic symbols 
used by the computer to model the world being de-
scribed and referred to during the conversation, and 
keeping track of them is an essential part of the dia-
logue processing task. 
For wh-question sentences, the DCG parser cre-
ates formulas in the form of Prolog predicates. For 
instance, the sentence ?Kim zeytin yedi?? (?Who ate 
(an/some) olive(s)??) is translated to the formula 
which(X,insan(X),some(Y,zeytin(Y),ye(Event,X,Y, 
Loc, Time,Goal,Source, Instrument,      
        definite_past, none, positive)) 
whose form matches the already available logic pro-
gram which(Item,Property1Item,Property2Item). See 
the next section for a discussion of these ?question-
word? routines. 
2       Applications Based on TOY 
In this section, we will present some applications 
that were developed using the TOY infrastructure. 
Each subsection will briefly explain the application, 
the TOY components used, and the modifications 
done on the infrastructure. 
 
 
2.1    Conversational agent ? TOYagent 
Smith (1994) classifies dialogue styles that can be 
adopted by the computer during human-computer 
interaction into four modes, depending on the degree 
of control that the computer has on the dialogue: 
Directive, suggestive, declarative, and passive. 
TOYagent?s original approach mostly suits the pas-
sive mode, where the user has complete control, and 
the computer passively acknowledges user state-
ments, and provides information only as a response 
to direct user requests. 
TOYagent (Demir 2003) enables users to make 
on-line additions to the lexicon without the need to 
know Prolog. When faced with a word that it is un-
able to parse morphologically, TOYagent engages in 
a (mostly menu-driven) subdialogue with the user to 
identify the root, category, and morphophonemic 
properties of the word, and adds the appropriate en-
tries to the lexicon. The meanings of these new 
words can be incorporated to the system by the logic 
program synthesis facility, which enables the user to 
provide natural language descriptions for new predi-
cates in terms of existing predicates. These descrip-
tions are automatically converted to Prolog clauses 
and added to the knowledge base of the program for 
future use.  
The original dialogue algorithm embedded in 
TOYagent can be summarized as follows: 
1. Read a sentence (this may cause a ?word learn-
ing? subdialogue if one or more words in the sen-
tence cannot be parsed by the morphological 
analyzer) 
2. Analyze the sentence using the DCG parser, re-
solving anaphors if necessary. If the syntactic 
parse is unsuccessful, report this to the user and 
GOTO 1.  
3. If the mood is ?statement?, then the user is mak-
ing a declarative statement; use the built-in theo-
rem prover to try to prove the logical formula 
corresponding to the sentence. There are two pos-
sibilities: (In the following, all the ?canned? re-
sponses are in Turkish, of course.) 
        a. If the formula can be proven using the cur-
rent contents of the knowledge base, the informa-
tion contained in the sentence is already there; 
respond with ?Thanks, I know that? 
      b. If Prolog fails to prove the formula with its 
current knowledge, then negate the formula and 
try to prove this negation. There are two possibili-
ties: 
            i. If this new formula can be proven using the 
current contents of the knowledge base, the infor-
mation contained in the sentence is contradictory 
with what we already know; respond with ?I do 
not think so? 
            ii. If Prolog fails to prove this new formula 
with its current knowledge, create the necessary 
discourse and event markers and assert the Prolog 
clauses representing the input sentence to the 
knowledge base, responding with ?Thanks for the 
information?  
4. If the mood is ?yes_no_question?, the user has 
asked a yes-no question; use the built-in the prover 
to try to prove the sentence?s logical formula. 
There are two possibilities: 
      a. If the formula can be proven using the current 
contents of the knowledge base, respond with 
?Yes? 
    b. If Prolog fails to prove the formula with its 
current knowledge, then negate the formula and 
try to prove this negation. There are two possibili-
ties: 
            i. If this new formula can be proven using the 
current contents of the knowledge base, respond 
with ?No? 
            ii. If Prolog fails to prove this new formula 
with its current knowledge, respond with ?I do not 
know.? 
5. If the mood is ?wh_question?, the user has asked 
a wh-question; use the built-in theorem prover on 
the sentence?s logical formula. The associated 
program of each question word scans the knowl-
edge base and produces the relevant answer. The 
answer can be printed out directly, or, if required, 
in the form of a grammatical sentence generated 
by a procedure that first prepares a new logical 
formula from the produced knowledge items and 
then uses the syntax and morphology components 
to form the statement corresponding to this for-
mula. GOTO 1. 
The following conversation fragments, in which 
user entries are shown in boldface, illustrate several 
aspects of TOYagent. (The English translations are 
not part of TOYagent?s input-output, and have been 
added ?manually.?) 
As an example to ?online? learning of lexical en-
tries, we deleted the word ?ana? (?mother?) from the 
lexicon, and carried out the following dialogue with 
the program: 
 Her ana g?zeldir. (Every mother is beautiful) 
?ana? kelimesini bilmiyorum. (I do not know the 
word ?ana?) 
 Kelimenin k?k? nedir? (What is the stem of this 
word?) 
 1: a   
2: an  
3: ana  
 Hangisi: 3. 
 (Please enter: 3.) 
?ana? kelimesinin tipi nedir? (What is the type of 
the word ?ana??) 
 
 1: Cins isim (1: Common noun) 
 2: S?fat        (2: Adjective) 
 3: ?zel isim(3: Proper noun) 
 Hangisi: 3. 
(Please enter: 3) 
 S?zl?e eklendi. (It has been added to the lexicon) 
 ?ana? ne demek? Anlat?r m?s?n? (Could you ex-
plain the word ?ana??) 
 ?ocuu olan bir bayan anad?r. (A female who has a 
child is a mother) 
 Teekk?rler, ?rendim. (Thanks for the information) 
 
Since the unknown word could have appeared in 
an inflected form in the input sentence, TOYagent 
first asks about the actual stem. A Prolog rule corre-
sponding to the relevant universally quantified logic 
formula is prepared and asserted for each of the 
user?s sentences read in this dialogue. The last line is 
the program?s response to the original input sen-
tence. 
Aye bir anad?r. (Aye is a mother) 
Teekk?rler, ?rendim. (Thanks for the information) 
Aye g?zel midir? (Is Aye beautiful?) 
Evet. (Yes)  
Neden? (Why?) 
Her ana g?zeldir. (Every mother is beautiful) 
Aye anad?r. (Aye is a mother) 
 
The affirmative answer requires stepping through 
the implication translated to Prolog during the proc-
essing of the original user entry. The question 
?why?? is answered by translating the Prolog formu-
las used for the previous answer back to sentence 
form. 
Definite noun phrases are treated as anaphors: 
Canan k???k bir ?ocuk. (Canan is a small child) 
Teekk?rler, ?rendim. (Thanks for the information) 
?ocuk kahvalt?da zeytin yedi. (The child ate olives 
for breakfast) 
Teekk?rler, ?rendim. (Thanks for the information) 
Kim zeytin yedi? (Who ate olives?) 
Canan zeytin yedi. (Canan ate olives) 
Ka? kii kahvalt?da zeytin yedi? (How many people 
ate olives for breakfast?) 
Bir kii kahvalt?da zeytin yedi. (One person ate ol-
ives for breakfast) 
 
The definite noun phrase in the second user entry 
(?the child?) is matched to the most recently men-
tioned child. As mentioned earlier, question words 
have small Prolog programs corresponding to them. 
The answer extracted from the knowledge base is 
presented in the form of a grammatically correct 
sentence. (The fact that every child is also a person 
is one of the commonsense items that have been 
preencoded in the knowledge base.) 
A rudimentary capability of commonsense rea-
soning about time is implemented: The ?time? ar-
gument in verb predicates has a substructure with 
slots for the beginning and ending points of the in-
terval corresponding to the event. (In the present 
version, only a small subset of the verbal lexicon 
entries have their time subslots manually encoded 
for this purpose.) Hours are used as the unit interval. 
Kemal k???k bir ?ocuk. B?t?n k???k ?ocuklar 10 
saat uyurlar. (Kemal is a small child. All small chil-
dren sleep for 10 hours) 
Teekk?rler, ?rendim. (Thanks for the information) 
Kemal saat 23?te uyudu. (Kemal fell asleep at 23 
hours) 
Teekk?rler, ?rendim. (Thanks for the information) 
Kemal ne zaman uyudu? (When did Kemal fall 
asleep?)  
Kemal yirmi??te uyudu. (Kemal fell asleep at twenty 
three) 
Kemal ne zaman uyand?? (When did Kemal wake 
up?)  
Kemal dokuzda uyand?. (Kemal woke up at nine) 
 
Note that the program is able to do the ?modulo 
24? calculation required for producing the appropri-
ate answer. 
To find pronominal references in the absence of 
gender information, the semantic network is utilized. 
In the following excerpt, the pronoun ?o? 
(?he/she/it?) is correctly deduced to correspond to 
??ay? (?tea?), since the network does not allow 
?Kemal?, a human name, to be the agent of the word 
?bit-? (?to be consumed entirely?), which can have 
only inanimate material at that role. 
Kemal kahvalt?da ne i?ti? (What did Kemal drink for 
breakfast?) 
Bilmiyorum. (I do not know) 
Kemal ?ay i?ti ise o bitmitir. (If Kemal drank tea, 
(he/she/it) must have been consumed entirely) 
Teekk?rler, ?rendim. (Thanks for the information) 
Kemal ?ay i?ti. (Kemal drank tea) 
Teekk?rler, ?rendim.(Thanks for the information) 
?ay bitmi midir? (Has the tea been consumed en-
tirely?)  
Evet (Yes) 
 
The latest release of TOYagent (??n 2003) is 
able to manage conversations with multiple agents, 
can adapt different ?attitudes? about whether to be-
lieve what a user says depending on the user?s pro-
file, and has the capability of detecting and pointing 
out inconsistencies among the statements made by 
different users. This version also supports an op-
tional ?inquisitive? dialogue mode, where the com-
puter questions the user about the values of currently 
empty slots in the verb predicates corresponding to 
previous user statements. 
2.2     Turkish Natural Language Interface 
For SQL Queries (NALAN-TS) 
NALAN-TS (Maden, Demir and ?zcan 2003) is a 
Turkish natural language query interface for SQL 
databases, formed of a syntactic parser, semantic 
analyzer, meaning extractor, SQL constructor and 
executer. It is a dictionary based application and in-
cludes Turkish and database dictionaries. 
  
 
Figure 6. NALAN-TS Flow Diagram. 
 
The shaded modules in Figure 6 were taken com-
pletely from the TOY infrastructure, except for a 
few modifications like the addition of new Turkish 
syntax rules and a different format for the semantic 
representation of the words in the dictionary. TOY?s 
knowledge base interface is taken as the basis by 
NALAN-TS.  
2.3   Turkish Speaking Assistant -TUSA 
TUSA (eker, 2003) is a natural language interface 
for an online personal calendar. The morphological 
analyzer/generator of TOY was taken as a basis in 
this project with modifications made for utilizing.  
2.4   Generating Java Class Skeleton Using a 
Natural Language Interface- TUJA 
TUJA (?zcan, eker and Karadeniz 2004) is a natu-
ral language interface for generating Java source 
code and creating an object-oriented semantic net-
work. This program uses TOY?s morphological ana-
lyzer/generator as the starting point. 
2.5     Other Applications 
Ballhysa (2000) used TOY to produce a prototypical 
sentence-level translator between Albanian, English, 
and Turkish. (To our knowledge, this is the first 
NLP work ever done on Albanian) Dutaac? (2002) 
used the morphological component to tag a Turkish 
corpus of nearly ten million words to collect statis-
tics and compared the performance of an N-gram 
model of speech recognition based on morphemes 
with those based on words or syllables. Tekeli 
(2002) made use of the word-level components to 
build an ?ELIZA-like? (Covington 1994) dialogue 
program which caricaturizes Fatih Terim, a famous 
soccer coach and an idiosyncratic Turkish speaker. 
The program?s ?bag of tricks? includes coming up 
with rhyming responses to user sentences. Bilsel 
(2000) developed a ?poem expert? for analyzing 
Turkish folk poems for their rhyme and meter prop-
erties, a demanding task which is part of the high-
school curriculum in Turkey. 
Conclusion 
Our work on TOY is continuing on many fronts: The 
DCG component is currently being extended to 
cover both a bigger subset of Turkish syntax, and 
some types of agrammatical sentences. We hope that 
TOY will be useful in the development of many 
other applications in the near future. 
References 
Can Tekeli 2002. TERIM_SON. B.S. Thesis, Department 
of Computer Engineering, Bogazici University. 
Douglas Lenat 1995. CYC: A Large-Scale Investment in 
Knowledge Infrastructure. In ?Comm. ACM, 38/11?, 
pages 33-38. 
Eda Bilsel 2000. Poem Analyzer, B.S. Thesis, Department 
of Computer Engineering, Bogazici University. 
Elton Ballhysa 2000. Albanian-Turkish-English Transla-
tor. B.S. Thesis, Department of Computer Engineering, 
Bogazici University. 
Ender ?zcan, adi E. eker and Zeynep. I. Karadeniz 
2004. Generating Java Class Skeleton Using A Natural 
Language Interface. In ?First International Workshop 
on Natural Language Understanding and Cognitive Sci-
ence-NLUCS?. 
Fatih ??n 2003. Design and Implementation of an Im-
proved Conversational Agent Infrastructure for Turk-
ish. M.S. Thesis, Department of Computer Engineering, 
Bogazici University. 
Helin Dutaac? 2002. Statistical Language Models for 
Large Vocabulary Turkish Speech Recognition. M.S. 
Thesis, Department of Computer Engineering, Bogazici 
University.  
Ibrahim Maden, eniz Demir and Ender ?zcan 2003. 
Turkish Natural Language Interface for Generating 
SQL Queries. In ?TBD 20. Ulusal Biliim Kurultayi?. 
Kemal Oflazer 1993. Two-Level Description of Turkish 
Morphology. In ?Proc. Second Turkish Symposium on 
Artificial Intelligence and Neural Networks?, Bogazici 
University Press, pages 86-93, Istanbul.  
Michael A .Covington 1994. Natural Language Process-
ing for Prolog Programmers. Prentice Hall, Englewood 
Cliffs, NJ. 
?zlem ?etinolu 2001. A Prolog Based Natural Lan-
guage Processing Infrastructure for Turkish. M.S. The-
sis, Department of Computer Engineering, Bogazici 
University. 
Ronnie W. Smith 1994. Spoken Variable Initiative Dia-
log: An Adaptable Natural-Language Interface. In 
?IEEE Expert: Intelligent Systems and Their Applica-
tions 9/1?,pages 45-50. 
adi E. eker 2003.  Design and Implementation of a 
Personal Calendar with a Natural Language Interface 
in Turkish. M.S. Thesis, Department of Computer En-
gineering, Yeditepe University. 
eniz Demir 2003. Improved Treatment of Word Meaning  
in a Turkish Conversational Agent. M.S.     Thesis, De-
partment of Computer Engineering, Bogazici Univer-
sity. 
Summarizing Information Graphics Textually
Seniz Demir?
TUBITAK-BILGEM
Sandra Carberry??
University of Delaware
Kathleen F. McCoy?
University of Delaware
Information graphics (such as bar charts and line graphs) play a vital role in many
multimodal documents. The majority of information graphics that appear in popular media
are intended to convey a message and the graphic designer uses deliberate communicative
signals, such as highlighting certain aspects of the graphic, in order to bring that message
out. The graphic, whose communicative goal (intended message) is often not captured by the
document?s accompanying text, contributes to the overall purpose of the document and cannot be
ignored. This article presents our approach to providing the high-level content of a non-scientific
information graphic via a brief textual summary which includes the intended message and the
salient features of the graphic. This work brings together insights obtained from empirical studies
in order to determine what should be contained in the summaries of this form of non-linguistic
input data, and how the information required for realizing the selected content can be extracted
from the visual image and the textual components of the graphic. This work also presents a
novel bottom?up generation approach to simultaneously construct the discourse and sentence
structures of textual summaries by leveraging different discourse related considerations such as
the syntactic complexity of realized sentences and clause embeddings. The effectiveness of our
work was validated by different evaluation studies.
1. Introduction
Graphical representations are widely used to depict quantitative data and the relations
among them (Friendly 2008). Although some graphics are constructed from raw data
only for visualization purposes, the majority of information graphics (such as bar charts
and line graphs) found in popular media (such as magazines and newspapers) are
? The Scientific and Technological Research Council of Turkey, Center of Research for Advanced
Technologies of Informatics and Information Security, Gebze, Kocaeli, TURKEY, 41470.
E-mail: senizd@uekae.tubitak.gov.tr. (This work was done while the author was a graduate student at
the Department of Computer and Information Sciences, University of Delaware, Newark, DE, USA 19716.)
?? Department of Computer and Information Sciences, University of Delaware, Newark, DE, USA 19716.
E-mail: carberry@cis.udel.edu.
? Department of Computer and Information Sciences, University of Delaware, Newark, DE, USA 19716.
E-mail: mccoy@cis.udel.edu.
Submission received: 20 April 2010; revised submission received: 8 July 2011; accepted for publication:
6 September 2011.
? 2012 Association for Computational Linguistics
Computational Linguistics Volume 38, Number 3
Figure 1
Graphic conveying a maximum bar.
constructed to convey a message. For example, the graphic in Figure 1 ostensibly is
intended to convey that ?The United States has the highest number of hacker attacks
among the countries listed.? The graphic designer made deliberate choices in order to
bring that message out. For example, the bar representing the United States is high-
lighted with a different color from the other bars and the bars are sorted with respect to
their values instead of their labels so that the bar with the highest value can be easily
recognized. Such choices, we argue, are examples of communicative signals that graphic
designers use. Under Clark?s definition (1996), language is not just text and utterances,
but instead includes any deliberate signal (such as gestures and facial expressions) that
is intended to convey a message; thus an information graphic is a form of language.
In popular media, information graphics often appear as part of a multimodal
document. Carberry, Elzer, and Demir (2006) conducted a corpus study of information
graphics from popular media, where the extent to which the message of a graphic is
also captured by the text of the accompanying document was analyzed. One hundred
randomly selected graphics of different kinds (e.g., bar charts and line graphs) were
collected from newspapers and magazines along with their articles. It was observed
that in 26% of the instances, the text conveyed only a small portion of the graphic?s
message and in 35% of the instances, the text didn?t capture the graphic?s message
at all. Thus graphics, together with the textual segments, contribute to the overall
purpose of a document (Grosz and Sidner 1986) and cannot be ignored. We argue that
information graphics are an important knowledge resource that should be exploited,
and understanding the intention of a graphic is the first step towards exploiting it.
This article presents our novel approach to identifying and textually conveying
the high-level content of an information graphic (the message and knowledge that one
would gain from viewing a graphic) from popular media. Our system summarizes this
form of non-linguistic input data by utilizing the inferred intention of the graphic de-
signer and the communicative signals present in the visual representation. Our overall
goal is to generate a succinct coherent summary of a graphic that captures the intended
message of the graphic and its visually salient features, which we hypothesize as being
related to the intended message. Input to our system is the intention of the graphic
inferred by the Bayesian Inference System (Elzer, Carberry, and Zukerman 2011), and
an XML representation of the visual graphic (Chester and Elzer 2005) that specifies the
components of the graphic such as the number of bars and the heights of each bar. Our
work focuses on the generation issues inherent in generating a textual summary of a
graphic given this information. The current implementation of the system is applicable
to only one kind of information graphic, simple bar charts, but we hypothesize that the
overall summarization approach could be extended to other kinds of graphics.
528
Demir, Carberry, and McCoy Summarizing Information Graphics Textually
In this article, we investigate answers to the following questions: (1) Among all
possible information that could be conveyed about a bar chart, what should be included
in its summary? (2) How should the content of a summary be organized into a coherent
text? (3) How should the text structure be best realized in natural language? Given the
intended message and the XML representation of a graphic, our system first determines
the content of the graphic?s summary (a list of propositions) by applying the content
identification rules constructed for that intended message category. Our system then
produces a coherent organization of the selected content by applying a bottom?up
approach which leverages a variety of considerations (such as the syntactic complexity
of the realized sentences and clause embeddings) in choosing how to aggregate informa-
tion into sentence-sized units. The system finally orders and realizes the sentence-sized
units in natural language and generates referring expressions for graphical elements
that are required in realization.
The rest of this article is structured as follows. Section 2 discusses related work
on summarization of non-linguistic input data and describes some natural language
applications which could benefit from summaries generated by our work. Section 3
outlines our summarization framework. Section 4 is concerned with identifying the
propositional content of a summary and presents our content-identification rules that
specify what should be included in the summary of a graphic. Section 5 describes
our bottom?up approach, which applies operators to relate propositions selected for
inclusion, explores aggregating them into sentence-sized units, and selects the best orga-
nization via an evaluation metric. Section 6 presents our sentence-ordering mechanism,
which incorporates centering theory to specify the order in which the sentence-sized
units should be presented. Section 7 describes how our system realizes the selected
content in natural language. Particular attention is devoted to our methodology for
generating referring expressions for certain graphical elements such as a descriptor
of what is being measured in the graphic. Section 8 presents a user study that was
conducted to evaluate the effectiveness of the generated summaries for the purposes
of this research by measuring readers? comprehension. Section 9 concludes the article
and outlines our future work.
2. Background
2.1 Related Work
There has been a growing interest in language systems that generate textual summaries
of non-linguistic input data (Reiter 2007). The overall goal of these systems, generally
referred to as data-to-text systems, is to enable efficient processing of large volumes
of numeric data by supporting traditional visualisation modalities and to reduce
the effort spent by human experts on analyzing the data. Various examples of data-
to-text systems in the literature include systems that summarize weather forecast
data (Goldberg, Driedger, and Kittredge 1994; Coch 1998), stock market data (Kukich
1983), and georeferenced data (Turner, Sripada, and Reiter 2009).
One of the most successful data-to-text generation research efforts is the SumTime
project, which uses pattern recognition techniques to generate textual summaries of
automatically generated time-series data in order to convey the significant and inter-
esting events (such as spikes and oscillations) that a domain expert would recognize
by analyzing the data. The SumTime-Mousam (Somayajulu, Reiter, and Davy 2003)
and SumTime-Turbine (Yu et al 2007) systems were designed to summarize weather
forecast data and the data from gas turbine engines, respectively. More recently, the
529
Computational Linguistics Volume 38, Number 3
project was extended to the medical domain. The BabyTalk (Gatt et al 2009) project
produces textual summaries of clinical data collected for babies in a neonatal intensive
care unit, where the summaries are intended to present key information to medical staff
for decision support. The implemented prototype (BT-45) (Portet et al 2009) generates
multi-paragraph summaries from large quantities of heterogeneous data (e.g., time
series sensor data and the records of actions taken by the medical staff). The overall
goal of these systems (identifying and presenting significant events) is similar to our
goal of generating a summary that conveys what a person would get by viewing an
information graphic, and these systems contend with each of the generation issues we
must face with our system. Our generation methodology, however, is different from the
approaches deployed in these systems in various respects. For example, BT-45 produces
multi-paragraph summaries where each paragraph presents first a key event (of highest
importance), then events related to the key event (e.g., an event that causes the key
event), and finally other co-temporal events. Our system, on the other hand, produces
single-paragraph summaries where the selected propositions are grouped and ordered
with respect to the kind of information they convey. In addition, BT-45 performs a
limited amount of aggregation at the conceptual level, where the aggregation is used
to express the relations between events with the use of temporal adverbials and cue
phrases (such as as a result). Contrarily, our system syntactically aggregates the selected
propositions with respect to the entities they share.
There is also a growing literature on summarizing numeric data visualized via
graphical representations. One of the recent studies, the iGRAPH-Lite (Ferres et al
2007) system, provides visually impaired users access to the information in a graphic via
keyboard commands. The system is specifically designed for the graphics that appear
in ?The Daily? (Statistics Canada?s main dissemination venue) and presents the user
with a template-based textual summary of the graphic. Although this system is very
useful for in depth analysis of statistical graphs and interpreting numeric data, it is
not appropriate for graphics from popular media where the intended message of the
graphic is important. In the iGRAPH-Lite system, the summary generated for a graphic
conveys the same information (such as the title of the graphic, and the maximum and
minimum values) no matter what the visual features of the graphic are. The content of
the summaries that our system generates, however, is dependent on the intention and
the visual features of the graphic. Moreover, that system does not consider many of the
generation issues that we address in our work.
Choosing an appropriate presentation for a large amount of quantitative data is
a difficult and time-consuming task (Foster 1999). A variety of systems were built to
automatically generate presentations of statistical data?such as the PostGraphe sys-
tem (Corio and Lapalme 1999; Fasciano and Lapalme 2000), which generates graphics
and complementary text based on the information explicitly given by the user such
as the intention to be conveyed in the graphic and the data of special interest to the
user. The content of the accompanying text is determined according to the intention
of the graphic and the features of the data. Moreover, the generated texts are intended
to reinforce some important facts that are visually present in the graphic. In this re-
spect, the generation in PostGraphe is similar to our work, although the output texts
have a limited range and are heavily dependent on the information explicitly given
by the user.
2.2 Role of Graphical Summaries in Natural Language Applications
2.2.1 Accessibility. Electronic documents that contain information graphics pose chal-
lenging problems for visually impaired individuals. The information residing in the
530
Demir, Carberry, and McCoy Summarizing Information Graphics Textually
text can be delivered via screen reader programs but visually impaired individuals are
generally stymied when they come across graphics. These individuals can only receive
the ALT text (human-generated text that conveys the content of a graphic) associated
with the graphic. Many electronic documents do not provide ALT texts and even in the
cases where ALT text is present, it is often very general or inadequate for conveying the
intended message of the graphic (Lazar, Kleinman, and Malarkey 2007).
Researchers have explored different techniques for providing access to the in-
formational content of graphics for visually impaired users, such as sound (Meijer
1992; Alty and Rigas 1998), touch (Ina 1996; Jayant et al 2007), or a combination
of the two (Kennel 1996; Ramloll et al 2000). Unfortunately, these approaches have
serious limitations such as requiring the use of special equipment (e.g., printers and
touch panels) or preparation work done by sighted individuals. Research has also
investigated language-based accessibility systems to provide access to graphics (Kurze
1995; Ferres et al 2007). As mentioned in Section 2.1, these language-based systems
are not appropriate for graphics in articles from popular media where the intended
message of the graphic is important. We hypothesize that providing alternative access
to what the graphic looks like is not enough and that the user should be provided
with the message and knowledge that one would gain from viewing the graphic. We
argue that the textual summaries generated by our approach could be associated with
graphics as ALT texts so that individuals with sight impairments would be provided
with the high-level content of graphics while reading electronic documents via screen
readers.
2.2.2 Document Summarization. Research has extensively investigated various techniques
for single (Hovy and Lin 1996; Baldwin andMorton 1998) and multi-document summa-
rization (Goldstein et al 2000; Schiffman, Nenkova, andMcKeown 2002). The summary
should provide the topic and an overview of the summarized documents by identifying
the important and interesting aspects of these documents. Document summarizers
generally evaluate and extract items of information from documents according to their
relevance to a particular request (such as a request for a person or an event) and address
discourse related issues such as removing redundancies (Radev et al 2004) and ordering
sentences (Barzilay, Elhadad, and McKeown 2002) in order to make the summary more
coherent.
It is widely accepted that to produce a good summary of a document, one must
understand the document and recognize the communicative intentions of the author.
Summarization work primarily focuses on the text of a document but, as mentioned
earlier, information graphics are an important part of many multimodal documents
that appear in popular media and these graphics contribute to the overall commu-
nicative intention of the document. We argue that document summarization should
capture the high-level content of graphics that are included in the document, because
information graphics often convey information that is not repeated elsewhere in the
document. We believe that the summary of a graphic generated by our system, which
provides the intended message of the graphic and the information that would be
perceived with a casual look at the graphic, might help in summarizing multi-modal
documents.1
1 Our colleagues are currently investigating how the findings from this work can be used in
communicating the content of multimodal documents.
531
Computational Linguistics Volume 38, Number 3
3. System Overview
Figure 2 provides an overview of the overall system architecture. The inputs to our
system are an XML representation of a bar chart and the intended message of the chart;
the former is the responsibility of a Visual Extraction System (Chester and Elzer 2005)
and the latter is the responsibility of a Bayesian Inference System (Elzer, Carberry, and
Zukerman 2011). Given these inputs, the Content Identification Module (CIM) first
identifies the salient and important features of a graphic that are used to augment its
inferred message in the summary. The propositions conveying the selected features and
the inferred message of the graphic are then passed to the Text Structuring and Aggre-
gation Module (TSAM). This module produces a partial ordering of the propositions
according to the kind of information they convey, and aggregates them into sentence-
sized units. The Sentence OrderingModule (SOM) then determines the final ordering of
the sentence-sized units. Finally, the Sentence Generation Module (SGM) realizes these
units in natural language, giving particular attention to generating referring expressions
for graphical elements when appropriate. In the rest of this section, we briefly present
the systems that provide input to our work and describe the corpus of bar charts
used for developing and testing our system. The following sections then describe the
modules implemented within our system in greater detail, starting from the Content
Identification Module.
Figure 2
System architecture.
532
Demir, Carberry, and McCoy Summarizing Information Graphics Textually
3.1 Visual Extraction System
The Visual Extraction System (Chester and Elzer 2005) analyzes a graphic image (visual
image of a bar chart) and creates an XML representation specifying the components
of the graphic, such as the height and color of each bar, any annotations on a bar, the
caption of the graphic, and so forth. The current implementation handles vertical and
horizontal bar charts that are clearly drawn with specific fonts and no overlapping
characters. The charts can have a variety of textual components such as axis labels,
caption, further descriptive text, text inside the graphic, and text below the graphic.
The current system cannot handle 3D charts, charts where the bars are represented by
icons, or charts containing texts at multiple angles, however.
3.2 Bayesian Inference System for Intention Recognition
The Bayesian Inference System (Elzer, Carberry, and Zukerman 2011) treats an informa-
tion graphic as a form of language with a communicative intention, and reasons about
the communicative signals present in the graphic to recognize its intendedmessage. The
system is currently limited to simple bar charts and takes as input the XML representa-
tion of the chart produced by the Visual Extraction System described previously.
Three kinds of communicative signals that appear in bar charts are extracted from a
graphic and utilized by the system. The first kind of signal is the relative effort required
for various perceptual and cognitive tasks. The system adopts the AutoBrief (Kerpedjiev
and Roth 2000) hypothesis that the graphic designer chooses the best design to facilitate
the perceptual and cognitive tasks that a viewer will need to perform on the graphic.
Thus, the relative effort for different perceptual tasks serves as a communicative signal
about what message the graphic designer intended to convey (Elzer et al 2006). The
second and third types of communicative signals used in the system are salience and
the presence of certain verbs and adjectives in the caption that suggest a particular
message category. The presence of any of these three kinds of communicative signals
are entered into a Bayesian network as evidence. The top level of the network captures
one of the 12 message categories that have been identified as the kinds of messages that
can be conveyed by a bar chart, such as conveying a change in trend (Changing Trend)
or conveying the bar with the highest value (Maximum Bar). The system produces as
output the hypothesized intended message of a bar chart as one of these 12 message
categories, along with the instantiated parameters of the message category, in the form
of a logical representation such as Maximum Bar(first bar) for the graphic in Figure 1
and Increasing Trend(first bar, last bar) for the graphic in Figure 3a.
3.3 Corpus of Graphics
We collected 82 groups of graphics along with their articles from 11 different magazines
(such as Newsweek and Business Week) and newspapers. These groups of graphics
varied in their structural organization: 60% consisted solely of a simple bar chart (e.g.,
the graphic in Figure 1 on Page 2) and 40% were composite graphics (e.g., the graphic
in Figure 8a in Section 7.1.1) consisting of at least one simple bar chart along with
other bar charts or other kinds of graphics (e.g., stacked bar charts or line graphs). We
selected at least one simple bar chart from each group and our corpus contained a total
of 107 bar charts. The Bayesian Inference System had an overall success rate of 79.1% in
recognizing the correct intended message for the bar charts in our corpus using leave-
one-out cross-validation (Elzer, Carberry, and Zukerman 2011).
533
Computational Linguistics Volume 38, Number 3
Figure 3
(a) Graphic conveying an increasing trend. (b) Graphic conveying the ranking of all bars.
In the work described in this article, we only used the bar charts whose intended
message was correctly recognized by the Bayesian Inference System and associated each
chart with the inferred message category. Here, our intent is to describe a generation
approach that works through a novel problem from beginning to end by handling a
multitude of generation issues. Thus, using bar charts with the perfect intention is
reasonably appropriate within the scope of the present work. For each bar chart, we
also used the XML representation that was utilized by the Bayesian Inference System.
Slightly less than half of the selected bar charts were kept for testing the system per-
formance (which we refer to as the test corpus), and the remaining graphs were used
for developing the system (which we refer to as the development corpus). Because the
number of graphics in the development corpus was quite limited, we constructed a
number of bar charts2 in order to examine the effects of individual salient features
observed in the graphics from the development corpus. These graphs, most of which
were obtained by modifying original graphics, enabled us to increase the number of
graphics in the development corpus and to explore the system behavior in various new
cases.
4. Content Identification Module (CIM)
Our ultimate goal is to generate a brief and coherent summary of a graphic. Identifying
and realizing the high-level informational content of a graphic is not an easy task,
however. First, a graphic depicts a large amount of information and therefore it would
be impractical to attempt to provide all of this information textually to a user. Second, a
graphic is chosen as the communication medium because a reader can get information
from it at many different levels. A casual look at the graphic is likely to convey the
intended message of the graphic and its salient features. At the same time, a reader
could spend much more time examining the graphic to further investigate something
of interest or something they noticed during their casual glance.
In order to address the task of identifying the content of a summary, we extend
to simple bar charts the insights obtained from an informal experiment where human
participants were asked to write a brief summary of a series of line graphs with the
same high-level intention (McCoy et al 2001). The most important insight gained from
2 The graphics that we constructed were not used in any of the evaluation experiments with human
participants described throughout this article.
534
Demir, Carberry, and McCoy Summarizing Information Graphics Textually
this study is that the intended message of a graphic was conveyed in all summaries no
matter what the visual features of the graphic were. It was observed that the participants
augmented the intended message with salient features of the graphic (e.g., if a line
graph is displaying an increasing trend and the variance in that trend is large, then
the variance is salient) and that what was found salient depended on the graphic?s
intended message. Because the participants generated similar summaries for a par-
ticular graphic, we hypothesize that they perceived the same salient features for that
graphic. Although the set of features that might be salient is the same for different
graphics sharing the same underlying intention, the differences observed between the
summaries generated for different graphics with the same intention can be explained
by whether or not the features are salient in those graphics. The fact that the summaries
did not include all information that could be extracted from the graphic (such as
the value of every point in a line graph) but only visually salient features, correlates
with Grice?s Maxim of Quantity (1975) which states that one?s discourse contribution
should be as informative as necessary for the purposes of the current exchange but not
more so.
To extend these observations to constructing brief summaries of bar charts, we
hypothesize that (1) the intended message of the bar chart should form the core of
its textual summary and (2) the most significant and salient features of the bar chart,
which are related to its intended message, should be identified and included in that
summary. The inferred intended message of a bar chart serves as a starting point for
our content identification approach. In the rest of this section, we first describe a series
of experiments that we conducted to identify what constitutes the salient features of
a given bar chart and in which circumstances these features should be included in its
textual summary. We then present the content identification rules that were constructed
to automatically select appropriate content for the summary of a bar chart.
4.1 Experiments
We conducted a set of formal experiments to find patterns between the intended mes-
sage of a graphic, salient visual features of the displayed data, and the propositions
selected for inclusion in a brief summary. We identified the set of all propositions
(PROPALL) that capture information that we envisioned someone might determine
by looking at a bar chart. This set included a wide variety of pieces of information
present in a bar chart and contained propositions common to all bar charts as well
as propositions which were applicable only to some of the message categories. The
following is a subset of the identified propositions. In this example, Propositions 1?4
are common to all bar charts; in contrast, Propositions 5?8 are only present when the
bar chart is intended to convey a trend:
 The labels of all bars (Proposition 1)
 The value of a bar (Proposition 2)
 The percentage difference between the values of two bars (Proposition 3)
 The average of all bar values (Proposition 4)
 The range of the bar values in the trend (Proposition 5)
 The overall percentage change in the trend (Proposition 6)
535
Computational Linguistics Volume 38, Number 3
 The change observed at a time period (Proposition 7)
 The difference between the largest and the smallest changes observed in
the trend (Proposition 8)
Some propositions, which we refer to as open propositions, require instantiation
(such as Propositions 2, 3, and 7 given here) and the information that they convey varies
according to their instantiations.3 In addition, the instantiation of an open proposition
may duplicate another proposition. For example, if the Proposition 3 is instantiatedwith
the first and the last bars of the trend, then the information conveyed by that proposition
is exactly the same as Proposition 6.
To keep the size of the experiment reasonable, we selected 8 message categories
from among the 12 categories that could be recognized by the Bayesian Inference Sys-
tem; these categories were the onesmost frequently observed in our corpus and could be
used as a model for the remaining message categories. These categories were Increasing
Trend, Decreasing Trend, Changing Trend, Contrast Point with Trend, Maximum Bar,
Rank Bar, Rank All, and Relative Difference. In the experiments, we did not use the
categoriesMinimumBar (which can bemodeled viaMaximumBar), Relative Difference
with Degree (which can be modeled via Relative Difference), Stable Trend (which was
not observed in the corpus), and Present Data (which is the default category selected
when the system cannot infer an intended message for the graphic).
For each message category, we selected two to three original graphics from the
development corpus, where the graphics with the same intended message presented
different visual features. For example, we selected two graphics conveying that a par-
ticular bar has the highest value among the bars listed, but only in one of these graphics
was the value of the maximum bar significantly larger than the values of the other bars
(such as the graphic in Figure 1). In total, 21 graphics were used in the experiments and
these graphics covered all selected intended message categories. Because the number of
propositions applicable to each message category was quite large, 10?12 propositions
were presented for each graphic. Each graphic was presented to at least four partici-
pants. Overall, the experiments covered all selected intended message categories and
all identified propositions.
Twenty participants, who were unaware of our system, participated in the experi-
ments. The participants were graduate students or recent Ph.D. graduates from a variety
of departments at the University of Delaware. Each experiment started with a brief
description of the task, where the participants were told to assume that in each case
the graphic was part of an article that the user is reading and that the most important
information depicted in the graphic should be conveyed in its summary. They were also
told that they would be given an information graphic along with a sentence conveying
the intended message of the graphic and a set of propositions, and would be asked to
classify these additional propositions into one of three classes according to how impor-
tant they felt it was to include that proposition in the textual summary:4 (1) Essential:
This proposition should be included in the brief textual summary, (2) Possible: This
proposition could be included in the brief textual summary but it?s not essential, and (3)
Not Important: This proposition should not be included in the brief textual summary.
3 We used open propositions in order to keep PROPALL within a manageable size.
4 The participants were also asked to instantiate the open propositions that they classified as Essential or
Possible.
536
Demir, Carberry, and McCoy Summarizing Information Graphics Textually
4.2 Analysis
To analyze the experiment?s results, we first assigned a numeric score to each
class indicating the level of importance assigned by the participants: Essential = 3,
Possible = 1, Not-important = 0. We then calculated an ?importance level? (IL) for each
proposition with respect to a particular graphic, where the importance level estimates
how important it is for that proposition to be included in the graphic?s summary.
The importance level of a proposition was computed by summing the numeric
scores associated with the classes assigned by the participants. For example, if three
participants classified a proposition as Essential and two participants as Possible, the
importance level of that proposition in the graphic was (3? 3) + (2? 1) = 11. In cases
where a proposition (Prop A) and an instantiated open proposition which conveyed
the same information were classified by a participant into different classes for the same
graphic, the classification of the proposition that came earlier in the presentation was
used in computing the importance level of Prop A.
Given these computed scores, we needed to identify which propositions to con-
sider further for inclusion in a summary. Because there was a divergence between
the sets of propositions that were classified as essential by different participants, we
decided to capture the general tendency of the participants. For this purpose, we
defined majority importance level as a ranking criteria, which is the importance level
that would be obtained if half of the participants classify a proposition as essential.
For example, the majority importance level would be (6? 3)/2 = 9 if there were six
participants. We classified a proposition as a highly rated proposition if its importance
level was equal to or above the majority importance level.5 The propositions that were
classified as highly rated for the graphics with a message category formed the set of
highly rated propositions that should be considered for inclusion for that message
category.
We had to ensure that the propositions presented to the participants (PROPALL)
actually covered all information that is important enough to include in the summary of
a bar chart. Thus, for each graphic, we also asked participants if there was anything else
they felt should be included in the brief summary of the graphic. We received only a
few isolated suggestions such as a proposition conveying what type of a curve could fit
the trend. Moreover, these suggestions were not common among the participants, and
nothing was mentioned by more than one participant (indeed most did not make any
suggestions). Thus, we concluded that these suggestions were not appropriate for the
textual summary of a bar chart.
4.3 Content Identification Rules for Message Categories
Using the importance level scores, we needed to identify the subset of the highly rated
propositions that should be included in the textual summary in addition to the graphic?s
intended message. For each message category, we examined the similarities and the
differences between the sets of highly rated propositions identified for the graphics
5 The reason behind assigning particular scores (3,1,0) to the classes is to guarantee that a proposition will
not be selected as a highly rated proposition if none of the participants thought that it was essential.
Assume k participants classified a proposition (Prop A). The majority importance level of this proposition
(MIL(Prop A)) is (3? k)/2. A proposition is classified as highly rated if its importance level (IL(Prop A))
is equal to or greater than the majority importance level (IL(Prop A) ?MIL(Prop A)). If all of the
participants classified the proposition as Possible, the IL(Prop A) is 1? k, which is less than MIL(Prop A).
537
Computational Linguistics Volume 38, Number 3
associated with that message category, related these differences to the visual features
present in these graphics, and constructed a set of content identification rules for
identifying propositions to be included in the summary of a graphic from that message
category. If a proposition was marked as highly rated for all graphics in a particular
message category, then its selection was not dependent on particular visual features
present in these graphics. In such cases, our content identification rule simply states that
the proposition should be included in the textual summary for every graphic whose
inferred message falls into that message category. For the other propositions that are
highly rated for only a subset of the graphics in a message category, we identified a fea-
ture that was present in the graphics where the proposition was marked as highly rated
and was absent when it was not marked as highly rated, and our content identification
rules use the presence of this feature in the graphic as a condition for the proposition
to be included in the textual summary. In addition, we observed that a highly rated
proposition for a message category might require inclusion of another proposition for
realization purposes. For example, in the Rank All message category, the proposition
indicating the rank of each bar was identified as highly rated and thus could be included
in the textual summary. Because the rank of a bar cannot be conveyed without its label,
we added the proposition indicating the label of each bar to the content identification
rule containing the rank proposition, although this extra proposition was not explicitly
selected by the participants for inclusion. Notice that these steps?identifying features
that distinguish one subset of graphs from the other and identifying propositions that
need to be included for realizing other propositions?make it difficult to use machine
learning for this task. In our case the number of possible features that can be extracted
from a graphic is very large and it is difficult to know which features from among
those may be important/defining in advance. In addition, the number of graphics in
our development corpus is too small to expect machine learning to be effective.
The following are glosses of two partial sets of representative content identification
rules. The first set is applicable to a graphic conveying an increasing trend and the
second set is applicable to a graphic conveying the rankings of all bars present in the
graph:
 Increasing Trend message category:6
1. If (message category equals ?increasing trend?) then
include(proposition conveying the rate of increase of the trend):
Include the proposition conveying the rate of increase of the trend
2. If (message category equals ?increasing trend?) and
notsteady7(trend) then include(proposition conveying the
period(s) with a decrease):8
If the trend is not steady and has variability, then include the proposition
indicating where the trend varies
6 The ?notsteady? function returns true if its argument is not a steady trend; the ?value? function returns
the values of all members of its argument; the ?greaterthan? function returns true if the left argument is
greater than the right argument; the ?withinrange? function returns true if all members of its left
argument are within the range given by its right argument; the ?average? function returns the average of
the values of all members of its argument.
7 A trend is unsteady if there is at least one period with a decrease in contrast with the increasing trend.
8 The inclusion of propositions whose absence might lead the user to draw false conclusions is consistent
with Joshi, Webber, and Weischedel?s (1984) maxim, which states that a system should not only produce
correct information but should also prevent the user from drawing false inferences.
538
Demir, Carberry, and McCoy Summarizing Information Graphics Textually
3. If (message category equals ?increasing trend?) and (value(last bar)
greaterthan (3*value(first bar))) then include(proposition
conveying the overall percentage increase in the trend):
If the overall percentage increase in the trend is significantly large, then
include the proposition conveying the percentage increase in the trend
 Rank All message category:
1. If (message category equals ?rank all?) then include(propositions
conveying the label and the value of the highest bar):
Include the propositions conveying the label and the value of the
highest bar
2. If (message category equals ?rank all?) and (value(all bars)
withinrange ((0.7*average(all bars)),(1.3*average(all bars)))) then
include(proposition indicating that the bar values vary slightly):
If the values of bars are close to each other, then include the proposition
indicating that the bar values vary slightly
3. If (message category equals ?rank all?) and (not(value(all bars)
withinrange ((0.7*average(all bars)),(1.3*average(all bars))))) then
include(propositions conveying the label and the value of the
lowest bar):
If the values of bars are not close to each other, then include the
propositions conveying the label and the value of the lowest bar
We defined the conditions of all content identification rules as a conjunction of one
or more expressions where some expressions required us to determine threshold values
to be used for comparison purposes. For example, we observed that the proposition
conveying the overall percentage change in the trend was marked as highly rated
only for graphics which depicted a significant change in the trend. We handled this
situation for graphics with an increasing trend by defining the third content identi-
fication rule (shown earlier) where we needed to set the lowest threshold at which
an overall increase observed in a trend can be accepted as significantly large. For
setting such threshold values, we examined all graphs in the development corpus
to which the corresponding content identification rule is applicable (i.e., the graphs
associated with the message category for which the rule is defined) and used our
intuitions about whether the proposition captured by the rule should be selected for
inclusion in the summaries of these graphs. We set the threshold values using the
results obtained from group discussions such that the final setting classified all of
the original graphics the way the participants did in the experiments described in
Section 4.1.
When the content identification rules constructed for the Increasing Trend message
category are applied to the bar chart in Figure 3a, the following pieces of information
are selected for inclusion in addition to the intended message of the graphic:
 The rate of increase of the trend, which is slight
 The small drop observed in the year 1999
 The overall percentage increase in the trend, which is 225%
539
Computational Linguistics Volume 38, Number 3
When the content identification rules constructed for the RankAll message category
are applied to the bar chart in Figure 3b, the following pieces of information are selected
for inclusion in addition to the intended message of the graphic:
 The label and the value of the highest bar, which is Army with 233,030
 The label and the value of the lowest bar, which is Other defense agencies
with 100,678
 The label and the ranking of each bar:9 Army is the highest, Navy is the
second highest, Air Force is the third highest, and Other defense agencies
is the lowest
4.4 Evaluation of the Content Identification Module
We conducted a user study to assess the effectiveness of our content identification
module in identifying the most important information that should be conveyed about
a bar chart. More specifically, the study had three goals: (1) to determine whether the
set of highly rated propositions that we identified for each message category contains
all propositions that should be considered for inclusion in the summaries of graphics
with that message category; (2) to determine how successful our content identification
rules are in selecting highly rated propositions for inclusion in the summary; and (3)
to determine whether the information conveyed by the highly rated propositions is
misleading or not.
Nineteen students majoring in different disciplines (such as Computer Science and
Materials Science and Engineering) at the University of Delaware were participants
in the study. These students neither participated in the earlier study described in Sec-
tion 4.1 nor were aware of our system. Twelve graphics from the test corpus (described
in Section 3.3) whose intended message was correctly identified by the Bayesian Infer-
ence System were used in the experiments. Once the intended message was recognized,
the corresponding content identification rules were executed in order to determine the
content of the graphic?s summary. Prior to the experiment, all participants were told
that they would be given a summary and that it should include the most important
information that they thought should be conveyed about the graphic. Each participant
was presented with three graphics from among the selected graphics such that each
graphic was viewed by at least four participants. For each graphic, the participants
were first given the summary of the graphic generated by our approach and then shown
the graphic. The participants were then asked to specify if there was anything omitted
that they thought was important and therefore should be included in the summary. In
addition, the participants were asked to specify whether or not they were surprised
or felt that the summary was misleading (i.e., whether the bar chart was similar to
what they expected to see after reading its summary). Note that our summaries with
relatively few propositions are quite short. Thus our evaluation focused on determin-
ing whether anything of importance was missing from the summary or whether the
summary was misleading. In the experiments, we did not ask the participants to rate
9 This piece of information is selected by a rule defined for the Rank All message category not shown in the
bulleted list on the previous page.
540
Demir, Carberry, and McCoy Summarizing Information Graphics Textually
the content of summaries on a numeric scale in order to restrict them to evaluating only
the selected content as opposed to its presentation (i.e., the organization and realization
of the summary).
Feedback that we received from the participants was very promising. In most of
the cases (43 out of 57 cases), the participants were satisfied with the content that our
approach selected for the presented bar charts. There were a number of suggestions for
what should be added to the summaries in addition towhat had already been conveyed,
and in a couple of these cases, we observed that a highly rated proposition which was
not selected by the corresponding content identification rule was contrarily suggested
by the participants. There was no consensus in these suggestions, however, as none
was made by more than two participants. Some of the participants (3 out of 19) even
commented that we provided more information than they could easily get from just
looking at the graphic. In addition, a few participants (2 out of 19) commented that,
in some graphics, they didn?t agree with the degree (e.g., moderate or steep) assessed
by our approach for differences between bar values (e.g., the rate of change of the
trend), and therefore they thought the summary was misleading. Because there wasn?t
any common consensus among the participants, we didn?t address this very subjective
issue. Overall, we conclude that the sets of highly rated propositions that we identified
contain the most important information that should be considered for inclusion in the
summaries of bar charts and that our system effectively selects highly rated propositions
for inclusion when appropriate.
5. Text Structuring and Aggregation Module (TSAM)
A coherent text has an underlying structure where the informational content is pre-
sented in some particular order. Good text structure and information ordering have
proven to enhance the text?s quality by improving user comprehension. For example,
Barzilay, Elhadad, and McKeown (2002) showed that the ordering has a significant
impact on the overall quality of the summaries generated in theMULTIGEN system. Al-
though previous research highlights a variety of structuring techniques, there are three
prominent approaches that we looked to for guidance: top?down planning, application
of schemata, and bottom?up planning.
In top?down planning (Hovy 1988, 1993; Moore and Paris 1993), the assumption is
that a discourse is coherent if the hearer can recognize the communicative role of each
of its segments and the relation between these segments (generally mapped from the
set of relations defined in rhetorical structure theory (RST; Mann and Thompson 1987).
The discourse is usually represented as a tree-like structure and the planner constructs
a text plan by applying plan operators starting from the initial goal.
In the TEXT system (McKeown 1985), a collection of naturally occurring texts
were analyzed to identify certain discourse patterns for different discourse goals, and
these patterns were represented as schemas which are defined in terms of rhetorical
predicates. The schemas both specify what should be included in the generated texts
and how they should be ordered given a discourse goal. Lester and Porter (1997)
used explanation design packages, schema-like structures with procedural constructs
(for example, the inclusion of a proposition can be constrained by a condition), in
the KNIGHT system, which is designed to generate explanations from a large-scale
biology knowledge base. Paris (1988) applied the idea of schemata in the TAILOR
system to tailor object descriptions according to the user?s level of knowledge about
the domain.
541
Computational Linguistics Volume 38, Number 3
Marcu (1998) argued that text coherence can be achieved by satisfying local con-
straints on ordering and clustering of semantic units to be realized. He developed a
constraint satisfaction based approach to select the best plan that can be constructed
from a given set of textual units and RST relations between them, and showed that
such bottom?up planning overcomes the major weakness of top?down approaches
by guaranteeing that all semantic units are subsumed by the resulting text plan. The
ILEX system (O?Donnell et al 2001), which generates descriptions for exhibits in a
museum gallery, utilizes a similar bottom?up planning approach (Mellish et al 1998)
where the best rhetorical structure tree over the semantic units is used as the text
structure.
Because our content identification rules identify a set of propositions to be con-
veyed, it appears that a bottom?up approach that ensures that all propositions will be
included is in order. At the same time, it is important that our generated text adheres to
an overall discourse organization such as is provided by the top?down approaches.
Because of the nature of the propositions (the kinds of rhetorical relations that can
exist between propositions in a descriptive domain are arguably limited [O?Donnell
et al 2001]), however, a structure such as RST is not helpful here. Thus, the top?down
planning approach does not appear to fit. Although something akin to a schema might
work, it is not clear that our individual propositions fit into the kind of patterns used
in the schema-based approach. Instead we use what can be considered a combination
of a schema and a bottom?up approach to structure the discourse. In particular, we
use the notion of global focus (Grosz and Sidner 1986) and group together proposi-
tions according to the kind of information they convey about the graphic. We define
three proposition classes (message-related, specific, and computational) to classify the
propositions selected for inclusion in a textual summary. The message-related class
contains propositions that convey the intended message of the graphic. The specific
class contains the propositions that focus on specific pieces of information in the
graphic, such as the proposition conveying the period with an exceptional drop in a
graphic with an increasing trend or the proposition conveying the period with a change
which is significantly larger than the changes observed in other periods in a graphic
with a trend. Lastly, propositions in the computational class capture computations or
abstractions over the whole graphic, such as the proposition conveying the rate of
increase in a graphic with an increasing trend or the proposition conveying the overall
percentage change in the trend. In our system, all propositions within a class will
be delivered as a block. But we must decide how to order these blocks with respect
to each other. In order to emphasize the intended message of the graphic (the most
important piece of the summary), we hypothesize that the message-related propositions
should be presented first. We also hypothesize that it is appropriate to close the textual
summary by bringing the whole graphic back into the user?s focus of attention (Grosz
and Sidner 1986) (via the propositions in the computational class). Thus, we define an
ordering of the proposition classes (creating a partial ordering over the propositions)
and present first the message-related propositions, then the specific propositions, and
finally the computational propositions. Section 6 will address the issue of ordering the
propositions within these three classes.
5.1 Representing Summary Content
First we needed to have a representation of content that would provide us with the
most flexibility in structuring and realizing content. For this we used a set of basic
propositions. These were minimal information units that could be combined to form
542
Demir, Carberry, and McCoy Summarizing Information Graphics Textually
the intended message and all of the propositions identified in our content identification
rules. This representation scheme increases the number of aggregation and realization
possibilities that could be explored by the system, which is described in the next
subsection. We defined two kinds of knowledge-base predicates to represent the basic
propositions:
(1) Relative Knowledge Base: These predicates are used to represent the basic
propositions which introduce graphical elements or express relations
between the graphical elements.
(2) Attributive Knowledge Base: These predicates are used to represent the
basic propositions which present an attribute or a characteristic of a
graphical element.
Each predicate contains at least two arguments and we refer to the first argument
as the main entity and the others as the secondary entities. The main entity of each
predicate is a graphical element and the secondary entities are either a string constant
or a graphical element. Some of the graphical elements that we used in this work are as
follows:
 graphic: ?the graphic itself?
 trend: ?the trend observed in the graphic?
 descriptor: ?a referring expression that represents what is being measured
in the graphic?10
 bar(x): ?a particular bar in the graphic?
1 <= x <= n where n = number of bars in the graph
 all bars: ?all bars depicted in the graphic? bset = {bar(x) | 1 <= x <= n}
 period(x,y): ?a period depicted in the graphic? 1 ? x < n and 1 < y ? n
 change(x,y): ? the change between the values of any two bars?
1 ? x < n and 1 < y ? n
 all changes: ?changes between all pairs of bars of the graphic?
cset = {change(x, y) | 1 ? x < n, 1 < y ? n}
 trend period: ?the period over which the trend is observed?
 graph period: ?the period which is depicted by the graphic?
 trend change: ?the overall change observed in the trend?
Table 1 presents sample instantiations of a subset of the predicates that we defined
for this work along with a possible realization for each instantiation. Although the
number of arguments in Relative Knowledge Base predicates (predicates 1 to 15) varies,
10 How that referring expression is extracted from the text associated with the graphic is described in
detail in Section 7.1. For example, the descriptor identified by our system for the graphic in Figure 4
is the dollar value of net profit.
543
Computational Linguistics Volume 38, Number 3
Table 1
Sample instantiations and possible realizations of a subset of our predicates.
1 shows(graphic,trend)
The graphic shows a trend
2 focuses(graphic,bar(3))
The graphic is about the third bar
3 covers(graphic, graph period)
The graphic covers the graph period
4 exists(trend,descriptor)
The trend is in the descriptor
5 has(trend,trend period)11
The trend is over the trend period
6 starts(trend period,?2001?)
The trend period starts at the year 2001
7 ends(trend period,?2010?)
The trend period ends at the year 2010
8 ranges(descriptor,?from?,?20 percent?,trend period)
The descriptor ranges from 20 percent over the trend period
9 hasextreme(descriptor,?largest?,change(3,4),period(3,4))
The descriptor shows the largest change between the third and the fourth bars
10 averages(descriptor,all bars,?55.4 billion dollars?)
The descriptor for all bars averages to 55.4 billion dollars
11 comprises(descriptor,trend change,trend period)
The descriptor comprises a trend change over the trend period
12 occurs(change(2,3),period(2,3))
A change occurs between the second and the third bars
13 hasdifference(change(1,5),bar(1),bar(5),descriptor)
A difference is observed between the descriptor of the first bar and that of the fifth bar
14 observed(all changes,?every?,interval,trend period)
Changes are observed every interval over the trend period
15 presents(descriptor,bar(3),?12 percent?)
The descriptor for the third bar is 12 percent
16 hasattr(trend change,?type?,?increase?)
The trend change is an increase
17 hasattr(change(2,3),?degree?,?moderate?)
The change is of degree moderate
18 hasattr(change(2,3),?amount?,?70 dollars?)
The change amount is 70 dollars
19 hasattr(trend change,?percentage amount?,?22 percent?)
The trend change percentage amount is 22 percent
20 hasattr(all changes,?rate?,?slight?)
Changes are slight changes
all Attributive Knowledge Base predicates (encoded as hasattr) consist of three argu-
ments, where the first argument is the graphical element being described, the second
is an attribute of the graphical element, and the third is the value of that attribute
(predicates 16 to 20).12
11 Notice that the graphical element trend period in 5 is the main entity in 6 and 7. These all might be
combined using the And operator to produce the realization The trend starts at the year 2001 and ends
at the year 2010.
12 Because all Attributive Knowledge Base predicates have the same form, the amount and unit of a change
are represented as a single string which is derived from the textual components of the graphic (such as
70 dollars in Predicate 18).
544
Demir, Carberry, and McCoy Summarizing Information Graphics Textually
Figure 4
Graphic conveying a decreasing trend.
For example, consider how the propositions given in Section 4.1 can be represented
with the predicates shown in Table 1. Some of those propositions require a single
predicate. For example, the proposition conveying the value of a bar (Proposition 2)
can be represented via the predicate ?presents? (Predicate 15) and the proposition
conveying the average of all bar values (Proposition 4) via the predicate ?aver-
ages? (Predicate 10). On the other hand, some propositions require more than one
predicate. For example, the proposition conveying the overall percentage change
in the trend shown in Figure 4 (Proposition 6) can be represented via the predi-
cates ?comprises(descriptor,trend change,trend period)? (Predicate 11), ?starts(trend
period,?1998?)? (Predicate 6), ?ends(trend period,?2006?)? (Predicate 7), ?hasattr(trend
change,?type?,?decrease?)? (Predicate 16), and ?hasattr(trend change,?percentage
amount?,?65 percent?)?(Predicate 19). The same set of predicates can be used to rep-
resent the overall amount of change in the trend by replacing the constant ?percentage
amount? with the string ?amount? in Predicate 19.
As is shown by the possible realizations included in Table 1, each basic proposition
can be realized as a single sentence. Although we determined a couple of different
ways (i.e., simple sentences) of realizing each basic proposition, our current imple-
mentation always chooses a single realization (which we refer to as ?the realization
associated with the proposition?) and the main entity is always realized in subject
position.13
5.2 Aggregating Summary Content
The straightforward way of presenting the informational content of a summary is to
convey each proposition as a single sentence while preserving the partial ordering of
the proposition classes. The resultant text would not be very natural and coherent,
however. Aggregation is the process of removing redundancies during the generation
of a more concise and fluent text (Shaw 1998; Dalianis 1999). Aggregation (typically
syntactic aggregation [Reiter and Dale 2000]) has received considerable attention
from the NLG community (McKeown et al 1997; O?Donnell et al 2001; Barzilay and
Lapata 2006), and has been applied in various existing generation systems such as the
intelligent tutoring application developed by Di Eugenio et al (2005). Our aggregation
mechanism works to combine propositions into more complex structures. It takes
13 We leave it as a future work to explore how different realizations for a proposition, including ones
where the main entity is not in subject position, can be utilized by our approach.
545
Computational Linguistics Volume 38, Number 3
advantage of the two types of predicates (Relative Knowledge Base and Attributive
Knowledge Base predicates) and the shared entities between predicates. In order to
relate propositions and explore syntactically aggregating them, our mechanism treats
each proposition as a single node tree which can be realized as a sentence and attempts
to form more complex trees by combining individual trees via four kinds of operators
in such a way so that the more complex tree (containing multiple propositions) can still
be realized as a single sentence. The first operator (Attribute Operator) works only on
propositions with an Attributive Knowledge Base predicate and essentially identifies
opportunities to realize such a proposition as an adjective attached to a noun object
in the realization of another proposition. The remaining three operators, which do
not work on propositions with an Attributive Knowledge Base predicate, introduce
new nodes corresponding to operational predicates (And, Same, and Which) with a
single entity into the tree structures. Two of these operators (And Operator and Which
Operator) work on trees rooted by a proposition with a Relative Knowledge Base or an
And predicate. These operators look for opportunities for VP conjunction and relative
clauses, respectively. The third operator (Same Operator) works on trees rooted by a
proposition with a Relative Knowledge Base predicate and identifies opportunities for
NP conjunction. Although each predicate is associated with a unique realization in the
current implementation, none of these operators depend on how the corresponding
predicates or the entities in those predicates are realized.
Having defined the operators we next had to turn to the problem of determining
how these operators should be applied (e.g., which combinations are preferred). The
operators we defined are similar to the clause-combining operations used by the SPoT
sentence planner (Walker, Rambow, and Rogati 2002; Stent, Prasad, and Walker 2004;
Walker et al 2007) in the travel planner system AMELIA. In AMELIA, for each of
the 100 different input text plans, a set of possible sentence plans (up to 20 plans)
were generated by randomly selecting which operations to apply according to assumed
preferences for operations. These possible sentence plans were then rated by two judges
and the collected ratings were used to train the SPoT planner. Although we greatly
drew from the work on SPoT as we developed our aggregation method, we chose
not to follow their learning methodology. In the SPoT system, some of the features
were domain- and task-dependent and thus porting to a new domain would require
retraining. In addition, the judgments of the two raters were collected in isolation and
it is unclear how these would translate to the task situation the texts were intended
for. Although this methodology was innovative and necessary for SPoT because of
the large number of possible text plans, we chose to select the best text plan on the
basis of theoretically informed complexity features balancing sentence complexity and
number of sentences. Because our text plans are significantly more constrained, it is
possible to enumerate each of them and choose the one that best fits our rating cri-
teria.14 This has the added benefit of better understanding the complexity features by
evaluating the resulting text. In addition, our method would be open to both upgrad-
ing the selection criteria and adding further aggregation operators without requiring
retraining.15
14 Although in our implementation we do enumerate all plans before the rating criteria are applied to select
the best one, it is in principle possible to generate the text plans in an order that would allow maximizing
the scoring functions without first enumerating all possibilities. This is left for future work.
15 Such modifications and additions need to be empirically evaluated with empirical data, however.
546
Demir, Carberry, and McCoy Summarizing Information Graphics Textually
Operator: Attribute Operator
Gloss: This operator attaches a single node tree that consists solely of a proposition with
an Attributive Knowledge Base predicate, as a direct subchild of a node N with a Relative
Knowledge Base predicate in another tree, if the main entity of the Attributive Knowledge Base
predicate is an entity (main or secondary) for the proposition at node N.
Input: T1 and T2
Constraints:
1. (pred(T1-root)==?hasattr?)
2. ((pred(T2-node)!=?hasattr?) ? (pred(T2-node)!=?And?) ?
(pred(T2-node)!=?Which?))
3. ((main ent(T1-root)==main ent(T2-node)) ?
(main ent(T1-root)==secondary ent(T2-node)))
Output: A modified T2 such that
1. left child(T2-node)?T1
Glossary:
1. Tx-root: the root node of tree Tx
2. Tx-node: any node in tree Tx (including Tx-root)
3. pred(Tx-node): the predicate at Tx-node
4. left/right child(Tx-node): the leftmost/rightmost child of Tx-node
5. main/secondary ent(Tx-node): the main/secondary entity of the proposition at Tx-node
6. !=: not equal, ==: equal, !: not,?: assign
Operator: And Operator
Gloss: This operator combines two trees if the propositions at their root share the same main
entity. A proposition containing an And predicate with the same main entity forms the root of
the new tree and the trees that are combined form the immediate descendents of this root.
Input: T1 and T2
Constraints:
1. ((pred(T1-root)!=?hasattr?) ? (pred(T2-root)!=?hasattr?))
2. !((pred(T1-root)==?And?) ? (pred(T2-root)==?And?))
3. (main ent(T1-root)==main ent(T2-root))
Output: a new tree T3 where the root node has two immediate children such that
1. pred(T3-root)??And? ?main ent(T3-root)?main ent(T1-root)
2. left child(T3-root)?T1 ? right child(T3-root)?T2
Operator:Which Operator
Gloss: This operator attaches a tree (Tree A) as a descendent of a node N in another tree (Tree B)
via a Which predicate, if the main entity of the proposition at the root of Tree A is a secondary
entity for the proposition at node N of the other tree (Tree B). That particular entity forms the
main entity of the Which predicate. Thus, Tree A will be an immediate child of the node with
the Which predicate and the node with the Which predicate will be an immediate child of node
N in Tree B.
Input: T1 and T2
Constraints:
1. ((pred(T1-root)!=?hasattr?)?(pred(T2-node)!=?hasattr?))
2. (main ent(T1-root)==secondary ent(T2-node))
Output: A modified T2 via the addition of a new node (Node x) with a single immediate child
such that
1. pred(Node x)? ?Which? ?main ent(Node x)?main ent(T1-root)
2. right child(T2-node)?Node x ? left child(Node x)?T1
547
Computational Linguistics Volume 38, Number 3
Operator: Same Operator
Gloss: This operator combines two trees if the propositions at their root contain the same
predicate but the main entities of these predicates are different. A proposition with a Same
predicate forms the root of the new tree, and the trees that are combined form the descendents
of this root. Since the descendents of the new tree have different main entities, the main entity of
the Same predicate is some unique element not occurring elsewhere in the tree. For instance, in
our implementation this element is obtained by appending a unique number, which isn?t used
in another Same predicate in the current forest, to the term random (such as random0).
Input: T1 and T2
Constraints:
1. ((pred(T1-root)!=?hasattr?)?(pred(T2-root)!=?hasattr?) ?
(pred(T1-root)!=?And?) ? (pred(T2-root)!=?And?) ?
(pred(T1-root)!=?Same?) ? (pred(T2-root)!=?Same?))
2. (pred(T1-root)==pred(T2-root))
3. (main ent(T1-root)!=main ent(T2-root))
Output: a new tree T3 where the root node has two immediate children such that
1. pred(T3-root)??Same? ?main ent(T3-root)? a unique element
2. left child(T3-root)?T1 ? right child(T3-root)?T2
In our work, we thus developed a method that would choose a text plan on prin-
cipled reasoning concerning the resulting text. In particular, we looked to balance sen-
tence complexity and the number of sentences in the generated text. Moreover, whereas
such a method was not applicable in the case of SPoT (because of the significantly larger
set of operators with few constraints resulting in potential text plans too numerous to
evaluate), our work differs in several aspects that make it reasonable to generate all
text plans and apply an evaluation metric. First, our system has a small number of
aggregation operators and all operators cannot be applied to all kinds of predicates (e.g.,
the Attribute Operator cannot be applied to the Relative Knowledge Base predicates).
Second, the number of possible sets of basic propositions that our system needs to
organize is significantly lower than the number of possible text plans that the SPoT
planner needs to consider. Finally, although it is not practical in SPoT to list all possible
sentence plans that might be generated for a particular text plan (since the possibilities
are too great), generating all possible combinations of propositions in a proposition class
(such as message related class) is practical in our work. This is due to the fact that the
number of basic propositions in each class is fairly small (e.g., usually between 5 to
15 propositions) and that the nature of the operators and constraints that we put on
their application enable us to prune the space of possible combinations. Some of these
constraints are: (1) The And Operator produces only one complex tree from a pair of
trees and it cannot combine two trees if both trees have a proposition with an And
predicate at their roots (thus we limit the number of conjuncts in a conjoined sentence
to three at most16), and (2) the Attribute Operator produces only one complex tree in
cases where a single node tree (Tree A) can be attached as a direct subchild of more than
one node in another tree (Tree B); the parent of Tree A is the first such node found by
preorder traversal of Tree B.
Our implementation first generates all possible text plans for the propositions
within each class (message-related, specific, and computational). Each text plan is
represented as a forest where each tree in the forest represents a sentence. Initially,
each proposition class is treated as a forest consisting of all single node trees in that
class (initial candidate forest), and the operators are applied to that forest in order
to produce new candidate forests for the proposition class. Anytime two trees in a
16 We set this limit in order to avoid sentences that are too complex to comprehend.
548
Demir, Carberry, and McCoy Summarizing Information Graphics Textually
Figure 5
A candidate forest for each proposition class.
candidate forest are combined via an operator, a new candidate forest is produced;
the new candidate forest is added to the existing set of candidate forests, thereby
increasing the number of candidate forests. Within each class, our approach first applies
the And Operator to all possible pairs of trees in the initial candidate forest, which
produces new candidate forests. The Same Operator is then applied to all possible
pairs of trees in each candidate forest. Similarly, the Which Operator and the At-
tribute Operator are applied to trees in the candidate forests produced earlier. The
result of this aggregation is a number of candidate forests with one or more trees
(each using different aggregation) for each of the proposition classes. For example,
Figure 5 shows one candidate forest that can be constructed for each proposition class
by applying these operators to the propositions selected for the graphic in Figure 4,
where each forest resulting from the aggregation consists of a single tree.17 In this
example, the Attributive Knowledge Base predicates (*) are attached to their parents
by the Attribute Operator, the nodes containing And predicates (**) are produced
by the And Operator, and the Which predicates (***) are produced by the Which
Operator.
17 The nodes represented with black circles correspond to the individual predicates. These individual
predicates within each class form the single node trees upon which the operators work.
549
Computational Linguistics Volume 38, Number 3
5.3 Evaluating a Text Structure
Different combinations of operators produce different candidate forests in each propo-
sition class and consequently lead to different realized text with a different complexity
of sentences. The set of candidate forests for each proposition class must be evaluated
to determine which one is best. Our objective is to find a forest that would produce
text which stands at a midpoint between two extremes: a text where each proposition
is realized as a single sentence and a text where groups of propositions are realized
with sentences that are too complex. Our evaluation metric to identify the best forest
leverages different considerations to balance these extremes. The first two criteria are
concerned with the number and syntactic complexity of sentences that will be used to
realize a forest. The third criteria takes into consideration how hard it is to comprehend
the relative clauses embedded in these sentences. The insights that we use in selecting
the best forest (e.g., balancing semantic importance, overall text structure, aggregation,
and readibility due to sentence complexity) represent our novel contributions to the
text structuring and aggregation literature. The theory that underlies our evaluation
metric (i.e., what it is we are balancing in the generation) is widely applicable to other
data-to-text generation domains because it uses general principles from the literature
and has the potential to be improved.
5.3.1 Sentence Complexity. Each tree (single node or complex) in a forest represents a
set of propositions that can be realized as a single sentence. Our aggregation rules
enable us to combine these simple sentences into more complex syntactic structures.
In the literature, different measures to assess syntactic complexity of written text and
spoken language samples have been proposed, with different considerations such as
the right branching nature of English (Yngve 1960) and dependency distance between
lexemes (Lin 1996). We apply the revised D-level sentence complexity scale (Covington
et al 2006) as the basis of our syntactic complexity measure. The D-level scale measures
the complexity of a sentence according to its syntactic structure and the sequence in
which children acquire the ability to use different types of syntactic structures. The
sentence types with the lowest score are those that children acquire first and there-
fore are the simplest types. Eight levels are defined in the study, some of which are
simple sentences, coordinated structures (conjoined phrases or sentences), non-finite
clause in adjunct positions, and sentences with more than one level of relative clause
embedding.
Among the eight levels defined in that study, the levels of interest in our work
are simple sentences, conjoined sentences, sentences with a relative clause modifying
the object of the main verb, non-finite clauses in adjunct positions, and sentences
with more than one level of embedding. However, the definition of sentence types at
each level is too general. For example, the sentences There is a trend and There is a
trend in the dollar value of net profit over the period from the year 1998 to the year 2006
are both classified as simple sentences with the lowest complexity score under the
D-level classification. We argue that although these sentences have a lower complexity
than the sentences with higher D-level scores, their complexities are not the same. We
make a finer distinction between sentence types defined in the D-level classification
and use the complexity levels shown in Table 2. For example, according to our clas-
sification, a simple sentence with more than one adjunct or preposition has a higher
complexity than a simple sentence without an adjunct. We preserve the ordering of
the complexity levels in the D-level classification. For example, in our classification,
550
Demir, Carberry, and McCoy Summarizing Information Graphics Textually
Table 2
Our syntactic complexity levels.
Complexity Syntactic Form
Level 0 Simple sentence with up to one prepositional phrase or adjunct
Level 1 Simple sentence with more than one prepositional phrase or adjunct
Level 2 Conjoined sentence (two simple sentences?Level 0 or 1)
Level 3 Conjoined sentence (more than two simple sentences?Level 0 or 1)
Level 4 Sentence with one level of embedding
(relative clause that is modifying object of main verb)
Level 5 Non-finite clause in adjunct positions
Level 6 Sentence with more than one level of embedding
Levels 0 and 1 correspond to the class of simple sentences in the D-level classification
and have a lower complexity than Levels 2 and 3, which correspond to the class of
coordinated structures with a higher complexity than simple sentences in the D-level
classification.
Each basic proposition in our system can be realized as a simple sentence containing
at most one prepositional phrase or adjunct.18 Each single node tree with a Relative or
Attributive Knowledge Base predicate at its root has the lowest syntactic complexity
(Level 0) in this classification.
Themost straightforwardway of realizing amore complex tree would be conjoining
the realizations of subtrees rooted by a proposition with an And or a Same predicate,
embedding the realization of a subtree rooted by a proposition with a Which predicate
as a relative clause, and realizing a subtree that consists solely of a proposition with
an Attributive Knowledge Base predicate as an adjective or a prepositional phrase.
For example, the tree rooted by shows(graphic,trend) in Figure 5 can be realized as
The graphic shows a decreasing trend, which is in the dollar value of net profit and is over
the period, which starts at the year 1998 and ends at the year 2006. The resultant text
is fairly complicated, however, and a more sophisticated realization would likely
lead to a lower syntactic complexity score. We defined a number of And predicate
and Which predicate complexity estimators to look for realization opportunities in
a complex tree structure so that a syntactic complexity score which is lower than
what the most straightforward realization would produce can be assigned to that tree.
These estimators compute the syntactic complexity of a complex tree by examining
the associated realizations of all aggregated propositions in that tree in a bottom?up
fashion. Because the complex trees that are rooted by a proposition with a Same
predicate would always be realized as a conjoined sentence (Level 2), we did not define
complexity estimators for this kind of predicate.
The And predicate complexity estimators check whether or not the realizations
of two subtrees rooted by a proposition with an And predicate can be combined into
a simple sentence (Level 1), or a conjoined sentence which consists of two independent
sentences (Level 2) if one of the subtrees is rooted by a proposition with an And
predicate. For example, the And predicate estimators can successfully identify the
18 In the current implementation, there is a single realization associated with each basic proposition with
the main entity in subject position.
551
Computational Linguistics Volume 38, Number 3
following realization opportunities (based on the representations of the sentences as
propositions):
 The period starts at the year 1998. AND The period ends at the year 2006.
can be combined into:
The period is from the year 1998 to the year 2006. (Level 1)
 The trend is in the dollar value of net profit. AND The trend is over the period
from the year 1998 to the year 2006. can be combined into:
The trend is in the dollar value of net profit over the period from the year 1998
to the year 2006. (Level 1)
 The dollar value of net profit ranges from 2.77 billion dollars over the period.
AND The dollar value of net profit ranges to 0.96 billion dollars over the
period. AND The dollar value of net profit shows the largest drop of about
0.56 billion dollars between the year 2000 and the year 2001. can be
combined into:19
The dollar value of net profit ranges from 2.77 to 0.96 billion dollars over the
period and shows the largest drop of about 0.56 billion dollars between the
year 2000 and the year 2001. (Level 2)
The Which predicate complexity estimators check whether a tree rooted by a
proposition with a Which predicate can be realized as a simple adjunct or a prepo-
sitional phrase attached to the modified entity rather than a more complex relative
clause (which could increase the complexity level). For example, the Which predicate
estimators can successfully identify the following realization opportunities (based on
the representations of the sentences as propositions):
 The trend is over the period.WHICH The period is from the year 1998 to the
year 2006. can be realized as:
The trend is over the period from the year 1998 to the year 2006. (Level 1)
 The graphic shows a trend.WHICH The trend is in the dollar value of net profit
over the period from the year 1998 to the year 2006. can be realized as:
The graphic shows a trend in the dollar value of net profit over the period from
the year 1998 to the year 2006. (Level 1)
In our generation approach, multiple realizations for each proposition can be incor-
porated by defining new complexity estimators in addition to the estimators that are
used in the current implementation. Defining such estimators, which will not change
the task complexity or the underlying methodology, would add to the generalizability
of our approach.
19 In this case, the single node trees that correspond to the propositions conveying the range of the trend
form the immediate descendents of a tree rooted by a proposition with an And predicate, and that tree
with the And predicate at its root and the tree corresponding to the proposition conveying the largest
drop constitute the immediate descendents of a tree rooted by another proposition with an And
predicate.
552
Demir, Carberry, and McCoy Summarizing Information Graphics Textually
5.3.2 Relative Clause Embedding. In cases where a tree rooted by a proposition with a
Which predicate cannot be realized as a simple adjunct or a prepositional phrase, it will
be realized by a relative clause. In the D-level classification, the complexity of a sentence
with an embedded clause is determined according to the grammatical role (subject
or object) of the entity that is modified by that clause, not the syntactic complexity
or position (center-embedded or right-branching) of the clause in the sentence. For
instance, a sentence with a complex center-embedded relative clause modifying an
object receives the same syntactic complexity score as a sentence with a simple right-
branching relative clause modifying an object. As argued in the literature, however,
center-embedded relative clauses are more difficult to comprehend than corresponding
right-branching clauses (Johnson 1998; Kidd and Bavin 2002). To capture this, our
evaluation metric for identifying the best structure penalizes Which predicates that
will be realized as a relative clause based on the clause?s syntactic complexity and
position in the sentence (which we refer to as ?comprehension complexity of a relative
clause?). For example, the following sentences receive different scores by our evaluation
metric with respect to clause embedding; the first one with a right-branching clause
(simpler) has a lower score than the second sentence with a center-embedded clause
(more complex):
 The graphic shows a decreasing trend over the period from the year 1998
to the year 2006 in the dollar value of net profit, which is 2.7 billion dollars
in the year 1999.
 The graphic shows a decreasing trend in the dollar value of net profit, which is
2.7 billion dollars in the year 1999, over the period from the year 1998 to the
year 2006.
The embedded clause (Level 0) in the first of the following sentences has a lower
syntactic score than the clause (Level 2) embedded in the second sentence. Because
our evaluation metric takes into consideration both the syntactic complexity of an
embedded clause and its position in the sentence, the first sentence receives a lower
score than the second sentence.
 The graphic shows a decreasing trend in the dollar value of net profit, which is
2.7 billion dollars in the year 1999, over the period from the year 1998 to the
year 2006.
 The graphic shows a decreasing trend in the dollar value of net profit, which is
2.7 billion dollars in the year 1999 and is 2.58 billion dollars in the year 2000,
over the period from the year 1998 to the year 2006.
5.3.3 Evaluation Metric. Our evaluation metric takes three criteria into account: the
number of sentences that will be used to realize a forest, the syntactic complexities of
these sentences, and the comprehension complexities of the embedded relative clauses.
Our metric evaluates the overall score of a candidate forest by summing the normalized
scores that the forest receives with respect to each criteria. The score of a forest (e.g.,
Forest A) is calculated as follows:
score(A) = nm1(sentence(A))+ nm2(complexity(A))+ nm3(clause(A)) (1)
553
Computational Linguistics Volume 38, Number 3
where:20
sentence(A): stands for the number of sentences that will be used to realize forest A
and equals the number of trees in that forest.
complexity(A): stands for the overall syntactic complexity of forest A and equals the
sum of the complexities of sentences that will be used to realize that forest.
clause(A): stands for the overall comprehension complexity of all relative clauses
in sentences used for realizing forest A and equals the sum of the comprehension
complexities of all clauses. The comprehension complexity of a relative clause equals
the product of its syntactic complexity and its position in the sentence, which is equal
to 2 if it is a center-embedded clause and is equal to 1 if it is a right-branching clause.
Consider, for example, the forest shown in Figure 6. Because the forest contains a
single tree, it receives a score of 1 for the sentence criteria. The syntactic complexity score
of the sentence that will be used to realize that tree is computed in a bottom?up fashion
as follows. The lowest syntactic complexity score (Level 0) is assigned to all leaf nodes,
and all inner nodes that only have single node trees with anAttributive Knowledge Base
predicate as descendents (as shown in Figure 6). Each of the remaining inner nodes is
then assessed with a syntactic complexity score once the complexity scores for all of
its descendents are computed (i.e., once the best realization possibility with the lowest
syntactic complexity for each descendent tree is determined). If an inner node contains
a proposition with an And predicate, its syntactic complexity score is computed via
the And predicate complexity estimators. Similarly, the Which predicate complexity
estimators are used to compute the syntactic complexity scores for all inner nodes with
aWhich predicate. The syntactic complexity score for the parent node of a tree rooted by
a proposition with aWhich predicate is computed based on whether or not that tree will
be realized as a relative clause (as indicated by the complexity score of the root node of
that tree). The forest shown in Figure 6 receives a score of 4 for the complexity criteria,
which is equal to the syntactic complexity score assigned to the parent node of the tree.
In Figure 6, only the tree rooted by Node 4 will be realized as a relative clause. Because
that relative clause, which receives a syntactic complexity score of 2, will be realized
as a center-embedded clause, the forest shown in Figure 6 receives a score of 4 for the
clause criteria.
In the current implementation, once the scores with respect to a criteria are com-
puted for each candidate forest, these scores (e.g., sentence(A)) are normalized with
respect to the maximum score (e.g., max(sentence(all forests))) by dividing each score
by the maximum of the computed scores. For instance, nm1(sentence(A)) is the nor-
malized score that the forest A receives with respect to the sentence criteria and is
equal to sentence(A)/max(sentence(all forests)). Thus, the normalized score that a forest
receives for each criteria is always between 0 and 1 and therefore each criteria has
an equal impact on the overall score of a forest.21 The normalized scores obtained
for a candidate forest are then summed to obtain the overall score for that forest. For
example, assume that three candidate forests, the first of which is shown in Figure 6,
20 The terms nm1, nm2, and nm3 stand for the normalized score of a given criteria.
21 The simplifying assumption of assigning equal weights to each criteria would be better optimized with
machine learning, as discussed in detail in Section 9.
554
Demir, Carberry, and McCoy Summarizing Information Graphics Textually
Figure 6
A forest containing a single tree.
555
Computational Linguistics Volume 38, Number 3
Table 3
Overall evaluation scores.
Sentence Complexity Clause Overall Score
First Forest 1(0.5) 4(1) 4(1) 2.5
Second Forest 1(0.5) 4(1) 2(0.5) 2
Third Forest 2(1) 3(0.75) 0(0) 1.75
are constructed from a set of propositions. One possible way of realizing the forest in
Figure 6 would be The graphic shows a decreasing trend in the dollar value of net profit,
which shows the largest drop of about 0.56 billion dollars between the year 2000 and the
year 2001, and shows the smallest drop of nearly 0.07 billion dollars between the year 1998
and the year 1999, over the period from the year 1998 to the year 2006. Suppose that the
second forest is similar to Figure 6 except that the children (Node 2 and Node 3) of
the And(trend) node are swapped.22 Suppose also that the third forest is similar to
Figure 6 except that the tree is decomposed into two trees, which are rooted by Node 1
and Node 5, respectively. The first tree rooted by Node 1 consists of the nodes marked
with (*) and the second tree rooted by Node 5 consists of the nodes marked with (**).
Table 3 shows the actual and the normalized scores (shown in parentheses) for each
forest with respect to each criteria, and the overall score assigned by our evaluation
metric.
The number of sentences (1) and the overall sentence complexity (Level 4) are the
same for the first and second forests. The third forest has more sentences (2) but lower
overall sentence complexity (Level 3) than the other two forests. The first forest has a
center-embedded relative clause and receives a score of 4 for the clause criteria: the
product of the complexity of the relative clause (2) and its position (2). On the other
hand, the second forest has a right-branching relative clause and receives a score of 2
for the same criteria: the product of the complexity of the relative clause (2) and its
position (1). The third forest doesn?t have an embedded clause and receives a score of 0
for the clause criteria.
5.4 Identifying the Best Text Structure
Our approach selects the forest which receives the lowest evaluation score as the best
forest that can be obtained from a set of input propositions. For example, according
to the scores shown in Table 3, the third forest, which could be realized as The graphic
shows a decreasing trend in the dollar value of net profit over the period from the year 1998
to the year 2006. The dollar value of net profit shows the largest drop of about 0.56 billion
dollars between the year 2000 and the year 2001 and shows the smallest drop of nearly 0.07
billion dollars between the year 1998 and the year 1999., would be selected as the best
among the three forests. The initial overall text structure of a brief summary con-
sists of the best forests identified for the message related, specific, and computational
classes.
As a final step, we check whether we can improve the evaluation of the overall
structure of the summary by moving trees (i.e., trees rooted by a Level-0 node such as
22 This swapping would cause the relative clause rooted by Node 4 to be a right-branching clause.
556
Demir, Carberry, and McCoy Summarizing Information Graphics Textually
And(descriptor) in Figure 5, Specific) or subtrees (i.e., trees rooted by a Level-1 node
with an And or a Relative Knowledge Base predicate such as hasextreme (descrip-
tor,?largest?,change(3,4),period(3,4)) in Figure 5, Specific) between the best forests for
the three proposition classes. For example, the best forest for the specific class might
contain a tree that conveys information about an entity introduced by a proposition
in the message related class. Moving this tree to the message related class and using an
operator to combine it with the tree introducing the entity might improve the evaluation
of the overall structure of the summary. For example, for the graphic in Figure 4, this
movement would allow our system to evaluate a structure where the tree rooted by the
specific proposition And(descriptor) (shown in Figure 5, Specific) is attached as a de-
scendent of the tree rooted by the message related proposition exists(trend,descriptor)
(shown in Figure 5,Message Related) via aWhich Operator.We explore all such possible
movements between best forests for the proposition classes (if any) and determine
the best overall text structure of the summary. To be consistent with the motivation
behind the initial groupings of the propositions, we do not allow movements out
of the message related class or any movement that will empty the computational
class.
5.5 Evaluation of the Text Structuring and Aggregation Module
Our text structuring and aggregation approach consists of several different components,
all of which contribute to the quality of the generated text. Our study focused on
whether or not our decisions with respect to these components contributed to the
perceived quality of the resultant summary: the organization and ordering (O) of
the content (partial ordering of the propositions within classes and classification of
the propositions), the aggregation (A) of the information into more complex tree struc-
tures (candidate forests constructed via operators), and the metric (E) used to evaluate
candidate forests that represent different possible aggregations of the informational
content.
We conducted an experiment with 15 participants (university students and gradu-
ate students) who were presented with six different summaries of twelve graphics from
the test corpus (described in Section 3.3). The participants neither participated in earlier
studies (described in Sections 4.1 and 4.4) nor were involved in this work. All presented
summaries were automatically produced by our generation approach. The participants
were not told how the presented summaries were produced (i.e., human-generated
or computer-generated), however. We focused on graphics with an increasing or a
decreasing trend, since these message categories exhibit the greatest variety of possible
summaries. For each of the graphics, the participants were given a set of summaries in
random order and asked to rank them in terms of their quality in conveying the content.
The summaries varied according to the test parameters as follows:23
 S O+A+E+: A summary that uses the ordering rules, the aggregation
rules, and receives the lowest (best) overall score by the evaluation
metric. This is the summary selected as best by the TSAMModule.
23 Although eight different summaries are logically possible with three different variables, we limited the
number to four (the second and the third in which exactly one of the components was turned off and the
fourth where all components were turned off) in order to keep the experiment within a manageable size.
557
Computational Linguistics Volume 38, Number 3
Table 4
Ranking of summary types.
Summary Type Best 2nd 3rd 4th
S O+A+E+ 65.6% 26.6% 6.7% 1.1%
S O+A+E- 16.7% 32.2% 33.3% 17.8%
S O-A+E+ 16.7% 30% 40% 13.3%
S O-A-E- 1% 11.2% 20% 67.8%
 S O+A+E-: A summary that uses the ordering and aggregation rules,
but does not receive the lowest overall score by the evaluation metric.
This is the summary that received the second lowest score.
 S O-A+E+: A summary where the propositions are randomly ordered,
but aggregation takes place, and it receives the lowest (best) overall
score by the evaluation metric.
 S O-A-E-: A summary consisting of single sentences that are randomly
ordered.
Table 4 presents the results of the experiment. It is particularly noteworthy that the
summary selected as the best by the Text Structuring andAggregationModulewasmost
often (65.6% of the time) rated as the best summary and overwhelmingly (92.2% of the
time) rated as one of the top two summaries. The table shows that omitting the eval-
uation metric (S O+A+E-) or omitting ordering of propositions (S O-A+E+) results in
summaries that are substantially less preferred by the participants. Overall, the results
shown in Table 4 validate our ordering, aggregation, and evaluation methodology.
6. Sentence Ordering Module (SOM)
With the use of different kinds of operators and an evaluation metric, our system
determines the partial ordering and the structure of sentences that will be used to realize
the selected content but doesn?t impose ordering constraints (final ordering) on the sen-
tences within each proposition class. To decide in which sequence the sentences should
be conveyed, we take advantage of the fact that each proposition has a defined main
entity, which will be realized in the subject position of the sentence that will be used to
realize the proposition. Identifying the subject of the realized sentences in advance al-
lows us to use centering theory (Grosz, Weinstein, and Joshi 1995) to generate a text that
is most coherent according to this theory.24 The theory outlines the principles of local
text coherence in terms of the way the discourse entities are introduced and discussed,
and the transitions between successive utterances in terms of the entities in the hearer?s
center of attention. Although some fundamental concepts of the theory, such as the
ranking of entities in an utterance, aren?t explicitly specified, various researchers have
applied centering theory to language generation (Kibble and Power 2004; Karamanis
et al 2009) with different interpretations. In our work, each sentence is regarded as an
24 If this assumption is relaxed, then centering theory would not be appropriate for an ordering component.
In that case, a focusing theory such as McCoy and Cheng (1991), or Suri and McCoy (1994) could be used
to order the sentences to be realized.
558
Demir, Carberry, and McCoy Summarizing Information Graphics Textually
utterance. Following Brennan, Friedman, and Pollard (1987) and Grosz, Weinstein, and
Joshi (1995), we rank the entities with respect to their grammatical functions where the
entity in subject position is the most salient entity. When ordering sentences, we take
into account the preference order for centering transitions: continue is preferred over
retain, which is preferred over smooth shift, which is in turn preferred over rough shift.
For all message categories, the number of sentences in a proposition class would
be limited (less than five) even if all of the highly rated propositions identified for
that message category are selected for inclusion. Thus, a straightforward ?generate and
test? strategy is appropriate for ordering sentences in our case. For each proposition
class, all possible orderings of the sentences within that class are generated. We assign
different numeric scores to each centering transition, where continue receives a score
of 3, and retain and smooth shift receive scores of 2 and 1, respectively. The rough shift
transitions are assessed a score of 0. For each candidate ordering, we sum the scores
for the kinds of transitions observed between consecutive sentences. The ordering that
receives the highest score is selected as the best ordering for that proposition class. First,
the best ordering of the sentences in the message related proposition class is selected.
The subject of the last sentence in that ordering is used as the backward-looking center
of the previous utterance when determining the best ordering of the sentences in the
specific proposition class. Similarly, the subject of the last sentence in the best ordering
for the specific class is used as the backward-looking center when identifying the best
ordering for the computational proposition class.
For graphics that depict a time period, we also utilize the time periods mentioned in
each conjunct of a conjoined sentence in order to specify in which order these conjuncts
will be conveyed in the realized text. If the time periods mentioned in each conjunct of
a conjoined sentence are different, these conjuncts are ordered such that the time period
in focus in the first conjunct subsumes or precedes the time period in focus in the second
conjunct. Consider how individual sentences in the following compound sentences are
ordered by our approach:
 The dollar value of net profit ranges from 2.77 to 0.96 billion dollars over the
period from the year 1998 to the year 2006 and shows the largest drop of about
0.56 billion dollars between the year 2000 and the year 2001.
 The dollar value of net profit shows the smallest drop of nearly 0.07 billion dollars
between the year 1998 and the year 1999 and shows the largest drop of about
0.56 billion dollars between the year 2000 and the year 2001.
Note that in the first conjoined sentence, the time period mentioned in the first con-
junct (from 1998 to 2006) subsumes the time period mentioned in the second conjunct
(between 2000 and 2001). On the other hand, in the second conjoined sentence, the time
period mentioned in the first conjunct (between 1998 to 1999) precedes the time period
mentioned in the second conjunct (between 2000 and 2001).
7. Sentence Generation Module (SGM)
To realize the summaries in natural language, we use the FUF/SURGE surface real-
izer (Elhadad and Robin 1999), which offers the richest knowledge of English syntax and
widest coverage among the publicly available realizers such as REALPRO (Lavoie and
Rambow 1997). The realization of the sentence-sized units requires referring expressions
559
Computational Linguistics Volume 38, Number 3
for certain graphical elements, however. Our system handles three different issues with
respect to referring expression generation:
 Generating a referring expression for the dependent axis. Information
graphics often do not label the dependent axis with a full descriptor of
what is being measured (which we call themeasurement axis descriptor).
In such a situation, a referring expression for this element must be
extracted from the text of the graphic. For example, to realize its summary,
a measurement axis descriptor (e.g., the dollar value of Chicago Federal Home
Loan Bank?s mortgage program assets) must be generated for the graphic in
Figure 7a, whose dependent axis is not explicitly labeled with the
descriptor.
 Generating a referring expression in order to refer to the bars on the
independent axis (e.g., the countries for the graphic in Figure 1). Such an
expression must be inferred from the bar labels or extracted from the text
of the graphic. This referring expression is often used in the summaries of
graphics in some message categories (e.g., Maximum Bar) that require
comparing a bar with others (e.g., distinguishing the bar with the
maximum value from all other bars).
 It was shown that people prefer less informative descriptions for
subsequent mentions of an entity (Krahmer and Theune 2002). In order to
generate more natural summaries, the syntactic forms of the subsequent
mentions of discourse entities should be constructed in a way that helps
text coherence.
7.1 Measurement Axis Descriptor
Generation of referring expressions (noun phrases) is one of the key problems explored
within the natural language generation literature. There is a growing body of research
in this area that, given a knowledge base of entities and their properties, deals with
Figure 7
(a) Graphic from Business Week. (b) Graphic from Business Week.
560
Demir, Carberry, and McCoy Summarizing Information Graphics Textually
determining the set of properties that would single out the target entity (Dale and Reiter
1995; Krahmer, Van Erk, and Verleg 2003). More recently, the generation of referring
expressions has been proposed as a postprocessing technique to deal with the lack of
text coherence in extractive multidocument summarization (Belz et al 2008). Nenkova
and McKeown (2003) developed a method to improve the coherence of a multidoc-
ument summary of newswire texts by regenerating referring expressions for the first
and subsequent mentions of people?s names where the expressions are extracted from
the text of the input documents according to a set of rewrite rules. The task that we
face is similar to this recent body of research in that contextually appropriate referring
expressions for certain graphical elements should be extracted from the text of the
graphic. At the same time, our task is more complex in some respects. First, it is often the
case that the required referring expression isn?t explicitly given as a single unit and thus
must be constructed by extracting and combining pieces of information from the text of
the graphic. Second, in some cases where the dependent axis is explicitly labelled with a
descriptor, it still needs to be augmented. We undertook a corpus study in order to iden-
tify how a measurement axis descriptor could be generated from the text of a graphic;
the results of the analysis form the basis for the heuristics and augmentation rules
we developed for generating the measurement axis descriptor for a graphic. In Demir,
Carberry, and Elzer (2009), we outlined this problem as generating a graphical element
required for realizing the intended message of a graphic and thoroughly described the
technical details of our approach. Here, however, we treat this particular aspect as a
novel text-to-text generation methodology which is combined with other data-to-text
approaches in a complete NLG system. Thus, our focus in this section is to highlight
a new way of using the text associated with images which has been earlier exploited
by various NLP tasks such as indexing and retrieval of images (Pastra, Saggion, and
Wilks 2003).
7.1.1 Corpus Analysis. Graphic designers generally use text within and around a graphic
to present information related to the graphic. We started our analysis by examining how
texts are distributed around each group of graphics. We observed that graphics (indi-
vidual or composite) contain a set of component texts that are visually distinguished
from one another by blank lines, by different fonts/styles, or by different directions
and positions in the graphic. Although the number of component texts present in a
graphic may vary, our analysis recognized an alignment or leveling of text contained in
a graphic, which we refer to as ?text levels.?
We observed seven text levels which we refer to as Overall Caption, Overall
Description, Caption, Description, Dependent Axis Label, Text In Graphic, and Text
Under Graphic. Not every level appears in every graphic. Overall Caption and Over-
all Description apply to composite graphics that contain more than one individual
graphic (the graphics might be of different kinds) and refer to the entire collection
of graphics in the composite. In composite graphics, Overall Caption is the text that
appears at the top of the overall group and serves as a caption for the whole set (such
as Tallying Up the Hits in Figure 8a). In composite graphics, there is often another
text component placed under the Overall Caption but distinguished from it by a line
break or a change in font. This text component, if present, is also pertinent to all
individual graphics in the composite graphic and elaborates on them. We refer to such
text as the Overall Description (such as Yahoo once relied entirely on banner ads. Now it?s
broadened its business mix in Figure 8a). Caption and Description serve the same roles
for an individual graphic. For example, the Caption for the bar chart in Figure 8a is
Active Users and the Description is Registered users in millions. The Caption of Figure 8b
561
Computational Linguistics Volume 38, Number 3
Figure 8
(a) A composite graph from Newsweek.25 (b) Graphic from Business Week.
Table 5
Text levels in bar charts.
Text level Frequency of occurrence
Overall Caption 31.8% (?34/107)
Overall Description 17.8% (?19/107)
Caption 99.0% (?106/107)
Description 54.2% (?58/107)
Text In Graphic 39.3% (?42/107)
Dependent Axis Label 18.7% (?20/107)
Text Under Graphic 7.5% (?8/107)
is A Growing Biotech Market but this graphic does not have a Description. There may be
a label on the dependent axis itself and we refer to it as Dependent Axis Label (such
as Revenues (in billions) in Figure 8b). In addition to the text levels described so far
which appear outside the borders of a graphic, we have observed that there is often
a text component residing within the borders of a graphic which we refer to as Text
In Graphic (such as U.S. Biotech Revenues, 1992?2001 in Figure 8b). Finally, Text Under
Graphic is the text under a graphic which usually starts with a marker symbol (such
as *) and is essentially a footnote (such as U.S. only, one available seat flown one mile, year
ending June 2002 in Figure 7b). Each Text Under Graphic has a referrer elsewhere that
ends with the same marker and that referrer could be at any text level of the graphic. A
graphic might have more than one Text Under Graphic but each is differentiated with
a different marker. For each of the 107 graphics in our corpus (described in Section 3.3),
the Visual Extraction System extracts these text levels from the graphical image of the
bar chart and inserts them into the graph?s XML representation. Table 5 lists the various
text levels, along with how often they appeared in the graphics in our corpus.
25 This figure displays two of the five individual graphs constituting the composite graphic that appeared in
Newsweek.
562
Demir, Carberry, and McCoy Summarizing Information Graphics Textually
Two annotators first analyzed each graphic in our corpus and determined a mea-
surement axis descriptor for the graphic; the annotators used both the information
residing within the text components of the graphic and the article, and commonsense
knowledge. All ideal descriptors were noun phrases or wh-phrases26 (such asWhat?s the
most important issue affecting voters? vote? on a graphic depicting survey results). After the
descriptors were identified, we analyzed the graphics to see how the descriptors could
be generated from the text components of the graphic. We observed that an acceptable
measurement axis descriptor often cannot be extracted as a whole from a single text
level and instead must be put together by extracting pieces from one or several different
text levels; the pieces, though coming from a single level, might not be contiguous in that
level and still need to be melded into a coherent whole. In some cases, the information
is also retrieved from other graphics in the same composite or from the article?s text.
Our analysis has also led us to hypothesize that the ideal measurement axis descriptor
can be viewed as consisting of a core?a basic noun or wh-phrase from one text level
that is often augmented with text from another level (or in some cases, from text in the
accompanying article or other graphs in the same composite) to be more descriptive
and complete. For example, for the bar chart in Figure 8a, registered users is the core
of the ideal measurement axis descriptor which is Yahoo?s registered users. The core is
found in the Description and the augmentation to the core is found in the Overall
Description. When more than one text level is used, the text levels that contain pieces of
themeasurement axis descriptor also vary among the graphics.We observed thatmostly
text levels particular to a graphic (such as Text In Graphic and Description) contain the
pieces of the descriptor as opposed to the levels containing shared information (such
as Overall Description), and with the exception of Text Under Graphic, the ordering of
text levels in Table 5 forms a hierarchy of textual components, with Overall Caption
and Dependent Axis Label respectively at the top and bottom of the hierarchy, such that
the core generally appears in the lowest text level present in the graphic. During the
corpus analysis, we observed three ways in which a core extracted from one text level
was augmented with text from another text level:
 Expansion of the noun phrase: The nouns in the core of the descriptor
were replaced with a noun phrase at another text level which has the same
noun as its head. The replaced noun phrase appeared in a text level higher
in the precedence order than the text level at which the core appears.
Consider, for example, Figure 8b. The core of the descriptor is Revenues
(appearing in the Dependent Axis Label), which is reasonable enough
to be the core, but the noun Revenues should be replaced with U.S. Biotech
Revenues in order to be complete.
 Specialization of the noun phrase: The core was augmented with a
proper noun which specialized the descriptor to a specific entity.
Consider, for example, Figure 8a, which shows a composite graph where
individual graphics present different attributes of the same particular
entity (Yahoo). The ideal measurement axis descriptor of the bar chart
(Yahoo?s registered users) consists of the core registered users (appearing in
the Description) augmented with the proper noun Yahoo that appears in
the Overall Description.
26 Generally seen in graphics presenting the results of a survey.
563
Computational Linguistics Volume 38, Number 3
 Addition of detail: Text Under Graphic typically serves as a footnote to
give specialized detail about the graphic which is not as important as the
information given in other text levels. If the Text Under Graphic began
with a footnote marker, such as an asterisk, and the core was followed by
the same marker, then Text Under Graphic added detail to the core.
Consider, for example, Figure 7b, where unit costs is the core but the ideal
measurement axis descriptor (Unit costs, U.S. only, one available seat flown
one mile, year ending June 2002) also contains the information from the
Text Under Graphic.
7.1.2Methodology. First, preprocessing deletes the scale and unit indicators (phrases used
to give the unit and/or a scale of the values presented in the graphic), and the ontolog-
ical category of the bar labels (if explicitly marked by the preposition by) from the text
levels. Next, heuristics are used to identify the core of the measurement axis descriptor
by extracting a noun phrase or a wh-phrase from a text level of the graphic. Three kinds
of augmentation rules, corresponding to the three kinds of augmentation observed in
our corpus, are then applied to the core to produce the measurement axis descriptor.
If none of the augmentation rules are applicable, then the core of the descriptor forms
the measurement axis descriptor. Finally, if the measurement axis descriptor does not
already contain the unit of measurement (such as percent), the phrase indicating the unit
of measurement is appended to the front of the measurement axis descriptor.
For identifying the core of the measurement axis descriptor, we developed nine
heuristics that are dependent on the parses of the text levels. Two of these heuristics are
restricted to Dependent Axis Label and Text In Graphic, and the remaining heuristics
are applicable to all other text levels. The application of the heuristics gives preference
to text levels that are lower in the hierarchy and if a core is not identified at one text
level, the applicable heuristics are applied, in order, to the next higher text level in
the hierarchy. For example, suppose that the graphic contains only a Description and
a Caption and thus the first two heuristics are not applicable. The next seven heuristics
are first applied to the Description and then to the Caption. The following presents three
representative heuristics where the first two heuristics are applicable only to Dependent
Axis Label and Text In Graphic:27
 Heuristic-1: If the Dependent Axis Label consists of a single noun phrase
that is not a scale or unit indicator, that noun phrase is the core of the
measurement axis descriptor.
 Heuristic-2: If Text In Graphic consists solely of a noun phrase, then that
noun phrase is the core; otherwise, if Text In Graphic is a sentence, the
noun phrase that is the subject of the sentence is the core.
 Heuristic-6: If a fragment at the text level consists solely of a noun phrase,
and the noun phrase is not a proper noun, that noun phrase is the core.
Once the core is identified, augmentation rules are applied to fill out the descriptor.
For example, consider the graphic in Figure 8b where Heuristic-1 identifies Revenues
in Dependent Axis Label as the core. Because the core and the Text In Graphic, U.S.
Biotech Revenues, have the same head noun, the augmentation rule for expansion
27 All of the heuristics and augmentation rules can be found in Demir, Carberry, and Elzer (2009).
564
Demir, Carberry, and McCoy Summarizing Information Graphics Textually
produces U.S. Biotech Revenues as the augmented core. After adding a phrase for
the unit of measurement, the referring expression for the dependent axis becomes
The dollar value of U.S. Biotech Revenues. As another example, consider the graphic in
Figure 7b. Our work uses Heuristic-2 and the augmentation rule for adding detail.
After adding a phrase representing the unit of measurement, the referring expression
for the dependent axis becomes The cent value of unit costs (U.S. only, one way available seat
flown one mile, year ending June 2002). Finally, consider the graphic in Figure 8a, where
Heuristic-6 identifies the noun phrase registered users as the core.28 The augmentation
rule for specialization finds that Yahoo is the only proper noun in the text levels and
does not match a bar label, and forms Yahoo?s registered users. After adding a phrase
representing the unit of measurement, the referring expression for the dependent axis
becomes The number of Yahoo?s registered users.
In order to evaluate our approach, we constructed a distinct test corpus consisting
of 205 randomly selected bar charts from 21 different newspapers and magazines;
only six of these sources were also used to gather the corpus described in Section 3.3.
For each graphic, we used our approach to generate the referring expression for the
dependent axis. Finally, the resultant output and three baselines were evaluated by
two evaluators (Demir, Carberry, and Elzer 2009). The evaluation results showed that
our approach performs much better than any of the baselines for the 205 graphics in
the corpus. The detailed analysis of the results also showed that our methodology is
applicable to a wider range of sources in popular media.
7.2 Generating an Expression for Referring to All Bars
For some message categories (for example, Maximum Bar), the identification of the
ontological category for the bar labels results in better natural language than merely
using a generic expression; for example, compare the phrase among the countries listed
with the phrase among the entities listed in producing natural language text for the
message conveyed by the graphic in Figure 1. There are a number of different pub-
licly available ontologies such as WordNet Fellbaum (1998) and OpenCyc (2011). In
our work, we need a knowledge base that offers both the semantic relations between
words and general commonsense knowledge. For example, WordNet could not iden-
tify Jacques Chirac, a former president of France, whereas OpenCyc ontology contains
this information. Therefore, we use OpenCyc ontology version v0.7.8b to identify the
ontological categories of bar labels in our work. Our implemented system currently
finds the most specific category that is a common category for at least two of the bar
labels and identifies it as the ontological category.
Grice?s Maxim of Quantity (1975) states that one?s discourse contribution should be
as informative as necessary for the purposes of the exchange but not more so. If our
system were to enumerate all entities involved in a comparison message, the realization
of the inferred message might be lengthy and the enumeration of little utility to the
user. Thus we set a cut-off C, such that if the number of entities involved in a Maximum
Bar or Rank Bar message exceeds C, they are not enumerated but rather we use the
generated referring expression for all bars. The cut-off value is currently set to 5 in our
implemented system.
28 The preprocessing of this text level would remove In millions because it is a scale indicator.
565
Computational Linguistics Volume 38, Number 3
7.3 Subsequent Mentions of Discourse Entities
In the current implementation of the system, the syntactic form of a subsequent mention
of an entity is determined based on its salience status in the context. In particular, the
backward-looking center of an utterance29 is replaced with a less informative definite
noun phrase after a continue or a retain transition because the backward-looking cen-
ter remains the same in the latter utterance. In such cases, the definite noun phrase
is constructed by adding the demonstrative this or these to the front of the head
noun of the backward-looking center, such as these revenues for the phrase U.S. biotech
revenues.
7.4 Example Summaries
For the graphic in Figure 1, our system generates the following textual summary: The
graphic shows that United States at 32,434 has the highest number of hacker attacks among the
countries Brazil, Britain, Germany, Italy, and United States. United States has 5.93 times more
attacks than the average of the other countries.
For the graphic in Figure 3a, the following textual summary is generated: The graphic
shows an increasing trend in the dollar value of Lands? End annual revenue over the period from
the year 1992 to the year 2001. The dollar value of Lands? End annual revenue shows an increase
of nearly 225 percent. Except for a small drop in the year 1999, slight increases are observed
almost every year.
For the graphic in Figure 3b, our system generates the following summary: The
graphic compares the defense agencies army, navy, air force, and other defense agencies, which
are sorted in descending order with respect to the number of civilian employees. The number
of civilian employees is highest for army at 233,030 and is lowest for other defense agencies at
100,678.30
For the graphic in Figure 4, the following textual summary is generated: The graphic
shows a decreasing trend in the dollar value of net profit over the period from the year 1998 to the
year 2006. The dollar value of net profit ranges from 2.77 to 0.96 billion dollars over the period
from the year 1998 to the year 2006 and shows the largest drop of about 0.56 billion dollars
between the year 2000 and the year 2001. Slight decreases are observed almost every year.
For the graphic in Figure 8b, our system generates the following summary: The
graphic shows an increasing trend in the dollar value of U.S. Biotech Revenues over the period
from the year 1992 to the year 2001. Increasing slightly every year, the dollar value of U.S.
Biotech Revenues shows an increase of nearly 265 percent and ranges from 7.87 to 28.52 billion
dollars.
For the graphic in Figure 7a, the following textual summary is generated: In the year
2003, the graphic shows a much higher rise in the dollar value of Chicago Federal Home Loan
Bank?s mortgage program assets in contrast with the moderate increases over the period from the
year 1998 to the year 2002. The dollar value of Chicago Federal Home Loan Bank?s mortgage
program assets reaches to 94.23 billion dollars in the year 2003. The dollar value of these assets
in the year 2003 is nearly 49.1 times higher than that in the year 1998.
For the graphic in Figure 9, the following textual summary is generated: This graphic
is about American Express. The graphic shows that American Express at 255 billion dollars is
29 The backward-looking center of a current utterance is the most highly ranked entity of the previous
utterance that is realized in the current utterance.
30 The reason for saying that the defense agencies are sorted in decreasing order is not to enable the reader
to visualize the graphic but rather that it subsumes giving the ranking of each bar.
566
Demir, Carberry, and McCoy Summarizing Information Graphics Textually
Figure 9
Graphic conveying the rank of a bar.
the third highest with respect to the dollar value of total credit-card purchases per year among
the entities Visa, Mastercard, Discover, Diner?s Club, and American Express.
8. Evaluation of the Effectiveness of the Textual Summaries
The earlier user studies (Sections 4.4 and 5.5) demonstrated the effectiveness of our gen-
eration methodology in identifying and presenting the high-level content of bar charts.
The success of a generation system depends not only on the quality of the produced text,
however, but also on whether the text achieves the impact that it is intended to make on
readers (such as enabling readers to perform a task or changing their opinions in some
context). We conducted an evaluation study to measure how adequate and effective
the summaries generated by our system are for our purpose of providing the message
and high-level knowledge that one would gain from viewing a graphic. Specifically, we
were interested in (1) what amount of information is retained by a reader from reading
the summary generated by our system, (2) whether someone reading the summary
garners the most important knowledge conveyed by the graphic, and (3) whether the
knowledge gleaned from the summary is consistent with the actual graphic.
In this study, we used four graphics from the test corpus (described in Section 3.3)
with different intended messages. These graphics conveyed an increasing trend (i.e.,
Figure 3a), a decreasing trend, the rank of the maximum bar (i.e., Figure 1), and the
rank of a bar (i.e., Figure 9) among all bars. In the first part of the study, each of the
18 participants (graduate students) was first presented with the summaries generated
by our system for these graphics; the participants neither saw the original graphics
(the graphical images of the bar charts) nor were aware of our system and how the
summaries were generated. For each summary, the participants were asked to draw
the bar chart being described in that summary. Although enabling a reader to redraw
the graphic is not a goal of our work, comparing a reader?s mental representation
of the graphic with the actual graphic allows us to identify whether there are any
inconsistencies between knowledge gleaned from the summary and the content of the
actual graphic.
In the second part of the study, we asked three evaluators not involved in this re-
search to evaluate the drawings that we collected from the participants. The evaluators
were Ph.D. students from the University of Delaware (none of them were the authors
of this work) and had an overall knowledge about our summarization approach (i.e.,
what is intended to be conveyed in the summaries of graphics). The evaluators were
567
Computational Linguistics Volume 38, Number 3
first told that a set of participants were presented with brief summaries of bar charts
and asked to draw the corresponding bar charts based on the information presented
in those summaries. They were also told that each summary only conveyed what is
identified by our system as the most important information that should be conveyed
about the bar chart being described. The evaluators were presented with the graphical
images of the four bar charts used in the study (none saw the summaries presented in
the first part of the study) and the drawings collected from the participants, and then
asked to rate each drawing using the following evaluation scores:
 5: The drawing is essentially the same as the original graphic
 4: The drawing captures all important information from the original
graphic but requires some minor modifications
 3: The drawing captures most of the important information from the
original graphic but is missing one significant piece of information
 2: The drawing reflects some information from the original graphic but
requires major modifications
 1: The drawing fails to reflect the original graphic
The average score that a drawing received from the evaluators ranged from 3 to 5.
The evaluators viewed the drawings drawn for the graphics with a trend more favor-
ably and assigned a score of 4 or more in most cases. For each graphic, we computed the
average score given by the evaluators to all drawings constructed from the summary
of that graphic (i.e., the average of the three scores given to each of the 18 drawings
drawn from the same summary). The graphics conveying an increasing (Figure 3a) and
a decreasing trend received a score of 4.22 and 4.63, respectively. The evaluators gave an
average score of 3.53 and 4.07 to the graphics which conveyed the rank of the maximum
bar (Figure 1) and the rank of a bar among all bars (Figure 9). Because we do not present
all features of a bar chart in its summary (such as all bar values), obtaining an average
score of less than 5 for all bar charts is not surprising.
We also asked the evaluators to specify a reason (i.e., what is missing or should be
changed) for the cases where they assigned a score of less than 4. Once we analyzed
their feedback for the drawings with a trend, we observed that missing values on
the dependent axis (i.e., tick mark labels) and missing measurement axis descriptors
(although given in the summaries) were the main reasons. We argue that presenting
tick mark labels is more appropriate for summaries that describe scientific graphics
(such as the summaries generated by the iGRAPH-Lite system [Ferres et al 2007]) in
contrast to the summaries that we generate for conveying the high-level content of
a graphic. The evaluators indicated incorrect rankings of some bars as the reason for
giving lower scores to the drawings that present the rank of the maximum bar or the
rank of a bar among all bars; this is due to the fact that our summaries did not convey
the rankings and the values of all bars in those cases. Because the intention of the
corresponding graphics is to convey the rank of a single bar (not all bars), we argue that
our summaries facilitate the readers to get the main purpose of these graphics. Overall,
this study demonstrated that our summarization approach is effective in conveying
the high-level content of bar charts so as to enable readers to correctly understand the
main point of the graphic. We are planning to conduct future studies to explore the
effectiveness of our approach further, however. One possible evaluation could be asking
568
Demir, Carberry, and McCoy Summarizing Information Graphics Textually
a different set of participants to draw the bar charts that we used in this study by reading
their summaries produced by an appropriate baseline approach, and comparing the
scores that those new set of drawings received from the same three evaluators with our
current results. This evaluation will also allow us to determine whether the evaluators
judged our summarization approach favorably because they were aware of the overall
approach (i.e., brief summaries are generated by our system and these summaries do
not contain everything that can be conveyed by the corresponding bar charts).
Favorable results were also achieved when people with visual impairments were
presented with the brief summaries generated by our work in an interactive system
which also enabled the user to ask follow-up questions to learn more about about the
graphic. More on that system and study can be found in Demir et al (2010).
9. Conclusion and Future Work
The majority of information graphics from popular media are intended to convey a
message that is often not captured by the text of the document. Thus graphics, along
with the textual segments, contribute to the overall purpose of a multimodal document
and cannot be ignored. The work presented in this article is the first to apply natural lan-
guage generation technology to provide themessage and high-level knowledge that one
would gain by viewing graphics from popular media via brief textual summaries. Our
summarization approach treats a graphic as a form of language and utilizes the inferred
intention of the graphic designer, the communicative signals present in the graphic, and
the significant visual features of the underlying data in determining what to convey
about that graph. Our approach uses a set of content identification rules constructed for
each intended message category of a bar chart in determining the content of the sum-
maries. The propositions selected for inclusion by these rules are organized into a text
structure by applying a novel bottom?up approach which leverages different discourse
related considerations such as the number and syntactic complexity of sentences and
clause embeddings that will be used for realization. Following the generation of refer-
ring expressions for certain graphical elements, the structure of a summary is realized
in natural language via a publicly available realizer. Three different evaluation studies
validated the effectiveness of our approach in selecting and coherently organizing the
most important information that should be conveyed about a bar chart, and enabling
readers to correctly understand the high-level knowledge of the graphic.
In addition to the application area, this article makes contributions to two broad
areas of research: data-to-text generation systems and text-to-text generation of referring
expressions. Here we have viewed the generation of a summary of an information
graphic as a data-to-text generation problem. Any data-to-text generation system must
solve several important problems: (1) out of all of the information in the data, extract
out that information that is important enough to be included in the text, (2) structure
the information so it can be realized as a coherent text, (3) aggregate propositions to
be conveyed in the text into more complex yet understandable sentence structures, (4)
order the resulting sentence structures so as to maintain text fluency, and (5) realize the
information as English sentences and generate appropriate referring expressions. Our
work has addressed each of these issues in a systematic fashion maintaining modularity
of system components and following a development methodology that includes human
input for making system decisions, and a thorough evaluation of each module as well
as final system evaluation.
Although the specific implementations that we developed are geared toward gen-
erating summaries of bar charts, the groundwork described in this article is currently
569
Computational Linguistics Volume 38, Number 3
being used by our own group to investigate summarizing other types of graphics (such
as line graphs and grouped bar charts) from popular media. A Bayesian system for
recognizing the intended message of line graphs has already been developed (Wu et al
2010) and the work on constructing the content identification rules for line graphs is
under way (Greenbacker, Carberry, and McCoy 2011).
The module that generates referring expressions represents a sophisticated study of
text-to-text referring expression generation. Referring expression generation is a vibrant
field. Although the particular rules used for extracting an appropriate referring expres-
sion are unique to referring expressions for graphical elements inside an information
graphic (note: not just bar charts), the work has uncovered some rather interesting
properties in terms of extracting expressions from text which may generalize to other
domains as well. Our work is unique in that a full referring expression is pieced together
from text not directly referring to the item in question. In order to do this, we identified
text levels and rules for extracting the core expression along with potential important
modifiers. One can imagine this message carrying over to other types of data-to-text
activities as well as to more standard text-to-text generation problems.
In future work, we intend to build on the work reported here in several ways.
Corpus studies where human subjects tasked with writing brief summaries of graph-
ics (the kind of summaries that our system intends to generate) would be of great
potential in informing generation decisions that our system makes at different levels.
Moreover, experts with extensive knowledge of the domain or the targeted end users
were shown to be of greater supply to the development of manyNLG systems. Learning
from summaries written by subjects (especially expert writers) would be an exciting
area of future research. We also believe that our approach would benefit from these
corpus studies towards exploring how the fluency of the summaries can be further
improved particularly by reducing the occasional verbosity in order to achieve tex-
tual economy (Stone and Webber 1998). For example, these efforts might help us to
determine how measurement axis descriptors can be more appropriately phrased in
different situations. Improving the current evaluation metric for choosing a text plan is
also in our research agenda. We utilize three criteria in our evaluation metric for deter-
mining the best structure that can be obtained by applying the operators to the selected
propositions. In addition, each criteria (the number of sentences, the overall syntactic
complexity of sentences, and the overall comprehension complexity of all embedded
clauses) has the same impact on the selection process. We are considering conducting
more user studies in order to identify what other criteria should be taken into account
and how important each criteria should be in relation to all the others. Moreover,
exploring the broader applicability of the novel aspects of our work in other settings
is an interesting topic for future work. Finally, evaluating the utility of our theoretically
informed aggregation mechanism in comparison to or in conjunction with the more
surface-oriented mechanism of the SPoT system would be a promising area for further
research.
To our best knowledge, what kinds of summaries best serve the needs of visually
impaired individuals has not been throughly studied before. As mentioned in Sec-
tion 2.2.1, we believe that our summaries, once associated with graphics as ALT texts,
might be of help to these individuals while reading electronic documents via screen
readers. One fruitful research direction would be to present such individuals with our
summaries in real-time scenarios and to mine their informational and presentational
needs. Such a study would probably provide insights with regard to this question
and potentially lead to guidelines that human summarizers could follow in generating
summaries for people with visual impairments.
570
Demir, Carberry, and McCoy Summarizing Information Graphics Textually
Acknowledgments
The authors would like to thank the study
volunteers for their time and willingness
to participate. This material is based upon
work supported by the National Institute on
Disability and Rehabilitation Research under
grant no. H133G080047. For all graphics that
were used in our evaluations and are given
as examples in this article, inputs (i.e., the
inferred intended message and the XML
representation of the graphic) and outputs
(i.e., the textual summary of the graphic) of
our system can be found at the following
Web page: www.cis.udel.edu/?carberry/
barcharts-corpus/.
References
Alty, James L. and Dimitrios I. Rigas. 1998.
Communicating graphical information
to blind users using music: the role of
context. In Proceedings of the SIGCHI
Conference on Human Factors in Computing
Systems, pages 574?581, Los Angeles, CA.
Baldwin, Breck and Thomas Morton.
1998. Dynamic coreference-based
summarization. In Proceedings of the
3rd Conference on Empirical Methods in
Natural Language Processing, pages 1?6,
Granada.
Barzilay, Regina, Noemie Elhadad, and
Kathleen McKeown. 2002. Inferring
strategies for sentence ordering in
multidocument news summarization.
Journal of Artificial Intelligence Research,
17:35?55.
Barzilay, Regina and Mirella Lapata.
2006. Aggregation via set partitioning
for natural language generation.
In Proceedings of the Human Language
Technology Conference of the North
American Chapter of the Association of
Computational Linguistics, pages 359?366,
New York, NY.
Belz, Anja, Eric Kow, Jette Viethen, and
Albert Gatt. 2008. The Grec challenge
2008: Overview and evaluation results.
In Proceedings of the 5th International
Natural Language Generation Conference,
pages 183?191, Salt Fork, OH.
Brennan, Susan E., Marilyn W. Friedman,
and Carl J. Pollard. 1987. A centering
approach to pronouns. In Proceedings
of the Annual Meeting of the Association of
Computational Linguistics, pages 155?162,
Stanford, CA.
Carberry, Sandra, Stephanie Elzer, and Seniz
Demir. 2006. Information graphics: An
untapped resource for digital libraries.
In Proceedings of the ACM Special Interest
Group on Information Retrieval Conference,
pages 581?588, Seattle, WA.
Chester, Daniel and Stephanie Elzer. 2005.
Getting computers to see information
graphics so users do not have to. In
Proceedings of the 15th International
Symposium on Methodologies for Intelligent
Systems, pages 660?668, Saratoga
Springs, NY.
Clark, Herbert. 1996. Using Language.
Cambridge University Press, Cambridge.
Coch, Jose. 1998. Interactive generation and
knowledge administration in multimeteo.
In Proceedings of 9th International Workshop
on Natural Language Generation,
pages 300?303, Niagara-on-the-Lake.
Corio, Marc and Guy Lapalme. 1999.
Generation of texts for information
graphics. In Proceedings of the 7th European
Workshop on Natural Language Generation,
pages 49?58, Toulouse.
Covington, M., C. He, C. Brown, L. Naci,
and J. Brown. 2006. How complex is that
sentence? A proposed revision of the
rosenberg and abbeduto D-level scale.
Research Report, Artificial Intelligence
Center, University of Georgia.
Cycorp. Open Cyc. 2011.
http://www.cyc.com.
Dale, Robert and Ehud Reiter. 1995.
Computational interpretations of the
gricean maxims in the generation of
referring expressions. Cognitive Science,
19(2):233?263.
Dalianis, Hercules. 1999. Aggregation
in natural language generation.
Computational Intelligence, 15(4):384?414.
Demir, Seniz, Sandra Carberry, and
Stephanie Elzer. 2009. Issues in Realizing
the Overall Message of a Bar Chart,
John Benjamins, 5th edition.
Amsterdam, pages 311?320.
Demir, Seniz, David Oliver, Edward
Schwartz, Stephanie Elzer, Sandra
Carberry, Kathleen F. McCoy, and Daniel
Chester. 2010. Interactive sight: Textual
access to simple bar charts. The New
Review of Hypermedia and Multimedia,
16(3):245?279.
Di Eugenio, Barbara, Davide Fossati,
Dan Yu, Susan Haller, and Michael Glass.
2005. Aggregation improves learning:
Experiments in natural language
generation for intelligent tutoring
systems. In Proceedings of the 43rd
Annual Meeting of the Association for
Computational Linguistics, pages 50?57,
Ann Arbor, MI.
571
Computational Linguistics Volume 38, Number 3
Elhadad, M. and J. Robin. 1999. SURGE:
A comprehensive plug-in syntactic
realization component for text generation.
Technical Report, Department of
Computer Science, Ben Gurion
University. Beersheba, Israel.
Elzer, Stephanie, Sandra Carberry, and
Ingrid Zukerman. 2011. The automated
understanding of simple bar charts.
Artificial Intelligence, 175(2):526?555.
Elzer, Stephanie, Nancy Green, Sandra
Carberry, and James Hoffman. 2006. A
model of perceptual task effort for bar
charts and its role in recognizing intention.
International Journal on User Modeling and
User-Adapted Interaction, 16(1):1?30.
Fasciano, Massimo and Guy Lapalme. 2000.
Intentions in the coordinated generation
of graphics and text from tabular data.
Knowledge and Information Systems,
2(3):310?339.
Fellbaum, Christiane. 1998.WordNet:
An Electronic Lexical Database.
The MIT Press, Cambridge, MA.
Ferres, Leo, Petro Verkhogliad, Gitte
Lindgaard, Louis Boucher, Antoine
Chretien, and Martin Lachance. 2007.
Improving accessibility to statistical
graphs: the igraph-lite system.
In Proceedings of the 9th International
ACM SIGACCESS Conference on
Computers and Accessibility, pages 67?74,
Tempe, AZ.
Foster, Mary Ellen. 1999. Automatically
generating text to accompany
information graphics. Master?s Thesis,
University of Toronto.
Friendly, Michael. 2008. A brief history of
data visualization. In C. Chen, W. Ha?rdle,
and A. Unwin, editors, Handbook of
Computational Statistics: Data Visualization,
volume III. Springer-Verlag, Heidelberg,
pages 1?34.
Gatt, Albert, Francois Portet, Ehud Reiter,
Jim Hunter, Saad Mahamood, Wendy
Moncur, and Somayajulu Sripada. 2009.
From data to text in the neonatal intensive
care unit: Using NLG technology for
decision support and information
management. AI Communications,
22(3):153?186.
Goldberg, Eli, Norbert Driedger, and
Richard I. Kittredge. 1994. Using
natural-language processing to produce
weather forecasts. IEEE Expert: Intelligent
Systems and Their Applications, 9(2):45?53.
Goldstein, Jade, Vibhu Mittal, Jaime
Carbonell, and Mark Kantrowitz. 2000.
Multi-document summarization by
sentence extraction. In Proceedings
of the NAACL-ANLP Workshop on
Automatic Summarization, pages 40?48,
Seattle, WA.
Greenbacker, Charlie, Sandra Carberry, and
Kathleen F. McCoy. 2011. A corpus of
human-written summaries of line graphs.
In Proceedings of the EMNLP 2011 Workshop
on Language Generation and Evaluation
(UCNLG+Eval), pages 23?27, Edinburgh.
Grice, H. Paul. 1975. Logic and conversation.
Speech Acts, 3:41?58.
Grosz, Barbara and Candace Sidner. 1986.
Attention, intentions, and the structure
of discourse. Computational Linguistics,
12(3):175?204.
Grosz, Barbara J., Scott Weinstein, and
Aravind K. Joshi. 1995. Centering:
A framework for modeling the local
coherence of discourse. Computational
Linguistics, 21(2):203?225.
Hovy, Eduard H. 1988. Planning coherent
multisentential text. In Proceedings of
the 26th Annual Meeting of the Association
for Computational Linguistics,
pages 163?169, Buffalo, NY.
Hovy, Eduard H. 1993. Automated discourse
generation using discourse structure
relations. Artificial Intelligence,
63(1-2):341?385.
Hovy, Eduard and Chin-Yew Lin. 1996.
Automated text summarization and the
summarist system. In Proceedings of the
Workshop on TIPSTER Text Program,
pages 197?214, Vienna, VA.
Ina, Satoshi. 1996. Computer graphics for
the blind. SIGCAPH Computers and the
Physically Handicapped, 55:16?23.
Jayant, Chandrika, Matt Renzelmann,
Dana Wen, Satria Krisnandi, Richard
Ladner, and Dan Comden. 2007.
Automated tactile graphics translation: in
the field. In Proceedings of the 9th
International ACM SIGACCESS Conference
on Computers and Accessibility, pages 75?82,
Tempe, AZ.
Johnson, Mark. 1998. Proof nets and the
complexity of processing center embedded
constructions. Journal of Logic, Language
and Information, 7(4):433?447.
Joshi, Aravind, Bonnie Webber, and
Ralph Weischedel. 1984. Living up to
expectations: Computing expert responses.
In Proceedings of the National Conference on
Artificial Intelligence, pages 169?175,
Austin, TX.
Karamanis, Nikiforos, Chris Mellish,
Massimo Poesio, and Jon Oberlander.
2009. Evaluating centering for information
572
Demir, Carberry, and McCoy Summarizing Information Graphics Textually
ordering using corpora. Computational
Linguistics, 35(1):29?46.
Kennel, A. 1996. Audiograf: A
diagram-reader for the blind. In
Proceedings of the 2nd Annual ACM
Conference on Assistive Technologies,
pages 51?56, Vancover, BC, Canada.
Kerpedjiev, Stephan and Steven Roth.
2000. Mapping communicative goals
into conceptual tasks to generate
graphics in discourse. In Proceedings
of the International Conference on
Intelligent User Interfaces, pages 60?67,
New Orleans, LA.
Kibble, Rodger and Richard Power. 2004.
Optimizing referential coherence in text
generation. Computational Linguistics,
30(4):401?416.
Kidd, Evan and Edith Bavin. 2002.
English-speaking children?s
comprehension of relative clauses:
Evidence for general-cognitive and
language-specific constraints on
development. Journal of Psycholinguistic
Research, 31(6):599?617.
Krahmer, E. and M. Theune. 2002. Efficient
context-sensitive generation of referring
expressions. In K. van Deemter and
R. Kibble, editors, Information Sharing:
Reference and Presupposition in Language
Generation and Interpretation, Center for the
Study of Language and Information-Lecture
Notes, volume 143 of CSLI Lecture Notes.
CSLI Publications, Stanford, CA,
pages 233?264.
Krahmer, Emiel, Sebastiaan Van Erk,
and Andre? Verleg. 2003. Graph-based
generation of referring expressions.
Computational Linguistics, 29(1):53?72.
Kukich, Karen. 1983. Design of a
knowledge-based report generator.
In Proceedings of the 21st Annual Meeting of
the Association for Computational
Linguistics, pages 145?150,
Cambridge, MA.
Kurze, Martin. 1995. Giving blind people
access to graphics (example: business
graphics). In Proceedings of the
Software-Ergonomie Workshop, Dannstadt,
Bremen.
Lavoie, Benoit and Owen Rambow. 1997.
A fast and portable realizer for text
generation systems. In Proceedings of
the 5th Conference on Applied Natural
Language Processing, pages 265?268,
Washington, DC.
Lazar, J., A. Allen, J. Kleinman, and
C. Malarkey. 2007. What frustrates screen
reader users on the web: A study of
100 blind users. International Journal of
Human-Computer Interaction, 22(3):247?269.
Lester, James C. and Bruce W. Porter. 1997.
Developing and empirically evaluating
robust explanation generators: The
KNIGHT experiments. Computational
Linguistics, 23(1):65?101.
Lin, Dekang. 1996. On the structural
complexity of natural language
sentences. In Proceedings of the International
Conference on Computational Linguistics,
pages 729?733, Copenhagen, Denmark.
Mann, William C. and Sandra A. Thompson.
1987. Rhetorical structure theory: A theory
of text organization. In Livia Polanyi,
editor, The Structure of Discourse. Ablex
Publishing Corporation, Norwood, NJ.
Marcu, Daniel. 1998. The Rhetorical Parsing,
Summarization, and Generation of Natural
Language Texts. Ph.D. thesis, Department of
Computer Science, University of Toronto.
McCoy, Kathleen F., Sandra Carberry, Tom
Roper, and Nancy Green. 2001. Towards
generating textual summaries of graphs.
In Proceedings of the 1st International
Conference on Universal Access in Human-
Computer Interaction, pages 695?699,
New Orleans, LA.
McCoy, Kathleen F. and Jeannette Cheng.
1991. Focus of attention: Constraining
what can be said next. In Cecile Paris,
William Swartout, and William Mann,
editors, Natural Language Generation in
Artificial Intelligence and Computational
Linguistics. Kluwer Academic Publishers,
Berlin, pages 103?124.
McKeown, Kathleen R. 1985. Discourse
strategies for generating natural-language
text. Artificial Intelligence, 27(1):1?41.
McKeown, Kathleen R., Shimei Pan, James
Shaw, Desmond A. Jordan, and Barry A.
Allen. 1997. Language generation for
multimedia healthcare briefings. In
Proceedings of the 5th Conference on Applied
Natural Language Processing, pages
277?282, Washington, DC.
Meijer, Peter B. 1992. An experimental
system for auditory image representations.
IEEE Transactions on Biomedical Engineering,
39(2):112?121.
Mellish, Chris, Alisdair Knott, Jon
Oberlander, and Mick O?Donnell. 1998.
Experiments using stochastic search
for text planning. In Proceedings of the
9th International Workshop on Natural
Language Generation, pages 98?107,
Niagara-on-the-Lake.
Moore, Johanna D. and Cecile Paris.
1993. Planning text for advisory
573
Computational Linguistics Volume 38, Number 3
dialogues: Capturing intentional and
rhetorical information. Computational
Linguistics, 19(4):651?694.
Nenkova, Ani and Kathleen McKeown. 2003.
References to named entities: a corpus
study. In Proceedings of the Conference of the
North American Chapter of the Association for
Computational Linguistics on Human
Language Technology, pages 70?72,
Edmonton.
O?Donnell, M., C. Mellish, J. Oberlander, and
A. Knott. 2001. Ilex: an architecture for a
dynamic hypertext generation system.
Natural Language Engineering, 7(3):225?250.
Paris, Cecile. 1988. Tailoring object
descriptions to a user?s level of expertise.
Computational Linguistics, 14(3):64?78.
Pastra, Katerina, Horacio Saggion, and
Yorick Wilks. 2003. Extracting relational
facts for indexing and retrieval of
crime-scene photographs. Knowledge-Based
Systems, 16(5-6):313?320.
Portet, Francois, Ehud Reiter, Albert Gatt,
Jim Hunter, Somayajulu Sripada, Yvonne
Freer, and Cindy Sykes. 2009. Automatic
generation of textual summaries from
neonatal intensive care data. Artificial
Intelligence, 173(7-8):789?816.
Radev, Dragomir R., Hongyan Jing,
Malgorzata Stys, and Daniel Tam. 2004.
Centroid-based summarization of multiple
documents. Information Processing and
Management: An International Journal,
40(6):919?938.
Ramloll, Rameshsharma, Wai Yu, Stephen
Brewster, Beate Riedel, Mike Burton, and
Gisela Dimigen. 2000. Constructing
sonified haptic line graphs for the blind
student: First steps. In Proceedings of the
4th International ACM Conference on
Assistive Technologies, pages 17?25,
Arlington, VA.
Reiter, Ehud. 2007. An architecture for
data-to-text systems. In Proceedings of
the 11th European Workshop on Natural
Language Generation, pages 97?104,
Schloss Dagstuhl.
Reiter, Ehud and Robert Dale. 2000. Building
Natural-language Generation Systems.
Cambridge University Press, Cambridge.
Schiffman, Barry, Ani Nenkova, and
Kathleen McKeown. 2002. Experiments
in multidocument summarization.
In Proceedings of the 2nd International
Conference on Human Language Technology
Research, pages 52?58, San Diego, CA.
Shaw, James. 1998. Clause aggregation using
linguistics knowledge. In Proceedings of
the 9th International Workshop on Natural
Language Generation, pages 138?147,
Niagara-on-the-Lake.
Somayajulu, Sripada, Ehud Reiter, and Ian
Davy. 2003. Sumtime-mousam:
Configurable marine weather forecast
generator. Expert Update, 6(3):4?10.
Stent, A., Rashmi Prasad, and Marilyn
Walker. 2004. Trainable sentence
planning for complex information
presentation in spoken dialog systems.
In Proceedings of the 42nd Annual Meeting of
the Association for Computational Linguistics,
pages 79?86, Barcelona.
Stone, Matthew and Bonnie Webber. 1998.
Textual economy through closely coupled
syntax and semantics. In Proceedings
of the International Natural Language
Generation Conference, pages 178?187,
Niagara-on-the-Lake.
Suri, Linda Z. and Kathleen F. McCoy. 1994.
Raft/rapr and centering: A comparison
and discussion of problems related to
processing complex sentences.
Computational Linguistics, 20(2):301?317.
Turner, Ross, Yaji Sripada, and Ehud Reiter.
2009. Generating approximate geographic
descriptions. In Proceedings of the 12th
European Workshop on Natural Language
Generation, pages 42?49, Athens.
Walker, M., O. Rambow, and M. Rogati. 2002.
Training a sentence planner for spoken
dialogue using boosting. Computer Speech
and Language: Special Issue on Spoken
Language Generation, 16(3):409?434.
Walker, M., A. Stent, F. Mairesse, and
R. Prasad. 2007. Individual and domain
adaptation in sentence planning for
dialogue. Journal of Artificial Intelligence
Research, 30(1):413?456.
Wu, Peng, Sandra Carberry, Stephanie Elzer,
and Daniel Chester. 2010. Recognizing
the intended message of line graphs.
In Proceedings of the International Conference
on the Theory and Application of Diagrams,
pages 220?234, Portland, OR.
Yngve, Victor H. 1960. A model and an
hypothesis for language structure.
American Philosophical Society, 104:444?466.
Yu, Jin, Ehud Reiter, Jim Hunter, and
Chris Mellish. 2007. Choosing the
content of textual summaries of large
time-series data sets. Natural Engineering,
13(1):25?49.
574
Generating Textual Summaries of Bar Charts
Seniz Demir Sandra Carberry Kathleen F. McCoy
Department of Computer Science
University of Delaware
Newark, DE 19716
{demir, carberry, mccoy}@cis.udel.edu
Abstract
Information graphics, such as bar charts and
line graphs, play an important role in mul-
timodal documents. This paper presents a
novel approach to producing a brief textual
summary of a simple bar chart. It outlines
our approach to augmenting the core message
of the graphic to produce a brief summary.
Our method simultaneously constructs both
the discourse and sentence structures of the
textual summary using a bottom-up approach.
The result is then realized in natural language.
An evaluation study validates our generation
methodology.
1 Introduction
Information graphics, such as bar charts and line
graphs, are an important component of a multi-
modal document. However, summarization has fo-
cused primarily on the text of a document. But as
shown in (Carberry et al, 2006), information graph-
ics in magazine and newspaper articles often convey
a message that is not repeated in the article?s text.
Thus information graphics cannot be ignored.
Individuals with sight impairments can access the
text of an electronic document via text to speech
systems. But such individuals are stymied when
they encounter information graphics. The SIGHT
system (Elzer et al, 2007) has the goal of provid-
ing the user with the message and knowledge that
one would gain from viewing the graphic, rather
than providing alternative access to what the graphic
looks like. In the current Bayesian network imple-
mentation, the system uses the communicative sig-
nals present in the graphic to recognize one of the
twelve message categories that can be conveyed by
a bar chart and produces a logical representation
of what we will refer to as the core message con-
veyed by the graphic; this representation is trans-
lated into natural language via templates. For exam-
ple, the system determines that the core message of
the graphic in Figure 1 is that the bar for the United
States has the maximum value of the entities listed,
and the system produces Maximum(First Bar) as the
logical representation of that message. However,
this is insufficient as a summary of the graphic since
it doesn?t convey the particularly significant features
of the graphic such as the fact that the number of
hacker attacks in the United States is far greater than
in the other countries listed.
In this paper, we explore the generation of an
effective initial summary of a bar chart within the
SIGHT system. Input to our system is a logical rep-
resentation of the graphic?s core message (as pro-
duced by SIGHT) and the XML representation of
the graphic which specifies the components of the
graphic such as the number of bars and the heights
of each bar. Our goal is to generate a succinct coher-
ent summary of a graphic that captures its core mes-
sage and the most important and significant features.
At the same time, the summary need not include all
information that could be extracted from the graphic
since future work on SIGHT includes a mechanism
for responding to follow-up questions from the user.
Our work is unique in that it generates a sum-
mary of the content of a bar chart with no domain
restriction and constructs a high-quality but brief
summary by incorporating the graphic?s core mes-
7
C o u n t r i e s  W i t h  t h e  M o s t  H a c k e r  
A t t a c k s ,  2 0 0 2
W o r l d w i d e ,  t h e  a t t a c k s  j u m p e d
U n i t e d  S t a t e s 2 4 , 4 3 4
B r a z i l
B r i t a i n
G e r m a n y
I ta ly
0 5 , 0 0 0 1 0 , 0 0 0 1 5 , 0 0 0 2 0 , 0 0 0
Figure 1: Graphic conveying a maximum bar.
sage and the most important and significant features
of the graphic. Our system produces a coherent or-
ganization of the content of the summary that we
hypothesize lends itself to follow-up questions, ap-
plies the notion of syntactic complexity in choosing
how to aggregate information into sentence-sized
pieces, utilizes a sentence realizer to convey the out-
put in natural language, and applies heuristics for
constructing referents for graphical elements. Thus,
our work addresses several generation problems.
Section 2 discusses related work in the area
of graph summarization. Section 3 presents our
methodology for identifying the content of initial
summaries. Section 4 describes our approach for or-
ganizing the summaries. Section 5 presents some
issues that we addressed in realizing the summaries
and a few example summaries generated by our sys-
tem. Section 6 discusses the results of an evaluation
study that validates our methodology.
2 Related Work
Graph summarization has received some attention.
The SumTime project uses pattern recognition tech-
niques to generate textual summaries of automati-
cally generated time-series data; different systems
have been designed in three domains, such as
SumTime-Turbine (Yu et al, 2007) for data from
gas turbine engines. However, each of these sys-
tems is domain-dependent. We view this as a very
different problem from the one that we address in
this paper, since we are working on domain indepen-
dent graphical representations. The iGRAPH-Lite
system (Ferres et al, 2007), whose main objective
is to make the information in a graphic accessible
to blind users via keyboard commands, uses tem-
plates to provide a short textual summary of what
the graphic looks like, but their summary is not con-
cerned with the higher level knowledge conveyed by
the graphic. The goal of Futrelle?s project (Futrelle,
1999) is to produce a summary graphic that captures
the content of one or more graphics. However, the
end result is itself a graphic, not text.
3 Informational Content of the Summaries
In (McCoy et al, 2001), we reported an informal
experiment in which human subjects were asked to
write a brief summary of a series of line graphs with
the same high-level intention. This experiment led
to three observations:
? the intended message of the graphic was con-
tained in all summaries.
? summaries of the same graphic by different
subjects were similar.
? summaries of different graphics in the same
message category (such as Rising Trend) var-
ied in the information provided.
Subjects included more than the core message of
the graphic in their summaries. The extra infor-
mation could be explained as capturing features of
the graphic that were visually salient. It was hy-
pothesized that what is taken as visually salient in a
graphic relates to the overall message of the graphic.
For example, in the line graphs analyzed, if the over-
all message of the graphic is an increasing trend and
the variance in that trend is large, then the variance
is salient. The fact that the summaries only included
the core message and the visually salient features,
correlates with Grice?s Maxim of Quantity (Grice,
1975) which states that one?s discourse contribution
should be as informative as necessary for the pur-
poses of the current exchange but not more so.
To extend these observations to other kinds of in-
formation graphics, particulary to simple bar charts,
we needed to identify the kinds of features that were
salient with respect to the graphic?s overall message.
For this purpose, we conducted a set of formal ex-
periments with human subjects. Our goal was a set
of rules that took into account the message category
(such as Rising Trend or Rank of Entity) and the vi-
sual features of the graphic to specify what proposi-
tions should be included in the initial summary.
8
3.1 Experimental Setup
Based on our experience with summarizing graph-
ics, we first identified the set of all propositions
(PROP ALL) that reflect information that we envi-
sioned someone might ask about a simple bar chart.
Twenty graduate students from a variety of depart-
ments participated in the experiments. Each subject
was given an information graphic along with a sen-
tence conveying the intended message of the graphic
(as identified by the SIGHT system) and a subset of
PROP ALL, and was asked to classify these addi-
tional propositions into one of three classes accord-
ing to how important they felt it was to include that
proposition in the initial summary: essential, pos-
sible, and not important. Subjects were told to
assume that the graphic was part of an article that
the user is reading and that he would be able to ask
follow-up questions after receiving the summary.
Twenty-one graphics were used in the experi-
ments and each participant was given six graphics
to view. The graphics were either selected from arti-
cles in popular media or constructed from scratch to
present a number of different salient features within
the same graphic. Eight different message categories
were tested in the experiments1 and the graphics
from the same message category reflected different
salient features that had been observed in a corpus
of collected graphics.
3.2 Computationalizing Proposition Selection
To analyze the experiment?s results, we assigned a
numeric score to each category (essential=3, possi-
ble=1, not important=0) indicating the level of im-
portance assigned by the subjects. We computed
an importance level (IL) for each proposition that
might be included in a summary of a graphic, esti-
mating how important it is to be included in the ini-
tial summary. The IL of a proposition is computed
based on the average of the importance assigned by
each subject. We classify a proposition as a highly-
rated proposition in a graphic (and therefore worthy
of being included in the initial summary) if its im-
portance level is equal to or above the importance
level that would be obtained if at least half of the
1We did not test message categories that are the opposite of
categories used in the experiments, such as Minimum which is
the opposite of Maximum and thus can be modeled similarly.
subjects had classified the proposition as essential.
The particular assignments of values to categories
shown above was used in order to emphasize in-
stances in which subjects placed a proposition in the
essential category.
For each message category, we analyzed the sets
of highly-rated propositions identified for the graph-
ics associated with that message category and hand
constructed a set of content identification rules spec-
ifying whether a proposition should be included in
the initial summary. If a proposition was classified
as highly-rated for all graphics in a particular mes-
sage category, the content identification rule stated
that the proposition should be included in the ini-
tial summary for every graphic whose core message
fell into that message category. For the other highly-
rated propositions for a message category, we iden-
tified a feature that was visually salient only in the
graphics where the proposition was marked as es-
sential, and our content identification rule used the
presence of this feature in the graphic as a condi-
tion for the proposition to be included in the initial
summary. Thirty content identification rules were
defined in the system.
Besides the propositions capturing salient features
of the graphic, we observed that the subjects se-
lected propositions whose absence might lead the
user to draw false conclusions by default (for exam-
ple, the propositions indicating that the trend start-
ing from 1997 does not cover the full range of bar
labels in the graphic in Figure 2). We constructed
rules to add such content. This correlates with the
maxim in (Joshi et al, 1984) which states that a sys-
tem should not only produce correct information but
should also prevent the user from drawing false in-
ferences.
The following are glosses of two representative
content identification rules applicable to a graphic
whose core message is an increasing trend:
? If (message category equals ?increas-
ing trend?) then include(propositions con-
veying the rate of increase of the trend)
? If (message category equals ?increas-
ing trend?) and (coverage(graphic) not equal
coverage(trend)) then include(propositions
indicating that the trend does not cover the full
range of bar labels)
To see how our rules might affect the generated
9
summary, consider the graphic in Figure 2. The
SIGHT system recognizes the core message as an
increasing trend from 1997 to 2002. The content
identification rules defined for increasing trend se-
lect the following pieces of additional information
to include in the initial summary of the graphic:
? The overall rate of increase of the trend, which
is moderate
? The range of the bar values in the trend:
$480,000 to $1,230,000
? The fact that there is an unusually steep rise be-
tween 2000 and 2001
? The period that the graphic covers, which is
from 1996 to 2002
4 Organizing Coherent Summaries
At this point in the processing, the system has iden-
tified the informational content to be conveyed in the
initial summary, which consists of the core message
and the set of propositions identified by the content
identification rules. We need to represent and orga-
nize the information to be communicated, determine
how the content should be aggregated into sentence-
sized pieces, and make decisions about referring ex-
pressions (Reiter and Dale, 1997).
To represent and help organize the propo-
sitions that should be included in the ini-
tial summary, we use two kinds of predicates.
Relative predicates, such as differs, are used to
express relations between the graphical elements.
For example, differs(bar(A),maximum bar,50%)
shows that the percentage difference between the
values of bar(A) and the maximum bar is 50%.
Attributive predicates, such as has attribute, are
used to elaborate the graphical elements. For ex-
ample, has attribute(trend,?type?,?increasing?)
shows that the trend observed in the graphic is an
increasing trend. We refer to the first argument of
a predicate as its main entity and the others as sec-
ondary entities.
Since a top-down planning approach does not
guarantee that all propositions will be covered by
the final text plan (Marcu, 1997), we use a bottom-
up planner for structuring the initial summaries. The
propositions selected for inclusion in the initial sum-
maries can be classified as message related, spe-
cific, or computational based on the type of in-
1 9 9 6 2 0 0 12 0 0 01 9 9 81 9 9 7 1 9 9 9 2 0 0 2
4 0 0
0
8 0 0
$ 1 , 2 0 0 , 0 0 0
R i s i n g  J u r y  A w a r d s
M e a n
4 8 0 , 0 0 0
1 , 2 3 0 , 0 0 0
Figure 2: Graphic with an increasing trend.
formation they convey. The core message of the
graphic is captured by the message related proposi-
tions. Specific propositions focus on specific pieces
of information in the graphic such as the propo-
sitions conveying the unusually large rise between
2000 and 2001 in the graphic in Figure 2. On the
other hand, computational propositions require com-
putations or abstractions over the whole graphic,
such as the propositions conveying the rate of in-
crease in the graphic in Figure 2. Once the propo-
sitions to be included in the summary are identified,
we assign them to one of these three classes.
We hypothesize that the message-related class
of propositions should be presented first since this
places emphasis on the core message of the graphic.
We anticipate that the user will ask follow-up ques-
tions after receiving the initial summary. Therefore,
it is appropriate to close the initial summary with
propositions from the computational class so that
the whole graphic is in the user?s focus of atten-
tion (Grosz and Sidner, 1986). Thus we hypothe-
size that a good ordering of propositions in the ini-
tial summary is the message-related class, the spe-
cific class, and finally the computational class. This
produces a partial ordering of the propositions to be
included in the summary.
Each proposition can be realized as a single sen-
tence. For example, shows(graphic,trend) can be
realized as ?The graphic shows a trend? or ?There
is a trend in the graphic?. Consequently, a set of
propositions can be viewed as a set of single sen-
tences. Figure 3 shows the propositions in the mes-
sage related class for the graphic in Figure 2, along
with a possible realization for each.
Although each proposition could be conveyed as
a single sentence, the result is unnatural and not very
10
shows(graphic,trend)
exists(trend,descriptor for dependent axis)
has_attr(trend,type,increasing)
has(trend,period)
starts(period,at,1997)
ends(period,at,2002)
The graphic shows a trend
There is a trend in the mean dollar
value of jury awards
The trend is an increasing trend
There is a trend over the period
The period starts at 1997
The period ends at 2002
Figure 3: Propositions in Message related Class.
coherent. Thus, we define operators to relate propo-
sitions and explore aggregating them. The opera-
tors work on trees; initially, each proposition is the
root of a single node tree. Each tree represents one
or more propositions that can be realized as a sin-
gle sentence, and operators combine individual trees
in a class into more complex trees. The following
are two such operators which work on relative pred-
icates:
? And Operator: This operator combines two
trees if their root propositions share the same
main entity. An And predicate with the same
main entity forms the root of the new tree, and
the trees that are combined form the descen-
dents of this root.
? Which Operator: This operator attaches one
tree as a descendent of another tree, connected
by a Which predicate, if the main entity of the
proposition at the root of the first tree is a sec-
ondary entity in the proposition at the root of
the second tree. That particular entity forms the
main entity of the Which predicate.
One possible result of applying our operators to
the set of propositions (single node trees) in Figure 3
produces the single but more complex tree shown
in Figure 4. The And predicate (***) in Figure 4
is produced by the And Operator and the resultant
subtree is attached to the predicate shows by the
Which Operator.
4.1 Evaluating Structures
Different combinations of operators produce differ-
ent sets of trees, each of which represents a differ-
ent text structure and consequently leads to differ-
shows(graphic,trend)
exists(trend,descriptor for dependent axis)
has_attr(trend,type,increasing)
has(trend,period)
starts(period,at,1997) ends(period,at,2002)
Which(trend)
And(trend)
Which(period)
And(period)
*
**
***
Figure 4: Best Structure of Message related Class.
ent realized text. These structures must be evalu-
ated to determine which one is the best. We don?t
want a structure where each proposition is realized
as a single sentence nor a structure where groups of
propositions are realized with sentences that are too
complex. Our objective is to find a structure which
stands at a mediatory point between these extremes.
Each tree in a structure represents a set of propo-
sitions that can be realized as a single sentence. The
most straightforward way of realizing a tree would
be conjoining the realizations of subtrees rooted by
an And predicate, embedding the realization of a
subtree rooted by a Which predicate as a relative
clause, and realizing a subtree that consists solely
of an attributive predicate as an adjective or a prepo-
sitional phrase. However, care must be taken that the
sentence realization of a tree is not too complex.
Research has used a number of different measures
to assess syntactic complexity of written text and
spoken language samples (Roark et al, 2007). We
apply the notion of syntactic complexity to evalu-
ate the semantic units (predicates) that will be re-
alized. The revised D-level sentence complexity
scale (Covington et al, 2006) forms the core of our
syntactic complexity measure. The D-Level scale
measures the complexity of a sentence according to
the sequence in which children acquire the ability
to use different types of sentences. The sentence
types with the lowest score are those that children
acquire first and therefore are the simplest types.
Among the seven levels defined in the revised D-
Level scale, the levels of interest in our work are
(in order of increasing complexity): simple sen-
tences, conjoined sentences, sentences with a rela-
11
Message Related:
The graphic shows an increasing trend in the mean dol-
lar value of jury awards over the period from 1997 to
2002.
Specific:
The value of these awards ranges from 480,000 dol-
lars to 1,230,000 dollars. The graphic covers a period
from 1996 to 2002. Between 2000 and 2001, an unusu-
ally steep rise is observed in the value of these awards.
Computational:
Moderate increases are observed every year during the
period from 1997 to 2002 in the value of these awards.
Table 1: Realization of the Initial Overall Structure.
Message Related:
Although the graphic covers a period from 1996 to 2002,
the graphic shows an increasing trend in the mean dollar
value of jury awards over the period from 1997 to 2002.
Specific:
The value of these awards ranges from 480,000 dollars to
1,230,000 dollars.
Computational:
Except for a steep rise between 2000 and 2001, moderate
increases are observed every year in the value of these
awards.
Table 2: Realization of the Final Overall Structure.
tive clause, and sentences with more than one level
of embedding. However, the definition of sentence
types at each level is too general. Therefore, we
use a finer distinction between sentence types within
each D-level, such as a simple sentence with more
than one preposition has a higher complexity than a
simple sentence with a single preposition.
To measure the complexity of sentences that will
realize a structure, we define a number of complex-
ity estimators. A tree consisting of a single node is
identified as having the lowest syntactic complex-
ity. We use And predicate and Which predicate es-
timators to estimate the complexity of the sentences
used to realize more complex trees. To do this, esti-
mators look for realization opportunities that would
produce lower complexity values than what the most
straightforward realization would produce. For in-
stance, the And predicate estimators check whether
or not the realizations of two subtrees rooted by
an And predicate can be combined into a single
sentence which is not a compound sentence con-
sisting of two independent sentences. These es-
timators have similar considerations to the clause-
combining operations used by Walker et.al (2002)
in the SPoT sentence planner. For example, one of
the And predicate estimators can successfully deter-
mine that the realizations ?There is a trend in the
mean dollar value of jury awards? and ?There is a
trend over the period from 1997 to 2002?2 can be
combined into ?There is a trend in the mean dollar
value of jury awards over the period from 1997 to
2Assume that the subtree in Figure 4 rooted by
Which(period) can be realized as ?from 1997 to 2002?.
2002? for the tree in Figure 4. This results in a com-
plexity score which is lower than the score of a com-
pound sentence consisting of two conjoined inde-
pendent sentences. The Which predicate estimators
check whether a tree rooted by a Which predicate
can be realized as an adjunct to the modified entity
rather than as a more complex relative clause. For
example, one of the Which predicate estimators can
identify that * and the subtree rooted at ** in Fig-
ure 4 can be realized as ?The graphic shows a trend
in the mean dollar value of jury awards over the pe-
riod from 1997 to 2002?.
Center-embedded relative clauses are more dif-
ficult to comprehend than corresponding right-
branching clauses (Kidd and Bavin, 2002). Our
complexity metric for evaluating the complexity of a
tree penalizes Which predicates that will be realized
as a relative clause in the middle of a sentence more
than one that appears at the end. Once the complex-
ity metric has evaluated the complexity of each tree
in a candidate structure, the complexities of the trees
are summed to get the complexity of the structure.
Our metric for selecting the best structure balances
the number of sentences and their complexity.
The initial overall structure of the summary con-
sists of the best structures for the message related,
specific, and computational classes. As a final step,
we check whether we can improve the evaluation of
the overall structure of the summary by moving trees
or subtrees between the best structures for the three
classes. For example, the best structure for the spe-
cific class might contain a tree that conveys infor-
mation about an entity introduced by a proposition
12
in the message related class. Moving this tree to the
message-related class and using the Which opera-
tor to combine it with the tree introducing the entity
(thereby realizing it as a relative clause) might im-
prove the evaluation of the overall structure of the
summary. To be consistent with the motivation be-
hind the initial groupings of the propositions, we do
not allow movements from the message related class
or any movement that will empty the computational
class. Table 1 presents the summary that our sys-
tem generates for the graphic in Figure 2 before the
movements between classes, and Table 2 presents
the summary after the movements.
5 Realizing Summaries
To realize the summaries in natural language, we
use the FUF/SURGE surface realizer (Elhadad and
Robin, 1996) with some changes made to address a
few problems encountered with respect to the use of
conjunctions and subject-ellipsises. Different strate-
gies are defined in the system for aggregating the
realizations of trees that are linked with operators.
The strategy selected by the system is based on the
relation (such as concession) that holds between the
propositions at the root of the trees and the syntac-
tic forms of their realization opportunities. For ex-
ample, the system uses different strategies for ag-
gregating the trees rooted by the And predicates in
Figure 4, where the tree rooted by And(period) is re-
alized as a combination of prepositional phrases and
the tree rooted by And(trend) is realized as a full
sentence containing a set of prepositional phrases.
5.1 A Descriptor for the Dependent Axis
The dependent axis of an information graphic is of-
ten not labelled with a full descriptor of what is be-
ing measured and therefore a mechanism for extract-
ing an appropriate descriptor had to be developed.
We undertook a corpus analysis and implemented a
system to realize the descriptors (Demir et al, 2007).
Our corpus analysis found that the full descriptor of-
ten must be built from pieces of text extracted from
different places in the graphic. We identified seven
text levels (text components) which form a hierar-
chy (the top being the Overall Caption and the bot-
tom being the Dependent Axis Label), and we ob-
served that the texts at the lower levels are more
? 95 ?00?99?97?96 ?98 ?01 ?02
6
4
2
0
1 0  p e r c e n t
8
A n n u a l  p e r c e n t  c h a n g e  i n  
g l o b a l  o u t p u t
Figure 5: Graphic conveying a contrast change.
likely to contribute to the descriptor than the texts
at the higher levels. We developed a set of heuristics
and augmentation rules for constructing the descrip-
tor for the dependent axis and validated them on a
previously unseen corpus of graphics.
5.2 Referent Generation
The descriptor that our system constructs is always a
noun phrase, but it may be quite long. Our referring
expression generator uses the full descriptor when
the dependent axis is first referenced in the text, but
only the head noun for subsequent references. To
relate the head noun to the descriptor in the text, the
demonstratives ?this? or ?these? is added to the front
of the head noun unless it follows a comparison such
as ?more?. This simple mechanism appears to work
well for our initial summaries.
5.3 Example Summaries
For the graphic in Figure 1, the SIGHT system posits
that the graphic?s core message is that the bar for
the United States has the maximum value among the
bars listed. Our system adds additional propositions
and produces the following summary:
?The graphic shows that United States with 24,434
has the highest number of hacker attacks among
the countries3 Brazil, Britain, Germany, Italy, and
United States. United States has 4.9 times more at-
tacks than the average of other countries.?
For the graphic in Figure 5, the SIGHT system
posits that the core message is that the change at the
last bar is in contrast with the previous increasing
trend. Our system generates the following summary:
3Our system has a module which identifies the ontological
category of the bar labels (Demir et al, 2007).
13
?The graphic shows a small drop in 2002 in contrast
with the increasing trend in annual percent change
in global output over the period from 1995 to 2001.
Except for a small drop in 1996, varying increases
are observed every year during the period from 1995
to 2001 in this change, which shows an overall in-
crease of 485.4 percent and shows a decrease of 8.7
percent between 2001 and 2002.?
6 Evaluation
The work presented in this paper consists of sev-
eral different components, all of which we claim
contribute to the quality of the graphical summary.
Our evaluation focused on three of these and eval-
uated whether or not our decisions with respect to
these components contributed to the perceived qual-
ity of the summary: the organization and ordering of
the informational content (O), the aggregation of the
information into more complex structures (A), and
the metric used to evaluate the structures that repre-
sent different possible aggregations of the informa-
tional content (E). We conducted an experiment with
fifteen participants where they were presented with
four different summaries of twelve graphics from a
variety of domains. We focused on increasing and
decreasing trend graphics since they have the great-
est variety of possible summaries. For each of the
graphics, the participants were asked to rank ran-
domly ordered summaries in terms of their quality
in conveying the informational content. The sum-
maries varied according to the test parameters:
? S O+A+E+: A summary that uses the ordering
rules, the aggregation rules, and is rated highest
by the evaluation metric. This is the summary
produced by our system.
? S O+A+E-: A summary that uses the ordering
and aggregation rules, but was not rated highest
by the evaluation metric.
? S O-A+E+: A summary where the proposi-
tions are randomly ordered, but aggregation
takes place, and the aggregation is rated best
by the evaluation metric.
? S O-A-E-: A summary consisting of single
sentences that are randomly ordered.
The results of the experiment are presented in Ta-
ble 3. It is particularly noteworthy that the summary
generated by our system was most often (65.6% of
Summary type Best 2nd 3rd 4th
S O+A+E+ 65.6% 26.6% 6.7% 1.1%
S O+A+E- 16.7% 32.2% 33.3% 17.8%
S O-A+E+ 16.7% 30% 40% 13.3%
S O-A-E- 1% 11.2% 20% 67.8%
Table 3: Ranking of Summary Types.
the time) rated as the best summary and overwhelm-
ingly (92.2% of the time) rated as one of the top two
summaries. The table shows that omitting the eval-
uation metric (S O+A+E-) or omitting ordering of
propositions (S O-A+E+) results in summaries that
are substantially less preferred by the subjects. We
gained insights into improving the system?s perfor-
mance by looking at the comments made by the sub-
jects when our summary was not selected as the best.
For example, it appears that the subjects did not like
summaries where exceptions were fronted on the
core message with an ?although? clause rather than
following the core message (S O+A+E-). We hy-
pothesize that fronting the exception detracted from
the core message. This is easily remedied in our
evaluation metric. Overall, the results shown in Ta-
ble 3 support our methodology for generating sum-
maries. In the future, we will test the summaries
with blind individuals to determine their effective-
ness in providing alternative access to graphics.
7 Conclusion
This paper has presented our methodology for gen-
erating brief and coherent summaries of simple bar
charts. Our work is the first to address the prob-
lem of summarizing the content of bar charts. We
have presented our approach for identifying the ap-
propriate content of an initial summary, ordering and
aggregating the included propositions, and evaluat-
ing the resultant summary structures to select the
best one. Overall, the results of an evaluation study
validate our ordering, aggregation, and evaluation
methodology.
Acknowledgements
We would like to thank Dr. Stephanie Elzer for her
advice, help, and implementation of the SIGHT sys-
tem upon which this work is built and Dr. Charles
Callaway for his valuable help in addressing the
problems encountered with the realizer.
14
References
Sandra Carberry, Stephanie Elzer, and Seniz Demir.
2006. Information graphics: an untapped resource for
digital libraries. In Proc. of SIGIR?2006.
Michael A. Covington, Congzhou He, Cati Brown, Lo-
rina Naci, and John Brown 2006. How complex is that
sentence? A proposed revision of the Rosenberg and
Abbeduto D-Level scale. Research Report, AI Center,
University of Georgia.
Seniz Demir, Sandra Carberry, and Stephanie Elzer.
2007. Effectively realizing the inferred message of an
information graphic. In Proc. of RANLP?2007.
Michael Elhadad and Jacques Robin. 1996. An overview
of SURGE: a re-usable comprehensive syntactic real-
ization component. In Proc. of INLG?1996.
Stephanie Elzer, Edward Schwartz, Sandra Carberry,
Daniel Chester, Seniz Demir, and Peng Wu. 2007.
A browser extension for providing visually impaired
users access to the content of bar charts on the web. In
Proc. of WEBIST?2007.
Leo Ferres, Petro Verkhogliad, Gitte Lindgaard, Louis
Boucher, Antoine Chretien, and Martin Lachance.
2007. Improving accessibility to statistical graphs: the
iGraph-Lite system. In Proc. of ASSETS?2007.
Robert Futrelle. 1999. Summarization of diagrams in
documents. In Advances in Automated Text Summa-
rization. MIT Press, pp. 403-421.
Paul Grice. 1975. Logic and conversation. In Syntax and
Semantics, Speech Acts, 3:41-58.
Barbara Grosz and Candace Sidner. 1986. Attention,
intentions, and the structure of discourse. In Compu-
tational Linguistics, 12(3):175?204.
Aravind Joshi, Bonnie Webber, and Ralph Weischedel.
1984. Living up to expectations: computing expert
responses. In Proc. of NCAI?1984.
Evan Kidd and Edith Bavin. 2002. English-speaking
childrens comprehension of relative clauses: evidence
for general-cognitive and language-specific constraints
on development. In Journal of Psycholinguistic Re-
search, 31(6):599-617.
Daniel Marcu. 1997. The rhetorical parsing, summariza-
tion, and generation of natural language texts. PhD
thesis, Department of Computer Science, University of
Toronto.
Kathleen F. McCoy, Sandra Carberry, Tom Roper, and
Nancy Green. 2001. Towards generating textual sum-
maries of graphs. In Proc. of HCI?2001.
Ehud Reiter and Robert Dale. 1997. Building applied
natural language generation systems. In Natural Lan-
guage Engineering, 3(1):57?87.
Brian Roark, Margaret Mitchell, and Kristy Holling-
shead. 2007. Syntactic complexity measures for
detecting mild cognitive impairment. In Proc. of
BioNLP?2007.
Marilyn Walker, Owen Rambow, and Monica Rogati.
2002. Training a sentence planner for spoken dialogue
using boosting. In Computer Speech and Language:
Special Issue on Spoken Language Generation, 16(3-
4):409-433.
Jin Yu, Ehud Reiter, Jim Hunter, and Chris Mellish.
2007. Choosing the content of textual summaries
of large time-series data sets. In Natural Language
Engineering,13(1):25?49.
15
A Discourse-Aware Graph-Based Content-Selection Framework
Seniz Demir Sandra Carberry Kathleen F. McCoy
Department of Computer Science
University of Delaware
Newark, DE 19716
{demir,carberry,mccoy}@cis.udel.edu
Abstract
This paper presents an easy-to-adapt,
discourse-aware framework that can be
utilized as the content selection compo-
nent of a generation system whose goal is
to deliver descriptive texts in several turns.
Our framework involves a novel use of a
graph-based ranking algorithm, to itera-
tively determine what content to convey to
a given request while taking into account
various considerations such as capturing a
priori importance of information, convey-
ing related information, avoiding redun-
dancy, and incorporating the effects of dis-
course history. We illustrate and evaluate
this framework in an accessibility system
for sight-impaired individuals.
1 Introduction
Content selection is the task responsible for deter-
mining what to convey in the output of a gener-
ation system at the current exchange (Reiter and
Dale, 1997). This very domain dependent task
is extremely important from the perspective of
users (Sripada et al, 2001) who have been ob-
served to be tolerant of realization problems as
long as the appropriate content is expressed. The
NLG community has proposed various content
selection approaches since early systems (Moore
and Paris, 1993; McKeown, 1985) which placed
emphasis on text structure and adapted planning
techniques or schemas to meet discourse goals.
This paper proposes a domain-independent
framework which can be incorporated as a content
selection component in a system whose goal is to
deliver descriptive or explanatory texts, such as the
ILEX (O?Donnell et al, 2001), KNIGHT (Lester
and Porter, 1997), and POLIBOX (Chiarcos and
Stede, 2004) systems. At the core of our frame-
work lies a novel use of a graph-based ranking al-
gorithm, which exploits discourse related consid-
erations in determining what content to convey in
response to a request for information. This frame-
work provides the ability to generate successive
history-aware texts and the flexibility to generate
different texts with different parameter settings.
One discourse consideration is the tenet that the
propositions selected for inclusion in a text should
be in some way related to one another. Thus,
the selection process should be influenced by the
relevance of information to what has already been
selected for inclusion. Moreover, we argue that
if the information given in a proposition can be
deduced from the information provided by any
other proposition in the text, this would introduce
redundancy and should be avoided.
Many systems (such as MATCH (Walker et al,
2004) and GEA (Carenini and Moore, 2006)) con-
tain a user model which is employed to adapt con-
tent selection to the user?s preferences (Reiter and
Dale, 1997). Our framework provides a facility
to model a stereotypical user by incorporating the
a priori importance of propositions. This facility
can also be used to capture the preferences of a
particular user.
In a dialogue system, utterances that are gen-
erated without exploiting the previous discourse
seem awkward and unnatural (Moore, 1993). Our
framework takes the previous discourse into ac-
count so as to omit recently communicated propo-
sitions and to determine when repetition of a pre-
viously communicated proposition is appropriate.
To our knowledge, our work is the first effort
utilizing a graph-based ranking algorithm for con-
tent selection, while taking into account what in-
formation preferably should and shouldn?t be con-
veyed together, the a priori importance of infor-
mation, and the discourse history. Our framework
is a domain-independent methodology containing
domain-dependent features that must be instanti-
ated when applying the methodology to a domain.
Section 2 describes our domain-independent
methodology for determining the content of a re-
sponse. Section 3 illustrates its application in an
accessibility system for sight-impaired individuals
and shows the generation flexibility provided by
this framework. Finally, Section 4 discusses the
results of user studies conducted to evaluate the
effectiveness of our methodology.
2 A Graph-based Content Selection
Framework
Our domain-independent framework can be ap-
plied to any domain where there is a set of proposi-
tions that might be conveyed and where a bottom-
up strategy for content selection is appropriate. It
is particularly useful when the set of propositions
should be delivered a little at a time. For exam-
ple, the ILEX system (O?Donnell et al, 2001) uses
multiple descriptions to convey the available infor-
mation about a museum artifact, since the length
of the text that can be displayed on a page is lim-
ited. In order to use our framework, an application
developer should identify the set of propositions
that might be conveyed in the domain, specify the
relations between these propositions, and option-
ally assess a priori importance of the propositions.
Our framework uses a weighted undirected
graph (relation graph), where the propositions
are captured as vertices of the graph and the
edges represent relations between these proposi-
tions. While the number and kinds of relations
represented is up to the developer, the frame-
work does require the use of one specific rela-
tion (Redundancy Relation) that is generalizable
to any descriptive domain. Redundancy Relation
must be specified between two propositions if they
provide similar kinds of information or the infor-
mation provided by one of the propositions can
be deduced from the information provided by the
other. For example, consider applying the frame-
work to the ILEX domain. Since the proposition
that ?this jewelry is produced by a single crafts-
man? can be deduced from the proposition that
?this jewelry is made by a British designer?, these
propositions should be connected with a Redun-
dancy Relation in the relation graph.
There is at most one edge between any two ver-
tices and the weight of that edge represents how
important it is to convey the corresponding propo-
sitions in the same text (which we refer to as
the strength of the relation between these proposi-
tions). For example, suppose that once a museum
artifact is introduced in ILEX, it is more impor-
tant to convey its design style in the same descrip-
tion as opposed to where it is produced. In this
case, the weight of the edge between the proposi-
tions introducing the artifact and its style should
be higher than the weight of the edge between the
propositions introducing the artifact and its pro-
duction place.
The framework incorporates a stereotyp-
ical user model via an additional vertex
(priority vertex) in the relation graph. The
priority vertex is connected to all other vertices
in the graph. The weight of the edge between
a vertex and the priority vertex represents the a
priori importance of that vertex, which in turn
specifies the importance of the corresponding
proposition. For example, suppose that in the
ILEX domain an artifact has two features that
are connected to the proposition introducing the
artifact by the ?feature-of? relation. The a priori
importance of one of these features over the
other can be specified by giving a higher weight
to the edge connecting this proposition to the
priority vertex than is given to the edge between
the other feature and the priority vertex. This
captures a priori importance and makes it more
likely that the important feature will be included
in the artifact?s description.
2.1 Our Ranking Algorithm
With this graph-based setting, the most important
thing to say is the proposition which is most cen-
tral. Several centrality algorithms have been pro-
posed in the literature (Freeman, 1979; Navigli
and Lapata, 2007) for calculating the importance
scores of vertices in a graph. The well-known
PageRank centrality (Brin and Page, 1998) calcu-
lates the importance of a vertex by taking into ac-
count the importance of all other vertices and the
relation of vertices to one another. This metric has
been applied to various tasks such as word sense
disambiguation (Sinha and Mihalcea, 2007) and
text summarization (Erkan and Radev, 2004). We
adopted the weighted PageRank metric (Sinha and
Mihalcea, 2007) for our framework and therefore
compute the importance score of a vertex (Vx) as:
PR(V x) = (1? d) + d ?
?
(V x,V y)?E
wyx
?
wyz
(V z ,V y)?E
PR(V y)
where wxy is the weight associated with the edge
between vertices (Vx) and (Vy), E is the set of all
edges, and d is the damping factor, set to 0.85,
which is its usual setting.
Once the propositions in a domain are captured
in a relation graph with weights assigned to the
edges between them, the straightforward way of
identifying the propositions to be conveyed in the
generated text would be to calculate the impor-
tance of each vertex via the formula above and
then select the k vertices with the highest scores.
However, this straightforward application would
fail to address the discourse issues cited earlier.
Thus we select propositions incrementally, where
with each proposition selected, weights in the
graph are adjusted causing related propositions to
be highlighted and redundant information to be re-
pelled. Because our responses are delivered over
several turns, we also adjust weights between re-
sponses to reflect that discourse situation.
Our algorithm, shown in Figure 1, is run each
time a response text is to be generated. For each
new response, the algorithm begins by adjusting
the importance of the priority vertex (making it
high) and clearing the list of selected propositions.
Step 2 is the heart of the algorithm for generating a
single response. It incrementally selects proposi-
tions to include in the current response, and ad-
justs weights to reflect what has been selected.
In particular, in order to select a proposition, im-
portance scores are computed using the weighted
PageRank metric for all vertices corresponding to
propositions that have not yet been selected for in-
clusion in this response (Step 2-a), and only the
proposition that receives the highest score is se-
lected (Step 2-b). Then, adjustments are made to
achieve four goals toward taking discourse infor-
mation into account (Steps 2-c thru 2-g) before the
PageRank algorithm is run again to select the next
proposition. Steps 3 and 4 adjust weights to reflect
the completed response and to prepare for gener-
ating the next response.
Our first goal is to reflect the a priori impor-
tance of propositions in the selection process. For
this purpose, we always assign the highest (or
one of the highest) importance scores to the pri-
ority vertex among the other vertices (Steps 1 and
2-g). This will make the priority vertex as influen-
tial as any other neighbor of a vertex when calcu-
lating its importance.
Our second goal is to select propositions that are
relevant to previously selected propositions, or in
terms of the graph-based notation, to attract the
selection of vertices that are connected to the se-
lected vertices. To achieve this, we increase the
importance of the vertices corresponding to se-
lected propositions so that the propositions related
to them have a higher probability of being chosen
as the next proposition to include (Step 2-g).
Our third goal is to avoid selecting propositions
that preferably shouldn?t be communicated with
previously selected propositions if other related
propositions are available. To accomplish this, we
introduce the term repellers to refer to the kinds
of relations between propositions that are dispre-
ferred over other relations once one of the propo-
sitions is selected for inclusion. Once a proposi-
tion is selected, we penalize the weights on the
edges between the corresponding vertex and other
vertices that are connected by a repeller (Step 2-
d). We don?t provide any general repellers in the
framework, but rather this is left for the developer
familiar with the domain; any number (zero or
more) and kinds of relations could be identified as
repellers for a particular application domain. For
example, suppose that in the ILEX domain, some
artifacts (such as necklaces) have as features both
a set of design characteristics and the person who
found the artifact. Once the artifact is introduced,
it becomes more important to present the design
characteristics rather than the person who found
that artifact. This preference might be captured by
classifying the relation connecting the proposition
conveying the person who found it to the proposi-
tion introducing the artifact as arepeller.
Our fourth goal is to avoid redundancy by dis-
couraging the selection of propositions connected
by a Redundancy Relation to previously selected
propositions. Once a proposition is selected, we
identify the vertices (redundant to selected ver-
tices) which are connected to the selected ver-
tex by the Redundancy Relation (Step 2-e). For
each redundant to selected vertex, we penalize the
weights on the edges of the vertex except the edge
connected to the priority vertex (Step 2-f) and
hence decrease the probability of that vertex being
chosen for inclusion in the same response.
We have so far described how the content of a
single response is constructed in our framework.
To capture a situation where the system is engaged
in a dialogue with the user and must generate addi-
tional responses for each subsequent user request,
we need to ensure that discourse flows naturally.
Thus, the ranking algorithm must take the previ-
Figure 1: Our Ranking Algorithm for Content Selection.
ous discourse into account in order to identify and
preferably select propositions that have not been
conveyed before and to determine when repetition
of a previously communicated proposition is ap-
propriate. So once a proposition is included in a
response, we have to reduce its ability to compete
for inclusion in subsequent responses. Thus once a
proposition is conveyed in a response, the weight
of the edge connecting the corresponding vertex
to the priority vertex is reduced (Step 2-c in Fig-
ure 1). Once a response is completed, we penal-
ize the weights of the edges of each vertex that
has been selected for inclusion in the current re-
sponse via a penalty factor (if they aren?t already
adjusted) (Step 3 in Figure 1). We use the same
penalty factor (which is used in Step 2-d in Fig-
ure 1) on each edge so that all edges connected to
a selected vertex are penalized equally. However,
it isn?t enough just to penalize the edges of the ver-
tices corresponding to the communicated proposi-
tions. Even after the penalties are applied, a propo-
sition that has just been communicated might re-
ceive a higher importance score than an uncommu-
nicated proposition1. In order to allow all propo-
sitions to become important enough to be said at
some point, the algorithm increases the weights
of the edges of all other vertices in the graph if
they haven?t already been decreased (Step 4 in Fig-
ure 1), thereby increasing their ability to compete
in subsequent responses. In the current implemen-
tation, the weight of an edge is increased via a
boost factor after a response if it is not connected
to a proposition included in that response. The
1We observed that it might happen if a vertex is connected
only to the priority vertex.
boost factor ensures that all propositions will even-
tually become important enough for inclusion.
3 Application in a Particular Domain
This section illustrates the application of our
framework to a particular domain and how our
framework facilitates flexible content selection.
Our example is content selection in the SIGHT
system (Elzer et al, 2007), whose goal is to pro-
vide visually impaired users with the knowledge
that one would gain from viewing information
graphics (such as bar charts) that appear in popu-
lar media. In the current implementation, SIGHT
constructs a brief initial summary (Demir et al,
2008) that conveys the primary message of a bar
chart along with its salient features. We enhanced
the current SIGHT system to respond to user?s
follow-up requests for more information about the
graphic, where the request does not specify the
kind of information that is desired.
The first step in using our framework is deter-
mining the set of propositions that might be con-
veyed in this domain. In our earlier work (Demir
et al, 2008), we identified a set of propositions
that capture information that could be determined
by looking at a bar chart, and for each message
type defined in SIGHT, specified a subset of these
propositions that are related to this message type.
In our example, we use these propositions as can-
didates for inclusion in follow-up responses. Fig-
ure 2 presents a portion of the relation graph,
where some of the identified propositions are rep-
resented as vertices.
The second step is optionally assessing the a
priori importance of each proposition. In user
Figure 2: Subgraph of the Relation graph for Increasing and Decreasing Trend Message Types.
studies (Demir et al, 2008), we asked subjects to
classify the propositions given for a message type
into one of three classes according to their impor-
tance for inclusion in the initial summary: essen-
tial, possible, and not important. We leverage
this information as the a priori importance of ver-
tices in our graph representation. We define three
priority classes. For the propositions that were not
selected as essential by any participant, we clas-
sify the edges connecting these propositions to the
priority vertex into Possible class. For the propo-
sitions which were selected as essential by a single
participant, we classify the edges connecting them
to the priority vertex into Important class. The
edges of the remaining propositions are classified
into Highly Important class. In this example in-
stantiation, we assigned different numeric scores
to these classes where Highly Important and Pos-
sible received the highest and lowest scores re-
spectively.
The third step requires specifying the relations
between every pair of related propositions and de-
termining the weights associated with these re-
lations in the relation graph. First, we identi-
fied propositions which we decided should be
connected by the Redundancy Relation (such as
the propositions conveying ?the overall amount of
change in the trend? and ?the range of the trend?).
Next, we had to determine other relations and as-
sign relative weights. Instead of defining a unique
relation for each related pair, we defined three re-
lation classes, and assigned the relations between
related propositions to one of these classes:
? Period Relation: expresses a relation be-
tween two propositions that span the same
time period
? Entity Relation: expresses a relation be-
tween two propositions if the entities in-
volved in the propositions overlap
? Contrast Relation: expresses a relation be-
tween two propositions if the information
provided by one of the propositions contrasts
with the information provided by the other
We determined that it was very common in
this domain to deliver contrasting propositions to-
gether (similar to other domains (Marcu, 1998))
and therefore we assigned the highest score to the
Contrast Relation class. For local focusing pur-
poses, it is desirable that propositions involving
common entities be delivered in the same response
and thus the Entity Relation class was given the
second highest score. On the other hand, two
propositions which only share the same period are
not very related and conveying such propositions
in the same response could cause the text to appear
?choppy?. We thus identified the Period Relation
class as a repeller and assigned the second low-
est score to relations in that class. Since we don?t
want redundancy in the generated text, the lowest
score was assigned to the Redundancy Relation
class. The next section shows how associating
particular weights with the priority and relation
classes changes the behavior of the framework.
In the domain of graphics, a collection of de-
scriptions of the targeted kind which would facil-
itate a learning based model isn?t available. How-
ever, the accessibility of a corpus in a new domain
would allow the identification of the propositions
along with their relations to each other and the de-
termination of what weighting scheme and adjust-
ment policy will produce the corpus within reason-
able bounds.
3.1 Generating Flexible Responses
The behavior of our framework is dependent on a
number of design parameters such as the weights
associated with various relations, the identification
of repellers, the a priori importance of informa-
tion (if applicable), and the extent to which con-
veying redundant information should be avoided.
The framework allows the application developer
to adjust these factors resulting in the selection of
different content and the generation of different re-
sponses. For instance, in a very straightforward
setting where the same numeric score is assigned
to all relations, the a priori importance of infor-
mation would be the major determining factor in
the selection process. In this section, we will il-
lustrate our framework?s behavior in SIGHT with
three different scenarios. In each case, the user is
assumed to post two consecutive requests for ad-
ditional information about the graphic in Figure 3
after receiving its initial summary.
In our first scenario (which we refer to as ?base-
setting?), the following values have been given to
various design parameters that must be specified in
order to run the ranking algorithm. 1) The weights
of the relations are set to the numeric scores shown
in the text labelled Edges at the bottom (right side)
of Figure 2. 2) The stopping criteria which speci-
fies the number of propositions selected for inclu-
sion in a follow-up response (Step 2 in Figure 1)
is set to four. 3) The amount of decrease in the
weight of the edge between the priority vertex and
the vertex selected for inclusion (Step 2-c in Fig-
ure 1) is set to that edge?s original weight. Thus,
in our example, the weight of that edge is set to 0
once a proposition has been selected for inclusion.
4) The penalty and the redundancy penalty factors
which are used to penalize the edges of a selected
vertex and the vertices redundant to the selected
vertex (Steps 2-d and 3, and 2-f in Figure 1) are
set to the quotient of the highest numeric score
initially assigned to a relation class divided by the
lowest numeric score initially assigned to a rela-
tion class. A penalized score for a relation class
is computed by dividing its initial score by the
penalty factor. The edges of a vertex are penalized
by assigning the penalized scores to these edges
based on the relations that they represent. This set-
ting guarantees that the weight of an edge which
represents the strongest relation cannot be penal-
ized to be lower than the score initially assigned
to the weakest relation. 5) The boost factor which
is used to favor the selection of previously uncon-
veyed propositions for inclusion in subsequent re-
sponses (Step 4 in Figure 1) is set to the square
root of the penalty factor. Thus, the weights of
the edges connected to vertices of previously com-
municated propositions are restored to their initial
scores slowly.
Since in our example, the initial summary has
already been presented, we treat the propositions
conveyed in that summary (P1 and P5 in Figure 2)
as if they had been conveyed in a follow-up re-
sponse and penalize the edges of their correspond-
ing vertices (Steps 2-c and 3 in Figure 1). Thus,
before we invoke the algorithm to construct the
first follow-up response, the weights of edges of
the graph are as shown in Figure 2-A. Within this
base-setting, SIGHT generates the set of follow-up
responses shown in Figure 3A.
In our first scenario (base-setting), we assumed
that the user is capable of making mathematical
deductions such as inferring ?the overall amount
of change in the trend? from ?the range of the
trend?; thus we identified such propositions as
sharing a Redundancy Relation. Young read-
ers (such as fourth graders) might not find these
propositions as redundant because they are lack-
ing in mathematical skills. In our second sce-
nario, we address this issue by setting the re-
dundancy penalty factor to 1 (Step 2-f in Fig-
ure 1) and thus eliminate the penalty on the Re-
dundancy Relation. Now, for the same graphic,
SIGHT generates, in turn, the second alternative
set of responses shown in Figure 3B. The re-
sponses for the two scenarios differ in the second
follow-up response. In the first scenario, a descrip-
tion of the smallest drop was included. However,
in the second scenario, this proposition is replaced
with the overall amount of change in the trend.
This proposition was excluded in the first sce-
nario because the redundancy penalty factor made
it drop in importance.
Our third scenario shows how altering the
weights assigned to relations may change the re-
sponses. Consider a situation where the Con-
trast Relation is given even higher importance by
doubling its score; this might occur in a univer-
sity course domain where courses on the same
general topic are contrasted. SIGHT would then
generate the third alternative set of follow-up re-
sponses shown in Figure 3C. The algorithm is
more strongly forced to group propositions that
Figure 3: Initial Summary and Follow-up Responses.
are in a contrast relation (shown in bold), which
changes the ranking of these propositions.
4 Evaluation
To determine whether our framework selects ap-
propriate content within the context of an applica-
tion, and to assess the contribution of the discourse
related considerations to the selected content and
their impact on readers? satisfaction, we conducted
two user studies. In both studies, the partici-
pants were told that the initial summary should
include the most important information about the
graphic and that the remaining pieces of informa-
tion should be conveyed via follow-up responses.
The participants were also told that the informa-
tion in the first response should be more important
than the information in subsequent responses.
Our goal in the first study was to evaluate the
effectiveness of our framework (base-setting) in
determining the content of follow-up responses in
SIGHT. To our knowledge, no one else has gener-
ated high-level descriptions of information graph-
ics, and therefore evaluation using implementa-
tions of existing content selection modules in the
domain of graphics as a baseline is not feasible.
Thus, we evaluated our framework by comparing
the content that it selects for inclusion in a follow-
up response for a particular graphic with the con-
tent chosen by human subjects for the same re-
sponse. Twenty one university students partici-
pated in the first study and each participant was
presented with the same four graphics. For each
graphic, the participants were first presented with
its initial summary and the set of propositions (18
different propositions) that were used to construct
the relation graph in our framework. The partic-
ipants were then asked to select the four propo-
sitions that they thought were most important to
convey in the first follow-up response.
For each graphic, we ranked the propositions
with respect to the number of times that they were
selected by the participants and determined the po-
sition of each proposition selected by our frame-
work for inclusion in the first follow-up response
with respect to this ranking. The propositions se-
lected by our framework were ranked by the par-
ticipants as the 1st, 2nd, 3rd, and 5th in the first
graphic, as the 1st, 3rd, 4th, and 5th in the sec-
ond graphic, as the 1st, 2nd, 3rd, and 6th in the
third graphic, and as the 2nd, 3rd, 4th, and 6th
in the fourth graphic. Thus for every graph, three
of the four propositions selected by our frame-
work were also in the top four highly-rated propo-
sitions selected by the participants. Therefore,
this study demonstrated that our content selection
framework selects the most important information
for inclusion in a response at the current exchange.
We argued that simply running PageRank to se-
lect the highly-rated propositions is likely to lead
to text that does not cohere because it may con-
tain unrelated or redundant propositions, or fail
to communicate related propositions. Thus, our
approach iteratively runs PageRank and includes
discourse related factors in order to allow what
has been selected to influence the future selections
and consequently improve text coherence. To ver-
ify this argument, we conducted a second study
with four graphics and two different sets of follow-
up responses (each consisting of two consecutive
responses) generated for each graphic. We con-
structed the first set of responses (baseline) by
running PageRank to completion and selecting the
top eight highly-rated propositions, where the top
four propositions form the first response. The con-
tent of the second set of responses was identified
by our approach. Twelve university students (who
did not participate in the first study) were pre-
sented with these four graphics along with their
initial summaries. Each participant was also pre-
sented with the set of responses generated by our
approach in two graphics and the set of responses
generated by the baseline in other cases; the par-
ticipants were unaware of how the follow-up re-
sponses were generated. Overall, each set of re-
sponses was presented to six participants.
We asked the participants to evaluate the set
of responses in terms of their quality in convey-
ing additional information (from 1 to 5 with 5 be-
ing the best). We also asked each participant to
choose which set of responses (from among the
four sets of responses presented to them) best pro-
vides further information about the correspond-
ing graphic. The participants gave the set of re-
sponses generated by our approach an average rat-
ing of 4.33. The average participant rating for
the set of responses generated by the baseline was
3.96. In addition, the lowest score given to the
set of responses generated by our approach was
3, whereas the lowest score that the baseline re-
ceived was 2. We also observed that the set of re-
sponses generated by our approach was selected
as the best set by eight of the twelve participants.
Three of the remaining four participants selected
the set of responses generated by the baseline as
best (although they gave the same score to a set
of responses generated by our approach). In these
cases, the participants emphasized the wording
of the responses as the reason for their selection.
Thus this study demonstrated that the inclusion of
discourse related factors in our approach, in addi-
tion to the use of PageRank (which utilizes the a
priori importance of the propositions and their re-
lations to each other), contributes to text coherence
and improves readers? satisfaction.
5 Conclusion
This paper has presented our implemented
domain-independent content selection framework,
which contains domain-dependent features that
must be instantiated when applying it to a particu-
lar domain. To our knowledge, our work is the first
to select appropriate content by using an incre-
mental graph-based ranking algorithm that takes
into account the tendency for some information to
seem related or redundant to other information, the
a priori importance of information, and what has
already been said in the previous discourse. Al-
though our framework requires a knowledge engi-
neering phase to port it to a new domain, it handles
discourse issues without requiring that the devel-
oper write code to address them. We have demon-
strated how our framework was incorporated in
an accessibility system whose goal is the genera-
tion of texts to describe information graphics. The
evaluation studies of our framework within that
accessibility system show its effectiveness in de-
termining the content of follow-up responses.
6 Acknowledgements
The authors would like to thank Debra Yarrington
and the members of the NLP-AI Lab at UD for
their help throughout the evaluation of this work.
This material is based upon work supported by the
National Institute on Disability and Rehabilitation
Research under Grant No. H133G080047.
References
S. Brin and L. Page. 1998. The anatomy of a large-
scale hypertextual Web search engine. Computer
Networks and ISDN Systems, 30(1-7):107?117.
G. Carenini and J. Moore. 2006. Generating and eval-
uating evaluative arguments. Artificial Intelligence,
170(11):925?452.
C. Chiarcos and M. Stede. 2004. Salience-Driven Text
Planning. In Proc. of INLG?04.
S. Demir, S. Carberry, and K. F. McCoy. 2008. Gener-
ating Textual Summaries of Bar Charts. In Proc. of
INLG?08.
S. Elzer, E. Schwartz, S. Carberry, D. Chester,
S. Demir, and P. Wu. 2007. A browser extension
for providing visually impaired users access to the
content of bar charts on the web. In Proc. of WE-
BIST?2007.
G. Erkan and D. Radev. 2004. LexRank: Graph-
based Lexical Centrality as Salience in Text Summa-
rization. Journal of Artificial Intelligence Research,
22:457?479.
L. C. Freeman. 1979. Centrality in Social Networks: I.
Conceptual Clarification. Social Networks, 1:215?
239.
J. Lester and B. Porter. 1997. Developing and empir-
ically evaluating robust explanation generators: the
KNIGHT experiments. Computational Linguistics,
23(1):65?101.
D. Marcu. 1998. The rhetorical parsing, summariza-
tion, and generation of natural language texts. PhD.
Thesis, Department of Computer Science, University
of Toronto.
K. McKeown. 1985. Discourse strategies for gener-
ating natural-language text. Artificial Intelligence,
27(1):1?41.
J. Moore and C. Paris. 1993. Planning text for advisory
dialogues: capturing intentional and rhetorical infor-
mation. Computational Linguistics, 19(4):651?694.
J. Moore. 1993. Indexing and exploiting a discourse
history to generate context-sensitive explanations.
In Proc. of HLT?93, 165?170.
R. Navigli and M. Lapata. 2007. Graph Connectiv-
ity Measures for Unsupervised Word Sense Disam-
biguation. In Proc. of IJCAI?07, 1683?1688.
M. O?Donnell, C. Mellish, J. Oberlander, and A. Knott.
2001. ILEX: an architecture for a dynamic hypertext
generation system. In Natural Language Engineer-
ing, 7(3):225?250.
E. Reiter and R. Dale. 1997. Building applied natural
language generation systems. In Natural Language
Engineering, 3(1):57?87.
R. Sinha and R. Mihalcea. 2007. Unsupervised Graph-
based Word Sense Disambiguation Using Measures
of Word Semantic Similarity. In Proc. of ICSC?07.
S. Sripada, E. Reiter, J. Hunter, and J. Yu. 2001. A
Two-Stage Model for Content Determination. In
Proc. of ENLGW?01.
M. Walker, S.J. Whittaker, A. Stent, P. Maloor,
J. Moore, M. Johnston, and G. Vasireddy. 2004.
Generation and evaluation of user tailored responses
in multimodal dialogue. In Cognitive Science,
28(5):811?840.
Proceedings of the 2nd Workshop on Speech and Language Processing for Assistive Technologies, pages 52?62,
Edinburgh, Scotland, UK, July 30, 2011. c?2011 Association for Computational Linguistics
Improving the Accessibility of Line Graphs in Multimodal Documents
Charles F. Greenbacker Peng Wu Sandra Carberry Kathleen F. McCoy
Stephanie Elzer* David D. McDonald? Daniel Chester Seniz Demir?
Dept. of Computer & Information Sciences, University of Delaware, USA
[charlieg|pwu|carberry|mccoy|chester]@cis.udel.edu
*Dept. of Computer Science, Millersville University, USA elzer@cs.millersville.edu
?SIFT LLC., Boston, Massachusetts, USA dmcdonald@sift.info
?TU?BI?TAK BI?LGEM, Gebze, Kocaeli, Turkey senizd@uekae.tubitak.gov.tr
Abstract
This paper describes our work on improv-
ing access to the content of multimodal docu-
ments containing line graphs in popular media
for people with visual impairments. We pro-
vide an overview of our implemented system,
including our method for recognizing and con-
veying the intended message of a line graph.
The textual description of the graphic gener-
ated by our system is presented at the most rel-
evant point in the document. We also describe
ongoing work into obtaining additional propo-
sitions that elaborate on the intended message,
and examine the potential benefits of analyz-
ing the text and graphical content together in
order to extend our system to produce sum-
maries of entire multimodal documents.
1 Introduction
Individuals with visual impairments have difficulty
accessing the information contained in multimodal
documents. Although screen-reading software can
render the text of the document as speech, the graph-
ical content is largely inaccessible. Here we con-
sider information graphics (e.g., bar charts, line
graphs) often found in popular media sources such
as Time magazine, Businessweek, and USA Today.
These graphics are typically intended to convey a
message that is an important part of the overall story,
yet this message is generally not repeated in the ar-
ticle text (Carberry et al, 2006). People who are
unable to see and assimilate the graphical material
will be left with only partial information.
While some work has addressed the accessibility
of scientific graphics through alternative means like
touch or sound (see Section 7), such graphs are de-
signed for an audience of experts trained to use them
for data visualization. In contrast, graphs in popular
media are constructed to make a point which should
be obvious without complicated scientific reasoning.
We are thus interested in generating a textual pre-
sentation of the content of graphs in popular media.
Other research has focused on textual descriptions
(e.g., Ferres et al (2007)); however in that work the
same information is included in the textual summary
for each instance of a graph type (i.e., all summaries
of line graphs contain the same sorts of informa-
tion), and the summary does not attempt to present
the overall intended message of the graph.
SIGHT (Demir et al, 2008; Elzer et al, 2011) is
a natural language system whose overall goal is pro-
viding blind users with interactive access to multi-
modal documents from electronically-available pop-
ular media sources. To date, the SIGHT project
has concentrated on simple bar charts. Its user in-
terface is implemented as a browser helper object
within Internet Explorer that works with the JAWS
screen reader. When the system detects a bar chart
in a document being read by the user, it prompts the
user to use keystrokes to request a brief summary of
the graphic capturing its primary contribution to the
overall communicative goal of the document. The
summary text can either be read to the user with
JAWS or read by the user with a screen magnifier
tool. The interface also enables the user to request
further information about the graphic, if desired.
However, SIGHT is limited to bar charts only.
In this work, we follow the methodology put forth
by SIGHT, but investigate producing a summary of
52
?
102468 1900?10
?20
?50?60
?70?80
?90?
03
?30?40
2000
10
8.9
1.979 inches over t
he past centu
ry. Annual d
ifference fro
m Seattle?s
In the seattl
e area, for e
xample, the
 Pacific Oce
an has risen
 nearly 
they are risi
ng about 0.0
4?0.09 of an
 inch each y
ear.
Sea levels fl
uctuate arou
nd the globe
, but oceano
graphers bel
ieve
Ocean level
s rising
1899 sea lev
el, in inches
:
Figure 1: From ?Worry flows from Arctic ice to tropical
waters? in USA Today, May 31, 2006.
line graphs. Line graphs have different discourse
goals and communicative signals than bar charts,1
and thus require significantly different processing.
In addition, our work addresses the issue of coher-
ent placement of a graphic?s summary when reading
the text to the user and considers the summarization
of entire documents ? not just their graphics.
2 Message Recognition for Line Graphs
This section provides an overview of our imple-
mented method for identifying the intended message
of a line graph. In processing a line graph, a vi-
sual extraction module first analyzes the image file
and produces an XML representation which fully
specifies the graphic (including the beginning and
ending points of each segment, any annotations on
points, axis labels, the caption, etc.). To identify
the intended message of a line graph consisting of
many short, jagged segments, we must generalize
it into a sequence of visually-distinguishable trends.
This is performed by a graph segmentation module
which uses a support vector machine and a variety
of attributes (including statistical tests) to produce a
model that transforms the graphic into a sequence of
straight lines representing visually-distinguishable
trends. For example, the line graph in Figure 1 is
divided into a stable trend from 1900 to 1930 and a
rising trend from 1930 to 2003. Similarly, the line
graph in Figure 2 is divided into a rising trend from
1Bar charts present data as discrete bars and are often used
to compare entities, while line graphs contain continuous data
series and are designed to portray longer trend relationships.
20062005200420032002
19971998
1999
20012000
200,000 150,0001999
: 189,840
70,6062006:
50,000100,000Declining Dur
ango sales
0
Figure 2: From ?Chrysler: Plant had $800 million im-
pact? in The (Wilmington) News Journal, Feb 15, 2007.
1997 to 1999 and a falling trend from 1999 to 2006.
In analyzing a corpus of around 100 line graphs
collected from several popular media sources, we
identified 10 intended message categories (includ-
ing rising-trend, change-trend, change-trend-return,
and big-jump, etc.), that seem to capture the kinds
of high-level messages conveyed by line graphs. A
suggestion generation module uses the sequence of
trends identified in the line graph to construct all
of its possible candidate messages in these message
categories. For example, if a graph contains three
trends, several candidate messages are constructed,
including two change-trend messages (one for each
adjacent pair of trends), a change-trend-return mes-
sage if the first and third trends are of the same type
(rising, falling, or stable), as well as a rising, falling,
or stable trend message for each individual trend.
Next, various communicative signals are ex-
tracted from the graphic, including visual features
(such as a point annotated with its value) that draw
attention to a particular part of the line graph, and
linguistic clues (such as the presence of certain
words in the caption) that suggest a particular in-
tended message category. Figure 2 contains several
such signals, including two annotated points and the
word declining in its caption. Next, a Bayesian net-
work is built to estimate the probability of the can-
didate messages; the extracted communicative sig-
nals serve as evidence for or against each candidate
message. For Figure 2, our system produces change-
trend(1997, rise, 1999, fall, 2006) as the logical rep-
resentation of the most probable intended message.
Since the dependent axis is often not explicitly la-
beled, a series of heuristics are used to identify an
appropriate referent, which we term the measure-
ment axis descriptor. In Figure 2, the measurement
axis descriptor is identified as durango sales. The
53
intended message and measurement axis descriptor
are then passed to a realization component which
uses FUF/SURGE (Elhadad and Robin, 1996) to
generate the following initial description:
This graphic conveys a changing trend in
durango sales, rising from 1997 to 1999
and then falling to 2006.
3 Identifying a Relevant Paragraph
In presenting a multimodal document to a user via a
screen reader, if the author does not specify a read-
ing order in the accessibility preferences, it is not
entirely clear where the description of the graph-
ical content should be given. The text of scien-
tific articles normally makes explicit references to
any graphs contained in the document; in this case,
it makes sense to insert the graphical description
alongside the first such reference. However, popular
media articles rarely contain explicit references to
graphics. We hypothesize that describing the graphi-
cal content together with the most relevant portion of
the article text will result in a more coherent presen-
tation. Results of an experiment described in Sec-
tion 3.3 suggest the paragraph which is geograph-
ically closest to the graphic is very often not rele-
vant. Thus, our task becomes identifying the portion
of the text that is most relevant to the graph.
We have developed a method for identifying the
most relevant paragraph by measuring the similarity
between the graphic?s textual components and the
content of each individual paragraph in the docu-
ment. An information graphic?s textual components
may consist of a title, caption, and any additional
descriptions it contains (e.g., the five lines of text in
Figure 1 beneath the caption Ocean levels rising).
An initial method (P-KL) based on KL divergence
measures the similarity between a paragraph and the
graphic?s textual component; a second method (P-
KLA) is an extension of the first that incorporates
an augmented version of the textual component.
3.1 Method P-KL: KL Divergence
Kullback-Leibler (KL) divergence (Kullback, 1968)
is widely used to measure the similarity between two
language models. It can be expressed as:
DKL(p||q) =
?
i?V
p(i)log
p(i)
q(i)
where i is the index of a word in vocabulary V , and
p and q are two distributions of words. Liu et al
(Liu and Croft, 2002) applied KL divergence to text
passages in order to improve the accuracy of docu-
ment retrieval. For our task, p is a smoothed word
distribution built from the line graph?s textual com-
ponent, and q is another smoothed word distribution
built from a paragraph in the article text. Smoothing
addresses the problem of zero occurrences of a word
in the distributions. We rank the paragraphs by their
KL divergence scores from lowest to highest, since
lower scores indicate a higher similarity.
3.2 Method P-KLA: Using Augmented Text
In analyzing paragraphs relevant to the graphics, we
realized that they included words that were germane
to describing information graphics in general, but
not related to the domains of individual graphs. This
led us to build a set of ?expansion words? that tend to
appear in paragraphs relevant to information graph-
ics. If we could identify domain-independent terms
that were correlated with information graphics in
general, these expansion words could then be added
to the textual component of a graphic when measur-
ing its similarity to a paragraph in the article text.
We constructed the expansion word set using an
iterative process. The first step is to use P-KL to
identify m pseudo-relevant paragraphs in the cor-
responding document for each graphic in the train-
ing set (the current implementation uses m = 3).
This is similar to pseudo-relevance feedback used in
IR (Zhai, 2008), except only a single query is used
in the IR application, whereas we consider many
pairs of graphics and documents to obtain an ex-
pansion set applicable to any subsequent informa-
tion graphic. Given n graphics in the training set,
we identify (up to) m ? n relevant paragraphs.
The second step is to extract a set of words re-
lated to information graphics from these m ?n para-
graphs. We assume the collection of pseudo-relevant
paragraphs was generated by two models, one pro-
ducing words relevant to the information graphics
and another producing words relevant to the topics
of the individual documents. Let Wg represent the
word frequency vector yielding words relevant to
the graphics, Wa represent the word frequency vec-
tor yielding words relevant to the document topics,
and Wp represent the word frequency vector of the
54
pseudo-relevant paragraphs. We compute Wp from
the pseudo-relevant paragraphs themselves, and we
estimate Wa using the word frequencies from the
article text in the documents. Finally, we compute
Wg by filtering-out the components ofWa fromWp.
This process is related to the work by Widdows
(2003) on orthogonal negation of vector spaces.
The task can be formulated as follows:
1. Wp = ?Wa + ?Wg where ? > 0 and ? > 0,
which means the word frequency vector for
the pseudo-relevant paragraphs is a linear com-
bination of the background (topic) word fre-
quency vector and the graphic word vector.
2. < Wa,Wg >= 0 which means the background
word vector is orthogonal to the graph descrip-
tion word vector, under the assumption that the
graph description word vector is independent of
the background word vector and that these two
share minimal information.
3. Wg is assumed to be a unit vector, since we are
only interested in the relative rank of the word
frequencies, not their actual values.
Solving the above equations, we obtain:
? =
< Wp,Wa >
< Wa,Wa >
Wg = normalized
(
Wp ?
< Wp,Wa >
< Wa,Wa >
?Wa
)
After computing Wg, we use WordNet to filter-
out words having a predominant sense other than
verb or adjective, under the assumption that nouns
will be mainly relevant to the domains or topics
of the graphs (and are thus ?noise?) whereas we
want a general set of words (e.g., ?increasing?)
that are typically used when describing the data in
any graph. As a rough estimate of whether a word
is predominantly a verb or adjective, we determine
whether there are more verb and adjective senses of
the word in WordNet than there are noun senses.
Next, we rank the words in the filteredWg accord-
ing to frequency and select the k most frequent as
our expansion word list (we used k = 25 in our ex-
periments). The two steps (identifyingm?n pseudo-
relevant paragraphs and then extracting a word list of
size k to expand the graphics? textual components)
are applied iteratively until convergence occurs or
minimal changes are observed between iterations.
In addition, parameters of the intended message
that represent points on the x-axis capture domain-
specific content of the graphic?s communicative
goal. For example, the intended message of the line
graph in Figure 1 conveys a changing trend from
1900 to 2003 with the change occurring in 1930. To
help identify relevant paragraphs mentioning these
years, we also add these parameters of the intended
message to the augmented word list.
The result of this process is the final expansion
word list used in method P-KLA. Because the tex-
tual component may be even shorter than the expan-
sion word list, we do not add a word from the expan-
sion word list to the textual component unless the
paragraph being compared also contains this word.
3.3 Results of P-KL and P-KLA
334 training graphs with their accompanying articles
were used to build the expansion word set. A sepa-
rate set of 66 test graphs and articles was analyzed
by two human annotators who identified the para-
graphs in each document that were most relevant to
its associated information graphic, ranking them in
terms of relevance. On average, annotator 1 selected
2.00 paragraphs and annotator 2 selected 1.71 para-
graphs. The annotators agreed on the top ranked
paragraph for only 63.6% of the graphs. Consid-
ering the agreement by chance, we can calculate the
kappa statistic as 0.594. This fact shows that the
most relevant paragraph is not necessarily obvious
and multiple plausible options may exist.
We applied both P-KL and P-KLA to the test set,
with each method producing a list of the paragraphs
ranked by relevance. Since our goal is to provide
the summary of the graphic at a suitable point in the
article text, two evaluation criteria are appropriate:
1. TOP: the method?s success rate in selecting
the most relevant paragraph, measured as how
often it chooses the paragraph ranked highest
by either of the annotators
2. COVERED: the method?s success rate in se-
lecting a relevant paragraph, measured as how
often it chooses one of the relevant paragraphs
identified by the annotators
Table 1 provides the success rates of both of our
methods for the TOP and COVERED criteria, along
with a simple baseline that selected the paragraph
55
geographically-closest to the graphic. These results
show that both methods outperform the baseline,
and that P-KLA further improves on P-KL. P-KLA
selects the best paragraph in 60.6% of test cases,
and selects a relevant paragraph in 71.2% of the
cases. For both TOP and COVERED, P-KLA nearly
doubles the baseline success rate. The improve-
ment of P-KLA over P-KL suggests that our expan-
sion set successfully adds salient words to the tex-
tual component. A one-sided Z-test for proportion
based on binomial distribution is shown in Table 1
and indicates that the improvements of P-KL over
the baseline and P-KLA over P-KL are statistically-
significant at the 0.05 level across both criteria. The
Z-test is calculated as:
p? p0
?
p0(1?p0)
n
where p0 is the lower result and p is the improved
result. The null hypothesis is H0 : p = p0 and the
alternative hypothesis is H1 : p > p0.
3.4 Using relevant paragraph identification to
improve the accessibility of line graphs
Our system improves on SIGHT by using method
P-KLA to identify the paragraph that is most rele-
vant to an information graphic. When this paragraph
is encountered, the user is asked whether he or she
would like to access the content of the graphic. For
example, our system identifies the following para-
graph as most relevant to Figure 2:
Doing so likely would require the com-
pany to bring in a new model. Sales of
the Durango and other gas-guzzling SUVs
have slumped in recent years as prices at
the pump spiked.
In contrast, the geographically-closest paragraph has
little relevance to the graphic:
?We have three years to prove to them
we need to stay open,? said Sam Latham,
president of the AFL-CIO in Delaware,
who retired from Chrysler after 39 years.
4 Identifying Additional Propositions
After the intended message has been identified, the
system next looks to identify elaborative informa-
tional propositions that are salient in the graphic.
These additional propositions expand on the initial
description of the graph by filling-in details about
the knowledge being conveyed (e.g., noteworthy
points, properties of trends, visual features) in order
to round-out a summary of the graphic.
We collected a corpus of 965 human-written sum-
maries for 23 different line graphs to discover which
propositions were deemed most salient under varied
conditions.2 Subjects received an initial description
of the graph?s intended message, and were asked to
write additional sentences capturing the most impor-
tant information conveyed by the graph. The propo-
sitions appearing in each summary were manually
coded by an annotator to determine which were most
prevalent. From this data, we developed rules to
identify important propositions in new graphs. The
rules assign weights to propositions indicating their
importance, and the weights can be compared to de-
cide which propositions to include in a summary.
Three types of rules were built. Type-1 (message
category-only) rules were created when a plurality
of summaries for all graphs having a given intended
message contained the same proposition (e.g., pro-
vide the final value for all rising-trend and falling-
trend graphs). Weights for type-1 rules were based
on the frequency with which the proposition ap-
peared in summaries for graphs in this category.
Type-2 (visual feature-only) rules were built when
there was a correlation between a visual feature and
the use of a proposition describing that feature, re-
gardless of the graph?s message category (e.g., men-
tion whether the graph is highly volatile). Type-2
rule weights are a function of the covariance be-
tween the magnitude of the visual feature (e.g., de-
gree of volatility) and the proportion of summaries
mentioning this proposition for each graph.
For propositions associated with visual features
linked to a particular message category (e.g., de-
scribe the trend immediately following a big-jump
or big-fall when it terminates prior to the end of the
graph), we constructed Type-3 (message category
+ visual feature) rules. Type-3 weights were cal-
culated just like Type-2 weights, except the graphs
were limited to the given category.
As an example of identifying additional proposi-
2This corpus is described in greater detail by Greenbacker et
al. (2011) and is available at www.cis.udel.edu/~mccoy/corpora
56
closest P-KL significance level over closest P-KLA significance level over P-KL
TOP 0.272 0.469 (z = 3.5966, p < 0.01) 0.606 (z = 2.2303, p < 0.025)
COVERED 0.378 0.606 (z = 3.8200, p < 0.01) 0.712 (z = 1.7624, p < 0.05)
Table 1: Success rates for baseline method (?closest?), P-KL, and P-KLA using the TOP and COVERED criteria.
tions, consider Figures 1 and 2. Both line graphs
belong to the same intended message category:
change-trend. However, the graph in Figure 1 is far
more volatile than Figure 2, and thus it is likely that
we would want to mention this proposition (i.e., ?the
graph shows a high degree of volatility...?) in a sum-
mary of Figure 1. By finding the covariance between
the visual feature (i.e., volatility) and the frequency
with which a corresponding proposition was anno-
tated in the corpus summaries, a Type-2 rule assigns
a weight to this proposition based on the magnitude
of the visual feature. Thus, the volatility proposi-
tion will be weighted strongly for Figure 1, and will
likely be selected to appear in the initial summary,
while the weight for Figure 2 will be very low.
5 Integrating Text and Graphics
Until now, our system has only produced summaries
for the graphical content of multimodal documents.
However, a user might prefer a summary of the en-
tire document. Possible use cases include examining
this summary to decide whether to invest the time re-
quired to read a lengthy article with a screen reader,
or simply addressing the common problem of having
too much material to review in too little time (i.e.,
information overload). We are developing a system
extension that will allow users to request summaries
of arbitrary length that cover both the text and graph-
ical content of a multimodal document.
Graphics in popular media convey a message that
is generally not repeated in the article text. For ex-
ample, the March 3, 2003 issue of Newsweek con-
tained an article entitled, ?The Black Gender Gap,?
which described the professional achievements of
black women. It included a line graph (Figure 3)
showing that the historical gap in income equality
between white women and black women had been
closed, yet this important message appears nowhere
in the article text. Other work in multimodal doc-
ument summarization has relied on image captions
and direct references to the graphic in the text (Bha-
tia et al, 2009); however, these textual elements do
Figure 3: From ?The Black Gender Gap? in Newsweek,
Mar 3, 2003.
not necessarily capture the message conveyed by in-
formation graphics in popular media. Thus, the user
may miss out on an essential component of the over-
all communicative goal of the document if the sum-
mary covers only material presented in the text.
One approach to producing a summary of the en-
tire multimodal document might be to ?concatenate?
a traditional extraction-based summary of the text
(Kupiec et al, 1995; Witbrock and Mittal, 1999)
with the description generated for the graphics by
our existing system. The summary of the graphi-
cal content could be simply inserted wherever it is
deemed most relevant in the text summary. How-
ever, such an approach would overlook the relation-
ships and interactions between the text and graphical
content. The information graphics may make certain
concepts mentioned in the text more salient, and vice
versa. Unless we consider the contributions of both
the text and graphics together during the content se-
lection phase, the most important information might
not appear in the summary of the document.
Instead, we must produce a summary that inte-
grates the content conveyed by the text and graphics.
We contend that this integration must occur at the se-
mantic level if it is to take into account the influence
of the graphic?s content on the salience of concepts
in the text and vice versa. Our tack is to first build
a single semantic model of the concepts expressed
in both the article text and information graphics, and
then use this model as the basis for generating an
abstractive summary of the multimodal document.
57
Drawing from a model of the semantic content of the
document, we select as many or as few concepts as
we wish, at any level of detail, to produce summaries
of arbitrary length. This will permit the user to re-
quest a quick overview in order to decide whether to
read the original document, or a more comprehen-
sive synopsis to obtain the most important content
without having to read the entire article.
5.1 Semantic Modeling of Multimodal
Documents
Content gathered from the article text by a seman-
tic parser and from the information graphics by
our graph understanding system is combined into
a single semantic model based on typed, struc-
tured objects organized under a foundational ontol-
ogy (McDonald, 2000a). For the semantic pars-
ing of text, we use Sparser (McDonald, 1992), a
bottom-up, phrase-structure-based chart parser, op-
timized for semantic grammars and partial parsing.3
Using a built-in model of core English grammar
plus domain-specific grammars, Sparser extracts in-
formation from the text and produces categorized
objects as a semantic representation (McDonald,
2000b). The intended message and salient additional
propositions identified by our system for the infor-
mation graphics are decomposed and added to the
model constructed by Sparser.4
Model entries contain slots for attributes in the
concept category?s ontology definition (fillable by
other concepts or symbols), the original phrasings
mentioning this concept in the text (represented as
parameterized synchronous TAG derivation trees),
and markers recording document structure (i.e.,
where in the text [including title, headings, etc.] or
graphic the concept appeared). Figure 4 shows some
of the information contained in a small portion of
the semantic model built for an article entitled ?Will
Medtronic?s Pulse Quicken?? from the May 29,
2006 edition of Businessweek magazine5, which in-
cluded a line graph. Nodes correspond to concepts
3https://github.com/charlieg/Sparser
4Although the framework is general enough to accommo-
date any modality (e.g., images, video) given suitable seman-
tic analysis tools, our prototype implementation focuses on bar
charts and line graphs analyzed by SIGHT.
5http://www.businessweek.com/magazine/
content/06_22/b3986120.htm
and edges denote relationships between concepts;
dashed lines indicate links to concepts not shown in
this figure. Nodes are labelled with the name of the
conceptual category they instantiate, and a number
to distinguish between individuals. The middle of
each box displays the attributes of the concept, while
the bottom portion shows some of the original text
phrasings. Angle brackets (<>) note references to
other concepts, and hash marks (#) indicate a sym-
bol that has not been instantiated as a concept.
P1S1: "medical device
    giant Medtronic"
P1S5: "Medtronic"
Name: "Medtronic"
Stock: "MDT"
Industry: (#pacemakers,
    #defibrillators,
    #medical devices)
Company1
P1S4: "Joanne
    Wuensch"
P1S7: "Wuensch"
FirstName: "Joanne"
LastName: "Wuensch"
Person1
P1S4: "a 12-month
    target of 62"
Person: <Person 1>
Company: <Company 1>
Price: $62.00
Horizon: #12_months
TargetStockPrice1
Figure 4: Detail of model for Businessweek article.
5.2 Rating Content in Semantic Models
The model is then rated to determine which items are
most salient. The concepts conveying the most in-
formation and having the most connections to other
important concepts in the model are the ones that
should be chosen for the summary. The importance
of each concept is rated according to a measure of
information density (ID) involving several factors:6
Saturation Level Completeness of attributes in
model entry: a concept?s filled-in slots (f ) vs. its
total slots (s), and the importance of the concepts
(ci) filling those slots:
f
s ? log(s) ?
?f
i=1 ID(ci)
Connectedness Number of connections (n) with
other concepts (cj), and the importance of these con-
nected concepts:
?n
j=1 ID(cj)
Frequency Number of observed phrasings (e) re-
alizing the concept in text of the current document
Prominence in Text Prominence based on docu-
ment structure (WD) and rhetorical devices (WR)
Graph Salience Salience assessed by the graph
understanding system (WG) ? only applies to con-
cepts appearing in the graphics
6The first three factors are similar to the dominant slot
fillers, connectivity patterns, and frequency criteria described
by Reimer and Hahn (1988).
58
Saturation corresponds to the completeness of the
concept in the model. The more attribute slots that
are filled, the more we know about a particular con-
cept instance. However, this measure is highly sen-
sitive to the degree of detail provided in the seman-
tic grammar and ontology class definition (whether
created by hand or automatically). A concept having
two slots, both of which are filled-out, is not neces-
sarily more important than a concept with only 12
of its 15 slots filled. The more important a concept
category is in a given domain, the more detailed its
ontology class definition will likely be. Thus, we
can assume that a concept definition having a dozen
or more slots is, broadly speaking, more important
in the domain than a less well-defined concept hav-
ing only one or two slots. This insight is the basis of
a normalization factor (log(s)) used in ID.
Saturation differs somewhat from repetition in
that it attempts to measure the amount of informa-
tion associated with a concept, rather than simply
the number of times a concept is mentioned in the
text. For example, a news article about a proposed
law might mention ?Washington? several times, but
the fact that the debate took place in Washington,
D.C. is unlikely to be an important part of the article.
However, the key provisions of the bill, which may
individually be mentioned only once, are likely more
important as a greater amount of detail is provided
concerning them. Simple repetition is not necessar-
ily indicative of the importance of a concept, but if a
large amount of information is provided for a given
concept, it is safe to assume the concept is important
in the context of that document.
Document structure (WD) is another important
clue in determining which elements of a text are
important enough to include in a summary (Marcu,
1997). If a concept is featured prominently in the
title, or appears in the first or final paragraphs, it is
likely more important than a concept buried in the
middle of the document. Importance is also affected
by certain rhetorical devices (WR) which serve to
highlight particular concepts. Being used in an id-
iom, or compared to another concept by means of
juxtaposition suggests that a given concept may hold
special significance. Finally, the weights assigned
by our graph understanding system for the additional
propositions identified in the graphics are incorpo-
rated into the ID of the concepts involved as WG.
5.3 Selecting Content for a Summary
To select concepts for inclusion in the summary,
the model will then be passed to a discourse-aware
graph-based content selection framework (Demir et
al., 2010), which selects concepts one at a time
and iteratively re-weights the remaining items so
as to include related concepts and avoid redun-
dancy. This algorithm incorporates PageRank (Page
et al, 1999), but with several modifications. In ad-
dition to centrality assessment based on relation-
ships between concepts, it includes apriori impor-
tance nodes enabling us to incorporate concept com-
pleteness, number of expressions, document struc-
ture, and rhetorical devices. More importantly from
a summary generation perspective, the algorithm it-
eratively picks concepts one at a time, and re-ranks
the remaining entries by increasing the weight of re-
lated items and discounting redundant ones. This
allows us to select concepts that complement each
other while simultaneously avoiding redundancy.
6 Generating an Abstractive Summary of
a Multimodal Document
Figure 4 shows the two most important concepts
(Company1 & Person1) selected from the Medtronic
article in Section 5.1. Following McDonald and
Greenbacker (2010), we use the phrasings observed
by the parser as the ?raw material? for expressing
these selected concepts. Reusing the original phras-
ings reduces the reliance on built-in or ?canned?
constructions, and allows the summary to reflect the
style of the original text. The derivation trees stored
in the model to realize a particular concept may use
different syntactic constituents (e.g., noun phrases,
verb phrases). Multiple trees are often available for
each concept, and we must select particular trees that
fit together to form a complete sentence.
The semantic model also contains concepts rep-
resenting propositions extracted from the graphics,
as well as relationships connecting these graphical
concepts with those derived from the text, and there
are no existing phrasings in the original document
that can be reused to convey this graphical content.
However, the set of proposition types that can be ex-
tracted from the graphics is finite. To ensure that we
have realizations for every concept in our model, we
create TAG derivation trees for each type of graphi-
59
cal proposition. As long as realizations are supplied
for every proposition that can be decomposed in the
model, our system will never be stuck with a concept
without the means to express it.
The set of expressions is augmented by many
built-in realizations for common semantic relation-
ships (e.g., ?is-a,? ?has-a?), as well as expressions
inherited from other conceptual categories in the hi-
erarchy. If the observed expressions are retained as
the system analyzes multiple documents over time,
making these realizations available for later use by
concepts in the same category, the variety of utter-
ances we can generate is increased greatly.
By using synchronous TAG trees, we know that
the syntactic realizations of two semantically-related
concepts will fit together syntactically (via substitu-
tion or adjunction). However, the concepts selected
for the summary of the Medtronic article (Com-
pany1 & Person1), are not directly connected in the
model. To produce a single summary sentence for
these two concepts, we must find a way of express-
ing them together with the available phrasings. This
can be accomplished by using an intermediary con-
cept that connects both of the selected items in the
semantic model, in order to ?bridge the gap? be-
tween them. In this example, a reasonable option
would be TargetStockPrice1, one of the many con-
cepts linking Company1 and Person1. Combining
original phrasings from all three concepts (via sub-
stitution and adjunction operations on the underly-
ing TAG trees), along with a ?built-in? realization
inherited by the TargetStockPrice category (a sub-
type of Expectation), yields this surface form:
Wuensch expects a 12-month target of 62
for medical device giant Medtronic.
7 Related Work
Research into providing alternative access to graph-
ics has taken both verbal and non-verbal approaches.
Kurze (1995) presented a verbal description of the
properties (e.g., diagram style, number of data sets,
range and labels of axes) of business graphics. Fer-
res et al (2007) produced short descriptions of the
information in graphs using template-driven genera-
tion based on the graph type. The SIGHT project
(Demir et al, 2008; Elzer et al, 2011) generated
summaries of the high-level message content con-
veyed by simple bar charts. Other modalities, like
sound (Meijer, 1992; Alty and Rigas, 1998; Choi
and Walker, 2010) and touch (Ina, 1996; Krufka et
al., 2007), have been used to impart graphics via a
substitute medium. Yu et al (2002) and Abu Doush
et al (2010) combined haptic and aural feedback,
enabling users to navigate and explore a chart.
8 Discussion
This paper presented our system for providing ac-
cess to the full content of multimodal documents
with line graphs in popular media. Such graph-
ics generally have a high-level communicative goal
which should constitute the core of a graphic?s sum-
mary. Rather than providing this summary at the
point where the graphic is first encountered, our sys-
tem identifies the most relevant paragraph in the
article and relays the graphic?s summary at this
point, thus increasing the presentation?s coherence.
System extensions currently in development will
provide a more integrative and accessible way for
visually-impaired readers to experience multimodal
documents. By producing abstractive summaries of
the entire document, we reduce the amount of time
and effort required to assimiliate the information
conveyed by such documents in popular media.
Several tasks remain as future work. The intended
message descriptions generated by our system need
to be evaluated by both sighted and non-sighted hu-
man subjects for clarity and accuracy. We intend
to test our hypothesis that graphics ought to be de-
scribed alongside the most relevant part of the text
by performing an experiment designed to determine
the presentation order preferred by people who are
blind. The rules developed to identify elaborative
propositions also must be validated by a corpus or
user study. Finally, once the system is fully imple-
mented, the abstractive summaries generated for en-
tire multimodal documents will need to be evaluated
by both sighted and sight-impaired judges.
Acknowledgments
This work was supported in part by the by the Na-
tional Institute on Disability and Rehabilitation Re-
search under grant H133G080047 and by the Na-
tional Science Foundation under grant IIS-0534948.
60
References
Iyad Abu Doush, Enrico Pontelli, Tran Cao Son, Dominic
Simon, and Ou Ma. 2010. Multimodal presenta-
tion of two-dimensional charts: An investigation using
Open Office XML and Microsoft Excel. ACM Trans-
actions on Accessible Computing (TACCESS), 3:8:1?
8:50, November.
James L. Alty and Dimitrios I. Rigas. 1998. Communi-
cating graphical information to blind users using mu-
sic: the role of context. In Proceedings of the SIGCHI
Conference on Human Factors in Computing Systems,
CHI ?98, pages 574?581, Los Angeles, April. ACM.
Sumit Bhatia, Shibamouli Lahiri, and Prasenjit Mitra.
2009. Generating synopses for document-element
search. In Proceeding of the 18th ACM Conference
on Information and Knowledge Management, CIKM
?09, pages 2003?2006, Hong Kong, November. ACM.
Sandra Carberry, Stephanie Elzer, and Seniz Demir.
2006. Information graphics: an untapped resource for
digital libraries. In Proceedings of the 29th Annual
International ACM SIGIR Conference on Research
and Development in Information Retrieval, SIGIR ?06,
pages 581?588, Seattle, August. ACM.
Stephen H. Choi and Bruce N. Walker. 2010. Digitizer
auditory graph: making graphs accessible to the visu-
ally impaired. In Proceedings of the 28th International
Conference on Human Factors in Computing Systems,
CHI ?10, pages 3445?3450, Atlanta, April. ACM.
Seniz Demir, Sandra Carberry, and Kathleen F. McCoy.
2008. Generating textual summaries of bar charts.
In Proceedings of the 5th International Natural Lan-
guage Generation Conference, INLG 2008, pages 7?
15, Salt Fork, Ohio, June. ACL.
Seniz Demir, Sandra Carberry, and Kathleen F. Mc-
Coy. 2010. A discourse-aware graph-based content-
selection framework. In Proceedings of the 6th In-
ternational Natural Language Generation Conference,
INLG 2010, pages 17?26, Trim, Ireland, July. ACL.
Michael Elhadad and Jacques Robin. 1996. An overview
of SURGE: a re-usable comprehensive syntactic re-
alization component. In Proceedings of the 8th In-
ternational Natural Language Generation Workshop
(Posters and Demonstrations), Sussex, UK, June.
ACL.
Stephanie Elzer, Sandra Carberry, and Ingrid Zukerman.
2011. The automated understanding of simple bar
charts. Artificial Intelligence, 175:526?555, February.
Leo Ferres, Petro Verkhogliad, Gitte Lindgaard, Louis
Boucher, Antoine Chretien, and Martin Lachance.
2007. Improving accessibility to statistical graphs: the
iGraph-Lite system. In Proceedings of the 9th Inter-
national ACM SIGACCESS Conference on Computers
and Accessibility, ASSETS ?07, pages 67?74, Tempe,
October. ACM.
Charles F. Greenbacker, Sandra Carberry, and Kathleen F.
McCoy. 2011. A corpus of human-written summaries
of line graphs. In Proceedings of the EMNLP 2011
Workshop on Language Generation and Evaluation,
UCNLG+Eval, Edinburgh, July. ACL. (to appear).
Satoshi Ina. 1996. Computer graphics for the blind. SIG-
CAPH Newsletter on Computers and the Physically
Handicapped, pages 16?23, June. Issue 55.
Stephen E. Krufka, Kenneth E. Barner, and Tuncer Can
Aysal. 2007. Visual to tactile conversion of vector
graphics. IEEE Transactions on Neural Systems and
Rehabilitation Engineering, 15(2):310?321, June.
Solomon Kullback. 1968. Information Theory and
Statistics. Dover, revised 2nd edition.
Julian Kupiec, Jan Pedersen, and Francine Chen. 1995.
A trainable document summarizer. In Proceedings
of the 18th Annual International ACM SIGIR Confer-
ence on Research and Development in Information Re-
trieval, SIGIR ?95, pages 68?73, Seattle, July. ACM.
Martin Kurze. 1995. Giving blind people access
to graphics (example: Business graphics). In Pro-
ceedings of the Software-Ergonomie ?95 Workshop
on Nicht-visuelle graphische Benutzungsoberfla?chen
(Non-visual Graphical User Interfaces), Darmstadt,
Germany, February.
Xiaoyong Liu and W. Bruce Croft. 2002. Passage re-
trieval based on language models. In Proceedings of
the eleventh international conference on Information
and knowledge management, CIKM ?02, pages 375?
382.
Daniel C. Marcu. 1997. The Rhetorical Parsing, Summa-
rization, and Generation of Natural Language Texts.
Ph.D. thesis, University of Toronto, December.
David D. McDonald and Charles F. Greenbacker. 2010.
?If you?ve heard it, you can say it? - towards an ac-
count of expressibility. In Proceedings of the 6th In-
ternational Natural Language Generation Conference,
INLG 2010, pages 185?190, Trim, Ireland, July. ACL.
David D. McDonald. 1992. An efficient chart-based
algorithm for partial-parsing of unrestricted texts. In
Proceedings of the 3rd Conference on Applied Natural
Language Processing, pages 193?200, Trento, March.
ACL.
David D. McDonald. 2000a. Issues in the repre-
sentation of real texts: the design of KRISP. In
Lucja M. Iwan?ska and Stuart C. Shapiro, editors, Nat-
ural Language Processing and Knowledge Represen-
tation, pages 77?110. MIT Press, Cambridge, MA.
David D. McDonald. 2000b. Partially saturated refer-
ents as a source of complexity in semantic interpreta-
tion. In Proceedings of the NAACL-ANLP 2000 Work-
shop on Syntactic and Semantic Complexity in Natural
61
Language Processing Systems, pages 51?58, Seattle,
April. ACL.
Peter B.L. Meijer. 1992. An experimental system for
auditory image representations. IEEE Transactions on
Biomedical Engineering, 39(2):112?121, February.
Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry
Winograd. 1999. The pagerank citation ranking:
Bringing order to the web. Technical Report 1999-
66, Stanford InfoLab, November. Previous number:
SIDL-WP-1999-0120.
Ulrich Reimer and Udo Hahn. 1988. Text condensation
as knowledge base abstraction. In Proceedings of the
4th Conference on Artificial Intelligence Applications,
CAIA ?88, pages 338?344, San Diego, March. IEEE.
Dominic Widdows. 2003. Orthogonal negation in vector
spaces for modelling word-meanings and document
retrieval. In Proceedings of the 41st Annual Meeting
on Association for Computational Linguistics - Volume
1, ACL ?03, pages 136?143, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Michael J. Witbrock and Vibhu O. Mittal. 1999. Ultra-
summarization: a statistical approach to generating
highly condensed non-extractive summaries. In Pro-
ceedings of the 22nd Annual International ACM SIGIR
Conference on Research and Development in Informa-
tion Retrieval, SIGIR ?99, pages 315?316, Berkeley,
August. ACM.
Wai Yu, Douglas Reid, and Stephen Brewster. 2002.
Web-based multimodal graphs for visually impaired
people. In Proceedings of the 1st Cambridge Work-
shop on Universal Access and Assistive Technology,
CWUAAT ?02, pages 97?108, Cambridge, March.
Chengxiang Zhai. 2008. Statistical Language Models
for Information Retrieval. Morgan and Claypool Pub-
lishers, December.
62
Proceedings of the 14th European Workshop on Natural Language Generation, pages 188?192,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
A Case Study Towards Turkish Paraphrase Alignment
Seniz Demir ?Ilknur Durgar El-Kahlout Erdem Unal
TUBITAK-BILGEM
Gebze, Kocaeli, TURKEY
{seniz.demir,ilknur.durgar,erdem.unal}@tubitak.gov.tr
Abstract
Paraphrasing is expressing the same se-
mantic content using different linguistic
means. Although previous work has ad-
dressed linguistic variations at different
levels of language, paraphrasing in Turk-
ish has not been yet thoroughly studied.
This paper presents the first study towards
Turkish paraphrase alignment. We per-
form an analysis of different types of para-
phrases on a modest Turkish paraphrase
corpus and present preliminary results on
that analysis from different standpoints.
We also explore the impact of human in-
terpretation of paraphrasing on the align-
ment of paraphrase sentence pairs.
1 Introduction
Paraphrases are alternative linguistic expressions
that convey the same content. Natural languages
allow linguistic variations at different levels (e.g.,
lexical and phrasal) and a change at a level of lan-
guage may trigger other changes at different lev-
els. Paraphrasing has attracted a growing inter-
est from the research community in a broad range
of tasks such as language generation (Power and
Scott, 2005), machine translation (Callison-Burch
et al, 2006), and question answering (France
et al, 2003). Moreover, research on acquisi-
tion (Max et al, 2012), generation (Zhao et al,
2010), and recognition (Qiu et al, 2006) of para-
phrases has been on the rise for the last decade.
Paraphrasing is also an increasingly studied prob-
lem by the generation community. One particular
text-to-text generation problem being addressed is
the generation of sentence-level paraphrases by
converting a sentence into a new one with approx-
imately the same meaning (Wubben et al, 2010).
One aspect of paraphrasing is the specifica-
tion of paraphrase types via a typology. Building
paraphrase typologies from different perspectives
(e.g., linguistics analysis and discourse analysis)
has been an active research area for a number of
years now (Vila et al, 2011). In particular, lin-
guistic grounds govern the typologies built by lan-
guage processing systems (Kozlowski et al, 2003)
which are often very generic or system specific.
Research on paraphrase alignment focuses on
identifying links between semantically related
word strings. Such monolingual alignments can
be later used as training data for several nat-
ural language processing approaches (e.g., tex-
tual entailment and multidocument summariza-
tion) (Thadani et al, 2012). Although a wealth
amount of research has studied various problems
related to Turkish, we here focus on a problem
which has not been studied earlier. We present our
initial explorations on Turkish paraphrase align-
ment by considering how alignment is affected by
human interpretation of paraphrasing. We con-
ducted a study on a modest corpus from four dif-
ferent sources to investigate answers to the follow-
ing questions: i) What are the types of paraphrases
that can be observed at different levels of Turk-
ish? ii) Do humans agree on the existence of para-
phrasing between Turkish paraphrase sentences?
iii) How does human interpretation of paraphras-
ing affect the alignment of paraphrase sentences?
Our study is unique in that it presents a generic
typology of paraphrase types found in our Turkish
paraphrase corpus and discusses the agreement of
human annotators on the identification and clas-
sification of observed correspondences between
paraphrases. This study also presents our aggre-
gated observations on the relation between inter-
pretation and alignment of paraphrase casts.
188
Figure 1: Sentence similarity
scores of the corpus.
Literary News Subtitles Parallel
Text Articles Corpus
# Tokens 1879 3379 1632 1581
# Unique Tokens 811 1473 824 609
# Shared Tokens 519 1125 402 354
Lexical Overlap 72.5 82.9 63.2 62.7
Lexical Overlap 68.4 67.2 48.6 45
(lem. cont. words)
Table 1: Characteristics of the selected 400 pairs.
2 Paraphrase Corpora
The Turkish paraphrase corpus (Demir et al,
2012) comprises 1270 paraphrase pairs from four
different sources: i) translations of a literary
text, ii) multiple reference translations of En-
glish tourism-related sentences, iii) news arti-
cles, and iv) subtitles of a movie. We mea-
sured sentence similarities of all paraphrase pairs
from each domain via three measures typically
used in statistical machine translation evaluations:
TER (Matthew Snover, 2006), BLEU (Papineni
et al, 2002), and METEOR (Lavie and Agar-
wal, 2007). As shown in Figure 1, the order-
ing of domains with respect to all metrics are the
same where the pairs from the news domain and
those from the parallel corpus are the most and the
least similar pairs respectively. Since there are di-
vergences across different domains, we randomly
drew from each domain an equal number of sen-
tences (i.e., 100 paraphrase pairs1). Some charac-
teristic features of the paraphrase pairs selected for
this study are shown in Table 1.
3 Paraphrase Typology
To our best knowledge, a Turkish paraphrase
typology that we can apply to this study does
not yet exist in the literature. On the other hand,
building a comprehensive typology is not one of
our objectives. There are a number of available
typologies built for English (Dras, 1999; Vila
et al, 2011). Since our focus in this work is on
characterizing paraphrasing at different levels of
language, we greatly drew from the linguistically-
motivated typology by Vila et al (2011) while
building our generic typology. We examined the
selected 400 paraphrase pairs and constructed a
typology that covers all paraphrases occurring
within these pairs. Our typology covers three
levels of language and consists of four classes.
1The number of paraphrase pairs in the subtitle domain
limits the study to 100 pairs from each domain.
The lexical class covers all changes that arise
from exchanging words within a phrase with
other words and includes four subclasses (i.e.,
substitution, substitution with opposite polarity,
deletion, and pronominalization)2 :
(1) ?Su bize takip edebileceg?imiz hic?bir1 iz1
b?rakm?yor1.? (Water leaves1 no1 trace1 that we
can follow.)
(2) ?Su olay?n takip edilebilecek bu?tu?n1 izlerini1
yok1 ediyor1.? (The water destroys1 all1 traces1
of the event that can be followed.)
The morphological class covers inflectional
and derivational changes within words and in-
cludes two subclasses (i.e., inflectional changes
and derivational changes):
(1) ?Bo?yle bir ilac? almaktansa hasta1 kalmak1
iyidir.? (Staying1 sick1 is better than taking such a
drug.)
(2) ?Hasta1 kal?r?m1 da yine de bu ilac? ic?mem.?(I1
stay1 sick1 still I don?t take this drug.)
The phrasal class includes changes that arise
from exchanging fragments with same meaning:
1) ?Bunlar? biliyorum fakat emri ben1
vermedim1.? (I know all that, but I1 did1
not1 give1 the order.)
(2) ?Bunlar? biliyorum ama, emri veren1 ben1
deg?ilim1.? (I know all that, but I?m1 not1 the1
one1 who1 gave1 the order.)
The other class is for all other changes that
imply different lexicalizations for the same
contextual meaning:
(1) ?Savas? c??k?nca pek c?ok c?ingene eskilerdeki
gibi ko?tu?1 kis?iler1 oldular1.? (When war broke
out, many gypsies became1 just1 as bad1 people1
as those of the past.)
(2) ?Savas?ta birc?ok c?ingene eskiden oldug?u gibi
yine c?ok1 ko?tu?lu?k1 yapt?lar1.? (Many gypsies did1
much1 evil1 in the war again as in the past.)
2Each word in a paraphrase cast receives the same sub-
script.
189
Although these classes are language indepen-
dent, they include several Turkish specific as-
pects such as morphophonemic processes. For in-
stance, Turkish word changes due to vowel har-
mony, vowel drops, and consonant drops/changes
are all covered by the morphological class.
4 Paraphrase Alignment
While manually aligning the paraphrase sentence
pairs, our goal was to jointly identify the para-
phrase casts (i.e., the substitutable word strings)
and specify the types correspondences between
them. We asked three native speakers to align the
selected paraphrase sentences by aligning word
strings3 as much as possible and marking the
strength of observed correspondences as either
?certain? (the correspondences that hold in any
context) or ?possible? (the correspondences that
are context-specific). The annotators were also
told to assign each identified correspondence be-
tween paraphrase casts to one of the classes in our
typology. In cases where the same word strings
were aligned, the correspondence was not classi-
fied with a class from the typology. Before align-
ing the corpus, the annotators were trained on a
different set of paraphrases using an annotation
guideline. Table 2 reports some statistics of the
alignment process. The column labelled as ?Com-
mon? represents the alignments common to all an-
notators. The rows labelled as ?C?, ?P?, and ?U?
represent the number of certain and possible align-
ments, and the number of unaligned words respec-
tively4. It is noteworthy that the percentage of
common certain alignments is significantly higher
than the percentage of common possible align-
ments in all domains.
5 Corpus Study Findings
In this study, we aim to explore whether hu-
mans agree on the existence (i.e., identifying two
word strings as paraphrases) and type of para-
phrasing between Turkish paraphrase sentences.
We are also interested in how the alignment of
paraphrase casts is affected from human inter-
pretation of paraphrasing between Turkish para-
3A word string consists of one or more words which may
not be contiguous. Two word strings are aligned when one or
more words in one string are paired with one or more words
in the other string.
4Please note that these scores represent all alignments in-
cluding the alignments of the same word strings.
Domain Ant.1 Ant. 2 Ant. 3 Common
Literary C 647 639 578 376 (58%)
Text P 88 121 178 10 (5.62%)
U 165 140 144 101 (61.2%)
News C 1384 1330 1259 988 (71.4%)
Articles P 53 186 214 3 (1.4%)
U 203 124 167 102 (50.2%)
Subtitles C 578 546 530 306 (52.9%)
P 101 112 119 13 (10.9%)
U 104 122 131 71 (54.2%)
Parallel C 565 531 542 313 (55.4%)
Corpus P 109 126 70 6 (4.8%)
U 112 129 174 80 (45.9%)
Table 2: Alignment statistics of paraphrase pairs.
phrase sentences. Please note that the alignment
of paraphrase casts consequently affects the sen-
tence alignment of paraphrase sentence pairs.
Our analysis started with examining how often
our annotators agreed on identifying paraphrasing
between two word strings. The agreement scores
in Table 3 show that the annotators (pairwise) had
a reasonable level of agreement in all domains. In
majority of these cases, the annotators also agreed
on the strength of the correspondence (i.e., both
annotators either classified the correspondence as
?Certain-Certain? or ?Possible-Possible?).
Domain Ant.1&2 Ant. 1&3 Ant. 2&3
Literary Text 0.78% 0.73% 0.77%
News Articles 0.81% 0.86% 0.81%
Subtitles 0.68% 0.72% 0.86%
Parallel Corpus 0.59% 0.61% 0.78%
Table 3: Agreement on paraphrase identification.
The agreement scores in Table 3 show the
agreement of annotators on the fact that two word
strings are paraphrases and thus should be aligned.
But it does not mean that the reason behind simi-
lar identifications is the same. We thus explored
whether the annotators similarly classified the
word strings that they identified as paraphrases. In
all domains, the agreement scores between the an-
notators (given in Table 4) are dramatically lower
than the scores in Table 3. It is particularly
noteworthy that the smallest drop is observed in
the parallel corpus domain (the domain that con-
tains the least similar sentences). In cases where
the annotators (pairwise) classified the same word
strings with the same paraphrase class, they had
a high agreement (between 78% and 91%) on the
strength of the correspondence in all domains. We
also computed the inter-annotator agreement via
Kappa (Cohen, 1960). Kappa scores (shown bold
in Table 4) represent fair to good agreement be-
190
(a) Annotator 1 (b) Annotator 2 (c) Annotator 3
Figure 2: Distribution of paraphrase classes across domains.
tween the annotators.
Domain Ant.1&2 Ant. 1&3 Ant. 2&3
Literary 0.37% 0.34% 0.56%
Text (0.34) (0.33) (0.63)
News 0.40% 0.51% 0.54%
Articles (0.38) (0.48) (0.53)
Subtitles 0.37% 0.37% 0.70%
(0.41) (0.38) (0.72)
Parallel 0.33% 0.35% 0.64%
Corpus (0.46) (0.46) (0.75)
Table 4: Agreement on paraphrase classes.
Figure 2 presents the distribution of paraphrase
classes identified by each annotator across differ-
ent domains. Notably, the identified paraphrase
classes between word strings appear to diverge in
several respects. We are currently exploring the
reason behind this poor annotator agreement on
paraphrase classes. One possible reason might be
different understanding of the typology.
As a second step, we explored the impact of
different interpretations of paraphrasing between
sentence pairs on the alignment of these sen-
tences. We analyzed the alignment differences of
sentences and classified them into four classes:
- Different Classification: Although both
annotators identify the same correspondence
between two word strings, they classify that
correspondence differently.
- Missing Alignment: One annotator identifies an
alignment between two word strings but the other
annotator does not identify a correspondence
between these word strings.
- Missing Word: The annotators identify a
correspondence of the same paraphrase class
between two word strings which differ only in one
word.
- Different Grouping: Two word strings are
identified as having a single correspondence
by one annotator whereas a number of disjoint
correspondences between these word strings are
identified by the other annotator.
All these differences except those classified as
?different classification? result in different align-
ments between word strings. Such different align-
ments of paraphrase casts then change the align-
ment of paraphrase sentences.
6 Conclusion and Future Work
In this paper, we present our initial explorations
on Turkish paraphrase alignment by exploiting a
modest corpus. We built a generic and linguisti-
cally grounded Turkish paraphrase typology that
covers the types of paraphrases observed in the
corpus. In the study, the paraphrases identified
by human annotators were aligned and annotated
with paraphrase classes from the typology. The
agreement of the annotators with respect to the ex-
istence and alignment of paraphrases as well as
the associated paraphrase classes were reported.
The study showed that the way how humans in-
terpret paraphrasing between Turkish paraphrase
sentences has an impact on how they align these
sentences.
We have two main directions for future re-
search: i) conducting a larger corpus study for
drawing generalizations about Turkish paraphras-
ing and enhancing the typology if necessary, and
ii) building Turkish paraphrase applications (e.g.,
automatic paraphrase acquisition) in correlation
with the collected insights. We believe that the
current findings for Turkish paraphrase alignment
and our corpus enriched with paraphrase types
enable future research on paraphrase phenomena
in different fields such as language generation,
textual entailment, summarization, and machine
translation to be empirically assessed.
191
References
Chris Callison-Burch, Philipp Koehn, and Miles Os-
borne. 2006. Improved statistical machine transla-
tion using paraphrases. In Proceedings of the main
conference on Human Language Technology Con-
ference of the North American Chapter of the Asso-
ciation of Computational Linguistics, HLT-NAACL
?06, pages 17?24.
Jacob Cohen. 1960. A coefficient of agreement
for nominal scales. Educational and Psychological
Measurement, 20(1):37?46.
Seniz Demir, Ilknur Durgar El-Kahlout, Erdem Unal,
and Hamza Kaya. 2012. Turkish paraphrase corpus.
In Language Resources and Evaluation Conference
- LREC.
Mark Dras. 1999. Tree Adjoining Grammar and the
Reluctant Paraphrasing of Text. Ph.D. thesis, Mac-
quarie University.
Florence Duclaye France, Franois Yvon, and Olivier
Collin. 2003. Learning paraphrases to improve
a question-answering system. In EACL Workshop
NLP for Question-Answering.
Raymond Kozlowski, Kathleen F. McCoy, and
K. Vijay-Shanker. 2003. Generation of single-
sentence paraphrases from predicate/argument
structure using lexico-grammatical resources. In
Proceedings of the second international workshop
on Paraphrasing - Volume 16, PARAPHRASE ?03,
pages 1?8.
Alon Lavie and Abhaya Agarwal. 2007. Meteor: an
automatic metric for mt evaluation with high levels
of correlation with human judgments. In Proceed-
ings of the Second Workshop on Statistical Machine
Translation, StatMT ?07, pages 228?231.
Richard Schwartz. Matthew Snover, Bonnie J. Dorr.
2006. A study of translation edit rate with targeted
human annotation. In Proceedings of AMTA.
Aure?lien Max, Houda Bouamor, and Anne Vilnat.
2012. Generalizing sub-sentential paraphrase acqui-
sition across original signal type of text pairs. In
EMNLP-CoNLL, pages 721?731.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting on Association for Computa-
tional Linguistics, ACL ?02, pages 311?318.
Richard Power and Donia Scott. 2005. Automatic gen-
eration of large-scale paraphrases. In IWP.
Long Qiu, Min-Yen Kan, and Tat-Seng Chua. 2006.
Paraphrase recognition via dissimilarity significance
classification. In Proceedings of the 2006 Confer-
ence on Empirical Methods in Natural Language
Processing, EMNLP ?06, pages 18?26.
Kapil Thadani, Scott Martin, and Michael White.
2012. A joint phrasal and dependency model for
paraphrase alignment. In COLING, pages 1229?
1238.
Marta Vila, M. Antonia Marti, and Horacio Rodriguez.
2011. Paraphrase concept and typology: A linguisti-
cally based and computationally oriented approach.
Procesamiento del Lenguaje Natural, 46:83?90.
Sander Wubben, Antal van den Bosch, and Emiel
Krahmer. 2010. Paraphrase generation as mono-
lingual translation: data and evaluation. In Proceed-
ings of the 6th International Natural Language Gen-
eration Conference, INLG ?10, pages 203?207.
Shiqi Zhao, Haifeng Wang, Xiang Lan, and Ting Liu.
2010. Leveraging multiple mt engines for para-
phrase generation. In Proceedings of the 23rd Inter-
national Conference on Computational Linguistics,
COLING ?10, pages 1326?1334.
192
Proceedings of the 8th International Natural Language Generation Conference, pages 128?132,
Philadelphia, Pennsylvania, 19-21 June 2014. c?2014 Association for Computational Linguistics
Generating Valence Shifted Turkish Sentences
Seniz Demir
TUBITAK-BILGEM
Gebze, Kocaeli, TURKEY
seniz.demir@tubitak.gov.tr
Abstract
Valence shifting is the task of rewrit-
ing a text towards more/less positively or
negatively slanted versions. This paper
presents a rule-based approach to produc-
ing Turkish sentences with varying senti-
ment. The approach utilizes semantic rela-
tions in the Turkish and English WordNets
to determine word polarities and involves
the use of lexical substitution and adver-
bial rules to alter the sentiment of a text in
the intended direction. In a user study, the
effectiveness of the generation approach is
evaluated on real product reviews.
1 Introduction
Language can express a content in a number of
different ways with varying emotion. Emotions
might equip sentences with connotations and have
a powerful effect on the disposition of the hearer in
a subtle way. Moreover, emotions induced through
words play an important role in written and verbal
communication. Sentence valence specifies the
degree of emotion present in a sentence and indi-
cates how positive or negative the sentence is. The
literature has shown that the sentiment characteris-
tics of a sentence are correlated with the valence of
words the sentence contains (Guerini et al., 2008).
Valence shifting is the task of altering a text to-
wards more/less positively or negatively slanted
versions while keeping much of its semantic
meaning (Gardiner and Dras, 2012). This rela-
tively new problem has many practical uses in lan-
guage based applications such as persuasive sys-
tems which are designed to influence users? behav-
iors. Slanting of texts can be achieved in a num-
ber of ways, the most popular of which is the lexi-
cal substitution of semantically related words with
varying valences (Whitehead and Cavedon, 2010).
To our knowledge, this work is the first to examine
the correlation between word polarities and sen-
tence valences in Turkish and to address the prob-
lem of valence shifting in Turkish sentences. Our
methodology for determining word polarities ex-
plores the semantic relations of words within and
between the Turkish and English WordNets. To
alter the sentiment carried by a sentence in the in-
tended direction, our approach utilizes word polar-
ities and a number of hand constructed rules based
on the insights gained from user studies. Two
strategies, namely lexical substitution and the use
of intensifiers/downtoners, are used to slant Turk-
ish texts. An evaluation study shows the effective-
ness of our approach in generating valence shifted
Turkish sentences.
2 Word Polarity
Word polarity (valence) stands for the semantic
orientation of a word and is one of positive, neg-
ative or neutral. Previous research has shown that
it is very common to retrieve word valences from
existing polarity lexicons. To our best knowledge,
there is only one available Turkish word polarity
lexicon (Tr L) which is built in a semi-automated
manner by traversing a multilingual word relat-
edness graph with a random walk model (O?zsert
and O?zgu?r, 2013). The lexicon consists of 1398
positive (e.g., ?o?vgu?#n? (praise#n)) and 1414 neg-
ative (e.g., ?anormal#a? (abnormal#a)) word en-
tries. Although all word entries are given along
with their PoS (i.e., one of noun, verb, or adjec-
tive), the lexicon neither contains word senses nor
the strength of polarities.
There are a number of available English polar-
ity lexicons. The General Inquirer lexicon (GI L)
annotates word entries with syntactic, semantic,
and pragmatic information including its sense and
PoS (Stone et al., 1966). In the MPQA Polarity
lexicon (MPQA L), word entries are annotated
with PoS, polarity, and the strength of polarity
(i.e., strong or weak) but no sense information is
128
Polarity Agreement with Tr L GI L MPQA L SWN L En L
Positive polarity match 646 468 423 950
Negative polarity match 761 775 742 1339
No Turkish polarity & Positive English polarity 326 376 750 1177
No Turkish polarity & Negative English polarity 373 577 1019 1390
Table 1: The agreement of word polarities.
given (MPQA, 2014). The SentiWordNet lexi-
con (SWN L), along with PoS and sense informa-
tion, annotates word entries with three sentiment
scores from positivity, negativity, and objectivity
perspectives (Esuli and Sebastiani, 2006)1.
Due to the limitations of the Turkish lexicon
(e.g., no adverbs exist in the lexicon), we explored
ways of expanding the scope of the lexicon by tak-
ing advantage of the semantic relations between
words. As described in the rest of this section,
we also examined how additional polarity infor-
mation can be retrieved from English polarity lex-
icons and applied to Turkish.
2.1 Bilingual WordNet Graph
The Turkish WordNet is fully compatible with but
not as comprehensive as some other WordNets
such as EuroWordNet. We represent the Turk-
ish WordNet as a directed graph where each ver-
tex corresponds to a word tagged with the sense
and PoS information (e.g., mekan#1,n2). The ver-
tices corresponding to the words that share a re-
lation are connected with a directed edge and the
edge is labeled with the kind of this relation (e.g.,
?synonym? or ?antonym?). A monolingual graph
consisting of 20343 vertices and 60164 edges is
built from the Turkish WordNet. Following the
same representation scheme, a monolingual graph
is built from the English WordNet 2.0 which con-
tains 177308 vertices and 786932 edges.
The Turkish and English monolingual graphs
are integrated into a big bilingual graph with the
use of the InterLingual Index (ILI) where words
having the same meaning are connected. ILI facil-
itates the mapping of concepts and similar synsets
between compatible WordNets. This integration
enabled us to explore the agreement of word polar-
ities between the Turkish polarity lexicon and each
of the three English polarity lexicons. The first and
the second rows in Table 1 show the number of
cases where a Turkish-English word pair with the
same ILI share a positive or a negative polarity re-
1Here, we classify a word as positive/negative if its posi-
tivity/negativity score is greater than or equal to 0.5.
2The noun mekan (location) is of the first sense.
spectively. The third and the fourth rows represent
the cases where a Turkish word does not have a
polarity in the Turkish lexicon whereas its English
correspondent has a positive or a negative polarity
in the English lexicon respectively. For instance,
the word ?bitmek bilmemek#a? does not have a
polarity in Tr L whereas its English correspondent
?endless#a? has a negative polarity in MPQA L.
We examined whether individual agreements
between the Turkish lexicon and each English lex-
icon can be improved by merging all English lex-
icons into a single polarity lexicon (En L). Dur-
ing this merge, words that have different polarities
in individual lexicons are omitted and the words
from MPQA L are considered as of the first sense.
The final En L lexicon consists of 9044 positive
and 13890 negative words with PoS and sense in-
formation. As shown in the fourth column of Ta-
ble 1, this merge improves the agreement between
the Turkish and English polarity lexicons.
2.2 Detecting Word Polarity
A two-step approach is developed for determin-
ing the polarities of Turkish words. Once given a
sentence, this approach first identifies prior word
polarities by utilizing the information contained in
polarity lexicons and then applies a set of polarity
alteration rules to the sentence for finalizing polar-
ity assignments.
To determine the polarity of a word, the pres-
ence of the word and its synonyms is first explored
in the Turkish polarity lexicon. If neither the word
nor any of its synonyms exists in the Tr L lexi-
con, English words that have the same ILI with the
Turkish word are explored in the English polarity
lexicon En L3. If the word polarity is still not de-
termined, the polarity of Turkish words that share
the antonym relation with the word is explored in
Tr L and the reverse of the retrieved word polarity
(if any) is taken. If the use of antonym relation in
Tr L does not return a polarity, the antonym rela-
tion is explored in the En L lexicon for the English
correspondents of the Turkish word.
3This enables us to benefit from English polarities shown
in the third and the fourth rows of Table 1.
129
We hand constructed a number of polarity al-
teration rules that are specific to Turkish. These
rules, once applied to a sentence, might alter the
lexicon-based prior polarity of words. For exam-
ple, the adjective ?mutsuz (unhappy)? with nega-
tive polarity according to the Tr L lexicon should
be treated as positive in the sentence ?Ahmet mut-
suz bir c?ocuk deg?il. (Ahmet is not an unhappy
child.)? since it is followed by the negative ?deg?il
(it is not)?. One of our polarity alteration rules re-
verses the polarity of all words that immediately
precede the negative ?deg?il? in a sentence4.
3 Sentence Valence
Our goal is to alter the sentiment of a Turkish sen-
tence while preserving its content. This requires
a means of assessing the sentiment change in
the slanted versions of a sentence and beforehand
computing their sentence valences. Literature has
proposed different methods to calculate sentence
valence using word polarities such as summing va-
lence scores of all words in a sentence (Inkpen et
al., 2004) or using the valence score of a present
word with a strong valence. We first examined
whether computing sentence valence by summing
word polarities, a commonly used approach in En-
glish, is applicable to Turkish.
We conducted a formal experiment with 24 par-
ticipants, all of which are Turkish native speak-
ers. The participants were presented with 20
sentences and asked to classify each sentence as
positive, negative, or neutral based on its con-
tent. The sentences, originally published in aca-
demic proses or newspapers, were manually se-
lected from the Turkish National Corpus (Aksan
et al., 2012). A strong attention was paid to se-
lect sentences that contain at least one word within
the Tr L lexicon. The valences of these sen-
tences were computed by summing the word po-
larities as determined by our polarity detection ap-
proach5. Unfortunately, this straightforward ap-
proach failed to classify sentences as participants
did in 13 sentences. The classifications of our
approach and the participants in these cases are;
positive-neutral in 6 sentences, negative-neutral in
4Evaluating the reliability of our polarity detection ap-
proach and how well the polarity assignments coincide with
human judgements is in our future work.
5The word polarity is +1 and -1 for positive and negative
words respectively. A sentence is considered as positive if the
sentence valence score>0 and as negative if the sentence va-
lence score<0. In each sentence, less than half of the content
words are annotated with a polarity.
3 sentences, neutral-negative in 2 sentences, and
positive-negative in the remaining 2 sentences.
For example, our approach classified the sentence
?Bir simulasyon modelinin amac? bir problemi
c?o?zmek ya da c?o?zu?mu?ne katk?da bulunmakt?r.
(The purpose of a simulation model is to solve a
problem or to contribute to its solution.)? with a
valence of +1 as positive, whereas the participants
classified it as neutral.
One reason for the divergence in classifications
might be the fact that our approach considers all
words in the Turkish lexicon as having the same
strength and of the first sense although senses are
not given in the lexicon. Since this study revealed
that sentence valences determined in this fashion
do not correspond with valences as assigned by
humans, we argue that slanting of texts cannot be
assessed by considering only sentence valences.
4 Generating Valence Shifted Sentences
To explore how Turkish speakers alter the senti-
ment characteristics of sentences, we conducted an
experiment with 19 participants where they were
presented with 20 sentences and asked to gener-
ate slanted versions of these texts toward a more
positive or more negative direction. The sentences
along with their sentiments (i.e., positive or nega-
tive) were selected from a database of movie and
product reviews. The analysis of this experiment
provided a number of insights into Turkish valence
shifting, the three main of which are: i) slanted
versions of texts are produced via three kinds of
sentential changes (lexical substitution, paraphras-
ing, and adverbial changes that behave as inten-
sifiers/downtoners), ii) adverbs of certainty, prob-
ability, and quantity are often used in adverbial
changes, and iii) the sentence sentiment, intended
shift direction, and sentence constituents deter-
mine the kind of sentential change and where in
the sentence it occurs. In this work, we limit our-
selves to exploring how valence shifted Turkish
sentences can be generated by lexical substitution
and adverbial changes6.
Lexical substitution of semantically related
words with different polarity strengths is a pop-
ular approach in English. Since the Turkish polar-
ity lexicon does not provide polarity strengths and
our polarity detection approach assigns the same
polarity to all synonym words, substituting a word
6Generating slanted versions of Turkish texts by para-
phrasing their content is left for future work.
130
with its synonym of the same strength to slant a
text is not applicable in our case. We rather substi-
tute words with other words that share either the
?similar to? or ?also see? relation if any of the 6
lexical substitution rules that we constructed is ap-
plicable. Below are two representative rules:
? If the intended shift is to increase the sen-
tence valence, then substitute a word having
a reverse polarity with the sentence sentiment
with a word that has the same polarity with
the sentence.
? If the intended shift is to decrease the sen-
tence valence, then substitute a word having
the same polarity with the sentence sentiment
with a word of the same polarity once the po-
larity strength of the English correspondent
of the substituted word is lower than that of
the replaced word according to MPQA L.
To capture adverbial changes, we constructed
10 rules whose applicability depends on sentence
constituents. We classified all certainty, probabil-
ity, and quantity adverbs as intensifiers or down-
toners. These adverbs are either inserted, deleted,
or substituted once an adverbial rule is applied to
a sentence. In the current setting, the selection of
which adverb will be used among all other pos-
sibilities is determined by a language model and
the adverbial rules that apply to adjectives have a
precedence over those that apply to verbs. Two
representative rules are shown below:
? If the sentence contains only one adjective
which has the same polarity with the sentence
sentiment and the intended shift is to increase
the sentence valence, then insert an intensifier
in front of the adjective.
? If the denominative verb of the sentence is
derived from an adjective which has the same
polarity with the sentence sentiment and the
intended shift is to increase the sentence va-
lence, then insert an intensifier in front of the
verb.
Our approach follows a straightforward strategy
for shifting sentence valences. Once a sentence
and the shift direction are given, the lexical substi-
tution rules are applied in an order until a slanted
version of the sentence is produced in the intended
direction. If these rules do not succeed in slanting
the sentence, then the adverbial rules are applied
to the sentence.
To evaluate the effectiveness of our valence
shifting approach, we conducted an experiment
with 15 Turkish native speakers. The participants
were presented with 21 sentence pairs, where one
sentence is an original product review and the
other sentence is its slanted version as produced by
our valence shifting approach. In total, 9 adverbial
and 3 lexical substitution rules were used for gen-
erating valence shifted sentences. We asked par-
ticipants to specify which sentence in a pair has
a higher valence according to the given sentence
sentiment. Our results demonstrated that all par-
ticipants agreed that our approach achieved the in-
tended shift in 3 sentence pairs and the majority of
them agreed on that in 8 of the remaining 18 sen-
tence pairs. This evaluation study also revealed
that the adverbial rules have a higher accuracy in
shifting the sentence valence as compared to that
of lexical substitution rules. Among the tested ad-
verbial rules, the one, which modifies the adjective
of the sentence subject if the polarity of the adjec-
tive contrasts with the sentence sentiment, did not
achieve the intended valence shift. Moreover, the
performance of the lexical substitution rules was
observed to be higher in cases where the ?simi-
lar to? relation is utilized than the cases where the
?also see? relation is used. Since this initial study
left many questions unexplored regarding the ap-
plicability and accuracy of all rules that we con-
structed, a more comprehensive study is necessary
to better predict their performances.
5 Conclusion
This paper has presented our initial explorations
on Turkish sentence valence and our methodology
for generating valence shifted sentences in accor-
dance with these explorations. To our knowledge,
our work is the first to address the problem of va-
lence shifting in Turkish by considering word po-
larities. We have presented our approach for pro-
ducing slanted versions of sentences by substitut-
ing words with the use of WordNet relations and
taking advantage of Turkish intensifiers and down-
toners. We constructed a set of rules for specifying
how and when words can be substituted or inten-
sifiers/downtoners can be used to shift the valence
of a sentence in the intended direction. In the fu-
ture, we will address the task of learning polar-
ity strengths of Turkish words and the learning of
paraphrase patterns from a big collection of texts
to improve the performance of our approach.
131
References
Yesim Aksan, Mustafa Aksan, Ahmet Koltuksuz, Taner
Sezer, Umit Mersinli, Umut Ufuk, Hakan Yilmazer,
Ozlem Kurtoglu, Gulsum Atasoy, Seda Oz, and Ipek
Yildiz. 2012. Construction of the turkish national
corpus (tnc). In Proceedings of the Language Re-
sources and Evaluation Conference (LREC), pages
3223?3227.
Andrea Esuli and Fabrizio Sebastiani. 2006. Senti-
wordnet: A publicly available lexical resource for
opinion mining. In Proceedings of the Language Re-
sources and Evaluation Conference (LREC), pages
417?422.
Mary Gardiner and Mak Dras. 2012. Valence shifting:
Is it a valid task? In Proceedings of the Australian
Language Technology Association Workshop, pages
42?51.
Marco Guerini, Carlo Strapparava, and Oliviero Stock.
2008. Valentino: A tool for valence shifting
of natural language text. In Proceedings of the
Language Resources and Evaluation Conference
(LREC), pages 243?246.
Diana Zaiu Inkpen, Olga Feiguina, and Graeme Hirst.
2004. Generating more-positive or more-negative
text. In Proceedings of the AAAI Spring Symposium
on Exploring Attitude and Affect in Text: Theories
and Applications.
MPQA. 2014. Mpqa opinion corpus,
http://mpqa.cs.pitt.edu/.
Cu?neyd Murad O?zsert and Arzucan O?zgu?r. 2013.
Word polarity detection using a multilingual ap-
proach. In Proceedings of the CicLing Conference,
pages 75?82.
Philip Stone, Dexter Dunphy, Marshall Smith, and
Daniel Ogilvie. 1966. General Inquirer: A Com-
puter Approach to Content Analysis. MIT Press.
Simon Whitehead and Lawrence Cavedon. 2010. Gen-
erating shifting sentiment for a conversational agent.
In Proceedings of the NAACL HLT 2010 Workshop
on Computational Approaches to Analysis and Gen-
eration of Emotion in Text (CAAGET), pages 89?97.
132
