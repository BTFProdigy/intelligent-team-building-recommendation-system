Answering Definition Questions Using Multiple Knowledge Sources
Wesley Hildebrandt, Boris Katz, and Jimmy Lin
MIT Computer Science and Artificial Intelligence Laboratory
32 Vassar Street, Cambridge, MA 02139
{wes,boris,jimmylin}@csail.mit.edu
Abstract
Definition questions represent a largely unex-
plored area of question answering?they are
different from factoid questions in that the
goal is to return as many relevant ?nuggets?
of information about a concept as possible.
We describe a multi-strategy approach to an-
swering such questions using a database con-
structed offline with surface patterns, a Web-
based dictionary, and an off-the-shelf docu-
ment retriever. Results are presented from
component-level evaluation and from an end-
to-end evaluation of our implemented system
at the TREC 2003 Question Answering Track.
1 Introduction
To date, research in question answering has concentrated
on factoid questions such as ?Who was Abraham Lincoln
married to?? The standard strategy for answering these
questions using a textual corpus involves a combination
of information retrieval and named-entity extraction tech-
nology; see (Voorhees, 2002) for an overview. Factoid
questions, however, represent but one facet of question
answering, whose broader goal is to provide humans with
intuitive information access using natural language.
In contrast to factoid questions, the objective for ?defi-
nition? questions is to produce as many useful ?nuggets?
of information as possible. For example, the answer to
?Who is Aaron Copland?? might include the following:
American composer
wrote ballets and symphonies
born in Brooklyn, New York, in 1900
son of a Jewish immigrant
American communist
civil rights advocate
Until recently, definition questions remained a largely
unexplored area of question answering. Standard factoid
question answering technology, designed to extract sin-
gle answers, cannot be directly applied to this task. The
solution to this interesting research challenge will draw
from related fields such as information extraction, multi-
document summarization, and answer fusion.
In this paper, we present an approach to answering
definition questions that combines knowledge from three
sources. We present results from our own component
analysis and the TREC 2003 Question Answering Track.
2 Answering Definition Questions
Our first step in answering a definition question is to ex-
tract the concept for which information is being sought?
called the target term, or simply, the target. Once the tar-
get term has been found, three techniques are employed
to retrieve relevant nuggets: lookup in a database created
from the AQUAINT corpus1, lookup in a Web dictionary
followed by answer projection, and lookup directly in the
AQUAINT corpus with an IR engine. Answers from the
three different sources are then merged to produce the fi-
nal system output. The following subsections describe
each of these modules in greater detail.
2.1 Target Extraction
We have developed a simple pattern-based parser to ex-
tract the target term using regular expressions. If the nat-
ural language question does not fit any of our patterns,
the parser heuristically extracts the last sequence of capi-
talized words in the question as the target.
Our simple target extractor was tested on all definition
questions from the TREC-9 and TREC-10 QA Track test-
sets and performed with one hundred percent accuracy on
those questions. However, there were several instances
where the target term was not correctly extracted from
1official corpus used for the TREC QA Track, available from
the Linguistic Data Consortium
Name Pattern Bindings
Copular1 (e1 is) NP1 be NP2 [NP1 = t, NP2 = n]
Become2 (e1 beca) NP1 become NP2 [NP1 = t, NP2 = n]
Verb3 (e1 verb): NP1 v NP2 [where v ? biography-verb; NP1 = t, NP2 = n]
Appositive4 (e1/2 appo) NP1, NP2 [NP1 = t ? n, NP2 = t ? n]
Occupation5 (e2 occu) NP1 NP2 [where head(NP1) ? occupation; NP1 = n, NP2 = t]
Parenthesis6 (e1 pare) NP1 (NP2) [NP1 = t, NP2 = n]
Also-known-as7 (e1/2 aka) NP1, (also) known as NP2 [NP1 = t ? n, NP2 = t ? n]
Also-called8 (e2 also) NP1, (also) called NP2 [NP1 = n, NP2 = t]
Or9 (e1 or) NP1, or NP2 [NP1 = t, NP2 = n]
Like10 (e2 like) NP1 (such as|like) NP2 [NP1 = n, NP2 = t]
Relative clause11 (e1 wdt) NP (which|that) VP [NP = t, VP = n]
1In order to filter out spurious nuggets (e.g., progressive tense), our system discards nuggets that do not begin with a determiner.
2The verb become, like be, often yields good nuggets that define a target.
3By statistically analyzing a corpus of biographies of famous people, we compiled a list of verbs commonly used to describe people
and their accomplishments, such as write, invent, and make.
4Either NP1 or NP2 can be the target; thus, we index both NPs as the target term.
5NPs preceding proper nouns provide information such as occupation or affiliation. To boost precision, our system discards nuggets
that do not contain an occupation (e.g., actor, spokesman, leader). We mined this list from WordNet and the Web.
6Parenthetical expressions usually contain interesting nuggets; for persons, they often include a lifespan or job description.
7Either NP1 or NP2 can be the target; thus, we index both NPs as the target term.
8This and the previous pattern frequently identify hyponymy relations; typically, NP1 is the hypernym of NP2.
9This pattern often identifies the discourse function of elaboration.
10This pattern typically identifies an exemplification relationship, where NP2 is an instance of NP1.
11Relative clauses often provide useful nuggets.
Table 1: Description of the surface patterns used in constructing our database. (t is short for target, n for nugget)
the definition questions in TREC 2003, which made it
difficult for downstream modules to find relevant nuggets
(see Section 3.2 for a discussion).
2.2 Database Lookup
The use of surface patterns for answer extraction has
proven to be an effective strategy for factoid question
answering (Soubbotin and Soubbotin, 2001; Brill et al,
2001; Hermjakob et al, 2002). Typically, surface patterns
are applied to a candidate set of documents returned by
a document or passage retriever. Although this strategy
often suffers from low recall, it is generally not a prob-
lem for factoid questions, where only a single instance
of the answer is required. Definition questions, however,
require a system to find as many relevant nuggets as pos-
sible, making recall very important.
To boost recall, we employed an alternative strategy:
by applying the set of surface patterns offline, we were
able to ?precompile? from the AQUAINT corpus a list
of nuggets about every entity mentioned within it. In
essence, we have automatically constructed an immense
relational database containing nuggets distilled from ev-
ery article in the corpus. The task of answering defini-
tion questions then becomes a simple lookup for the rel-
evant term. This approach is similar in spirit to the work
reported by Fleischman et al (2003) and Mann (2002),
except that our system benefits from a greater variety of
patterns and answers a broader range of questions.
Our surface patterns operated both at the word and
part-of-speech level. Rudimentary chunking, such as
marking the boundaries of noun phrases, was performed
by grouping words based on their part-of-speech tags. In
total, we applied eleven surface patterns over the entire
corpus?these are detailed in Table 1, with examples in
Table 2.
Typically, surface patterns identify nuggets on the or-
der of a few words. In answering definition questions,
however, we decided to return responses that include ad-
ditional context?there is evidence that contextual in-
formation results in higher-quality answers (Lin et al,
2003). To accomplish this, all nuggets were expanded
around their center point to encompass one hundred char-
acters. We found that this technique enhances the read-
ability of the responses, because many nuggets seem odd
and out of place without context.
The results of applying our surface patterns to the en-
tire AQUAINT corpus?the target, pattern type, nugget,
and source sentence?are stored in a relational database.
To answer a definition question, the target is used to query
for all relevant nuggets in the database.
2.3 Dictionary Lookup
Another component of our system for answering
definition questions utilizes an existing Web-based
dictionary?dictionary definitions often supply knowl-
edge that can be directly exploited. Previous factoid ques-
Copular A fractal is a pattern that is irregular, but self-similar at all size scales
Become Althea Gibson became the first black tennis player to win a Wimbledon singles title
Verb Francis Scott Key wrote ?The Star-Spangled Banner?
Appositive The Aga Khan, Spiritual Leader of the Ismaili Muslims
Occupation steel magnate Andrew Carnegie
Parenthesis Alice Rivlin (director of the Office of Management and Budget)
Also-known-as special proteins, known as enzymes // amitriptyline, also known as Elavil
Also-called amino acid called phenylalanine
Or caldera, or cauldron-like cavity on the summit
Like prominent human rights leaders like Desmond Tutu
Relative clause Solar cells which currently produce less than one percent of global power supplies
Table 2: Example nuggets for each pattern. (target term in bold, textual landmark in italics, and nugget underlined)
tion answering systems have already demonstrated the
value of semistructured resources on the Web (Lin and
Katz, 2003); we believe that some of these resources can
be similarly employed to answer definition questions.
The setup of the TREC evaluations requires every an-
swer to be paired with a supporting document; therefore,
a system cannot simply return the dictionary definition
of a term as its response. To address this issue, we de-
veloped answer projection techniques to ?map? dictio-
nary definitions back onto AQUAINT documents. Simi-
lar techniques have been employed for factoid questions,
for example, in (Brill et al, 2001).
We have constructed a wrapper around the Merriam-
Webster online dictionary. To answer a question using
this technique, keywords from the target term?s dictio-
nary definition and the target itself are used as the query
to Lucene, a freely-available open-source IR engine. Our
system retrieves the top one hundred documents returned
by Lucene and tokenizes them into individual sentences,
discarding candidate sentences that do not contain the tar-
get term. The remaining sentences are scored by their
keyword overlap with the dictionary definition, weighted
by the inverse document frequency of each keyword. All
sentences with a non-zero score are retained and short-
ened to one hundred characters centered around the target
term, if necessary.
The following are two examples of results from our
dictionary lookup component:
What is the vagus nerve?
Dictionary definition: either of the 10th pair of
cranial nerves that arise from the medulla and
supply chiefly the viscera especially with auto-
nomic sensory and motor fibers
Projected answer: The vagus nerve is some-
times called the 10th cranial nerve. It runs from
the brain . . .
What is feng shui?
Dictionary definition: a Chinese geomantic
practice in which a structure or site is chosen
or configured so as to harmonize with the spir-
itual forces that inhabit it
Projected answer: In case you?ve missed the
feng shui bandwagon, it is, according to Web-
ster?s, ?a Chinese geomantic practice . . .
This strategy was inspired by query expansion
techniques often employed in document retrieval?
essentially, the dictionary definition of a term is used as
the source of expansion terms. Creative use of Web-
based resources combined with proven information re-
trieval techniques enables this component to provide high
quality responses to definition questions.
2.4 Document Lookup
If no answers are found by the previous two techniques,
as a last resort our system employs traditional document
retrieval to extract relevant nuggets. The target term is
used as a Lucene query to gather a set of one hundred can-
didate documents. These documents are tokenized into
individual sentences, and all sentences containing the tar-
get term are retained as responses (ranked by the Lucene-
generated score of the document from which they came).
These sentences are also shortened if necessary.
2.5 Answer Merging
The answer merging component of our system is re-
sponsible for integrating results from all three sources:
database lookup, dictionary lookup, and document
lookup. As previously mentioned, responses extracted
using document lookup are used only if the other two
methods returned no answers.
Redundancy presents a major challenge for integrating
knowledge from multiple sources. This problem is espe-
cially severe for nuggets stored in our database. Since
we precompiled knowledge about every entity instance
in the entire AQUAINT corpus, common nuggets are of-
ten repeated. In order to deal with this problem, we ap-
plied a simple heuristic to remove duplicate information:
if two responses share more than sixty percent of their
keywords, one of them is randomly discarded.
After duplicate removal, all responses are ordered by
the expected accuracy of the technique used to extract
the nugget. To determine this expected accuracy, we per-
formed a fine-grained evaluation for each surface pattern
as well as the dictionary lookup strategy; we discuss these
results further in Section 3.1.
Finally, the answer merging component decides how
many responses to return. Given n total responses, we
calculate the final number of responses to return as:
n if n ? 10
n +
?
n? 10 if n > 10
Having described the architecture of our system, we
proceed to present evaluation results.
3 Evaluation
In this section we present two separate evaluations of
our system. The first is a component analysis of our
database and dictionary techniques, and the second in-
volves our participation in the TREC 2003 Question An-
swering Track.
3.1 Component Evaluation
We evaluated the performance of each individual surface
pattern and the dictionary lookup technique on 160 def-
inition questions selected from the TREC-9 and TREC-
10 QA Track testsets. Since we primarily generated our
patterns by directly analyzing the corpus, these questions
can be considered a blind testset. The performance of our
surface patterns and our dictionary lookup technique is
shown in Table 3.
Overall, database lookup retrieved approximately eight
nuggets per question at an accuracy nearing 40%; dictio-
nary lookup retrieved about 1.5 nuggets per question at
an accuracy of 45%. Obviously, recall of our techniques
is extremely hard to measure directly; instead, we use the
prevalence of each pattern as a poor substitute. As shown
in Table 3, some patterns occur frequently (e.g., e1 is and
e1 appo), but others are relatively rare, such as the rela-
tive clause pattern, which yielded only six nuggets for the
entire testset.
These results represent a baseline for the performance
of each technique. Our focus was not on perfecting each
individual pattern and the dictionary matching algorithm,
but on building a complete working system. We will dis-
cuss future improvements and refinements in Section 5.
3.2 TREC 2003 Results
Our system for answering definition questions was in-
dependently and formally evaluated at the TREC 2003
Question Answering Track. For the first time, TREC
evaluated definition questions in addition to factoid and
list questions. Although our entry handled all three types
Pattern accuracy nuggets
e2 also 85.71 7
e2 aka 80.00 5
e2 occu 69.35 62
e1 or 67.74 31
e1 wdt 66.67 6
e2 like 64.60 113
e2 appo 60.00 20
e1 aka 50.00 2
e1 is 35.37 246
e1 pare 34.91 106
e1 appo 30.40 579
e1 verb 26.09 92
e1 beca 25.00 8
average 38.37 98.2
total 1277
dictionary 45.23 241
Table 3: Performance of each surface pattern and the dic-
tionary lookup technique for all 160 test questions.
Group Run F-measure
MITCSAIL03a 0.309
MIT MITCSAIL03b 0.282
MITCSAIL03c 0.282
best 0.555
Overall baseline IR 0.493
median 0.192
Table 4: Official TREC 2003 results.
of questions, we only report the results of the definition
questions here; see (Katz et al, 2003) for description of
the other components.
Overall, our system performed well, ranking eighth out
of twenty-five groups that participated (Voorhees, 2003).
Our official results for the definition sub-task are shown
in Table 4, along with overall statistics for all groups. The
formula used to calculate the F-measure is given in Fig-
ure 1. The ? value of five indicates that recall is consid-
ered five times more important than precision, an arbi-
trary value set for the purposes of the evaluation.
Nugget precision is computed based on a length al-
lowance of one hundred non-whitespace characters per
relevant response, because a pilot study demonstrated that
it was impossible for assessors to consistently enumer-
ate the total set of ?concepts? contained in a system re-
sponse (Voorhees, 2003). The assessors? nugget list (i.e.,
the ground truth) was created by considering the union
of all responses returned by all participants. All rele-
vant nuggets are divided into ?vital? and ?non-vital? cat-
egories, where vital nuggets are items of information that
Let r # of vital nuggets returned in a response
a # of non-vital nuggets returned in a response
R total # of vital nuggets in the assessors? list
l # of non-whitespace characters in the entire
answer string
Then
recall (R) = r/R
allowance (?) = 100? (r + a)
precision (P) =
{
1 if l < ?
1? l??l otherwise
Finally, the F (? = 5) = (?
2 + 1)? P ?R
?2 ? P +R
Figure 1: Official definition of F-measure.
must be in a definition for it to be considered ?good?.
Non-vital nuggets may also provide relevant information,
but a ?good? definition does not need to include them.
Nugget recall is thus only a function of vital nuggets.
The best run, with an F-measure of 0.555, was submit-
ted by BBN (Xu et al, 2003). The system used many of
the same techniques we described here, with one impor-
tant exception?they did not precompile nuggets into a
database. In their own error analysis, they cited recall as
a major cause of bad performance; this is an issue specif-
ically addressed by our approach.
Interestingly, Xu et al also reported an IR baseline
which essentially retrieved the top 1000 sentences in the
corpus that mentioned the target term (subjected to sim-
ple heuristics to remove redundant answers). This base-
line technique achieved an F-measure of 0.493, which
beat all other runs (expect for BBN?s own runs). Because
the F-measure heavily favored recall over precision, sim-
ple IR techniques worked extremely well. This issue is
discussed in Section 4.1.
To identify areas for improvement, we analyzed the
questions on which we did poorly and found that many
of the errors can be traced back to problems with target
extraction. If the target term is not correctly identified,
then all subsequent modules have little chance of provid-
ing relevant nuggets. For eight questions, our system did
not identify the correct target. The presence of stopwords
and special characters in names was not anticipated:
What is Bausch & Lomb?
Who is Vlad the Impaler?
Who is Akbar the Great?
Our naive pattern-based parser extracted Lomb, Im-
paler, and Great as the target terms for the above ques-
tions. Fortunately, because Lomb and Impaler were
rare terms, our system did manage to return relevant
nuggets. However, since Great is a very common word,
our nuggets for Akbar the Great were meaningless.
The system?s inability to parse certain names is related
to our simple assumption that the final consecutive se-
quence of capitalized words in a question is likely to be
the target. This simply turned out to be an incorrect as-
sumption, as seen in the following questions:
Who was Abraham in the Old Testament?
What is ETA in Spain?
What is Friends of the Earth?
Our parser extracted Old Testament, Spain, and Earth
as the targets for these questions, which directly resulted
in the system?s failure to return relevant nuggets.
Our target extractor also had difficulty with apposi-
tion. Given the question ?What is the medical condition
shingles??, the extractor incorrectly identified the entire
phrase medical condition shingles as the target term. Fi-
nally, our policy of ignoring articles before the target term
caused problems with the question ?What is the Hague??
Since we extracted Hague as the target term, we returned
answers about a British politician as well as the city in
Holland. Our experiences show that while target extrac-
tion seems relatively straightforward, there are instances
where a deeper linguistic understanding is necessary.
Overall, our database and dictionary lookup techniques
worked well. For six questions (out of fifty), however,
neither technique found any nuggets, and therefore our
system resorted to document lookup.
4 Evaluation Reconsidered
This section takes a closer look at the setup of the defini-
tion question evaluation at TREC 2003. In particular, we
examine three issues: the scoring metric, error inherent in
the evaluation process, and variations in judgments.
4.1 The Scoring Metric
As defined, nugget recall is only a function of the nuggets
considered ?vital?. This, however, leads to a counter-
intuitive situation where a system that returned every
non-vital nugget but no vital nuggets would receive a
score of zero. This certainly does not reflect the informa-
tion needs of a real user?even in the absence of ?vital?
information, related knowledge might still be useful to a
user. One solution might be to assign a relative weight to
distinguish vital and non-vital nuggets.
The distinction between vital and non-vital nuggets
is itself somewhat arbitrary. Consider some relevant
nuggets for the question ?What is Bausch & Lomb??:
world?s largest eye care company
about 12000 employees
in 50 countries
Run Total Relevant Recall
Nuggets Returned
official 407 118 28.99%
fixed 407 120 29.48%
Table 5: Nugget recall, disregarding the distinction be-
tween vital and non-vital nuggets.
Figure 2: F-measure as a function of ?.
approx. $1.8 billion annual revenue
based in Rochester, New York
According to the official assessment, the first four
nuggets are vital and the fifth is not. This means that
the location of Bausch & Lomb?s headquarters is consid-
ered less important than employee count and revenue. We
disagree and also believe that ?based in Rochester, New
York? is more important than ?in 50 countries?. Since
it appears that the difference between vital and non-vital
cannot be easily operationalized, there is little hope for
systems to learn and exploit this distinction.
As a reference, we decided to reevaluate our sys-
tem, ignoring the distinction between vital and non-vital
nuggets. The overall nugget recall is reported in Table 5.
We also report the nugget recall of our system after fixing
our target extractor to handle the variety of target terms
in the testset (the ?fixed? run). Unfortunately, our perfor-
mance for the fixed run did not significantly increase be-
cause the problem associated with unanticipated targets
extended beyond the target extractor. Since our surface
patterns did not handle these special entities, the database
did not contain relevant entries for those targets.
Another important issue in the evaluation concerns the
value of ?, the relative importance between precision
and recall in calculating the F-measure. The top entry
achieved an F-measure of 0.555, but the response length
averaged 2059 non-whitespace characters per question.
In contrast, our run with an F-measure of 0.309 averaged
only 620 non-whitespace characters per answer (only two
other runs in the top ten had average response lengths
lower than ours; the lowest was 338). Figure 2 shows F-
measure of our system, the top run, and the IR baseline
plotted against the value of ?. As can be seen, if precision
and recall are considered equally important (i.e., ? = 1),
the difference in performance between our system and
that of the top system is virtually indistinguishable (and
our system performs significantly better than the IR base-
line). At the level of ? = 5, it is obvious that standard IR
technology works very well. The advantages of surface
patterns, linguistic processing, answer fusion, and other
techniques become more obvious if the F-measure is not
as heavily biased towards recall.
What is the proper value of ?? As this was the first for-
mal evaluation of definition questions, the value was set
arbitrarily. However, we believe that there is no ?correct?
value of ?. Instead, the relative importance of precision
and recall varies dramatically from application to applica-
tion, depending on the user information need. A college
student writing a term paper, for example, would most
likely value recall highly, whereas the opposite would be
true for a user asking questions on a PDA. We believe that
these tradeoffs are worthy of further research.
4.2 Evaluation Error
In the TREC 2003 evaluation, we submitted three iden-
tical runs, but nevertheless received different scores for
each of the runs. This situation can be viewed as a probe
into the error margin of the evaluation?assessors are hu-
man and naturally make mistakes, and to ensure the qual-
ity of the evaluation we need to quantify this variation.
Voorhees? analysis (2003) revealed that scores for pairs of
identical runs differed by as much as 0.043 in F-measure.
For the three identical runs we submitted, there was
one nugget missed in our first run that was found in the
other two runs, ten nuggets from six questions missed
for our second run that were found in the other runs, and
ten nuggets from five questions missed in our third run.
There were also nine nuggets from seven questions that
were missed for all three runs, even though they were
clearly present in our answers.
Together over our three runs, there were 48 nuggets
from 13 questions that were clearly present in our re-
sponses but were not consistently recognized by the as-
sessors. The question affected most by these discrepan-
cies was ?Who is Alger Hiss??, for which we received an
F-measure of 0.671 in our first run, while for the second
and third runs we received a score of zero.
If the 48 missed nuggets had been recognized by the
assessors, our F-measure would be 0.327, 0.045 higher
than the score we actually received for runs b and c. This
single-point investigation is not meant to contest the rel-
ative rankings of submitted runs, but simply to demon-
strate the magnitude of the human error currently present
in the evaluation of definition questions (presumably, all
groups suffered equally from these variations).
4.3 Variations in Judgment
The answers to definition questions were judged by hu-
mans, and humans naturally have differing opinions as to
the quality of a response. These differences of opinion are
not mistakes (unlike the issues discussed in the previous
section), but legitimate variations in what assessors con-
sider to be acceptable. These variations are compounded
by the small size of the testset?only fifty questions. In
a post-evaluation analysis, Voorhees (2003) determined
that a score difference of at least 0.1 in F-measure is re-
quired in order for two evaluation results to be consid-
ered statistically different (at 95% confidence). A range
of ?0.1 around our F-measure of 0.309 could either push
our results up to fifth place or down to eleventh place.
A major source of variation is whether or not a pas-
sage matches a particular nugget in the assessor?s list (the
ground truth). Obviously, the assessors are not merely
doing a string comparison, but are instead performing a
?semantic match? of the relevant concepts involved. The
following passages were rejected as matches to the asses-
sors? nuggets:
Who is Al Sharpton?
Nugget: Harlem civil rights leader
Our answer: New York civil rights activist
Who is Ari Fleischer?
Nugget: Elizabeth Dole?s Press Secretary
Our answer: Ari Fleischer, spokesman for . . .
Elizabeth Dole
What is the medical condition shingles?
Nugget: tropical [sic] capsaicin relieves pain of
shingles
Our answer: Epilepsy drug relieves pain from
. . . shingles
Consider the nugget for Al Sharpton: although an ?ac-
tivist? may not be a ?leader?, and someone from New
York may not necessarily be from Harlem, one might ar-
gue that the two nuggets are ?close enough? to warrant a
semantic match. The same situation is true of the other
two questions. The important point here is that different
assessors may judge these nuggets differently, contribut-
ing to detectable variations in score.
Another important issue is the composition of the as-
sessors? nugget list, which serves as ?ground truth?. To
insure proper assessment, each nugget should ideally rep-
resent an ?atomic? concept?which in many cases, it
does not. Again consider the nugget for Al Sharpton; ?a
Harlem civil rights leader? includes the concepts that he
was an important civil rights figure and that he did much
of his work in Harlem. It is entirely conceivable that a
response would provide one fact but not the other. How
then should this situation be scored? As another example,
one of the nuggets for Alexander Pope is ?English poet?,
which is clearly two separate facts.
Another desirable characteristic of the assessor?s
nugget list is uniqueness?nuggets should be unique, not
only in their text but also in their meaning. In the TREC
2003 testset, three questions had exact duplicate nuggets.
Furthermore, there were also several questions for which
multiple nuggets are nearly synonymous (or are implied
by other nuggets), such as the following:
What is TB?
highly infectious lung disease
contagious respiratory disease
common communicable disease
Who is Allen Iverson?
professional basketball player
philadelphia 76 er
What is El Shaddai?
catholic charismatic group
christian organization
catholic sect
religious group
Because the nuggets overlap greatly with each other
in the concepts they denote, consistent and reproducible
evaluation results are difficult.
Another desirable property of the ground truth is com-
pleteness, or coverage of the nuggets?which we also
found to be lacking. There were many relevant items of
information returned by our runs that did not make it onto
the assessors? nugget list (even as non-vital nuggets). For
the question ?Who is Alberto Tomba??, the fact that he
is Italian was not judged to be relevant. For ?What are
fractals??, the ground truth does not contain the idea that
they can be described by simple formulas, which is one
of their most important characteristics. Some more ex-
amples are shown below:
Aga Khan is the founder and principal share-
holder of the Nation Media Group.
The vagus nerve is the sometimes known as the
10th cranial nerve.
Alexander Hamilton was an author, a general,
and a founding father.
Andrew Carnegie established a library system
in Canada.
Angela Davis taught at UC Berkeley.
This coverage issue also points to a deeper method-
ological problem with evaluating definition questions by
pooling the results of all participants. Vital nuggets may
be excluded simply because no system returned them.
Unfortunately, there is no easy way to quantify this phe-
nomenon.
Clearly, evaluating answers to definition questions is
a challenging task. Nevertheless, consistent, repeatable,
and meaningful scoring guidelines are critical to driving
the development of the field. We believe that lessons
learned from our analysis can lead to a more refined eval-
uation in the coming years.
5 Future Work
The results of our work highlight several areas for future
improvement. As mentioned earlier, target extraction is a
key, non-trivial capability critical to the success of a sys-
tem. Similarly, database lookup works only if the relevant
target terms are identified and indexed while preprocess-
ing the corpus. Both of these issues point to the need for a
more robust named-entity extractor, capable of handling
specialized names (e.g., ?Bausch & Lomb?, ?Destiny?s
Child?, ?Akbar the Great?). At the same time, the named-
entity extractor must not be confused by sentences such
as ?Raytheon & Boeing are defense contractors? or ?She
gave John the Honda for Christmas?.
Another area for improvement is the accuracy of the
surface patterns. In general, our patterns only used lo-
cal information; we expect that expanding the context on
which these patterns operate will reduce the number of
false matches. As an example, consider our e1 is pattern;
in one test, over 60% of irrelevant nuggets were cases
where the target is the object of a preposition and not
the subject of the copular verb immediately following it.
For example, this pattern matched the question ?What is
mold?? to the sentence ?tools you need to look for mold
are . . .?. If we endow our patterns with better linguis-
tic notions of constituency, we can dramatically improve
their precision. Another direction we are pursuing is the
use of machine learning techniques to learn predictors
of good nuggets, much like the work of Fleischman et
al. (2003). Separating ?good? from ?bad? nuggets fits
very naturally into a binary classification task.
6 Conclusion
In this paper, we have described a novel set of strategies
for answering definition questions from multiple sources:
a database of nuggets precompiled offline using surface
patterns, a Web-based electronic dictionary, and doc-
uments retrieved using traditional information retrieval
technology. We have also demonstrated how answers
derived using multiple strategies can be smoothly inte-
grated to produce a final set of answers. In addition, our
analyses have shown the difficulty of evaluating defini-
tion questions and inability of present metrics to accu-
rately capture the information needs of real-world users.
We believe that our research makes significant contribu-
tions toward the understanding of definition questions, a
largely unexplored area of question answering.
7 Acknowledgement
This work was supported in part by the ARDA?s Ad-
vanced Question Answering for Intelligence (AQUAINT)
Program.
References
Eric Brill, Jimmy Lin, Michele Banko, Susan Dumais,
and Andrew Ng. 2001. Data-intensive question an-
swering. In Proceedings of the Tenth Text REtrieval
Conference (TREC 2001).
Michael Fleischman, Eduard Hovy, and Abdessamad
Echihabi. 2003. Offline strategies for online ques-
tion answering: Answering questions before they are
asked. In Proceedings of the 41st Annual Meeting
of the Association for Computational Linguistics (ACL
2003).
Ulf Hermjakob, Abdessamad Echihabi, and Daniel
Marcu. 2002. Natural language based reformulation
resource and Web exploitation for question answering.
In Proceedings of the Eleventh Text REtrieval Confer-
ence (TREC 2002).
Boris Katz, Jimmy Lin, Daniel Loreto, Wesley Hilde-
brandt, Matthew Bilotti, Sue Felshin, Aaron Fernan-
des, Gregory Marton, and Federico Mora. 2003. In-
tegrating Web-based and corpus-based techniques for
question answering. In Proceedings of the Twelfth Text
REtrieval Conference (TREC 2003).
Jimmy Lin and Boris Katz. 2003. Question answering
from the Web using knowledge annotation and knowl-
edge mining techniques. In Proceedings of the Twelfth
International Conference on Information and Knowl-
edge Management (CIKM 2003).
Jimmy Lin, Dennis Quan, Vineet Sinha, Karun Bakshi,
David Huynh, Boris Katz, and David R. Karger. 2003.
What makes a good answer? The role of context in
question answering. In Proceedings of the Ninth IFIP
TC13 International Conference on Human-Computer
Interaction (INTERACT 2003).
Gideon Mann. 2002. Fine-grained proper noun ontolo-
gies for question answering. In Proceedings of the Se-
maNet?02 Workshop at COLING 2002 on Building and
Using Semantic Networks.
Martin M. Soubbotin and Sergei M. Soubbotin. 2001.
Patterns of potential answer expressions as clues to the
right answers. In Proceedings of the Tenth Text RE-
trieval Conference (TREC 2001).
Ellen M. Voorhees. 2002. Overview of the TREC
2002 question answering track. In Proceedings of the
Eleventh Text REtrieval Conference (TREC 2002).
Ellen M. Voorhees. 2003. Overview of the TREC
2003 question answering track. In Proceedings of the
Twelfth Text REtrieval Conference (TREC 2003).
Jinxi Xu, Ana Licuanan, and Ralph Weischedel. 2003.
TREC2003 QA at BBN: Answering definitional ques-
tions. In Proceedings of the Twelfth Text REtrieval
Conference (TREC 2003).
A Computational Framework for Non-Lexicalist Semantics
Jimmy Lin
MIT Computer Science and Artificial Intelligence Laboratory
Cambridge, MA 02139
jimmylin@csail.mit.edu
Abstract
Under a lexicalist approach to semantics, a verb
completely encodes its syntactic and semantic
structures, along with the relevant syntax-to-
semantics mapping; polysemy is typically at-
tributed to the existence of different lexical en-
tries. A lexicon organized in this fashion con-
tains much redundant information and is un-
able to capture cross-categorial morphological
derivations. The solution is to spread the ?se-
mantic load? of lexical entries to other mor-
phemes not typically taken to bear semantic
content. This approach follows current trends
in linguistic theory, and more perspicuously ac-
counts for alternations in argument structure.
I demonstrate how such a framework can be
computationally realized with a feature-based,
agenda-driven chart parser for the Minimalist
Program.
1 Introduction
The understanding of natural language text includes not
only analysis of syntactic structure, but also of semantic
content. Due to advances in statistical syntactic parsing
techniques (Collins, 1997; Charniak, 2001), attention has
recently shifted towards the harder question of analyzing
the meaning of natural language sentences.
A common lexical semantic representation in the com-
putational linguistics literature is a frame-based model
where syntactic arguments are associated with various se-
mantic roles (essentially frame slots). Verbs are viewed
as simple predicates over their arguments. This approach
has its roots in Fillmore?s Case Grammar (1968), and
serves as the foundation for two current large-scale se-
mantic annotation projects: FrameNet (Baker et al, 1998)
and PropBank (Kingsbury et al, 2002).
Underlying the semantic roles approach is a lexical-
ist assumption, that is, each verb?s lexical entry com-
pletely encodes (more formally, projects) its syntactic and
semantic structures. Alternations in argument structure
are usually attributed to multiple lexical entries (i.e., verb
senses). Under the lexicalist approach, the semantics of
the verb break might look something like this:
(1) break(agent, theme)
agent: subject theme: object
break(agent, theme, instrument)
agent: subject theme: object
instrument: oblique(with)
break(theme)
theme: subject
. . .
The lexicon explicitly specifies the different subcate-
gorization frames of a verb, e.g., the causative frame, the
causative instrumental frame, the inchoative frame, etc.
The major drawback of this approach, however, is the
tremendous amount of redundancy in the lexicon?for
example, the class of prototypical transitive verbs where
the agent appears as the subject and the theme as the di-
rect object must all duplicate this pattern.
The typical solution to the redundancy problem is
to group verbs according to their argument realization
patterns (Levin, 1993), possibly arranged in an inheri-
tance hierarchy. The argument structure and syntax-to-
semantics mapping would then only need to be specified
once for each verb class. In addition, lexical rules could
be formulated to derive certain alternations from more ba-
sic forms.
Nevertheless, the lexicalist approach does not capture
productive morphological processes that pervade natu-
ral language, for example, flat.V ? flatten.ADJ or ham-
mer.N ? hammer.V; most frameworks for computational
semantics fail to capture the deeper derivational relation-
ship between morphologically-related terms. For lan-
guages with rich derivational morphology, this problem
is often critical: the standard architectural view of mor-
phological analysis as a preprocessor presents difficulties
in handling semantically meaningful affixes.
In this paper, I present a computational implementation
of Distributed Morphology (Halle and Marantz, 1993), a
non-lexicalist linguistic theory that erases the distinction
between syntactic derivation and morphological deriva-
tion. This framework leads to finer-grained semantics ca-
pable of better capturing linguistic generalizations.
2 Event Structure
It has previously been argued that representations based
on a fixed collection of semantic roles cannot adequately
capture natural language semantics. The actual inventory
of semantic roles, along with precise definitions and di-
agnostics, remains an unsolved problem; see (Levin and
Rappaport Hovav, 1996). Fixed roles are too coarse-
grained to account for certain semantic distinctions?the
only recourse, to expand the inventory of roles, comes
with the price of increased complexity, e.g., in the syntax-
to-semantics mapping.
There is a general consensus among theoretical lin-
guists that the proper representation of verbal argument
structure is event structure?representations grounded in
a theory of events that decompose semantic roles in
terms of primitive predicates representing concepts such
as causality and inchoativity (Dowty, 1979; Jackendoff,
1983; Pustejovsky, 1991b; Rappaport Hovav and Levin,
1998). Consider the following example:
(2) He sweeps the floor clean.
[ [ DO(he, sweeps(the floor)) ] CAUSE
[ BECOME [ clean(the floor) ] ] ]
Dowty breaks the event described by (2) into two
subevents, the activity of sweeping the floor and its result,
the state of the floor being clean. A more recent approach,
advocated by Rappaport Hovav and Levin (1998), de-
scribes a basic set of event templates corresponding to
Vendler?s event classes (Vendler, 1957):
(3) a. [ x ACT<MANNER> ] (activity)
b. [ x <STATE> ] (state)
c. [ BECOME [ x <STATE> ] ] (achievement)
d. [ x CAUSE [ BECOME [ x <STATE> ] ] ]
(accomplishment)
e. [ [ x ACT<MANNER> ] CAUSE [ BECOME
[ x <STATE> ] ] ] (accomplishment)
A process called Template Augmentation allows basic
event templates to be freely ?augmented? to any other
event template. This process, for example, explains the
resultative form of surface contact verbs like sweep:
(4) a. Phil swept the floor.
[ Phil ACT<SWEEP> floor ]
b. Phil swept the floor clean.
[ [ Phil ACT<SWEEP> floor ] CAUSE
[ BECOME [ floor <CLEAN> ] ] ]
Following this long tradition of research, I propose a
syntactically-based event representation specifically de-
signed to handle alternations in argument structure. Fur-
thermore, I will show how this theoretical analysis can
be implemented in a feature-driven computational frame-
work. The product is an agenda-driven, chart-based
parser for the Minimalist Program.
3 A Decompositional Framework
A primary advantage of decompositional (non-lexicalist)
theories of lexical semantics is the ability to transpar-
ently relate morphologically related words?explaining,
for example, categorial divergences in terms of differ-
ences in event structure. Consider the adjective flat and
the deadjectival verb flatten:
(5) a. The tire is flat.
b. The tire flattened.
Clearly, (5a) is a stative sentence denoting a static situ-
ation, while (5b) denotes an inchoative event, i.e., a tran-
sition from ?tire is not flat? to ?tire is flat?. One might
assign the above two sentence the following logical form:
(6) a. BE(tire, [state flat])
b. ARG?(tire, e) ? BECOME(BE([state flat]), e)
In Davidsonian terms, dynamic events introduce event
arguments, whereas static situations do not. In (6b), the
semantic argument that undergoes the change of state
(ARG?) is introduced externally via the event argument.
Considering that the only difference between flat.ADJ
and flatten.V is the suffix -en, it must be the source of
inchoativity and contribute the change of state reading
that distinguishes the verb from the adjective. Here, we
have evidence that derivational affixes affect the seman-
tic representation of lexical items, that is, fragments of
event structure are directly associated with derivational
morphemes. We have the following situation:
(7) JflatK = [state flat]
Jis flatK = ?xBE(x, [state flat])
J-enK = ?s?xARG?(x, e) ? BECOME(BE(s), e)
Jflat-enK = ?x.ARG?(x, e)?
BECOME(BE([state flat]), e)
In this case, the complete event structure of a word
can be compositionally derived from its component mor-
phemes. This framework, where the ?semantic load? is
spread more evenly throughout the lexicon to lexical cat-
egories not typically thought to bear semantic content, is
essentially the model advocated by Pustejovsky (1991a),
among many others. Note that such an approach is no
longer lexicalist: each lexical item does not fully encode
its associated syntactic and semantic structures. Rather,
meanings are composed from component morphemes.
In addition to -en, other productive derivational suf-
fixes in English such as -er, -ize, -ion, just to name a
few, can be analyzed in a similar way. In fact, we may
view morphological rules for composing morphemes into
larger phonological units the same way we view syntac-
tic rules for combining constituents into higher-level pro-
jections, i.e., why distinguish VP ? V + NP from V
? Adj + -en? With this arbitrary distinction erased, we
are left with a unified morpho-syntactic framework for
integrating levels of grammar previously thought to be
separate?this is indeed one of the major goals of Dis-
tributed Morphology. This theoretical framework trans-
lates into a computational model better suited for analyz-
ing the semantics of natural language, particularly those
rich in morphology.
A conclusion that follows naturally from this analysis
is that fragments of event structure are directly encoded
in the syntactic structure. We could, in fact, further pos-
tulate that all event structure is encoded syntactically, i.e.,
that lexical semantic representation is isomorphic to syn-
tactic structure. Sometimes, these functional elements are
overtly realized, e.g., -en. Often, however, these func-
tional elements responsible for licensing event interpre-
tations are not phonologically realized.
These observations and this line of reasoning has not
escaped the attention of theoretical linguists: Hale and
Keyser (1993) propose that argument structure is, in fact,
encoded syntactically. They describe a cascading verb
phrase analysis with multiple phonetically empty verbal
projections corresponding to concepts such as inchoativ-
ity and agentivity. This present framework builds on the
work of Hale and Keyser, but in addition to advancing a
more refined theory of verbal argument structure, I also
describe a computational implementation.
4 Event Types
Although the study of event types can be traced back
to Aristotle, it wasn?t until the twentieth century when
philosophers and linguists developed classifications of
events that capture logical entailments and the co-
occurrence restrictions between verbs and other syntactic
elements such as tenses and adverbials. Vendler?s (1957)
four-way classification of events into states, activities, ac-
complishments, and achievements serves as a good start-
ing point for a computational ontology of event types.
Examples of the four event types are given below:
(8)
States Activities
know run
believe walk
Accomplishments Achievements
paint a picture recognize
make a chair find
Under Vendler?s classification, activities and states
both depict situations that are inherently temporally un-
bounded (atelic); states denote static situations, whereas
activities denote on-going dynamic situations. Accom-
plishments and achievements both express a change of
state, and hence are temporally bounded (telic); achieve-
ments are punctual, whereas accomplishments extend
over a period of time. Tenny (1987) observes that ac-
complishments differ from achievements only in terms of
event duration, which is often a question of granularity.
From typological studies, it appears that states, change
of states, and activities form the most basic ontology of
event types. They correspond to the primitives BE, BE-
COME, and DO proposed by a variety of linguists; let us
adopt these conceptual primitives as the basic vocabulary
of our lexical semantic representation.
Following the non-lexicalist tradition, these primitives
are argued to occupy functional projections in the syntac-
tic structure, as so-called light verbs. Here, I adopt the
model proposed by Marantz (1997) and decompose lexi-
cal verbs into verbalizing heads and verbal roots. Verbal-
izing heads introduce relevant eventive interpretations in
the syntax, and correspond to (assumed) universal primi-
tives of the human cognitive system. On the other hand,
verbal roots represent abstract (categoryless) concepts
and basically correspond to open-class items drawn from
encyclopedic knowledge. I assume an inventory of three
verbalizing heads, each corresponding to an aforemen-
tioned primitive:
(9) vDO [+dynamic, ?inchoative] = DO
v? [+dynamic, +inchoative] = BECOME
vBE [?dynamic] = BE
The light verb vDO licenses an atelic non-inchoative
event, and is compatible with verbal roots expressing ac-
tivity. It projects a functional head, voice (Kratzer, 1994),
whose specifier is the external argument.
(10) John ran.
voiceP
DP
John
voice vDOP
vDO
?
run
ARGext(John, e) ? DO([activity run], e)
The entire voiceP is further embedded under a tense
projection (not shown here), and the verbal complex un-
dergoes head movement and left adjoins to any overt
tense markings. Similarly, the external argument raises to
[Spec, TP]. This is in accordance with modern linguistic
theory, more specifically, the subject-internal hypothesis.
The verbal root can itself idiosyncratically license a
DP to give rise to a transitive sentence (subjected, nat-
urally, to selectional restrictions). These constructions
correspond to what Levin calls ?non-core transitive sen-
tences? (1999):
(11) John ran the marathon.
voiceP
DP
John voice vDOP
vDO
?
P
run DP
the marathon
ARGext(John, e) ? DO([activity run(marathon)], e)
Similarly, vBE licenses static situations, and is compat-
ible with verbal roots expressing state:
(12) Mary is tall.
vBEP
DP
Mary
vBE
is
?
tall
BE(Mary, [state tall])
The light verb v? licenses telic inchoative events (i.e.,
change of states), which correspond to the BECOME
primitive:
(13) The window broke:
v?P
DP
window
v?
vBE
?
break
ARG?(window, e) ? BECOME(BE([state break]), e)
The structure denotes an event where an entity under-
goes a change of state to the end state specified by the
root. v?P can be optionally embedded as the complement
of a vDO, accounting for the causative/inchoative alterna-
tion. Cyclic head movement (incorporation) of the verbal
roots into the verbalizing heads up to the highest verbal
projection accounts for the surface form of the sentence.
(14) John broke the window.
voiceP
DP
John voice vDOP
vDO v?P
DP
window
v?
vBE
?
break
CAUSE(e1, e2) ? ARGext(John, e1) ?
DO([activity undef], e1) ? ARG?(window, e2) ?
BECOME(BE([state break]), e2)
Note that in the causative form, vDO is unmodified by
a verbal root?the manner of activity is left unspecified,
i.e., ?John did something that caused the window to un-
dergo the change of state break.?
Given this framework, deadjectival verbs such as flat-
ten can be directly derived in the syntax:
(15) The tire flattened.
v?P
DP
tire
v?
-en
vBEP
vBE
?
flat
ARG?(tire, e) ? BECOME(BE([state flat]), e)
In (Lin, 2004), I present evidence from Mandarin Chi-
nese that this analysis is on the right track. The rest of
this paper, however, will be concerned with the computa-
tional implementation of my theoretical framework.
5 Minimalist Derivations
My theory of verbal argument structure can be imple-
mented in a unified morpho-syntactic parsing model
that interleaves syntactic and semantic parsing. The
system is in the form of an agenda-driven chart-based
parser whose foundation is similar to previous formaliza-
tions of Chomsky?s Minimalist Program (Stabler, 1997;
Harkema, 2000; Niyogi, 2001).
Lexical entries in the system are minimally specified,
each consisting of a phonetic form, a list of relevant fea-
tures, and semantics in the form of a ? expression.
The basic structure building operation, MERGE, takes
two items and creates a larger item. In the process,
compatible features are canceled and one of the items
projects. Simultaneously, the ? expression associated
with the licensor is applied to the ? expression associated
with the licensee (in theoretical linguistic terms, Spell-
Out).
The most basic feature is the =x licensor feature,
which cancels out a corresponding x licensee feature and
projects. A simple example is a determiner selecting a
noun to form a determiner phrase (akin to the context free
rule DP ? det noun). This is shown below (underline in-
dicates canceled features, and the node label < indicates
that the left item projects):
(16) <
the
:::=n d -k
shelf
:n
The features >x and <x trigger head movement (in-
corporation), i.e., the phonetic content of the licensee is
affixed to the left or right of the licensor?s phonetic con-
tent, respectively. These licensor features also cancel cor-
responding x licensee features:
(17) <
book -s
:::>n d -k
book
:n
<
de- bone
::<n V
bone
:n
Finally, feature checking is implemented by +x/-x fea-
tures. The +x denotes a need to discharge features, and
the -x denotes a need for features. A simple example of
this is the case assignment involved in building a preposi-
tional phrase, i.e., prepositions must assign case, and DPs
much receive case.
(18) <
on
:::=d:::+k ploc
<
the
:::=n:d ::-k
shelf
:n
Niyogi (2001) has developed an agenda-driven chart
parser for the feature-driven formalism described above;
please refer to his paper for a description of the parsing
algorithm. I have adapted it for my needs and developed
grammar fragments that reflect my non-lexicalist seman-
tic framework. As an example, a simplified derivation of
the sentence ?The tire flattened.? is shown in Figure 1.
The currently implemented system is still at the ?toy
parser? stage. Although the effectiveness and coverage
<
//
::>s vbe?x.BE(x)
/flat/
:s[state flat]
<
/flat -en/
::::>be =d
?x.?y.ARG?(y, e)?
BECOME(x, e)
<
::>s :::vbe
BE([state flat])
:s
>
/the tire/
::d
tire
<
/flat -en/
::::>be ::=d
?y.ARG?(y, e)?
BECOME(BE([state tall]), e)
<
:::>s::::vbe :s
ARG?(he, e) ? BECOME(BE([state tall(3cm)]), e)
Figure 1: Simplified derivation for the sentence ?The tire
flattened.?
of my parser remains to be seen, similar approaches have
been successful at capturing complex linguistic phenom-
ena. With a minimal set of features and a small num-
ber of lexical entries, Niyogi (2001) has successfully
modeled many of the argument alternations described by
Levin (1993) using a Hale and Keyser (1993) style anal-
ysis. I believe that with a suitable lexicon (either hand
crafted or automatically induced), my framework can be
elaborated into a system whose performance is compara-
ble to that of current statistical parsers, but with the added
advantage of simultaneously providing a richer lexical se-
mantic representation of the input sentence than flat pred-
icate argument structures based on semantic roles.
6 Conclusion
A combination of factors in the natural development of
computational linguistics as a field has conspired to nar-
row the diversity of techniques being explored by re-
searchers. While empirical and quantitative research is
the mark of a mature field, such an approach is not with-
out its adverse side-effects. Both syntactic and semantic
parsing technology faces a classic chicken-and-egg prob-
lem. In order for any new framework to become widely
adopted, it must prove to be competitive with state-of-
the-art systems in terms of performance. However, ro-
bust parsing cannot be achieved without either labori-
ously crafting grammars or a massive dedicated annota-
tion effort (and experience has shown the latter method
to be superior). Therein, however, lies the catch: neither
effort is likely to be undertaken unless a new framework
proves to be quantitatively superior than previously es-
tablished methodologies. Lacking quantitative measures
currently, the merits of my proposed framework can only
be gauged on theoretical grounds and its future potential
to better capture a variety of linguistic phenomena.
References
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet project. In Proceedings
of the 36th Annual Meeting of the Association for Com-
putational Linguistics and 17th International Con-
ference on Computational Linguistics (COLING/ACL
1998).
Eugene Charniak. 2001. Immediate head parsing for
language models. In Proceedings of the 39th Annual
Meeting of the Association for Computational Linguis-
tics (ACL-2001).
Michael Collins. 1997. Three generative lexicalized
models for statistical parsing. In Proceedings of the
35th Annual Meeting of the Association for Computa-
tional Linguistics (ACL-1997).
David Dowty. 1979. Word Meaning and Montague
Grammar. D. Reidel Publishing Company, Dordrecht,
The Netherlands.
Charles J. Fillmore. 1968. The case for case. In E. Bach
and R. Harms, editors, Universals in Linguistic The-
ory, pages 1?88. Holt, Rinehart, and Winston, New
York.
Kenneth Hale and Samuel Jay Keyser. 1993. On argu-
ment structure and the lexical expression of syntactic
relations. In Kenneth Hale and Samuel Jay Keyser,
editors, The View from Building 20: Essays in Linguis-
tics in Honor of Sylvain Bromberger. MIT Press, Cam-
bridge, Massachusetts.
Morris Halle and Alec Marantz. 1993. Distributed mor-
phology and the pieces of inflection. In Kenneth Hale
and S. Jay Keyser, editors, In The View from Build-
ing 20, pages 111?176. MIT Press, Cambridge, Mas-
sachusetts.
Henk Harkema. 2000. A recognizer for minimalist
grammars. In Proceedings of the Sixth International
Workshop on Parsing Technologies (IWPT 2000).
Ray Jackendoff. 1983. Semantics and Cognition. MIT
Press, Cambridge, Massachusetts.
Paul Kingsbury, Martha Palmer, and Mitch Marcus.
2002. Adding semantic annotation to the Penn Tree-
Bank. In Proceeding of 2002 Human Language Tech-
nology Conference (HLT 2002).
Angelika Kratzer. 1994. The event argument and the se-
mantics of voice. Unpublished manuscript, University
of Massachusetts, Amherst.
Beth Levin and Malka Rappaport Hovav. 1996. From
lexical semantics to argument realization. Unpub-
lished manuscript, Northwestern University and Bar
Ilan University.
Beth Levin. 1993. English Verb Classes and Alter-
nations: A Preliminary Investigation. University of
Chicago Press, Chicago, Illinois.
Beth Levin. 1999. Objecthood: An event structure per-
spective. In Proceedings of the 35th Annual Meeting
of the Chicago Linguistics Society.
Jimmy Lin. 2004. Event Structure and the Encoding of
Arguments: The Syntax of the English and Mandarin
Verb Phrase. Ph.D. thesis, Department of Electrical
Engineering and Computer Science, Massachusetts In-
stitute of Technology.
Alec Marantz. 1997. No escape from syntax: Don?t try
morphological analysis in the privacy of your own lex-
icon. In Proceedings of the 21st Annual Penn Linguis-
tics Colloquium.
Sourabh Niyogi. 2001. A minimalist implementation
of verb subcategorization. In Proceedings of the Sev-
enth International Workshop on Parsing Technologies
(IWPT-2001).
James Pustejovsky. 1991a. The generative lexicon.
Computational Linguistics, 17(4):409?441.
James Pustejovsky. 1991b. The syntax of event structure.
Cognition, 41:47?81.
Malka Rappaport Hovav and Beth Levin. 1998. Building
verb meanings. In Miriam Butt and Wilhelm Geuder,
editors, The Projection of Arguments: Lexical and
Compositional Factors. CSLI Publications, Stanford,
California.
Edward Stabler. 1997. Derivational minimalism. In
Christian Retore?, editor, Logical Aspects of Computa-
tional Linguistics. Springer.
Carol Tenny. 1987. Grammaticalizing Aspect and Affect-
edness. Ph.D. thesis, Massachusetts Institute of Tech-
nology.
Zeno Vendler. 1957. Verbs and times. Philosophical
Review, 56:143?160.
REXTOR: A System for Generating Relations 
from Natural Language 
Boris Katz and J immy Lin 
MIT Artificial Intelligence Laboratory 
200 Technology Square 
Cambridge, MA 02139 USA 
{boris, j immylin}@ai.mit, edu 
Abst ract  
This paper argues that a finite-state 
language model with a ternary expres- 
sion representation is currently the most 
practical and suitable bridge between 
natural language processing and infor- 
mation retrieval. Despite the theoreti- 
cal computational inadequacies of finite- 
state grammars, they are very cost ef- 
fective (in time and space requirements) 
and adequate for practical purposes. 
The ternary expressions that we use 
are not only linguistically-motivated, but
also amenable to rapid large-scale index- 
ing. REXTOR (Relations EXtracTOR) is 
an implementation f this model; in one 
uniform framework, the system provides 
two separate grammars for extracting 
arbitrary patterns of text and building 
ternary expressions from them. These 
content representational structures serve 
as the input to our ternary expressions 
indexer. This approach to natural lan- 
guage information retrieval promises to 
significantly raise the performance of 
current systems. 
1 In t roduct ion  
Traditional information retrieval (IR) has been 
built on the "bag-of-words" assumption, which 
equates the weighted component keywords of a 
document with its semantic ontent. Obviously, 
a document is much more than the sum of its in- 
dividual keywords. Although keywords may offer 
some indication of "meaning," they alone cannot 
capture the richness and expressiveness of natu- 
ral language. Consider the following sets of sen- 
tences/phrases that have similar word content, 
but (dramatically) different meanings: 1 
IExamples taken from (Loper, 2000) 
(1) The big man ate the dog. 
(1') The big dog ate the man. 
(2) The meaning of life 
(2') A me~ningfll\] life 
(3) The bank of the river 
(3') The bank near the river 
Due to the inability of keywords to capture the 
"meaning" of documents, a traditional informa- 
tion retrieval system (i.e., one using the bag-of- 
words paradigm) will suffer from poor precision in 
response to a user query accurately and precisely 
formulated in natural anguage. 
The application of natural language process- 
ing (NLP) techniques to information retrieval 
promises to generate representational structures 
that better capture the semantic ontent of docu- 
ments. In particular, syntactic analysis can high- 
light the relationships between various terms and 
phrases in a sentence, which will allow us to distin- 
guish between the example pairs given above and 
answer queries with higher precision than tradi- 
tional IR systems. 
However, a syntactically-informed representa- 
tional structure faces the problem of Hnguistic 
variations, the phenomenon in which similar se- 
mantic content may be expressed in different sur- 
face forms. Consider the following sets of sen- 
tences that express the same meaning using dif- 
ferent constructions: 
(4) What is Bill Gates' net worth? 
(4')What is the net worth of Bill Gates? 
(5) John gave the book to Mary. 
(5') John gave Mary the book. 
(5") Mary was given the book by John. 
(6) The president surprised the country with 
his actions. 
(6') The president's actions surprised the 
country. 
(7) Over 22 million people live in Taiwan. 
(7') The population of Taiwan is 22 million. 
An effective linguistically-motivated informa- 
tion retrieval system must not only handle rel- 
67 
atively simple syntactic variations (e.g., (4) and 
(5)), alternate realization of verb arguments (e.g., 
(6) and (6')), but also more complicated semantic 
variations (e.g., (7) and (7')). This can be ac- 
complished by linguistic normalization, a process 
by which linguistic variants that contain the same 
semantic ontent are mapped onto the same rep- 
resentational structure. 
The precision of information retrieval systems 
can be dramatically improved if they index not 
only single terms, but normelized representational 
structures derived from language. However, the 
optimal structure of this representation a d the 
efficient generation ofthese structures remains an 
open research problem. 
This paper argues that, for the purposes of 
information retrieval systems, the most suitable 
representational structure of document content is 
ternary expressions (compared to, for example, 
keywords, trees or case frames). Ternary (three- 
place) expressions may be thought of as typed 
binary relations (e.g., subject-relation-object) or 
two-place predicates (e.g., transitive verbs like 
'hit'); they are linguistically-motivated an  effi- 
cient to index. Also, for information retrieval, a
finite-state grammar isthe most practical and cost 
effective method by which to extract these ternary 
expressions from documents. Combined together, 
a finite-state language model and ternary expres- 
sion representation provide a convenient and pow- 
erful framework for integrating natural anguage 
processing with information retrieval. 
REXTOR (Relations EXtracTOR) is a docu- 
ment content analysis ystem designed to unify 
and generalize many previous natural anguage 
information retrieval techniques into one single 
framework. The system provides two separate 
grammars: one for extracting arbitrary entities 
from documents, and the other for building re- 
lations from the extracted items. REXTOI~ also 
provides a playground and testbed for future ex- 
perimentation in linguistically-motivated indexing 
schemes. 
2 Mot ivat ion  
We believe that, for humans, natural language is
the best mechanism for information access. It is 
intuitive, easy to use, rapidly deployable, and re- 
quires no specialized training, 
The REXTOR System builds on the experi- 
ence of START (SynTactic Analysis using Re- 
versible Transformations), a natural language sys- 
tem available for question answering on the World 
Wide Web. 2 Since December, 1993, when it first 
2hZZp :/\[~ww. ai .mit. edu/projects/infolab 
came online, START has engaged in millions of ex- 
changes with hundreds of thousands of people all 
over the world, supplying users with knowledge 
regarding eography, weather, movies, and many 
many other areas. Despite the successes of START 
in serving actual users, its domain of knowledge is
relatively small and expanding its knowledge base 
is a time-consuming task. The goal of REXTOR is 
to overcome this bottleneck and to provide a gen- 
eral framework for natural-language information 
retrieval. REXTOR not only draws its inspiration 
from START (in providing question answering ca- 
pabilities), but also borrows a simplified form of 
its representational structures (Katz, 1980; Katz, 
1990). 
The START System (Katz, 1990; Katz, 1997) 
analyzes English text and builds a knowledge base 
from information found in the text. The knowl- 
edge is expressed in the form of embedded ternary 
expressions (T-expressions) - -  subject-relation- 
object triples where the subject and object can 
themselves be ternary expressions. For exam- 
ple, "The population of Zimbabwe is 11,044,147" 
would be represented astwo ternary expressions: 
\[POPULATION-1 IS 11044147\] 
\[POPULATION-1 RELATED-T0 ZIMBABWE\] 
Experience from START has shown that a robust 
full-text natural language question-answering sys- 
tem cannot be realistically expected any time 
soon. Numerous problems uch as intersenten- 
tial reference, paraphrasing, summarization, com- 
mon sense implication, and many more, will take 
a long time to solve satisfactorily. In order to by- 
pass intractable complexities of language, START 
uses computer-analyzable natural language anno- 
tations, which consist of simplified English sen- 
tences and phrases, to describe various informa- 
tion segments (which may be text, images, or even 
video and other multimedia content). These nat- 
ural language annotations serve as metadata and 
inform START regarding the type of questions that 
a particular information segment is capable of an- 
swering (Katz, 1997). By performing retrieval on 
natural anguage annotations, the system is able 
to provide knowledge that it may not be able to 
analyze itself (either language that is too com- 
plex or non-textual segments). Because these an- 
notations must be manually generated, expand- 
ing START'S knowledge base is relatively time- 
intensive. 
REXTOR attempts to eliminate the need for hu- 
man involvement during content analysis, and also 
aims to serve as the foundation of a natural lan- 
guage information retrieval system. Ultimately, 
68 
we hope that REXTOR will serve as a stepping 
stone towards a comprehensive system capable of 
providing users with "just the right information" 
to queries posed in natural language. 
3 P rev ious  Work  
The concept of indexing more than simple key- 
words is not new; the idea of indexing (parts 
of) phrases, for example, is more than a decade 
old (Fagan, 1987). Arampatzis (1998) introduced 
the phrase retrieval hypothesis, which asserted 
that phrases are a better indication of document 
content han keywords. Several researchers have 
also explored ifferent techniques oflinguistic nor- 
realization for information retrieval (Strzalkowski 
et al, 1996; Zhai et al, 1996; Arampatzis et 
al., 2000). The performance improvements were 
neither negligible nor dramatic, but despite the 
lack of any significant breakthroughs, the au- 
thors affirmed the potential value of linguistically- 
motivated indexing schemes and the advantages 
they offer over traditional IR. 
Previous research in linguistically motivated 
information retrieval concentrated primarily on 
noun phrases and their attached prepositional 
phrases. Techniques that involve head/modifier 
relations have been tried, e.g., indexing adjec- 
tive/noun and noun/right adjunct pairs (which 
normalizes variants such as "information re- 
trievai" and "retrieval of information"). How- 
ever, there has been little experimentation with 
other types of linguistic relations, e.g., apposi- 
tives, predicate nominatives (i.e., the i s -a  rela- 
tion), predicate adjectives (i.e., the has-property 
relation), etc. Furthermore, indexing of word 
pairs and phrases in many previous ystems was 
accomplished by converting those representations 
into lexical items and atomic terms, indexed in 
the same manner as single words. The treat- 
ment of these representational structures using a 
restrictive bag-of-words paradigm limits the type 
of queries that may be formulated. For example, 
treating adjective/noun pairs (\[adj., noun\]) as lex- 
ical atoms renders it impossible to find the equiv- 
alent of "all big things," corresponding to the pair 
\[big, *\]. 
The extraction of these relations from docu- 
ments has been relatively inefficient and unsys- 
tematic. One approach is to first parse the 
document using a full-text parser, and then ex- 
tract interesting relations from the resulting parse 
tree (Fagan, 1987; Grishman and Sterling, 1993; 
Loper, 2000). This approach isslow and inefficient 
because full-text parsing is very time-intensive. 
Due to current limitations of computational tech- 
nology, only a small fraction of the information 
gathered by a full parser can be efficiently indexed. 
For the most part, relations that can be effec- 
tively utilized for information retrieval purposes 
only occupy a few nodes of a (possibly dense) 
parse tree; thus, most of the knowledge gathered 
by the parser is thrown away. Also, extracting 
non-linguistic relations from parse trees is very 
difficult; many interesting relations (from an IR 
point of view) have no linguistic foundation, e.g., 
adjacent word pairs. The other approach to ex- 
tracting relations from text is to build simple fil- 
ters for every new relation. This approach is un- 
systematic, and does not allow for rapid addition 
of new relations to a system. 
The REXTOR System utilizes an integrated 
model to systematically extract arbitrary textual 
patterns and relations (ternary expressions) from 
documents. The concept of coupling structure- 
building actions with parsing originated with aug- 
mented transition etworks (ATNs)(Thorne t al., 
1968; Woods, 1970). Similarly, PLNLP (Heidorn, 
1972; Jensen et al, 1993) is a programming lan- 
guage for writing phrase structure rules that in- 
clude specific conditions under which the rule can 
be applied. These rules may also be augmented 
by structure-building actions that are to be taken 
when the rule is applied. However, these sys- 
tems that attempt full-text parsing are less effi- 
cient for information retrieval applications due to 
the long time necessary to generate full linguistic 
parse trees. REXTOR was designed with a simple 
language model and an equally simple, yet expres- 
sive, representation f "meaning." 
4 Br idg ing  Natura l  Language and  
In fo rmat ion  Ret r ieva l  
In order to bridge the gap between atural an- 
guage and information retrieval, natural language 
text must be distilled into a representational 
structure that is amenable to fast, large-scale in- 
dexing. We argue that a finite-state model of nat- 
ural language with ternary expressions is currently 
the most suitable combination for this task. 
4.1 Finite-State Language Model  
Despite its limitations, a finite-state grammar 
seems to provide the best natural language model 
for information retrieval purposes. One of the 
most notable computational inadequacies of the 
finite-state model is the absence of a pushdown 
mechanism to suspend the processing of a con- 
stituent at a given level while using the same 
grammar to process an embedded constituent 
(Woods, 1970). Due to this inadequacy, certain 
69 
English constructions, such as center embedding, 
cannot be described by any finite-state gram- 
mar (Chomsky, 1959a; Chomsky, 1959b). How- 
ever, Church (1980) demonstrated that the finite- 
state language model is adequate to describe a 
performance model of language (i.e., constrained 
by memory, attention, and other realistic limi- 
tations) that approximates competence (i.e., lan- 
guage ability under optimal conditions without re- 
source constraints). Many phenomena that can- 
not be handled by fiuite-state grammars are awk- 
ward from a psycholinguistic point of view, and 
hence rarely seen. More recently, Pereira and 
Wright (1991) developed formal methods of ap- 
proximating context-free grammars with finite- 
state grammars, s Thus, for practical purposes, 
computationally simple finite-state grammars can 
be utilized to adequately model natural language. 
Empirically, the effectiveness of the finite- 
state language model has been demonstrated in 
the Message Understanding Conferences (MUCs), 
which evaluated information extraction (IE) sys- 
terns on a variety of domain-specific tasks. The 
conferences have shown that superficial parsing 
using finite-state grammars performs better than 
deep parsing using context-free grammars (at least 
under the current constraints oftechnology). The 
NYU team switched over from a system that per- 
formed full parsing (PROTEUS) in MUC-5 (Gr- 
ishman and Sterling, 1993) to a regular expres- 
sion matching parser in MUC-6 (Grishman, 1995). 
Full parsing was slow and error-prone, and the 
process of building a full syntactic analysis in- 
volved relatively unconstrained search which con- 
sumed large amounts of both time and space. The 
longer debug-cycles that resulted from this trans- 
lated into fewer iterations with which to tune the 
system within a given amount of time. Further- 
more, the complexity of a full context-free gram- 
mar contributed to maintenance problems; com- 
plex interactions within the grammar prevented 
rapid updating of the system to handle new con- 
structions. 
Finite-state grammars have been used to ex- 
tract entities such as proper nouns, names, lo- 
cations, etc., with relatively high precision. To 
a lesser extent, these grammars have proven to 
be effective in identifying syntactic onstructions 
such as noun phrases and verb phrases. FASTUS 
(Hobbs et al, 1996), the most notable of these sys- 
tems, is modeled after cascaded, nondeterrninistic 
finite-state automata. The finite-state transduc- 
ers are "cascaded" in that they are arranged in 
SHowever, these approximations overgenerate, al-
though in predictable, systematic ways. 
series; each one maps the output structures from 
the previous transducer into structures that com- 
prise the input to the next transducer. 
There are many similarities between informa- 
tion extraction and building effective representa- 
tional structures for information retrieval. Both 
tasks involve identifying entities (e.g., phrases) 
and the relationships between those entities. 
Thus, the application of proven information ex- 
traction techniques (i.e., finite-state technology) 
to information retrieval offers promise in raising 
the performance of IR systems. 
4.2 Ternary Expressions 
Ternary (three-place) expressions currently ap- 
pear to be the most suitable representational 
structure for meaning extracted from text. They 
may be intuitively viewed as subject-relation- 
object riples, and can easily express many types 
of relations, e.g., subject-verb-object relations, 
possession relations, etc. From a syntactic point of 
view, ternary expressions may be viewed as typed 
binary relations. Given the binary branching hy- 
pothesis of linguistic theory, ternary expressions 
are theoretically capable of expressing any arbi- 
trary tree - -  thus, ternary expressions are com- 
patible with linguistic theory. From a semantic 
point of view, ternary expressions may be viewed 
as two-place predicates, and can be manipulated 
using predicate logic. Finally, ternary expressions 
are highly amenable to rapid large-scale indexing, 
which is a necessary prerequisite of information re- 
trieval systems. Although other representational 
structures (e.g., trees or case frames) may be bet- 
ter adapted for some purposes, they are much 
more difficult to index and retrieve efficiently due 
to their size and complexity. 
In fact, indexing linguistic tree structures has 
been attempted (Smeaton et al, 1994), with very 
disappointing results: precision actually decreased 
due to the inability to handle variations in tree 
structure (i.e., the same semantic content could 
be expressed using different syntactic structures), 
and to the poor quality of the full-text natural lan- 
guage parser, which was also rather slow. Despite 
recent advances, full-text natural language parsers 
are still relatively error-prone; indexing incorrect 
parse trees is a source of performance degrada- 
tion. Furthermore, matching trees and sub-trees is 
a computationally intensive task, especially since 
full linguistic parse trees may be relatively deep. 
Relations are easier to match because they are 
typically much simpler than parse trees. For ex- 
ample, the tree 
\[\[shiny happy people \] \[of \[Wonderland\]\]\] 
70 
may be "flattened" into three relations: 
< shiny describes people > 
< happy describes people > 
< people related-to Wonderland > 
Indexing chse frames has also been attempted 
(Croft and Lewis, 1987; Loper, 2000), but with 
limited success. Full semantic analysis is still 
an open research problem, especially in the gen- 
eral domain. Since full semantic analysis can- 
not be performed without full-text parsing, case 
frame analysis inherits the unreliability of current 
parsers. Furthermore, semantic analysis requires 
extensive knowledge in the lexicon, which is ex- 
tremely time-intensive to construct. Finally, due 
to the complex structure of case frames, they are 
more difficult o store and index than ternary ex- 
pressions. 
Since ternary expressions are merely three-place 
relations, they may be indexed and retrieved much 
in the same way as rows within the table of a rela- 
tional database; 4 hence, well-known optimizations 
for databases may be applied for extremely high 
performance. 
Previous linguistically-motivated indexing 
schemes may easily be reformulated using ternary 
expressions. For example, indexing adjacent 
word pairs consists of indexing adjacent words 
with the adjacent relation. In fact, all pairs 
(e.g., adjective-noun, head-modifier) can be 
reformulated as ternary expressions by assigning 
a type to the pair. This finer granulaxity allows 
the capture of more intricate relations between 
words in a document. 
5 The  REXTOR System 
Using its finite-state language model, the REXTOR 
System generates a set of ternary expressions 
that correspond to content of a part-of-speech- 
tagged input document. Currently, the Brill Tag- 
ger (Brill, 1992) (with minor postprocessing) is 
used for the part-of-speech (POS) tagging. The 
relations construction process consists of two dis- 
tinct processes, each guided by its own externally 
specified grammar file. Extraction rules are ap- 
plied to match arbitrary patterns of text, based 
either on one of thirty-nine POS tags or on exact 
words. Whenever an item is extracted, a corre- 
sponding relation rule is triggered, which handles 
the actual generation of the ternary expressions 
(relations). 
4In fact, our first implementation f a ternary ex- 
pressions indexer used a SQL database. 
5.1 Ext rac t ion  Ru les  
Extraction rules are used to extract arbitrary pat- 
terns of text according to a grammar specification. 
The REXTOR grammar is written as regular ex- 
pression rules, which are computationally equiv- 
alent to finite-state automata, s Writing gram- 
mar rules in this fashion allows for perspicuity, 
the property whereby permitted types of construc- 
tions are readily apparent from the rules. Such 
a human-readable formulation simplifies mainte- 
nance of the grammar. 
The extraction stage of the REXTOR System 
performs a no-lookahead left-to-right scan of ev- 
ery input sentence, identifies the longest match- 
ing pattern (from any grammar rule), reduces the 
input sequence based on the matched rule, and 
continues with the next unmatched word. If a 
word cannot be included in any grammar rule, it 
is skipped. 
An extraction rule takes the following form: 
En?ityType := template; 
The rule can be read as Ent?tyType is defined 
as template. A successful match of the pattern in 
template signifies a successfully extracted entity. 
The template consists of a series of legal tokens, 
which are shown in Table 1. In addition, token 
modifiers (also in Table 1) can alter the meaning 
of the immediately preceding token. Tokens sur- 
rounded by curly braces ({}) are saved as bound 
variables, which can be later utilized to build re- 
lations (ternary expressions). These variables are 
referenced numerically starting at zero (e.g., the 
0th bound variable). 
5.2 Relat ion Rules 
A relation rule is triggered by the successful ex- 
traction of a particular entity (Ent?tyType). The 
relations grammar directs the construction ofthe 
actual ternary expression. A relation rule takes 
the following form: 
EntityType :=> <atoml atom2 acorn3>; 
The EntityType is the trigger for the relation, 
i.e., the rule is applied whenever a string of that 
type is extracted. The right hand side of the re- 
lation rule is the ternary expression to be gener- 
ated, which is a triple composed of three atoms. 
Valid atoms are shown in Table 2. They are either 
string literals or they manipulate the bound vari- 
ables saved from the extraction process in some 
manner. 
5For an algorithm converting regular expressions 
to nondeterministic finite-state automata, please refer 
to (Aho et al, 1988), Chapter 3. 
71 
POS 
POS \[string\] 
Enl;il~yType 
(tokeno I tokenx I . .  ? ) 
Descr ipt ion 
This matches any word tagged as the part-of-speech POS. 
This matches a specific word (string) of a specific part-of-speech (POS). 
This matches any extracted string of type EntityType. 
This expression matches any one of the alternative tokens given within 
the parentheses. Matches are attempted in the order in which they are 
written, e.g., the first token is tried first. 
Token Mod i f ie r  Descr ipt ion 
This modifier matches zero or more occurrences of the previous token. 
This modifier matches zero or one occurrence of the previous token. 
This modifier matches one or more occurrences of the previous token. 
Table 1: Valid tokens and token modif iers for ext ract ion  rules. 
Mod i f ie r  
In \ ]  
{~ 
\[Q ,Ent i tyTypel  \[3\] . . . .  
( alternativex l alternative21 ? ? ? ) 
' str ing' 
Descr ipt ion 
Evaluates to the nth bound variable of the trigger EntityType, 
interpreted as a string. 
Evaluates to the nth bound variable of the trigger EntityType, 
interpreted as a list of strings. The extraction rule token inside 
the bound variable is stripped of its outermost * or +, and the 
bound variable is broken into a list according to this pattern. For 
example, {JJX*} is interpreted as a list of JJX, or adjectives. 
This expression extracts a bound variable nested inside other 
bound variables. The ith bound variable of trigger FaxtityType 
is extracted; if this item is of type FEntityTypel, then the j th  
bound variable is extracted (the expression returns fa l se  if the 
entity types do not match); each comma separated unit is inter- 
preted in this manner, up to an arbitrary depth. 
This compound expression evaluates to the disjunction of an 
arbitrary number of valid atoms (as defined in this table). Each 
alternative is evaluated in a left to right order; the disjunction 
evaluates to the first alternative that returns a non-empty string. 
A literal string. 
Table 2: Valid atoms for the relat ion rules. 
Ezt ract ion  Rules:  
Relat ion Rules: 
NounGroup := (Pi~.PZIDT)? {JJX*} {(IflfPXINI~IXIINlfPSII~INS)+}; 
PrepositionalPhrase : = IN {NounGroup} ; 
ComplexNounGroup := {NounGroup} {PrepositionalPhrase}; 
NounGroup :=> <{0} 'describes ~ \[I\]>; 
ComplexNounGroup : ffi> 
< \[0\] ,Nou~Group \[1\] 
re la ted- to  ~ 
\[I\], PrepositionalPhrase \[0\] ,NounGroup \[1\] >; 
Figure 1: F.xample of re lat ion and extract ion rules. (PRPZ is the part-of-speech tag for possessive pronouns, 
DT for determiners, JJX for adjectives, J JR for comparative adjectives, JJS for superlative adjectives, NNX for 
singular or mass nouns, NNS for plural nouns, NNPX for singular proper nouns, NNPS for plural proper nouns, IN 
for prepositions.) 
72 
5.3 Examples  
A few extraction and relation rules are given 
in Figure 1. The first extraction rule defines 
a NounGroup as a sequence consisting of: an 
optional possessive pronoun or determiner, any 
number of adjectives, one or more nouns (of 
any type). Also, the sequence of adjectives is 
saved as the 0th bound variable, and the se- 
quence of nouns is saved as the 1st bound vari- 
able. The rules for PrepositionalPhrase and 
ComplexNounGroup can be interpreted similarly. 
Consider the following noun phrase: 
the big, bad wolf of the dark forest 
REXTOR recognizes two NounGroups in the 
above phrase: the big, bad wolf and the dark for- 
est. The corresponding relation rule triggers, and 
generates the following relations: 
< (big, bad) describes wolf > 
< (dark) describes forest > 
Note that the first bound variable in NounGroup 
is interpreted as a list; thus, the above two re- 
lations expand into three distinct relations when 
completely enumerated: 
< big describes wolf > 
< bad describes wolf > 
< dark describes forest > 
The ability to interpret bound variables as a list 
of strings allows for easy manipulation of repeated 
structure, like textual lists or enumerations. 
In addition, the entire noun phrase the big, bad 
wolf of the dark/forest will be recognized as a 
ComplexNounGroup. This will result in the fop 
lowing relation: 
< wolf related-to forest > 
The relation rule associated with 
ComplexNounGroup involves extracting nested 
bound variables. The first atom evaluates to 
the lth bound variable (a NounGroup) inside 
the 0th bound variable inside the trigger item 
ComplexNounGroup. The third atom is similarly 
evaluated. 
6 D iscuss ion  
Informal analysis of documents using REXTOR re- 
veals that it can potentially serve as an effective 
framework for extracting "meaning" from docu- 
ments. In particular, the system is capable of 
identifying the following types of linguistic con- 
structions and generating relations from them: 
? S imple sentences can be extracted by 
noting a simple NounGroup VerbGroup 
NounGroup pattern. From this, subject-verl>- 
object (SVO) relations can be derived. 
? Predicative nominat ives can be recog- 
nized by identifying the "be" verb and the 
NounGroup directly following it. These con- 
structions may be useful in establishing onto- 
logical hierarchies, i.e., is-a trees. 
? Predicative adjectives can be recognized 
by the "be" verb and a succession of one or 
more adjectives (or adjectival phrase). They 
may provide addition information regarding 
the attributes of entities, e.g., has-property. 
? Appositives are characteristically offset by 
commas and usually contain a single noun 
phrase; thus, they can be recognized rela- 
tively easily. Common in prose, appositives 
offer a wealth of additional information re- 
garding various entities, e.g., location of sites, 
age or position of people, etc. 
? Prepositional phrases are relatively easy 
to extract, and may supply valuable relations 
that increase the precision of information re- 
trieval systems. Ternary expressions allow 
for a better representation of prepositional 
phrases (compared to pairs) because they al- 
low the preposition to more specifically de- 
termine the type of relation (thus, examples 
like "boat by the water" and "boat under 
the water," which have completely different 
meanings, may be indexed separately and dis- 
tinctly). 
However, the prepositional phrase attach- 
ment problem (in the general-domain case) 
is still an open research topic, and thus poses 
some problems to content analysis. Regard- 
less, for the purposes of information retrieval, 
it may be acceptable to err on the side of over- 
generation in considering attachment, i.e., 
enumerate all possible relations. This will 
no doubt generate a large number of (pos- 
sibly incorrect) relations, and more research 
is required to determine effective methods of 
controlling this explosion. 
? Relative clauses of some types can be iden- 
tiffed by a finite-state language model. They 
may supply additional useful SVO relations 
for indexing purposes. 
We believe that future breakthroughs in natu- 
ral language information retrieval will occur in the 
generation of meaningful relations. Although the 
finite-state language model of REXTOR is powerful 
73 
enough to extract many linguistically interesting 
constructions, the approach is not fundamentally 
new. What differentiates our system from pre- 
vious work such as FASTUS (Hobbs et al, 1996) 
is that REXTOR not only provides a mechanism 
for extraction, but also introduces the paradigm 
of ternary expressions to capture document con- 
tent for information retrieval. The relations view 
of natural language documents i highly amenable 
to integration with information retrieval systems. 
Through a relations representation, REXTOR is 
able to distinguish the subtle differences in mean- 
ing between the pairs of sentences and phrases 
given in the introduction: 
(1) The man ate the dog. 
< man is-subject-of eat > 
< dog is-object-of eat > 
(I') The dog ate the man. 
< man is-object-of eat > 
< dog is-subject-of eat > 
(2) The meaning of life 
< meaning possessive-relation life > 
(2') A meaningful life 
< meaningful describes life > 
(3) The bank of the river 
< bank possessive-relation river > 
(3') The bank near the river 
< bank near-relation river > 
The ability to extract subject-verb-object re- 
lations, e.g., (1) and (1'), allows an IR system 
to distinguish between two very different state- 
ments. Similarly, REXTOR can differentiate be- 
tween prepositional phrases (2) and adjectival 
modification (2'). Although the system does not 
have any notion of semantics (e.g., word sense), 
syntax may offer crucial clues to meaning in cases 
such as (3) and (3'). 
Similarly, REXTOR is capable of performing lin- 
guistic normalization at the syntactic and mor- 
phological levels. Consider these sets of examples 
originally presented in the introduction: 
(4) What is Bill Gates' net worth? 
(4') What is the net worth of Bill Gates? 
< "net worth" related-to "Bill Gates" > 
(5) John gave the book to Mary. 
(5') John gave Mary the book. 
(5") Mary was given the book by John. 
< John is-subject-of give > 
< book is-direct-object-of give > 
< Mary is-indirect-object-of give > 
(6) The president surprised the country with 
his actions. 
< president is-subject-of surprise > 
< country is-object-of surprise > 
< surprise with actions > 
(6') The president's actions surprised his 
country. 
< actions re la ted- to  president > 
< actions is-subject-of surprise > 
< country is-object-of surprise > 
(7) Over 22 million people live in Waiwan. 
< "22 million" is-quantity-of people > 
< people is-subject-of live > 
< live in Taiwan > 
(7') The population of Taiwan is 22 million. 
< population is "22 million" > 
< population related-to Taiwan > 
With relations, different surface forms of ex- 
pressing the "possession relation" may be nor- 
malized into the same structure, e.g., (4) and 
(4'). Similarly, alternative surface realization of 
the same verb-headed relation can be recognized 
and equated with each other by writing different 
extraction rules that generate the same relations, 
e.g., (5), (5'), and (5"). The process of normal- 
ization will hopefully lead to greater ecall in in- 
formation retrieval systems. Note that (6) and 
(6') demonstrate a limitation of REXTOR, namely 
its inability to deal with alternative r alizations of 
verb arguments. Also, the system does not have 
any notion of semantics, and thus is unable to 
equate two sentences that have the same meaning, 
e.g., (7) and (7'). Although it is certainly possible 
to manually encode such semantic knowledge as 
extraction and relation rules, this solution is far 
from elegant. 
A potential solution to this semantic variations 
problem is to borrow the solution employed by 
START. A ternary expression representation of
natural language mimics its syntactic organiza- 
tion, and hence sentences that differ in surface 
form but are close in meaning will not map into 
the same structure. In order to solve this problem, 
START deploys "S-rules" (Katz and Levin, 1988), 
which are reversible syntactic/semantic transfor- 
mational rules that render explicit he relationship 
between alternate realizations of the same mean- 
ing. For example, a buy expression is semantically 
equivalent to a sell expression, except he subject 
and indirect objects are exchanged. Because many 
verbs can undergo the same alternations, they can 
in fact be grouped into verb classes, and hence 
governed by the same S-rules. Thus, S-rules can 
be viewed as metarules applied over ternary ex- 
pressions. A similar technique for handling both 
syntactic and semantic variations can be found in 
(Grishman, 1995; Jacquemin et al, 1997). Both 
utilize metarules (e.g., for passive/active transfor- 
mation) applied over textual patterns in order to 
generate and handle variations. 
74 
Below we present a concrete example of how 
REXTOR could potentially improve the perfor- 
mance of existing keyword search engines dramat- 
ically. We indexed an electronic version of the 
Worldbook Encyclopedia at the sentence level us- 
ing the following two techniques: 
1. A simple inverted keyword index. All stop- 
words are thrown out, and all content words 
are stemmed. Retrieval was performed by  
matching content words in the query with 
content words in the encyclopedia articles. 
2. A ternary expressions index using the rela- 
tions generated by REXTOR. The grammar 
was written to extract possessive relations, 
description relations (adjective-noun modifi- 
cation), prepositional relations, subject-verb 
relations, and verb-object relations. Re- 
trieval was performed by matching ternary 
expressions from the query (extracted using a 
separate grammar) with ternary expressions 
extracted from the encyclopedia articles. 
The following shows the results of the keyword 
search engine: 
Question: What do frogs eat? 
Answer:  
(R1) Adult frogs eat mainly insects and 
other small animals, including earthworms, 
minnows, and spiders. 
(R2) Bow'fms eat mainly other fish, frogs, 
and crayfish. 
(R3) Most cobras eat many kinds of ani- 
mals, such as frogs, fishes, birds, and various 
small mammals. 
(R4) One group of South American frogs 
feeds mainly on other frogs. 
(RS) Cranes eat a variety of foods, including 
frogs, fishes, birds, and various small mam- 
mals. 
(R6) Frogs eat many other animals, includ- 
ing spiders, flies, and worms. 
(R7) ... 
After removing stopwords from the query, our 
simple keyword search engine returned 33 results 
that contain the keywords frog and eat. How- 
ever, only (R1), (R4), and (R6) correctly answer 
the user query; the other results answer the ques- 
tion "What eats frogs?" or otherwise coinciden- 
tally contain those two terms. (Apparently, our 
poor frog has more predators than prey.) A bag- 
of-words approach fundamentally cannot differen- 
tiate between a query in which the frog is in the 
subject position and a query in which the frog is in 
the object position. However, by parsing subject- 
verb-object relations using REXTOR, a ternary ex- 
pressions indexer can effectively filter out irrele- 
vant results, returning the three correct responses. 
While indexing relations may potentially ower re- 
call, due to unanticipated constructions, it has a 
tremendous potential in increasing precision. 
Furthermore, consider the following queries, in 
which REXTOR would outperform traditional key- 
word engines: 
(8) How many South Koreans were recently 
allowed to visit their North Korean rela- 
tives? 
(9) Where did John see Mary? 
(10) Regarding what issue did the president 
of Russia criticize China? 
(11) Are electronics the biggest export from 
Japan to the United States? 
A traditional search engine using the bag-of- 
words approach would suffer from poor precision 
when faced with the above queries. Many verbs 
take arguments of the same semantic type, and in 
most of these sentences, reordering the verb argu- 
ments drastically alters their meaning. For exam- 
ple, a keyword search engine would not be able to 
distinguish between a question regarding South 
Koreans visiting North Korea and North Kore- 
ans visiting South Korea (8) because both queries 
have the same keyword content. Similarly, the 
keyword approach would be unable to determine 
who did the seeing (9), or who did the criticiz- 
ing (I0). Modification relations also pose difllcul- 
ties to the bag-of-words paradigm, e.g., was it the 
North Korean or South Korean relatives (8)? Was 
it the president of Russia or the president of China 
(10)? Furthermore, there are some constructions 
whose meaning critically depends on relations be- 
tween the entities, e.g., (11), because "from X to 
Y" and "from Y to X" usually differ in meaning. 
The current version of REXTOR is merely apro- 
totype; thus, we have made minimal attempts 
to optimize its processing speed. On a Pentium 
Ill 933 MHz Linux system with 512 megabytes 
of RAM,  s analyzing a sentence in the Worldbook 
Encyclopedia required 0.0378 seconds on average. 
This translates into a content analysis rate of 
roughly 340 words a second, or approximately 11.4 
megabytes of text per hour. Although the system 
composed of REXTOR and the ternary expressions 
indexer is slower than the simple keyword indexer, 
we believe that the potential to dramatically in- 
crease precision offsets the longer processing time. 
SHowever, REXTOR is not a memory-intensive sys- 
tem; RAM utilization during trial runs was rather low. 
75 
This paper presents only the first stage of an 
linguistically-motivated information retrieval sys- 
tem. Although we have presented the results of 
a preliminary investigation i to the effectiveness 
of this approach, we cannot draw any conclu- 
sions until more comprehensive t sts have been 
conducted. However, many prior techniques used 
in natural language information retrieval (e.g., 
head/modifier pairs) can be expressed within the 
ItEXTOR framework, and furthermore the system 
provides a playground for experimenting with new 
techniques. Thus, we believe that our approach 
shows great promise in moving towards higher 
performance information retrieval systems. 
7 Conc lus ion  
This paper presented a scheme for integrating nat- 
ural language processing and information retrieval 
by adopting a finite-state model of language and 
a ternary expression representation f document 
content. We provided justification for our lan- 
guage model and representational structures in 
both linguistic and empirical terms. ItEXTOR is 
an implementation of our ideas - -  it not only in- 
tegrates many previous natural anguage indexing 
techniques, but also provides a sufficiently gen- 
eral framework for much future experimentation. 
Although we have not yet conducted comprehen- 
sive tests, the extraction of "meaning" from doc- 
uments using ItEXTOR promises to better fulfill 
users' information eeds. 
8 Acknowledgments  
We would like to thank Sue Felshin for her insight- 
ful comments in reviewing drafts of this paper. 
Re ferences  
Alfred V. Aho, Ravi Sethi, and Jeffrey D. Ullman. 
1988. Compilers- Principles, Techniques, and 
Tools. Addison-Wesley. 
Avi Arampatzis, Th.P. van der Weide, C.H.A. 
Koster, and P. van Bommel. 1998. Phrase- 
based information retrieval. Information Pro- 
cessing and Management, 34(6):693-707, De- 
cember. 
Avi Arampatzis, Th.P. van der Weide, C.H.A. 
Koster, and P. van Bommel. 2000. An 
evaluation of linguistically-motivated in exing 
schemes. In Proceedings o.f BCS-IRSG 2000 
Colloquium on IR Research. 
Eric Brill. 1992. A simple rule-based part of 
speech tagger. In Proceedings of the Third Con- 
.ference on Applied Natural Language Process- 
ing. 
Noam Chomsky. 1959a. A note on phrase 
structure grammars. Information and Control, 
2:393-395. 
Noam Chomsky. 1959b. On certain formal prop- 
erties of grammars. Information and Control, 
2:137-167. 
Kenneth W. Church. 1980. On memory limita- 
tions in natural anguage processing. Technical 
Iteport Tit-245, MIT Laboratory for Computer 
Science. 
Bruce Croft and David D. Lewis. 1987. An ap- 
proach to natural anguage processing for doc- 
ument retrieval. In Proceedings of the lOth An- 
nual International ACM SIGIR Conference on 
Research and Development in Information Re- 
trieval (SIGIR-87). 
Joel L. Fagan. 1987. Experiments in Auto- 
matic Phrase Indexing .for Document Retrieval: 
A Comparisons of Syntactic and Non-Syntactic 
Methods. Ph.D. thesis, Cornell University. 
Ralph Grishman and John Sterling. 1993. New 
York University: Description of the PROTEUS 
system as used for MUC-5. In Proceedings of the 
5th Message Understanding Conference (MUG- 
5). 
Ralph Grishman. 1995. The NYU system for 
MUC-6 or where's the syntax. In Proceedings 
of the 6th Message Understanding Conference 
(MUC-6). 
George E. Heidorn. 1972, Natural lan- 
guage inputs to a simulation programming sys- 
tems. Technical Iteport NPS-55HD72101A, 
Naval Postgraduate School. 
Jerry It. Hobbs, Douglas Appelt, John Bear, 
David Israel, Megalmi Kameyama, Mark Stickel, 
and Mabry Tyson. 1996. FASTUS: A cascaded 
finite-state transducer for extracting informa- 
tion from natural-language text. In Roche and 
Schabes, editors, Finite State Devices .for Nat- 
ural Language Processing. MIT Press. 
Christian Jacquemin, Judith L. Klavans, and Eve- 
lyne Tzoukermann. 1997. Expansion of multi- 
word terms for indexing and retrieval using 
morphology and syntax. In Proceedings of the 
35th Annual Meeting of the Association for 
Computational Linguistics (A CL '97). 
Karen Jensen, George E. Heidorn, and Stephen D. 
Richardson, editors. 1993. Natural Language 
Processing: The PLNLP Approach. Kluwer 
Academic Publishers. 
76 
Boris Katz and Beth Levin. 1988. Exploiting lex- 
ical regularities in designing natural language 
systems. In Proceedings of the l~h Interna- 
tional Conference on Computational Linguistics 
(COLING '88). 
Boris Katz. 1980. A three-step rocedure for lan- 
guage generation. Technical Report 599, MIT 
Artificial Intelligence Laboratory. 
Boris Katz. 1990. Using English for indexing and 
retrieving. In P.H. Winston and S.A. Shellard, 
editors, Artificial Intelligence at MIT: Expand- 
ing Frontiers, volume 1. MIT Press. 
Boris Katz. 1997. Annotating the World Wide 
Web using natural anguage. In Proceedings of 
the 5th RIA O Conference on Computer Assisted 
Information Searching on the Internet (RIAO 
'97). 
Edward Loper. 2000. Applying semantic rela- 
tion extraction to information retrieval. Mas- 
ter's thesis, Massachusetts Institute of Technol- 
ogy. 
Fernando Pereira and Rebecca Wright. 1991. 
Finite-state approximation of phrase structure 
grammars. In Proceedings of the 29th Meeting 
of the ACL. 
Alan F. Smeaton, Ruairi O'Donnell, and Fergus 
Kelledy. 1994. Indexing structures derived 
from syntax in TREC-3: System description. 
In Proceedings of the 3rd Text REtrieval Con- 
ference (TREC-3). 
Tomek Strzalkowski, Louise Guthrie, Jussi Karl- 
gren, Jim Leistensnider, Fang Lin, Jose Perez- 
Carballo, Troy Straszheim, Jin Wang, and Jon 
Wilding. 1996. Natural language information 
retrieval: TREC-5 report. In Proceedings of the 
5th Text REtrieval Conference (TREC-5). 
J. Thorne, P. Bratley, and H. Dewar. 1968. The 
syntactic analysis of English by machine. In 
Donald Michie, editor, Machine Intelligence 3. 
Edinburgh University Press. 
William A. Woods. 1970. Transition network 
grammars for natural anguage analysis. Com- 
munications of the ACM, 13(10). 
Chengxiang Zhai, Xiang Tong, Natasa Milic- 
Frayling, and David A. Evans. 1996. Evalu- 
ation of syntactic phrase indexing - CLARIT 
NLP track report. In Proceedings of the 5th 
Text REtrieval Conference (TREC-5). 
77 
Gathering Knowledge for a Question Answering System from
Heterogeneous Information Sources
Boris Katz and Jimmy Lin and Sue Felshin
MIT Articial Intelligence Laboratory
200 Technology Square
Cambridge, MA 02139
fboris, jimmylin, sfelshing@ai.mit.edu
Abstract
Although vast amounts of information
are available electronically today, no ef-
fective information access mechanism ex-
ists to provide humans with convenient
information access. A general, open-
domain question answering system is a
solution to this problem. We propose an
architecture for a collaborative question
answering system that contains four pri-
mary components: an annotations sys-
tem for storing knowledge, a ternary ex-
pression representation of language, a
transformational rule system for han-
dling some complexities of language, and
a collaborative mechanism by which or-
dinary users can contribute new knowl-
edge by teaching the system new infor-
mation. We have developed a initial pro-
totype, called Webnotator, with which to
test these ideas.
1 Introduction
A tremendous amount of heterogenous informa-
tion exists in electronic format (the most promi-
nent example being the World Wide Web), but the
potential of this large body of knowledge remains
unrealized due to the lack of an eective informa-
tion access method. Because natural language is
the most convenient and most intuitive method
of accessing this information, people should be
able to access information using a system capa-
ble of understanding and answering natural lan-
guage questions|in short, a system that com-
bines human-level understanding with the infal-
lible memory of a computer.
Natural language processing has had its suc-
cesses and failures over the past decades; while the
successes are signicant, computers will not soon
be able to fully process and understand language.
In addition to the traditional di?culties associ-
ated with syntactic analysis, there remains many
other problems to be solved, e.g., semantic inter-
pretation, ambiguity resolution, discourse model-
ing, inferencing, common sense, etc. Furthermore,
not all information on the Web is textual|some
is sound, pictures, video, etc. While natural lan-
guage processing is advanced enough to under-
stand typical interactive questions about knowl-
edge (interactive questions are typically fairly sim-
ple in structure), it cannot understand the knowl-
edge itself. For the time being, therefore, the
only way for computers to access their own knowl-
edge is for humans to tell the computers what the
knowledge means in a language that the comput-
ers can understand|but still in a language that
humans can produce. A good way to accomplish
this is with the use of natural language annota-
tions, sentences which are simple enough for a
computer to analyze, yet which are in natural hu-
man language. Once knowledge is so annotated,
and indexed in a knowledge repository, a question
answering system can retrieve it.
The Start (SynTactic Analysis using Re-
versible Transformations) Natural Language Sys-
tem (Katz, 1990; Katz, 1997) is an example of a
question answering system that uses natural lan-
guage annotations. Start is a natural language
question answering system that has been available
to users on the World Wide Web
1
since Decem-
ber, 1993. During this time, it has engaged in
millions of exchanges with hundreds of thousands
of people all over the world, supplying users with
knowledge regarding geography, weather, movies,
corporations, and many many other areas. De-
spite the success of Start in serving real users,
its domain of expertise is relatively small and ex-
panding its knowledge base is a time-consuming
task that requires trained individuals.
We believe that the popularity of the Web may
oer a solution to this knowledge acquisition prob-
lem by providing collaborative mechanisms on a
scale that has not existed before. We can poten-
tially leverage millions of users on the World Wide
1
http://www.ai.mit.edu/projects/infolab
Web to construct and annotate a knowledge base
for question answering. In fact, we had proposed
a distributed mechanism for gathering knowledge
from the World Wide Web in 1997 (Katz, 1997),
but only recently have we attempted to implement
this idea.
An advantage of natural language annotations
is that it paves a smooth path of transition as nat-
ural language processing technology improves. As
natural language analysis techniques advance, the
annotations may become more and more complex.
Eventually, a textual information segment could
be its own annotation; someday, through other
technologies such as speech and image recognition,
etc., annotations could even be automatically con-
structed for non-textual information.
A further advantage is that natural language
annotations can be processed via techniques that
only partially understand them|via IR engines,
or less-than-ideal natural language systems|yet
they retain their more complex content and can be
reanalyzed at a later date by more sophisticated
systems.
2 Overview
We propose a collaborative question answering ar-
chitecture composed of the four following compo-
nents:
1. Natural Language Annotation is a tech-
nique of describing the content of informa-
tion segments in machine parsable natural
language sentences and phrases.
2. Ternary Expressions are subject-relation-
object triples that are expressive enough
to represent natural language, and also
amenable to rapid, large-scale indexing.
3. Transformational Rules handle the prob-
lem of linguistic variation (the phenomenon
in which sentences with dierent surface
structures share the same semantic content)
by explicitly equating representational struc-
tures (derived from dierent surface forms)
that have approximately the same meaning.
4. Collaborative Knowledge Gathering is a
technique by which the World Wide Web may
be viewed not only as a knowledge resource,
but also a human resource. The knowledge
base of a question answering system could be
constructed by enlisting the help of millions
of ordinary users all over the Web.
3 Annotations
Natural language annotations are machine-
parsable sentences or phrases that describe the
content of various information segments. They de-
scribe the questions that a particular segment of
information is capable of answering. For example,
the following paragraph about polar bears:
Most polar bears live along the northern
coasts of Canada, Greenland, and Russia,
and on islands of the Arctic Ocean. . .
may be annotated with one or more of the follow-
ing:
Polar bears live in the Arctic.
Where do polar bears live?
habitat of polar bears
A question answering system would parse these
annotations and store the parsed structures with
pointers back to the original information segment
that they described. To answer a question, the
user query would be compared against the anno-
tations stored in the knowledge base. Because this
match occurs at the level of ternary expressions,
structural relations and transformation (to be dis-
cussed in Section 5) can equate queries and anno-
tations even if their surface forms were dierent.
Furthermore, linguistically sophisticated machin-
ery such as synonymy/hyponymy, ontologies, can
be brought to bear on the matching process. If a
match were found, the segment corresponding to
the annotation would be returned to the user as
the answer.
The annotation mechanism we have outlined
serves as a good basis for constructing a question
answering system because annotating information
segments with natural language is simple and intu-
itive. The only requirement is that annotations be
machine parsable, and thus the sophistication of
annotations depends on the parser itself. As natu-
ral language understanding technology improves,
we can use more and more sophisticated annota-
tions.
In addition, annotations can be written to de-
scribe any type of information, e.g., text, im-
ages, sound clips, videos, and even multimedia.
This allows integration of heterogenous informa-
tion sources into a single framework.
Due to the vast size of the World Wide Web,
trying to catalog all knowledge on the World Wide
Web is a daunting task. Instead, focusing on
meta-knowledge is a more promising approach to
building a knowledge base that spans more than a
tiny fraction of the Web. Consider that reference
librarians at large libraries obviously don't know
all the knowledge stored in the reference books,
but they are nevertheless helpful in nding infor-
mation, precisely because they have a lot of knowl-
edge about the knowledge. Natural language anno-
tations can assist in creating a smart \reference
librarian" for the World Wide Web.
4 Representing Natural Language
A good representational structure for natural lan-
guage is ternary expressions.
2
They may be in-
tuitively viewed as subject-relation-object triples,
and can express most types of syntactic relations
between various entities within a sentence. We be-
lieve that the expressiveness of ternary relations
is adequate for capturing the information need of
users and the meaning of annotations. For ex-
ample, \What is the population of Zimbabwe?"
would be represented as two ternary expressions:
[what is population]
[population of Zimbabwe]
Ternary expressions can capture many rela-
tionships between entities within a sentence.
Such a representational structure is better than
a keyword-based scheme which equates a doc-
ument's keyword statistics with its semantic
content. Consider the following sets of sen-
tences/phrases that have similar word content,
but (dramatically) dierent meanings:
3
(1) The bird ate the young snake.
(1
0
) The snake ate the young bird.
(2) The meaning of life
(2
0
) A meaningful life
(3) The bank of the river
(3
0
) The bank near the river
Ternary expressions abstract away the linear or-
der of words in a sentence into a structure that is
closer to meaning, and therefore a relations-based
information access system will produce much more
precise results.
We have conducted some initial information re-
trieval experiments comparing a keyword-based
approach with one that performs matching based
on relations
4
. Using Minipar (Lin, 1999), we
parsed the entire contents of the Worldbook En-
cyclopedia and extracted salient relations from
it (e.g., subject-verb-object, possessives, prepo-
sitional phrase, etc.) We found that precision
2
See (Katz, 1990; Katz, 1997) for details about
such representation in Start.
3
Examples taken from (Loper, 2000)
4
to be published
for relations-based retrieval was much higher than
for keyword-based retrieval. In one test, retrieval
based on relations returned the database's three
correct entries:
Question: What do frogs eat?
Answer:
(R1) Adult frogs eat mainly insects and
other small animals, including earthworms,
minnows, and spiders.
(R4) One group of South American frogs
feeds mainly on other frogs.
(R6) Frogs eat many other animals, includ-
ing spiders, ies, and worms.
compared to 33 results containing the keywords
frog and eat which were returned by the keyword-
based system|the additional results all answer a
dierent question (\What eats frogs?") or other-
wise coincidentally contain those two terms.
Question: What do frogs eat?
Answer:
. . .
(R7) Adult frogs eat mainly insects and
other small animals, including earthworms,
minnows, and spiders.
(R8) Bowns eat mainly other sh, frogs,
and craysh.
(R9) Most cobras eat many kinds of animals,
such as frogs, shes, birds, and various small
mammals.
(R10) One group of South American frogs
feeds mainly on other frogs.
(R11) Cranes eat a variety of foods, includ-
ing frogs, shes, birds, and various small
mammals.
(R12) Frogs eat many other animals, includ-
ing spiders, ies, and worms.
(R13) . . .
Another advantage of ternary expressions is
that it becomes easier to write explicit transfor-
mational rules that encode specic linguistic vari-
ations. These rules are capable of equating struc-
tures derived from dierent sentences with the
same meaning (to be discussed in detail later).
In addition to being adequately expressive for
our purposes, ternary expressions are also highly
amenable to rapid large-scale indexing and re-
trieval. This is an important quality because
a large question answering system could poten-
tially contain answers to millions of questions.
Thus, compactness of representation and e?-
ciency of retrieval become an important consid-
eration. Ternary expressions may be indexed and
retrieved e?ciently because they may be viewed
using a relational model of data and manipulated
using relational databases.
5 Handling Linguistic Variation
Linguistic variation is the phenomenon in which
the same meaning can be expressed in a variety
of dierent ways. Consider these questions, which
ask for exactly the same item of information:
(4) What is the capital of Taiwan?
(5) What's the capital city of Taiwan?
(6) What is Taiwan's capital?
Linguistic variations can occur at all levels of
language; the examples above demonstrate lexical,
morphological and syntactic variations. Linguistic
variations may sometimes be quite complicated,
as in the following example, which demonstrates
verb argument alternation.
5
(7) Whose declaration of guilt shocked the
country?
(8) Who shocked the country with his dec-
laration of guilt?
Transformational rules provide a mechanism to
explicitly equate alternate realizations of the same
meaning at the level of ternary expressions.
As an example, Figure 1 shows a sample trans-
formational rule for (7) and (8).
6
Thus, through
application of this rule, question (7) can be
equated with question (8).
[n
1
shock n
2
] [n
3
shock n
2
]
[shock with n
3
] $
[n
3
related-to n
1
] [n
3
related-to n
1
]
where n 2 Nouns where n 2 Nouns
Figure 1: Sample Transformational Rule
Transformational rules may be generalized by
associating arbitrary conditions with them; e.g.,
verb 2 shock, surprise, excite : : :
A general observation about English verbs is
that they divide into \classes," where verbs in
the same class undergo the same alternations.
For example, the verbs `shock', `surprise', `excite',
etc., participate in the alternation shown in Sen-
tence (7) and (8) not by coincidence, but because
5
Beth Levin (Levin, 1993) oers an excellent treat-
ment on English verb classes and verb argument al-
ternations.
6
This rule is bidirectional in the sense that each
side of the rule implies the other side. The rule is
actually used in only one direction, so that we canon-
icalize the representation.
they share certain semantic qualities. Although
the transformational rule required to handle this
alternation is very specic (in that it applies to a
very specic pattern of ternary expression struc-
ture), the rule can nevertheless be generalized over
all verbs in the same class by associating with the
rule conditions that must be met for the rule to
re, i.e., verb 2 emotional-reaction-verbs; see Fig-
ure 2.
[n
1
v
1
n
2
] [n
3
v
1
n
2
]
[v
1
with n
3
] $
[n
3
related-to n
1
] [n
3
related-to n
1
]
where n 2 Nouns and v 2 emotional-reaction-verbs
Figure 2: Sample Transformational Rule
Note that transformational rules can also en-
code semantic knowledge and even elements of
common sense. For example, a rule can be written
that equates a selling action with a buying action
(with verb arguments in dierent positions). Or
as another example, rules can even encode impli-
catures, e.g., A murdered B implies that B is dead.
Transformational rules can apply at the syntac-
tic, semantic, or even pragmatic levels, and oer
a convenient, powerful, and expressive framework
for handling linguistic variations.
In order for a question answering system to be
successful and have adequate linguistic coverage,
it must have a large number of these rules. A lexi-
con which classied verbs by argument alternation
patterns would be a good start, but this is another
resource lacking in the world today. Rules gener-
ally may be quite complex, and it would be di?-
cult to gather such knowledge from average Web
users with little linguistic background. Requesting
that users describe segments with multiple anno-
tations (each representing a dierent phrasing of
the description), might serve as a preliminary so-
lution to the linguistic variation problem. Another
possible solution will involve learning transforma-
tional rules from a corpus. The di?culty in cre-
ating transformational rules is a serious problem
and unless and until this problem is solved, an
NL-based QA system would have to be restricted
to a limited domain where a small number of ex-
perts could provide enough transformational rule
coverage, or would require a large commitment of
resources to attain su?cient coverage.
6 Collaboration on the Web
A critical component of a successful natural lan-
guage question answering system is the knowledge
base itself. Although the annotation mechanism
simplies the task of building a knowledge base,
the accumulation of knowledge is nevertheless a
time consuming and labor intensive task. How-
ever, due to the simplicity of natural language an-
notations (i.e., describing knowledge in everyday
English), ordinary users with no technical skills
may contribute to a knowledge base. Thus, by
providing a general framework in which people on
the World Wide Web can enter additional knowl-
edge, we can engage millions of potential users all
over the world to collaboratively construct a ques-
tion answering system. We can distribute the ef-
fort of building a knowledge base across many or-
dinary users by allowing them to teach the system
new knowledge.
The idea of using the Internet as a tool for
collaboration across geographically distributed re-
gions is not a new idea. The Open Source move-
ment rst demonstrated the eectiveness and sus-
tainability of programming computer systems in
a distributed manner. Made possible in part by
the World Wide Web, the Open Source move-
ment promotes software development by nurtur-
ing a community of individual contributors work-
ing on freely distributed source code. Under this
development model, software reliability and qual-
ity is ensured through independent peer review by
a large number of programmers. Successful Open
Source projects include Linux, a popular Unix-like
operating system; Apache, the most popular Web
server in the World; SendMail, an utility on vir-
tually every Unix machine; and dmoz, the Open
Directory Project, whose goal is to produce the
most comprehensive directory of the Web by rely-
ing on volunteer editors.
7
Another example of Web-based collaboration
is the Open Mind Initiative (Stork, 1999; Stork,
2000), which is a recent eort to organize ordi-
nary users on the World Wide Web (netizens) to
assist in developing intelligent software. Based on
the observation that many tasks such as speech
recognition and character recognition require vast
quantities of training data, the initiative attempts
to provide a collaborate framework for collecting
data from the World Wide Web. The three pri-
mary contributors within such a framework are
domain experts, who provide fundamental algo-
rithms, tool/infrastructure developers, who de-
velop the framework for capturing data, and non-
expert netizens, who supply the raw training data.
Open Mind Commonsense
8
is an attempt at
constructing a large common sense database by
7
http://www.dmoz.org
8
http://openmind.media.mit.edu
collecting assertions from users all over the Web.
9
Other projects have demonstrated the viabil-
ity of Web-enabled collaborative problem-solving
by harnessing the computational power of idle
processors connected to the Web.
10
The SETI
(Search for Extraterrestrial Intelligence) Institute
was founded after Nasa canceled its High Resolu-
tion Microwave Survey project. The institute or-
ganizes thousands of individuals who donate their
idle processor cycles to search small segments of
radio telescope logs for signs of extraterrestrial
intelligence.
11
Other similar projects that orga-
nize the usage of idle processor time on personal
computers include the Internet Mersenne Prime
Search,
12
and the RC5 Challenge.
13
Recent technical, social, and economic devel-
opments have made the abovementioned models
of collaboration possible. Furthermore, numerous
successful projects have already demonstrated the
eectiveness of these collaborative models. Thus,
it is time to capitalize on these emerging trends
to create the rst collaborative question answer-
ing system on the World Wide Web.
Even with the components such as those de-
scribed above, there still remains a major hurdle
in jumpstarting the construction of a collaborative
question answering system. We are faced with a
classic chicken-and-egg problem: in order to at-
tract users to contribute knowledge, the system
must serve a real information need (i.e., actually
provide users with answers). However, in order
to serve user information needs, the system needs
knowledge, which must be contributed by users.
In the initial stages of building a question an-
swering system, the knowledge base will be too
sparse to be useful. Furthermore, the system may
be very brittle, and might not retrieve the correct
information segment, even if it did exist within
the knowledge base (e.g., due to a missing trans-
formational rule).
It may be possible to address this dilemma with
an incremental approach. The system can rst
be restricted to a very limited domain (e.g., \an-
imals" or \geography"). Users' expectations will
be carefully managed so that they realize the sys-
tem is highly experimental and has a very lim-
ited range of knowledge. In eect, the users will
9
A non-collaborative approach to building a com-
mon sense knowledge base is taken by Lenat whose
Cyc project (Lenat, 1995) is an attempt to build a
common sense knowledge base through a small team
of dedicated and highly trained specialists.
10
http://www.distributed.org
11
http://setiathome.ssl.berkeley.edu
12
http://www.mersenne.org
13
http://www.distributed.org/rc5/
be populating a domain-specic knowledge base.
Over time, the system will be able to answer more
and more questions in that domain, and hence be-
gin to oer interesting answers to real users. After
this, a critical mass will form so that users are not
only teaching the system new knowledge, but also
receiving high quality answers to their questions.
At that point, a decision can be made to increase
the domain coverage of the system.
In order to initialize this process, we can boot-
strap o the curiosity and altruism of individual
users. As an example, the Openmind Common
Sense project has accumulated over 280 thousand
items of information by over six thousand users
based on a data collection model that does not
supply the user with any useful service. The
dream of building \smart" systems has always
been a fascination in our culture (e.g., HAL from
2001: A Space Odyssey); we believe that this will
serve to attract rst-time users.
7 Evolving the System
While the collaborative information gathering
task proceeds, we are then faced with the prob-
lem of maintaining the system and ensuring that
it will provide users with useful information. Two
immediate issues arise: quality control and lin-
guistic variation.
How can we insure the quality of the contributed
material? In general, any system that solicits in-
formation from the World Wide Web faces a prob-
lem of quality control and moderation. Although
most Web users are well-meaning, a small frac-
tion of Web users may have malicious intentions.
Therefore, some ltering mechanisms must be im-
plemented to exclude inappropriate content (e.g.,
pornography or commercial advertisement) from
being inserted into the knowledge base. More
troublesome is the possibility of well-meant but
incorrect information which is probably more com-
mon and denitely harder to detect.
How can we handle linguistic variation? There
are often dierent ways of asking the same ques-
tion; the annotation of a particular segment might
not match the user query, and hence the correct
answer may not be returned as a result. Transfor-
mational rules may be a solution to the problem,
but writing and compiling these rules remain a
di?cult problem.
We propose a variety of solutions for the main-
tenance of a collaborative question answering sys-
tem, depending on the level of human intervention
and supervision.
At one end of the spectrum, an unsupervised
approach to quality control can be implemented
through a distributed system of moderation with
dierent trust levels. The scheme essentially calls
for self-management of the knowledge repository
by the users themselves (i.e., the users with high
trust levels). Dierent trust levels will allow users
various levels of access to the knowledge base, e.g.,
the ability to modify or delete information seg-
ments and their associated annotations or to mod-
ify other users' trust levels. To initiate the process,
only a small group of core editors is required.
In such an unsupervised system, the problem of
linguistic variation could be addressed by prompt-
ing users to give multiple annotations, each de-
scribing the information content of a particular
segment in a dierent way. With a su?ciently
large user base, wide coverage might still be
achieved in the absence of broad-coverage trans-
formational rules.
At the other end of the spectrum, a large or-
ganization may commit signicant amounts of re-
sources to maintaining a supervised collaborative
knowledge base. For example, an organization
may be willing to commit resources to preserve
its organizational memory in the form of an \in-
telligent FAQ" supported by natural language an-
notations. Computers can be eectively utilized
to augment the memory of an organization (Allen,
1977), and have been successfully deployed in real-
world environments with relative success (Acker-
man, 1998).
If an organization were willing to commit signi-
cant resources to a collaborative knowledge reposi-
tory, then transformational rules can be written by
experts with linguistic background. Such experts
could constantly review the annotations entered
by ordinary users and formulate transformational
rules to capture generalizations.
Supervised use of natural language annotation
falls short of the grandiose goal of accessing the
entire World Wide Web, but is the practical and
useful way to apply NL annotation until the trans-
formational rule problem can be solved for unlim-
ited domains.
8 Initial Prototype
Webnotator is a prototype test-bed to evaluate
the practicality of NL-based annotation and re-
trieval through Web-based collaboration. It pro-
vides e?cient facilities for retrieving answers al-
ready stored within the knowledge base and a scal-
able framework for ordinary users to contribute
knowledge.
The system analyzes natural language annota-
tions to produce ternary expressions by postpro-
cessing the results of Minipar (Lin, 1993; Lin,
1994), a fast and robust functional dependency
parser that is freely available for non-commercial
purposes. The quality of the representational
structures depends ultimately on the quality of
whatever parser Webnotator is made to access. In
the current implementation, ternary expressions
are not embedded, elements of ternary expres-
sions are not indexed, and coreference is not de-
tected. Words are stemmed to their root form and
morphological information is discarded. The sys-
tem also implements a version of transformational
rules described above as a simple forward-chaining
rule-based system.
Using a relational database, Webnotator imple-
ments a knowledge base that stores ternary ex-
pressions derived from annotations and their asso-
ciated information segments. Ternary expressions
t neatly into a relational model of data, and thus
manipulation of the knowledge (including answer-
ing queries and inserting new knowledge) can be
formulated as SQL queries. This vastly simplies
development eorts while maintaining robustness
and performance.
Webnotator provides an interface through
which users may teach the system new knowl-
edge by supplying new information segments and
adding new annotations. Essentially, the user en-
ters, in a CGI form, an information segment and
annotations that describe the knowledge. Since
the segment of information can contain any valid
HTML, images, tables, and even multimedia con-
tent may be included. Alternatively, the user may
simply provide a URL to annotate, and Webnota-
tor will automatically create a link to the URL in
its knowledge base.
Currently, Webnotator is a prototype that has
been released to a small community of developers
and testers within the MIT Articial Intelligence
Laboratory. We plan on releasing the system to
the general public in the near future. By col-
lecting knowledge from the general public and by
varying the representations and transformations
applied by Webnotator, it should be possible to
discover which features are most important for
a natural-language-based annotation system and
whether the state of the art is indeed su?ciently
advanced to make such a system practical and ef-
fective.
9 Related Work
A variety of research has been conducted on bet-
ter information access methods on the World
Wide Web (e.g., the \Semantic Web" (Berners-
Lee, 1999)). However, most of these approaches
have concentrated on methods of annotating exist-
ing web pages with metadata such as XML/RDF
(Resource Description Framework) (Staab et al,
2000), extensions to HTML (Luke et al, 1997;
Hein et al, 1999; Staab et al, 2000), special-
ized descriptions (W. Dalitz and Lugger, 1997),
or even conceptual graphs (Martin and Eklund,
1999).
The common thread among previous work is the
embedding of metadata directly into Web docu-
ments, which are then gathered via crawling or
spidering. This approach only works if the target
community of the system is well-dened; adop-
tion of various metadata techniques are presently
limited, and thus it would be pointless to crawl
the entire web to search for metadata. A model
in which distributed metadata are gathered by a
spider will not work with a constantly changing
community that is ill-dened. In principle, there
is no reason why our natural language annotations
cannot be embedded into Web documents also; the
issue is strictly a practical concern.
Another common theme in previous work is
the organization of knowledge in accordance with
some pre-established ontology. This presents sev-
eral challenges for building a general system for
gathering knowledge. Ontologies are often ei-
ther too specic to be of general use (e.g., Ri-
boWeb's ontology for ribosome data (Altmann et
al., 1999)), or too weak to provide much structure
(e.g., Yahoo). Since the ontology is static and
must be agreed upon prior to any knowledge base
development, it may be too constricting and too
inconvenient for the expression of new or unantic-
ipated concepts. Although systems do allow for
arbitrary extension of the ontology (Hein et al,
1999; Staab et al, 2000), such extensions defeat
the purpose of a structure-imposing ontology. Our
proposed alternative to a ontological hierarchy is
to take advantage of the expressiveness of natu-
ral language, and use linguistic devices to relate
concepts. The combination of lexical resources
(e.g., synonyms and meronyms in WordNet) and
transformational rules provide a natural, extensi-
ble way to relate and structure dierent concepts.
A compelling argument for natural language an-
notations is their expressiveness and compactness.
Martin and Eklund (Martin and Eklund, 1999) ar-
gue against an XML-based system of metadata be-
cause XML was primarily intended to be machine
readable, not human readable. In their paper,
they started with an English phrase, and then pro-
ceeded to demonstrate the encoding of that sen-
tence in various formalisms. A constraint graph
encoding was simpler than a KIF (Knowledge In-
terchange Format) encoding, which was in turn
shorter than a RDF format. Of course, this begs
the question: why not just annotate the document
with the original English phrase? Current NLP
technology can handle a large variety of English
sentences and phrases, which may serve as the
annotations directly. Such is system is not only
simpler, more intuitive, but also more compact.
10 Conclusion
Recent social, technical, and economic develop-
ments have made possible a new paradigm of soft-
ware development and problem solving through
loosely-organized collaboration of individuals on
the World Wide Web. Many successful prece-
dents have already proven the viability of this ap-
proach. By leveraging this trend with existing an-
notation and natural language technology, we can
provide a exible framework for a question an-
swering system that grows and \evolves" as each
user contributes to the knowledge base, with only
minimal outside supervision. Testing will reveal
whether such a system can help users realize some
of the untapped potential of the World Wide Web
and other sources of digital information as a vast
repository of human knowledge.
References
Mark S. Ackerman. 1998. Augmenting organi-
zational memory: A eld study of answer gar-
den. ACM Transactions on Information Sys-
tems, 16(3):203{224, July.
Thomas Allen. 1977. Managing the Flow of Tech-
nology. MIT Press.
R. Altmann, M. Bada, X. Chai, M. W. Car-
illo, R. Chen, and N. Abernethy. 1999. Ri-
boWeb: An ontology-based system for collabo-
rative molecular biology. IEEE Intelligent Sys-
tems, 14(5):68{76.
T. Berners-Lee. 1999. Weaving the Web. Harper,
New York.
Je Hein, James Hendler, and Sean Luke. 1999.
SHOE: A knowledge representation language
for internet applications. Technical Report
CS-TR-4078, Institute of Advanced Computer
Studies, University of Maryland, College Park.
Boris Katz. 1990. Using English for indexing and
retrieving. In P.H. Winston and S.A. Shellard,
editors, Articial Intelligence at MIT: Expand-
ing Frontiers, volume 1. MIT Press.
Boris Katz. 1997. Annotating the World Wide
Web using natural language. In Proceedings of
the 5th RIAO Conference on Computer Assisted
Information Searching on the Internet (RIAO
'97).
Doug Lenat. 1995. CYC: A large-scale investment
in knowledge infrastructure. Communications
of the ACM, 38(11):33{38.
Beth Levin. 1993. English Verb Classes and Al-
ternations: A Preliminary Investigation. Uni-
versity of Chicago Press.
Dekang Lin. 1993. Principled-based parsing with-
out overgeneration. In Proceedings of the 31th
Annual Meeting of the Association for Compu-
tational Linguistics (ACL'93).
Dekang Lin. 1994. PRINCIPAR|An e?cient,
broad-coverage, principle-based parser. In Pro-
ceedings of the 15th International Conference on
Computational Linguistics (COLING '94).
Dekang Lin. 1999. Minipar|a minimalist parser.
In Maryland Linguistics Colloquium, University
of Maryland, College Park, March 12,.
Edward Loper. 2000. Applying semantic relation
extraction to information retrieval. Master's
thesis, Massachusetts Institute of Technology.
S. Luke, L. Spector, D. Rager, and J. Hendler.
1997. Ontology-based web agents. In Proceed-
ings of the First International Conference on
Autonomous Agents.
Philippe Martin and Peter Eklund. 1999. Em-
bedding knowledge in web documents. In Pro-
ceedings of the Eighth International World Wide
Web Conference.
S. Staab, J. Angele, S. Decker, M. Erd-
mann, A. Hotho, A. Maedche, H.-P. Schnurr,
R. Studer, and Y. Sure. 2000. Semantic com-
munity web portals. In Proceedings of the Ninth
International World Wide Web Conference.
David G. Stork. 1999. Character and document
research in the open mind initiative. In Pro-
ceedings of the Fifth International Conference
on Document Analysis and Recognition.
David G. Stork. 2000. Open data collection for
training intelligent software in the open mind
initiative. In Proceedings of the Engineering In-
telligent Systems Symposium (EIS '2000).
M. Grotschel W. Dalitz and J. Lugger. 1997.
Information services for mathematics and the
internet. In A. Sydow, editor, Proceedings of
the 15th IMACS World Congress on Scientic
Computation: Modelling and Applied Mathe-
matics. Wissenschaft und Technik Verlag.
 	
	
Extracting Structural Paraphrases from Aligned Monolingual Corpora
Ali Ibrahim Boris Katz Jimmy Lin
MIT Artificial Intelligence Laboratory
200 Technology Square
Cambridge, MA 02139
{aibrahim,boris,jimmylin}@ai.mit.edu
Abstract
We present an approach for automatically
learning paraphrases from aligned mono-
lingual corpora. Our algorithm works
by generalizing the syntactic paths be-
tween corresponding anchors in aligned
sentence pairs. Compared to previous
work, structural paraphrases generated by
our algorithm tend to be much longer
on average, and are capable of captur-
ing long-distance dependencies. In ad-
dition to a standalone evaluation of our
paraphrases, we also describe a question
answering application currently under de-
velopment that could immensely bene-
fit from automatically-learned structural
paraphrases.
1 Introduction
The richness of human language allows people to
express the same idea in many different ways; they
may use different words to refer to the same entity or
employ different phrases to describe the same con-
cept. Acquisition of paraphrases, or alternative ways
to convey the same information, is critical to many
natural language applications. For example, an ef-
fective question answering system must be equipped
to handle these variations, because it should be able
to respond to differently phrased natural language
questions.
While there are many resources that help sys-
tems deal with single-word synonyms, e.g., Word-
Net, there are few resources for multiple-word or
domain-specific paraphrases. Because manually
collecting paraphrases is time-consuming and im-
practical for large-scale applications, attention has
recently focused on techniques for automatically ac-
quiring paraphrases.
We present an unsupervised method for acquir-
ing structural paraphrases, or fragments of syntactic
trees that are roughly semantically equivalent, from
aligned monolingual corpora. The structural para-
phrases produced by our algorithm are similar to the
S-rules advocated by Katz and Levin for question
answering (1988), except that our paraphrases are
automatically generated. Because there is disagree-
ment regarding the exact definition of paraphrases
(Dras, 1999), we employ that operating definition
that structural paraphrases are roughly interchange-
able within the specific configuration of syntactic
structures that they specify.
Our approach is a synthesis of techniques devel-
oped by Barzilay and McKeown (2001) and Lin
and Pantel (2001), designed to overcome the limi-
tations of both. In addition to the evaluation of para-
phrases generated by our method, we also describe
a novel information retrieval system under develop-
ment that is designed to take advantage of structural
paraphrases.
2 Previous Work
There has been a rich body of research on automati-
cally deriving paraphrases, including equating mor-
phological and syntactic variants of technical terms
(Jacquemin et al, 1997), and identifying equiva-
lent adjective-noun phrases (Lapata, 2001). Unfor-
tunately, both are limited in types of paraphrases that
they can extract. Other researchers have explored
distributional clustering of similar words (Pereira et
al., 1993; Lin, 1998), but it is unclear to what extent
such techniques produce paraphrases.1
Most relevant to this paper is the work of Barzi-
lay and McKeown and the work of Lin and Pan-
tel. Barzilay and McKeown (2001) extracted
both single- and multiple-word paraphrases from a
sentence-aligned corpus for use in multi-document
summarization. They constructed an aligned corpus
from multiple translations of foreign novels. From
this, they co-trained a classifier that decided whether
or not two phrases were paraphrases of each other
based on their surrounding context. Barzilay and
McKeown collected 9483 paraphrases with an av-
erage precision of 85.5%. However, 70.8% of the
paraphrases were single words. In addition, the
paraphrases were required to be contiguous.
Lin and Pantel (2001) used a general text corpus
to extract what they called inference rules, which
we can take to be paraphrases. In their algorithm,
rules are represented as dependency tree paths be-
tween two words. The words at the ends of a path
are considered to be features of that path. For each
path, they recorded the different features (words)
that were associated with the path and their respec-
tive frequencies. Lin and Pantel calculated the sim-
ilarity of two paths by looking at the similarity of
their features. This method allowed them to extract
inference rules of moderate length from general cor-
pora. However, the technique is computationally
expensive, and furthermore can give misleading re-
sults, i.e., paths having the opposite meaning often
share similar features.
3 Approach
Our approach, like Barzilay and McKeown?s, is built
on the application of sentence-alignment techniques
used in machine translation to generate paraphrases.
The insight is simple: if we have pairs of sentences
with the same semantic content, then the difference
in lexical content can be attributed to variations in
the surface form. By generalizing these differences
we can automatically derive paraphrases. Barzilay
and McKeown perform this learning process by only
1For example, ?dog? and ?cat? are recognized to be similar,
but they are obviously not paraphrases of one another.
considering the local context of words and their fre-
quencies; as a result, paraphrases must be contigu-
ous, and in the majority of cases, are only one word
long. We believe that disregarding the rich syntactic
structure of language is an oversimplification, and
that structural paraphrases offer several distinct ad-
vantages over lexical paraphrases. Long distance re-
lations can be captured by syntactic trees, so that
words in the paraphrases do not need to be contigu-
ous. Use of syntactic trees also buffers against mor-
phological variants (e.g., different inflections) and
some syntactic variants (e.g., active vs. passive).
Finally, because paraphrases are context-dependent,
we believe that syntactic structures can encapsulate
a richer context than lexical phrases.
Based on aligned monolingual corpora, our tech-
nique for extracting paraphrases builds on Lin and
Pantel?s insight of using dependency paths (derived
from parsing) as the fundamental unit of learning
and using parts of those paths as features. Based
on the hypothesis that paths between identical words
in aligned sentences are semantically equivalent, we
can extract paraphrases by scoring the path fre-
quency and context. Our approach addresses the
limitations of both Barzilay and McKeown?s and
Lin and Pantel?s work: using syntactic structures al-
lows us to generate structural paraphrases, and using
aligned corpora renders the process more computa-
tionally tractable. The following sections describe
our approach in greater detail.
3.1 Corpus Alignment
Multiple English translations of foreign novels, e.g.,
Twenty Thousand Leagues Under the Sea by Jules
Verne, were used for extraction of paraphrases.
Although translations by different authors differ
slightly in their literary interpretation of the origi-
nal text, it was usually possible to find correspond-
ing sentences that have the same semantic content.
Sentence alignment was performed using the Gale
and Church algorithm (1991) with the following cost
function:
cost of substitution = 1? ncw
anw
ncw: number of common words
anw: average number of words in two strings
Here is a sample from two different translations
of Twenty Thousand Leagues Under the Sea:
Ned Land tried the soil with his feet, as if to take
possession of it.
Ned Land tested the soil with his foot, as if he were
laying claim to it.
To test the accuracy of our alignment, we man-
ually aligned 454 sentences from two different ver-
sions of Chapter 21 from Twenty Thousand Leagues
Under the Sea and compared the results of our au-
tomatic alignment algorithm against the manually
generated ?gold standard.? We obtained a precision
of 0.93 and recall of 0.88, which is comparable to
the numbers (P.94/R.85) reported by Barzilay and
McKeown, who used a different cost function for the
alignment process.
3.2 Parsing and Postprocessing
The sentence pairs produced by the alignment al-
gorithm are then parsed by the Link Parser (Sleator
and Temperly, 1993), a dependency-based parser de-
veloped at CMU. The resulting parse structures are
post-processed to render the links more consistent:
Because the Link Parser does not directly identify
the subject of a passive sentence, our postprocessor
takes the object of the by-phrase as the subject by
default. For our purposes, auxiliary verbs are ig-
nored; the postprocessor connects verbs directly to
their subjects, discarding links through any auxiliary
verbs. In addition, subjects and objects within rela-
tive clauses are appropriately modified so that the
linkages remained consistent with subject and object
linkages in the matrix clause. For sentences involv-
ing verbs that have particles, the Link Parser con-
nects the object of the verb directly to the verb it-
self, attaching the particle separately. Our postpro-
cessor modifies the link structure so that the object
is connected to the particle in order to form a contin-
uous path. Predicate adjectives are converted into an
adjective-noun modification link instead of a com-
plete verb-argument structure. Also, common nouns
denoting places and people are marked by consult-
ing WordNet.
3.3 Paraphrase Extraction
The paraphrase extraction process starts by finding
anchors within the aligned sentence pairs. In our ap-
proach, only nouns and pronouns serve as possible
anchors. The anchor words from the sentence pairs
are brought into alignment and scored by a simple
set of ordered heuristics:
? Exact string matches denote correspondence.
? Noun and matching pronoun (same gender and
number) denote correspondence. Such a match
penalizes the score by 50%.
? Unique semantic class (e.g., places and people)
denotes correspondence. Such a match penal-
izes the score by 50%.
? Unique part of speech (i.e., the only noun
pair in the sentences) denotes correspondence.
Such a match penalizes the score by 50%.
? Otherwise, attempt to find correspondence by
finding longest common substrings. Such a
match penalizes the score by 50%.
? If a word occurs more than once in the aligned
sentence pairs, all possible combinations are
considered, but the score for such a correspond-
ing anchor pair is further penalized by 50%.
For each pair of anchors, a breadth-first search is
used to find the shortest path between the anchor
words. The search algorithm explicitly rejects paths
that contain conjunctions and punctuation. If valid
paths are found between anchor pairs in both of the
aligned sentences, the resulting paths are considered
candidate paraphrases, with a default score of one
(subjected to penalties imposed by imperfect anchor
matching).
Scores of candidate paraphrases take into account
two factors: the frequency of anchors with respect
to a particular candidate paraphrase and the variety
of different anchors from which the paraphrase was
produced. The initial default score of any paraphrase
is one (assuming perfect anchor matches), but for
each additional occurrence the score is incremented
by 1
2
n
, where n is the number of times the current
set of anchors has been seen. Therefore, the effect of
seeing new sets of anchors has a big initial impact on
the score, but the additional increase in score is sub-
jected to diminishing returns as more occurrences of
the same anchor are encountered.
count
aligned sentences 27479
parsed aligned sentences 25292
anchor pairs 43974
paraphrases 5925
unique paraphrases 5502
gathered paraphrases (score ? 1.0) 2886
Table 1: Summary of the paraphrase generation pro-
cess
Figure 1: Distribution of paraphrase length
4 Results
Using the approach described in previous sections,
we were able to extract nearly six thousand different
paraphrases (see Table 1) from our corpus, which
consisted of two translations of 20,000 Leagues Un-
der the Sea, two translations of The Kreutzer Sonata,
and three translations of Madame Bouvary.
Our corpus was essentially the same as the one
used by Barzilay and McKeown, with the exception
of some short fairy tale translations that we found to
be unsuitable. Due to the length of sentences (some
translations were noted for their paragraph-length
sentences), the Link Parser was unable to produce
a parse for approximately eight percent of the sen-
tences. Although the Link Parser is capable of pro-
ducing partial linkages, accuracy deteriorated signif-
icantly as the length of the input string increased.
The distribution of paraphrase length is shown in
Figure 1. The length of paraphrases is measured by
the number of words that it contains (discounting the
anchors on both sides).
To evaluate the accuracy of our results, 130
Evaluator Precision
Evaluator 1 36.2%
Evaluator 2 40.0%
Evaluator 3 44.6%
Average 41.2%
Table 2: Summary of judgments by human evalua-
tors for 130 unique paraphrases
unique paraphrases were randomly chosen to be
assessed by human judges. The human assessors
were specifically asked whether they thought the
paraphrases were roughly interchangeable with each
other, given the context of the genre. We believe that
the genre constraint was important because some
paraphrases captured literary or archaic uses of par-
ticular words that were not generally useful. This
should not be viewed as a shortcoming of our ap-
proach, but rather an artifact of our corpus. In ad-
dition, sample sentences containing the structural
paraphrases were presented as context to the judges;
structural paraphrases are difficult to comprehend
without this information.
A summary of the judgments provided by human
evaluators is shown in Table 2. The average preci-
sion of our approach stands at just over forty per-
cent; the average length of the paraphrases learned
was 3.26 words long. Our results also show that
judging structural paraphrases is a difficult task and
inter-assessor agreement is rather low. All of the
evaluators agreed on the judgments (either positive
or negative) only 75.4% of the time. The average
correlation constant of the judgments is only 0.66.
The highest scoring paraphrase was the equiva-
lence of the possessive morpheme ?s with the prepo-
sition of. We found it encouraging that our algo-
rithm was able to induce this structural paraphrase,
complete with co-indexed anchors on the ends of the
paths, i.e., A?s B ?? B of A. Some other interest-
ing examples include:2
A
1
?
?? liked O?? A
2
??
A
1
?
?? fond OF?? of J?? A
2
Example: The clerk liked Monsieur Bovary. ??
2Brief description of link labels: S: subject to verb; O: object
to verb; OF: certain verbs to of; K: verbs to particles; MV: verbs
to certain modifying phrases. See Link Parser documentation
for full descriptions.
Score Threshold Avg. Precision Avg. Length Count
? 1.0 40.2% 3.24 130
? 1.25 46.0% 2.88 58
? 1.5 47.8% 2.22 23
? 1.75 38.9% 1.67 12
Table 3: Breakdown of our evaluation results
The clerk was fond of Monsieur Bovary.
A
1
s
?? rush K?? over MV?? to J?? A
2
??
A
1
s
?? run
MV
?? to
J
?? A
2
Example: And he rushed over to his son, who had
just jumped into a heap of lime to whiten his shoes.
?? And he ran to his son, who had just precipi-
tated himself into a heap of lime in order to whiten
his boots.
A
1
s
?? put K?? on O?? A
2
??
A
1
s
?? wear
O
?? A
2
Example: That is why he puts on his best waistcoat
and risks spoiling it in the rain. ?? That?s why he
wears his new waistcoat, even in the rain!
A
1
?
?? fit MV?? to I?? give O?? A
2
??
A
1
?
?? appropriate MV?? to I?? supply O?? A
2
Example: He thought fit, after the first few mouth-
fuls, to give some details as to the catastrophe. ??
After the first few mouthfuls he considered it appro-
priate to supply a few details concerning the catas-
trophe.
A more detailed breakdown of the evaluation re-
sults can be seen in Table 3. Increasing the thresh-
old for generating paraphrases tends to increase their
precision, up to a certain point. In general, the high-
est ranking structural paraphrases consisted of sin-
gle word paraphrases of prepositions, e.g., at ??
in. Our algorithm noticed that different prepositions
were often interchangeable, which is something that
our human assessors disagreed widely on. Beyond a
certain threshold, the accuracy of our approach ac-
tually decreases.
5 Discussion
An obvious first observation about our algorithm is
the dependence on parse quality; bad parses lead to
many bogus paraphrases. Although the parse results
from the Link Parser are far from perfect, it is un-
clear whether other purely statistical parsers would
fare any better, since they are generally trained on
corpora containing a totally different genre of text.
However, future work will most likely include a
comparison of different parsers.
Examination of our results show that a better no-
tion of constituency would increase the accuracy
of our results. Our algorithm occasionally gener-
ates non-sensical paraphrases that cross constituent
boundaries, for example, including the verb of a
subordinate clause with elements from the matrix
clause. Other problems arise because our current al-
gorithm has no notion of verb phrases; it often gen-
erates near misses such as fail?? succeed, neglect-
ing to include not as part of the paraphrase.
However, there are problems inherent in para-
phrase generation that simple knowledge of con-
stituency alone cannot solve. Consider the following
two sentences:
John made out gold at the bottom of the well.
John discovered gold near the bottom of the well.
Which structural paraphrases should we be able to
extract?
made out X at Y?? discovered X near Y
made out X?? discovered X
at X?? near X
Arguably, all three paraphrases are valid, although
opinions vary more regarding the last paraphrase.
What is the optimal level of structure for para-
phrases? Obviously, this represents a tradeoff be-
tween specificity and accuracy, but the ability of
structural paraphrases to capture long-distance re-
lationships across large numbers of lexical items
complicates the problem. Due to the sparseness of
our data, our algorithm cannot make a good deci-
sion on what constituents to generalize as variables;
naturally, greater amounts of data would alleviate
this problem. This current inability to decide on a
good ?scope? for paraphrasing was a primary rea-
son why we were unable to perform a strict eval-
uation of recall. Our initial attempts at generating
a gold standard for estimating recall failed because
human judges could not agree on the boundaries of
paraphrases.
The accuracy of our structural paraphrases is
highly dependent on the corpus size. As can be seen
from the numbers in Table 1, paraphrases are rather
sparse?nearly 93% of them are unique. Without
adequate statistical evidence, validating candidate
paraphrases can be very difficult. Although our data
spareness problem can be alleviated simply by gath-
ering a larger corpus, the type of parallel text our
algorithm requires is rather hard to obtain, i.e., there
are only so many translations of so many foreign
novels. Furthermore, since our paraphrases are ar-
guably genre-specific, different applications may re-
quire different training corpora. Similar to the work
of Barzilay and Lee (2003), who have applied para-
phrase generation techniques to comparable corpora
consisting of different newspaper articles about the
same event, we are currently attempting to solve the
data sparseness problem by extending our approach
to non-parallel corpora.
We believe that generating paraphrases at the
structural level holds several key advantages over
lexical paraphrases, from the capturing of long-
distance relationships to the more accurate modeling
of context. The paraphrases generated by our ap-
proach could prove to be useful in any natural lan-
guage application where understanding of linguis-
tic variations is important. In particular, we are at-
tempting to apply our results to improve the perfor-
mance of question answering system, which we will
describe in the following section.
6 Paraphrases and Question Answering
The ultimate goal of our work on paraphrases is
to enable the development of high-precision ques-
tion answering system (cf. (Katz and Levin, 1988;
Soubbotin and Soubbotin, 2001; Hermjakob et al,
2002)). We believe that a knowledge base of para-
phrases is the key to overcoming challenges pre-
sented by the expressiveness of natural languages.
Because the same semantic content can be expressed
in many different ways, a question answering sys-
tem must be able to cope with a variety of alternative
phrasings. In particular, an answer stated in a form
that differs from the form of the question presents
significant problems:
When did Colorado become a state?
(1a) Colorado became a state in 1876.
(1b) Colorado was admitted to the Union in 1876.
Who killed Abraham Lincoln?
(2a) John Wilkes Booth killed Abraham Lincoln.
(2b) John Wilkes Booth ended Abraham Lincoln?s
life with a bullet.
In the above examples, question answering sys-
tems have little difficulty extracting answers if the
answers are stated in a form directly derived from
the question, e.g., (1a) and (2a); simple keyword
matching techniques with primitive named-entity
detection technology will suffice. However, ques-
tion answering systems will have a much harder time
extracting answers from sentences where they are
not obviously stated, e.g., (1b) and (2b). To re-
late question to answers in those examples, a system
would need access to rules like the following:
X became a state in Y??
X was admitted to the Union in Y
X killed Y?? X ended Y?s life
We believe that such rules are best formulated at
the syntactic level: structural paraphrases represent
a good level of generality and provide much more
accurate results than keyword-based approaches.
The simplest approach to overcoming the ?para-
phrase problem? in question answering is via key-
word query expansion when searching for candidate
answers:
(AND X became state)??
(AND X admitted Union)
(AND X killed)??
(AND X ended life)
The major drawback of such techniques is over-
generation of bogus answer candidates. For ex-
ample, it is a well-known result that query expan-
sion based on synonymy, hyponymy, etc. may actu-
ally degrade performance if done in an uncontrolled
manner (Voorhees, 1994). Typically, keyword-
based query expansion techniques sacrifice signifi-
cant amounts of precision for little (if any) increase
in recall.
The problems associated with keyword query ex-
pansion techniques stem from the fundamental de-
ficiencies of ?bag-of-words? approaches; in short,
they simply cannot accurately model the semantic
content of text, as illustrated by the following pairs
of sentences and phrases that have the same word
content, but dramatically different meaning:
(3a) The bird ate the snake.
(3b) The snake ate the bird.
(4a) the largest planet?s volcanoes
(4b) the planet?s largest volcanoes
(5a) the house by the river
(5b) the river by the house
(6a) The Germans defeated the French.
(6b) The Germans were defeated by the French.
The above examples are nearly indistinguishable
in terms of lexical content, yet their meanings are
vastly different. Naturally, because one text frag-
ment might be an appropriate answer to a question
while the other fragment may not be, a question an-
swering system seeking to achieve high precision
must provide mechanisms for differentiating the se-
mantic content of the pairs.
While paraphrase techniques at the keyword-level
vastly overgenerate, paraphrase techniques at the
phrase-level undergenerate, that is, they are often
too specific. Although paraphrase rules can eas-
ily be formulated at the string-level, e.g., using
regular expression matching and substitution tech-
niques (Soubbotin and Soubbotin, 2001; Hermjakob
et al, 2002), such a treatment fails to capture im-
portant linguistic generalizations. For example, the
addition of an adverb typically does not alter the va-
lidity of a paraphrase; thus, a phrase-level rule ?X
killed Y? ?? ?X ended Y?s life? would not be able
to match an answer like ?John Wilkes Booth sud-
denly ended Abraham Lincoln?s life with a bullet?.
String-level paraphrases are also unable to handle
syntactic phenomenona like passivization, which are
easily captured at the syntactic level.
We believe that answering questions at level of
syntactic relations, that is, matching parsed rep-
resentations of questions with parsed representa-
tions of candidates, addresses the issues presented
above. Syntactic relations, basically simplified ver-
sions of dependency structures derived from the
Link Parser, can capture significant portions of the
meaning present in text documents, while providing
a flexible foundation on which to build machinery
for paraphrases.
Our position is that question answering should be
performed at the level of ?key relations? in addition
to keywords. We have begun to experiment with re-
lations indexing and matching techniques described
above using an electronic encyclopedia as the test
corpus. We identified a particular set of linguistic
phenomena where relation-based indexing can dra-
matically boost the precision of a question answer-
ing system (Katz and Lin, 2003). As an example,
consider a sample output from a baseline keyword-
based IR system:
What do frogs eat?
(R1) Alligators eat many kinds of small animals
that live in or near the water, including fish, snakes,
frogs, turtles, small mammals, and birds.
(R2) Some bats catch fish with their claws, and a
few species eat lizards, rodents, birds, and frogs.
(R3) Bowfins eat mainly other fish, frogs, and cray-
fish.
(R4) Adult frogs eat mainly insects and other small
animals, including earthworms, minnows, and spi-
ders.
. . .
(R32) Kookaburras eat caterpillars, fish, frogs, in-
sects, small mammals, snakes, worms, and even
small birds.
Of the 32 sentences returned, only (R4) correctly
answers the user query; the other results answer a
different question??What eats frogs?? A bag-of-
words approach fundamentally cannot differentiate
between a query in which the frog is in the subject
position and a query in which the frog is in the object
position. Compare this to the results produced by
our relations matcher:
What do frogs eat?
(R4) Adult frogs eat mainly insects and other small
animals, including earthworms, minnows, and spi-
ders.
By examining subject-verb-object relations, our
system can filter out irrelevant results and return
only the correct responses.
We are currently working on combining this
relations-indexing technology with the automatic
paraphrase generation technology described earlier.
For example, our approach would be capable of au-
tomatically learning a paraphrase like X eat Y?? Y
is a prey of X; a large collection of such paraphrases
would go a long way in overcoming the brittleness
associated with a relations-based indexing scheme.
7 Contributions
We have presented a method for automatically learn-
ing structural paraphrases from aligned monolingual
corpora that overcomes the limitation of previous
approaches. In addition, we have sketched how this
technology can be applied to enhance the perfor-
mance of a question answering system based on in-
dexing relations. Although we have not completed
a task-based evaluation, we believe that the ability
to handle variations in language is key to building
better question answering systems.
8 Acknowledgements
This research was funded by DARPA under contract
number F30602-00-1-0545 and administered by the
Air Force Research Laboratory.
References
Regina Barzilay and Lillian Lee. 2003. Learning to
paraphrase: An unsupervised approach using multiple-
sequence alignment. In Proceedings of HLT-NAACL
2003.
Regina Barzilay and Kathleen McKeown. 2001. Extract-
ing paraphrases from a parallel corpus. In Proceed-
ings of the 39th Annual Meeting of the Association for
Computational Linguistics (ACL-2001).
Mark Dras. 1999. Tree Adjoining Grammar and the Re-
luctant Paraphrasing of Text. Ph.D. thesis, Macquarie
University, Australia.
William A. Gale and Kenneth Ward Church. 1991. A
program for aligning sentences in bilingual corpora.
In Proceedings of the 29th Annual Meeting of the As-
sociation for Computational Linguistics (ACL-1991).
Ulf Hermjakob, Abdessamad Echihabi, and Daniel
Marcu. 2002. Natural language based reformulation
resource andWeb exploitation for question answering.
In Proceedings of the Eleventh Text REtrieval Confer-
ence (TREC 2002).
Christian Jacquemin, Judith L. Klavans, and Evelyne
Tzoukermann. 1997. Expansion of multi-word terms
for indexing and retrieval using morphology and syn-
tax. In Proceedings of the 35th Annual Meeting of
the Association for Computational Linguistics (ACL-
1997).
Boris Katz and Beth Levin. 1988. Exploiting lexical
regularities in designing natural language systems. In
Proceedings of the 12th International Conference on
Computational Linguistics (COLING-1988).
Boris Katz and Jimmy Lin. 2003. Selectively using re-
lations to improve precision in question answering. In
Proceedings of the EACL-2003 Workshop on Natural
Language Processing for Question Answering.
Maria Lapata. 2001. A corpus-based account of reg-
ular polysemy: The case of context-sensitive adjec-
tives. In Proceedings of the Second Meeting of the
North American Chapter of the Association for Com-
putational Linguistics (NAACL-2001).
Dekang Lin and Patrick Pantel. 2001. DIRT?discovery
of inference rules from text. In Proceedings of the
ACM SIGKDD Conference on Knowledge Discovery
and Data Mining.
Dekang Lin. 1998. Extracting collocations from text cor-
pora. In Proceedings of the First Workshop on Com-
putational Terminology.
Fernando Pereira, Naftali Tishby, and Lillian Lee. 1993.
Distributional clustering of English words. In Pro-
ceedings of the 30th AnnualMeeting of the Association
for Computational Linguistics (ACL-1991).
Daniel Sleator and Davy Temperly. 1993. Parsing En-
glish with a link grammar. In Proceedings of the Third
International Workshop on Parsing Technology.
Martin M. Soubbotin and Sergei M. Soubbotin. 2001.
Patterns of potential answer expressions as clues to the
right answers. In Proceedings of the Tenth Text RE-
trieval Conference (TREC 2001).
Ellen M. Voorhees. 1994. Query expansion using
lexical-semantic relations. In Proceedings of the
17th Annual International ACM SIGIR Conference on
Research and Development in Information Retrieval
(SIGIR-1994).
Fine-Grained Lexical Semantic Representations and
Compositionally-Derived Events in Mandarin Chinese
Jimmy Lin
MIT Computer Science and Artificial Intelligence Laboratory
Cambridge, MA 02139
jimmylin@csail.mit.edu
Abstract
Current lexical semantic representations for
natural language applications view verbs as
simple predicates over their arguments. These
structures are too coarse-grained to capture
many important generalizations about verbal
argument structure. In this paper, I specifi-
cally defend the following two claims: verbs
have rich internal structure expressible in terms
of finer-grained primitives of meaning, and at
least for some languages, verbal meaning is
compositionally derived from these primitive
elements. I primarily present evidence from
Mandarin Chinese, whose verbal system is very
different from that of English. Many empiri-
cal facts about the typology of verbs in Man-
darin cannot be captured by a ?flat? lexical se-
mantic representation. These theoretical results
hold important practical consequences for nat-
ural language processing applications.
1 Introduction
Lexical semantics is becoming increasingly important in
a variety of natural language applications from machine
translation to text summarization to question answering.
Since it is generally agreed that the verb is the locus of
?meaning? in a natural language sentence, theories of ver-
bal argument structure are extremely important for our
understanding of lexical semantics.
An appropriate lexical semantic representation can il-
luminate difficult problems in language processing, ex-
pose facets of meaning relevant to the surface realization
of sentential elements, and reveal insights about the or-
ganization of the human language faculty. In machine
translation, a ?good? representation of verbs can straight-
forwardly capture cross-linguistic divergences in the ex-
pression of arguments. In question answering, lexical se-
mantics can be leveraged to bridge the gap between the
way a question is asked and the way an answer is stated.
This paper explores fine-grained lexical seman-
tic representations?approaches that view a verb as
more than a simple predicate of its arguments (e.g.,
Dang et al, 2000). This contrasts with recent semantic
annotation projects such as PropBank (Kingsbury and
Palmer, 2002) and FrameNet (Baker et al, 1998). For
example, while it is undeniable that throw(John, the ball,
Mary), is a valid representation for the sentence ?John
threw the ball to Mary?, it is widely believed (at least by
theoretical linguists) that decomposing verbs in terms of
more basic primitives can better capture generalizations
about verb meaning and argument realization. I will ar-
gue that finer-grained semantics is not only theoretically
motivated, but necessary for building applications.
I first provide a brief overview of theories of verbal ar-
gument structure, and then contrast the typology of Man-
darin verbs with that of English verbs. I will present evi-
dence from Chinese that verb meaning is compositionally
?built up? from primitive notions of stativity and activity.
The consequence, therefore, is that ?flat? representations
lacking internal structure are unable to capture the verbal
semantics of a language like Mandarin. Productive phe-
nomena such as verbal compounding render enumeration
of all permissible verbs impossible. Verb meaning, there-
fore, must be represented decompositionally in terms of
underlying primitives. This paper does not propose a con-
crete lexical semantic representation, but rather focuses
on the requirements, for natural language applications, of
such a representation.
2 Event Types
The earliest theory of verbal argument structure involves
generalized collections of semantic roles, known as a
case frame (Fillmore, 1968) or a theta-grid (Stowell,
1981) under the framework of Government and Binding
Theory. The idea of semantic roles was first explicated
in Fillmore?s seminal paper, ?The Case for Case? (1968),
which argues that the propositional component of a sen-
tence can be represented as an array consisting of the verb
and a number of noun phrases specifically marked with
roles such as agent, patient, instrument, and goal. These
labels identify the grammatically relevant aspects of the
roles that pertain to argument realization in the syntax. A
verb is defined by the semantic roles that it ?takes?, i.e.,
its case frame. For example, love takes an agent and a pa-
tient, while frighten takes an experiencer and a stimulus.
A theory of argument structure is not complete with-
out an associated linking theory that explicitly maps ar-
guments in the lexical semantic representation (semantic
roles) to syntactic arguments. Approaches based on se-
mantic roles often formulate a linking theory in terms of a
thematic hierarchy (Jackendoff, 1972): semantic roles are
arranged in an abstract ?prominence? hierarchy, and the
realization of syntactic arguments is based on the position
of roles in this hierarchy. The highest role in the thematic
hierarchy is assigned the highest argument position in the
syntactic structure (the subject), the next highest role is
assigned the next highest argument, and so forth. The-
matic hierarchies are believed to be an independent and
irreducible module of grammar.
There has been considerable debate over the ordering
of roles on thematic hierarchies. In fact, the actual in-
ventory of semantic roles, along with precise definitions
and diagnostics, remains an unsolved problem. These are
not the only drawbacks associated with theories of argu-
ment structure that rely on semantic roles:1 Some anal-
yses show that semantic roles are too coarse-grained to
account for certain semantic distinctions. The only re-
course, to expand the collection of roles, comes with the
price of increased complexity, e.g., in the linking rules.
Fillmore?s original assumption that each noun phrase in
an utterance occupies a unique thematic role is often
called into question. For some verbs, e.g., resemble, mul-
tiple noun phrases appear to have the same semantic role.
Finally, because case frames are ?flat?, i.e., lacking any
internal structure, a theory based purely on semantic roles
lacks real explanatory power. Why is it, for example, that
love takes an obligatory agent and an obligatory patient?
Why is the instrument role in open optional? These theo-
ries cannot offer satisfactory answers because they do not
directly refer to the meaning of predicates.
Recognizing the drawbacks of theories based purely on
semantic roles, there is now a general consensus among
linguists that argument structure is (to a large extent)
predictable from event semantics?hence, patterns of ar-
gument realization should be inferable from lexical se-
mantic representations grounded in a theory of events.
These event representations typically decompose seman-
1see (Dowty, 1991) and (Levin and Rappaport Hovav, 1996)
tic roles in terms of primitive predicates representing
concepts such as causality, agentivity, inchoativity, and
stativity (Dowty, 1979; Jackendoff, 1983; Pustejovsky,
1991b; Rappaport Hovav and Levin, 1998).
3 From Event Types to Event Structure
Although Aristotle (Metaphysics 1048b) observed that
the meanings of some verbs involve an ?end? or a ?re-
sult?, and other do not, it wasn?t until the twentieth cen-
tury that philosophers and linguists developed a clas-
sification of event types which captures logical entail-
ments and the co-occurrence restrictions between verbs
and other syntactic elements such as tenses and adver-
bials. Vendler?s (1957) classification of events into states,
activities, accomplishments, and achievements is ground-
breaking in this respect. In his event ontology, activi-
ties and states both depict situations that are inherently
temporally unbounded (atelic); states denote static situa-
tions, whereas activities denote on-going dynamic situa-
tions. Accomplishments and achievements both express a
change of state, and hence are temporally bounded (telic);
achievements are punctual, whereas accomplishments ex-
tend over a period of time. Examples of the four event
types are given below:
(1)
States Activities
know run
believe walk
desire push a cart
Accomplishments Achievements
paint a picture recognize
make a chair find
deliver a sermon lose
Although activities group naturally with states
and accomplishments with achievements in terms
of telicity, it has also been observed that states
can be grouped with achievements and activities
with accomplishments in that that first pair lacks
the progressive tense, while the second pair allows
them (cf. Lakoff, 1966; Shi, 1988). To capture these
properties, Vendler?s classes can be further decomposed
in terms of independent features (cf. Andersen, 1990;
Van Valin and LaPolla, 1997:91-102):
(2) a. State: [?telic, ?durative, ?dynamic]
b. Activity: [?telic, +durative, +dynamic]
c. Achievement: [+telic, ?durative +dynamic]
d. Accomplishment: [+telic, +durative
+dynamic]
Vendler?s work on ontological types of events serves
as a foundation upon which others have grounded lexi-
cal semantic representations and theories of verbal argu-
ment structure. Dowty?s seminal work (1979) attempts
to decompose states, activities, accomplishments, and
achievements in terms of the primitives DO, CAUSE, and
BECOME:
(3) a. state: pin(?1, . . . , ?n)
b. activity: DO(?1, [pin(?1, . . . , ?n)])
c. achievement: BECOME[pin(?1, . . . , ?n)]
d. accomplishment:
[[ DO(?1, [pin(?1, . . . , ?n)])] CAUSE
[ BECOME [pin(?1, . . . , ?n)]]]
(Dowty, 1979:123-124)
Examples of Dowty?s theory applied to English sen-
tences are shown below:
(4) a. He sweeps the floor clean.
[ [ DO(he, sweeps(the floor)) ] CAUSE
[ BECOME [ clean(the floor) ] ] ]
b. John walks.
[ DO(John, walk) ]
In what later becomes a standard analysis adopted by
subsequent linguists, Dowty breaks causative sentences
down into two subevents: a causing subevent and a result
subevent. The representation of the resultative sentence
(4a) is comprised of the causing subevent ?he sweeps
the floor? and the result subevent ?the floor is clean?.
Unergative verbs, on the other hand, are represented by
a single subevent with the primitive DO.
Rappaport Hovav and Levin?s more recent theory of
event templates (1998) also defines a basic inventory of
event building blocks in terms of Vendler?s event types:
(5) a. [ x ACT<MANNER> ] (activity)
b. [ x <STATE> ] (state)
c. [ BECOME [ x <STATE> ] ] (achievement)
d. [ x CAUSE [ BECOME [ y <STATE> ] ] ]
(accomplishment)
e. [ [ x ACT<MANNER> ] CAUSE [ BECOME
[ y <STATE> ] ] ] (accomplishment)
(Rappaport Hovav and Levin, 1998:108)
A verb?s meaning consists of a constant paired with a
particular event template drawn from the basic inventory
above. Constants are open-class items drawn from a fixed
ontology (e.g., manner, instrument, state, etc.) and are
represented within the angle brackets of the event tem-
plate. An important claim of this theory is that verbs di-
rectly encode, or lexicalize, complex event structures.
To account for complex events and secondary predi-
cation, Rappaport Hovav and Levin propose a process
called Template Augmentation that allows basic event
templates to be freely ?augmented? to any other event
template. This process, for example, explains the resulta-
tive form of surface contact verbs like sweep:
(6) a. Phil swept the floor.
[ Phil ACT<SWEEP> floor ]
b. Phil swept the floor clean.
[ [ Phil ACT<SWEEP> floor ] CAUSE
[ BECOME [ floor <CLEAN> ] ] ]
In this case, an activity has been augmented into an ac-
complishment through the addition of another subevent,
i.e., the floor becoming clean (note similarities with
Dowty?s representation). In order to bring the lexical
semantic representation ?into alignment? with syntactic
structure for the purpose of argument realization, Levin
and Rappaport Hovav (1995) propose well-formedness
constraints and linking rules such as the following:
(7) a. Immediate Cause Linking Rule. The argument
of a verb that denotes the immediate cause of
the eventuality described by that verb is its
external argument.
b. Directed change Linking Rule. The argument
of the verb that corresponds to the entity
undergoing the directed change described by
that verb is its internal argument.
Vendler?s ontology of verbal types has paved the way
for many important developments in lexical semantics.
Although the role of lexical aspect in argument realiza-
tion has been called into question (Levin, 2000), this
generally-accepted classification of events figures promi-
nently in most theories of verbal argument structure. Of
great interest both theoretically and for the purposes of
building language applications, therefore, is the typologi-
cal organization of verbal systems in different languages.
Can Vendler?s event type ontology, which was originally
developed for English, be directly applied to other lan-
guages as well? The answer, I will demonstrate, at least
for Mandarin Chinese, is no.
4 The Mandarin Verbal System
I will argue that the typology of Mandarin Chinese verbs
is very different from that of English verbs. Specifically,
I make the following claims:
(8) a. Activity and state are the only two primitive
verbal types in Mandarin Chinese.
Accomplishments and achievements are
derived compositionally.
b. With a small number of exceptions, there are
no monomorphemic verbs in Mandarin that
are telic?no monomorphemic verb
necessarily encodes a result, an end state, or
the attainment of a goal.
c. The particle le, among other uses, signals
inchoativity.
The somewhat controversial claim that Mandarin lacks
monomorphemic accomplishments and achievements has
been previously made by a number of linguists, most no-
tably Tai (1984); see also (Shi, 1988). These works serve
as a starting point for my inquiry into the typological or-
ganization of Mandarin verbs.
One important bit of evidence is the existence of ac-
tivity/achievement verb pairs in English, which are not
present in Mandarin:
(9)
English
activity achievement
look (at) see
listen (to) hear
study learn
look for find
(10)
Mandarin
activity achievement
kan4 ?look? kan4 jian4 ?look-perceive?
= see
ting1 ?listen? ting1 jian4 ?listen-perceive?
= hear
xue2 ?study? xue2 hui4 ?study-able?
= learn
zhao3 ?look for? zhao3 dao4 ?look.for-arrive?
= find
In English, for example, the verb look expresses
an atelic activity, while the verb see expresses a telic
achievement that lexicalizes the attainment of a goal (i.e.,
the successful act of perception). Mandarin Chinese,
however, does not have monomorphemic counterparts
for English achievements. To encode an end state, Chi-
nese speakers must resort to resultative verb compounds,
where the first verb denotes the activity, and the second
verb denotes the result. For verbs of perception, two dif-
ferent result morphemes are typically used: jian4, best
glossed as ?perceive?, and dao4, literally ?arrive?.
The claim that resultative verb compounds are required
to explicitly encode the result state is supported by the
grammaticality of sentences that explicitly deny the at-
tainment of the goal:
(11) ta1
he
kan4
look
le5
LE
ban4
half
tian1,
day
ke3shi4
but
mei2
not-have
kan4
look
jian4
perceive
?He looked for a long time, but couldn?t see it.?
In contrast, using a resultative verb compound in the
first clause triggers a contradiction:
(12) *ta1
he
kan4
look
jian4
perceive
le5
LE
ban4
half
tian1,
day
ke3shi4
but
mei2
not-have
kan4
look
jian4
perceive
intended: ?He saw for a long time, but couldn?t see
it.?
Another important bit of evidence comes from the in-
terpretations of accomplishments. In English, accom-
plishments are compatible with both in and for adver-
bials, the standard diagnostic for telicity:
(13) a. John wrote a letter for an hour.
b. John wrote a letter in a hour.
As demonstrated in the above example, writing a letter
can be interpreted as either atelic (13a) or telic (13b). The
atelic interpretation is to be understood as ?John engaged
in the activity of letter writing for an hour?, whereas the
telic interpretation implies the completion of the letter.
Both readings are generally available, but in the past
tense, the telic accomplishment is much more salient.
Thus, to deny the completion of the goal renders the sen-
tence decidedly odd:
(14) #John wrote a letter yesterday, but he didn?t finish
it.
It is, however, not very difficult to construct a context
that renders the above sentence felicitous:
(15) John is always writing letters, but he never finishes
any of them. In fact, John wrote a letter yesterday,
but as usual, he didn?t finish it.
The situation in Mandarin, however, is very different.
It appears that the Chinese counterpart of write, xie3, has
no reading that necessarily implies completion of the di-
rect object (incremental theme):
(16) wo3
I
zou2tian1
yesterday
xie3
write
le5
LE
yi1
one
feng1
CL
xin4,
letter
ke3shi4
but
mei2
not-have
xie3
write
wan2
finish
?I wrote a letter yesterday, but I didn?t finish it.?
In fact, the only way to encode completion of the letter
writing is, once again, through a resultative verb com-
pound such as xie3 wan2 ?write-finish?.
I have thus far demonstrated that the Mandarin
equivalent of many English verbs cannot be expressed
monomorphemically, but rather must involve a verbal
compound. In order to defend my claims, however, the
following (apparent) counterexamples must be explained:
(17) a. shu4
tree
dao3
fall
le5
LE
?The tree fell.?
b. bo1li2
glass
sui4
shatter
le5
LE
?The glass shattered.?
It appears that dao3 and sui4 are monomorphemic
verbs that express change of state. In order for my claims
to be correct, I would have to demonstrate that such verbs
are actually derived from more basic forms. Indeed, this
is the case: the examples above are derived from underly-
ing stative predicates?the particle le signals inchoativity.
The following stative/inchoative minimal pair presents
evidence for my theory:
(18) a. shu4
tree
gao1
tall
shi2
ten
gung1fen1
centimeter
?The tree is ten centimeters tall.?
b. shu4
tree
gao1
tall
le5
LE
shi2
ten
gung1fen1
centimeter
?The tree grew ten centimeters.?
The only difference in the two above sentences is the
presence/absence of le. The particle, therefore, must con-
tribute the semantic component of inchoativity. Similar
minimal pairs related to prenominal modifiers show this
same contrast:
(19) a. sui4
shattered
(de5)
DE
bo1li2
glass
?shattered glass? (stative/adjective)
b. sui4
shattered
le5
LE
de5
DE
bo1li2
glass
?glass that was shattered? (resultative
participle)
The above pair represents a subtle but detectable dif-
ference in meaning; whereas (19a) describes a pure state,
(19b) describes the result of an event. This distinction ex-
actly parallels the difference between an open door and
an opened door in English. Once again, since the sen-
tences differ only by le, the particle must be contributing
that semantic component. As further evidence, consider
the following minimal pair:
(20) a. Zhang1san1
Zhangsan
you3
has
yi1
one
da4
big
bi3
amount
qian2
money
?Zhangsan has a lot of money.?
b. Zhang1san1
Zhangsan
you4
has
le5
LE
yi1
one
da4
big
bi3
amount
qian2
money
?Zhangsan has acquired a lot of money.?
Once again, the addition of le creates a change of state
acquire out of a simple stative predicate have. The se-
mantic contribution of the particle le is also seen in a sub-
ordinate clause:
(21) a. wo3
I
kan4
see
jian4
perceive
shu4
tree
dao3
fall
zhai4
at
lu4
road
bian1
side
?I see the fallen tree at the side of the road.?
(tree may have fallen a long time ago)
b. wo3
I
kan4
see
jian4
perceive
shu4
tree
dao3
fall
le5
LE
zhai4
at
lu4
road
bian1
side
?I see the tree falling at the side of the road.?
(eye witness account)
Once again, the stative reading is contrasted with the
change of state reading. The interpretation of the above
two sentences is consistent with the analysis of le as a
signal of inchoativity.
It is clear from the above minimal pairs that the particle
le combines with stative predicates to gives rise to change
of state interpretations. Are these derived events achieve-
ments or accomplishments? Dowty (1979) provides the
following diagnostics:
(22)
compatible with complement
progressive? of stop
state no ok
activity yes ok
accomplishment yes ok
achievement maybe bad
Accomplishments are generally compatible with the
progressive; some achievements appear felicitous (e.g.,
okis winning), while others do not (e.g., *is noticing).
Accomplishments, since they are durative, are generally
acceptable as the complement of stop, whereas the punc-
tual nature of achievements renders them ungrammatical.
These diagnostics clearly demonstrate that the addition of
le shifts stative predicates into achievements:
(23) a. *bo1li2
glass
zheng4zai4
in.process.of
sui4
shatter
le5
LE
man3
whole
di4
floor
intended: ?The glass is in the process of
shattering all over the floor.?
b. *bo1li2
glass
ting2zhi3
stop
sui4
shatter
le5
LE
man3
whole
di4
floor
intended: ?The glass stopped shattering all
over the floor.?
It is interesting to note that many achievements in Man-
darin cannot directly causativize into the transitive form:
(24) a. *Zhang1san1
Zhangsan
dao3
fall
le5
LE
shu4
tree
intended: ?Zhangsan fell the tree.?
b. ??Zhang1san1
Zhangsan
sui4
shatter
le5
LE
bo1li2
glass
intended: ?Zhangsan shattered the glass.?
Instead, a resultative verb compound is necessary to
express an accomplishment. Typically, the second verb
denotes the result (end state) of the event, while the first
verb denotes the activity that brings about the end state:
(25) a. Zhang1san1
Zhangsan
kan3
chop
dao3
fall
le5
LE
shu4
tree
?Zhangsan chopped the tree down.?
b. Zhang1san1
Zhangsan
da3
hit
sui4
shatter
le5
LE
bo1li2
glass
?Zhangsan shattered the glass.?
Putting all the pieces together, the organization of the
Mandarin verbal system can be summarized as follows:
(26) primitive event types: activity, state
state + le ? achievement
activity + achievement ? accomplishment
Activity and state are the two primitive verbal cate-
gories in Mandarin. Non-causative change of state predi-
cates (achievements) are derived from states with the ad-
dition of the particle le. Accomplishments are further de-
rived from achievements through the formation of resul-
tative verb compounds in which the first verb denotes an
activity, and the second verb the end state.
Traditionally, the particle le that appears post-verbally
has been analyzed as an aspectual marker denoting per-
fectivity (Li and Thompson, 1981). This contrasts with
my analysis of it as a signal of inchoativity. How are
these two approaches to be reconciled? In (Lin, 2004b),
I argue that le is a reflex, rather than an overt realiza-
tion of the underlying inchoative marker. As generally
defined, perfective aspect is not compatible with stative
predicates. However, the addition of a covert inchoative
functional head, in effect, licenses the perfective aspect.
5 Computational Significance?
Why is this peculiar organization of the Mandarin verbal
system important for lexical semantic representations de-
signed for language applications? It demonstrates that, at
least for languages such as Mandarin Chinese, the verb
phrase must be rich in internal structure; a verb cannot be
simply viewed as a predicate of its arguments. Evidence
from Mandarin resultative verb compounds demonstrate
that verbal predicates themselves must be composition-
ally built from underlying primitives.
It is important to note that the formation of verbal com-
pounds in Chinese is a fully productive process?the only
constraint on verb combinations appears to stem from
plausible real-world associations between cause and ef-
fect. The following shows but a small range of possible
resultative verb compounds with the dao3 ?fall? result:
(27) kan3 dao3 chop-fall to chop down
zhuang4 dao3 crash-fall to knock over
tui1 dao3 push-fall to push over
la1 dao3 pull-fall to pull down
In principle, verbal compound formation in Mandarin
could be a lexical process, but I present elsewhere in-
dependent evidence for a non-lexicalist approach that
captures these constraints in the theoretical framework
of Distributed Morphology, an extension of Chomsky?s
Minimalist Program (Lin, 2004a; Lin, 2004b). How-
ever, the actual machinery for formalizing these insights
is not important for the present discussion. The important
lessons are the theoretical constraints imposed by verbal
typology on lexical semantic representations designed for
language applications. More specifically:
(28) a. verbs have rich internal structure expressible
in terms of finer-grained primitives of
meaning, and
b. at least for some languages, verbal meaning is
compositionally derived from these primitive
elements.
These claims imply that a PropBank or FrameNet ap-
proach to lexical semantics will not be sufficient for many
language applications, at least for languages such as
Mandarin Chinese. While I may disagree with the tech-
nical details, I believe that the approach taken by (Dang
et al, 2000) is on the right path. Due to the produc-
tivity of verbal phenomena in Mandarin, it is impossi-
ble to exhaustively enumerate all felicitous predicates?
verbal meaning, therefore, must be compositionally de-
rived from primitive elements. This however, does not
mean that PropBank or FrameNet are not useful; quite
the contrary! Existing semantic resources serve as the
foundation from which we can bootstrap finer-grained se-
mantic representations.
While the approach Palmer and Wu (1995) take to lex-
ical semantics captures many selectional restrictions and
finer-grained facets of meaning, it still does not model
the arbitrary productivity of verbal compounds. For the
purposes of translating English change of state verbs into
Mandarin, they developed a conceptual lattice that uni-
fies verbs from both languages. Distances between nodes
in this lattice correspond to ?semantic distance?, and is
used to find the closest translation if a specific meaning is
unavailable. Although this approach results in better lex-
ical selection, the semantic lattice still assumes that all
verbal forms can be exhaustively enumerated. Although
this certainly may be true within the context of a specific
corpus, the productivity of Mandarin verbal phenomena
is limitless in the real world.
I believe that, for all languages in general, verbal
meanings are compositionally built up from states and
activities. Furthermore, this process is syntactic in na-
ture (Lin, 2004b), governed by well-known syntactic pro-
cesses such as MERGE (Chomsky, 1995) and subjected
to well-studied constraints such as selectional restric-
tions and the Head Movement Constraint (Travis, 1984).
This contrasts with Rappaport Hovav and Levin?s (1998)
?event template? approach, which is lexicalist in that
large chunks of event structure are directly associated
with verbs. Under their analysis, the lexical entry associ-
ated with sui4 ?shatter? would be something like:
(29) sui4 ?shatter? =
[ [ x ACT<UNDEF> ] CAUSE [ BECOME
[ x <SHATTERED> ] ] ]
Rappaport Hovav and Levin?s theory argues that a
verb?s meaning is composed of an event template that
captures the structural component of meaning and open-
class constants that capture the idiosyncratic component
of meaning (represented by items in angle brackets). This
separation is a major breakthrough in lexical semantic
theories because it allows grammatically relevant facets
of meaning to be untangled from facets of meaning not
directly relevant to the encoding of arguments. Descrip-
tively, the structural component of meaning is what a verb
shares with other verbs in the same verb class, whereas
the idiosyncratic component of meaning is what separates
verbs within the same verb class.
In Rappaport Hovav and Levin?s account of verbal ar-
gument structure, complex event representations are di-
rectly introduced in the syntax; that is, the verb lexi-
calizes a complete causative accomplishment?to shatter
implicates an agent participating in an unspecified activ-
ity that brings about a change of state where an entity
becomes shattered. In English, they propose that intran-
sitive verbs are derived by a process of ?decausativiza-
tion? through which the external argument is ?absorbed?,
and therefore remains unexpressed (Levin and Rappaport
Hovav, 1995). Such a theory is unable to account for the
derivation of Mandarin resultatives such as da3 sui4 ?hit-
shatter?. If (29) is indeed the representation of sui4 ?shat-
ter?, then what is the lexical semantic representation of
da3 ?hit?? There are, in principle, two alternatives:
(30) Option 1: da3 ?hit? = [ x ACT<HIT> ]
Option 2: da3 ?hit? = < HIT >
One might suggest that da3 ?hit? is associated with
its own event template that somehow gets merged with
the lexical entry of sui4 ?shatter?. In order for this ap-
proach to be tenable, one has to explicate the process by
which verbs are ?fused? (and in many cases, how argu-
ments of both verbs are sometimes merged or remain un-
expressed); Li (1990) provides exactly such a lexical ac-
count, although it has been found to be problematic for
many cases (Cheng and Huang, 1994). The other op-
tion is to suggest that da3 ?hit? merely encodes the id-
iosyncratic component of meaning, without an associated
event template. This, however, cannot be true because
da3 ?hit? itself can be used as a main verb:
(31) Zhang1san1
Zhangsan
da3
hit
le5
LE
bo1li2
glass
?Zhangsan hit the glass.?
The only plausible solution is that verbs encode small
fragments of event structure, which are compositionally
built up by regular syntactic processes. This approach
also provides a natural solution for handling verbs that
are derivationally related to other lexical categories, e.g.,
deadjectival verbs such as flatten, widen, modernize, and
legalize. These derivational affixes obviously contribute
the inchoative component of meaning that turns states
(adjectives) into change of states:
(32) flat: [state flat]
-en: ?s?x.BECOME(x, BE(s))
flat-en: ?x.BECOME(x, BE([state flat]))
In such a treatment, for example, the complete se-
mantics of a word can be compositionally derived from
its component morphemes. This framework, where the
?semantic load? is spread more evenly throughout the
lexicon to lexical categories not typically thought to
bear semantic content, is essentially the model advo-
cated by Pustejovsky (1991a), among others. Such an
analysis of verbal phenomena marks a departure from
the standard architectural view of morphological analysis
as a preprocessor?instead, morphological and syntactic
derivation can be integrated under a common framework.
6 Conclusion
The key claim of this paper is that results from the
theoretical study of verbal argument structure are rele-
vant to computational lexical semantic representations for
language applications. Although the simplest possible
argument representation treats verbs as predicates over
their arguments, I have demonstrated that this approach
is woefully inadequate for handling a language such as
Mandarin Chinese. I have presented evidence that verb
meaning in Mandarin is compositionally built up from
underlying state and activity primitives?this organiza-
tion of the verbal system must be mirrored by any lex-
ical semantic representation aspiring to capture general-
izations about argument realization patterns. This paper
takes an important step in laying out some of the con-
straints for such a representation.
References
Roger W. Andersen. 1990. Papiamentu tense-aspect,
with special attention to discourse. In J. V. Singler,
editor, Pidgin and Creole Tense-Mood-Aspect Sys-
tems, pages 59?96. John Benjamins, Amsterdam, The
Netherlands.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet project. In Proceedings
of the 36th Annual Meeting of the Association for Com-
putational Linguistics and 17th International Con-
ference on Computational Linguistics (COLING/ACL
1998).
Lisa Lai-Shen Cheng and C.-T. James Huang. 1994. On
the argument structure of resultative compounds. In
Matthew Chen and Ovid Tzeng, editors, In honor of
William S.-Y. Wang Interdisciplinary Studies on Lan-
guage and Language Change, pages 187?221. Pyra-
mid Press, Taipei, Taiwan.
Noam Chomsky. 1995. The Minimalist Program. MIT
Press, Cambridge, Massachusetts.
Hoa Trang Dang, Karin Kipper, and Martha Palmer.
2000. Integrating compositional semantics into a verb
lexicon. In Proceedings of the 18th International
Conference on Computational Linguistics (COLING
2000).
David Dowty. 1979. Word Meaning and Montague
Grammar. D. Reidel Publishing Company, Dordrecht,
The Netherlands.
David Dowty. 1991. Thematic proto-roles and argument
selection. Language, 67(3):547?619.
Charles J. Fillmore. 1968. The case for case. In E. Bach
and R. Harms, editors, Universals in Linguistic The-
ory, pages 1?88. Holt, Rinehart, and Winston, New
York.
Ray Jackendoff. 1972. Semantic Interpretation in
Generative Grammar. MIT Press, Cambridge, Mas-
sachusetts.
Ray Jackendoff. 1983. Semantics and Cognition. MIT
Press, Cambridge, Massachusetts.
Paul Kingsbury and Martha Palmer. 2002. From Tree-
Bank to PropBank. In Proceedings of the Third In-
ternational Conference on Language Resources and
Evaluation (LREC-2002).
George Lakoff. 1966. Stative adjectives and verbs in En-
glish. NSF-Report 17, Harvard Computational Labo-
ratory.
Beth Levin and Malka Rappaport Hovav. 1995. Unac-
cusativity: At the Syntax-Lexical Semantics Interface,
volume 26 of Linguistic Inquiry Monograph. MIT
Press, Cambridge, Massachusetts.
Beth Levin and Malka Rappaport Hovav. 1996. From
lexical semantics to argument realization. Unpub-
lished manuscript, Northwestern University and Bar
Ilan University.
Beth Levin. 2000. Aspect, lexical semantic representa-
tion, and argument expression. In Proceedings of the
26th Annual Meeting of the Berkeley Linguistics Soci-
ety.
Charles N. Li and Sandra A. Thompson. 1981. Man-
darin Chinese: A Functional Reference Grammar.
University of California Press, Berkeley, California.
Yafei Li. 1990. On V-V compounds in Chinese. Natural
Language and Linguistic Theory, 9:177?207.
Jimmy Lin. 2004a. A computational framework for
non-lexicalist semantics. In Proceedings of the 2004
HLT/NAACL Student Research Workshop.
Jimmy Lin. 2004b. Event Structure and the Encoding of
Arguments: The Syntax of the English and Mandarin
Verb Phrase. Ph.D. thesis, Department of Electrical
Engineering and Computer Science, Massachusetts In-
stitute of Technology.
Martha Palmer and Zhibiao Wu. 1995. Verb semantics
for English-Chinese translation. IRCS Report 95-22,
Institute for Research in Cognitive Science, University
of Pennsylvania.
James Pustejovsky. 1991a. The generative lexicon.
Computational Linguistics, 17(4):409?441.
James Pustejovsky. 1991b. The syntax of event structure.
Cognition, 41:47?81.
Malka Rappaport Hovav and Beth Levin. 1998. Building
verb meanings. In Miriam Butt and Wilhelm Geuder,
editors, The Projection of Arguments: Lexical and
Compositional Factors. CSLI Publications, Stanford,
California.
Ziqiang Shi. 1988. The Present of Past of the Particle Le
in Mandarin Chinese. Ph.D. dissertation, University
of Pennsylvania.
Tim Stowell. 1981. Elements of Phrase Structure. Ph.D.
thesis, Massachusetts Institute of Technology.
James H-Y. Tai. 1984. Verbs and times in chinese:
Vendler?s four categories. In David Testen, Veena
Mishra, and Joseph Drogo, editors, Papers from the
Parasession on Lexical Semantics. Chicago Linguistic
Society, Chicago, Illinois.
Lisa Travis. 1984. Parameters and Effects of Word Order
Variation. Ph.D. dissertation, Massachusetts Institute
of Technology, Department of Linguistics.
Robert D. Van Valin and Randy J. LaPolla. 1997. Syntax.
Structure, Meaning and Function. Cambridge Univer-
sity Press, Cambridge, England.
Zeno Vendler. 1957. Verbs and times. Philosophical
Review, 56:143?160.
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 419?428,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Scalable Language Processing Algorithms for the Masses: A Case Study in
Computing Word Co-occurrence Matrices with MapReduce
Jimmy Lin
The iSchool, University of Maryland
National Center for Biotechnology Information, National Library of Medicine
jimmylin@umd.edu
Abstract
This paper explores the challenge of scaling
up language processing algorithms to increas-
ingly large datasets. While cluster comput-
ing has been available in commercial environ-
ments for several years, academic researchers
have fallen behind in their ability to work on
large datasets. I discuss two barriers contribut-
ing to this problem: lack of a suitable pro-
gramming model for managing concurrency
and difficulty in obtaining access to hardware.
Hadoop, an open-source implementation of
Google?s MapReduce framework, provides a
compelling solution to both issues. Its simple
programming model hides system-level de-
tails from the developer, and its ability to run
on commodity hardware puts cluster comput-
ing within the reach of many academic re-
search groups. This paper illustrates these
points with a case study in building word co-
occurrence matrices from large corpora. I con-
clude with an analysis of an alternative com-
puting model based on renting instead of buy-
ing computer clusters.
1 Introduction
Over the past couple of decades, the field of compu-
tational linguistics (and more broadly, human lan-
guage technologies) has seen the emergence and
later dominance of empirical techniques and data-
driven research. Concomitant with this trend is a
coherent research thread that focuses on exploiting
increasingly-large datasets. Banko and Brill (2001)
were among the first to demonstrate the importance
of dataset size as a significant factor governing pre-
diction accuracy in a supervised machine learning
task. In fact, they argued that size of training set
was perhaps more important than the choice of ma-
chine learning algorithm itself. Similarly, exper-
iments in question answering have shown the ef-
fectiveness of simple pattern-matching techniques
when applied to large quantities of data (Brill et al,
2001; Dumais et al, 2002). More recently, this
line of argumentation has been echoed in experi-
ments with Web-scale language models. Brants et
al. (2007) showed that for statistical machine trans-
lation, a simple smoothing technique (dubbed Stupid
Backoff) approaches the quality of the Kneser-Ney
algorithm as the amount of training data increases,
and with the simple method one can process signifi-
cantly more data.
Challenges in scaling algorithms to increasingly-
large datasets have become a serious issue for re-
searchers. It is clear that datasets readily available
today and the types of analyses that researchers wish
to conduct have outgrown the capabilities of individ-
ual computers. The only practical recourse is to dis-
tribute the computation across multiple cores, pro-
cessors, or machines. The consequences of failing
to scale include misleading generalizations on arti-
ficially small datasets and limited practical applica-
bility in real-world contexts, both undesirable.
This paper focuses on two barriers to develop-
ing scalable language processing algorithms: chal-
lenges associated with parallel programming and
access to hardware. Google?s MapReduce frame-
work (Dean and Ghemawat, 2004) provides an at-
tractive programming model for developing scal-
able algorithms, and with the release of Hadoop,
an open-source implementation of MapReduce lead
419
by Yahoo, cost-effective cluster computing is within
the reach of most academic research groups. It
is emphasized that this work focuses on large-
data algorithms from the perspective of academia?
colleagues in commercial environments have long
enjoyed the advantages of cluster computing. How-
ever, it is only recently that such capabilities have
become practical for academic research groups.
These points are illustrated by a case study in build-
ing large word co-occurrence matrices, a simple task
that underlies many NLP algorithms.
The remainder of the paper is organized as fol-
lows: the next section overviews the MapReduce
framework and why it provides a compelling solu-
tion to the issues sketched above. Section 3 intro-
duces the task of building word co-occurrence ma-
trices, which provides an illustrative case study. Two
separate algorithms are presented in Section 4. The
experimental setup is described in Section 5, fol-
lowed by presentation of results in Section 6. Im-
plications and generalizations are discussed follow-
ing that. Before concluding, I explore an alternative
model of computing based on renting instead of buy-
ing hardware, which makes cluster computing prac-
tical for everyone.
2 MapReduce
The only practical solution to large-data challenges
today is to distribute the computation across mul-
tiple cores, processors, or machines. The de-
velopment of parallel algorithms involves a num-
ber of tradeoffs. First is that of cost: a decision
must be made between ?exotic? hardware (e.g.,
large shared memory machines, InfiniBand inter-
connect) and commodity hardware. There is signif-
icant evidence (Barroso et al, 2003) that solutions
based on the latter are more cost effective?and for
resource-constrained academic NLP groups, com-
modity hardware is often the only practical route.
Given appropriate hardware, researchers must
still contend with the challenge of developing soft-
ware. Quite simply, parallel programming is diffi-
cult. Due to communication and synchronization
issues, concurrent operations are notoriously chal-
lenging to reason about. Reliability and fault tol-
erance become important design considerations on
clusters containing large numbers of unreliable com-
modity parts. With traditional parallel programming
models (e.g., MPI), the developer shoulders the bur-
den of explicitly managing concurrency. As a result,
a significant amount of the programmer?s attention
is devoted to system-level details, leaving less time
for focusing on the actual problem.
Recently, MapReduce (Dean and Ghemawat,
2004) has emerged as an attractive alternative to
existing parallel programming models. The Map-
Reduce abstraction shields the programmer from
having to explicitly worry about system-level is-
sues such as synchronization, inter-process commu-
nication, and fault tolerance. The runtime is able
to transparently distribute computations across large
clusters of commodity hardware with good scaling
characteristics. This frees the programmer to focus
on solving the problem at hand.
MapReduce builds on the observation that many
information processing tasks have the same basic
structure: a computation is applied over a large num-
ber of records (e.g., Web pages, bitext pairs, or nodes
in a graph) to generate partial results, which are
then aggregated in some fashion. Naturally, the per-
record computation and aggregation function vary
according to task, but the basic structure remains
fixed. Taking inspiration from higher-order func-
tions in functional programming, MapReduce pro-
vides an abstraction at the point of these two opera-
tions. Specifically, the programmer defines a ?map-
per? and a ?reducer? with the following signatures:
map: (k1, v1)? [(k2, v2)]
reduce: (k2, [v2])? [(k3, v3)]
Key-value pairs form the basic data structure in
MapReduce. The mapper is applied to every input
key-value pair to generate an arbitrary number of in-
termediate key-value pairs ([. . .] is used to denote a
list). The reducer is applied to all values associated
with the same intermediate key to generate output
key-value pairs. This two-stage processing structure
is illustrated in Figure 1.
Under the framework, a programmer needs only
to provide implementations of the mapper and re-
ducer. On top of a distributed file system (Ghe-
mawat et al, 2003), the runtime transparently han-
dles all other aspects of execution, on clusters rang-
ing from a few to a few thousand nodes. The run-
time is responsible for scheduling map and reduce
420
Shu
fflin
g: g
rou
p va
lues
 by 
key
s
ma
p
ma
p
ma
p
ma
p
red
uce
red
uce
red
uce
inp
ut
inp
ut
inp
ut
inp
ut
out
put
out
put
out
put
Figure 1: Illustration of the MapReduce framework: the
?mapper? is applied to all input records, which generates
results that are aggregated by the ?reducer?. The runtime
groups together values by keys.
workers on commodity hardware assumed to be un-
reliable, and thus is tolerant to various faults through
a number of error recovery mechanisms. In the dis-
tributed file system, data blocks are stored on the
local disks of machines in the cluster?the Map-
Reduce runtime handles the scheduling of mappers
on machines where the necessary data resides. It
also manages the potentially very large sorting prob-
lem between the map and reduce phases whereby in-
termediate key-value pairs must be grouped by key.
As an optimization, MapReduce supports the use
of ?combiners?, which are similar to reducers except
that they operate directly on the output of mappers
(in memory, before intermediate output is written to
disk). Combiners operate in isolation on each node
in the cluster and cannot use partial results from
other nodes. Since the output of mappers (i.e., the
key-value pairs) must ultimately be shuffled to the
appropriate reducer over a network, combiners al-
low a programmer to aggregate partial results, thus
reducing network traffic. In cases where an opera-
tion is both associative and commutative, reducers
can directly serve as combiners.
Google?s proprietary implementation of Map-
Reduce is in C++ and not available to the public.
However, the existence of Hadoop, an open-source
implementation in Java spearheaded by Yahoo, al-
lows anyone to take advantage of MapReduce. The
growing popularity of this technology has stimu-
lated a flurry of recent work, on applications in ma-
chine learning (Chu et al, 2006), machine transla-
tion (Dyer et al, 2008), and document retrieval (El-
sayed et al, 2008).
3 Word Co-occurrence Matrices
To illustrate the arguments outlined above, I present
a case study using MapReduce to build word co-
occurrence matrices from large corpora, a common
task in natural language processing. Formally, the
co-occurrence matrix of a corpus is a square N ?
N matrix where N corresponds to the number of
unique words in the corpus. A cell mij contains the
number of times word wi co-occurs with word wj
within a specific context?a natural unit such as a
sentence or a certain window of m words (where m
is an application-dependent parameter). Note that
the upper and lower triangles of the matrix are iden-
tical since co-occurrence is a symmetric relation.
This task is quite common in corpus linguistics
and provides the starting point to many other algo-
rithms, e.g., for computing statistics such as point-
wise mutual information (Church and Hanks, 1990),
for unsupervised sense clustering (Schu?tze, 1998),
and more generally, a large body of work in lexi-
cal semantics based on distributional profiles, dat-
ing back to Firth (1957) and Harris (1968). The
task also has applications in information retrieval,
e.g., (Schu?tze and Pedersen, 1998; Xu and Croft,
1998), and other related fields as well. More gen-
erally, this problem relates to the task of estimating
distributions of discrete events from a large number
of observations (more on this in Section 7).
It is obvious that the space requirement for this
problem is O(N2), where N is the size of the vocab-
ulary, which for real-world English corpora can be
hundreds of thousands of words. The computation
of the word co-occurrence matrix is quite simple if
the entire matrix fits into memory?however, in the
case where the matrix is too big to fit in memory,
a naive implementation can be very slow as mem-
ory is paged to disk. For large corpora, one needs
to optimize disk access and avoid costly seeks. As
illustrated in the next section, MapReduce handles
exactly these issues transparently, allowing the pro-
grammer to express the algorithm in a straightfor-
ward manner.
A bit more discussion of the task before mov-
ing on: in many applications, researchers have
discovered that building the complete word co-
occurrence matrix may not be necessary. For ex-
ample, Schu?tze (1998) discusses feature selection
421
techniques in defining context vectors; Mohammad
and Hirst (2006) present evidence that conceptual
distance is better captured via distributional profiles
mediated by thesaurus categories. These objections,
however, miss the point?the focus of this paper
is on practical cluster computing for academic re-
searchers; this particular task serves merely as an
illustrative example. In addition, for rapid proto-
typing, it may be useful to start with the complete
co-occurrence matrix (especially if it can be built ef-
ficiently), and then explore how algorithms can be
optimized for specific applications and tasks.
4 MapReduce Implementation
This section presents two MapReduce algorithms
for building word co-occurrence matrices for large
corpora. The goal is to illustrate how the prob-
lem can be concisely captured in the MapReduce
programming model, and how the runtime hides
many of the system-level details associated with dis-
tributed computing.
Pseudo-code for the first, more straightforward,
algorithm is shown in Figure 2. Unique document
ids and the corresponding texts make up the input
key-value pairs. The mapper takes each input doc-
ument and emits intermediate key-value pairs with
each co-occurring word pair as the key and the inte-
ger one as the value. In the pseudo-code, EMIT de-
notes the creation of an intermediate key-value pair
that is collected (and appropriately sorted) by the
MapReduce runtime. The reducer simply sums up
all the values associated with the same co-occurring
word pair, arriving at the absolute counts of the joint
event in the corpus (corresponding to each cell in the
co-occurrence matrix).
For convenience, I refer to this algorithm as the
?pairs? approach. Since co-occurrence is a symmet-
ric relation, it suffices to compute half of the matrix.
However, for conceptual clarity and to generalize to
instances where the relation may not be symmetric,
the algorithm computes the entire matrix.
The Java implementation of this algorithm is quite
concise?less than fifty lines long. Notice the Map-
Reduce runtime guarantees that all values associated
with the same key will be gathered together at the re-
duce stage. Thus, the programmer does not need to
explicitly manage the collection and distribution of
1: procedure MAP1(n, d)
2: for all w ? d do
3: for all u ? NEIGHBORS(w) do
4: EMIT((w, u), 1)
1: procedure REDUCE1(p, [v1, v2, . . .])
2: for all v ? [v1, v2, . . .] do
3: sum? sum+ v
4: EMIT(p, sum)
Figure 2: Pseudo-code for the ?pairs? approach for com-
puting word co-occurrence matrices.
1: procedure MAP2(n, d)
2: INITIALIZE(H)
3: for all w ? d do
4: for all u ? NEIGHBORS(w) do
5: H{u} ? H{u}+ 1
6: EMIT(w,H)
1: procedure REDUCE2(w, [H1, H2, H3, . . .])
2: INITIALIZE(Hf )
3: for all H ? [H1, H2, H3, . . .] do
4: MERGE(Hf , H)
5: EMIT(w,Hf )
Figure 3: Pseudo-code for the ?stripes? approach for
computing word co-occurrence matrices.
partial results across a cluster. In addition, the pro-
grammer does not need to explicitly partition the in-
put data and schedule workers. This example shows
the extent to which distributed processing can be
dominated by system issues, and how an appropriate
abstraction can significantly simplify development.
It is immediately obvious that Algorithm 1 gen-
erates an immense number of key-value pairs. Al-
though this can be mitigated with the use of a com-
biner (since addition is commutative and associa-
tive), the approach still results in a large amount of
network traffic. An alternative approach is presented
in Figure 3, first reported in Dyer et al (2008).
The major difference is that counts of co-occurring
words are first stored in an associative array (H).
The output of the mapper is a number of key-value
pairs with words as keys and the corresponding asso-
ciative arrays as the values. The reducer performs an
element-wise sum of all associative arrays with the
same key (denoted by the function MERGE), thus ac-
422
cumulating counts that correspond to the same cell
in the co-occurrence matrix. Once again, a com-
biner can be used to cut down on the network traffic
by merging partial results. In the final output, each
key-value pair corresponds to a row in the word co-
occurrence matrix. For convenience, I refer to this
as the ?stripes? approach.
Compared to the ?pairs? approach, the ?stripes?
approach results in far fewer intermediate key-value
pairs, although each is significantly larger (and there
is overhead in serializing and deserializing associa-
tive arrays). A critical assumption of the ?stripes?
approach is that at any point in time, each associa-
tive array is small enough to fit into memory (other-
wise, memory paging may result in a serious loss of
efficiency). This is true for most corpora, since the
size of the associative array is bounded by the vo-
cabulary size. Section 6 compares the efficiency of
both algorithms.1
5 Experimental Setup
Work reported in this paper used the English Gi-
gaword corpus (version 3),2 which consists of
newswire documents from six separate sources, to-
taling 7.15 million documents (6.8 GB compressed,
19.4 GB uncompressed). Some experiments used
only documents from the Associated Press World-
stream (APW), which contains 2.27 million docu-
ments (1.8 GB compressed, 5.7 GB uncompressed).
By LDC?s count, the entire collection contains ap-
proximately 2.97 billion words.
Prior to working with Hadoop, the corpus was
first preprocessed. All XML markup was removed,
followed by tokenization and stopword removal us-
ing standard tools from the Lucene search engine.
All tokens were replaced with unique integers for a
more efficient encoding. The data was then packed
into a Hadoop-specific binary file format. The entire
Gigaword corpus took up 4.69 GB in this format; the
APW sub-corpus, 1.32 GB.
Initial experiments used Hadoop version 0.16.0
running on a 20-machine cluster (1 master, 19
slaves). This cluster was made available to the Uni-
1Implementations of both algorithms are included in
Cloud9, an open source Hadoop library that I have been de-
veloping to support research and education, available from my
homepage.
2LDC catalog number LDC2007T07
versity of Maryland as part of the Google/IBM Aca-
demic Cloud Computing Initiative. Each machine
has two single-core processors (running at either 2.4
GHz or 2.8 GHz), 4 GB memory. The cluster has an
aggregate storage capacity of 1.7 TB. Hadoop ran on
top of a virtualization layer, which has a small but
measurable impact on performance; see (Barham et
al., 2003). Section 6 reports experimental results
using this cluster; Section 8 explores an alternative
model of computing based on ?renting cycles?.
6 Results
First, I compared the running time of the ?pairs? and
?stripes? approaches discussed in Section 4. Run-
ning times on the 20-machine cluster are shown
in Figure 4 for the APW section of the Gigaword
corpus: the x-axis shows different percentages of
the sub-corpus (arbitrarily selected) and the y-axis
shows running time in seconds. For these experi-
ments, the co-occurrence window was set to two,
i.e., wi is said to co-occur with wj if they are no
more than two words apart (after tokenization and
stopword removal).
Results demonstrate that the stripes approach is
far more efficient than the pairs approach: 666 sec-
onds (11m 6s) compared to 3758 seconds (62m 38s)
for the entire APW sub-corpus (improvement by a
factor of 5.7). On the entire sub-corpus, the map-
pers in the pairs approach generated 2.6 billion in-
termediate key-value pairs totally 31.2 GB. After the
combiners, this was reduced to 1.1 billion key-value
pairs, which roughly quantifies the amount of data
involved in the shuffling and sorting of the keys. On
the other hand, the mappers in the stripes approach
generated 653 million intermediate key-value pairs
totally 48.1 GB; after the combiners, only 28.8 mil-
lion key-value pairs were left. The stripes approach
provides more opportunities for combiners to aggre-
gate intermediate results, thus greatly reducing net-
work traffic in the sort and shuffle phase.
Figure 4 also shows that both algorithms exhibit
highly desirable scaling characteristics?linear in
the corpus size. This is confirmed by a linear regres-
sion applied to the running time data, which yields
R2 values close to one. Given that the stripes algo-
rithm is more efficient, it is used in the remainder of
the experiments.
423
 0 500 1000 1500
 2000 2500 3000 3500
 4000
 0  20  40  60  80  100running time (seconds) percentage of the APW sub-corpora of the English Gigaword
Efficiency comparison of approaches to computing word co-occurrence matrices
R2 = 0.992
R2 = 0.999"stripes" approach"pairs" approach
Figure 4: Running time of the two algorithms (?stripes? vs. ?pairs?) for computing word co-occurrence matrices on
the APW section of the Gigaword corpus. The cluster used for this experiment contains 20 machines, each with two
single-core processors.
 0 1000 2000
 3000 4000 5000
 6000
 1  2  3  4  5  6  7running time (seconds) window size (number of words)
Running time for different widow sizesR2 = 0.992
Figure 5: Running times for computing word co-occurrence matrices from the entire Gigaword corpus with varying
window sizes. The cluster used for this experiment contains 20 machines, each with two single-core processors.
424
With a window size of two, computing the word
co-occurrence matrix for the entire Gigaword corpus
(7.15 million documents) takes 37m 11s on the 20-
machine cluster. Figure 5 shows the running time
as a function of window size. With a window of
six words, running time on the complete Gigaword
corpus rises to 1h 23m 45s. Once again, the stripes
algorithm exhibits the highly desirable characteris-
tic of linear scaling in terms of window size, as con-
firmed by the linear regression with an R2 value very
close to one.
7 Discussion
The elegance of the programming model and good
scaling characteristics of resulting implementations
make MapReduce a compelling tool for a variety
of natural language processing tasks. In fact, Map-
Reduce excels at a large class of problems in NLP
that involves estimating probability distributions of
discrete events from a large number of observations
according to the maximum likelihood criterion:
PMLE(B|A) =
c(A,B)
c(A)
=
c(A,B)
?
B? c(A,B?)
(1)
In practice, it matters little whether these events
are words, syntactic categories, word alignment
links, or any construct of interest to researchers. Ab-
solute counts in the stripes algorithm presented in
Section 4 can be easily converted into conditional
probabilities by a final normalization step. Recently,
Dyer et al (2008) used this approach for word align-
ment and phrase extraction in statistical machine
translation. Of course, many applications require
smoothing of the estimated distributions?this prob-
lem also has known solutions in MapReduce (Brants
et al, 2007).
Synchronization is perhaps the single largest bot-
tleneck in distributed computing. In MapReduce,
this is handled in the shuffling and sorting of key-
value pairs between the map and reduce phases. De-
velopment of efficient MapReduce algorithms criti-
cally depends on careful control of intermediate out-
put. Since the network link between different nodes
in a cluster is by far the component with the largest
latency, any reduction in the size of intermediate
output or a reduction in the number of key-value
pairs will have significant impact on efficiency.
8 Computing on Demand
The central theme of this paper is practical clus-
ter computing for NLP researchers in the academic
environment. I have identified two key aspects of
what it means to be ?practical?: the first is an appro-
priate programming model for simplifying concur-
rency management; the second is access to hardware
resources. The Hadoop implementation of Map-
Reduce addresses the first point and to a large ex-
tent the second point as well. The cluster used for
experiments in Section 6 is modest by today?s stan-
dards and within the capabilities of many academic
research groups. It is not even a requirement for the
computers to be rack-mounted units in a machine
room (although that is clearly preferable); there are
plenty of descriptions on the Web about Hadoop
clusters built from a handful of desktop machines
connected by gigabit Ethernet.
Even without access to hardware, cluster comput-
ing remains within the reach of resource-constrained
academics. ?Utility computing? is an emerging con-
cept whereby anyone can provision clusters on de-
mand from a third-party provider. Instead of up-
front capital investment to acquire a cluster and re-
occurring maintenance and administration costs, one
could ?rent? computing cycles as they are needed?
this is not a new idea (Rappa, 2004). One such ser-
vice is provided by Amazon, called Elastic Compute
Cloud (EC2).3 With EC2, researchers could dynam-
ically create a Hadoop cluster on-the-fly and tear
down the cluster once experiments are complete. To
demonstrate the use of this technology, I replicated
some of the previous experiments on EC2 to provide
a case study of this emerging model of computing.
Virtualized computation units in EC2 are called
instances. At the time of these experiments, the ba-
sic instance offers, according to Amazon, 1.7 GB
of memory, 1 EC2 Compute Unit (1 virtual core
with 1 EC2 Compute Unit), and 160 GB of instance
storage. Each instance-hour costs $0.10 (all prices
given in USD). Computational resources are simply
charged by the instance-hour, so that a ten-instance
cluster for ten hours costs the same as a hundred-
instance cluster for one hour (both $10)?the Ama-
zon infrastructure allows one to dynamically provi-
sion and release resources as necessary. This is at-
3http://www.amazon.com/ec2
425
 0 1000 2000
 3000 4000 5000
 10  20  30  40  50  60  70  80  901x2x
3x4x1x 2x 3x 4xrunning time (seconds) relative speedupsize of EC2 cluster (number of slave instances)
Computing word co-occurrence matrices on Amazon EC2relative size of EC2 cluster$2.76 $2.92 $2.89 $2.64 $2.69 $2.63 $2.5920-machine cluster R2 = 0.997
Figure 6: Running time analysis on Amazon EC2 with various cluster sizes; solid squares are annotated with the cost
of each experiment. Alternate axes (circles) plot scaling characteristics in terms increasing cluster size.
tractive for researchers, who could on a limited basis
allocate clusters much larger than they could other-
wise afford if forced to purchase the hardware out-
right. Through virtualization technology, Amazon
is able to parcel out allotments of processor cycles
while maintaining high overall utilization across a
data center and exploiting economies of scale.
Using EC2, I built word co-occurrence matrices
from the entire English Gigaword corpus (window
of two) on clusters of various sizes, ranging from
20 slave instances all the way up to 80 slave in-
stances. The entire cluster consists of the slave in-
stances plus a master controller instance that serves
as the job submission queue; the clusters ran Hadoop
version 0.17.0 (the latest release at the time these
experiments were conducted). Running times are
shown in Figure 6 (solid squares), with varying clus-
ter sizes on the x-axis. Each data point is anno-
tated with the cost of running the complete experi-
ment.4 Results show that computing the complete
word co-occurrence matrix costs, quite literally, a
couple of dollars?certainly affordable by any aca-
demic researcher without access to hardware. For
reference, Figure 6 also plots the running time of
the same experiment on the 20-machine cluster used
4Note that Amazon bills in whole instance-hour increments;
these figures assume fractional accounting.
in Section 6 (which contains 38 worker cores, each
roughly comparable to an instance).
The alternate set of axes in Figure 6 shows the
scaling characteristics of various cluster sizes. The
circles plot the relative size and speedup of the
EC2 experiments, with respect to the 20-slave clus-
ter. The results show highly desirable linear scaling
characteristics.
The above figures include only the cost of running
the instances. One must additionally pay for band-
width when transferring data in and out of EC2. At
the time these experiments were conducted, Ama-
zon charged $0.10 per GB for data transferred in and
$0.17 per GB for data transferred out. To comple-
ment EC2, Amazon offers persistent storage via the
Simple Storage Service (S3),5 at a cost of $0.15 per
GB per month. There is no charge for data transfers
between EC2 and S3. The availability of this service
means that one can choose between paying for data
transfer or paying for persistent storage on a cyclic
basis?the tradeoff naturally depends on the amount
of data and its permanence.
The cost analysis presented above assumes
optimally-efficient use of Amazon?s services; end-
to-end cost might better quantify real-world usage
conditions. In total, the experiments reported in this
5http://www.amazon.com/s3
426
section resulted in a bill of approximately thirty dol-
lars. The figure includes all costs associated with in-
stance usage and data transfer costs. It also includes
time taken to learn the Amazon tools (I previously
had no experience with either EC2 or S3) and to
run preliminary experiments on smaller datasets (be-
fore scaling up to the complete corpus). The lack of
fractional accounting on instance-hours contributed
to the larger-than-expected costs, but such wastage
would naturally be reduced with more experiments
and higher sustained use. Overall, these cost appear
to be very reasonable, considering that the largest
cluster in these experiments (1 master + 80 slave in-
stances) might be too expensive for most academic
research groups to own and maintain.
Consider another example that illustrates the pos-
sibilities of utility computing. Brants et al (2007)
described experiments on building language models
with increasingly-large corpora using MapReduce.
Their paper reported experiments on a corpus con-
taining 31 billion tokens (about an order of magni-
tude larger than the English Gigaword): on 400 ma-
chines, the model estimation took 8 hours.6 With
EC2, such an experiment would cost a few hundred
dollars?sufficiently affordable that availability of
data becomes the limiting factor, not computational
resources themselves.
The availability of ?computing-on-demand? ser-
vices and Hadoop make cluster computing practi-
cal for academic researchers. Although Amazon is
currently the most prominent provider of such ser-
vices, they are not the sole player in an emerging
market?in the future there will be a vibrant market
with many competing providers. Considering the
tradeoffs between ?buying? and ?renting?, I would
recommend the following model for an academic re-
search group: purchase a modest cluster for devel-
opment and for running smaller experiments; use a
computing-on-demand service for scaling up and for
running larger experiments (since it would be more
difficult to economically justify a large cluster if it
does not receive high sustained utilization).
If the concept of utility computing takes hold, it
would have a significant impact on computer sci-
ence research in general: the natural implication is
6Brants et al were affiliated with Google, so access to hard-
ware was not an issue.
that algorithms should not only be analyzed in tradi-
tional terms such as asymptotic complexity, but also
in terms of monetary costs, in relationship to dataset
and cluster size. One can argue that cost is a more di-
rect and practical measure of algorithmic efficiency.
9 Conclusion
This paper address two challenges faced by aca-
demic research groups in scaling up natural lan-
guage processing algorithms to large corpora: the
lack of an appropriate programming model for ex-
pressing the problem and the difficulty in getting ac-
cess to hardware. With this case study in building
word co-occurrence matrices from large corpora, I
demonstrate that MapReduce, via the open source
Hadoop implementation, provides a compelling so-
lution. A large class of algorithms in computa-
tional linguistics can be readily expressed in Map-
Reduce, and the resulting code can be transparently
distributed across commodity clusters. Finally, the
?cycle-renting? model of computing makes access
to large clusters affordable to researchers with lim-
ited resources. Together, these developments dra-
matically lower the entry barrier for academic re-
searchers who wish to explore large-data issues.
Acknowledgments
This work was supported by the Intramural Research
Program of the NIH, National Library of Medicine;
NSF under awards IIS-0705832 and IIS-0836560;
DARPA/IPTO Contract No. HR0011-06-2-0001 un-
der the GALE program. Any opinions, findings,
conclusions, or recommendations expressed in this
paper are the author?s and do not necessarily reflect
those of the sponsors. I would like to thank Ya-
hoo! for leading the development of Hadoop, IBM
and Google for hardware support via the Academic
Cloud Computing Initiative (ACCI), and Amazon
for EC2/S3 support. This paper provides a neutral
evaluation of EC2 and S3, and should not be inter-
preted as endorsement for the commercial services
offered by Amazon. I wish to thank Philip Resnik
and Doug Oard for comments on earlier drafts of
this paper, and Ben Shneiderman for helpful editing
suggestions. I am, as always, grateful to Esther and
Kiri for their kind support.
427
References
Michele Banko and Eric Brill. 2001. Scaling to very very
large corpora for natural language disambiguation. In
Proceedings of the 39th Annual Meeting of the As-
sociation for Computational Linguistics (ACL 2001),
pages 26?33, Toulouse, France.
Paul Barham, Boris Dragovic, Keir Fraser, Steven Hand,
Tim Harris, Alex Ho, Rolf Neugebauer, Ian Pratt, and
Andrew Warfield. 2003. Xen and the art of virtualiza-
tion. In Proceedings of the 19th ACM Symposium on
Operating Systems Principles (SOSP-03), pages 164?
177, Bolton Landing, New York.
Luiz Andre? Barroso, Jeffrey Dean, and Urs Ho?lzle. 2003.
Web search for a planet: The Google cluster architec-
ture. IEEE Micro, 23(2):22?28.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och,
and Jeffrey Dean. 2007. Large language models in
machine translation. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 858?867, Prague, Czech Re-
public.
Eric Brill, Jimmy Lin, Michele Banko, Susan Dumais,
and Andrew Ng. 2001. Data-intensive question an-
swering. In Proceedings of the Tenth Text REtrieval
Conference (TREC 2001), pages 393?400, Gaithers-
burg, Maryland.
Cheng-Tao Chu, Sang Kyun Kim, Yi-An Lin, YuanYuan
Yu, Gary Bradski, Andrew Ng, and Kunle Olukotun.
2006. Map-Reduce for machine learning on multi-
core. In Advances in Neural Information Processing
Systems 19 (NIPS 2006), pages 281?288, Vancouver,
British Columbia, Canada.
Kenneth W. Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicog-
raphy. Computational Linguistics, 16(1):22?29.
Jeffrey Dean and Sanjay Ghemawat. 2004. MapReduce:
Simplified data processing on large clusters. In Pro-
ceedings of the 6th Symposium on Operating System
Design and Implementation (OSDI 2004), pages 137?
150, San Francisco, California.
Susan Dumais, Michele Banko, Eric Brill, Jimmy Lin,
and Andrew Ng. 2002. Web question answering:
Is more always better? In Proceedings of the 25th
Annual International ACM SIGIR Conference on Re-
search and Development in Information Retrieval (SI-
GIR 2002), pages 291?298, Tampere, Finland.
Chris Dyer, Aaron Cordova, Alex Mont, and Jimmy Lin.
2008. Fast, easy, and cheap: Construction of statistical
machine translation models with MapReduce. In Pro-
ceedings of the Third Workshop on Statistical Machine
Translation at ACL 2008, pages 199?207, Columbus,
Ohio.
Tamer Elsayed, Jimmy Lin, and Douglas Oard. 2008.
Pairwise document similarity in large collections with
MapReduce. In Proceedings of the 46th Annual Meet-
ing of the Association for Computational Linguis-
tics (ACL 2008), Companion Volume, pages 265?268,
Columbus, Ohio.
John R. Firth. 1957. A synopsis of linguistic theory
1930?55. In Studies in Linguistic Analysis, Special
Volume of the Philological Society, pages 1?32. Black-
well, Oxford.
Sanjay Ghemawat, Howard Gobioff, and Shun-Tak Le-
ung. 2003. The Google File System. In Proceedings
of the 19th ACM Symposium on Operating Systems
Principles (SOSP-03), pages 29?43, Bolton Landing,
New York.
Zelig S. Harris. 1968. Mathematical Structures of Lan-
guage. Wiley, New York.
Saif Mohammad and Graeme Hirst. 2006. Distribu-
tional measures of concept-distance: A task-oriented
evaluation. In Proceedings of the 2006 Conference on
Empirical Methods in Natural Language Processing
(EMNLP 2006), pages 35?43, Sydney, Australia.
Michael A. Rappa. 2004. The utility business model
and the future of computing services. IBM Systems
Journal, 34(1):32?42.
Hinrich Schu?tze and Jan O. Pedersen. 1998. A
cooccurrence-based thesaurus and two applications to
information retrieval. Information Processing and
Management, 33(3):307?318.
Hinrich Schu?tze. 1998. Automatic word sense discrimi-
nation. Computational Linguistics, 24(1):97?123.
Jinxi Xu and W. Bruce Croft. 1998. Corpus-based
stemming using cooccurrence of word variants. ACM
Transactions on Information Systems, 16(1):61?81.
428
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 931?938, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Automatically Evaluating Answers to Definition Questions
Jimmy Lin1,3 and Dina Demner-Fushman2,3
1College of Information Studies
2Department of Computer Science
3Institute for Advanced Computer Studies
University of Maryland
College Park, MD 20742, USA
jimmylin@umd.edu, demner@cs.umd.edu
Abstract
Following recent developments in the au-
tomatic evaluation of machine translation
and document summarization, we present
a similar approach, implemented in a mea-
sure called POURPRE, for automatically
evaluating answers to definition questions.
Until now, the only way to assess the cor-
rectness of answers to such questions in-
volves manual determination of whether
an information nugget appears in a sys-
tem?s response. The lack of automatic
methods for scoring system output is an
impediment to progress in the field, which
we address with this work. Experiments
with the TREC 2003 and TREC 2004 QA
tracks indicate that rankings produced by
our metric correlate highly with official
rankings, and that POURPRE outperforms
direct application of existing metrics.
1 Introduction
Recent interest in question answering has shifted
away from factoid questions such as ?What city is
the home to the Rock and Roll Hall of Fame??,
which can typically be answered by a short noun
phrase, to more complex and difficult questions.
One interesting class of information needs con-
cerns so-called definition questions such as ?Who is
Vlad the Impaler??, whose answers would include
?nuggets? of information about the 16th century
warrior prince?s life, accomplishments, and legacy.
Actually a misnomer, definition questions can be
better paraphrased as ?Tell me interesting things
about X.?, where X can be a person, an organiza-
tion, a common noun, etc. Taken another way, defi-
nition questions might be viewed as simultaneously
asking a whole series of factoid questions about the
same entity (e.g., ?When was he born??, ?What was
his occupation??, ?Where did he live??, etc.), except
that these questions are not known in advance; see
Prager et al (2004) for an implementation based on
this view of definition questions.
Much progress in natural language processing and
information retrieval has been driven by the creation
of reusable test collections. A test collection con-
sists of a corpus, a series of well-defined tasks, and
a set of judgments indicating the ?correct answers?.
To complete the picture, there must exist meaning-
ful metrics to evaluate progress, and ideally, a ma-
chine should be able to compute these values auto-
matically. Although ?answers? to definition ques-
tions are known, there is no way to automatically
and objectively determine if they are present in a
given system?s response (we will discuss why in
Section 2). The experimental cycle is thus tortuously
long; to accurately assess the performance of new
techniques, one must essentially wait for expensive,
large-scale evaluations that employ human assessors
to judge the runs (e.g., the TREC QA track). This
situation mirrors the state of machine translation and
document summarization research a few years ago.
Since then, however, automatic scoring metrics such
as BLEU and ROUGE have been introduced as stop-
gap measures to facilitate experimentation.
Following these recent developments in evalua-
931
1 vital 32 kilograms plutonium powered
2 vital seven year journey
3 vital Titan 4-B Rocket
4 vital send Huygens to probe atmosphere of Titan, Saturn?s largest moon
5 okay parachute instruments to planet?s surface
6 okay oceans of ethane or other hydrocarbons, frozen methane or water
7 vital carries 12 packages scientific instruments and a probe
8 okay NASA primary responsible for Cassini orbiter
9 vital explore remote planet and its rings and moons, Saturn
10 okay European Space Agency ESA responsible for Huygens probe
11 okay controversy, protest, launch failure, re-entry, lethal risk, humans, plutonium
12 okay Radioisotope Thermoelectric Generators, RTG
13 vital Cassini, NASA?S Biggest and most complex interplanetary probe
14 okay find information on solar system formation
15 okay Cassini Joint Project between NASA, ESA, and ASI (Italian Space Agency)
16 vital four year study mission
Table 1: The ?answer key? to the question ?What is the Cassini space probe??
tion research, we propose POURPRE, a technique for
automatically evaluating answers to definition ques-
tions. Like the abovementioned metrics, POURPRE
is based on n-gram co-occurrences, but has been
adapted for the unique characteristics of the question
answering task. This paper will show that POUR-
PRE can accurately assess the quality of answers
to definition questions without human intervention,
allowing experiments to be performed with rapid
turnaround. We hope that this will enable faster ex-
ploration of the solution space and lead to acceler-
ated advances in the state of the art.
This paper is organized as follows: In Section 2,
we briefly describe how definition questions are cur-
rently evaluated, drawing attention to many of the
intricacies involved. We discuss previous work in
Section 3, relating POURPRE to evaluation metrics
for other language applications. Section 4 discusses
metrics for evaluating the quality of an automatic
scoring algorithm. The POURPRE measure itself is
outlined in Section 5; POURPRE scores are corre-
lated with official human-generated scores in Sec-
tion 6, and also compared to existing metrics. In
Section 7, we explore the effect that judgment vari-
ability has on the stability of definition question
evaluation, and its implications for automatic scor-
ing algorithms.
2 Evaluating Definition Questions
To date, NIST has conducted two formal evaluations
of definition questions, at TREC 2003 and TREC
2004.1 In this section, we describe the setup of the
task and the evaluation methodology.
Answers to definition questions are comprised of
an unordered set of [document-id, answer string]
pairs, where the strings are presumed to provide
some relevant information about the entity being
?defined?, usually called the target. Although no
explicit limit is placed on the length of the answer
string, the final scoring metric penalizes verbosity
(discussed below).
To evaluate system responses, NIST pools answer
strings from all systems, removes their association
with the runs that produced them, and presents them
to a human assessor. Using these responses and re-
search performed during the original development of
the question, the assessor creates an ?answer key??
a list of ?information nuggets? about the target. An
information nugget is defined as a fact for which the
assessor could make a binary decision as to whether
a response contained that nugget (Voorhees, 2003).
The assessor also manually classifies each nugget as
1TREC 2004 questions were arranged around ?topics?; def-
inition questions were implicit in the ?other? questions.
932
[XIE19971012.0112] The Cassini space probe, due to be launched from Cape Canaveral in Florida of
the United States tomorrow, has a 32 kilogram plutonium fuel payload to power its seven year journey
to Venus and Saturn.
Nuggets assigned: 1, 2
[NYT19990816.0266] Early in the Saturn visit, Cassini is to send a probe named Huygens into the
smog-shrouded atmosphere of Titan, the planet?s largest moon, and parachute instruments to its hidden
surface to see if it holds oceans of ethane or other hydrocarbons over frozen layers of methane or water.
Nuggets assigned: 4, 5, 6
Figure 1: Examples of judging actual system responses.
either vital or okay. Vital nuggets represent con-
cepts that must be present in a ?good? definition;
on the other hand, okay nuggets contribute worth-
while information about the target but are not essen-
tial; cf. (Hildebrandt et al, 2004). As an example,
nuggets for the question ?What is the Cassini space
probe?? are shown in Table 1.
Once this answer key of vital/okay nuggets is cre-
ated, the assessor then manually scores each run. For
each system response, he or she decides whether or
not each nugget is present. Assessors do not sim-
ply perform string matches in this decision process;
rather, this matching occurs at the conceptual level,
abstracting away from issues such as vocabulary
differences, syntactic divergences, paraphrases, etc.
Two examples of this matching process are shown
in Figure 1: nuggets 1 and 2 were found in the top
passage, while nuggets 4, 5, and 6 were found in the
bottom passage. It is exactly this process of concep-
tually matching nuggets from the answer key with
system responses that we attempt to capture with an
automatic scoring algorithm.
The final F-score for an answer is calculated in
the manner described in Figure 2, and the final score
of a run is simply the average across the scores of all
questions. The metric is a harmonic mean between
nugget precision and nugget recall, where recall is
heavily favored (controlled by the ? parameter, set
to five in 2003 and three in 2004). Nugget recall is
calculated solely on vital nuggets, while nugget pre-
cision is approximated by a length allowance given
based on the number of both vital and okay nuggets
returned. Early on in a pilot study, researchers dis-
covered that it was impossible for assessors to con-
sistently enumerate the total set of nuggets contained
Let
r # of vital nuggets returned in a response
a # of okay nuggets returned in a response
R # of vital nuggets in the answer key
l # of non-whitespace characters in the entire
answer string
Then
recall (R) = r/R
allowance (?) = 100? (r + a)
precision (P) =
{
1 if l < ?
1? l??l otherwise
Finally, the F (?) = (?
2 + 1)? P ?R
?2 ? P +R
? = 5 in TREC 2003, ? = 3 in TREC 2004.
Figure 2: Official definition of F-measure.
in a system response, given that they were usually
extracted text fragments from documents (Voorhees,
2003). Thus, a penalty for verbosity serves as a sur-
rogate for precision.
3 Previous Work
The idea of employing n-gram co-occurrence statis-
tics to score the output of a computer system against
one or more desired reference outputs was first suc-
cessfully implemented in the BLEU metric for ma-
chine translation (Papineni et al, 2002). Since then,
the basic method for scoring translation quality has
been improved upon by others, e.g., (Babych and
Hartley, 2004; Lin and Och, 2004). The basic idea
has been extended to evaluating document summa-
rization with ROUGE (Lin and Hovy, 2003).
933
Recently, Soricut and Brill (2004) employed n-
gram co-occurrences to evaluate question answer-
ing in a FAQ domain; unfortunately, the task differs
from definition question answering, making their re-
sults not directly applicable. Xu et al (2004) applied
ROUGE to automatically evaluate answers to defi-
nition questions, viewing the task as a variation of
document summarization. Because TREC answer
nuggets were terse phrases, the authors found it nec-
essary to rephrase them?two humans were asked
to manually create ?reference answers? based on the
assessors? nuggets and IR results, which was a labor-
intensive process. Furthermore, Xu et al did not
perform a large-scale assessment of the reliability of
ROUGE for evaluating definition answers.
4 Criteria for Success
Before proceeding to our description of POURPRE, it
is important to first define the basis for assessing the
quality of an automatic evaluation algorithm. Cor-
relation between official scores and automatically-
generated scores, as measured by the coefficient of
determination R2, seems like an obvious metric for
quantifying the performance of a scoring algorithm.
Indeed, this measure has been employed in the eval-
uation of BLEU, ROUGE, and other related metrics.
However, we believe that there are better mea-
sures of performance. In comparative evaluations,
we ultimately want to determine if one technique
is ?better? than another. Thus, the system rank-
ings produced by a particular scoring method are
often more important than the actual scores them-
selves. Following the information retrieval litera-
ture, we employ Kendall?s ? to capture this insight.
Kendall?s ? computes the ?distance? between two
rankings as the minimum number of pairwise adja-
cent swaps necessary to convert one ranking into the
other. This value is normalized by the number of
items being ranked such that two identical rankings
produce a correlation of 1.0; the correlation between
a ranking and its perfect inverse is ?1.0; and the ex-
pected correlation of two rankings chosen at random
is 0.0. Typically, a value of greater than 0.8 is con-
sidered ?good?, although 0.9 represents a threshold
researchers generally aim for. In this study, we pri-
marily focus on Kendall?s ? , but also report R2 val-
ues where appropriate.
5 POURPRE
Previously, it has been assumed that matching
nuggets from the assessors? answer key with sys-
tems? responses must be performed manually be-
cause it involves semantics (Voorhees, 2003). We
would like to challenge this assumption and hypoth-
esize that term co-occurrence statistics can serve as
a surrogate for this semantic matching process. Ex-
perience with the ROUGE metric has demonstrated
the effectiveness of matching unigrams, an idea we
employ in our POURPRE metric. We hypothesize
that matching bigrams, trigrams, or any other longer
n-grams will not be beneficial, because they primar-
ily account for the fluency of a response, more rele-
vant in a machine translation task. Since answers to
definition questions are usually document extracts,
fluency is less important a concern.
The idea behind POURPRE is relatively straight-
forward: match nuggets by summing the unigram
co-occurrences between terms from each nugget and
terms from the system response. We decided to start
with the simplest possible approach: count the word
overlap and divide by the total number of terms in
the answer nugget. The only additional wrinkle is to
ensure that all words appear within the same answer
string. Since nuggets represent coherent concepts,
they are unlikely to be spread across different an-
swer strings (which are usually different extracts of
source documents). As a simple example, let?s say
we?re trying to determine if the nugget ?A B C D? is
contained in the following system response:
1. A
2. B C D
3. D
4. A D
The match score assigned to this nugget would be
3/4, from answer string 2; no other answer string
would get credit for this nugget. This provision re-
duces the impact of coincidental term matches.
Once we determine the match score for every
nugget, the final F-score is calculated in the usual
way, except that the automatically-derived match
scores are substituted where appropriate. For exam-
ple, nugget recall now becomes the sum of the match
scores for all vital nuggets divided by the total num-
ber of vital nuggets. In the official F-score calcula-
934
POURPRE ROUGE
Run micro, cnt macro, cnt micro, idf macro, idf +stop ?stop
TREC 2004 (? = 3) 0.785 0.833 0.806 0.812 0.780 0.786
TREC 2003 (? = 3) 0.846 0.886 0.848 0.876 0.780 0.816
TREC 2003 (? = 5) 0.890 0.878 0.859 0.875 0.807 0.843
Table 2: Correlation (Kendall?s ? ) between rankings generated by POURPRE/ROUGE and official scores.
POURPRE ROUGE
Run micro, cnt macro, cnt micro, idf macro, idf +stop ?stop
TREC 2004 (? = 3) 0.837 0.929 0.904 0.914 0.854 0.871
TREC 2003 (? = 3) 0.919 0.963 0.941 0.957 0.876 0.887
TREC 2003 (? = 5) 0.954 0.965 0.957 0.964 0.919 0.929
Table 3: Correlation (R2) between values generated by POURPRE/ROUGE and official scores.
tion, the length allowance?for the purposes of com-
puting nugget precision?was 100 non-whitespace
characters for every okay and vital nugget returned.
Since nugget match scores are now fractional, this
required some adjustment. We settled on an al-
lowance of 100 non-whitespace characters for every
nugget match that had non-zero score.
A major drawback of this basic unigram over-
lap approach is that all terms are considered equally
important?surely, matching ?year? in a system?s re-
sponse should count for less than matching ?Huy-
gens?, in the example about the Cassini space
probe. We decided to capture this intuition using in-
verse document frequency, a commonly-used mea-
sure in information retrieval; idf(ti) is defined as
log(N/ci), where N is the number of documents in
the collection, and ci is the number of documents
that contain the term ti. With scoring based on idf,
term counts are simply replaced with idf sums in
computing the match score, i.e., the match score of
a particular nugget is the sum of the idfs of match-
ing terms in the system response divided by the sum
of all term idfs from the answer nugget. Finally,
we examined the effects of stemming, i.e., matching
stemmed terms derived from the Porter stemmer.
In the next section, results of experiments with
submissions to TREC 2003 and TREC 2004 are re-
ported. We attempted two different methods for ag-
gregating results: microaveraging and macroaverag-
ing. For microaveraging, scores were calculated by
computing the nugget match scores over all nuggets
for all questions. For macroaveraging, scores for
each question were first computed, and then aver-
aged across all questions in the testset. With mi-
croaveraging, each nugget is given equal weight,
while with macroaveraging, each question is given
equal weight.
As a baseline, we revisited experiments by Xu
et al (2004) in using ROUGE to evaluate definition
questions. What if we simply concatenated all the
answer nuggets together and used the result as the
?reference summary? (instead of using humans to
create custom reference answers)?
6 Evaluation of POURPRE
We evaluated all definition question runs submitted
to the TREC 20032 and TREC 2004 question an-
swering tracks with different variants of our POUR-
PRE metric, and then compared the results with the
official F-scores generated by human assessors. The
Kendall?s ? correlations between rankings produced
by POURPRE and the official rankings are shown in
Table 2. The coefficients of determination (R2) be-
tween the two sets of scores are shown in Table 3.
We report four separate variants along two different
parameters: scoring by term counts only vs. scoring
by term idf, and microaveraging vs. macroaveraging.
Interestingly, scoring based on macroaveraged term
2In TREC 2003, the value of ? was arbitrarily set to five,
which was later determined to favor recall too heavily. As a
result, it was readjusted to three in TREC 2004. In our experi-
ments with TREC 2003, we report figures for both values.
935
Figure 3: Scatter graph of official scores plotted
against the POURPRE scores (macro, count) for
TREC 2003 (? = 5).
counts outperformed any of the idf variants.
A scatter graph plotting official F-scores against
POURPRE scores (macro, count) for TREC 2003
(? = 5) is shown in Figure 3. Corresponding graphs
for other variants appear similar, and are not shown
here. The effect of stemming on the Kendall?s ? cor-
relation between POURPRE (macro, count) and of-
ficial scores in shown in Table 4. Results from the
same stemming experiment on the other POURPRE
variants are similarly inconclusive.
For TREC 2003 (? = 5), we performed an anal-
ysis of rank swaps between official and POURPRE
scores. A rank swap is said to have occurred if the
relative ranking of two runs is different under dif-
ferent conditions?they are significant because rank
swaps might prevent researchers from confidently
drawing conclusions about the relative effectiveness
of different techniques. We observed 81 rank swaps
(out of a total of 1431 pairwise comparisons for 54
runs). A histogram of these rank swaps, binned by
the difference in official score, is shown in Figure 4.
As can be seen, 48 rank swaps (59.3%) occurred
when the difference in official score is less than
0.02; there were no rank swaps observed for runs
in which the official scores differed by more than
0.061. Since measurement error is an inescapable
fact of evaluation, we need not be concerned with
rank swaps that can be attributed to this factor. For
TREC 2003, Voorhees (2003) calculated this value
to be approximately 0.1; that is, in order to conclude
with 95% confidence that one run is better than an-
Run unstemmed stemmed
TREC 2004 (? = 3) 0.833 0.825
TREC 2003 (? = 3) 0.886 0.897
TREC 2003 (? = 5) 0.878 0.895
Table 4: The effect of stemming on Kendall?s ? ; all
runs with (macro, count) variant of POURPRE.
Figure 4: Histogram of rank swaps for TREC 2003
(? = 5), binned by difference in official score.
other, an absolute F-score difference greater than 0.1
must be observed. As can be seen, all the rank swaps
observed can be attributed to error inherent in the
evaluation process.
From these results, we can see that evaluation
of definition questions is relatively coarse-grained.
However, TREC 2003 was the first formal evalua-
tion of definition questions; as methodologies are re-
fined, the margin of error should go down. Although
a similar error analysis for TREC 2004 has not been
performed, we expect a similar result.
Given the simplicity of our POURPRE metric,
the correlation between our automatically-derived
scores and the official scores is remarkable. Starting
from a set of questions and a list of relevant nuggets,
POURPRE can accurately assess the performance of
a definition question answering system without any
human intervention.
6.1 Comparison Against ROUGE
We choose ROUGE over BLEU as a baseline for
comparison because, conceptually, the task of an-
swering definition questions is closer to summariza-
tion than it is to machine translation, in that both are
recall-oriented. Since the majority of question an-
936
swering systems employ extractive techniques, flu-
ency (i.e., precision) is not usually an issue.
How does POURPRE stack up against using
ROUGE3 to directly evaluate definition questions?
The Kendall?s ? correlations between rankings pro-
duced by ROUGE (with and without stopword re-
moval) and the official rankings are shown in Ta-
ble 2; R2 values are shown in Table 3. In all cases,
ROUGE does not perform as well.
We believe that POURPRE better correlates with
official scores because it takes into account special
characteristics of the task: the distinction between
vital and okay nuggets, the length penalty, etc. Other
than a higher correlation, POURPRE offers an advan-
tage over ROUGE in that it provides a better diag-
nostic than a coarse-grained score, i.e., it can reveal
why an answer received a particular score. This al-
lows researchers to conduct failure analyses to iden-
tify opportunities for improvement.
7 The Effect of Variability in Judgments
As with many other information retrieval tasks,
legitimate differences in opinion about relevance
are an inescapable fact of evaluating definition
questions?systems are designed to satisfy real-
world information needs, and users inevitably dis-
agree on which nuggets are important or relevant.
These disagreements manifest as scoring variations
in an evaluation setting. The important issue, how-
ever, is the degree to which variations in judgments
affect conclusions that can be drawn in a compar-
ative evaluation, i.e., can we still confidently con-
clude that one system is ?better? than another? For
the ad hoc document retrieval task, research has
shown that system rankings are stable with respect to
disagreements about document relevance (Voorhees,
2000). In this section, we explore the effect of judg-
ment variability on the stability and reliability of
TREC definition question answering evaluations.
The vital/okay distinction on nuggets is one major
source of differences in opinion, as has been pointed
out previously (Hildebrandt et al, 2004). In the
Cassini space probe example, we disagree with the
assessors? assignment in many cases. More impor-
tantly, however, there does not appear to be any op-
3We used ROUGE-1.4.2 with n set to 1, i.e. unigram match-
ing, and maximum matching score rating.
Figure 5: Distribution of rank placement using ran-
dom judgments (for top two runs from TREC 2004).
erationalizable rules for classifying nuggets as either
vital or okay. Without any guiding principles, how
can we expect our systems to automatically recog-
nize this distinction?
How do differences in opinion about vital/okay
nuggets impact the stability of system rankings? To
answer this question, we measured the Kendall?s ?
correlation between the official rankings and rank-
ings produced by different variations of the answer
key. Three separate variants were considered:
? all nuggets considered vital
? vital/okay flipped (all vital nuggets become
okay, and all okay nuggets become vital)
? randomly assigned vital/okay labels
Results are shown in Table 5. Note that this exper-
iment was conducted with the manually-evaluated
system responses, not our POURPRE metric. For the
last condition, we conducted one thousand random
trials, taking into consideration the original distri-
bution of the vital and okay nuggets for each ques-
tion using a simplified version of the Metropolis-
Hastings algorithm (Chib and Greenberg, 1995); the
standard deviations are reported.
These results suggest that system rankings are
sensitive to assessors? opinion about what consti-
tutes a vital or okay nugget. In general, the Kendall?s
? values observed here are lower than values com-
puted from corresponding experiments in ad hoc
document retrieval (Voorhees, 2000). To illustrate,
the distribution of ranks for the top two runs from
937
Run everything vital vital/okay flipped random judgments
TREC 2004 (? = 3) 0.919 0.859 0.841 ? 0.0195
TREC 2003 (? = 3) 0.927 0.802 0.822 ? 0.0215
TREC 2003 (? = 5) 0.920 0.796 0.808 ? 0.0219
Table 5: Correlation (Kendall?s ? ) between scores under different variations of judgments and the official
scores. The 95% confidence interval is presented for the random judgments case.
TREC 2004 (RUN-12 and RUN-8) over the one
thousand random trials is shown in Figure 5. In 511
trials, RUN-12 was ranked as the highest-scoring
run; however, in 463 trials, RUN-8 was ranked as
the highest-scoring run. Factoring in differences of
opinion about the vital/okay distinction, one could
not conclude with certainty which was the ?best? run
in the evaluation.
It appears that differences between POURPRE and
the official scores are about the same as (or in some
cases, smaller than) differences between the official
scores and scores based on variant answer keys (with
the exception of ?everything vital?). This means that
further refinement of the metric to increase correla-
tion with human-generated scores may not be par-
ticularly meaningful; it might essentially amount to
overtraining on the whims of a particular human as-
sessor. We believe that sources of judgment variabil-
ity and techniques for managing it represent impor-
tant areas for future study.
8 Conclusion
We hope that POURPRE can accomplish for defini-
tion question answering what BLEU has done for
machine translation, and ROUGE for document sum-
marization: allow laboratory experiments to be con-
ducted with rapid turnaround. A much shorter ex-
perimental cycle will allow researchers to explore
different techniques and receive immediate feedback
on their effectiveness. Hopefully, this will translate
into rapid progress in the state of the art.4
9 Acknowledgements
This work was supported in part by ARDA?s
Advanced Question Answering for Intelligence
(AQUAINT) Program. We would like to thank
4A toolkit implementing the POURPRE metric can be down-
loaded at http://www.umiacs.umd.edu/?jimmylin/downloads/
Donna Harman and Bonnie Dorr for comments on
earlier drafts of this paper. In addition, we would
like to thank Kiri for her kind support.
References
Bogdan Babych and Anthony Hartley. 2004. Extend-
ing the BLEU MT evaluation method with frequency
weightings. In Proc. of ACL 2004.
Siddhartha Chib and Edward Greenberg. 1995. Under-
standing the Metropolis-Hastings algorithm. Ameri-
can Statistician, 49(4):329?345.
Wesley Hildebrandt, Boris Katz, and Jimmy Lin. 2004.
Answering definition questions with multiple knowl-
edge sources. In Proc. of HLT/NAACL 2004.
Chin-Yew Lin and Eduard Hovy. 2003. Automatic evalu-
ation of summaries using n-gram co-occurrence statis-
tics. In Proc. of HLT/NAACL 2003.
Chin-Yew Lin and Franz Josef Och. 2004. ORANGE: A
method for evaluating automatic evaluation metrics for
machine translation. In Proc. of COLING 2004.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proc. of ACL 2002.
John Prager, Jennifer Chu-Carroll, and Krzysztof Czuba.
2004. Question answering using constraint satisfac-
tion: QA?by?Dossier?with?Constraints. In Proc. of
ACL 2004.
Radu Soricut and Eric Brill. 2004. A unified framework
for automatic evaluation using n-gram co-occurrence
statistics. In Proc. of ACL 2004.
Ellen M. Voorhees. 2000. Variations in relevance judg-
ments and the measurement of retrieval effectiveness.
Information Processing and Management, 36(5):697?
716.
Ellen M. Voorhees. 2003. Overview of the TREC 2003
question answering track. In Proc. of TREC 2003.
Jinxi Xu, Ralph Weischedel, and Ana Licuanan. 2004.
Evaluation of an extraction-based approach to answer-
ing definition questions. In Proc. of SIGIR 2004.
938
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 383?390,
New York, June 2006. c?2006 Association for Computational Linguistics
Will Pyramids Built of Nuggets Topple Over?
Jimmy Lin1,2,3 and Dina Demner-Fushman2,3
1College of Information Studies
2Department of Computer Science
3Institute for Advanced Computer Studies
University of Maryland
College Park, MD 20742, USA
jimmylin@umd.edu, demner@cs.umd.edu
Abstract
The present methodology for evaluating
complex questions at TREC analyzes an-
swers in terms of facts called ?nuggets?.
The official F-score metric represents the
harmonic mean between recall and pre-
cision at the nugget level. There is an
implicit assumption that some facts are
more important than others, which is im-
plemented in a binary split between ?vi-
tal? and ?okay? nuggets. This distinc-
tion holds important implications for the
TREC scoring model?essentially, sys-
tems only receive credit for retrieving vi-
tal nuggets?and is a source of evalua-
tion instability. The upshot is that for
many questions in the TREC testsets, the
median score across all submitted runs is
zero. In this work, we introduce a scor-
ing model based on judgments from mul-
tiple assessors that captures a more refined
notion of nugget importance. We demon-
strate on TREC 2003, 2004, and 2005 data
that our ?nugget pyramids? address many
shortcomings of the present methodology,
while introducing only minimal additional
overhead on the evaluation flow.
1 Introduction
The field of question answering has been moving
away from simple ?factoid? questions such as ?Who
invented the paper clip?? to more complex informa-
tion needs such as ?Who is Aaron Copland?? and
?How have South American drug cartels been using
banks in Liechtenstein to launder money??, which
cannot be answered by simple named-entities. Over
the past few years, NIST through the TREC QA
tracks has implemented an evaluation methodology
based on the notion of ?information nuggets? to as-
sess the quality of answers to such complex ques-
tions. This paradigm has gained widespread accep-
tance in the research community, and is currently be-
ing applied to evaluate answers to so-called ?defini-
tion?, ?relationship?, and ?opinion? questions.
Since quantitative evaluation is arguably the sin-
gle biggest driver of advances in language technolo-
gies, it is important to closely examine the charac-
teristics of a scoring model to ensure its fairness, re-
liability, and stability. In this work, we identify a
potential source of instability in the nugget evalua-
tion paradigm, develop a new scoring method, and
demonstrate that our new model addresses some of
the shortcomings of the original method. It is our
hope that this more-refined evaluation model can
better guide the development of technology for an-
swering complex questions.
This paper is organized as follows: Section 2
provides a brief overview of the nugget evaluation
methodology. Section 3 draws attention to the vi-
tal/okay nugget distinction and the problems it cre-
ates. Section 4 outlines our proposal for building
?nugget pyramids?, a more-refined model of nugget
importance that combines judgments from multiple
assessors. Section 5 describes the methodology for
evaluating this new model, and Section 6 presents
our results. A discussion of related issues appears in
Section 7, and the paper concludes with Section 8.
383
2 Evaluation of Complex Questions
To date, NIST has conducted three large-scale eval-
uations of complex questions using a nugget-based
evaluation methodology: ?definition? questions in
TREC 2003, ?other? questions in TREC 2004 and
TREC 2005, and ?relationship? questions in TREC
2005. Since relatively few teams participated in
the 2005 evaluation of ?relationship? questions, this
work focuses on the three years? worth of ?defini-
tion/other? questions. The nugget-based paradigm
has been previously detailed in a number of pa-
pers (Voorhees, 2003; Hildebrandt et al, 2004; Lin
and Demner-Fushman, 2005a); here, we present
only a short summary.
System responses to complex questions consist of
an unordered set of passages. To evaluate answers,
NIST pools answer strings from all participants, re-
moves their association with the runs that produced
them, and presents them to a human assessor. Us-
ing these responses and research performed during
the original development of the question, the asses-
sor creates an ?answer key? comprised of a list of
?nuggets??essentially, facts about the target. Ac-
cording to TREC guidelines, a nugget is defined as
a fact for which the assessor could make a binary
decision as to whether a response contained that
nugget (Voorhees, 2003). As an example, relevant
nuggets for the target ?AARP? are shown in Table 1.
In addition to creating the nuggets, the assessor also
manually classifies each as either ?vital? or ?okay?.
Vital nuggets represent concepts that must be in a
?good? definition; on the other hand, okay nuggets
contribute worthwhile information about the target
but are not essential. The distinction has important
implications, described below.
Once the answer key of vital/okay nuggets is cre-
ated, the assessor goes back and manually scores
each run. For each system response, he or she de-
cides whether or not each nugget is present. The
final F-score for an answer is computed in the man-
ner described in Figure 1, and the final score of a
system run is the mean of scores across all ques-
tions. The per-question F-score is a harmonic mean
between nugget precision and nugget recall, where
recall is heavily favored (controlled by the ? param-
eter, set to five in 2003 and three in 2004 and 2005).
Nugget recall is computed solely on vital nuggets
vital 30+ million members
okay Spends heavily on research & education
vital Largest seniors organization
vital Largest dues paying organization
vital Membership eligibility is 50+
okay Abbreviated name to attract boomers
okay Most of its work done by volunteers
okay Receives millions for product endorsements
okay Receives millions from product endorsements
Table 1: Answer nuggets for the target ?AARP?.
Let
r # of vital nuggets returned in a response
a # of okay nuggets returned in a response
R # of vital nuggets in the answer key
l # of non-whitespace characters in the entire
answer string
Then
recall (R) = r/R
allowance (?) = 100? (r + a)
precision (P) =
{
1 if l < ?
1? l??l otherwise
Finally, the F? = (?
2 + 1)? P ?R
?2 ? P +R
? = 5 in TREC 2003, ? = 3 in TREC 2004, 2005.
Figure 1: Official definition of F-score.
(which means no credit is given for returning okay
nuggets), while nugget precision is approximated by
a length allowance based on the number of both vi-
tal and okay nuggets returned. Early in a pilot study,
researchers discovered that it was impossible for as-
sessors to enumerate the total set of nuggets con-
tained in a system response (Voorhees, 2003), which
corresponds to the denominator in the precision cal-
culation. Thus, a penalty for verbosity serves as a
surrogate for precision.
Note that while a question?s answer key only
needs to be created once, assessors must manually
determine if each nugget is present in a system?s re-
sponse. This human involvement has been identified
as a bottleneck in the evaluation process, although
we have recently developed an automatic scoring
metric called POURPRE that correlates well with hu-
man judgments (Lin and Demner-Fushman, 2005a).
384
Testset # q?s 1 vital 2 vital
TREC 2003 50 3 10
TREC 2004 64 2 15
TREC 2005 75 5 16
Table 2: Number of questions with few vital nuggets
in the different testsets.
3 What?s Vital? What?s Okay?
Previously, we have argued that the vital/okay dis-
tinction is a source of instability in the nugget-
based evaluation methodology, especially given the
manner in which F-score is calculated (Hildebrandt
et al, 2004; Lin and Demner-Fushman, 2005a).
Since only vital nuggets figure into the calculation
of nugget recall, there is a large ?quantization ef-
fect? for system scores on topics that have few vital
nuggets. For example, on a question that has only
one vital nugget, a system cannot obtain a non-zero
score unless that vital nugget is retrieved. In reality,
whether or not a system returned a passage contain-
ing that single vital nugget is often a matter of luck,
which is compounded by assessor judgment errors.
Furthermore, there does not appear to be any reliable
indicators for predicting the importance of a nugget,
which makes the task of developing systems even
more challenging.
The polarizing effect of the vital/okay distinction
brings into question the stability of TREC evalua-
tions. Table 2 shows statistics about the number of
questions that have only one or two vital nuggets.
Compared to the size of the testset, these numbers
are relatively large. As a concrete example, ?F16? is
the target for question 71.7 from TREC 2005. The
only vital nugget is ?First F16s built in 1974?. The
practical effect of the vital/okay distinction in its
current form is the number of questions for which
the median system score across all submitted runs is
zero: 22 in TREC 2003, 41 in TREC 2004, and 44
in TREC 2005.
An evaluation in which the median score for many
questions is zero has many shortcomings. For one,
it is difficult to tell if a particular run is ?better? than
another?even though they may be very different in
other salient properties such as length, for exam-
ple. The discriminative power of the present F-score
measure is called into question: are present systems
that bad, or is the current scoring model insufficient
to discriminate between different (poorly perform-
ing) systems?
Also, as pointed out by Voorhees (2005), a score
distribution heavily skewed towards zero makes
meta-analysis of evaluation stability hard to per-
form. Since such studies depend on variability in
scores, evaluations would appear more stable than
they really are.
While there are obviously shortcomings to the
current scheme of labeling nuggets as either ?vital?
or ?okay?, the distinction does start to capture the
intuition that ?not all nuggets are created equal?.
Some nuggets are inherently more important than
others, and this should be reflected in the evaluation
methodology. The solution, we believe, is to solicit
judgments from multiple assessors and develop a
more refined sense of nugget importance. However,
given finite resources, it is important to balance the
amount of additional manual effort required with the
gains derived from those efforts. We present the idea
of building ?nugget pyramids?, which addresses the
shortcomings noted here, and then assess the impli-
cations of this new scoring model against data from
TREC 2003, 2004, and 2005.
4 Building Nugget Pyramids
As previously pointed out (Lin and Demner-
Fushman, 2005b), the question answering and sum-
marization communities are converging on the task
of addressing complex information needs from com-
plementary perspectives; see, for example, the re-
cent DUC task of query-focused multi-document
summarization (Amigo? et al, 2004; Dang, 2005).
From an evaluation point of view, this provides op-
portunities for cross-fertilization and exchange of
fresh ideas. As an example of this intellectual dis-
course, the recently-developed POURPRE metric for
automatically evaluating answers to complex ques-
tions (Lin and Demner-Fushman, 2005a) employs
n-gram overlap to compare system responses to ref-
erence output, an idea originally implemented in the
ROUGE metric for summarization evaluation (Lin
and Hovy, 2003). Drawing additional inspiration
from research on summarization evaluation, we
adapt the pyramid evaluation scheme (Nenkova and
Passonneau, 2004) to address the shortcomings of
385
the vital/okay distinction in the nugget-based evalu-
ation methodology.
The basic intuition behind the pyramid
scheme (Nenkova and Passonneau, 2004) is
simple: the importance of a fact is directly related
to the number of people that recognize it as such
(i.e., its popularity). The evaluation methodology
calls for assessors to annotate Semantic Content
Units (SCUs) found within model reference sum-
maries. The weight assigned to an SCU is equal
to the number of annotators that have marked the
particular unit. These SCUs can be arranged in a
pyramid, with the highest-scoring elements at the
top: a ?good? summary should contain SCUs from a
higher tier in the pyramid before a lower tier, since
such elements are deemed ?more vital?.
This pyramid scheme can be easily adapted for
question answering evaluation since a nugget is
roughly comparable to a Semantic Content Unit.
We propose to build nugget pyramids for answers
to complex questions by soliciting vital/okay judg-
ments from multiple assessors, i.e., take the original
reference nuggets and ask different humans to clas-
sify each as either ?vital? or ?okay?. The weight as-
signed to each nugget is simply equal to the number
of different assessors that deemed it vital. We then
normalize the nugget weights (per-question) so that
the maximum possible weight is one (by dividing
each nugget weight by the maximum weight of that
particular question). Therefore, a nugget assigned
?vital? by the most assessors (not necessarily all)
would receive a weight of one.1
The introduction of a more granular notion of
nugget importance should be reflected in the calcu-
lation of F-score. We propose that nugget recall be
modified to take into account nugget weight:
R =
?
m?Awm
?
n?V wn
Where A is the set of reference nuggets that are
matched within a system?s response and V is the set
of all reference nuggets; wm and wn are the weights
of nuggetsm and n, respectively. Instead of a binary
distinction based solely on matching vital nuggets,
all nuggets now factor into the calculation of recall,
1Since there may be multiple nuggets with the highest score,
what we?re building is actually a frustum sometimes. :)
subjected to a weight. Note that this new scoring
model captures the existing binary vital/okay dis-
tinction in a straightforward way: vital nuggets get
a score of one, and okay nuggets zero.
We propose to leave the calculation of nugget pre-
cision as is: a system would receive a length al-
lowance of 100 non-whitespace characters for ev-
ery nugget it retrieved (regardless of importance).
Longer answers would be penalized for verbosity.
Having outlined our revisions to the standard
nugget-based scoring method, we will proceed to
describe our methodology for evaluating this new
model and demonstrate how it overcomes many of
the shortcomings of the existing paradigm.
5 Evaluation Methodology
We evaluate our methodology for building ?nugget
pyramids? using runs submitted to the TREC 2003,
2004, and 2005 question answering tracks (2003
?definition? questions, 2004 and 2005 ?other? ques-
tions). There were 50 questions in the 2003 testset,
64 in 2004, and 75 in 2005. In total, there were 54
runs submitted to TREC 2003, 63 to TREC 2004,
and 72 to TREC 2005. NIST assessors have man-
ually annotated nuggets found in a given system?s
response, and this allows us to calculate the final F-
score under different scoring models.
We recruited a total of nine different assessors for
this study. Assessors consisted of graduate students
in library and information science and computer sci-
ence at the University of Maryland as well as volun-
teers from the question answering community (ob-
tained via a posting to NIST?s TREC QA mailing
list). Each assessor was given the reference nuggets
along with the original questions and asked to clas-
sify each nugget as vital or okay. They were pur-
posely asked to make these judgments without refer-
ence to documents in the corpus in order to expedite
the assessment process?our goal is to propose a re-
finement to the current nugget evaluation methodol-
ogy that addresses shortcomings while minimizing
the amount of additional effort required. Combined
with the answer key created by the original NIST
assessors, we obtained a total of ten judgments for
every single nugget in the three testsets.2
2Raw data can be downloaded at the following URL:
http://www.umiacs.umd.edu/?jimmylin
386
2003 2004 2005
Assessor Kendall?s ? zeros Kendall?s ? zeros Kendall?s ? zeros
0 1.00 22 1.00 41 1.00 44
1 0.908 20 0.933 36 0.888 43
2 0.896 21 0.916 43 0.900 41
3 0.903 21 0.917 38 0.897 39
4 0.912 20 0.914 42 0.879 56
5 0.873 23 0.926 40 0.841 53
6 0.889 29 0.908 32 0.894 39
7 0.900 22 0.930 37 0.890 54
8 0.909 18 0.932 29 0.891 35
9 0.879 26 0.908 49 0.877 58
average 0.896 22.2 0.920 38.7 0.884 46.2
Table 3: Kendall?s ? correlation between system scores generated using ?official? vital/okay judgments and
each assessor?s judgments. (Assessor 0 represents the original NIST assessors.)
We measured the correlation between system
ranks generated by different scoring models using
Kendall?s ? , a commonly-used rank correlation mea-
sure in information retrieval for quantifying the sim-
ilarity between different scoring methods. Kendall?s
? computes the ?distance? between two rankings as
the minimum number of pairwise adjacent swaps
necessary to convert one ranking into the other. This
value is normalized by the number of items being
ranked such that two identical rankings produce a
correlation of 1.0; the correlation between a rank-
ing and its perfect inverse is ?1.0; and the expected
correlation of two rankings chosen at random is
0.0. Typically, a value of greater than 0.8 is con-
sidered ?good?, although 0.9 represents a threshold
researchers generally aim for.
We hypothesized that system ranks are relatively
unstable with respect to individual assessor?s judg-
ments. That is, how well a given system scores
is to a large extent dependent on which assessor?s
judgments one uses for evaluation. This stems from
an inescapable fact of such evaluations, well known
from studies of relevance in the information retrieval
literature (Voorhees, 1998). Humans have legitimate
differences in opinion regarding a nugget?s impor-
tance, and there is no such thing as ?the correct an-
swer?. However, we hypothesized that these varia-
tions can be smoothed out by building ?nugget pyra-
mids? in the manner we described. Nugget weights
reflect the combined judgments of many individual
assessors, and scores generated with weights taken
into account should correlate better with each indi-
vidual assessor?s opinion.
6 Results
To verify our hypothesis about the instability of us-
ing any individual assessor?s judgments, we calcu-
lated the Kendall?s ? correlation between system
scores generated using the ?official? vital/okay judg-
ments (provide by NIST assessors) and each individ-
ual assessor?s judgments. This is shown in Table 3.
The original NIST judgments are listed as ?assessor
0? (and not included in the averages). For all scoring
models discussed in this paper, we set ?, the param-
eter that controls the relative importance of preci-
sion and recall, to three.3 Results show that although
official rankings generally correlate well with rank-
ings generated by our nine additional assessors, the
agreement is far from perfect. Yet, in reality, the
opinions of our nine assessors are not any less valid
than those of the NIST assessors?NIST does not
occupy a privileged position on what constitutes a
good ?definition?. We can see that variations in hu-
man judgments do not appear to be adequately cap-
tured by the current scoring model.
Table 3 also shows the number of questions for
which systems? median score was zero based on
each individual assessor?s judgments (out of 50
3Note that ? = 5 in the official TREC 2003 evaluation.
387
2003 2004 2005
0 0.934 0.943 0.901
1 0.962 0.940 0.950
2 0.938 0.948 0.952
3 0.938 0.947 0.950
4 0.936 0.922 0.914
5 0.916 0.956 0.887
6 0.916 0.950 0.958
7 0.949 0.933 0.927
8 0.964 0.972 0.953
9 0.912 0.899 0.881
average 0.936 0.941 0.927
Table 4: Kendall?s ? correlation between system
rankings generated using the ten-assessor nugget
pyramid and those generated using each individual
assessor?s judgments. (Assessor 0 represents the
original NIST assessors.)
questions for TREC 2003, 64 for TREC 2004, and
75 for TREC 2005). These numbers are worrisome:
in TREC 2004, for example, over half the questions
(on average) have a median score of zero, and over
three quarters of questions, according to assessor 9.
This is problematic for the various reasons discussed
in Section 3.
To evaluate scoring models that combine the opin-
ions of multiple assessors, we built ?nugget pyra-
mids? using all ten sets of judgments in the manner
outlined in Section 4. All runs submitted to each
of the TREC evaluations were then rescored using
the modified F-score formula, which takes into ac-
count a finer-grained notion of nugget importance.
Rankings generated by this model were then com-
pared against those generated by each individual as-
sessor?s judgments. Results are shown in Table 4.
As can be seen, the correlations observed are higher
than those in Table 3, meaning that a nugget pyramid
better captures the opinions of each individual asses-
sor. A two-tailed t-test reveals that the differences in
averages are statistically significant (p << 0.01 for
TREC 2003/2005, p < 0.05 for TREC 2004).
What is the effect of combining judgments from
different numbers of assessors? To answer this
question, we built ten different nugget pyramids
of varying ?sizes?, i.e., combining judgments from
one through ten assessors. The Kendall?s ? corre-
 0.86
 0.88
 0.9
 0.92
 0.94
 0.96
 0.98
 1
 1  2  3  4  5  6  7  8  9  10
Ke
nd
al
l's
 ta
u
Number of assessors
TREC 2003
TREC 2004
TREC 2005
Figure 2: Average agreement (Kendall?s ? ) between
individual assessors and nugget pyramids built from
different numbers of assessors.
 0.3
 0.35
 0.4
 0.45
 0.5
 0.55
 0.6
 0.65
 0.7
 1  2  3  4  5  6  7  8  9  10
Fr
ac
tio
n 
of
 q
ue
st
io
ns
 w
ho
se
 m
ed
ia
n 
sc
or
e 
is 
ze
ro
Number of assessors
TREC 2003
TREC 2004
TREC 2005
Figure 3: Fraction of questions whose median score
is zero plotted against number of assessors whose
judgments contributed to the nugget pyramid.
lations between scores generated by each of these
and scores generated by each individual assessor?s
judgments were computed. For each pyramid, we
computed the average across all rank correlations,
which captures the extent to which that particular
pyramid represents the opinions of all ten assessors.
These results are shown in Figure 2. The increase
in Kendall?s ? that comes from adding a second as-
sessor is statistically significant, as revealed by a
two-tailed t-test (p << 0.01 for TREC 2003/2005,
p < 0.05 for TREC 2004), but ANOVA reveals no
statistically significant differences beyond two as-
sessors.
From these results, we can conclude that adding
a second assessor yields a scoring model that is sig-
nificantly better at capturing the variance in human
relevance judgments. In this respect, little is gained
beyond two assessors. If this is the only advantage
388
provided by nugget pyramids, then the boost in rank
correlations may not be sufficient to justify the ex-
tra manual effort involved in building them. As we
shall see, however, nugget pyramids offer other ben-
efits as well.
Evaluation by our nugget pyramids greatly re-
duces the number of questions whose median score
is zero. As previously discussed, a strict vital/okay
split translates into a score of zero for systems that
do not return any vital nuggets. However, nugget
pyramids reflect a more refined sense of nugget im-
portance, which results in fewer zero scores. Fig-
ure 3 shows the number of questions whose median
score is zero (normalized as a fraction of the en-
tire testset) by nugget pyramids built from varying
numbers of assessors. With four or more assessors,
the number of questions whose median is zero for
the TREC 2003 testset drops to 17; for TREC 2004,
23 for seven or more assessors; for TREC 2005, 27
for nine or more assessors. In other words, F-scores
generated using our methodology are far more dis-
criminative. The remaining questions with zero me-
dians, we believe, accurately reflect the state of the
art in question answering performance.
An example of a nugget pyramid that combines
the opinions of all ten assessors is shown in Table 5
for the target ?AARP?. Judgments from the original
NIST assessors are also shown (cf. Table 1). Note
that there is a strong correlation between the original
vital/okay judgments and the refined nugget weights
based on the pyramid, indicating that (in this case,
at least) the intuition of the NIST assessor matches
that of the other assessors.
7 Discussion
In balancing the tradeoff between advantages pro-
vided by nugget pyramids and the additional man-
ual effort necessary to create them, what is the opti-
mal number of assessors to solicit judgments from?
Results shown in Figures 2 and 3 provide some an-
swers. In terms of better capturing different asses-
sors? opinions, little appears to be gained from going
beyond two assessors. However, adding more judg-
ments does decrease the number of questions whose
median score is zero, resulting in a more discrim-
inative metric. Beyond five assessors, the number
of questions with a zero median score remains rela-
1.0 vital Largest seniors organization
0.9 vital Membership eligibility is 50+
0.8 vital 30+ million members
0.7 vital Largest dues paying organization
0.2 okay Most of its work done by volunteers
0.1 okay Spends heavily on research & education
0.1 okay Receives millions for product endorsements
0.1 okay Receives millions from product endorsements
0.0 okay Abbreviated name to attract boomers
Table 5: Answer nuggets for the target ?AARP? with
weights derived from the nugget pyramid building
process.
tively stable. We believe that around five assessors
yield the smallest nugget pyramid that confers the
advantages of the methodology.
The idea of building ?nugget pyramids? is an ex-
tension of a similarly-named evaluation scheme in
document summarization, although there are impor-
tant differences. Nenkova and Passonneau (2004)
call for multiple assessors to annotate SCUs, which
is much more involved than the methodology pre-
sented here, where the nuggets are fixed and asses-
sors only provide additional judgments about their
importance. This obviously has the advantage of
streamlining the assessment process, but has the po-
tential to miss other important nuggets that were not
identified in the first place. Our experimental results,
however, suggest that this is a worthwhile tradeoff.
The explicit goal of this work was to develop scor-
ing models for nugget-based evaluation that would
address shortcomings of the present approach, while
introducing minimal overhead in terms of additional
resource requirements. To this end, we have been
successful.
Nevertheless, there are a number of issues that
are worth mentioning. To speed up the assessment
process, assessors were instructed to provide ?snap
judgments? given only the list of nuggets and the tar-
get. No additional context was provided, e.g., docu-
ments from the corpus or sample system responses.
It is also important to note that the reference nuggets
were never meant to be read by other people?NIST
makes no claim for them to be well-formed de-
scriptions of the facts themselves. These answer
389
keys were primarily note-taking devices to assist in
the assessment process. The important question,
however, is whether scoring variations caused by
poorly-phrased nuggets are smaller than the varia-
tions caused by legitimate inter-assessor disagree-
ment regarding nugget importance. Our experiments
appear to suggest that, overall, the nugget pyramid
scheme is sound and can adequately cope with these
difficulties.
8 Conclusion
The central importance that quantitative evaluation
plays in advancing the state of the art in language
technologies warrants close examination of evalua-
tion methodologies themselves to ensure that they
are measuring ?the right thing?. In this work, we
have identified a shortcoming in the present nugget-
based paradigm for assessing answers to complex
questions. The vital/okay distinction was designed
to capture the intuition that some nuggets are more
important than others, but as we have shown, this
comes at a cost in stability and discriminative power
of the metric. We proposed a revised model that in-
corporates judgments from multiple assessors in the
form of a ?nugget pyramid?, and demonstrated how
this addresses many of the previous shortcomings. It
is hoped that our work paves the way for more ac-
curate and refined evaluations of question answering
systems in the future.
9 Acknowledgments
This work has been supported in part by DARPA
contract HR0011-06-2-0001 (GALE), and has
greatly benefited from discussions with Ellen
Voorhees, Hoa Dang, and participants at TREC
2005. We are grateful for the nine assessors who
provided nugget judgments. The first author would
like to thank Esther and Kiri for their loving support.
References
Enrique Amigo?, Julio Gonzalo, Victor Peinado, Anselmo
Pen?as, and Felisa Verdejo. 2004. An empirical study
of information synthesis task. In Proceedings of the
42nd Annual Meeting of the Association for Computa-
tional Linguistics (ACL 2004).
Hoa Dang. 2005. Overview of DUC 2005. In Proceed-
ings of the 2005 Document Understanding Conference
(DUC 2005) at NLT/EMNLP 2005.
Wesley Hildebrandt, Boris Katz, and Jimmy Lin. 2004.
Answering definition questions with multiple knowl-
edge sources. In Proceedings of the 2004 Human Lan-
guage Technology Conference and the North American
Chapter of the Association for Computational Linguis-
tics Annual Meeting (HLT/NAACL 2004).
Jimmy Lin and Dina Demner-Fushman. 2005a. Auto-
matically evaluating answers to definition questions.
In Proceedings of the 2005 Human Language Technol-
ogy Conference and Conference on Empirical Methods
in Natural Language Processing (HLT/EMNLP 2005).
Jimmy Lin and Dina Demner-Fushman. 2005b. Evalu-
ating summaries and answers: Two sides of the same
coin? In Proceedings of the ACL 2005 Workshop on
Intrinsic and Extrinsic Evaluation Measures for MT
and/or Summarization.
Chin-Yew Lin and Eduard Hovy. 2003. Automatic
evaluation of summaries using n-gram co-occurrence
statistics. In Proceedings of the 2003 Human Lan-
guage Technology Conference and the North American
Chapter of the Association for Computational Linguis-
tics Annual Meeting (HLT/NAACL 2003).
Ani Nenkova and Rebecca Passonneau. 2004. Evalu-
ating content selection in summarization: The pyra-
mid method. In Proceedings of the 2004 Human Lan-
guage Technology Conference and the North American
Chapter of the Association for Computational Linguis-
tics Annual Meeting (HLT/NAACL 2004).
Ellen M. Voorhees. 1998. Variations in relevance judg-
ments and the measurement of retrieval effectiveness.
In Proceedings of the 21st Annual International ACM
SIGIR Conference on Research and Development in
Information Retrieval (SIGIR 1998).
Ellen M. Voorhees. 2003. Overview of the TREC
2003 question answering track. In Proceedings of the
Twelfth Text REtrieval Conference (TREC 2003).
Ellen M. Voorhees. 2005. Using question series to eval-
uate question answering system effectiveness. In Pro-
ceedings of the 2005 Human Language Technology
Conference and Conference on Empirical Methods in
Natural Language Processing (HLT/EMNLP 2005).
390
Proceedings of NAACL HLT 2007, pages 212?219,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Is Question Answering Better Than Information Retrieval?
Towards a Task-Based Evaluation Framework for Question Series
Jimmy Lin
College of Information Studies
Department of Computer Science
Institute for Advanced Computer Studies
University of Maryland
College Park, MD 20742, USA
jimmylin@umd.edu
Abstract
This paper introduces a novel evaluation
framework for question series and em-
ploys it to explore the effectiveness of QA
and IR systems at addressing users? infor-
mation needs. The framework is based on
the notion of recall curves, which char-
acterize the amount of relevant informa-
tion contained within a fixed-length text
segment. Although it is widely assumed
that QA technology provides more effi-
cient access to information than IR sys-
tems, our experiments show that a simple
IR baseline is quite competitive. These re-
sults help us better understand the role of
NLP technology in QA systems and sug-
gest directions for future research.
1 Introduction
The emergence of question answering (QA) has
been driven to a large extent by its intuitive appeal.
Instead of ?hits?, QA technology promises to de-
liver ?answers?, obviating the user from the tedious
task of sorting through lists of potentially-relevant
documents. The success of factoid QA systems,
particularly in the NIST-sponsored TREC evalua-
tions (Voorhees, 2003), has reinforced the percep-
tion about the superiority of QA systems over tradi-
tional IR engines.
However, is QA really better than IR? This work
challenges existing assumptions and critically exam-
ines this question, starting with the development of a
novel evaluation framework that better models user
tasks and preferences. The framework is then ap-
plied to compare top TREC QA systems against an
off-the-shelf IR engine. Surprisingly, experiments
show that the IR baseline is quite competitive. These
results help us better understand the added value of
NLP technology in QA systems, and are also useful
in guiding future research.
2 Evolution of QA Evaluation
Although most question answering systems rely on
information retrieval technology, there has always
been the understanding that NLP provides signifi-
cant added value beyond simple IR. Even the earli-
est open-domain factoid QA systems, which can be
traced back to the late nineties (Voorhees and Tice,
1999), demonstrated the importance and impact of
linguistic processing. Today?s top systems deploy
a wide range of advanced NLP technology and can
answer over three quarters of factoid questions in an
open domain (Voorhees, 2003). However, present
QA evaluation methodology does not take into ac-
count two developments, discussed below.
First, despite trends to the contrary in TREC eval-
uations, users don?t actually like or want exact an-
swers. Most question answering systems are de-
signed to pinpoint the exact named entity (person,
date, organization, etc.) that answers a particular
question?and the development of such technology
has been encouraged by the setup of the TREC QA
tracks. However, a study by Lin et al (2003) shows
that users actually prefer answers embedded within
some sort of context, e.g., the sentence or the para-
graph that the answer was found in. Context pro-
212
3. Hale Bopp comet
1. fact When was the comet discovered?
2. fact How often does it approach the earth?
3. list In what countries was the comet visi-
ble on its last return?
4. other
68. Port Arthur Massacre
1. fact Where is Port Arthur?
2. fact When did the massacre occur?
3. fact What was the final death toll of the
massacre?
4. fact Who was the killer?
5. fact What was the killer?s nationality?
6. list What were the names of the victims?
7. list What were the nationalities of the vic-
tims?
8. other
Table 1: Sample question series.
vides a means by which the user can establish the
credibility of system responses and also provides a
vehicle for ?serendipitous knowledge discovery??
finding answers to related questions. As the early
TRECs have found (Voorhees and Tice, 1999), lo-
cating a passage that contains an answer is consider-
ably easier than pinpointing the exact answer. Thus,
real-world user preferences may erode the advantage
that QA has over IR techniques such as passage re-
trieval, e.g., (Zobel et al, 1995; Tellex et al, 2003).
Second, the focus of question answering research
has shifted away from isolated factoid questions to
more complex information needs embedded within
a broader context (e.g., a user scenario). Since
2004, the main task at the TREC QA tracks has
consisted of question series organized around topics
(called ?targets?)?which can be people, organiza-
tions, entities, or events (Voorhees, 2004; Voorhees,
2005). Questions in a series inquire about differ-
ent facets of a target, but are themselves either fac-
toid or list questions. In addition, each series con-
tains an explicit ?other? question (always the last
one), which can be paraphrased as ?Tell me other
interesting things about this target that I don?t know
enough to ask directly.? See Table 1 for examples
of question series. Separately, NIST has been ex-
ploring other types of complex information needs,
for example, the relationship task in TREC 2005
and the ciQA (complex, interactive Question An-
swering) task in TREC 2006 (Dang et al, 2006).
One shared feature of these complex questions is
that they cannot be answered by simple named en-
tities. Answers usually span passages, which makes
the task very similar to the query-focused summa-
rization task in DUC (Dang, 2005). On these tasks,
it is unclear whether QA systems actually outper-
form baseline IR methods. As one bit of evidence,
in TREC 2003, a simple IR-based sentence ranker
outperformed all but the best system on definition
questions, the precursor to current ?other? ques-
tions (Voorhees, 2003).
We believe that QA evaluation methodology has
lagged behind these developments and does not ade-
quately characterize the performance of current sys-
tems. In the next section, we present an evaluation
framework that takes into account users? desire for
context and the structure of more complex QA tasks.
Focusing on question series, we compare the perfor-
mance of top TREC systems to a baseline IR engine
using this evaluation framework.
3 An Evaluation Framework
Question series in TREC represent an attempt at
modeling information-seeking dialogues between a
user and a system (Kato et al, 2004). Primarily
because dialogue systems are difficult to evaluate,
NIST has adopted a setup in which individual ques-
tions are evaluated in isolation?this implicitly mod-
els a user who types in a question, receives an an-
swer, and then moves on to the next question in the
series. Component scores are aggregated using a
weighted average, and no attempt is made to capture
dependencies across different question types.
Simultaneously acknowledging the challenges in
evaluating dialogue systems and recognizing the
similarities between complex QA and query-focused
summarization, we propose an alternative frame-
work for QA evaluation that considers the quality
of system responses as a whole. Instead of gener-
ating individual answers to each question, a system
might alternatively produce a segment of text (i.e., a
summary) that attempts to answer all the questions.
This slightly different conception of QA brings it
into better alignment with recent trends in multi-
213
document summarization, which may yield previ-
ously untapped synergies (see Section 7).
To assess the quality of system responses,
we adopt the nugget-based methodology used
previously for many types of complex ques-
tions (Voorhees, 2003), which shares similarities
with the pyramid evaluation scheme used in sum-
marization (Nenkova and Passonneau, 2004). A
nugget can be described as an ?atomic fact? that ad-
dresses an aspect of an information need. Instead of
the standard nugget F-score, which hides important
tradeoffs between precision and recall, we propose
to measure nugget recall as a function of response
length. The goal is to quantify the number of rel-
evant facts that a user will have encountered after
reading a particular amount of text. Intuitively, we
wish to model how quickly a hypothetical user could
?learn? about a topic by reading system responses.
Within this framework, we compared existing
TREC QA systems against an IR baseline. Pro-
cessed outputs from the top-ranked, second-ranked,
third-ranked, and median runs in TREC 2004 and
TREC 2005 were compared to a baseline IR run
generated by Lucene, an off-the-shelf open-source
IR engine. Our experiments focused on factoid and
?other? questions; as the details differ for these two
types, we describe each separately and then return to
a unified picture.
4 Factoid Series
Our first set of experiments focuses on the factoid
questions within a series. In what follows, we de-
scribe the data preparation process, the evaluation
methodology, and experimental results.
4.1 Data Preparation
We began by preparing answer responses from the
top-ranked, second-ranked, third-ranked, and me-
dian runs from TREC 2004 and TREC 2005.1 Con-
sider the third-ranked run from TREC 2004 as a run-
ning example; for the two factoid questions in tar-
get 3 (Table 1), the system answers were ?July 22,
1995? and ?4,200 years? (both correct).
Since Lin et al (2003) suggest that users prefer
answers situated within some sort of context, we
1In cases where teams submitted multiple runs, we consid-
ered only the best performing of each.
projected these exact answers onto their source sen-
tences. This was accomplished by selecting the first
sentence in the source document (drawn from the
AQUAINT corpus) that contains the answer string.2
In our example, this procedure yielded the following
text segment:
The comet was named after its two observers?two
amateur astronomers in the United States who dis-
covered it on July 22, 1995. Its visit to the solar
system?just once every 4,200 years, will give mil-
lions of people a rare heavenly treat when it reaches
its full brightness next year.
Since projected sentences are simply concate-
nated, the responses often exhibit readability prob-
lems (although by chance this particular response is
relatively coherent). Nevertheless, one might imag-
ine that such output forms the basis for generating
coherent query-focused summaries with sentence-
rewrite techniques, e.g., (Barzilay et al, 1999). In
this work, we set aside problems with fluency since
our evaluation framework is unable to measure this
(desirable) characteristic.
System responses were prepared for four runs
from TREC 2004 and four runs from TREC 2005
in the manner described above. As a baseline, we
employed Lucene to retrieve the top 100 documents
from the AQUAINT corpus using the target as the
query (in our example, ?Hale Bopp comet?). From
the result set, we retained all sentences that contain
at least a term from the target. Sentence order within
each document and across the ranked list was pre-
served. Answer responses for this baseline condi-
tion were limited to 10,000 characters. Following
TREC convention, all character counts include only
non-whitespace characters. Finally, since responses
prepared from TREC runs were significantly shorter
than this baseline condition, the baseline Lucene re-
sponse was appended to the end of each TREC run
to fill a quota of 10,000 characters.
4.2 Evaluation Methodology
Our evaluation framework is designed to measure
the amount of useful information contained in a sys-
tem response. For factoid series, this can be quan-
2As a backoff, if the exact answer string is not found in the
text, the sentence with the most terms in common with the an-
swer string is selected.
214
 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7
 0.8 0.9
 10  100  1000  10000recall length of response (non-whitespace characters)
Evaluation of TREC 2004 factoid questionstop-ranked run2nd-ranked run3rd-ranked runmedian runLucene
 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7
 0.8 0.9
 10  100  1000  10000recall length of response (non-whitespace characters)
Evaluation of TREC 2005 factoid questionstop-ranked run2nd-ranked run3rd-ranked runmedian runLucene
Figure 1: Factoid recall curves for runs from TREC 2004 (left) and TREC 2005 (right).
Run 2004 2005
top-ranked run 0.770 0.713
2nd-ranked run 0.643 0.666
3rd-ranked run 0.626 0.326
median run 0.170 0.177
Table 2: Official scores of selected TREC 2004 and
TREC 2005 factoid runs.
tified by recall?the fraction of questions within
a series whose answers could be found within a
given passage. By varying the passage length, we
can characterize systems in terms of recall curves
that represent how quickly a hypothetical user can
?learn? about the target. Below, we describe the im-
plementation of such a metric.
First, we need a method to automatically deter-
mine if an answer string is contained within a seg-
ment of text. For this, regular expression answer
patterns distributed by NIST were employed?they
have become a widely-accepted evaluation tool.
Second, we must determine when a fact is ?ac-
quired? by our hypothetical user. Since previous
studies suggest that context is needed to interpret an
answer, we assess system output on a sentence-by-
sentence basis. In our example, the lengths of the
two sentences are 105 and 130 characters, respec-
tively. Thus, for this series, we obtain a recall of 0.5
at 105 characters and 1.0 at 235 characters.
Finally, we must devise a method for aggregating
across different question series to factor out vari-
ations. We accomplish this through interpolation,
much in the same way that precision?recall curves
are plotted in IR experiments. First, all lengths are
interpolated to their nearest larger fifty character in-
crement. In our case, they are 150 and 250. Once
this is accomplished for each question series, we can
directly average across all question series at each
length increment. Plotting these points gives us a
recall-by-length performance curve.
4.3 Results
Results of our evaluation are shown in Figure 1, for
TREC 2004 (left) and TREC 2005 (right). These
plots have a simple interpretation?curves that rise
faster and higher represent ?better? systems. The
?knee? in some of the curves indicate approximately
the length of the original system output (recall
that the baseline Lucene run was appended to each
TREC run to produce responses of equal lengths).
For reference, official factoid scores of the same runs
are shown in Table 2.
Results from TREC 2004 are striking: while the
top three systems appear to outperform the baseline
IR run, it is unclear if the median system is better
than Lucene, especially at longer response lengths.
This suggests that if a user wanted to obtain answers
to a series of factoid questions about a topic, using
the median QA system isn?t any more efficient than
simply retrieving a few articles using an IR engine
and reading them. Turning to the 2005 results, the
median system fares better when compared to the
IR baseline, although the separation between the top
and median systems has narrowed.
In the next two sections, we present additional ex-
periments on question series. A detailed analysis is
saved for Section 7.
215
 0 0.001 0.002 0.003 0.004
 0.005 0.006
 10  100  1000  10000POURPRE recall length of response (non-whitespace characters)
Evaluation of TREC 2004 "other" questionstop-ranked run2nd-ranked run3rd-ranked runmedian runLucene
 0 0.001 0.002 0.003 0.004
 0.005 0.006
 10  100  1000  10000POURPRE recall length of response (non-whitespace characters)
Evaluation of TREC 2005 "other" questionstop-ranked run2nd-ranked run3rd-ranked runmedian runLucene
Figure 2: POURPRE recall curves for ?other? runs from TREC 2004 (left) and TREC 2005 (right).
Run 2004 2005
top-ranked run 0.460 0.248
2nd-ranked run 0.404 0.232
3rd-ranked run 0.367 0.228
median run 0.197 0.152
Table 3: Official scores of selected TREC 2004 and
TREC 2005 ?other? runs.
5 ?Other? Questions
Our second set of experiments examine the perfor-
mance of TREC systems on ?other? questions. Once
again, we selected the top-ranked, second-ranked,
third-ranked, and median runs from TREC 2004 and
TREC 2005. Since system submissions were al-
ready passages, no additional processing was nec-
essary. The IR baseline was exactly the same as the
run used in the previous experiment. Below, we de-
scribe the evaluation methodology and results.
5.1 Evaluation Methodology
The evaluation of ?other? questions closely mir-
rors the procedure developed for factoid series. We
employed POURPRE (Lin and Demner-Fushman,
2005), a recently developed method for automati-
cally evaluating answers to complex questions. The
metric relies on n-gram overlap as a surrogate for
manual nugget matching, and has been shown to cor-
relate well with official human judgments. We mod-
ified the POURPRE scoring script to return only the
nugget recall (of vital nuggets only).
Formally, systems? responses to ?other? questions
consist of unordered sets of answer strings. We de-
cided to break each system?s response into individ-
ual answer strings and compute nugget recall on a
string-by-string basis. Since these answer strings
are for the most part sentences, results are compara-
ble to the factoid series experiments. Taking answer
strings as the basic response unit also makes sense
because it respects segment boundaries that are pre-
sumably meaningful to the original systems.
Computing POURPRE recall at different response
lengths yielded an uninterpolated data series for
each topic. Results across topics were aggregated
in the same manner as the factoid series: first by
interpolating to the nearest larger fifty-character in-
crement, and then averaging all topics across each
length increment.3
5.2 Results
Results of our experiment are shown in Figure 2. For
reference, the official nugget F-scores of the TREC
runs are shown in Table 3. Most striking is the ob-
servation that the baseline Lucene run is highly com-
petitive with submitted TREC systems. For TREC
2004, it appears that the IR baseline outperforms all
but the top two systems at higher recall levels. For
TREC 2005, differences between all the analyzed
runs are difficult to distinguish. Although scores
of submitted runs in TREC 2005 were more tightly
clustered, the strong baseline IR performance is sur-
prising. For ?other? questions, it doesn?t appear that
QA is better than IR!
We believe that relative differences in QA and IR
3It is worth noting that this protocol treats the answer strings
as if they were ordered?but we do not believe this has an im-
pact on the results or our conclusions.
216
 0 0.001 0.002 0.003 0.004 0.005 0.006
 0.007 0.008
 10  100  1000  10000POURPRE recall length of response (non-whitespace characters)
Evaluation of TREC 2004 factoid and "other" questionstop-ranked run2nd-ranked run3rd-ranked runmedian runLucene
 0 0.001 0.002 0.003 0.004 0.005 0.006
 0.007 0.008
 10  100  1000  10000POURPRE recall length of response (non-whitespace characters)
Evaluation of TREC 2005 factoid and "other" questionstop-ranked run2nd-ranked run3rd-ranked runmedian runLucene
Figure 3: POURPRE recall curves for runs from TREC 2004 (left) and TREC 2005 (right), combining both
factoid and ?other? questions.
performance between the 2004 and 2005 test sets
can be attributed to the nature of the targets. In
TREC 2005, allowable semantic categories of tar-
gets were expanded to include events such as ?Miss
Universe 2000 crowned?, which by their very nature
are narrower in scope. This, combined with many
highly-specific targets, meant that the corpus con-
tained fewer topically-relevant documents for each
target to begin with. As a result, an IR-based sen-
tence extraction approach performs quite well?this
explanation is consistent with the observations of
Lin and Demner-Fushman (2006).
6 Combining Question Types
In the previous two sections, factoid and ?other?
questions were examined in isolation, which ignores
their complementary role in supplying information
about a target. To provide a more complete pic-
ture of system performance, we devised a method by
which both question types can be evaluated together.
At the conceptual level, there is little difference
between factoid and ?other? questions. The first type
asks for explicit facts, while the second type asks
for facts that the user didn?t know enough to ask
about directly. We can unify the evaluation of both
types by treating regular expression factoid patterns
as if they were (vital) nuggets. Many patterns don?t
contain any special symbols, and read quite like
nugget descriptions already. For others, we man-
ually converted regular expressions into plain text,
e.g., ?(auto|car) crash? becomes ?auto car crash?.
To validate this method, we first evaluated fac-
toid series using POURPRE, with nugget descrip-
tions prepared from answer patterns in the manner
described above. For both TREC 2004 and TREC
2005, we did not notice any qualitative differences
in the results, suggesting that factoid answers can
indeed be treated like nuggets.
We then proceeded to evaluate both factoid and
?other? questions together using the above proce-
dure. Runs were prepared by appending the 1st
?other? run to the 1st factoid run, the 2nd ?other?
run to the 2nd factoid run, etc.4 The Lucene base-
line run remained the same as before.
Plots of POURPRE recall by answer length are
shown in Table 3. These graphs provide a more com-
plete picture of QA performance on question series.
The same trends observed in the two previous exper-
iments are seen here also: it does not appear that the
median run in TREC 2004 performs any better than
the IR baseline. Considering the TREC 2005 runs,
the IR baseline remains surprisingly competitive.
Note that integration of list questions, the third
component of question series, remains a challenge.
Whereas the answer to a factoid question can be nat-
urally viewed as a vital nugget describing the target,
the relative importance of a single answer instance to
a list question cannot be easily quantified. We leave
this issue for future work.
7 Discussion
It can be argued that quantitative evaluation is the
single most important driver for advancing the state
4Note that we?re mixing sections from different runs, so
these do not correspond to any actual TREC submissions.
217
of the art in language processing technology today.
As a result, evaluation metrics and methodologies
need to be carefully considered to insure that they
provide proper guidance to researchers. Along these
lines, this paper makes two arguments: that recall
curves better capture aspects of complex QA tasks
than the existing TREC evaluation metrics; and that
this novel evaluation framework allows us to explore
the relationship between QA and IR technology in a
manner not possible before.
7.1 Advantages of Recall Curves
We see several advantages to the evaluation frame-
work introduced here, beyond those already dis-
cussed in Sections 2 and 3.
Previously, QA and IR techniques were not di-
rectly comparable since they returned different re-
sponse units. To make evaluation even more com-
plex, different types of questions (e.g., factoid vs.
?other?) require different metrics?in TREC, these
incomparable values were then aggregated based on
arbitrary weights to produce a final composite score.
By noting similarities between factoid answers and
nuggets, we were able to develop a unified evalu-
ation framework for factoid and ?other? questions.
By emphasizing the similarities between complex
QA and summarization, it becomes possible to com-
pare QA and IR technology directly?this work pro-
vides a point of reference much in the same way that
IR-based sentence extraction has served as a starting
point for summarization research, e.g., (Goldstein et
al., 1999).
In addition, characterizing system performance in
terms of recall curves allows researchers to com-
pare the effectiveness of systems under different task
models. Measuring recall at short response lengths
might reflect time-constrained scenarios, e.g., pro-
ducing an action-oriented report with a 30-minute
deadline. Measuring recall at longer response
lengths might correspond to in-depth research, e.g.,
writing a summary article due by the end of the day.
Recall curves are able to capture potential system
tradeoffs that might otherwise be hidden in single-
point metrics.
7.2 Understanding QA and IR
Beyond answering a straightforward question, the
results of our experiments yield insights about the
relationship between QA and IR technology.
Most question answering systems today employ a
two-stage architecture: IR techniques are first used
to select a candidate set of documents (or alter-
natively, passages, sentences, etc.), which is then
analyzed by more sophisticated NLP techniques.
For factoids, analysis usually involves named-entity
recognition using some sort of answer type ontol-
ogy; for ?other? questions, analysis typically in-
cludes filtering for definitions based on surface pat-
terns and other features. The evaluation framework
described in this paper is able to isolate the per-
formance contribution of this second NLP stage?
which corresponds to the difference between the
baseline IR and QA recall curves.
For factoid questions, NLP technology provides
a lot of added value: the set of techniques devel-
oped for pinpointing exact answers allows users to
acquire information more quickly than they other-
wise could with an IR system (shown by Figure 1).
The added value of NLP techniques for answering
?other? questions is less clear?in many instances,
those techniques do not appear to be contributing
much (shown by Figure 2). Whereas factoid QA
technology is relatively mature, researchers have
made less progress in developing general techniques
for answering complex questions.
Our experiments also illuminate when exactly QA
works. For short responses, there is little differ-
ence between QA and IR, or between all QA sys-
tems for that matter, since it is difficult to cram
much information into a short response with cur-
rent (extractive) technology. For extremely long re-
sponses, the advantages provided by the best QA
systems are relatively small, since there?s an upper
limit to their accuracy (and researchers have yet to
develop a good backoff strategy). In the middle
range of response lengths is where QA technology
really shines?where a user can much more effec-
tively gather knowledge using a QA system.
7.3 Implications for Future Research
Based on the results presented here, we suggest two
future directions for the field of question answering.
First, we believe there is a need to focus on an-
swer generation. High-precision answer extraction
alone isn?t sufficient to address users? complex in-
formation needs?information nuggets must be syn-
218
thesized and presented for efficient human consump-
tion. The coherence and fluency of system responses
should be factored into the evaluation methodology
as well. In this regard, QA researchers have much
to learn from the summarization community, which
has already grappled with these issues.
Second, more effort is required to developed task-
based QA evaluations. The ?goodness? of answers
can only be quantified with respect to a task?
examples range from winning a game show (Clarke
et al, 2001) to intelligence gathering (Small et al,
2004). It is impossible to assess the real-world
impact of QA technology without considering how
such systems will be used to solve human problems.
Our work takes a small step in this direction.
8 Conclusion
Is QA better than IR? The short answer, somewhat to
our relief, is yes. But this work provides more than
a simple affirmation. We believe that our contribu-
tions are two-fold: a novel framework for evaluating
QA systems that more realistically models user tasks
and preferences, and an exploration of QA and IR
performance within this framework that yields new
insights about these two technologies. We hope that
these results are useful in guiding the development
of future question answering systems.
9 Acknowledgments
This work has been supported in part by DARPA
contract HR0011-06-2-0001 (GALE), and has bene-
fited from discussions with Ellen Voorhees and Hoa
Dang. I would also like to thank Esther and Kiri for
their loving support.
References
Regina Barzilay, Kathleen R. McKeown, and Michael
Elhadad. 1999. Information fusion in the context
of multi-document summarization. In Proc. of ACL
1999.
Charles Clarke, Gordon Cormack, and Thomas Lynam.
2001. Exploiting redundancy in question answering.
In Proc. of SIGIR 2001.
Hoa Trang Dang, Jimmy Lin, and Diane Kelly. 2006.
Overview of the TREC 2006 question answering track.
In Proc. of TREC 2006.
Hoa Dang. 2005. Overview of DUC 2005. In Proc. of
DUC 2005.
Jade Goldstein, Mark Kantrowitz, Vibhu Mittal, and
Jaime Carbonell. 1999. Summarizing text documents:
Sentence selection and evaluation metrics. In Proc. of
SIGIR 1999.
Tsuneaki Kato, Jun?ichi Fukumoto, Fumito Masui, and
Noriko Kando. 2004. Handling information access
dialogue through QA technologies?a novel challenge
for open-domain question answering. In Proc. of the
HLT-NAACL 2004 Workshop on Pragmatics of Ques-
tion Answering.
Jimmy Lin and Dina Demner-Fushman. 2005. Automat-
ically evaluating answers to definition questions. In
Proc. of HLT/EMNLP 2005.
Jimmy Lin and Dina Demner-Fushman. 2006. Will
pyramids built of nuggets topple over? In Proc. of
HLT/NAACL 2006.
Jimmy Lin, Dennis Quan, Vineet Sinha, Karun Bakshi,
David Huynh, Boris Katz, and David R. Karger. 2003.
What makes a good answer? The role of context in
question answering. In Proc. of INTERACT 2003.
Ani Nenkova and Rebecca Passonneau. 2004. Evaluat-
ing content selection in summarization: The pyramid
method. In Proc. of HLT/NAACL 2004.
Sharon Small, Tomek Strzalkowski, Ting Liu, Sean
Ryan, Robert Salkin, Nobuyuki Shimizu, Paul Kan-
tor, Diane Kelly, Robert Rittman, and Nina Wacholder.
2004. HITIQA: towards analytical question answer-
ing. In Proc. of COLING 2004.
Stefanie Tellex, Boris Katz, Jimmy Lin, Gregory Marton,
and Aaron Fernandes. 2003. Quantitative evaluation
of passage retrieval algorithms for question answering.
In Proc. of SIGIR 2003.
Ellen M. Voorhees and DawnM. Tice. 1999. The TREC-
8 question answering track evaluation. In Proc. of
TREC-8.
Ellen M. Voorhees. 2003. Overview of the TREC 2003
question answering track. In Proc. of TREC 2003.
Ellen M. Voorhees. 2004. Overview of the TREC 2004
question answering track. In Proc. of TREC 2004.
Ellen M. Voorhees. 2005. Using question series to evalu-
ate question answering system effectiveness. In Proc.
of HLT/EMNLP 2005.
Justin Zobel, Alistair Moffat, and Ross Wilkinson Ron
Sacks-Davis. 1995. Efficient retrieval of partial docu-
ments. IPM, 31(3):361?377.
219
Proceedings of NAACL HLT 2009: Tutorials, pages 1?2,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Data?Intensive?Text?Processing?with?MapReduce?
?
Jimmy?Lin?and?Chris?Dyer?
University?of?Maryland,?College?Park?
{jimmylin,redpony}@umd.edu?
Overview?
This?half?day?tutorial?introduces?participants?to?data?intensive?text?processing?with?the?MapReduce?
programming?model?[1],?using?the?open?source?Hadoop?implementation.?The?focus?will?be?on?scalability?
and?the?tradeoffs?associated?with?distributed?processing?of?large?datasets.?Content?will?include?general?
discussions?about?algorithm?design,?presentation?of?illustrative?algorithms,?case?studies?in?HLT?
applications,?as?well?as?practical?advice?in?writing?Hadoop?programs?and?running?Hadoop?clusters.?
Amazon?has?generously?agreed?to?provide?each?participant?with?$100?in?Amazon?Web?Services?(AWS)?
credits?that?can?used?toward?its?Elastic?Compute?Cloud?(EC2)??utility?computing??service?(sufficient?for?
1000?instance?hours).?EC2?allows?anyone?to?rapidly?provision?Hadoop?clusters??on?the?fly??without?
upfront?hardware?investments,?and?provides?a?low?cost?vehicle?for?exploring?Hadoop.?
Intended?Audience?
The?tutorial?is?targeted?at?any?NLP?researcher?interested?in?data?intensive?processing?and?scalability?
issues?in?general.?No?background?in?parallel?or?distributed?computing?is?necessary,?but?a?prior?knowledge?
of?HLT?is?assumed.?
Course?Objectives?
? Acquire?understanding?of?the?MapReduce?programming?model?and?how?it?relates?to?alternative?
approaches?to?concurrent?programming.?
? Acquire?understanding?of?how?data?intensive?HLT?problems?(e.g.,?text?retrieval,?iterative?
optimization?problems,?etc.)?can?be?solved?using?MapReduce.?
? Acquire?understanding?of?the?tradeoffs?involved?in?designing?MapReduce?algorithms?and?
awareness?of?associated?engineering?issues.?
Tutorial?Topics?
The?following?lists?topics?that?will?be?covered:?
? MapReduce?algorithm?design?
? Distributed?counting?applications?(e.g.,?relative?frequency?estimation)?
? Applications?to?text?retrieval?
? Applications?to?graph?algorithms?
? Applications?to?iterative?optimization?algorithms?(e.g.,?EM)?
? Practical?Hadoop?issues?
? Limitations?of?MapReduce?
Instructor?Bios?
Jimmy?Lin?is?an?assistant?professor?in?the?iSchool?at?the?University?of?Maryland,?College?Park.?He?joined?
the?faculty?in?2004?after?completing?his?Ph.D.?in?Electrical?Engineering?and?Computer?Science?at?MIT.?Dr.?
Lin?s?research?interests?lie?at?the?intersection?of?natural?language?processing?and?information?retrieval.?
1
He?leads?the?University?of?Maryland?s?effort?in?the?Google/IBM?Academic?Cloud?Computing?Initiative.?Dr.?
Lin?has?taught?two?semester?long?Hadoop?courses?[2]?and?has?given?numerous?talks?about?MapReduce?
to?a?wide?audience.?
Chris?Dyer?is?a?Ph.D.?student?at?the?University?of?Maryland,?College?Park,?in?the?Department?of?
Linguistics.??His?current?research?interests?include?statistical?machine?translation,?machine?learning,?and?
the?relationship?between?artificial?language?processing?systems?and?the?human?linguistic?processing?
system.?He?has?served?on?program?committees?for?AMTA,?ACL,?COLING,?EACL,?EMNLP,?NAACL,?ISWLT,?
and?the?ACL?Workshops?on?Machine?translation,?and?is?one?of?the?developers?of?the?Moses?open?source?
machine?translation?toolkit.?He?has?practical?experience?solving?NLP?problems?with?both?the?Hadoop?
MapReduce?framework?and?Google?s?MapReduce?implementation,?which?was?made?possible?by?an?
internship?with?Google?Research?in?2008.??
Acknowledgments?
This?work?is?supported?by?NSF?under?awards?IIS?0705832?and?IIS?0836560;?the?Intramural?Research?
Program?of?the?NIH,?National?Library?of?Medicine;?DARPA/IPTO?Contract?No.?HR0011?06?2?0001?under?
the?GALE?program.?Any?opinions,?findings,?conclusions,?or?recommendations?expressed?here?are?the?
instructors??and?do?not?necessarily?reflect?those?of?the?sponsors.?We?are?grateful?to?Amazon?for?its?
support?of?tutorial?participants.?
References?
[1]??Dean,?Jeffrey?and?Sanjay?Ghemawat.?MapReduce:?Simplified?Data?Processing?on?Large?Clusters.?
Proceedings?of?the?6th?Symposium?on?Operating?System?Design?and?Implementation?(OSDI?2004),?p.?
137?150,?2004,?San?Francisco,?California.?
[2]??Jimmy?Lin.?Exploring?Large?Data?Issues?in?the?Curriculum:?A?Case?Study?with?MapReduce.?
Proceedings?of?the?Third?Workshop?on?Issues?in?Teaching?Computational?Linguistics?(TeachCL?08)?at?
ACL?2008,?p.?54?61,?2008,?Columbus,?Ohio.?
?
2
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 841?848,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Answer Extraction, Semantic Clustering, and Extractive Summarization
for Clinical Question Answering
Dina Demner-Fushman1,3 and Jimmy Lin1,2,3
1Department of Computer Science
2College of Information Studies
3Institute for Advanced Computer Studies
University of Maryland
College Park, MD 20742, USA
demner@cs.umd.edu, jimmylin@umd.edu
Abstract
This paper presents a hybrid approach
to question answering in the clinical
domain that combines techniques from
summarization and information retrieval.
We tackle a frequently-occurring class of
questions that takes the form ?What is
the best drug treatment for X?? Starting
from an initial set of MEDLINE citations,
our system first identifies the drugs un-
der study. Abstracts are then clustered us-
ing semantic classes from the UMLS on-
tology. Finally, a short extractive sum-
mary is generated for each abstract to pop-
ulate the clusters. Two evaluations?a
manual one focused on short answers and
an automatic one focused on the support-
ing abstracts?demonstrate that our sys-
tem compares favorably to PubMed, the
search system most widely used by physi-
cians today.
1 Introduction
Complex information needs can rarely be ad-
dressed by single documents, but rather require the
integration of knowledge from multiple sources.
This suggests that modern information retrieval
systems, which excel at producing ranked lists of
documents sorted by relevance, may not be suffi-
cient to provide users with a good overview of the
?information landscape?.
Current question answering systems aspire to
address this shortcoming by gathering relevant
?facts? from multiple documents in response to
information needs. The so-called ?definition?
or ?other? questions at recent TREC evalua-
tions (Voorhees, 2005) serve as good examples:
?good answers? to these questions include inter-
esting ?nuggets? about a particular person, organi-
zation, entity, or event.
The importance of cross-document information
synthesis has not escaped the attention of other re-
searchers. The last few years have seen a conver-
gence between the question answering and sum-
marization communities (Amigo? et al, 2004), as
highlighted by the shift from generic to query-
focused summaries in the 2005 DUC evalua-
tion (Dang, 2005). Despite a focus on document
ranking, different techniques for organizing search
results have been explored by information retrieval
researchers, as exemplified by techniques based on
clustering (Hearst and Pedersen, 1996; Dumais et
al., 2001; Lawrie and Croft, 2003).
Our work, which is situated in the domain of
clinical medicine, lies at the intersection of ques-
tion answering, information retrieval, and summa-
rization. We employ answer extraction to identify
short answers, semantic clustering to group sim-
ilar results, and extractive summarization to pro-
duce supporting evidence. This paper describes
how each of these capabilities contributes to an in-
formation system tailored to the requirements of
physicians. Two separate evaluations demonstrate
the effectiveness of our approach.
2 Clinical Information Needs
Although the need to answer questions related
to patient care has been well documented (Cov-
ell et al, 1985; Gorman et al, 1994; Ely et al,
1999), studies have shown that existing search sys-
tems, e.g., PubMed, the U.S. National Library of
Medicine?s search engine, are often unable to sup-
ply physicians with clinically-relevant answers in
a timely manner (Gorman et al, 1994; Cham-
bliss and Conley, 1996). Clinical information
841
Disease: Chronic Prostatitis
I anti-microbial
1. [temafloxacin] Treatment of chronic bacterial prostatitis with temafloxacin. Temafloxacin 400 mg b.i.d. adminis-
tered orally for 28 days represents a safe and effective treatment for chronic bacterial prostatitis.
2. [ofloxacin] Ofloxacin in the management of complicated urinary tract infections, including prostatitis. In chronic
bacterial prostatitis, results to date suggest that ofloxacin may be more effective clinically and as effective micro-
biologically as carbenicillin.
3. ...
I Alpha-adrenergic blocking agent
1. [terazosine] Terazosin therapy for chronic prostatitis/chronic pelvic pain syndrome: a randomized, placebo con-
trolled trial. CONCLUSIONS: Terazosin proved superior to placebo for patients with chronic prostatitis/chronic
pelvic pain syndrome who had not received alpha-blockers previously.
2. ...
Table 1: System response to the question ?What is the best drug treatment for chronic prostatitis??
systems for decision support represent a poten-
tially high-impact application. From a research
perspective, the clinical domain is attractive be-
cause substantial knowledge has already been cod-
ified in the Unified Medical Language System
(UMLS) (Lindberg et al, 1993). The 2004 version
of the UMLS Metathesaurus contains information
about over 1 million biomedical concepts and 5
million concept names. This and related resources
allow us to explore knowledge-based techniques
with substantially less upfront investment.
Naturally, physicians have a wide spectrum of
information needs, ranging from questions about
the selection of treatment options to questions
about legal issues. To make the retrieval problem
more tractable, we focus on a subset of therapy
questions taking the form ?What is the best drug
treatment for X??, where X can be any number of
diseases. We have chosen to tackle this class of
questions because studies of physicians? behavior
in natural settings have revealed that such ques-
tions occur quite frequently (Ely et al, 1999). By
leveraging the natural distribution of clinical in-
formation needs, we can make the greatest impact
with the least effort.
Our research follows the principles of evidence-
based medicine (EBM) (Sackett et al, 2000),
which provides a well-defined model to guide the
process of clinical question answering. EBM is
a widely-accepted paradigm for medical practice
that involves the explicit use of current best ev-
idence, i.e., high-quality patient-centered clinical
research reported in the primary medical literature,
to make decisions about patient care. As shown
by previous work (Cogdill and Moore, 1997; De
Groote and Dorsch, 2003), citations from the
MEDLINE database (maintained by the U.S. Na-
tional Library of Medicine) serve as a good source
of clinical evidence. As a result of these findings,
our work focuses on MEDLINE abstracts as the
source for answers.
3 Question Answering Approach
Conflicting desiderata shape the characteristics of
?answers? to clinical questions. On the one hand,
conciseness is paramount. Physicians are always
under time pressure when making decisions, and
information overload is a serious concern. Fur-
thermore, we ultimately envision deploying ad-
vanced retrieval systems in portable packages such
as PDAs to serve as tools in bedside interac-
tions (Hauser et al, 2004). The small form factor
of such devices limits the amount of text that can
be displayed. However, conciseness exists in ten-
sion with completeness. For physicians, the im-
plications of making potentially life-altering deci-
sions mean that all evidence must be carefully ex-
amined in context. For example, the efficacy of a
drug is always framed in the context of a specific
sample population, over a set duration, at some
fixed dosage, etc. A physician simply cannot rec-
ommend a particular course of action without con-
sidering all these factors.
Our approach seeks to balance conciseness and
completeness by providing hierarchical and inter-
842
active ?answers? that support multiple levels of
drill-down. A partial example is shown in Fig-
ure 1. Top-level answers to ?What is the best drug
treatment for X?? consist of categories of drugs
that may be of interest to the physician. Each cat-
egory is associated with a cluster of abstracts from
MEDLINE about that particular treatment option.
Drilling down into a cluster, the physician is pre-
sented with extractive summaries of abstracts that
outline the clinical findings. To obtain more detail,
the physician can pull up the complete abstract
text, and finally the electronic version of the en-
tire article (if available). In the example shown in
Figure 1, the physician can see that two classes of
drugs (anti-microbial and alpha-adrenergic block-
ing agent) are relevant for the disease ?chronic
prostatitis?. Drilling down into the first cluster, the
physician can see summarized evidence for two
specific types of anti-microbials (temafloxacin and
ofloxacin) extracted from MEDLINE abstracts.
Three major capabilities are required to produce
the ?answers? described above. First, the system
must accurately identify the drugs under study in
an abstract. Second, the system must group ab-
stracts based on these substances in a meaningful
way. Third, the system must generate short sum-
maries of the clinical findings. We describe a clin-
ical question answering system that implements
exactly these capabilities (answer extraction, se-
mantic clustering, and extractive summarization).
4 System Implementation
Our work is primarily concerned with synthesiz-
ing coherent answers from a set of search results?
the actual source of these results is not important.
For convenience, we employ MEDLINE citations
retrieved by the PubMed search engine (which
also serves as a baseline for comparison). Given
an initial set of citations, answer generation pro-
ceeds in three phases, described below.
4.1 Answer Extraction
Given a set of abstracts, our system first identi-
fies the drugs under study; these later become the
short answers. In the parlance of evidence-based
medicine, drugs fall into the category of ?interven-
tions?, which encompasses everything from surgi-
cal procedures to diagnostic tests.
Our extractor for interventions relies on
MetaMap (Aronson, 2001), a program that au-
tomatically identifies entities corresponding to
UMLS concepts. UMLS has an extensive cov-
erage of drugs, falling under the semantic type
PHARMACOLOGICAL SUBSTANCE and a few oth-
ers. All such entities are identified as candidates
and each is scored based on a number of features:
its position in the abstract, its frequency of occur-
rence, etc. A separate evaluation on a blind test
set demonstrates that our extractor is able to accu-
rately recognize the interventions in a MEDLINE
abstract; see details in (Demner-Fushman and Lin,
2005; Demner-Fushman and Lin, 2006 in press).
4.2 Semantic Clustering
Retrieved MEDLINE citations are organized into
semantic clusters based on the main interventions
identified in the abstract text. We employed a
variant of the hierarchical agglomerative cluster-
ing algorithm (Zhao and Karypis, 2002) that uti-
lizes semantic relationships within UMLS to com-
pute similarities between interventions.
Iteratively, we group abstracts whose interven-
tions fall under a common ancestor, i.e., a hyper-
nym. The more generic ancestor concept (i.e., the
class of drugs) is then used as the cluster label.
The process repeats until no new clusters can be
formed. In order to preserve granularity at the
level of practical clinical interest, the tops of the
UMLS hierarchy were truncated; for example, the
MeSH category ?Chemical and Drugs? is too gen-
eral to be useful. This process was manually per-
formed during system development. We decided
to allow an abstract to appear in multiple clusters
if more than one intervention was identified, e.g.,
if the abstract compared the efficacy of two treat-
ments. Once the clusters have been formed, all
citations are then sorted in the order of the origi-
nal PubMed results, with the most abstract UMLS
concept as the cluster label. Clusters themselves
are sorted in decreasing size under the assumption
that more clinical research is devoted to more per-
tinent types of drugs.
Returning to the example in Figure 1, the ab-
stracts about temafloxacin and ofloxacin were
clustered together because both drugs are hy-
ponyms of anti-microbials within the UMLS on-
tology. As can be seen, this semantic resource pro-
vides a powerful tool for organizing search results.
4.3 Extractive Summarization
For each MEDLINE citation, our system gener-
ates a short extractive summary consisting of three
elements: the main intervention (which is usu-
843
ally more specific than the cluster label); the ti-
tle of the abstract; and the top-scoring outcome
sentence. The ?outcome?, another term from
evidence-based medicine, asserts the clinical find-
ings of a study, and is typically found towards
the end of a MEDLINE abstract. In our case,
outcome sentences state the efficacy of a drug in
treating a particular disease. Previously, we have
built an outcome extractor capable of identifying
such sentences in MEDLINE abstracts using su-
pervised machine learning techniques (Demner-
Fushman and Lin, 2005; Demner-Fushman and
Lin, 2006 in press). Evaluation on a blind held-
out test set shows high classification accuracy.
5 Evaluation Methodology
Given that our work draws from QA, IR, and sum-
marization, a proper evaluation that captures the
salient characteristics of our system proved to be
quite challenging. Overall, evaluation can be de-
composed into two separate components: locating
a suitable resource to serve as ground truth and
leveraging it to assess system responses.
It is not difficult to find disease-specific pharma-
cology resources. We employed Clinical Evidence
(CE), a periodic report created by the British Med-
ical Journal (BMJ) Publishing Group that summa-
rizes the best known drugs for a few dozen dis-
eases. Note that the existence of such secondary
sources does not obviate the need for automated
systems because they are perpetually falling out of
date due to rapid advances in medicine. Further-
more, such reports are currently created by highly-
experienced physicians, which is an expensive and
time-consuming process.
For each disease, CE classifies drugs into one of
six categories: beneficial, likely beneficial, trade-
offs (i.e., may have adverse side effects), un-
known, unlikely beneficial, and harmful. Included
with each entry is a list of references?citations
consulted by the editors in compiling the resource.
Although the completeness of the drugs enumer-
ated in CE is questionable, it nevertheless can be
viewed as ?authoritative?.
5.1 Previous Work
How can we leverage a resource such as CE to as-
sess the responses generated by our system? A
survey of evaluation methodologies reveals short-
comings in existing techniques.
Answers to factoid questions are automatically
scored using regular expression patterns (Lin,
2005). In our application, this is inadequate
for many reasons: there is rarely an exact string
match between system output and drugs men-
tioned in CE, primarily due to synonymy (for ex-
ample, alpha-adrenergic blocking agent and ?-
blocker refer to the same class of drugs) and on-
tological mismatch (for example, CE might men-
tion beta-agonists, while a retrieved abstract dis-
cusses formoterol, which is a specific represen-
tative of beta-agonists). Furthermore, while this
evaluation method can tell us if the drugs proposed
by the system are ?good?, it cannot measure how
well the answer is supported by MEDLINE cita-
tions; recall that answer justification is important
for physicians.
The nugget evaluation methodology (Voorhees,
2005) developed for scoring answers to com-
plex questions is not suitable for our task, since
there is no coherent notion of an ?answer text?
that the user reads end?to?end. Furthermore, it
is unclear what exactly a ?nugget? in this case
would be. For similar reasons, methodologies for
summarization evaluation are also of little help.
Typically, system-generated summaries are either
evaluated manually by humans (which is expen-
sive and time-consuming) or automatically using
a metric such as ROUGE, which compares sys-
tem output against a number of reference sum-
maries. The interactive nature of our answers vio-
lates the assumption that systems? responses are
static text segments. Furthermore, it is unclear
what exactly should go into a reference summary,
because physicians may want varying amounts of
detail depending on familiarity with the disease
and patient-specific factors.
Evaluation methodologies from information re-
trieval are also inappropriate. User studies have
previously been employed to examine the effect
of categorized search results. However, they often
conflate the effectiveness of the interface with that
of the underlying algorithms. For example, Du-
mais et al (2001) found significant differences in
task performance based on different ways of using
purely presentational devices such as mouseovers,
expandable lists, etc. While interface design is
clearly important, it is not the focus of our work.
Clustering techniques have also been evaluated
in the same manner as text classification algo-
rithms, in terms of precision, recall, etc. based
on some ground truth (Zhao and Karypis, 2002).
844
This, however, assumes the existence of stable,
invariant categories, which is not the case since
our output clusters are query-specific. Although
it may be possible to manually create ?reference
clusters?, we lack sufficient resources to develop
such a data set. Furthermore, it is unclear if suffi-
cient interannotator agreement can be obtained to
support meaningful evaluation.
Ultimately, we devised two separate evaluations
to assess the quality of our system output based
on the techniques discussed above. The first is
a manual evaluation focused on the cluster labels
(i.e., drug categories), based on a factoid QA eval-
uation methodology. The second is an automatic
evaluation of the retrieved abstracts using ROUGE,
drawing elements from summarization evaluation.
Details of the evaluation setup and results are pre-
ceded by a description of the test collection we
created from CE.
5.2 Test Collection
We were able to mine the June 2004 edition of
Clinical Evidence to create a test collection for
system evaluation. We randomly selected thirty
diseases, generating a development set of five
questions and a test set of twenty-five questions.
Some examples include: acute asthma, chronic
prostatitis, community acquired pneumonia, and
erectile dysfunction. CE listed an average of 11.3
interventions per disease; of those, 2.3 on average
were marked as beneficial and 1.9 as likely benefi-
cial. On average, there were 48.4 references asso-
ciated with each disease, representing the articles
consulted during the compilation of CE itself. Of
those, 34.7 citations on average appeared in MED-
LINE; we gathered all these abstracts, which serve
as the reference summaries for our ROUGE-based
automatic evaluation.
Since the focus of our work is not on retrieval al-
gorithms per se, we employed PubMed to fetch an
initial set of MEDLINE citations and performed
answer synthesis using those results. The PubMed
citations also serve as a baseline, since it repre-
sents a system commonly used by physicians.
In order to obtain the best possible set of ci-
tations, the first author (an experienced PubMed
searcher), manually formulated queries, taking
advantage of MeSH (Medical Subject Headings)
terms when available. MeSH terms are controlled
vocabulary concepts assigned manually by trained
medical indexers (based on the full text of the ar-
ticles), and encode a substantial amount of knowl-
edge about the contents of the citation. PubMed
allows searches on MeSH terms, which usually
yield accurate results. In addition, we limited re-
trieved citations to those that have theMeSH head-
ing ?drug therapy? and those that describe a clin-
ical trial (another metadata field). Finally, we re-
stricted the date range of the queries so that ab-
stracts published after our version of CE were ex-
cluded. Although the query formulation process
currently requires a human, we envision automat-
ing this step using a template-based approach in
the future.
6 System Evaluation
We adapted existing techniques to evaluate our
system in two separate ways: a factoid-style man-
ual evaluation focused on short answers and an
automatic evaluation with ROUGE using CE-cited
abstracts as the reference summaries. The setup
and results for both are detailed below.
6.1 Manual Evaluation of Short Answers
In our manual evaluation, system outputs were as-
sessed as if they were answers to factoid ques-
tions. We gathered three different sets of answers.
For the baseline, we used the main intervention
from each of the first three PubMed citations. For
our test condition, we considered the three largest
clusters, taking the main intervention from the first
abstract in each cluster. This yields three drugs
that are at the same level of ontological granularity
as those extracted from the unclustered PubMed
citations. For our third condition, we assumed the
existence of an oracle which selects the three best
clusters (as determined by the first author, a med-
ical doctor). From each of these three clusters,
we extracted the main intervention of the first ab-
stracts. This oracle condition represents an achiev-
able upper bound with a human in the loop. Physi-
cians are highly-trained professionals that already
have significant domain knowledge. Faced with a
small number of choices, it is likely that they will
be able to select the most promising cluster, even
if they did not previously know it.
This preparation yielded up to nine drug names,
three from each experimental condition. For short,
we refer to these as PubMed, Cluster, and Oracle,
respectively. After blinding the source of the drugs
and removing duplicates, each short answer was
presented to the first author for evaluation. Since
845
Clinical Evidence Physician
B LB T U UB H N Good Okay Bad
PubMed 0.200 0.213 0.160 0.053 0.000 0.013 0.360 0.600 0.227 0.173
Cluster 0.387 0.173 0.173 0.027 0.000 0.000 0.240 0.827 0.133 0.040
Oracle 0.400 0.200 0.133 0.093 0.013 0.000 0.160 0.893 0.093 0.013
Table 2: Manual evaluation of short answers: distribution of system answers with respect to CE cat-
egories (left side) and with respect to the assessor?s own expertise (right side). (Key: B=beneficial,
LB=likely beneficial, T=tradeoffs, U=unknown, UB=unlikely beneficial, H=harmful, N=not in CE)
the assessor had no idea from which condition an
answer came, this process guarded against asses-
sor bias.
Each answer was evaluated in two different
ways: first, with respect to the ground truth in CE,
and second, using the assessor?s own medical ex-
pertise. In the first set of judgments, the asses-
sor determined which of the six categories (ben-
eficial, likely beneficial, tradeoffs, unknown, un-
likely beneficial, harmful) the system answer be-
longed to, based on the CE recommendations. As
we have discussed previously, a human (with suf-
ficient domain knowledge) is required to perform
this matching due to synonymy and differences in
ontological granularity. However, note that the as-
sessor only considered the drug name when mak-
ing this categorization. In the second set of judg-
ments, the assessor separately determined if the
short answer was ?good?, ?okay? (marginal), or
?bad? based both on CE and her own experience,
taking into account the abstract title and the top-
scoring outcome sentence (and if necessary, the
entire abstract text).
Results of this manual evaluation are presented
in Table 2, which shows the distribution of judg-
ments for the three experimental conditions. For
baseline PubMed, 20% of the examined drugs fell
in the beneficial category; the values are 39% for
the Cluster condition and 40% for the Oracle con-
dition. In terms of short answers, our system
returns approximately twice as many beneficial
drugs as the baseline, a marked increase in answer
accuracy. Note that a large fraction of the drugs
evaluated were not found in CE at all, which pro-
vides an estimate of its coverage. In terms of the
assessor?s own judgments, 60% of PubMed short
answers were found to be ?good?, compared to
83% and 89% for the Cluster and Oracle condi-
tions, respectively. From a factoid QA point of
view, we can conclude that our system outper-
forms the PubMed baseline.
6.2 Automatic Evaluation of Abstracts
A major limitation of the factoid-based evaluation
methodology is that it does not measure the qual-
ity of the abstracts from which the short answers
were extracted. Since we lacked the necessary
resources to manually gather abstract-level judg-
ments for evaluation, we sought an alternative.
Fortunately, CE can be leveraged to assess the
?goodness? of abstracts automatically. We assume
that references cited in CE are examples of high
quality abstracts, since they were used in gener-
ating the drug recommendations. Following stan-
dard assumptions made in summarization evalu-
ation, we considered abstracts that are similar in
content with these ?reference abstracts? to also be
?good? (i.e., relevant). Similarity in content can
be quantified with ROUGE.
Since physicians demand high precision, we as-
sess the cumulative relevance after the first, sec-
ond, and third abstract that the clinician is likely
to have examined (where the relevance for each
individual abstract is given by its ROUGE-1 pre-
cision score). For the baseline PubMed condition,
the examined abstracts simply correspond to the
first three hits in the result set. For our test system,
we developed three different orderings. The first,
which we term cluster round-robin, selects the first
abstract from the top three clusters (by size). The
second, which we term oracle cluster order, selects
three abstracts from the best cluster, assuming the
existence of an oracle that informs the system. The
third, which we term oracle round-robin, selects
the first abstract from each of the three best clus-
ters (also determined by an oracle).
Results of this evaluation are shown in Table 3.
The columns show the cumulative relevance (i.e.,
ROUGE score) after examining the first, second,
and third abstract, under the different ordering
conditions. To determine statistical significance,
we applied the Wilcoxon signed-rank test, the
846
Rank 1 Rank 2 Rank 3
PubMed Ranked List 0.170 0.349 0.523
Cluster Round-Robin 0.181 (+6.3%)? 0.356 (+2.1%)? 0.526 (+0.5%)?
Oracle Cluster Order 0.206 (+21.5%)M 0.392 (+12.6%)M 0.597 (+14.0%)N
Oracle Round-Robin 0.206 (+21.5%)M 0.396 (+13.6%)M 0.586 (+11.9%)N
Table 3: Cumulative relevance after examining the first, second, and third abstracts, according to different
orderings. (? denotes n.s., M denotes sig. at 0.90, N denotes sig. at 0.95)
standard non-parametric test for applications of
this type. Due to the relatively small test set (only
25 questions), the increase in cumulative relevance
exhibited by the cluster round-robin condition is
not statistically significant. However, differences
for the oracle conditions were significant.
7 Discussion and Related Work
According to two separate evaluations, it appears
that our system outperforms the PubMed baseline.
However, our approach provides more advantages
over a linear result set that are not highlighted in
these evaluations. Although difficult to quantify,
categorized results provide an overview of the in-
formation landscape that is difficult to acquire by
simply browsing a ranked list?user studies of cat-
egorized search have affirmed its value (Hearst
and Pedersen, 1996; Dumais et al, 2001). One
main advantage we see in our application is bet-
ter ?redundancy management?. With a ranked list,
the physician may be forced to browse through
multiple redundant abstracts that discuss the same
or similar drugs to get a sense of the different
treatment options. With our cluster-based ap-
proach, however, potentially redundant informa-
tion is grouped together, since interventions dis-
cussed in a particular cluster are ontologically re-
lated through UMLS. The physician can examine
different clusters for a broad overview, or peruse
multiple abstracts within a cluster for a more thor-
ough review of the evidence. Our cluster-based
system is able to support both types of behaviors.
This work demonstrates the value of semantic
resources in the question answering process, since
our approach makes extensive use of the UMLS
ontology in all phases of answer synthesis. The
coverage of individual drugs, as well as the rela-
tionship between different types of drugs within
UMLS enables both answer extraction and seman-
tic clustering. As detailed in (Demner-Fushman
and Lin, 2006 in press), UMLS-based features are
also critical in the identification of clinical out-
comes, on which our extractive summaries are
based. As a point of comparison, we also im-
plemented a purely term-based approach to clus-
tering PubMed citations. The results are so inco-
herent that a formal evaluation would prove to be
meaningless. Semantic relations between drugs,
as captured in UMLS, provide an effective method
for organizing results?these relations cannot be
captured by keyword content alone. Furthermore,
term-based approaches suffer from the cluster la-
beling problem: it is difficult to automatically gen-
erate a short heading that describes cluster content.
Nevertheless, there are a number of assump-
tions behind our work that are worth pointing
out. First, we assume a high quality initial re-
sult set. Since the class of questions we examine
translates naturally into accurate PubMed queries
that can make full use of human-assigned MeSH
terms, the overall quality of the initial citations
can be assured. Related work in retrieval algo-
rithms (Demner-Fushman and Lin, 2006 in press)
shows that accurate relevance scoring of MED-
LINE citations in response to more general clin-
ical questions is possible.
Second, our system does not actually perform
semantic processing to determine the efficacy of a
drug: it only recognizes ?topics? and outcome sen-
tences that state clinical findings. Since the sys-
tem by default orders the clusters based on size, it
implicitly equates ?most popular drug? with ?best
drug?. Although this assumption is false, we have
observed in practice that more-studied drugs are
more likely to be beneficial.
In contrast with the genomics domain, which
has received much attention from both the IR and
NLP communities, retrieval systems for the clin-
ical domain represent an underexplored area of
research. Although individual components that
attempt to operationalize principles of evidence-
based medicine do exist (Mendonc?a and Cimino,
2001; Niu and Hirst, 2004), complete end?to?
end clinical question answering systems are dif-
847
ficult to find. Within the context of the PERSI-
VAL project (McKeown et al, 2003), researchers
at Columbia have developed a system that lever-
ages patient records to rerank search results. Since
the focus is on personalized summaries, this work
can be viewed as complementary to our own.
8 Conclusion
The primary contribution of this work is the de-
velopment of a clinical question answering system
that caters to the unique requirements of physi-
cians, who demand both conciseness and com-
pleteness. These competing factors can be bal-
anced in a system?s response by providing mul-
tiple levels of drill-down that allow the informa-
tion space to be viewed at different levels of gran-
ularity. We have chosen to implement these capa-
bilities through answer extraction, semantic clus-
tering, and extractive summarization. Two sepa-
rate evaluations demonstrate that our system out-
performs the PubMed baseline, illustrating the ef-
fectiveness of a hybrid approach that leverages se-
mantic resources.
9 Acknowledgments
This work was supported in part by the U.S. Na-
tional Library of Medicine. The second author
thanks Esther and Kiri for their loving support.
References
E. Amigo?, J. Gonzalo, V. Peinado, A. Pen?as, and
F. Verdejo. 2004. An empirical study of informa-
tion synthesis task. In ACL 2004.
A. Aronson. 2001. Effective mapping of biomedi-
cal text to the UMLS Metathesaurus: The MetaMap
program. In AMIA 2001.
M. Chambliss and J. Conley. 1996. Answering clinical
questions. The Journal of Family Practice, 43:140?
144.
K. Cogdill and M. Moore. 1997. First-year medi-
cal students? information needs and resource selec-
tion: Responses to a clinical scenario. Bulletin of
the Medical Library Association, 85(1):51?54.
D. Covell, G. Uman, and P. Manning. 1985. Informa-
tion needs in office practice: Are they being met?
Annals of Internal Medicine, 103(4):596?599.
H. Dang. 2005. Overview of DUC 2005. In DUC
2005 Workshop at HLT/EMNLP 2005.
S. De Groote and J. Dorsch. 2003. Measuring use
patterns of online journals and databases. Journal of
the Medical Library Association, 91(2):231?240.
D. Demner-Fushman and J. Lin. 2005. Knowledge ex-
traction for clinical question answering: Preliminary
results. In AAAI 2005 Workshop on QA in Restricted
Domains.
D. Demner-Fushman and J. Lin. 2006, in press. An-
swering clinical questions with knowledge-based
and statistical techniques. Comp. Ling.
S. Dumais, E. Cutrell, and H. Chen. 2001. Optimizing
search by showing results in context. In CHI 2001.
J. Ely, J. Osheroff, M. Ebell, G. Bergus, B. Levy,
M. Chambliss, and E. Evans. 1999. Analysis of
questions asked by family doctors regarding patient
care. BMJ, 319:358?361.
P. Gorman, J. Ash, and L. Wykoff. 1994. Can pri-
mary care physicians? questions be answered using
the medical journal literature? Bulletin of the Medi-
cal Library Association, 82(2):140?146, April.
S. Hauser, D. Demner-Fushman, G. Ford, and
G. Thoma. 2004. PubMed on Tap: Discovering
design principles for online information delivery to
handheld computers. In MEDINFO 2004.
M. Hearst and J. Pedersen. 1996. Reexaming the clus-
ter hypothesis: Scatter/gather on retrieval results. In
SIGIR 1996.
D. Lawrie and W. Croft. 2003. Generating hierarchical
summaries for Web searches. In SIGIR 2003.
J. Lin. 2005. Evaluation of resources for question an-
swering evaluation. In SIGIR 2005.
D. Lindberg, B. Humphreys, and A. McCray. 1993.
The Unified Medical Language System. Methods of
Information in Medicine, 32(4):281?291.
K. McKeown, N. Elhadad, and V. Hatzivassiloglou.
2003. Leveraging a common representation for per-
sonalized search and summarization in a medical
digital library. In JCDL 2003.
E. Mendonc?a and J. Cimino. 2001. Building a knowl-
edge base to support a digital library. In MEDINFO
2001.
Y. Niu and G. Hirst. 2004. Analysis of semantic
classes in medical text for question answering. In
ACL 2004 Workshop on QA in Restricted Domains.
David Sackett, Sharon Straus, W. Richardson, William
Rosenberg, and R. Haynes. 2000. Evidence-
Based Medicine: How to Practice and Teach EBM.
Churchill Livingstone, second edition.
E. Voorhees. 2005. Using question series to eval-
uate question answering system effectiveness. In
HLT/EMNLP 2005.
Y. Zhao and G. Karypis. 2002. Evaluation of hierar-
chical clustering algorithms for document datasets.
In CIKM 2002.
848
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 945?952,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Leveraging Reusability: Cost-effective Lexical Acquisition 
for Large-scale Ontology Translation 
 
G. Craig Murray 
Bonnie J. Dorr 
Jimmy Lin 
Institute for Advanced Computer Studies 
University of Maryland 
{gcraigm,bdorr,jimmylin}@umd.edu 
Jan Haji? 
Pavel Pecina 
 
Institute for Formal and Applied Linguistics 
Charles University 
{hajic,pecina}@ufal.mff.cuni.cz 
 
  
 
Abstract 
Thesauri and ontologies provide impor-
tant value in facilitating access to digital 
archives by representing underlying prin-
ciples of organization.  Translation of 
such resources into multiple languages is 
an important component for providing 
multilingual access.  However, the speci-
ficity of vocabulary terms in most on-
tologies precludes fully-automated ma-
chine translation using general-domain 
lexical resources.  In this paper, we pre-
sent an efficient process for leveraging 
human translations when constructing 
domain-specific lexical resources.  We 
evaluate the effectiveness of this process 
by producing a probabilistic phrase dic-
tionary and translating a thesaurus of 
56,000 concepts used to catalogue a large 
archive of oral histories.  Our experi-
ments demonstrate a cost-effective tech-
nique for accurate machine translation of 
large ontologies. 
1 Introduction 
Multilingual access to digital collections is an 
important problem in today?s increasingly inter-
connected world.  Although technologies such as 
cross-language information retrieval and ma-
chine translation help humans access information 
they could not otherwise find or understand, they 
are often inadequate for highly specific domains. 
Most digital collections of any significant size 
use a system of organization that facilitates easy 
access to collection contents. Generally, the or-
ganizing principles are captured in the form of a 
controlled vocabulary of keyword phrases (de-
scriptors) representing specific concepts.  These 
descriptors are usually arranged in a hierarchic 
thesaurus or ontology, and are assigned to collec-
tion items as a means of providing access (either 
via searching for keyword phases, browsing the 
hierarchy, or a combination both).  MeSH (Medi-
cal Subject Headings) serves as a good example 
of such an ontology; it is a hierarchically-
arranged collection of controlled vocabulary 
terms manually assigned to medical abstracts in a 
number of databases.  It provides multilingual 
access to the contents of these databases, but 
maintaining translations of such a complex struc-
ture is challenging (Nelson, et al 2004). 
For the most part, research in multilingual in-
formation access focuses on the content of digital 
repositories themselves, often neglecting signifi-
cant knowledge that is explicitly encoded in the 
associated ontologies.  However, information 
systems cannot utilize such ontologies by simply 
applying off-the-shelf machine translation. Gen-
eral-purpose translation resources provide insuf-
ficient coverage of the vocabulary contained 
within these domain-specific ontologies. 
This paper tackles the question of how one 
might efficiently translate a large-scale ontology 
to facilitate multilingual information access.  If 
we need humans to assist in the translation proc-
ess, how can we maximize access while mini-
mizing cost?  Because human translation is asso-
ciated with a certain cost, it is preferable not to 
incur costs of retranslation whenever compo-
nents of translated text are reused. Moreover, 
when exhaustive human translation is not practi-
cal, the most ?useful? components should be 
translated first.  Identifying reusable elements 
and prioritizing their translation based on utility 
is essential to maximizing effectiveness and re-
ducing cost. 
945
We present a process of prioritized translation 
that balances the issues discussed above.  Our 
work is situated in the context of the MALACH 
project, an NSF-funded effort to improve multi-
lingual information access to large archives of 
spoken language (Gustman, et al, 2002).  Our 
process leverages a small set of manually-
acquired English-Czech translations to translate a 
large ontology of keyword phrases, thereby pro-
viding Czech speakers access to 116,000 hours 
of video testimonies in 32 languages. Starting 
from an initial out-of-vocabulary (OOV) rate of 
85%, we show that a small set of prioritized 
translations can be elicited from human infor-
mants, aligned, decomposed and then recom-
bined to cover 90% of the access value in a com-
plex ontology.  Moreover, we demonstrate that 
prioritization based on hierarchical position and 
frequency of use facilitates extremely efficient 
reuse of human input.  Evaluations show that our 
technique is able to boost performance of a sim-
ple translation system by 65%. 
2 The Problem 
The USC Shoah Foundation Institute for Vis-
ual History and Education manages what is pres-
ently the world's largest archive of videotaped 
oral histories (USC, 2006). The archive contains 
116,000 hours of video from the testimonies of 
over 52,000 survivors, liberators, rescuers and 
witnesses of the Holocaust.  If viewed end to 
end, the collection amounts to 13 years of con-
tinuous video.  The Shoah Foundation uses a hi-
erarchically arranged thesaurus of 56,000 key-
word phrases representing domain-specific con-
cepts.  These are assigned to time-points in the 
video testimonies as a means of indexing the 
video content.  Although the testimonies in the 
collection represent 32 different languages, the 
thesaurus used to catalog them is currently avail-
able only in English.  Our task was to translate 
this resource to facilitate multilingual access, 
with Czech as the first target language. 
Our first pass at automating thesaurus transla-
tion revealed that only 15% of the words in the 
vocabulary could be found in an available 
aligned corpus (?mejrek, et al, 2004).  The rest 
of the vocabulary was not available from general 
resources.  Lexical information for translating 
these terms had to be acquired from human in-
put.  Reliable access to digital archives requires 
accuracy. Highly accurate human translations 
incur a cost that is generally proportional to the 
number of words being translated.  However, the 
keyword phrases in the Shoah Foundation?s ar-
chive occur in a Zipfian distribution?a rela-
tively small number of terms provide access to a 
large portion of the video content.  Similarly, a 
great number of highly specific terms describe 
only a small fraction of content.  Therefore, not 
every keyword phrase in the thesaurus carries the 
same value for access to the archive.  The hierar-
chical arrangement of keyword phrases presents 
another issue: some concepts, while not of great 
value for access to segments of video, may be 
important for organizing other concepts and for 
browsing the hierarchy.  These factors must be 
balanced in developing a cost-effective process 
that maximizes utility. 
3 Our Solution 
This paper presents a prioritized human-in-the-
loop approach to translating large-scale ontolo-
gies that is fast, efficient, and cost effective.  Us-
ing this approach, we collected 3,000 manual 
translations of keyword phrases and reused the 
translated terms to generate a lexicon for auto-
mated translation of the rest of the thesaurus.  
The process begins by prioritizing keyword 
phrases for manual translation in terms of their 
value in accessing the collection and the reus-
ability of their component terms.  Translations 
collected from one human informant are then 
checked and aligned to the original English terms 
by a second informant.  From these alignments 
we induce a probabilistic English-Czech phrase 
dictionary.   
To test the effectiveness of this process we 
implemented a simple translation system that 
utilizes the newly generated lexical resources.  
Section 4 reports on two evaluations of the trans-
lation output that quantify the effectiveness of 
our human-in-the-loop approach. 
3.1 Maximizing Value and Reusability 
To quantify their utility, we defined two values 
for each keyword phrase in the thesaurus: a the-
saurus value, representing the importance of the 
keyword phrase for providing access to the col-
lection, and a translation value, representing the 
usefulness of having the keyword phrase trans-
lated.  These values are not identical, but the 
second is related to the first. 
Thesaurus value: Keyword phrases in the 
Shoah Foundation?s thesaurus are arranged into a 
poly-hierarchy in which child nodes may have 
multiple parents.  Internal (non-leaf) nodes of the 
hierarchy are used to organize concepts and sup-
port concept browsing.  Some internal nodes are 
also used to index video content.  Leaf nodes are 
946
very specific and are only used to index video 
content.  Thus, the usefulness of any keyword 
phrase for providing access to the digital collec-
tion is directly related to the concept?s position in 
the thesaurus hierarchy. 
A fragment of the hierarchy is shown in Fig-
ure 1. The keyword phrase ?Auschwitz II-
Birkenau (Poland: Death Camp)?, which de-
scribes a Nazi death camp, is assigned to 17,555 
video segments in the collection.  It has broader 
(parent) terms and narrower (child) terms.  Some 
of the broader and narrower terms are also as-
signed to segments, but not all.  Notably, ?Ger-
man death camps? is not assigned to any video 
segments.  However, ?German death camps? has 
very important narrower terms including 
?Auschwitz II-Birkenau? and others. 
From this example, we can see that an internal 
node is valuable in providing access to its chil-
dren, even if the keyword phrase itself is not as-
signed to any segments.  The value we assign to 
any term must reflect this fact.  If we were to 
reduce cost by translating only the nodes as-
signed to video segments, we would neglect 
nodes that are crucial for browsing.  However, if 
we value a node by the sum value of all its chil-
dren, grandchildren, etc., the resulting calcula-
tion would bias the top of the hierarchy.  Any 
prioritization based on this method would lead to 
translation of the top of the hierarchy first.  
Given limited resources, leaf nodes might never 
be translated.  Support for searching and brows-
ing calls for different approaches to prioritization. 
To strike a balance between these factors, we 
calculate a thesaurus value, which represents the 
importance of each keyword phrase to the the-
saurus as a whole.  This value is computed as: 
( ) ( )kchildren
h
scounth kchildreni ikk
? ?+= )(  
For leaf nodes in our thesaurus, this value is sim-
ply the number of video segments to which the 
concept has been assigned.  For parent nodes, the 
thesaurus value is the number of segments (if 
any) to which the node has been assigned, plus 
the average of the thesaurus value of any child 
nodes. 
This recursive calculation yields a micro-
averaged value that represents the reachability of  
segments via downward edge traversals from a 
given node in the hierarchy.  That is, it gives a 
kind of weighted value for the number of seg-
ments described by a given keyword phrase or its 
narrower-term keyword phrases. 
 
For example, in Figure 2 each of the leaf 
nodes n3, n4, and n5 have values based solely on 
the number of segments to which they are as-
signed. Node n1 has value both as an access point 
to the segments at s2 and as an access point to the 
keyword phrases at nodes n3 and n4.  Other inter-
nal nodes, such as n2 have value only in provid-
ing access to other nodes/keyword phrases. 
Working from the bottom of the hierarchy up to 
the primary node (n0) we can compute the the-
saurus value for each node in the hierarchy.  In 
our example, we start with nodes n3 through n5, 
counting the number of the segments that have 
been assigned each keyword phrase.  Then we 
move up to nodes n1 and n2.  At n1 we count the 
number of segments s2 to which n1 was assigned 
and add that count to the average of the thesau-
rus values for n3, and n4.  At n2 we simply aver-
age the thesaurus values for n4 and n5.  The final 
values quantify how valuable the translation of 
any given keyword phrase would be in providing 
access to video segments. 
Translation value: After obtaining the the-
saurus value for each node, we can compute the 
translation value for each word in the vocabulary 
Figure 2. Bottom-up micro-averaging 
Figure 1. Sample keyword phrase  
with broader and narrower terms 
Auschwitz II-Birkenau (Poland : Death Camp) 
 Assigned to 17555 video segments 
 Has as broader term phrases: 
Cracow (Poland : Voivodship) 
  [ 534 narrower terms] [ 204 segments] 
German death camps 
  [  6 narrower terms] [ 0 segments] 
 Has seven narrower term phrases including: 
Block 25 (Auschwitz II-Birkenau) 
  [leaf node] [ 35 segments]  
Kanada (Auschwitz II-Birkenau) 
  [leaf node] [ 378 segments] 
  ...  
disinfection chamber (Auschwitz II-Birkenau) 
  [leaf node] [ 9 segments]  
primary 
keyword 
segments 
n2 
n4 n3 
n0 
n5 
keyword 
phrases 
s2 
n1 
s1 s3 s4 
947
as the sum of the thesaurus value for every key-
word phrase that contains that word: 
tw= ?
?? wk
kh   where Kw={x | phrase x contains w} 
For example, the word ?Auschwitz? occurs in 35 
concepts.  As a candidate for translation, it car-
ries a large impact, both in terms of the number 
of keyword phrases that contains this word, and 
the potential value of those keyword phrases 
(once they are translated) in providing access to 
segments in the archive.  The end result is a list 
of vocabulary words and the impact that correct 
translation of each word would have on the over-
all value of the translated thesaurus. 
We elicited human translations of entire key-
word phrases rather than individual vocabulary 
terms.  Having humans translate individual 
words without their surrounding context would 
have been less efficient.  Also, the value any 
keyword phrase holds for translation is only indi-
rectly related to its own value as a point of access 
to the collection (i.e., its thesaurus value).  Some 
keyword phrases contain words with high trans-
lation value, but the keyword phrase itself has 
low thesaurus value.  Thus, the value gained by 
translating any given phrase is more accurately 
estimated by the total value of any untranslated 
words it contains. Therefore, we prioritized the 
order of keyword phrase translations based on 
the translation value of the untranslated words in 
each keyword phrase. 
Our next step was to iterate through the the-
saurus keyword phrases, prioritizing their trans-
lation based on the assumption that any words 
contained in a keyword phrase of higher priority 
would already have been translated.  Starting 
from the assumption that the entire thesaurus is 
untranslated, we select the one keyword phrase 
that contains the most valuable un-translated 
words?we simply add up the translation value 
of all the untranslated words in each keyword 
phrase, and select the keyword phrase with the 
highest value.  We add this keyword phrase to a 
prioritized list of items to be manually translated 
and we remove it from the list of untranslated 
phrases.  We update our vocabulary list and, as-
suming translations of all the words in the prior 
keyword phrase to now be translated (neglecting 
issues such as morphology), we again select the 
keyword phrase that contains the most valuable 
untranslated words.  We iterate the process until 
all vocabulary terms have been included at least 
one keyword phrases on the prioritized list.  Ul-
timately we end up with an ordered list of the 
keyword phrases that should be translated to 
cover the entire vocabulary, with the most impor-
tant words being covered first. 
A few words about additional characteristics 
of this approach: note that it is greedy and biased 
toward longer keyword phrases.  As a result, 
some words may be translated more than once 
because they appear in more than one keyword 
phrase with high translation value.  This side 
effect is actually desirable.  To build an accurate 
translation dictionary, it is helpful to have more 
than one translation of frequently occuring words, 
especially for morphologically rich languages 
such as Czech.  Our technique makes the opera-
tional assumption that translations of a word 
gathered in one context can be reused in another 
context.  Obviously this is not always true, but 
contexts of use are relatively stable in controlled 
vocabularies.  Our evaluations address the ac-
ceptability of this operational assumption and 
demonstrate that the technique yields acceptable 
translations. 
Following this process model, the most impor-
tant elements of the thesaurus will be translated 
first, and the most important vocabulary terms 
will quickly become available for automated 
translation of keyword phrases with high thesau-
rus value that do not make it onto the prioritized 
list for manual translation (i.e., low translation 
value).  The overall access value of the thesaurus 
rises very quickly after initial translations.  With 
each subsequent human translation of keyword 
phrases on the prioritized list, we gain tremen-
dous value in terms of providing non-English 
access to the collection of video testimonies.  
Figure 3 shows this rate of gain.  It can be seen 
that prioritization based on translation value 
gives a much higher yield of total access than 
prioritization based on thesaurus value. 
Figure 3. Gain rate of access value based on  
number of human translations 
Gain rate of prioritized translation schemes
0%
20%
40%
60%
80%
100%
0 500 1000 1500 2000
number of translations
pe
rc
en
t o
f t
o
ta
l a
cc
es
s 
v
al
u
e
priority by thesaurus value priority by translation value
948
3.2 Alignment and Decomposition 
Following the prioritization scheme above, we 
obtained professional translations for the top 
3000 English keyword phrases.  We tokenized 
these translations and presented them to another 
bilingual Czech speaker for verification and 
alignment.  This second informant marked each 
Czech word in a translated keyword phrase with 
a link to the equivalent English word(s).  Multi-
ple links were used to convey the relationship 
between a single word in one language and a 
string of words in another.  The output of the 
alignment process was then used to build a prob-
abilistic dictionary of words and phrases. 
 
Figure 4. Sample alignment 
Figure 4 shows an example of an aligned 
tranlsation.  The word ?stills? is recorded as a 
translation for ?statick? sn?mky? and ?kl??tery? 
is recorded as a translation for ?convents and 
monasteries.?  We count the number of occur-
rences of each alignment in all of the translations 
and calculate probabilities for each Czech word 
or phrase given an English word or phrase.  For 
example, in the top 3000 keyword phrases 
?stills? appears 29 times.  It was aligned with 
?statick? sn?mky? 28 times and only once with 
?statick? z?b?ry?, giving us a translation prob-
ability of 28/29=0.9655 for ?statick? sn?mky?. 
Human translation of the 3000 English key-
word phrases into Czech took approximately 70 
hours, and the alignments took 55 hours.  The 
overall cost of human input (translation and 
alignment) was less than 1000 ?.  The projected 
cost of full translation for the entire thesaurus 
would have been close to 20000 ? and would not 
have produced any reusable resources.  Naturally, 
costs for building resources in this manner will 
vary, but in our case the cost savings is approxi-
mately twenty fold. 
3.3 Machine Translation 
To demonstrate the effectiveness of our approach, 
we show that a probabilistic dictionary, induced 
through the process we just described, facilitates 
high quality machine translation of the rest of the 
thesaurus.  We evaluated translation quality us-
ing a relatively simple translation system.  How-
ever, more sophisticated systems can draw equal 
benefit from the same lexical resources. 
Our translation system implemented a greedy 
coverage algorithm with a simple back-off strat-
egy.  It first scans the English input to find the 
longest matching substring in our dictionary, and 
replaces it with the most likely Czech translation.  
Building on the example above, the system looks 
up ?monasteries and convents stills? in the dic-
tionary, finds no translation, and backs off to 
?monasteries and convents?, which is translated 
to ?kl??tery?.  Had this phrase translation not 
been found, the system would have attempted to 
find a match for the individual tokens.  Failing a 
match in our dictionary, the system then backs 
off to the Prague Czech-English Dependency 
Treebank dictionary, a much larger dictionary 
with broader scope.  If no match is found in ei-
ther dictionary for the full token, we stem the 
token and look for matches based on the stem.  
Finally, tokens whose translations can not be 
found are simply passed through untranslated. 
A minimal set of heuristic rules was applied to 
reordering the Czech tokens but the output is 
primarily phrase by phrase/word by word transla-
tion.  Our evaluation scores below will partially 
reflect the simplicity of our system.  Our system 
is simple by design.  Any improvement or degra-
dation to the input of our system has direct influ-
ence on the output.  Thus, measures of transla-
tion accuracy for our system can be directly in-
terpreted as quality measures for the lexical re-
sources used and the process by which they were 
developed. 
4 Evaluation 
We performed two different types of evaluation 
to validate our process.  First, we compared our 
system output to human reference translations 
using Bleu (Papineni, et al, 2002), a widely-
accepted objective metric for evaluation of ma-
chine translations.  Second, we showed corrected 
and uncorrected machine translations to Czech 
speakers and collected subjective judgments of 
fluency and accuracy. 
For evaluation purposes, we selected 418 
keyword phrases to be used as target translations.  
These phrases were selected using a stratified 
sampling technique so that different levels of 
thesaurus value would be represented.  There 
was no overlap between these keyword phrases 
and the 3000 prioritized keyword phrases used to 
build our lexicon.  Prior to machine translation 
we obtained at least two independent human-
generated reference translations for each of the 
418 keyword phrases. 
monasteries convents and stills ( ) 
statick? kl??tery sn?mky ( ) 
949
After collecting the first 2500 prioritized 
translations, we induced a probabilistic diction-
ary and generated machine translations of the 
418 target keyword phrases. These were then 
corrected by native Czech speakers, who ad-
justed word order, word choice, and morphology. 
We use this set of human-corrected machine 
translations as a second reference for evaluation. 
Measuring the difference between our uncor-
rected machine translations (MT) and the human-
generated reference establishes how accurate our 
translations are compared to an independently 
established target.  Measuring the difference be-
tween our MT and the human-corrected machine 
translations (corrected MT) establishes how ac-
ceptable our translations are.  We also measured 
the difference between corrected MT and the 
human-generated translations.  We take this to be 
an upper bound on realistic system performance. 
The results from our objective evaluation are 
shown in Figure 5.  Each set of bars in the graph 
shows performance after adding a different num-
ber of aligned translations into the lexicon (i.e., 
performance after adding 500, 1000, ..., 3000 
aligned translations.)  The zero condition is our 
baseline: translations generated using only the 
dictionary available in the Prague Czech-English 
Dependency Treebank.  Three different reference 
sets are shown: human-generated, corrected MT, 
and a combination of the two. 
There is a notable jump in Bleu score after the 
very first translations are added into our prob-
abilistic dictionary.  Without any elicitation and 
alignment we got a baseline score of 0.46 
(against the human-generated reference transla-
tions).  After the aligned terms from only 500 
translations were added to our dictionary, our 
Bleu score rose to 0.66.  After aligned terms 
from 3000 translations were added, we achieved 
0.69.  Using corrected MT as the reference our 
Bleu scores improve from 0.48 to 0.79.  If hu-
man-generated and human-corrected references 
are both considered to be correct translations, the 
improvement goes from .49 to .80.  Regardless 
of the reference set, there is a consistent per-
formance improvement as more and more trans-
lations are added.  We found the same trend us-
ing the TER metric on a smaller data set 
(Murray, et al, 2006).  The fact that the Bleu 
scores continue to rise indicates that our ap-
proach is successful in quickly expanding the 
lexicon with accurate translations.  It is important 
to point out that Bleu scores are not meaningful 
in an absolute sense; the scores here should be 
interpreted with respect to each other.  The trend 
in scores strongly indicates that our prioritization 
scheme is effective for generating a high-quality 
translation lexicon at relatively low cost.   
To determine an upper bound on machine per-
formance, we compared our corrected MT output 
to the initial human-generated reference transla-
tions, which were collected prior to machine 
translation.  Corrected MT achieved a Bleu score 
of 0.82 when compared to the human-generated 
reference translations.  This upper bound is the 
?limit? indicated in Figure 5. 
To determine the impact of external resources, 
we removed the Prague Czech-English Depend-
ency Treebank dictionary as a back-off resource 
and retranslated keyword phrases using only the 
lexicons induced from our aligned translations.  
The results of this experiment showed only mar-
ginal degradation of the output.  Even when as 
few as 500 aligned translations were used for our 
dictionary, we still achieved a Bleu score of 0.65 
against the human reference translations.  This 
means that even for languages where prior re-
sources are not available our prioritization 
scheme successfully addresses the OOV problem. 
In our subjective evaluation, we presented a 
random sample of our system output to seven 
Distribution of Subjective Judgment Scores
0%
20%
40%
60%
80%
100%
1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
fluency accuracy fluency accuracy
MT Corrected MT
Judgment scores
Pe
rc
en
t o
f s
co
re
s
Bleu Scores After Increasing Translations
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
0 500 1000 1500 2000 2500 3000
Number of Translations
B
le
u
-
4
corrected human reference both limit
Figure 5. Objective evaluation results 
Figure 6. Subjective evaluation results 
950
native Czech speakers and collected judgments 
of accuracy and fluency using a 5-point Likert 
scale (1=good, 3=neutral, 5=bad).  An overview 
of the results is presented in Figure 6.  Scores are 
shown for corrected and uncorrected MT.  In all 
cases, the mode is 1 (i.e., good fluency and good 
accuracy).  59% of the machine translated 
phrases were rated 2 or better for fluency.  66% 
were rated 2 or better for accuracy.  Only a small 
percentage of the translations had meanings that 
were far from the intended meaning.  Disfluen-
cies were primarily due to errors in morphology 
and word order.  
5 Related Work  
Several studies have taken a knowledge-
acquisition approach to collecting multilingual 
word pairs.  For example, Sadat et al (2003) 
automatically extracted bilingual word pairs 
from comparable corpora.  This approach is 
based on the simple assumption that if two words 
are mutual translations, then their most frequent 
collocates are likely to be mutual translations as 
well.  However, the approach requires large com-
parable corpora, the collection of which presents 
non-trivial challenges.  Others have made similar 
mutual-translation assumptions for lexical acqui-
sition (Echizen-ya, et al, 2005; Kaji & Aizono, 
1996; Rapp, 1999; Tanaka & Iwasaki, 1996).  
Most make use of either parallel corpora or a 
bilingual dictionary for the task of bilingual term 
extraction.  Echizen-ya, et al (2005) avoided 
using a bilingual dictionary, but required a paral-
lel corpus to achieve their goal; whereas Fung 
(2000) and others have relied on pre-existing 
bilingual dictionaries.  In either case, large bilin-
gual resources of some kind are required.  In ad-
dition, these approaches focused on the extrac-
tion of single-word pairs, not phrasal units. 
Many recent approaches to dictionary and the-
saurus translation are geared toward providing 
domain-specific thesauri to specialists in a par-
ticular field, e.g., medical terminology (D?jean, 
et al, 2005) and agricultural terminology (Chun 
& Wenlin, 2002).  Researchers on these projects 
are faced with either finding human translators 
who are specialized enough to manage the do-
main-particular translations?or applying auto-
matic techniques to large-scale parallel corpora 
where data sparsity poses a problem for low-
frequency terms.  Data sparsity is also an issue 
for more general state-of-the-art bilingual align-
ment approaches (Brown, et al, 2000; Och & 
Ney, 2003; Wantanabe & Sumita, 2003). 
6 Conclusion 
The task of translating large ontologies can be 
recast as a problem of implementing fast and ef-
ficient processes for acquiring task-specific lexi-
cal resources.  We developed a method for pri-
oritizing keyword phrases from an English the-
saurus of concepts and elicited Czech transla-
tions for a subset of the keyword phrases.  From 
these, we decomposed phrase elements for reuse 
in an English-Czech probabilistic dictionary.  We 
then applied the dictionary in machine translation 
of the rest of the thesaurus.   
Our results show an overall improvement in 
machine translation quality after collecting only 
a few hundred human translations.  Translation 
quality continued to rise as more and more hu-
man translations were added.  The test data used 
in our evaluations are small relative to the overall 
task.  However, we fully expect these results to 
hold across larger samples and for more sophisti-
cated translation systems.   
We leveraged the reusability of translated 
words to translate a thesaurus of 56,000 keyword 
phrases using information gathered from only 
3000 manual translations.  Our probabilistic dic-
tionary was acquired at a fraction of the cost of 
manually translating the entire thesaurus.  By 
prioritizing human translations based on the 
translation value of the words and the thesaurus 
value of the keyword phrases in which they ap-
pear, we optimized the rate of return on invest-
ment. This allowed us to choose a trade-off point 
between cost and utility.  For this project we 
chose to stop human translation at a point where 
less than 0.01% of the value of the thesaurus 
would be gained from each additional human 
translation.  This choice produced a high-quality 
lexicon with significant positive impact on ma-
chine translation systems.  For other applications, 
a different trade-off point will be appropriate, 
depending on the initial OOV rate and the impor-
tance of detailed coverage. 
The value of our work lies in the process 
model we developed for cost-effective elicitation 
of lexical resources.  The metrics we established 
for assessing the impact of each translation item 
are key to our approach.  We use these to opti-
mize the value gained from each human transla-
tion.  In our case the items were keyword phrases 
arranged in a hierarchical thesaurus that de-
scribes an ontology of concepts.  The operational 
value of these keyword phrases was determined 
by the access they provide to video segments in a 
large archive of oral histories.  However, our 
technique is not limited to this application. 
951
We have shown that careful prioritization of 
elicited human translations facilitates cost-
effective thesaurus translation with minimal hu-
man input.  Our use of a prioritization scheme 
addresses the most important deficiencies in the 
vocabulary first.  We induced a framework 
where the utility of lexical resources gained from 
each additional human translation becomes 
smaller and smaller.  Under such a framework, 
choosing the number of human translation to 
elicit becomes merely a function of the financial 
resources available for the task. 
Acknowledgments 
Our thanks to Doug Oard for his contribution to 
this work.  Thanks also to our Czech informants: 
Robert Fischmann, Eliska Kozakova, Alena 
Prunerova and Martin Smok; and to Soumya 
Bhat for her programming efforts. 
This work was supported in part by NSF IIS 
Award 0122466 and NSF CISE RI Award 
EIA0130422.  Additional support also came 
from grants of the MSMT CR #1P05ME786 and 
#MSM0021620838, and the Grant Agency of the 
CR #GA405/06/0589.   
References 
Brown, P. F., Della-Pietra, V. J., Della-Pietra, S. A., 
& Mercer, R. L. (1993). The mathematics of statis-
tical machine translation: Parameter estimation. 
Computational Linguistics, 19(2), 263-311. 
Chun, C., & Wenlin, L. (2002). The translation of 
agricultural multilingual thesaurus. In Proceedings 
of the Third Asian Conference for Information 
Technology in Agriculture. Beijing, China: Chinese 
Academy of Agricultural Sciences (CAAS) and 
Asian Federation for Information Technology in 
Agriculture (AFITA). 
?mejrek, M., Cu??n, J., Havelka, J., Haji?, J., & Ku-
bon, V. (2004). Prague Czech-English dependecy 
treebank: Syntactically annotated resources for ma-
chine translation. In 4th International Conference 
on Language Resources and Evaluation Lisbon, 
Portugal. 
D?jean, H., Gaussier, E., Renders, J.-M., & Sadat, F. 
(2005). Automatic processing of multilingual 
medical terminology: Applications to thesaurus en-
richment and cross-language information retrieval. 
Artificial Intelligence in Medicine, 33(2 ), 111-124. 
Echizen-ya, H., Araki, K., & Momouchi, Y. (2005). 
Automatic acquisition of bilingual rules for extrac-
tion of bilingual word pairs from parallel corpora. 
In Proceedings of the ACL-SIGLEX Workshop on 
Deep Lexical Acquisition (pp. 87-96).  
Fung, P. (2000). A statistical view of bilingual lexicon 
extraction: From parallel corpora to non-parallel 
corpora. In Jean Veronis (ed.), Parallel Text Proc-
essing. Dordrecht: Kluwer Academic Publishers. 
Gustman, Soergel, Oard, Byrne, Picheny, Ramabhad-
ran, & Greenberg. (2002). Supporting access to 
large digital oral history archives.  In Proceedings 
of the Joint Conference on Digital Libraries. Port-
land, Oregon. (pp. 18-27). 
Kaji, H., & Aizono, T. (1996). Extracting word corre-
spondences from bilingual corpora based on word 
co-occurrence information. In Proceedings of 
COLING '96 (pp. 23-28).  
Murray, G. C., Dorr, B., Lin, J., Haji?, J., & Pecina, P. 
(2006).  Leveraging recurrent phrase structure in 
large-scale ontology translation.  In Proceedings of 
the 11th Annual Conference of the European Asso-
ciation for Machine Translation.  Oslo, Norway. 
Nelson, S. J., Schopen, M., Savage, A. G., Schulman, 
J.-L., & Arluk, N. (2004). The MeSH translation 
maintenance system: Structure, interface design, 
and implementation. In Proceedings of the 11th 
World Congress on Medical Informatics. (pp. 67-
69). Amsterdam: IOS Press. 
Och, F. J., & Ney, H. (2003). A systematic compari-
son of various statistical alignment models. Com-
putational Linguistics, 29(1), 19-51. 
Papineni, K., Roukos, S., Ward, T., & Zhu, W.-J. 
(2002). BLEU: A method for automatic evaluation 
of machine translation. In Proceedings of the 40th 
Annual Meeting of the Association for Computa-
tional Linguistics (pp. 331-318). 
Rapp, R. (1999). Automatic identification of word 
translations from unrelated English and German 
corpora. In Proceedings of the 37th Annual Meet-
ing of the Association for Computational Linguis-
tics. (pp. 519-526). 
Sadat, F., Yoshikawa, M., & Uemura, S. (2003). En-
hancing cross-language information retrieval by an 
automatic acquisition of bilingual terminology 
from comparable corpora . In Proceedings of the 
26th Annual International ACM SIGIR Conference 
on Research and Development in Information Re-
trieval (pp. 397-398).  
Tanaka, K., & Iwasaki, H. (1996). Extraction of lexi-
cal translations from non-aligned corpora. In Pro-
ceedings of COLING '96. (pp. 580-585). 
USC. (2006) USC Shoah Foundation Institute for 
Visual History and Education, [online] 
http://www.usc.edu/schools/college/vhi 
Wantanabe, T., & Sumita, E. (2003). Example-based 
decoding for statistical machine translation. In Pro-
ceedings of MT Summit IX (pp. 410-417).  
952
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 523?530,
Sydney, July 2006. c?2006 Association for Computational Linguistics
The Role of Information Retrieval in Answering Complex Questions
Jimmy Lin
College of Information Studies
Department of Computer Science
Institute for Advanced Computer Studies
University of Maryland
College Park, MD 20742, USA
jimmylin@umd.edu
Abstract
This paper explores the role of informa-
tion retrieval in answering ?relationship?
questions, a new class complex informa-
tion needs formally introduced in TREC
2005. Since information retrieval is of-
ten an integral component of many ques-
tion answering strategies, it is important
to understand the impact of different term-
based techniques. Within a framework of
sentence retrieval, we examine three fac-
tors that contribute to question answer-
ing performance: the use of different re-
trieval engines, relevance (both at the doc-
ument and sentence level), and redun-
dancy. Results point out the limitations
of purely term-based methods to this chal-
lenging task. Nevertheless, IR-based tech-
niques provide a strong baseline on top
of which more sophisticated language pro-
cessing techniques can be deployed.
1 Introduction
The field of question answering arose from the
recognition that the document does not occupy a
privileged position in the space of information ob-
jects as the most ideal unit of retrieval. Indeed, for
certain types of information needs, sub-document
segments are preferred?an example is answers to
factoid questions such as ?Who won the Nobel
Prize for literature in 1972?? By leveraging so-
phisticated language processing capabilities, fac-
toid question answering systems are able to pin-
point the exact span of text that directly satisfies
an information need.
Nevertheless, IR engines remain integral com-
ponents of question answering systems, primar-
ily as a source of candidate documents that are
subsequently analyzed in greater detail. Al-
though this two-stage architecture was initially
conceived as an expedient to overcome the com-
putational processing bottleneck associated with
more sophisticated but slower language process-
ing technology, it has worked quite well in prac-
tice. The architecture has since evolved into a
widely-accepted paradigm for building working
systems (Hirschman and Gaizauskas, 2001).
Due to the reliance of QA systems on IR tech-
nology, the relationship between them is an im-
portant area of study. For example, how sensi-
tive is answer extraction performance to the ini-
tial quality of the result set? Does better docu-
ment retrieval necessarily translate into more ac-
curate answer extraction? These answers can-
not be solely determined from first principles,
but must be addressed through empirical experi-
ments. Indeed, a number of works have specifi-
cally examined the effects of information retrieval
on question answering (Monz, 2003; Tellex et al,
2003), including a dedicated workshop at SIGIR
2004 (Gaizauskas et al, 2004). More recently, the
importance of document retrieval has prompted
NIST to introduce a document ranking subtask in-
side the TREC 2005 QA track.
However, the connection between QA and IR
has mostly been explored in the context of factoid
questions such as ?Who shot Abraham Lincoln??,
which represent only a small fraction of all infor-
mation needs. In contrast to factoid questions,
which can be answered by short phrases found
within an individual document, there is a large
class of questions whose answers require synthe-
sis of information from multiple sources. The so-
called definition/other questions at recent TREC
evaluations (Voorhees, 2005) serve as good exam-
ples: ?good answers? to these questions include in-
523
Qid 25: The analyst is interested in the status of Fidel Castro?s brother. Specifically, the analyst would like
information on his current plans and what role he may play after Fidel Castro?s death.
vital Raul Castro was formally designated his brother?s successor
vital Raul is the head of the Armed Forces
okay Raul is five years younger than Castro
okay Raul has enjoyed a more public role in running Cuba?s Government.
okay Raul is the number two man in the government?s ruling Council of State
Figure 1: An example relationship question from TREC 2005 with its answer nuggets.
teresting ?nuggets? about a particular person, or-
ganization, entity, or event. No single document
can provide a complete answer, and hence systems
must integrate information from multiple sources;
cf. (Amigo? et al, 2004; Dang, 2005).
This work focuses on so-called relationship
questions, which represent a new and underex-
plored area in question answering. Although they
require systems to extract information nuggets
frommultiple documents (just like definition/other
questions), relationship questions demand a differ-
ent approach (see Section 2). This paper explores
the role of information retrieval in answering such
questions, focusing primarily on three aspects:
document retrieval performance, term-based mea-
sures of relevance, and term-based approaches to
reducing redundancy. The overall goal is to push
the limits of information retrieval technology and
provide strong baselines against which linguistic
processing capabilities can be compared.
The rest of this paper is organized as follows:
Section 2 provides an overview of relationship
questions. Section 3 describes experiments fo-
cused on document retrieval performance. An ap-
proach to answering relationship questions based
on sentence retrieval is discussed in Section 4. A
simple utility model that incorporates both rele-
vance and redundancy is explored in Section 5.
Before concluding, we discuss the implications of
our experimental results in Section 6.
2 Relationship Questions
Relationship questions represent a new class of in-
formation needs formally introduced as a subtask
in the NIST-sponsored TREC QA evaluations in
2005 (Voorhees, 2005). Previously, they were the
focus of a small pilot study within the AQUAINT
program, which resulted in an understanding of a
?relationship? as the ability for one object to in-
fluence another. Objects in these questions can
denote either entities (people, organization, coun-
tries, etc.) or events. Consider the following ex-
amples:
? Has pressure from China affected America?s
willingness to sell weaponry to Taiwan?
? Do the military personnel exchanges between
Israel and India show an increase in cooper-
ation? If so, what are the driving factors be-
hind this increase?
Evidence for a relationship includes both the
means to influence some entity and the motiva-
tion for doing so. Eight types of relationships
(?spheres of influence?) were noted: financial,
movement of goods, family ties, co-location, com-
mon interest, and temporal connection.
Relationship questions are significantly dif-
ferent from definition questions, which can be
paraphrased as ?Tell me interesting things about
x.? Definition questions have received significant
amounts of attention recently, e.g., (Hildebrandt et
al., 2004; Prager et al, 2004; Xu et al, 2004; Cui
et al, 2005). Research has shown that certain cue
phrases serve as strong indicators for nuggets, and
thus an approach based on matching surface pat-
terns (e.g., appositives, parenthetical expressions)
works quite well. Unfortunately, such techniques
do not generalize to relationship questions because
their answers are not usually captured by patterns
or marked by surface cues.
Unlike answers to factoid questions, answers to
relationship questions consist of an unsorted set
of passages. For assessing system output, NIST
employs the nugget-based evaluation methodol-
ogy originally developed for definition questions;
see (Voorhees, 2005) for a detailed description.
Answers consist of units of information called
?nuggets?, which assessors manually create from
system submissions and their own research (see
example in Figure 1). Nuggets are divided into
524
two types (?vital? and ?okay?), and this distinc-
tion plays an important role in scoring. The offi-
cial metric is an F3-score, where nugget recall is
computed on vital nuggets, and precision is based
on a length allowance derived from the number of
both vital and okay nuggets retrieved.
In the original NIST setup, human assessors
were required to manually determine whether a
particular system?s response contained a nugget.
This posed a problem for researchers who wished
to conduct formative evaluations outside the an-
nual TREC cycle?the necessity of human in-
volvement meant that system responses could
not be rapidly, consistently, and automatically
assessed. However, the recent introduction of
POURPRE, an automatic evaluation metric for the
nugget-based evaluation methodology (Lin and
Demner-Fushman, 2005), fills this evaluation gap
and makes possible the work reported here; cf.
Nuggeteer (Marton and Radul, 2006).
This paper describes experiments with the 25
relationship questions used in the secondary task
of the TREC 2005 QA track (Voorhees, 2005),
which attracted a total of eleven submissions. Sys-
tems used the AQUAINT corpus, a three gigabyte
collection of approximately one million news ar-
ticles from the Associated Press, the New York
Times, and the Xinhua News Agency.
3 Document Retrieval
Since information retrieval systems supply the ini-
tial set of documents on which a question answer-
ing system operates, it makes sense to optimize
document retrieval performance in isolation. The
issue of end?to?end system performance will be
taken up in Section 4.
Retrieval performance can be evaluated based
on the assumption that documents which contain
one or more relevant nuggets (either vital or okay)
are themselves relevant. From system submissions
to TREC 2005, we created a set of relevance judg-
ments, which averaged 8.96 relevant documents
per question (median 7, min 1, max 21).
Our first goal was to examine the effect
of different retrieval systems on performance.
Two freely-available IR engines were compared:
Lucene and Indri. The former is an open-source
implementation of what amounts to be a modified
tf.idf weighting scheme, while the latter employs
a language modeling approach. In addition, we
experimented with blind relevance feedback, a re-
MAP R50
Lucene 0.206 0.469
Lucene+brf 0.190 (?7.6%)? 0.442 (?5.6%)?
Indri 0.195 (?5.2%)? 0.442 (?5.6%)?
Indri+brf 0.158 (?23.3%)O 0.377 (?19.5%)O
Table 1: Document retrieval performance, with
and without blind relevance feedback.
trieval technique commonly employed to improve
performance (Salton and Buckley, 1990). Fol-
lowing settings in typical IR experiments, the top
twenty terms (by tf.idf value) from the top twenty
documents were added to the original query in the
feedback iteration.
For each question, fifty documents from the
AQUAINT collection were retrieved, represent-
ing the number of documents that a typical QA
system might consider. The question itself was
used verbatim as the IR query (see Section 6 for
discussion). Performance is shown in Table 1.
We measured Mean Average Precision (MAP), the
most informative single-point metric for ranked
retrieval, and recall, since it places an upper bound
on the number of relevant documents available for
subsequent downstream processing.
For all experiments reported in this paper, we
applied the Wilcoxon signed-rank test to deter-
mine the statistical significance of the results. This
test is commonly used in information retrieval
research because it makes minimal assumptions
about the underlying distribution of differences.
Significance at the 0.90 level is denoted with a ?
or ?, depending on the direction of change; at the
0.95 level, M or O; at the 0.99 level, N or H. Differ-
ences not statistically significant are marked with
?. Although the differences between Lucene and
Indri are not significant, blind relevance feedback
was found to hurt performance, significantly so in
the case of Indri. These results are consistent with
the findings of Monz (2003), who made the same
observation in the factoid QA task.
There are a few caveats to consider when in-
terpreting these results. First, the test set of 25
questions is rather small. Second, the number of
relevant documents per question is also relatively
small, and hence likely to be incomplete. Buck-
ley and Voorhees (2004) have shown that evalua-
tion metrics are not stable with respect to incom-
plete relevance judgments. Third, the distribution
of relevant documents may be biased due to the
small number of submissions, many of which used
525
Lucene. Due to these factors, one should interpret
the results reported here as suggestive, not defini-
tive. Follow-up experiments with larger data sets
are required to produce more conclusive results.
4 Selecting Relevant Sentences
We adopted an extractive approach to answering
relationship questions that views the task as sen-
tence retrieval, a conception in line with the think-
ing of many researchers today (but see discussion
in Section 6). Although oversimplified, there are
several reasons why this formulation is produc-
tive: since answers consist of unordered text seg-
ments, the task is similar to passage retrieval, a
well-studied problem (Callan, 1994; Tellex et al,
2003) where sentences form a natural unit of re-
trieval. In addition, the TREC novelty tracks have
specifically tackled the questions of relevance and
redundancy at the sentence level (Harman, 2002).
Empirically, a sentence retrieval approach per-
forms quite well: when definition questions
were first introduced in TREC 2003, a simple
sentence-ranking algorithm outperformed all but
the highest-scoring system (Voorhees, 2003). In
addition, viewing the task of answering relation-
ship questions as sentence retrieval allows one
to leverage work in multi-document summariza-
tion, where extractive approaches have been ex-
tensively studied. This section examines the task
of independently selecting the best sentences for
inclusion in an answer; attempts to reduce redun-
dancy will be discussed in the next section.
There are a number of term-based features as-
sociated with a candidate sentence that may con-
tribute to its relevance. In general, such features
can be divided into two types: properties of the
document containing the sentence and properties
of the sentence itself. Regarding the former type,
two features come into play: the relevance score
of the document (from the IR engine) and its rank
in the result set. For sentence-based features, we
experimented with the following:
? Passage match score, which sums the idf val-
ues of unique terms that appear in both the
candidate sentence (S) and the question (Q):
?
t?S?Q
idf(t)
? Term idf precision and recall scores; cf. (Katz
et al, 2005):
P =
?
t?S?Q idf(t)
?
t?A idf(t)
,R =
?
t?S?Q idf(t)
?
t?Q idf(t)
? Length of the sentence (in non-whitespace
characters).
Note that precision and recall values are
bounded between zero and one, while the passage
match score and the length of the sentence are both
unbounded features.
Our baseline sentence retriever employed the
passage match score to rank all sentences in the
top n retrieved documents. By default, we used
documents retrieved by Lucene, using the ques-
tion verbatim as the query. To generate answers,
the system selected sentences based on their scores
until a hard length quota has been filled (trim-
ming the final sentence if necessary). After ex-
perimenting with different values, we discovered
that a document cutoff of ten yielded the highest
performance in terms of POURPRE scores, i.e., all
but the ten top-ranking documents were discarded.
In addition, we built a linear regression model
that employed the above features to predict the
nugget score of a sentence (the dependent vari-
able). For the training samples, the nugget match-
ing component within POURPRE was employed
to compute the nugget score?this value quanti-
fied the ?goodness? of a particular sentence in
terms of nugget content.1 Due to known issues
with the vital/okay distinction (Hildebrandt et al,
2004), it was ignored for this computation; how-
ever, see (Lin and Demner-Fushman, 2006b) for
recent attempts to address this issue.
When presented with a test question, the sys-
tem ranked all sentences from the top ten retrieved
documents using the regression model. Answers
were generated by filling a quota of characters,
just as in the baseline. Once again, no attempt was
made to reduce redundancy.
We conducted a five-fold cross validation ex-
periment using all sentences from the top 100
Lucene documents as training samples. After ex-
perimenting with different features, we discov-
ered that a regression model with the following
performed best: passage match score, document
score, and sentence length. Surprisingly, adding
1Since the count variant of POURPRE achieved the highest
correlation with official rankings, the nugget score is simply
the highest fraction in terms of word overlap between the sen-
tence and any of the reference nuggets.
526
Length 1000 2000 3000 4000 5000
F-Score
baseline 0.275 0.268 0.255 0.234 0.225
regression 0.294 (+7.0%)? 0.268 (+0.0%)? 0.257 (+1.0%)? 0.240 (+2.5%)? 0.228 (+1.6%)?
Recall
baseline 0.282 0.308 0.333 0.336 0.352
regression 0.302 (+7.2%)? 0.308 (+0.0%)? 0.336 (+0.8%)? 0.343 (+2.3%)? 0.358 (+1.7%)?
F-Score (all-vital)
baseline 0.699 0.672 0.632 0.592 0.558
regression 0.722 (+3.3%)? 0.672 (+0.0%)? 0.632 (+0.0%)? 0.593 (+0.2%)? 0.554 (?0.7%)?
Recall (all-vital)
baseline 0.723 0.774 0.816 0.834 0.856
regression 0.747 (+3.3%)? 0.774 (+0.0%)? 0.814 (?0.2%)? 0.834 (+0.0%)? 0.848 (?0.8%)?
Table 2: Question answering performance at different answer length cutoffs, as measured by POURPRE.
Length 1000 2000 3000 4000 5000
F-Score
Lucene 0.275 0.268 0.255 0.234 0.225
Lucene+brf 0.278 (+1.3%)? 0.268 (+0.0%)? 0.251 (?1.6%)? 0.231 (?1.2%)? 0.215 (?4.3%)?
Indri 0.264 (?4.1%)? 0.260 (?2.7%)? 0.241 (?5.4%)? 0.222 (?5.0%)? 0.212 (?5.8%)?
Indri+brf 0.270 (?1.8%)? 0.257 (?3.8%)? 0.235 (?7.8%)? 0.221 (?5.7%)? 0.206 (?8.2%)?
Recall
Lucene 0.282 0.308 0.333 0.336 0.352
Lucene+brf 0.285 (+1.3%)? 0.308 (+0.0%)? 0.319 (?4.2%)? 0.322 (?4.2%)? 0.324 (?7.9%)?
Indri 0.270 (?4.1%)? 0.300 (?2.5%)? 0.306 (?8.2%)? 0.308 (?8.1%)? 0.320 (?9.2%)?
Indri+brf 0.276 (?2.0%)? 0.296 (?3.6%)? 0.299 (?10.4%)? 0.307 (?8.5%)? 0.312 (?11.3%)?
Table 3: The effect of using different document retrieval systems on answer quality.
the term match precision and recall features to the
regression model decreased overall performance
slightly. We believe that precision and recall en-
codes information already captured by the other
features.
Results of our experiments are shown in Ta-
ble 2 for different answer lengths. Following
the TREC QA track convention, all lengths are
measured in non-whitespace characters. Both the
baseline and regression conditions employed the
top ten documents supplied by Lucene. In addi-
tion to the F3-score, we report the recall compo-
nent only (on vital nuggets). For this and all sub-
sequent experiments, we used the (count, macro)
variant of POURPRE, which was validated as pro-
ducing the highest correlation with official rank-
ings. The regression model yields higher scores
at shorter lengths, although none of these differ-
ences were significant. In general, performance
decreases with longer answers because both vari-
ants tend to rank relevant sentences before non-
relevant ones.
Our results compare favorably to runs submit-
ted to the TREC 2005 relationship task. In that
evaluation, the best performing automatic run ob-
tained a POURPRE score of 0.243, with an average
answer length of 4051 character per question.
Since the vital/okay nugget distinction was ig-
nored when training our regression model, we also
evaluated system output under the assumption that
all nuggets were vital. These scores are also shown
in Table 2. Once again, results show higher POUR-
PRE scores for shorter answers, but these differ-
ences are not statistically significant. Why might
this be so? It appears that features based on term
statistics alone are insufficient to capture nugget
relevance. We verified this hypothesis by building
a regression model for all 25 questions: the model
exhibited an R2 value of only 0.207.
How does IR performance affect the final sys-
tem output? To find out, we applied the base-
line sentence retrieval algorithm (which uses the
passage match score only) on the output of differ-
ent document retrieval variants. These results are
shown in Table 3 for the four conditions discussed
in the previous section: Lucene and Indri, with and
without blind relevance feedback.
Just as with the document retrieval results,
Lucene alone (without blind relevance feedback)
yielded the highest POURPRE scores. However,
none of the differences observed were statistically
significant. These numbers point to an interesting
interaction between document retrieval and ques-
tion answering. The decreases in performance at-
527
Length 1000 2000 3000 4000 5000
F-Score
baseline 0.275 0.268 0.255 0.234 0.225
baseline+max 0.311 (+13.2%)? 0.302 (+12.8%)N 0.281 (+10.5%)N 0.256 (+9.5%)M 0.235 (+4.6%)?
baseline+avg 0.301 (+9.6%)? 0.294 (+9.8%)? 0.271 (+6.5%)? 0.256 (+9.5%)M 0.237 (+5.6%)?
regression+max 0.275 (+0.3%)? 0.303 (+13.3%)? 0.275 (+8.1%)? 0.258 (+10.4%)? 0.244 (+8.4%)?
Recall
baseline 0.282 0.308 0.333 0.336 0.352
baseline+max 0.324 (+15.1%)? 0.355 (+15.4%)M 0.369 (+10.6%)M 0.369 (+9.8%)M 0.369 (+4.7%)?
baseline+avg 0.314 (+11.4%)? 0.346 (+12.3%)? 0.354 (+6.2%)? 0.369 (+9.8%)M 0.371 (+5.5%)?
regression+max 0.287 (+2.0%)? 0.357 (+16.1%)? 0.360 (+8.0%)? 0.371 (+10.4%)? 0.379 (+7.6%)?
Table 4: Evaluation of different utility settings.
tributed to blind relevance feedback in end?to?end
QA were in general less than the drops observed
in the document retrieval runs. It appears possi-
ble that the sentence retrieval algorithm was able
to recover from a lower-quality result set, i.e., one
with relevant documents ranked lower. Neverthe-
less, just as with factoid QA, the coupling between
IR and answer extraction merits further study.
5 Reducing Redundancy
The methods described in the previous section
for choosing relevant sentences do not take into
account information that may be conveyed more
than once. Drawing inspiration from research in
sentence-level redundancy within the context of
the TREC novelty track (Allan et al, 2003) and
work in multi-document summarization, we ex-
perimented with term-based approaches to reduc-
ing redundancy.
Instead of selecting sentences for inclusion in
the answer based on relevance alone, we imple-
mented a simple utility model, which takes into
account sentences that have already been added to
the answer A. For each candidate c, utility is de-
fined as follows:
Utility(c) = Relevance(c)? ?max
s?A
sim(s, c)
This model is the baseline variant of the Maxi-
mal Marginal Relevance method for summariza-
tion (Goldstein et al, 2000). Each candidate is
compared to all sentences that have already been
selected for inclusion in the answer. The maxi-
mum of these pairwise similarity comparisons is
deducted from the relevance score of the sentence,
subjected to ?, a parameter that we tune. For our
experiments, we used cosine distance as the simi-
larity function. All relevance scores were normal-
ized to a range between zero and one.
At each step in the answer generation process,
utility values are computed for all candidate sen-
tences. The one with the highest score is selected
for inclusion in the final answer. Utility values are
then recomputed, and the process iterates until the
length quota has been filled.
We experimented with two different sources
for the relevance scores: the baseline sentence re-
triever (passage match score only) and the regres-
sion model. In addition to taking the max of all
pairwise similarity values, as in the above formula,
we also experimented with the average.
Results of our runs are shown in Table 4. We
report values for the baseline relevance score with
the max and avg aggregation functions, as well as
the regression relevance scores with max. These
experimental conditions were compared against
the baseline run that used the relevance score only
(no redundancy penalty). To compute the optimal
?, we swept across the parameter space from zero
to one in increments of a tenth. We determined the
optimal value of ? by averaging POURPRE scores
across all length intervals. For all three conditions,
we discovered 0.4 to be the optimal value.
These experiments suggest that a simple term-
based approach to reducing redundancy yields sta-
tistically significant gains in performance. This
result is not surprising since similar techniques
have proven effective in multi-document summa-
rization. Empirically, we found that the max op-
erator outperforms the avg operator in quantify-
ing the degree of redundancy. The observation
that performance improvements are more notice-
able at shorter answer lengths confirms our intu-
itions. Redundancy is better tolerated in longer
answers because a redundant nugget is less likely
to ?squeeze out? a relevant, novel nugget.
While it is productive to model the relationship
task as sentence retrieval where independent de-
cisions are made about sentence-level relevance,
528
this simplification fails to capture overlap in infor-
mation content, and leads to redundant answers.
We found that a simple term-based approach was
effective in tackling this issue.
6 Discussion
Although this work represents the first formal
study of relationship questions that we are aware
of, by no means are we claiming a solution?we
see this as merely the first step in addressing a
complex problem. Nevertheless, information re-
trieval techniques lay the groundwork for systems
aimed at answering complex questions. The meth-
ods described here will hopefully serve as a start-
ing point for future work.
Relationship questions represent an important
problem because they exemplify complex infor-
mation needs, generally acknowledged as the fu-
ture of QA research. Other types of complex needs
include analytical questions such as ?How close is
Iran to acquiring nuclear weapons??, which are the
focus of the AQUAINT program in the U.S., and
opinion questions such as ?How does the Chilean
government view attempts at having Pinochet tried
in Spanish Court??, which were explored in a 2005
pilot study also funded by AQUAINT. In 2006,
there will be a dedicated task within the TREC
QA track exploring complex questions within an
interactive setting. Furthermore, we note the con-
vergence of the QA and summarization commu-
nities, as demonstrated by the shift from generic
to query-focused summaries starting with DUC
2005 (Dang, 2005). This development is also
compatible with the conception of ?distillation?
in the current DARPA GALE program. All these
trends point to same problem: how do we build
advanced information systems to address complex
information needs?
The value of this work lies in the generality
of IR-based approaches. Sophisticated linguis-
tic processing algorithms are typically unable to
cope with the enormous quantities of text avail-
able. To render analysis more computationally
tractable, researchers commonly employ IR tech-
niques to reduce the amount of text under consid-
eration. We believe that the techniques introduced
in this paper are applicable to the different types
of information needs discussed above.
While information retrieval techniques form a
strong baseline for answering relationship ques-
tions, there are clear limitations of term-based ap-
proaches. Although we certainly did not exper-
iment with every possible method, this work ex-
amined several common IR techniques (e.g., rel-
evance feedback, different term-based features,
etc.). In our regression experiments, we discov-
ered that our feature set was unable to adequately
capture sentence relevance. On the other hand,
simple IR-based techniques appeared to work well
at reducing redundancy, suggesting that determin-
ing content overlap is a simpler problem.
To answer relationship questions well, NLP
technology must take over where IR techniques
leave off. Yet, there are a number of challenges,
the biggest of which is that question classification
and named-entity recognition, which have worked
well for factoid questions, are not applicable to re-
lationship questions, since answer types are diffi-
cult to anticipate. For factoids, there exists a sig-
nificant amount of work on question analysis?the
results of which include important query terms and
the expected answer type (e.g., person, organiza-
tion, etc.). Relationship questions are more diffi-
cult to process: for one, they are often not phrased
as direct wh-questions, but rather as indirect re-
quests for information, statements of doubt, etc.
Furthermore, since these complex questions can-
not be answered by short noun phrases, existing
answer type ontologies are not very useful. For our
experiments, we decided to simply use the ques-
tion verbatim as the query to the IR systems, but
undoubtedly performance can be gained by bet-
ter query formulation strategies. These are diffi-
cult challenges, but recent work on applying se-
mantic models to QA (Narayanan and Harabagiu,
2004; Lin and Demner-Fushman, 2006a) provide
a promising direction.
While our formulation of answering relation-
ship questions as sentence retrieval is produc-
tive, it clearly has limitations. The assumption
that information nuggets do not span sentence
boundaries is false and neglects important work in
anaphora resolution and discourse modeling. The
current setup of the task, where answers consist
of unordered strings, does not place any value on
coherence and readability of the responses, which
will be important if the answers are intended for
human consumption. Clearly, there are ample op-
portunities here for NLP techniques to shine.
The other value of this work lies in its use of an
automatic evaluation metric (POURPRE) for sys-
tem development?the first instance in complex
529
QA that we are aware of. Prior to the introduc-
tion of this automatic scoring technique, studies
such as this were difficult to conduct due to the
necessity of involving humans in the evaluation
process. POURPRE was developed to enable rapid
exploration of the solution space, and experiments
reported here demonstrate its usefulness in doing
just that. Although automatic evaluation metrics
are no stranger to other fields such as machine
translation (e.g., BLEU) and document summa-
rization (e.g., ROUGE, BE, etc.), this represents a
new development in question answering research.
7 Conclusion
Although many findings in this paper are negative,
the conclusions are positive for NLP researchers.
An exploration of a variety of term-based ap-
proaches for answering relationship questions has
demonstrated the impact of different techniques,
but more importantly, this work highlights limita-
tions of purely IR-based methods. With a strong
baseline as a foundation, the door is wide open for
the integration of natural language understanding
techniques.
8 Acknowledgments
This work has been supported in part by DARPA
contract HR0011-06-2-0001 (GALE). I would like
to thank Esther and Kiri for their loving support.
References
J. Allan, C. Wade, and A. Bolivar. 2003. Retrieval
and novelty detection at the sentence level. In SIGIR
2003.
E. Amigo?, J. Gonzalo, V. Peinado, A. Pen?as, and
F. Verdejo. 2004. An empirical study of informa-
tion synthesis task. In ACL 2004.
C. Buckley and E. Voorhees. 2004. Retrieval evalua-
tion with incomplete information. In SIGIR 2004.
J. Callan. 1994. Passage-level evidence in document
retrieval. In SIGIR 1994.
H. Cui, M.-Y. Kan, and T.-S. Chua. 2005. Generic soft
pattern models for definitional question answering.
In SIGIR 2005.
H. Dang. 2005. Overview of DUC 2005. In DUC
2005.
R. Gaizauskas, M. Hepple, and M. Greenwood. 2004.
Proceedings of the SIGIR 2004 Workshop on Infor-
mation Retrieval for Question Answering (IR4QA).
J. Goldstein, V. Mittal, J. Carbonell, and J. Callan.
2000. Creating and evaluating multi-document sen-
tence extract summaries. In CIKM 2000.
D. Harman. 2002. Overview of the TREC 2002 nov-
elty track. In TREC 2002.
W. Hildebrandt, B. Katz, and J. Lin. 2004. Answer-
ing definition questions with multiple knowledge
sources. In HLT/NAACL 2004.
L. Hirschman and R. Gaizauskas. 2001. Natural
language question answering: The view from here.
Natural Language Engineering, 7(4):275?300.
B. Katz, G. Marton, G. Borchardt, A. Brownell,
S. Felshin, D. Loreto, J. Louis-Rosenberg, B. Lu,
F. Mora, S. Stiller, O. Uzuner, and A. Wilcox. 2005.
External knowledge sources for question answering.
In TREC 2005.
J. Lin and D. Demner-Fushman. 2005. Automati-
cally evaluating answers to definition questions. In
HLT/EMNLP 2005.
J. Lin and D. Demner-Fushman. 2006a. The role of
knowledge in conceptual retrieval: A study in the
domain of clinical medicine. In SIGIR 2006.
J. Lin and D. Demner-Fushman. 2006b. Will pyramids
built of nuggets topple over? In HLT/NAACL 2006.
G. Marton and A. Radul. 2006. Nuggeteer: Au-
tomatic nugget-based evaluation using descriptions
and judgements. In HLT/NAACL 2006.
C. Monz. 2003. From Document Retrieval to Question
Answering. Ph.D. thesis, Institute for Logic, Lan-
guage, and Computation, University of Amsterdam.
S. Narayanan and S. Harabagiu. 2004. Question an-
swering based on semantic structures. In COLING
2004.
J. Prager, J. Chu-Carroll, and K. Czuba. 2004. Ques-
tion answering using constraint satisfaction: QA?
by?Dossier?with?Constraints. In ACL 2004.
G. Salton and C. Buckley. 1990. Improving re-
trieval performance by relevance feedback. Jour-
nal of the American Society for Information Science,
41(4):288?297.
S. Tellex, B. Katz, J. Lin, G. Marton, and A. Fernandes.
2003. Quantitative evaluation of passage retrieval
algorithms for question answering. In SIGIR 2003.
E. Voorhees. 2003. Overview of the TREC 2003 ques-
tion answering track. In TREC 2003.
E. Voorhees. 2005. Overview of the TREC 2005 ques-
tion answering track. In TREC 2005.
J. Xu, R. Weischedel, and A. Licuanan. 2004. Evalu-
ation of an extraction-based approach to answering
definition questions. In SIGIR 2004.
530
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 768?775,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Different Structures for Evaluating Answers to Complex Questions:
Pyramids Won?t Topple, and Neither Will Human Assessors
Hoa Trang Dang
Information Access Division
National Institute of Standards and Technology
Gaithersburg, MD 20899
hoa.dang@nist.gov
Jimmy Lin
College of Information Studies
University of Maryland
College Park, MD 20742
jimmylin@umd.edu
Abstract
The idea of ?nugget pyramids? has re-
cently been introduced as a refinement to the
nugget-based methodology used to evaluate
answers to complex questions in the TREC
QA tracks. This paper examines data from
the 2006 evaluation, the first large-scale de-
ployment of the nugget pyramids scheme.
We show that this method of combining
judgments of nugget importance from multi-
ple assessors increases the stability and dis-
criminative power of the evaluation while in-
troducing only a small additional burden in
terms of manual assessment. We also con-
sider an alternative method for combining
assessor opinions, which yields a distinction
similar to micro- and macro-averaging in the
context of classification tasks. While the
two approaches differ in terms of underly-
ing assumptions, their results are neverthe-
less highly correlated.
1 Introduction
The emergence of question answering (QA) systems
for addressing complex information needs has ne-
cessitated the development and refinement of new
methodologies for evaluating and comparing sys-
tems. In the Text REtrieval Conference (TREC) QA
tracks organized by the U.S. National Institute of
Standards and Technology (NIST), improvements in
evaluation processes have kept pace with the evolu-
tion of QA tasks. For the past several years, NIST
has implemented an evaluation methodology based
on the notion of ?information nuggets? to assess an-
swers to complex questions. As it has become the
de facto standard for evaluating such systems, the
research community stands to benefit from a better
understanding of the characteristics of this evalua-
tion methodology.
This paper explores recent refinements to the
nugget-based evaluation methodology developed by
NIST. In particular, we examine the recent so-called
?pyramid extension? that incorporates relevance
judgments from multiple assessors to improve eval-
uation stability (Lin and Demner-Fushman, 2006).
We organize our discussion as follows: The next
section begins by providing a brief overview of
nugget-based evaluations and the pyramid exten-
sion. Section 3 presents results from the first large-
scale implementation of nugget pyramids for QA
evaluation in TREC 2006. Analysis shows that this
extension improves both stability and discriminative
power. In Section 4, we discuss an alternative for
combining multiple judgments that parallels the dis-
tinction between micro- and macro-averaging often
seen in classification tasks. Experiments reveal that
the methods yield almost exactly the same results,
despite operating on different granularities (individ-
ual nuggets vs. individual users).
2 Evaluating Complex Questions
Complex questions are distinguished from factoid
questions such as ?Who shot Abraham Lincoln?? in
that they cannot be answered by named entities (e.g.,
persons, organizations, dates, etc.). Typically, these
information needs are embedded in the context of a
scenario (i.e., user task) and often require systems to
768
synthesize information from multiple documents or
to generate answers that cannot be easily extracted
(e.g., by leveraging inference capabilities).
To date, NIST has already conducted several
large-scale evaluations of complex questions: def-
inition questions in TREC 2003, ?Other? ques-
tions in TREC 2004?2006, ?relationship? questions
in TREC 2005, and the complex, interactive QA
(ciQA) task in TREC 2006. Definition and Other
questions are similar in that they both request novel
facts about ?targets?, which can be persons, orga-
nizations, things, and events. Relationship ques-
tions evolved into the ciQA task and focus on in-
formation needs such as ?What financial relation-
ships exist between South American drug cartels and
banks in Liechtenstein?? Such complex questions
focus on ties (financial, military, familial, etc.) that
connect two or more entities. All of these evalua-
tions have employed the nugget-based methodology,
which demonstrates its versatility and applicability
to a wide range of information needs.
2.1 Basic Setup
In the TREC QA evaluations, an answer to a
complex question consists of an unordered set of
[document-id, answer string] pairs, where the strings
are presumed to provide some relevant information
that addresses the question. Although no explicit
limit is placed on the length of the answer, the final
metric penalizes verbosity (see below).
Evaluation of system output proceeds in two
steps. First, answer strings from all submissions
are gathered together and presented to a single as-
sessor. The source of each answer string is blinded
so that the assessor can not obviously tell which
systems generated what output. Using these an-
swers and searches performed during question de-
velopment, the assessor creates a list of relevant
nuggets. A nugget is a piece of information (i.e.,
?fact?) that addresses one aspect of the user?s ques-
tion. Nuggets should be atomic, in the sense that
an assessor should be able to make a binary de-
cision as to whether the nugget appears in an an-
swer string. Although a nugget represents a con-
ceptual entity, the assessor provides a natural lan-
guage description?primarily as a memory aid for
the subsequent evaluation steps. These descriptions
range from sentence-length document extracts to
r = # of vital nuggets returned
a = # of okay nuggets returned
R = # of vital nuggets in the answer key
l = # of non-whitespace characters in entire run
recall: R = r/R
allowance: ? = 100? (r + a)
precision: P =
{
1 if l < ?
1? l??l otherwise
F (?) = (?
2 + 1)? P ?R
?2 ? P +R
Figure 1: Official definition of F-score for nugget
evaluation in TREC.
key phrases to telegraphic short-hand notes?their
readability greatly varies from assessor to assessor.
The assessor also manually classifies each nugget
as either vital or okay (non-vital). Vital nuggets rep-
resent concepts that must be present in a ?good? an-
swer. Okay nuggets may contain interesting infor-
mation, but are not essential.
In the second step, the same assessor who cre-
ated the nuggets reads each system?s output in turn
and marks the appearance of the nuggets. An an-
swer string contains a nugget if there is a conceptual
match; that is, the match is independent of the partic-
ular wording used in the system?s output. A nugget
match is marked at most once per run?i.e., a sys-
tem is not rewarded for retrieving a nugget multiple
times. If the system?s output contains more than one
match for a nugget, the best match is selected and
the rest are left unmarked. A single [document-id,
answer string] pair in a system response can match
0, 1, or multiple nuggets.
The final F-score for an answer is calculated in the
manner described in Figure 1, and the final score of
a run is the average across the F-scores of all ques-
tions. The metric is a weighted harmonic mean be-
tween nugget precision and nugget recall, where re-
call is heavily favored (controlled by the ? parame-
ter, usually set to three). Nugget recall is calculated
solely on vital nuggets, while nugget precision is ap-
proximated by a length allowance based on the num-
ber of both vital and okay nuggets returned. In an
769
earlier pilot study, researchers discovered that it was
not possible for assessors to consistently enumer-
ate the total set of nuggets contained in an answer,
which corresponds to the denominator in a precision
calculation (Voorhees, 2003). Thus, a penalty for
verbosity serves as a surrogate for precision.
2.2 The Pyramid Extension
The vital/okay distinction has been identified as
a weakness in the TREC nugget-based evalua-
tion methodology (Hildebrandt et al, 2004; Lin
and Demner-Fushman, 2005; Lin and Demner-
Fushman, 2006). There do not appear to be any re-
liable indicators for predicting nugget importance,
which makes it challenging to develop algorithms
sensitive to this consideration. Since only vital
nuggets affect nugget recall, it is difficult for sys-
tems to achieve non-zero scores on topics with few
vital nuggets in the answer key. Thus, scores are
easily affected by assessor errors and other random
variations in evaluation conditions.
One direct consequence is that in previous TREC
evaluations, the median score for many questions
turned out to be zero. A binary distinction on nugget
importance is insufficient to discriminate between
the quality of runs that return no vital nuggets but
different numbers of okay nuggets. Also, a score
distribution heavily skewed towards zero makes
meta-analyses of evaluation stability difficult to per-
form (Voorhees, 2005).
The pyramid extension (Lin and Demner-
Fushman, 2006) was proposed to address the issues
mentioned above. The idea was relatively simple: by
soliciting vital/okay judgments from multiple asses-
sors (after the list of nuggets has been produced by
a primary assessor), it is possible to define nugget
importance with greater granularity. Each nugget is
assigned a weight between zero and one that is pro-
portional to the number of assessors who judged it
to be vital. Nugget recall from Figure 1 can be rede-
fined to incorporate these weights:
R =
?
m?Awm
?
n?V wn
Where A is the set of reference nuggets that are
matched in a system?s output and V is the set of all
reference nuggets; wm and wn are the weights of
nuggets m and n, respectively.1 The calculation of
nugget precision remains the same.
3 Nugget Pyramids in TREC 2006
Lin and Demner-Fushman (2006) present exper-
imental evidence in support of nugget pyramids
by applying the proposal to results from previous
TREC QA evaluations. Their simulation studies ap-
pear to support the assertion that pyramids address
many of the issues raised in Section 2.2. Based on
the results, NIST proceeded with a trial deployment
of nugget pyramids in the TREC 2006 QA track. Al-
though scores based on the binary vital/okay distinc-
tion were retained as the ?official? metric, pyramid
scores were simultaneously computed. This pro-
vided an opportunity to compare the two method-
ologies on a large scale.
3.1 The Data
The basic unit of evaluation for the main QA task
at TREC 2006 was the ?question series?. Each se-
ries focused on a ?target?, which could be a person,
organization, thing, or event. Individual questions
in a series inquired about different facets of the tar-
get, and were explicitly classified as factoid, list, or
Other. One complete series is shown in Figure 2.
The Other questions can be best paraphrased as ?Tell
me interesting things about X that I haven?t already
explicitly asked about.? It was the system?s task to
retrieve interesting nuggets about the target (in the
opinion of the assessor), but credit was not given
for retrieving facts already explicitly asked for in the
factoid and list questions. The Other questions were
evaluated using the nugget-based methodology, and
are the subject of this analysis.
The QA test set in TREC 2006 contained 75 se-
ries. Of the 75 targets, 19 were persons, 19 were
organizations, 19 were events, and 18 were things.
The series contained a total of 75 Other questions
(one per target). Each series contained 6?9 ques-
tions (counting the Other question), with most se-
ries containing 8 questions. The task employed the
AQUAINT collection of newswire text (LDC cat-
alog number LDC2002T31), consisting of English
data drawn from three sources: the New York Times,
1Note that this new scoring model captures the existing
binary vital/okay distinction in a straightforward way: vital
nuggets get a score of one, and okay nuggets zero.
770
147 Britain?s Prince Edward marries
147.1 FACTOID When did Prince Edward engage to marry?
147.2 FACTOID Who did the Prince marry?
147.3 FACTOID Where did they honeymoon?
147.4 FACTOID Where was Edward in line for the throne at the time of the wedding?
147.5 FACTOID What was the Prince?s occupation?
147.6 FACTOID How many people viewed the wedding on television?
147.7 LIST What individuals were at the wedding?
147.8 OTHER
Figure 2: Sample question series from TREC 2006.
Nugget 0 1 2 3 4 5 6 7 8
The couple had a long courtship 1 0 0 0 0 0 1 1 0
Queen Elizabeth II was delighted with the match 0 1 0 1 0 0 0 0 1
Queen named couple Earl and Contessa of Wessex 0 1 0 0 1 1 1 0 0
All marriages of Edward?s siblings ended in divorce 0 0 0 0 0 1 0 0 1
Edward arranged for William to appear more cheerful in photo 0 0 0 0 0 0 0 0 0
they were married in St. Georges Chapel, Windsor 1 1 1 0 1 0 1 1 0
Figure 3: Multiple assessors? judgments of nugget importance for Series 147 (vital=1, okay=0). Assessor 2
was the same as the primary assessor (assessor 0), but judgments were elicited at different times.
the Associated Press, and the Xinhua News Service.
There are approximately one million articles in the
collection, totaling roughly three gigabytes. In to-
tal, 59 runs from 27 participants were submitted to
NIST. For more details, see (Dang et al, 2006).
For the Other questions, nine sets of judgments
were elicited from eight judges (the primary assessor
who originally created the nuggets later annotated
the nuggets once again). Each assessor was asked to
assign the vital/okay label in a rapid fashion, without
giving each decision much thought. Figure 3 gives
an example of the multiple judgments for nuggets in
Series 147. There is variation in notions of impor-
tance not only between different assessors, but also
for a single assessor over time.
3.2 Results
After the human annotation process, nugget pyra-
mids were built in the manner described by Lin and
Demner-Fushman (2006). Two scores were com-
puted for each run submitted to the TREC 2006 main
QA task: one based on the vital/okay judgments of
the primary assessor (which we call the binary F-
score) and one based on the nugget pyramids (the
pyramid F-score). The characteristics of the pyra-
mid method can be inferred by comparing these two
sets of scores.
Figure 4 plots the average binary and average
pyramid F-scores for each run (which represents av-
erage performance across all series). Even though
the nugget pyramid does not represent any single
real user (a point we return to later), pyramid F-
scores do correlate highly with the binary F-scores.
The Pearson?s correlation is 0.987, with a 95% con-
fidence interval of [0.980, 1.00].
While the average F-score for a run is stable given
a sufficient number of questions, the F-score for
a single Other question exhibits greater variability
across assessors. This is shown in Figure 5, which
plots binary and pyramid F-scores for individual
questions from all runs. In this case, the Pearson
correlation is 0.870, with a 95% confidence interval
of [0.863, 1.00].
For 16.4% of all Other questions, the nugget pyra-
mid assigned a non-zero F-score where the origi-
nal binary F-score was zero. This can be seen in
the band of points on the left edge of the plot in
Figure 5. This highlights the strength of nugget
771
0.00
0.05
0.10
0.15
0.20
0.25
0.000.050.100.150.200.25
Aver
age 
binar
y F?
score
Average pyramid F?score
Figure 4: Scatter plot comparing the binary and
pyramid F-scores for each run.
pyramids?their ability to smooth out assessor dif-
ferences and more finely discriminate among sys-
tem outputs. This is a key capability that is useful
for system developers, particularly since algorithmic
improvements are often incremental and small.
Because it is more stable than the single-assessor
method of evaluation, the pyramid method also ap-
pears to have greater discriminative power. We fit
a two-way analysis of variance model with the se-
ries and run as factors, and the binary F-score as
the dependent variable. We found significant differ-
ences between series and between runs (p essentially
equal to 0 for both factors). To determine which runs
were significantly different from each other, we per-
formed a multiple comparison using Tukey?s hon-
estly significant difference criterion and controlling
for the experiment-wise Type I error so that the prob-
ability of declaring a difference between two runs to
be significant, when it is actually not, is at most 5%.
With 59 runs, there are C592 = 1711 different pairs
that can be compared. The single-assessor method
was able to declare one run to be significantly better
than the other in 557 of these pairs. Using the pyra-
mid F-scores, it was possible to find significant dif-
ferences in performance between runs in 617 pairs.
3.3 Discussion
Any evaluation represents a compromise between
effort (which correlates with cost) and insightful-
ness of results. The level of detail and meaning-
0.0
0.2
0.4
0.6
0.8
0.00.20.40.60.8
Bina
ry F?
scor
e
Pyramid F?score
Figure 5: Scatter plot comparing the binary and
pyramid F-scores for each Other question.
fulness of evaluations are constantly in tension with
the availability of resources. Modifications to exist-
ing processes usually come at a cost that needs to be
weighed against potential gains. Based on these con-
siderations, the balance sheet for nugget pyramids
shows a favorable orientation. In the TREC 2006
QA evaluation, soliciting vital/okay judgments from
multiple assessors was not very time-consuming (a
couple of hours per assessor). Analysis confirms
that pyramid scores confer many benefits at an ac-
ceptable cost, thus arguing for its adoption in future
evaluations.
Cost considerations precluded exploring other re-
finements to the nugget-based evaluation methodol-
ogy. One possible alternative would involve ask-
ing multiple assessors to create different sets of
nuggets from scratch. Not only would this be time-
consuming, one would then need to deal with the
additional complexities of aligning each assessor?s
nuggets list. This includes resolving issues such as
nugget granularity, overlap in information content,
implicature and other relations between nuggets, etc.
4 Exploration of Alternative Structures
Despite the demonstrated effectiveness of nugget
pyramids, there are a few potential drawbacks that
are worth discussing. One downside is that the
nugget pyramid does not represent a single assessor.
The nugget weights reflect the aggregation of opin-
ions across a sample population, but there is no guar-
772
antee that the method for computing those weights
actually captures any aspect of real user behavior.
It can be argued that the binary F-score is more re-
alistic since it reflects the opinion of a real user (the
primary assessor), whereas the pyramid F-score tries
to model the opinion of a mythical average user.
Although this point may seem somewhat counter-
intuitive, it represents a well-established tradition
in the information retrieval literature (Voorhees,
2002). In document retrieval, for example, relevance
judgments are provided by a single assessor?even
though it is well known that there are large indi-
vidual differences in notions of relevance. IR re-
searchers believe that human idiosyncrasies are an
inescapable fact present in any system designed for
human users, and hence any attempt to remove those
elements in the evaluation setup is actually undesir-
able. It is the responsibility of researchers to develop
systems that are robust and flexible. This premise,
however, does not mean that IR evaluation results
are unstable or unreliable. Analyses have shown
that despite large variations in human opinions, sys-
tem rankings are remarkably stable (Voorhees, 2000;
Sormunen, 2002)?that is, one can usually be confi-
dent about system comparisons.
The philosophy in IR sharply contrasts with work
in NLP annotation tasks such as parsing, word sense
disambiguation, and semantic role labeling?where
researchers strive for high levels of interannota-
tor agreement, often through elaborate guidelines.
The difference in philosophies arises because unlike
these NLP annotation tasks, where the products are
used primarily by other NLP system components, IR
(and likewise QA) is an end-user task. These sys-
tems are intended for real world use. Since people
differ, systems must be able to accommodate these
differences. Hence, there is a strong preference in
QA for evaluations that maintain a model of the in-
dividual user.
4.1 Micro- vs. Macro-Averaging
The current nugget pyramid method leverages mul-
tiple judgments to define a weight for each individ-
ual nugget, and then incorporates this weight into
the F-score computation. As an alternative, we pro-
pose another method for combining the opinions of
multiple assessors: evaluate system responses indi-
vidually against N sets of binary judgments, and
then compute the mean across those scores. We de-
fine the macro-averaged binary F-score over a set
A = {a1, ..., aN} of N assessors as:
F =
?
a?A Fa
N
Where Fa is the binary F-score according to the
vital/okay judgments of assessor a. The differ-
ences between the pyramid F-score and the macro-
averaged binary F-score correspond to the distinc-
tion between micro- and macro-averaging discussed
in the context of text classification (Lewis, 1991).
In those applications, both measures are mean-
ingful depending on focus: individual instances or
entire classes. In tasks where it is important
to correctly classify individual instances, micro-
averaging is more appropriate. In tasks where it
is important to correctly identify a class, macro-
averaging better quantifies performance. In classi-
fication tasks, imbalance in the prevalence of each
class can lead to large differences in macro- and
micro-averaged scores. Analogizing to our work,
the original formulation of nugget pyramids corre-
sponds to micro-averaging (since we focus on indi-
vidual nuggets), while the alternative corresponds to
macro-averaging (since we focus on the assessor).
We additionally note that the two methods en-
code different assumptions. Macro-averaging as-
sumes that there is nothing intrinsically interesting
about a nugget?it is simply a matter of a particular
user with particular needs finding a particular nugget
to be of interest. Micro-averaging, on the other hand,
assumes that some nuggets are inherently interest-
ing, independent of the particular interests of users.2
Each approach has characteristics that make it
desirable. From the perspective of evaluators, the
macro-averaged binary F-score is preferable be-
cause it models real users; each set of binary judg-
ments represents the information need of a real user,
each binary F-score represents how well an answer
will satisfy a real user, and the macro-averaged bi-
nary F-score represents how well an answer will sat-
isfy, on average, a sample population of real users.
From the perspective of QA system developers, the
micro-averaged nugget pyramid F-score is prefer-
able because it allows finer discrimination in in-
2We are grateful to an anonymous reviewer for this insight.
773
dividual nugget performance, which enables better
techniques for system training and optimization.
The macro-averaged binary F-score has the same
desirable properties as the micro-averaged pyramid
F-score in that fewer responses will have zero F-
scores as compared to the single-assessor binary F-
score. We demonstrate this as follows. Let X be a
response that receives a non-zero pyramid F-score.
Let A = {a1, a2, a3, ..., aN} be the set of N asses-
sors. Then it can be proven that X also receives a
non-zero macro-averaged binary F-score:
1. There exists some nugget v with weight greater
than 0, such that an answer string r in X
matches v. (def. of pyramid recall)
2. There exists some assessor ap ? Awhomarked
v as vital. (def. of pyramid nugget weight)
3. To show that X will also receive a non-zero
macro-averaged binary score, it is sufficient to
show that there is some assessor am ? A such
thatX receives a non-zero F-score when evalu-
ated using just the vital/okay judgments of am.
(def. of macro-averaged binary F-score)
4. But, such an assessor does exist, namely asses-
sor ap: Consider the binary F-score assigned
to X according to just assessor ap. The re-
call of X is greater than zero, since X contains
the response r that matches the nugget v that
was marked as vital by ap (from (2), (1), and
the def. of recall). The precision must also be
greater than zero (def. of precision). Therefore,
the macro-averaged binary F-score ofX is non-
zero. (def. of F-score)
4.2 Analysis from TREC 2006
While the macro-averaged method is guaranteed to
produce no more zero-valued scores than the micro-
averaged pyramid method, it is not guaranteed that
the scores will be the same for any given response.
What are the empirical characteristics of each ap-
proach? To explore this question, we once again ex-
amined data from TREC 2006.
Figure 6 shows a scatter plot of the pyramid F-
score and macro-averaged binary F-score for every
Other questions in all runs from the TREC 2006
QA track main task. Despite focusing on differ-
ent aspects of the evaluation setup, these measures
0.0
0.2
0.4
0.6
0.8
0.00.20.40.60.8
Pyra
mid 
F?sc
ore
Macro?averaged binary F?score
Figure 6: Scatter plot comparing the pyramid and
macro-averaged binary F-scores for all questions.
binary micro macro
binary 1.000/1.000 0.870/0.987 0.861/0.988
micro - 1.000/1.000 0.985/0.996
macro - - 1.000/1.000
Table 1: Pearson?s correlation of F-scores, by ques-
tion and by run.
are highly correlated, even at the level of individ-
ual questions. Table 1 provides a summary of the
correlations between the original binary F-score, the
(micro-averaged) pyramid F-score, and the macro-
averaged binary F-score. Pearson?s r is given for
F-scores at the individual question level (first num-
ber) and at the run level (second number). The cor-
relation between all three variants are about equal at
the level of system runs. At the level of individual
questions, the micro- and macro-averaged F-scores
(using multiple judgments) are still highly correlated
with each other, but each is less correlated with the
single-assessor binary F-score.
4.3 Discussion
The differences between macro- and micro-
averaging methods invoke a more general discus-
sion on notions of nugget importance. There are
actually two different issues we are attempting to
address with our different approaches: the first is
a more granular scale of nugget importance, the
second is variations across a population of users. In
774
the micro-averaged pyramid F-scores, we achieve
the first by leveraging the second, i.e., binary
judgments from a large population are combined
to yield weights for individual nuggets. In the
macro-averaged binary F-score, we focus solely on
population effects without addressing granularity of
nugget importance.
Exploring this thread of argument, we can for-
mulate additional approaches for tackling these is-
sues. We could, for example, solicit more granular
individual judgments on each nugget from each as-
sessor, perhaps on a Likert scale or as a continuous
quantity ranging from zero to one. This would yield
two more methods for computing F-scores, both a
macro-averaged and a micro-averaged variant. The
macro-averaged variant would be especially attrac-
tive because it reflects real users and yet individual
F-scores remain discriminative. Despite its possi-
ble advantages, this extension is rejected based on
resource considerations; making snap binary judg-
ments on individual nuggets is much quicker than a
multi-scaled value assignment?at least at present,
the additional costs are not sufficient to offset the
potential gains.
5 Conclusion
The important role that large-scale evaluations play
in guiding research in human language technologies
means that the community must ?get it right.? This
would ordinarily call for a more conservative ap-
proach to avoid changes that might have unintended
consequences. However, evaluation methodologies
must evolve to reflect the shifting interests of the re-
search community to remain relevant. Thus, orga-
nizers of evaluations must walk a fine line between
progress and chaos. Nevertheless, the introduction
of nugget pyramids in the TREC QA evaluation pro-
vides a case study showing how this fine balance can
indeed be achieved. The addition of multiple judg-
ments of nugget importance yields an evaluation that
is both more stable and more discriminative than the
original single-assessor evaluation, while requiring
only a small additional cost in terms of human labor.
We have explored two different methods for com-
bining judgments from multiple assessors to address
shortcomings in the original nugget-based evalua-
tion setup. Although they make different assump-
tions about the evaluation, results from both ap-
proaches are highly correlated. Thus, we can con-
tinue employing the pyramid-based method, which
is well-suited for developing systems, and still be as-
sured that the results remain consistent with an eval-
uation method that maintains a model of real indi-
vidual users.
Acknowledgments
This work has been supported in part by DARPA
contract HR0011-06-2-0001 (GALE). The second
author would like to thank Kiri and Esther for their
kind support.
References
H. Dang, J. Lin, and D. Kelly. 2006. Overview of the
TREC 2006 question answering track. In Proc. of
TREC 2006.
W. Hildebrandt, B. Katz, and J. Lin. 2004. Answering
definition questions with multiple knowledge sources.
In Proc. HLT/NAACL 2004.
D. Lewis. 1991. Evaluating text categorization. In Proc.
of the Speech and Natural Language Workshop.
J. Lin and D. Demner-Fushman. 2005. Automatically
evaluating answers to definition questions. In Proc. of
HLT/EMNLP 2005.
J. Lin and D. Demner-Fushman. 2006. Will pyramids
built of nuggets topple over? In Proc. of HLT/NAACL
2006.
E. Sormunen. 2002. Liberal relevance criteria of
TREC?counting on negligible documents? In Proc.
of SIGIR 2002.
E. Voorhees. 2000. Variations in relevance judgments
and the measurement of retrieval effectiveness. IP&M,
36(5):697?716.
E. Voorhees. 2002. The philosophy of information re-
trieval evaluation. In Proc. of CLEF Workshop.
E. Voorhees. 2003. Overview of the TREC 2003 ques-
tion answering track. In Proc. of TREC 2003.
E. Voorhees. 2005. Using question series to evaluate
question answering system effectiveness. In Proc. of
HLT/EMNLP 2005.
775
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 265?268,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Pairwise Document Similarity in Large Collections with MapReduce
Tamer Elsayed,?Jimmy Lin,? and Douglas W. Oard?
Human Language Technology Center of Excellence and
UMIACS Laboratory for Computational Linguistics and Information Processing
University of Maryland, College Park, MD 20742
{telsayed,jimmylin,oard}@umd.edu
Abstract
This paper presents a MapReduce algorithm
for computing pairwise document similarity
in large document collections. MapReduce is
an attractive framework because it allows us
to decompose the inner products involved in
computing document similarity into separate
multiplication and summation stages in a way
that is well matched to efficient disk access
patterns across several machines. On a col-
lection consisting of approximately 900,000
newswire articles, our algorithm exhibits lin-
ear growth in running time and space in terms
of the number of documents.
1 Introduction
Computing pairwise similarity on large document
collections is a task common to a variety of prob-
lems such as clustering and cross-document coref-
erence resolution. For example, in the PubMed
search engine,1 which provides access to the life sci-
ences literature, a ?more like this? browsing feature
is implemented as a simple lookup of document-
document similarity scores, computed offline. This
paper considers a large class of similarity functions
that can be expressed as an inner product of term
weight vectors.
For document collections that fit into random-
access memory, the solution is straightforward. As
collection size grows, however, it ultimately be-
comes necessary to resort to disk storage, at which
point aligning computation order with disk access
patterns becomes a challenge. Further growth in the
?Department of Computer Science
?The iSchool, College of Information Studies
1http://www.ncbi.nlm.nih.gov/PubMed
document collection will ultimately make it desir-
able to spread the computation over several proces-
sors, at which point interprocess communication be-
comes a second potential bottleneck for which the
computation order must be optimized. Although
tailored implementations can be designed for spe-
cific parallel processing architectures, the MapRe-
duce framework (Dean and Ghemawat, 2004) offers
an attractive solution to these challenges. In this pa-
per, we describe how pairwise similarity computa-
tion for large collections can be efficiently imple-
mented with MapReduce. We empirically demon-
strate that removing high frequency (and therefore
low entropy) terms results in approximately linear
growth in required disk space and running time with
increasing collection size for collections containing
several hundred thousand documents.
2 MapReduce Framework
MapReduce builds on the observation that many
tasks have the same structure: a computation is ap-
plied over a large number of records (e.g., docu-
ments) to generate partial results, which are then ag-
gregated in some fashion. Naturally, the per-record
computation and aggregation vary by task, but the
basic structure remains fixed. Taking inspiration
from higher-order functions in functional program-
ming, MapReduce provides an abstraction that in-
volves the programmer defining a ?mapper? and a
?reducer?, with the following signatures:
map: (k1, v1)? [(k2, v2)]
reduce: (k2, [v2])? [(k3, v3)]
Key/value pairs form the basic data structure in
MapReduce. The ?mapper? is applied to every input
265
Shu
fflin
g: g
rou
p va
lues
 by 
key
s
ma
p
ma
p
ma
p
ma
p
red
uce
red
uce
red
uce
inp
ut
inp
ut
inp
ut
inp
ut
out
put
out
put
out
put
Figure 1: Illustration of the MapReduce framework: the
?mapper? is applied to all input records, which generates
results that are aggregated by the ?reducer?.
key/value pair to generate an arbitrary number of in-
termediate key/value pairs. The ?reducer? is applied
to all values associated with the same intermediate
key to generate output key/value pairs (see Figure 1).
On top of a distributed file system (Ghemawat
et al, 2003), the runtime transparently handles all
other aspects of execution (e.g., scheduling and fault
tolerance), on clusters ranging from a few to a few
thousand nodes. MapReduce is an attractive frame-
work because it shields the programmer from dis-
tributed processing issues such as synchronization,
data exchange, and load balancing.
3 Pairwise Document Similarity
Our work focuses on a large class of document simi-
larity metrics that can be expressed as an inner prod-
uct of term weights. A document d is represented as
a vector Wd of term weights wt,d, which indicate
the importance of each term t in the document, ig-
noring the relative ordering of terms (?bag of words?
model). We consider symmetric similarity measures
defined as follows:
sim(di, dj) =
?
t?V
wt,di ? wt,dj (1)
where sim(di, dj) is the similarity between docu-
ments di and dj and V is the vocabulary set. In this
type of similarity measure, a term will contribute to
the similarity between two documents only if it has
non-zero weights in both. Therefore, t ? V can be
replaced with t ? di ? dj in equation 1.
Generalizing this to the problem of computing
similarity between all pairs of documents, we note
Algorithm 1 Compute Pairwise Similarity Matrix
1: ?i, j : sim[i, j]? 0
2: for all t ? V do
3: pt ? postings(t)
4: for all di, dj ? pt do
5: sim[i, j]? sim[i, j] + wt,di ? wt,dj
that a term contributes to each pair that contains it.2
For example, if a term appears in documents x, y,
and z, it contributes only to the similarity scores be-
tween (x, y), (x, z), and (y, z). The list of docu-
ments that contain a particular term is exactly what
is contained in the postings of an inverted index.
Thus, by processing all postings, we can compute
the entire pairwise similarity matrix by summing
term contributions.
Algorithm 1 formalizes this idea: postings(t) de-
notes the list of documents that contain term t. For
simplicity, we assume that term weights are also
stored in the postings. For small collections, this al-
gorithm can be run efficiently to compute the entire
similarity matrix in memory. For larger collections,
disk access optimization is needed?which is pro-
vided by the MapReduce runtime, without requiring
explicit coordination.
We propose an efficient solution to the pairwise
document similarity problem, expressed as two sep-
arate MapReduce jobs (illustrated in Figure 2):
1) Indexing: We build a standard inverted in-
dex (Frakes and Baeza-Yates, 1992), where each
term is associated with a list of docid?s for docu-
ments that contain it and the associated term weight.
Mapping over all documents, the mapper, for each
term in the document, emits the term as the key, and
a tuple consisting of the docid and term weight as the
value. The MapReduce runtime automatically han-
dles the grouping of these tuples, which the reducer
then writes out to disk, thus generating the postings.
2) Pairwise Similarity: Mapping over each post-
ing, the mapper generates key tuples corresponding
to pairs of docids in the postings: in total, 12m(m?1)
pairs where m is the posting length. These key tu-
ples are associated with the product of the corre-
sponding term weights?they represent the individ-
2Actually, since we focus on symmetric similarity functions,
we only need to compute half the pairs.
266
d 1
(A,(
d 1,
2))
(B,(
d 1,
1))
(C,(
d 1,
1))
(B,(
d 2,
1))
(D,(
d 2,
2))
(A,(
d 3,
1))
(B,(
d 3,
2))
(E,(
d 3,
1))
(A,[
(d 1,
2),
(d 3,
1)])
(B,[
(d 1,
1), (d 2,
1), 
(d 3,
2)])
(C,[
(d 1,
1)])
(D,[
(d 2,
2)])
(E,[
(d 3,
1)])
d 2 d 3
((d 1
,d 3
),2)
((d 1
,d 2
),1)
((d 1
,d 3
),2)
((d 2
,d 3
),2)
((d 1
,d 2
),[1]
)
((d 1
,d 3
),[2, 2
])
((d 2
,d 3
),[2]
)
((d 1
,d 2
),1)
((d 1
,d 3
),4)
((d 2
,d 3
),2)
?A 
A B
 
C?
?B 
D D
?
?A 
B B
 
E?
ma
p
ma
p
ma
p
re
duc
e
re
duc
e
re
duc
e
ma
p
ma
p
ma
p
shu
ffle
ma
p
ma
p
shu
ffle
Ind
ex
ing
Pa
irw
ise
Sim
ilar
ity
re
duc
e
re
duc
e
re
duc
e
re
duc
e
re
duc
e
(A,[
(d 1,
2),
(d 3,
1)])
(B,[
(d 1,
1), (d 2,
1), 
(d 3,
2)])
(C,[
(d 1,
1)])
(D,[
(d 2,
2)])
(E,[
(d 3,
1)])
Figure 2: Computing pairwise similarity of a toy collection of 3 documents. A simple term weighting scheme (wt,d =
tft,d) is chosen for illustration.
ual term contributions to the final inner product. The
MapReduce runtime sorts the tuples and then the re-
ducer sums all the individual score contributions for
a pair to generate the final similarity score.
4 Experimental Evaluation
In our experiments, we used Hadoop ver-
sion 0.16.0,3 an open-source Java implementation
of MapReduce, running on a cluster with 20 ma-
chines (1 master, 19 slave). Each machine has two
single-core processors (running at either 2.4GHz or
2.8GHz), 4GB memory, and 100GB disk.
We implemented the symmetric variant of Okapi-
BM25 (Olsson and Oard, 2007) as the similarity
function. We used the AQUAINT-2 collection of
newswire text, containing 906k documents, totaling
approximately 2.5 gigabytes. Terms were stemmed.
To test the scalability of our technique, we sampled
the collection into subsets of 10, 20, 25, 50, 67, 75,
80, 90, and 100 percent of the documents.
After stopword removal (using Lucene?s stop-
word list), we implemented a df-cut, where a frac-
tion of the terms with the highest document frequen-
cies is eliminated.4 This has the effect of remov-
ing non-discriminative terms. In our experiments,
we adopt a 99% cut, which means that the most fre-
quent 1% of terms were discarded (9,093 terms out
of a total vocabulary size of 909,326). This tech-
nique greatly increases the efficiency of our algo-
rithm, since the number of tuples emitted by the
3http://hadoop.apache.org/
4In text classification, removal of rare terms is more com-
mon. Here we use df-cut to remove common terms.
R2  = 0.
997
020406080100120140 0
10
20
30
40
50
60
70
80
90
100
Corpu
s Size
 
(%)
Computation Time (minutes)
Figure 3: Running time of pairwise similarity compar-
isons, for subsets of AQUAINT-2.
mappers in the pairwise similarity phase is domi-
nated by the length of the longest posting (in the
worst case, if a term appears in all documents, it
would generate approximately 1012 tuples).
Figure 3 shows the running time of the pairwise
similarity phase for different collection sizes.5 The
computation for the entire collection finishes in ap-
proximately two hours. Empirically, we find that
running time increases linearly with collection size,
which is an extremely desirable property. To get a
sense of the space complexity, we compute the num-
ber of intermediate document pairs that are emit-
ted by the mappers. The space savings are large
(3.7 billion rather than 8.1 trillion intermediate pairs
for the entire collection), and space requirements
grow linearly with collection size over this region
(R2 = 0.9975).
5The entire collection was indexed in about 3.5 minutes.
267
01,0002,0003,0004,0005,0006,0007,0008,0009,000
0
10
20
30
40
50
60
70
80
90
100
Corpu
s Size
 
(%)
Intermediate Pairs (billions)
df-cut
 
at 99%
df-cut
 
at 99.9
%
df-cut
 
at 99.9
9%
df-cut
 
at 99.9
99%
no df-
cut
Figure 4: Effect of changing df -cut thresholds on the
number of intermediate document-pairs emitted, for sub-
sets of AQUAINT-2.
5 Discussion and Future Work
In addition to empirical results, it would be desir-
able to derive an analytical model of our algorithm?s
complexity. Here we present a preliminary sketch of
such an analysis and discuss its implications. The
complexity of our pairwise similarity algorithm is
tied to the number of document pairs that are emit-
ted by the mapper, which equals the total number of
products required in O(N2) inner products, where
N is the collection size. This is equal to:
1
2
?
t?V
dft(dft ? 1) (2)
where dft is the document frequency, or equivalently
the length of the postings for term t. Given that to-
kens in natural language generally obey Zipf?s Law,
and vocabulary size and collection size can be re-
lated via Heap?s Law, it may be possible to develop
a closed form approximation to the above series.
Given the necessity of computing O(N2) inner
products, it may come as a surprise that empirically
our algorithm scales linearly (at least for the collec-
tion sizes we explored). We believe that the key to
this behavior is our df-cut technique, which elimi-
nates the head of the df distribution. In our case,
eliminating the top 1% of terms reduces the number
of document pairs by several orders of magnitude.
However, the impact of this technique on effective-
ness (e.g., in a query-by-example experiment) has
not yet been characterized. Indeed, a df-cut thresh-
old of 99% might seem rather aggressive, removing
meaning-bearing terms such as ?arthritis? and ?Cor-
nell? in addition to perhaps less problematic terms
such as ?sleek? and ?frail.? But redundant use of
related terms is common in news stories, which we
would expect to reduce the adverse effect on many
applications of removing these low entropy terms.
Moreover, as Figure 4 illustrates, relaxing the df-
cut to a 99.9% threshold still results in approxi-
mately linear growth in the requirement for interme-
diate storage (at least over this region).6 In essence,
optimizing the df-cut is an efficiency vs. effective-
ness tradeoff that is best made in the context of a
specific application. Finally, we note that alternative
approaches to similar problems based on locality-
sensitive hashing (Andoni and Indyk, 2008) face
similar tradeoffs in tuning for a particular false pos-
itive rate; cf. (Bayardo et al, 2007).
6 Conclusion
We present a MapReduce algorithm for efficiently
computing pairwise document similarity in large
document collections. In addition to offering spe-
cific benefits for a number of real-world tasks, we
also believe that our work provides an example of
a programming paradigm that could be useful for a
broad range of text analysis problems.
Acknowledgments
This work was supported in part by the Intramural
Research Program of the NIH/NLM/NCBI.
References
A. Andoni and P. Indyk. 2008. Near-optimal hashing
algorithms for approximate nearest neighbor in high
dimensions. CACM, 51(1):117?122.
R. Bayardo, Y. Ma, and R. Srikant. 2007. Scaling up all
pairs similarity search. In WWW ?07.
J. Dean and S. Ghemawat. 2004. MapReduce: Simpli-
fied data processing on large clusters. In OSDI ?04.
W. Frakes and R. Baeza-Yates. 1992. Information Re-
trieval: Data Structures and Algorithms.
S. Ghemawat, H. Gobioff, and S. Leung. 2003. The
Google File System. In SOSP ?03.
J. Olsson and D. Oard. 2007. Improving text classifi-
cation for oral history archives with temporal domain
knowledge. In SIGIR ?07.
6More recent experiments suggest that a df-cut of 99.9% re-
sults in almost no loss of effectiveness on a query-by-example
task, compared to no df-cut.
268
Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation
and/or Summarization, pages 41?48, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Evaluating Summaries and Answers: Two Sides of the Same Coin?
Jimmy Lin1,3 and Dina Demner-Fushman2,3
1College of Information Studies
2Department of Computer Science
3Institute for Advanced Computer Studies
University of Maryland
College Park, MD 20742, USA
jimmylin@umd.edu, demner@cs.umd.edu
Abstract
This paper discusses the convergence
between question answering and multi-
document summarization, pointing out
implications and opportunities for knowl-
edge transfer in both directions. As a
case study in one direction, we discuss
the recent development of an automatic
method for evaluating definition questions
based on n-gram overlap, a commonly-
used technique in summarization evalua-
tion. In the other direction, the move to-
wards topic-oriented summaries requires
an understanding of relevance and topi-
cality, issues which have received atten-
tion in the question answering literature.
It is our opinion that question answering
and multi-document summarization repre-
sent two complementary approaches to the
same problem of satisfying complex user
information needs. Although this points
to many exciting opportunities for system-
building, here we primarily focus on im-
plications for system evaluation.
1 Introduction
Recent developments in question answering (QA)
and multi-document summarization point to many
interesting convergences that present exciting oppor-
tunities for collaboration and cross-fertilization be-
tween these largely independent communities. This
position paper attempts to draw connections be-
tween the task of answering complex natural lan-
guage questions and the task of summarizing mul-
tiple documents, the boundaries between which are
beginning to blur, as anticipated half a decade
ago (Carbonell et al, 2000).
Although the complementary co-evolution of
question answering and document summarization
presents new directions for system-building, this
paper primarily focuses on implications for evalu-
ation. Although assessment of answer and sum-
mary quality employs different methodologies, there
are many lessons that each community can learn
from the other. The summarization community has
extensive experience in intrinsic metrics based on
n-gram overlap for automatically scoring system
outputs against human-generated reference texts?
these techniques would help streamline aspects of
question answering evaluation. In the other direc-
tion, because question answering has its roots in
information retrieval, much work has focused on
extrinsic metrics based on relevance and topical-
ity, which may be valuable to summarization re-
searchers.
This paper is organized as follows: In Section 2,
we discuss the evolution of question answering re-
search and how recent trends point to the conver-
gence of question answering and multi-document
summarization. In Section 3, we present a case
study of automatically evaluating definition ques-
tions by employing metrics based on n-gram over-
lap, a general technique widely used in summariza-
tion and machine translation evaluations. Section 4
highlights some opportunities for knowledge trans-
fer in the other direction: how the notions of rele-
41
vance and topicality, well-studied in the information
retrieval literature, can guide the evaluation of topic-
oriented summaries. We conclude with thoughts
about the future in Section 5.
2 Convergence of QA and Summarization
Question answering was initially conceived as es-
sentially a fine-grained information retrieval task.
Much research has focused on so-called factoid
questions, which can typically be answered by
named entities such as people, organizations, loca-
tions, etc. As an example, a system might return
?Bee Gees? as the answer to the question ?What
band did the music for the 1970?s film ?Saturday
Night Fever???. For such well-specified information
needs, question answering systems represent an im-
provement over traditional document retrieval sys-
tems because they do not require a user to manu-
ally browse through a ranked list of ?hits?. Since
1999, the NIST-organized question answering tracks
at TREC (see, for example, Voorhees 2003a) have
served as a focal point of research in the field, pro-
viding an annual forum for evaluating systems de-
veloped by teams from all over the world. The
model has been duplicated and elaborated on by
CLEF in Europe and NTCIR in Asia, both of which
have also introduced cross-lingual elements.
Recently, research in question answering has
shifted away from factoid questions to more com-
plex information needs. This new direction can be
characterized as a move towards answers that can
only be arrived at through some form of reason-
ing and answers that require drawing information
from multiple sources. Indeed, there are many types
of questions that would require integration of both
capabilities: extracting raw information ?nuggets?
from potentially relevant documents, reasoning over
these basic facts to draw additional inferences, and
synthesizing an appropriate answer based on this
knowledge. ?What is the role of the Libyan gov-
ernment in the Lockerbie bombing?? is an example
of such a complex question.
Commonalities between the task of answering
complex questions and summarizing multiple doc-
uments are evident when one considers broader re-
search trends. Both tasks require the ability to
draw together elements from multiple sources and
cope with redundant, inconsistent, and contradic-
tory information. Both tasks require extracting finer-
grained (i.e., sub-document) segments, albeit based
on different criteria. These observations point to
the convergence of question answering and multi-
document summarization.
Complementary developments in the summariza-
tion community mirror the aforementioned shifts
in question answering research. Most notably, the
DUC 2005 task requires systems to generate an-
swers to natural language questions based on a col-
lection of known relevant documents: ?The system
task in 2005 will be to synthesize from a set of 25?
50 documents a brief, well-organized, fluent answer
to a need for information that cannot be met by just
stating a name, date, quantity, etc.? (DUC 2005
guidelines). These guidelines were modeled after
the information synthesis task suggested by Amigo?
et al (2004), which they characterize as ?the process
of (given a complex information need) extracting,
organizing, and inter-relating the pieces of informa-
tion contained in a set of relevant documents, in or-
der to obtain a comprehensive, non-redundant report
that satisfies the information need?. One of the ex-
amples they provide, ?I?m looking for information
concerning the history of text compression both be-
fore and with computers?, looks remarkably like a
user information need current question answering
systems aspire to satisfy. The idea of topic-oriented
multi-document summarization isn?t new (Goldstein
et al, 2000), but only recently have the connections
to question answering become explicit. Incidentally,
it appears that the current vision of question answer-
ing is more ambitious than the information synthesis
task because in the former, the set of relevant doc-
uments is not known in advance, but must first be
discovered within a larger corpus.
There is, however, an important difference be-
tween question answering and topic-focused multi-
document summarization: whereas summaries are
compressible in length, the same cannot be said of
answers.1 For question answering, it is difficult to
fix the length of a response a priori: there may be
cases where it is impossible to fit a coherent, com-
plete answer into an allotted space. On the other
1Wewould like to thank an anonymous reviewer for pointing
this out.
42
1 vital american composer
2 vital musical achievements ballets symphonies
3 vital born brooklyn ny 1900
4 okay son jewish immigrant
5 okay american communist
6 okay civil rights advocate
7 okay had senile dementia
8 vital established home for composers
9 okay won oscar for ?the Heiress?
10 okay homosexual
11 okay teacher tanglewood music center boston symphony
Table 1: The ?answer key? to the question ?Who is Aaron Copland??
hand, summaries are condensed representations of
content, and should theoretically be expandable and
compressible based on the level of detail desired.
What are the implications, for system evaluations,
of this convergence between question answering and
multi-document summarization? We believe that the
two fields have much to benefit from each other. In
one direction, the question answering community
currently lacks experience in automatically evalu-
ating unstructured answers, which has been the fo-
cus of much research in document summarization.
In the other direction, the question answering com-
munity, due to its roots in information retrieval, has
a good grasp on the notions of relevance and topi-
cality, which are critical to the assessment of topic-
oriented summaries. In the next section, we present
a case study in leveraging summarization evaluation
techniques to automatically evaluate definition ques-
tions. Following that, we discuss how lessons from
question answering (and more broadly, information
retrieval) can be applied to assist in evaluating sum-
marization systems.
3 Definition Questions: A Case Study
Definition questions represent complex information
needs that involve integrating facts from multiple
documents. A typical definition question is ?What
is the Cassini space probe??, to which a system
might respond with answers that include ?interplan-
etary probe to Saturn?, ?carries the Huygens probe
to study the atmosphere of Titan, Saturn?s largest
moon?, and ?a joint project between NASA, ESA,
and ASI?. The goal of the task is to return as
many interesting ?nuggets? of information as possi-
ble about the target entity being defined (the Cassini
space probe, in this case) while minimizing the
amount of irrelevant information retrieved. In the
two formal evaluations of definition questions that
have been conducted at TREC (in 2003 and 2004),
an information nugget is operationalized as a fact for
which an assessor could make a binary decision as to
whether a response contained that nugget (Voorhees,
2003b). Additionally, information nuggets are clas-
sified as either vital or okay. Vital nuggets rep-
resent facts central to the target entity, and should
be present in a ?good? definition. Okay nuggets
contribute worthwhile information about the target,
but are not essential. As an example, assessors?
nuggets for the question ?Who is Aaron Copland??
are shown in Table 1. The distinction between vi-
tal and okay nuggets is consequential for the score
calculation, which we will discuss below.
In the TREC setup, a system response to a defi-
nition question is comprised of an unordered set of
answer strings paired with the identifier of the doc-
ument from which it was extracted. Each of these
answer strings is presumed to have one or more in-
formation nuggets contained within it. Although
there is no explicit limit on the length of each answer
string and the number of answer strings a system is
allowed to return, verbosity is penalized against, as
we shall see below.
To evaluate system output, NIST gathers answer
strings from all participants, hides their association
43
[NYT19990708.0196] Once past a rather routine apprenticeship, which included three years of study
with Nadia Boulanger in Paris, Copland became one of the few American composers to make a living
from composition.
Nugget present: 1
[NYT20000107.0305] A passionate advocate of civil rights, Copland conducted a performance of the
?Lincoln Portrait? with Coretta Scott King as narrator.
Nuggets present: 6
[NYT19991117.0369] after four prior nominations, he won an Oscar in 1949 for his music for ?The
Heiress?
Nugget present: 9
Figure 1: Examples of judging actual system responses.
with the runs that produced them, and presents all
answer strings to a human assessor. Using these re-
sponses and research performed during the original
development of the question (with an off-the-shelf
document retrieval system), the assessor creates an
?answer key?; Table 1 shows the official answer key
for the question ?Who is Aaron Copland??.
After this answer key has been created, NIST as-
sessors then go back over each run and manually
judge whether or not each nugget is present in a par-
ticular system?s response. Figure 1 shows a few ex-
amples of real system output and the nuggets that
were found in them.
The final score of a particular answer is com-
puted as an F-measure, the harmonic mean between
nugget precision and recall. The ? parameter con-
trols the relative importance of precision and recall,
and is heavily biased towards the latter to model the
nature of the task. Nugget recall is calculated solely
as a function of the vital nuggets, which means that
a system receives no ?credit? (in terms of recall) for
returning okay nuggets. Nugget precision is approx-
imated by a length allowance based on the number
of vital and okay nuggets returned; a response longer
than the allowed length is subjected to a verbosity
penalty. Using answer length as a proxy to precision
appears to be a reasonable compromise because a
pilot study demonstrated that it was impossible for
humans to consistently enumerate the total number
of nuggets in a response, a necessary step in calcu-
lating nugget precision (Voorhees, 2003b).
The current TREC setup for evaluating definition
Let
r # of vital nuggets returned in a response
a # of okay nuggets returned in a response
R # of vital nuggets in the answer key
l # of non-whitespace characters in the entire
answer string
Then
recall (R) = r/R
allowance (?) = 100? (r + a)
precision (P) =
{
1 if l < ?
1? l??l otherwise
Finally, F (?) = (?
2 + 1)? P ?R
?2 ? P +R
? = 5 in TREC 2003, ? = 3 in TREC 2004.
Figure 2: Official definition of F-measure.
questions necessitates having a human ?in the loop?.
Even though answer keys are available for questions
from previous years, determining if a nugget was ac-
tually retrieved by a system currently requires hu-
man judgment. Without a fully-automated evalu-
ation method, it is difficult to consistently and re-
producibly assess the performance of a system out-
side the annual TREC cycle. Thus, researchers can-
not carry out controlled laboratory experiments to
rapidly explore the solution space. In many other
fields in computational linguistics, the ability to con-
duct evaluations with quick turnaround has lead to
rapid progress in the state of the art. Question an-
44
swering for definition questions appears to be miss-
ing this critical ingredient.
To address this evaluation gap, we have re-
cently developed POURPRE, a method for automat-
ically evaluating definition questions based on idf-
weighted unigram co-occurrences (Lin and Demner-
Fushman, 2005). This idea of employing n-gram
co-occurrence statistics to score the output of a com-
puter system against one or more desired reference
outputs has its roots in the BLEU metric for ma-
chine translation (Papineni et al, 2002) and the
ROUGE (Lin and Hovy, 2003) metric for summa-
rization. Note that metrics for automatically eval-
uating definitions should be, like metrics for eval-
uating summaries, biased towards recall. Fluency
(i.e., precision) is not usually of concern because
most systems employ extractive techniques to pro-
duce answers. Our study reports good correlation
between the automatically computed POURPRE met-
ric and official TREC system ranks. This measure
will hopefully spur progress in definition question
answering systems.
The development of automatic evaluation metrics
based on n-gram co-occurrence for question answer-
ing is an example of successful knowledge transfer
from summarization to question answering evalua-
tion. We believe that there exist many more op-
portunities for future exploration; as an example,
there are remarkable similarities between informa-
tion nuggets in definition question answering and
recently-proposed methods for assessing summaries
based on fine-grained semantic units (Teufel and van
Halteren, 2004; Nenkova and Passonneau, 2004).
Another promising direction of research in defini-
tion question answering involves applying the Pyra-
mid Method (Nenkova and Passonneau, 2004) to
better model the vital/okay nuggets distinction. As
it currently stands, the vital/okay dichotomy is trou-
blesome because there is no way to operationalize
such a classification scheme within a system; see
Hildebrandt et al (2004) for more discussion. Yet,
the effects on score are significant: a system that re-
turns, for example, all the okay nuggets but none of
the vital nuggets would receive a score of zero. In
truth, the vital/okay distinction is a poor attempt at
modeling the fact that some nuggets about a target
are more important than others?this is exactly what
the Pyramid Method is designed to capture. ?Build-
ing pyramids? for definition questions is an avenue
of research that we are currently pursuing.
In the next section, we discuss opportunities for
knowledge transfer in the other direction; i.e., how
summarization evaluation can benefit from work in
question answering evaluation.
4 Putting the Relevance in Summarization
The definition of a meaningful extrinsic evalua-
tion metric (e.g., a task-based measure) is an issue
that the summarization community has long grap-
pled with (Mani et al, 2002). This issue has been
one of the driving factors towards summaries that
are specifically responsive to complex information
needs. The evaluation of such summaries hinges on
the notions of relevance and topicality, two themes
that have received much research attention in the in-
formation retrieval community, from which question
answering evolved.
Debates about the nature of relevance are al-
most as old as the field of information retrieval it-
self (Cooper, 1971; Saracevic, 1975; Harter, 1992;
Barry and Schamber, 1998; Mizzaro, 1998; Spink
and Greisdorf, 2001). Theoretical discussions aside,
there is evidence suggesting that there exist sub-
stantial inter-assessor differences in document-level
relevance judgments (Voorhees, 2000; Voorhees,
2002); in the TREC ad hoc tracks, for example,
overlap between two humans can be less than 50%.
For factoid question answering, it has also been
shown that the notion of answer correctness is less
well-defined than one would expect (Voorhees and
Tice, 2000; Lin and Katz, 2005 in press). This
inescapable fact about the nature of information
needs represents a fundamental philosophical differ-
ence between research in information retrieval and
computational linguistics. Information retrieval re-
searchers accept the fact that the notion of ?ground
truth? is not particularly meaningful, and any pre-
scriptive attempt to dictate otherwise would result in
brittle and overtrained systems of limited value. A
retrieval system must be sensitive to the inevitable
variations in relevance exhibited by different users.
This philosophy represents a contrast from com-
putational linguistics research, where ground truth
does in fact exist. For example, there is a single cor-
rect parse of a natural language sentence (modulo
45
truly ambiguous sentences), there is the notion of a
correct word sense (modulo granularity issues), etc.
This view also pervades evaluation in machine trans-
lation and document summarization, and is implic-
itly codified in intrinsic metrics, except that there is
now the notion of multiple correct answers (i.e., the
reference texts).
Faced with the inevitability of variations in hu-
mans? notion of relevance, how can information
retrieval researchers confidently draw conclusions
about system performance and the effectiveness of
various techniques? Meta-evaluations have shown
that while some measures such as recall are rela-
tively meaningless in absolute terms (e.g., the to-
tal number of relevant documents cannot be known
without exhaustive assessment of the entire corpus,
which is impractical for current document collec-
tions), relative comparisons between systems are re-
markably stable. That is, if system A performs bet-
ter than system B (by a metric such as mean average
precision, for example), system A is highly likely
to out-perform system B with any alternative sets of
relevance judgments that represent different notions
of relevance (Voorhees, 2000; Voorhees, 2002).
Thus, it remains possible to determine the relative
effectiveness of different retrieval techniques, and
use evaluation results to guide system development.
We believe that this philosophical starting point
for conducting evaluations is an important point that
summarization researchers should take to heart, con-
sidering that notions such as relevance and topicality
are central to the evaluation of the information syn-
thesis task. What concrete implications of this view
are there? We outline some thoughts below:
First, we believe that summarization metrics
should embrace variations in human judgment as an
inescapable part of the evaluation process. Mea-
sures for automatically assessing the quality of a
system?s output such as ROUGE implicitly assume
that the ?best summary? is a statistical agglomera-
tion of the reference summaries, which is not likely
to be true. Until recently, ROUGE ?hard-coded? the
so-called ?jackknifing? procedure to estimate aver-
age human performance. Fortunately, it appears re-
searchers have realized that ?model averaging? may
not be the best way to capture the existence of many
?equally good? summaries. As an example, the
Pyramid Method (Nenkova and Passonneau, 2004),
represents a good first attempt at a realistic model of
human variations.
Second, the view that variations in judgment are
an inescapable part of extrinsic evaluations would
lead one to conclude that low inter-annotator agree-
ment isn?t necessarily bad. Computational linguis-
tics research generally attaches great value to high
kappa measures (Carletta, 1996), which indicate
high human agreement on a particular task. Low
agreement is seen as a barrier to conducting repro-
ducible research and to drawing generalizable con-
clusions. However, this is not necessarily true?low
agreement in information retrieval has not been a
handicap for advancing the state of the art. When
dealing with notions such as relevance, low kappa
values can most likely be attributed to the nature
of the task itself. Attempting to raise agreement
by, for example, developing rigid assessment guide-
lines, may do more harm than good. Prescriptive
attempts to define what a good answer or summary
should be will lead to systems that are not useful
in real-world settings. Instead, we should focus re-
search on adaptable, flexible systems.
Third, meta-evaluations are important. The infor-
mation retrieval literature has an established tradi-
tion of evaluating evaluations post hoc to insure the
reliability and fairness of the results. The aforemen-
tioned studies examining the impact of different rel-
evance judgments are examples of such work. Due
to the variability in human judgments, systems are
essentially aiming at a moving target, which neces-
sitates continual examination as to whether evalu-
ations are accurately answering the research ques-
tions and producing trustworthy results.
Fourth, a measure for assessing the quality of au-
tomatic scoring metrics should reflect the philosoph-
ical starting points that we have been discussing.
As a specific example, the correlation between an
automatically-calculated metric and actual human
preferences is better quantified by Kendall?s ? than
by the coefficient of determination R2. Since rela-
tive system comparisons are more meaningful than
absolute scores, we are generally less interested in
correlations among the scores than in the rankings of
systems produced by those scores. Kendall?s ? com-
putes the ?distance? between two rankings as the
minimum number of pairwise adjacent swaps neces-
sary to convert one ranking into the other. This value
46
is normalized by the number of items being ranked
such that two identical rankings produce a correla-
tion of 1.0; the correlation between a ranking and its
perfect inverse is ?1.0; and the expected correlation
of two rankings chosen at random is 0.0. Typically,
a value of greater than 0.8 is considered ?good?, al-
though 0.9 represents a threshold researchers gener-
ally aim for.
5 Conclusion
What?s in store for the ongoing co-evolution of sum-
marization and question answering? Currently, def-
inition questions exercise a system?s ability to inte-
grate information from multiple documents. In the
process, it needs to automatically recognize similar
information units to avoid redundant information,
much like in multi-document summarization. The
other research direction in advanced question an-
swering, integration of reasoning capabilities to gen-
erate answers that cannot be directly extracted from
text, remains more elusive for a variety of reasons.
Finer-grained linguistic analysis at a large scale and
sufficiently-rich domain ontologies to support po-
tentially long inference chains are necessary pre-
requisites?both of which represent open research
problems. Furthermore, it is unclear how exactly
one would operationalize the evaluation of such ca-
pabilities.
Nevertheless, we believe that advanced reasoning
capabilities based on detailed semantic analyses of
text will receive much attention in the future. The
recent flurry of work on semantic analysis, based
on resources such as FrameNet (Baker et al, 1998)
and PropBank (Kingsbury et al, 2002), provide the
substrate for reasoning engines. Developments in
the automatic construction, adaptation, and merg-
ing of ontologies will supply the knowledge nec-
essary to draw inferences. In order to jump-start
the knowledge acquisition process, we envision the
development of domain-specific question answering
systems, the lessons from which will be applied to
systems that operate on broader domains. In terms
of operationalizing evaluations for these advanced
capabilities, the field has already made important
first steps, e.g., the Pascal Recognising Textual En-
tailment Challenge.
What effect will these developments have on sum-
marization research? We believe that future sys-
tems will employ more detailed linguistic analysis.
As a simple example, the ability to reason about
people?s age based on their birthdates would un-
doubtedly be useful for answering particular types
of questions, but may also play a role in redundancy
detection, for example. In general, we anticipate a
move towards more abstractive techniques in multi-
document summarization. Fluent, cohesive, and top-
ical summaries cannot be generated solely using
an extractive approach?sentences are at the wrong
level of granularity, a source of problems ranging
from dangling anaphoric references to verbose sub-
ordinate clauses. Only through more detailed lin-
guistic analysis can information from multiple doc-
uments be truly synthesized. Already, there are
hybrid approaches to multi-document summariza-
tion that employ natural language generation tech-
niques (McKeown et al, 1999; Elson, 2004), and
researchers have experimented with sentential op-
erations to improve the discourse structure of sum-
maries (Otterbacher et al, 2002).
The primary purpose of this paper was to identify
similarities between multi-document summarization
and complex question answering, pointing out po-
tential synergistic opportunities in the area of system
evaluation. We hope that this is merely a small part
of a sustained dialogue between researchers from
these two largely independent communities. An-
swering complex questions and summarizing mul-
tiple documents are essentially opposite sides of the
same coin, as they represent different approaches to
the common problem of addressing complex user in-
formation needs.
6 Acknowledgements
We would like to thank Donna Harman and Ellen
Voorhees for many insights about the intricacies of
IR evaluation, Bonnie Dorr for introducing us to
DUC and bringing us into the summarization com-
munity, and Kiri for her kind support.
References
Enrique Amigo?, Julio Gonzalo, Victor Peinado, Anselmo
Pen?as, and Felisa Verdejo. 2004. An empirical study
of information synthesis task. In Proceedings of ACL
2004.
47
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet project. In Proceed-
ings of COLING/ACL 1998.
Carol Barry and Linda Schamber. 1998. Users? crite-
ria for relevance evaluation: A cross-situational com-
parison. Information Processing and Management,
34(2/3):219?236.
Jaime Carbonell, Donna Harman, Eduard Hovy, Steve
Maiorano, John Prange, and Karen Sparck-Jones.
2000. Vision statement to guide research in Question
& Answering (Q&A) and Text Summarization.
Jean Carletta. 1996. Assessing agreement on classifi-
cation tasks: The kappa statistic. Computational Lin-
guistics, 22(2):249?254.
William S. Cooper. 1971. A definition of relevance for
information retrieval. Information Storage and Re-
trieval, 7:19?37.
David K. Elson. 2004. Categorization of narrative se-
mantics for use in generative multidocument summa-
rization. In Proceedings of INLG 2004, pages 192?
197.
Jade Goldstein, Vibhu Mittal, Jaime Carbonell, and
Jamie Callan. 2000. Creating and evaluating multi-
document sentence extract summaries. In Proceedings
of CIKM 2000.
Stephen P. Harter. 1992. Psychological relevance and
information science. Journal of the American Society
for Information Science, 43(9):602?615.
Wesley Hildebrandt, Boris Katz, and Jimmy Lin. 2004.
Answering definition questions with multiple knowl-
edge sources. In Proceedings of HLT/NAACL 2004.
Paul Kingsbury, Martha Palmer, and Mitch Marcus.
2002. Adding semantic annotation to the Penn Tree-
Bank. In Proceeding of HLT 2002.
Jimmy Lin and Dina Demner-Fushman. 2005. Au-
tomatically evaluating answers to definition ques-
tions. Technical Report LAMP-TR-119/CS-TR-
4695/UMIACS-TR-2005-04, University of Maryland,
College Park.
Chin-Yew Lin and Eduard Hovy. 2003. Automatic evalu-
ation of summaries using n-gram co-occurrence statis-
tics. In Proceedings of HLT/NAACL 2003.
Jimmy Lin and Boris Katz. 2005, in press. Building a
reusable test collection for question answering. Jour-
nal of the American Society for Information Science
and Technology.
Inderjeet Mani, Therese Firmin, David House, Gary
Klein, Beth Sundheim, and Lynette Hirschman. 2002.
The TIPSTER SUMMAC text summarization evalua-
tion. Natural Language Engineering, 8(1):43?68.
Kathleen R. McKeown, Judith L. Klavans, Vasileios
Hatzivassiloglou, Regina Barzilay, and Eleazar Eskin.
1999. Towards multidocument summarization by re-
formulation: Progress and prospects. In Proceedings
of AAAI-1999.
Stefano Mizzaro. 1998. How many relevances in in-
formation retrieval? Interacting With Computers,
10(3):305?322.
Ani Nenkova and Rebecca Passonneau. 2004. Evaluat-
ing content selection in summarization: The Pyramid
Method. In Proceedings of HLT/NAACL 2004.
Jahna C. Otterbacher, Dragomir R. Radev, and Airong
Luo. 2002. Revisions that improve cohesion in multi-
document summaries: A preliminary study. In Pro-
ceedings of the ACL 2002 Workshop on Automatic
Summarization.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of ACL
2002.
Tefko Saracevic. 1975. Relevance: A review of and a
framework for thinking on the notion in information
science. Journal of the American Society for Informa-
tion Science, 26(6):321?343.
Amanda H. Spink and Howard Greisdorf. 2001. Regions
and levels: Mapping and measuring users relevance
judgments. Journal of the American Society for Infor-
mation Science and Technology, 52(2):161?173.
Simone Teufel and Hans van Halteren. 2004. Evaluating
information content by factoid analysis: Human anno-
tation and stability. In Proceedings of EMNLP 2004.
Ellen M. Voorhees and Dawn M. Tice. 2000. Building a
question answering test collection. In Proceedings of
SIGIR 2000.
Ellen M. Voorhees. 2000. Variations in relevance judg-
ments and the measurement of retrieval effectiveness.
Information Processing and Management, 36(5):697?
716.
Ellen M. Voorhees. 2002. The philosophy of information
retrieval evaluation. In Evaluation of Cross-Language
Information Retrieval Systems, Springer-Verlag LNCS
2406.
Ellen M. Voorhees. 2003a. Evaluating the evaluation: A
case study using the TREC 2002 question answering
track. In Proceedings of HLT/NAACL 2003.
Ellen M. Voorhees. 2003b. Overview of the TREC 2003
question answering track. In Proceedings of TREC
2003.
48
Proceedings of the Workshop on Task-Focused Summarization and Question Answering, pages 24?31,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Situated Question Answering in the Clinical Domain:
Selecting the Best Drug Treatment for Diseases
Dina Demner-Fushman1,3 and Jimmy Lin1,2,3
1Department of Computer Science
2College of Information Studies
3Institute for Advanced Computer Studies
University of Maryland
College Park, MD 20742, USA
demner@cs.umd.edu, jimmylin@umd.edu
Abstract
Unlike open-domain factoid questions,
clinical information needs arise within the
rich context of patient treatment. This en-
vironment establishes a number of con-
straints on the design of systems aimed
at physicians in real-world settings. In
this paper, we describe a clinical ques-
tion answering system that focuses on a
class of commonly-occurring questions:
?What is the best drug treatment for X??,
where X can be any disease. To evalu-
ate our system, we built a test collection
consisting of thirty randomly-selected dis-
eases from an existing secondary source.
Both an automatic and a manual evalua-
tion demonstrate that our system compares
favorably to PubMed, the search system
most commonly-used by physicians today.
1 Introduction
Over the past several years, question answering
(QA) has emerged as a general framework for ad-
dressing users? information needs. Instead of re-
turning ?hits?, as information retrieval systems do,
QA systems respond to natural language questions
with concise, targeted information. Recently, re-
search focus has shifted away from so-called fac-
toid questions such as ?What are pennies made
of?? and ?What country is Aswan High Dam lo-
cated in?? to more complex questions such as
?How have South American drug cartels been us-
ing banks in Liechtenstein to launder money?? and
?What was the Pentagon panel?s position with re-
spect to the dispute over the US Navy training
range on the island of Vieques???so-called ?re-
lationship? and ?opinion? questions, respectively.
These complex information needs differ from
factoid questions in many important ways. Un-
like factoids, they cannot be answered by named-
entities and other short noun phrases. They do not
occur in isolation, but are rather embedded within
a broader context, i.e., a ?scenario?. These com-
plex questions set forth parameters of the desired
knowledge, which may include additional facts
about the motivation of the information seeker,
her assumptions, her current state of knowledge,
etc. Presently, most systems that attempt to tackle
such complex questions are aimed at serving in-
telligence analysts, for activities such as counter-
terrorism and war-fighting.
Systems for addressing complex information
needs are interesting because they provide an op-
portunity to explore the role of semantic struc-
tures in question answering, e.g., (Narayanan and
Harabagiu, 2004). Opportunities include explicit
semantic representations for capturing the con-
tent of questions and documents, deep inferential
mechanisms (Moldovan et al, 2002), and attempts
to model task-specific influences in information-
seeking environments (Freund et al, 2005).
Our own interest in question answering falls
in line with these recent developments, but we
focus on a different type of user?the primary
care physician. The need to answer questions re-
lated to patient care at the point of service has
been well studied and documented (Gorman et
al., 1994; Ely et al, 1999; Ely et al, 2005).
However, research has shown that existing search
systems, e.g., PubMed, are often unable to sup-
ply clinically-relevant answers in a timely man-
ner (Gorman et al, 1994; Chambliss and Conley,
1996). Clinical question answering represents a
high-impact application that has the potential to
improve the quality of medical care.
24
From a research perspective, the clinical do-
main is attractive because substantial medical
knowledge has already been codified in the Uni-
fied Medical Language System (UMLS) (Lind-
berg et al, 1993). This large ontology en-
ables us to explore knowledge-rich techniques and
move beyond question answering methods primar-
ily driven by keyword matching. In this work, we
describe a paradigm of medical practice known as
evidence-based medicine and explain how it can
be computationally captured in a semantic domain
model. Two separate evaluations demonstrate that
semantic modeling yields gains in question an-
swering performance.
2 Considerations for Clinical QA
We begin our exploration of clinical question an-
swering by first discussing design constraints im-
posed by the domain and the information-seeking
environment. The practice of evidence-based
medicine (EBM) provides a well-defined process
model for situating our system. EBM is a widely-
accepted paradigm for medical practice that in-
volves the explicit use of current best evidence,
i.e., high-quality patient-centered clinical research
reported in the primary medical literature, to make
decisions about patient care. As shown by pre-
vious work (De Groote and Dorsch, 2003), cita-
tions from the MEDLINE database maintained by
the National Library of Medicine serve as a good
source of evidence.
Thus, we conceive of clinical question answer-
ing systems as fulfilling a decision-support role
by retrieving highly-relevant MEDLINE abstracts
in response to a clinical question. This repre-
sents a departure from previous systems, which fo-
cus on extracting short text segments from larger
sources. The implications of making potentially
life-altering decisions mean that all evidence must
be carefully examined in context. For example, the
efficacy of a drug in treating a disease is always
framed in the context of a specific study on a sam-
ple population, over a set duration, at some fixed
dosage, etc. The physician simply cannot recom-
mend a particular course of action without consid-
ering all these complex factors. Thus, an ?answer?
without adequate support is not useful. Given that
a MEDLINE abstract?on the order of 250 words,
equivalent to a long paragraph?generally encap-
sulates the context of a clinical study, it serves as a
logical answer unit and an entry point to the infor-
mation necessary to answer the physician?s ques-
tion (e.g., via drill-down to full text articles).
In order for a clinical QA system to be success-
ful, it must be suitably integrated into the daily ac-
tivities of a physician. Within a clinic or a hos-
pital setting, the traditional desktop application is
not the most ideal interface for a retrieval system.
In most cases, decisions about patient care must
be made by the bedside. Thus, a PDA is an ideal
vehicle for delivering question answering capabil-
ities (Hauser et al, 2004). However, the form fac-
tor and small screen size of such devices places
constraints on system design. In particular, since
the physician is unable to view large amounts of
text, precision is of utmost importance.
In summary, this section outlines considerations
for question answering in the clinical domain: the
necessity of contextualized answers, the rationale
for adopting MEDLINE abstract as the response
unit, and the importance of high precision.
3 EBM and Clinical QA
Evidence-based medicine not only supplies a pro-
cess model for situating question answering capa-
bilities, but also provides a framework for codify-
ing the knowledge involved in retrieving answers.
This section describes how the EBM paradigm
provides the basis of the semantic domain model
for our question answering system.
Evidence-based medicine offers three facets of
the clinical domain, that, when taken together,
describe a model for addressing complex clini-
cal information needs. The first facet, shown in
Table 1 (left column), describes the four main
tasks that physicians engage in. The second
facet pertains to the structure of a well-built clin-
ical question. Richardson et al (1995) identify
four key elements, as shown in Table 1 (middle
column). These four elements are often refer-
enced with a mnemonic PICO, which stands for
Patient/Problem, Intervention, Comparison, and
Outcome. Finally, the third facet serves as a tool
for appraising the strength of evidence, i.e., how
much confidence should a physician have in the
results? For this work, we adopted a system with
three levels of recommendations, as shown in Ta-
ble 1 (right column).
By integrating these three perspectives of
evidence-based medicine, we conceptualize clin-
ical question answering as ?semantic unifica-
tion? between information needs expressed in a
25
Clinical Tasks PICO Elements Strength of Evidence
Therapy: Selecting effective treat-
ments for patients, taking into account
other factors such as risk and cost.
Diagnosis: Selecting and interpret-
ing diagnostic tests, while considering
their precision, accuracy, acceptabil-
ity, cost, and safety.
Prognosis: Estimating the patient?s
likely course with time and anticipat-
ing likely complications.
Etiology: Identifying the causes for a
patient?s disease.
Patient/Problem: What is the pri-
mary problem or disease? What are
the characteristics of the patient (e.g.,
age, gender, co-existing conditions,
etc.)?
Intervention: What is the main inter-
vention (e.g., diagnostic test, medica-
tion, therapeutic procedure, etc.)?
Comparison: What is the main in-
tervention compared to (e.g., no inter-
vention, another drug, another thera-
peutic procedure, a placebo, etc.)?
Outcome: What is the effect of the
intervention (e.g., symptoms relieved
or eliminated, cost reduced, etc.)?
A-level evidence is based on con-
sistent, good quality patient-oriented
evidence presented in systematic re-
views, randomized controlled clini-
cal trials, cohort studies, and meta-
analyses.
B-level evidence is inconsistent, lim-
ited quality patient-oriented evidence
in the same types of studies.
C-level evidence is based on disease-
oriented evidence or studies less rigor-
ous than randomized controlled clin-
ical trials, cohort studies, systematic
reviews and meta-analyses.
Table 1: The three facets of evidence-based medicine.
PICO-based knowledge structure and correspond-
ing structures extracted fromMEDLINE abstracts.
Naturally, this matching process should be sensi-
tive to the clinical task and the strength of evidence
of the retrieved abstracts. As conceived, clini-
cal question answering is a knowledge-intensive
endeavor that requires automatic identification of
PICO elements from MEDLINE abstracts.
Ideally, a clinical question answering system
should be capable of directly performing this
semantic match on abstracts, but the size of
the MEDLINE database (over 16 million ci-
tations) makes this approach currently unfeasi-
ble. As an alternative, we rely on PubMed,1
a boolean search engine provided by the Na-
tional Library of Medicine, to retrieve an initial
set of results that we then postprocess in greater
detail?this is the standard two-stage architecture
commonly-employed by many question answer-
ing systems (Hirschman and Gaizauskas, 2001).
The complete architecture of our system is
shown in Figure 1. The query formulation mod-
ule converts the clinical question into a PubMed
search query, identifies the clinical task, and ex-
tracts the appropriate PICO elements. PubMed re-
turns an initial list of MEDLINE citations, which
is analyzed by the knowledge extractor to identify
clinically-relevant elements. These elements serve
as input to the semantic matcher, and are com-
pared to corresponding elements extracted from
the question. Citations are then scored and the top
ranking ones are returned as answers.
1http://www.ncbi.nih.gov/entrez/
Figure 1: Architecture of our clinical question an-
swering system.
Although we have outlined a general framework
for clinical question answering, the space of all
possible patient care questions is immense, and at-
tempts to develop a comprehensive system is be-
yond the scope of this paper. Instead, we focus on
a subset of therapy questions: specifically, ques-
tions of the form ?What is the best drug treatment
for X??, where X can be any disease. We have cho-
sen to tackle this class of questions because studies
of physicians? question-asking behavior in natural
settings have revealed that this question type oc-
curs frequently (Ely et al, 1999). By leveraging
the natural distribution of clinical questions, we
can make the greatest impact with the least amount
26
of development effort. For this class of questions,
we have implemented a working system with the
architecture described in Figure 1. The next three
sections detail each module.
4 Query Formulator
Since our system only handles one question type,
the query formulator is relatively simple: the task
is known in advance to be therapy and the Prob-
lem PICO element is the disease asked about in the
clinical question. In order to facilitate the semantic
matching process, we employMetaMap (Aronson,
2001) to identify the concept in the UMLS ontol-
ogy that corresponds to the disease; UMLS also
provides alternative names and other expansions.
The query formulator also generates a query
to PubMed, the National Library of Medicine?s
boolean search engine for MEDLINE. As an ex-
ample, the following query is issued to retrieve hits
for the disease ?meningitis?:
(Meningitis[mh:noexp]) AND drug therapy[sh]
AND hasabstract[text] AND Clinical Trial[pt]
AND English[Lang] AND humans[mh] AND
(1900[PDAT] : 2003/03[PDAT])
In order to get the best possible set of initial ci-
tations, we employ MeSH (Medical Subject Head-
ings) terms when available. MeSH terms are con-
trolled vocabulary concepts assigned manually by
trained medical librarians in the indexing process
(based on the full text of the article), and encode
a substantial amount of knowledge about the con-
tents of the citation. PubMed allows searches on
MeSH headings, which usually yield highly accu-
rate results. In addition, we limit retrieved cita-
tions to those that have the MeSH heading ?drug
therapy?and those that describe a clinical trial (an-
other metadata field). By default, PubMed orders
citations chronologically in reverse.
5 Knowledge Extractor
The knowledge extraction module provides the
basic frame elements used in the semantic
matching process, described in the next sec-
tion. We employ previously-implemented com-
ponents (Demner-Fushman and Lin, 2005) that
identify PICO elements within a MEDLINE cita-
tion using a combination of knowledge-based and
statistical machine-learning techniques. Of the
four PICO elements prescribed by evidence-based
medicine practitioners, only the Problem and Out-
come elements are relevant for this application
(there are no Interventions and Comparisons for
our question type). The Problem is the main dis-
ease under consideration in an abstract, and out-
comes are statements that assert clinical findings,
e.g., efficacy of a drug or a comparison between
two drugs. The ability to precisely identify these
clinically-relevant elements provides the founda-
tion for semantic question answering capabilities.
6 Semantic Matcher
Evidence-based medicine identifies three differ-
ent sets of factors that must be taken into account
when assessing citation relevance. These consid-
erations are computationally operationalized in the
semantic matcher, which takes as input elements
identified by the knowledge extractor and scores
the relevance of each PubMed citation with re-
spect to the question. After matching, the top-
scoring abstracts are presented to the physician as
answers. The individual score of a citation is com-
prised of three components:
SEBM = SPICO + SSoE + SMeSH (1)
By codifying the principles of evidence-based
medicine, our semantic matcher attempts to sat-
isfy information needs through conceptual analy-
sis, as opposed to simple keyword matching. In
the following subsections, we describe each of
these components in detail.
6.1 PICO Matching
The score of an abstract based on PICO elements,
SPICO, is broken up into two separate scores:
SPICO = Sproblem + Soutcome (2)
The first component in the above equation,
Sproblem, reflects a match between the primary prob-
lem in the query frame and the primary problem
identified in the abstract. A score of 1 is given if
the problems match exactly, based on their unique
UMLS concept id (as provided by MetaMap).
Matching based on concept ids addresses the issue
of terminological variation. Failing an exact match
of concept ids, a partial string match is given a
score of 0.5. If the primary problem in the query
has no overlap with the primary problem from the
abstract, a score of ?1 is given.
The outcome-based score Soutcome is the value as-
signed to the highest-scoring outcome sentence,
27
as determined by the knowledge extractor. Since
the desired outcome (i.e., improve the patient?s
condition) is implicit in the clinical question, our
system only considers the inherent quality of out-
come statements in the abstract. Given a match on
the primary problem, most clinical outcomes are
likely to be of interest to the physician.
For the drug treatment scenario, there is no in-
tervention or comparison, and so these elements
do not contribute to the semantic matching.
6.2 Strength of Evidence
The relevance score of a citation based on the
strength of evidence is calculated as follows:
SSoE = Sjournal + Sstudy + Sdate (3)
Citations published in core and high-impact
journals such as Journal of the American Medical
Association (JAMA) get a score of 0.6 for Sjournal,
and 0 otherwise. In terms of the study type, Sstudy,
clinical trials receive a score of 0.5; observational
studies, 0.3; all non-clinical publications, ?1.5;
and 0 otherwise. The study type is directly en-
coded as metadata in a MEDLINE citation.
Finally, recency factors into the strength of evi-
dence score according to the formula below:
Sdate = (yearpublication ? yearcurrent)/100 (4)
A mild penalty decreases the score of a citation
proportionally to the time difference between the
date of the search and the date of publication.
6.3 MeSH Matching
The final component of the EBM score reflects
task-specific considerations, and is computed from
MeSH terms associated with each citation:
SMeSH =
?
t?MeSH
?(t) (5)
The function ?(t) maps MeSH terms to positive
scores for positive indicators, negative scores for
negative indicators, or zero otherwise.
Negative indicators include MeSH headings as-
sociated with genomics, such as ?genetics? and
?cell physiology?. Positive indicators for therapy
were derived from the clinical query filters used in
PubMed searches (Haynes et al, 1994); examples
include ?drug administration routes? and any of its
children in the MeSH hierarchy. A score of ?1 is
given if theMeSH descriptor or qualifier is marked
as the main theme of the article (indicated via the
star notation by indexers), and ?0.5 otherwise.
7 Evaluation Methodology
Clinical Evidence (CE) is a periodic report cre-
ated by the British Medical Journal (BMJ) Pub-
lishing Group that summarizes the best treatments
for a few dozen diseases at the time of publica-
tion. We were able to mine the June 2004 edition
to create a test collection to evaluate our system.
Note that the existence of such secondary sources
does not obviate the need for clinical question an-
swering because they are perpetually falling out of
date due to rapid advances in medicine. Further-
more, such reports are currently created by highly-
experienced physicians, which is an expensive and
time-consuming process. From CE, we randomly
extracted thirty diseases, creating a development
set of five questions and a test set of twenty-five
questions. Some examples include: acute asthma,
chronic prostatitis, community acquired pneumo-
nia, and erectile dysfunction.
We conducted two evaluations?one auto-
matic and one manual?that compare the origi-
nal PubMed hits and the output of our semantic
matcher. The first evaluation is based on ROUGE,
a commonly-used summarization metric that com-
putes the unigram overlap between a particular
text and one or more reference texts.2 The treat-
ment overview for each disease in CE is accompa-
nied by a number of citations (used in writing the
overview itself)?the abstract texts of these cited
articles serve as our references. We adopt this ap-
proach because medical journals require abstracts
that provide factual information summarizing the
main points of the studies. We assume that the
closer an abstract is to these reference abstracts (as
measured by ROUGE-1 precision), the more rele-
vant it is. On average, each disease overview con-
tains 48.4 citations; however, we were only able
to gather abstracts of those that were contained in
MEDLINE (34.7 citations per disease, min 8, max
100). For evaluation purposes, we restricted ab-
stracts under consideration to those that were pub-
lished before our edition of CE. To quantify the
performance of our system, we computed the av-
erage ROUGE score over the top one, three, five,
and ten hits of our EBM and baseline systems.
To supplement our automatic evaluation, we
also conducted a double-blind manual evaluation
2We ran ROUGE-1.5.5 with DUC 2005 settings.
28
PubMed EBM PICO SoE MeSH
1 0.160 0.205 (+27.7%)M 0.186 (+16.1%)? 0.192 (+20.0%)? 0.166 (+3.6%)?
3 0.162 0.202 (+24.6%)N 0.192 (+18.0%)N 0.204 (+25.5%)N 0.172 (+6.1%)?
5 0.166 0.198 (+19.5%)N 0.196 (+18.0%)N 0.201 (+21.3%)N 0.168 (+1.2%)?
10 0.170 0.196 (+15.5%)N 0.191 (+12.5%)N 0.195 (+15.1%)N 0.174 (+2.8%)?
Table 2: Results of automatic evaluation: average ROUGE score using cited abstracts in CE as references.
The EBM column represents performance of our complete domain model. PICO, SoE, and MeSH rep-
resent performance of each component. (? denotes n.s., M denotes sig. at 0.95, N denotes sig. at 0.99)
PubMed results EBM-reranked results
Effect of vitamin A supplementation on childhood morbid-
ity and mortality.
Intrathecal chemotherapy in carcinomatous meningitis from
breast cancer.
Isolated leptomeningeal carcinomatosis (carcinomatous
meningitis) after taxane-induced major remission in patients
with advanced breast cancer.
A comparison of ceftriaxone and cefuroxime for the treat-
ment of bacterial meningitis in children.
Randomised comparison of chloramphenicol, ampicillin,
cefotaxime, and ceftriaxone for childhood bacterial menin-
gitis.
The beneficial effects of early dexamethasone administra-
tion in infants and children with bacterial meningitis.
Table 3: Titles of the top abstracts retrieved in response to the question ?What is the best treatment for
meningitis??, before and after applying our semantic reranking algorithm.
of the system. The top five citations from both
the original PubMed results and the output of our
semantic matcher were gathered, blinded, and ran-
domized (see Table 3 for an example of top results
obtained by PubMed and our system). The first
author of this paper, who is a medical doctor, man-
ually evaluated the abstracts. Since the sources of
the abstracts were hidden, judgments were guar-
anteed to be impartial. All abstracts were evalu-
ated on a four point scale: not relevant, marginally
relevant, relevant, and highly relevant, which cor-
responds to a score of zero to three.
8 Results
The results of our automatic evaluation are shown
in Table 2: the rows show average ROUGE scores
at one, three, five, and ten hits, respectively. In
addition to the PubMed baseline and our com-
plete EBM model, we conducted a component-
level analysis of our semantic matching algorithm.
Three separate ablation studies isolate the effects
of the PICO-based score, the strength of evi-
dence score, and the MeSH-based score (columns
?PICO?, ?SoE?, and ?MeSH?).
At all document cutoffs, the quality of the
EBM-reranked hits is higher than that of the origi-
nal PubMed hits, as measured by ROUGE. The dif-
ferences are statistically significant, according to
the Wilcoxon signed-rank test, the standard non-
parametric test employed in IR.
Based on the component analysis, we can see
that the strength of evidence score is responsi-
ble for the largest performance gain, although
the combination of all three components outper-
forms each one individually (for the most part).
All three components of our semantic model con-
tribute to the overall QA performance, which is
expected because clinical relevance is a multi-
faceted property that requires a multitude of con-
siderations. Evidence-based medicine provides a
theory of these factors, and we have shown that a
question answering algorithm which operational-
izes EBM yields good results.
The distribution of human judgments from our
manual evaluation is shown in Figure 2. For
the development set, the average human judg-
ment of the original PubMed hits is 1.52 (be-
tween ?marginally relevant? and ?relevant?); after
semantic matching, 2.32 (better than ?relevant?).
For the test set, the averages are 1.49 before rank-
ing and 2.10 after semantic matching. These re-
sults show that our system performs significantly
better than the PubMed baseline.
The performance improvement observed in our
experiments is encouraging, considering that we
were starting off with a strong state-of-the-art
29
Figure 2: Results of our manual evaluation: distribution of judgments, for development set (left) and test
set (right). (0=not relevant, 1=marginally relevant, 2=relevant, 3=highly relevant)
PubMed baseline that leverages MeSH terms. All
initial citations retrieved by PubMed were clinical
trials and ?about? the disease in question, as deter-
mined by human indexers. Our work demonstrates
that principles of evidence-based medicine can be
codified in an algorithm.
Since a number of abstracts were both auto-
matically evaluated with ROUGE and manually
assessed, it is possible to determine the degree
to which automatic metrics predict human judg-
ments. For the 125 human judgments gathered
on the test set, we computed a Pearson?s r score
of 0.544, which indicates moderate predictiveness.
Due to the structure of our PubMed query, the key-
word content of retrieved abstracts are relatively
homogeneous. Nevertheless, automatic evaluation
with ROUGE appears to be useful.
9 Discussion and Related Work
Recently, researchers have become interested
in restricted-domain question answering because
it provides an opportunity to explore the use
of knowledge-rich techniques without having
to tackle the commonsense reasoning problem.
Knowledge-based techniques dependent on rich
semantic representations contrast with TREC-
style factoid question answering, which is primar-
ily driven by keyword matching and named-entity
detection.
Our work represents a successful case study of
how semantic models can be employed to capture
domain knowledge (the practice of medicine, in
our case). The conception of question answer-
ing as the matching of knowledge frames provides
us with an opportunity to experiment with seman-
tic representations that capture the content of both
documents and information needs. In our case,
PICO-based scores were found to have a positive
impact on performance. The strength of evidence
and the MeSH-based scores represent attempts to
model user requirements by leveraging meta-level
information not directly present in either questions
or candidate answers. Both contribute positively
to performance. Overall, the construction of our
semantic model is enabled by the UMLS ontol-
ogy, which provides an enumeration of relevant
concepts (e.g., the names of diseases, drugs, etc.)
and semantic relations between those concepts.
Question answering in the clinical domain is an
emerging area of research that has only recently
begun to receive serious attention. As a result,
there exist relatively few points of comparison to
our own work, as the research space is sparsely
populated.
The idea that information systems should
be sensitive to the practice of evidence-based
medicine is not new. Many researchers have stud-
ied MeSH terms associated with basic clinical
tasks (Mendonc?a and Cimino, 2001; Haynes et al,
1994). Although originally developed as a tool to
assist in query formulation, Booth (2000) pointed
out that PICO frames can be employed to struc-
ture IR results for improving precision; PICO-
based querying is merely an instance of faceted
querying, which has been widely used by librari-
ans since the invention of automated retrieval sys-
tems. The feasibility of automatically identifying
outcome statements in secondary sources has been
demonstrated by Niu and Hirst (2004), but our
work differs in its focus on the primary medical lit-
erature. Approaching clinical needs from a differ-
ent perspective, the PERSIVAL system leverages
patient records to rerank search results (McKeown
et al, 2003). Since the primary focus is on person-
30
alization, this work can be viewed as complemen-
tary to our own.
The dearth of related work and the lack of a pre-
existing clinical test collection to a large extent ex-
plains the ad hoc nature of some aspects of our
semantic matching algorithm. All weights were
heuristically chosen to reflect our understanding
of the domain, and were not optimized in a prin-
cipled manner. Nevertheless, performance gains
observed in the development set carried over to
the blind held-out test collection, providing con-
fidence in the generality of our methods. Devel-
oping a more formal scoring model for evidence-
based medicine will be the subject of future work.
10 Conclusion
We see this work as having two separate contribu-
tions. From the viewpoint of computational lin-
guistics, we have demonstrated the effectiveness
of a knowledge-rich approach to QA based on
matching questions with answers at the semantic
level. From the viewpoint of medical informat-
ics, we have shown how principles of evidence-
based medicine can be operationalized in a sys-
tem to support physicians. We hope that this work
paves the way for future high-impact applications.
11 Acknowledgments
This work was supported in part by the National
Library of Medicine. The second author wishes to
thank Esther and Kiri for their loving support.
References
A. Aronson. 2001. Effective mapping of biomedi-
cal text to the UMLS Metathesaurus: The MetaMap
program. In Proceeding of the AMIA 2001.
A. Booth. 2000. Formulating the question. In
A. Booth and G. Walton, editors, Managing Knowl-
edge in Health Services. Facet Publishing.
M. Chambliss and J. Conley. 1996. Answering clinical
questions. The Journal of Family Practice, 43:140?
144.
S. De Groote and J. Dorsch. 2003. Measuring use
patterns of online journals and databases. Journal
of the Medical Library Association, 91(2):231?240,
April.
D. Demner-Fushman and J. Lin. 2005. Knowledge ex-
traction for clinical question answering: Preliminary
results. In Proceedings of the AAAI-05 Workshop on
Question Answering in Restricted Domains.
J. Ely, J. Osheroff, M. Ebell, G. Bergus, B. Levy,
M. Chambliss, and E. Evans. 1999. Analysis of
questions asked by family doctors regarding patient
care. BMJ, 319:358?361.
J. Ely, J. Osheroff, M. Chambliss, M. Ebell, and
M. Rosenbaum. 2005. Answering physicians? clin-
ical questions: Obstacles and potential solutions.
Journal of the American Medical Informatics Asso-
ciation, 12(2):217?224, March-April.
L. Freund, E. Toms, and C. Clarke. 2005. Modeling
task-genre relationships for IR in the Workplace. In
Proceedings of SIGIR 2005.
P. Gorman, J. Ash, and L. Wykoff. 1994. Can pri-
mary care physicians? questions be answered using
the medical journal literature? Bulletin of the Medi-
cal Library Association, 82(2):140?146, April.
S. Hauser, D. Demner-Fushman, G. Ford, and
G. Thoma. 2004. PubMed on Tap: Discovering
design principles for online information delivery to
handheld computers. In Proceedings of MEDINFO
2004.
R. Haynes, N. Wilczynski, K. McKibbon, C. Walker,
and J. Sinclair. 1994. Developing optimal search
strategies for detecting clinically sound studies in
MEDLINE. Journal of the American Medical In-
formatics Association, 1(6):447?458.
L. Hirschman and R. Gaizauskas. 2001. Natural
language question answering: The view from here.
Natural Language Engineering, 7(4):275?300.
D. Lindberg, B. Humphreys, and A. McCray. 1993.
The Unified Medical Language System. Methods of
Information in Medicine, 32(4):281?291, August.
K. McKeown, N. Elhadad, and V. Hatzivassiloglou.
2003. Leveraging a common representation for per-
sonalized search and summarization in a medical
digital library. In Proceedings JCDL 2003.
E. Mendonc?a and J. Cimino. 2001. Building a knowl-
edge base to support a digital library. In Proceedings
of MEDINFO 2001.
D. Moldovan, M. Pas?ca, S. Harabagiu, and M. Sur-
deanu. 2002. Performance issues and error analysis
in an open-domain question answering system. In
Proceedings of ACL 2002.
S. Narayanan and S. Harabagiu. 2004. Question an-
swering based on semantic structures. In Proceed-
ings of COLING 2004.
Y. Niu and G. Hirst. 2004. Analysis of semantic
classes in medical text for question answering. In
Proceedings of the ACL 2004 Workshop on Question
Answering in Restricted Domains.
W. Richardson, M. Wilson, J. Nishikawa, and R. Hay-
ward. 1995. The well-built clinical question: A
key to evidence-based decisions. American Col-
lege of Physicians Journal Club, 123(3):A12?A13,
November-December.
31
Proceedings of the BioNLP Workshop on Linking Natural Language Processing and Biology at HLT-NAACL 06, pages 65?72,
New York City, June 2006. c?2006 Association for Computational Linguistics
Generative Content Models for Structural Analysis of Medical Abstracts
Jimmy Lin1,2, Damianos Karakos3, Dina Demner-Fushman2, and Sanjeev Khudanpur3
1College of Information Studies 3Center for Language and
2Institute for Advanced Computer Studies Speech Processing
University of Maryland Johns Hopkins University
College Park, MD 20742, USA Baltimore, MD 21218, USA
jimmylin@umd.edu, demner@cs.umd.edu (damianos, khudanpur)@jhu.edu
Abstract
The ability to accurately model the con-
tent structure of text is important for
many natural language processing appli-
cations. This paper describes experi-
ments with generative models for analyz-
ing the discourse structure of medical ab-
stracts, which generally follow the pattern
of ?introduction?, ?methods?, ?results?,
and ?conclusions?. We demonstrate that
Hidden Markov Models are capable of ac-
curately capturing the structure of such
texts, and can achieve classification ac-
curacy comparable to that of discrimina-
tive techniques. In addition, generative
approaches provide advantages that may
make them preferable to discriminative
techniques such as Support Vector Ma-
chines under certain conditions. Our work
makes two contributions: at the applica-
tion level, we report good performance
on an interesting task in an important do-
main; more generally, our results con-
tribute to an ongoing discussion regarding
the tradeoffs between generative and dis-
criminative techniques.
1 Introduction
Certain types of text follow a predictable structure,
the knowledge of which would be useful in many
natural language processing applications. As an
example, scientific abstracts across many different
fields generally follow the pattern of ?introduction?,
?methods?, ?results?, and ?conclusions? (Salanger-
Meyer, 1990; Swales, 1990; Ora?san, 2001). The
ability to explicitly identify these sections in un-
structured text could play an important role in ap-
plications such as document summarization (Teufel
and Moens, 2000), information retrieval (Tbahriti
et al, 2005), information extraction (Mizuta et al,
2005), and question answering. Although there is
a trend towards analysis of full article texts, we
believe that abstracts still provide a tremendous
amount of information, and much value can still be
extracted from them. For example, Gay et al (2005)
experimented with abstracts and full article texts in
the task of automatically generating index term rec-
ommendations and discovered that using full article
texts yields at most a 7.4% improvement in F-score.
Demner-Fushman et al (2005) found a correlation
between the quality and strength of clinical conclu-
sions in the full article texts and abstracts.
This paper presents experiments with generative
content models for analyzing the discourse struc-
ture of medical abstracts, which has been con-
firmed to follow the four-section pattern discussed
above (Salanger-Meyer, 1990). For a variety of rea-
sons, medicine is an interesting domain of research.
The need for information systems to support physi-
cians at the point of care has been well studied (Cov-
ell et al, 1985; Gorman et al, 1994; Ely et al,
2005). Retrieval techniques can have a large im-
pact on how physicians access and leverage clini-
cal evidence. Information that satisfies physicians?
needs can be found in theMEDLINE database main-
tained by the U.S. National Library of Medicine
65
(NLM), which also serves as a readily available
corpus of abstracts for our experiments. Further-
more, the availability of rich ontological resources,
in the form of the Unified Medical Language Sys-
tem (UMLS) (Lindberg et al, 1993), and the avail-
ability of software that leverages this knowledge?
MetaMap (Aronson, 2001) for concept identification
and SemRep (Rindflesch and Fiszman, 2003) for re-
lation extraction?provide a foundation for studying
the role of semantics in various tasks.
McKnight and Srinivasan (2003) have previously
examined the task of categorizing sentences in med-
ical abstracts using supervised discriminative ma-
chine learning techniques. Building on the work of
Ruch et al (2003) in the same domain, we present a
generative approach that attempts to directly model
the discourse structure of MEDLINE abstracts us-
ing Hidden Markov Models (HMMs); cf. (Barzilay
and Lee, 2004). Although our results were not ob-
tained from the same exact collection as those used
by authors of these two previous studies, comparable
experiments suggest that our techniques are compet-
itive in terms of performance, and may offer addi-
tional advantages as well.
Discriminative approaches (especially SVMs)
have been shown to be very effective for many
supervised classification tasks; see, for exam-
ple, (Joachims, 1998; Ng and Jordan, 2001). How-
ever, their high computational complexity (quadratic
in the number of training samples) renders them pro-
hibitive for massive data processing. Under certain
conditions, generative approaches with linear com-
plexity are preferable, even if their performance is
lower than that which can be achieved through dis-
criminative training. Since HMMs are very well-
suited to modeling sequences, our discourse model-
ing task lends itself naturally to this particular gener-
ative approach. In fact, we demonstrate that HMMs
are competitive with SVMs, with the added advan-
tage of lower computational complexity. In addition,
generative models can be directly applied to tackle
certain classes of problems, such as sentence order-
ing, in ways that discriminative approaches cannot
readily. In the context of machine learning, we see
our work as contributing to the ongoing debate be-
tween generative and discriminative approaches?
we provide a case study in an interesting domain that
begins to explore some of these tradeoffs.
2 Methods
2.1 Corpus and Data Preparation
Our experiments involved MEDLINE, the biblio-
graphical database of biomedical articles maintained
by the U.S. National Library of Medicine (NLM).
We used the subset of MEDLINE that was extracted
for the TREC 2004 Genomics Track, consisting of
citations from 1994 to 2003. In total, 4,591,008
records (abstract text and associated metadata) were
extracted using the Date Completed (DCOM) field
for all references in the range of 19940101 to
20031231.
Viewing structural modeling of medical abstracts
as a sentence classification task, we leveraged the
existence of so-called structured abstracts (see Fig-
ure 1 for an example) in order to obtain the appro-
priate section label for each sentence. The use of
section headings is a device recommended by the
Ad Hoc Working Group for Critical Appraisal of the
Medical Literature (1987) to help humans assess the
reliability and content of a publication and to facil-
itate the indexing and retrieval processes. Although
structured abstracts loosely adhere to the introduc-
tion, methods, results, and conclusions format, the
exact choice of section headings varies from ab-
stract to abstract and from journal to journal. In our
test collection, we observed a total of 2688 unique
section headings in structured abstracts?these were
manually mapped to the four broad classes of ?intro-
duction?, ?methods?, ?results?, and ?conclusions?.
All sentences falling under a section heading were
assigned the label of its appropriately-mapped head-
ing (naturally, the actual section headings were re-
moved in our test collection). As a concrete exam-
ple, in the abstract shown in Figure 1, the ?OBJEC-
TIVE? section would be mapped to ?introduction?,
the ?RESEARCH DESIGN AND METHODS? sec-
tion to ?methods?. The ?RESULTS? and ?CON-
CLUSIONS? sections map directly to our own la-
bels. In total, 308,055 structured abstracts were ex-
tracted and prepared in this manner, serving as the
complete dataset. In addition, we created a reduced
collection of 27,075 abstracts consisting of only
Randomized Controlled Trials (RCTs), which rep-
resent definitive sources of evidence highly-valued
in the clinical decision-making process.
Separately, we manually annotated 49 unstruc-
66
Integrating medical management with diabetes self-management training: a randomized control trial of the Diabetes
Outpatient Intensive Treatment program.
OBJECTIVE? This study evaluated the Diabetes Outpatient Intensive Treatment (DOIT) program, a multiday group educa-
tion and skills training experience combined with daily medical management, followed by case management over 6 months.
Using a randomized control design, the study explored how DOIT affected glycemic control and self-care behaviors over a
short term. The impact of two additional factors on clinical outcomes were also examined (frequency of case management
contacts and whether or not insulin was started during the program). RESEARCH DESIGN AND METHODS? Patients
with type 1 and type 2 diabetes in poor glycemic control (A1c ?8.5%) were randomly assigned to DOIT or a second con-
dition, entitled EDUPOST, which was standard diabetes care with the addition of quarterly educational mailings. A total
of 167 patients (78 EDUPOST, 89 DOIT) completed all baseline measures, including A1c and a questionnaire assessing
diabetes-related self-care behaviors. At 6 months, 117 patients (52 EDUPOST, 65 DOIT) returned to complete a follow-up
A1c and the identical self-care questionnaire. RESULTS? At follow-up, DOIT evidenced a significantly greater drop in A1c
than EDUPOST. DOIT patients also reported significantly more frequent blood glucose monitoring and greater attention to
carbohydrate and fat contents (ACFC) of food compared with EDUPOST patients. An increase in ACFC over the 6-month
period was associated with improved glycemic control among DOIT patients. Also, the frequency of nurse case manager
follow-up contacts was positively linked to better A1c outcomes. The addition of insulin did not appear to be a significant
contributor to glycemic change. CONCLUSIONS? DOIT appears to be effective in promoting better diabetes care and posi-
tively influencing glycemia and diabetes-related self-care behaviors. However, it demands significant time, commitment, and
careful coordination with many health care professionals. The role of the nurse case manager in providing ongoing follow-up
contact seems important.
Figure 1: Sample structured abstract from MEDLINE.
tured abstracts of randomized controlled trials re-
trieved to answer a question about the manage-
ment of elevated low-density lipoprotein cholesterol
(LDL-C). We submitted a PubMed query (?elevated
LDL-C?) and restricted results to English abstracts
of RCTs, gathering 49 unstructured abstracts from
26 journals. Each sentence was annotated with its
section label by the third author, who is a medical
doctor?this collection served as our blind held-out
testset. Note that the annotation process preceded
our experiments, which helped to guard against
annotator-introduced bias. Of 49 abstracts, 35 con-
tained all four sections (which we refer to as ?com-
plete?), while 14 abstracts were missing one or more
sections (which we refer to as ?partial?).
Two different types of experiments were con-
ducted: the first consisted of cross-validation on the
structured abstracts; the second consisted of train-
ing on the structured abstracts and testing on the
unstructured abstracts. We hypothesized that struc-
tured and unstructured abstracts share the same un-
derlying discourse patterns, and that content models
trained with one can be applied to the other.
2.2 Generative Models of Content
Following Ruch et al (2003) and Barzilay and
Lee (2004), we employed Hidden Markov Models
to model the discourse structure of MEDLINE ab-
stracts. The four states in our HMMs correspond
to the information that characterizes each section
(?introduction?, ?methods?, ?results?, and ?conclu-
sions?) and state transitions capture the discourse
flow from section to section.
Using the SRI language modeling toolkit, we
first computed bigram language models for each
of the four sections using Kneser-Ney discounting
and Katz backoff. All words in the training set
were downcased, all numbers were converted into
a generic symbol, and all singleton unigrams and bi-
grams were removed. Using these results, each sen-
tence was converted into a four dimensional vector,
where each component represents the log probabil-
ity, divided by the number of words, of the sentence
under each of the four language models.
We then built a four-state Hidden Markov Model
that outputs these four-dimensional vectors. The
transition probability matrix of the HMM was ini-
tialized with uniform probabilities over a fully
connected graph. The output probabilities were
modeled as four-dimensional Gaussians mixtures
with diagonal covariance matrices. Using the sec-
tion labels, the HMM was trained using the HTK
toolkit (Young et al, 2002), which efficiently per-
forms the forward-backward algorithm and Baum-
Welch estimation. For testing, we performed a
Viterbi (maximum likelihood) estimation of the la-
bel of each test sentence/vector (also using the HTK
toolkit).
67
In an attempt to further boost performance, we
employed Linear Discriminant Analysis (LDA) to
find a linear projection of the four-dimensional vec-
tors that maximizes the separation of the Gaussians
(corresponding to the HMM states). Venables and
Ripley (1994) describe an efficient algorithm (of lin-
ear complexity in the number of training sentences)
for computing the LDA transform matrix, which en-
tails computing the within- and between-covariance
matrices of the classes, and using Singular Value De-
composition (SVD) to compute the eigenvectors of
the new space. Each sentence/vector is then mul-
tiplied by this matrix, and new HMM models are
re-computed from the projected data.
An important aspect of our work is modeling con-
tent structure using generative techniques. To as-
sess the impact of taking discourse transitions into
account, we compare our fully trained model to
one that does not take advantage of the Markov
assumption?i.e., it assumes that the labels are in-
dependently and identically distributed.
To facilitate comparison with previous work, we
also experimented with binary classifiers specifi-
cally tuned to each section. This was done by creat-
ing a two-state HMM: one state corresponds to the
label we want to detect, and the other state corre-
sponds to all the other labels. We built four such
classifiers, one for each section, and trained them in
the same manner as above.
3 Results
We report results on three distinct sets of experi-
ments: (1) ten-fold cross-validation (90/10 split) on
all structured abstracts from the TREC 2004 MED-
LINE corpus, (2) ten-fold cross-validation (90/10
split) on the RCT subset of structured abstracts from
the TREC 2004 MEDLINE corpus, (3) training on
the RCT subset of the TREC 2004 MEDLINE cor-
pus and testing on the 49 hand-annotated held-out
testset.
The results of our first set of experiments are
shown in Tables 1(a) and 1(b). Table 1(a) reports
the classification error in assigning a unique label to
every sentence, drawn from the set {?introduction?,
?methods?, ?results?, ?conclusions?}. For this task,
we compare the performance of three separate mod-
els: one that does not make the Markov assumption,
Model Error
non-HMM .220
HMM .148
HMM + LDA .118
(a)
Section Acc Prec Rec F
Introduction .957 .930 .840 .885
Methods .921 .810 .875 .843
Results .921 .898 .898 .898
Conclusions .963 .898 .896 .897
(b)
Table 1: Ten-fold cross-validation results on all
structured abstracts from the TREC 2004 MED-
LINE corpus: multi-way classification on complete
abstract structure (a) and by-section binary classifi-
cation (b).
the basic four-state HMM, and the improved four-
state HMM with LDA. As expected, explicitly mod-
eling the discourse transitions significantly reduces
the error rate. Applying LDA further enhances clas-
sification performance. Table 1(b) reports accuracy,
precision, recall, and F-measure for four separate bi-
nary classifiers specifically trained for each of the
sections (one per row in the table). We only dis-
play results with our best model, namely HMM with
LDA.
The results of our second set of experiments (with
RCTs only) are shown in Tables 2(a) and 2(b).
Table 2(a) reports the multi-way classification er-
ror rate; once again, applying the Markov assump-
tion to model discourse transitions improves perfor-
mance, and using LDA further reduces error rate.
Table 2(b) reports accuracy, precision, recall, and F-
measure for four separate binary classifiers (HMM
with LDA) specifically trained for each of the sec-
tions (one per row in the table). The table also
presents the closest comparable experimental re-
sults reported by McKnight and Srinivasan (2003).1
McKnight and Srinivasan (henceforth, M&S) cre-
ated a test collection consisting of 37,151 RCTs
from approximately 12 million MEDLINE abstracts
dated between 1976 and 2001. This collection has
1After contacting the authors, we were unable to obtain the
same exact dataset that they used for their experiments.
68
Model Error
non-HMM .238
HMM .212
HMM + LDA .209
(a)
Present study McKnight and Srinivasan
Section Acc Prec Rec F Acc Prec Rec F
Introduction .931 .898 .715 .807 .967 .920 .970 .945
Methods .904 .812 .847 .830 .895 .810 .830 .820
Results .902 .902 .831 .867 .860 .810 .830 .820
Conclusions .929 .772 .790 .781 .970 .880 .910 .820
(b)
Table 2: Ten-fold cross-validation results on the structured RCT subset of the TREC 2004 MEDLINE
corpus: multi-way classification (a) and binary classification (b). Table (b) also reproduces the results from
McKnight and Srinivasan (2003) for a comparable task on a different RCT-subset of structured abstracts.
Model Complete Partial
non-HMM .247 .371
HMM .226 .314
HMM + LDA .217 .279
(a)
Complete Partial McKnight and Srinivasan
Section Acc Prec Rec F Acc Prec Rec F Acc Prec Rec F
Introduction .923 .739 .723 .731 .867 .368 .636 .502 .896 .630 .450 .524
Methods .905 .841 .793 .817 .859 .958 .589 .774 .897 .880 .730 .799
Results .899 .913 .857 .885 .892 .942 .830 .886 .872 .840 .880 .861
Conclusions .911 .639 .847 .743 .884 .361 .995 .678 .941 .830 .750 .785
(b)
Table 3: Training on the structured RCT subset of the TREC 2004 MEDLINE corpus, testing on corpus of
hand-annotated abstracts: multi-way classification (a) and binary classification (b). Unstructured abstracts
with all four sections (complete), and with missing sections (partial) are shown. Table (b) again repro-
duces the results from McKnight and Srinivasan (2003) for a comparable task on a different subset of 206
unstructured abstracts.
69
significantly more training examples than our corpus
of 27,075 abstracts, which could be a source of per-
formance differences. Furthermore, details regard-
ing their procedure for mapping structured abstract
headings to one of the four general labels was not
discussed in their paper. Nevertheless, our HMM-
based approach is at least competitive with SVMs,
perhaps better in some cases.
The results of our third set of experiments (train-
ing on RCTs and testing on a held-out testset of
hand-annotated abstracts) is shown in Tables 3(a)
and 3(b). Mirroring the presentation format above,
Table 3(a) shows the classification error for the four-
way label assignment problem. We noticed that
some unstructured abstracts are qualitatively differ-
ent from structured abstracts in that some sections
are missing. For example, some unstructured ab-
stracts lack an introduction, and instead dive straight
into methods; other unstructured abstracts lack a
conclusion. As a result, classification error is higher
in this experiment than in the cross-validation ex-
periments. We report performance figures for 35 ab-
stracts that contained all four sections (?complete?)
and for 14 abstracts that had one or more miss-
ing sections (?partial?). Table 3(b) reports accu-
racy, precision, recall, and F-measure for four sep-
arate binary classifiers (HMM with LDA) specifi-
cally trained for each section (one per row in the
table). The table also presents the closest compa-
rable experimental results reported by M&S?over
206 hand-annotated unstructured abstracts. Interest-
ingly, M&S did not specifically note missing sec-
tions in their testset.
4 Discussion
An interesting aspect of our generative approach
is that we model HMM outputs as Gaussian vec-
tors (log probabilities of observing entire sentences
based on our language models), as opposed to se-
quences of terms, as done in (Barzilay and Lee,
2004). This technique provides two important ad-
vantages. First, Gaussian modeling adds an ex-
tra degree of freedom during training, by capturing
second-order statistics. This is not possible when
modeling word sequences, where only the probabil-
ity of a sentence is actually used in the HMM train-
ing. Second, using continuous distributions allows
us to leverage a variety of tools (e.g., LDA) that have
been shown to be successful in other fields, such as
speech recognition (Evermann et al, 2004).
Table 2(b) represents the closest head-to-head
comparison between our generative approach
(HMM with LDA) and state-of-the-art results
reported by M&S using SVMs. In some ways, the
results reported by M&S have an advantage because
they use significantly more training examples. Yet,
we can see that generative techniques for the model-
ing of content structure are at least competitive?we
even outperform SVMs on detecting ?methods?
and ?results?. Moreover, the fact that the training
and testing of HMMs have linear complexity (as
opposed to the quadratic complexity of SVMs)
makes our approach a very attractive alternative,
given the amount of training data that is available
for such experiments.
Although exploration of the tradeoffs between
generative and discriminative machine learning
techniques is one of the aims of this work, our ul-
timate goal, however, is to build clinical systems
that provide timely access to information essential
to the patient treatment process. In truth, our cross-
validation experiments do not correspond to any
meaningful naturally-occurring task?structured ab-
stracts are, after all, already appropriately labeled.
The true utility of content models is to struc-
ture abstracts that have no structure to begin with.
Thus, our exploratory experiments in applying con-
tent models trained with structured RCTs on un-
structured RCTs is a closer approximation of an
extrinsically-valid measure of performance. Such a
component would serve as the first stage of a clin-
ical question answering system (Demner-Fushman
and Lin, 2005) or summarization system (McKe-
own et al, 2003). We chose to focus on randomized
controlled trials because they represent the standard
benchmark by which all other clinical studies are
measured.
Table 3(b) shows the effectiveness of our trained
content models on abstracts that had no explicit
structure to begin with. We can see that although
classification accuracy is lower than that from our
cross-validation experiments, performance is quite
respectable. Thus, our hypothesis that unstructured
abstracts are not qualitatively different from struc-
tured abstracts appears to be mostly valid.
70
5 Related Work
Although not the first to employ a generative ap-
proach to directly model content, the seminal work
of Barzilay and Lee (2004) is a noteworthy point
of reference and comparison. However, our study
differs in several important respects. Barzilay and
Lee employed an unsupervised approach to building
topic sequence models for the newswire text genre
using clustering techniques. In contrast, because
the discourse structure of medical abstracts is well-
defined and training data is relatively easy to ob-
tain, we were able to apply a supervised approach.
Whereas Barzilay and Lee evaluated their work in
the context of document summarization, the four-
part structure of medical abstracts allows us to con-
duct meaningful intrinsic evaluations and focus on
the sentence classification task. Nevertheless, their
work bolsters our claims regarding the usefulness of
generative models in extrinsic tasks, which we do
not describe here.
Although this study falls under the general topic
of discourse modeling, our work differs from previ-
ous attempts to characterize text in terms of domain-
independent rhetorical elements (McKeown, 1985;
Marcu and Echihabi, 2002). Our task is closer to the
work of Teufel and Moens (2000), who looked at the
problem of intellectual attribution in scientific texts.
6 Conclusion
We believe that there are two contributions as a re-
sult of our work. From the perspective of machine
learning, the assignment of sequentially-occurring
labels represents an underexplored problem with re-
spect to the generative vs. discriminative debate?
previous work has mostly focused on stateless clas-
sification tasks. This paper demonstrates that Hid-
den Markov Models are capable of capturing dis-
course transitions from section to section, and are
at least competitive with Support Vector Machines
from a purely performance point of view.
The other contribution of our work is that it con-
tributes to building advanced clinical information
systems. From an application point of view, the abil-
ity to assign structure to otherwise unstructured text
represents a key capability that may assist in ques-
tion answering, document summarization, and other
natural language processing applications.
Much research in computational linguistics has
focused on corpora comprised of newswire articles.
We would like to point out that clinical texts provide
another attractive genre in which to conduct experi-
ments. Such texts are easy to acquire, and the avail-
ability of domain ontologies provides new opportu-
nities for knowledge-rich approaches to shine. Al-
though we have only experimented with lexical fea-
tures in this study, the door is wide open for follow-
on studies based on semantic features.
7 Acknowledgments
The first author would like to thank Esther and Kiri
for their loving support.
References
Ad Hoc Working Group for Critical Appraisal of the
Medical Literature. 1987. A proposal for more infor-
mative abstracts of clinical articles. Annals of Internal
Medicine, 106:595?604.
Alan R. Aronson. 2001. Effective mapping of biomed-
ical text to the UMLS Metathesaurus: The MetaMap
program. In Proceeding of the 2001 Annual Sympo-
sium of the American Medical Informatics Association
(AMIA 2001), pages 17?21.
Regina Barzilay and Lillian Lee. 2004. Catching the
drift: Probabilistic content models, with applications
to generation and summarization. In Proceedings
of the 2004 Human Language Technology Confer-
ence and the North American Chapter of the Associ-
ation for Computational Linguistics Annual Meeting
(HLT/NAACL 2004).
David G. Covell, Gwen C. Uman, and Phil R. Manning.
1985. Information needs in office practice: Are they
being met? Annals of Internal Medicine, 103(4):596?
599, October.
Dina Demner-Fushman and Jimmy Lin. 2005. Knowl-
edge extraction for clinical question answering: Pre-
liminary results. In Proceedings of the AAAI-05 Work-
shop on Question Answering in Restricted Domains.
Dina Demner-Fushman, Susan E. Hauser, and George R.
Thoma. 2005. The role of title, metadata and ab-
stract in identifying clinically relevant journal arti-
cles. In Proceeding of the 2005 Annual Symposium of
the American Medical Informatics Association (AMIA
2005), pages 191?195.
John W. Ely, Jerome A. Osheroff, M. Lee Chambliss,
Mark H. Ebell, and Marcy E. Rosenbaum. 2005. An-
swering physicians? clinical questions: Obstacles and
71
potential solutions. Journal of the American Medical
Informatics Association, 12(2):217?224, March-April.
Gunnar Evermann, H. Y. Chan, Mark J. F. Gales, Thomas
Hain, Xunying Liu, David Mrva, Lan Wang, and Phil
Woodland. 2004. Development of the 2003 CU-HTK
Conversational Telephone Speech Transcription Sys-
tem. In Proceedings of the 2004 International Con-
ference on Acoustics, Speech and Signal Processing
(ICASSP04).
Clifford W. Gay, Mehmet Kayaalp, and Alan R. Aronson.
2005. Semi-automatic indexing of full text biomedi-
cal articles. In Proceeding of the 2005 Annual Sympo-
sium of the American Medical Informatics Association
(AMIA 2005), pages 271?275.
Paul N. Gorman, Joan S. Ash, and Leslie W. Wykoff.
1994. Can primary care physicians? questions be an-
swered using the medical journal literature? Bulletin
of the Medical Library Association, 82(2):140?146,
April.
Thorsten Joachims. 1998. Text categorization with Sup-
port Vector Machines: Learning with many relevant
features. In Proceedings of the European Conference
on Machine Learning (ECML 1998).
Donald A. Lindberg, Betsy L. Humphreys, and Alexa T.
McCray. 1993. The Unified Medical Language Sys-
tem. Methods of Information in Medicine, 32(4):281?
291, August.
Daniel Marcu and Abdessamad Echihabi. 2002. An
unsupervised approach to recognizing discourse rela-
tions. In Proceedings of the 40th Annual Meeting of
the Association for Computational Linguistics (ACL
2002).
Kathleen McKeown, Noemie Elhadad, and Vasileios
Hatzivassiloglou. 2003. Leveraging a common rep-
resentation for personalized search and summarization
in a medical digital library. In Proceedings of the
3rd ACM/IEEE Joint Conference on Digital Libraries
(JCDL 2003).
Kathleen R. McKeown. 1985. Text Generation: Using
Discourse Strategies and Focus Constraints to Gen-
erate Natural Language Text. Cambridge University
Press, Cambridge, England.
Larry McKnight and Padmini Srinivasan. 2003. Catego-
rization of sentence types in medical abstracts. In Pro-
ceeding of the 2003 Annual Symposium of the Ameri-
can Medical Informatics Association (AMIA 2003).
Yoko Mizuta, Anna Korhonen, Tony Mullen, and Nigel
Collier. 2005. Zone analysis in biology articles as a
basis for information extraction. International Journal
of Medical Informatics, in press.
Andrew Y. Ng and Michael Jordan. 2001. On discrim-
inative vs. generative classifiers: A comparison of lo-
gistic regression and naive Bayes. In Advances in Neu-
ral Information Processing Systems 14.
Constantin Ora?san. 2001. Patterns in scientific abstracts.
In Proceedings of the 2001 Corpus Linguistics Confer-
ence.
Thomas C. Rindflesch and Marcelo Fiszman. 2003. The
interaction of domain knowledge and linguistic struc-
ture in natural language processing: Interpreting hy-
pernymic propositions in biomedical text. Journal of
Biomedical Informatics, 36(6):462?477, December.
Patrick Ruch, Christine Chichester, Gilles Cohen, Gio-
vanni Coray, Fre?de?ric Ehrler, Hatem Ghorbel, Hen-
ning Mu?ller, and Vincenzo Pallotta. 2003. Report
on the TREC 2003 experiment: Genomic track. In
Proceedings of the Twelfth Text REtrieval Conference
(TREC 2003).
Franc?oise Salanger-Meyer. 1990. Discoursal movements
in medical English abstracts and their linguistic expo-
nents: A genre analysis study. INTERFACE: Journal
of Applied Linguistics, 4(2):107?124.
John M. Swales. 1990. Genre Analysis: English in Aca-
demic and Research Settings. Cambridge University
Press, Cambridge, England.
Imad Tbahriti, Christine Chichester, Fre?de?rique Lisacek,
and Patrick Ruch. 2005. Using argumentation to re-
trieve articles with similar citations: An inquiry into
improving related articles search in the MEDLINE
digital library. International Journal of Medical In-
formatics, in press.
Simone Teufel and Marc Moens. 2000. What?s yours
and what?s mine: Determining intellectual attribu-
tion in scientific text. In Proceedings of the Joint
SIGDAT Conference on Empirical Methods in Nat-
ural Language Processing and Very Large Corpora
(EMNLP/VLC-2000).
William N. Venables and Brian D. Ripley. 1994. Modern
Applied Statistics with S-Plus. Springer-Verlag.
Steve Young, Gunnar Evermann, Thomas Hain, Dan Ker-
shaw, Gareth Moore, Julian Odell, Dave Ollason, Dan
Povey, Valtcho Valtchev, and Phil Woodland. 2002.
The HTK Book. Cambridge University Press.
72
Proceedings of the Workshop on Language Technology for Cultural Heritage Data (LaTeCH 2007), pages 25?32,
Prague, 28 June 2007. c?2007 Association for Computational Linguistics
Concept Disambiguation for Improved Subject Access  
Using Multiple Knowledge Sources 
 
Tandeep Sidhu, Judith Klavans, and Jimmy Lin 
College of Information Studies 
University of Maryland 
College Park, MD 20742 
tsidhu@umiacs.umd.edu, {jklavans, jimmylin}@umd.edu 
 
 
Abstract 
 
We address the problem of mining text for 
relevant image metadata.  Our work is situ-
ated in the art and architecture domain, 
where highly specialized technical vocabu-
lary presents challenges for NLP tech-
niques.  To extract high quality metadata, 
the problem of word sense disambiguation 
must be addressed in order to avoid leading 
the searcher to the wrong image as a result 
of ambiguous ? and thus faulty ? meta-
data.  In this paper, we present a disam-
biguation algorithm that attempts to select 
the correct sense of nouns in textual de-
scriptions of art objects, with respect to a 
rich domain-specific thesaurus, the Art and 
Architecture Thesaurus (AAT).  We per-
formed a series of intrinsic evaluations us-
ing a data set of 600 subject terms ex-
tracted from an online National Gallery of 
Art (NGA) collection of images and text.  
Our results showed that the use of external 
knowledge sources shows an improvement 
over a baseline. 
      
1. Introduction 
We describe an algorithm that takes noun phrases 
and assigns a sense to the head noun or phrase, 
given a large domain-specific thesaurus, the Art 
and Architecture Thesaurus1 (published by the 
Getty Research Institute).  This research is part of 
the Computational Linguistics for Metadata 
                                                                 
1http://www.getty.edu/research/conducting_research/vocabul
aries/aat/ 
Building (CLiMB) project (Klavans 2006, Kla-
vans in preparation), which aims to improve im-
age access by automatically extracting metadata 
from text associated with images.  We present 
here a component of an overall architecture that 
automatically mines scholarly text for metadata 
terms.  In order to filter and associate a term with 
a related concept, ambiguous terms must be clari-
fied.  The disambiguation of terms is a basic chal-
lenge in computational linguistics (Ide and Vero-
nis 1990, Agirre and Edmonds 2006). 
As more non-specialists in digital libraries 
search for images, the need for subject term ac-
cess has increased.  Subject terms enrich catalog 
records with valuable broad-reaching metadata 
and help improve image access (Layne 1994).  
Image seekers will receive more relevant results 
if image records contain terms that reflect con-
ceptual, semantic, and ontological relationships.  
Furthermore, subject terms associated with hier-
archical and faceted thesaural senses promise to 
further improve precision in image access.  Such 
terms map to standardized thesaurus records that 
include the term?s preferred, variant, and related 
names, including both broader and specific con-
cepts, and other related concepts.  This informa-
tion can then be filtered, linked, and subsequently 
tested for usefulness in performing richer image 
access.  As with other research on disambigua-
tion, our hypothesis is that accurate assignment of 
senses to metadata index terms will results in 
higher precision for searchers.  This hypothesis 
will be fully tested as we incorporate the disam-
biguation module in our end-to-end CLiMB 
Toolkit, and as we perform user studies. 
Finding subject terms and mapping them to a 
thesaurus is a time-intensive task for catalogers 
25
(Rasmussen 1997, Ferguson and Intner 1998).  
Doing so typically involves reading image-related 
text or other sources to find subject terms.  Even 
so, the lack of standard vocabulary in extensive 
subject indexing means that the enriched number 
of subject terms could be inadvertently offset by 
the vocabulary naming problem (Baca 2002).   
This paper reports on our results using the 
subject terms in the AAT; the CLiMB project is 
also using the Thesaurus of Geographic Names 
(TGN) and the Union List of Artist Names 
(ULAN).  Since the focus of this paper is on dis-
ambiguation of common nouns rather than proper 
nouns, the AAT is our primary resource. 
2. Resources 
2.1 Art and Architecture Thesaurus (AAT)  
The AAT is a widely-used multi-faceted thesau-
rus of terms for the cataloging and indexing of 
art, architecture, artifactual, and archival materi-
als. Since the AAT offers a controlled vocabulary 
for recording and retrieval of data in object, bib-
liographic, and visual databases, it is of interest to 
a wide community. 
In the AAT, each concept is described 
through a record which has a unique ID, preferred 
name, record description, variant names, broader, 
narrower, and related terms.  In total, AAT has 
31,000 such records.  For the purpose of this arti-
cle, a record can be viewed as synonymous with 
sense. Within the AAT, there are 1,400 homo-
nyms, i.e., records with same preferred name.  
For example, the term wings has five senses in 
the AAT (see Figure 1 below).   
Wings (5 senses): 
? Sense#1: Used for accessories that project outward 
from the shoulder of a garment and are made of cloth 
or metal.   
? Sense#2: Lateral parts or appendages of a work of 
art, such as those found on a triptych.  
? Sense#3: The areas offstage and to the side of the 
acting area. 
? Sense#4: The two forward extensions to the sides of 
the back on an easy chair.  
? Sense#5: Subsidiary parts of buildings extending out 
from the main portion. 
Figure 1:  Selection of AAT records for term ?wings? 
Table 1 shows the breakdown of the AAT vo-
cabulary by number of senses with a sample lexi-
cal item for each frequency. 
# of 
Senses 
# of  
Homonyms 
Example 
2 1097 bells 
3 215 painting 
4 50 alabaster 
5 39 wings 
6 9 boards 
7 5 amber 
8 2 emerald 
9 1 plum 
10 1 emerald green 
11 1 magenta 
12 1 ocher 
13 1 carmine 
14 2 slate 
Table 1:  Scope of the disambiguation problem in AAT 
Note that there are potentially three tasks that 
could be addressed with our algorithm: (i) map-
ping a term to the correct sense in the AAT, (ii) 
selecting amongst closely related terms in the 
AAT, and (iii) mapping synonyms onto a single 
AAT entry.  In this paper, our primary focus is on 
task (i); we handle task (ii) with a simple ranking 
approach; we do not address task (iii).  
Table 1 shows that multiple senses per term 
makes mapping subject terms to AAT very chal-
lenging.  Manual disambiguation would be slow, 
tedious, and unrealistic.  Thus we explore auto-
matic methods since, in order to identify the cor-
rect sense of a term in running text, each of these 
senses needs to be viewed in context. 
2.2 The Test Collection 
The data set of terms that we use for evaluation 
comes from the National Gallery of Art (NGA) 
online archive2.  This collection covers paintings, 
sculpture, decorative arts, and works from the 
Middle Ages to the present.  We randomly se-
lected 20 images with corresponding text from 
this collection and extracted noun phrases to form 
the data set.  The data set was divided into two 
categories: the training set and the test set.  The 
training set consisted of 326 terms and was used 
                                                                 
2 http://www.nga.gov/home.htm 
26
to develop the algorithm.  The test set consisted 
of 275 terms and was used to evaluate. 
Following standard procedure in word sense 
disambiguation tasks (Palmer et al 2006), 
groundtruth for the data set was created manually 
by two labelers (referred to as Labeler 1 and La-
beler 2 in Section 4 below).  These labelers were 
part of the larger CLiMB project but they were 
not involved in the development of the disam-
biguation algorithm.  The process of creating the 
groundtruth involved picking the correct AAT 
record for each of the terms in the data set.  
Terms not appearing in the AAT (as determined 
by the labelers) were given an AAT record value 
of zero.  Each labeler worked independently on 
this task and had access to the online version of 
the AAT and the text where each term appeared. 
Interannotator agreement for the task was encour-
agingly high, at 85% providing a notional upper 
bound for automatic system performance (Gale et 
al.  1992).  
Not all terms in this dataset required disam-
biguation; 128 terms (out of 326) under the train-
ing set and 96 terms (out of 275) under the test 
set required disambiguation, since they matched 
more than one AAT record.  The dataset we se-
lected was adequate to test our different ap-
proaches and to refine our techniques.  We intend 
to run over more data as we collect and annotate 
more resources for evaluation. 
2.3 SenseRelate AllWords3 and WordNet4 
SenseRelate AllWords (Banerjee and Pederson 
2003, Patwardhan et al 2003) is a Perl program 
that our algorithm employs to perform basic dis-
ambiguation of words. We have adapted Sen-
seRelate for the purpose of disambiguating AAT 
senses.  
Given a sentence, SenseRelate AllWords dis-
ambiguates all the words in that sentence.  It uses 
word sense definitions from WordNet (in this 
case WordNet 2.1), a large lexical database of 
English nouns, verbs, adjectives, and adverbs.  As 
an example, consider the text below: 
                                                                 
3 http://sourceforge.net/projects/senserelate 
4 http://wordnet.princeton.edu/ 
With more than fifty individual scenes, the al-
tarpiece was about fourteen feet wide. 
 
The SenseRelate result is: 
With more#a#2 than fifty#n#1 individual#n#1 
scene#n#10 the altarpiece#n#1 be#v#1 about#r#1 
fourteen#n#1 foot#n#2 wide#a#1 
 
In the above example, more#a#2 means SenseRe-
late labeled more as an adjective and mapped it to 
second meaning of more (found in WordNet). 
fifty#n#1 means SenseRelate labeled fifty as a 
noun and mapped it to first meaning of fifty 
(found in WordNet).  Note, that fifty#n#1 maps to 
a sense in WordNet, whereas in our algorithm it 
needs to map to an AAT sense.  In Section 3, we 
show how we translate a WordNet sense to an 
AAT sense for use in our algorithm. 
To perform disambiguation, SenseRelate re-
quires that certain parameters be set:  (1) the 
number of words around the target word (also 
known as the context window), and  (2) the simi-
larity measure.  We used a value of 20 for the 
context window, which means that SenseRelate 
will use 10 words to the left and 10 words to the 
right of the target word to determine the correct 
sense.  We used lesk as the similarity measure in 
our algorithm which is based on Lesk (1986).  
This decision was based on several experiments 
we did with various context window sizes and 
various similarity measures on a data set of 60 
terms.   
27
3. Methodology 
3.1 Disambiguation Algorithm  
 
Figure 2:  Disambiguation Algorithm 
Figure 2 above shows that first we identify the 
noun phrases from the input document.  Then we 
disambiguate each noun phrase independently by 
first looking it up in the AAT.  If a record is 
found, we move on to the next step; otherwise we 
look up the head noun (as the noun phrase) in the 
AAT.  
Second, we filter out any AAT records where 
the noun phrase (or the head noun) is used as an 
adjective (for a term like painting this would be 
painting techniques, painting knives, painting 
equipment, etc). Third, if zero records are found 
in the AAT, we label the term as ?not found in 
AAT.?  If only one matching record is found, we 
label the term with the ID of this record.  Fourth, 
if more than one record is found, we use the dis-
ambiguation techniques outlined in the next sec-
tion to find the correct record.  
3.2 Techniques for Disambiguation 
For each of the terms, the following techniques 
were applied in the order they are given in this 
section. If a technique failed to disambiguate a 
term, we applied the next technique. If none of 
these techniques was able to disambiguate, we 
selected the first AAT record as the correct re-
cord.  Findings for each technique are provided in 
the Results section below. 
First, we used all modifiers that are in the 
noun phrase to find the correct AAT record.  We 
searched for the modifiers in the record descrip-
tion, variant names, and the parent hierarchy 
names of all the matching AAT senses.  If this 
technique narrowed down the option set to one 
record, then we found our correct record.  For 
example, consider the term ceiling coffers.  For 
this term we found two records: coffers (coffered 
ceiling components) and coffers (chests).  The 
first record has the modifier ceiling in its record 
description, so we were able to determine that 
this was the correct record. 
Second, we used SenseRelate AllWords and 
WordNet.  This gave us the WordNet sense of our 
noun phrase (or its head noun).  Using that sense 
definition from WordNet, we next examined 
which of the AAT senses best matches with the 
WordNet sense definition.  For this, we used the 
word overlapping technique where we awarded a 
score of N to an AAT record where N words 
overlap with the sense that SenseRelate picked.  
The AAT record with the highest score was se-
lected as the correct record.  If none of the AAT 
records received any positive score (above a cer-
tain threshold), then it was decided that this tech-
nique could not find the one correct match.  
As an example, consider finding the correct 
sense for the single word noun bells using Sen-
seRelate: 
1. Given the input sentence: 
?? city officials, and citizens were followed by 
women and children ringing bells for joy.? 
2. Search for AAT records.  There are two records 
for the bells in AAT: 
a. bells: ?Flared or bulbous terminals found on 
many open-ended aerophone tubes?. 
b. bells: ?Percussion vessels consisting of a hollow 
object, usually of metal but in some cultures of 
hard clay, wood, or glass, which when struck emits 
a sound by the vibration of most of its mass;?? 
3. Submit the input sentence to SenseRelate, which 
provides a best guess for the corresponding 
WordNet senses for each word. 
4. Get SenseRelate output, which indicates that the 
WordNet definition for bells is WordNet-Sense1, 
i.e., ?a hollow device made of metal that makes a 
ringing sound when struck? 
28
SenseRelate output: 
city#n#1 official#n#1 and citizen#n#1 be#v#1 
follow#v#20 by#r#1 woman#n#1 and child#n#1 
ringing#a#1 bell#n#1 for joy#n#1 
5. Find the correct AAT match using word overlap of 
the WordNet definition and the two AAT defini-
tions for bells: 
 
WordNet:  ?a hollow device made of metal that 
makes a ringing sound when struck? 
compared with: 
AAT: ?Flared or bulbous terminals found on many 
open-ended aerophone tubes? 
and compared with: 
AAT:  ?Percussion vessels consisting of a hollow 
object, usually of metal but in some cultures of 
hard clay, wood, or glass, which when struck 
emits a sound by the vibration of most of its 
mass;?? 
  
6. The second AAT sense is the correct sense accord-
ing to the word overlap (see Table 2 below): 
 
Comparison Score Word Overlap 
AAT ? Definition 1 and 
WordNet Sense1 
0 None 
AAT ? Definition 2 and 
WordNet Sense1 
4 hollow, metal, 
sound, struck 
Table 2: Word Overlap to Select AAT Definition 
Notice that we only used the AAT record descrip-
tion for performing the word overlap.  We ex-
perimented by including other information pre-
sent in the AAT record (like variant names, par-
ent AAT record names) also, but simply using the 
record description yielded the best results.   
Third, we used AAT record names (preferred 
and variant) to find the one correct match.  If one 
of the record names matched better than the other 
record names to the noun phrase name, that re-
cord was deemed to be the correct record.  For 
example, the term altar more appropriately 
matches altars (religious building fixtures) than 
altarpieces (religious visual works).  Another 
example is children, which better matches chil-
dren (youth) than offspring (people by family re-
lationship).   
Fourth, if none of the above techniques 
succeeded in selecting one record, we used the 
most common sense definition for a term (taken 
from WordNet) in conjunction with the AAT re-
sults and word overlapping mentioned above to 
find the one correct record.  
4. Results and Evaluation 
4.1 Methodologies 
We used three different evaluation methods to 
assess the performance of our algorithm.  The 
first evaluation method computes whether our 
algorithm picked the correct AAT record (i.e., the 
AAT sense picked is in agreement with the 
groundtruth).  The second method computes 
whether the correct record is among the top three 
records picked by our algorithm.  In Table 3 be-
low, this is referred to as Top3.  The third evalua-
tion method computes whether the correct record 
is in top five records picked by our algorithm, 
Top5.  The last two evaluations helped us deter-
mine the usability of our algorithm in situations 
where it does not pick the correct record but it 
still narrows down to top three or top five results.  
We ranked the AAT records according to 
their preferred name for the baseline, given the 
absence of any other disambiguation algorithm. 
Thus, AAT records that exactly matched the term 
in question appear on top, followed by records 
that partially matched the term.  For example, for 
term feet, the top three records were feet (terminal 
elements of objects), French feet (bracket feet), 
and Spanish feet (furniture components).  For the 
noun wings, the top three records were wings 
(shoulder accessories), wings (visual works com-
ponents), and wings (backstage spaces). 
4.2 Overall Results 
In this section, we present evaluation results for 
all the terms.  In the next section, we present re-
sults for only those terms that required disam-
biguation. 
Overall results for the training set (326 terms) 
are shown in Table 3. This table shows that over-
all accuracy of our algorithm is 76% and 68% for 
Labeler 1 and Labeler 2, respectively.  The base-
line accuracy is 69% for Labeler 1 and 62% for 
Labeler 2. The other two evaluations show much 
better results.  The Top 3 and Top5 evaluations 
have accuracy of 84% and 88% for Labeler 1 and 
accuracy of 78% and 79% for Labeler 2. This 
argues for bringing in additional techniques to 
29
enhance the SenseRelate approach in order to 
select from Top3 or Top5. 
Evaluation Labeler 1 Labeler 2 
Algorithm Accuracy 76% 68% 
Baseline Accuracy 69% 62% 
Top3 84% 78% 
Top5 88% 79% 
Table 3: Results for Training Set (n=326 terms) 
In contrast to Table 3 for the training set, Table 4 
shows results for the test set.  Labeler 1 shows an 
accuracy of 74% on the algorithm and 72% on the 
baseline; Labeler 2 has an accuracy of 73% on 
the algorithm and 69% on the baseline.  
Evaluation Labeler 1 Labeler 2 
Algorithm Accuracy 74% 73% 
Baseline Accuracy 72% 69% 
Top3 79% 79% 
Top5 81% 80% 
Table 4: Results for Test Set (n=275 terms) 
4.3 Results for Ambiguous Terms 
This section shows the results for the terms from 
the training set and the test set that required dis-
ambiguation.  Table 5 below shows that our algo-
rithm?s accuracy for Labeler 1 is 55% compared 
to the baseline accuracy of 35%. For Labeler 2, 
the algorithm accuracy is 48% compared to base-
line accuracy of 32%. This is significantly less 
than the overall accuracy of our algorithm.  Top3 
and Top5 evaluations have accuracy of 71% and 
82% for Labeler 1 and 71% and 75% for Labeler 
2.  
Evaluation Labeler 1 Labeler 2 
Algorithm Accuracy 55% 48% 
Baseline Accuracy 35% 32% 
Top3 71% 71% 
Top5 82% 75% 
Table 5: Ambiguous Terms for Training (n=128 terms) 
Similar results can be seen for the test set (96 
terms) in Table 6 below.  Labeler 1 shows an ac-
curacy of 50% on the algorithm and 42% on the 
baseline; Labeler 2 has an accuracy of 53% on 
the algorithm and 39% on the baseline.   
Evaluation Labeler 1 Labeler 2 
Algorithm Accuracy 50% 53% 
Baseline Accuracy 42% 39% 
Top3 63% 68% 
Top5 68% 71% 
Table 6: Results for Ambiguous Terms  
under the Test Set (n=96 terms) 
4.4 Analysis 
Table 7 shows that SenseRelate is used for most 
of the AAT mappings, and provides a breakdown 
based upon the disambiguation technique used.   
Row One in Table 7 shows how few terms were 
disambiguated using the lookup modifier tech-
nique, just 1 in the training set and 3 in the test 
set. 
Row Technique Training 
Set(n=128) 
Test  Set 
(n=96) 
One Lookup  
Modifier 
1 3 
Two SenseRelate 108 63 
Three Best Record 
Match 
14 12 
Four Most Common 
Sense 
5 18 
Table 7: Breakdown of AAT mappings  
by Disambiguation Technique 
Rows Two and Three show that most of the terms 
were disambiguated using the SenseRelate tech-
nique followed by the Best Record Match tech-
nique. The Most Common Sense technique (Row 
Four) accounted for the rest of the labelings.  
Table 8 gives insight into the errors of our algo-
rithm for the training set terms: 
Technique Reason for Error Error 
Count 
SenseRelate picked wrong 
WordNet sense 
16 
WordNet does not have the 
sense 
8 
Definitions did not overlap 11 
SenseRelate 
Other reasons 10 
Best Record 
Match 
 10 
Lookup 
Modifier 
 0 
Most Com-
mon Sense 
 3 
Table 8: Breakdown of the errors in our algorithm  
under training set (58 total errors) 
Table 8 shows the following: 
(1) Out of the total of 58 errors, 16 errors were 
caused because SenseRelate picked the wrong 
WordNet sense.  
(2) 8 errors were caused because WordNet did 
not  contain the sense of the word in which it was 
30
being used.  For example, consider the term work-
shop.  WordNet has two definitions of workshop: 
i. ?small workplace where handcrafts or manufac-
turing are done? and 
ii. ?a brief intensive course for a small group; em-
phasizes problem solving? 
but AAT has an additional definition that was 
referred by term workshop in the NGA text: 
?In the context of visual and decorative arts, refers 
to groups of artists or craftsmen collaborating to 
produce works, usually under a master's name? 
(3) 11 errors occurred because the AAT record 
definition and the WordNet sense definition did 
not overlap.  Consider the term figures in the sen-
tence, ?As with The Holy Family, the style of the 
figures offers no clear distinguishing characteris-
tic.?  Then examine the AAT and WordNet sense 
definitions below for figures: 
AAT sense: ?Representations of humans or ani-
mals? 
WordNet sense: ?a model of a bodily form (espe-
cially of a person)? 
These definitions do not have any words in com-
mon, but they discuss the same concept. 
(4) 10 errors occurred in the Best Record Match 
technique, 0 errors occurred under the Lookup 
Modifier Technique, and 3 errors occurred under 
the Most Common Sense technique. 
5. Conclusion  
We have shown that it is possible to create an 
automated program to perform word sense dis-
ambiguation in a field with specialized vocabu-
lary.   Such an application could have great poten-
tial in rapid development of metadata for digital 
collections.   Still, much work must be done in 
order to integrate our disambiguation program 
into the CLiMB Toolkit, including the following: 
(1) Our algorithm?s disambiguation accuracy is 
between 48-55% (Table 5 and Table 6), and so 
there is room for improvement in the algorithm.  
Currently we depend on an external program 
(SenseRelate) to perform much of the disam-
biguation (Table 7).  Furthermore, SenseRelate 
maps terms to WordNet and we then map the 
WordNet sense to an AAT sense.  This extra step 
is overhead, and it causes errors in our algorithm.  
We can either explore the option of re-
implementing concepts behind SenseRelate to 
directly map terms to the AAT, or we may need 
to find additional approaches to employ hybrid 
techniques (including machine learning) for dis-
ambiguation.  At the same time, we may benefit 
from the fact that WordNet, as a general resource, 
is domain independent and thus offers wider cov-
erage.  We will need to explore the trade-off in 
precision between different configurations using 
these different resources. 
(2) We need more and better groundtruth.  Our 
current data set of noun phrases includes term 
like favor, kind, and certain aspects.  These terms 
are unlikely to be used as meaningful subject 
terms by a cataloger and will never be mapped to 
AAT.  Thus, we need to develop reliable heuris-
tics to determine which noun phrases are poten-
tially high value subject index terms.  A simple 
frequency count does not achieve this purpose.  
Currently we are evaluating based on ground-
truth that our project members created.  Instead, 
we would like to extend the study to a wider set 
of image catalogers as labelers, since they will be 
the primary users of the CLiMB tool.  Image 
catalogers have experience in finding subject 
terms and mapping subject terms to the AAT.  
They can also help determine which terms are 
high quality subject terms.   
In contrast to working with the highly experi-
enced image cataloger, we also want to extend the 
study to include various groups with different 
user needs.  For example, journalists have ongo-
ing needs for images, and they tend to search by 
subject.  Using participants like these for markup 
and evaluation promises to provide comparative 
results, ones which will enable us to effectively 
reach a broad audience. 
We also would like to test our algorithm on 
more collections.  This will help us ascertain 
what kind of improvements or additions would 
make CLiMB a more general tool. 
6. Acknowledgements 
We thank Rachel Wadsworth and Carolyn Shef-
field.  We also acknowledge Philip Resnik for 
valuable discussion. 
31
7. References 
Baca, Murtha, ed. 2002. Introduction to art image 
access: issues, tools, standards, strategies. Getty 
Research Institute. 
Banerjee, S., and T. Pedersen. 2003. Extended 
gloss overlaps as a measure of semantic relat-
edness. In Proceedings of the Eighteenth Inter-
national Joint Conference on ArtificialIntelli-
gence, 805?810. 
Ferguson, Bobby and Sheila Intner. 1998. Subject 
Analysis: Blitz Cataloging Workbook. West-
port, CT:Libraries Unlimited Inc.  
Gale, W. A., K. W. Church, and D. Yarowsky. 
1992. Using bilingual materials to develop 
word sense disambiguation methods. In Pro-
ceedings of the Fourth International Confer-
ence on Theoretical and Methodological Issues 
in Machine Translation, 101-112, Montreal, 
Canada. 
Ide, Nancy M. and Jean Veronis. 1990.  Mapping 
Dictionaries: A Spreading Activation Ap-
proach. In Proceedings of the 6th Annual Con-
ference of the UW Centre for the New OED and 
Text Research, 52-64 Waterloo, Ontario. 
Lesk, Michael. 1986. Automatic Sense Disam-
biguation Using Machine Readable Dictionar-
ies: How to Tell a Pine Cone from an Ice 
Cream Cone. In Proceedings of ACM SIGDOC 
Conference, 24-26, Toronto, Canada. 
Klavans, Judith L. 2006. Computational Linguis-
tics for Metadata Building (CLiMB). In Pro-
cedings of the OntoImage Workshop, G. Gref-
fenstette, ed.  Language Resources and Evalua-
tion Conference (LREC), Genova, Italy. 
Klavans, Judith L. (in preparation). Using Com-
putational Linguistic Techniques and Thesauri 
for Enhancing Metadata Records in Image 
Search:  The CLiMB Project.  
Layne, Sara Shatford. 1994. Some issues in the 
indexing of images. Journal of the American 
Society for Information Science, 583-588. 
Palmer, Martha, Hwee Tou Ng, & Hoa Trang 
Dang. 2006. Evaluation of WSD Systems. 
Word Sense Disambiguation: Algorithms and 
Applications. Eneko Agirre and Philip Ed-
monds, ed. 75-106. Dordrecht, The Nether-
lands:Springer. 
Patwardhan, S., S. Banerjee, S. and T. Pedersen. 
2003. Using measures of semantic relatedness 
for word sense disambiguation. Proceedings of 
the Fourth International Conference on Intelli-
gent Text Processing and Computational Lin-
guistics, 241?257. 
Rasmussen, Edie. M. 1997. Indexing images. An-
nual Review of Information Science and Tech-
nology (ARIST), 32, 169-196. 
32
Proceedings of the Third Workshop on Issues in Teaching Computational Linguistics (TeachCL-08), pages 54?61,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Exploring Large-Data Issues in the Curriculum:
A Case Study with MapReduce
Jimmy Lin
The iSchool, College of Information Studies
Laboratory for Computational Linguistics and Information Processing
University of Maryland, College Park
jimmylin@umd.edu
Abstract
This paper describes the design of a pilot re-
search and educational effort at the Univer-
sity of Maryland centered around technologies
for tackling Web-scale problems. In the con-
text of a ?cloud computing? initiative lead by
Google and IBM, students and researchers are
provided access to a computer cluster running
Hadoop, an open-source Java implementation
of Google?s MapReduce framework. This
technology provides an opportunity for stu-
dents to explore large-data issues in the con-
text of a course organized around teams of
graduate and undergraduate students, in which
they tackle open research problems in the hu-
man language technologies. This design rep-
resents one attempt to bridge traditional in-
struction with real-world, large-data research
challenges.
1 Introduction
Over the past couple of decades, the field of compu-
tational linguistics, and more broadly, human lan-
guage technologies, has seen the emergence and
later dominance of empirical techniques and data-
driven research. Concomitant with this trend is the
requirement of systems and algorithms to handle
large quantities of data. Banko and Brill (2001)
were among the first to demonstrate the importance
of dataset size as a significant factor governing pre-
diction accuracy in a supervised machine learning
task. In fact, they argue that size of training set
is perhaps more important than the choice of ma-
chine learning algorithm itself. Similarly, exper-
iments in question answering have shown the ef-
fectiveness of simple pattern-matching techniques
when applied to large quantities of data (Brill et
al., 2001). More recently, this line of argumenta-
tion has been echoed in experiments with large-scale
language models. Brants et al (2007) show that
for statistical machine translation, a simple smooth-
ing method (dubbed Stupid Backoff) approaches the
quality of Kneser-Ney Smoothing as the amount of
training data increases, and with the simple method
one can process significantly more data.
Given these observations, it is important to in-
tegrate discussions of large-data issues into any
course on human language technology. Most ex-
isting courses focus on smaller-sized problems and
datasets that can be processed on students? personal
computers, making them ill-prepared to cope with
the vast quantities of data in operational environ-
ments. Even when larger datasets are leveraged in
the classroom, they are mostly used as static re-
sources. Thus, students experience a disconnect as
they transition from a learning environment to one
where they work on real-world problems.
Nevertheless, there are at least two major chal-
lenges associated with explicit treatment of large-
data issues in an HLT curriculum:
? The first concerns resources: it is unclear where
one might acquire the hardware to support ed-
ucational activities, especially if such activities
are in direct competition with research.
? The second involves complexities inherently
associated with parallel and distributed pro-
cessing, currently the only practical solution to
large-data problems. For any course, it is diffi-
54
cult to retain focus on HLT-relevant problems,
since the exploration of large-data issues ne-
cessitates (time-consuming) forays into parallel
and distributed computing.
This paper presents a case study that grapples
with the issues outlined above. Building on previ-
ous experience with similar courses at the Univer-
sity of Washington (Kimball et al, 2008), I present
a pilot ?cloud computing? course currently under-
way at the University of Maryland that leverages a
collaboration with Google and IBM, through which
students are given access to hardware resources. To
further alleviate the first issue, research is brought
into alignment with education by structuring a team-
oriented, project-focused course. The core idea is to
organize teams of graduate and undergraduate stu-
dents focused on tackling open research problems in
natural language processing, information retrieval,
and related areas. Ph.D. students serve as leaders
on projects related to their research, and are given
the opportunity to serve as mentors to undergradu-
ate and masters students.
Google?s MapReduce programming framework is
an elegant solution to the second issue raised above.
By providing a functional abstraction that isolates
the programmer from parallel and distributed pro-
cessing issues, students can focus on solving the
actual problem. I first provide the context for this
academic?industrial collaboration, and then move
on to describe the course setup.
2 Cloud Computing and MapReduce
In October 2007, Google and IBM jointly an-
nounced the Academic Cloud Computing Initiative,
with the goal of helping both researchers and stu-
dents address the challenges of ?Web-scale? com-
puting. The initiative revolves around Google?s
MapReduce programming paradigm (Dean and
Ghemawat, 2004), which represents a proven ap-
proach to tackling data-intensive problems in a dis-
tributed manner. Six universities were involved
in the collaboration at the outset: Carnegie Mellon
University, Massachusetts Institute of Technology,
Stanford University, the University of California at
Berkeley, the University of Maryland, and Univer-
sity of Washington. I am the lead faculty at the Uni-
versity of Maryland on this project.
As part of this initiative, IBM and Google have
dedicated a large cluster of several hundred ma-
chines for use by faculty and students at the partic-
ipating institutions. The cluster takes advantage of
Hadoop, an open-source implementation of MapRe-
duce in Java.1 By making these resources available,
Google and IBM hope to encourage faculty adop-
tion of cloud computing in their research and also
integration of the technology into the curriculum.
MapReduce builds on the observation that many
information processing tasks have the same basic
structure: a computation is applied over a large num-
ber of records (e.g., Web pages) to generate par-
tial results, which are then aggregated in some fash-
ion. Naturally, the per-record computation and ag-
gregation function vary according to task, but the ba-
sic structure remains fixed. Taking inspiration from
higher-order functions in functional programming,
MapReduce provides an abstraction at the point of
these two operations. Specifically, the programmer
defines a ?mapper? and a ?reducer? with the follow-
ing signatures:
map: (k1, v1)? [(k2, v2)]
reduce: (k2, [v2])? [(k3, v3)]
Key/value pairs form the basic data structure in
MapReduce. The mapper is applied to every input
key/value pair to generate an arbitrary number of in-
termediate key/value pairs. The reducer is applied to
all values associated with the same intermediate key
to generate output key/value pairs. This two-stage
processing structure is illustrated in Figure 1.
Under the framework, a programmer need only
provide implementations of the mapper and reducer.
On top of a distributed file system (Ghemawat et al,
2003), the runtime transparently handles all other
aspects of execution, on clusters ranging from a
few to a few thousand nodes. The runtime is re-
sponsible for scheduling map and reduce workers
on commodity hardware assumed to be unreliable,
and thus is tolerant to various faults through a num-
ber of error recovery mechanisms. The runtime also
manages data distribution, including splitting the in-
put across multiple map workers and the potentially
very large sorting problem between the map and re-
duce phases whereby intermediate key/value pairs
must be grouped by key.
1http://hadoop.apache.org/
55
inp
ut
inp
ut
inp
ut
inp
ut
ma
p
ma
p
ma
p
ma
p
inp
ut
inp
ut
inp
ut
inp
ut
Bar
rier
:?gr
oup
?val
ues
?by
?key
s
red
uce
red
uce
red
uce
out
put
out
put
out
put
Figure 1: Illustration of the MapReduce framework: the
?mapper? is applied to all input records, which generates
results that are aggregated by the ?reducer?.
The biggest advantage of MapReduce from a ped-
agogical point of view is that it allows an HLT
course to retain its focus on applications. Divide-
and-conquer algorithms running on multiple ma-
chines are currently the only effective strategy for
tackling Web-scale problems. However, program-
ming parallel and distributed systems is a difficult
topic for students to master. Due to communica-
tion and synchronization issues, concurrent opera-
tions are notoriously challenging to reason about?
unanticipated race conditions are hard to detect and
even harder to debug. MapReduce allows the pro-
grammer to offload these problems (no doubt im-
portant, but irrelevant from the perspective of HLT)
onto the runtime, which handles the complexities as-
sociated with distributed processing on large clus-
ters. The functional abstraction allows a student to
focus on problem solving, not managing the details
of error recovery, data distribution, etc.
3 Course Design
This paper describes a ?cloud computing? course at
the University of Maryland being offered in Spring
2008. The core idea is to assemble small teams of
graduate and undergraduate students to tackle re-
search problems, primarily in the areas of informa-
tion retrieval and natural language processing. Ph.D.
students serve as team leaders, overseeing small
groups of masters and undergraduates on topics re-
lated to their doctoral research. The roles of ?team
leader? and ?team member? are explicitly assigned
at the beginning of the semester, and are associated
with different expectations and responsibilities. All
course material and additional details are available
on the course homepage.2
3.1 Objectives and Goals
I identified a list of desired competencies for stu-
dents to acquire and refine throughout the course:
? Understand and be able to articulate the chal-
lenges associated with distributed solutions to
large-scale problems, e.g., scheduling, load
balancing, fault tolerance, memory and band-
width limitations, etc.
? Understand and be able to explain the concepts
behind MapReduce as one framework for ad-
dressing the above issues.
? Understand and be able to express well-known
algorithms (e.g., PageRank) in the MapReduce
framework.
? Understand and be able to reason about engi-
neering tradeoffs in alternative approaches to
processing large datasets.
? Gain in-depth experience with one research
problem in Web-scale information processing
(broadly defined).
With respect to the final bullet point, the students
are expected to acquire the following abilities:
? Understand how current solutions to the par-
ticular research problem can be cast into the
MapReduce framework.
? Be able to explain what advantages the MapRe-
duce framework provides over existing ap-
proaches (or disadvantages if a MapReduce
formulation turns out to be unsuitable for ex-
pressing the problem).
? Articulate how adopting the MapReduce
framework can potentially lead to advances in
the state of the art by enabling processing not
possible before.
I assumed that all students have a strong foun-
dation in computer science, which was operational-
ized in having completed basic courses in algo-
rithms, data structures, and programming languages
2http://www.umiacs.umd.edu/?jimmylin/cloud-computing/
56
We
ek
Mo
nda
y
We
dne
sda
y
1 2 3
Ha
do
op
 Bo
ot 
Ca
mp
3 4 5 6
Pro
jec
t M
ee
ting
s:
Ph
I
Pro
po
sal
 Pr
ese
nta
tion
s
6 7 8 9
Ph
ase
 I
G
tS
k
10 11 12
Pro
jec
t M
ee
ting
s:
Ph
ase
 II
Gu
est
 Sp
ea
ker
s
13 14 15
Fin
al P
roj
ect
Pre
sen
tat
ion
s
Figure 2: Overview of course schedule.
(in practice, this was trivially met for the graduate
students, who all had undergraduate degrees in com-
puter science). I explicitly made the decision that
previous courses in parallel programming, systems,
or networks was not required. Finally, prior experi-
ence with natural language processing, information
retrieval, or related areas was not assumed. How-
ever, strong competency in Java programming was a
strict requirement, as the Hadoop implementation of
MapReduce is based in Java.
In the project-oriented setup, the team leaders
(i.e., Ph.D. students) have additional roles to play.
One of the goals of the course is to give them experi-
ence in mentoring more junior colleagues and man-
aging a team project. As such, they were expected to
acquire real-world skills in project organization and
management.
3.2 Schedule and Major Components
As designed, the course spans a standard fifteen
week semester, meeting twice a week (Monday and
Wednesday) for one hour and fifteen minutes each
session. The general setup is shown in Figure 2. As
this paper goes to press (mid-April), the course just
concluded Week 11.
During the first three weeks, all students are im-
mersed in a ?Hadoop boot camp?, where they are
introduced to the MapReduce programming frame-
work. Material was adapted from slides developed
by Christophe Bisciglia and his colleagues from
Google, who have delivered similar content in var-
ious formats.3 As it was assumed that all students
had strong foundations in computer science, the
pace of the lectures was brisk. The themes of the
five boot camp sessions are listed below:
? Introduction to parallel/distributed processing
? From functional programming to MapReduce
and the Google File System (GFS)
? ?Hello World? MapReduce lab
? Graph algorithms with MapReduce
? Information retrieval with MapReduce
A brief overview of parallel and distributed pro-
cessing provides a natural transition into abstrac-
tions afforded by functional programming, the inspi-
ration behind MapReduce. That in turn provides the
context to introduce MapReduce itself, along with
the distributed file system upon which it depends.
The final two lectures focus on specific case stud-
ies of MapReduce applied to graph analysis and in-
formation retrieval. The first covers graph search
and PageRank, while the second covers algorithms
for information retrieval. With the exception of the
?Hello World? lab session, all lecture content was
delivered at the conceptual level, without specific
reference to the Hadoop API and implementation
details (see Section 5 for discussion). The boot
camp is capped off with a programming exercise
(implementation of PageRank) to ensure that stu-
dents have a passing knowledge of MapReduce con-
cepts in general and the Hadoop API in particular.
Concurrent with the boot camp, team leaders are
expected to develop a detailed plan of research:
what they hope to accomplish, specific tasks that
would lead to the goals, and possible distribution of
those tasks across team members. I recommend that
each project be structured into two phases: the first
phase focusing on how existing solutions might be
recast into the MapReduce framework, the second
phase focusing on interesting extensions enabled by
MapReduce. In addition to the detailed research
3http://code.google.com/edu/parallel/
57
plan, the leaders are responsible for organizing intro-
ductory material (papers, tutorials, etc.) since team
members are not expected to have any prior experi-
ence with the research topic.
The majority of the course is taken up by the re-
search project itself. The Monday class sessions
are devoted to the team project meetings, and the
team leader is given discretion on how this is man-
aged. Typical activities include evaluation of deliv-
erables (code, experimental results, etc.) from the
previous week and discussions of plans for the up-
coming week, but other common uses of the meeting
time include whiteboard sessions and code review.
During the project meetings I circulate from group
to group to track progress, offer helpful suggestions,
and contribute substantially if possible.
To the extent practical, the teams adopt standard
best practices for software development. Students
use Eclipse as the development environment and
take advantage of a plug-in that provides a seamless
interface to the Hadoop cluster. Code is shared via
Subversion, with both project-specific repositories
and a course-wide repository for common libraries.
A wiki is also provided as a point of collaboration.
Concurrent with the project meetings on Mon-
days, a speaker series takes place on Wednesdays.
Attendance for students is required, but otherwise
the talks are open to the public. One of the goals
for these invited talks is to build an active commu-
nity of researchers interested in large datasets and
distributed processing. Invited talks can be clas-
sified into one of two types: infrastructure-focused
and application-focused. Examples of the first in-
clude alternative architectures for processing large
datasets and dynamic provisioning of computing
services. Examples of the second include survey
of distributed data mining techniques and Web-scale
sentiment analysis. It is not a requirement for the
talks to focus on MapReduce per se?rather, an em-
phasis on large-data issues is the thread that weaves
all these presentations together.
3.3 Student Evaluation
At the beginning of the course, students are assigned
specific roles (team leader or team member) and
are evaluated according to different criteria (both in
grade components and relative weights).
The team leaders are responsible for producing
the detailed research plan at the beginning of the
semester. The entire team is responsible for three
checkpoint deliverables throughout the course: an
initial oral presentation outlining their plans, a short
interim progress report at roughly the midpoint of
the semester, and a final oral presentation accompa-
nied by a written report at the end of the semester.
On a weekly basis, I request from each stu-
dent a status report delivered as a concise email: a
paragraph-length outline of progress from the previ-
ous week and plans for the following week. This,
coupled with my observations during each project
meeting, provides the basis for continuous evalua-
tion of student performance.
4 Course Implementation
Currently, 13 students (7 Ph.D., 3 masters, 3 under-
graduates) are involved in the course, working on
six different projects. Last fall, as planning was
underway, Ph.D. students from the Laboratory for
Computational Linguistics and Information Process-
ing at the University of Maryland were recruited
as team leaders. Three of them agreed, developing
projects around their doctoral research?these repre-
sent cases with maximal alignment of research and
educational goals. In addition, the availability of this
opportunity was announced on mailing lists, which
generated substantial interest. Undergraduates were
recruited from the Computer Science honors pro-
gram; since it is a requirement for those students to
complete an honors project, this course provided a
suitable vehicle for satisfying that requirement.
Three elements are necessary for a successful
project: interested students, an interesting research
problem of appropriate scope, and the availability
of data to support the work. I served as a broker
for all three elements, and eventually settled on five
projects that satisfied all the desiderata (one project
was a later addition). As there was more interest
than spaces available for team members, it was pos-
sible to screen for suitable background and matching
interests. The six ongoing projects are as follows:
? Large-data statistical machine translation
? Construction of large latent-variable language
models
? Resolution of name mentions in large email
archives
58
? Network analysis for enhancing biomedical
text retrieval
? Text-background separation in children?s pic-
ture books
? High-throughput biological sequence align-
ment and processing
Of the six projects, four of them fall squarely in
the area of human language technology: the first two
are typical of problems in natural language process-
ing, while the second two are problems in informa-
tion retrieval. The final two projects represent at-
tempts to push the boundaries of the MapReduce
paradigm, into image processing and computational
biology, respectively. Short project descriptions can
be found on the course homepage.
5 Pedagogical Discussion
The design of any course is an exercise in tradeoffs,
and this pilot project is no exception. In this section,
I will attempt to justify course design decisions and
discuss possible alternatives.
At the outset, I explicitly decided against a ?tradi-
tional? course format that would involve carefully-
paced delivery of content with structured exercises
(e.g., problem sets or labs). Such a design would
perhaps be capped off with a multi-week final
project. The pioneering MapReduce course at the
University of Washington represents an example of
this design (Kimball et al, 2008), combining six
weeks of standard classroom instruction with an op-
tional four week final project. As an alternative, I or-
ganized my course around the research project. This
choice meant that the time devoted to direct instruc-
tion on foundational concepts was very limited, i.e.,
the three-week boot camp.
One consequence of the boot-camp setup is some
disconnect between the lecture material and imple-
mentation details. Students were expected to rapidly
translate high-level concepts into low-level pro-
gramming constructs and API calls without much
guidance. There was only one ?hands on? session
in the boot camp, focusing on more mundane is-
sues such as installation, configuration, connecting
to the server, etc. Although that session also in-
cluded overview of a simple Hadoop program, that
by no means was sufficient to yield in-depth under-
standing of the framework.
The intensity of the boot camp was mitigated by
the composition of the students. Since students were
self-selected and further screened by me in terms of
their computational background, they represent the
highest caliber of students at the university. Further-
more, due to the novel nature of the material, stu-
dents were highly motivated to rapidly acquire what-
ever knowledge was necessary outside the class-
room. In reality, the course design forced students
to spend the first few weeks of the project simulta-
neously learning about the research problem and the
details of the Hadoop framework. However, this did
not appear to be a problem.
Another interesting design choice is the mixing
of students with different backgrounds in the same
classroom environment. Obviously, the graduate
students had stronger computer science backgrounds
than the undergraduates overall, and the team lead-
ers had far more experience on the particular re-
search problem than everyone else by design. How-
ever, this was less an issue than one would have ini-
tially thought, partially due to the selection of the
students. Since MapReduce requires a different ap-
proach to problem solving, significant learning was
required from everyone, independent of prior expe-
rience. In fact, prior knowledge of existing solutions
may in some cases be limiting, since it precludes a
fresh approach to the problem.
6 Course Evaluation
Has the course succeeded? Before this question can
be meaningfully answered, one needs to define mea-
sures for quantifying success. Note that the evalua-
tion of the course is distinct from the evaluation of
student performance (covered in Section 3.3). Given
the explicit goal of integrating research and educa-
tion, I propose the following evaluation criteria:
? Significance of research findings, as measured
by the number of publications that arise directly
or indirectly from this project.
? Placement of students, e.g., internships and
permanent positions, or admission to graduate
programs (for undergraduates).
? Number of projects with sustained research ac-
tivities after the conclusion of the course.
59
? Amount of additional research support from
other funding agencies (NSF, DARPA, etc.)
for which the projects provided preliminary re-
sults.
Here I provide an interim assessment, as this pa-
per goes to press in mid-April. Preliminary results
from the projects have already yielded two sepa-
rate publications: one on statistical machine trans-
lation (Dyer et al, 2008), the other on information
retrieval (Elsayed et al, 2008). In terms of student
placement, I believe that experience from this course
has made several students highly attractive to com-
panies such as Google, Yahoo, and Amazon?both
for permanent positions and summer internships. It
is far too early to have measurable results with re-
spect to the final two criteria, but otherwise prelim-
inary assessment appears to support the overall suc-
cess of this course.
In addition to the above discussion, it is also worth
mentioning that the course is emerging as a nexus
of cloud computing on the Maryland campus (and
beyond), serving to connect multiple organizations
that share in having large-data problems. Already,
the students are drawn from a variety of academic
units on campus:
? The iSchool
? Department of Computer Science
? Department of Linguistics
? Department of Geography
And cross-cut multiple research labs:
? The Institute for Advanced Computer Studies
? The Laboratory for Computational Linguistics
and Information Processing
? The Human-Computer Interaction Laboratory
? The Center for Bioinformatics and Computa-
tional Biology
Off campus, there are ongoing collaborations
with the National Center for Biotechnology In-
formation (NCBI) within the National Library of
Medicine (NLM). Other information-based organi-
zations around the Washington, D.C. area have also
expressed interest in cloud computing technology.
7 Conclusion
This paper describes the design of an integrated re-
search and educational initiative focused on tackling
Web-scale problems in natural language processing
and information retrieval using MapReduce. Pre-
liminary assessment indicates that this project rep-
resents one viable approach to bridging classroom
instruction and real-world research challenges. With
the advent of clusters composed of commodity ma-
chines and ?rent-a-cluster? services such as Ama-
zon?s EC2,4 I believe that large-data issues can be
practically incorporated into an HLT curriculum at a
reasonable cost.
Acknowledgments
I would like to thank the generous hardware sup-
port of IBM and Google via the Academic Cloud
Computing Initiative. Specifically, thanks go out
to Dennis Quan and Eugene Hung from IBM for
their tireless support of our efforts. This course
would not have been possible without the participa-
tion of 13 enthusiastic, dedicated students, for which
I feel blessed to have the opportunity to work with.
In alphabetical order, they are: Christiam Camacho,
George Caragea, Aaron Cordova, Chris Dyer, Tamer
Elsayed, Denis Filimonov, Chang Hu, Greg Jablon-
ski, Alan Jackoway, Punit Mehta, Alexander Mont,
Michael Schatz, and Hua Wei. Finally, I would like
to thank Esther and Kiri for their kind support.
References
Michele Banko and Eric Brill. 2001. Scaling to very very
large corpora for natural language disambiguation. In
Proceedings of the 39th Annual Meeting of the As-
sociation for Computational Linguistics (ACL 2001),
pages 26?33, Toulouse, France.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och,
and Jeffrey Dean. 2007. Large language models in
machine translation. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 858?867, Prague, Czech Re-
public.
Eric Brill, Jimmy Lin, Michele Banko, Susan Dumais,
and Andrew Ng. 2001. Data-intensive question an-
swering. In Proceedings of the Tenth Text REtrieval
4http://aws.amazon.com/ec2
60
Conference (TREC 2001), pages 393?400, Gaithers-
burg, Maryland.
Jeffrey Dean and Sanjay Ghemawat. 2004. MapReduce:
Simplified data processing on large clusters. In Pro-
ceedings of the 6th Symposium on Operating System
Design and Implementation (OSDI 2004), pages 137?
150, San Francisco, California.
Chris Dyer, Aaron Cordova, Alex Mont, and Jimmy Lin.
2008. Fast, easy, and cheap: Construction of statistical
machine translation models with MapReduce. In Pro-
ceedings of the Third Workshop on Statistical Machine
Translation at ACL 2008, Columbus, Ohio.
Tamer Elsayed, Jimmy Lin, and Douglas Oard. 2008.
Pairwise document similarity in large collections with
MapReduce. In Proceedings of the 46th Annual Meet-
ing of the Association for Computational Linguistics
(ACL 2008), Companion Volume, Columbus, Ohio.
Sanjay Ghemawat, Howard Gobioff, and Shun-Tak Le-
ung. 2003. The Google File System. In Proceedings
of the 19th ACM Symposium on Operating Systems
Principles (SOSP-03), pages 29?43, Bolton Landing,
New York.
Aaron Kimball, Sierra Michels-Slettvet, and Christophe
Bisciglia. 2008. Cluster computing for Web-scale
data processing. In Proceedings of the 39th ACM
Technical Symposium on Computer Science Education
(SIGCSE 2008), pages 116?120, Portland, Oregon.
61
Proceedings of the Third Workshop on Statistical Machine Translation, pages 199?207,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Fast, Easy, and Cheap: Construction of
Statistical Machine Translation Models with MapReduce
Christopher Dyer, Aaron Cordova, Alex Mont, Jimmy Lin
Laboratory for Computational Linguistics and Information Processing
University of Maryland
College Park, MD 20742, USA
redpony@umd.edu
Abstract
In recent years, the quantity of parallel train-
ing data available for statistical machine trans-
lation has increased far more rapidly than
the performance of individual computers, re-
sulting in a potentially serious impediment
to progress. Parallelization of the model-
building algorithms that process this data on
computer clusters is fraught with challenges
such as synchronization, data exchange, and
fault tolerance. However, the MapReduce
programming paradigm has recently emerged
as one solution to these issues: a powerful
functional abstraction hides system-level de-
tails from the researcher, allowing programs to
be transparently distributed across potentially
very large clusters of commodity hardware.
We describe MapReduce implementations of
two algorithms used to estimate the parame-
ters for two word alignment models and one
phrase-based translation model, all of which
rely on maximum likelihood probability esti-
mates. On a 20-machine cluster, experimental
results show that our solutions exhibit good
scaling characteristics compared to a hypo-
thetical, optimally-parallelized version of cur-
rent state-of-the-art single-core tools.
1 Introduction
Like many other NLP problems, output quality of
statistical machine translation (SMT) systems in-
creases with the amount of training data. Brants et
al. (2007) demonstrated that increasing the quantity
of training data used for language modeling signifi-
cantly improves the translation quality of an Arabic-
English MT system, even with far less sophisticated
backoff models. However, the steadily increas-
ing quantities of training data do not come with-
out cost. Figure 1 shows the relationship between
the amount of parallel Arabic-English training data
used and both the translation quality of a state-of-
the-art phrase-based SMT system and the time re-
quired to perform the training with the widely-used
Moses toolkit on a commodity server.1 Building
a model using 5M sentence pairs (the amount of
Arabic-English parallel text publicly available from
the LDC) takes just over two days.2 This represents
an unfortunate state of affairs for the research com-
munity: excessively long turnaround on experiments
is an impediment to research progress.
It is clear that the needs of machine translation re-
searchers have outgrown the capabilities of individ-
ual computers. The only practical recourse is to dis-
tribute the computation across multiple cores, pro-
cessors, or machines. The development of parallel
algorithms involves a number of tradeoffs. First is
that of cost: a decision must be made between ?ex-
otic? hardware (e.g., large shared memory machines,
InfiniBand interconnect) and commodity hardware.
There is significant evidence (Barroso et al, 2003)
that solutions based on the latter are more cost ef-
fective (and for resource-constrained academic in-
stitutions, often the only option).
Given appropriate hardware, MT researchers
must still contend with the challenge of developing
software. Quite simply, parallel programming is dif-
ficult. Due to communication and synchronization
1http://www.statmt.org/moses/
2All single-core timings reported in this paper were per-
formed on a 3GHz 64-bit Intel Xeon server with 8GB memory.
199
15 min
30 min45 min
1.5 hrs
3 hrs
6 hrs
12 hrs
1 day
2 days
 10000  100000  1e+06  1e+07 0.3
 0.35
 0.4
 0.45
 0.5
 0.55
 0.6
Tim
e (se
cond
s)
Tran
slati
on q
ualit
y (BL
EU)
Corpus size (sentences)
Training timeTranslation quality
Figure 1: Translation quality and training time as a func-
tion of corpus size.
issues, concurrent operations are notoriously chal-
lenging to reason about. In addition, fault tolerance
and scalability are serious concerns on commodity
hardware prone to failure. With traditional paral-
lel programming models (e.g., MPI), the developer
shoulders the burden of handling these issues. As a
result, just as much (if not more) effort is devoted to
system issues as to solving the actual problem.
Recently, Google?s MapReduce framework (Dean
and Ghemawat, 2004) has emerged as an attractive
alternative to existing parallel programming models.
The MapReduce abstraction shields the programmer
from having to explicitly worry about system-level
issues such as synchronization, data exchange, and
fault tolerance (see Section 2 for details). The run-
time is able to transparently distribute computations
across large clusters of commodity hardware with
good scaling characteristics. This frees the program-
mer to focus on actual MT issues.
In this paper we present MapReduce implementa-
tions of training algorithms for two kinds of models
commonly used in statistical MT today: a phrase-
based translation model (Koehn et al, 2003) and
word alignment models based on pairwise lexi-
cal translation trained using expectation maximiza-
tion (Dempster et al, 1977). Currently, such models
take days to construct using standard tools with pub-
licly available training corpora; our MapReduce im-
plementation cuts this time to hours. As an benefit
to the community, it is our intention to release this
code under an open source license.
It is worthwhile to emphasize that we present
these results as a ?sweet spot? in the complex design
space of engineering decisions. In light of possible
tradeoffs, we argue that our solution can be consid-
ered fast (in terms of running time), easy (in terms
of implementation), and cheap (in terms of hard-
ware costs). Faster running times could be achieved
with more expensive hardware. Similarly, a custom
implementation (e.g., in MPI) could extract finer-
grained parallelism and also yield faster running
times. In our opinion, these are not worthwhile
tradeoffs. In the first case, financial constraints
are obvious. In the second case, the programmer
must explicitly manage all the complexities that
come with distributed processing (see above). In
contrast, our algorithms were developed within a
matter of weeks, as part of a ?cloud computing?
course project (Lin, 2008). Experimental results
demonstrate that MapReduce provides nearly opti-
mal scaling characteristics, while retaining a high-
level problem-focused abstraction.
The remainder of the paper is structured as fol-
lows. In the next section we provide an overview of
MapReduce. In Section 3 we describe several gen-
eral solutions to computing maximum likelihood es-
timates for finite, discrete probability distributions.
Sections 4 and 5 apply these techniques to estimate
phrase translation models and perform EM for two
word alignment models. Section 6 reviews relevant
prior work, and Section 7 concludes.
2 MapReduce
MapReduce builds on the observation that many
tasks have the same basic structure: a computation is
applied over a large number of records (e.g., parallel
sentences) to generate partial results, which are then
aggregated in some fashion. The per-record compu-
tation and aggregation function are specified by the
programmer and vary according to task, but the ba-
sic structure remains fixed. Taking inspiration from
higher-order functions in functional programming,
MapReduce provides an abstraction at the point of
these two operations. Specifically, the programmer
defines a ?mapper? and a ?reducer? with the follow-
ing signatures (square brackets indicate a list of ele-
ments):
map: ?k1, v1? ? [?k2, v2?]
reduce: ?k2, [v2]? ? [?k3, v3?]
200
inp
ut
inp
ut
inp
ut
inp
ut
ma
p
ma
p
ma
p
ma
p
inp
ut
inp
ut
inp
ut
inp
ut
Bar
rier
:?gr
oup
?val
ues
?by
?key
s
red
uce
red
uce
red
uce
out
put
out
put
out
put
Figure 2: Illustration of the MapReduce framework: the
?mapper? is applied to all input records, which generates
results that are aggregated by the ?reducer?.
Key/value pairs form the basic data structure in
MapReduce. The ?mapper? is applied to every input
key/value pair to generate an arbitrary number of in-
termediate key/value pairs. The ?reducer? is applied
to all values associated with the same intermediate
key to generate output key/value pairs. This two-
stage processing structure is illustrated in Figure 2.
Under this framework, a programmer need only
provide implementations of map and reduce. On top
of a distributed file system (Ghemawat et al, 2003),
the runtime transparently handles all other aspects
of execution, on clusters ranging from a few to a few
thousand workers on commodity hardware assumed
to be unreliable, and thus is tolerant to various faults
through a number of error recovery mechanisms.
The runtime also manages data exchange, includ-
ing splitting the input across multiple map workers
and the potentially very large sorting problem be-
tween the map and reduce phases whereby interme-
diate key/value pairs must be grouped by key.
For the MapReduce experiments reported in this
paper, we used Hadoop version 0.16.0,3 which is
an open-source Java implementation of MapRe-
duce, running on a 20-machine cluster (1 master,
19 slaves). Each machine has two processors (run-
ning at either 2.4GHz or 2.8GHz), 4GB memory
(map and reduce tasks were limited to 768MB), and
100GB disk. All software was implemented in Java.
3http://hadoop.apache.org/
Method 1
Map1 ?A,B? ? ??A,B?, 1?
Reduce1 ??A,B?, c(A,B)?
Map2 ??A,B?, c(A,B)? ? ??A,? ?, c(A,B)?
Reduce2 ??A,? ?, c(A)?
Map3 ??A,B?, c(A,B)? ? ?A, ?B, c(A,B)??
Reduce3 ?A, ?B,
c(A,B)
c(A) ??
Method 2
Map1 ?A,B? ? ??A,B?, 1?; ??A,? ?, 1?
Reduce1 ??A,B?,
c(A,B)
c(A) ?
Method 3
Map1 ?A,Bi? ? ?A, ?Bi : 1??
Reduce1 ?A, ?B1 :
c(A,B1)
c(A) ?, ?B2 :
c(A,B2)
c(A) ? ? ? ? ?
Table 1: Three methods for computing PMLE(B|A).
The first element in each tuple is a key and the second
element is the associated value produced by the mappers
and reducers.
3 Maximum Likelihood Estimates
The two classes of models under consideration are
parameterized with conditional probability distribu-
tions over discrete events, generally estimated ac-
cording to the maximum likelihood criterion:
PMLE(B|A) =
c(A,B)
c(A)
=
c(A,B)
?
B? c(A,B
?)
(1)
Since this calculation is fundamental to both ap-
proaches (they distinguish themselves only by where
the counts of the joint events come from?in the case
of the phrase model, they are observed directly, and
in the case of the word-alignment models they are
the number of expected events in a partially hidden
process given an existing model of that process), we
begin with an overview of how to compute condi-
tional probabilities in MapReduce.
We consider three possible solutions to this prob-
lem, shown in Table 1. Method 1 computes the count
for each pair ?A,B?, computes the marginal c(A),
and then groups all the values for a given A together,
such that the marginal is guaranteed to be first and
then the pair counts follow. This enables Reducer3
to only hold the marginal value in memory as it pro-
cesses the remaining values. Method 2 works simi-
larly, except that the original mapper emits two val-
ues for each pair ?A,B? that is encountered: one that
201
will be the marginal and one that contributes to the
pair count. The reducer groups all pairs together by
the A value, processes the marginal first, and, like
Method 1, must only keep this value in memory as
it processes the remaining pair counts. Method 2 re-
quires more data to be processed by the MapReduce
framework, but only requires a single sort operation
(i.e., fewer MapReduce iterations).
Method 3 works slightly differently: rather than
computing the pair counts independently of each
other, the counts of all the B events jointly occurring
with a particular A = a event are stored in an asso-
ciative data structure in memory in the reducer. The
marginal c(A) can be computed by summing over
all the values in the associative data structure and
then a second pass normalizes. This requires that
the conditional distribution P (B|A = a) not have
so many parameters that it cannot be represented
in memory. A potential advantage of this approach
is that the MapReduce framework can use a ?com-
biner? to group many ?A,B? pairs into a single value
before the key/value pair leaves for the reducer.4 If
the underlying distribution from which pairs ?A,B?
has certain characteristics, this can result in a signifi-
cant reduction in the number of keys that the mapper
emits (although the number of statistics will be iden-
tical). And since all keys must be sorted prior to the
reducer step beginning, reducing the number of keys
can have significant performance impact.
The graph in Figure 3 shows the performance
of the three problem decompositions on two model
types we are estimating, conditional phrase trans-
lation probabilities (1.5M sentences, max phrase
length=7), and conditional lexical translation prob-
abilities as found in a word alignment model (500k
sentences). In both cases, Method 3, which makes
use of more memory to store counts of all B events
associated with event A = a, completes at least 50%
more quickly. This efficiency is due to the Zipfian
distribution of both phrases and lexical items in our
corpora: a few frequent items account for a large
portion of the corpus. The memory requirements
were also observed to be quite reasonable for the
4Combiners operate like reducers, except they run directly
on the output of a mapper before the results leave memory.
They can be used when the reduction operation is associative
and commutative. For more information refer to Dean and Ghe-
mawat (2004).
 0
 200
 400
 600
 800
 1000
 1200
 1400
 1600
Met
hod
 1
Met
hod
 2
Mat
hod
 3
Tim
e (se
cond
s)
Estimation method
Phrase pairsWord pairs
Figure 3: PMLE computation strategies.
Figure 4: A word-aligned sentence. Examples
of consistent phrase pairs include ?vi, i saw?,
?la mesa pequen?a, the small table?, and
?mesa pequen?a, small table?; but, note that, for
example, it is not possible to extract a consistent phrase
corresponding to the foreign string la mesa or the English
string the small.
models in question: representing P (B|A = a) in the
phrase model required at most 90k parameters, and
in the lexical model, 128k parameters (i.e., the size
of the vocabulary for language B). For the remainder
of the experiments reported, we confine ourselves to
the use of Method 3.
4 Phrase-Based Translation
In phrase-based translation, the translation process
is modeled by splitting the source sentence into
phrases (a contiguous string of words) and translat-
ing the phrases as a unit (Och et al, 1999; Koehn
et al, 2003). Phrases are extracted from a word-
aligned parallel sentence according to the strategy
proposed by Och et al (1999), where every word in
a phrase is aligned only to other words in the phrase,
and not to any words outside the phrase bounds. Fig-
ure 4 shows an example aligned sentence and some
of the consistent subphrases that may be extracted.
202
1.5 min
5 min
20 min
60 min
3 hrs
12 hrs
2 days
 10000  100000  1e+06  1e+07
Tim
e (se
cond
s)
Corpus size (sentences)
Moses training timeMapReduce training (38 M/R)Optimal (Moses/38)
Figure 5: Phrase model extraction and scoring times at
various corpus sizes.
Constructing a model involves extracting all the
phrase pairs ?e, f? and computing the conditional
phrase translation probabilities in both directions.5
With a minor adjustment to the techniques intro-
duced in Section 3, it is possible to estimate P (B|A)
and P (A|B) concurrently.
Figure 5 shows the time it takes to construct
a phrase-based translation model using the Moses
tool, running on a single core, as well as the time
it takes to build the same model using our MapRe-
duce implementation. For reference, on the same
graph we plot a hypothetical, optimally-parallelized
version of Moses, which would run in 138 of the time
required for the single-core version on our cluster.6
Although these represent completely different im-
plementations, this comparison offers a sense of
MapReduce?s benefits. The framework provides a
conceptually simple solution to the problem, while
providing an implementation that is both scalable
and fault tolerant?in fact, transparently so since
the runtime hides all these complexities from the re-
searcher. From the graph it is clear that the overhead
associated with the framework itself is quite low, es-
pecially for large quantities of data. We concede that
it may be possible for a custom solution (e.g., with
MPI) to achieve even faster running times, but we
argue that devoting resources to developing such a
solution would not be cost-effective.
Next, we explore a class of models where the stan-
5Following Och and Ney (2002), it is customary to combine
both these probabilities as feature values in a log-linear model.
6In our cluster, only 19 machines actually compute, and each
has two single-core processors.
dard tools work primarily in memory, but where the
computational complexity of the models is greater.
5 Word Alignment
Although word-based translation models have been
largely supplanted by models that make use of larger
translation units, the task of generating a word align-
ment, the mapping between the words in the source
and target sentences that are translationally equiva-
lent, remains crucial to nearly all approaches to sta-
tistical machine translation.
The IBM models, together with a Hidden Markov
Model (HMM), form a class of generative mod-
els that are based on a lexical translation model
P (fj |ei) where each word fj in the foreign sentence
fm1 is generated by precisely one word ei in the sen-
tence el1, independently of the other translation de-
cisions (Brown et al, 1993; Vogel et al, 1996; Och
and Ney, 2000). Given these assumptions, we let
the sentence translation probability be mediated by
a latent alignment variable (am1 in the equations be-
low) that specifies the pairwise mapping between
words in the source and target languages. Assum-
ing a given sentence length m for fm1 , the translation
probability is defined as follows:
P (fm1 |e
l
1) =
?
am1
P (fm1 , a
m
1 |e
l
1)
=
?
am1
P (am1 |e
l
1, f
m
1 )
m?
j=1
P (fj |eaj )
Once the model parameters have been estimated, the
single-best word alignment is computed according
to the following decision rule:
a?m1 = argmax
am1
P (am1 |e
l
1, f
m
1 )
m?
j=1
P (fj |eaj )
In this section, we consider the MapReduce imple-
mentation of two specific alignment models:
1. IBM Model 1, where P (am1 |e
l
1, f
m
1 ) is uniform
over all possible alignments.
2. The HMM alignment model where
P (am1 |e
l
1, f
m
1 ) =
?m
j=1 P (aj |aj?1).
203
Estimating the parameters for these models is more
difficult (and more computationally expensive) than
with the models considered in the previous section:
rather than simply being able to count the word pairs
and alignment relationships and estimate the mod-
els directly, we must use an existing model to com-
pute the expected counts for all possible alignments,
and then use these counts to update the new model.7
This training strategy is referred to as expectation-
maximization (EM) and is guaranteed to always im-
prove the quality of the prior model at each iteration
(Brown et al, 1993; Dempster et al, 1977).
Although it is necessary to compute a sum over all
possible alignments, the independence assumptions
made in these models allow the total probability of
generating a particular observation to be efficiently
computed using dynamic programming.8 The HMM
alignment model uses the forward-backward algo-
rithm (Baum et al, 1970), which is also an in-
stance of EM. Even with dynamic programming,
this requires O(Slm) operations for Model 1, and
O(Slm2) for the HMM model, where m and l are
the average lengths of the foreign and English sen-
tences in the training corpus, and S is the number of
sentences. Figure 6 shows measurements of the av-
erage iteration run-time for Model 1 and the HMM
alignment model as implemented in Giza++ (Och
and Ney, 2003), a state-of-the-art C++ implemen-
tation of the IBM and HMM alignment models that
is widely used. Five iterations are generally neces-
sary to train the models, so the time to carry out full
training of the models is approximately five times the
per-iteration run-time.
5.1 EM with MapReduce
Expectation-maximization algorithms can be ex-
pressed quite naturally in the MapReduce frame-
work (Chu et al, 2006). In general, for discrete gen-
erative models, mappers iterate over the training in-
stances and compute the partial expected counts for
all the unobservable events in the model that should
7For the first iteration, when there is no prior model, a
heuristic, random, or uniform distribution may be chosen.
8For IBM Models 3-5, which are not our primary focus, dy-
namic programming is not possible, but the general strategy for
computing expected counts from a previous model and updat-
ing remains identical and therefore the techniques we suggest
in this section are applicable to those models as well.
3 s
10 s
30 s
90 s
3m20s
20 min
60 min
3 hrs
 10000  100000  1e+06
Ave
rage
 itera
tion 
laten
cy (se
cond
s)
Corpus size (sentences)
Model 1HMM
Figure 6: Per-iteration average run-times for Giza++ im-
plementations of Model 1 and HMM training on corpora
of various sizes.
be associated with the given training instance. Re-
ducers aggregate these partial counts to compute
the total expected joint counts. The updated model
is estimated using the maximum likelihood crite-
rion, which just involves computing the appropri-
ate marginal and dividing (as with the phrase-based
models), and the same techniques suggested in Sec-
tion 3 can be used with no modification for this
purpose. For word alignment models, Method 3
is possible since word pairs distribute according to
Zipf?s law (meaning there is ample opportunity for
the combiners to combine records), and the number
of parameters for P (e|fj = f) is at most the num-
ber of items in the vocabulary of E, which tends to
be on the order of hundreds of thousands of words,
even for large corpora.
Since the alignment models we are considering
are fundamentally based on a lexical translation
probability model, i.e., the conditional probability
distribution P (e|f), we describe in some detail how
EM updates the parameters for this model.9 Using
the model parameters from the previous iteration (or
starting from an arbitrary or heuristic set of param-
eters during the first iteration), an expected count is
computed for every l ?m pair ?ei, fj? for each par-
allel sentence in the training corpus. Figure 7 illus-
9Although computation of expected count for a word pair
in a given training instance obviously depends on which model
is being used, the set of word pairs for which partial counts are
produced for each training instance, as well as the process of ag-
gregating the partial counts and updating the model parameters,
is identical across this entire class of models.
204
the
blue
house
maison la bleue fleur
flower
la maison
the house
la maison bleue la fleur
the blue house the flower
(a)
(b)
Figure 7: Each cell in (a) contains the expected counts for
the word pair ?ei, fj?. In (b) the example training data is
marked to show which training instances contribute par-
tial counts for the pair ?house, maison?.
3 s
10 s
30 s
90 s
3m20s
20 min
60 min
3 hrs
 10000  100000  1e+06
Tim
e (se
cond
s)
Corpus size (sentences)
Optimal Model 1 (Giza/38)Optimal HMM (Giza/38)MapReduce Model 1 (38 M/R)MapReduce HMM (38 M/R)
Figure 8: Average per-iteration latency to train HMM
and Model 1 using the MapReduce EM trainer, compared
to an optimal parallelization of Giza++ across the same
number of processors.
trates the relationship between the individual train-
ing instances and the global expected counts for a
particular word pair. After collecting counts, the
conditional probability P (f |e) is computed by sum-
ming over all columns for each f and dividing. Note
that under this training regime, a non-zero probabil-
ity P (fj |ei) will be possible only if ei and fj co-
occur in at least one training instance.
5.2 Experimental Results
Figure 8 shows the timing results of the MapReduce
implementation of Model 1 and the HMM alignment
model. Similar to the phrase extraction experiments,
we show as reference the running time of a hy-
pothetical, optimally-parallelized version of Giza++
on our cluster (i.e., values in Figure 6 divided by
38). Whereas in the single-core implementation the
added complexity of the HMM model has a signif-
icant impact on the per-iteration running time, the
data exchange overhead dominates in the perfor-
mance of both models in a MapReduce environment,
making running time virtually indistinguishable. For
these experiments, after each EM iteration, the up-
dated model parameters (which are computed in a
distributed fashion) are compiled into a compressed
representation which is then distributed to all the
processors in the cluster at the beginning of the next
iteration. The time taken for this process is included
in the iteration latencies shown in the graph. In fu-
ture work, we plan to use a distributed model repre-
sentation to improve speed and scalability.
6 Related work
Expectation-maximization algorithms have been
previously deployed in the MapReduce framework
in the context of several different applications (Chu
et al, 2006; Das et al, 2007; Wolfe et al, 2007).
Wolfe et al (2007) specifically looked at the perfor-
mance of Model 1 on MapReduce and discuss how
several different strategies can minimize the amount
of communication required but they ultimately ad-
vocate abandoning the MapReduce model. While
their techniques do lead to modest performance im-
provements, we question the cost-effectiveness of
the approach in general, since it sacrifices many of
the advantages provided by the MapReduce envi-
ronment. In our future work, we instead intend to
make use of an approach suggested by Das et al
(2007), who show that a distributed database run-
ning in tandem with MapReduce can be used to
provide the parameters for very large mixture mod-
els efficiently. Moreover, since the database is dis-
tributed across the same nodes as the MapReduce
jobs, many of the same data locality benefits that
Wolfe et al (2007) sought to capitalize on will be
available without abandoning the guarantees of the
MapReduce paradigm.
Although it does not use MapReduce, the MTTK
tool suite implements distributed Model 1, 2 and
HMM training using a ?home-grown? paralleliza-
tion scheme (Deng and Byrne, 2006). However, the
tool relies on a cluster where all nodes have access to
the same shared networked file storage, a restriction
that MapReduce does not impose.
205
There has been a fair amount of work inspired by
the problems of long latencies and excessive space
requirements in the construction of phrase-based
and hierarchical phrase-based translation models.
Several authors have advocated indexing the train-
ing data with a suffix array and computing the nec-
essary statistics during or immediately prior to de-
coding (Callison-Burch et al, 2005; Lopez, 2007).
Although this technique works quite well, the stan-
dard channel probability P (f |e) cannot be com-
puted, which is not a limitation of MapReduce.10
7 Conclusions
We have shown that an important class of model-
building algorithms in statistical machine transla-
tion can be straightforwardly recast into the MapRe-
duce framework, yielding a distributed solution
that is cost-effective, scalable, robust, and exact
(i.e., doesn?t resort to approximations). Alterna-
tive strategies for parallelizing these algorithms ei-
ther impose significant demands on the developer,
the hardware infrastructure, or both; or, they re-
quire making unwarranted independence assump-
tions, such as dividing the training data into chunks
and building separate models. We have further
shown that on a 20-machine cluster of commodity
hardware, the MapReduce implementations have ex-
cellent performance and scaling characteristics.
Why does this matter? Given the difficulty of im-
plementing model training algorithms (phrase-based
model estimation is difficult because of the size of
data involved, and word-based alignment models are
a challenge because of the computational complex-
ity associated with computing expected counts), a
handful of single-core tools have come to be widely
used. Unfortunately, they have failed to scale with
the amount of training data available. The long la-
tencies associated with these tools on large datasets
imply that any kind of experimentation that relies on
making changes to variables upstream of the word
alignment process (such as, for example, altering the
training data f ? f ?, building a new model P (f ?|e),
and reevaluating) is severely limited by this state of
affairs. It is our hope that by reducing the cost of this
10It is an open question whether the channel probability
and inverse channel probabilities are both necessary. Lopez
(2008) presents results suggesting that P (f |e) is not necessary,
whereas Subotin (2008) finds the opposite.
these pieces of the translation pipeline, we will see a
greater diversity of experimental manipulations. To-
wards that end, we intend to release this code under
an open source license.
For our part, we plan to continue pushing the lim-
its of current word alignment models by moving to-
wards a distributed representation of the model pa-
rameters used in the expectation step of EM and
abandoning the compiled model representation. Fur-
thermore, initial experiments indicate that reorder-
ing the training data can lead to better data local-
ity which can further improve performance. This
will enable us to scale to larger corpora as well as
to explore different uses of translation models, such
as techniques for processing comparable corpora,
where a strict sentence alignment is not possible un-
der the limitations of current tools.
Finally, we note that the algorithms and tech-
niques we have described here can be readily ex-
tended to problems in other areas of NLP and be-
yond. HMMs, for example, are widely used in
ASR, named entity detection, and biological se-
quence analysis. In these areas, model estimation
can be a costly process, and therefore we believe
this work will be of interest for these applications
as well. It is our expectation that MapReduce will
also provide solutions that are fast, easy, and cheap.
Acknowledgments
This work was supported by the GALE program of
the Defense Advanced Research Projects Agency,
Contract No. HR0011-06-2-0001. We would also
like to thank the generous hardware support of IBM
and Google via the Academic Cloud Computing Ini-
tiative. Specifically, thanks go out to Dennis Quan
and Eugene Hung from IBM for their tireless sup-
port of our efforts. Philip Resnik and Miles Osborne
provided helpful comments on an early draft. The
last author would like to thank Esther and Kiri for
their kind support.
References
Luiz Andre? Barroso, Jeffrey Dean, and Urs Ho?lzle. 2003.
Web search for a planet: The Google cluster architec-
ture. IEEE Micro, 23(2):22?28.
Leonard E. Baum, Ted Petrie, George Soules, and Nor-
man Weiss. 1970. A maximization technique occur-
206
ring in the statistical analysis of probabilistic functions
of Markov chains. Annals of Mathematical Statistics,
41(1):164?171.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och,
and Jeffrey Dean. 2007. Large language models in
machine translation. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 858?867, Prague, Czech Re-
public.
Peter F. Brown, Vincent J. Della Pietra, Stephen A.
Della Pietra, and Robert L. Mercer. 1993. The mathe-
matics of statistical machine translation: parameter es-
timation. Computational Linguistics, 19(2):263?311.
Chris Callison-Burch, Colin Bannard, and Josh
Schroeder. 2005. Scaling phrase-based statisti-
cal machine translation to larger corpora and longer
phrases. In Proceedings of the 43rd Annual Meeting
of the Association for Computational Linguistics (ACL
2005), pages 255?262, Ann Arbor, Michigan.
Cheng T. Chu, Sang K. Kim, Yi A. Lin, Yuanyuan Yu,
Gary R. Bradski, Andrew Y. Ng, and Kunle Oluko-
tun. 2006. Map-Reduce for machine learning on mul-
ticore. In Advances in Neural Information Processing
Systems 19 (NIPS 2006), pages 281?288, Vancouver,
British Columbia, Canada.
Abhinandan S. Das, Mayur Datar, Ashutosh Garg, and
Shyam Rajaram. 2007. Google news personalization:
scalable online collaborative filtering. In Proceedings
of the 16th International Conference on World Wide
Web (WWW 2007), pages 271?280, Banff, Alberta,
Canada.
Jeffrey Dean and Sanjay Ghemawat. 2004. MapReduce:
Simplified data processing on large clusters. In Pro-
ceedings of the 6th Symposium on Operating System
Design and Implementation (OSDI 2004), pages 137?
150, San Francisco, California.
Arthur Dempster, Nan Laird, and Donald Rubin. 1977.
Maximum likelihood from incomplete data via the EM
algorithm. Journal of the Royal Statistics Society,
39(1):1?38.
Yonggang Deng and William J. Byrne. 2006. MTTK:
An alignment toolkit for statistical machine transla-
tion. In Proceedings of the 2006 Human Language
Technology Conference of the North American Chap-
ter of the Association for Computational Linguistics
(HLT/NAACL 2006), Companion Volume, pages 265?
268, New York, New York.
Sanjay Ghemawat, Howard Gobioff, and Shun-Tak Le-
ung. 2003. The Google File System. In Proceedings
of the 19th ACM Symposium on Operating Systems
Principles (SOSP-03), pages 29?43, Bolton Landing,
New York.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the 2003 Human Language Technology
Conference of the North American Chapter of the As-
sociation for Computational Linguistics (HLT/NAACL
2003), pages 48?54, Edmonton, Alberta, Canada.
Jimmy Lin. 2008. Exploring large-data issues in the cur-
riculum: A case study with MapReduce. In Proceed-
ings of the Third Workshop on Issues in Teaching Com-
putational Linguistics at ACL 2008, Columbus, Ohio.
Adam Lopez. 2007. Hierarchical phrase-based trans-
lation with suffix arrays. In Proceedings of the
2007 Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural
Language Learning, pages 976?985, Prague, Czech
Republic.
Adam Lopez. 2008. Machine Translation by Pattern
Matching. Ph.D. dissertation, University of Maryland,
College Park, MD.
Franz Josef Och and Hermann Ney. 2000. A comparison
of alignment models for statistical machine translation.
In Proceedings of the 18th International Conference
on Computational Linguistics (COLING 2000), pages
1086?1090, Saarbrucken, Germany.
Franz Josef Och and Hermann Ney. 2002. Discrimini-
tive training and maximum entropy models for statis-
tical machine translation. In Proceedings of the 40th
Annual Meeting of the Association for Computational
Linguistics (ACL 2002), pages 295?302, Philadelphia,
Pennsylvania.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Franz Josef Och, Christoph Tillmann, and Hermann Ney.
1999. Improved alignment models for statistical ma-
chine translation. In Proceedings of the 1999 Joint
SIGDAT Conference on Empirical Methods in Natural
Language Processing and Very Large Corpora, pages
20?28, College Park, Maryland.
Michael Subotin. 2008. Exponential models for machine
translation. Master?s thesis, University of Maryland,
College Park, MD.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical trans-
lation. In Proceedings of the 16th conference on Com-
putational linguistics (COLING 1996), pages 836?
841, Copenhagen, Denmark.
Jason Wolfe, Aria Delier Haghighi, and Daniel Klein.
2007. Fully distributed EM for very large datasets.
Technical Report UCB/EECS-2007-178, EECS De-
partment, University of California, Berkeley.
207
Answering Clinical Questions with
Knowledge-Based and Statistical Techniques
Dina Demner-Fushman?
University of Maryland, College Park
Jimmy Lin?
University of Maryland, College Park
The combination of recent developments in question-answering research and the availability of
unparalleled resources developed specifically for automatic semantic processing of text in the
medical domain provides a unique opportunity to explore complex question answering in the
domain of clinical medicine. This article presents a system designed to satisfy the information
needs of physicians practicing evidence-based medicine. We have developed a series of knowl-
edge extractors, which employ a combination of knowledge-based and statistical techniques, for
automatically identifying clinically relevant aspects of MEDLINE abstracts. These extracted
elements serve as the input to an algorithm that scores the relevance of citations with respect to
structured representations of information needs, in accordance with the principles of evidence-
based medicine. Starting with an initial list of citations retrieved by PubMed, our system
can bring relevant abstracts into higher ranking positions, and from these abstracts generate
responses that directly answer physicians? questions. We describe three separate evaluations: one
focused on the accuracy of the knowledge extractors, one conceptualized as a document reranking
task, and finally, an evaluation of answers by two physicians. Experiments on a collection
of real-world clinical questions show that our approach significantly outperforms the already
competitive PubMed baseline.
1. Introduction
Recently, the focus of question-answering research has shifted away from simple fact-
based questions that can be answered with relatively little linguistic knowledge to
?harder? questions that require fine-grained text analysis, reasoning capabilities, and
the ability to synthesize information from multiple sources. General purpose reasoning
on anything other than superficial lexical relations is exceedingly difficult because there
is a vast amount of world knowledge that must be encoded, either manually or auto-
matically, to overcome the brittleness often associated with long chains of evidence. This
situation poses a serious bottleneck to ?advanced? question-answering systems. How-
ever, the availability of existing knowledge sources and ontologies in certain domains
provides exciting opportunities to experiment with knowledge-rich approaches. How
might one go about leveraging these resources effectively? How might one integrate
? Department of Computer Science and Institute for Advanced Computer Studies. E-mail:
demner@umd.edu.
? College of Information Studies, Department of Computer Science, and Institute for Advanced Computer
Studies. E-mail: jimmylin@umd.edu.
Submission received: 4 July 2005; revised submission received: 7 January 2006; accepted for publication:
12 April 2006.
? 2007 Association for Computational Linguistics
Computational Linguistics Volume 33, Number 1
statistical techniques to overcome the brittleness often associated with knowledge-
based approaches?
We explore these interesting research questions in the domain of medicine, fo-
cusing on the information needs of physicians in clinical settings. This domain is
well-suited for exploring the posed research questions for several reasons. First,
substantial understanding of the domain has already been codified in the Unified
Medical Language System (UMLS) (Lindberg, Humphreys, and McCray 1993). Sec-
ond, software for utilizing this ontology already exists: MetaMap (Aronson 2001)
identifies concepts in free text, and SemRep (Rindflesch and Fiszman 2003) extracts
relations between the concepts. Both systems utilize and propagate semantic infor-
mation from UMLS knowledge sources: the Metathesaurus, the Semantic Network,
and the SPECIALIST lexicon. The 2004 version of the UMLS Metathesaurus (used
in this work) contains information about over 1 million biomedical concepts and
5 million concept names from more than 100 controlled vocabularies. The Seman-
tic Network provides a consistent categorization of all concepts represented in the
UMLS Metathesaurus. Third, the paradigm of evidence-based medicine (Sackett et al
2000) provides a task-based model of the clinical information-seeking process. The
PICO framework (Richardson et al 1995) for capturing well-formulated clinical queries
(described in Section 2) can serve as the basis of a knowledge representation that
bridges the needs of clinicians and analytical capabilities of a system. The conflu-
ence of these many factors makes clinical question answering a very exciting area of
research.
Furthermore, the need to answer questions related to patient care at the point of
service has been well studied and documented (Covell, Uman, and Manning 1985;
Gorman, Ash, and Wykoff 1994; Ely et al 1999, 2005). MEDLINE, the authoritative
repository of abstracts from the medical and biomedical primary literature maintained
by the National Library of Medicine, provides the clinically relevant sources for answer-
ing physicians? questions, and is commonly used in that capacity (Cogdill and Moore
1997; De Groote and Dorsch 2003). However, studies have shown that existing systems
for searching MEDLINE (such as PubMed, the search service provided by the National
Library of Medicine) are often inadequate and unable to supply clinically relevant
answers in a timely manner (Gorman, Ash, and Wykoff 1994; Chambliss and Conley
1996). Furthermore, it is clear that traditional document retrieval technology applied
to MEDLINE abstracts is insufficient for satisfactory information access; research and
experience point to the need for systems that automatically analyze text and return
only the relevant information, appropriately summarizing and fusing segments from
multiple texts. Not only is clinical question answering interesting from a research
perspective, it also represents a potentially high-impact, real-world application of lan-
guage processing and information retrieval technology?better information systems to
provide decision support for physicians have the potential to improve the quality of
health care.
Our question-answering system supports the practice of evidence-based medi-
cine (EBM), a widely accepted paradigm for medical practice that stresses the impor-
tance of evidence from patient-centered clinical research in the health care process.
EBM prescribes an approach to structuring clinical information needs and identi-
fies elements (for example, the problem at hand and the interventions under con-
sideration) that factor into the assessment of clinically relevant studies for medical
practice. The foundation of our question-answering strategy is built on knowledge
extractors that automatically identify these elements in MEDLINE abstracts. Using
these knowledge extractors, we have developed algorithms for scoring the relevance
64
Demner-Fushman and Lin Answering Clinical Questions
of MEDLINE citations in accordance with the principles of EBM. Our scorer is em-
ployed to rerank citations retrieved by the PubMed search engine, with the goal of
bringing as many topically relevant abstracts to higher ranking positions as possible.
From this reranked list of citations, our system is then able to generate textual re-
sponses that directly address physicians? information needs. We evaluated our system
with a collection of real-world clinical questions and demonstrate that our combined
knowledge-based and statistical approach delivers significantly better document re-
trieval and question-answering performance, compared to systems used by physicians
today.
This article is organized in the following manner: We start in the next section with
an overview of evidence-based medicine and its basic principles. Section 3 provides an
overview of MEDLINE, the bibliographic database used by our system, and PubMed,
the public gateway for accessing this database. Section 4 describes our system architec-
ture and outlines our conception of clinical question answering as ?semantic unifica-
tion? between query frames and knowledge frames derived from MEDLINE citations.
The knowledge extractors that underlie our approach are described in Section 5, along
with intrinsic evaluations of each component. In Section 6, we detail an algorithm for
scoring the relevance of MEDLINE citations with respect to structured query represen-
tations. This scoring algorithm captures the principles of EBM and uses the results of
the knowledge extractors as basic features. To evaluate the performance of this citation
scoring algorithm, we have gathered a corpus of real-world clinical questions. Section 7
presents results from a document reranking experiment where our EBM scores were
used to rerank citations retrieved by PubMed. Section 8 provides additional details on
attempts to optimize the performance of our EBM citation scoring algorithm. Answer
generation, based on reranked results, is described in Section 9. Answers from our
system were manually assessed by two physicians; results are presented in Section 10.
Related work is discussed in Section 11, followed by future work in Section 12. Finally,
we conclude in Section 13.
2. The Framework of Evidence-Based Medicine
Evidence-based medicine (EBM) is a widely accepted paradigm for medical practice
that involves the explicit use of current best evidence, that is, high-quality patient-
centered clinical research such as reports from randomized controlled trials, in mak-
ing decisions about patient care. Naturally, such evidence, as reported in the primary
medical literature, must be suitably integrated with the physician?s own expertise and
patient-specific factors. It is argued by many that practicing medicine in this manner
leads to better patient outcomes and higher quality health care. The goal of our work is
to develop question-answering techniques that complement this paradigm of medical
practice.
EBM offers three orthogonal facets that, when taken together, provide a framework
for codifying the knowledge involved in answering clinical questions. These three
complementary facets are outlined below.
The first facet describes the four main clinical tasks that physicians engage in
(arranged roughly in order of prevalence):
Therapy: Selecting treatments to offer a patient, taking into account effectiveness, risk,
cost, and other relevant factors (includes Prevention?selecting actions to reduce
the chance of a disease by identifying and modifying risk factors).
65
Computational Linguistics Volume 33, Number 1
Diagnosis: This encompasses two primary types:
Differential diagnosis: Identifying and ranking by likelihood potential diseases
based on findings observed in a patient.
Diagnostic test: Selecting and interpreting diagnostic tests for a patient, consid-
ering their precision, accuracy, acceptability, cost, and safety.
Etiology/Harm: Identifying factors that cause a disease or condition in a patient.
Prognosis: Estimating a patient?s likely course over time and anticipating likely
complications.
These activities represent what Ingwersen (1999) calls ?work tasks.? It is important
to note that they exist independently of information needs, namely, searching is not
necessarily implicated in any of these activities. We are, however, interested in situations
where questions arise during one of these clinical tasks?only then does the physician
engage in information-seeking behavior. These activities translate into natural ?search
tasks.? For therapy, the search task is usually therapy selection (for example, determining
which course of action is the best treatment for a disease) or prevention (for example,
selecting preemptive measures with respect to a particular disease). For diagnosis,
there are two different possibilities: in differential diagnosis, a physician is consider-
ing multiple hypotheses regarding what disease a patient has; in diagnostic methods
selection, the clinician is attempting to ascertain the relative utility of different tests.
For etiology, cause determination is the search task, and for prognosis, patient outcome
prediction.
Terms and the types of studies relevant to each of the four tasks have been exten-
sively studied by the Hedges Project at the McMaster University (Haynes et al 1994;
Wilczynski, McKibbon, and Haynes 2001). The results of this research are implemented
in the PubMed Clinical Queries tools, which can be used to retrieve task-specific cita-
tions (more about this in the next section).
The second facet is independent of the clinical task and pertains to the struc-
ture of a well-built clinical question. The following four components have been iden-
tified as the key elements of a question related to patient care (Richardson et al
1995):
 What is the primary problem or disease? What are the characteristics of
the patient (e.g., age, gender, or co-existing conditions)?
 What is the main intervention (e.g., a diagnostic test, medication, or
therapeutic procedure)?
 What is the main intervention compared to (e.g., no intervention, another
drug, another therapeutic procedure, or a placebo)?
 What is the desired effect of the intervention (e.g., cure a disease, relieve
or eliminate symptoms, reduce side effects, or lower cost)?
These four elements are often referenced with the mnemonic PICO, which stands
for Patient/Problem, Intervention, Comparison, and Outcome.
Finally, the third facet serves as a tool for appraising the strength of evidence
presented in the study, that is, how much confidence should a physician have in the
results? Several taxonomies for appraising the strength of evidence based on the type
and quality of the study have been developed. We chose the Strength of Recommenda-
tions Taxonomy (SORT) as the basis for determining the potential upper bound on the
66
Demner-Fushman and Lin Answering Clinical Questions
quality of evidence, due to its emphasis on the use of patient-oriented outcomes and its
attempt to unify other existing taxonomies (Ebell et al 2004). There are three levels of
recommendations according to SORT:
 A-level evidence is based on consistent, good-quality patient
outcome-oriented evidence presented in systematic reviews, randomized
controlled clinical trials, cohort studies, and meta-analyses.
 B-level evidence is inconsistent, limited-quality, patient-oriented evidence
in the same types of studies.
 C-level evidence is based on disease-oriented evidence or studies less
rigorous than randomized controlled clinical trials, cohort studies,
systematic reviews, and meta-analyses.
A question-answering system designed to support the practice of evidence-based
medicine must be sensitive to the multifaceted considerations that go into evaluating
an abstract?s relevance to a clinical information need. It is exactly these three comple-
mentary facets that we attempt to encode in a question-answering system for clinical
decision support.
3. MEDLINE and PubMed
MEDLINE is a large bibliographic database maintained by the U.S. National Library
of Medicine (NLM). This database is viewed by medical professionals, biomedical
researchers, and many other users as the authoritative source of clinical evidence, and
hence we have adopted it as the target corpus for our clinical question-answering sys-
tem. MEDLINE contains over 15 million references to articles from approximately 4,800
journals in 30 languages, dating back to the 1960s. In 2004, over 571,000 new citations
were added to the database, and it continues to grow at a steady pace. The subject
scope of MEDLINE is biomedicine and health, broadly defined to encompass those
areas of the life sciences, behavioral sciences, chemical sciences, and bioengineering
needed by health professionals and others engaged in basic research and clinical care,
public health, health policy development, or related educational activities. MEDLINE
also covers life sciences vital to biomedical practitioners, researchers, and educators,
including aspects of biology, environmental science, marine biology, plant and animal
science, as well as biophysics and chemistry.1
Each MEDLINE citation includes basic information such as the title of the article,
name of the authors, name of the publication, publication type, date of publication,
language, and so on. Of the entries added over the last decade or so, approximately
76% have English abstracts written by the authors of the articles?these texts provide
the source for answers extracted by our system.
Additional metadata are associated with each MEDLINE citation. The most impor-
tant of these is the controlled vocabulary terms assigned by human indexers. NLM?s
controlled vocabulary thesaurus, Medical Subject Headings (MeSH),2 contains approx-
imately 23,000 descriptors arranged in a hierarchical structure and more than 151,000
Supplementary Concept Records (additional chemical substance names) within a
1 http://www.nlm.nih.gov/pubs/factsheets/medline.html
2 Commonly referred to as MeSH terms or MeSH headings, although technically the latter is redundant.
67
Computational Linguistics Volume 33, Number 1
separate thesaurus. Indexing is performed by approximately 100 indexers with at least
bachelor?s degrees in life sciences and formal training in indexing provided by NLM.
Since mid-2002, the Library has been employing software that automatically suggests
MeSH headings based on content (Aronson et al 2004). Nevertheless, the indexing
process remains firmly human-centered.
As a concrete example, an abstract titled ?Antipyretic efficacy of ibuprofen vs.
acetaminophen? might have the following MeSH headings associated with it:
MH - Acetaminophen/*therapeutic use
MH - Child
MH - Comparative Study
MH - Fever/*drug therapy
MH - Ibuprofen/*therapeutic use
To represent different aspects of the topic described by a particular MeSH heading,
up to three subheadings may be assigned, as indicated by the slash notation. In this
example, a trained user could interpret from the MeSH terms that the article is about
drug therapy for fever and the therapeutic use of ibuprofen and acetaminophen. An
asterisk placed next to a MeSH heading indicates that the human indexer interprets the
term to be the main focus of the article. Multiple MeSH terms can be notated in this
manner.
MEDLINE is publicly accessible on the World Wide Web through PubMed, the Na-
tional Library of Medicine?s gateway, or through third-party organizations that license
MEDLINE from NLM. PubMed is a sophisticated boolean search engine that allows
users to query not only on abstract text, but also on metadata fields such as MeSH terms.
In addition, PubMed provides a number of pre-defined ?search templates? called Clin-
ical Queries (Haynes et al 1994; Wilczynski, McKibbon, and Haynes 2001) that allow
users to narrow the scope of retrieved articles. These filters are implemented as fixed
boolean query fragments (containing restrictions on MeSH terms, for example) that are
appended to the original user query. Our experiments involve the use of PubMed to
retrieve an initial set of candidate citations for subsequent processing.
4. System Architecture
We view clinical question answering as ?semantic unification? between information
needs expressed in a PICO-based frame and corresponding structures automatically
extracted from MEDLINE citations. In accordance with the principles of EBM, this
matching process should be sensitive to the nature of the clinical task and the strength
of evidence of retrieved abstracts.
As a concrete example, consider the following clinical question:
In children with an acute febrile illness, what is the efficacy of single-medication
therapy with acetaminophen or ibuprofen in reducing fever?
The information need might be formally encoded in the following manner:
Search Task: therapy selection
Problem/Population: acute febrile illness/in children
Intervention: acetaminophen
Comparison: ibuprofen
Outcome: reducing fever
68
Demner-Fushman and Lin Answering Clinical Questions
This query representation explicitly encodes the search task and the PICO structure
of the clinical question. After processing MEDLINE citations, automatically extracting
PICO elements from the abstracts, and semantically matching these elements with the
query, a system might produce the following answer:
Ibuprofen provided greater temperature decrement and longer duration of antipyresis
than acetaminophen when the two drugs were administered in approximately equal
doses.
PMID: 1621668
Strength of Evidence: grade A
Physicians are usually most interested in outcome statements that assert a patient-
oriented clinical finding?for example, the relative efficacy of two drugs. Thus, out-
comes can serve as the basis for good answers and an entry point into the full text. The
system should automatically evaluate the strength of evidence of the citations supplying
the answer, but the decision to adopt the recommendations as suggested ultimately rests
with the physician.
What is the best input to a clinical question-answering system? Two possibilities
include a natural language question or a structured PICO query frame. We advocate
the latter. With a frame-based query interface, the physician shoulders the burden of
translating an information need into a frame-based representation, but this provides
several advantages. Most importantly, formal representations force physicians to ?think
through? their questions, ensuring that relevant elements are captured. Poorly formu-
lated queries have been identified by Ely et al (2005) as one of the obstacles to finding
answers to clinical questions. Because well-formed questions should have concretely
instantiated PICO slots, a frame representation clearly lets the physician see missing
elements. In addition, a structured query representation obviates the need for linguistic
analysis of a natural language question, where ambiguities may negatively impact
overall performance. We discuss alternative interfaces in Section 12.
Ideally, we would like to match structured representations derived from the ques-
tion with those derived from MEDLINE citations (taking into consideration other EBM-
relevant factors). However, we do not have access to the computational resources nec-
essary to apply knowledge extractors to the 15 million plus citations in the MEDLINE
database and directly index their results. As an alternative, we rely on PubMed to
retrieve an initial set of hits that we then postprocess in greater detail?this is the
standard pipeline architecture commonly employed in other question-answering sys-
tems (Voorhees and Tice 1999; Hirschman and Gaizauskas 2001).
The architecture of our system is shown in Figure 1. The query formulator is respon-
sible for converting a clinical question (in the form of a query frame) into a PubMed
search query. Presently, these queries are already encoded in our test collection (see
Section 6). PubMed returns an initial list of MEDLINE citations, which is then analyzed
by our knowledge extractors (see Section 5). The input to the semantic matcher, which
implements our EBM citation scoring algorithm, is the query frame and annotated
MEDLINE citations. The module outputs a ranked list of citations that have been scored
in accordance with the principles of EBM (see Section 6). Finally, the answer generator
takes these citations and extracts appropriate answers (see Section 9).
In summary, our conception of clinical question answering as semantic frame
matching suggests the need for a number of capabilities, which correspond to the
bold outlined boxes in Figure 1: knowledge extraction, semantic matching for scoring
69
Computational Linguistics Volume 33, Number 1
Figure 1
Architecture of our clinical question-answering system.
citations, and answer generation. We have realized all three capabilities in an imple-
mented clinical question-answering system and conducted three separate evaluations
to assess the effectiveness of our developed capabilities. We do not tackle the query
formulator, although see discussion in Section 12. Overall, results indicate that our
implemented system significantly outperforms the PubMed baseline.
5. Knowledge Extraction for Evidence-Based Medicine
The automatic extraction of PICO elements from MEDLINE citations represents a key
capability integral to clinical question answering. This section, which elaborates on
preliminary results reported in Demner-Fushman and Lin (2005), describes extraction
algorithms for population, problems, interventions, outcomes, and the strength of evi-
dence. For an example of a completely annotated abstract, see Figure 2. Each individual
PICO extractor takes as input the abstract text of a MEDLINE citation and identifies the
relevant elements: Outcomes are complete sentences, while population, problems, and
interventions are short noun phrases.
Our knowledge extractors rely extensively on MetaMap (Aronson 2001), a system
for identifying segments of text that correspond to concepts in the UMLS Metathe-
saurus. Many of our algorithms operate at the level of coarser-grained semantic
types called Semantic Groups (McCray, Burgun, and Bodenreider 2001), which capture
higher-level generalizations about entities (e.g., CHEMICALS & DRUGS). An additional
feature we take advantage of (when present) is explicit section markers present in some
abstracts. These so-called structured abstracts were recommended by the Ad Hoc Work-
ing Group for Critical Appraisal of the Medical Literature (1987) to help humans assess
the reliability and content of a publication and to facilitate the indexing and retrieval
processes. These abstracts loosely adhere to the introduction, methods, results, and
conclusions format common in scientific writing, and delineate a study using explicitly
marked sections with variations of the above headings. Although many core clinical
journals require structured abstracts, there is a great deal of variation in the actual
headings. Even when present, the headings are not organized in a manner focused on
patient care. In addition, abstracts of much high-quality work remain unstructured. For
these reasons, explicit section markers are not entirely reliable indicators for the various
semantic elements we seek to extract, but must be considered along with other sources
of evidence.
The extraction of each PICO element relies to a different extent on an annotated
corpus of MEDLINE abstracts, created through an effort led by the first author at
the National Library of Medicine (Demner-Fushman et al 2006). As will be described
herein, the population, problem, and the intervention extractors are based largely on
recognition of semantic types and a few manually constructed rules; the outcome extrac-
70
Demner-Fushman and Lin Answering Clinical Questions
tor, in contrast, is implemented as an ensemble of classifiers trained using supervised
machine learning techniques (Demner-Fushman et al 2006). These two very different
approaches can be attributed to differences in the nature of the frame elements: Whereas
problems and interventions can be directly mapped to UMLS concepts, and populations
easily mapped to patterns that include UMLS concepts, outcome statements follow no
predictable pattern. The initial goal of the annotation effort was to identify outcome
statements in abstract text. A physician, two registered nurses, and an engineering
researcher manually identified sentences that describe outcomes in 633 MEDLINE
abstracts; a post hoc analysis demonstrates good agreement (? = 0.77). The annotated
abstracts were retrieved using PubMed and attempted to model different user behaviors
ranging from naive to expert (where advanced search features were employed). With the
exception of 50 citations retrieved to answer a question about childhood immunization,
the rest of the results were retrieved by querying on a disease, for example, diabetes. Of
the 633 citations, 100 abstracts were also fully annotated with population, problems, and
interventions. These 100 abstracts were set aside as a held-out test set. Of the remaining
citations, 275 were used for training and rule derivation, as described in the following
sections.
After much exploration, Demner-Fushman et al (2006) discovered that it was not
practical to annotate PICO entities at the phrase level due to significant unresolvable
disagreement and interannotator reliability issues. Consider the following segment:
This double-blind, placebo-controlled, randomized, 3-period, complete block, 6-week
crossover study examined the efficacy of simvastatin in adult men and women (N =
151) with stable type 2 DM, low density lipoprotein-cholesterol 100 mg/dL, HDL-C <
40 mg/dL, and fasting triglyceride level > 150 and < 700 mg/dL.
All annotators agreed that the sentence contained the problem, population, and
intervention. However, they could not agree on the exact phrasal boundaries of each
element, and more importantly, general guidelines for ensuring consistent annotations.
For example, should the whole clause starting with adult men and women be marked as
population, or should type 2 Diabetes Mellitus (type 2 DM) be marked up only as the
problem? How should we indicate that the cholesterol levels description belongs to 151
subjects of the study, and so forth? This issue becomes important for evaluation because
there is a mismatch between annotated ground truth and the output of our knowledge
extractors, as we will discuss.
In what follows, we describe each of the individual PICO extractors and a series of
component evaluations that assess their accuracy. This section is organized such that the
description of each extractor and its evaluation are paired together. Results are reported
in terms of the percentage of correctly identified instances, percentage of instances for
which the extractor had no answer, and percentage of incorrectly identified instances.
The baselines and gold standards for each extractor vary, and will be described in-
dividually. The goal of these component evaluations is a general characterization of
performance, as we focused the majority of our efforts on the two other evaluations.
5.1 Population Extractor
The PICO framework makes no distinction between the population and the problem,
which is rooted in the concept of the population in clinical studies, as exemplified by
text such as POPULATION: Fifty-five postmenopausal women with a urodynamic diagnosis
of genuine urinary stress incontinence. Although this fragment simultaneously describes
71
Computational Linguistics Volume 33, Number 1
the population (of which a particular patient can be viewed as a sample therefrom) and
the problem, we chose to separate the extraction of the two elements because they are
not always specified together in abstracts (issues with respect to exact boundaries men-
tioned previously notwithstanding). Furthermore, many clinical questions ask about a
particular problem without specifying a population.
Population elements, which are typically noun phrases, are identified using a series
of manually crafted rules that codify the following assumptions:
 The concept describing the population belongs to the semantic type
GROUP or any of its children. In addition, certain nouns are often used to
describe study participants in medical texts; for example, an often
observed pattern is ?subjects? or ?cases? followed by a concept from the
semantic group DISORDER.
 The number of subjects that participated in the study often precedes or
follows a concept identified as a GROUP. In the latter case, the number is
sometimes given in parentheses using a common pattern n = number,
where ?n = ? is a shorthand for the number of subjects, and number
provides the actual number of study participants.
 The confidence that a clause with an identified number and GROUP
contains information about the population is inversely proportional to the
distance between the two entities.
 The confidence that a clause contains the population is influenced by the
position of the clause, with respect to headings in the case of structured
abstracts and with respect to the beginning of the abstract in the case of
unstructured abstracts.
Given these assumptions, the population extractor searches for the following
patterns:
 GROUP ([Nn]=[0?9]+)
for example, in 5?6-year-old French children (n = 234), Subjects (n = 54)
 number* GROUP
for example, forty-nine infants
 number* DISORDER* GROUP?
for example, 44 HIV-infected children
The confidence score assigned to a particular pattern match is a function of both its
position in the abstract and its position in the clause from which it was extracted. If a
number is followed by a measure, for example, year or percent, the number is discarded,
and pattern matching continues. After the entire abstract is processed in this manner,
the match with the highest confidence value is retained as the population description.
5.2 Evaluation of Population Extractor
Ninety of the 100 fully annotated abstracts in our collection were agreed upon by the
annotators as being clinical in nature, and were used as test data for our population
extractor. Because these abstracts were not examined in the process of developing the
extractor rules, they can be viewed as a blind held-out test set. The output of our popu-
72
Demner-Fushman and Lin Answering Clinical Questions
Table 1
Evaluation of the population extractor.
Correct (%) Unknown (%) Wrong (%)
Baseline 53 ? 47
Extractor 80 10 10
lation extractor was judged to be correct if it occurred in a sentence that was annotated
as containing the population in the gold standard. Note that this evaluation presents an
upper bound on the performance of the population extractor, whose outputs are noun
phrases. We adopted such a lenient evaluation setup because of the boundary issues
previously discussed, and also to forestall potential difficulties with scoring partially
overlapping string matches.
For comparison, our baseline simply returned the first three sentences of the ab-
stract. We considered the baseline correct if any one of the sentences were annotated
as containing the population in the gold standard (an even more lenient criterion).
This baseline was motivated by the observation that the aim and methods sections of
structured abstracts are likely to contain the population information?for structured ab-
stracts, explicit headings provide structural cues; for unstructured abstracts, positional
information serves as a surrogate.
The performance of the population extractor is shown in Table 1. A manual error
analysis revealed three sources of error: First, not all population descriptions contain
a number explicitly, for example, The medical charts of all patients who were treated with
etanercept for back or neck pain at a single private medical clinic in 2003. Second, not all study
populations are population groups, as for example in All primary care trusts in England.
Finally, tagging and chunking errors propagate to the semantic type assignment level
and affect the quality of MetaMap output. For example, consider the following sentence:
We have compared the LD and recombination patterns defined by single-nucleotide
polymorphisms in ENCODE region ENm010, chromosome 7p15 2, in Korean, Japanese,
and Chinese samples.
Both Korean and Japanese were mistagged as nouns, which lead to the following
erroneous chunking:
[We] [have] [compared] [the LD] [and] [recombination patterns] [defined] [by
single-nucleotide polymorphisms] [in] [ENCODE] [region ENm010,] [chromosome
7p15 2,] [in Korean,] [Japanese,] [and] [Chinese samples.]
This resulted in the tagging of Japanese as a population. Errors of this type affect
other extractors as well. For example, lead was mistagged as a noun in the phrase
Echocardiographic findings lead to the right diagnosis, which caused MetaMap to identify
the word as a PHARMACOLOGICAL SUBSTANCE (lead is sometimes used as a homeo-
pathic preparation).
5.3 Problem Extractor
The problem extractor relies on the recognition of concepts belonging to the UMLS
semantic group DISORDER. In short, it returns a ranked list of all such concepts within
a given span of text. We evaluate the performance of this simple heuristic on segments
73
Computational Linguistics Volume 33, Number 1
Table 2
Evaluation of the problem extractor.
Correct (%) Unknown (%) Wrong (%)
Abstract title 85 10 5
Title + 1st two sentences 90 5 5
Entire abstract 86 2 12
of the abstract varying in length: abstract title only, abstract title and first two sentences,
and entire abstract text. Concepts in the title, in the introduction section of structured
abstracts, or in the first two sentences in unstructured abstracts, are given higher confi-
dence values due to their discourse prominence. Finally, the highest-scoring problem
is designated as the primary problem in order to differentiate it from co-occurring
conditions identified in the abstract.
5.4 Evaluation of Problem Extractor
Although our problem extractor returns a list of clinical problems, we only evalu-
ate performance on identification of the primary problem. For some abstracts, MeSH
headings can be used as ground truth, because one of the human indexers? tasks in
assigning terms is to identify the main topic of the article (sometimes a disorder). For
this evaluation, we randomly selected 50 abstracts with disorders indexed as the main
topic from abstracts retrieved using PubMed on the five clinical questions described
in Sneiderman et al (2005).
We applied our problem extractor on different segments of the abstract: the title
only, the title and first two sentences, and the entire abstract. These results are shown in
Table 2. Here, a problem was considered correctly identified only if it shared the same
concept ID as the ground truth problem (from the MeSH heading). The performance of
our best variant (abstract title and first two sentences) approaches the upper bound on
MetaMap performance?which is limited by human agreement on the identification of
semantic concepts in medical texts, as established in Pratt and Yetisgen-Yildiz (2003).
Although problem extraction largely depends on disease coverage in UMLS and
MetaMap performance, the error rate could be further reduced by more sophisticated
recognition of implicitly stated problems. For example, with respect to a question about
immunization in children, an abstract about the measles-mumps-rubella vaccination
never mentioned the disease without the word vaccination; hence, no concept of the
type DISEASE OR SYNDROME was identified.
5.5 Intervention Extractor
The intervention extractor identifies both the intervention and comparison elements in
a PICO frame; processing of these two frame elements can be collapsed because they
belong to the same semantic group. In many abstracts, it is unclear which intervention is
the primary one and which are the comparisons, and hence our extractor simply returns
a list of all interventions under study.
For interventions, we are primarily interested in entities that may participate in the
UMLS Semantic Network relations associated with each clinical task. Restrictions on
the semantic types allowed in these relations prescribe the set of possible clinical in-
terventions. For therapy these relations include treats, prevents, and carries out; diagnoses
74
Demner-Fushman and Lin Answering Clinical Questions
Table 3
Evaluation of the intervention extractor.
Correct (%) Unknown (%) Wrong (%)
Baseline 60 ? 40
Extractor 80 ? 20
for diagnosis; causes and result of for etiology; and prevents for prognosis. At present, the
identification of nine semantic types, for example, DIAGNOSTIC PROCEDURE, CLINICAL
DRUG, and HEALTH CARE ACTIVITY, serves as the foundation for our intervention
extraction algorithm.
Candidate scores are further adjusted to reflect a few different factors. In structured
abstracts, concepts of the relevant semantic type are given additional weight if they
appear in the title, aims, and methods sections. In unstructured abstracts, concepts
towards the beginning of the abstract text are favored. Finally, the intervention extractor
takes into account the presence of certain cue phrases that describe the aim and/or
methods of the study, such as This study examines or This paper describes.
5.6 Evaluation of Intervention Extractor
The intervention extractor was evaluated in the same manner as the population extrac-
tor and compared to the same baseline. To iterate, 90 held-out clinical abstracts that
contained human-annotated interventions served as ground truth. The output of our
intervention extractor was judged to be correct if it occurred in a sentence that was
annotated as containing the intervention in the gold standard. As with the evaluation
of the population extractor, this represents an upper bound on performance. Results are
shown in Table 3.
Some of the errors were caused by ambiguity of terms. For example, in the clause
serum levels of anti-HBsAg and presence of autoantibodies (ANA, ENA) were evaluated,
serum is recognized as a TISSUE, levels as INTELLECTUAL PRODUCT, and autoantibodies
and ANA as IMMUNOLOGIC FACTORS. In this case, however, autoantibodies should
be considered a LABORATORY OR TEST RESULT.3 In other cases, extraction errors
were caused by summary sentences that were very similar to intervention statements,
for example, This study compared the effects of 52 weeks? treatment with pioglitazone, a
thiazolidinedione that reduces insulin resistance, and glibenclamide, on insulin sensitivity,
glycaemic control, and lipids in patients with Type 2 diabetes. For this particular abstract,
the correct interventions are contained in the sentence Patients with Type 2 diabetes
were randomized to receive either pioglitazone (initially 30 mg QD, n = 91) or micronized
glibenclamide (initially 1.75 mg QD, n = 109) as monotherapy.
5.7 Outcome Extractor
We approached outcome extraction as a classification problem at the sentence level, that
is, the outcome extractor assigns a probability of being an outcome to each sentence
in an abstract. Our preliminary work has led to a strategy based on an ensemble of
classifiers, which includes a rule-based classifier, a unigram ?bag of words? classifier,
3 MetaMap does provide alternative mappings, but the current extractor only considers the best candidate.
75
Computational Linguistics Volume 33, Number 1
an n-gram classifier, a position classifier, an abstract length classifier, and a semantic
classifier. With the exception of the rule-based classifier, all classifiers were trained on
the 275 citations from the annotated collection of abstracts described previously.
Knowledge for the rule-based classifier was hand-coded, prior to the annotation
effort, by a registered nurse with 20 years of clinical experience. This classifier estimates
the likelihood that a sentence states an outcome based on cue phrases such as signif-
icantly greater, well tolerated, and adverse events. The likelihood of a sentence being an
outcome as indicated by cue phrases is the ratio of the cumulative score for recognized
phrases to the maximum possible score. For example, the sentence The dropout rate due to
adverse events was 12.4% in the moxonidine and 9.8% in the nitrendipine group is segmented
into eight phrases by MetaMap, which sets the maximum score to 8. The two phrases
dropout rate and adverse events contribute one point each to the cumulative score, which
results in a likelihood estimate of 0.25 for this sentence.
The unigram ?bag of words? classifier is a naive Bayes classifier implemented with
the API provided by the MALLET toolkit.4 This classifier outputs the probability of a
class assignment.
The n-gram based classifier is also a naive Bayes classifier, but it operates on a differ-
ent set of features. We first identified the most informative unigrams and bigrams using
the information gain measure (Yang and Pedersen 1997), and then selected only the
positive outcome predictors using odds ratio (Mladenic and Grobelnik 1999). Disease-
specific terms, such as rheumatoid arthritis, were then manually removed. Finally, the
list of features was revised by the registered nurse who participated in the annotation
effort. This classifier also outputs the probability of a class assignment.
The position classifier returns the maximum likelihood estimate that a sentence is
an outcome based on its position in the abstract (for structured abstracts, with respect
to the results or conclusions sections; for unstructured abstracts, with respect to the end
of the abstract).
The abstract length classifier returns a smoothed (add one smoothing) probability
that an abstract of a given length (in the number of sentences) contains an outcome
statement. For example, the probability that an abstract four sentences long contains an
outcome statement is 0.25, and the probability of finding an outcome in a ten sentence?
long abstract is 0.92. This feature turns out to be useful because the average length of
abstracts with and without outcome statements differs: 11.7 sentences for the former,
7.95 sentences for the latter.
The semantic classifier assigns to a sentence an ad hoc score based on the presence
of UMLS concepts belonging to semantic groups highly associated with outcomes
such as THERAPEUTIC PROCEDURE or PHARMACOLOGICAL SUBSTANCE. The score is
given a boost if the concept has already been identified as the primary problem or an
intervention.
The outputs of our basic classifiers are combined using a simple weighted linear
interpolation scheme:
Soutcome = ?1Scues + ?2Sunigram + ?3Sn-gram + ?4Sposition + ?5Slength + ?6Ssemantic type (1)
We attempted two approaches for assigning these weights. The first method relied
on ad hoc weight selection based on intuition. The second involved a more principled
method using confidence values generated by the base classifiers and least squares lin-
4 http://mallet.cs.umass.edu/
76
Demner-Fushman and Lin Answering Clinical Questions
ear regression adapted for classification (Ting and Witten 1999), which can be described
by the following equation:
LR(x) =
N
?
k=1
?kPk(X) (2)
Pk is the probability that a sentence specifies an outcome, as determined by classifier
k (for classifiers that do not return actual probabilities, we normalized the scores and
treated them as such). To predict the class of a sentence, the probabilities generated
by n classifiers are combined using the coefficients (?0, ...,?n). These values are de-
termined in the training stage as follows: Probabilities predicted by base classifiers
for each sentence are represented in an N ? M matrix A, where M is the number of
sentences in the training set, and N is the number of classifiers. The gold standard
class assignments for each sentence is stored in a vector b, and weights are found by
computing the vector ? that minimizes ||A?? b||. The solution can be found using
singular value decomposition, as provided in the JAMA basic linear algebra package
released by NIST.5
5.8 Evaluation of Outcome Extractor
Because outcome statements were annotated in each of the 633 citations in our collec-
tion, it was possible to evaluate the outcome extractor on a broader set of abstracts. From
those not used in training the outcome classifiers, 153 citations pertaining to therapy
were selected. Of these, 143 contained outcome statements and were used as the blind
held-out test set. In addition, outcome statements in abstracts pertaining to diagnosis
(57), prognosis (111), and etiology (37) were also used.
The output of our outcome extractor is a ranked list of sentences sorted by con-
fidence. Based on the observation that human annotators typically mark two to three
sentences in each abstract as outcomes, we evaluated the performance of our extractor
at cutoffs of two and three sentences. These results are shown in Table 4: The columns
marked AH2 and AH3 show performance of the weighted linear interpolation approach
with ad hoc weight assignment at two- and three-sentence cutoffs, respectively; the
columns marked LR2 and LR3 show performance of the least squares linear regression
model at the same cutoffs. In the evaluation, our outcome extractor was considered
correct if the returned sentences intersected with sentences judged as outcomes by
our human annotators. Although this is a lenient criterion, it does roughly capture
the performance of our knowledge extractor. Because outcome statements are typically
found in the conclusion of a structured abstract (or near the end of the abstract in the
case of unstructured abstracts), we compared our answer extractor to the baseline of
returning either the final two or final three sentences in the abstract (B2 and B3 in
Table 4).
As can be seen, variants of our outcome extractor performed better than the baseline
at the two-sentence cutoff, for the most part. Bigger improvements, however, can be
seen at the three-sentence cutoff level. It is evident that the assignment of weights in
our ad hoc model is primarily geared towards therapy questions, perhaps overly so.
Better overall performance is obtained with the least squares linear regression model.
5 http://math.nist.gov/javanumerics/jama/
77
Computational Linguistics Volume 33, Number 1
Table 4
Evaluation of the outcome extractor. B = baseline, returns last sentences in abstract; AH = ad hoc
weight assignment; LR = least squares linear regression. Statistically significant improvement
over the baseline at the 1% level is indicated by  .
2-sentence cutoff (%) 3-sentence cutoff (%)
B2 AH2 LR2 B3 AH3 LR3
Therapy 74 75 77 75 95 93
Diagnosis 72 70 78 75 78 89
Prognosis 73 76 79 85 87 89
Etiology 64 68 74 78 83 88
Table 5
Examples of strength of evidence categories based on Publication Type and MeSH headings.
Strength of Evidence Publication Type/MeSH
Level A(1) Meta-analysis, randomized controlled trials, cohort study,
follow-up study
Level B(2) Case-control study, case series
Level C(3) Case report, in vitro, animal and animal testing,
alternatives studies
The majority of errors made by the outcome extractor were related to inaccurate
sentence boundary identification, chunking errors, and word sense ambiguity in the
Metathesaurus.
5.9 Determining the Strength of Evidence
The strength of evidence is a classification scheme that helps physicians assess the
quality of a particular citation for clinical purposes. Metadata associated with most
MEDLINE citations (MeSH terms) are extensively used to determine the strength of
evidence and in our EBM citation scoring algorithm (Section 6).
The potential highest level of the strength of evidence for a given citation can be
identified using the Publication Type (a metadata field) and MeSH terms pertaining
to the type of the clinical study. Table 5 shows our mapping from publication type
and MeSH headings to evidence grades based on principles defined in the Strength
of Recommendations Taxonomy (Ebell et al 2004).
5.10 Sample Output
A complete example of our knowledge extractors working in unison is shown in
Figure 2, which contains an abstract retrieved in response to the following question:
?In children with an acute febrile illness, what is the efficacy of single-medication
therapy with acetaminophen or ibuprofen in reducing fever?? (Kauffman, Sawyer, and
Scheinbaum 1992). Febrile illness is the only concept mapped to DISORDER, and hence
is identified as the primary problem. 37 otherwise healthy children aged 2 to 12 years is
correctly identified as the population. Acetaminophen, ibuprofen, and placebo are correctly
78
Demner-Fushman and Lin Answering Clinical Questions
Antipyretic efficacy of ibuprofen vs acetaminophen
Kauffman RE, Sawyer LA, Scheinbaum ML
Am J Dis Child. 1992 May;146(5):622-5
OBJECTIVE?To compare the antipyretic efficacy of ibuprofen, placebo, and
acetaminophen. DESIGN?Double-dummy, double-blind, randomized, placebo-
controlled trial. SETTING?Emergency department and inpatient units of a large,
metropolitan, university-based, children?s hospital in Michigan. PARTICIPANTS?
??
37
???????????
otherwise
????????
healthy
?????????
children
?????
aged
??
2
???
to
???
12
??????
yearsPopulation with
??????
acute,
?????????????
intercurrent,
??????
febrile
???????
illnessProblem. INTERVENTIONS?Each child was randomly assigned to receive
a single dose of
???????????????
acetaminophenIntervention (10 mg/kg),
??????????
ibuprofenIntervention (10 mg/kg) (7.5
or 10 mg/kg), or
????????
placeboIntervention (10 mg/kg). MEASUREMENTS/MAIN RESULTS?
Oral temperature was measured before dosing, 30 minutes after dosing, and
hourly thereafter for 8 hours after the dose. Patients were monitored for ad-
verse effects during the study and 24 hours after administration of the as-
signed drug.
???
All
?????
three
??????
active
???????????
treatments
??????????
produced
??????????
significant
???????????
antipyresis
???????????
compared
????
with
?????????
placebo.Outcome
??????????
Ibuprofen
??????????
provided
???????
greater
?????????????
temperature
??????????
decrement
?????
and
???????
longer
????????
duration
???
of
???????????
antipyresis
?????
than
???????????????
acetaminophen
??????
when
???
the
????
two
??????
drugs
?????
were
??????????????
administered
??
in
???????????????
approximately
??????
equal
??????
doses.Outcome No adverse effects were observed in any treat-
ment group. CONCLUSION?
?????????
Ibuprofen
???
is
??
a
????????
potent
???????????
antipyretic
???????
agent
?????
and
??
is
???
a
????
safe
???????????
alternative
????
for
????
the
?????????
selected
???????
febrile
??????
child
?????
who
?????
may
????????
benefit
?????
from
????????????
antipyretic
???????????
medication
????
but
?????
who
??????
either
???????
cannot
????
take
???
or
?????
does
????
not
????????
achieve
???????????
satisfactory
????????????
antipyresis
????
with
????????????????
acetaminophen.Outcome
Publication Type: Clinical Trial, Randomized Controlled Trial
PMID: 1621668
Strength of Evidence: grade A
Figure 2
Sample output from our PICO extractors.
extracted as the interventions under study. The three outcome sentences are correctly
classified; the short sentence concerning adverse effects was ranked lower than the
other three sentences and hence below the cutoff. The study design, from metadata
associated with the citation, allows our strength of evidence extractor to classify this
article as grade A.
6. Operationalizing Evidence-Based Medicine
In our view of clinical question answering, the knowledge extractors just described sup-
ply the features on which semantic matching occurs. This section describes an algorithm
that, when presented with a structured representation of an information need and a
MEDLINE citation, automatically computes a topical relevance score in accordance with
the principles of EBM.
In order to develop algorithms that operationalize the three facets of EBM, it is
necessary to possess a corpus of clinical questions on which to experiment. Because no
such test collection exists, we had to first manually create one. Fortunately, collections
of clinical questions (representing real-world information needs of physicians), are
79
Computational Linguistics Volume 33, Number 1
Table 6
Composition of our clinical questions collection.
Therapy Diagnosis Prognosis Etiology Total
Development 10 6 3 5 24
Test 12 6 3 5 26
available on-line. From two sources, the Journal of Family Practice6 and the Parkhurst
Exchange,7 we gathered 50 clinical questions, which capture a realistic sampling of the
scenarios that a clinical question-answering system would be confronted with. These
questions were minimally modified from their original form as downloaded from the
World Wide Web. In a few cases, a single question actually consisted of several smaller
questions; such clusters were simplified by removing questions more peripheral to the
central clinical problem. All questions were manually classified into one of the four
clinical tasks; the distribution of the questions roughly follows the prevalence of each
task type as observed in natural settings, noted by Ely et al (1999). The final step in
the preparation process was manual translation of the natural language questions into
PICO query frames.
Our collection was divided into a development set and a blind held-out test set for
verification purposes. The breakdown of these questions into the four clinical tasks and
the development/test split is shown in Table 6. An example of each question type from
our development set is presented here, along with its query frame:
Does quinine reduce leg cramps for young athletes? (Therapy)
search task: therapy selection
primary problem: leg cramps
co-occurring problems: muscle cramps, cramps
population: young adult
intervention: quinine
How often is coughing the presenting complaint in patients with gastroesophageal
reflux disease? (Diagnosis)
search task: differential diagnosis
primary problem: gastroesophageal reflux disease
co-occurring problems: cough
What?s the prognosis of lupoid sclerosis? (Prognosis)
search task: patient outcome prediction
primary problem: lupus erythematosus
co-occurring problems: multiple sclerosis
What are the causes of hypomagnesemia? (Etiology)
search task: cause determination
primary problem: hypomagnesemia
6 http://www.jfponline.com/
7 http://www.parkhurstexchange.com/qa/
80
Demner-Fushman and Lin Answering Clinical Questions
As discussed earlier, we do not believe that natural language text is the best input
for a question-answering system. Instead, a structured PICO-based representation cap-
tures physicians? information needs in a more perspicuous manner?primarily because
clinicians are trained to analyze clinical situations with this framework.
Mirroring the organization of our knowledge extractors, we broke up the P in PICO
into population, primary problem, and co-occurring problems in the query representa-
tion. The justification for this will become apparent when we present our algorithm for
scoring MEDLINE citations, as each of these three facets must be treated differently.
Note that many elements are specified only to the extent that they were explicit in
the original natural language question; for example, if the clinician does not specify
a population, that element will be empty. Finally, outcomes are not directly encoded in
the query representation because they are implicit most of the time; for example, in Does
quinine reduce leg cramps for young athletes?, the desired outcome, naturally, is to reduce
the occurrence and severity of leg cramps. Nevertheless, outcome identification is an
important component of the citation scoring algorithm, as we shall see later.
What is the relevance of an abstract with respect to a particular clinical question?
Evidence-based medicine outlines the need to consider three different facets (see Sec-
tion 2), which we operationalize in the following manner:
SEBM = SPICO + SSoE + Stask (3)
The relevance of a particular citation, with respect to a structured query, includes
contributions from matching PICO structures, the strength of evidence of the citation,
and factors specifically associated with the search tasks (and indirectly, the clinical
tasks). In what follows, we describe each of these contributions in detail.
Viewed as a whole, each score component is a heuristic reflection of the factors that
enter into consideration when a physician examines a MEDLINE citation. Although the
assignment of numeric scores is based on intuition and may seem ad hoc in many cases,
evaluation results in the next section demonstrate the effectiveness of our algorithm.
This issue will be taken up further in Section 8.
6.1 Scores Based on PICO Elements
The score of an abstract based on extracted PICO elements, SPICO, is broken into individ-
ual components according to the following formula:
SPICO = Sproblem + Spopulation + Sintervention + Soutcome (4)
The first component in the equation, Sproblem, reflects a match between the primary
problem in the query frame and the primary problem in the abstract (i.e., the highest-
scoring problem identified by the problem extractor). A score of 1 is given if the prob-
lems match exactly based on their unique UMLS concept ID as provided by MetaMap.
Matching based on concept IDs has the advantage that it abstracts away from termino-
logical variation; in essence, MetaMap performs terminological normalization. Failing
an exact match of concept IDs, a partial string match is given a score of 0.5. If the primary
problem in the query has no overlap with the primary problem from the abstract, a score
of ?1 is given. Finally, if our problem extractor could not identify a problem (but the
query frame does contain a problem), a score of ?0.5 is given.
Co-occurring problems must be taken into consideration in the differential diagnosis
and cause determination search tasks because knowledge of the problems is typically
81
Computational Linguistics Volume 33, Number 1
incomplete in these scenarios. Therefore, physicians would normally be interested in
any problems mentioned in the abstracts in addition to the primary problem specified
in the query frame. As an example, consider the question What is the differential diagnosis
of chronic diarrhea in immunocompetent patients? Although chronic diarrhea is the primary
problem, citations that discuss additional related disorders should be favored over those
that don?t. In terms of actual scoring, disorders mentioned in the title receive three
points, and disorders mentioned anywhere else receive one point (in addition to the
match score based on the primary problem, as discussed).
Scores based on population and intervention, Spopulation and Sintervention respectively, mea-
sure the overlap between query frame elements and corresponding elements extracted
from abstracts. A point is given to each matching intervention and matching population.
For example, finding the population group children from a query frame in the abstract
increments the match score; the remaining words in the abstract population are ignored.
Thus, if the query frame contains a population element and an intervention element, the
score for an abstract that contains the same UMLS concepts in the corresponding slots
is incremented by two.
The outcome-based score, Soutcome, is simply the value assigned to the highest-scoring
outcome sentence (we employed the outcome extractor based on the linear regression
model for our experiments). As outcomes are rarely explicitly specified in the original
question, we decided to omit them in the query representation. Our citation scoring
algorithm simply considers the inherent quality of the outcome statements in an ab-
stract, independent of the query. This is justified because, given a match on the primary
problem, all clinical outcomes are likely to be of interest to the physician.
6.2 Scores Based on Strength of Evidence
The relevance score component based on the strength of evidence is calculated in the
following manner:
SSoE = Sjournal + Sstudy + Sdate (5)
Citations published in core and high-impact journals such as Journal of the American
Medical Association (JAMA) get a score of 0.6 for Sjournal, and 0 otherwise. In terms of the
study type, Sstudy, clinical trials, such as randomized controlled trials, receive a score of
0.5; observational studies, for example, case reports, 0.3; all non-clinical publications,
?1.5; and 0 otherwise. The study type is directly encoded in the Publication Type field
of a MEDLINE citation.
Finally, recency factors into the strength of evidence score according to the formula:
Sdate = (yearpublication ? yearcurrent )/100 (6)
A mild penalty decreases the score of a citation proportionally to the time difference
between the date of the search and the date of publication.
6.3 Scores Based on Specific Tasks
The final component of our EBM score is based on task-specific considerations, as
reflected in manually assigned MeSH terms. For search tasks falling into each clinical
task, we gathered a list of terms that are positive and negative indicators of relevance.
82
Demner-Fushman and Lin Answering Clinical Questions
The task score, Stask, is given by:
Stask =
?
t?MeSH
?(t) (7)
The function ?(t) maps a MeSH term to a positive score if the term is a positive
indicator for that particular task type, or a negative score if the term is a negative indi-
cator for the clinical task. Note that although our current system uses MeSH headings
assigned by human indexers, manually assigned terms can be replaced with automatic
processing if needed (Aronson et al 2004).
Below, we enumerate the relevant indicator terms by clinical task. However, there
is a set of negative indicators common to all tasks; these were extracted from the set
of genomics articles provided for the secondary task in the TREC 2004 genomics track
evaluation (Hersh, Bhupatiraju, and Corley 2004); examples include genetics and cell
physiology. The positive and negative weights assigned to each term heuristically encode
the relative importance of different MeSH headings and are derived from the Clinical
Queries filters in PubMed, from the JAMA EBM tutorial series on critical appraisal of
medical literature, from MeSH scope notes, and based on a physician?s understanding
of the domain (the first author).
Indicators for Therapy Tasks. Positive indicators for therapy were derived from the
PubMed?s Clinical Queries filters; examples include drug administration routes and any
of its children in the MeSH hierarchy. A score of ?1 is given if the MeSH descriptor
or qualifier is marked as the main theme of the article (indicated via the star notation
by human indexers), and a score of ?0.5 otherwise. If the question pertains to the
search task of prevention, three additional headings are considered positive indicators:
prevention and control, prevention measures, and prophylaxis.
Indicators for Diagnosis Tasks. Positive indicators for therapy are also used as negative
indicators for diagnosis because the relevant studies are usually disjoint; it is highly
unlikely that the same clinical trial will study both diagnostic methods and treatment
methods. The MeSH term diagnosis and any of its children are considered positive
indicators. As with therapy questions, terms marked as the major theme get a score of
?1.0, and ?0.5 otherwise. This general assignment of indicator terms allows a system
to differentiate between questions such as Does a Short Symptom Checklist accurately
diagnose ADHD? and What is the most effective treatment for ADHD in children?, which
might retrieve very similar sets of citations.
Indicators for Prognosis Tasks. Positive indicators for prognosis include the following
MeSH terms: survival analysis, disease-free survival, treatment outcome, health status, preva-
lence, risk factors, disability evaluation, quality of life, and recovery of function. For terms
marked as the major theme, a score of +2 is given; +1 otherwise. There are no negative
indicators, other than those common to all tasks previously described.
Indicators for Etiology Tasks. Negative indicators for etiology include therapy-oriented
MeSH terms; these terms are given a score of ?0.3. Positive indicators for the diag-
nosis task are weak positive indicators for etiology, and receive a positive score of
+0.1. The following MeSH terms are considered highly indicative of citations rele-
vant to etiology: population at risk, risk factors, etiology, causality, and physiopathology. If
83
Computational Linguistics Volume 33, Number 1
one of these terms is marked as the major theme, a score of +2 is given; otherwise, a
score of +1 is given.
7. Evaluation of Citation Scoring
The previous section describes a relevance-scoring algorithm for MEDLINE citations
that attempts to capture the principles of EBM. In this section, we present an evaluation
of this algorithm.
Ideally, questions should be answered by directly comparing queries to knowl-
edge structures derived from MEDLINE citations. However, knowledge extraction on
such large scales is impractical given our computational resources, so we opted for
an IR-based pipeline approach. Under this strategy, an existing search engine would
be employed to generate a candidate list of citations to be rescored, according to our
algorithm. PubMed is a logical choice for gathering this initial list of citations because
it represents one of the most widely used tools employed by physicians and other
health professionals today. The system supports boolean operators and sorts results
chronologically, most recent citations first.
This two-stage retrieval process immediately suggests an evaluation methodology
for our citation scoring algorithm?as a document reranking task. Given an initial hit
list, can our algorithm automatically re-sort the results such that relevant documents
are brought to higher ranks? Not only is such a task intuitive to understand, this
conceptualization also lends itself to an evaluation based on widely accepted practices
in information retrieval.
For each question in our test collection, PubMed queries were manually crafted to
fetch an initial set of hits. These queries took advantage of existing advanced search
features to simulate the types of results that would be currently available to a knowl-
edgeable physician. Specifically, widely accepted tools for narrowing down PubMed
search results such as Clinical Queries were employed whenever appropriate.
As a concrete example, consider the following question: What is the best treatment for
analgesic rebound headaches? The search started with the initial terms ?analgesic rebound
headache? with a ?narrow therapy filter.? In PubMed, this query is:
((?headache disorders?[TIAB] NOT Medline[SB]) OR ?headache disorders?[MeSH
Terms] OR analgesic rebound headache[Text Word]) AND (randomized controlled
trial[Publication Type] OR (randomized[Title/Abstract] AND
controlled[Title/Abstract] AND trial[Title/Abstract])) AND hasabstract[text] AND
English[Lang] AND ?humans?[MeSH Terms]
Note that PubMed automatically identifies concepts and attempts matching both
in abstract text and MeSH headings. We always restrict searches to articles that have
abstracts, are published in English, and are assigned the MeSH term humans (as opposed
to say, experiments on animals)?these are all strategies commonly used by clinicians.
In this case, because none of the top 20 results were relevant, the query was ex-
panded with the term side effects to emphasize the aspect of the problem requiring an
intervention. The final query for the question became:
(((?analgesics?[TIAB] NOT Medline[SB]) OR ?analgesics?[MeSH Terms] OR
?analgesics?[Pharmacological Action] OR analgesic[Text Word]) AND
((?headache?[TIAB] NOT Medline[SB]) OR ?headache?[MeSH Terms] OR
headaches[Text Word]) AND (?adverse effects?[Subheading] OR side effects[Text
Word])) AND hasabstract[text] AND English[Lang] AND ?humans?[MeSH Terms]
84
Demner-Fushman and Lin Answering Clinical Questions
The first author, who is a medical doctor, performed the query formulation process
manually for every question in our collection; she verified that each hit list contained at
least some relevant documents and that the results were as good as could be reasonably
achieved. The process of generating queries averaged about 40 minutes per question.
The top 50 results for each query were retained for our experiments. In total, 2,309
citations were gathered because some queries returned fewer than 50 hits. The process
of generating a ?good? PubMed query is not a trivial task, which we have side-stepped
in this work by placing a human in the loop. We return to this issue in Section 12.
All abstracts gathered by this process were exhaustively examined for relevance by
the first author. It is important to note that relevance assessment in the clinical domain
requires significant medical knowledge (in short, a medical degree). After careful con-
sideration, we decided to assess only topical relevance, with the understanding that
the applicability of information from a specific citation in real-world settings depends
on a variety of other factors (see Section 10 for further discussion). Each citation was
assigned one of four labels:
 Contains answer: The citation directly contains information that answers
the question.
 Relevant: The citation does not directly answer the question, but provides
topically relevant information.
 Partially relevant: The citation provides information that is marginally
relevant.
 Not relevant: The citation does not provide any topically relevant
information.
Because all abstracts were judged, we did not have to worry about impartiality
issues when comparing different systems. In total, the relevance assessment process
took approximately 100 hours, or about an average of 2 hours per question.
Our reranking experiment compared four different systems:
 The baseline PubMed results.
 A term-based reranker that computes term overlap between the natural
language question and the citation (i.e., counted words shared between the
two strings). Each term match was weighted by the outcome score of the
sentence from which it came (see Section 5.7). This simple algorithm favors
term matches that occur in sentences recognized as outcome statements.
 A reranker based on the EBM scorer described in the previous section.
 A reranker that combines normalized scores from the term-based reranker
and the EBM-based reranker (weighted linear interpolation).
Questions in the development set were used to debug the EBM-based reranker as
we implemented the scoring algorithm. The development questions were also used to
tune the weight for combining scores from the term-based scorer and EBM-based scorer;
by simply trying all possible values, we settled on a ? of 0.8, that is, 80% weight to the
EBM score, and 20% weight to the term-based score. As we shall see later, it is unclear
if evidence combination in this simple manner helps at all; for one, it is debatable
which metric should be optimized. The test questions were hidden during the system
85
Computational Linguistics Volume 33, Number 1
development phase and served as a blind held-out test set for assessing the generality
of our algorithm.
In our experiment, we collected the following metrics, all computed automatically
using our relevance judgments:
 Precision at ten retrieved documents (P10) measures the fraction of
relevant documents in the top ten results.
 Mean Average Precision (MAP) is the average of precision values after
each relevant document is retrieved (Baeza-Yates and Ribeiro-Neto 1999).
It is the most widely accepted single-value metric in information retrieval,
and is seen to balance the need for both precision and recall.
 Mean Reciprocal Rank (MRR) is a measure of how far down a hit list the
user must browse before encountering the first relevant result. The score is
equal to the reciprocal of the rank, that is, a relevant document at rank 1
gets a score of 1, 1/2 at rank 2, 1/3 at rank 3, and so on. Note that this
measure only captures the appearance of the first relevant document.
Furthermore, due to its discretization, MRR values are noisy on small
collections.
 Total Document Reciprocal Rank (TDRR) is the sum of the reciprocal
ranks of all relevant documents. For example, if relevant documents were
found at ranks 2 and 5, the TDRR would be 1/2 + 1/5 = 0.7. TDRR
provides an advantage over MRR in that it captures the ranks of all
relevant documents?emphasizing their appearance at higher ranks. The
downside, however, is that TDRR does not have an intuitive interpretation.
For our reranking experiment, we applied the Wilcoxon signed-rank test to deter-
mine the statistical significance of the results. This test is commonly used in information
retrieval research because it makes minimal assumptions about the underlying distrib-
ution of differences. For each evaluation metric, significance at the 1% level is indicated
by either  or , depending on the direction of change; significance at the 5% level
is indicated by  or , depending on the direction of change. Differences that are not
statistically significant are marked with the symbol ?.
We report results under two different scoring criteria. Under the lenient condition,
documents marked ?contains answer? and ?relevant? were given credit; these results
are shown in Table 7 (for the development set) and Table 8 (for the blind held-out test
set). Across all questions, both the EBM-based reranker and combination reranker sig-
nificantly outperform the PubMed baseline on all metrics. In many cases, the differences
are particularly noteworthy?for example, our EBM citation scoring algorithm more
than doubles the baseline in terms of MAP and P10 on the test set. There are enough
therapy questions to achieve statistical significance in the task-specific results; however,
due to the smaller number of questions for the other clinical tasks, those results are
not statistically significant. Results also show that the simple term-based reranker out-
performs the PubMed baseline, demonstrating the importance of recognizing outcome
statements in MEDLINE abstracts.
Are the differences in performance between the term-based, EBM, and combination
rerankers statistically significant? Results of Wilcoxon signed-rank tests are shown in
Table 11. Both the EBM and combination rerankers significantly outperform the term-
based reranker (at the 1% level, on all metrics, on both development and test set), with
86
Demner-Fushman and Lin Answering Clinical Questions
Table 7
(Lenient, Development) Lenient results of reranking experiment on development questions
for the baseline PubMed condition, term-based reranker, EBM-based reranker, and combination
reranker.
Therapy Diagnosis Prognosis Etiology
Precision at 10 (P10)
PubMed 0.300 0.367 0.400 0.533
Term 0.520 (+73%) 0.383 (+4.5%)? 0.433 (+8.3%)? 0.553 (+3.8%)?
EBM 0.730 (+143%) 0.800 (+118%) 0.633 (+58%)? 0.553 (+3.7%)?
Combo 0.750 (+150%) 0.783 (+114%) 0.633 (+58%)? 0.573 (+7.5%)?
Mean Average Precision (MAP)
PubMed 0.354 0.421 0.385 0.608
Term 0.622 (+76%) 0.438 (+4.0%)? 0.464 (+21%)? 0.720 (+18%)?
EBM 0.819 (+131%) 0.794 (+89%) 0.635 (+65%)? 0.649 (+6.7%)?
Combo 0.813 (+130%) 0.759 (+81%) 0.644 (+67%)? 0.686 (+13%)?
Mean Reciprocal Rank (MRR)
PubMed 0.428 0.792 0.733 0.900
Term 0.853 (+99%) 0.739 (?6.7%)? 0.833 (+14%)? 1.000 (+11%)?
EBM 0.933 (+118%) 0.917 (+16%)? 0.667 (?9.1%)? 1.000 (+11%)?
Combo 0.933 (+118%) 0.917 (+16%)? 1.000 (+36%)? 0.900 (+0.0%)?
Total Document Reciprocal Rank (TDRR)
PubMed 1.317 1.805 1.778 2.008
Term 2.305 (+75%) 1.887 (+4.6%)? 1.923 (+8.2%)? 2.291 (+14%)?
EBM 2.869 (+118%) 2.944 (+63%) 2.238 (+26%)? 2.104 (+4.8%)?
Combo 2.833 (+115%) 2.870 (+59%) 2.487 (+40%)? 2.108 (+5.0%)?
(a) Breakdown by clinical task
P10 MAP MRR TDRR
PubMed 0.378 0.428 0.656 1.640
Term 0.482 (+28%)? 0.577 (+35%) 0.853 (+30%)? 2.150 (+31%)
EBM 0.699 (+85%) 0.754 (+76%) 0.910 (+39%) 2.650 (+62%)
Combo 0.707 (+87%) 0.752 (+76%) 0.931 (+42%) 2.648 (+61%)
(b) Performance across all clinical tasks
 Significance at the 1% level, depending on direction of change.
 Significance at the 5% level, depending on direction of change.
?Difference not statistically significant.
the exception of MRR on the development set. However, for all metrics, on both the
development set and test set, there is no significant difference between the EBM and
combination reranker (which combines both term-based and EBM-based evidence). In
the parameter tuning process, we could not find a weight where performance across all
measures was higher; in the end, we settled on what we felt was a reasonable weight
that improved P10 and MRR on the development set.
Under the strict condition, only documents marked ?contains answer? were given
credit; these results are shown in Table 9 (for the development set) and Table 10
(for the blind held-out test set). The same trend is observed?in fact, larger relative
gains were achieved under the strict scoring criteria for our EBM and combination
87
Computational Linguistics Volume 33, Number 1
Table 8
(Lenient, Test) Lenient results of reranking experiment on blind held-out test questions for
the baseline PubMed condition, term-based reranker, EBM-based reranker, and combination
reranker.
Therapy Diagnosis Prognosis Etiology
Precision at 10 (P10)
PubMed 0.350 0.150 0.200 0.320
Term 0.575 (+64%) 0.383 (+156%)? 0.333 (+67%)? 0.460 (+43%)?
EBM 0.783 (+124%) 0.583 (+289%) 0.467 (+133%)? 0.660 (+106%)?
Combo 0.792 (+126%) 0.633 (+322%) 0.433 (+117%)? 0.660 (+106%)?
Mean Average Precision (MAP)
PubMed 0.421 0.279 0.235 0.364
Term 0.563 (+34%) 0.489 (+76%)? 0.415 (+77%)? 0.480 (+32%)?
EBM 0.765 (+82%) 0.637 (+129%) 0.722 (+207%)? 0.701 (+93%)?
Combo 0.770 (+83%) 0.653 (+134%) 0.690 (+194%)? 0.687 (+89%)?
Mean Reciprocal Rank (MRR)
PubMed 0.579 0.443 0.456 0.540
Term 0.660 (+14%)? 0.765 (+73%)? 0.611 (+34%)? 0.650 (+20%)?
EBM 0.917 (+58%) 0.889 (+101%)? 1.000 (+119%)? 1.000 (+85%)?
Combo 0.958 (+66%) 0.917 (+107%)? 1.000 (+119%)? 1.000 (+85%)?
Total Document Reciprocal Rank (TDRR)
PubMed 1.669 0.926 0.895 1.381
Term 2.204 (+32%) 1.880 (+103%)? 1.390 (+55%)? 1.736 (+26%)?
EBM 2.979 (+79%) 2.341 (+153%) 2.101 (+138%)? 2.671 (+93%)?
Combo 3.025 (+81%) 2.380 (+157%) 2.048 (+129%)? 2.593 (+88%)?
(a) Breakdown by clinical task
P10 MAP MRR TDRR
PubMed 0.281 0.356 0.526 1.353
Term 0.481 (+71%) 0.513 (+44%) 0.677 (+29%)? 1.945 (+44%)
EBM 0.677 (+141%) 0.718 (+102%) 0.936 (+78%) 2.671 (+98%)
Combo 0.688 (+145%) 0.718 (+102%) 0.962 (+83%) 2.680 (+98%)
(b) Performance across all clinical tasks
Significance at the 1% level, depending on direction of change.
Significance at the 5% level, depending on direction of change.
?Difference not statistically significant.
rerankers. Results of Wilcoxon signed-rank tests on the term-based, EBM, and com-
bination rerankers are also shown in Table 11 for the strict scoring condition. In most
cases, combining term scoring with EBM scoring does not help. In almost all cases,
the EBM and combination reranker perform significantly better than the term-based
reranker.
How does better ranking of citations impact end-to-end question answering perfor-
mance? We shall return to this issue in Sections 9 and 10, which describe and evaluate
the answer generation module, respectively. In the next section, we describe more
detailed experiments with our EBM citation scoring algorithm.
88
Demner-Fushman and Lin Answering Clinical Questions
Table 9
(Strict, Development) Strict results of reranking experiment on development questions for the
baseline PubMed condition, term-based reranker, EBM-based reranker, and combination
reranker.
Therapy Diagnosis Prognosis Etiology
Precision at 10 (P10)
PubMed 0.130 0.133 0.100 0.253
Term 0.230 (+77%)? 0.217 (+63%)? 0.233 (+133%)? 0.293 (+16%)?
EBM 0.350 (+170%) 0.350 (+163%)? 0.267 (+167%)? 0.293 (+16%)?
Combo 0.350 (+170%) 0.367 (+175%)? 0.300 (+200%)? 0.313 (+24%)?
Mean Average Precision (MAP)
PubMed 0.088 0.108 0.058 0.164
Term 0.205 (+134%)? 0.142 (+32%)? 0.090 (+54%)? 0.246 (+50%)?
EBM 0.314 (+260%)? 0.259 (+140%)? 0.105 (+79%)? 0.265 (+62%)?
Combo 0.301 (+244%)? 0.248 (+130%)? 0.129 (+122%)? 0.273 (+67%)?
Mean Reciprocal Rank (MRR)
PubMed 0.350 0.453 0.394 0.367
Term 0.409 (+17%)? 0.581 (+28%)? 0.528 (+34%)? 0.700 (+91%)?
EBM 0.675 (+93%) 0.756 (+67%)? 0.444 (+13%)? 0.800 (+118%)?
Combo 0.569 (+63%) 0.676 (+49%)? 0.833 (+111%)? 0.700 (+91%)?
Total Document Reciprocal Rank (TDRR)
PubMed 0.610 0.711 0.568 0.721
Term 0.872 (+43%)? 1.022 (+44%)? 0.804 (+42%)? 1.224 (+70%)?
EBM 1.434 (+135%) 1.601 (+125%)? 0.824 (+45%)? 1.298 (+80%)?
Combo 1.282 (+110%) 1.502 (+111%)? 1.173 (+106%)? 1.241 (+72%)?
(a) Breakdown by clinical task
P10 MAP MRR TDRR
PubMed 0.153 0.105 0.385 0.653
Term 0.240 (+57%) 0.183 (+75%)? 0.527 (+37%) 0.974 (+49%)
EBM 0.328 (+115%) 0.264 (+152%) 0.693 (+80%) 1.371 (+110%)
Combo 0.340 (+123%) 0.260 (+148%) 0.656 (+71%) 1.315 (+101%)
(b) Performance across all clinical tasks
 Significance at the 1% level, depending on direction of change.
 Significance at the 5% level, depending on direction of change.
?Difference not statistically significant.
8. Optimizing Citation Scoring
A potential, and certainly valid, criticism of our EBM citation scoring algorithm is its
ad hoc nature. Weights for various features were assigned based on intuition, reflecting
our understanding of the domain and our knowledge about the principles of evidence-
based medicine. Parameters were fine-tuned during the system implementation process
by actively working with the development set; however, this was not done in any
systematic fashion. Nevertheless, results on the blind held-out test set confirm the
generality of our citation scoring algorithm.
89
Computational Linguistics Volume 33, Number 1
Table 10
(Strict, Test) Strict results of reranking experiment on blind held-out test questions for the
baseline PubMed condition, term-based reranker, EBM-based reranker, and combination
reranker.
Therapy Diagnosis Prognosis Etiology
Precision at 10 (P10)
PubMed 0.108 0.017 0.000 0.080
Term 0.192 (+77%)? 0.133 (+700%)? 0.033 ? 0.140 (+75%)?
EBM 0.233 (+115%)? 0.167 (+900%)? 0.100 ? 0.200 (+150%)?
Combo 0.258 (+139%) 0.200 (+1100%)? 0.100 ? 0.220 (+175%)?
Mean Average Precision (MAP)
PubMed 0.061 0.024 0.015 0.050
Term 0.082 (+36%)? 0.118 (+386%)? 0.086 (+464%)? 0.086 (+74%)?
EBM 0.109 (+80%)? 0.091 (+276%)? 0.234 (+1442%)? 0.159 (+220%)?
Combo 0.120 (+99%)? 0.107 (+339%)? 0.224 (+1372%)? 0.165 (+232%)?
Mean Reciprocal Rank (MRR)
PubMed 0.282 0.073 0.031 0.207
Term 0.368 (+31%)? 0.429 (+488%)? 0.146 (+377%)? 0.314 (+52%)?
EBM 0.397 (+41%)? 0.431 (+490%)? 0.465 (+1422%)? 0.500 (+142%)?
Combo 0.556 (+97%) 0.422 (+479%)? 0.438 (+1331%)? 0.467 (+126%)?
Total Document Reciprocal Rank (TDRR)
PubMed 0.495 0.137 0.038 0.331
Term 0.700 (+41%)? 0.759 (+454%)? 0.171 (+355%)? 0.596 (+80%)?
EBM 0.807 (+63%)? 0.654 (+377%)? 0.513 (+1262%)? 0.946 (+186%)?
Combo 0.969 (+96%) 0.698 (+409%)? 0.479 (+1172%)? 0.975 (+195%)?
(a) Breakdown by clinical task
P10 MAP MRR TDRR
PubMed 0.069 0.045 0.190 0.328
Term 0.150 (+117%) 0.092 (+105%) 0.346 (+82%) 0.632 (+93%)
EBM 0.196 (+183%) 0.129 (+187%) 0.433 (+127%) 0.765 (+133%)
Combo 0.219 (+217%) 0.138 (+207%) 0.494 (+160%) 0.851 (+159%)
(b) Performance across all clinical tasks
 Significance at the 1% level, depending on direction of change.
 Significance at the 5% level, depending on direction of change.
?Difference not statistically significant.
In the development of various language technology applications, it is common for
the first materialization of a new capability to be rather ad hoc in its implementation.
This is a reflection of an initial attempt to understand both the problem and solution
spaces. Subsequent systems, with a better understanding of the possible technical ap-
proaches and their limitations, are then able to implement a more principled solution.
Because our clinical question-answering system is the first of its type that we are aware
of, in terms of both depth and scope, it is inevitable that our algorithms suffer from
some of these limitations. Similarly, our collection of clinical questions is the first test
collection of its type that we are aware of. Typically, construction of formal models is
only made possible by the existence of test collections. We hope that our work sheds new
insight on question answering in the clinical domain and paves the way for future work.
90
Demner-Fushman and Lin Answering Clinical Questions
Table 11
Performance differences between various rerankers.
P10 MAP MRR TDRR
Development Set
EBM vs. Term +45.0%  +30.8%  +6.7% ? +23.3% 
Combo vs. Term +46.7%  +30.4%  +9.1% ? +23.2% 
Combo vs. EBM +1.2% ? ?0.3% ? +2.3% ? ?0.1% ?
Test Set
EBM vs. Term +40.8  +40.1%  +38.3%  +37.3% 
Combo vs. Term +43.2  +40.0%  +42.1%  +37.8% 
Combo vs. EBM +1.7 ? ?0.1% ? +2.7% ? +0.3% ?
(a) Lenient Scoring
P10 MAP MRR TDRR
Development Set
EBM vs. Term +36.4%  +43.8%  +31.3% ? +40.7% 
Combo vs. Term +41.6%  +41.9%  +24.5% ? +35.0% 
Combo vs. EBM +3.8% ? ?1.3% ? ?5.2% ? ?4.1% ?
Test Set
EBM vs. Term +30.8 ? +40.4%  +24.9% ? +20.9% ?
Combo vs. Term +46.2  +50.0%  +42.8%  +34.6% 
Combo vs. EBM +11.8  +6.8% ? +14.3% ? +11.3% ?
(b) Strict Scoring
 Significance at the 1% level, depending on direction of change.
 Significance at the 5% level, depending on direction of change.
?Difference not statistically significant.
In addition, there are some theoretical obstacles for developing a more formal (say,
generative) model. Most methods for training such models require independently and
identically distributed samples from the underlying distribution?which is certainly not
the case with our test collection. Moreover, the event space of queries and documents
is extremely large or even infinite, depending on how it is defined. Our training data,
assumed to be samples from this underlying distribution, is extremely small compared
to the event space, and hence it is unlikely that popular methods (e.g., maximum
likelihood estimates) would yield an accurate characterization of the true distribution.
Furthermore, many techniques for automatically setting parameters make use
of maximum likelihood techniques?which do not maximize the correct objective
function. Maximizing the likelihood of generating the training data does not mean
that the evaluation metric under consideration (e.g., mean average precision) is also
maximized?this phenomenon is known as metric divergence.
Nevertheless, it is important to better understand the effects of parameter settings
in our system. This section describes a few experiments aimed at this goal.
The EBM score of a MEDLINE citation is the sum of three separate components,
each representing a facet of evidence-based medicine. This structure naturally suggests
a modification to Equation (3) that weights each score component differently:
SEBM = ?1SPICO + ?2SSoE + (1 ? ?1 ? ?2)Stask (8)
91
Computational Linguistics Volume 33, Number 1
Figure 3
The MAP performance surface for ?1 and ?2.
Table 12
Results of optimizing ?1 and ?2 on therapy questions.
P10 MAP MRR TDRR
Development Test
Baseline 0.730 0.819 0.933 2.869
Optimized 0.760 (+4.1%)? 0.822 (+0.4%)? 0.933 (+0.0%)? 2.878 (+0.3%)?
Test Test
Baseline 0.783 0.765 0.917 2.979
Optimized 0.783 (+0.0%)? 0.762 (?0.4%)? 0.917 (+0.0%)? 2.972 (?0.2%)?
?Difference not statistically significant.
The parameters ?1 and ?2 can be derived from our development set. For therapy
questions, we exhaustively searched through the entire parameter space, in increments
of hundredths, and determined the optimal settings to be ?1 = 0.38, ?2 = 0.34 (which
was found to slightly improve all metrics). The performance surface for mean average
precision is shown in Figure 3, which plots results for all possible parameter values
on the development set. Numeric results are shown in Table 12. It can be seen that
optimizing the parameters in this fashion does not lead to a statistically significant
increase in any of the metrics. Furthermore, these gains do not carry over to the blind
held-out test set. We also tried optimizing the ??s on all questions in the development
set. These results are shown in Table 13. Once again, differences are not statistically
significant.
Why does parameter optimization not help? We believe that there are two factors
at play here: On the one hand, parameter settings should be specific to the clinical
task. This explains why optimizing across all question types at the same time did
not improve performance. On the other hand, there are too few questions of any
particular type to represent an accurate sampling of all possible questions. This is why
parameter tuning on therapy questions did not significantly alter performance. These
experiments point to the need for larger test collections, which is an area for future
work.
92
Demner-Fushman and Lin Answering Clinical Questions
Table 13
Results of optimizing ?1 and ?2 on all questions.
P10 MAP MRR TDRR
Development Test
Baseline 0.699 0.754 0.910 2.650
Optimized 0.707 (+1.2%)? 0.755 (+0.1%)? 0.918 (+0.9%)? 2.660 (+0.4%)?
Test Test
Baseline 0.677 0.718 0.936 2.671
Optimized 0.669 (?1.1%)? 0.716 (?0.3%)? 0.936 (+0.0%)? 2.662 (?0.3%)?
?Difference not statistically significant.
Table 14
Results of assigning uniform weights to the EBM score component based on the clinical task.
P10 MAP MRR TDRR
Development Test
Baseline 0.699 0.754 0.910 2.650
?(t) = ?1 0.690 (?1.2%)? 0.738 (?2.1%)? 0.927 (+1.9%)? 2.646 (?0.2%)?
Test Test
Baseline 0.677 0.718 0.936 2.671
?(t) = ?1 0.627 (?7.4%) 0.681 (?5.2%)? 0.913 (?2.4%)? 2.519 (?5.7%)?
 Significance at the 5% level, depending on direction of change.
?Difference not statistically significant.
Another component of our EBM citation scoring algorithm that contains many
ad hoc weights is Stask, defined in Equation (7) and repeated here:
Stask =
?
t?MeSH
?(t) (9)
The function ?(t) maps a particular MeSH term to a weight that quantifies the
degree to which it is a positive or negative indicator for the particular clinical task.
Because these weights were heuristically assigned, it would be worthwhile to examine
the impact they have on performance. As a variant, we modified ?(t) so that all MeSH
terms were mapped to ?1; in other words, we did not encode granular levels of
?goodness.? These results are shown in Table 8. Although performance dropped across
all metrics, none of the differences were statistically significant except for P10 on the
test set.
The series of experiments described herein help us better understand the effects of
parameter settings on abstract reranking performance. As can be seen from the results,
our algorithm is relatively invariant with respect to the choice of parameters, con-
firming that our primary contribution is the EBM-based approach to clinical question
93
Computational Linguistics Volume 33, Number 1
answering, and that our performance gains cannot be simply attributed to a fortunate
choice of parameters.
9. From Scoring Citations to Answering Questions
The aim of question-answering technology is to move from the ?hit list? paradigm of
information retrieval, where users receive a list of potentially relevant documents that
they must then browse through, to a mode of interaction where users directly receive
responses that satisfy their information needs. In our current architecture, fetching a
higher-quality ranked list is a step towards generating responsive answers.
The most important characteristic of answers, as recommended by Ely et al (2005)
in their study of real-world physicians, is that they focus on bottom-line clinical
advice?information that physicians can directly act on. Ideally, answers should in-
tegrate information from multiple clinical studies, pointing out both similarities and
differences. The system should collate concurrences, that is, if multiple abstracts ar-
rive at the same conclusion?it need not be repeated unless the physician wishes to
?drill down?; the system should reconcile contradictions, for example, if two abstracts
disagree on a particular treatment because they studied different patient populations.
We have noted that many of these desiderata make complex question answering quite
similar to multi-document summarization (Lin and Demner-Fushman 2005b), but these
features are also beyond the capabilities of current summarization systems.
It is clear that the type of answers desired by physicians require a level of semantic
analysis that is beyond the current state of the art, even with the aid of existing medical
ontologies. For example, even the seemingly straightforward task of identifying simi-
larities and differences in outcome statements is rendered exceedingly complex by the
tremendous amount of background medical knowledge that must be brought to bear
in interpreting clinical results and subtle differences in study design, objectives, and
results; the closest analogous task in computational linguistics?redundancy detection
for multi-document summarization?seems easy by comparison. Furthermore, it is
unclear if textual strings make ?good answers.? Perhaps a graphical rendering of the
semantic predicates present in relevant abstracts might more effectively convey the
desired information; see, for example, Fiszman, Rindflesch, and Kilicoglu (2004). Per-
haps some variation of multi-level bulleted lists, appropriately integrated with interface
elements for expanding and hiding items, might provide physicians a better overview
of the information landscape; see, for example, Demner-Fushman and Lin (2006).
Recognizing this complex set of issues, we decided to take a simple extractive
approach to answer generation. For each abstract in our reranked list of citations,
our system produces an answer by combining the title of the abstract and the top
three outcome sentences (in the order they appeared in the abstract). We employed the
outcome scores generated by the regression model. No attempt was made to synthesize
information from multiple citations. A formal evaluation of this simple approach to
answer generation is presented in the next section.
10. Evaluation of Clinical Answers
Evaluation of answers within a clinical setting involves a complex decision that must
not only take into account topical relevance (i.e., ?Does the answer address the infor-
mation need??), but also situational relevance (e.g., Saracevic 1975, Barry and Schamber
94
Demner-Fushman and Lin Answering Clinical Questions
1998). The latter factor includes many issues such as the strength of evidence, recency
of results, and reputation of the journal. Clinicians need to carefully consider all these
elements before acting on any information for the purposes of patient care. Within the
framework of evidence-based medicine, the physician is the final arbiter of how clinical
answers are integrated into the broader activities of medical care, but this complicates
any attempt to evaluate answers generated by our system.
In assessing answers produced by our system, we decided to focus only on the
evaluation of topical relevance?assessors were only presented with answer strings,
generated in the manner described in the previous section. Metadata that would con-
tribute to judgments about situational relevance, such as the strength of evidence,
names of the authors and the journal, and so on, were purposefully suppressed. Our
evaluation compared the top five answers generated from the original PubMed hit list
and the top five answers generated from our reranked list of citations. Answers were
prepared for all 24 questions in our development set.
We recruited two medical doctors (one family practitioner, one surgeon) from the
National Library of Medicine to evaluate the textual answers. Our instructions clearly
stated that only topical relevance was to be assessed. We asked the physicians to provide
three-valued judgments:
 A plus (+) indicates that the response directly answers the question.
Naturally, the physicians would need to follow up and examine the source
citation in more detail.
 A check (
?
) indicates that the response provides clinically relevant
information that may factor into decisions about patient treatment, and
that the source citation was worth examining in more detail.
 A minus (?) indicates that the response does not provide useful
information in answering the clinical question, and that the source citation
was not worth examining.
We purposely avoided short linguistic labels for the judgments so as to sidestep
the question of ?What exactly is an answer to a clinical question?? Informally, an-
swers marked with a plus can be considered ?actionable? clinical advice. Answers
marked with a check provide relevant information that may influence the physician?s
actions.
We adopted a double-blind study design for the actual assessment process: Answers
from both systems were presented in a randomized order without any indication of
which system the response came from (duplicates were suppressed). A paper printout,
containing each question followed by the blinded answers, was presented to each
assessor. We then coded the relevance judgments in a plain text file manually. During
this entire time, the key that maps answers to systems was kept in a separate file and
hidden from everyone, including the authors. All scores were computed automatically
without human intervention.
Answer precision was calculated for two separate conditions: Under the strict
condition (Table 15), only ?plus? judgments were considered good; under the lenient
condition (Table 16), both ?plus? and ?check? judgments were considered good. As can
be seen, our EBM algorithm significantly outperforms the baseline under both the strict
and lenient conditions, according to both assessors. On average, the length of answers
generated from the original PubMed list of citations was 90 words; answers generated
from the reranked list of citations averaged 87 words. Answers from both sources
95
Computational Linguistics Volume 33, Number 1
Table 15
Strict answer precision (considering only ?plus? judgments).
Therapy Diagnosis Prognosis Etiology All
Assessor 1
Baseline .160 .233 .333 .480 .267
EBM .260 (+63%) .367 (+58%) .333 (+0%) .600 (+25%) .367 (+37%)
Assessor 2
Baseline .040 .233 .200 .400 .183
EBM .200 (+400%) .300 (+29%) .266 (+33%) .560 (+40%) .308 (+68%)
Table 16
Lenient answer precision (considering both ?plus? and ?check? judgments).
Therapy Diagnosis Prognosis Etiology All
Assessor 1
Baseline .400 .300 .533 .520 .417
EBM .640 (+60%) .567 (+89%) .400 (?25%) .640 (+23%) .592 (+42%)
Assessor 2
Baseline .240 .267 .333 .440 .300
EBM .520 (+117%) .600 (+125%) .400 (+20%) .560 (+27%) .533 (+78%)
were significantly shorter than the abstracts from which they were extracted (250 word
average for original PubMed results and 270 word average for reranked results).
To give a feel for the types of responses that are generated by our system, consider
the following question:
What is the best treatment for analgesic rebound headaches?
The following is an example of a response that received a ?plus? judgment:
Medication overuse headache from antimigraine therapy: clinical features,
pathogenesis and management: Because of easy availability and low expense, the
greatest problem appears to be associated with barbiturate-containing combination
analgesics and over-the-counter caffeine-containing combination analgesics. The best
management advice is to raise awareness and strive for prevention. Reduction in
headache risk factors should include behavioural modification approaches to headache
control earlier in the natural history of migraine.
This answer was accepted by both physicians because it clearly states that specific
analgesics are most likely to cause the problem, and gives a direct guideline for preven-
tive treatment.
In contrast, the following response to the same question received a ?check?:
Does chronic daily headache arise de novo in association with regular use of
analgesics? Regular use of analgesics preceded the onset of daily headache in 5 patients
by a mean of 5.4 years (range, 2 to 10 years). In 1 patient, the onset of daily headache
preceded regular use of analgesics by almost 30 years. These findings suggest that
individuals with primary headache, specifically migraine, are predisposed to
developing chronic daily headache in association with regular use of analgesics.
96
Demner-Fushman and Lin Answering Clinical Questions
Although this answer provides information about the risks and causes of the
headaches, neither prevention nor treatment is explicitly mentioned. For these reasons
this response was marked as potentially leading to an answer, but not as containing one.
To summarize, we have presented a simple answer generation algorithm that
is capable of supplying clinically relevant responses to physicians. Compared to
PubMed, which does not take into account the principles of evidence-based medicine,
our question-answering system represents a leap forward in information access
capabilities.8
11. Related Work and Discussion
Clinical question answering is an emerging area of research that has only recently begun
to receive serious attention. As a result, there exist relatively few points of comparison to
our own work, as the research space is sparsely populated. In this section, however, we
will attempt to draw connections to other clinical information systems (although not
necessarily for question answering) and related domain-specific question-answering
systems. For an overview of systems designed to answer open-domain factoid ques-
tions, the TREC QA track overview papers are a good place to start (Voorhees and
Tice 1999). In addition, there has been much work on the application of linguistic and
semantic knowledge to information retrieval; see Lin and Demner-Fushman (2006a) for
a brief overview.
The idea that clinical information systems should be sensitive to the practice of
evidence-based medicine is not new. Based on analyses of 4,000 MEDLINE citations,
Mendonc?a and Cimino (2001) have studied MeSH terms associated with the four basic
clinical tasks of therapy, diagnosis, prognosis, and etiology. The goal was to auto-
matically classify citations for task-specific retrieval, similar in spirit to the Hedges
Project (Haynes et al 1994; Wilczynski, McKibbon, and Haynes 2001). Cimino and
Mendonc?a reported good performance for etiology, diagnosis, and in particular therapy,
but not prognosis. Although originally developed as a tool to assist in query formu-
lation, Booth (2000) pointed out that PICO frames can be employed to structure IR
results for improving precision. PICO-based querying in information retrieval is merely
an instance of faceted querying, which has been widely used by librarians since the
introduction of automated retrieval systems (e.g., Meadow et al 1989). The work of
Hearst (1996) demonstrates that faceted queries can be converted into simple filtering
constraints to boost precision.
The feasibility of automatically identifying outcome statements in secondary
sources has been demonstrated by Niu and Hirst (2004). Their study also illustrates
the importance of semantic classes and relations. However, extraction of outcome state-
ments from secondary sources (meta-analyses, in this case) differs from extraction of
outcomes from MEDLINE citations because secondary sources represent knowledge
that has already been distilled by humans (which may limit its scope). Because sec-
ondary sources are often more consistently organized, it is possible to depend on
certain surface cues for reliable extraction (which is not possible for MEDLINE ab-
stracts in general). Our study tackles outcome identification in primary medical sources
and demonstrates that respectable performance is possible with a feature-combination
approach.
8 Although note that answer generation from the PubMed results also requires the use of the outcome
extractor.
97
Computational Linguistics Volume 33, Number 1
The literature also contains work on sentence-level classification of MEDLINE
abstracts for non-clinical purposes. For example, McKnight and Srinivasan (2003) de-
scribe a machine learning approach to automatically label sentences as belonging to
introduction, methods, results, or conclusion using structured abstracts as training data
(see also Lin et al 2006). Tbahriti et al (2006) have demonstrated that differential
weighting of automatically labeled sections can lead to improved retrieval performance.
Note, however, that such labels are orthogonal to PICO frame elements, and hence
are not directly relevant to knowledge extraction for clinical question answering. In a
similar vein, Light, Qiu, and Srinivasan (2004) report on the identification of speculative
statements in MEDLINE abstracts, but once again, this work is not directly applicable
to clinical question answering.
In addition to question answering, multi-document summarization provides a com-
plementary approach to addressing clinical information needs. The PERSIVAL project,
the most comprehensive study of such techniques applied on medical texts to date,
leverages patient records to generate personalized summaries in response to physicians?
queries (McKeown, Elhadad, and Hatzivassiloglou 2003; Elhadad et al 2005). Although
the system incorporates both a user and a task model, it does not explicitly capture
the principles of evidence-based medicine. Patient information is no doubt important
to answering clinical questions, and our work could certainly benefit from experiences
gained in the PERSIVAL project.
The application of domain models and deep semantic knowledge to question
answering has been explored by a variety of researchers (e.g., Jacquemart and
Zweigenbaum 2003, Rinaldi et al 2004), and was also the focus of recent workshops
on question answering in restricted domains at ACL 2004 and AAAI 2005. Our work
contributes to this ongoing discourse by demonstrating a specific application in the
domain of clinical medicine.
Finally, the evaluation of answers to complex questions remains an open research
problem. Although it is clear that measures designed for open-domain factoid questions
are not appropriate, the community has not agreed on a methodology that will allow
meaningful comparisons of results from different systems. In Sections 9 and 10, we
have discussed many of these issues. Recently, there is a growing consensus that an
evaluation methodology based on the notion of ?information nuggets? may provide
an appropriate framework for assessing the quality of answers to complex questions.
Nugget F-score has been employed as a metric in the TREC question-answering track
since 2003, to evaluate so-called definition and ?other? questions (Voorhees 2003). A
number of studies (e.g., Hildebrandt, Katz, and Lin 2004) have pointed out shortcom-
ings of the original nugget scoring model, although a number of these issues have been
recently addressed (Lin and Demner-Fushman 2005a, 2006b). However, adaptation of
the nugget evaluation methodology to a domain as specific as clinical medicine is an
endeavor that has yet to be undertaken.
12. Future Work
The design and implementation of our current system leaves many open avenues for
future exploration, one of which concerns our assumptions about the query interface.
Previously, a user study (Lin et al 2003) has shown that people are reluctant to type
full natural language questions, even after being told that they were using a question-
answering system and that typing complete questions would result in better perform-
ance. We have argued that a query interface based on structured PICO frames will
yield better-formulated queries, although it is unclear whether physicians would invest
98
Demner-Fushman and Lin Answering Clinical Questions
the upfront effort necessary to accomplish this. Issuing extremely short queries appears
to be an ingrained habit of information seekers today, and the dominance of World Wide
Web searches reinforce this behavior. Given these trends, physicians may actually prefer
the rapid back-and-forth interaction style that comes with short queries. We believe
that if systems can produce noticeably better results with richer queries, users will
make more of an effort to formulate them. This, however, presents a chicken-and-egg
problem: One possible solution is to develop models that can automatically fill query
frames given a couple of keywords?this would serve to kick-start the query generation
process.
The astute reader will have noticed that the initial retrieval of abstracts in our
study was performed with high-quality manually crafted queries (that were part of
the test collection). Although this was intended to demonstrate the performance of our
EBM citation scoring algorithm with respect to a strong baseline, it also means that we
have omitted a component in the automatic question-answering process. Translating a
clinical question into a good PubMed query is not a trivial task?in our experiments,
it required an experienced searcher approximately 40 minutes on average per question.
However, it is important to note that query formulation in the clinical domain is not
a problem limited to question-answering systems, but one that users of all retrieval
systems must contend with.
Nevertheless, there are three potential solutions to this problem: First, although
there is an infinite variety of clinical questions, the number of query types is bounded
and far smaller in number; see Huang, Lin, and Demner-Fushman (2006) for an analysis.
In a query interface based on PICO frames, it is possible to identify a number of proto-
typical query frames. From these prototypes, one can generate query templates that ab-
stract over the actual slot fillers?this is the idea behind Clinical Queries. Although this
method will probably not retrieve citations as high in quality as custom-crafted queries,
there is reason to believe that as long as a reasonable set of citations is retrieved, our sys-
tem will be able to extract relevant answers (given the high accuracy of our knowledge
extractors and citation scoring algorithm). The second approach to tackling this problem
is to bypass PubMed altogether and index MEDLINE with another search engine.
Due to the rapidly changing nature of the entire MEDLINE database, experiments for
practical purposes would most likely be conducted on a static subset of the collection,
for example, the ten-year portion created for the TREC 2004 genomics track (Hersh,
Bhupatiraju, and Corley 2004). Recent results from TREC have demonstrated that high
performance ad hoc retrieval is possible in the genomics domain (Hersh et al 2005),
and it is not a stretch to imagine adopting these technologies for clinical tasks. Using
a separate search engine would provide other benefits as well: Greater control over
the document retrieval process would allow one to examine the effects of different
indexing schemes, different query operators, and techniques such as query expansion;
see, for example, Aronson, Rindflesch, and Browne (1994). Finally, yet another way to
solve the document retrieval problem is to eliminate that stage completely. Recall that
our two-stage architecture was a practical expediency, because we did not have access
to the computing resources necessary to pre-extract PICO elements from the entire
MEDLINE database and directly index the results. Given access to more resources,
a system could index identified PICO elements and directly match queries against a
knowledge store.
Finally, answer generation remains an area that awaits further exploration, although
we would have to first define what a good answer should be. We have empirically
verified that an extractive approach based on outcome sentences is actually quite
satisfactory, but our algorithm does not currently integrate evidence from multiple
99
Computational Linguistics Volume 33, Number 1
abstracts; although see Demner-Fushman and Lin (2006). Furthermore, the current an-
swer generator does not handle complex issues such as contradictory and inconsistent
statements. To address these very difficult challenges, finer-grained semantic analysis
of medical texts is required.
13. Conclusion
Our experiments in clinical question answering provide some answers to the broader
research question regarding the role of knowledge-based and statistical techniques in
advanced question answering. This work demonstrates that the two approaches are
complementary and can be seamlessly integrated into algorithms that draw from the
best of both worlds. Explicitly coded semantic knowledge, in the form of UMLS, and
software for leveraging this resource?for example, MetaMap?combine to simplify
many knowledge extraction tasks that would be far more difficult otherwise. The re-
spectable performance of our population, problem, and intervention extractors, all of
which use relatively simple rules, provides evidence that complex clinical problems
can be tackled by appropriate use of ontological knowledge. Explicitly coded semantic
knowledge is less helpful for outcome identification due to the large variety of possi-
ble ?outcomes;? nevertheless, knowledge-rich features can be combined with simple,
statistically derived features to build a good outcome classifier. Overall, this work
demonstrates that the application of a semantic domain model yields clinical question
answering capabilities that significantly outperform presently available technology,
especially when coupled with traditional statistical methods (classification, evidence
combination, etc.).
We have taken an important step in building a complete question-answering system
that assists physicians in the patient care process. Our work demonstrates that the
principles of evidence-based medicine can be computationally captured and imple-
mented in a system, and although we are still far from operational deployment, these
positive results are certainly encouraging. Information systems in support of the clinical
decision-making process have the potential to improve the quality of health care, which
is a worthy goal indeed.
Acknowledgments
We would like to thank Dr. Charles
Sneiderman and Dr. Kin Wah Fung for
the evaluation of the answers. For this
work, D. D-F. was supported by an
appointment to the National Library of
Medicine Research Participation Program
administered by the Oak Ridge Institute
for Science and Education through an
inter-agency agreement between the U.S.
Department of Energy and the National
Library of Medicine. For this work, J. L.
was supported in part by a grant from the
National Library of Medicine, where he
was a visiting researcher during the
summer of 2005. We would like to thank
the anonymous reviewers for their valuable
comments. J. L. would like to thank Kiri
and Esther for their kind support.
References
Ad Hoc Working Group for Critical
Appraisal of the Medical Literature. 1987.
A proposal for more informative abstracts
of clinical articles. Annals of Internal
Medicine, 106:595?604.
Aronson, Alan R. 2001. Effective mapping
of biomedical text to the UMLS
Metathesaurus: The MetaMap program.
In Proceeding of the 2001 Annual Symposium
of the American Medical Informatics
Association (AMIA 2001), pages 17?21,
Portland, OR.
Aronson, Alan R., James G. Mork,
Clifford W. Gay, Susanne M. Humphrey,
and Willie J. Rogers. 2004. The NLM
Indexing Initiative?s Medical Text
Indexer. In Proceedings of the 11th
World Congress on Medical Informatics
100
Demner-Fushman and Lin Answering Clinical Questions
(MEDINFO 2004), pages 268?272,
San Francisco, CA.
Aronson, Alan R., Thomas C. Rindflesch,
and Allen C. Browne. 1994. Exploiting a
large thesaurus for information retrieval.
In Proceedings of RIAO 1994: Intelligent
Multimedia Information Retrieval Systems
and Management, pages 197?216, New York.
Baeza-Yates, Ricardo and Berthier
Ribeiro-Neto. 1999. Modern Information
Retrieval. ACM Press, New York.
Barry, Carol and Linda Schamber. 1998.
Users? criteria for relevance evaluation:
A cross-situational comparison.
Information Processing and Management,
34(2/3):219?236.
Booth, Andrew. 2000. Formulating the
question. In Andrew Booth and Graham
Walton, editors, Managing Knowledge in
Health Services. Library Association
Publishing, London, England.
Chambliss, M. Lee and Jennifer Conley. 1996.
Answering clinical questions. The Journal of
Family Practice, 43:140?144.
Cogdill, Keith W. and Margaret E. Moore.
1997. First-year medical students?
information needs and resource selection:
Responses to a clinical scenario. Bulletin of
the Medical Library Association, 85(1):51?54.
Covell, David G., Gwen C. Uman, and
Phil R. Manning. 1985. Information needs
in office practice: Are they being met?
Annals of Internal Medicine, 103(4):596?599.
De Groote, Sandra L. and Josephine L.
Dorsch. 2003. Measuring use patterns
of online journals and databases.
Journal of the Medical Library Association,
91(2):231?240.
Demner-Fushman, Dina, Barbara Few,
Susan E. Hauser, and George Thoma. 2006.
Automatically identifying health outcome
information in MEDLINE records. Journal
of the American Medical Informatics
Association, 13(1):52?60.
Demner-Fushman, Dina and Jimmy Lin.
2005. Knowledge extraction for clinical
question answering: Preliminary results.
In Proceedings of the AAAI-05 Workshop on
Question Answering in Restricted Domains,
pages 1?10, Pittsburgh, PA.
Demner-Fushman, Dina and Jimmy Lin.
2006. Answer extraction, semantic
clustering, and extractive summarization
for clinical question answering. In
Proceedings of the 21st International
Conference on Computational Linguistics and
44th Annual Meeting of the Association for
Computational Linguistics (COLING/ACL
2006), pages 841?848, Sydney, Australia.
Ebell, Mark H., Jay Siwek, Barry D. Weiss,
Steven H. Woolf, Jeffrey Susman, Bernard
Ewigman, and Marjorie Bowman. 2004.
Strength of Recommendation Taxonomy
(SORT): A patient-centered approach to
grading evidence in the medical literature.
The Journal of the American Board of Family
Practice, 17(1):59?67.
Elhadad, Noemie, Min-Yen Kan, Judith
Klavans, and Kathleen McKeown. 2005.
Customization in a unified framework for
summarizing medical literature. Journal of
Artificial Intelligence in Medicine,
33(2):179?198.
Ely, John W., Jerome A. Osheroff, Mark H.
Ebell, George R. Bergus, Barcey T. Levy,
M. Lee Chambliss, and Eric R. Evans. 1999.
Analysis of questions asked by family
doctors regarding patient care. BMJ,
319:358?361.
Ely, John W., Jerome A. Osheroff, M. Lee
Chambliss, Mark H. Ebell, and Marcy E.
Rosenbaum. 2005. Answering physicians?
clinical questions: Obstacles and potential
solutions. Journal of the American Medical
Informatics Association, 12(2):217?224.
Fiszman, Marcelo, Thomas C. Rindflesch,
and Halil Kilicoglu. 2004. Abstraction
summarization for managing the
biomedical research literature. In
Proceedings of the HLT/NAACL 2004
Workshop on Computational Lexical
Semantics, pages 76?83, Boston, MA.
Gorman, Paul N., Joan S. Ash, and
Leslie W. Wykoff. 1994. Can primary care
physicians? questions be answered using
the medical journal literature? Bulletin of
the Medical Library Association,
82(2):140?146.
Haynes, R. Brian, Nancy Wilczynski, K. Ann
McKibbon, Cynthia J. Walker, and John C.
Sinclair. 1994. Developing optimal search
strategies for detecting clinically sound
studies in MEDLINE. Journal of the
American Medical Informatics Association,
1(6):447?458.
Hearst, Marti A. 1996. Improving full-text
precision on short queries using simple
constraints. In Proceedings of the Fifth
Annual Symposium on Document Analysis
and Information Retrieval (SDAIR 1996),
pages 217?232, Las Vegas, NV.
Hersh, William, Ravi Teja Bhupatiraju,
and Sarah Corley. 2004. Enhancing
access to the bibliome: The TREC
genomics track. In Proceedings of the
11th World Congress on Medical Informatics
(MEDINFO 2004), pages 773?777,
San Francisco, CA.
101
Computational Linguistics Volume 33, Number 1
Hersh, William, Aaron Cohen, Jianji Yang,
Ravi Teja Bhupatiraju1, Phoebe Roberts,
and Marti Hearst. 2005. TREC 2005
genomics track overview. In Proceedings
of the Fourteenth Text REtrieval Conference
(TREC 2005), Gaithersburg, MD.
Hildebrandt, Wesley, Boris Katz, and Jimmy
Lin. 2004. Answering definition questions
with multiple knowledge sources. In
Proceedings of the 2004 Human Language
Technology Conference and the North
American Chapter of the Association
for Computational Linguistics Annual
Meeting (HLT/NAACL 2004), pages 49?56,
Boston, MA.
Hirschman, Lynette and Robert Gaizauskas.
2001. Natural language question
answering: The view from here. Natural
Language Engineering, 7(4):275?300.
Huang, Xiaoli, Jimmy Lin, and Dina
Demner-Fushman. 2006. Evaluation of
PICO as a knowledge representation for
clinical questions. In Proceeding of the 2006
Annual Symposium of the American Medical
Informatics Association (AMIA 2006),
pages 359?363, Washington, D.C.
Ingwersen, Peter. 1999. Cognitive
information retrieval. Annual Review of
Information Science and Technology, 34:3?52.
Jacquemart, Pierre and Pierre Zweigenbaum.
2003. Towards a medical
question-answering system: A feasibility
study. In Robert Baud, Marius Fieschi,
Pierre Le Beux, and Patrick Ruch, editors,
The New Navigators: From Professionals to
Patients, volume 95 of Actes Medical
Informatics Europe, Studies in Health
Technology and Informatics. IOS Press,
Amsterdam, pages 463?468.
Kauffman, Ralph E., L. A. Sawyer, and
M. L. Scheinbaum. 1992. Antipyretic
efficacy of ibuprofen vs acetaminophen.
American Journal of Diseases of Children,
146(5):622?625.
Light, Marc, Xin Ying Qiu, and Padmini
Srinivasan. 2004. The language of
bioscience: Facts, speculations, and
statements in between. In Proceedings of the
BioLink 2004 Workshop at HLT/NAACL
2004, pages 17?24, Boston, MA.
Lin, Jimmy and Dina Demner-Fushman.
2005a. Automatically evaluating answers
to definition questions. In Proceedings of the
2005 Human Language Technology Conference
and Conference on Empirical Methods in
Natural Language Processing (HLT/EMNLP
2005), pages 931?938, Vancouver, Canada.
Lin, Jimmy and Dina Demner-Fushman.
2005b. Evaluating summaries and
answers: Two sides of the same coin? In
Proceedings of the ACL 2005 Workshop on
Intrinsic and Extrinsic Evaluation Measures
for MT and/or Summarization, pages 41?48,
Ann Arbor, MI.
Lin, Jimmy and Dina Demner-Fushman.
2006a. The role of knowledge in
conceptual retrieval: A study in the
domain of clinical medicine. In Proceedings
of the 29th Annual International ACM SIGIR
Conference on Research and Development in
Information Retrieval (SIGIR 2006),
pages 99?106, Seattle, WA.
Lin, Jimmy and Dina Demner-Fushman.
2006b. Will pyramids built of nuggets
topple over? In Proceedings of the 2006
Human Language Technology Conference
and North American Chapter of the
Association for Computational Linguistics
Annual Meeting (HLT/NAACL 2006),
pages 383?390, New York.
Lin, Jimmy, Damianos Karakos, Dina
Demner-Fushman, and Sanjeev
Khudanpur. 2006. Generative content
models for structural analysis of medical
abstracts. In Proceedings of the HLT/
NAACL 2006 Workshop on Biomedical
Natural Language Processing (BioNLP?06),
pages 65?72, New York.
Lin, Jimmy, Dennis Quan, Vineet Sinha,
Karun Bakshi, David Huynh, Boris Katz,
and David R. Karger. 2003. What makes a
good answer? The role of context in
question answering. In Proceedings of the
Ninth IFIP TC13 International Conference on
Human-Computer Interaction (INTERACT
2003), pages 25?32, Zu?rich, Switzerland.
Lindberg, Donald A., Betsy L. Humphreys,
and Alexa T. McCray. 1993. The Unified
Medical Language System. Methods of
Information in Medicine, 32(4):281?291.
McCray, Alexa T., Anita Burgun, and Olivier
Bodenreider. 2001. Aggregating UMLS
semantic types for reducing conceptual
complexity. In Proceedings of 10th World
Congress on Medical Informatics (MEDINFO
2001), pages 216?220, London, England.
McKeown, Kathleen, Noemie Elhadad,
and Vasileios Hatzivassiloglou. 2003.
Leveraging a common representation for
personalized search and summarization
in a medical digital library. In Proceedings
of the 3rd ACM/IEEE Joint Conference on
Digital Libraries (JCDL 2003), pages
159?170, Houston, TX.
McKnight, Larry and Padmini Srinivasan.
2003. Categorization of sentence types
in medical abstracts. In Proceeding of the
2003 Annual Symposium of the American
102
Demner-Fushman and Lin Answering Clinical Questions
Medical Informatics Association (AMIA
2003), pages 440?444, Washington, D.C.
Meadow, Charles T., Barbara A. Cerny,
Christine L. Borgman, and Donald O.
Case. 1989. Online access to knowledge:
System design. Journal of the American
Society for Information Science, 40(2):86?98.
Mendonc?a, Eneida A. and James J. Cimino.
2001. Building a knowledge base to
support a digital library. In Proceedings
of 10th World Congress on Medical
Informatics (MEDINFO 2001),
pages 222?225, London, England.
Mladenic, Dunja and Marko Grobelnik. 1999.
Feature selection for unbalanced class
distribution and Na??ve Bayes. In
Proceedings of the Sixteenth International
Conference on Machine Learning (ICML
1999), pages 258?267, Bled, Slovenia.
Nenkova, Ani and Rebecca Passonneau.
2004. Evaluating content selection in
summarization: The pyramid method. In
Proceedings of the 2004 Human Language
Technology Conference and the North
American Chapter of the Association for
Computational Linguistics Annual Meeting
(HLT/NAACL 2004), pages 145?152,
Boston, MA.
Niu, Yun and Graeme Hirst. 2004. Analysis
of semantic classes in medical text for
question answering. In Proceedings of the
ACL 2004 Workshop on Question Answering
in Restricted Domains, pages 54?61,
Barcelona, Spain.
Pratt, Wanda and Meliha Yetisgen-Yildiz.
2003. A study of biomedical concept
identification: MetaMap vs. people.
In Proceeding of the 2003 Annual
Symposium of the American Medical
Informatics Association (AMIA 2003),
pages 529?533, Washington, D.C.
Richardson, W. Scott, Mark C. Wilson, James
Nishikawa, and Robert S. Hayward. 1995.
The well-built clinical question: A key to
evidence-based decisions. American College
of Physicians Journal Club, 123(3):A12?A13.
Rinaldi, Fabio, James Dowdall, Gerold
Schneider, and Andreas Persidis. 2004.
Answering questions in the genomics
domain. In Proceedings of the ACL 2004
Workshop on Question Answering in
Restricted Domains, pages 46?53,
Barcelona, Spain.
Rindflesch, Thomas C. and Marcelo Fiszman.
2003. The interaction of domain
knowledge and linguistic structure in
natural language processing: Interpreting
hypernymic propositions in biomedical
text. Journal of Biomedical Informatics,
36(6):462?477.
Sackett, David L., Sharon E. Straus, W. Scott
Richardson, William Rosenberg, and
R. Brian Haynes. 2000. Evidence-Based
Medicine: How to Practice and Teach EBM,
second edition. Churchill Livingstone,
Edinburgh, Scotland.
Saracevic, Tefko. 1975. Relevance: A review
of and a framework for thinking on the
notion in information science. Journal of the
American Society for Information Science,
26(6):321?343.
Sneiderman, Charles, Dina
Demner-Fushman, Marcelo Fiszman, and
Thomas C. Rindflesch. 2005. Semantic
characteristics of MEDLINE citations
useful for therapeutic decision-making.
In Proceeding of the 2005 Annual Symposium
of the American Medical Informatics
Association (AMIA 2005), page 1117,
Washington, D.C.
Tbahriti, Imad, Christine Chichester,
Fre?de?rique Lisacek, and Patrick Ruch.
2006. Using argumentation to retrieve
articles with similar citations: An inquiry
into improving related articles search in
the MEDLINE digital library. International
Journal of Medical Informatics, 75(6):488?495.
Ting, Kai Ming and Ian H. Witten. 1999.
Issues in stacked generalization. Journal of
Artificial Intelligence Research, 10:271?289.
Voorhees, Ellen M. 2003. Overview of the
TREC 2003 question answering track. In
Proceedings of the Twelfth Text REtrieval
Conference (TREC 2003), pages 54?68,
Gaithersburg, MD.
Voorhees, Ellen M. and Dawn M. Tice.
1999. The TREC-8 question answering
track evaluation. In Proceedings of the
Eighth Text REtrieval Conference (TREC-8),
pages 83?106, Gaithersburg, MD.
Wilczynski, Nancy, K. Ann McKibbon, and
R. Brian Haynes. 2001. Enhancing retrieval
of best evidence for health care from
bibliographic databases: Calibration
of the hand search of the literature. In
Proceedings of 10th World Congress on
Medical Informatics (MEDINFO 2001),
pages 390?393, London, England.
Yang, Yiming and Jan O. Pedersen. 1997.
A comparative study on feature selection
in text categorization. In Proceedings
of the Fourteenth International Conference
on Machine Learning (ICML 1997),
pages 412?420, Nashville, TN.
103

Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 305?308,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Putting the User in the Loop: Interactive Maximal Marginal Relevance for
Query-Focused Summarization
Jimmy Lin, Nitin Madnani, and Bonnie J. Dorr
University of Maryland
College Park, MD 20742, USA
jimmylin@umd.edu, {nmadnani,bonnie}@umiacs.umd.edu
Abstract
This work represents an initial attempt to
move beyond ?single-shot? summarization to
interactive summarization. We present an ex-
tension to the classic Maximal Marginal Rel-
evance (MMR) algorithm that places a user
?in the loop? to assist in candidate selec-
tion. Experiments in the complex interac-
tive Question Answering (ciQA) task at TREC
2007 show that interactively-constructed re-
sponses are significantly higher in quality than
automatically-generated ones. This novel al-
gorithm provides a starting point for future
work on interactive summarization.
1 Introduction
Document summarization, as captured in modern
comparative evaluations such as TAC and DUC, is
mostly conceived as a ?one-shot? task. However, re-
searchers have long known that information seeking
is an iterative activity, which suggests that an inter-
active approach might be worth exploring.
This paper present a simple extension of a well-
known algorithm, Maximal Marginal Relevance
(MMR) (Goldstein et al, 2000), that places the user
in the loop. MMR is an iterative algorithm, where
at each step a candidate extract c (e.g., a sentence) is
assigned the following score:
?Rel(q, c)? (1? ?)max
s?S
Sim(s, c)
The score consists of two components: the rele-
vance of the candidate c with respect to the query
q (Rel) and the similarity of the candidate c to each
extract s in the current summary S (Sim). The maxi-
mum score from these similarity comparisons is sub-
tracted from the relevance score, subjected to a tun-
ing parameter that controls the emphasis on rele-
vance and anti-redundancy. Scores are recomputed
after each step and the algorithm iterates until a stop-
ping criterion has been met (e.g., length quota).
We propose a simple extension to MMR: at each
step, we interactively ask the user to select the best
sentence for inclusion in the summary. That is, in-
stead of the system automatically selecting the can-
didate with the highest score, it presents the user
with a ranked list of candidates for selection.
2 Complex, Interactive QA
One obstacle to assessing the effectiveness of in-
teractive summarization algorithms is the lack of a
suitable evaluation vehicle. Given the convergence
of complex QA and summarization (particularly the
query-focused variant) in recent years, we found an
appropriate evaluation vehicle in the ciQA (com-
plex, interactive Question Answering) task at TREC
2007 (Dang et al, 2007).
Information needs in the ciQA task, called top-
ics, consist of two parts: the question template and
the narrative. The question template is a stylized in-
formation need that has a fixed structure and free
slots whose instantiation varies across different top-
ics. The narrative is unstructured prose that elabo-
rates on the information need. For the evaluation,
NIST assessors developed 30 topics, grouped into
five templates. See Figure 1 for an example.
Participants in the task were able to deploy fully-
functional web-based QA systems, with which the
305
Template: What evidence is there for transport of
[drugs] from [Mexico] to [the U.S.]?
Narrative: The analyst would like to know of efforts
to curtail the transport of drugs from Mexico to the
U.S. Specifically, the analyst would like to know of
the success of the efforts by local or international au-
thorities.
Figure 1: Example topic from the TREC 2007 ciQA task.
NIST assessors interacted (serving as surrogates for
users). Upon receiving the topics, participants first
submitted an initial run. During a pre-arranged pe-
riod of time shortly thereafter, each assessor was
given five minutes to interact with the participant?s
system, live over the web. After this interaction pe-
riod, participants submitted a final run, which had
presumably gained the benefit of user interaction.
By comparing initial and final runs, it was possible
to quantify the effect of the interaction.
The target corpus was AQUAINT-2, which con-
sists of around 970k documents totaling 2.5 GB.
System responses consisted of multi-line answers
and were evaluated using the ?nugget? methodol-
ogy with the ?nugget pyramid? extension (Lin and
Demner-Fushman, 2006).
3 Experiment Design
This section describes our experiments for the
TREC 2007 ciQA task. In summary: the initial run
was generated automatically using standard MMR.
The web-based interactions consisted of iterations of
interactive MMR, where the user selected the best
candidate extract at each step. The final run con-
sisted of the output of interactive MMR padded with
automatically-generated output.
Sentence extracts were used as the basic re-
sponse unit. For each topic, the top 100 documents
were retrieved from the AQUAINT-2 collection with
Lucene, using the topic template verbatim as the
query. Neither the template structure nor the narra-
tive text were exploited. All documents were then
broken into individual sentences, which served as
the pool of candidates. The relevance of each sen-
tence was computed as the sum of the inverse doc-
ument frequencies of matching terms from the topic
template. Redundancy was computed as the cosine
similarity between the current answer (consisting of
Figure 2: Screenshot of the interface for interactive
MMR, which shows the current topic (A), the current an-
swer (B), and a ranked list of document extracts (C).
all previously-selected sentences) and the current
candidate. The relevance and redundancy scores
were then normalized and combined (? = 0.8). For
the initial run, the MMR algorithm iterated until 25
candidates had been selected.
For interactive MMR, a screenshot of the web-
based system is shown in Figure 2. The interface
consists of three elements: at the top (label A) is the
current topic; in the middle (label B) is the current
answer, containing user selections from previous it-
erations; the bottom area (label C) shows a ranked
list of candidate sentences ordered by MMR score.
At each iteration, the user is asked to select one can-
didate by clicking the ?Add to answer? button next
to that candidate. The selected candidate is then
added to the current answer. Ten answer candidates
are shown per page. Clicking on a button labeled
?Show more candidates? at the bottom of the page
(not shown in the screenshot) displays the next ten
candidates. In the ciQA 2007 evaluation, NIST as-
sessors engaged with this interface for the entire al-
lotted five minute interaction period. Note that this
simple interface was designed only to assess the ef-
fectiveness of interactive MMR, and not intended to
represent an actual interactive system.
To prevent users from seeing the same sentences
repeatedly once a candidate selection has been
recorded, we divide the scores of all candidates
ranked higher than the selected candidate by two (an
306
 0 5 10 15 20 25 30
 35 40
 56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85Number of Extracts Topic
Length of ciQA 2007 Final Answers: Number of Extracts complete answerinteractive MMRmean, interactive MMR
Figure 3: Per-topic lengths of final run in terms of num-
ber of extracts. Bars show contribution from interactive
MMR (darker) and ?padding? (lighter).
arbitrary constant). For example, if the user clicked
on candidate five, scores for candidates one through
four are cut in half. Previous studies have shown
that users generally examine ranked lists in order, so
the lack of a selection can be interpreted as negative
feedback (Joachims et al, 2007).
The answers constructed interactively were sub-
mitted to NIST as the final (post-interaction) run.
However, since these answers were significantly
shorter than the initial run (given the short interac-
tion period), the responses were ?padded? by run-
ning additional iterations of automatic MMR until a
length quota of 4000 characters had been achieved.
4 Results and Discussion
First, we present descriptive statistics of the final
run submitted to NIST. Lengths of the answers on
a per-topic basis are shown in Figure 3 in terms of
number of extracts: darker bars show the number of
manually-selected extracts for each topic during the
five-minute interaction period (i.e., the number of in-
teractive MMR iterations). The average across all
topics was 6.5 iterations, shown by the horizontal
line; the average length of answers (all user selec-
tions) was 1186 characters. The average rank of the
user selection was 4.9, and the user selected the top
ranking sentence 28% of the time. Note that the in-
teraction period included system processing as well
as delays caused by network traffic. The number of
extracts contained in the padding is shown by the
lighter gray portions of the bars. For topic 68, the
system did not record any user interactions (possi-
bly resulting from a network glitch).
 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35
 0.4 0.45
 0  500  1000  1500  2000  2500  3000  3500  4000Weighted Recall Length of Answer (non-whitespace characters)
TREC 2007 ciQA: interactive vs. non-interactive MMR
non-interactive MMRinteractive MMRsig., p<0.05
Figure 4: Weighted recall at different length increments,
comparing interactive and non-interactive MMR.
The official metric for the ciQA task was F-
measure, but a disadvantage of this single-point met-
ric is that it doesn?t account for answers of vary-
ing lengths. An alternative proposed by Lin (2007)
and used as the secondary metric in the evalua-
tion is recall-by-length plots, which characterize
weighted nugget recall at varying length incre-
ments. Weighted recall captures how much rele-
vant information is contained in the system response
(weighted by each nugget?s importance, with an up-
per bound of one). Responses that achieve higher
nugget recall at shorter length increments are desir-
able in providing concise, informative answers.
Recall-by-length plots for both the initial run
(non-interactive MMR) and final run (interactive
MMR with padding) are shown in Figure 4, in length
increments of 1000 characters. The vertical dotted
line denotes the average length of interactive MMR
answers (without padding). Taking length as a proxy
for time, one natural interpretation of this plot is how
quickly users are able to ?learn? about the topic of
interest under the two conditions.
We see that interactive MMR yields higher
weighted recall at all length increments. The
Wilcoxon signed-rank test was applied to assess the
statistical significance of the differences in weighted
recall at each length increment. Solid circles in the
graph represent improvements that are statistically
significant (p < 0.05). Furthermore, in the 700?
1000 character range, weighted recall is significantly
higher for interactive MMR at the 99% level.
Viewing weighted recall as a proxy for answer
quality, interactive MMR yields responses that are
significantly better than non-interactive MMR at
307
a range of length increments. This is an impor-
tant finding, since effective interaction techniques
that require little training and work well in limited-
duration settings are quite elusive. Often, user in-
put actually makes answers worse. Results from
both ciQA 2006 and ciQA 2007 show that, overall,
F-measure improved little between initial and final
runs. Although it is widely accepted that user feed-
back can enhance interactive IR, effective interac-
tion techniques to exploit this feedback are by no
means obvious.
To better understand the characteristics of interac-
tive MMR, it is helpful to compare our experiments
with the ciQA task-wide baseline. As a reference
for all participants, the organizers of the task sub-
mitted a pair of runs to help calibrate effectiveness.
According to Dang et al (2007), the first run was
prepared by submitting the question template ver-
batim as a query to Lucene to retrieve the top 20
documents. These documents were then tokenized
into individual sentences. Sentences that contained
at least one non-stopword from the question were re-
tained and returned as the initial run (up to a quota
of 5,000 characters). Sentence order within each
document and across the ranked list was preserved.
The interaction associated with this run asked the as-
sessor for relevance judgments on each of the sen-
tences. Three options were given: ?relevant?, ?not
relevant?, and ?no opinion?. The final run was pre-
pared by removing sentences judged not relevant.
Other evidence suggests that the task-wide sen-
tence retrieval algorithm represents a strong base-
line. Similar algorithms performed well in other
complex QA tasks?in TREC 2003, a sentence re-
trieval variant beat all but one run on definition ques-
tions (Voorhees, 2003). The sentence retrieval base-
line also performed well in ciQA 2006.
The MMR runs are compared to the task-wide
reference runs in Figure 5: diamonds denote the
sentence retrieval baseline and triangles mark the
manual sentence selection final run. The manual
sentence selection run outperforms the sentence re-
trieval baseline (as expected), but its weighted recall
is still below that of interactive MMR across almost
all length increments. The weighted recall of inter-
active MMR is significantly better at 1000 characters
(at the 95% level), but nowhere else. So, the bottom
line is: for limited-duration interactions, interactive
 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35
 0.4 0.45
 0  500  1000  1500  2000  2500  3000  3500  4000Weighted Recall Length of Answer (non-whitespace characters)
TREC 2007 ciQA: MMR vs. task-wide baseline
non-interactive MMRinteractive MMRsentence retrieval baselinemanual sentence selection
Figure 5: Weighted recall at different length increments,
comparing MMR with the task-wide baseline.
MMR is more effective than simply asking for rele-
vance judgments, but not significantly so.
5 Conclusion
We present an interactive extension of the Maximal
Marginal Relevance algorithm for query-focused
summarization. Results from the TREC 2007 ciQA
task demonstrate it is a simple yet effective tech-
nique for involving users in interactively construct-
ing responses to complex information needs. These
results provide a starting point for future work in in-
teractive summarization.
Acknowledgments
This work was supported in part by NLM/NIH. The
first author would like to thank Esther and Kiri for
their loving support.
References
H. Dang, J. Lin, and D. Kelly. 2007. Overview of the
TREC 2007 question answering track. TREC 2007.
J. Goldstein, V. Mittal, J. Carbonell, and J. Callan. 2000.
Creating and evaluating multi-document sentence ex-
tract summaries. CIKM 2000.
T. Joachims, L. Granka, B. Pan, H. Hembrooke,
F. Radlinski, and G. Gay. 2007. Evaluating the ac-
curacy of implicit feedback from clicks and query re-
formulations in Web search. TOIS, 25(2):1?27.
J. Lin and D. Demner-Fushman. 2006. Will pyramids
built of nuggets topple over? HLT/NAACL 2006.
J. Lin. 2007. Is question answering better than informa-
tion retrieval? Towards a task-based evaluation frame-
work for question series. HLT/NAACL 2007.
E. Voorhees. 2003. Overview of the TREC 2003 ques-
tion answering track. TREC 2003.
308
Proceedings of the NAACL HLT 2010: Tutorial Abstracts, pages 1?2,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Data-Intensive Text Processing with MapReduce
Jimmy Lin and Chris Dyer
University of Maryland, College Park
{jimmylin,redpony}@umd.edu
1. Overview
This half-day tutorial introduces participants to data-intensive text
processing with the MapReduce programming model [1], using the
open-source Hadoop implementation. The focus will be on scalability
and the tradeoffs associated with distributed processing of large
datasets. Content will include general discussions about algorithm
design, presentation of illustrative algorithms, case studies in HLT
applications, as well as practical advice in writing Hadoop programs
and running Hadoop clusters.
2. Intended Audience
The tutorial is targeted at any NLP researcher who is interested in
data-intensive processing and scalability issues in general. No
background in parallel or distributed computing is necessary, but a
prior knowledge of HLT is assumed.
3. Course Objectives
* Acquire understanding of the MapReduce programming model and how it
  relates to alternative approaches to concurrent programming.
* Acquire understanding of how data-intensive HLT problems (e.g., text
  retrieval, iterative optimization problems, and graph algorithms)
  can be solved using MapReduce.
* Acquire understanding of the tradeoffs involved in designing
  MapReduce algorithms and awareness of associated engineering issues.
4. Tutorial Topics
The following represents a tentative list of topics that will be covered:
* Introduction to parallel and distributed processing
* Introduction to MapReduce
* Tradeoffs and issues in algorithm design
* Simple counting applications (e.g., relative frequency estimation)
* Applications to inverted indexing and text retrieval
* Applications to graph algorithms
1
* Applications to iterative optimization algorithms (e.g., EM)
* Case study in machine translation
* Tips and tricks in writing Hadoop programs
* Practical issues in running Hadoop clusters
5. Instructor Bios
Jimmy Lin is an Associate Professor in the iSchool at the University
of Maryland, College Park. He joined the faculty in 2004 after
completing his Ph.D. in Electrical Engineering and Computer Science at
MIT. Dr Lin's research interests lie at the intersection of natural
language processing and information retrieval. He leads the University
of Maryland's effort in the Google/IBM Academic Cloud Computing
Initiative. Dr. Lin has taught two semester-long Hadoop courses and
has given numerous talks and tutorials about MapReduce to a wide
audience.
Chris Dyer is a Ph.D. student at the University of Maryland, College
Park, in the Department of Linguistics.  His current research
interests include statistical machine translation, machine learning,
and the relationship between artificial language processing systems
and the human linguistic processing system. He has served on program
committees for AMTA, ACL, COLING, EACL, EMNLP, NAACL, ISWLT, and the
ACL Workshops on Machine translation, and is one of the developers of
the Moses open source machine translation toolkit. He has practical
experience solving NLP problems with both the Hadoop MapReduce
framework and Google's MapReduce implementation, which was made
possible by an internship with Google Research in 2008.
References
[1] Dean, Jeffrey and Sanjay Ghemawat. MapReduce: Simplified Data
Processing on Large Clusters. Proceedings of the 6th Symposium on
Operating System Design and Implementation (OSDI 2004), p. 137-150,
2004, San Francisco, California.
[2] Jimmy Lin. Exploring Large-Data Issues in the Curriculum: A Case
Study with MapReduce. Proceedings of the Third Workshop on Issues in
Teaching Computational Linguistics (TeachCL-08) at ACL 2008, p. 54-61,
2008, Columbus, Ohio.
2
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 626?630,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Why Not Grab a Free Lunch? Mining Large Corpora for
Parallel Sentences to Improve Translation Modeling
Ferhan Ture
Dept. of Computer Science,
University of Maryland
fture@cs.umd.edu
Jimmy Lin
The iSchool
University of Maryland
jimmylin@umd.edu
Abstract
It is well known that the output quality of
statistical machine translation (SMT) systems
increases with more training data. To ob-
tain more parallel text for translation mod-
eling, researchers have turned to the web to
mine parallel sentences, but most previous ap-
proaches have avoided the difficult problem
of pairwise similarity on cross-lingual docu-
ments and instead rely on heuristics. In con-
trast, we confront this challenge head on us-
ing the MapReduce framework. On a mod-
est cluster, our scalable end-to-end processing
pipeline was able to automatically gather 5.8m
parallel sentence pairs from English and Ger-
man Wikipedia. Augmenting existing bitext
with these data yielded significant improve-
ments over a state-of-the-art baseline (2.39
BLEU points in the best case).
1 Introduction
It has been repeatedly shown that ?throwing more
data at the problem? is effective in increasing SMT
output quality, both for translation modeling (Dyer
et al, 2008) and for language modeling (Brants et
al., 2007). In this paper, we bring together two re-
lated research threads to gather parallel sentences for
improved translation modeling: cross-lingual pair-
wise similarity to mine comparable documents and
classification to identify sentence pairs that are mu-
tual translations.
Unlike most previous work, which sidesteps the
computationally-intensive task of pairwise compar-
isons to mine comparable documents and instead re-
lies on heuristics, we tackle the challenge head on.
This paper describes a fully open-source, scalable
MapReduce-based processing pipeline that is able to
automatically extract large quantities of parallel sen-
tences. Experiments examine the impact data size
has on a state-of-the-art SMT system.
We acknowledge that different components of this
work are not novel and the general principles behind
?big data? MT are well known. However, when con-
sidered together with our previous work (Ture et al,
2011), to our knowledge this is the first exposition
in which all the pieces have been ?put together? in
an end-to-end pipeline that is accessible to academic
research groups. The framework described in this
paper is entirely open source, and the computational
resources necessary to replicate our results are rela-
tively modest.
Starting from nothing more than two corpora in
different languages (in German and English, in our
case), we are able to extract bitext and improve
translation quality by a significant margin (2.39
BLEU points), essentially ?for free?. By varying
both the quantity and quality of the bitext, we char-
acterize the tradeoffs between the amount of data,
computational costs, and translation quality.
2 Related Work
The idea of mining parallel sentences, particularly
from the web, is of course not new. Most adopt a
two step process: 1. identify comparable documents
and generate candidate sentence pairs, and 2. filter
candidate pairs to retain parallel sentences.
The general solution to the first step involves com-
puting pairwise similarities across multi-lingual cor-
pora. As this is computationally intensive, most
626
studies fall back to heuristics, e.g., comparing news
articles close in time (Munteanu and Marcu, 2005),
exploiting ?inter-wiki? links in Wikipedia (Smith et
al., 2010), or bootstrapping off an existing search
engine (Resnik and Smith, 2003). In contrast, we
adopt a more exhaustive approach by directly tack-
ling the cross-lingual pairwise similarity problem,
using MapReduce on a modest cluster. We perform
experiments on German and English Wikipedia (two
largest available), but our technique is general and
does not depend on sparse, manually-created inter-
wiki links. Thus, compared to those approaches, we
achieve much higher recall.
The second step (filtering candidate sentence
pairs) is relatively straightforward, and we adopt
the classification approach of Munteanu and
Marcu (2005). However, unlike in previous work,
we need to classify large volumes of data (due to
higher recall in the first step). Therefore, we care
about the relationship between classification accu-
racy and the speed of the classifier. Our two-stage
approach gives us both high effectiveness (accuracy)
and efficiency (speed).
A recent study from Google describes a general
solution to our problem that scales to web collec-
tions (Uszkoreit et al, 2010). The authors translate
all documents from one language into another, thus
transforming the problem into identifying similar
mono-lingual document pairs. Nevertheless, our ap-
proach makes several additional contributions. First,
we explore the effect of dataset size on results. Our
conclusions are more nuanced than simply ?more
data is better?, since there is a tradeoff between qual-
ity and quantity. Our experiments involve orders
of magnitude less data, but we nevertheless observe
significant gains over a strong baseline. Overall, our
approach requires far less computational resources
and thus is within the reach of academic research
groups: we do not require running an MT system
on one side of the entire collection, and we care-
fully evaluate and control the speed of sentence-
classification. Finally, in support of open science,
our code1 and data2 are available as part of Ivory, an
open-source Hadoop toolkit for web-scale informa-
tion retrieval (Lin et al, 2009).
1ivory.cc
2github.com/ferhanture/WikiBitext
3 Generating Candidate Sentences
We applied our approach on English Wikipedia
(10.9m documents, 30.6GB) and German Wikipedia
(2.4m articles, 8.5GB), using XML dumps from Jan-
uary 2011. English and German Wikipedia were se-
lected because they are the largest Wikipedia collec-
tions available, and we want to measure effects in a
language for which we already have lots of bitext.
In both collections, redirect pages and stub articles
were discarded.
To mine comparable documents, we used our
previously described algorithm (Ture et al, 2011),
based on local-sensitive hashing, also implemented
in Hadoop MapReduce. The reader is referred to
the paper for details. On a 16 node (96 core) cluster,
we were able to extract 64m (de, df ) document pairs
(with cosine similarity ? 0.3) in 8.8 hours.
For each of the (de, df ) pairs, the next process-
ing step involves generating the Cartesian product of
sentences in both documents as candidate sentence
pairs: this itself is a non-trivial problem. Although
in this particular case it may be possible to load both
document collections in memory, we envision scal-
ing up to collections in the future for which this is
not possible. Therefore, we devised a scalable, dis-
tributed, out-of-memory solution using Hadoop.
The algorithm works as follows: We map over
(docid n, document d) pairs from both the German
and English collections. In each mapper all (de, df )
similarity pairs are loaded in memory. If the input
document is not found in any of these pairs, no work
is performed. Otherwise, we extract all sentences
and retain only those that have at least 5 terms and
at least 3 unique terms. Sentences are converted into
BM25-weighted vectors in the English term space;
for German sentences, translation into English is ac-
complished using the technique proposed by Dar-
wish and Oard (2003). For every (de, df ) pair that
the input document is found in, the mapper emits the
list of weighted sentence vectors, with the (de, df )
pair as the key. As all intermediate key-value pairs
in MapReduce are grouped by their keys for reduce-
side processing, the reducer receives the key (de, df )
and weighted sentence vectors for both the German
and English articles. From there, we generate the
Cartesian product of sentences in both languages.
As an initial filtering step, we discard all pairs where
627
the ratio of sentence lengths is more than two, a
heuristic proposed in (Munteanu and Marcu, 2005).
Each of the remaining candidate sentences are then
processed by two separate classifiers: a less accurate,
fast classifier and a more accurate, slow classifier.
This is described in the next section.
This algorithm is a variant of what is commonly
known as a reduce-side join in MapReduce (Lin
and Dyer, 2010), where (de, df ) serves as the
join key. Note that in this algorithm, sentence
vectors are emitted multiple times, one for each
(de, df ) pair that they participate in: this results
in increased network traffic during the sort/shuffle
phase. We experimented with an alternative algo-
rithm that processes all foreign documents similar
to the same English document together, e.g., pro-
cessing (de, [df1, df2, . . .]) together. This approach,
counter-intuitively, was slower despite reduced net-
work traffic, due to skew in the distribution of sim-
ilar document pairs. In our experiments, half of the
source collection was not linked to any target docu-
ment, whereas 4% had more than 100 links. This re-
sults in reduce-side load imbalance, and while most
of the reducers finish quickly, a few reducers end
up performing substantially more computation, and
these ?stragglers? increase end-to-end running time.
4 Parallel Sentence Classification
We built two MaxEnt parallel sentence classifiers us-
ing the OpenNLP package, with data from a sam-
ple of the Europarl corpus of European parliament
speeches. For training, we sampled 1000 parallel
sentences from the German-English subset of the
corpus as positive instances, and 5000 non-parallel
sentence pairs as negative instances. For testing, we
sampled another 1000 parallel pairs and generated
all possible non-parallel pairs by the Cartesian prod-
uct of these samples. This provides a better approx-
imation of the task we?re interested in, since most of
the candidate sentence pairs will be non-parallel in a
comparable corpus. We report precision, recall, and
F-score, using different classifier confidence scores
as the decision threshold (see Table 1).
Our first, simple classifier, which uses cosine sim-
ilarity between the sentences as the only feature,
achieved a maximum F-score of 74%, with 80%
precision and 69% recall. Following previous work
Classifier Measure Value
Simple
Recall @ P90 0.59
Recall @ P80 0.69
Best F-score 0.74
Complex
Recall @ P90 0.69
Recall @ P80 0.79
Best F-score 0.80
Table 1: Accuracy of the simple and complex sentence
classifiers on Europarl data.
(Smith et al, 2010), we also report recall with pre-
cision at 80% and 90% in Table 1; the classifier ef-
fectiveness is comparable to the previous work. The
second, complex classifier uses the following addi-
tional features: ratio of sentence lengths, ratio of
source-side tokens that have translations on the tar-
get side, ratio of target-side tokens that have trans-
lations on the source side. We also experimented
with features using the word alignment output, but
there was no improvement in accuracy. The com-
plex classifier showed better performance: recall of
79% at 80% precision and 69% at precision of 90%,
with a maximum F-score of 80%.
Due to the large amounts of data involved in our
experiments, we were interested in speed/accuracy
tradeoffs between the two classifiers. Microbench-
marks were performed on a commodity laptop run-
ning Mac OS X on a 2.26GHz Intel Core Duo CPU,
measuring per-instance classification speed (includ-
ing feature computation time). The complex classi-
fier took 100 ?s per instance, about 4 times slower
than the simple one, which took 27 ?s.
The initial input of 64m similar document pairs
yielded 400b raw candidate sentence pairs, which
were first reduced to 214b by the per-sentence length
filter, and then to 132b by enforcing a maximum sen-
tence length ratio of 2. The simple classifier was
applied to the remaining pairs, with different confi-
dence thresholds. We adjusted the threshold to ob-
tain different amounts of bitext, to see the effect on
translation quality (this condition is called S1 here-
after). The positive results of the first classifier was
then processed by the second classifier (this two-
level approach is called S2 hereafter).
Candidate generation was completed in 2.4 hours
on our cluster with 96 cores. These candidates went
through the MapReduce shuffle-and-sort process in
0.75 hours, which were then classified in 4 hours.
628
Processing by the more complex classifier in S2 took
an additional 0.52 hours.
5 End-to-End MT Experiments
In all experiments, our MT system learned a syn-
chronous context-free grammar (Chiang, 2007), us-
ing GIZA++ for word alignments, MIRA for pa-
rameter tuning (Crammer et al, 2006), cdec for de-
coding (Dyer et al, 2010), a 5-gram SRILM for
language modeling, and single-reference BLEU for
evaluation. The baseline system was trained on the
German-English WMT10 training data, consisting
of 3.1m sentence pairs. For development and test-
ing, we used the newswire datasets provided for
WMT10, including 2525 sentences for tuning and
2489 sentences for testing.
Our baseline system includes all standard fea-
tures, including phrase translation probabilities in
both directions, word and arity penalties, and lan-
guage model scores. It achieves a BLEU score
of 21.37 on the test set, which would place it 5th
out of 9 systems that reported comparable results
in WMT10 (only three systems achieved a BLEU
score over 22). Many of these systems used tech-
niques that exploited the specific aspects of the task,
e.g., German-specific morphological analysis. In
contrast, we present a knowledge-impoverished, en-
tirely data-driven approach, by simply looking for
more data in large collections.
For both experimental conditions (one-step classi-
fication, S1, and two-step classification, S2) we var-
ied the decision threshold to generate new bitext col-
lections of different sizes. Each of these collections
was added to the baseline training data to induce
an entirely new translation model (note that GIZA
additionally filtered out some of the pairs based on
length). The final dataset sizes, along with BLEU
scores on the test data, are shown in Fig. 1. In S1, we
observe that increasing the amount of data (by low-
ering the decision threshold) initially leads to lower
BLEU scores (due to increased noise), but there is a
threshold after which the improvement coming from
the added data supersedes the noise. The S2 condi-
tion increases the quality of bitext by reducing this
noise: the best run, with 5.8m pairs added to the
baseline (final dataset has 8.1m pairs), yields 23.76
BLEU (labeled P on figure), 2.39 points above the
 23
 23.5
 3  3.5  4  4.5  5  5.5  6  6.5  7  7.5  8  8.5
BLE
U sc
ore
Training data size (millions)
Baseline BLEU = 21.37
P
S1 (1-step classification)S2 (2-step classification)Sampling data from training set P
Figure 1: Evaluation results on the WMT10 test set.
baseline (and higher than the best WMT10 result).
These results show that the two-step classification
process, while slower, is worth the additional pro-
cessing time.
Our approach yields solid improvements even
with less data added: with only 382k pairs added
to the baseline, the BLEU score increases by 1.84
points. In order to better examine the effect of
data size alone, we created partial datasets from P
by randomly sampling sentence pairs, and then re-
peated experiments, also shown in Fig. 1. We see
an increasing trend of BLEU scores with respect to
data size. By comparing the three plots, we see that
S2 and random sampling from P work better than
S1. Also, random sampling is not always worse than
S2, since some pairs that receive low classifier con-
fidence turn out to be helpful.
6 Conclusions
In this paper, we describe a scalable MapReduce im-
plementation for automatically mining parallel sen-
tences from arbitrary comparable corpora. We show,
at least for German-English MT, that an impover-
ished, data-driven approach is more effective than
task-specific engineering. With the distributed bi-
text mining machinery described in this paper, im-
provements come basically ?for free? (the only cost
is a modest amount of cluster resources). Given the
availability of data and computing power, there is
simply no reason why MT researchers should not
ride the large-data ?tide? that lifts all boats. For the
benefit of the community, all code necessary to repli-
cate these results have been open sourced, as well as
the bitext we?ve gathered.
629
Acknowledgments
This research was supported in part by the BOLT
program of the Defense Advanced Research Projects
Agency, Contract No. HR0011-12-C-0015; NSF un-
der awards IIS-0916043 and CCF-1018625. Any
opinions, findings, conclusions, or recommenda-
tions expressed in this paper are those of the authors
and do not necessarily reflect the view of the spon-
sors. The second author is grateful to Esther and
Kiri for their loving support and dedicates this work
to Joshua and Jacob.
References
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och,
and Jeffrey Dean. 2007. Large language models in
machine translation. Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 858?867, Prague, Czech Re-
public.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33:201?228.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. Journal of Machine Learning
Research, 7:551?585.
Kareem Darwish and Douglas W. Oard. 2003. Analysis
of anchor text for web search. Proceedings of the 26th
Annual International ACM SIGIR Conference on Re-
search and Development in Information Retrieval (SI-
GIR 2003), pages 261?268, Toronto, Canada.
Chris Dyer, Aaron Cordova, Alex Mont, and Jimmy Lin.
2008. Fast, easy, and cheap: Construction of statisti-
cal machine translation models with MapReduce. Pro-
ceedings of the Third Workshop on Statistical Machine
Translation at ACL 2008, pages 199?207, Columbus,
Ohio.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Jonathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Setiawan,
Vladimir Eidelman, and Philip Resnik. 2010. cdec: A
decoder, alignment, and learning framework for finite-
state and context-free translation models. Proceedings
of the ACL 2010 System Demonstrations, pages 7?12,
Uppsala, Sweden, July.
Jimmy Lin and Chris Dyer. 2010. Data-Intensive Text
Processing with MapReduce. Morgan & Claypool
Publishers.
Jimmy Lin, Donald Metzler, Tamer Elsayed, and Lidan
Wang. 2009. Of Ivory and Smurfs: Loxodontan
MapReduce experiments for web search. Proceedings
of the Eighteenth Text REtrieval Conference (TREC
2009), Gaithersburg, Maryland.
Dragos Stefan Munteanu and Daniel Marcu. 2005. Im-
proving machine translation performance by exploit-
ing non-parallel corpora. Computational Linguistics,
31(4):477?504.
Philip Resnik and Noah A. Smith. 2003. The web
as a parallel corpus. Computational Linguistics,
29(3):349?380.
Jason R. Smith, Chris Quirk, and Kristina Toutanova.
2010. Extracting parallel sentences from comparable
corpora using document level alignment. Proceedings
of Human Language Technologies: The 2010 Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics (HLT/NAACL
2010), pages 403?411, Los Angeles, California.
Ferhan Ture, Tamer Elsayed, and Jimmy Lin. 2011.
No free lunch: Brute force vs. locality-sensitive hash-
ing for cross-lingual pairwise similarity. Proceedings
of the 34th Annual International ACM SIGIR Confer-
ence on Research and Development in Information Re-
trieval (SIGIR 2011), pages 943?952, Beijing, China.
Jakob Uszkoreit, Jay M. Ponte, Ashok C. Popat, and
Moshe Dubiner. 2010. Large scale parallel document
mining for machine translation. Proceedings of the
23rd International Conference on Computational Lin-
guistics (COLING 2010), pages 1101?1109, Beijing,
China.
630
Proceedings of NAACL-HLT 2013, pages 325?334,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Massively Parallel Suffix Array Queries and On-Demand Phrase Extraction
for Statistical Machine Translation Using GPUs
Hua He
Dept. of Computer Science
University of Maryland
College Park, Maryland
huah@cs.umd.edu
Jimmy Lin
iSchool and UMIACS
University of Maryland
College Park, Maryland
jimmylin@umd.edu
Adam Lopez
HLTCOE
Johns Hopkins University
Baltimore, Maryland
alopez@cs.jhu.edu
Abstract
Translation models in statistical machine
translation can be scaled to large corpora
and arbitrarily-long phrases by looking up
translations of source phrases ?on the fly?
in an indexed parallel corpus using suffix
arrays. However, this can be slow because
on-demand extraction of phrase tables is
computationally expensive. We address this
problem by developing novel algorithms for
general purpose graphics processing units
(GPUs), which enable suffix array queries
for phrase lookup and phrase extraction to
be massively parallelized. Compared to
a highly-optimized, state-of-the-art serial
CPU-based implementation, our techniques
achieve at least an order of magnitude
improvement in terms of throughput. This
work demonstrates the promise of massively
parallel architectures and the potential
of GPUs for tackling computationally-
demanding problems in statistical machine
translation and language processing.
1 Introduction
Efficiently handling large translation models is a
perennial problem in statistical machine translation.
One particularly promising solution (?2) is to use
the parallel text itself as an implicit representation
of the translation model and extract translation units
?on the fly? when they are needed to decode new
input (Brown, 2004). This idea has been applied
to phrase-based (Callison-Burch et al, 2005; Zhang
and Vogel, 2005), hierarchical (Lopez, 2007; Lopez,
2008b; Lopez, 2008a), and syntax-based (Cromieres
and Kurohashi, 2011) models. A benefit of this
technique is that it scales to arbitrarily large models
with very little pre-processing. For instance, Lopez
(2008b) showed that a translation model trained on
a large corpus with sparse word alignments and
loose extraction heuristics substantially improved
Chinese-English translation. An explicit represen-
tation of the model would have required nearly a
terabyte of memory, but its implicit representation
using the parallel text required only a few gigabytes.
Unfortunately, there is substantial computational
cost in searching a parallel corpus for source
phrases, extracting their translations, and scoring
them on the fly. Since the number of possible
translation units may be quite large (for example,
all substrings of a source sentence) and their
translations are numerous, both phrase lookup and
extraction are performance bottlenecks. Despite
considerable research and the use of efficient
indexes like suffix arrays (Manber and Myers,
1990), this problem remains not fully solved.
We show how to exploit the massive parallelism
offered by modern general purpose graphics pro-
cessing units (GPUs) to eliminate the computational
bottlenecks associated with ?on the fly? phrase ex-
traction. GPUs have previously been applied to
DNA sequence matching using suffix trees (Schatz
et al, 2007) and suffix arrays (Gharaibeh and Ri-
peanu, 2010). Building on this work, we present
two novel contributions: First, we describe improved
GPU algorithms for suffix array queries that achieve
greater parallelism (?3). Second, we propose novel
data structures and algorithms for phrase extraction
(?4) and scoring (?5) that are amenable to GPU par-
325
allelization. The resulting implementation achieves
at least an order of magnitude higher throughput
than a state-of-the-art single-threaded CPU imple-
mentation (?6). Since our experiments verify that
the GPU implementation produces exactly the same
results as a CPU reference implementation on a full
extraction, we can simply replace that component
and reap significant performance advantages with no
impact on translation quality. To the best of our
knowledge, this is the first reported application of
GPU acceleration techniques for statistical machine
translation. We believe these results reveal a promis-
ing yet unexplored future direction in exploiting par-
allelism to tackle perennial performance bottlenecks
in state-of-the-art translation models.
2 Phrase Extraction On Demand
Lopez (2008b) provides the following recipe for
?translation by pattern matching?, which we use as
a guide for the remainder of this paper:
Algorithm 1 Translation by pattern matching
1: for each input sentence do
2: for each possible phrase in the sentence do
3: Find its occurrences in the source text
4: for each occurrence do
5: Extract its aligned target phrase (if any)
6: for each extracted phrase pair do
7: Compute feature values
8: Decode as usual using the scored rules
The computational bottleneck occurs in lines 2?7:
there are vast numbers of query phrases, matching
occurrences, and extracted phrase pairs to process in
the loops. In the next three sections, we attack each
problem in turn.
3 Finding Every Phrase
First, we must find all occurrences of each source
phrase in the input (line 3, Algorithm 1). This
is a classic application of string pattern matching:
given a short query pattern, the task is to find all
occurrences in a much larger text. Solving the
problem efficiently is crucial: for an input sentence
F of length |F |, each of its O(|F |2) substrings is a
potential query pattern.
3.1 Pattern Matching with Suffix Arrays
Although there are many algorithms for pattern
matching, all of the examples that we are aware
of for machine translation rely on suffix arrays.
We briefly review the classic algorithms of Manber
and Myers (1990) here since they form the basis
of our techniques and analysis, but readers who
are familiar with them can safely skip ahead to
additional optimizations (?3.2).
A suffix array represents all suffixes of a corpus
in lexicographical order. Formally, for a text T , the
ith suffix of T is the substring of the text beginning
at position i and continuing to the end of T . Each
suffix can therefore be uniquely identified by the
index i of its first word. A suffix array S(T )
of T is a permutation of these suffix identifiers
[1, |T |] arranged by the lexicographical order of the
corresponding suffixes?in other words, the suffix
array represents a sorted list of all suffixes in T .
With both T and S(T ) in memory, we can find any
query pattern Q in O(|Q| log |T |) time by compar-
ing pattern Q against the first |Q| characters of up to
log |T | different suffixes using binary search.
An inefficiency in this solution is that each com-
parison in the binary search algorithm requires com-
paring all |Q| characters of the query pattern against
some suffix of text T . We can improve on this using
an observation about the longest common prefix
(LCP) of the query pattern and the suffix against
which it is compared. Suppose we search for a query
pattern Q in the span of the suffix array beginning at
suffix L and ending at suffix R. For any suffix M
which falls lexicographically between those at L and
R, the LCP of Q and M will be at least as long as
the LCP of Q and L or Q and R. Hence if we know
the quantity h = MIN(LCP(Q,L), LCP(Q,R)) we
can skip comparisons of the first h symbols between
Q and the suffix M , since they must be the same.
The solution of Manber and Myers (1990) ex-
ploits this fact along with the observation that each
comparison in binary search is carried out accord-
ing to a fixed recursion scheme: a query is only
ever compared against a specific suffix M for a
single range of suffixes bounded by some fixed L
and R. Hence if we know the longest common
prefix between M and each of its corresponding
L and R according to the fixed recursions in the
326
algorithm, we can maintain a bound on h and reduce
the aggregate number of symbol comparisons to
O(|Q| + log |T |). To accomplish this, in addition
to the suffix array, we pre-compute two other arrays
of size |T | for both left and right recursions (called
the LCP arrays).
Memory use is an important consideration, since
GPUs have less memory than CPUs. For the algo-
rithms described here, we require four arrays: the
original text T , the suffix array S(T ), and the two
LCP arrays. We use a representation of T in which
each word has been converted to a unique integer
identifier; with 32-bit integers the total number of
bytes is 16|T |. As we will show, this turns out to be
quite modest, even for large parallel corpora (?6).
3.2 Suffix Array Efficiency Tricks
Previous work on translation by pattern matching
using suffix arrays on serial architectures has pro-
duced a number of efficiency optimizations:
1. Binary search bounds for longer substrings are
initialized to the bounds of their longest prefix.
Substrings are queried only if their longest
prefix string was matched in the text.
2. In addition to conditioning on the longest pre-
fix, Zhang and Vogel (2005) and Lopez (2007)
condition on a successful query for the longest
proper suffix.
3. Lopez (2007) queries each unique substring
of a sentence exactly once, regardless of how
many times it appears in an input sentence.
4. Lopez (2007) directly indexes one-word sub-
strings with a small auxiliary array, so that
their positions in the suffix array can be found
in constant time. For longer substrings, this
optimization reduces the log |T | term of query
complexity to log(count(a)), where a is the
first word of the query string.
Although these efficiency tricks are important in the
serial algorithms that serve as our baseline, not all
of them are applicable to parallel architectures. In
particular, optimizations (1), (2), and (3) introduce
order dependencies between queries; they are disre-
garded in our GPU implementation so that we can
fully exploit parallelization opportunities. We have
not yet fully implemented (4), which is orthogonal
to parallelization: this is left for future work.
3.3 Finding Every Phrase on a GPU
Recent work in computational biology has shown
that suffix arrays are particularly amenable to GPU
acceleration: the suffix-array-based DNA sequence
matching system MummurGPU++ (Gharaibeh and
Ripeanu, 2010) has been reported to outperform the
already fast MummurGPU 2 (Trapnell and Schatz,
2009), based on suffix trees (an alternative indexing
structure). Here, we apply the same ideas to ma-
chine translation, introducing some novel improve-
ments to their algorithms in the process.
A natural approach to parallelism is to perform
all substring queries in parallel (Gharaibeh and Ri-
peanu, 2010). There are no dependencies between
iterations of the loop beginning on line 2 of Algo-
rithm 1, so for input sentence F , we can parallelize
by searching for all O(|F |2) substrings concurrently.
We adopt this approach here.
However, na??ve application of query-level paral-
lelism leads to a large number of wasted threads,
since most long substrings of an input sentence will
not be found in the text. Therefore, we employ
a novel two-pass strategy: in the first pass, we
simply compute, for each position i in the input
sentence, the length j of the longest substring in F
that appears in T . These computations are carried
out concurrently for every position i. During this
pass, we also compute the suffix array bounds of the
one-word substring F [i], to be used as input to the
second pass?a variant of optimizations (1) and (4)
discussed in ?3.2. On the second pass, we search
for all substrings F [i, k] for all k ? [i + 1, i + j].
These computations are carried out concurrently for
all substrings longer than one word.
Even more parallelization is possible. As we saw
in ?3.1, each query in a suffix array actually requires
two binary searches: one each for the first and last
match in S(T ). The abundance of inexpensive
threads on a GPU permits us to perform both queries
concurrently on separate threads. By doing this in
both passes we utilize more of the GPU?s processing
power and obtain further speedups.
As a simple example, consider an input sentence
?The government puts more tax on its citizens?, and
suppose that substrings ?The government?, ?gov-
ernment puts?, and ?puts more tax? are found in
the training text, while none of the words in ?on
327
Initial Word Longest Match Substrings Threads
1st pass 2nd pass
The 2 The, The government 2 2
government 2 government, government puts 2 2
puts 3 puts, puts more, puts more tax 2 4
more 2 more, more tax 2 2
tax 1 tax 2 0
on 0 ? 2 0
its 0 ? 2 0
citizens 0 ? 2 0
Total Threads: 16 10
Table 1: Example of how large numbers of suffix array queries can be factored across two highly parallel passes on a
GPU with a total of 26 threads to perform all queries for this sample input sentence.
its citizens? are found. The number of threads
spawned is shown in Table 1: all threads during a
pass execute in parallel, and each thread performs a
binary search which takes no more than O(|Q| +
log |T |) time. While spawning so many threads
may seem wasteful, this degree of parallelization
still under-utilizes the GPU; the hardware we use
(?6) can manage up to 21,504 concurrent threads
in its resident occupancy. To fully take advantage
of the processing power, we process multiple input
sentences in parallel. Compared with previous
algorithms, our two-pass approach and our strategy
of thread assignment to increase the amount of
parallelism represent novel contributions.
4 Extracting Aligned Target Phrases
The problem at line 5 of Algorithm 1 is to extract the
target phrase aligned to each matching source phrase
instance. Efficiency is crucial since some source
phrases occur hundreds of thousands of times.
Phrase extraction from word alignments typically
uses the consistency check of Och et al (1999). A
consistent phrase is one for which no words inside
the phrase pair are aligned to words outside the
phrase pair. Usually, consistent pairs are computed
offline via dynamic programming over the align-
ment grid, from which we extract all consistent
phrase pairs up to a heuristic bound on phrase length.
The online extraction algorithm of Lopez (2008a)
checks for consistent phrases in a different manner.
Rather than finding all consistent phrase pairs in
a sentence, the algorithm asks: given a specific
source phrase, is there a consistent phrase pair
Figure 1: Source phrase f2f3f4 and target phrase
e2e3e4 are extracted as a consistent pair, since the back-
projection is contained within the original source span.
Figure 2: Source phrase f2f3f4 and target phrase e2e3e4
should not be extracted, since the back-projection is not
contained within the original source span.
of which it is one side? To answer this, it first
computes the projection of the source phrase in the
target sentence: the minimum span containing all
words that are aligned to any word of the source
span. It then computes the projection of the target
span back into the source; if this back-projection
is contained within the original source span, the
phrase pair is consistent, and the target span is
extracted as the translation of the source. Figure 1
shows a ?good? pair for source phrase f2f3f4, since
the back-projection is contained within the original
source span, whereas Figure 2 shows a ?bad? pair
for source phrase f2f3f4 since the back-projection
is not contained within the original source span.
328
4.1 Sampling Consistent Phrases
Regardless of how efficient the extraction of a single
target phrase is made, the fact remains that there
are many phrases to extract. For example, in our
Chinese Xinhua dataset (see ?6), from 8,000 input
query sentences, about 20 million source substrings
can be extracted. The standard solution to this
problem is to sample a set of occurrences of each
source phrase, and only extract translations for those
occurrences (Callison-Burch et al, 2005; Zhang and
Vogel, 2005). As a practical matter, this can be done
by sampling at uniform intervals from the matching
span of a suffix array. Lopez (2008a) reports a
sample size of 300; for phrases occurring fewer than
300 times, all translations are extracted.
4.2 GPU Implementation
We present novel data structures and an algorithm
for efficient phrase extraction, which together are
amenable to massive parallelization on GPUs. The
basic insight is to pre-compute data structures for
the source-to-target algnment projection and back-
projection procedure described by Lopez (2008a)
for checking consistent alignments.
Let us consider a single matching substring (from
the output of the suffix array queries), span [i, j] in
the source text T . For each k, we need to know the
leftmost and rightmost positions that it aligns to in
the target T ?. For this purpose we can define the
target span [i?, j?], along with leftmost and rightmost
arrays L and R as follows:
i? := min
k?[i,j]
L(k)
j? := max
k?[i,j]
R(k)
The arrays L and R are each of length |T |, in-
dexed by absolute corpus position. Each array
element contains the leftmost and rightmost extents
of the source-to-target algnments (in the target),
respectively. Note that in order to save space,
the values stored in the arrays are sentence-relative
positions (e.g., token count from the beginning of
each sentence), so that we only need one byte per
array entry. Thus, i? and j? are sentence-relative
positions (in the target).
Similarly, for the back-projection, we use two
arrays L? and R? on the target side (length |T ?|) to
keep track of the leftmost and rightmost positions
that k? in the target training text align to, as below:
i?? := min
k??[s?+i?,s?+j?]
L?(k?)
j?? := max
k??[s?+i?,s?+j?]
R?(k?)
The arrays L? and R? are indexed by absolute corpus
positions, but their contents are sentence relative
positions (on the source side). To index the arrays
L? and R?, we also need to obtain the corresponding
target sentence start position s?. Note that the back-
projected span [i??, j??] may or may not be the same
as the original span [i, j]. In fact, this is exactly what
we must check for to ensure a consistent alignment.
The suffix array gives us i, which is an ab-
solute corpus position, but we need to know the
sentence-relative position, since the spans computed
by R,L,R?, L? are all sentence relative. To solve
this, we introduce an array P (length |T |) that gives
the relative sentence position of each source word.
We then pack the three source side arrays (R, L,
and P ) into a single RLP array of 32-bit integers
(note that we are actually wasting one byte per array
element). Finally, since the end-of-sentence special
token is not used in any of R, L, or P , its position
in RLP can be used to store an index to the start
of the corresponding target sentence in the target
array T ?. Now, given a source phrase spanning
[i, j] (recall, these are absolute corpus positions), our
phrase extraction algorithm is as follows:
Algorithm 2 Efficient Phrase Extraction Algorithm
1: for each source span [i, j] do
2: Compute [i?, j?]
3: s := i? P [i]? 1
4: s? := RLP [s]
5: i?? := mink??[s?+i?,s?+j?] L
?(k?)
6: j?? := maxk??[s?+i?,s?+j?]R
?(k?)
7: If i? s = i?? and j ? s = j?? then
8: Extract T [i, j] with T ?[s? + i?, s? + j?]
where s is the source sentence start position of a
given source phrase and s? is the target sentence
start position. If the back-projected spans match the
original spans, the phrase pair T [i, j] and T ?[s? +
i?, s? + j?] is extracted.
In total, the data structures RLP , R?, and L?
require 4|T | + 2|T ?| bytes. Not only is this phrase
329
extraction algorithm fast?requiring only a few in-
direct array references?the space requirements for
the auxiliary data structures are quite modest.
Given sufficient resources, we would ideally par-
allelize the phrase table creation process for each
occurrence of the matched source substring. How-
ever, the typical number of source substring matches
for an input sentence is even larger than the number
of threads available on GPUs, so this strategy does
not make sense due to context switching overhead.
Instead, GPU thread blocks (groups of 512 threads)
are used to process each source substring. This
means that for substrings with large numbers of
matches, one thread in the GPU block would process
multiple occurrences. This strategy is widely used,
and according to GPU programming best practices
from NVIDIA, allocating more work to a single
thread maintains high GPU utilization and reduces
the cost of context switches.
5 Computing Every Feature
Finally, we arrive at line 7 in Algorithm 3, where
we must compute feature values for each extracted
phrase pair. Following the implementation of gram-
mar extraction used in cdec (Lopez, 2008a), we
compute several widely-used features:
1. Pair count feature, c(e, f).
2. The joint probability of all target-to-source
phrase translation probabilities, p(e|f)
= c(e, f)/c(f), where e is target phrase, f is
the source phrase.
3. The logarithm of the target-to-source lexical
weighting feature.
4. The logarithm of the source-to-target lexical
weighting feature.
5. The coherence probability, defined as the ratio
between the number of successful extractions
of a source phrase to the total count of the
source phrase in the suffix array.
The output of our phrase extraction is a large
collection of phrase pairs. To extract the above fea-
tures, aggregate statistics need to be computed over
phrase pairs. To make the solution both compact
and efficient, we first sort the unordered collection
of phrases from the GPU into an array, then the
aggregate statistics can be obtained in a single pass
over the array, since identical phrase pairs are now
grouped together.
6 Experimental Setup
We tested our GPU-based grammar extraction im-
plementation under the conditions in which it would
be used for a Chinese-to-English machine transla-
tion task, in particular, replicating the data condi-
tions of Lopez (2008b). Experiments were per-
formed on two data sets. First, we used the source
(Chinese) side of news articles collected from the
Xinhua Agency, with around 27 million words of
Chinese in around one million sentences (totaling
137 MB). Second, we added source-side parallel text
from the United Nations, with around 81 million
words of Chinese in around four million sentences
(totaling 561 MB). In a pre-processing phase, we
mapped every word to a unique integer, with two
special integers representing end-of-sentence and
end-of-corpus, respectively.
Input query data consisted of all sentences from
the NIST 2002?2006 translation campaigns, tok-
enized and integerized identically to the training
data. On average, sentences contained around 29
words. In order to fully stress our GPU algorithms,
we ran tests on batches of 2,000, 4,000, 6,000,
8,000, and 16,000 sentences. Since there are only
around 8,000 test sentences in the NIST data, we
simply duplicated the test data as necessary.
Our experiments used NVIDIA?s Tesla C2050
GPU (Fermi Generation), which has 448 CUDA
cores with a peak memory bandwidth 144 GB/s.
Note that the GPU was released in early 2010
and represents previous generation technology.
NVIDIA?s current GPUs (Kepler) boasts raw
processing power in the 1.3 TFlops (double
precision) range, which is approximately three
times the GPU we used. Our CPU is a 3.33 GHz
Intel Xeon X5260 processor, which has two cores.
As a baseline, we compared against the publicly
available implementation of the CPU-based algo-
rithms described by Lopez (2008a) found in the
pycdec (Chahuneau et al, 2012) extension of the
cdec machine translation system (Dyer et al, 2010).
Note that we only tested grammar extraction for
continuous pairs of phrases, and we did not test the
slower and more complex queries for hierarchical
330
Input Sentences 2,000 4,000 6,000 8,000 16,000
Number of Words 57,868 117,854 161,883 214,246 428,492
Xinhua
With Sampling (s300)
GPU (words/second)
3811
(21.9)
4723
(20.4)
5496
(32.1)
6391
(29.7)
12405
(36.0)
CPU (words/second) 200 (1.5)
Speedup 19? 24? 27? 32? 62?
No Sampling (s?)
GPU (words/second)
1917
(8.5)
2859
(11.1)
3496
(19.9)
4171
(23.2)
8186
(27.6)
CPU (words/second) 1.13 (0.02)
Speedup 1690? 2520? 3082? 3677? 7217?
Xinhua + UN
With Sampling (s300)
GPU (words/second)
2021
(5.3)
2558
(10.7)
2933
(13.9)
3439
(15.2)
6737
(29.0)
CPU (words/second) 157 (1.8)
Speedup 13? 16? 19? 22? 43?
No Sampling (s?)
GPU (words/second)
500.5
(2.5)
770.1
(3.9)
984.6
(5.8)
1243.8
(5.4)
2472.3
(12.0)
CPU (words/second) 0.23 (0.002)
Speedup 2194? 3375? 4315? 5451? 10836?
Table 2: Comparing the GPU and CPU implementations for phrase extraction on two different corpora. Throughput
is measured in words per second under different test set sizes; the 95% confidence intervals across five trials are given
in parentheses, along with relative speedups comparing the two implementations.
(gappy) patterns described by Lopez (2007). Both
our implementation and the baseline are written
primarily in C/C++.1
Our source corpora and test data are the same
as that presented in Lopez (2008b), and using the
CPU implementation as a reference enabled us to
confirm that our extracted grammars and features
are identical (modulo sampling). We timed our
GPU implementation as follows: from the loading
of query sentences, extractions of substrings and
grammar rules, until all grammars for all sentences
are generated in memory. Timing does not include
offline preparations such as the construction of the
suffix array on source texts and the I/O costs for
writing the per-sentence grammar files to disk. This
timing procedure is exactly the same for the CPU
1The Chahuneau et al (2012) implementation is in Cython,
a language for building Python applications with performance-
critical components in C. In particular, all of the suffix array
code that we instrumented for these experiments are compiled
to C/C++. The implementation is a port of the original code
written by Lopez (2008a) in Pyrex, a precursor to Cython.
Much of the code is unchanged from the original version.
baseline. We are confident that our results represent
a fair comparison between the GPU and CPU, and
are not attributed to misconfigurations or other flaws
in experimental procedures. Note that the CPU
implementation runs in a single thread, on the same
machine that hosts the GPU (described above).
7 Results
Table 2 shows performance results comparing our
GPU implementation against the reference CPU
implementation for phrase extraction. In one ex-
perimental condition, the sampling parameter for
frequently-matching phrases is set to 300, per Lopez
(2008a), denoted s300. The experimental condition
without sampling is denoted s?. Following stan-
dard settings, the maximum length of the source
phrase is set to 5 and the maximum length of the
target phrase is set to 15 (same for both GPU
and CPU implementations). The table is divided
into two sections: the top shows results on the
Xinhua data, and the bottom on Xinhua + UN
data. Columns report results for different numbers
331
# Sent. 2000 4000 6000 8000 16000
Speedup 9.6? 14.3? 17.5? 20.9? 40.9?
Phrases 2.1? 1.8? 1.7? 1.6? 1.6?
Table 3: Comparing no sampling on the GPU with sam-
pling on the CPU in terms of performance improvements
(GPU over CPU) and increases in the number of phrase
pairs extracted (GPU over CPU).
of input sentences. Performance is reported in terms
of throughput: the number of processed words per
second on average (i.e., total time divided by the
batch size in words). The results are averaged over
five trials, with 95% confidence intervals shown in
parentheses. Note that as the batch size increases,
we achieve higher throughput on the GPU since
we are better saturating its full processing power.
In contrast, performance is constant on the CPU
regardless of the number of sentences processed.
The CPU throughput on the Xinhua data is 1.13
words per second without sampling and 200 words
per second with sampling. On 16,000 test sentences,
we have mostly saturated the GPU?s processing
power, and observe a 7217? speedup over the CPU
implementation without sampling and 62? speedup
with sampling. On the larger (Xinhua + UN)
corpus, we observe 43? and 10836? speedup with
sampling and no sampling, respectively.
Interestingly, a run without sampling on the GPU
is still substantially faster than a run with sampling
on the CPU. On the Xinhua corpus, we observe
speedups ranging from nine times to forty times, as
shown in Table 3. Without sampling, we are able to
extract up to twice as many phrases.
In previous CPU implementations of on-the-fly
phrase extraction, restrictions were placed on the
maximum length of the source and target phrases
due to computational constraints (in addition to sam-
pling). Given the massive parallelism afforded by
the GPU, might we be able to lift these restrictions
and construct the complete phrase table? To answer
this question, we performed an experiment without
sampling and without any restrictions on the length
of the extracted phrases. The complete phrase
table contained about 0.5% more distinct pairs, with
negligible impact on performance.
When considering these results, an astute reader
might note that we are comparing performance
of a single-threaded implementation with a fully-
saturated GPU. To address this concern, we
conducted an experiment using a multi-threaded
version of the CPU reference implementation to
take full advantage of multiple cores on the CPU (by
specifying the -j option in cdec); we experimented
with up to four threads to fully saturate the
dual-core CPU. In terms of throughput, the CPU
implementation scales linearly, i.e., running on four
threads achieves roughly 4? throughput. Note that
the CPU and GPU implementations take advantage
of parallelism in completely different ways: cdec
can be characterized as embarrassingly parallel, with
different threads processing each complete sentence
in isolation, whereas our GPU implementation
achieves intra-sentential parallelism by exploiting
many threads to concurrently process each sentence.
In terms of absolute performance figures, even
with the 4? throughput improvement from fully
saturating the CPU, our GPU implementation
remains faster by a wide margin. Note that neither
our GPU nor CPU represents state-of-the-art
hardware, and we would expect the performance
advantage of GPUs to be even greater with latest
generation hardware, since the number of available
threads on a GPU is increasing faster than the
number of threads available on a CPU.
Since phrase extraction is only one part of an
end-to-end machine translation system, it makes
sense to examine the overall performance of the
entire translation pipeline. For this experiment, we
used our GPU implementation for phrase extrac-
tion, serialized the grammar files to disk, and used
cdec for decoding (on the CPU). The comparison
condition used cdec for all three stages. We used
standard phrase length constraints (5 on source side,
15 on target side) with sampling of frequent phrases.
Finally, we replicated the data conditions in Lopez
(2008a), where our source corpora was the Xinhua
data set and our development/test sets were the
NIST03/NIST05 data; the NIST05 test set contains
1,082 sentences.
Performance results for end-to-end translation are
shown in Table 4, broken down in terms of total
amount of time for each of the processing stages
for the entire test set under different conditions.
In the decoding stage, we varied the number of
CPU threads (note here we do not observe linear
332
Phrase Extraction I/O Decoding
GPU: 11.0
3.7
1 thread 55.7
2 threads 35.3
CPU: 166.5
3 threads 31.5
4 threads 26.2
Table 4: End-to-end machine translation performance:
time to process the NIST05 test set in seconds, broken
down in terms of the three processing stages.
speedup). In terms of end-to-end results, complete
translation of the test set takes 41 seconds with the
GPU for phrase extraction and CPU for decoding,
compared to 196 seconds using the CPU for both
(with four decoding threads in both cases). This rep-
resents a speedup of 4.8?, which suggests that even
selective optimizations of individual components in
the MT pipeline using GPUs can make a substantial
difference in overall performance.
8 Future Work
There are a number of directions that we have
identified for future work. For computational ef-
ficiency reasons, previous implementations of the
?translation by pattern matching? approach have
had to introduce approximations, e.g., sampling and
constraints on phrase lengths. Our results show that
the massive amounts of parallelism available in the
GPU make these approximations unnecessary, but
it is unclear to what extent they impact translation
quality. For example, Table 3 shows that we extract
up to twice as many phrase pairs without sampling,
but do these pairs actually matter? We have begun to
examine the impact of various settings on translation
quality and have observed small improvements in
some cases (which, note, come for ?free?), but so
far the results have not been conclusive.
The experiments in this paper focus primarily
on throughput, but for large classes of applications
latency is also important. One current limitation of
our work is that large batch sizes are necessary to
fully utilize the available processing power of the
GPU. This and other properties of the GPU, such as
the high latency involved in transferring data from
main memory to GPU memory, make low-latency
processing a challenge, which we hope to address.
Another broad future direction is to ?GPU-ify?
other machine translation models and other com-
ponents in the machine translation pipeline. An
obvious next step is to extend our work to the
hierarchical phrase-based translation model (Chi-
ang, 2007), which would involve extracting ?gappy?
phrases. Lopez (2008a) has tackled this problem
on the CPU, but it is unclear to what extent the
same types of algorithms he proposed can execute
efficiently in the GPU environment. Beyond phrase
extraction, it might be possible to perform decoding
itself in the GPU?not only will this exploit massive
amounts of parallelism, but also reduce costs in
moving data to and from the GPU memory.
9 Conclusion
GPU parallelism offers many promises for practical
and efficient implementations of language process-
ing systems. This promise has been demonstrated
for speech recognition (Chong et al, 2008; Chong
et al, 2009) and parsing (Yi et al, 2011), and we
have demonstrated here that it extends to machine
translation as well. We believe that explorations of
modern parallel hardware architectures is a fertile
area of research: the field has only begun to exam-
ine the possibilities and there remain many more
interesting questions to tackle. Parallelism is critical
not only from the perspective of building real-world
applications, but for overcoming fundamental com-
putational bottlenecks associated with models that
researchers are developing today.
Acknowledgments
This research was supported in part by the BOLT
program of the Defense Advanced Research Projects
Agency, Contract No. HR0011-12-C-0015; NSF
under award IIS-1144034. Any opinions, findings,
conclusions, or recommendations expressed in this
paper are those of the authors and do not necessarily
reflect views of the sponsors. The second author is
grateful to Esther and Kiri for their loving support
and dedicates this work to Joshua and Jacob. We
would like to thank three anonymous reviewers for
providing helpful suggestions and also acknowledge
Benjamin Van Durme and CLIP labmates for useful
discussions. We also thank UMIACS for providing
hardware resources via the NVIDIA CUDA Center
of Excellence, UMIACS IT staff, especially Joe
Webster, for excellent support.
333
References
R. D. Brown. 2004. A modified Burrows-Wheeler
Transform for highly-scalable example-based transla-
tion. In Proceedings of the 6th Conference of the
Association for Machine Translation in the Americas
(AMTA 2004), pages 27?36.
C. Callison-Burch, C. Bannard, and J. Schroeder. 2005.
Scaling phrase-based statistical machine translation to
larger corpora and longer phrases. In Proceedings
of the 43rd Annual Meeting on Association for
Computational Linguistics (ACL 2005), pages 255?
262.
V. Chahuneau, N. A. Smith, and C. Dyer. 2012. pycdec:
A Python interface to cdec. In Proceedings of the 7th
Machine Translation Marathon (MTM 2012).
D. Chiang. 2007. Hierarchical phrase-based translation.
Computational Linguistics, 33(2):201?228.
J. Chong, Y. Yi, A. Faria, N. R. Satish, and K. Keutzer.
2008. Data-parallel large vocabulary continuous
speech recognition on graphics processors. In Pro-
ceedings of the Workshop on Emerging Applications
and Manycore Architectures.
J. Chong, E. Gonina, Y. Yi, and K. Keutzer. 2009. A fully
data parallel WFST-based large vocabulary continuous
speech recognition on a graphics processing unit.
In Proceedings of the 10th Annual Conference of
the International Speech Communication Association
(INTERSPEECH 2009), pages 1183?1186.
F. Cromieres and S. Kurohashi. 2011. Efficient retrieval
of tree translation examples for syntax-based machine
translation. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Processing,
EMNLP 2011, pages 508?518.
C. Dyer, A. Lopez, J. Ganitkevitch, J. Weese, F. Ture,
P. Blunsom, H. Setiawan, V. Eidelman, and P. Resnik.
2010. cdec: A decoder, alignment, and learning
framework for finite-state and context-free translation
models. In Proceedings of the ACL 2010 System
Demonstrations, pages 7?12.
A. Gharaibeh and M. Ripeanu. 2010. Size matters:
Space/time tradeoffs to improve GPGPU applications
performance. In Proceedings of the 2010 ACM/IEEE
International Conference for High Performance Com-
puting, Networking, Storage and Analysis (SC 2010),
pages 1?12.
A. Lopez. 2007. Hierarchical phrase-based translation
with suffix arrays. In Proceedings of the 2007
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning, pages 976?985.
A. Lopez. 2008a. Machine translation by pattern
matching. Ph.D. dissertation, University of Maryland,
College Park, Maryland, USA.
A. Lopez. 2008b. Tera-scale translation models via
pattern matching. In Proceedings of the 22nd In-
ternational Conference on Computational Linguistics
(COLING 2008), pages 505?512.
U. Manber and G. Myers. 1990. Suffix arrays: a new
method for on-line string searches. In Proceedings of
the First Annual ACM-SIAM Symposium on Discrete
Algorithms (SODA ?90), pages 319?327.
F. J. Och, C. Tillmann, and H. Ney. 1999. Improved
alignment models for statistical machine translation.
In Proceedings of the 1999 Joint SIGDAT Conference
on Empirical Methods in Natural Language Process-
ing and Very Large Corpora, pages 20?28.
M. Schatz, C. Trapnell, A. Delcher, and A. Varshney.
2007. High-throughput sequence alignment using
graphics processing units. BMC Bioinformatics,
8(1):474.
C. Trapnell and M. C. Schatz. 2009. Optimizing data
intensive GPGPU computations for DNA sequence
alignment. Parallel Computing, 35(8-9):429?440.
Y. Yi, C.-Y. Lai, S. Petrov, and K. Keutzer. 2011.
Efficient parallel CKY parsing on GPUs. In
Proceedings of the 12th International Conference on
Parsing Technologies, pages 175?185.
Y. Zhang and S. Vogel. 2005. An efficient phrase-to-
phrase alignment model for arbitrarily long phrase and
large corpora. In Proceedings of the Tenth Conference
of the European Association for Machine Translation
(EAMT-05).
334
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 199?204,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Mr. MIRA: Open-Source Large-Margin Structured Learning on
MapReduce
Vladimir Eidelman1, Ke Wu1, Ferhan Ture1, Philip Resnik2, Jimmy Lin3
1 Dept. of Computer Science 2 Dept. of Linguistics 3 The iSchool
Institute for Advanced Computer Studies
University of Maryland
{eidelman,wuke,fture,resnik,jimmylin}@umd.edu
Abstract
We present an open-source framework
for large-scale online structured learning.
Developed with the flexibility to handle
cost-augmented inference problems such
as statistical machine translation (SMT),
our large-margin learner can be used with
any decoder. Integration with MapReduce
using Hadoop streaming allows efficient
scaling with increasing size of training
data. Although designed with a focus
on SMT, the decoder-agnostic design of
our learner allows easy future extension to
other structured learning problems such as
sequence labeling and parsing.
1 Introduction
Structured learning problems such as sequence la-
beling or parsing, where the output has a rich in-
ternal structure, commonly arise in NLP. While
batch learning algorithms adapted for structured
learning such as CRFs (Lafferty et al, 2001)
and structural SVMs (Joachims, 1998) have re-
ceived much attention, online methods such as
the structured perceptron (Collins, 2002) and a
family of Passive-Aggressive algorithms (Cram-
mer et al, 2006) have recently gained promi-
nence across many tasks, including part-of-speech
tagging (Shen, 2007), parsing (McDonald et
al., 2005) and statistical machine translation
(SMT) (Chiang, 2012), due to their ability to deal
with large training sets and high-dimensional in-
put representations.
Unlike batch learners, which must consider all
examples when optimizing the objective, online
learners operate in rounds, optimizing using one
example or a handful of examples at a time. This
online nature offers several attractive properties,
facilitating scaling to large training sets while re-
maining simple and offering fast convergence.
Mr. MIRA, the open source system1 de-
scribed in this paper, implements an online large-
margin structured learning algorithm based on
MIRA (?2.1), for cost-augmented online large-
scale training in high-dimensional feature spaces.
Our contribution lies in providing the first pub-
lished decoder-agnostic parallelization of MIRA
with Hadoop for structured learning.
While the current demonstrated application fo-
cuses on large-scale discriminative training for
machine translation, the learning algorithm is gen-
eral with respect to the inference algorithm em-
ployed. We are able to decouple our learner en-
tirely from the MT decoder, allowing users to
specify their own inference procedure through a
simple text communication protocol (?2.2). The
learner only requires k-best output with feature
vectors, as well as the specification of a cost func-
tion. Standard automatic evaluation metrics for
MT, such as BLEU (Papineni et al, 2002) and TER
(Snover et al, 2006), have already been imple-
mented. Furthermore, our system can be extended
to other structured learning problems with a min-
imal amount of effort, simply by implementing a
task-specific cost function and specifying an ap-
propriate decoder.
Through Hadoop streaming, our system can
take advantage of commodity clusters to handle
large-scale training (?3), while also being capable
of running in environments ranging from a single
machine to a PBS-managed batch cluster. Experi-
mental results (?4) show that it scales linearly and
makes fast parameter tuning on large tuning sets
for SMT practical.
2 Learning and Inference
2.1 Online Large-Margin Learning
MIRA is a popular online large-margin structured
learning method for NLP tasks (McDonald et al,
2005; Chiang et al, 2009; Chiang, 2012). The
1https://github.com/kho/mr-mira
199
main intuition is that we want our model to enforce
a margin between the correct and incorrect out-
puts of a sentence that agrees with our cost func-
tion. This is done by making the smallest update
we can to our parameters, w, on every sentence,
that will ensure that the difference in model scores
?fi(y?) = w>(f(xi, y+) ? f(xi, y?)) between the
correct output y+ and incorrect output y? is at least
as large as the cost, ?i(y?), incurred by predicting
the incorrect output:2
wt+1 = arg minw
1
2 ||w ?wt||
2 + C?i
s.t. ?y? ? Y(xi), ?fi(y?) ? ?i(y?)? ?i
where Y(xi) is the space of possible structured
outputs we are able to produce from xi, and
C is a regularization parameter that controls the
size of the update. In practice, we can de-
fine Y(xi) to be the k-best output. With a
passive-aggressive (PA) update, the ?y? constraint
above can be approximated by selecting the sin-
gle most violated constraint, which maximizes
y? ? arg maxy?Y(xi) w>f(xi, y) + ?i(y). This
optimization problem is attractive because it re-
duces to a simple analytical solution, essentially
performing a subgradient descent step with the
step size adjusted based on each example:
?? min
(
C, ?i(y
?)? ?fi(y?)
?f(xi, y+)? f(xi, y?)?2
)
w? w + ??
(
f(xi, y+)? f(xi, y?)
)
The user-defined cost function is a task-specific
external measure of quality that relays how bad se-
lecting y? truly is on the task we care about. The
cost can take any form as long as it decomposes
across the local parts of the structure, just as the
feature functions. For instance, it could be the
Hamming loss for sequence labeling, F-score for
parsing, or an approximate BLEU score for SMT.
Cost-augmented Inference For most struc-
tured prediction problems in machine learning,
yi ? Y(xi), that is, the model is able to produce,
and thus score, the correct output structure, mean-
ing y+ = yi. However, for certain NLP prob-
lems this may not be the case. For instance in
SMT, our model may not be able to produce or
reach the correct reference translation, which pro-
hibits our model from scoring it. This problem
2For a more formal description we refer the reader
to (Crammer et al, 2006; Chiang, 2012).
necessitates cost-augmented inference, where we
select y+ ? arg maxy?Y(xi) w>f(xi, y)??i(y)
from the space of structures our model can pro-
duce, to stand in for the correct output in optimiza-
tion. Our system was developed to handle both
cases, with the decoder providing the k-best list
to the learner, specifying whether to perform cost-
augmented selection.
Sparse Features While utilizing sparse features
is a primary motivation for performing large-scale
discriminative training, which features to use and
how to learn their weights can have a large im-
pact on the potential benefit. To this end, we in-
corporate `1/`2 regularization for joint feature se-
lection in order to improve efficiency and counter
overfitting effects (Simianer et al, 2012). Further-
more, the PA update has a single learning rate ?
for all features, which specifies how much the fea-
ture weights can change at each update. How-
ever, since dense features (e.g., language model)
are observed far more frequently than sparse fea-
tures (e.g., rule id), we may instead want to use
a per-feature learning rate that allows larger steps
for features that do not have much support. Thus,
we allow setting an adaptive per-feature learning
rate (Green et al, 2013; Crammer et al, 2009;
Duchi et al, 2011).
2.2 Learner/Decoder Communication
Training requires communication between the de-
coder and the learner. The decoder needs to re-
ceive weight updates and the input sentence from
the learner; and the learner needs to receive k-best
output with feature vectors from the decoder. This
is essentially all the required communication be-
tween the learner and the decoder. Below, we de-
scribe a simple line-based text protocol.
Input sentence and weight updates Follow-
ing common practice in machine translation, the
learner encodes each input sentence as a single-
line SGML entry named seg and sends it to the
decoder. The first line of Figure 1 is an exam-
ple sentence in this format. In addition to the
required sentence ID (useful in parallel process-
ing), an optional delta field is used to encode
the weight updates, as a sparse vector indexed
by feature names. First, for each name and up-
date pair, a binary record consisting of a null-
terminated string (name) and a double-precision
floating point number in native byte order (up-
date) is created. Then, all binary records are con-
200
<seg id="123" delta="TE0AexSuR+F6hD8="> das ist ein kleine haus </seg>
<seg id="124"> ein kleine haus </seg>\tein kleine ||| a small\thaus ||| house
Figure 1: Example decoder input in SGML
5
123 ||| 5 ||| this is a small house ||| TE0AAAAA... <base64> ||| 120.3
123 ||| 5 ||| this is the small house ||| <base64> ||| 118.4
123 ||| 5 ||| this was small house ||| <base64> ||| 110.5
<empty>
<empty>
Figure 2: Example k-best output
catenated and encoded in base64. In the example
above, the value of delta is the base64 encod-
ing of 0x4c 0x4d 0x00 0x7b 0x14 0xae 0x47
0xe1 0x7a 0x84 0x3f. The first 3 bytes store the
feature name (LM) and the next 8 bytes is its update
(0.01), to be added to the decoder?s current value
of the corresponding feature weight.
The learner also allows the user to pass any ad-
ditional information to the decoder, as long as it
can be encoded as a single-line text string. Such
information, if given, is appended after the seg en-
try, with a leading tab character as the delimiter.
For example, the second line of Figure 1 passes
two phrase translation rules to the decoder.
k-best output The decoder reads from standard
input and outputs the k-best output for one input
sentence before consuming the next line. For the
k-best output, the decoder first outputs to standard
output a line consisting of a single integerN . Next
the decoder outputs N lines where each line can
be either empty or an actual hypothesis. When the
line is an actual hypothesis, it consists of the fol-
lowing parts:
SID ||| LEN ||| TOK ||| FEAT [ REST ]
SID is the sentence ID of the corresponding input;
LEN is the length of source sentence;3 TOK contains
the tokens of the hypothesis sentence separated by
spaces; FEAT is the feature vector, encoded in the
same way as the weight updates, delimited by a
whitespace. Everything after FEAT until the end of
the line is discarded. See Figure 2 for an example
of k-best output. Note the scores after the last |||
are discarded by the learner.
Overall workflow The learner reads lines from
standard input in the following tab-delimited for-
mat:
3This is used in computing the smoothed cost. Usually
this is identical for all hypotheses if the input is a plain sen-
tence. But in applications such as lattice-based translation,
each hypothesis can be produced from different source sen-
tences, resulting in different lengths.
SRC<tab>REF<tab>REST
SRC is the actual input sentence as a seg entry; REF
is the gold output for the input sentence, for ex-
ample, reference translations in MT;4 REST is the
additional information that will be appended after
the seg entry and passed to the decoder.
The learner creates a sub-process for the de-
coder and connects to the sub-process? standard
input and output with pipes. Then it processes the
input lines one by one. For each line, it first sends
a composed input message to the decoder, combin-
ing the input sentence, weight updates, and user-
supplied information. Next it collects the k-best
output from the decoder, solves the QP problem to
obtain weight updates and repeats.
The learner produces two types of output. First,
the 1-best hypothesis for each input sentence, in
the following format:
SID<tab>TOK
Second, when there are no more input lines, the
learner outputs final weights and the number of
lines processed, in the following format:
-1<tab>NUM ||| WEIGHTS
The 1-best hypotheses can be scored against ref-
erences to obtain an estimate of cost. The final
weights are stored in a way convenient for averag-
ing in a parallel setting, as we shall discuss next.
3 Large-Scale Discriminative Training
3.1 MapReduce
With large amounts of data available today,
distributed computations have become essen-
tial. MapReduce (Dean and Ghemawat, 2004)
has emerged as a popular distributed process-
ing framework for commodity clusters that has
gained widespread adoption in both industry and
academia, thanks to its simplicity and the avail-
ability of the Hadoop open-source implementa-
tion. MapReduce provides a higher level of
4There can be multiple references, separated by |||.
201
abstraction for designing distributed algorithms
compared to, say, MPI or pthreads, by hiding
system-level details (e.g., deadlock, race condi-
tions, machine failures) from the developer.
A single MapReduce program begins with a
map phase, where mapper processes input key-
value pairs to produce an arbitrary number of in-
termediate key-value pairs. The mappers execute
in parallel, consuming data splits independently.
Following the map phase, all key-value pairs emit-
ted by the mappers are sorted by key and dis-
tributed to the reducers, such that all pairs shar-
ing the same key are guaranteed to arrive at the
same reducer. Finally, in the reduce phase, each
reducer processes the intermediate key-value pairs
it receives and emits final output key-value pairs.
3.2 System Architecture
Algorithm design We use Hadoop streaming to
parallelize the training process. Hadoop stream-
ing allows any arbitrary executable to serve as the
mapper or reducer, as long as it handles key-value
pairs properly.5 One iteration of training is im-
plemented as a single Hadoop streaming job. In
the map step, our learner can be directly used as
the mapper. Each mapper loads the same initial
weights, processes a single split of data and pro-
duces key-value pairs: the one-best hypothesis of
each sentence is output with the sentence ID as
the key (non-negative); the final weights with re-
spect to the split are output with a special negative
key. In the reduce step, a single reducer collects all
key-value pairs, grouped and sorted by keys. The
one-best hypotheses are output to disk in the or-
der they are received, so that the order matches the
reference translation set. The reducer also com-
putes the feature selection and weighted average
of final weights received from all of the mappers.
Assuming mapper i produces the final weights wi
after processing ni sentences, the weighted aver-
aged is defined as w? =
?
iwi?ni?
i ni
. Although aver-
aging yields different result from running a single
learner over the entire data, we have found the dif-
ference to be quite small in terms of convergence
and quality of tuned weights in practice.
After the reducer finishes, the averaged weights
are extracted and used as the initial weights for the
next iteration; the emitted hypotheses are scored
5By default, each line is treated as a key-value pair en-
coded in text, where the key and the value are separated by a
<tab>.
against the references, which allows us to track the
learning curve and the progress of convergence.
Scalability In an application such as SMT, the
decoder requires access to the translation gram-
mar and language model to produce translation hy-
potheses. For small tuning sets, which have been
typical in MT research, having these files trans-
ferred across the network to individual servers
(which then load the data into memory) is not
a problem. However, for even modest input on
the order of tens of thousands of sentences, this
creates a challenge. For example, distributing
thousands of per-sentence grammar files to all the
workers in a Hadoop cluster is time-consuming,
especially when this needs to be performed prior
to every iteration.
To benefit from MapReduce, it is essential to
avoid dependencies on ?side data? as much as
possible, due to the challenges explained above
with data transfer. To address this issue, we ap-
pend the per-sentence translation grammar as user-
supplied additional information to each input sen-
tence. This results in a large input file (e.g., 75 gi-
gabytes for 50,000 sentences), but this is not an is-
sue since the data reside on the Hadoop distributed
file system and MapReduce optimizes for data lo-
cality when scheduling mappers.
Unfortunately, it is much more difficult to ob-
tain per-sentence language models that are small
enough to handle in this same manner. Currently,
the best solution we have found is to use Hadoop?s
distributed cache to ship the single large language
model to each worker.
4 Evaluation
We evaluated online learning in Hadoop Map-
Reduce by applying it to German-English ma-
chine translation, using our hierarchical phrase-
based translation system with cdec as the de-
coder (Dyer et al, 2010). The parallel training
data consist of the Europarl and News Commen-
tary corpora from the WMT12 translation task,6
containing 2.08M sentences. A 5-gram language
model was trained on the English side of the bi-
text along with 750M words of news using SRILM
with modified Kneser-Ney smoothing (Chen and
Goodman, 1996).
We experimented with two feature sets: (1) a
small set with standard MT features, including
6http://www.statmt.org/wmt12/translation-task.html
202
Tuning set size Time/iteration # splits # features Tuning BLEU Test
(corpus) (on disk, GB) (in seconds) BLEU TER
dev 3.3 119 120 16 22.38 22.69 60.61
5k 7.8 289 120 16 32.60 22.14 59.60
10k 15.2 432 120 16 33.16 22.06 59.43
25k 37.2 942 300 16 32.48 22.21 59.54
50k 74.5 1802 600 16 32.21 22.21 59.39
dev 3.3 232 120 85k 23.08 23.00 60.19
5k 7.8 610 120 159k 33.70 22.26 59.26
10k 15.2 1136 120 200k 34.00 22.12 59.24
25k 37.2 2395 300 200k 32.96 22.35 59.29
50k 74.5 4465 600 200k 32.86 22.40 59.15
Table 1: Evaluation of our Hadoop implementation of MIRA, showing running time as well as BLEU
and TER values for tuning and testing data.
dev test 5k 10k 25k 50k
Sentences 3003 3003 5000 10000 25000 50000
Tokens en 75k 74k 132k 255k 634k 1258k
Tokens de 74k 73k 133k 256k 639k 1272k
Table 2: Corpus statistics
phrase and lexical translation probabilities in both
directions, word and arity penalties, and language
model scores; and (2) a large set containing the top
200k sparse features that might be useful to train
on large numbers of instances: rule id and shape,
target bigrams, insertions and deletions, and struc-
tural distortion features.
All experiments were conducted on a Hadoop
cluster (running Cloudera?s distribution, CDH
4.2.1) with 16 nodes, each with two quad-core 2.2
GHz Intel Nehalem Processors, 24 GB RAM, and
three 2 TB drives. In total, the cluster is configured
for a capacity of 128 parallel workers, although
we do not have direct control over the number
of simultaneous mappers, which depends on the
number of input splits. If the number of splits is
smaller than 128, then the cluster is under-utilized.
To note this, we report the number of splits for
each setting in our experimental results (Table 1).
We ran MIRA on a number of tuning sets, de-
scribed in Table 2, in order to test the effective-
ness and scalability of our system. First, we used
the standard development set from WMT12, con-
sisting of 3,003 sentences from news domain. In
order to show the scaling characteristics of our ap-
proach, we then used larger portions of the train-
ing bitext directly to tune parameters. In order to
avoid overfitting, we used a jackknifing method
to split the training data into n = 10 folds, and
built a translation system on n ? 1 folds, while
adjusting the sampling rate to sample sentences
from the other fold to obtain tuning sets ranging
from 5,000 sentences to 50,000 sentences. Table 1
shows details of experimental results for each set-
ting. The second column shows the space each
tuning set takes up on disk when we include refer-
ence translations and grammar files along with the
sentences. The reported tuning BLEU is from the
iteration with best performance, and running times
are reported from the top-scoring iteration as well.
Even though our focus in this evaluation is to
show the scalability of our implementation to large
input and feature sets, it is also worthwhile to men-
tion the effectiveness aspect. As we increase the
tuning set size by sampling sentences from the
training data, we see very little improvement in
BLEU and TER with the smaller feature set. This
is not surprising, since sparse features are more
likely to gain from additional tuning instances. In-
deed, tuning scores for all sets improve substan-
tially with sparse features, accompanied by small
increases on test.
While tuning on dev data results in better BLEU
on test data than when tuning on the larger sets, it
is important to note that although we are able to
tune more features on the larger bitext tuning sets,
they are not composed of the same genre as the
dev and test sets, resulting in a domain mismatch.
203
Therefore, we are actually comparing a smaller in-
domain tuning set with a larger out-of-domain set.
While this domain adaptation is problematic (Had-
dow and Koehn, 2012), the ability to discrimina-
tively tune on larger sets remains highly desirable.
In terms of running time, we observe that the al-
gorithm scales linearly with respect to input size,
regardless of the feature set. With more features,
running time increases due to a more complex
translation model, as well as larger intermediate
output (i.e., amount of information passed from
mappers to reducers). The scaling characteristics
point out the strength of our system: our scalable
MIRA implementation allows one to tackle learn-
ing problems where there are many parameters,
but also many training instances.
Comparing the wall clock time of paralleliza-
tion with Hadoop to the standard mode of 10?20
learner parallelization (Haddow et al, 2011; Chi-
ang et al, 2009), for the small 25k feature set-
ting, after one iteration, which takes 4625 sec-
onds using 15 learners on our PBS cluster, the tun-
ing score is 19.5 BLEU, while in approximately
the same time, we can perform five iterations
with Hadoop and obtain 30.98 BLEU. While this
is not a completely fair comparison, as the two
clusters utilize different resources and the num-
ber of learners, it suggests the practical benefits
that Hadoop can provide. Although increasing the
number of learners on our PBS cluster to the num-
ber of mappers used in Hadoop would result in
roughly equivalent performance, arbitrarily scal-
ing out learners on the PBS cluster to handle larger
training sets can be challenging since we?d have to
manually coordinate the parallel processes in an
ad-hoc manner. In contrast, Hadoop provides scal-
able parallelization in a manageable framework,
providing data distribution, synchronization, fault
tolerance, as well as other features, ?for free?.
5 Conclusion
In this paper, we presented an open-source
framework that allows seamlessly scaling struc-
tured learning to large feature-rich problems with
Hadoop, which lets us take advantage of large
amounts of data as well as sparse features. The
development of Mr. MIRA has been motivated pri-
marily by application to SMT, but we are planning
to extend our system to other structured prediction
tasks in NLP such as parsing, as well as to facili-
tate its use in other domains.
Acknowledgments
This research was supported in part by the DARPA
BOLT program, Contract No. HR0011-12-C-
0015; NSF under awards IIS-0916043 and IIS-
1144034. Vladimir Eidelman is supported by a
NDSEG Fellowship. Any opinions, findings, con-
clusions, or recommendations expressed are those
of the authors and do not necessarily reflect views
of the sponsors.
References
S. Chen and J. Goodman. 1996. An empirical study of
smoothing techniques for language modeling. In ACL.
D. Chiang, K. Knight, and W. Wang. 2009. 11,001 new fea-
tures for statistical machine translation. In NAACL-HLT.
D. Chiang. 2012. Hope and fear for discriminative training
of statistical translation models. JMLR, 13:1159?1187.
M. Collins. 2002. Ranking algorithms for named-entity ex-
traction: boosting and the voted perceptron. In ACL.
K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz, and
Y. Singer. 2006. Online passive-aggressive algorithms.
JMLR, 7:551?585.
K. Crammer, A. Kulesza, and M. Dredze. 2009. Adaptive
regularization of weight vectors. In NIPS.
J. Dean and S. Ghemawat. 2004. MapReduce: Simplified
data processing on large clusters. In OSDI.
J. Duchi, E. Hazan, and Y. Singer. 2011. Adaptive subgra-
dient methods for online learning and stochastic optimiza-
tion. JMLR, 12:2121?2159.
C. Dyer, A. Lopez, J. Ganitkevitch, J. Weese, F. Ture, P. Blun-
som, H. Setiawan, V. Eidelman, and P. Resnik. 2010.
cdec: A decoder, alignment, and learning framework for
finite-state and context-free translation models. In ACL
System Demonstrations.
S. Green, S. Wang, D. Cer, and C. Manning. 2013. Fast and
adaptive online training of feature-rich translation models.
In ACL.
B. Haddow and P. Koehn. 2012. Analysing the effect of out-
of-domain data on smt systems. In WMT.
B. Haddow, A. Arun, and P. Koehn. 2011. SampleRank
training for phrase-based machine translation. In WMT.
T. Joachims. 1998. Text categorization with support vec-
tor machines: Learning with many relevant features. In
ECML.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional
random fields: Probabilistic models for segmenting and
labeling sequence data. In ICML.
R. McDonald, K. Crammer, and F. Pereira. 2005. Online
large-margin training of dependency parsers. In ACL.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. In ACL.
L. Shen. 2007. Guided learning for bidirectional sequence
classification. In ACL.
P. Simianer, S. Riezler, and C. Dyer. 2012. Joint feature
selection in distributed stochastic learning for large-scale
discriminative training in SMT. In ACL.
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A study of translation edit rate with
targeted human annotation. In AMTA.
204
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 128?133,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
Towards Efficient Large-Scale Feature-Rich Statistical Machine
Translation
Vladimir Eidelman1, Ke Wu1, Ferhan Ture1, Philip Resnik2, Jimmy Lin3
1 Dept. of Computer Science 2 Dept. of Linguistics 3 The iSchool
Institute for Advanced Computer Studies
University of Maryland
{vlad,wuke,fture,resnik,jimmylin}@umiacs.umd.edu
Abstract
We present the system we developed to
provide efficient large-scale feature-rich
discriminative training for machine trans-
lation. We describe how we integrate with
MapReduce using Hadoop streaming to
allow arbitrarily scaling the tuning set and
utilizing a sparse feature set. We report our
findings on German-English and Russian-
English translation, and discuss benefits,
as well as obstacles, to tuning on larger
development sets drawn from the parallel
training data.
1 Introduction
The adoption of discriminative learning methods
for SMT that scale easily to handle sparse and lex-
icalized features has been increasing in the last
several years (Chiang, 2012; Hopkins and May,
2011). However, relatively few systems take full
advantage of the opportunity. With some excep-
tions (Simianer et al, 2012), most still rely on
tuning a handful of common dense features, along
with at most a few thousand others, on a relatively
small development set (Cherry and Foster, 2012;
Chiang et al, 2009). While more features tuned
on more data usually results in better performance
for other NLP tasks, this has not necessarily been
the case for SMT.
Thus, our main focus in this paper is to improve
understanding into the effective use of sparse fea-
tures, and understand the benefits and shortcom-
ings of large-scale discriminative training. To
this end, we conducted experiments for the shared
translation task of the 2013 Workshop on Statis-
tical Machine Translation for the German-English
and Russian-English language pairs.
2 Baseline system
We use a hierarchical phrase-based decoder im-
plemented in the open source translation system
cdec1 (Dyer et al, 2010). For tuning, we use
Mr. MIRA2 (Eidelman et al, 2013), an open
source decoder agnostic implementation of online
large-margin learning in Hadoop MapReduce. Mr.
MIRA separates learning from the decoder, allow-
ing the flexibility to specify the desired inference
procedure through a simple text communication
protocol. The decoder receives input sentences
and weight updates from the learner, while the
learner receives k-best output with feature vectors
from the decoder.
Hadoop MapReduce (Dean and Ghemawat,
2004) is a popular distributed processing frame-
work that has gained widespread adoption, with
the advantage of providing scalable parallelization
in a manageable framework, taking care of data
distribution, synchronization, fault tolerance, as
well as other features. Thus, while we could oth-
erwise achieve the same level of parallelization, it
would be in a more ad-hoc manner.
The advantage of online methods lies in their
ability to deal with large training sets and high-
dimensional input representations while remain-
ing simple and offering fast convergence. With
Hadoop streaming, our system can take advantage
of commodity clusters to handle parallel large-
scale training while also being capable of running
on a single machine or PBS-managed batch clus-
ter.
System design To efficiently encode the infor-
mation that the learner and decoder require (source
sentence, reference translation, grammar rules) in
a manner amenable to MapReduce, i.e. avoiding
dependencies on ?side data? and large transfers
across the network, we append the reference and
1http://cdec-decoder.org
2https://github.com/kho/mr-mira
128
per-sentence grammar to each input source sen-
tence. Although this file?s size is substantial, it is
not a problem since after the initial transfer, it re-
sides on Hadoop distributed file system, and Map-
Reduce optimizes for data locality when schedul-
ing mappers.
A single iteration of training is performed as
a Hadoop streaming job. Each begins with a
map phase, with every parallel mapper loading the
same initial weights and decoding and updating
parameters on a shard of the data. This is followed
by a reduce phase, with a single reducer collect-
ing final weights from all mappers and computing
a weighted average to distribute as initial weights
for the next iteration.
Parameter Settings We tune our system toward
approximate sentence-level BLEU (Papineni et al,
2002),3 and the decoder is configured to use cube
pruning (Huang and Chiang, 2007) with a limit
of 200 candidates at each node. For optimiza-
tion, we use a learning rate of ?=1, regularization
strength of C=0.01, and a 500-best list for hope
and fear selection (Chiang, 2012) with a single
passive-aggressive update for each sentence (Ei-
delman, 2012).
Baseline Features We used a set of 16 stan-
dard baseline features: rule translation relative
frequency P (e|f), lexical translation probabilities
Plex(e|f) and Plex(f |e), target n-gram language
model P (e), penalties for source and target words,
passing an untranslated source word to the tar-
get side, singleton rule and source side, as well
as counts for arity-0,1, or 2 SCFG rules, the total
number of rules used, and the number of times the
glue rule is used.
2.1 Data preparation
For both languages, we used the provided Eu-
roparl and News Commentary parallel training
data to create the translation grammar neces-
sary for our model. For Russian, we addi-
tionally used the Common Crawl and Yandex
data. The data were lowercased and tokenized,
then filtered for length and aligned using the
GIZA++ implementation of IBM Model 4 (Och
and Ney, 2003) to obtain one-to-many align-
ments in both directions and symmetrized sing the
grow-diag-final-and method (Koehn et al, 2003).
3We approximate corpus BLEU by scoring sentences us-
ing a pseudo-document of previous 1-best translations (Chi-
ang et al, 2009).
We constructed a 5-gram language model us-
ing SRILM (Stolcke, 2002) from the provided
English monolingual training data and parallel
data with modified Kneser-Ney smoothing (Chen
and Goodman, 1996), which was binarized using
KenLM (Heafield, 2011). The sentence-specific
translation grammars were extracted using a suffix
array rule extractor (Lopez, 2007).
For German, we used the 3,003 sentences in
newstest2011 as our Dev set, and report results
on the 3,003 sentences of the newstest2012 Test
set using BLEU and TER (Snover et al, 2006).
For Russian, we took the first 2,000 sentences of
newstest2012 for Dev, and report results on the re-
maining 1,003. For both languages, we selected
1,000 sentences from the bitext to be used as an
additional testing set (Test2).
Compound segmentation lattices As German
is a morphologically rich language with produc-
tive compounding, we use word segmentation lat-
tices as input for the German translation task.
These lattices encode alternative segmentations of
compound words, allowing the decoder to auto-
matically choose which segmentation is best. We
use a maximum entropy model with recommended
settings to create lattices for the dev and test sets,
as well as for obtaining the 1-best segmentation of
the training data (Dyer, 2009).
3 Evaluation
This section describes the experiments we con-
ducted in moving towards a better understanding
of the benefits and challenges posed by large-scale
high-dimensional discriminative tuning.
3.1 Sparse Features
The ability to incorporate sparse features is the pri-
mary reason for the recent move away from Min-
imum Error Rate Training (Och, 2003), as well as
for performing large-scale discriminative training.
We include the following sparse Boolean feature
templates in our system in addition to the afore-
mentioned baseline features: rule identity (for ev-
ery unique rule in the grammar), rule shape (map-
ping rules to sequences of terminals and nontermi-
nals), target bigrams, lexical insertions and dele-
tions (for the top 150 unaligned words from the
training data), context-dependent word pairs (for
the top 300 word pairs in the training data), and
structural distortion (Chiang et al, 2008).
129
Dev Test Test2 5k 10k 25k 50k
en 75k 74k 27k 132k 255k 634k 1258k
de 74k 73k 26k 133k 256k 639k 1272k
Table 1: Corpus statistics in tokens for German.
Dev Test Test2 15k
ru 46k 24k 24k 350k
en 50k 27k 25k 371k
Table 2: Corpus statistics in tokens for
Russian.
Set # features Tune Test
?BLEU ?BLEU ?TER
de-en 16 22.38 22.69 60.61
+sparse 108k 23.86 23.01 59.89
ru-en 16 30.18 29.89 49.05
+sparse 77k 32.40 30.81 48.40
Table 3: Results with the addition of sparse fea-
tures for German and Russian.
All of these features are generated from the
translation rules on the fly, and thus do not have
to be stored as part of the grammar. To allow for
memory efficiency while scaling the training data,
we hash all the lexical features from their string
representation into a 64-bit integer.
Altogether, these templates result in millions of
potential features, thus how to select appropriate
features, and how to properly learn their weights
can have a large impact on the potential benefit.
3.2 Adaptive Learning Rate
The passive-aggressive update used in MIRA has a
single learning rate ? for all features, which along
with ? limits the amount each feature weight can
change at each update. However, since the typical
dense features (e.g., language model) are observed
far more frequently than sparse features (e.g., rule
identity), it has been shown to be advantageous
to use an adaptive per-feature learning rate that
allows larger steps for features that do not have
much support (Green et al, 2013; Duchi et al,
2011). Essentially, instead of having a single pa-
rameter ?,
?? min
(
C, cost(y
?)?w>(f(y+)? f(y?))
?f(y+)? f(y?)?2
)
w? w + ??
(
f(y+)? f(y?)
)
we instead have a vector ? with one entry for each
feature weight:
??1 ? ??1 + ?diag
(
ww>
)
w? w + ??1/2
(
f(y+)? f(y?)
)
?=1 
?=0.01 
?=0.1 
22.2 
22.4 
22.6 
22.8 
23 
23.2 
23.4 
23.6 
23.8 
24 
BLE
U 
Iteration 
Figure 1: Learning curves for tuning when using
a single step size (?) versus different per-feature
learning rates.
In practice, this update is very similar to that of
AROW (Crammer et al, 2009; Chiang, 2012).
Figure 1 shows learning curves for sparse mod-
els with a single learning rate, and adaptive learn-
ing with ?=0.01 and ?=0.1, with associated re-
sults on Test in Table 4.4 As can be seen, using
a single ? produces almost no gain on Dev. How-
ever, while both settings using an adaptive rate fare
better, the proper setting of ? is important. With
?=0.01 we observe 0.5 BLEU gain over ?=0.1 in
tuning, which translates to a small gain on Test.
Henceforth, we use an adaptive learning rate with
?=0.01 for all experiments.
Table 3 presents baseline results for both lan-
guages. With the addition of sparse features, tun-
ing scores increase by 1.5 BLEU for German, lead-
ing to a 0.3 BLEU increase on Test, and 2.2 BLEU
for Russian, with 1 BLEU increase on Test. The
majority of active features for both languages are
rule id (74%), followed by target bigrams (14%)
and context-dependent word pairs (11%).
3.3 Feature Selection
As the tuning set size increases, so do the num-
ber of active features. This may cause practi-
cal problems, such as reduced speed of computa-
tion and memory issues. Furthermore, while some
4All sparse models are initialized with the same tuned
baseline weights. Learning rates are local to each mapper.
130
Adaptive # feat. Tune Test
?BLEU ?BLEU ?TER
none 74k 22.75 22.87 60.19
?=0.01 108k 23.86 23.01 59.89
?=0.1 62k 23.32 22.92 60.09
Table 4: Results with different ? settings for using a per-feature learning rate with sparse features.
Set # feat. Tune Test
?BLEU ?BLEU ?TER
all 510k 32.99 22.36 59.26
top 200k 200k 32.96 22.35 59.29
all 373k 34.26 28.84 49.29
top 200k 200k 34.45 28.98 49.30
Table 5: Comparison of using all features versus
top k selection.
sparse features will generalize well, others may
not, thereby incurring practical costs with no per-
formance benefit. Simianer et al (2012) recently
explored `1/`2 regularization for joint feature se-
lection for SMT in order to improve efficiency and
counter overfitting effects. When performing par-
allel learning, this allows for selecting a reduced
set of the top k features at each iteration that are
effective across all learners.
Table 5 compares selecting the top 200k fea-
tures versus no selection for a larger German and
Russian tuning set (?3.4). As can be seen, we
achieve the same performance with the top 200k
features as we do when using double that amount,
while the latter becomes increasing cumbersome
to manage. Therefore, we use a top 200k selection
for the remainder of this work.
3.4 Large-Scale Training
In the previous section, we saw that learning
sparse features on the small development set leads
to substantial gains in performance. Next, we
wanted to evaluate if we can obtain further gains
by scaling the tuning data to learn parameters di-
rectly on a portion of the training bitext. Since the
bitext is used to learn rules for translation, using
the same parallel sentences for grammar extrac-
tion as well as for tuning feature weights can lead
to severe overfitting (Flanigan et al, 2013). To
avoid this issue, we used a jackknifing method to
split the training data into n = 10 folds, and built
a translation system on n?1 folds, while sampling
sentences from the News Commentary portion of
the held-out fold to obtain tuning sets from 5,000
to 50,000 sentences for German, and 15,000 sen-
tences for Russian.
Results for large-scale training for German are
presented in Table 6. Although we cannot com-
pare the tuning scores across different size sets,
we can see that tuning scores for all sets improve
substantially with sparse features. Unfortunately,
with increasing tuning set size, we see very little
improvement in Test BLEU and TER with either
feature set. Similar findings for Russian are pre-
sented in Table 7. Introducing sparse features im-
proves performance on each set, respectively, but
Dev always performs better on Test.
While tuning on Dev data results in better BLEU
on Test than when tuning on the larger sets, it is
important to note that although we are able to tune
more features on the larger bitext tuning sets, they
are not composed of the same genre as the Tune
and Test sets, resulting in a domain mismatch.
This phenomenon is further evident in German
when testing each model on Test2, which is se-
lected from the bitext, and is thus closer matched
to the larger tuning sets, but is separate from both
the parallel data used to build the translation model
and the tuning sets. Results on Test2 clearly show
significant improvement using any of the larger
tuning sets versus Dev for both the baseline and
sparse features. The 50k sparse setting achieves
almost 1 BLEU and 2 TER improvement, showing
that there are significant differences between the
Dev/Test sets and sets drawn from the bitext.
For Russian, we amplified the effects by select-
ing Test2 from the portion of the bitext that is sepa-
rate from the tuning set, but is among the sentences
used to create the translation model. The effects of
overfitting are markedly more visible here, as there
is almost a 7 BLEU difference between tuning on
Dev and the 15k set with sparse features. Further-
more, it is interesting to note when looking at Dev
that using sparse features has a significant nega-
tive impact, as the baseline tuned Dev performs
131
Tuning Test
?BLEU ?TER
5k 22.81 59.90
10k 22.77 59.78
25k 22.88 59.77
50k 22.86 59.76
Table 8: Results for German with 2 iterations of
tuning on Dev after tuning on larger set.
reasonably well, while the introduction of sparse
features leads to overfitting the specificities of the
Dev/Test genre, which are not present in the bitext.
We attempted two strategies to mitigate this
problem: combining the Dev set with the larger
bitext tuning set from the beginning, and tuning
on a larger set to completion, and then running 2
additional iterations of tuning on the Dev set using
the learned model. Results for tuning on Dev and a
larger set together are presented in Table 7 for Rus-
sian and Table 6 for German. As can be seen, the
resulting model improves somewhat on the other
genre and strikes a middle ground, although it is
worse on Test than Dev.
Table 8 presents results for tuning several ad-
ditional iterations after learning a model on the
larger sets. Although this leads to gains of around
0.5 BLEU on Test, none of the models outperform
simply tuning on Dev. Thus, neither of these two
strategies seem to help. In future work, we plan
to forgo randomly sampling the tuning set from
the bitext, and instead actively select the tuning
set based on similarity to the test set.
4 Conclusion
We explored strategies for scaling learning for
SMT to large tuning sets with sparse features.
While incorporating an adaptive per-feature learn-
ing rate and feature selection, we were able to
use Hadoop to efficiently take advantage of large
amounts of data. Although discriminative training
on larger sets still remains problematic, having the
capability to do so remains highly desirable, and
we plan to continue exploring methods by which
to leverage the power of the bitext effectively.
Acknowledgments
This research was supported in part by the DARPA
BOLT program, Contract No. HR0011-12-C-
0015; NSF under awards IIS-0916043 and IIS-
1144034. Vladimir Eidelman is supported by a
NDSEG Fellowship.
References
S. Chen and J. Goodman. 1996. An empirical study
of smoothing techniques for language modeling. In
ACL.
Colin Cherry and George Foster. 2012. Batch tun-
ing strategies for statistical machine translation. In
NAACL.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In EMNLP.
D. Chiang, K. Knight, and W. Wang. 2009. 11,001
new features for statistical machine translation. In
NAACL-HLT.
D. Chiang. 2012. Hope and fear for discriminative
training of statistical translation models. JMLR,
13:1159?1187.
K. Crammer, A. Kulesza, and M. Dredze. 2009. Adap-
tive regularization of weight vectors. In NIPS.
J. Dean and S. Ghemawat. 2004. MapReduce: Simpli-
fied data processing on large clusters. In OSDI.
J. Duchi, E. Hazan, and Y. Singer. 2011. Adaptive sub-
gradient methods for online learning and stochastic
optimization. JMLR, 12:2121?2159.
C. Dyer, A. Lopez, J. Ganitkevitch, J. Weese,
F. Ture, P. Blunsom, H. Setiawan, V. Eidelman, and
P. Resnik. 2010. cdec: A decoder, alignment, and
learning framework for finite-state and context-free
translation models. In ACL System Demonstrations.
Chris Dyer. 2009. Using a maximum entropy model to
build segmentation lattices for mt. In Proceedings
of NAACL-HLT.
Vladimir Eidelman, Ke Wu, Ferhan Ture, Philip
Resnik, and Jimmy Lin. 2013. Mr. MIRA: Open-
source large-margin structured learning on map-
reduce. In ACL System Demonstrations.
Vladimir Eidelman. 2012. Optimization strategies for
online large-margin learning in machine translation.
In WMT.
Jeffrey Flanigan, Chris Dyer, and Jaime Carbonell.
2013. Large-scale discriminative training for statis-
tical machine translation using held-out line search.
In NAACL.
S. Green, S. Wang, D. Cer, and C. Manning. 2013.
Fast and adaptive online training of feature-rich
translation models. In ACL.
Kenneth Heafield. 2011. KenLM: faster and smaller
language model queries. In WMT.
Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In EMNLP.
132
Tuning # mappers # features Tune Test Test2
?BLEU ?BLEU ?TER ?BLEU ?TER
Dev 120 16 22.38 22.69 60.61 29.31 54.26
5k 120 16 32.60 22.14 59.60 29.69 52.96
10k 120 16 33.16 22.06 59.43 29.93 52.37
Dev+10k 120 16 19.40 22.32 59.37 30.17 52.45
25k 300 16 32.48 22.21 59.54 30.03 51.71
50k 600 16 32.21 22.21 59.39 29.94 52.55
Dev 120 108k 23.86 23.01 59.89 29.65 53.86
5k 120 159k 33.70 22.26 59.26 30.53 51.84
10k 120 200k 34.00 22.12 59.24 30.51 51.71
Dev+10k 120 200k 19.62 22.42 59.17 30.26 52.21
25k 300 200k 32.96 22.35 59.29 30.39 52.14
50k 600 200k 32.86 22.40 59.15 30.54 51.88
Table 6: German evaluation with large-scale tuning, showing numbers of mappers employed, number of
active features for best model, and test scores on Test and bitext Test2 domains.
Tuning # mappers # features Tune Test Test2
?BLEU ?BLEU ?TER ?BLEU ?TER
Dev 120 16 30.18 29.89 49.05 57.14 32.56
15k 200 16 34.65 28.60 49.63 59.64 30.65
Dev+15k 200 16 33.97 28.88 49.37 58.24 31.81
Dev 120 77k 32.40 30.81 48.40 52.90 36.85
15k 200 200k 35.05 28.34 49.69 59.81 30.59
Dev+15k 200 200k 34.45 28.98 49.30 57.61 32.71
Table 7: Russian evaluation with large-scale tuning, showing numbers of mappers employed, number of
active features for best model, and test scores on Test and bitext Test2 domains.
Liang Huang and David Chiang. 2007. Forest rescor-
ing: Faster decoding with integrated language mod-
els. In ACL.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
NAACL.
Adam Lopez. 2007. Hierarchical phrase-based trans-
lation with suffix arrays. In EMNLP.
Franz Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
In Computational Linguistics.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In ACL.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
BLEU: a method for automatic evaluation of ma-
chine translation. In ACL.
P. Simianer, S. Riezler, and C. Dyer. 2012. Joint fea-
ture selection in distributed stochastic learning for
large-scale discriminative training in SMT. In ACL.
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A study of translation edit rate
with targeted human annotation. In AMTA.
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In ICSLP.
133
