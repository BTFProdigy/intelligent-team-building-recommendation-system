Proceedings of NAACL HLT 2007, Companion Volume, pages 141?144,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Stating with Certainty or Stating with Doubt:  
Intercoder Reliability Results for  
Manual Annotation of Epistemically Modalized Statements 
 
Victoria L. Rubin 
Faculty of Information and Media Studies 
University of Western Ontario 
London, Ontario, Canada N6A 5B7 
vrubin@uwo.ca 
 
Abstract 
Texts exhibit subtle yet identifiable mo-
dality about writers? estimation of how 
true each statement is (e.g., definitely true 
or somewhat true). This study is an analy-
sis of such explicit certainty and doubt 
markers in epistemically modalized 
statements for a written news discourse. 
The study systematically accounts for five 
levels of writer?s certainty (ABSOLUTE, 
HIGH, MODERATE, LOW CERTAINTY and 
UNCERTAINTY) in three news pragmatic 
contexts: perspective, focus, and time. 
The study concludes that independent 
coders? perceptions of the boundaries be-
tween shades of certainty in epistemically 
modalized statements are highly subjec-
tive and present difficulties for manual 
annotation and consequent automation for 
opinion extraction and sentiment analysis. 
While stricter annotation instructions and 
longer coder training can improve inter-
coder agreement results, it is not entirely 
clear that a five-level distinction of cer-
tainty is preferable to a simplistic distinc-
tion between statements with certainty 
and statements with doubt.  
1 Introduction 
1.1 Epistemic Modality, or Certainty 
Text conveys more than just a writer?s proposi-
tional context of assertions (Coates, 1987), e.g., X 
is true. Text can also transfer the writers? attitudes 
to the propositions, assessments of possibilities, 
and the writer?s certainty, or lack thereof, in the 
validity of the truth of the statements, e.g., X must 
be true, Y thinks that X is true, or perhaps X is 
true. A statement is qualified in such a way (be-
yond its mere referential function) is modal, or 
epistemically modalized (Coates, 1987; Westney, 
1986).  
    CERTAINTY, or EPISTEMIC MODALITY, concerns 
a linguistic expression of an estimation of the like-
lihood that a certain state of affairs is, has been, or 
will be true (Nuyts, 2001). Pragmatic and dis-
course literatures are abundant in discussions of 
epistemic modality (Coates, 1987; Nuyts, 2001); 
mood (Palmer, 1986); evidentiality and evidentials 
(Mushin, 2001); expressions of doubt and certainty 
(Holmes, 1982; Hoye, 1997) and hedging 
(Lackoff, 1972) and hedging in news writing 
(Hyland, 1999; Zuck & Zuck, 1986). Little at-
tempt, however, has been made in natural language 
computing literature to manually annotate and con-
sequently automate identification of statements 
with an explicitly expressed certainty or doubt, or 
shades of epistemic qualifications in between. This 
lack is possibly due to the complexity of comput-
ing epistemic interpretations in different pragmatic 
contexts; and due to unreliability of variety of lin-
guistic expressions in English that could explicitly 
qualify a statement. Another complication is a lack 
of agreed-upon and easily identifiable discrete 
categories on the continuum from certainty to 
doubt. Several annotation projects have success-
fully addressed closely related subjective issues 
such as private states in news writing (Wiebe, Wil-
son, & Cardie, 2005) and hedging in scientific 
writing (Light, Qiu, & Srinivasan, 2004; Mercer, 
DiMarco, & Kroon, 2004). Having access to the 
opinion holder?s evaluation of how true a state-
ment is valuable in predicting reliability of argu-
ments and claims, and stands to benefit the tasks of 
141
opinion and sentiment analysis and extraction in 
natural language computing.     
1.2 Certainty Level Scales 
While there is an on-going discussion in pragmatic 
literature on whether epistemic modality markers 
should be arranged on a continuum or in discrete 
categories, there seems to be an agreement that 
there are at least three articulated points on a pre-
sumed continuum from certainty to doubt. Hoye 
(1997) suggested an epistemic trichotomy of CER-
TAINTY, PROBABILITY, and POSSIBILITY, consistent 
with Holmes? (1982) scale of certainty of asser-
tions and negations where the writer asserts WITH 
CERTAINTY that a proposition is true or not true; or 
that the proposition is PROBABLY or POSSIBLY true 
or not true. In attitude and affect computational 
analysis literature, the context of extracting opin-
ions from news article corpora, Rubin and col-
leagues (2004; 2005) extended Hoye-Holmes 
models by adding two extremes on the epistemic 
continuum scales: ABSOLUTE CERTAINTY (defined 
as a stated unambiguous indisputable conviction or 
reassurance) and UNCERTAINTY (defined as hesi-
tancy or stated lack of clarity or knowledge), and 
re-defined the middle categories as HIGH CER-
TAINTY (i.e., high probability or firm knowledge), 
MODERATE CERTAINTY (i.e., estimation of an aver-
age likelihood or reasonable chances), and LOW 
CERTAINTY (i.e., distant possibility, see Fig. 1). 
   
 
Figure 1. Revised Explicit Certainty Categorization 
Model (redrawn from Rubin, 2006). 
 
While Rubin?s (2006) model is primarily con-
cerned with identification of certainty levels en-
coded in explicit certainty markers in propositions, 
it also takes into account three contextual dimen-
sions relevant to news discourse. Perspective at-
tributes explicit certainty either to the writer or two 
types of reported sources ? direct participants and 
experts in a field. Focus separates certainty in facts 
and opinions. Time is an organizing principle of 
news production and presentation, and if relevant, 
is separated into past, present, or future. 
2 Methodology 
This study uses the above-described conceptual 
certainty categorization model to annotate a news 
dataset, and produce a typology of syntactic, se-
mantic and lexical classes of certainty markers that 
map statements into 5 levels of certainty ranging 
from absolutely certain to uncertain.  
    The dataset consisted of 80 randomly selected 
articles (from the AQUAINT Corpus of English 
Texts, distributed by The New York Times Ser-
vices in 2000). It constituted a total of 2,243 sen-
tences, with 866 sentences in the editorials and 
1377 sentence in the news reports (Rubin, 2006). A 
subset of 10 articles (272 sentences, about 12% of 
the full dataset) was analyzed by 4 independently 
trained annotators (excluding the author). The 
agreement results were evaluated in 2 consecutive 
intercoder reliability experiments.  
2.1 Annotation Process 
The manual annotation scheme was defined in the 
codebook instructions that specified the procedures 
for determining certainty-qualified statements, the 
order of assigning categories, and exemplified each 
certainty category (Rubin, 2006). In Experiment 1, 
three coders received individual one-hour training 
regarding the use of the annotation scheme, and 
were instructed to use the original codebook writ-
ten in a general suggestive tone. In Experiment 2, 
the fourth annotator went through a more thorough 
five-hour training and used a revised, more rigidly-
specified codebook with an alphabetized key-word 
index mapped certainty markers into 5 levels. 
Each statement in a news article (be it a sentence 
or its constituent part such as a clause) was a po-
tential locus of explicit certainty. In both experi-
ments coders were asked to decide if a sentence 
had an explicit indication of a certainty level. If so, 
they then looked for explicit certainty markers that 
contributed to that indication. If a sentence con-
tained a certainty marker, the annotators were in-
142
structed to consider such a sentence certainty-
qualified. The statement was assigned a certainty 
level and placed in its pragmatic context (i.e., into 
one of the categories) within the perspective, fo-
cus, and time dimensions (see D2 ? D4, Fig. 1) 
relevant to the news discourse. Each marker was 
only assigned one category from each dimension.  
2.2 Intercoder Agreement Measures. 
Each pair of coders were evaluated on whether 
they agreed regarding 1) the sentences that contai-
ned explicit certainty markers; 2) the specific cer-
tainty markers within agreed upon certainty-
qualified sentences; and 3) classification of the 
agreed upon markers into one of the categories 
within each dimension (i.e., level, perspective, fo-
cus and time). The sentence and marker agreement 
measures were calculated with percent agreement. 
Partial word string matches were considered a 
marker match but were weight-adjusted. The 
agreed-upon marker category assignments were 
assessed in each pair of independent coders with 
Cohen?s kappa statistic (Cohen, 1960), averaged, 
and compared to the author?s annotation.  
3 Results and Discussion 
3.1 Typology of Certainty Markers 
The content analysis of the dataset generated a 
group of 1,330 explicitly certainty-qualified sen-
tences with 1,727 occurrences of markers. The 
markers were grouped into a typology of 43 syn-
tactico-lexical classes; each class is likely to occur 
within one of the 5 levels of certainty. The typol-
ogy will become a basis for an automated certainty 
identification algorithm. Among the most fre-
quently used certainty markers are central modal 
auxiliary verbs (e.g., must, could), gradable adjec-
tives in their superlative degree, and adverbial in-
tensifiers (e.g., much and so), while adjectival 
downtoners (e.g., feeble + NP) and adverbial 
value disjuncts (e.g., annoyingly, rightly) are 
rarely used to express explicit certainty. 
3.2 Intercoder Reliability Test Results 
In Experiment 1, 1) three coders agreed on whether 
a sentences was modalized by an explicit certainty 
marker or not 71% of the time with 0.33 Cohen?s 
kappa, on average. 2) Within agreed-upon cer-
tainly-qualified sentences, three coders agreed on 
actual certainty markers 54% of the time, on aver-
age, based on a combined count of the full and 
weight-adjusted partial matches. 3) In the categori-
zation task for the agreed-upon markers, the three 
coders, on average, were able to reach a slight 
agreement in the level and focus dimensions (0.15 
and 0.13 kappa statistics, respectively), and a fair 
agreement in perspective and time dimensions 
(0.44 and 0.41 kappa) according to the Landis and 
Koch (1977) agreement interpretation scale. 
The subsequent Experiment 2 showed promising 
results in agreement on explicit certainty markers 
(67%) and overall ability to distinguish certainty-
qualified statements from unmarked statements 
(0.51 kappa), and in the relatively intuitive catego-
rization of the perspective dimension (0.65 kappa).  
Although stricter instructions may have imposed 
a more orderly way of looking at the epistemic 
continuum, the 5 level certainty boundaries are still 
subject to individual perceptions (0.41 kappa) and 
may present difficulties in automation. In spite of 
its large inventory of certainty markers, English 
may not be precise enough to reliably distinguish 
multiple epistemic shades between certainty and 
doubt. Alternatively, people might be using same 
expressions but underlying categorization systems 
for different individuals do not overlap accurately. 
Recent pragmatic, discourse, and philosophy of 
language studies in mood and modality call for 
more comprehensive and truer to natural language 
description of epistemic modality in English refer-
ence grammar materials (Hoye, 2005). The latest 
modality scholarship will undoubtedly contribute 
to natural language applications such as opinion 
extraction and sentiment analysis. 
Time categorization in the context of certainty 
remained a challenge in spite of more vigorous 
training in Experiment 2 (0.31 kappa). The inter-
pretation of the reference point of ?the present? in 
the reported speech and nested events can be am-
biguous in the certainty identification task. Distin-
guishing facts versus opinions in combination with 
certainty identification also presented a particularly 
puzzling cognitive task (0.16 kappa), possibly due 
to necessity to evaluate closely related facets of a 
statement: whether the statement is purely factual, 
and how sure the author is about the proposition. 
The possibility of epistemically modalized facts is 
particularly intriguing. 
143
4 Conclusions and Applications 
This study reported the results of the manual an-
notation of texts in written news discourse, and 
identified the most prominent patterns and regu-
larities in explicitly stated markers occurrences in 
modalized statements. The linguistic means of ex-
pressing varying levels of certainty are docu-
mented and arranged into the typology of 
syntactico-semantic classes. This study implies that 
boundaries between shades of certainty in epis-
temically modalized statements (such as probabil-
ity and possibility) are highly subjective and 
present difficulties in manual annotation. This con-
clusion may warrant a simplification of the exist-
ing 5 certainty levels to a basic binary distinction 
between certainty and doubt. A baseline for future 
attempts to improve the calibration of levels and 
their boundaries was established. These modest 
intercoder reliability results attest to the complex-
ity of the automation of the epistemically modal-
ized statements ranging from certainty to doubt.  
In the future studies, I intend to revise the num-
ber of the discrete categories on the epistemic con-
tinuum and further re-define certainty levels 
conceptually. I plan to further validate the collec-
tion of agreed-upon certainty markers on a much 
larger dataset and by using the typology as input 
data to machine learning algorithms for certainty 
identification and extraction.  
References  
Coates, J. (1987). Epistemic Modality and Spoken Dis-
course. Transactions of the Philological Soci-
ety, 110-131. 
Cohen, J. (1960). A coefficient of agreement for nomi-
nal scales. Educational and Psychological 
Measurement, 20, 37-46. 
Holmes, J. (1982). Expressing Certainty and Doubt in 
English. RELC Journal, 13(2), 9-29. 
Hoye, L. (1997). Adverbs and Modality in English. 
London, New York: Longman. 
Hoye, L. (2005). "You may think that; I couldn't possi-
bly comment!" Modality Studies: Contempo-
rary Research and Future Directions. Part II. 
Journal of Pragmatics, 37, 1481-1506. 
Hyland, K. (1999). Academic attribution: Citation and 
the construction of disciplinary knowledge. 
Applied Linguistics, 20(3), 341-367. 
Lackoff, G. (1972). Hedges: a study of meaning criteria 
and the logic of fuzzy concepts. Paper pre-
sented at the Chicago Linguistic Society Pa-
pers. 
Landis, J., & Koch, G. G. (1977). The measurement of 
observer agreement for categorical data. Bio-
metrics, 33, 159-174. 
Light, M., Qiu, X. Y., & Srinivasan, P. (2004). The 
Language of Bioscience: Facts, Speculations, 
and Statements in Between. Paper presented at 
the BioLINK 2004: Linking Biological Litera-
ture, Ontologies, and Databases. 
Mercer, R. E., DiMarco, C., & Kroon, F. W. (2004). 
The Frequency of Hedging Cues in Citation 
Contexts in Scientific Writing. Paper presented 
at the Proceedings of the 17th Conference of 
the CSCSI/SCEIO (AI'2004). 
Mushin, I. (2001). Evidentiality and Epistemological 
Stance: Narrative Retelling (Vol. 87). Amster-
dam, Philadelphia: John Benjamins Publishing 
Company. 
Nuyts, J. (2001). Epistemic Modality, Language, and 
Conceptualization: A cognitive-pragmatic pre-
spective (Vol. 5). Amsterdam, Philadelphia: 
John Benjamin Publishing Company. 
Palmer, F. R. (1986). Mood and Modality. Cambridge: 
Cambridge University Press. 
Rubin, V. L. (2006). Identifying Certainty in Texts. Un-
published Doctoral Thesis, Syracuse Univer-
sity, Syracuse, NY. 
Rubin, V. L., Kando, N., & Liddy, E. D. (2004). Cer-
tainty Categorization Model. Paper presented 
at the AAAI Spring Symposium: Exploring At-
titude and Affect in Text: Theories and Appli-
cations, Stanford, CA. 
Rubin, V. L., Liddy, E. D., & Kando, N. (2005). Cer-
tainty Identification in Texts: Categorization 
Model and Manual Tagging Results. In J. 
Wiebe (Ed.), Computing Attitude and Affect in 
Text: Theory and Applications (The Informa-
tion Retrieval Series): Springer-Verlag New 
York, Inc. 
Westney, P. (1986). How to Be More-or-Less Certain in 
English - Scalarity in Epistemic Modality. 
IRAL: International Review of Applied Lin-
guistics in Language Teaching, 24(4), 311-320. 
Wiebe, J., Wilson, T., & Cardie, C. (2005). Annotating 
Expressions of Opinions and Emotions in Lan-
guage. Netherlands: Kluwer Academic Pub-
lishers. 
Zuck, J. G., & Zuck, L. V. (1986). Hedging in News-
writing. beads or braclets? How do we ap-
proach LSP? Paper presented at the Fifth 
European Symposium on LSP. 
 
 
144
Proceedings of the EACL 2012 Workshop on Computational Approaches to Deception Detection, pages 97?106,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Identification of Truth and Deception in Text:                       
Application of Vector Space Model to Rhetorical Structure Theory 
Victoria L. Rubin and Tatiana Vashchilko 
Language and Information Technology Research Lab (LiT.RL)  
Faculty of Information and Media Studies, University of Western Ontario 
London, Ontario, Canada 
{vrubin,tvashchi}@uwo.ca 
  
 
Abstract 
The paper proposes to use Rhetorical 
Structure Theory (RST) analytic 
framework to identify systematic 
differences between deceptive and 
truthful stories in terms of their 
coherence and structure. A sample of 36 
elicited personal stories, self-ranked as 
completely truthful or completely 
deceptive, is manually analyzed by 
assigning RST discourse relations among 
a story?s constituent parts. Vector Space 
Model (VSM) assesses each story?s 
position in multi-dimensional RST space 
with respect to its distance to truth and 
deceptive centers as measures of the 
story?s level of deception and 
truthfulness. Ten human judges evaluate 
if each story is deceptive or not, and 
assign their confidence levels, which 
produce measures of the human expected 
deception and truthfulness levels. The 
paper contributes to deception detection 
research and RST twofold: a) 
demonstration of discourse structure 
analysis in pragmatics as a prominent 
way of automated deception detection 
and, as such, an effective complement to 
lexico-semantic analysis, and b) 
development of RST-VSM methodology 
to interpret RST analysis in identification 
of previously unseen deceptive texts.  
Introduction 
Automated deception detection is a challenging 
task (DePaulo, Charlton, Cooper, Lindsay, and 
Muhlenbruck, 1997), only recently proven 
feasible with natural language processing and 
machine learning techniques (Bachenko, 
Fitzpatrick, and Schonwetter, 2008; Fuller, Biros, 
and Wilson, 2009; Hancock, Curry, Goorha, and 
Woodworth, 2008; Rubin, 2010; Zhou, Burgoon, 
Nunamaker, and Twitchell, 2004). The idea is to 
distinguish truthful information from deceptive, 
where deception usually implies an intentional 
and knowing attempt on the part of the sender to 
create a false belief or false conclusion in the 
mind of the receiver of the information (e.g., 
Buller and Burgoon, 1996; Zhou, et al, 2004). In 
this paper we focus solely on textual information, 
in particular, in computer-mediated personal 
communications such as e-mails or online posts. 
Previously suggested techniques for detecting 
deception in text reach modest accuracy rates at 
the level of lexico-semantic analysis. Certain 
lexical items are considered to be predictive 
linguistic cues, and could be derived, for 
examples, from the Statement Validity Analysis 
techniques used in law enforcement for 
credibility assessments (as in Porter and Yuille, 
1996). Though there is no clear consensus on 
reliable predictors of deception, deceptive cues 
are identified in texts, extracted and clustered 
conceptually, for instance, to represent diversity, 
complexity, specificity, and non-immediacy of 
the analyzed texts (e.g., Zhou, Burgoon, 
Nunamaker, and Twitchell (2004)). When 
implemented with standard classification 
algorithms (such as neural nets, decision trees, 
and logistic regression), such methods achieve 
74% accuracy (Fuller, et al, 2009). Existing 
psycholinguistic lexicons (e.g., LWIC by 
Pennebaker and Francis, 1999) have been 
adapted to perform binary text classifications for 
truthful versus deceptive opinions, with an 
average classifier demonstrating 70% accuracy 
rate (Mihalcea and Strapparava, 2009).  
These modest results, though usually achieved 
on restricted topics, are promising since they 
supersede notoriously unreliable human abilities 
in lie-truth discrimination tasks. On average, 
people are not very good at spotting lies (Vrij, 
2000), succeeding generally only about half of 
the time (Frank, Paolantinio, Feeley, and 
97
Servoss, 2004). For instance, a meta-analytical 
review of over 100 experiments with over 1,000 
participants, showed a 54% mean accuracy rate 
at identifying deception (DePaulo, et al, 1997). 
Human judges achieve 50 ? 63% success rates, 
depending on what is considered deceptive on a 
seven-point scale of truth-to-deception 
continuum (Rubin and Conroy, 2011, Rubin and 
Conroy, 2012), but the higher the actual self-
reported deception level of the story, the more 
likely a story would be confidently assigned as 
deceptive. In other words, extreme degrees of 
deception are more transparent to judges. 
The task for current automated deception 
detection techniques has been formulated as 
binary text categorization ? is a message 
deceptive or truthful ? and the decision applies to 
the whole analyzed text. Since it is an overall 
discourse level decision, it may be reasonable to 
consider discourse or pragmatic features of each 
message. Thus far, discourse is surprisingly 
rarely considered, if at all, and the majority of the 
effort has been restricted to lexico-semantic 
verbal predictors. A rare exception up to date has 
been a Bachenko, Fitzpatrick and Schonwetter?s 
(2008) study that focuses on truth or falsity of 
individual propositions, achieving a finer-grained 
level of analysis 1 , but the propositional inter-
relations within the discourse structure are not 
considered. To the best of our knowledge there 
have been no advances in that automation 
deception detection task to incorporate discourse 
structure features and/or text coherence analysis 
at the pragmatic levels of story interpretation. 
 
Study Objective 
With the recent advances in the identification of 
verbal cues of deception in mind, and the 
realization that they focus on linguistic levels 
below discourse and pragmatic analysis, the 
study focuses on one main question:  
? What is the impact of the relations 
between discourse constituent parts on 
the discourse composition of deceptive 
and truthful messages?  
We hypothesize that if the relations between 
discourse constituent parts in deceptive messages 
differ from the ones in truthful messages, then 
systematic analysis of such relations will help to 
                                                          
1 Using a corpus of criminal statements, police interrogations and 
legal testimonies, their regression and tree-based classification 
automatic tagger performs at average 69% recall and 85% precision 
rates, as compared to the performance of human taggers on the 
same subset (Bachenko, et al, 2008). 
detect deception. To investigate this question, we 
propose to use a novel methodology for 
deception detection research, Rhetorical 
Structure Theory (RST) analysis with subsequent 
application of the Vector Space Model (VSM). 
RST analysis is promising in deception detection, 
since RST analysis captures coherence of a story 
in terms of functional relations among different 
meaningful text units, and describes a 
hierarchical structure of each story (Mann and 
Thompson, 1988). The result is that each story is 
a set of RST relations connected in a hierarchical 
manner with more salient text units heading this 
hierarchical tree. We also propose to utilize the 
VSM model for conversion of the derived RST 
relations? frequencies into meaningful clusters of 
diverse deception levels. To evaluate the 
proposed RST-VSM methodology of deception 
detection in texts, we compare human assessment 
to the RST-analysis of deception levels for the 
sets of deceptive and truthful stories. The main 
findings demonstrate that RST resembles, to 
some degree, human judges in deceptive and 
truthful stories, and RST deception detection in 
self-rated deceptive stories has greater 
consistency than in truthful ones, which signifies 
the prominence of using RST-VSM methodology 
for deception detection 2 . However, RST 
conclusions regarding levels of deception in the 
truthful stories requires further research about the 
diversity of RST relations for the expressions of 
truths and deception as well as the types of 
clustering algorithms most suitable for clustering 
unevaluated by human judges? written 
communication in RST space to detect deception 
with certain degree of precision.  
The paper has three main parts. The next part 
discusses methodological foundations of RST-
VSM approach. Then, the data and collection 
method describe the sample. Finally, the results 
section demonstrates the identified levels of 
deception and truthfulness as well as their 
distribution across truthful and deceptive stories.  
 
RST-VSM Methodology: Combining 
Vector Space Model and Rhetorical 
Structure Theory 
Vector space model (VSM) seemed to be very 
useful in the identification of truth and deception 
types of written stories especially if the meaning 
                                                          
2 The authors recognize that the results are preliminary and should 
be generalized with caution due to very small dataset and certain 
methodological issues that require further development. 
98
of the stories is represented as RST relations. 
RST differentiates between rhetorically stand-
alone parts of a text, some of which are more 
salient (nucleolus) than the others (satellite). In 
the past couple of decades, empirical 
observations and previous RST research 
confirmed that writers tend to emphasize certain 
parts of a text in order to express their most 
essential idea to reach the purpose of the written 
message. These parts can be systematically 
identified through the analysis of the rhetorical 
connections among more and less essential parts 
of a text. RST helps to describe and quantify text 
coherence through a set of constraints on 
nucleolus and satellites. The main function of 
these constraints is to describe in the meaningful 
way why and how one part of a text connects to 
the others within a hierarchical tree structure, 
which is an RST representation of a coded text. 
The names of the RST relations also resemble 
the purpose of using the connected text parts 
together.  
For example, one of the RST relations, which 
appear in truthful stories and never appear in the 
deceptive stories in our sample, is EVIDENCE. 
The main purpose of using EVIDENCE to 
connect two parts of text is to present additional 
information in satellite, so that the reader?s belief 
about the information in the nucleolus increases. 
However, this can happen only if the information 
in the satellite is credible from reader?s point of 
view. For some reason, the RST coding of 18 
deceptive stories has never used EVIDENCE, but 
used it rather often in 18 truthful stories. This 
might indicates that either 1) writers of deceptive 
stories did not see any purpose in supplying 
additional information to the readers to increase 
their beliefs in communicating writer?s essential 
ideas, or 2) the credibility of presented 
information in satellite was not credible from the 
readers? points of view, which did not qualify the 
relationship between nucleolus and satellite for 
?EVIDENCE? relation, or 3) both (See an 
example of RST diagram in Appendix A).   
Our premise is that if there are systematic 
differences between deceptive and truthful 
written stories in terms of their coherence and 
structure, then the RST analysis of these stories 
can identify two sets of RST relations and their 
structure. One set is specific for the deceptive 
stories, and the other one is specific for the 
truthful stories.  
We propose to use a vector space model for 
the identification of these sets of RST relations. 
Mathematically speaking, written stories have to 
be modeled in a way suitable for the application 
of various computational algorithms based on 
linear algebra. Using a vector space model, the 
written stories could be represented as RST 
vectors in a high dimensional space (Salton and 
McGill 1983, Manning and Schutse 1999). 
According to the VSM, stories are represented as 
vectors, and the dimension of the vector space 
equals to the number of RST relations in a set of 
all written stories under consideration. Such 
representation of written stories makes the VSM 
very attractive in terms of its simplicity and 
applicability (Baeza-Yates and Ribeiro-Neto 
1999).  
Vector space model3 is the basis for almost all 
clustering techniques when dealing with the 
analysis of texts. Once the texts are represented 
according to VSM, as vectors in an n-
dimensional space, we can apply the myriad of 
cluster methods that have been developed in 
Computational Science, Data Mining, 
Bioinformatics. Cluster analysis methods can be 
divided into two big groups (Zhong and Ghosh 
2004): discriminative (or similarity based) 
approaches (Indyk 1999, Scholkopf and Smola 
2001, Vapnik 1998) and generative (or model-
based) approaches (Blimes 1998, Rose 1998, 
Cadez et al 2000).  
The main benefit of applying vector space 
model to RST analysis is that the VSM allows a 
formal identification of coherence and structural 
similarities among stories of the same type 
(truthful or deceptive). For this purpose, RST 
relations are vectors in a story space. Visually we 
could think about the set of stories or RST 
relations as a cube (Figure 1), in which each 
dimension is an RST relation.  
 
Figure 1: Cluster Representation of Story Sets or RST  
Relations (Cluto Graphical Frontend Project, 2002). 
                                                          
3 Tombros (2002) maintains that most of the research related to the 
retrieval of information is based on vector space model.  
99
The main subsets of this set of stories are two 
clusters, deceptive stories and truthful stories. 
The element of a cluster is a story, and a cluster 
is a set of elements that share enough similarity 
to be grouped together, the deceptive stories or 
truthful stories (Berkhin 2002). That is, there is a 
number of distinctive features (RST relations, 
their co-occurrences and positions in a 
hierarchical structure) that make each story 
unique and being a member of a particular 
cluster. These distinctive features of the stories 
are compared, and when some similarity 
threshold is met, they are placed in one of two 
groups, deceptive or truthful stories.  
Similarity 4  is one of the key concepts in 
cluster analysis, since most of the classical 
techniques (k-means, unsupervised Bayes, 
hierarchical agglomerative clustering) and rather 
recent ones (CLARANS, DBSCAN, BIRCH, 
CLIQUE, CURE, etc.) ?are based on distances 
between the samples in the original vector space? 
(Strehl et al2000). Such algorithms form a 
similarity based clustering framework (Figure 1) 
as it is described in Strehl et al(2000) , or as 
Zhong and Ghosh (2004) define it as 
discriminative (or similarity ? based) clustering 
approaches.  
That is why, this paper modifies Strehl et als 
(2004) similarity based clustering framework 
(Figure 2) to develop a unique RST-VSM 
methodology for deception detection in text. The 
RST-VSM methodology includes three main 
steps: 
1) The set of written stories, X, is transformed 
into the vector space description, X, using some 
rule, Y, that in our case corresponds to an RST 
analysis and identification of RST relations as 
well as their hierarchy in each story. 
2) This vector space description X is 
transformed into a similarity space description, 
S, using some rule, ? , which in our case is the 
Euclidian distance of every story from a 
deception and truth centers correspondingly 
based on normalized frequency of RST relations 
in a written story5.  
3) The similarity space description, S, is 
mapped into clusters based on the rule? , which 
we define as the relative closeness of a story to a 
                                                          
4 ?Interobject similarity is a measure of correspondence or 
resemblance between objects to be clustered? (Hair et al 
1995, p. 429). 
5 Since RST stories as vectors differ in length, the 
normalization assures their comparability. The coordinates 
of every story (the frequency of an RST relation in a story) 
are divided on the vector?s length.  
deception or a truth center: if a story is closer to 
the truth center, then a story is placed in a truth 
cluster, whereas if a story is closer to a deception 
center, then a story is placed in a deception 
cluster. 
 Figure 2: Similarity Based Clustering Framework 
(Strehl et al 2004) 
Data Collection and Sample 
The dataset contains 36 rich unique personal 
stories, elicited using Amazon?s online survey 
service, Mechanical Turk (www.mturk.com). 
Respondents in one group were asked to write a 
rich unique story, which is completely true or 
with some degree of deception. Respondents in 
another group were asked to evaluate the stories 
written by the respondents in the first group (For 
further details on the data collection process and 
the discussion of associated challenges, see 
Rubin and Conroy 2012).  
Two groups of 18 stories each compile the 
data sample. The first group consists of 18 stories 
that were self-ranked by their authors as 
completely deceptive on a seven-point Likhert 
scale from complete truth to complete deception 
(deceptive self-reported group). The second 
group includes stories, which their authors rated 
as completely truthful stories (truthful self-
reported group). The second group was matched 
in numbers for direct comparisons to the first 
group by selecting random 18 stories from a 
larger group of 39 completely truthful stories 
(Rubin and Conroy, 2011, Rubin and Conroy, 
2012). Each story in both groups, truthful self-
reported and deceptive self-reported, has 10 
unique human judgments associated with it. Each 
judgment is binary (?judged truthful? or ?judged 
deceptive?), and has an associated confidence 
level assigned by the judge (either ?totally 
uncertain?, ?somewhat uncertain?, ?I?m 
guessing?, ?somewhat certain?, or ?totally 
certain?). Each writer and judge was encouraged 
to provide explanations for defining a story as 
truthful or deceptive, and assigning a particular 
confidence level. In total, 396 participants 
contributed to the study, 36 of them were story 
authors, and 360 ? were judges performing lie-
truth discrimination task by confidence level.  
100
We combine the 10 judges? evaluations of a 
story into one measure, the expected level of a 
story?s deception or truthfulness. Since judges? 
confidence levels reflect the likelihood of a story 
being truthful or deceptive, the probability of a 
story being completely true or deceptive equals 
one and corresponds to a ?totally certain? 
confidence level that the story is true or 
deceptive6. Two dummy variables are created for 
each story. One dummy, a deception dummy, 
equals 1, if a judge rated the story is ?judged 
deceptive?, and 0 otherwise. The second dummy, 
the truthfulness dummy, equals 1 if a judge rated 
the story as ?judged truthful?, and 0 otherwise. 
Then the expected level of deception of a story 
equals the product of the probability (confidence 
level) of deception and the deception dummy 
across 10 judges. Similarly, the expected level of 
truthfulness is equals the product of the 
probability of truthfulness (confidence level) and 
the truthfulness dummy across 10 judges. The 
distribution of expected levels of deception and 
the expected levels of truthfulness of the 
deceptive and truthful subsets of the sample are 
in Appendix B1-B2.  
Thirty six stories, evenly divided between 
truthful and deceptive self-report groups, were 
manually analyzed using the classical set of 
Mann and Thompson?s (1988) RST relations, 
extensively tested empirically (Taboada and 
Mann, 2006). As a first stage of RST-VSM 
methodology development, the manual RST 
coding was required to deepen the understanding 
of the rhetorical relations and structures specific 
for deceptive and truthful stories. Moreover, 
manual analysis aided by Mick O?Donnell?s 
RSTTool (http://www.wagsoft.com/RSTTool/) 
might ensure higher reliability of the analysis and 
avoid compilation of errors, as the RST output 
further served as the VSM input. Taboada (2004) 
reports on the existence of Daniel Marcu?s RST 
Annotation Tool: www.isi.edu/licensed-
sw/RSTTool/ and Hatem Ghorbel?s 
RhetAnnotate (lithwww.epfl.ch/~ghorbel/rhet 
annotate/) and provides a good overview of other 
recent RST resources and applications. The 
acquired knowledge during manual coding of 
deceptive stories along with recent advances in 
automated RST analysis will help later on to 
evaluate RST-VSM methodology and design a 
                                                          
6 In the same way, the other levels of confidence have the 
following probability correspondences: ?totally uncertain? 
has probability 0.2 of a story being deceptive or truthful, 
?somewhat uncertain? ? 0.4, ?I?m guessing? ? 0.6, and 
?somewhat certain? ? 0.8. 
completely automated deception detection tool 
relying on the automated procedures to recognize 
rhetorical relations, which utilize the full 
rhetorical parsing (Marcu 1997, 2002). 
Results  
The preliminary clustering of 36 stories in RST 
space using various clustering algorithms shows 
that RST dimensions can systematically 
differentiate between truthful and deceptive 
stories as well as diverse levels of deception 
(Figure 3). 
 
 Figure 3. Four Clusters in RST Space by Level of 
Deception. 
 
The visualization uses GLUTO software 
(http://glaros.dtc.umn.edu/gkhome/cluto/gcluto/o
verview), which finds the clustering solution as a 
result of the optimization of a ?particular 
function that reflects the underlying definition of 
the ?goodness? of the cluster? (Rasmussen and 
Karypis 2004, p.3). Among the four clusters in 
RST space, two clusters are composed of 
completely deceptive stories (far back left peak 
in green) or entirely truthful stories (front peak in 
red), the other two clusters have a mixture with 
the prevalence of either truthful or deceptive 
stories. This preliminary investigation of using 
RST space for deception detection indicates that 
the RST analysis seems to offer a systematical 
way of distinguishing between truth and 
deceptive features of texts. 
This paper develops an RST-VSM 
methodology by using RST analysis of each 
story in N-dimensional RST space with 
subsequent application of vector space model to 
identify the level of a story?s deception. A 
normalized frequency of an RST relation in a 
story is a distinct coordinate in the RST space. 
The authors? ratings are used to calculate the 
101
centers for the truth and deception clusters based 
on corresponding authors? self-rated deception 
and truthful sets of stories in the sample. The 
normalized Euclidian distances between a story 
and each of the centers are defined as the degree 
of deception of that story depending on its 
closeness to the deception center. The closer a 
story is to the deception center, the higher is its 
level of deception. The closer a story is to the 
truthful center, the higher is its level of 
truthfulness7.  
RST seems to differentiate between truthful 
and deceptive stories. The difference in means 
test demonstrates that the truthful stories have a 
statistically significantly lower average number 
of text units per statement than the deceptive 
stories (t= -1.3104), though these differences are 
not large, only at 10% significance level. The 
normalized frequencies of the RST relations 
appearing in the truthful and deceptive stories 
differ for about one third of all RST relations 
based on the difference in means test (Appendix 
C).  
The comparison of the distribution of RST 
relations across deceptive and truth centers 
demonstrates that on average, the frequencies 
and the usage of such RST relations as 
conjunction, elaboration, evaluation, list, means, 
non-volitional cause, non-volitional result, 
sequence, and solutionhood in deceptive stories 
exceeds those in the truthful ones (Figure 4). On 
the other hand, the average usage and 
frequencies of such RST relations as volitional 
result, volitional cause, purpose, interpretation, 
concession, circumstance and antithesis in 
truthful stories exceeds those in the deceptive 
ones. Some of the RST relations are only specific 
for one type of the story: enablement, 
restatement and evidence appear only in truthful 
stories, whereas summary, preparation, 
unconditional and disjunction appear only in 
deceptive stories.  
The histograms of distributions of deception 
(truthfulness) levels assigned by judges and 
derived from RST-VSM analysis demonstrate 
some similarities between the two for truth and 
for deceptive stories (Appendices D-E). More 
rigorous statistical testing reveals that only 
truthfulness levels in deceptive stories assigned 
by judges do not have statistically significant 
difference from the RST-VSM ones8. For other 
                                                          
7 All calculations are performed in STATA. 
8 We use the Wilcoxon signed rank sum test, which is the non-
parametric version of a paired samples t-test (STATA command 
signrank (STATA 2012)).  
groups, the judges? assessments and RST ones do 
differ significantly.  
 
 Figure 4: Comparison of the RST Relations? 
Composing the Deceptive Cluster Center (top red bar) 
and the Truthful Cluster Center (bottom blue bar). 
 
The difference is especially apparent in the 
distributions of deception and truthfulness in 
truthful stories. Among them, RST-VSM 
methodology counted 44.44% of stories having 
50% deception level, whereas judges counted 
61.11 percent of the same stories having low 
deception level of no more than 20%. The level 
of truthfulness was also much higher in judges? 
assessment than based on RST-VSM 
calculations.  
0 0.2 0.4 0.6
Antithesis
Background
Circumstance
Concession
Condition
Conjunction
Disjunction
Elaboration
Enablement
Evaluation
Evidence
Interpretation
Joint
List
Means
Nonvolitional?cause
Nonvolitional?result
Preparation
Purpose
Restatement
Sequence
Solutionhood
Summary
Top
Unconditional
Volitional?cause
Volitional?result
102
The distribution of the levels of deception and 
truthfulness across all deceptive stories 
(Appendices D1-D4) and across all truthful 
stories (Appendices E1-E4) shows variations in 
patterns of deception levels based on RST-VSM. 
In deception stories, the RST-VSM levels of 
deception are consistently higher than the RST-
VSM levels of truthfulness. Assuming that the 
authors of the stories did make them up, the 
RST-VSM methodology seems to offer a 
systematic way of detecting a high level of 
deception with rather good precision.  
The RST-VSM deception levels are not as 
high as human judges? ones, with human judges 
assigning much higher levels of deception to 
deceptive stories than to truthful stories. 
Assuming that the stories are indeed made up, 
the human judges have greater precision than the 
RST-VSM methodology. Nevertheless, RST-
VSM analysis assigns higher deception levels to 
stories, which also receive higher human judges? 
deception levels. This pattern is consistent across 
all deceptive stories.  
Discussion  
The analysis of truthful stories shows some 
systematic and some slightly contradictory 
findings. On one hand, the levels of truthfulness 
assigned by judges are predominantly higher 
than the levels of deception. Again, assuming 
that the stories in the truthful set are completely 
true because the authors ranked them so, the 
human judges have greater likelihood of rating 
these stories as truthful than as deceptive. This 
can be an indicator of a good precision of 
deception detection by human judges.  
On the other hand, the RST-VSM analysis 
also demonstrates that large subsample (but not 
as large as indicated by human judges) of truthful 
stories is closer to the truth center than to the 
deceptive one. However, it seems that RST-VSM 
methodology overestimates the levels of 
deception in the truthful stories compared to 
human judges 
Overall, however, the RST-VSM analysis 
demonstrates a positive support for the proposed 
hypothesis. The apparent and consistent 
closeness of deceptive stories to RST deception 
center (compared to the closeness of the 
deceptive stories to the truthful center) and 
truthful stories to RST truthful center can 
indicate that the relations between discourse 
constituent parts differ between truthful and 
deceptive messages. Thus, since the truthful and 
deceptive relations exhibit systematic differences 
in RST space, the proposed RST-VSM 
methodology seemed to be a prominent tool in 
deception detection. The results, however, have 
to be interpreted with caution, since the sample 
was very small, and only one expert conducted 
RST coding.  
The discussion, however, might be extended 
to the case, where the assumption of self-ranked 
levels of deception and truthfulness do not hold. 
In this case we still suspect that even deceptive 
story might contain elements of truth (though 
much less), and the truth story will have some 
elements of deception. RST-VSM analysis 
demonstrated greater levels of deception in truth 
and deceptive stories compared to the human 
judges. This might indicate that RST-VSM 
potentially offers an alternative to human judges 
way of detecting deception when it is least 
expected in text (as in the example of supposedly 
truthful stories) or detecting it in a more accurate 
way (if some level of deception is assumed as in 
the completely deceptive stories). The advantage 
of RST-VSM methodology is in its rigorous and 
systematic approach of coding discourse 
relations and their subsequent analysis in RST 
space using vector space models. As a result, the 
relations between units exhibiting different 
degrees of salience in text because of writers? 
purposes with their subsequent readers? 
perceptions become indicators of diversity in 
deception levels.  
Conclusions  
To conclude, relations between discourse parts 
along with its structure seem to have different 
patterns in truthful and deception stories. If so, 
RST-VSM methodology can be a prominent way 
of detecting deception and complementing the 
existing lexical ones.  
Our contribution to deception detection 
research and RST twofold: a) we demonstrate 
that discourse structure analysis and pragmatics 
as a promising way of automated deception 
detection and, as such, an effective complement 
to lexico-semantic analysis, and b) we develop 
the unique RST-VSM methodology of 
interpreting RST analysis in identification of 
previously unseen deceptive texts.  
Acknowledgments 
This research is funded by the New Research and 
Scholarly Initiative Award (10-303) from the 
Academic Development Fund at Western.  
103
References  
Bachenko, J., Fitzpatrick, E., and Schonwetter, M. 
2008. Verification and implementation of 
language-based deception indicators in civil and 
criminal narratives. In Proceedings of the 22nd 
International Conf. on Computational Linguistics.  
Baeza-Yates, R. and B. Ribeiro-Neto. 1999. Modern 
Information Retrieval. New York: Addison-Wesley 
Buller, D. B., and Burgoon, J. K. 1996. Interpersonal 
Deception Theory. Communication Theory, 6(3), 
203-242. 
Berkhin, P. 2002. Survey of Clustering Data Mining 
Techniques. DOI: 10.1.1.18.3739. 
Blimes, J. A. 1998. A Gentle Tutorial of the EM 
Algorithm and Its Application to Parameter 
Estimation for Gaussian Mixture and Hidden 
Markov Models: Univ. of California, Berkeley.  
Cadez, I. V, Gaffney, S. and P. Smyth. 2000. A 
General Probabilistic Framework for Clustering 
Individuals and Objects. In Proceedings of the 6th 
ACM SIGKDD International  Conference on 
Knowledge Discovery and Data Mining.  
DePaulo, B. M., Charlton, K., Cooper, H., Lindsay, J. 
J., and Muhlenbruck, L. 1997. The Accuracy-
Confidence Correlation in the Detection of 
Deception. Personality and Social Psychology 
Review, 1(4), 346-357. 
Frank, M. G., Paolantinio, N., Feeley, T., and Servoss, 
T. 2004. Individual and Small Group Accuracy in 
Judging Truthful and Deceptive Communication. 
Group Decision and Negotiation, 13, 45-59. 
Fuller, C. M., Biros, D. P., and Wilson, R. L. 2009. 
Decision support for determining veracity via 
linguistic-based cues. Decision Support Systems 
46(3), 695-703. 
gCLUTO: Graphical Clustering Toolkit 1.2. Dept. of 
Computer Science, University of Minnesota. 
Hair, J.F., Anderson, R.E., Tathman, R.L. and W.C. 
Black. 1995. Multivariate Data Analysis with 
Readings. Upper Saddle River, NJ: Princeton Hall. 
Hancock, J. T., Curry, L. E., Goorha, S., and 
Woodworth, M. 2008. On lying and being lied to: 
A linguistic analysis of deception in computer-
mediated communication. Discourse Processes, 
45(1), 1-23. 
Indyk, P. 1999. A Sublinear- time Approximation 
Scheme for Clustering in Metric Spaces. In 
Proceedings of the 40th Annual Symposium on 
Foundations of Computer Science.  
Karypis, G. 2003. Cluto: A Clustering Toolkit. Min-
neapolis: Univ. of Minnesota, Comp. Sci. Dept. 
Mann, W. C., and Thompson, S. A. 1988. Rhetorical 
Structure Theory: Toward a Functional Theory of 
Text Organization. Text, 8(3), 243-281. 
Manning, C.D. and H. Schutze. 1999. Foundations of 
Statistical Natural Language Processing. 
Cambridge, MA: MIT Press. 
Mihalcea, R., and Strapparava, C. 2009. The Lie 
Detector: Explorations in the Automatic Recogni-
tion of Deceptive Language. In Proceedings of the 
ACL, Aug. 2-7, Singapore.  
Pennebaker, J., and Francis, M. 1999. Linguistic 
inquiry and word count: LIWC. Erlbaum Publisher 
Porter, S., and Yuille, J. C. 1996. The language of 
deceit: An investigation of the verbal clues to 
deception in the interrogation context. Law and 
Human Behavior, 20(4), 443-458. 
Rasmussen, M. and G. Karypis. 2004. gCLUTO: An 
Interactive Clustering, Vizualization and Analysis 
System. UMN-CS TR-04-021. 
Rose, K. 1998. Deterministic Annealing for 
Clustering, Compression, Classification, 
Regression, and Related Optimization Problems. 
In Proceedings of the IEEE 86(11).  
Rubin, V.L. 2010. On Deception and Deception 
Detection: Content Analysis of Computer-
Mediated Stated Beliefs. In Proceedings of the 
American Soc. for Information Science and Tech. 
Annual Meeting, Oct. 22-27, Pittsburgh. 
Rubin, V.L., and Conroy, N. 2011. Challenges in 
Automated Deception Detection in Computer-
Mediated Communication. In Proceedings of the 
American Soc. for Information Science and Tech. 
Annual Meeting, Oct. 9-12, New Orleans. 
Rubin V.L., Conroy, N. 2012. Discerning Truth from  
Deception: Human Judgments and Automation 
Efforts. First Monday 17(3), http://firstmonday.org  
Salton, G. and M.J. McGill. 1983. Introduction to 
Modern Information Retrieval.  New York: 
McGraw-Hill. 
Scholkopf, B. and A. Smola. 2001. Learning With 
Kernels. Cambridge, MA: MIT Press. 
Strehl, A., Ghosh, J. and R. Mooney. 2000. In  AAAI 
Workshop of Artificial Intelligence for Web 
Search, July 30, 58-64. 
Taboada, M. 2004. Building Coherence and 
Cohesion: Task-Oriented Dialogue in English and 
Spanish. Amsterdam, Netherlands: Benjamins. 
Taboada, M. and W.C. Mann. (2006). Rhetorical 
structure theory: looking back and moving ahead. 
Discourse Studies, 8(3), 423-459. 
Tombros, A. 2002. The effectiveness of query-based 
hierarchic clustering of documents for information 
retrieval. PhD dissertation, Dept. of Computing 
Science, University of Glasgow. 
Vapnik, V. 1998. Statistical Learning Theory. NY. 
Wiley. 
Vrij, A. 2000. Detecting Lies and Deceit. NY: Wiley. 
Zhong, S. and Ghosh. J., 2004. A Comparative Study 
of Generative Models for Document Clustering. In 
SIAM Int. Conf. Data Mining Workshop on 
Clustering High Dimensional Data and Its 
Applications. 
Zhou, L., Burgoon, J. K., Nunamaker, J. F., and 
Twitchell, D. 2004. Automating Linguistics-Based 
Cues for Detecting Deception in Text-Based 
Asynchronous Computer-Mediated Communi-
cations. Group Decision and Negotiation, 13(1), 
81-106. 
104
Appendix A. Sample RST Analysis. 
  
Appendix B1. Distributions of Expected Levels of Deception and Truthfulness in Deceptive Stories.  
    Legend:         Expected level of Deception (Judges);            Expected Level of Truthfulness (Judges) 
       RST Level of Deception;              RST Level of Truthfulness (transformed to the interval (0,1) with 0 min) 
 
  
Appendix B2. Distributions of Expected Levels of Deception and Truthfulness in Truthful Stories.  
 
 
  
Appendix C. Comparison of the Normalized Frequencies of the RST Relationships in Truthful and 
Deceptive Stories: Difference in Means Test. 
RST?relationships?appearing?in?truthful?and?
deceptive?stories?with?NO?statistically?significant?
differences??
RST?relationships?appearing?in?
the?truthful?stories?with?
statistically?significantly?GREATER?
normalized?frequencies?than?the?
deceptive?ones?
RST?relationships?appearing?in?
the?truthful?stories?with?
statistically?significantly?LOWER?
normalized?frequencies?than?the?
deceptive?ones?
Background,?Circumstance,?Concession,?Condi?
tion,?Conjunction,?Elaboration,?Enablement,?Inter?
pretation,?List,?Means,?Non?volitional?cause,?Non?
volitional?result,?Purpose,?Restatement,?Se?
quence,?Solutionhood,?Summary,?Unconditional?
Antithesis?(t=2.3299)?
Evidence?(t=3.7996)?
Joint?(t=1.5961)?
Volitional?cause?(t=1.8597)?
Volitional?result?(t=1.8960)?
Preparation?(t=??1.7533)?
Evaluation?(t=??2.0762)?
Disjunction?(t=??1.7850)?
?
.1
.2
.3
.4
.5
.6
.7
.8
.9
Low Deception
High Deception
Le
ve
l o
f D
ec
ep
tio
n o
r T
rut
h
De
ce
pti
ve
 St
ory
 1
De
ce
pti
ve
 St
ory
 2
De
ce
pti
ve
 St
ory
 3
De
ce
pti
ve
 St
ory
 4
De
ce
pti
ve
 St
ory
 5
De
ce
pti
ve
 St
ory
 6
De
ce
pti
ve
 St
ory
 7
De
ce
pti
ve
 St
ory
 8
De
ce
pti
ve
 St
ory
 9
De
ce
pti
ve
 St
ory
 10
De
ce
pti
ve
 St
ory
 11
De
ce
pti
ve
 St
ory
 12
De
ce
pti
ve
 St
ory
 13
De
ce
pti
ve
 St
ory
 14
De
ce
pti
ve
 St
ory
 15
De
ce
pti
ve
 St
ory
 16
De
ce
pti
ve
 St
ory
 17
De
ce
pti
ve
 St
ory
 18
Expected Level of Deception (Judges) transformed to the interval (0,1) with 0 mi
RST Level of Deception Transformed to the interval (0,1) with 0 min
RST Level of Truth Transformed to the interval (0,1) with 0 min
Expected Level of Truthfulness (Judges) transformed to the interval (0,1) with 0
.1
.2
.3
.4
.5
.6
.7
.8
.9
Low Deception or Truth
Le
ve
l o
f D
ec
ep
tio
n o
r T
rut
h
Tru
thf
ul 
Sto
ry 
1
Tru
thf
ul 
Sto
ry 
2
Tru
thf
ul 
Sto
ry 
3
Tru
thf
ul 
Sto
ry 
4
Tru
thf
ul 
Sto
ry 
5
Tru
thf
ul 
Sto
ry 
6
Tru
thf
ul 
Sto
ry 
7
Tru
thf
ul 
Sto
ry 
8
Tru
thf
ul 
Sto
ry 
9
Tru
thf
ul 
Sto
ry 
10
Tru
thf
ul 
Sto
ry 
11
Tru
thf
ul 
Sto
ry 
12
Tru
thf
ul 
Sto
ry 
13
Tru
thf
ul 
Sto
ry 
14
Tru
thf
ul 
Sto
ry 
15
Tru
thf
ul 
Sto
ry 
16
Tru
thf
ul 
Sto
ry 
17
Tru
thf
ul
Sto
ry
18
RST Level of Truth Transformed to the interval (0,1) with 0 min
Expected Level of Truthfulness (Judges) transformed to the interval (0,1) with 0
105
Appendices D1 ?D4. Distribution of Deception and Truthfulness Levels for Deceptive Stories 
 D1. Distribution of Deception Level (Judges) D2. Distribution of Truthfulness Level (Judges) 
 D3. Distribution of Deception Level (RST) 
 
 D4. Distribution of Truthfulness Level (RST) 
 Appendices E1-E4. Distribution of Deception and Truthfulness Levels for True Stories 
 E1. Distribution of Deception Level (Judges) 
 
E2. Distribution of Truthfulness Level (Judges) 
E3. Distribution of Deception Level (RST)
 
 E4. Distribution of Truthfulness Level (RST) 
 
27.78
5.556
27.78
38.89
0
10
20
30
40
Pe
rce
nt
0 .1 .2 .3 .4 .5Expected Level of Deception (Judges) transformed to the interval (0,1) with 0 mi
44.44
27.78
5.556
22.22
0
10
20
30
40
Pe
rce
nt
.2 .4 .6 .8Expected Level of Truthfulness (Judges) transformed to the interval (0,1) with 0
11.11
33.33
22.22
33.33
0
10
20
30
40
Pe
rce
nt
.2 .3 .4 .5 .6RST Level of Deception Transformed to the interval (0,1) with 0 min
5.556
33.33
38.89
22.22
0
10
20
30
40
Pe
rce
nt
.1 .2 .3 .4 .5RST Level of Truth Transformed to the interval (0,1) with 0 min
16.67
61.11
11.11 11.11
0
20
40
60
Pe
rce
nt
0 .1 .2 .3 .4Expected Level of Deception (Judges) transformed to the interval (0,1) with 0 mi
11.11
38.89 38.89
11.11
0
10
20
30
40
Pe
rce
nt
.2 .4 .6 .8
Expected Level of Truthfulness (Judges) transformed to the interval (0,1) with 0
11.11 11.11
33.33
44.44
0
10
20
30
40
50
Pe
rce
nt
0 .1 .2 .3 .4 .5RST Level of Deception Transformed to the interval (0,1) with 0 min
16.67 16.67
27.78
38.89
0
10
20
30
40
Pe
rce
nt
.2 .3 .4 .5 .6RST Level of Truth Transformed to the interval (0,1) with 0 min
106
