Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 36?46,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Discovering Latent Structure in Task-Oriented Dialogues
Ke Zhai
?
Computer Science, University of Maryland
College Park, MD 20740
zhaike@cs.umd.edu
Jason D. Williams
Microsoft Research
Redmond, WA 98052
jason.williams@microsoft.com
Abstract
A key challenge for computational conver-
sation models is to discover latent struc-
ture in task-oriented dialogue, since it pro-
vides a basis for analysing, evaluating, and
building conversational systems. We pro-
pose three new unsupervised models to
discover latent structures in task-oriented
dialogues. Our methods synthesize hidden
Markov models (for underlying state) and
topic models (to connect words to states).
We apply them to two real, non-trivial
datasets: human-computer spoken dia-
logues in bus query service, and human-
human text-based chats from a live tech-
nical support service. We show that our
models extract meaningful state represen-
tations and dialogue structures consistent
with human annotations. Quantitatively,
we show our models achieve superior per-
formance on held-out log likelihood eval-
uation and an ordering task.
1 Introduction
Modeling human conversation is a fundamental
scientific pursuit. In addition to yielding ba-
sic insights into human communication, compu-
tational models of conversation underpin a host
of real-world applications, including interactive
dialogue systems (Young, 2006), dialogue sum-
marization (Murray et al, 2005; Daum?e III and
Marcu, 2006; Liu et al, 2010), and even medi-
cal applications such as diagnosis of psychological
conditions (DeVault et al, 2013).
Computational models of conversation can be
broadly divided into two genres: modeling and
control. Control is concerned with choosing ac-
tions in interactive settings?for example to maxi-
mize task completion?using reinforcement learn-
?
Work done at Microsoft Research.
ing (Levin et al, 2000), supervised learning (Hur-
tado et al, 2010), hand-crafted rules (Larsson and
Traum, 2000), or mixtures of these (Henderson
and Lemon, 2008). By contrast, modeling?the
genre of this paper?is concerned with inferring
a phenomena in an existing corpus, such as di-
alogue acts in two-party conversations (Stolcke
et al, 2000) or topic shifts in multi-party dia-
logues (Galley et al, 2003; Purver et al, 2006;
Hsueh et al, 2006; Banerjee and Rudnicky, 2006).
Many past works rely on supervised learning or
human annotations, which usually requires man-
ual labels and annotation guidelines (Jurafsky et
al., 1997). It constrains scaling the size of training
examples, and application domains. By contrast,
unsupervised methods operate only on the observ-
able signal (e.g. words) and are estimated with-
out labels or their attendant limitations (Crook et
al., 2009). They are particularly relevant because
conversation is a temporal process where models
are trained to infer a latent state which evolves as
the dialogue progresses (Bangalore et al, 2006;
Traum and Larsson, 2003).
Our basic approach is to assume that each ut-
terance in the conversation is in a latent state,
which has a causal effect on the words the conver-
sants produce. Inferring this model yields basic
insights into the structure of conversation and also
has broad practical benefits, for example, speech
recognition (Williams and Balakrishnan, 2009),
natural language generation (Rieser and Lemon,
2010), and new features for dialogue policy opti-
mization (Singh et al, 2002; Young, 2006).
There has been limited past work on unsuper-
vised methods for conversation modeling. Choti-
mongkol (2008) studies task-oriented conversa-
tion and proposed a model based on a hidden
Markov model (HMM). Ritter et al (2010) ex-
tends it by introducing additional word sources,
and applies to non-task-oriented conversations?
social interactions on Twitter, where the subjects
36
discussed are very diffuse. The additional word
sources capture the subjects, leaving the state-
specific models to express common dialogue flows
such as question/answer pairs.
In this paper, we retain the underlying HMM,
but assume words are emitted using topic models
(TM), exemplified by latent Dirichlet alocation
(Blei et al, 2003, LDA). LDA assumes each word
in an utterance is drawn from one of a set of latent
topics, where each topic is a multinomial distri-
bution over the vocabulary. The key idea is that
the set of topics is shared across all states, and
each state corresponds to a mixture of topics. We
propose three model variants that link topics and
states in different ways.
Sharing topics across states is an attractive
property in task-oriented dialogue, where a sin-
gle concept can be discussed at many points in a
dialogue, yet different topics often appear in pre-
dictable sequences. Compared to past works, the
decoupling of states and topics gives our mod-
els more expressive power and the potential to be
more data efficient. Empirically, we find that our
models outperform past approaches on two real-
world corpora of task-oriented dialogues.
This paper is organized as follows: Section 2 in-
troduces two task-oriented domains and corpora;
Section 3 details three new unsupervised genera-
tive models which combine HMMs and LDA and
efficient inference schemes; Section 4 evaluates
our models qualitatively and quantitatively, and fi-
nally conclude in Section 5.
2 Data
To test the generality of our models, we study two
very different datasets: a set of human-computer
spoken dialogues in quering bus timetable (Bus-
Time), and a set of human-human text-based dia-
logues in the technical support domain (TechSup-
port). In BusTime, the conversational structure is
known because the computer followed a determin-
istic program (Williams, 2012), making it possible
to directly compare an inferred model to ground
truth on this corpus.
1
In TechSupport, there is no
known flowchart,
2
making this a realistic applica-
tion of unsupervised methods.
1
Available for download at http://research.microsoft.
com/en-us/events/dstc/
2
Technical support human agents use many types of
documentation?mainly checklists and guidelines, but in
general, there are no flowcharts.
BusTime This corpus consists of logs of tele-
phone calls between a spoken dialogue system and
real bus users in Pittsburgh, USA (Black et al,
2010). For the user side, the words logged are the
words recognized by the automatic speech recog-
nizer. The vocabulary of the recognizer was con-
strained to the bus timetable task, so only words
known to the recognizer in advance are output.
Even so, the word error rate is approximately 30-
40%, due to the challenging audio conditions of
usage?with traffic noise and extraneous speech.
The system asked users sequentially for a bus
route, origin and destination, and optionally date
and time. The system confirmed low-confidence
speech recognition results. Due to the speech
recognition channel, system and user turns always
alternate. An example dialogue is given below:
System: Say a route like ?bus-route?, or say I?m not sure.
User: ?bus-route?.
System: I thought you said ?bus-route?, is that right?
User: Yes.
System: Say where?re you leaving from, like ?location?.
User: ?location?.
System: Okay, ?location?, where are you going to?
...
We discard dialogues with fewer than 20 ut-
terances. We also map all named entities (e.g.,
?downtown? and ?28X?) to their semantic types
(resp. ?location? and ?bus-route?) to reduce vo-
cabulary size. The corpus we use consists of ap-
proximately 850 dialogue sessions or 30, 000 ut-
terances. It contains 370, 000 tokens (words or se-
mantic types) with vocabulary size 250.
TechSupport This corpus consists of logs of
real web-based human-human text ?chat? con-
versations between clients and technical support
agents at a large corporation. Usually, clients and
agents first exchange names and contact informa-
tion; after that, dialogues are quite free-form, as
agents ask questions and suggest fixes. Most dia-
logues ultimately end when the client?s issue has
been resolved; some clients are provided with a
reference number for future follow-up. An exam-
ple dialogue is given below:
Agent: Welcome to the answer desk! My name is ?agent-
name?. How can I help you today?
Agent: May I have your name, email and phone no.?
Client: Hi, ?agent-name?. I recently installed new soft-
ware but I kept getting error, can you help me?
Agent: Sorry to hear that. Let me help you with that.
Agent: May I have your name, email and phone no.?
Client: The error code is ?error-code?.
Client: It appears every time when I launch it.
Client: Sure. My name is ?client-name?.
Client: My email and phone are ?email?, ?phone?.
Agent: Thanks, ?client-name?, please give me a minute.
37
s0
w
0,i
N
0
M
s
0
M
s
0
w
0,i
N
0
s
1
M
s
0
w
0,i
N
0
s
1
w
1,i
N
1
M
s
0
w
0,i
N
0
s
1
w
1,i
N
1
...
s
n
w
n,i
N
n
M
M
(a) LM-HMM
s
0
w
0,i
N
0
s
1
...
s
n
M
w
1,i
N
1
w
n,i
N
n
r
1,i
r
0,i
r
n,i?m
? E
?m
s
0
w
0,i
N
0
s
1
...
s
n
M
w
1,i
N
1
w
n,i
N
n
r
1,i
r
0,i
r
n,i
t
m
g
E
u
m
s
0
w
0,i
N
0
s
1
M
w
1,i
N
1
r
1,i
r
0,i
t
m
g
E
u
m
s
0
w
0,i
N
0
s
1
M
r
0,i
t
m
g
E
u
m
s
0
w
0,i
N
0
s
1
M
N
1
r
1,i
r
0,i
t
m
g
E
u
m
s
0
N
0
M
r
0,i
t
m
g
E
u
m
s
0
w
0,i
N
0
M
r
0,i
t
m
g
E
u
m
M
t
m
g
E
u
m
s
0
N
0
M
t
m
g
E
u
m
(b) LM-HMMS
Figure 1: Plate diagrams of baseline models, from
existing work (Chotimongkol, 2008; Ritter et al,
2010). Variable definitions are given in the text.
...
This data is less structured than BusTime;
clients? issues span software, hardware, network-
ing, and other topics. In addition, clients use com-
mon internet short-hand (e.g., ?thx?, ?gtg?, ?ppl?,
?hv?, etc), with mis-spellings (e.g., ?ofice?, ?off-
fice?, ?erorr?, etc). In addition, chats from the web
interface are segmented into turns when a user hits
?Enter? on a keyboard. Therefore, clients? input
and agents? responses do not necessarily alternate
consecutively, e.g., an a ent?s response may take
multiple turns as in the above example. Also, it
is unreasonable to group consecutive chats from
the same party to form a ?alternating? structure
like BusTime dataset due to the asynchronism of
different states. For instance, the second block
of client inputs clearly comes from two different
states which should not be merged together.
We discard dialogues with fewer than 30 utter-
ances. We map named entities to their semantic
types, apply stemming, and remove stop words.
3
The corpus we use contains approximately 2, 000
dialogue sessions or 80, 000 conversation utter-
ances. It consists of 770, 000 tokens, with a a vo-
cabulary size of 6, 600.
3 Latent Structure in Dialogues
In this work, our goal is to infer latent structure
presented in task-oriented conversation. We as-
sume that the structure can be encoded in a prob-
abilistic state transition diagram, where the dia-
logue is in one state at each utterance, and states
have a causal effect on the words observed. We as-
sume the boundaries between utterances are given,
which is trivial in many corpora.
The simplest formulation we consider is an
HMM where each state contains a unigram lan-
guage model (LM), proposed by Chotimongkol
(2008) for task-oriented dialogue and originally
3
We used regular expression to map named entities, and
Porter stemmer in NLTK to stem all tokens.
developed for discourse analysis by Barzilay and
Lee (2004). We call it LM-HMM as in Figure 1(a).
For a corpus of M dialogues, the m-th dialogue
contains n utterances, each of which contains N
n
words (we omit index m from terms because it
will be clear from context). At n-th utterance,
we assume the dialogue is in some latent state s
n
.
Words in n-th utterance w
n,1
, . . . , w
n,N
n
are gen-
erated (independently) according to the LM. When
an utte ance is complete, the next state is drawn
according to HMM, i.e., P (s
?
|s).
While LM-HMM captures the basic intuition of
conversation structure, it assumes words are con-
ditioned only on state. Ritter et al (2010) extends
LM-HMM to allow words to be emitted from two
additional sources: the topic of current dialogue
?, or a background LM ? shared across all dia-
logues. A multinomial pi indicates the expected
fraction of words from these three sources. For
every word in an utterance, first draw a source in-
dicator r from pi, and then generate the word from
the corresponding source. We call it LM-HMMS
(Figure 1(b)). Ritter et al (2010) finds these al-
ternate sources are important in non-task-oriented
domains, where events are diffuse and fleeting.
For example, Twitter exchanges often focus on a
particular event (labeled X), and follow patterns
like ?saw X last night??, ?X was amazing?. Here
X appears throughout the dialogue but does not
help to distinguish conversational states in social
media. We also explore similar variants.
In this paper, these two models form our base-
lines. For all models, we use Markov chain Monte
Carlo (MCMC) inference (Neal, 2000) to find la-
tent variables that best fit observed data. We also
assume symmetric Dirichlet priors on all multino-
mial distributions and apply collapsed Gibbs sam-
pling. In the rest of this section, we present our
models and their inference algorithms in turn.
3.1 TM-HMM
Our approach is to modify the emission probabil-
ities of states to be distributions over topics rather
than distributions over words. In other words, in-
stead of generating words via a LM, we generate
words from a topic model (TM), where each state
maps to a mixture of topics. The key benefit of this
additional layer of abstraction is to enable states
to express higher-level concepts through pooling
of topics across states. For example, topics might
be inferred for content like ?bus-route? or ?lo-
38
s0
w
0,i
N
0
s
1
...
s
n
M
z
0,i
w
1,i
N
1
z
1,i
w
n,i
N
n
z
n,i
T
K
?t
? k
s
0
w
0,i
N
0
s
1
...
s
n
M
z
0,i
w
1,i
N
1
z
1,i
w
n,i
N
n
z
n,i
T
K
h
t
g
k
s
0
w
0,i
N
0
s
1
M
z
0,i
w
1,i
N
1
z
1,i
T
K
h
t
g
k
s
0
w
0,i
N
0
s
1
M
z
0,i
N
1
z
1,i
T
K
h
t
g
k
s
0
w
0,i
N
0
s
1
M
z
0,i
T
K
h
t
g
k
s
0
w
0,i
N
0
M
z
0,i
T
K
h
t
g
k
s
0
N
0
M
z
0,i
T
K
h
t
g
k
s
0
M
T
K
h
t
g
k
M
T
K
h
t
g
k
(a) TM-HMM
s
0
w
0,i
N
0
s
1
...
s
n
M
z
0,i
w
1,i
N
1
z
1,i
w
n,i
N
n
z
n,i
K
r
1,i
r
0,i
r
n,i
?m
?m
? k
s
0
w
0,i
N
0
s
1
...
s
n
M
z
0,i
w
1,i
N
1
z
1,i
w
n,i
N
n
z
n,i
K
r
1,i
r
0,i
r
n,i
h
m
u
m
g
k
s
0
w
0,i
N
0
s
1
M
z
0,i
N
1
z
1,i
K
r
1,i
r
0,i
h
m
u
m
g
k
s
0
w
0,i
N
0
s
1
M
z
0,i
w
1,i
N
1
z
1,i
K
r
1,i
r
0,i
h
m
u
m
g
k
s
0
w
0,i
N
0
s
1
M
z
0,i
N
1
z
1,i
K
r
0,i
h
m
u
m
g
k
s
0
w
0,i
N
0
s
1
M
z
0,i
K
r
0,i
h
m
u
m
g
k
s
0
w
0,i
N
0
M
z
0,i
K
r
0,i
h
m
u
m
g
k
s
0
N
0
M
z
0,i
K
r
0,i
h
m
u
m
g
k
s
0
N
0
M
z
0,i
K
h
m
u
m
g
k
s
0
M
K
h
m
u
m
g
k
M
K
h
m
u
m
g
k
M
K
g
k
(b) TM-HMMS
s
0
w
0,i
N
0
s
1
...
s
n
M
z
0,i
w
1,i
N
1
z
1,i
w
n,i
N
n
z
n,i
K
r
1,i
r
0,i
r
n,i
T
? m
? k
?t
s
0
w
0,i
N
0
s
1
...
s
n
M
z
0,i
w
1,i
N
1
z
1,i
w
n,i
N
n
z
n,i
K
r
1,i
r
0,i
r
n,i
T
h
m
g
k
u
t
s
0
w
0,i
N
0
s
1
M
z
0,i
w
1,i
N
1
z
1,i
K
r
1,i
r
0,i
T
h
m
g
k
u
t
s
0
w
0,i
N
0
s
1
M
z
0,i
N
1
z
1,i
K
r
1,i
r
0,i
T
h
m
g
k
u
t
s
0
w
0,i
N
0
s
1
M
z
0,i
N
1
z
1,i
K
r
0,i
T
h
m
g
k
u
t
s
0
w
0,i
N
0
s
1
M
z
0,i
K
r
0,i
T
h
m
g
k
u
t
s
0
w
0,i
N
0
M
z
0,i
K
r
0,i
T
h
m
g
k
u
t
s
0
N
0
M
z
0,i
K
r
0,i
T
h
m
g
k
u
t
s
0
N
0
M
z
0,i
K
T
h
m
g
k
u
t
s
0
M
K
T
h
m
g
k
u
t
M
K
T
h
m
g
k
u
t
M
K
T
g
k
u
t
(c) TM-HMMSS
Figure 2: Plate diagrams of proposed models. TM-HMM is an HMM with state-wise topic distributions.
TM-HMMS adds session-wise topic distribution and a source generator. TM-HMMSS adds a state-wise
source generator. Variable definitions are given in the text.
cations?; and other topics for dialogue acts, like
to ?ask? or ?confirm? information. States could
then be combinations of these, e.g., a state might
express ?ask bus route? or ?confirm location?.
This approach also decouples the number of top-
ics from the number of states. Throughout this pa-
per, we denote the number of topics as K and the
number of states as T . We index words, turns and
dialogues in the same ways as baseline models.
We develop three generative models. In the first
variant (TM-HMM, Figure 2(a)), we assume every
state s in HMM is as ociated with a distribution
over topics ?, and topics generate wordsw at each
utterance. The other two models allow words to
be generated from different sources (in addition to
states), akin to the LM-HMMS model.
TM-HMM generates a dialogue as following:
1: For each utterance n in that dialogue, sample
a state s
n
based on the previous state s
n?1
.
2: For each word in utterance n, first draw a
topic z from the state-specified distribution
over topics ?
s
n
conditioned on s
n
, then gener-
ate word w from the topic-specified distribu-
tion over vocabulary ?
z
based on z.
We assume ??s and ??s are drawn from corre-
sponding Dirichlet priors, as in LDA.
The posterior distributions of state assignment
s
n
and topic assignment z
n,i
are
p(s
n
|s
?n
, z,?,?) ? p(s
n
|s
?n
,?)
? p(z
n
|s, z
?n
,?), (1)
p(z
n,i
|s,w, z
?(n,i)
,?,?) ? p(z
n,i
|s, z
?(n,i)
,?)
? p(w
n,i
|s
n
,w
?(n,i)
, z,?),
where ?, ?, ? are symmetric Dirichlet priors on
state-wise topic distribution ?
t
?s, topic-wise word
distribution ?
t
?s and state transition multinomials,
respectively. All probabilities can be computed
using collapsed Gibbs sampler for LDA (Griffiths
and Steyvers, 2004) and HMM (Goldwater and
Griffiths, 2007). We iteratively sample all param-
eters until convergence.
3.2 TM-HMMS
TM-HMMS (Figure 2(b)) extends TM-HMM to al-
low words to be generated either from state LM
(as in LM-HMM), or a set of dialogue topics
(akin to LM-HMMS). Because task-oriented dia-
logues usually focus on a specific domain, a set
of words appears repeatedly throughout a given
dialogue. Therefore, the topic distribution is of-
ten stable throughout the entire dialogue, and
does not vary from turn to turn. For example,
in the troubleshooting domain, dialogues about
network connections, desktop productivity, and
anti-virus software could each map to different
session-wide topics. To express this, words in
the TM-HMMS model are generated either from
a dialogue-specific topic distribution, or from a
state-specific language model.
4
A distribution
over sources is sampled once at the beginning of
each dialogue and selects the expected fraction of
words generated from different sources.
The generative story for a dialogue session is:
1: At the beginning of each session, draw a dis-
tribution over topics ? and a distribution over
word sources ? .
2: For each utterance n in the conversation, draw
a state s
n
based on previous state s
n?1
.
3: For each word in utterance n, first choose a
word source r according to ? , and then de-
pending on r, generate a word w either from
the session-wide topic distribution ? or the
language model specified by the state s
n
.
4
Note that a TM-HMMS model with state-specific topic
models (instead of state-specific language models) would be
subsumed by TM-HMM, since one topic could be used as the
background topic in TM-HMMS.
39
Again, we impose Dirichlet priors on distributions
over topics ??s and distributions over words ??s
as in LDA. We also assume the distributions over
sources ? ?s are governed by a Beta distribution.
The session-wide topics is slightly different
from that used in LM-HMMS: LM-HMMS was de-
veloped for social chats on Twitter where topics
are very diffuse and unlikely to repeat; hence of-
ten unique to each dialogue. By contrast, our mod-
els are designed for task-oriented dialogues which
pertain to a given domain where topics are more
tightly clustered; thus, in TM-HMMS session-wide
topics are shared across the corpus.
The posterior distributions of state assignment
s
n
, word source r
n,i
and topic assignment z
n,i
are
p(s
n
|r, s
?n
,w,?,pi) ? p(s
n
|s
?n
,?)
? p(w
n
|r, s,pi),
p(r
n,i
|r
?(n,i)
, s,w,pi) ? p(r
n,i
|r
?(n,i)
,pi)
? p(w
n,i
|r, s,w
?(n,i)
, z,?), (2)
p(z
n,i
|r,w, z
?(n,i)
,?,?) ? p(z
n,i
|r, z
?(n,i)
,?)
? p(w
n,i
|r,w
?(n,i)
, z,?),
where pi is a symmetric Dirichlet prior on session-
wise word source distribution ?
m
?s, and other
symbols are defined above. All these probabilities
are Dirichlet-multinomial distributions and there-
fore can be computed efficiently.
3.3 TM-HMMSS
The TM-HMMSS (Figure 2(c)) model modifies
TM-HMMS to re-sample the distribution over
word sources ? at every utterance, instead of once
at the beginning of each session. This modifica-
tion allows the fraction of words drawn from the
session-wide topics to vary over the course of the
dialogue. This is attractive in task-oriented di-
alogue, where some sections of the dialogue al-
ways follow a similar script, regardless of session
topic?for example, the opening, closing, or ask-
ing the user if they will take a survey. To support
these patterns, TM-HMMSS conditions the source
generator distribution on the current state.
The generative story of TM-HMMSS is very
similar to TM-HMMS, except the distribution over
word sources ? ?s are sampled at every state. A
dialogue is generated as following:
1: For each session, draw a topic distribution ?.
2: For each utterance n in the conversation, draw
a state s
n
based on previous state s
n?1
, and
subsequently retrieve the state-specific distri-
bution over word sources ?
s
n
.
3: For each word in utterance n, first sample a
word source r according to ?
s
n
, and then de-
pending on r, generate a word w either from
the session-wide topic distribution ? or the
language model specified by the state s
n
.
As in TM-HMMS, we assume multinomial distri-
butions ??s and ??s are drawn from Dirichlet pri-
ors; and ? ?s are governed by Beta distributions.
The inference for TM-HMMSS is exactly same
as the inference for TM-HMMS, except the poste-
rior distributions over word source r
n,i
is now
p(r
n,i
|r
?(n,i)
, s,w,pi) ? p(r
n,i
|r
?(n,i)
, s
n
,pi)
? p(w
n,i
|r, s,w
?(n,i)
, z,?), (3)
where the first term is integrated over all sessions
and conditioned on the state assignment.
3.4 Supporting Multiple Parties
Since our primary focus is task-oriented dia-
logues between two parties, we assume every
word source is associated with two sets of LMs?
one for system/agent and another for user/client.
This configuration is similar to PolyLDA (Mimno
et al, 2009) or LinkLDA (Yano et al, 2009), such
that utterances from different parties are treated
as different languages or blog-post and comments
pairs. In this work, we implement all models un-
der this setting, but omit details in plate diagrams
for the sake of simplicity.
In settings where the agent and client always al-
ternate, each state emits both text before transi-
tioning to the next state. This is the case in the
BusTime dataset, where the spoken dialogue sys-
tem enforces strict turn-taking. In settings where
agents or client may produce more than one utter-
ance in a row, each state emits either agent text or
client text, then transitions to the next state. This
is the case in the TechSupport corpus, where either
conversant may send a message at any time.
3.5 Likelihood Estimation
To evaluate performance across different models,
we compute the likelihood on held-out test set.
For TM-HMM model, there are no local depen-
dencies, and we therefore compute the marginal
likelihood using the forward algorithm. However,
for TM-HMMS and TM-HMMSS models, the la-
tent topic distribution ? creates local dependen-
cies, rendering computation of marginal likeli-
40
hoods intractable. Hence, we use a Chib-style
estimator (Wallach et al, 2009). Although it is
computationally more expensive, it gives less bi-
ased approximation of marginal likelihood, even
for finite samples. This ensures likelihood mea-
surements are comparable across models.
4 Experiments
In this section, we examine the effectiveness of our
models. We first evaluate our models qualitatively
by exploring the inferred state diagram. We then
perform quantitative analysis with log likelihood
measurements and an ordering task on a held-out
test set. We train all models with 80% of the en-
tire dataset and use the rest for testing. We run
the Gibbs samplers for 1000 iterations and update
all hyper-parameters using slice sampling (Neal,
2003; Wallach, 2008) every 10 iterations. The
training likelihood suggest all models converge
within 500?800 iterations. For all Chib-style esti-
mators, we collect 100 samples along the Markov
chain to approximate the marginal likelihood.
4.1 Qualitative Evaluation
Figure 3 shows the state diagram for BusTime cor-
pus inferred by TM-HMM without any supervi-
sion.
5
Every dialogue is opened by asking the user
to say a bus route, or to say ?I?m not sure.? It then
transits to a state about location, e.g., origin and
destination. Both these two states may continue
to a confirmation step immediately after. After
verifying all the necessary information, the system
asks if the user wants ?the next few buses?.
6
Oth-
erwise, the system follows up with the user on the
particular date and time information. After system
reads out bus times, the user has options to ?re-
peat? or ask for subsequent schedules.
In addition, we also include the human-
annotated dialogue flow in Figure 4 for refer-
ence (Williams, 2012). It only illustrates the most
common design of system actions, without show-
ing edge cases. Comparing these two figures, the
dialogue flow inferred by our model along the
most probable path (highlighted in bold red in Fig-
ure 3) is consistent with underlying design. Fur-
thermore, our models are able to capture edge
cases?omitted for space?through a more gen-
eral and probabilistic fashion. In summary, our
5
Recall in BusTime, state transitions occur after each pair
of system/user utterances, so we display them synchronously.
6
The system was designed this way because most users
say ?yes? to this question, obviating the date and time.
models yield a very similar flowchart to the under-
lying design in a completely unsupervised way.
7
Figure 5 shows part of the flowchart for
the TechSupport corpus, generated by the TM-
HMMSS model.
8
A conversation usually starts
with a welcome message from a customer support
agent. Next, clients sometimes report a problem;
otherwise, the agent gathers the client?s identity.
After these preliminaries, the agent usually checks
the system version or platform settings. Then, in-
formation about the problem is exchanged, and a
cycle ensues where agents propose solutions, and
clients attempt them, reporting results. Usually,
a conversation loops among these states until ei-
ther the problem is resolved (as the case shown
in the figure) or the client is left with a reference
number for future follow-up (not shown due to
space limit). Although technical support is task-
oriented, the scope of possible issues is vast and
not prescribed. The table in Figure 5 lists the top
ranked words of selected topics?the categories
clients often report problems in. It illustrates that,
qualitatively, TM-HMMSS discovers both problem
categories and conversation structures on our data.
As one of the baseline model, we also include a
part of flowchart generated by LM-HMM model
with similar settings of T = 20 states. Illus-
trated by the highlighted states in 6, LM-HMM
model conflates interactions that commonly occur
at the beginning and end of a dialogue?i.e., ?ac-
knowledge agent? and ?resolve problem?, since
their underlying language models are likely to pro-
duce similar probability distributions over words.
By incorporating topic information, our proposed
models (e.g., TM-HMMSS in Figure 5) are able to
enforce the state transitions towards more frequent
flow patterns, which further helps to overcome the
weakness of language model.
4.2 Quantitative Evaluation
In this section, we evaluate our models using log
likelihood and an ordering task on a held-out test
set. Both evaluation metrics measure the predic-
tive power of a conversation model.
7
We considered various ways of making a quantitative
evaluation of the inferred state diagram, and proved difficult.
Rather than attempt to justify a particular sub-division of each
?design states?, we instead give several straightforward quan-
titative evaluations in the next section.
8
Recall in this corpus, state transitions occur after emit-
ting each agent or client utterances, which does not necessar-
ily alternate in a dialogue, so we display client request and
agent response separately.
41
state: ask for bus route
(route:0.14), (say:0.13), (<bus-route>:0.12), (not:0.10), 
(sure:0.10), (im:0.09), (a:0.08), (bus:0.07), (like:0.06), ...
e.g.: say a bus route like <bus-route> or say i am not sure
(<bus-route>:0.7), (the:0.07), (im:0.06), (not:
0.05), (sure:0.04), (route:0.02), (any:0.01), ...
e.g.: <bus-route>/im not sure
0.53
state: confirm low-confidence speech recognition results
(right:0.19), (is:0.19), (that:0.19), (<location>:0.12), (<bus-route>:
0.05), (i:0.04), (you:0.03), (said:0.03), (thought:0.03), (over:0.03), ...
e.g.: i thought you said (<bus-route>/<location>) is that right
(yes:0.45), (no:0.3), (yeah:0.12), (wrong:0.04), 
(correct:0.03), (back:0.02), (go:0.02), (nope:0.01), ...
e.g.: yes/no/yeah/wrong/correct/go back/nope
0.12
0.32
0.53
0.15
state: ask for locations
(you:0.1), (are:0.09), (where:0.08), (to:0.07), (say:0.06), (from:0.05), 
(leaving:0.05), (going:0.05), (<location>:0.05), (okay:0.04), ...
e.g.: (okay <location>) say where are you (going to/leaving from)
(<location>:0.84), (back:
0.05), (go:0.05), ...
e.g.: <location>
0.21
0.23
0.85
0.44
0.28
state: ask if user is traveling now
(say:0.8), (the:0.07), (you:0.07), (no:0.06), (yes:0.06), (do:
0.06), (want:0.06), (buses:0.05), (few:0.05), (next:0.04), ...
e.g.: do you want the next few buses say yes or no
(yes:0.5), (no:0.17), (yeah:0.16), (<bus-route>:
0.07), (back:0.04), (go:0.04), (nope:0.01), ...
e.g.: yes/no/yeah
0.31
state: read out bus timetables
(<location>:0.08), (at:0.05), (<time>:0.05), (next:0.05), (say:0.05), (from:0.04), (there:
0.04), (<bus-route>:0.04), (to:0.04), ...
e.g.: there is a <bus-route> from <location> to <location> at <time> say next or repeat
(next:0.4), (repeat:0.16), (over:0.11), (start:0.11), 
(previous:0.07), (go:0.06), (back:0.06), (goodbye:0.05), ...
e.g.: next/repeat/start over/previous
0.12
0.42
state: ask for date and time (optional)
(<time>:0.14), (<date>:0.1), (the:0.06), (or:0.05), (like:0.05), 
(say:0.05), (you:0.05), (want:0.05), (at:0.04), (depart:0.04), ...
e.g.: say the time you want to depart like <time>
(<time>:0.26), (<date>:0.14), (m:0.11), 
(depart:0.07), (a:0.07), (at:0.07), (by:0.03), ...
e.g.: depart (at/by) <time> a m <date>
0.55
Start	 ?
I	 ?heard	 ?61C,	 ?is	 ?that	 ?right?	 ?
Downtown,	 ?is	 ?that	 ?correct?	 ?
Did	 ?you	 ?just	 ?say	 ?Norwood?	 ?
Say	 ?just	 ?the	 ?day	 ?you	 ?want.	 ?
Say	 ?just	 ?the	 ??me	 ?you	 ?want.	 ?
I'm	 ?sorry,	 ?I	 ?can't	 ?find	 ?any	 ?bus	 ?
at	 ?all	 ?that	 ?run	 ?from	 ?Milton	 ?to	 ?
Norwell.	 ?I	 ?checked	 ?route	 ?61C	 ?
and	 ?I	 ?also	 ?checked	 ?all	 ?the	 ?
other	 ?bus	 ?routes	 ?I	 ?know	 ?too.	 ?
Repeat,	 ?next,	 ?previous	 ?
At	 ?11:45	 ?PM	 ?today,	 ?
there	 ?is	 ?a	 ?61	 ?C	 ?from	 ?
5th	 ?Ave	 ?and	 ?Main	 ?St	 ?
Canton,	 ?arriving	 ?2nd	 ?St	 ?
and	 ?Grant	 ?Ave	 ?in	 ?
Norwood	 ?at	 ?12:34	 ?AM.	 ?
Say	 ?a	 ?bus	 ?route,	 ?or	 ?
say	 ?I?m	 ?not	 ?sure.	 ?
Where	 ?are	 ?you	 ?leaving	 ?
from?	 ?(query	 ?database)	 ?
Where	 ?are	 ?you	 ?going	 ?
to?	 ?(query	 ?database)	 ?
Do	 ?you	 ?want	 ??mes	 ?for	 ?
the	 ?next	 ?few	 ?buses?	 ?
(query	 ?database)	 ?
Figure 3: (Upper) Part of the flowchart inferred on Bus-
Time, by TM-HMM model with K = 10 topics and
T = 10 states. The most probable path is highlighted,
which is consistent with the underlying design (Figure 4).
Cyan blocks are system actions and yellow blocks are
user responses. In every block, the upper cell shows the
top ranked words marginalized over all topics and the
lower cell shows some examples of that state. Transition
probability cut-off is 0.1. States are labelled manually.
Figure 4: (Left) Hand-crafted reference flowchart for
BusTime (Williams, 2012). Only the most common di-
alogue flows are displayed. System prompts shown are
example paraphrases. Edge cases are not included.
Log Likelihood The likelihood metric measures
the probability of generating the test set under a
specified model. As shown in Figure 7, our models
yield as good or better likelihood than LM-HMM
and LM-HMMS models on both datasets under all
settings. For our proposed models, TM-HMMS
and TM-HMMSS perform better than TM-HMM
on TechSupport, but not necessarily on BusTime.
In addition, we notice that the marginal benefit of
TM-HMMSS over TM-HMM is greater on Tech-
Support dataset, where each dialogue focuses on
one of many possible tasks. This coincides with
our belief that topics are more conversation de-
pendent and shared across the entire corpus in cus-
tomer support data?i.e., different clients in differ-
ent sessions might ask about similar issues.
Ordering Test Ritter et al (2010) proposes an
evaluation based on rank correlation coefficient,
which measures the degree of similarity between
any two orderings over sequential data. They use
Kendall?s ? as evaluation metric, which is based
on the agreement between pairwise orderings of
two sequences (Kendall, 1938). It ranges from ?1
to +1, where +1 indicates an identical ordering
and ?1 indicates a reverse ordering. The idea is
to generate all permutations of the utterances in
a dialogue (including true ordering), and compute
the log likelihood for each under the model. Then,
Kendall?s ? is computed between the most proba-
ble permutation and true ordering. The result is the
average of ? values for all dialogues in test corpus.
Ritter et al (2010) limits their dataset by choos-
ing Twitter dialogues containing 3 to 6 posts (ut-
terances), making it tractable to enumerate all per-
mutations. However, our datasets are much larger,
and enumerating all possible permutations of dia-
logues with more than 20 or 30 utterances is infea-
sible. Instead, we incrementally build up the per-
mutation set by adding one random permutation at
a time, and taking the most probable permutation
after each addition. If this process were continued
(intractably!) until all permutations are enumer-
ated, the true value of Kendall?s ? test would be
reached. In practice, the value appears to plateau
after a few dozen measurements.
We present our results in Figure 8. Our mod-
els consistently perform as good or better than
42
Agent: conversation opening + identity check
help, answer, desk, microsoft, may, 
<agent>, welcom, name, number, phone, ...
e.g.: welcome to microsoft answer desk, i'm 
<agent>, how can i help you, may i have 
your name?
Client: report problem
tri, get, comput, cant, window, message, 
error, problem, instal, say, ...
e.g.: get problem in windows, cant install 
on computer, it says error message
Agent: conversation closure
thank, answer, microsoft, desk, <client>, 
contact, help, chat, day, welcom, ...
e.g.: thank you for contacting microsoft answer 
desk, you are welcome, have a nice day
Agent: acknowledge identity
thank, minut, pleas, let, <client>, check, 
give, moment, ok, wait, ...
e.g.: thank you, <client>, please give me 
a moment, wait for a minute, let me check
Agent: system check 
window, comput, instal, 7, use, 8, 
system, version, may, oper, ...
e.g.: may i know what version is 
operating system you used? windows 7?
Client: system verification
ok, ye(s), sure, pleas, thank, k, 
<prodkey>, one, problem, fine, ...
e.g.: ok, thanks, sure, <prodkey>, 
one problem
Agent: acknowledge problem
error, messag, see, issu, sorri, help, get, 
thank, <client>, oh, ...
e.g.: sorry to hear that, thanks for error 
message, i see, let me help you on issue
Agent: troubleshoot attempt
click, <href>, pleas, link, code, let, go, 
download, run, ok, ...
e.g.: please click <href> and go download 
the code, let it run and see it is ok 
Client: troubleshoot acknowledgement
ok, link, click, ye(s), code, dont, tri, 
download, get, say, ...
e.g.: ok, i am trying to download the code
Client: identity verification
<email>, <phoneno>, <client>, ye(s), 
number, phone, email, name, sure, call, ...
e.g.: yes, my name is <client>
sure, <client>, <phoneno>, <email>
Agent: troubleshoot attempt
instal, comput, program, tri, issu, 
system, file, work, run, see, ...
e.g.: try to install file or run program 
and see the issue goes away
Client: resolved problem
thank, ok, help, great, good, much, 
<agent>, ye(s), day, bye, ...
e.g.: yes, thank you <agent> so much 
for your great help, good day, bye
0.240581
0.20916
0.207256
0.145682
0.131558
0.122512
0.117276
0.108547
0.1291865
0.0798235
0.0764909
0.0893519
0.0801476
0.085454
0.077694
0.088172
0.0728498
0.083912
0.0917323
0.0888547
Agent: conversation opening + identity check
help, answer, desk, microsoft, may, #agent#, welcom, 
name, number, phone, ...
e.g.: welcome to microsoft answer desk, i'm #agent#, 
how can i help you, may i have your name?
Client: report problem
tri, get, comput, cant, window, message, error, 
problem, instal, say, ...
e.g.: i get this problem in windows, cant install 
on my computer, it says this error message
Agent: conversation closure
thank, answer, microsoft, desk, #client#, 
contact, help, chat, day, welcom, ...
e.g.: thank you for contacting microsoft answer 
desk, you are welcome, have a nice day
Agent: acknowledge identity
thank, minut, pleas, let, #client#, check, 
give, moment, ok, wait, ...
e.g.: thank you, #client#, please give me a 
moment, wait for a minute, let me check
Agent: system check 
window, comput, instal, 7, use, 8, 
system, version, may, oper, ...
e.g.: may i know what version is 
operating system you used? windows 7?
Client: system verification
ok, ye(s), sure, pleas, thank, k, #prodkey#, 
one, problem, fine, ...
e.g.: ok, thanks, sure, #prodkey#, one problem
Agent: acknowledge problem
error, messag, see, issu, sorri, help, get, 
thank, #client#, oh, ...
e.g.: sorry to hear that, thanks for error 
message, i see, let me help you on issue
Agent: troubleshoot attempt
click, #href#, pleas, link, code, let, go, 
download, run, ok, ...
e.g.: please click #href# and go download 
the code, let it run and see it is ok 
Client: troubleshoot acknowledgement
ok, link, click, ye(s), code, 
dont, tri, download, get, say, ...
e.g.: ok, i am trying to 
download the code, yes
Client: identity verification
#email#, #phoneno#, #client#, ye(s), 
number, phone, email, name, sure, call, ...
e.g.: yes, my name is #client#
sure, #client#, #phoneno#, #email#
Agent: troubleshoot attempt
instal, comput, program, tri, issu, 
system, file, work, run, see, ...
e.g.: try to install file or run program 
and see the issue goes away
Client: resolved problem
thank, ok, help, great, good, much, 
#agent#, ye(s), day, bye, ...
e.g.: yes, thank you #agent# so much 
for your great help, good day, bye
0.0240581
0.020916
0.0207256
0.0145682
0.0131558
0.0122512
0.0117276
0.0108547
0.00891865
0.00798235
0.00764909
0.00593519
0.00801476
0.00455454
0.00377694
0.00568172
0.00328498
0.0053912
0.00617323
0.00588547
Agent: conversation opening + identity check
help, answer, desk, may, <agent-name>, 
welcom, name, number, phone, ...
e.g.: welcome to answer desk, i'm <agent-
name>, how can i help you, may i have 
your name?
Client: report problem
tri, get, comput, cant, window, message, 
error, problem, instal, say, ...
e.g.: get problem in windows, cant install 
on computer, it says error message
Agent: conversation closure
thank, answer, desk, <client-name>, contact, 
help, chat, day, welcom, ...
e.g.: thank you for contacting answer desk, 
you are welcome, have a nice day
Agent: acknowledge identity
thank, minut, pleas, let, <client-name>, 
check, give, moment, ok, wait, ...
e.g.: thank you, <client-name>, please 
give me a moment, let me check
Agent: system check 
window, comput, instal, 7, use, 8, 
system, version, may, oper, ...
e.g.: may i know what version is 
operating system you used? windows 7?
Client: system verification
ok, ye(s), sure, pleas, thank, k, 
<prodkey>, one, problem, fine, ...
e.g.: ok, thanks, sure, <prodkey>, 
one problem
Agent: acknowledge problem
error, messag, see, issu, sorri, help, get, 
thank, <client-name>, oh, ...
e.g.: sorry to hear that, thanks for error 
message, i see, let me help you on issue
Agent: troubleshoot attempt
click, <href>, pleas, link, code, let, go, 
download, run, ok, ...
e.g.: please click <href> and go download 
the code, let it run and see it is ok 
Client: troubleshoot acknowledgement
ok, link, click, ye(s), code, dont, tri, 
download, get, say, ...
e.g.: ok, i am trying to download the code
Client: identity verification
<email>, <phone>, <client-name>, ye(s), 
number, phone, email, name, sure, call, ...
e.g.: yes, my name is <client-name>
sure, <client-name>, <phone>, <email>
Agent: troubleshoot attempt
instal, comput, program, tri, issu, 
system, file, work, run, see, ...
e.g.: try to install file or run program 
and see the issue goes away
Client: resolved problem
thank, ok, help, great, good, much, 
<agent-name>, ye(s), day, bye, ...
e.g.: great, thanks <agent-name> so 
much for your help, good day, bye
0.24
0.21
0.21
0.15
0.13
0.12
0.12
0.11
0.13
0.08
0.08
0.09
0.08
0.09
0.08
0.09
0.07
0.08
0.09
0.09
Agent: conversation opening + identity check
help, answer, desk, microsoft, may, <agent-
name>, welcom, name, number, phone, ...
e.g.: welcome to microsoft answer desk, i'm 
<agent-name>, how can i help you, may i 
have your name?
Client: report problem
tri, get, comput, cant, window, message, 
error, problem, instal, say, ...
e.g.: get problem in windows, cant install 
on computer, it says error message
Agent: conversation closure
thank, answer, microsoft, desk, <client-name>, 
contact, help, chat, day, welcom, ...
e.g.: thank you for contacting microsoft answer 
desk, you are welcome, have a nice day
Agent: acknowledge identity
thank, minut, pleas, let, <client-name>, 
check, give, moment, ok, wait, ...
e.g.: thank you, <client-name>, please 
give me a moment, let me check
Agent: system check 
window, comput, instal, 7, use, 8, 
system, version, may, oper, ...
e.g.: may i know what version is 
operating system you used? windows 7?
Client: system verification
ok, ye(s), sure, pleas, thank, k, 
<prodkey>, one, problem, fine, ...
e.g.: ok, thanks, sure, <prodkey>, 
one problem
Agent: acknowledge problem
error, messag, see, issu, sorri, help, get, 
thank, <client-name>, oh, ...
e.g.: sorry to hear that, thanks for error 
message, i see, let me help you on issue
Agent: troubleshoot attempt
click, <href>, pleas, link, code, let, go, 
download, run, ok, ...
e.g.: please click <href> and go download 
the code, let it run and see it is ok 
Client: troubleshoot acknowledgement
ok, link, click, ye(s), code, dont, tri, 
download, get, say, ...
e.g.: ok, i am trying to download the code
Client: identity verification
<email>, <phone>, <client-name>, ye(s), 
number, phone, email, name, sure, call, ...
e.g.: yes, my name is <client-name>
sure, <client-name>, <phone>, <email>
Agent: troubleshoot attempt
instal, comput, program, tri, issu, 
system, file, work, run, see, ...
e.g.: try to install file or run program 
and see the issue goes away
Client: resolved problem
thank, ok, help, great, good, much, 
<agent-name>, ye(s), day, bye, ...
e.g.: great, thanks <agent-name> so 
much for your help, good day, bye
0.24
0.21
0.21
0.15
0.13
0.12
0.12
0.11
0.13
0.08
0.08
0.09
0.08
0.09
0.08
0.09
0.07
0.08
0.09
0.09
Topic Top Ranked Words
p
u
r
c
h
a
s
e
microsoft, store, purchas, able, get,
sir, order, site, mr, contact, mac, . . .
b
r
o
w
s
e
r
internet, explor, browser, ie, open,
websit, googl, download, click,
chrome, . . .
b
a
c
k
u
p
file, restor, system, comput, back,
folder, creat, option, dont, delet, . . .
b
o
o
t
comput, boot, mode, option, disc,
safe, recoveri, repair, back, clean,
cd, disk, . . .
u
p
d
a
t
e
updat, window, servic, instal, pack,
run, comput, download, check,
restart, inform, system, error, fix, . . .
n
e
t
w
o
r
k
connect, internet, printer, comput,
network, pc, print, access, wireless,
hp, cable, adapt, router, speed, . . .
a
n
t
i
-
v
i
r
u
s viru, scan, comput, remov, secur,
run, system, anti, essenti, infect, de-
fend, softwar, program, protect, an-
tiviru, malwar, . . .
h
a
r
d
w
a
r
e
driver, devic, drive, dvd, cd, hard-
war, issu, model, laptop, plug, soft-
ware, usb, . . .
w
i
n
d
o
w
s window, upgrad, 8, download, 7, in-
stal, bit, vista, pro, system, . . .
o
f
fi
c
e
offic, 2010, word, microsoft, home,
excel, version, 2007, student, docu-
ment, trial, 2013, . . .
o
u
t
l
o
o
k
outlook, account, email, mail, mi-
crosoft, com, live, password, profil,
contact, creat, server, access, . . .
l
i
c
e
n
s
e
key, product, activ, purchas, licens,
valid, verifi, id, disc, pro, grenuin,
. . .
f
a
c
i
l
i
t
y
window, 8, comput, instal, manufac-
tur, system, oem, 7, pc, hp, . . .
Figure 5: Part of flowchart (left) and topic table (right) on TechSupport dataset, generated by TM-HMMSS
model under settings ofK = 20 topics and T = 20 states. The topic table lists top ranked words in issues
discussed in the chats. Cyan blocks are system actions and yellow blocks are user responses. In every
block, the upper cell shows top ranked words, and the lower cell shows example string patterns of that
state. Transition probability cut-off is 0.05. States and topics are labelled manually.
Agent: conversation opening + identity check
answer, desk, help, <agent-name>, 
welcom, today, may, name, number, ...
e.g.: welcome to answer desk, i'm <agent-
name>, how can i help you, may i have 
your name, case/phone number, account?
Client: acknowledge agent / resolved problem
thank, ok, help, much, good, great, 
<agent-name>, day, appreci, bye, ...
e.g.: ok, thanks, great, <agent-name> 
appreciate your help, good day, bye
Agent: conversation closure
answer, desk, thank, contact, day, chat, 
great, session, com, help, ...
e.g.: thank you for contacting answer desk, 
you are welcome, have a nice day
Agent: acknowledge problem
issu, sorri, call, help, number, suport, 
concern, <client-name>, <phone>, best, ...
e.g.: sorry to hear that, let me help with 
your concern, <client-name>
Client: confirm identity
call, number, phone, case, <time>, would, 
<agent-name>, pleas, <phone>, time, ...
e.g.: <agent-time>, my phone number is 
<phone>. would you pleas call number...
Agent: conversation closure
anyth, els, welcom, help, <client-name>, 
today, assist, question, would, answer, ...
e.g.: you are welcome, anything else today 
i would help/assist you, <client-name>?
Agent: acknowledge identity
give, minut, pleas, check, let, 
thank, moment, 3, one, 5, ...
e.g.: thanks, one moment please, 
give me 3 minutes, let me check
Client: report problem
updat, window, install, <agent-name>, hello, 
error, get, problem, download, message, ...
e.g.: hello, <agent-name>, i get problem/error 
when install/update/download in windows
0.08
0.1
0.08
0.07
0.14
0.05
0.07
0.06
0.05
0.05
0.08
0.06
state 0 0
(0, answer) 0.14307
(0, desk) 0.140908
(0, help) 0.139542
(0, #agentname#) 0.122925
(0, microsoft) 0.121104
(0, welcom) 0.119283
(0, today) 0.0638546
(0, may) 0.0453027
(0, name) 0.0382462
(0, hello) 0.023564
state 0 1
(0, offic) 0.129144
(0, instal) 0.0691015
(0, 2013) 0.0479101
(0, purchas) 0.0395219
(0, 365) 0.0366522
(0, home) 0.0362107
(0, product) 0.0322373
(0, download) 0.0280432
(0, key) 0.0260565
(0, 2010) 0.024732
state 1 0
(1, offic) 0.211557
(1, instal) 0.0920019
(1, 2013) 0.057979
(1, 365) 0.0517066
(1, version) 0.0410626
(1, purchas) 0.028898
(1, home) 0.026237
(1, use) 0.0245264
(1, 2010) 0.0222455
(1, product) 0.0188242
state 1 1
(1, updat) 0.117013
(1, window) 0.0831741
(1, instal) 0.0484751
(1, #agentname#) 0.0456075
(1, hello) 0.0335632
(1, error) 0.0304088
(1, get) 0.0246734
(1, problem) 0.0212322
(1, download) 0.0209454
(1, messag) 0.0195116
state 3 0
(3, give) 0.0975515
(3, minut) 0.0942818
(3, pleas) 0.0906489
(3, check) 0.0884691
(3, let) 0.0703043
(3, thank) 0.0675796
(3, moment) 0.0523213
(3, 3) 0.0485067
(3, one) 0.0308869
(3, 5) 0.0305236
state 3 1
(3, window) 0.129853
(3, 7) 0.0926994
(3, home) 0.074722
(3, bit) 0.0595411
(3, premium) 0.0479556
(3, 64) 0.0463576
(3, vista) 0.0419631
(3, instal) 0.0351717
(3, servic) 0.0323752
(3, pack) 0.0303777
state 4 0
(4, support) 0.0786022
(4, premium) 0.0519191
(4, servic) 0.0504038
(4, #dollaramt#) 0.0501403
(4, issu) 0.0425636
(4, softwar) 0.0376882
(4, warranti) 0.035514
(4, day) 0.0312315
(4, 30) 0.0210854
(4, fix) 0.0206901
state 4 1
(4, pay) 0.0413008
(4, #dollaramt#) 0.0393345
(4, dont) 0.0325617
(4, fix) 0.0310324
(4, support) 0.0275368
(4, much) 0.0231672
(4, ok) 0.0216379
(4, cost) 0.0214194
(4, issu) 0.0192346
(4, money) 0.0192346
state 5 0
(5, thank) 0.313335
(5, #clientname#) 0.149405
(5, inform) 0.0777987
(5, wait) 0.0738004
(5, much) 0.0363618
(5, correct) 0.0243669
(5, patienc) 0.021459
(5, patient) 0.0200051
(5, #email#) 0.0156433
(5, card) 0.0138259
state 6 0
(6, comput) 0.0520198
(6, instal) 0.0475635
(6, system) 0.038489
(6, file) 0.0370306
(6, viru) 0.035248
(6, updat) 0.0345188
(6, program) 0.0290093
(6, run) 0.0215552
(6, tri) 0.0201778
(6, caus) 0.0179091
state 6 1
(6, window) 0.146467
(6, 8) 0.0713756
(6, instal) 0.0647403
(6, 7) 0.0538974
(6, comput) 0.0349627
(6, upgrad) 0.0216923
(6, laptop) 0.0199121
(6, new) 0.0189411
(6, ye) 0.0179701
(6, use) 0.0178082
state 7 0
(7, answer) 0.0739787
(7, microsoft) 0.0701565
(7, desk) 0.0603621
(7, thank) 0.0573362
(7, contact) 0.042764
(7, day) 0.0422066
(7, chat) 0.0359159
(7, great) 0.0321733
(7, session) 0.0280325
(7, com) 0.0226177
state 7 1
(7, call) 0.0779531
(7, number) 0.0643551
(7, phone) 0.0394806
(7, case) 0.0305258
(7, #time#) 0.0232293
(7, would) 0.0222343
(7, #agentname#) 0.0219027
(7, pleas) 0.021571
(7, #phoneno#) 0.0199127
(7, time) 0.0169278
state 8 0
(8, click) 0.105887
(8, right) 0.0329512
(8, pleas) 0.0285344
(8, start) 0.0279375
(8, type) 0.0272213
(8, open) 0.0236401
(8, press) 0.0205365
(8, window) 0.020059
(8, see) 0.0197009
(8, option) 0.0193427
state 8 1
(8, ok) 0.0348863
(8, click) 0.0326788
(8, internet) 0.0309129
(8, screen) 0.0262772
(8, open) 0.0238491
(8, see) 0.0236283
(8, dont) 0.0203172
(8, right) 0.0200964
(8, say) 0.0200964
(8, window) 0.0194342
state 9 1
(9, #email#) 0.244926
(9, #phoneno#) 0.240782
(9, #clientname#) 0.144641
(9, phone) 0.0414562
(9, number) 0.0298531
(9, ye) 0.0277811
(9, email) 0.0273667
(9, name) 0.0219795
(9, 4) 0.0136915
(9, cell) 0.0124484
state 10 0
(10, window) 0.155062
(10, instal) 0.0641206
(10, comput) 0.0618775
(10, 8) 0.0571107
(10, 7) 0.0501943
(10, system) 0.0422498
(10, use) 0.0337444
(10, oper) 0.0298189
(10, version) 0.0250522
(10, manufactur) 0.0186031
state 10 1
(10, updat) 0.0326156
(10, instal) 0.0297734
(10, comput) 0.0230421
(10, problem) 0.0224438
(10, ago) 0.0221446
(10, program) 0.0218454
(10, system) 0.0210975
(10, dont) 0.0209479
(10, restor) 0.0190033
(10, fix) 0.0185545
state 11 0
(11, number) 0.13071
(11, phone) 0.0902297
(11, email) 0.0720301
(11, may) 0.0627649
(11, case) 0.0584632
(11, address) 0.0581323
(11, pleas) 0.0576911
(11, name) 0.05361
(11, chat) 0.030447
(11, disconnect) 0.0212921
state 12 0
(12, issu) 0.0431294
(12, sorri) 0.0419008
(12, call) 0.0310888
(12, help) 0.0286316
(12, number) 0.0264201
(12, support) 0.0243314
(12, concern) 0.0229799
(12, #clientname#) 0.0218741
(12, #phoneno#) 0.0202769
(12, best) 0.0194169
state 12 1
(12, thank) 0.282447
(12, ok) 0.0864091
(12, help) 0.069528
(12, much) 0.0459309
(12, good) 0.0419375
(12, great) 0.0410299
(12, #agentname#) 0.0401223
(12, day) 0.0217892
(12, appreci) 0.0192479
(12, bye) 0.0179773
state 14 0
(14, comput) 0.0877508
(14, access) 0.061482
(14, remot) 0.0601575
(14, ok) 0.0432704
(14, let) 0.0359858
(14, connect) 0.0334472
(14, tri) 0.0309086
(14, issu) 0.0302464
(14, restart) 0.0281493
(14, check) 0.0224099
state 14 1
(14, ok) 0.436864
(14, ye) 0.106903
(14, thank) 0.101863
(14, pleas) 0.0313091
(14, sure) 0.0262695
(14, great) 0.0199037
(14, wait) 0.0191079
(14, oh) 0.0164555
(14, k) 0.015925
(14, let) 0.0148641
state 15 0
(15, #href#) 0.129779
(15, click) 0.118174
(15, link) 0.0862138
(15, pleas) 0.0784769
(15, code) 0.055727
(15, download) 0.0259771
(15, run) 0.0230297
(15, accept) 0.019806
(15, open) 0.0179639
(15, remot) 0.0166745
state 15 1
(15, ok) 0.100091
(15, link) 0.0744436
(15, click) 0.0557658
(15, download) 0.0376455
(15, code) 0.0292823
(15, dont) 0.0281672
(15, copi) 0.0245431
(15, #href#) 0.0242643
(15, page) 0.0237068
(15, past) 0.0237068
state 16 0
(16, anyth) 0.1478
(16, els) 0.146359
(16, welcom) 0.0844204
(16, help) 0.0792348
(16, #clientname#) 0.0754896
(16, today) 0.066847
(16, assist) 0.0518665
(16, question) 0.0233458
(16, would) 0.0193126
(16, answer) 0.0161437
state 19 0
(19, issu) 0.0736645
(19, troubleshoot) 0.0553393
(19, support) 0.0440807
(19, step) 0.0358164
(19, link) 0.0340198
(19, fix) 0.0339001
(19, help) 0.0325826
(19, resolv) 0.0285103
(19, advanc) 0.0240787
(19, option) 0.0230008
0.018599
0.0175358
0.0162186
0.015441
0.0149967
0.0147745
0.0147745
0.0143936
0.0138858
0.0127432
0.0122989
0.0113943
0.0107436
0.0106643
0.00910909
0.00833148
0.00761735
0.0072841
0.00674453
0.0063954
0.00631606
0.00625258
0.00609388
0.00590345
0.00579236
0.00576062
0.00569715
0.00568128
0.00518932
0.00509411
0.0050465
0.0048878
0.00453867
0.00439585
0.00412607
0.00407846
0.00403085
0.00403085
0.00382455
0.00377694
0.00366585
0.0035389
0.0035389
0.0035389
0.00349129
0.00342781
0.00326911
0.00326911
0.00325325
0.00318977
0.0031739
0.00307868
0.00299933
0.00296759
0.00284064
0.00277716
0.00276129
0.00266607
0.00266607
0.0026502
0.00261847
0.00255499
0.00255499
0.00250738
Figure 6: Part of flowchart on Tech-
Support dataset, generated by LM-HMM
model with T = 20 states. Cyan blocks
are system actions and yellow blocks are
user responses. In very block, the upper
cell shows the top ranked words, and the
lower cell shows example word sequences
or string patterns of that state. Transition
probability cut-off is 0.05. States are la-
belled manually. A poorly-inferred state
is highlighted, which seems to confla e
the ?acknowledge agent? and ?resolve
problem? states, a d TM-HMMSS model
has properly disentangled (Figure 5).
the baseline models. For BusTime data, all
models perform relatively well except LM-HMM
which only indicates weak correlations. TM-
HMM out-performs all other models under all set-
tings. This is also true for TechSupport dataset.
LM-HMMS, TM-HMMS and TM-HMMSS mod-
els perform considerably well on BusTime, but
not on TechSupport data. These three models al-
low words to be generated from additional sources
other than states. Although this improves log like-
lihood, it is possible these models encode less in-
formation about the state sequences, at least in
the more diffuse TechSupport data. In summary,
under both quantitative evaluation measures, our
models advance state-of-the-art, however which of
our models is best depends on the application.
43
K10 K20 K30
150
000200
000250
000
150
000200
000250
000
150
000200
000250
000
T10
T20
T30
LM?
HM
M
LM?
HM
MS
TM?
HM
M
TM?
HM
MS
TM?
HM
MS
S
LM?
HM
M
LM?
HM
MS
TM?
HM
M
TM?
HM
MS
TM?
HM
MS
S
LM?
HM
M
LM?
HM
MS
TM?
HM
M
TM?
HM
MS
TM?
HM
MS
S
model
ne
ga
tive
 log
 lik
elih
oo
d
K10 K20 K30
6e+
057e+
058e+
05
6e+
057e+
058e+
05
6e+
057e+
058e+
05
T10
T20
T30
LM?
HM
M
LM?
HM
MS
TM?
HM
M
TM?
HM
MS
TM?
HM
MS
S
LM?
HM
M
LM?
HM
MS
TM?
HM
M
TM?
HM
MS
TM?
HM
MS
S
LM?
HM
M
LM?
HM
MS
TM?
HM
M
TM?
HM
MS
TM?
HM
MS
S
model
ne
ga
tive
 log
 lik
elih
oo
d
Figure 7: Negative log likelihood on BusTime (upper) and TechSupport (lower) datasets (smaller is better)
under different settings of topics K and states T .
K10 K20 K30
0.00
.51.
0
0.00
.51.
0
0.00
.51.
0
T10
T20
T30
0 25 50 75 100 0 25 50 75 100 0 25 50 75 100# of random permutations
av
era
ge 
ken
dal
l's t
au
model LM?HMM LM?HMMS TM?HMM TM?HMMS TM?HMMSS
K10 K20 K30
0.00
.51.
0
0.00
.51.
0
0.00
.51.
0
T10
T20
T30
0 25 50 75 100 0 25 50 75 100 0 25 50 75 100# of random permutations
av
era
ge 
ken
dal
l's t
au
model LM?HMM LM?HMMS TM?HMM TM?HMMS TM?HMMSS
Figure 8: Average Kendall?s ? measure on BusTime (upper) and TechSupport (lower) datasets (larger is
better) against number of random permutations, under various settings of topics K and states T .
5 Conclusion and Future Work
We have presented three new unsupervised mod-
els to discover latent structures in task-oriented
dialogues. We evaluated on two very different
corpora?logs from spoken, human-computer dia-
logues about bus time, and logs of textual, human-
human dialogues about technical support. We
have shown our models yield superior perfor-
mance both qualitatively and quantitatively.
One possible avenue for future work is scala-
bility. Parallelization (Asuncion et al, 2012) or
online learning (Doucet et al, 2001) could signif-
icantly speed up inference. In addition to MCMC,
another class of inference method is variational
Bayesian analysis (Blei et al, 2003; Beal, 2003),
which is inherently easier to distribute (Zhai et al,
2012) and online update (Hoffman et al, 2010).
Acknowledgments
We would like to thank anonymous reviewers and
Jordan Boyd-Graber for their valuable comments.
We are also grateful to Alan Ritter and Bill Dolan
for their helpful discussions; and Kai (Anthony)
Lui for providing TechSupport dataset.
44
References
Arthur Asuncion, Padhraic Smyth, Max Welling,
David Newman, Ian Porteous, and Scott Triglia,
2012. Distributed Gibbs sampling for latent vari-
able models.
Satanjeev Banerjee and Alexander I Rudnicky. 2006.
A texttiling based approach to topic boundary detec-
tion in meetings. In INTERSPEECH.
Srinivas Bangalore, Giuseppe Di Fabbrizio, and
Amanda Stent. 2006. Learning the structure of task-
driven human-human dialogs. In ACL, Stroudsburg,
PA, USA.
Regina Barzilay and Lillian Lee. 2004. Catching the
drift: Probabilistic content models, with applications
to generation and summarization. In NAACL, pages
113?120.
Matthew J. Beal. 2003. Variational Algorithms for Ap-
proximate Bayesian Inference. Ph.D. thesis.
Alan W Black, Susanne Burger, Alistair Conkie,
Helen Hastie, Simon Keizer, Nicolas Merigaud,
Gabriel Parent, Gabriel Schubiner, Blaise Thomson,
D. Williams, Kai Yu, Steve Young, and Maxine Es-
kenazi. 2010. Spoken dialog challenge 2010: Com-
parison of live and control test results. In SIGDIAL.
David M. Blei, Andrew Ng, and Michael Jordan. 2003.
Latent Dirichlet alocation. JMLR.
Ananlada Chotimongkol. 2008. Learning the Struc-
ture of Task-oriented Conversations from the Corpus
of In-domain Dialogs. Ph.D. thesis.
Nigel Crook, Ramn Granell, and Stephen G. Pulman.
2009. Unsupervised classification of dialogue acts
using a dirichlet process mixture model. In SIG-
DIAL.
Hal Daum?e III and Daniel Marcu. 2006. Bayesian
query-focused summarization. In ACL-44: Pro-
ceedings of the 21st International Conference on
Computational Linguistics and the 44th annual
meeting of the Association for Computational Lin-
guistics, pages 305?312, Morristown, NJ, USA. As-
sociation for Computational Linguistics.
David DeVault, Kallirroi Georgila, Ron Artstein, Fab-
rizio Morbini, David Traum, Stefan Scherer, Albert
Rizzo, and Louis-Philippe Morency. 2013. Verbal
indicators of psychological distress in interactive di-
alogue with a virtual human. In SIGDIAL.
Arnaud Doucet, Nando De Freitas, and Neil Gordon,
editors. 2001. Sequential Monte Carlo methods in
practice. Springer Texts in Statistics.
Michel Galley, Kathleen McKeown, Eric Fosler-
Lussier, and Hongyan Jing. 2003. Discourse seg-
mentation of multi-party conversation. In ACL.
Sharon Goldwater and Thomas L. Griffiths. 2007.
A fully Bayesian approach to unsupervised part-of-
speech tagging. In ACL.
Thomas L. Griffiths and Mark Steyvers. 2004. Finding
scientific topics. PNAS, 101(Suppl 1):5228?5235.
James Henderson and Oliver Lemon. 2008. Mixture
model POMDPs for efficient handling of uncertainty
in dialogue management. In ACL.
Matthew Hoffman, David M. Blei, and Francis Bach.
2010. Online learning for latent Dirichlet alocation.
In NIPS.
Pei-yun Hsueh, Johanna D. Moore, and Steve Renals.
2006. Automatic segmentation of multiparty dia-
logue. In EACL.
Llu??s F. Hurtado, Joaquin Planells, Encarna Segarra,
Emilio Sanchis, and David Griol. 2010. A stochas-
tic finite-state transducer approach to spoken dialog
management. In INTERSPEECH.
Dan Jurafsky, Elizabeth Shriberg, and Debra Bi-
asca. 1997. Switchboard SWBD-DAMSL shallow-
discourse-function annotation coders manual. Insti-
tute of Cognitive Science Technical Report, pages
97?02.
Maurice G. Kendall. 1938. A new measure of rank
correlation. Biometrika Trust.
Staffan Larsson and David R. Traum. 2000. Informa-
tion state and dialogue management in the TRINDI
dialogue move engine toolkit. Natural Language
Engineering, 5(3/4):323?340.
Esther Levin, Roberto Pieraccini, and Wieland Eckert.
2000. A stochastic model of human-machine inter-
action for learning dialogue strategies. IEEE Trans
on Speech and Audio Processing, 8(1):11?23.
Jingjing Liu, Stephanie Seneff, and Victor Zue. 2010.
Dialogue-oriented review summary generation for
spoken dialogue recommendation systems. In
NAACL.
David Mimno, Hanna Wallach, Jason Naradowsky,
David Smith, and Andrew McCallum. 2009.
Polylingual topic models. In EMNLP.
Gabriel Murray, Steve Renals, and Jean Carletta. 2005.
Extractive summarization of meeting recordings. In
European Conference on Speech Communication
and Technology.
Radford M. Neal. 2000. Markov chain sampling meth-
ods for Dirichlet process mixture models. Journal of
Computational and Graphical Statistics, 9(2):249?
265.
Radford M. Neal. 2003. Slice sampling. Annals of
Statistics, 31:705?767.
45
Matthew Purver, Konrad K?ording, Thomas L. Griffiths,
and Joshua Tenenbaum. 2006. Unsupervised topic
modelling for multi-party spoken discourse. In ACL.
Verena Rieser and Oliver Lemon. 2010. Natural lan-
guage generation as planning under uncertainty for
spoken dialogue systems. In EMNLP.
Alan Ritter, Colin Cherry, and Bill Dolan. 2010. Un-
supervised modeling of twitter conversations. In
NAACL.
Satinder Singh, Diane Litman, Michael Kearns, and
Marilyn Walker. 2002. Optimizing dialogue man-
agement with reinforcement learning: Experiments
with the NJFun system. Journal of Artificial Intelli-
gence Research.
Andreas Stolcke, Noah Coccaro, Rebecca Bates, Paul
Taylor, Carol Van Ess-Dykema, Klaus Ries, Eliza-
beth Shriberg, Daniel Jurafsky, Rachel Martin, and
Marie Meteer. 2000. Dialogue act modeling for
automatic tagging and recognition of conversational
speech. Computational Linguistics, September.
David R Traum and Staffan Larsson. 2003. The in-
formation state approach to dialogue management.
In Current and new directions in discourse and dia-
logue, pages 325?353.
Hanna M. Wallach, Iain Murray, Ruslan Salakhutdinov,
and David Mimno. 2009. Evaluation methods for
topic models. In ICML.
Hanna M. Wallach. 2008. Structured Topic Models for
Language. Ph.D. thesis, University of Cambridge.
Jason D. Williams and Suhrid Balakrishnan. 2009. Es-
timating probability of correctness for ASR N-best
lists. In SIGDIAL.
Jason D. Williams. 2012. Challenges and opportuni-
ties for state tracking in statistical spoken dialog sys-
tems: Results from two public deployments. Jour-
nal of Selected Topics in Signal Processing.
Tae Yano, William W. Cohen, and Noah A. Smith.
2009. Predicting response to political blog posts
with topic models. In NAACL, pages 477?485,
Stroudsburg, PA, USA. ACL.
Steve Young. 2006. Using POMDPs for dialog man-
agement. In Proceedings of the 1st IEEE/ACL Work-
shop on Spoken Language Technologies (SLT06).
Ke Zhai, Jordan Boyd-Graber, Nima Asadi, and Mo-
hamad Alkhouja. 2012. Mr. LDA: A flexible large
scale topic modeling package using variational in-
ference in mapreduce. In WWW.
46
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1166?1176,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Polylingual Tree-Based Topic Models for Translation Domain Adaptation
Yuening Hu
?
Computer Science
University of Maryland
ynhu@cs.umd.edu
Ke Zhai
?
Computer Science
University of Maryland
zhaike@cs.umd.edu
Vladimir Eidelman
FiscalNote Inc.
Washington DC
vlad@fiscalnote.com
Jordan Boyd-Graber
iSchool and UMIACS
University of Maryland
jbg@umiacs.umd.edu
Abstract
Topic models, an unsupervised technique
for inferring translation domains improve
machine translation quality. However, pre-
vious work uses only the source language
and completely ignores the target language,
which can disambiguate domains. We pro-
pose new polylingual tree-based topic mod-
els to extract domain knowledge that con-
siders both source and target languages and
derive three different inference schemes.
We evaluate our model on a Chinese to En-
glish translation task and obtain up to 1.2
BLEU improvement over strong baselines.
1 Introduction
Probabilistic topic models (Blei and Lafferty,
2009), exemplified by latent Dirichlet aloca-
tion (Blei et al, 2003, LDA), are one of the most
popular statistical frameworks for navigating large
unannotated document collections. Topic models
discover?without any supervision?the primary
themes presented in a dataset: the namesake topics.
Topic models have two primary applications: to
aid human exploration of corpora (Chang et al,
2009) or serve as a low-dimensional representa-
tion for downstream applications. We focus on
the second application, which has been fruitful for
computer vision (Li Fei-Fei and Perona, 2005),
computational biology (Perina et al, 2010), and
information retrieval (Kataria et al, 2011).
In particular, we use topic models to aid statisti-
cal machine translation (Koehn, 2009, SMT). Mod-
ern machine translation systems use millions of
examples of translations to learn translation rules.
These systems work best when the training corpus
has consistent genre, register, and topic. Systems
that are robust to systematic variation in the train-
ing set are said to exhibit domain adaptation.
? indicates equal contributions.
As we review in Section 2, topic models are
a promising solution for automatically discover-
ing domains in machine translation corpora. How-
ever, past work either relies solely on monolingual
source-side models (Eidelman et al, 2012; Hasler
et al, 2012; Su et al, 2012), or limited modeling
of the target side (Xiao et al, 2012). In contrast,
machine translation uses inherently multilingual
data: an SMT system must translate a phrase or sen-
tence from a source language to a different target
language, so existing applications of topic mod-
els (Eidelman et al, 2012) are wilfully ignoring
available information on the target side that could
aid domain discovery.
This is not for a lack of multilingual topic mod-
els. Topic models bridge the chasm between lan-
guages using document connections (Mimno et
al., 2009), dictionaries (Boyd-Graber and Resnik,
2010), and word alignments (Zhao and Xing, 2006).
In Section 2, we review these models for discover-
ing topics in multilingual datasets and discuss how
they can improve SMT.
However, no models combine multiple bridges
between languages. In Section 3, we create a
model?the polylingual tree-based topic models
(ptLDA)?that uses information from both external
dictionaries and document alignments simultane-
ously. In Section 4, we derive both MCMC and
variational inference for this new topic model.
In Section 5, we evaluate our model on the task
of SMT using aligned datasets. We show that ptLDA
offers better domain adaptation than other topic
models for machine translation. Finally, in Sec-
tion 6, we show how these topic models improve
SMT with detailed examples.
2 Topic Models for Machine Translation
Before considering past approaches using topic
models to improve SMT, we briefly review lexical
weighting and domain adaptation for SMT.
1166
2.1 Statistical Machine Translation
Statistical machine translation casts machine trans-
lation as a probabilistic process (Koehn, 2009). For
a parallel corpus of aligned source and target sen-
tences (F , E), a phrase
?
f ? F is translated to a
phrase e? ? E according to a distribution p
w
(e?|
?
f).
One popular method to estimate the probability
p
w
(e?|
?
f) is via lexical weighting features.
Lexical Weighting In phrase-based SMT, lexi-
cal weighting features estimate the phrase pair
quality by combining lexical translation probabil-
ities of words in a phrase (Koehn et al, 2003).
Lexical conditional probabilities p(e|f) are maxi-
mum likelihood estimates from relative lexical fre-
quencies c(f, e)/
?
e
c(f, e) , where c(f, e) is the
count of observing lexical pair (f, e) in the train-
ing dataset. The phrase pair probabilities p
w
(e?|
?
f)
are the normalized product of lexical probabili-
ties of the aligned word pairs within that phrase
pair (Koehn et al, 2003). In Section 2.2, we create
topic-specific lexical weighting features.
Cross-Domain SMT A SMT system is usu-
ally trained on documents with the same genre
(e.g., sports, business) from a similar style (e.g.,
newswire, blog-posts). These are called domains.
Translations within one domain are better than
translations across domains since they vary dra-
matically in their word choices and style. A correct
translation in one domain may be inappropriate in
another domain. For example, ???? in a newspa-
per usually means ?underwater diving?. On social
media, it means a non-contributing ?lurker?.
Domain Adaptation for SMT Training a SMT
system using diverse data requires domain adap-
tation. Early efforts focus on building separate
models (Foster and Kuhn, 2007) and adding fea-
tures (Matsoukas et al, 2009) to model domain
information. Chiang et al (2011) combine these
approaches by directly optimizing genre and col-
lection features by computing separate translation
tables for each domain.
However, these approaches treat domains as
hand-labeled, constant, and known a priori. This
setup is at best expensive and at worst infeasible for
large data. Topic models provide a solution where
domains can be automatically induced from raw
data: treat each topic as a domain.
1
1
Henceforth we will use the term ?topic? and ?domain?
interchangeably: ?topic? to refer to the concept in topic models
and ?domain? to refer to SMT corpora.
2.2 Inducing Domains with Topic Models
Topic models take the number of topics K and a
collection of documents as input, where each docu-
ment is a bag of words. They output two distribu-
tions: a distribution over topics for each document
d; and a distribution over words for each topic. If
each topic defines a SMT domain, the document?s
topic distribution is a soft domain assignment for
that document.
Given the soft domain assignments, Eidelman et
al. (2012) extract lexical weighting features condi-
tioned on the topics, optimizing feature weights us-
ing the Margin Infused Relaxed Algorithm (Cram-
mer et al, 2006, MIRA). The topics come from
source documents only and create topic-specific
lexical weights from the per-document topic distri-
bution p(k | d). The lexical probability conditioned
on the topic is expected count e
k
(e, f) of a word
translation pair under topic k,
c?
k
(e, f) =
?
d
p(k|d)c
d
(e, f), (1)
where c
d
(?) is the number of occurrences of the
word pair in document d. The lexical probability
conditioned on topic k is the unsmoothed probabil-
ity estimate of those expected counts
p
w
(e|f ; k) =
c?
k
(e,f)?
e
c?
k
(e,f)
, (2)
from which we can compute the phrase pair proba-
bilities p
w
(e?|
?
f ; k) by multiplying the lexical prob-
abilities and normalizing as in Koehn et al (2003).
For a test document d, the document topic dis-
tribution p(k | d) is inferred based on the topics
learned from training data. The feature value of a
phrase pair (e?,
?
f) is
f
k
(e?|
?
f) = ? log
{
p
w
(e?|
?
f ; k) ? p(k|d)
}
, (3)
a combination of the topic dependent lexical weight
and the topic distribution of the document, from
which we extract the phrase. Eidelman et al (2012)
compute the resulting model score by combining
these features in a linear model with other standard
SMT features and optimizing the weights.
Conceptually, this approach is just reweighting
examples. The probability of a topic given a docu-
ment is never zero. Every translation observed in
the training set will contribute to p
k
(e|f); many of
the expected counts, however, will be less than one.
This obviates the explicit smoothing used in other
domain adaptation systems (Chiang et al, 2011).
1167
We adopt this framework in its entirety. Our
contribution are topics that capture multilingual
information and thus better capture the domains in
the parallel corpus.
2.3 Beyond Vanilla Topic Models
Eidelman et al (2012) ignore a wealth of infor-
mation that could improve topic models and help
machine translation. Namely, they only use mono-
lingual data from the source language, ignoring all
target-language data and available lexical semantic
resources between source and target languages.
Different complement each other to reduce ambi-
guity. For example, ???? in a Chinese document
can be either ?hobbyhorse? in a children?s topic,
or ?Trojan virus? in a technology topic. A short
Chinese context obscures the true topic. However,
these terms are unambiguous in English, revealing
the true topic.
While vanilla topic models (LDA) can only be
applied to monolingual data, there are a number
of topic models for parallel corpora: Zhao and
Xing (2006) assume aligned word pairs share same
topics; Mimno et al (2009) connect different lan-
guages through comparable documents. These
models take advantage of word or document align-
ment information and infer more robust topics from
the aligned dataset.
On the other hand, lexical information can in-
duce topics from multilingual corpora. For in-
stance, orthographic similarity connects words with
the same meaning in related languages (Boyd-
Graber and Blei, 2009), and dictionaries are a
more general source of information on which words
share meaning (Boyd-Graber and Resnik, 2010).
These two approaches are not mutually exclu-
sive, however; they reveal different connections
across languages. In the next section, we combine
these two approaches into a polylingual tree-based
topic model.
3 Polylingual Tree-based Topic Models
In this section, we bring existing tree-based topic
models (Boyd-Graber et al, 2007, tLDA) and
polylingual topic models (Mimno et al, 2009,
pLDA) together and create the polylingual tree-
based topic model (ptLDA) that incorporates both
word-level correlations and document-level align-
ment information.
Word-level Correlations Tree-based topic mod-
els incorporate the correlations between words by
encouraging words that appear together in a con-
cept to have similar probabilities given a topic.
These concepts can come from WordNet (Boyd-
Graber and Resnik, 2010), domain experts (An-
drzejewski et al, 2009), or user constrains (Hu et
al., 2013). When we gather concepts from bilin-
gual resources, these concepts can connect different
languages. For example, if a bilingual dictionary
defines ???? as ?computer?, we combine these
words in a concept.
We organize the vocabulary in a tree structure
based on these concepts (Figure 1): words in the
same concept share a common parent node, and
then that concept becomes one of many children of
the root node. Words that are not in any concept?
uncorrelated words?are directly connected to
the root node. We call this structure the tree prior.
When this tree serves as a prior for topic models,
words in the same concept are correlated in topics.
For example, if ???? has high probability in a
topic, so will ?computer?, since they share the same
parent node. With the tree priors, each topic is no
longer a distribution over word types, instead, it is a
distribution over paths, and each path is associated
with a word type. The same word could appear in
multiple paths, and each path represents a unique
sense of this word.
Document-level Alignments Lexical resources
connect languages and help guide the topics. How-
ever, these resources are sometimes brittle and may
not cover the whole vocabulary. Aligned document
pairs provide a more corpus-specific, flexible asso-
ciation across languages.
Polylingual topic models (Mimno et al, 2009)
assume that the aligned documents in different lan-
guages share the same topic distribution and each
language has a unique topic distribution over its
word types. This level of connection between lan-
guages is flexible: instead of requiring the exact
matching on words and sentences, only a coarse
document alignment is necessary, as long as the
documents discuss the same topics.
Combine Words and Documents We propose
polylingual tree-based topic models (ptLDA),
which connect information across different lan-
guages by incorporating both word correlation (as
in tLDA) and document alignment information (as
in pLDA). We initially assume a given tree struc-
ture, deferring the tree?s provenance to the end of
this section.
1168
Generative Process As in LDA, each word to-
ken is associated with a topic. However, tree-based
topic models introduce an additional step of select-
ing a concept in a topic responsible for generating
each word token. This is represented by a path y
d,n
through the topic?s tree.
The probability of a path in a topic depends on
the transition probabilities in a topic. Each concept
i in topic k has a distribution over its children nodes
is governed by a Dirichlet prior: pi
k,i
? Dir(?
i
).
Each path ends in a word (i.e., a leaf node) and
the probability of a path is the product of all of
the transitions between topics it traverses. Topics
have correlations over words because the Dirichlet
parameters can encode positive or negative correla-
tions (Andrzejewski et al, 2009).
With these correlated in topics in hand, the gen-
eration of documents are very similar to LDA. For
every document d, we first sample a distribution
over topics ?
d
from a Dirichlet prior Dir(?). For
every token in the documents, we first sample a
topic z
dn
from the multinomial distribution ?
d
, and
then sample a path y
dn
along the tree according to
the transition distributions specified by topic z
dn
.
Because every path y
dn
leads to a word w
dn
in lan-
guage l
dn
, we append the sampled word w
dn
to
document d
l
dn
. Aligned documents have words in
both languages; monolingual documents only have
words in a single language.
The full generative process is:
1: for topic k ? 1, ? ? ? ,K do
2: for each internal node n
i
do
3: draw a distribution pi
ki
? Dir(?
i
)
4: for document set d ? 1, ? ? ? , D do
5: draw a distribution ?
d
? Dir(?)
6: for each word in documents d do
7: choose a topic z
dn
? Mult(?
d
)
8: sample a path y
dn
with probability
?
(i,j)?y
dn
pi
z
dn
,i,j
9: y
dn
leads to word w
dn
in language l
dn
10: append token w
dn
to document d
l
dn
If we use a flat symmetric Dirichlet prior instead
of the tree prior, we recover pLDA; and if all docu-
ments are monolingual (i.e., with distinct distribu-
tions over topics ?), we recover tLDA. ptLDA con-
nects different languages on both the word level (us-
ing the word correlations) and the document level
(using the document alignments). We compare
these models? machine translation performance in
Section 5.
computer, 
market, ?
government, ??
science, ??
Dictionary: Vocabulary: English (0), Chinese (1)
computer

market ?
government
??
science
??
??scientific policy
0    scientific
0    policy
1    
1    ?
0    computer  
0    market
0    government
0    science
1    ??
1    ??
1    ??
Prior Tree:
 0  1
Figure 1: An example of constructing a prior tree
from a bilingual dictionary: word pairs with the
same meaning but in different languages are con-
cepts; we create a common parent node to group
words in a concept, and then connect to the root; un-
correlated words are connected to the root directly.
Each topic uses this tree structure as a prior.
Build Prior Tree Structures One remaining
question is the source of the word-level connections
across languages for the tree prior. We consider
two resources to build trees that correlate words
across languages. The first are a multilingual dic-
tionaries (dict), which match words with the same
meaning in different languages together. These re-
lations between words are used as the concepts in
the prior tree (Figure 1).
In addition, we extract the word alignments from
aligned sentences in a parallel corpus. The word
pairs define concepts for the prior tree (align). We
use both resources for our models (denoted as
ptLDA-dict and ptLDA-align) in our experiments
(Section 5) and show that they yield comparable
performance in SMT.
4 Inference
Inference of probabilistic models discovers the pos-
terior distribution over latent variables. For a col-
lection of D documents, each of which contains
N
d
number of words, the latent variables of ptLDA
are: transition distributions pi
ki
for every topic k
and internal node i in the prior tree structure; multi-
nomial distributions over topics ?
d
for every docu-
ment d; topic assignments z
dn
and path y
dn
for the
n
th
word w
dn
in document d. The joint distribution
of polylingual tree-based topic models is
p(w, z,y,?,pi;?, ?) =
?
k
?
i
p(pi
ki
|?
i
) (4)
?
?
d
p(?
d
|?) ?
?
d
?
n
p(z
dn
|?
d
)
?
?
d
?
n
(
p(y
dn
|z
dn
,pi)p(w
dn
|y
dn
)
)
.
Exact inference is intractable, so we turn to ap-
1169
proximate posterior inference to discover the latent
variables that best explain our data. Two widely
used approximation approaches are Markov chain
Monte Carlo (Neal, 2000, MCMC) and variational
Bayesian inference (Blei et al, 2003, VB). Both
frameworks produce good approximations of the
posterior mode (Asuncion et al, 2009). In addition,
Mimno et al (2012) propose hybrid inference that
takes advantage of parallelizable variational infer-
ence for global variables (Wolfe et al, 2008) while
enjoying the sparse, efficient updates for local vari-
ables (Neal, 1993). In the rest of this section, we
discuss all three methods in turn.
We explore multiple inference schemes because
while all of these methods optimize likelihood be-
cause they might give different results on the trans-
lation task.
4.1 Markov Chain Monte Carlo Inference
We use a collapsed Gibbs sampler for tree-based
topic models to sample the path y
dn
and topic as-
signment z
dn
for word w
dn
,
p(z
dn
= k, y
dn
= s|?z
dn
,?y
dn
,w;?,?)
? I [?(s) = w
dn
] ?
N
k|d
+?
?
k
?
(N
k
?
|d
+?)
?
?
i?j?s
N
i?j|k
+?
i?j?
j
?
(N
i?j
?
|k
+?
i?j
?
)
,
where ?(s) represents the word that path s leads
to, N
k|d
is the number of tokens assigned to topic k
in document d and N
i?j|k
is the number of times
edge i? j in the tree assigned to topic k, exclud-
ing the topic assignment z
dn
and its path y
dn
of
current token w
dn
. In practice, we sample the la-
tent variables using efficient sparse updates (Yao et
al., 2009; Hu and Boyd-Graber, 2012).
4.2 Variational Bayesian Inference
Variational Bayesian inference approximates the
posterior distribution with a simplified variational
distribution q over the latent variables: document
topic proportions ?, transition probabilities pi, topic
assignments z, and path assignments y.
Variational distributions typically assume a
mean-field distribution over these latent variables,
removing all dependencies between the latent vari-
ables. We follow this assumption for the transi-
tion probabilities q(pi |?) and the document topic
proportions q(? |?); both are variational Dirichlet
distributions. However, due to the tight coupling
between the path and topic variables, we must
model this joint distribution as one multinomial,
q(z,y |?). If word token w
dn
has K topics and
S paths, it has a K ? S length variational multino-
mial ?
dnks
, which represents the probability that
the word takes path s in topic k. The complete
variational distribution is
q(?,pi, z,y|?,?,?) =
?
d
q(?
d
|?
d
)? (5)
?
k
?
i
q(pi
ki
|?
ki
) ?
?
d
?
n
q(z
dn
, y
dn
|?
dn
).
Our goal is to find the variational distribution q
that is closest to the true posterior, as measured by
the Kullback-Leibler (KL) divergence between the
true posterior p and variational distribution q. This
induces an ?evidence lower bound? (ELBO, L) as a
function of a variational distribution q: L =
E
q
[log p(w, z,y,?,pi)]? E
q
[log q(?,pi, z,y)]
=
?
k
?
i
E
q
[log p(pi
ki
|?
i
)]
+
?
d
E
q
[log p(?
d
|?)]
+
?
d
?
n
E
q
[log p(z
dn
, y
dn
|?
d
,pi)p(w
dn
|y
dn
)]
+ H[q(?)] + H[q(pi)] + H[q(z,y)], (6)
where H[?] represents the entropy of a distribution.
Optimizing L using coordinate descent provides
the following updates:
?
dnkt
? exp{?(?
dk
)??(
?
k
?
dk
) (7)
+
?
i?j?s
(
?(?
k,i?j
)??(
?
j
?
?
k,i?j
?
)
)
};
?
dk
= ?
k
+
?
n
?
s??
?1
(w
dn
)
?
dnkt
; (8)
?
k,i?j
= ?
i?j
(9)
+
?
d
?
n
?
s??
?
(w
dn
)
?
dnkt
I [i? j ? s] ;
where ?
?
(w
dn
) is the set of all paths that lead to
wordw
dn
in the tree, and t represents one particular
path in this set. I [i? j ? s] is the indicator of
whether path s contains an edge from node i to j.
4.3 Hybrid Stochastic Inference
Given the complementary strengths of MCMC and
VB, and following hybrid inference proposed by
Mimno et al (2012), we also derive hybrid infer-
ence for ptLDA.
The transition distributions pi are treated identi-
cally as in variational inference. We posit a varia-
tional Dirichlet distribution ? and choose the one
that minimizes the KL divergence between the true
posterior and the variational distribution.
For topic z and path y, instead of variational
updates, we use a Gibbs sampler within a document.
We sample z
dn
and y
dn
conditioned on the topic
1170
and path assignments of all other document tokens,
based on the variational expectation of pi,
q(z
dn
= k, y
dn
= s|?z
dn
,?y
dn
;w) ? (10)
(?+
?
m 6=n
I [z
dm
= k])
? exp{E
q
[log p(y
dn
|z
dn
,pi)p(w
dn
|y
dn
)]}.
This equation embodies how this is a hybrid algo-
rithm: the first term resembles the Gibbs sampling
term encoding how much a document prefers a
topic, while the second term encodes the expecta-
tion under the variational distribution of how much
a path is preferred by this topic,
E
q
[log p(y
dn
|z
dn
,pi)p(w
dn
|y
dn
)] = I
[?(y
dn
)=w
dn
]
?
?
i?j?y
dn
E
q
[log ?
z
dn
,i?j
].
For every document, we sweep over all its to-
kens and resample their topic z
dn
and path y
dn
conditioned on all the other tokens? topic and path
assignments ?z
dn
and ?y
dn
. To avoid bias, we
discard the first B burn-in sweeps and take the
following M samples. We then use the empirical
average of these samples update the global varia-
tional parameter q(pi|?) based on how many times
we sampled these paths
?
k,i?j
=
1
M
?
d
?
n
?
s??
?1
(w
dn
)
(
I [i? j ? s]
? I [z
dn
= k, y
dn
= s]
)
+ ?
i?j
. (11)
For our experiments, we use the recommended set-
tingsB = 5 andM = 5 from Mimno et al (2012).
5 Experiments
We evaluate our new topic model, ptLDA, and exist-
ing topic models?LDA, pLDA, and tLDA?on their
ability to induce domains for machine translation
and the resulting performance of the translations
on standard machine translation metrics.
Dataset and SMT Pipeline We use the NIST MT
Chinese-English parallel corpus (NIST), excluding
non-UN and non-HK Hansards portions as our train-
ing dataset. It contains 1.6M sentence pairs, with
40.4M Chinese tokens and 44.4M English tokens.
We replicate the SMT pipeline of Eidelman et al
(2012): word segmentation (Tseng et al, 2005),
align (Och and Ney, 2003), and symmetrize (Koehn
et al, 2003) the data. We train a modified Kneser-
Ney trigram language model on English (Chen and
Goodman, 1996). We use CDEC (Dyer et al, 2010)
for decoding, and MIRA (Crammer et al, 2006)
for parameter training. To optimize SMT system,
we tune the parameters on NIST MT06, and report
results on three test sets: MT02, MT03 and MT05.
2
Topic Models Configuration We compare our
polylingual tree-based topic model (ptLDA) against
tree-based topic models (tLDA), polylingual topic
models (pLDA) and vanilla topic models (LDA).
3
We also examine different inference algorithms?
Gibbs sampling (gibbs), variational inference
(variational) and hybrid approach (variational-
hybrid)?on the effects of SMT performance. In
all experiments, we set the per-document Dirichlet
parameter ? = 0.01 and the number of topics to
10, as used in Eidelman et al (2012).
Resources for Prior Tree To build the tree for
tLDA and ptLDA, we extract the word correla-
tions from a Chinese-English bilingual dictio-
nary (Denisowski, 1997).
4
We filter the dictionary
using the NIST vocabulary, and keep entries map-
ping single Chinese and single English words. The
prior tree has about 1000 word pairs (dict).
We also extract the bidirectional word align-
ments between Chinese and English using
GIZA++ (Och and Ney, 2003). We then remove
the word pairs appearing more than 50K times or
fewer than 500 times and construct a second prior
tree with about 2500 word pairs (align).
We apply both trees to tLDA and ptLDA, denoted
as tLDA-dict, tLDA-align, ptLDA-dict, and ptLDA-
align. However, tLDA-align and ptLDA-align do
worse than tLDA-dict and ptLDA-dict, so we omit
tLDA-align in the results.
Domain Adaptation using Topic Models We
examine the effectiveness of using topic models
for domain adaptation on standard SMT evalua-
tion metrics?BLEU (Papineni et al, 2002) and
TER (Snover et al, 2006). We report the results
on three different test sets (Figure 2), and all SMT
results are averaged over five runs.
We refer to the SMT model without domain adap-
tation as baseline.
5
LDA marginally improves ma-
chine translation (less than half a BLEU point).
2
The NIST datasets contain 878, 919, 1082 and 1664 sen-
tences for MT02, MT03, MT05 and MT06 respectively.
3
For Gibbs sampling, we use implementations available in
Hu and Boyd-Graber (2012) for tLDA; and Mallet (McCallum,
2002) for LDA and pLDA.
4
This is a two-level tree structure. However, one could
build a more sophisticated tree prior with a hierarchical dictio-
nary such as multilingual WordNet.
5
Our replication of Eidelman et al (2012) yields slightly
higher baseline performance, but the trend is consistent.
1171
gibbs variational variational?hybrid
34.8 +0.3 +0.6 +0.4
+1.2 +0.5
35.1 +0.1 +0.3 +0.2 +0.7 +0.4
31.4 +0.4 +0.7 +0.4 +1 +0.4
34.8 +0.4 +0.5 +0.4 +0.8 +0.5
35.1
?0.1 +0.2 ?0.1 +0.2 +0.2
31.4 +0.3 +0.5 +0.3 +0.8 +0.4
34.8 +0.2 +0.4 +0.2 +0.7 +0.4
35.1
?0.1 ?0.1 ?0.1 +0.2 +0.2
31.4 +0.3 +0.3 +0.1 +0.6 +0.3
3132
3334
3536
37
3132
3334
3536
37
3132
3334
3536
37
mt02
mt03
mt05
BLE
U S
cor
e
model baseline LDA pLDA ptLDA?align ptLDA?dict tLDA?dict
gibbs variational variational?hybrid
61.9 ?0.1
?1 ?1.2
?2.5 ?1.1
60.1
?0.3
?0.9 ?0.8
?1.9 ?0.9
63.3
?0.9
?1.3 ?1.2
?2.6 ?1.1
61.9
?0.4
?1 ?0.6
?1.6 ?1.3
60.1
?0.2
?0.5 ?0.1
?1 ?0.7
63.3
?0.5
?1 ?0.4
?1.5 ?1.2
61.9
?0.3
?0.7 ?0.1
?1.6 ?0.9
60.1 0 ?0.2 +0.2
?1.1 ?0.5
63.3
?0.4
?0.7 ?0.1
?1.6 ?0.8
5658
6062
6466
5658
6062
6466
5658
6062
6466
mt02
mt03
mt05
TE
R S
cor
e
model baseline LDA pLDA ptLDA?align ptLDA?dict tLDA?dict
Figure 2: Machine translation performance for different models and inference algorithms against the
baseline, on BLEU (top, higher the better) and TER (bottom, lower the better) scores. Our proposed ptLDA
performs best. Results are averaged over 5 random runs. For model ptLDA-dict with different inference
schemes, the BLEU improvement on three test sets is mostly significant with p = 0.01, except the results
on MT03 using variational and variational-hybrid inferences.
Polylingual topic models pLDA and tree-based
topic models tLDA-dict are consistently better than
LDA, suggesting that incorporating additional bilin-
gual knowledge improves topic models. These im-
provements are not redundant: our new ptLDA-dict
model, which has aspects of both models yields the
best performance among these approaches?up to a
1.2 BLEU point gain (higher is better), and -2.6 TER
improvement (lower is better). The BLEU improve-
ment is significant (Koehn, 2004) at p = 0.01,
6
except on MT03 with variational and variational-
hybrid inference.
While ptLDA-align performs better than base-
line SMT and LDA, it is worse than ptLDA-dict,
possibly because of errors in the word alignments,
making the tree priors less effective.
Scalability While gibbs has better translation
scores than variational and variational-hybrid, it
is less scalable to larger datasets. With 1.6M NIST
6
Because we have multiple runs of each topic model (and
thus different translation models), we select the run closest to
the average BLEU for the translation significance test.
training sentences, gibbs takes nearly a week to
run 1000 iterations. In contrast, the parallelized
variational and variational-hybrid approaches,
which we implement in MapReduce (Dean and
Ghemawat, 2004; Wolfe et al, 2008; Zhai et al,
2012), take less than a day to converge.
6 Discussion
In this section, we qualitatively analyze the trans-
lation results and investigate how ptLDA and its
cousins improve SMT. We also discuss other ap-
proaches to improve unsupervised domain adapta-
tion for SMT.
6.1 How do Topic Models Help SMT?
We present two examples of how topic models can
improve SMT. The first example shows both LDA
and ptLDA improve the baseline. The second exam-
ple shows how LDA introduce biases that mislead
SMT and how ptLDA?s bilingual constraints correct
these mistakes.
Figure 3 shows a sentence about a company
1172
source ???????????
 , ????
reference
sony has already sold about 570,000 units of narrowband connection 
kits in north america at the price of about 39 us dollars and some 20 
compatible games .
baseline
LDA
ptLDA
? internet links set ...
? internet links kit ? 
? internet links kit ?  
? with about 20 of the game .
? , there are about 20 compatible games .
? , there are about 20 compatible games .
source ?  ... ? ???

LDA-Topic 0 (business)
ptLDA-Topic 0 (business)
reference
? connection kits ... ? some 20 compatible games .

	, ???

??(company), ??(China), ?(service), ?
(market), ?(technology), ?(industry), ??
(provide), (develop), ?(year), 
(product), 
?, ??(coorporate), ?, ??(manage), ?
(invest), (economy), ?(international), ?
(system), (bank)
??(company), ?(service), ?(market), ?
(technology), china, ?(industry), 

(product), market, company, technology, services, 
?(system), year, industry, products, business, 
(economy), information, ??(manage), ?
(invest), percent, ?	(internet), companies, world, 
system, ??(information), ?(increase), 
(device), service, (service)
Figure 3: Better SMT result using topic models for domain adaptation. Top row: the source sentence and
its reference translation. Middle row: the highlighted translations from different approaches. Bottom row:
the change of relevant translation probabilities after incorporating the domain knowledge from LDA and
ptLDA. Right: most-probable words of the topic the source sentence is assigned to under LDA (top) and
ptLDA (bottom). The Chinese translations are in parenthesis.
introducing new technology gadgets where both
LDA and ptLDA improve translations. The base-
line translates ???? to ?set? (red), and ???? to
?with? (blue), which do not capture the reference
meaning of a add-on device that works with com-
patible games. Both LDA and ptLDA assign this
sentence to a business domain, which makes the
translations probabilities shift toward correct trans-
lations: the probability of translating ???? to
?compatible? and the probability of translating ??
?? to ?kit? in the business domain are both signif-
icantly larger than without the domain knowledge;
and the probabilities of translating ???? to ?with?
and the probability of translating ?set? to ????
in the business domain decrease.
The second example (Figure 4) illustrates how
ptLDA offers further improvements over LDA. The
source sentence discusses foreign affairs. The
baseline correctly translates the word ???? to
?affect?. However, LDA?which only takes mono-
lingual information from the source language?
assigns this sentence to economic development.
This misleads SMT to lower the probability for
the correct translation ?affect?; it chooses ?impact?
instead. In contrast, ptLDA?which incorporates
bilingual constraints?successfully labels this sen-
tence as foreign affairs and produces a softer, more
nuanced translation that better matches the refer-
ence. The translation of ???? is very similar,
except in this case, both the baseline and LDA
produce the incorrect translation ?the commitment
of?. This is possible because the probabilities of
translating ???? to ?promised to? and translat-
ing ?promised to? to ???? (the correct transla-
tion, in both directions) increase when conditioned
on ptLDA?s correct topic but decrease when condi-
tioned on LDA?s incorrect topic.
6.2 Other Approaches
Other approaches have used topic models for ma-
chine translation. Xiao et al (2012) present a topic
similarity model based on LDA that produces a fea-
ture that weights grammar rules based on topic
compatibility. They also model the source and tar-
get side of rules and compare the target similarity
during decoding by projecting the target distribu-
tion into the source space. Hasler et al (2012)
use the source-side topic assignments from hidden
topic Markov models (Gruber et al, 2007, HTMM)
which models documents as a Markov chain and
assign one topic to the whole sentence, instead of
a mixture of topics. Su et al (2012) also apply
HTMM to monolingual data and apply the results to
machine translation. To our knowledge, however,
this is the first work to use multilingual topic mod-
els for domain adaptation in machine translation.
6.3 Improving Language Models
Topic models capture document-level properties
of language, but a critical component of machine
translation systems is the language model, which
provides local constraints and preferences. Do-
main adaptation for language models (Bellegarda,
2004; Wood and Teh, 2009) is an important avenue
for improving machine translation. Models that si-
multaneously discover global document themes as
well as local, contextual domain-specific informa-
1173
source
????, ?????????, ???????????