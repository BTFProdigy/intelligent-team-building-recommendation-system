Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 961?968
Manchester, August 2008
Coreference Systems based on Kernels Methods
Yannick Versley
SFB 441
University of Tu?bingen
versley@sfs.uni-tuebingen.de
Alessandro Moschitti
DISI
University of Trento
moschitti@disi.unitn.it
Massimo Poesio
DISI
University of Trento
massimo.poesio@unitn.it
Xiaofeng Yang
Data Mining Department
Institute for Infocomm Research
xiaofengy@i2r.a-star.edu.sg
Abstract
Various types of structural information -
e.g., about the type of constructions in
which binding constraints apply, or about
the structure of names - play a central role
in coreference resolution, often in combi-
nation with lexical information (as in ex-
pletive detection). Kernel functions ap-
pear to be a promising candidate to capture
structure-sensitive similarities and com-
plex feature combinations, but care is re-
quired to ensure they are exploited in the
best possible fashion. In this paper we
propose kernel functions for three subtasks
of coreference resolution - binding con-
straint detection, expletive identification,
and aliasing - together with an architec-
ture to integrate them within the standard
framework for coreference resolution.
1 Introduction
Information about coreference relations?i.e.,
which noun phrases are mentions of the same
entity?has been shown to be beneficial in a great
number of NLP tasks, including information
extraction (McCarthy and Lehnert 1995), text
planning (Barzilay and Lapata 2005) and sum-
marization (Steinberger et al 2007). However,
the performance of coreference resolvers on
unrestricted text is still quite low. One reason
for this is that coreference resolution requires a
great deal of information, ranging from string
matching to syntactic constraints to semantic
knowledge to discourse salience information to
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
full common sense reasoning (Sidner 1979; Hobbs
1978, 1979; Grosz et al 1995; Vieira and Poesio
2000; Mitkov 2002). Much of this information
won?t be available to robust coreference resolvers
until better methods are found to represent and
encode common sense knowledge; but part of
the problem is also the need for better methods
to encode information that is in part structural,
in part lexical. Enforcing binding constraints
?e.g., ruling out Peter as antecedent of him in (1a)
requires recognizing that the anaphor occurs in a
particular type of construction (Chomsky 1981;
Lappin and Leass 1994; Yang et al 2006) whose
exact definition however has not yet been agreed
upon by linguists (indeed, it may only be definable
in a graded sense (Sturt 2003; Yang et al 2006)),
witness examples like (1b). Parallelism effects are
a good example of structural information inducing
preferences rather than constraints. Recognizing
that It in examples such as (1c,d) are expletives
requires a combination of structural information
and lexical information (Lappin and Leass 1994;
Evans 2001). But some sort of structure also
underlies our interpretation of other types of
coreference: e.g., knowledge about the structure
of names certainly plays a role in recognizing
that BJ Habibie is a possible antecedent for Mr.
Habibie.
(1) a. John thinks that Peter hates him.
b. John hopes that Jane is speaking only to
himself.
c. It?s lonely here.
d. It had been raining all day.
The need to capture such information suggests
a role for kernel methods (Vapnik 1995) in coref-
erence resolution. Kernel functions make it pos-
sible to capture the similarity between structures
961
without explicitly enumerating all the substruc-
tures, and have therefore been shown to be a vi-
able approach to feature engineering for natural
language processing for any task in which struc-
tural information plays a role, e.g. (Collins and
Duffy 2002; Zelenko et al 2003; Giuglea and Mos-
chitti 2006; Zanzotto and Moschitti 2006; Mos-
chitti et al 2007). Indeed, they have already been
used in NLP to encode the type of structural in-
formation that plays a role in binding constraints
(Yang et al 2006); however, the methods used in
this previous work do not make it possible to ex-
ploit the full power of kernel functions. In this
work, we extend the use of kernel functions for
coreference by designing and testing kernels for
three subtasks of the coreference task:
? Binding constraints
? Expletive detection
? Aliasing
and developing distinct classifiers for each of these
tasks. We show that our developed kernels produce
high accuracy for both distinct classifiers for these
subtasks as well as for the complete coreference
system.
In the remainder: Section 2, briefly describes
the basic kernel functions that we used; Section
3 illustrates our new kernels for expletive, binding
and name alias detection along with a coreference
context kernel; Section 4 reports the experiments
on individual classifiers on expletives, binding and
names whereas Section 5 shows the results on the
complete coreference task; Finally, Section 6 de-
rives the conclusions.
2 Kernel for Structured Data
We used three kernel functions in this work: the
String Kernel (SK) proposed in Shawe-Taylor and
Cristianini (2004) to evaluate the number of sub-
sequences between two sequences, the Syntactic
Tree Kernel (STK; see Collins and Duffy 2002)
which computes the number of syntactic tree frag-
ments and the Partial Tree Kernel (PTK; see Mos-
chitti 2006) which provides a more general repre-
sentation of trees in terms of tree fragments. We
discuss each in turn.
2.1 String Kernels (SK)
The string kernels that we consider count the num-
ber of substrings shared by two sequences contain-
ing gaps, i.e. some of the characters of the original
NP 
D N 
a 
  cat 
NP 
D N 
NP 
D N 
a 
NP 
D N 
NP 
D N 
VP 
V 
brought 
a 
   cat 
  cat 
NP 
D N 
VP 
V 
a 
   cat 
NP 
D N 
VP 
V 
N 
   cat 
D 
a 
V 
brought 
N 
Mary 
? 
Figure 1: A tree with some of its STFs .
NP 
D N 
VP 
V 
brought 
a 
   cat 
NP 
D N 
VP 
V 
a 
   cat 
NP 
D N 
VP 
a 
   cat 
NP 
D N 
VP 
a 
NP 
D 
VP 
a 
NP 
D 
VP 
NP 
N 
VP 
NP 
N 
NP NP 
D N D 
NP 
? 
VP 
Figure 2: A tree with some of its PTFs.
string are skipped. Gaps penalize the weight asso-
ciated with the matched substrings. More in detail,
(a) longer subsequences receive lower weights.
(b) Valid substrings are sequences of the original
string with some characters omitted, i.e. gaps. (c)
Gaps are accounted by weighting functions and (d)
symbols of a string can also be whole words, i.e.
the word sequence kernel Cancedda et al (2003).
2.2 Tree Kernels
The main idea underlying tree kernels is to com-
pute the number of common tree fragments be-
tween two trees without explicitly considering the
whole fragment space. The type of fragments char-
acterize different kernel functions. We consider
syntactic tree fragments (STFs) and partial tree
fragments (PTFs)
2.2.1 Syntactic Tree Kernels (STK)
An STF is a connected subset of the nodes and
edges of the original tree, with the constraint that
any node must have all or none of its children. This
is equivalent to stating that the production rules
contained in the STF cannot be partial. For ex-
ample, Figure 1 shows a tree with its PTFs: [VP [V
NP]] is an STF, [VP [V]] or [VP [NP]] are not STFs.
2.2.2 Partial Tree Kernel (PTK)
If we relax the production rule constraint over
the STFs, we obtain a more general substructure
type, i.e. PTF, generated by the application of par-
tial production rules, e.g. Figure 2 shows that [VP
[NP[D]]] is indeed a valid fragment. Note that
PTK can be seen as a STK applied to all possible
child sequences of the tree nodes, i.e. a string ker-
nel combined with a STK.
2.3 Kernel Engineering
The Kernels of previous section are basic functions
that can be applied to feature vectors, strings and
962
trees. In order to make them effective for a specific
task, e.g. for coreference resolution: (a) we can
combine them with additive or multiplicative op-
erators and (b) we can design specific data objects
(vectors, sequences and tree structures) for the tar-
get tasks.
It is worth noting that a basic kernel applied to
an innovative view of a structure yields a new ker-
nel (e.g. Moschitti and Bejan (2004); Moschitti
et al (2006)), as we show below:
Let K(t
1
, t
2
) = ?(t
1
) ? ?(t
2
) be a basic ker-
nel, where t
1
and t
2
are two trees. If we map t
1
and t
2
into two new structures s
1
and s
2
with a
mapping ?
M
(?), we obtain: K(s
1
, s
2
) = ?(s
1
) ?
?(s
2
) = ?(?
M
(t
1
)) ? ?(?
M
(t
2
)) = ?
?
(t
1
) ?
?
?
(t
2
)=K?(t
1
, t
2
), which is a noticeably different
kernel induced by the mapping ?? = ? ? ?
M
.
3 Kernels for Coreference Resolution
In this paper we follow the standard learning ap-
proach to coreference developed by Soon et al
(2001) and also used the few variants in Ng and
Cardie (2002). In this framework, training and
testing instances consist of a pair (anaphor, an-
tecedent). During training, a positive instance is
created for each anaphor encountered by pairing
the anaphor with its closest antecedent; each of the
non-coreferential mentions between anaphor and
antecedent is used to produce a negative instance.
During resolution, every mention to be resolved is
paired with each preceding antecedent candidate
to form a testing instance. This instance is pre-
sented to the classifier which then returns a class
label with a confidence value indicating the likeli-
hood that the candidate is the antecedent.
The nearest candidate with a positive classifica-
tion will be selected as the antecedent of the pos-
sible anaphor. The crucial point is that in this ap-
proach, the classifier is trained to identify positive
and negative instances of the resolution process. In
previous work on using kernel functions for coref-
erence (Yang et al 2006), structural information
in the form of tree features was included in the
instances. This approach is appropriate for iden-
tifying contexts in which the binding constraints
apply, but not, for instance, to recognize exple-
tives. In this work we adopted therefore a more
general approach, in which separate classifiers are
used to recognize each relevant configuration, and
their output is then used as an input to the coref-
erence classifier. In this section we discuss the
types of structures and kernel functions we used
for three different kinds of classifiers: expletive,
binding and alias classifiers. We then present the
results of these classifiers, and finally the results
with the coreference resolver as a whole.
3.1 Expletive Kernels
In written text, about a third of the occurrences
of the pronoun it are not coreferent to a previ-
ous mention, but either refer to a general discourse
topic (it?s a shame) or do not refer at all, as in the
case of extraposed subjects (it is thought that . . . )
or weather verbs (it?s raining). It is desirable to
minimize the impact that these non-anaphoric pro-
nouns have on the accuracy of a anaphora resolu-
tion: Lappin and Leass (1994), for example, use
several heuristics to filter out expletive pronouns,
including a check for patterns including modal ad-
jectives (it is good/necessary/. . . that . . . ), and cog-
nitive verbs (it is thought/believed/. . . that . . . ).
Newer approaches to the problem use machine-
learning on hand-annotated examples: Evans
(2001) compares a shallow approach based on
surrounding lemmas, part-of-speech tags, and the
presence of certain elements such as modal adjec-
tives and cognitive verbs, trained on 3171 exam-
ples from Susanne and the BNC to a reimplemen-
tation of a pattern-based approach due to Paice and
Husk (1987) and finds that the shallower machine-
learning approach compares favorably to it. Boyd
et al (2005) use an approach that combines some
of Evans? shallow features with hand-crafted pat-
terns in a memory based learning approach and
find that the more informative features are ben-
eficial for the system?s performance (88% accu-
racy against 71% for their reimplementation using
Evans? shallow features).
Evans? study also mentions that incorporating
the expletive classifier as a filter for a pronoun re-
solver gives a gain between 2.86% (for manually
determined weights) and 1% (for automatically op-
timized weights).
Tree kernels are a good fit for expletive classi-
fication since they can naturally represent the lex-
ical and structural context around a word. Our fi-
nal classifier uses the combination of an unmodi-
fied tree (UT) (where the embedding clause or verb
phrase of the pronoun is used as a tree), and a tree
that only preserves the most salient structural fea-
tures (ST).
The reduced representation prunes all nodes that
963
would not be seen as indicative in a pattern ap-
proach, essentially keeping verb argument struc-
ture and important lexical items, such as the gov-
erning verb and, in the case of copula construc-
tions, the predicate. For example, the phrase
(S (NP (PRP It))
(VP (VBZ has)
(NP (NP (DT no) (NN bearing))
(PP (IN on)
(NP (NP (PRP$ our)
(NN work)
(NN force))
(NP (NN today)))))
(. .))
would be reduced to the ST:
(S-I (NP-I (PRP-I It))
(VP (VBX have)
(NP))
(.))
or, in a similar fashion,
(S (NP (PRP it))
(VP (VBZ ?s)
(NP (NP (NN time))
(PP (IN for)
(NP (PRP$ their)
(JJ biannual)
(NN powwow))))))
would just be represented as the ST:
(S-I (NP-I (PRP-I it))
(VP (BE VBZ)
(NP-PRD (NN time))))
3.2 Binding Kernels
The resolution of pronominal anaphora heavily re-
lies on the syntactic information and relationships
between the anaphor and the antecedent candi-
dates, including binding and other constraints, but
also context-induced preferences in sub-clauses.
Some researchers (Lappin and Leass 1994;
Kennedy and Boguraev 1996) use manually de-
signed rules to take into account the grammati-
cal role of the antecedent candidates as well as
the governing relations between the candidate and
the pronoun, while others use features determined
over the parse tree in a machine-learning approach
(Aone and Bennett 1995; Yang et al 2004; Luo
and Zitouni 2005). However, such a solution has
limitations, since the syntactic features have to be
selected and defined manually, and it is still partly
an open question which syntactic properties should
be considered in anaphora resolution.
We follow (Yang et al 2006; Iida et al 2006) in
using a tree kernel to represent structural informa-
tion using the subtree that covers a pronoun and its
antecedent candidate. Given a sentence like ?The
Figure 3: The structure for binding detection for
the instance inst(?the man?, ?him?) in the sentence
?the man in the room saw him?
man in the room saw him.?, we represent the syn-
tactic relation between ?The man? and ?him?, by
the shortest node path connecting the pronoun and
the candidate, along with the first-level of the node
children in the path.
Figure 3 graphically shows such tree highlighted
with dash lines. More in detail we operate the fol-
lowing tree transformation:
(a) To distinguish from other words, we explic-
itly mark up in the structured feature the pronoun
and the antecedent candidate under consideration,
by appending a string tag ?ANA? and ?CANDI?
in their respective nodes, i.e. ?NN-CANDI? for
?man? and ?PRP-ANA? for ?him?.
(b) To reduce the data sparseness, the leaf nodes
representing the words are not incorporated in the
feature, except that the word is the word node of
the ?DET? type (this is to indicate the lexical prop-
erties of an expression, e.g., whether it is a definite,
indefinite or bare NP).
(c) If the pronoun and the candidate are not in the
same sentence, we do not include the nodes denot-
ing the sentences (i.e., ?S? nodes) before the can-
didate or after the pronoun.
The above tree structures will be jointly used
with the basic STK which extracts tree fragments
able to characterize the following information: (a)
the candidate is post-modified by a preposition
phrase, (the node ?PP? for ?in the room? is in-
cluded), (b) the candidate is a definite noun phrase
(the article word ?the? is included), (c) the candi-
date is in a subject position (NP-S-VP structure),
(d) the anaphor is an object of a verb (the node
?VB? for ?saw? is included) and (e) the candidate
is c-commanding the anaphor (the parent of the
NP node for ?the main in the room? is dominat-
ing the anaphor (?him?), which are important for
reference determination in the pronoun resolution.
964
3.3 Encoding Context via Word Sequence
Kernel
The previous structures aim at describing the in-
teraction between one referential and one referent;
if such interaction is observed on another mention
pair, an automatic algorithm can establish if they
corefer or not. This kind of information is the most
useful to characterize the target problem, however,
the context in which such interaction takes place is
also very important. Indeed, natural language pro-
poses many exceptions to linguistic rules and these
can only be detect by looking at the context. To be
able to represent context words or phrases, we use
context word windows around the mentions and
the subsequence kernel function (see section 2.1)
to extract many features from it.
For example, in the context of ?and so Bill
Gates says that?, a string kernel would ex-
tract features including: Bill Gates says that,
says that, Gates, Gates says that, Bill says that,
so Gates says that, and so that and so on.
Name Alias
BJ Habibie Mr. Habibie
Federal Express Fedex
Ju Rong Zhi Ju
Table 1: Examples of coreferent named entities
(aliases) taken from the MUC 6 corpus.
3.4 Kernels for Alias Resolution
Most methods currently employed by coreference
resolution (CR) systems for identifying coreferent
named entities, i.e. aliases, are fairly simplistic in
nature, relying on simple surface features such as
the edit distance between two strings representing
names. We investigate the potential of using the
structure contained within names. This can be very
useful to solve complex cases like those shown in
Table 1, taken from the MUC 6 corpus (Chinchor
and Sundheim 2003). For this purpose, we add
syntactic information to the feature set by tagging
the parts of a name (e.g. first name, last name, etc.)
as illustrated in Figure 4.
To automatically extract such structure we used
the High Accuracy Parsing of Name Internal Struc-
ture (HAPNIS) script1. HAPNIS takes a name as
input and returns a tagged name like what is shown
in Figure 4. It uses a series of heuristics in making
its classifications based on information such as the
1The script is freely available at
http://www.cs.utah.edu/ hal/HAPNIS/.
Figure 4: A proper name labeled with syntactic in-
formation.
serial positions of tokens in a name, the total num-
ber of tokens, the presence of meaningful punctua-
tion such as periods and dashes, as well as a library
of common first names which can be arbitrarily ex-
tended to any size. The tag set consists of the fol-
lowing: surname, forename, middle, link, role, and
suffix2.
Once the structure for a name has been de-
rived, we can apply tree kernels to represent it in
the learning algorithms thus avoiding the manual
feature design. Such structures are not based on
any particular grammar, therefore, any tree sub-
part may be relevant. In this case the most suitable
kernel is PTK, which extracts any tree subpart. It
is worth to note that the name tree structure can
be improved by inserting a separate node for each
name character and exploiting the string matching
approximation carried out by PTK. For example,
Microsoft Inc. will have a large match with Mi-
crosoft Incorporated whereas the standard string
matching would be null.
4 Experiments with Coreference Subtask
Classifiers
In these experiments we test the kernels devised for
expletive (see Section 3.1), binding (see Section
3.2) and alias detection (see Section 3.4), to study
the level of accuracy reachable by our kernel-based
classifiers. The baseline framework is constituted
by SVMs along with a polynomial kernel over the
Soon et al?s features.
4.1 Experiments on Expletive Classification
We used the BBN Pronoun corpus3 as a source of
examples, with the training set consisting of sec-
tions 00-19, yielding more than 5800 instances of
2Daume? reports a 99.1% accuracy rate on his test data set.
We therefore concluded that it was sufficient for our purposes.
3Ralph Weischedel and Ada Brunstein (2005): BBN Pro-
noun Coreference and Entity Type Corpus, LDC2005T33
965
it, with the testing set consisting of sections 20 and
21, using the corresponding parses from the Penn
Treebank for the parse trees. Additionally, we re-
port on the performance of the classifier learnt on
only the first 1000 instances to verify that our ap-
proach also works for small datasets. The results
in Table 2 show that full tree (UT) achieves good
results whereas the salient tree (ST) leads to a bet-
ter ability to generalize, and the combination ap-
proach outperforms both individual trees.
BBN large BBN small
Prec Recl Acc Prec Recl Acc
UT 83.87 61.54 84.35 78.76 52.66 80.85
ST 78.08 67.46 83.98 77.61 61.54 82.50
UT+ST 81.12 68.64 85.27 80.74 64.50 84.16
Table 2: Results for kernel-based expletive detec-
tion (using STK)
Note that the accuracy we get by training on
1000 examples (84% accuracy; see the small col-
umn in Table 2) is better than Boyd?s replication of
Evans (76% accuracy) or their decision tree clas-
sifier (81% accuracy) even though Boyd et al?s
dataset is three times bigger. On the other hand,
Boyd et als full system, which uses substantial
hand-crafted knowledge, gets a still better result
(88% accuracy), which is also higher than the ac-
curacy of our classifier even when trained on the
full 5800 instances.
MUC-6
Prec Recl F
Soon et al 51.25 55.51 53.29
STK 71.93 55.41 62.59
Table 3: Binding classifier: coreference classifica-
tion on same-sentence pronouns
4.2 Experiments with the Binding Classifier
To assess the effect of the binding classifier on
same-sentence pronoun links, we extracted 1398
mention pairs from the MUC-6 training data where
both mentions were in the same sentence and at
least one item of the pair included a pronoun, us-
ing the first 1000 for training and the remaining
398 examples for testing. The results (see Table 3)
show that the syntactic tree kernel (STK) consider-
ably improves the precision of classification of the
Soon et al?s features.
4.3 Experiments on Alias Classification
For our preliminary experiments, we extracted
only pairs in the MUC 6 testing set in which both
mentions were proper names, as determined by
the coreference resolver?s named entity recognizer.
This set of proper names contained about 37,000
pairs of proper names of which about 600 were
positive instances. About 5,500 pairs were ran-
domly selected as test instances and the rest were
used for training.
In the first experiment, we trained a decision
tree classifier to detect if two names are aliases.
For this task, we used either the string kernel score
over the sequence of characters or the edit distance.
The results in Table 4 show that the string kernel
score performs better by 21.6 percentage points in
F-measure.
In the second experiments we used SVMs
trained with the string kernel over the name-
character sequences and with PTK, which takes
into account the structure of names. The re-
sults in Table 5 show that the structure improves
alias detection by almost 5 absolute percent points.
This suggests that an effective coreference sys-
tem should embed PTK and name structures in the
coreference pair representation.
Recall Precision F-measure
String kernel 49.5% 60.8% 54.6%
Edit distance 23.9% 53.1% 33.0%
Table 4: Decision-tree based classification of name
aliases using string kernels and edit distance.
Recall Precision F-measure
String kernel 58.4% 67.5% 62.6%
PTK 64.8% 70.0% 67.3%
Table 5: SVM-based classification of name aliases
using string kernels and tree-based feature.
5 Experiments on Coreference Systems
In this section we evaluate the contribution in the
whole coreference task of the expletive classifier
and the binding kernel. The predictions of the for-
mer are used as a feature of our basic coreference
system whereas the latter is used directly in the
coreference classifier by adding it to the polyno-
mial kernel of the basic system.
Our basic system is based on the standard learn-
ing approach to coreference developed by Soon
et al (2001). It uses the features from Soon et
al?s work, including lexical properties, morpho-
logic type, distance, salience, parallelism, gram-
matical role and so on. The main difference with
966
Soon et al (2001) is the use of SVMs along with a
polynomial kernel.
MUC-6
Prec Recl F
plain 65.2 66.9 66.0
plain+expletive 66.1 66.9 66.5
upper limit 70.0 66.9 68.4
Table 6: Expletive classification: influence on pro-
noun resolution
5.1 Influence of Expletive classification
To see how useful a classifier for expletives can
be, we conducted experiments using the expletive
classifier learned on the BBN pronoun corpus on
the MUC-6 corpus. Preliminary experiments indi-
cated that perfect detection of expletives (i.e. using
gold standard annotation) could raise the precision
of pronoun resolution from 65.2% to 70.0%, yield-
ing a 2.4% improvement in the F-score for pronoun
resolution alone, or 0.6% improvement in the over-
all coreference F-score (see Table 6).
For a more realistic assessment, we used the
classifier learned on the BBN pronoun corpus ex-
amples as an additional feature to gauge the im-
provement that could be achieved using it. While
the gain in precision is small even in comparison
to the achievable error reduction, we need to keep
in mind that our baseline is in fact a well-tuned
system.
MUC-6 ACE02-BNews
R P F R P F
PK 64.3 63.1 63.7 58.9 68.1 63.1
PK+TK 65.2 80.1 71.9 65.6 69.7 67.6
Table 7: Results of the pronoun resolution
5.2 Binding and Context Kernels
In these experiments, we compared our corefer-
ence system based on Polynomial Kernel (PK)
against its combinations with Syntactic Tree Ker-
nels (STK) over the binding structures (Sec. 3.2)
and Word Sequence Kernel (WSK) on context
windows (Sec. 3.3). We experimented with
both the only pronoun and the complete corefer-
ence resolution tasks on the standard MUC-6 and
ACE03-BNews data sets.
On the validation set, the best kernel combina-
tion between PK and STK was STK(T1, T2) ?
PK(~x
1
, ~x
2
)+PK(~x
1
, ~x
2
). Then an improvement
arises when simply summing WSK.
Table 7 lists the results for the pronoun resolu-
tion. We used PK on the Soon et al?s features as
the baseline. On MUC-6, the system achieves a
recall of 64.3% and precision 63.1% and an over-
all F-measure of 63.7%. On ACE02-BNews, the
recall is lower 58.9% but the precision is higher,
i.e. 68.1%, for a resulting F-measure of 63.1%.
In contrast, adding the binding kernel (PK+STK)
leads to a significant improvement in 17% preci-
sion for MUC-6 with a small gain (1%) in recall,
whereas on the ACE data set, it also helps to in-
crease the recall by 7%. Overall, we can see an
increase in F-measure of around 8% for MUC and
4.5% for ACE02-BNews. These results suggest
that the structured feature is very effective for pro-
noun resolution.
MUC-6 ACE02-BNews
R P F R P F
PK 61.5 67.2 64.2 54.8 66.1 59.9
PK+STK 63.4 67.5 65.4 56.6 66.0 60.9
PK+STK+WSK 64.4 67.8 66.0 57.1 65.4 61.0
Table 8: Results of the coreference resolution
Table 8 lists the results on the coreference res-
olution. We note that adding the structured fea-
ture to the polynomial kernel, i.e. using the model
PK+STK, improves the recall of 1.9% for MUC-
6 and 1.8% for ACE-02-BNews and keeps invari-
ant the precision. Compared to pronoun resolu-
tion, the improvement of the overall F-measure is
smaller (about 1%). This occurs since the resolu-
tion of non-pronouns case does not require a mas-
sive use of syntactic knowledge as in the pronoun
resolution problem. WSK further improves the
system?s F1 suggesting that adding structured fea-
tures of different types helps in solving the coref-
erece task.
6 Conclusions
We presented four examples of using kernel-based
methods to take advantage of a structured repre-
sentation for learning problems that arise in coref-
erence systems, presenting high-accuracy classi-
fiers for expletive detection, binding constraints
and same-sentence pronoun resolution, and name
alias matching. We have shown the accuracy
of the individual classifiers for the above tasks
and the impact of expletives and binding classi-
fiers/kernels in the complete coreference resolu-
tion system. The improvement over the individual
and complete tasks suggests that kernel methods
967
are a promising research direction to achieve state-
of-the-art coreference resolution systems.
Future work is devoted on making the use of ker-
nels for coreference more efficient since the size of
the ACE-2 corpora prevented us to directly use the
combination of all kernels that we designed. In this
paper, we have also studied a solution which re-
lates to factoring out decisions into separate clas-
sifiers and using the decisions as binary features.
However, this solution shows some loss in terms of
accuracy. We are currently investigating methods
that allow us to combine the accuracy and flexibil-
ity of the integrated approach with the speed of the
separate classifier approach.
Acknowledgements Y. Versley was funded by the
Deutsche Forschungsgemeinschaft as part of SFB (Collabora-
tive Research Centre) 441. A. Moschitti has been partly sup-
ported by the FP6 IST LUNA project (contract No. 33549).
Part of the work reported in this paper was done at the Johns
Hopkins Summer Workshop in 2007, funded by NSF and
DARPA. We are especially grateful for Alan Jern?s implemen-
tation help for name structure identification.
References
Aone, C. and Bennett, S. W. (1995). Evaluating automated
and manual acquisition of anaphora resolution strategies.
In Proc. ACL 1995, pages 122?129.
Barzilay, R. and Lapata, M. (2005). Modelling local coher-
ence: An entity-based approach. In Proc. of ACL, Ann
Arbor, MI.
Boyd, A., Gegg-Harrison, W., and Byron, D. (2005). Iden-
tifying non-referential it: a machine learning approach in-
corporating linguistically motivated features. In Proc. ACL
WS on Feature Engineering for Machine Learning in Nat-
ural Language Processing.
Cancedda, N., Gaussier, E., Goutte, C., and Renders, J. M.
(2003). Word sequence kernels. JMLR, 3:1059?1082.
Chinchor, N. and Sundheim, B. (2003). Muc 6 corpus. Mes-
sage Understanding Conference (MUC) 6.
Chomsky, N. (1981). Lectures on government and binding.
Foris, Dordrecht, The Netherlands.
Collins, M. and Duffy, N. (2002). New ranking algorithms for
parsing and tagging: kernels over discrete structures and
the voted perceptron. In Proc. ACL 2002, pages 263?270.
Evans, R. (2001). Applying machine learning toward an au-
tomatic classification of it. Literary and Linguistic Com-
puting, 16(1):45?57.
Giuglea, A.-M. and Moschitti, A. (2006). Semantic role la-
beling via framenet, verbnet and propbank. In Proceedings
of Coling-ACL, Sydney, Australia.
Grosz, B., Joshi, A., and Weinstein, S. (1995). Centering: a
framework for modeling the local coherence of discourse.
CL, 21(2):203?225.
Hobbs, J. (1978). Resolving pronoun references. Lingua,
44:339?352.
Hobbs, J. (1979). Resolving pronoun references. Coherence
and Coreference, 3(1):67?90.
Iida, R., Inui, K., and Matsumoto, Y. (2006). Exploiting syn-
tactic patterns as clues in zero-anaphora resolution. In
Proc. Coling/ACL 2006, pages 625?632.
Kennedy, C. and Boguraev, B. (1996). Anaphora for every-
one: pronominal anaphora resolution without a parser. In
Proc. Coling 1996.
Lappin, S. and Leass, H. (1994). An algorithm for pronominal
anaphora resolution. CL, 20(4):525?561.
Luo, X. and Zitouni, I. (2005). Multi-lingual coreference res-
olution with syntactic features. In Proc. HLT/EMNLP 05.
McCarthy, J. and Lehnert, W. (1995). Using decision trees for
coreference resolution. In Proc. IJCAI 1995.
Mitkov, R. (2002). Anaphora resolution. Longman.
Moschitti, A. (2006). Efficient convolution kernels for depen-
dency and constituent syntactic trees. Proc. ECML 2006.
Moschitti, A. and Bejan, C. A. (2004). A semantic kernel for
predicate argument classification. In CoNLL-2004, USA.
Moschitti, A., Pighin, D., and Basili, R. (2006). Semantic
Role Labeling via Tree Kernel Joint Inference. In Pro-
ceedings of CoNLL-X.
Moschitti, A., Quarteroni, S., Basili, R., and Manandhar, S.
(2007). Exploiting syntactic and shallow semantic kernels
for question answer classification. In Proceedings ACL,
Prague, Czech Republic.
Ng, V. and Cardie, C. (2002). Improving machine learning
approaches to coreference resolution. In Proc. ACL 2002.
Paice, C. D. and Husk, G. D. (1987). Towards an automatic
recognition of anaphoric features in english text: The im-
personal pronoun ?it?. Computer Speech and Language,
2:109?132.
Shawe-Taylor, J. and Cristianini, N. (2004). Kernel Methods
for Pattern Analysis. Cambridge University Press.
Sidner, C. (1979). Toward a computational theory of definite
anaphora comprehension in english. Technical report AI-
TR-537, MIT, Cambridge, MA.
Soon, W., Ng, H., and Lim, D. (2001). A machine learning
approach to coreference resolution of noun phrases. CL,
27(4):521?544.
Steinberger, J., Poesio, M., Kabadjov, M., and Jezek, K.
(2007). Two uses of anaphora resolution in summarization.
Information Processing and Management, 43:1663?1680.
Special issue on Summarization.
Sturt, P. (2003). The time-course of the application of binding
constraints in reference resolution. Journal of Memory and
Language.
Vapnik, V. (1995). The Nature of Statistical Learning Theory.
Springer.
Vieira, R. and Poesio, M. (2000). An empirically based sys-
tem for processing definite descriptions. CL, 27(4):539?
592.
Yang, X., Su, J., and Tan, C. (2006). Kernel-based pronoun
resolution with structured syntactic knowledge. In Proc.
COLING-ACL 06.
Yang, X., Su, J., Zhou, G., and Tan, C. (2004). Improving pro-
noun resolution by incorporating coreferential information
of candidates. In Proc. ACL 2004.
Zanzotto, F. M. and Moschitti, A. (2006). Automatic learn-
ing of textual entailments with cross-pair similarities. In
Proceedings of Coling-ACL, Sydney, Australia.
Zelenko, D., Aone, C., and Richardella, A. (2003). Kernel
methods for relation extraction. JMLR, 3(6):1083 ? 1106.
968
Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 496?505, Prague, June 2007. c?2007 Association for Computational Linguistics
Antecedent Selection Techniques for High-Recall Coreference Resolution
Yannick Versley
versley@sfs.uni-tuebingen.de
SFB 441 / Seminar fu?r Sprachwissenschaft
Universita?t Tu?bingen
Abstract
We investigate methods to improve the re-
call in coreference resolution by also trying
to resolve those definite descriptions where
no earlier mention of the referent shares the
same lexical head (coreferent bridging). The
problem, which is notably harder than iden-
tifying coreference relations among men-
tions which have the same lexical head, has
been tackled with several rather different ap-
proaches, and we attempt to provide a mean-
ingful classification along with a quantita-
tive comparison. Based on the different mer-
its of the methods, we discuss possibilities to
improve them and show how they can be ef-
fectively combined.
1 Introduction
Coreference resolution, the task of grouping men-
tions in a text that refer to the same referent in the
real world, has been shown to be beneficial for a
number of higher-level tasks such as information ex-
traction (McCarthy and Lehnert, 1995), question an-
swering (Morton, 2000) and summarisation (Stein-
berger et al, 2005).
While the resolution of pronominal anaphora and
tracking of named entities is possible with good
accuracy, the resolution of definite NPs (having a
common noun as their head) is usually limited to
the cases that Vieira and Poesio (2000) call direct
coreference, where both coreferent mentions have
the same head. The other cases, called coreferent
bridging by Vieira and Poesio1, are notably harder
because the number of potential candidates is much
1Because bridging (in the sense of Clark, 1975, or Asher and
larger when it is no longer possible to rely on surface
similarity.
To overcome the limit of recall that is encoun-
tered when only relying on surface features, newer
systems for coreference resolutions (Daume? III and
Marcu, 2005; Ponzetto and Strube, 2006; Versley,
2006; Ng, 2007, inter alia) use lexical semantic in-
formation as an indication for semantic compati-
bility in the absence of head equality. Most cur-
rent systems integrate the identification of discourse-
new definites (i.e., cases like ?the sun? or ?the man
that Ben met yesterday?, which are definite, but
not anaphoric) with the antecedent selection proper,
which implies that the gain obtained for new features
is dependent on the feature?s usefulness both in find-
ing semantically related mentions and for the use in
detecting discourse-new definites.
One goal of this paper is to provide a better under-
standing of these information sources by comparing
proposed (and partly new) approaches for resolv-
ing coreferent bridging by separately considering
the task of antecedent selection (i.e., presupposing
that discourse-new markables have been identified
beforehand). Although state of the art methods for
modular discourse-new detection (Uryupina, 2003;
Poesio et al, 2005) do not achieve near-perfect accu-
racy for discourse-new detection, the results we give
for antecedent selection represent an upper bound
on recall and precision for the full coreference task,
and we think that this upper bound will be useful for
Lascarides, 1998) is a much broader concept, the term ?corefer-
ent bridging? is potentially confusing, as many cases are exam-
ples of perfectly well-behaved anaphoric definite noun phrases.
Because we want to emphasise the important difference to the
more easily resolved cases of same-head coreference, we will
stick with ?coreferent bridging? as the only term that has been
established for this in the literature.
496
the design of features in both systems using a mod-
ular approach, such as (Poesio et al, 2005), where
the decision on discourse-newness is taken before-
hand, and those that integrate discourse-new classifi-
cation with the actual resolution of coreferent bridg-
ing cases. In contrast to earlier investigations (Mark-
ert and Nissim, 2005; Garera and Yarowsky, 2006),
we provide a more extensive overview on features
and also discuss properties that influence their com-
binability.
Several approaches have been proposed for the
treatment of coreferent bridging. Poesio et al (1997)
use WordNet, looking for a synonymy or hypernymy
relation (additionally, for coordinate sisters in Word-
Net). The system of Cardie and Wagstaff (1999)
uses the node distance in WordNet (with an upper
limit of 4) as one component in the distance measure
that guides their clustering algorithm. Harabagiu
et al (2001) use paths through Wordnet, using not
only synonym and is-a relations, but also parts, mor-
phological derivations, gloss texts and polysemy,
which are weighted with a measure based on the re-
lation types and number of path elements. Other ap-
proaches use large corpora to get an indication for
bridging relations: Poesio et al (1998) use a general
word association metric based on common terms oc-
curing in a fixed-width window, Gasperin and Vieira
(2004) use syntactic contexts of words in a large cor-
pus to induce a semantic similarity measure (similar
to the one introduced by Lin, 1998), and then use
lists of the n nouns that are (globally) most sim-
ilar to a given noun. Markert and Nissim (2005)
mine the World Wide Web for shallow patterns like
?China and other countries?, indicating an is-a rela-
tionship. Finally, Garera and Yarowsky (2006) pro-
pose an association-based approach using nouns that
occur in a 2-sentence window before a definite de-
scription that has no same-head antecedent.
1.1 Lexical vs. Referential Relations
One important property of these information sources
is the kind of lexical relations that they detect. The
lexical relations that we expect in coreferent bridg-
ing cases are:
? instance: The antecedent is an instance of the
concept denoted by the anaphor
Corsica . . . the island
? synonymy: The antecedent and the anaphor are
synonyms
the automobile . . . the car
? hyperonymy: The anaphor is a strict generali-
sation of the antecedent
the murderer . . . the man
? near-synonymy: The anaphor and antecedent
are semantically related but not synonyms in
the strict sense
the CD . . . the album
Of course, not all cases of coreferent bridging realise
such a lexical relation, as sometimes the anaphor
takes up information introduced elsewhere than in
the lexical noun phrase head (Peter was found dead
in his flat . . . the deceased), or the coreference rela-
tion is forced by the discourse structure, without the
items being lexically related.
As an illustrating example, in
(1) John walked towards [1 the house].
(2) a. [1 The building] was illuminated.
b. [1 The manor] was guarded by dogs.
c. [2 The door] was open.
Typical cases of coreference include cases like
1,2a (hypernym) or 1,2b (compatible but non-
synonymous term). The discourse in 1,2c is an
example of associative bridging between the NP
?the door? and its antecedent to ?the house?; it
is inferred that the door must be part of the house
mentioned earlier (since doors are typically part of
a house), which is not compatible with coreferent
bridging, but is also ranked highly by association
measures.
While hypernym relations (as found by hypernym
lookup in WordNet, or patterns indicating such rela-
tions in unannotated texts) are usually a strong in-
dicator of coreference, they can only cover some
of the cases, while the near-synonymous cases are
left undiscovered. Similarity and association mea-
sures can help for the cases of near-synonymy. How-
ever, while similarity measures (such as WordNet
distance or Lin?s similarity metric) only detect cases
of semantic similarity, association measures (such
as the ones used by Poesio et al, or by Garera
and Yarowsky) also find cases of associative bridg-
497
Lin98 RFF TheY TheY:G2 PL03
Land (country/state/land)
Staat Staat Kemalismus Regierung Kontinent
state state Kemalism government continent
Stadt Stadt Bauernfamilie Pra?sident Region
city city agricultural family president region
Region Landesregierung Bankgesellschaft Dollar Stadt
region country government banking corporation dollar city
Bundesrepublik Bundesregierung Baht Albanien Staat
federal republic federal government Baht Albania state
Republik Gewerkschaft Gasag Hauptstadt Bundesland
republic trade union (a gas company) capital state
Medikament (medical drug)
Arzneimittel Pille RU Patient Arzneimittel
pharmaceutical pill (a drug?) patient pharmaceutical
Pra?parat Droge Abtreibungspille Arzt Lebensmittel
preparation drug (non-medical) abortion pill doctor foodstuff
Pille Pra?parat Viagra Pille Pra?parat
pill preparation Viagra pill preparation
Hormon Pestizid Pharmakonzern Behandlung Behandlung
hormone pesticide pharmaceutical company treatment treatment
Lebensmittel Lebensmittel Pra?parat Abtreibungspille Arznei
foodstuff foodstuff preparation abortion pill drug
highest ranked words, with very rare words removed
?: RU 486, an abortifacient drug
Lin98: Lin?s distributional similarity measure (Lin, 1998)
RFF: Geffet and Dagan?s Relative Feature Focus measure (Geffet and Dagan, 2004)
TheY: association measure introduced by Garera and Yarowsky (2006)
TheY:G2: similar method using a log-likelihood-based statistic (see Dunning 1993)
this statistic has a preference for higher-frequency terms
PL03: semantic space association measure proposed by Pado? and Lapata (2003)
Table 1: Similarity and association measures: most similar items
ing like 1a,b; the result of this can be seen in ta-
ble (2): while the similarity measures (Lin98, RFF)
list substitutable terms (which behave like synonyms
in many contexts), the association measures (Garera
and Yarowsky?s TheY measure, Pado? and Lapata?s
association measure) also find non-compatible asso-
ciations such as country?capital or drug?treatment,
which is why they are commonly called relation-
free. For the purpose of coreference resolution, how-
ever we do not want to resolve ?the door? to the an-
tecedent ?the house? as the two descriptions do not
corefer, and it may be useful to filter out non-similar
associations.
1.2 Information Sources
Different resources may be differently suited for
the recognition of the various relations. Gener-
ally, it would be expected that using a wordnet
is the best solution if we are interested in an isa-
like relation between two words. On the other
hand, wordnets usually have limited coverage both
in terms of lexical items and in terms of relations
encoded (as their construction is necessarily labor-
intensive), and ? as Markert and Nissim remark
? they do not (and arguably should not) contain
context-dependent relations that do not hold gener-
ally but only in some rather specific context, for ex-
ample steel being anaphorically described as a com-
modity in a financial text. Context-dependent rela-
tions, Markert and Nissim argue, can be found using
shallow patterns (for example, steel and other com-
modities), since a use in such a context would mean
that the idiosyncratic conceptual relation holds in
that context. Wordnets also have usually have poor
(or non-existant) coverage of named entities, which
are especially relevant for instance relations; this
kind of instance relations can often be found in large
text corpora. The high-precision patterns that Mark-
ert and Nissim use only occur infrequently, but the
approach using shallow patterns allows to perform
498
the search of the World Wide Web, which somewhat
alleviates the sparse data problem.
While some near-synonyms can be found by look-
ing at the distance in a wordnet, they may be far
apart from each other because of ontological mod-
eling decisions, or lexical items not covered by the
wordnet. Similarity and association measures can
provide greater coverage for these near-synonym re-
lations.
The measures both of Lin (1998) and of Pado? and
Lapata (2003, 2007) are distributional methods; for
each word, they create a distribution of the contexts
they occur in, and similarity between two words is
calculated as the similarity of these distributions.2
The difference in these two methods is the repre-
sentation of the contexts. While Lin uses contexts
that are expected to determine semantic preferences
(like being in the direct object position of one verb),
Pado? and Lapata only use the co-occuring words,
weighted by syntax-based distance. For example, in
(3) Peter subj? likes dobj? ice-cream.
Lin?s approach would yield ?subj :like for Peter
and ?dobj :like for ice-cream, while Pado? and
Lapata?s approach would yield the contexts like
(with a weight of 1.0) and ice-cream (with a
weight of 0.5) for Peter. As a consequence, Pado?
and Lapata?s measure is more robust against data
sparseness but also finds related non-similar terms
(which are ultimately unwanted for coreference res-
olution). Pado? and Lapata show their dependency-
based measure to perform better in a word sense
disambiguation task than the measure of Lund et al
(1995), on which Poesio et al (1998) based their ex-
periments and which is based on the surface distance
of words.
We also reimplemented the approach of Gar-
era and Yarowsky (2006), who extract potential
anaphor-antecedent pairs from unlabeled texts and
rank these potentially related pairs by the mutual in-
formation statistic. As an example, in a text like
(4) Peter likes ice-cream.
The boy devours tons of it.
2Both measures use a weighted Jaccard metric on mutual
information vectors to calculate the similarity. See Weeds and
Weir (2005) for an overview of other measures.
we would extract the pairs ?boy,(person)? and
?boy,ice-cream?, in the hope that the former
pair occurs comparatively more often and gets a
higher mutual information value.
2 Experiments on Antecedent Selection
In a setting similar to Markert and Nissim (2005),
we evaluate the precision (proportion of correct
cases in the resolved cases) and recall (correct cases
to all cases) for the resolution of discourse-old def-
inite noun phrases. Before trying to resolve coref-
erent bridging cases, we look for compatible an-
tecedent candidates with the same lexical head and
resolve to the nearest such candidate if there is one.
For our experiments, we used the first 125 articles
of the coreferentially annotated Tu?Ba-D/Z corpus of
written newspaper text (Hinrichs et al, 2005), to-
talling 2239 sentences with 633 discourse-old defi-
nite descriptions, and the latest release of GermaNet
(Kunze and Lemnitzer, 2002), which is the German-
language part of EuroWordNet.
Unlike Markert and Nissim, we did not limit the
evaluation to discourse-old noun phrases where an
antecedent is in the 4 preceding sentences, but also
included cases where the antecedent is further away.
As a real coreference resolution system would have
to either resolve them correctly or leave them unre-
solved, we feel that this is less unrealistic and thus
preferable even when it gives less optimistic evalu-
ation results. Because overall precision is a mixture
of the precision of the same-head resolver and the
precision of the resolution for coreferent bridging,
which is lower than that for same-head cases, we
forcibly get less precision if we resolve more coref-
erent bridging cases. As it is always possible to im-
prove overall precision by resolving fewer cases of
coreferent bridging, we separately mention the pre-
cision for coreferent bridging cases alone (i.e., num-
ber of correct coreferent bridging cases by all re-
solved coreferent bridging cases), which we deem
more informative.
In our evaluation, we included hypernymy search
and a simple edge-based distance based on Ger-
maNet, as well as a baseline using semantic classes
(automatically determined by a combination of sim-
ple named entity classification and GermaNet sub-
sumption), as well as an evolved version of Markert
499
Prec Recl F?=1 Prec.NSH
same-head 0.87 0.50 0.63 ?
nearest(1) (only number check) 0.57 0.55 0.56 0.12
semantic class+gender check(1) 0.68 0.61 0.64 0.35
semantic class+gender check(2) 0.67 0.62 0.65 0.36
GermaNet, hypernymy lookup 0.83 0.58 0.68 0.67
GermaNet, node distance(1) 0.71 0.61 0.65 0.39
single pattern: ?Y wie X?(1) 0.83 0.54 0.66 0.55
TheY(1) (only number checking) 0.66 0.59 0.62 0.29
TheY(2) (only number checking) 0.66 0.60 0.63 0.31
Lin(1) (only number checking) 0.66 0.60 0.63 0.30
Lin(2) (only number checking) 0.69 0.64 0.66 0.39
PL03(1) (only number checking) 0.68 0.63 0.65 0.38
PL03(2) (only number checking) 0.70 0.64 0.65 0.42
15-most-similar(1) 0.82 0.54 0.65 0.50
100-most-similar(2,3) 0.73 0.60 0.66 0.42
Prec.NSH: precision for coreferent bridging cases
(1): consider candidates in the 4 preceding sentences
(2): consider candidates in the 16 preceding sentences
(3): also try candidates such that the anaphor is
in the antecedent?s similarity list
Table 2: Baseline results
and Nissim?s approach, which is presented in (Ver-
sley, 2007). For the methods based on similarity
and association measures, we implemented a simple
ranking by the respective similarity or relatedness
value. Additionally, we included an approach due to
Gasperin and Vieira (2004), who tackle the problem
of similarity by using lists of most similar words to a
certain word, based on a similarity measure closely
related to Lin?s. They allow resolution if either (i)
the candidate is among the words most similar to the
anaphor, (ii) the anaphor is among the words most
similar to the candidate, (iii) the similarity lists of
anaphor and candidate share a common item. We
tried out several variations in the length of the simi-
lar words list (Gasperin and Vieira used 15, we also
tried lists with 25, 50 and 100 items). The third pos-
sibility that Gasperin and Vieira mention (a common
item in the similarity lists of both anaphor and an-
tecedent) resolves some correct cases, but leads to a
much larger number of false positives, which is why
we did not include it in our evaluation.
To induce the similarity and association measures
presented earlier, we used texts from the German
newspaper die tageszeitung, comprising about 11M
sentences. For the extraction of anaphor-antecedent
candidates, we used a chunked version of the cor-
pus (Mu?ller and Ule, 2002). The identification of
grammatical relations, was carried out on a subset
of all sentences (those with length ? 30), with an
unlexicalised PCFG parser and subsequent extrac-
tion of dependency relations (Versley, 2005). For
the last approach, where dependency relations were
needed but labeling accuracy was not as important,
we used a deterministic shift-reduce parser that Foth
and Menzel (2006) used as input source in hybrid
dependency parsing.3
For all three approaches, we lemmatised the
words by using a combination of SMOR (Schmid
et al, 2004), a derivational finite-state morphology
for German, and lexical information derived from
the lexicon of a German dependency parser (Foth
and Menzel, 2006). We mitigated the problem of vo-
cabulary growth in the lexicon, due to German syn-
thetic compounds, by using a frequency-sensitive
unsupervised compound splitting technique, and
(for semantic similarity) normalised common person
and location names to ?(person)? and ?(location)?, re-
spectively.
Same-head resolution (including a check for
modifier compatibility) allows to correctly resolve
49.8% of all cases, with a precision of 86.5%.
The most simple approach for coreferent bridging,
just resolving coreferent bridging cases to the near-
est possible antecedent (only checking for number
agreement), yields very poor precision (12% for the
coreferent bridging cases), and as a result, the re-
call gain is very limited. If we use semantic classes
(based on both GermaNet and a simple classification
for named entities) to constrain the candidates and
then use the nearest number- and gender-compatible
antecedent4, we get a much better precision (35%
for coreferent bridging cases), and a much better
recall of 61.1%. Hyponymy lookup in GermaNet,
without a limit on sentence distance, achieves a re-
call of 57.5% (with a precision of 67% for the re-
solved coreferent bridging cases), whereas using the
best single pattern (Y wie X , which corresponds to
3Arguably, it would have been more convenient to use a sin-
gle parser for all three approaches, but differing tradeoffs be-
tween speed on one hand and accuracy for relevant information
and/or fitness of representation on the other hand made the re-
spective parser or chunker a compelling choice.
4In German, grammatical gender is not as predictive as in
English as it does not reproduce ontological distinctions. For
persons, grammatical and natural gender almost always coin-
cide, and we check gender equality iff the anaphor is a person.
500
the English Y s such as X), with a distance limit of
4 sentences5, on the Web only improves the recall
to 54.3% (with a lower precision of 55% for coref-
erent bridging cases). This is in contrast to the re-
sults of Markert and Nissim, who found that Web
pattern search performs better than wordnet lookup;
see (Versley, 2007) for a discussion. Ranking all
candidates that are within a distance of 4 hyper-
/hyponymy edges in GermaNet by their edge dis-
tance, we get a relatively good recall of 60.5%, but
the precision (for the coreferent bridging cases) is
only at 39%, which is quite poor in comparison.
The results for Garera and Yarowsky?s TheY al-
gorithm are quite disconcerting ? recall and the pre-
cision on coreferent bridging cases are lower than
the respective baseline using (wordnet-based) se-
mantic class information or Pado? and Lapata?s asso-
ciation measure. The technique based on Lin?s simi-
larity measure does outperform the baseline, but still
suffers from bad precision, along with Pado? and La-
pata?s association measure. In other words, the simi-
larity and association measures seem to be too noisy
to be used directly for ranking antecedents. The ap-
proach of Gasperin and Vieira performs compara-
bly to the approach using Web-based pattern search
(although the precision is poorer than for the best-
performing pattern for German, ?X wie Y ? ? X
such as Y , it is comparable to that of other patterns).
2.1 Improving Distributional Similarity?
While it would be na??ve to think that the methods
purely based on statistical similarity measures could
reach the accuracy that can be achieved with a hand-
constructed lexicalised ontology, it would of course
be nice if we could improve the quality of the se-
mantic similarity measure used in ranking and the
most-similar-word lists.
Geffet and Dagan (2004) propose an approach
to improve the quality of the feature vectors used
in distributional similarity measures: instead of
weighting features using the mutual information
value between the word and the feature, they pro-
pose to use a measure they call Relative Feature Fo-
cus: the sum of the similarities to the (globally) most
5There is a degradation in precision for the pattern-based
approach, but not for the GermaNet-based approach, which is
why we do not use a distance limit for the GermaNet-based ap-
proach.
similar words that share this feature.
By replacing mutual information values with RFF
values in Lin?s association measure, Geffet and Da-
gan were able to significantly improve the propor-
tion of substitutable words in the list of the most sim-
ilar words. In our experiments, however, using the
RFF-based similarity measure did not improve the
similarity-list-based resolution or the simple rank-
ing, to the contrary, both recall and precision are less
than for the Weighted Jaccard measure that we used
originally.6
We attribute this to two factors: Firstly, Geffet
and Dagan?s evaluation emphasises the precision in
terms of types, whereas the use in resolving coref-
erent bridging does not punish unrelated rare words
being ranked high ? since these are rare, the like-
lihood that they occur together, changing a reso-
lution decision, is quite low, whereas rare related
words that are ranked high can allow a correct res-
olution. Secondly, Geffet and Dagan focus on high-
frequency words, which makes sense in the context
of ontology learning, but the applicability for tasks
like coreference resolution (directly or in the ap-
proach of Gasperin and Vieira) also depends on a
sensible treatment of lower-frequency words.
Using the framework of Weeds et al (2004), we
found that the bias of lower frequency words for
preferring high-frequency neighbours was higher for
RFF (0.58 against 0.35 for Lin?s measure). Weeds
and Weir (2005) discuss the influence of bias to-
wards high- or low-frequency items for different
tasks (correlation with WordNet-derived neighbour
sets and pseudoword disambiguation), and it would
not be surprising if the different high-frequency bias
were leading to different results.
2.2 Combining Information Sources
The information sources that we presented earlier
and the corpus-based methods based on similarity
or association measures draw from different kinds of
evidence and thus should be rather complementary.
To put it another way, it should be possible to get
the best from all methods, achieving the recall of the
high-recall methods (like using semantic class in-
6Simple ranking with RFF gives a precision of 33% for
coreferent bridging cases, against 39% for Lin?s original mea-
sure; for an approach based on similarity lists, we get 39%
against 44%.
501
15-most-similar(2,3)
100-most-similar(2,3)
GWN5: hypernymy
Web (combined)
semclass+gend(2)
Lin similarity(2)
Lin(2)+sem+gend
TheY+sem+gend
Lin+Bnd
PL03+Bnd
GWN5?Web
GWN5?TheY+s+g
GWN5?Web?25-m.s.(2,3) ?LinBnd
all combined
Recall (total)
Pre
cisi
on
(No
n-
sa
m
e-h
ea
d)
0.550 0.600 0.650 0.7000.30
0.40
0.50
0.60
Prec Recl F?=1 Prec.NSH
sem. class+gender checking 0.68 0.61 0.64 0.35
GermaNet, hypernymy lookup 0.83 0.57 0.68 0.67
GermaNet ? ?Y wie X? 0.81 0.60 0.69 0.63
GermaNet ? all patterns 0.81 0.61 0.70 0.64
TheY(2)+semclass+gender 0.76 0.60 0.67 0.47
TheY+sem+gend+Bnd 0.78 0.59 0.67 0.50
Lin(2)+semclass+gender 0.71 0.63 0.67 0.43
Lin+sem+gend+Bnd 0.80 0.58 0.67 0.53
PL03(2)+semclass+gender 0.72 0.64 0.68 0.45
PL03+sem+gend+Bnd 0.80 0.59 0.68 0.57
GermaNet ? all patterns 0.81 0.62 0.70 0.64
? 25-most-similar(2,3) 0.79 0.65 0.72 0.62
? LinBnd 0.79 0.68 0.73 0.63
? Lin ? TheY+sem+gend 0.74 0.70 0.72 0.54
(2): consider candidates in the 16 preceding sentences
(3): also try candidates such that the anaphor is
in the antecedent?s similarity list
Table 3: Combination-based approaches
formation, or similarity and association measures),
with a precision closer to the most precise method
using GermaNet. In the case of web-based patterns,
Versley (2007) combines several pattern searches on
the web and uses the combined positive and nega-
tive evidence to compute a composite score ? with a
suitably chosen cutoff, it outperforms all single pat-
terns both in terms of precision and recall. First re-
solving via hyponymy in GermaNet and then using
the pattern-combination approach outperforms the
semantic class-based baseline in terms of recall and
is reasonably close to the GermaNet-based approach
in terms of precision (i.e., much better than the ap-
proach based only on the semantic class).
As a first step to improve the precision of the
corpus-based approaches, we added filtering based
on automatically assigned semantic classes (per-
sons, organisations, events, other countable objects,
and everything else). Very surprisingly, Garera and
Yarowsky?s TheY approach, despite starting out at a
lower precision (31%, against 39% for Lin and 42%
for PL03), profits much more from the semantic fil-
ter and reaches the best precision (47%), whereas
Lin?s semantic similarity measure profits the least.
Since limiting the distance to the 4 previous sen-
tences had quite a devastating effect for the approach
based on Lin?s similarity measure (which achieves
39% precision when all the candidates are avail-
able and 30% precision if it choses the most se-
mantically similar out of the candidates that are in
the last 4 sentences), we also wanted to try and ap-
ply the distance-based filtering after finding seman-
tically related candidates.
The approach we tried was as follows: we rank all
candidates using the similarity function, and keep
only the 3 top-rated candidates. From these 3 top-
rated candidates, we keep only those within the last
4 sentences. Without filtering by semantic class, this
improves the precision to 41% (from 30% for lim-
iting the distance beforehand, or 39% without lim-
iting the distance). Adding filtering based on se-
mantic classes to this (only keeping those from the
3 top-rated candidates which have a compatible se-
mantic class and are within the last 4 sentences), we
get a much better precision of 53%, with a recall
that can still be seen as good (57.8%). In compari-
son with the similarity-list-based approach, we get a
much better precision than we would get for meth-
ods with comparable recall (the version with the 100
most similar items has 44% precision, the version
with 50 most similar items and matching both ways
has 46% precision).
Applying this distance-bounding method to Gar-
era and Yarowsky?s association measure still leads
to an improvement over the case with only seman-
tic and gender checking, but the improvement (from
47% to 50%) is not as large as with the semantic
similarity measure or Pado? and Lapata?s association
measure (from 45% to 57%).
For the final system, we back off from the most
precise information sources to the less precise. Start-
ing with the combination of GermaNet and pattern-
based search on the World Wide Web, we begin
by adding the distance-bounded semantic similarity-
based resolver (LinBnd) and resolution based on
the list of 25 most similar words (following the
502
approach of Gasperin and Vieira 2004). This re-
sults in visibly improved recall (from 62% to 68%),
while the precision for coreferent bridging cases
does not suffer much. Adding resolution based on
Lin?s semantic similarity measure and Garera and
Yarowsky?s TheY value leads to a further improve-
ment in recall to 69.7%, but also leads to a larger
loss in precision.
3 Conclusion
In this paper, we compared several approaches to re-
solve cases of coreferent bridging in open-domain
newspaper text. While none of the information
sources can match the precision of the hypernymy
information encoded in GermaNet, or that of using
a combination of high-precision patterns with the
World Wide Web as a very large corpus, it is possi-
ble to achieve a considerable improvement in terms
of recall without sacrificing too much precision by
combining these methods.
Very interestingly, the distributional methods
based on intra-sentence relations (Lin, 1998;
Pado? and Lapata, 2003) outperformed Garera and
Yarowsky?s (2006) association measure when used
for ranking, which may due to sparse data problems
or simply too much noise for the latter. For the asso-
ciation measures, the fact that they are relation-free
also means that they can profit from added semantic
filtering.
The novel distance-bounded semantic similarity
method (where we use the most similar words in the
previous discourse together with a semantic class-
based filter and a distance limit) comes near the pre-
cision of using surface patterns, and offers better ac-
curacy than Gasperin and Vieira?s method of using
the globally most similar words.
By combining existing higher-precision informa-
tion sources such as hypernym search in GermaNet
and the Web-based approach presented in (Vers-
ley, 2007) together with similarity- and association-
based resolution, it is possible to get a large im-
provement in recall even compared to the combined
GermaNet+Web approach or an approach combin-
ing GermaNet with a semantically filtered version
of Garera and Yarowsky?s TheY approach.
In independent research, Goecke et al (2006)
combined the original LSA-based method of Lund
et al (1995) with wordnet relations and pattern
search on a fixed-size corpus.7 However, they eval-
uate only on a small subset of discourse-old definite
descriptions (those where a wordnet-compatible se-
mantic relation was identified and which were rea-
sonably close to their antecedent), and they did not
distinguish coreferent from associative bridging an-
tecedents. Although the different evaluation method
disallows a meaningful comparison, we think that
the more evolved information sources we use (Pado?
and Lapata?s association measure instead of Lund
et als, combined pattern search on the World Wide
Web instead of search for patterns in a fixed-size
corpus), as well as the additional information based
on semantic similarity, lead to superior results when
evaluated in a comparable task.
3.1 Ongoing and Future Work
Both the distributional similarity statistics and the
association measure can profit from more training
data, something which is bound by availability of
similar text (Gasperin et al, 2004 point out that us-
ing texts from a different genre strongly limits the
usefulness of the learned semantic similarity mea-
sure), and by processing costs (which are more se-
rious for distributional similarity measures than for
non-grammar-related association measures, as the
former necessitate parsed input).
Based on existing results for named entity coref-
erence, a hypothetical coreference resolver combin-
ing our information sources with a perfect detec-
tor for discourse-new mentions would be able to
achieve a precision of 88% and a recall of 83% con-
sidering all full noun phrases (i.e., including names,
but not pronouns). This is both much higher than
state-of-the art results for the same data set (Versley,
2006, gets 62% precision and 70% recall), but such
accuracy may be very difficult to achieve in prac-
tice, as perfect (or even near-perfect) discourse-new
detection does not seem to achievable in the near fu-
ture. Preliminary experiments show that the inte-
gration of pattern-based information leads to an in-
crease in recall of 0.6% for the whole system (or
46% more coreferent bridging cases), but the inte-
gration of distributional similarity (loosely based on
the approach by Gasperin and Vieira) does not lead
7Thanks to Tonio Wandmacher for pointing this out to me at
GLDV?07.
503
to a noticeable improvement over GermaNet alne;
in isolation, the distributional similarity information
did improve the recall, albeit less than information
from GermaNet did.
The fact that only a small fraction of the achiev-
able recall gain is currently attained seems to sug-
gest that better identification of discourse-old men-
tions could potentially lead to larger improvements.
It also seems that firstly, it makes more sense to com-
bine information sources that cover different rela-
tions (e.g. GermaNet for hypernymy and synonymy
and the pattern-based approach for instance rela-
tions) than those that yield independent evidence for
the same relation(s), as GermaNet and the Gasperin
and Vieira approach do for (near-)synonymy; and
secondly, that good precision is especially important
in the context of integrating antecedent selection and
discourse-new identification, which means that the
finer view that we get using antecedent selection ex-
periments (compared to direct use in a coreference
resolver) is indeed helpful.
Acknowledgements I am very grateful to Sabine
Schulte im Walde, Piklu Gupta and Sandra Ku?bler
for useful criticism of an earlier version, and to
Simone Ponzetto and Michael Strube for feedback
on a talk related to this paper. The research re-
ported in this paper was supported by the Deutsche
Forschungsgemeinschaft (DFG) as part of Collab-
orative Research Centre (Sonderforschungsbereich)
441 ?Linguistic Data Structures?.
References
Asher, N. and Lascarides, A. (1998). Bridging. Jour-
nal of Semantics, 15(1):83?113.
Cardie, C. and Wagstaff, K. (1999). Noun phrase
coreference as clustering. In Proceedings of the
Joint Conference on Empirical Methods in Natu-
ral Language Processing and Very Large Corpora
(EMNLP/VLC 1999), pages 82?89.
Clark, H. H. (1975). Bridging. In Schank, R. C. and
Nash-Webber, B. L., editors, Proceedings of the
1975 workshop on Theoretical issues in natural
language processing, pages 169?174, Cambridge,
MA. Association for Computing Machinery.
Daume? III, H. and Marcu, D. (2005). A large-
scale exploration of effective global features for
a joint entity detection and tracking model. In
HLT/EMNLP?05, pages 97?104.
Dunning, T. (1993). Accurate methods for the statis-
tics of surprise and coincidence. Computational
Linguistics, 19(1):61?74.
Foth, K. and Menzel, W. (2006). Hybrid pars-
ing: Using probabilistic models as predictors for
a symbolic parser. In ACL 2006.
Garera, N. and Yarowsky, D. (2006). Resolving and
generating definite anaphora by modeling hyper-
nymy using unlabeled corpora. In CoNLL 2006.
Gasperin, C., Salmon-Alt, S., and Vieira, R. (2004).
How useful are similarity word lists for indirect
anaphora resolution? In Proc. DAARC 2004.
Gasperin, C. and Vieira, R. (2004). Using word
similarity lists for resolving indirect anaphora. In
ACL?04 workshop on reference resolution and its
applications.
Geffet, M. and Dagan, I. (2004). Feature vector
quality and distributional similarity. In CoLing
2004.
Goecke, D., Stu?hrenberg, M., and Wandmacher, T.
(2006). Extraction and representation of seman-
tic relations for resolving definite descriptions. In
Workshop on Ontologies in Text Technology (OTT
2006). extended abstract.
Harabagiu, S., Bunescu, R., and Maiorano, S.
(2001). Text and knowledge mining for corefer-
ence resolution. In Proceedings of the 2nd Meet-
ing of the North American Chapter of the Associa-
tion of Computational Linguistics (NAACL-2001).
Hinrichs, E., Ku?bler, S., and Naumann, K. (2005). A
unified representation for morphological, syntac-
tic, semantic and referential annotations. In ACL
Workshop on Frontiers in Corpus Annotation II:
Pie in the Sky, Ann Arbor.
Kunze, C. and Lemnitzer, L. (2002). Germanet ?
representation, visualization, application. In Pro-
ceedings of LREC 2002.
Lin, D. (1998). Automatic retrieval and clustering
of similar words. In Proc. CoLing/ACL 1998.
Lund, K., Atchley, R. A., and Burgess, C.
(1995). Semantic and associative priming in high-
dimensional semantic space. In Proc. of the 17th
Annual Conference of the Cognitive Science Soci-
ety, pages 660?665.
504
Markert, K. and Nissim, M. (2005). Comparing
knowledge sources for nominal anaphora resolu-
tion. Computational Linguistics, 31(3):367?402.
McCarthy, J. F. and Lehnert, W. G. (1995). Using
decision trees for coreference resolution. In IJCAI
1995, pages 1050?1055.
Morton, T. S. (2000). Coreference for NLP applica-
tions. In ACL-2000.
Mu?ller, F. H. and Ule, T. (2002). Annotating topo-
logical fields and chunks ? and revising POS tags
at the same time. In Proceedings of the Nineteenth
International Conference on Computational Lin-
guistics (COLING 2002).
Ng, V. (2007). Shallow semantics for coreference
resolution. In IJCAI 2007, pages 1689?1694.
Pado?, S. and Lapata, M. (2003). Constructing se-
mantic space models from parsed corpora. In Pro-
ceedings of ACL 2003.
Pado?, S. and Lapata, M. (2007). Dependency-based
construction of semantic space models. Compu-
tational Linguistics, to appear.
Poesio, M., Alexandrov-Kabadjov, M., Vieira, R.,
Goulart, R., and Uryupina, O. (2005). Does
discourse-new detection help definite description
resolution? In Proceedings of the 6th Inter-
national Workshop on Computational Semantics
(IWCS-6).
Poesio, M., Schulte im Walde, S., and Brew, C.
(1998). Lexical clustering and definite descrip-
tion interpretation. In AAAI Spring Symposium
on Learning for Discourse.
Poesio, M., Vieira, R., and Teufel, S. (1997). Re-
solving bridging descriptions in unrestricted text.
In ACL-97 Workshop on Operational Factors in
Practical, Robust, Anaphora Resolution For Un-
restricted Texts.
Ponzetto, S. P. and Strube, M. (2006). Exploiting
semantic role labeling, wordnet and wikipedia for
coreference resolution. In HLT-NAACL 2006.
Schmid, H., Fitschen, A., and Heid, U. (2004).
SMOR: A german computational morphology
covering derivation, composition and inflection.
In Proceedings of LREC 2004.
Steinberger, J., Kabadjov, M., Poesio, M., and
Sanchez-Graillet, O. (2005). Improving LSA-
based summarization with anaphora resolution.
In Proceedings of Human Language Technology
Conference and Conference on Empirical Meth-
ods in Natural Language Processing, pages 1?8.
Uryupina, O. (2003). High-precision identification
of discourse new and unique noun phrases. In
Proceedings of the ACL Student Workshop.
Versley, Y. (2005). Parser evaluation across text
types. In Proceedings of the Fourth Workshop on
Treebanks and Linguistic Theories (TLT 2005).
Versley, Y. (2006). A constraint-based approach
to noun phrase coreference resolution in German
newspaper text. In Konferenz zur Verarbeitung
Natu?rlicher Sprache (KONVENS 2006).
Versley, Y. (2007). Using the Web to resolve
coreferent bridging in German newspaper text.
In Proceedings of GLDV-Fru?hjahrstagung 2007,
Tu?bingen. Narr.
Vieira, R. and Poesio, M. (2000). An empirically
based system for processing definite descriptions.
Computational Linguistics, 26(4):539?593.
Weeds, J. and Weir, D. (2005). Co-occurrence re-
trieval: A flexible framework for lexical distri-
butional similarity. Computational Linguistics,
31(4):439?475.
Weeds, J., Weir, D., and McCarthy, D. (2004). Char-
acterizing measures of lexical distributional simi-
larity. In CoLing 2004.
505
Proceedings of the ACL-08: HLT Demo Session (Companion Volume), pages 9?12,
Columbus, June 2008. c?2008 Association for Computational Linguistics
BART: A Modular Toolkit for Coreference Resolution
Yannick Versley
University of Tu?bingen
versley@sfs.uni-tuebingen.de
Simone Paolo Ponzetto
EML Research gGmbH
ponzetto@eml-research.de
Massimo Poesio
University of Essex
poesio@essex.ac.uk
Vladimir Eidelman
Columbia University
vae2101@columbia.edu
Alan Jern
UCLA
ajern@ucla.edu
Jason Smith
Johns Hopkins University
jsmith@jhu.edu
Xiaofeng Yang
Inst. for Infocomm Research
xiaofengy@i2r.a-star.edu.sg
Alessandro Moschitti
University of Trento
moschitti@dit.unitn.it
Abstract
Developing a full coreference system able
to run all the way from raw text to seman-
tic interpretation is a considerable engineer-
ing effort, yet there is very limited avail-
ability of off-the shelf tools for researchers
whose interests are not in coreference, or for
researchers who want to concentrate on a
specific aspect of the problem. We present
BART, a highly modular toolkit for de-
veloping coreference applications. In the
Johns Hopkins workshop on using lexical
and encyclopedic knowledge for entity dis-
ambiguation, the toolkit was used to ex-
tend a reimplementation of the Soon et al
(2001) proposal with a variety of additional
syntactic and knowledge-based features, and
experiment with alternative resolution pro-
cesses, preprocessing tools, and classifiers.
1 Introduction
Coreference resolution refers to the task of identify-
ing noun phrases that refer to the same extralinguis-
tic entity in a text. Using coreference information
has been shown to be beneficial in a number of other
tasks, including information extraction (McCarthy
and Lehnert, 1995), question answering (Morton,
2000) and summarization (Steinberger et al, 2007).
Developing a full coreference system, however, is
a considerable engineering effort, which is why a
large body of research concerned with feature en-
gineering or learning methods (e.g. Culotta et al
2007; Denis and Baldridge 2007) uses a simpler but
non-realistic setting, using pre-identified mentions,
and the use of coreference information in summa-
rization or question answering techniques is not as
widespread as it could be. We believe that the avail-
ability of a modular toolkit for coreference will sig-
nificantly lower the entrance barrier for researchers
interested in coreference resolution, as well as pro-
vide a component that can be easily integrated into
other NLP applications.
A number of systems that perform coreference
resolution are publicly available, such as GUITAR
(Steinberger et al, 2007), which handles the full
coreference task, and JAVARAP (Qiu et al, 2004),
which only resolves pronouns. However, literature
on coreference resolution, if providing a baseline,
usually uses the algorithm and feature set of Soon
et al (2001) for this purpose.
Using the built-in maximum entropy learner
with feature combination, BART reaches 65.8%
F-measure on MUC6 and 62.9% F-measure on
MUC7 using Soon et al?s features, outperforming
JAVARAP on pronoun resolution, as well as the
Soon et al reimplementation of Uryupina (2006).
Using a specialized tagger for ACE mentions and
an extended feature set including syntactic features
(e.g. using tree kernels to represent the syntactic
relation between anaphor and antecedent, cf. Yang
et al 2006), as well as features based on knowledge
extracted from Wikipedia (cf. Ponzetto and Smith, in
preparation), BART reaches state-of-the-art results
on ACE-2. Table 1 compares our results, obtained
using this extended feature set, with results from
Ng (2007). Pronoun resolution using the extended
feature set gives 73.4% recall, coming near special-
ized pronoun resolution systems such as (Denis and
Baldridge, 2007).
9
Figure 1: Results analysis in MMAX2
2 System Architecture
The BART toolkit has been developed as a tool to
explore the integration of knowledge-rich features
into a coreference system at the Johns Hopkins Sum-
mer Workshop 2007. It is based on code and ideas
from the system of Ponzetto and Strube (2006), but
also includes some ideas from GUITAR (Steinberger
et al, 2007) and other coreference systems (Versley,
2006; Yang et al, 2006). 1
The goal of bringing together state-of-the-art ap-
proaches to different aspects of coreference res-
olution, including specialized preprocessing and
syntax-based features has led to a design that is very
modular. This design provides effective separation
of concerns across several several tasks/roles, in-
cluding engineering new features that exploit dif-
ferent sources of knowledge, designing improved or
specialized preprocessing methods, and improving
the way that coreference resolution is mapped to a
machine learning problem.
Preprocessing To store results of preprocessing
components, BART uses the standoff format of the
MMAX2 annotation tool (Mu?ller and Strube, 2006)
with MiniDiscourse, a library that efficiently imple-
ments a subset of MMAX2?s functions. Using a
generic format for standoff annotation allows the use
of the coreference resolution as part of a larger sys-
tem, but also performing qualitative error analysis
using integrated MMAX2 functionality (annotation
1An open source version of BART is available from
http://www.sfs.uni-tuebingen.de/?versley/BART/.
diff, visual display).
Preprocessing consists in marking up noun
chunks and named entities, as well as additional in-
formation such as part-of-speech tags and merging
these information into markables that are the start-
ing point for the mentions used by the coreference
resolution proper.
Starting out with a chunking pipeline, which
uses a classical combination of tagger and chun-
ker, with the Stanford POS tagger (Toutanova et al,
2003), the YamCha chunker (Kudoh and Mat-
sumoto, 2000) and the Stanford Named Entity Rec-
ognizer (Finkel et al, 2005), the desire to use richer
syntactic representations led to the development of
a parsing pipeline, which uses Charniak and John-
son?s reranking parser (Charniak and Johnson, 2005)
to assign POS tags and uses base NPs as chunk
equivalents, while also providing syntactic trees that
can be used by feature extractors. BART also sup-
ports using the Berkeley parser (Petrov et al, 2006),
yielding an easy-to-use Java-only solution.
To provide a better starting point for mention de-
tection on the ACE corpora, the Carafe pipeline
uses an ACE mention tagger provided by MITRE
(Wellner and Vilain, 2006). A specialized merger
then discards any base NP that was not detected to
be an ACE mention.
To perform coreference resolution proper, the
mention-building module uses the markables cre-
ated by the pipeline to create mention objects, which
provide an interface more appropriate for corefer-
ence resolution than the MiniDiscourse markables.
These objects are grouped into equivalence classes
by the resolution process and a coreference layer is
written into the document, which can be used for de-
tailed error analysis.
Feature Extraction BART?s default resolver goes
through all mentions and looks for possible an-
tecedents in previous mentions as described by Soon
et al (2001). Each pair of anaphor and candi-
date is represented as a PairInstance object,
which is enriched with classification features by fea-
ture extractors, and then handed over to a machine
learning-based classifier that decides, given the fea-
tures, whether anaphor and candidate are corefer-
ent or not. Feature extractors are realized as sepa-
rate classes, allowing for their independent develop-
10
Figure 2: Example system configuration
ment. The set of feature extractors that the system
uses is set in an XML description file, which allows
for straightforward prototyping and experimentation
with different feature sets.
Learning BART provides a generic abstraction
layer that maps application-internal representations
to a suitable format for several machine learning
toolkits: One module exposes the functionality of
the the WEKA machine learning toolkit (Witten
and Frank, 2005), while others interface to special-
ized state-of-the art learners. SVMLight (Joachims,
1999), in the SVMLight/TK (Moschitti, 2006) vari-
ant, allows to use tree-valued features. SVM Classi-
fication uses a Java Native Interface-based wrapper
replacing SVMLight/TK?s svm classify pro-
gram to improve the classification speed. Also in-
cluded is a Maximum entropy classifier that is
based upon Robert Dodier?s translation of Liu and
Nocedal?s (1989) L-BFGS optimization code, with
a function for programmatic feature combination.2
Training/Testing The training and testing phases
slightly differ from each other. In the training phase,
the pairs that are to be used as training examples
have to be selected in a process of sample selection,
whereas in the testing phase, it has to be decided
which pairs are to be given to the decision function
and how to group mentions into equivalence rela-
tions given the classifier decisions.
This functionality is factored out into the en-
2see http://riso.sourceforge.net
coder/decoder component, which is separate from
feature extraction and machine learning itself. It
is possible to completely change the basic behav-
ior of the coreference system by providing new
encoders/decoders, and still rely on the surround-
ing infrastructure for feature extraction and machine
learning components.
3 Using BART
Although BART is primarily meant as a platform for
experimentation, it can be used simply as a corefer-
ence resolver, with a performance close to state of
the art. It is possible to import raw text, perform
preprocessing and coreference resolution, and either
work on the MMAX2-format files, or export the re-
sults to arbitrary inline XML formats using XSL
stylesheets.
Adapting BART to a new coreferentially anno-
tated corpus (which may have different rules for
mention extraction ? witness the differences be-
tween the annotation guidelines of MUC and ACE
corpora) usually involves fine-tuning of mention cre-
ation (using pipeline and MentionFactory settings),
as well as the selection and fine-tuning of classi-
fier and features. While it is possible to make rad-
ical changes in the preprocessing by re-engineering
complete pipeline components, it is usually possi-
ble to achieve the bulk of the task by simply mix-
ing and matching existing components for prepro-
cessing and feature extraction, which is possible by
modifying only configuration settings and an XML-
11
BNews NPaper NWire
Recl Prec F Recl Prec F Recl Prec F
basic feature set 0.594 0.522 0.556 0.663 0.526 0.586 0.608 0.474 0.533
extended feature set 0.607 0.654 0.630 0.641 0.677 0.658 0.604 0.652 0.627
Ng 2007? 0.561 0.763 0.647 0.544 0.797 0.646 0.535 0.775 0.633
?: ?expanded feature set? in Ng 2007; Ng trains on the entire ACE training corpus.
Table 1: Performance on ACE-2 corpora, basic vs. extended feature set
based description of the feature set and learner(s)
used.
Several research groups focusing on coreference
resolution, including two not involved in the ini-
tial creation of BART, are using it as a platform
for research including the use of new information
sources (which can be easily incorporated into the
coreference resolution process as features), different
resolution algorithms that aim at enhancing global
coherence of coreference chains, and also adapting
BART to different corpora. Through the availability
of BART as open source, as well as its modularity
and adaptability, we hope to create a larger com-
munity that allows both to push the state of the art
further and to make these improvements available to
users of coreference resolution.
Acknowledgements We thank the CLSP at Johns
Hopkins, NSF and the Department of Defense for
ensuring funding for the workshop and to EML
Research, MITRE, the Center for Excellence in
HLT, and FBK-IRST, that provided partial support.
Yannick Versley was supported by the Deutsche
Forschungsgesellschaft as part of SFB 441 ?Lin-
guistic Data Structures?; Simone Paolo Ponzetto has
been supported by the Klaus Tschira Foundation
(grant 09.003.2004).
References
Charniak, E. and Johnson, M. (2005). Coarse-to-fine n-best
parsing and maxent discriminative reranking. In Proc. ACL
2005.
Culotta, A., Wick, M., and McCallum, A. (2007). First-order
probabilistic models for coreference resolution. In Proc.
HLT/NAACL 2007.
Denis, P. and Baldridge, J. (2007). A ranking approach to pro-
noun resolution. In Proc. IJCAI 2007.
Finkel, J. R., Grenager, T., and Manning, C. (2005). Incorpo-
rating non-local information into information extraction sys-
tems by Gibbs sampling. In Proc. ACL 2005, pages 363?370.
Joachims, T. (1999). Making large-scale SVM learning prac-
tical. In Scho?lkopf, B., Burges, C., and Smola, A., editors,
Advances in Kernel Methods - Support Vector Learning.
Kudoh, T. and Matsumoto, Y. (2000). Use of Support Vector
Machines for chunk identification. In Proc. CoNLL 2000.
Liu, D. C. and Nocedal, J. (1989). On the limited memory
method for large scale optimization. Mathematical Program-
ming B, 45(3):503?528.
McCarthy, J. F. and Lehnert, W. G. (1995). Using decision trees
for coreference resolution. In Proc. IJCAI 1995.
Morton, T. S. (2000). Coreference for NLP applications. In
Proc. ACL 2000.
Moschitti, A. (2006). Making tree kernels practical for natural
language learning. In Proc. EACL 2006.
Mu?ller, C. and Strube, M. (2006). Multi-level annotation of
linguistic data with MMAX2. In Braun, S., Kohn, K., and
Mukherjee, J., editors, Corpus Technology and Language
Pedagogy: New Resources, New Tools, New Methods. Peter
Lang, Frankfurt a.M., Germany.
Ng, V. (2007). Shallow semantics for coreference resolution. In
Proc. IJCAI 2007.
Petrov, S., Barett, L., Thibaux, R., and Klein, D. (2006). Learn-
ing accurate, compact, and interpretable tree annotation. In
COLING-ACL 2006.
Ponzetto, S. P. and Strube, M. (2006). Exploiting semantic role
labeling, WordNet and Wikipedia for coreference resolution.
In Proc. HLT/NAACL 2006.
Qiu, L., Kan, M.-Y., and Chua, T.-S. (2004). A public reference
implementation of the RAP anaphora resolution algorithm.
In Proc. LREC 2004.
Soon, W. M., Ng, H. T., and Lim, D. C. Y. (2001). A machine
learning approach to coreference resolution of noun phrases.
Computational Linguistics, 27(4):521?544.
Steinberger, J., Poesio, M., Kabadjov, M., and Jezek, K. (2007).
Two uses of anaphora resolution in summarization. Informa-
tion Processing and Management, 43:1663?1680. Special
issue on Summarization.
Toutanova, K., Klein, D., Manning, C. D., and Singer, Y.
(2003). Feature-rich part-of-speech tagging with a cyclic de-
pendency network. In Proc. NAACL 2003, pages 252?259.
Uryupina, O. (2006). Coreference resolution with and without
linguistic knowledge. In Proc. LREC 2006.
Versley, Y. (2006). A constraint-based approach to noun phrase
coreference resolution in German newspaper text. In Kon-
ferenz zur Verarbeitung Natu?rlicher Sprache (KONVENS
2006).
Wellner, B. and Vilain, M. (2006). Leveraging machine read-
able dictionaries in discriminative sequence models. In Proc.
LREC 2006.
Witten, I. and Frank, E. (2005). Data Mining: Practical ma-
chine learning tools and techniques. Morgan Kaufmann.
Yang, X., Su, J., and Tan, C. L. (2006). Kernel-based pronoun
resolution with structured syntactic knowledge. In Proc.
CoLing/ACL-2006.
12
Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 134?137,
Paris, October 2009. c?2009 Association for Computational Linguistics
Scalable Discriminative Parsing for German
Yannick Versley
SFB 833
Universita?t Tu?bingen
versley@sfs.uni-tuebingen.de
Ines Rehbein
Dep. of Computational Linguistics
Universita?t des Saarlandes
rehbein@coli.uni-sb.de
Abstract
Generative lexicalized parsing models,
which are the mainstay for probabilistic
parsing of English, do not perform as well
when applied to languages with differ-
ent language-specific properties such as
free(r) word order or rich morphology. For
German and other non-English languages,
linguistically motivated complex treebank
transformations have been shown to im-
prove performance within the framework
of PCFG parsing, while generative lexical-
ized models do not seem to be as easily
adaptable to these languages.
In this paper, we show a practical way
to use grammatical functions as first-class
citizens in a discriminative model that al-
lows to extend annotated treebank gram-
mars with rich feature sets without hav-
ing to suffer from sparse data problems.
We demonstrate the flexibility of the ap-
proach by integrating unsupervised PP at-
tachment and POS-based word clusters
into the parser.
1 Introduction
To capture the semantic relations inherent in a
text, parsing has to recover both structural infor-
mation and grammatical functions, which com-
monly coincide in English, but not in freer
word order languages such as German. In-
stead one has to make use of morphological fea-
tures in addition to exploiting ordering preferences
such as the (violatable) default ordering of (sub-
ject<)dative<accusative.
Because of this fact, many successful ap-
proaches for German PCFG parsing (Schiehlen,
2004; Dubey, 2005; Versley, 2005) use annotated
treebank grammars where the constituent trees
from the treebank are enriched with further lin-
guistic information that allows an adequate recon-
struction of syntactic relationships, suggesting that
probabilistic context-free grammars are an ade-
quate tool for parsing these languages.
In the ACL 2008 workshop on Parsing Ger-
man (Ku?bler, 2008), Rafferty and Manning (2008)
used a lexicalized PCFG parser using markoviza-
tion and parent annotation, but no linguistically in-
spired transformations; Rafferty and Manning did
quite well on constituents, but were not success-
ful in reconstructing grammatical functions, with
results considerably worse than for other submis-
sions in the shared task.
The framework we present in this paper ? an-
notated treebank grammars with a discriminative
model that allows lexicalization based on gram-
matical function assignment, as well as the ad-
dition of features based on unsupervised learn-
ing, including PP attachment and word clusters ?
shows that it is possible to achieve good improve-
ments over generative lexicalized models by using
the additional flexibility gained over standard lex-
icalized PCFG models. Our approach offers more
flexibility than generative PCFG models, while
computational costs for development and practi-
cal use are still acceptable. While we only present
results for German, we are confident that the re-
sults carry over to other languages where anno-
tated treebank grammars have been used success-
fully.
2 Parsing German with Morphology and
Valence Information
As a base parser, we use BitPar (Schmid, 2004),
a fast unlexicalized PCFG parser based on a first
pass where non-probabilistic bottom-up parsing
and top-down filtering is carried out efficiently by
storing the chart in bit vectors, and construct the
probabilistic chart only after top-down filtering.
We use an annotated treebank PCFG that is de-
134
rived from the Tiger treebank and largely inspired
by earlier work on annotated treebank grammars
for German (Schiehlen, 2004; Dubey, 2005; Vers-
ley, 2005).
Subcategorization With respect to the treebank
grammar, we refine the node labels with linguisti-
cally important information that is only implicit in
the treebank but would be tedious (and pointless)
to annotate by hand:
Firstly, we annotate NPs by case; clause nodes
(S and VP) are subcategorized by the clause type
(fin,inf,izu,rel), and NPs and PPs with a relative
pronoun are marked. Comparative phrases (e.g.,
bigger [than a house], marked as NP in Tiger and
Tu?Ba-D/Z) are marked by adding a ?CC? ending
to the node label. Finally, auxiliaries are split ac-
cording to their verb lemma into sein (be), haben
(have), werden (become).
To aid the identification of noun phrase case,
we add information related to case/number/gender
syncretism to the preterminal labels of determin-
ers, nouns, and adjectives (for details, see Versley,
2005) that allows to accurately determine the set of
possible cases while keeping the size of the tagset
relatively small .
Verb Valence We use information from the lex-
icon of the WCDG parser for German (Foth and
Menzel, 2006) to mark verbs according to the ar-
guments that they can take. While the WCDG
lexicon contains more information, we only en-
code the possibility of accusative and dative com-
plements, ignoring entries for genitive or clausal
complements.
Markovization with Argument Marking It
has been noted consistently (Klein and Manning,
2003; Schiehlen, 2004) that using markovization
- replacing the original treebank rules by an ap-
proximation that only considers a limited context
window of one or two siblings - improves re-
sults at least for a constituency-based evaluation.
However, in some cases this simple markoviza-
tion scheme leads to undesirable results includ-
ing sentences with multiple subjects, as predica-
tive arguments also have nominative case. To
avoid this, we additionally mark which arguments
have already been seen, yielding node labels such
as S fin<VVFIN a<RNP a<sa in the case of a
partial constituent for a finite sentence (S fin)
expanding to the right (<R) where both subject (s)
and accusative object (a) have already been seen.
Unknown Words For the base PCFG parse, we
use a decision tree with 43 regular expressions as
features, five of which are tailored towards rec-
ognizing the past and zu-infinitive form of sep-
arable prefix verbs (abarbeiten ? abgearbeitet,
abzuarbeiten), which cannot be recognized by
considering suffixes only. The extended part of
speech tags for verbs (which contain valency in-
formation) are interpolated between the distribu-
tion at the concrete leaf of the decision tree and the
global valency distribution for the (coarse) part-of-
speech tag.
Additionally, we use SMOR (Schmid et al,
2004) in conjunction with the verb lexicon and
a gazetteer list containing person and location
names to determine possible fine-grained part-of-
speech tags for unknown words.
Restoring Grammatical Functions Adding
edge labels to the nodes in PCFG parsing easily
creates sparse data problems, as reported by
Rafferty and Manning (2008), who witness a drop
in constituent F-measure (excluding grammatical
function labels) when they include function labels
in the symbols of their PCFG. On the other hand,
the informativity of grammatical function labels
for the contents of the node does not always
justify their cost in terms of data sparseness.
Thus, we chose an approach where we include
linguistically relevant information in the node
labels (see above), and use the finer categoriza-
tion to restore the grammatical function labels
automatically: Using the most frequent function
label sequence associated with a rule yields good
results even in the presence of markovization,
where some of the surrounding context is lost.
Furthermore, this approach allows us to use the
grammatical function label assignments in the
subsequent discriminative model, thus yielding
typed dependencies rather than the unlabeled
dependencies that are used in the lexicalization
model of the Stanford parser.
3 Discriminative Parsing
Generative parsing models are based on few dis-
tributions that use different feature combinations
based on smoothing; incorporating additional fea-
tures into these is very difficult at best.
As a result, the use of external preferences in
such parsers is usually limited to approaches that
reattach dependents in the output of the parser
rather than integrating them in the parsing process.
135
Settings no GFs with GFs
Rafferty and Manning (2008) 77.40 NA
?, training with GFs 72.09 60.48
markov[unlex] 74.66 62.47
markov+parent[unlex] 73.94 61.63
markovGF[unlex] 75.00 63.58
markov[lex] 77.68 66.05
markovGF[lex] 77.55 66.69
markovGF[+pp] 78.43 67.90
Table 1: Evaluation results: PARSEVAL F1 on
PaGe development set
Discriminative parsing for unification-based
grammar commonly uses the conditional random
field formulation introduced by Miyao and Tsu-
jii (2002) and Geman and Johnson (2002), which
uses local features to select a parse from a packed
forest. The much larger cost in terms of mem-
ory and time compared to generative models has
until recently made this approach largely unattrac-
tive (but see Finkel et al, 2008, who distributes the
learning process over several powerful machines).
An alternative use of discriminative models
has been to incorporate global features, either by
reranking (e.g. Charniak and Johnson, 2005, or
Ku?bler et al, 2009 for German) or by beam search
over a pruned parse forest (Huang, 2008). How-
ever, Huang shows that a discriminative model us-
ing only local features reaps most of the benefits
of the global model and performs at a similar level
than earlier reranking-based approaches, pointing
to the fact that local ambiguities often result in the
n-best list not containing the correct parse.
The model we propose here extracts a pruned
parse forest from a simple unlexicalized parser and
then uses a factored discriminative model to apply
a rich set of features using the lexicalized parse
tree and its typed dependencies.
CRF parsing on pruned forests We extract a
pruned forest that contains exactly those nodes and
edges that can occur in trees that have a probabil-
ity ? pbest ? t, where in practice a threshold of
t = 10?3 ensures that no good parse is pruned
away while at the same time, the resulting forest
has only few nodes and edges.
For training, we extract an oracle tree, which is
selected according to a combination of correct (an-
notated grammar) constituents, the absence of in-
correct constituents, and the likelihood of the tree,
to account for the fact that the forest does not al-
fW-w-pos, CW-w-pos word form, cluster
f-sp, fS-sp-size node label, node size(1)
f-sp-RHS rule expansion
LDdir-sp-sd-hsd daughter attachment
LH-sp-sd-hsd-hld head projection
Lddir-hsp-hsd dependency (pos-pos)
Lddir-hsp-hsd-dist attachment length(1)
Ledir-hsp-hsd-hld dependency (pos-lemma)
Lfdir-hsp-hlp-hsd dependency (pos-lemma)
Lfdir-hsp-hlp-hsd-GF typed dep. (lemma-pos)
LhGF-hcp-hlp-hcd-hld typed dep. (lemma-lemma)
MIpp-prep, MIpp0-prep PP attach (noun)
MIppV-prep, MIppV0-prep PP attach (verb)
1) node sizes and attachment distances are discretized.
dir: one of H(head), L/R(head dep), B/I/E(nonheaded dep)
sp/d constituent symbol (parent/dep), hsp/d head cat, hc
head cat (coarse), hl head lemma
Table 2: List of Features
ways contain the exact gold tree. We then use
the AMIS maximum entropy learner of Miyao and
Tsujii (2002) to learn the discriminative model by
creating a forest from a grammar learned on the
remaining 4/5 of the training data.
Efficiency Parsing using the discriminative
model is quite efficient, with a memory con-
sumption for the whole system at about 270MB,
including the data used to determine the corpus
derived features (word clusters, mutual informa-
tion statistics, semantic role clusters). Parsing
speed is at 1.65sec./sentence on a 1.5GHz Pen-
tium M, against 1.84sec./sent for BitPar alone
when not using the tag filter for unknown words.
The time needed for learning can be reduced
by keeping the pruned parse charts and only re-
running the part of lexicalization and discrimina-
tive feature extraction; when reusing the old pa-
rameters as a starting point for AMIS? model esti-
mation, the turn-around time including feature ex-
traction is below two hours.
3.1 Clustering for unknown words
To improve the behaviour on unknown words
where morphological analyzer and regular expres-
sions do not yield informative preferences, we ex-
ploit a large, part-of-speech-tagged corpus to in-
duce clusters which provide robust information
that is useful even in our case where preterminals
in the PCFG are finer than standard POS tags.
The following features were gathered and used
by weighting by the pointwise mutual information
between the word and feature occurrences:
The context feature retrieves windows of high-
frequent words surrounding the word in question
136
(e.g. der mit for ?der Mann mit den Blumen?).
The context2 feature retrieves windows of one
high-frequent word and one part-of-speech tag
surrounding the word in question (e.g. der NN for
?der scho?ne Mann?).
The postag feature simply retrieves the part-of-
speech tag that is assigned to the word.
The result of using the repeated bisecting
k-means implementation of CLUTO (Steinbach
et al, 2000) on the resulting features yields syntac-
tically sensible clusters containing years, money
sums, last names, or place names.
3.2 Unsupervised PP Attachment and
Subject-Object preferences
We used simple part-of-speech tag patterns to
gather statistics on the association between nouns
and immediately following prepositions, as well
as between prepositions and closely following
verbs on the DE-WaC corpus (Baroni and Kilgar-
iff, 2006), an 1.7G words sample of the German-
language WWW. The mutual information values
for PP attachment are made available to the parser
as features that are weighted by the mutual infor-
mation value.
4 Evaluation and Discussion
To evaluate our approach, we use the dataset
used for the ACL-2008 Parsing German Workshop
(Ku?bler, 2008) that contains 26,116 sentences of
the TIGER treebank (Brants et al, 2002), in a 8:1:1
split of training, testing, and evaluation data, and
validate our approach on the development data,
where the results published by Rafferty and Man-
ning (2008) provide a useful comparison. All our
experiments are done using tags automatically as-
signed by the parser, which reaches a tagging ac-
curacy of about 97.5% according to the EVALB
output.
We find that our final model, combining aug-
menting the treebank labels with lingustic infor-
mation in addition to lexicalization and unsuper-
vised PP attachment works better than the best-
performing models of Rafferty and Manning, with
a very large improvement in grammatical func-
tions that is only surpassed by the Berkeley Parser
(Petrov and Klein, 2008), showing that our combi-
nation of annotated treebank grammars with a fac-
tored discriminative model not only allows great
control and flexibility for experimenting with the
inclusion of novel features, but also yields very
good results compared with the state of the art
for German (see table 1 for results on the Tiger
treebank). Preliminary results on Tu?Ba-D/Z with
a subset of the transformations of Versley (2005)
show the same tendency as the results for Tiger,
with 91.3% for constituents only, and 80.1% in-
cluding function labels (compared to 88.9% and
77.2% for the Stanford parser).
Future work will investigate the impact of in-
cluding additional features into the discriminative
parsing model.
References
Baroni, M. and Kilgariff, A. (2006). Large linguistically-
processed web corpora for multiple languages. In EACL
2006.
Brants, S., Dipper, S., Hansen, S., Lezius, W., and Smith, G.
(2002). The TIGER treebank. In Proc. TLT 2002.
Charniak, E. and Johnson, M. (2005). Coarse-to-fine n-best
parsing and maxent discriminative reranking. In Proc.
ACL 2005.
Dubey, A. (2005). What to do when lexicalization fails: pars-
ing German with suffix analysis and smoothing. In ACL-
2005.
Finkel, J. R., Kleeman, A., and Manning, C. D. (2008). Effi-
cient, feature-based, conditional random field parsing. In
ACL/HLT-2008.
Foth, K. and Menzel, W. (2006). Hybrid parsing: Using prob-
abilistic models as predictors for a symbolic parser. In
ACL 2006.
Geman, S. and Johnson, M. (2002). Dynamic programming
for parsing and estimation of stochastic unification-based
grammars. In ACL 2002.
Huang, L. (2008). Forest reranking: Discriminative parsing
with non-local features. In HLT/ACL 2008.
Klein, D. and Manning, C. D. (2003). Accurate unlexicalized
parsing. In ACL 2003.
Ku?bler, S. (2008). The PaGe 2008 shared task on parsing
German. In Proceedings of the ACL-2008 Workshop on
Parsing German.
Ku?bler, S., Hinrichs, E., Maier, W., and Klett, E. (2009). Pars-
ing coordinations. In EACL 2009.
Miyao, Y. and Tsujii, J. (2002). Maximum entropy estimation
for feature forests. In HLT 2002.
Petrov, S. and Klein, D. (2008). Parsing German with latent
variable grammars. In Parsing German Workshop at ACL-
HLT 2008.
Rafferty, A. and Manning, C. D. (2008). Parsing three Ger-
man treebanks: Lexicalized and unlexicalized baselines.
In ACL?08 workshop on Parsing German.
Schiehlen, M. (2004). Annotation strategies for probabilistic
parsing in German. In Proc. Coling 2004.
Schmid, H. (2004). Efficient parsing of highly ambiguous
context-free grammars with bit vectors. In Proc. Coling
2004.
Schmid, H., Fitschen, A., and Heid, U. (2004). SMOR: A
German computational morphology covering derivation,
composition and inflection. In Proceedings of LREC 2004.
Steinbach, M., Karypis, G., and Kumar, V. (2000). A com-
parison of document clustering techniques. In KDD Work-
shop on Text Mining.
Versley, Y. (2005). Parser evaluation across text types. In
Proceedings of the Fourth Workshop on Treebanks and
Linguistic Theories (TLT 2005).
137
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 1?8,
Uppsala, Sweden, 15-16 July 2010.
c
?2010 Association for Computational Linguistics
SemEval-2010 Task 1: Coreference Resolution in Multiple Languages
Marta Recasens
?
Llu??s M
`
arquez
?
Emili Sapena
?
M. Ant
`
onia Mart??
?
Mariona Taul
?
e
?
V
?
eronique Hoste
?
Massimo Poesio

Yannick Versley
??
?: CLiC, University of Barcelona, {mrecasens,amarti,mtaule}@ub.edu
?: TALP, Technical University of Catalonia, {lluism,esapena}@lsi.upc.edu
?: University College Ghent, veronique.hoste@hogent.be
: University of Essex/University of Trento, poesio@essex.ac.uk
??: University of T?ubingen, versley@sfs.uni-tuebingen.de
Abstract
This paper presents the SemEval-2010
task on Coreference Resolution in Multi-
ple Languages. The goal was to evaluate
and compare automatic coreference reso-
lution systems for six different languages
(Catalan, Dutch, English, German, Italian,
and Spanish) in four evaluation settings
and using four different metrics. Such a
rich scenario had the potential to provide
insight into key issues concerning corefer-
ence resolution: (i) the portability of sys-
tems across languages, (ii) the relevance of
different levels of linguistic information,
and (iii) the behavior of scoring metrics.
1 Introduction
The task of coreference resolution, defined as the
identification of the expressions in a text that re-
fer to the same discourse entity (1), has attracted
considerable attention within the NLP community.
(1) Major League Baseball sent its head of se-
curity to Chicago to review the second in-
cident of an on-field fan attack in the last
seven months. The league is reviewing se-
curity at all ballparks to crack down on
spectator violence.
Using coreference information has been shown to
be beneficial in a number of NLP applications
including Information Extraction (McCarthy and
Lehnert, 1995), Text Summarization (Steinberger
et al, 2007), Question Answering (Morton, 1999),
and Machine Translation. There have been a few
evaluation campaigns on coreference resolution in
the past, namely MUC (Hirschman and Chinchor,
1997), ACE (Doddington et al, 2004), and ARE
(Orasan et al, 2008), yet many questions remain
open:
? To what extent is it possible to imple-
ment a general coreference resolution system
portable to different languages? How much
language-specific tuning is necessary?
? How helpful are morphology, syntax and se-
mantics for solving coreference relations?
How much preprocessing is needed? Does its
quality (perfect linguistic input versus noisy
automatic input) really matter?
? How (dis)similar are different coreference
evaluation metrics?MUC, B-CUBED,
CEAF and BLANC? Do they all provide the
same ranking? Are they correlated?
Our goal was to address these questions in a
shared task. Given six datasets in Catalan, Dutch,
English, German, Italian, and Spanish, the task
we present involved automatically detecting full
coreference chains?composed of named entities
(NEs), pronouns, and full noun phrases?in four
different scenarios. For more information, the
reader is referred to the task website.
1
The rest of the paper is organized as follows.
Section 2 presents the corpora from which the task
datasets were extracted, and the automatic tools
used to preprocess them. In Section 3, we describe
the task by providing information about the data
format, evaluation settings, and evaluation met-
rics. Participating systems are described in Sec-
tion 4, and their results are analyzed and compared
in Section 5. Finally, Section 6 concludes.
2 Linguistic Resources
In this section, we first present the sources of the
data used in the task. We then describe the auto-
matic tools that predicted input annotations for the
coreference resolution systems.
1
http://stel.ub.edu/semeval2010-coref
1
Training Development Test
#docs #sents #tokens #docs #sents #tokens #docs #sents #tokens
Catalan 829 8,709 253,513 142 1,445 42,072 167 1,698 49,260
Dutch 145 2,544 46,894 23 496 9,165 72 2,410 48,007
English 229 3,648 79,060 39 741 17,044 85 1,141 24,206
German 900 19,233 331,614 199 4,129 73,145 136 2,736 50,287
Italian 80 2,951 81,400 17 551 16,904 46 1,494 41,586
Spanish 875 9,022 284,179 140 1,419 44,460 168 1,705 51,040
Table 1: Size of the task datasets.
2.1 Source Corpora
Catalan and Spanish The AnCora corpora (Re-
casens and Mart??, 2009) consist of a Catalan and
a Spanish treebank of 500k words each, mainly
from newspapers and news agencies (El Peri?odico,
EFE, ACN). Manual annotation exists for ar-
guments and thematic roles, predicate semantic
classes, NEs, WordNet nominal senses, and coref-
erence relations. AnCora are freely available for
research purposes.
Dutch The KNACK-2002 corpus (Hoste and De
Pauw, 2006) contains 267 documents from the
Flemish weekly magazine Knack. They were
manually annotated with coreference information
on top of semi-automatically annotated PoS tags,
phrase chunks, and NEs.
English The OntoNotes Release 2.0 corpus
(Pradhan et al, 2007) covers newswire and broad-
cast news data: 300k words from The Wall Street
Journal, and 200k words from the TDT-4 col-
lection, respectively. OntoNotes builds on the
Penn Treebank for syntactic annotation and on the
Penn PropBank for predicate argument structures.
Semantic annotations include NEs, words senses
(linked to an ontology), and coreference informa-
tion. The OntoNotes corpus is distributed by the
Linguistic Data Consortium.
2
German The T?uBa-D/Z corpus (Hinrichs et al,
2005) is a newspaper treebank based on data taken
from the daily issues of ?die tageszeitung? (taz). It
currently comprises 794k words manually anno-
tated with semantic and coreference information.
Due to licensing restrictions of the original texts, a
taz-DVD must be purchased to obtain a license.
2
Italian The LiveMemories corpus (Rodr??guez
et al, 2010) will include texts from the Italian
Wikipedia, blogs, news articles, and dialogues
2
Free user license agreements for the English and German
task datasets were issued to the task participants.
(MapTask). They are being annotated according
to the ARRAU annotation scheme with coref-
erence, agreement, and NE information on top
of automatically parsed data. The task dataset
included Wikipedia texts already annotated.
The datasets that were used in the task were ex-
tracted from the above-mentioned corpora. Ta-
ble 1 summarizes the number of documents
(docs), sentences (sents), and tokens in the train-
ing, development and test sets.
3
2.2 Preprocessing Systems
Catalan, Spanish, English Predicted lemmas
and PoS were generated using FreeLing
4
for
Catalan/Spanish and SVMTagger
5
for English.
Dependency information and predicate semantic
roles were generated with JointParser, a syntactic-
semantic parser.
6
Dutch Lemmas, PoS and NEs were automat-
ically provided by the memory-based shallow
parser for Dutch (Daelemans et al, 1999), and de-
pendency information by the Alpino parser (van
Noord et al, 2006).
German Lemmas were predicted by TreeTagger
(Schmid, 1995), PoS and morphology by RFTag-
ger (Schmid and Laws, 2008), and dependency in-
formation by MaltParser (Hall and Nivre, 2008).
Italian Lemmas and PoS were provided by
TextPro,
7
and dependency information by Malt-
Parser.
8
3
The German and Dutch training datasets were not com-
pletely stable during the competition period due to a few er-
rors. Revised versions were released on March 2 and 20, re-
spectively. As to the test datasets, the Dutch and Italian doc-
uments with formatting errors were corrected after the eval-
uation period, with no variations in the ranking order of sys-
tems.
4
http://www.lsi.upc.es/ nlp/freeling
5
http://www.lsi.upc.edu/ nlp/SVMTool
6
http://www.lsi.upc.edu// xlluis/?x=cat:5
7
http://textpro.fbk.eu
8
http://maltparser.org
2
3 Task Description
Participants were asked to develop an automatic
system capable of assigning a discourse entity to
every mention,
9
thus identifying all the NP men-
tions of every discourse entity. As there is no
standard annotation scheme for coreference and
the source corpora differed in certain aspects, the
coreference information of the task datasets was
produced according to three criteria:
? Only NP constituents and possessive deter-
miners can be mentions.
? Mentions must be referential expressions,
thus ruling out nominal predicates, appos-
itives, expletive NPs, attributive NPs, NPs
within idioms, etc.
? Singletons are also considered as entities
(i.e., entities with a single mention).
To help participants build their systems, the
task datasets also contained both gold-standard
and automatically predicted linguistic annotations
at the morphological, syntactic and semantic lev-
els. Considerable effort was devoted to provide
participants with a common and relatively simple
data representation for the six languages.
3.1 Data Format
The task datasets as well as the participants?
answers were displayed in a uniform column-
based format, similar to the style used in previous
CoNLL shared tasks on syntactic and semantic de-
pendencies (2008/2009).
10
Each dataset was pro-
vided as a single file per language. Since corefer-
ence is a linguistic relation at the discourse level,
documents constitute the basic unit, and are de-
limited by ?#begin document ID? and ?#end doc-
ument ID? comment lines. Within a document, the
information of each sentence is organized verti-
cally with one token per line, and a blank line after
the last token of each sentence. The information
associated with each token is described in several
columns (separated by ?\t? characters) represent-
ing the following layers of linguistic annotation.
ID (column 1). Token identifiers in the sentence.
Token (column 2). Word forms.
9
Following the terminology of the ACE program, a men-
tion is defined as an instance of reference to an object, and
an entity is the collection of mentions referring to the same
object in a document.
10
http://www.cnts.ua.ac.be/conll2008
ID Token Intermediate columns Coref
1 Major . . . (1
2 League . . .
3 Baseball . . . 1)
4 sent . . .
5 its . . . (1)|(2
6 head . . .
7 of . . .
8 security . . . (3)|2)
9 to . . .
. . . . . . . . . . . .
27 The . . . (1
28 league . . . 1)
29 is . . .
Table 2: Format of the coreference annotations
(corresponding to example (1) in Section 1).
Lemma (column 3). Token lemmas.
PoS (column 5). Coarse PoS.
Feat (column 7). Morphological features (PoS
type, number, gender, case, tense, aspect,
etc.) separated by a pipe character.
Head (column 9). ID of the syntactic head (?0? if
the token is the tree root).
DepRel (column 11). Dependency relations cor-
responding to the dependencies described in
the Head column (?sentence? if the token is
the tree root).
NE (column 13). NE types in open-close notation.
Pred (column 15). Predicate semantic class.
APreds (column 17 and subsequent ones). For
each predicate in the Pred column, its seman-
tic roles/dependencies.
Coref (last column). Coreference relations in
open-close notation.
The above-mentioned columns are ?gold-
standard columns,? whereas columns 4, 6, 8, 10,
12, 14, 16 and the penultimate contain the same
information as the respective previous column but
automatically predicted?using the preprocessing
systems listed in Section 2.2. Neither all layers
of linguistic annotation nor all gold-standard and
predicted columns were available for all six lan-
guages (underscore characters indicate missing in-
formation).
The coreference column follows an open-close
notation with an entity number in parentheses (see
Table 2). Every entity has an ID number, and ev-
ery mention is marked with the ID of the entity
it refers to: an opening parenthesis shows the be-
ginning of the mention (first token), while a clos-
ing parenthesis shows the end of the mention (last
3
token). For tokens belonging to more than one
mention, a pipe character is used to separate mul-
tiple entity IDs. The resulting annotation is a well-
formed nested structure (CF language).
3.2 Evaluation Settings
In order to address our goal of studying the effect
of different levels of linguistic information (pre-
processing) on solving coreference relations, the
test was divided into four evaluation settings that
differed along two dimensions.
Gold-standard versus Regular setting. Only
in the gold-standard setting were participants al-
lowed to use the gold-standard columns, includ-
ing the last one (of the test dataset) with true
mention boundaries. In the regular setting, they
were allowed to use only the automatically pre-
dicted columns. Obtaining better results in the
gold setting would provide evidence for the rel-
evance of using high-quality preprocessing infor-
mation. Since not all columns were available for
all six languages, the gold setting was only possi-
ble for Catalan, English, German, and Spanish.
Closed versus Open setting. In the closed set-
ting, systems had to be built strictly with the in-
formation provided in the task datasets. In con-
trast, there was no restriction on the resources that
participants could utilize in the open setting: sys-
tems could be developed using any external tools
and resources to predict the preprocessing infor-
mation, e.g., WordNet, Wikipedia, etc. The only
requirement was to use tools that had not been de-
veloped with the annotations of the test set. This
setting provided an open door into tools or re-
sources that improve performance.
3.3 Evaluation Metrics
Since there is no agreement at present on a stan-
dard measure for coreference resolution evalua-
tion, one of our goals was to compare the rank-
ings produced by four different measures. The
task scorer provides results in the two mention-
based metrics B
3
(Bagga and Baldwin, 1998) and
CEAF-?
3
(Luo, 2005), and the two link-based
metrics MUC (Vilain et al, 1995) and BLANC
(Recasens and Hovy, in prep). The first three mea-
sures have been widely used, while BLANC is a
proposal of a new measure interesting to test.
The mention detection subtask is measured with
recall, precision, and F
1
. Mentions are rewarded
with 1 point if their boundaries coincide with those
of the gold NP, with 0.5 points if their boundaries
are within the gold NP including its head, and
with 0 otherwise.
4 Participating Systems
A total of twenty-two participants registered for
the task and downloaded the training materials.
From these, sixteen downloaded the test set but
only six (out of which two task organizers) sub-
mitted valid results (corresponding to nine system
runs or variants). These numbers show that the
task raised considerable interest but that the final
participation rate was comparatively low (slightly
below 30%).
The participating systems differed in terms of
architecture, machine learning method, etc. Ta-
ble 3 summarizes their main properties. Systems
like BART and Corry support several machine
learners, but Table 3 indicates the one used for the
SemEval run. The last column indicates the exter-
nal resources that were employed in the open set-
ting, thus it is empty for systems that participated
only in the closed setting. For more specific details
we address the reader to the system description pa-
pers in Erk and Strapparava (2010).
5 Results and Evaluation
Table 4 shows the results obtained by two naive
baseline systems: (i) SINGLETONS considers each
mention as a separate entity, and (ii) ALL-IN-ONE
groups all the mentions in a document into a sin-
gle entity. These simple baselines reveal limita-
tions of the evaluation metrics, like the high scores
of CEAF and B
3
for SINGLETONS. Interestingly
enough, the naive baseline scores turn out to be
hard to beat by the participating systems, as Ta-
ble 5 shows. Similarly, ALL-IN-ONE obtains high
scores in terms of MUC. Table 4 also reveals dif-
ferences between the distribution of entities in the
datasets. Dutch is clearly the most divergent cor-
pus mainly due to the fact that it only contains sin-
gletons for NEs.
Table 5 displays the results of all systems for all
languages and settings in the four evaluation met-
rics (the best scores in each setting are highlighted
in bold). Results are presented sequentially by lan-
guage and setting, and participating systems are
ordered alphabetically. The participation of sys-
tems across languages and settings is rather irreg-
ular,
11
thus making it difficult to draw firm conclu-
11
Only 45 entries in Table 5 from 192 potential cases.
4
System Architecture ML Methods External Resources
BART
(Broscheit et al, 2010) Closest-first with entity-
mention model (English),
Closest-first model (German,
Italian)
MaxEnt (English, Ger-
man), Decision trees
(Italian)
GermaNet & gazetteers (Ger-
man), I-Cab gazetteers (Italian),
Berkeley parser, Stanford NER,
WordNet, Wikipedia name list,
U.S. census data (English)
Corry
(Uryupina, 2010) ILP, Pairwise model SVM Stanford parser & NER, Word-
Net, U.S. census data
RelaxCor
(Sapena et al, 2010) Graph partitioning (solved by
relaxation labeling)
Decision trees, Rules WordNet
SUCRE
(Kobdani and Sch?utze, 2010) Best-first clustering, Rela-
tional database model, Regular
feature definition language
Decision trees, Naive
Bayes, SVM, MaxEnt
?
TANL-1
(Attardi et al, 2010) Highest entity-mention simi-
larity
MaxEnt PoS tagger (Italian)
UBIU
(Zhekova and K?ubler, 2010) Pairwise model MBL ?
Table 3: Main characteristics of the participating systems.
sions about the aims initially pursued by the task.
In the following, we summarize the most relevant
outcomes of the evaluation.
Regarding languages, English concentrates the
most participants (fifteen entries), followed by
German (eight), Catalan and Spanish (seven each),
Italian (five), and Dutch (three). The number of
languages addressed by each system ranges from
one (Corry) to six (UBIU and SUCRE); BART and
RelaxCor addressed three languages, and TANL-1
five. The best overall results are obtained for En-
glish followed by German, then Catalan, Spanish
and Italian, and finally Dutch. Apart from differ-
ences between corpora, there are other factors that
might explain this ranking: (i) the fact that most of
the systems were originally developed for English,
and (ii) differences in corpus size (German having
the largest corpus, and Dutch the smallest).
Regarding systems, there are no clear ?win-
ners.? Note that no language-setting was ad-
dressed by all six systems. The BART system,
for instance, is either on its own or competing
against a single system. It emerges from par-
tial comparisons that SUCRE performs the best in
closed?regular for English, German, and Italian,
although it never outperforms the CEAF or B
3
sin-
gleton baseline. While SUCRE always obtains the
best scores according to MUC and BLANC, Re-
laxCor and TANL-1 usually win based on CEAF
and B
3
. The Corry system presents three variants
optimized for CEAF (Corry-C), MUC (Corry-M),
and BLANC (Corry-B). Their results are consis-
tent with the bias introduced in the optimization
(see English:open?gold).
Depending on the evaluation metric then, the
rankings of systems vary with considerable score
differences. There is a significant positive corre-
lation between CEAF and B
3
(Pearson?s r = 0.91,
p< 0.01), and a significant lack of correlation be-
tween CEAF and MUC in terms of recall (Pear-
son?s r = 0.44, p< 0.01). This fact stresses the
importance of defining appropriate metrics (or a
combination of them) for coreference evaluation.
Finally, regarding evaluation settings, the re-
sults in the gold setting are significantly better than
those in the regular. However, this might be a di-
rect effect of the mention recognition task. Men-
tion recognition in the regular setting falls more
than 20 F
1
points with respect to the gold setting
(where correct mention boundaries were given).
As for the open versus closed setting, there is only
one system, RelaxCor for English, that addressed
the two. As expected, results show a slight im-
provement from closed?gold to open?gold.
6 Conclusions
This paper has introduced the main features of
the SemEval-2010 task on coreference resolution.
5
CEAF MUC B
3
BLANC
R P F
1
R P F
1
R P F
1
R P Blanc
SINGLETONS: Each mention forms a separate entity.
Catalan 61.2 61.2 61.2 0.0 0.0 0.0 61.2 100 75.9 50.0 48.7 49.3
Dutch 34.5 34.5 34.5 0.0 0.0 0.0 34.5 100 51.3 50.0 46.7 48.3
English 71.2 71.2 71.2 0.0 0.0 0.0 71.2 100 83.2 50.0 49.2 49.6
German 75.5 75.5 75.5 0.0 0.0 0.0 75.5 100 86.0 50.0 49.4 49.7
Italian 71.1 71.1 71.1 0.0 0.0 0.0 71.1 100 83.1 50.0 49.2 49.6
Spanish 62.2 62.2 62.2 0.0 0.0 0.0 62.2 100 76.7 50.0 48.8 49.4
ALL-IN-ONE: All mentions are grouped into a single entity.
Catalan 11.8 11.8 11.8 100 39.3 56.4 100 4.0 7.7 50.0 1.3 2.6
Dutch 19.7 19.7 19.7 100 66.3 79.8 100 8.0 14.9 50.0 3.2 6.2
English 10.5 10.5 10.5 100 29.2 45.2 100 3.5 6.7 50.0 0.8 1.6
German 8.2 8.2 8.2 100 24.8 39.7 100 2.4 4.7 50.0 0.6 1.1
Italian 11.4 11.4 11.4 100 29.0 45.0 100 2.1 4.1 50.0 0.8 1.5
Spanish 11.9 11.9 11.9 100 38.3 55.4 100 3.9 7.6 50.0 1.2 2.4
Table 4: Baseline scores.
The goal of the task was to evaluate and compare
automatic coreference resolution systems for six
different languages in four evaluation settings and
using four different metrics. This complex sce-
nario aimed at providing insight into several as-
pects of coreference resolution, including portabil-
ity across languages, relevance of linguistic infor-
mation at different levels, and behavior of alterna-
tive scoring metrics.
The task attracted considerable attention from a
number of researchers, but only six teams submit-
ted their final results. Participating systems did not
run their systems for all the languages and evalu-
ation settings, thus making direct comparisons be-
tween them very difficult. Nonetheless, we were
able to observe some interesting aspects from the
empirical evaluation.
An important conclusion was the confirmation
that different evaluation metrics provide different
system rankings and the scores are not commen-
surate. Attention thus needs to be paid to corefer-
ence evaluation. The behavior and applicability of
the scoring metrics requires further investigation
in order to guarantee a fair evaluation when com-
paring systems in the future. We hope to have the
opportunity to thoroughly discuss this and the rest
of interesting questions raised by the task during
the SemEval workshop at ACL 2010.
An additional valuable benefit is the set of re-
sources developed throughout the task. As task
organizers, we intend to facilitate the sharing of
datasets, scorers, and documentation by keeping
them available for future research use. We believe
that these resources will help to set future bench-
marks for the research community and will con-
tribute positively to the progress of the state of the
art in coreference resolution. We will maintain and
update the task website with post-SemEval contri-
butions.
Acknowledgments
We would like to thank the following peo-
ple who contributed to the preparation of the
task datasets: Manuel Bertran (UB), Oriol
Borrega (UB), Orph?ee De Clercq (U. Ghent),
Francesca Delogu (U. Trento), Jes?us Gim?enez
(UPC), Eduard Hovy (ISI-USC), Richard Johans-
son (U. Trento), Xavier Llu??s (UPC), Montse
Nofre (UB), Llu??s Padr?o (UPC), Kepa Joseba
Rodr??guez (U. Trento), Mihai Surdeanu (Stan-
ford), Olga Uryupina (U. Trento), Lente Van Leu-
ven (UB), and Rita Zaragoza (UB). We would also
like to thank LDC and die tageszeitung for dis-
tributing freely the English and German datasets.
This work was funded in part by the Span-
ish Ministry of Science and Innovation through
the projects TEXT-MESS 2.0 (TIN2009-13391-
C04-04), OpenMT-2 (TIN2009-14675-C03), and
KNOW2 (TIN2009-14715-C04-04), and an FPU
doctoral scholarship (AP2006-00994) held by
M. Recasens. It also received financial sup-
port from the Seventh Framework Programme
of the EU (FP7/2007-2013) under GA 247762
(FAUST), from the STEVIN program of the Ned-
erlandse Taalunie through the COREA and SoNaR
projects, and from the Provincia Autonoma di
Trento through the LiveMemories project.
6
Mention detection CEAF MUC B
3
BLANC
R P F
1
R P F
1
R P F
1
R P F
1
R P Blanc
Catalan
closed?gold
RelaxCor 100 100 100 70.5 70.5 70.5 29.3 77.3 42.5 68.6 95.8 79.9 56.0 81.8 59.7
SUCRE 100 100 100 68.7 68.7 68.7 54.1 58.4 56.2 76.6 77.4 77.0 72.4 60.2 63.6
TANL-1 100 96.8 98.4 66.0 63.9 64.9 17.2 57.7 26.5 64.4 93.3 76.2 52.8 79.8 54.4
UBIU 75.1 96.3 84.4 46.6 59.6 52.3 8.8 17.1 11.7 47.8 76.3 58.8 51.6 57.9 52.2
closed?regular
SUCRE 75.9 64.5 69.7 51.3 43.6 47.2 44.1 32.3 37.3 59.6 44.7 51.1 53.9 55.2 54.2
TANL-1 83.3 82.0 82.7 57.5 56.6 57.1 15.2 46.9 22.9 55.8 76.6 64.6 51.3 76.2 51.0
UBIU 51.4 70.9 59.6 33.2 45.7 38.4 6.5 12.6 8.6 32.4 55.7 40.9 50.2 53.7 47.8
open?gold
open?regular
Dutch
closed?gold
SUCRE 100 100 100 58.8 58.8 58.8 65.7 74.4 69.8 65.0 69.2 67.0 69.5 62.9 65.3
closed?regular
SUCRE 78.0 29.0 42.3 29.4 10.9 15.9 62.0 19.5 29.7 59.1 6.5 11.7 46.9 46.9 46.9
UBIU 41.5 29.9 34.7 20.5 14.6 17.0 6.7 11.0 8.3 13.3 23.4 17.0 50.0 52.4 32.3
open?gold
open?regular
English
closed?gold
RelaxCor 100 100 100 75.6 75.6 75.6 21.9 72.4 33.7 74.8 97.0 84.5 57.0 83.4 61.3
SUCRE 100 100 100 74.3 74.3 74.3 68.1 54.9 60.8 86.7 78.5 82.4 77.3 67.0 70.8
TANL-1 99.8 81.7 89.8 75.0 61.4 67.6 23.7 24.4 24.0 74.6 72.1 73.4 51.8 68.8 52.1
UBIU 92.5 99.5 95.9 63.4 68.2 65.7 17.2 25.5 20.5 67.8 83.5 74.8 52.6 60.8 54.0
closed?regular
SUCRE 78.4 83.0 80.7 61.0 64.5 62.7 57.7 48.1 52.5 68.3 65.9 67.1 58.9 65.7 61.2
TANL-1 79.6 68.9 73.9 61.7 53.4 57.3 23.8 25.5 24.6 62.1 60.5 61.3 50.9 68.0 49.3
UBIU 66.7 83.6 74.2 48.2 60.4 53.6 11.6 18.4 14.2 50.9 69.2 58.7 50.9 56.3 51.0
open?gold
Corry-B 100 100 100 77.5 77.5 77.5 56.1 57.5 56.8 82.6 85.7 84.1 69.3 75.3 71.8
Corry-C 100 100 100 77.7 77.7 77.7 57.4 58.3 57.9 83.1 84.7 83.9 71.3 71.6 71.5
Corry-M 100 100 100 73.8 73.8 73.8 62.5 56.2 59.2 85.5 78.6 81.9 76.2 58.8 62.7
RelaxCor 100 100 100 75.8 75.8 75.8 22.6 70.5 34.2 75.2 96.7 84.6 58.0 83.8 62.7
open?regular
BART 76.1 69.8 72.8 70.1 64.3 67.1 62.8 52.4 57.1 74.9 67.7 71.1 55.3 73.2 57.7
Corry-B 79.8 76.4 78.1 70.4 67.4 68.9 55.0 54.2 54.6 73.7 74.1 73.9 57.1 75.7 60.6
Corry-C 79.8 76.4 78.1 70.9 67.9 69.4 54.7 55.5 55.1 73.8 73.1 73.5 57.4 63.8 59.4
Corry-M 79.8 76.4 78.1 66.3 63.5 64.8 61.5 53.4 57.2 76.8 66.5 71.3 58.5 56.2 57.1
German
closed?gold
SUCRE 100 100 100 72.9 72.9 72.9 74.4 48.1 58.4 90.4 73.6 81.1 78.2 61.8 66.4
TANL-1 100 100 100 77.7 77.7 77.7 16.4 60.6 25.9 77.2 96.7 85.9 54.4 75.1 57.4
UBIU 92.6 95.5 94.0 67.4 68.9 68.2 22.1 21.7 21.9 73.7 77.9 75.7 60.0 77.2 64.5
closed?regular
SUCRE 79.3 77.5 78.4 60.6 59.2 59.9 49.3 35.0 40.9 69.1 60.1 64.3 52.7 59.3 53.6
TANL-1 60.9 57.7 59.2 50.9 48.2 49.5 10.2 31.5 15.4 47.2 54.9 50.7 50.2 63.0 44.7
UBIU 50.6 66.8 57.6 39.4 51.9 44.8 9.5 11.4 10.4 41.2 53.7 46.6 50.2 54.4 48.0
open?gold
BART 94.3 93.7 94.0 67.1 66.7 66.9 70.5 40.1 51.1 85.3 64.4 73.4 65.5 61.0 62.8
open?regular
BART 82.5 82.3 82.4 61.4 61.2 61.3 61.4 36.1 45.5 75.3 58.3 65.7 55.9 60.3 57.3
Italian
closed?gold
SUCRE 98.4 98.4 98.4 66.0 66.0 66.0 48.1 42.3 45.0 76.7 76.9 76.8 54.8 63.5 56.9
closed?regular
SUCRE 84.6 98.1 90.8 57.1 66.2 61.3 50.1 50.7 50.4 63.6 79.2 70.6 55.2 68.3 57.7
UBIU 46.8 35.9 40.6 37.9 29.0 32.9 2.9 4.6 3.6 38.4 31.9 34.8 50.0 46.6 37.2
open?gold
open?regular
BART 42.8 80.7 55.9 35.0 66.1 45.8 35.3 54.0 42.7 34.6 70.6 46.4 57.1 68.1 59.6
TANL-1 90.5 73.8 81.3 62.2 50.7 55.9 37.2 28.3 32.1 66.8 56.5 61.2 50.7 69.3 48.5
Spanish
closed?gold
RelaxCor 100 100 100 66.6 66.6 66.6 14.8 73.8 24.7 65.3 97.5 78.2 53.4 81.8 55.6
SUCRE 100 100 100 69.8 69.8 69.8 52.7 58.3 55.3 75.8 79.0 77.4 67.3 62.5 64.5
TANL-1 100 96.8 98.4 66.9 64.7 65.8 16.6 56.5 25.7 65.2 93.4 76.8 52.5 79.0 54.1
UBIU 73.8 96.4 83.6 45.7 59.6 51.7 9.6 18.8 12.7 46.8 77.1 58.3 52.9 63.9 54.3
closed?regular
SUCRE 74.9 66.3 70.3 56.3 49.9 52.9 35.8 36.8 36.3 56.6 54.6 55.6 52.1 61.2 51.4
TANL-1 82.2 84.1 83.1 58.6 60.0 59.3 14.0 48.4 21.7 56.6 79.0 66.0 51.4 74.7 51.4
UBIU 51.1 72.7 60.0 33.6 47.6 39.4 7.6 14.4 10.0 32.8 57.1 41.6 50.4 54.6 48.4
open?gold
open?regular
Table 5: Official results of the participating systems for all languages, settings, and metrics.
7
References
Giuseppe Attardi, Stefano Dei Rossi, and Maria Simi.
2010. TANL-1: coreference resolution by parse
analysis and similarity clustering. In Proceedings
of SemEval-2.
Amit Bagga and Breck Baldwin. 1998. Algorithms for
scoring coreference chains. In Proceedings of the
LREC Workshop on Linguistic Coreference, pages
563?566.
Samuel Broscheit, Massimo Poesio, Simone Paolo
Ponzetto, Kepa Joseba Rodr??guez, Lorenza Ro-
mano, Olga Uryupina, Yannick Versley, and Roberto
Zanoli. 2010. BART: A multilingual anaphora res-
olution system. In Proceedings of SemEval-2.
Walter Daelemans, Sabine Buchholz, and Jorn Veen-
stra. 1999. Memory-based shallow parsing. In Pro-
ceedings of CoNLL 1999.
George Doddington, Alexis Mitchell, Mark Przybocki,
Lance Ramshaw, Stephanie Strassel, and Ralph
Weischedel. 2004. The Automatic Content Extrac-
tion (ACE) program ? Tasks, data, and evaluation.
In Proceedings of LREC 2004, pages 837?840.
Katrin Erk and Carlo Strapparava, editors. 2010. Pro-
ceedings of SemEval-2.
Johan Hall and Joakim Nivre. 2008. A dependency-
driven parser for German dependency and con-
stituency representations. In Proceedings of the ACL
Workshop on Parsing German (PaGe 2008), pages
47?54.
Erhard W. Hinrichs, Sandra K?ubler, and Karin Nau-
mann. 2005. A unified representation for morpho-
logical, syntactic, semantic, and referential annota-
tions. In Proceedings of the ACL Workshop on Fron-
tiers in Corpus Annotation II: Pie in the Sky, pages
13?20.
Lynette Hirschman and Nancy Chinchor. 1997.
MUC-7 Coreference Task Definition ? Version 3.0.
In Proceedings of MUC-7.
V?eronique Hoste and Guy De Pauw. 2006. KNACK-
2002: A richly annotated corpus of Dutch written
text. In Proceedings of LREC 2006, pages 1432?
1437.
Hamidreza Kobdani and Hinrich Sch?utze. 2010. SU-
CRE: A modular system for coreference resolution.
In Proceedings of SemEval-2.
Xiaoqiang Luo. 2005. On coreference resolution
performance metrics. In Proceedings of HLT-
EMNLP 2005, pages 25?32.
Joseph F. McCarthy and Wendy G. Lehnert. 1995. Us-
ing decision trees for coreference resolution. In Pro-
ceedings of IJCAI 1995, pages 1050?1055.
Thomas S. Morton. 1999. Using coreference in ques-
tion answering. In Proceedings of TREC-8, pages
85?89.
Constantin Orasan, Dan Cristea, Ruslan Mitkov, and
Ant?onio Branco. 2008. Anaphora Resolution Exer-
cise: An overview. In Proceedings of LREC 2008.
Sameer S. Pradhan, Eduard Hovy, Mitch Mar-
cus, Martha Palmer, Lance Ramshaw, and Ralph
Weischedel. 2007. Ontonotes: A unified rela-
tional semantic representation. In Proceedings of
the International Conference on Semantic Comput-
ing (ICSC 2007), pages 517?526.
Marta Recasens and Eduard Hovy. in prep. BLANC:
Implementing the Rand Index for Coreference Eval-
uation.
Marta Recasens and M. Ant`onia Mart??. 2009. AnCora-
CO: Coreferentially annotated corpora for Spanish
and Catalan. Language Resources and Evaluation,
DOI:10.1007/s10579-009-9108-x.
Kepa Joseba Rodr??guez, Francesca Delogu, Yannick
Versley, Egon Stemle, and Massimo Poesio. 2010.
Anaphoric annotation of Wikipedia and blogs in
the Live Memories Corpus. In Proceedings of
LREC 2010, pages 157?163.
Emili Sapena, Llu??s Padr?o, and Jordi Turmo. 2010.
RelaxCor: A global relaxation labeling approach to
coreference resolution for the SemEval-2 Corefer-
ence Task. In Proceedings of SemEval-2.
Helmut Schmid and Florian Laws. 2008. Estimation
of conditional probabilities with decision trees and
an application to fine-grained POS tagging. In Pro-
ceedings of COLING 2008, pages 777?784.
Helmut Schmid. 1995. Improvements in part-of-
speech tagging with an application to German. In
Proceedings of the ACL SIGDAT Workshop, pages
47?50.
Josef Steinberger, Massimo Poesio, Mijail A. Kabad-
jov, and Karel Jeek. 2007. Two uses of anaphora
resolution in summarization. Information Process-
ing and Management: an International Journal,
43(6):1663?1680.
Olga Uryupina. 2010. Corry: A system for corefer-
ence resolution. In Proceedings of SemEval-2.
Gertjan van Noord, Ineke Schuurman, and Vincent
Vandeghinste. 2006. Syntactic annotation of large
corpora in STEVIN. In Proceedings of LREC 2006.
Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A model-
theoretic coreference scoring scheme. In Proceed-
ings of MUC-6, pages 45?52.
Desislava Zhekova and Sandra K?ubler. 2010. UBIU:
A language-independent system for coreference res-
olution. In Proceedings of SemEval-2.
8
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 104?107,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
BART: A Multilingual Anaphora Resolution System
Samuel Broscheit?, Massimo Poesio?, Simone Paolo Ponzetto?, Kepa Joseba Rodriguez?,
Lorenza Romano?, Olga Uryupina?, Yannick Versley?, Roberto Zanoli?
?Seminar fu?r Computerlinguistik, University of Heidelberg
?CiMeC, University of Trento
?Fondazione Bruno Kessler
?SFB 833, University of Tu?bingen
broscheit@cl.uni-heidelberg.de, massimo.poesio@unitn.it,
ponzetto@cl.uni-heidelberg.de, kepa.rodriguez@unitn.it,
romano@fbk.eu, uryupina@gmail.com,
versley@sfs.uni-tuebingen.de, zanoli@fbk.eu
Abstract
BART (Versley et al, 2008) is a highly mod-
ular toolkit for coreference resolution that
supports state-of-the-art statistical approaches
and enables efficient feature engineering. For
the SemEval task 1 on Coreference Resolu-
tion, BART runs have been submitted for Ger-
man, English, and Italian.
BART relies on a maximum entropy-based
classifier for pairs of mentions. A novel entity-
mention approach based on Semantic Trees is
at the moment only supported for English.
1 Introduction
This paper presents a multilingual coreference reso-
lution system based on BART (Versley et al, 2008).
BART is a modular toolkit for coreference resolution
that supports state-of-the-art statistical approaches
to the task and enables efficient feature engineer-
ing. BART has originally been created and tested
for English, but its flexible modular architecture en-
sures its portability to other languages and domains.
In SemEval-2010 task 1 on Coreference Resolution,
BART has shown reliable performance for English,
German and Italian.
In our SemEval experiments, we mainly focus on
extending BART to cover multiple languages. Given
a corpus in a new language, one can re-train BART
to obtain baseline results. Such a language-agnostic
system, however, is only used as a starting point:
substantial improvements can be achieved by incor-
porating language-specific information with the help
of the Language Plugin. This design provides ef-
fective separation between linguistic and machine
learning aspects of the problem.
2 BART Architecture
The BART toolkit has five main components: pre-
processing pipeline, mention factory, feature extrac-
tion module, decoder and encoder. In addition, an
independent LanguagePlugin module handles all the
language specific information and is accessible from
any component. The architecture is shown on Figure
1. Each module can be accessed independently and
thus adjusted to leverage the system?s performance
on a particular language or domain.
The preprocessing pipeline converts an input doc-
ument into a set of lingustic layers, represented
as separate XML files. The mention factory uses
these layers to extract mentions and assign their
basic properties (number, gender etc). The fea-
ture extraction module describes pairs of mentions
{M
i
,M
j
}, i < j as a set of features.
The decoder generates training examples through
a process of sample selection and learns a pairwise
classifier. Finally, the encoder generates testing ex-
amples through a (possibly distinct) process of sam-
ple selection, runs the classifier and partitions the
mentions into coreference chains.
3 Language-specific issues
Below we briefly describe our language-specific ex-
tensions to BART. These issues are addressed in
more details in our recent papers (Broscheit et al,
2010; Poesio et al, 2010).
3.1 Mention Detection
Robust mention detection is an essential component
of any coreference resolution system. BART sup-
ports different pipelines for mention detection. The
104
Parser
Dep-to-Const
Converter
Morphology
Preprocessing
Mention
Factory
Decoder
Basic features
Syntactic features
Knowledge-based
features
MaxEnt
Classifier
Mention
(with basic
 properties):
- Number
- Gender
- Mention Type
- Modifiers
Unannotated
Text
Coreference
Chains
LanguagePlugin
Figure 1: BART architecture
choice of a pipeline depends crucially on the avail-
ability of linguistic resources for a given language.
For English and German, we use the Parsing
Pipeline and Mention Factory to extract mentions.
The parse trees are used to identify minimal and
maximal noun projections, as well as additional fea-
tures such as number, gender, and semantic class.
For English, we use parses from a state-of-the-art
constituent parser (Petrov et al, 2006) and extract
all base noun phrases as mentions. For German,
the SemEval dependency tree is transformed to a
constituent representation and minimal and maxi-
mal phrases are extracted for all nominal elements
(pronouns, common nouns, names), except when the
noun phrase is in a non-referring syntactic position
(for example, expletive ?es?, predicates in copula
constructions).
For Italian, we use the EMD Pipeline and Men-
tion Factory. The Typhoon (Zanoli et al, 2009)
and DEMention (Biggio et al, 2009) systems were
used to recognize mentions in the test set. For each
mention, its head and extension were considered.
The extension was learned by using the mention an-
notation provided in the training set (13th column)
whereas the head annotation was learned by exploit-
ing the information produced by MaltParser (Nivre
et al, 2007). In addition to the features extracted
from the training set, such as prefixes and suffixes
(1-4 characters) and orthographic information (capi-
talization and hyphenation), a number of features ex-
tracted by using external resources were used: men-
tions recognized by TextPro (http://textpro.fbk.eu),
gazetteers of generic proper nouns extracted from
the Italian phone-book and Wikipedia, and other fea-
tures derived from WordNet. Each of these features
was extracted in a local context of ?2 words.
3.2 Features
We view coreference resolution as a binary classifi-
cation problem. Each classification instance consists
of two markables, i.e. an anaphor and potential an-
tecedent. Instances are modeled as feature vectors
(cf. Table 1) and are handed over to a binary clas-
sifier that decides, given the features, whether the
anaphor and the candidate are coreferent or not. All
the feature values are computed automatically, with-
out any manual intervention.
Basic feature set. We use the same set of rela-
tively language-independent features as a backbone
of our system, extending it with a few language-
specific features for each subtask. Most of them are
used by virtually all the state-of-the-art coreference
resolution systems. A detailed description can be
found, for example, in (Soon et al, 2001).
English. Our English system is based on a novel
model of coreference. The key concept of our model
is a Semantic Tree ? a filecard associated with each
discourse entity containing the following fields:
? Types: the list of types for mentions of a given
entity. For example, if an entity contains the
mention ?software from India?, the shallow
predicate ?software? is added to the types.
? Attributes: this field collects the premodifiers.
For instance, if one of the mentions is ?the ex-
pensive software? the shallow attribute ?expen-
sive? is added to the list of attributes.
? Relations: this field collects the prepositional
postmodifiers. If an entity contains the men-
tion ?software from India?, the shallow relation
?from(India)? is added to the list of relations.
105
For each mention BART creates such a filecard
using syntactic information. If the classifier decides
that both mentions are corefering, the filecard of
the anaphora is merged into the filecard of the an-
tecedent (cf. Section 3.3 below).
The SemanticTreeCompatibility feature
extractor checks whether individual slots of the
anaphor?s filecard are compatible with those of the
antecedent?s.
The StrudelRelatedness feature relies on
Strudel ? a distributional semantic model (Baroni et
al., 2010). We compute Strudel vectors for the sets
of types of the anaphor and the antecedent. The re-
latedness value is determined as the cosine between
the two.
German. We have tested extra features for Ger-
man in our previous study (Broscheit et al, 2010).
The NodeDistance feature measures the num-
ber of clause nodes (SIMPX, R-SIMPX) and preposi-
tional phrase nodes (PX) along the path between M
j
and M
i
in the parse tree.
The PartialMorphMatch feature is a sub-
string match with a morphological extension for
common nouns. In German the frequent use of
noun composition makes a simple string match for
common nouns unfeasible. The feature checks for
a match between the noun stems of M
i
and M
j
.
We extract the morphology with SMOR/Morphisto
(Schmid et al, 2004).
The GermanetRelatedness feature uses the
Pathfinder library for GermaNet (Finthammer and
Cramer, 2008) that computes and discretizes raw
scores into three categories of semantic relatedness.
In our experiments we use the measure from Wu and
Palmer (1994), which has been found to be the best
performing on our development data.
Italian. We have designed a feature to cover Ital-
ian aliasing patterns. A list of company/person des-
ignators (e.g., ?S.p.a? or ?D.ssa?) has been manually
crafted. We have collected patterns of name variants
for locations. Finally, we have relaxed abbreviation
constraints, allowing for lower-case characters in the
abbreviations. Our pilot experiments suggest that,
although a universal aliasing algorithm is able to re-
solve some coreference links between NEs, creating
a language-specific module boosts the system?s per-
formance for Italian substantially.
Basic feature set
MentionType(M
i
),MentionType(M
j
)
SemanticClass(M
i
), SemanticClass(M
j
)
GenderAgreement(M
i
,M
j
)
NumberAgreement(M
i
,M
j
)
AnimacyAgreement(M
i
,M
j
)
StringMatch(M
i
,M
j
)
Distance(M
i
,M
j
)
Basic features used for English and Italian
Alias(M
i
,M
j
)
Apposition(M
i
,M
j
)
FirstMention(M
i
)
English
IsSubject(M
i
)
SemanticTreeCompatibility(M
i
,M
j
)
StrudelRelatedness(M
i
,M
j
)
German
InQuotedSpeech(M
i
), InQuotedSpeech(M
j
)
NodeDistance(M
i
,M
j
)
PartialMorphMatch(M
i
,M
j
)
GermanetRelatedness(M
i
,M
j
)
Italian
AliasItalian(M
i
,M
j
)
Table 1: Features used by BART: each feature describes
a pair of mentions {M
i
,M
j
}, i < j, where M
i
is a can-
didate antecedent and M
j
is a candidate anaphor
3.3 Resolution Algorithm
The BART toolkit supports several models of coref-
erence (pairwise modeling, rankers, semantic trees),
as well as different machine learning algorithms.
Our final setting relies on a pairwise maximum en-
tropy classifier for Italian and German.
Our English system is based on an entity-mention
model of coreference. The key concept of our model
is a Semantic Tree - a filecard associated to each dis-
course entity (cf. Section 3.2). Semantic trees are
used for both computing feature values and guiding
the resolution process.
We start by creating a Semantic Tree for each
mention. We process the document from left to
right, trying to find an antecedent for each men-
tion (candidate anaphor). When the antecedent is
found, we extend its Semantic Tree with the types,
attributes and relations of the anaphor, provided
they are mutually compatible. Consider, for ex-
106
ample, a list of mentions, containing, among oth-
ers, ?software from India?, ?the software? and ?soft-
ware from China?. Initially, BART creates the fol-
lowing semantic trees: ?(type: software) (relation:
from(India))?, ?(type: software)? and ?(type: soft-
ware) (relation: from(China))?. When the second
mention gets resolved to the first one, their seman-
tic trees are merged to ?(type: software) (relation:
from(India)?. Therefore, when we attempt to resolve
the third mention, both candidate antecedents are re-
jected, as their relation attributes are incompatible
with ?from(China)?. This approach helps us avoid
erroneous links (such as the link between the second
and the third mentions in our example) by leveraging
entity-level information.
4 Evaluation
The system was evaluated on the SemEval task 1
corpus by using the SemEval scorer.
First, we have evaluated our mention detection
modules: the system?s ability to recognize both the
mention extensions and the heads in the regular set-
ting. BART has achieved the best score for men-
tion detection in German and has shown reliable
figures for English. For Italian, the moderate per-
formance level is due to the different algorithms
for identifying the heads: the MaltParser (trained
on TUT: http://www.di.unito.it/?tutreeb) produces a
more semantic representation, while the SemEval
scorer seems to adopt a more syntactic approach.
Second, we have evaluated the quality of our
coreference resolution modules. For German, BART
has shown better performance than all the other sys-
tems on the regular track.
For English, the only language targeted by all sys-
tems, BART shows good performance over all met-
rics in the regular setting, usually only outperformed
by systems that were tuned to a particular metric.
Finally, the Italian version of BART shows re-
liable figures for coreference resolution, given the
mention alignment problem discussed above.
5 Conclusion
We have presented BART ? a multilingual toolkit
for coreference resolution. Due to its highly modu-
lar architecture, BART allows for efficient language-
specific feature engineering. Our effort represents
the first steps towards building a freely available
coreference resolution system for many languages.
References
Marco Baroni, Brian Murphy, Eduard Barbu, and Mas-
simo Poesio. 2010. Strudel: A corpus-based semantic
model based on properties and types. Cognitive Sci-
ence, 34(2):222?254.
Silvana Marianela Bernaola Biggio, Claudio Giuliano,
Massimo Poesio, Yannick Versley, Olga Uryupina, and
Roberto Zanoli. 2009. Local entity detection and
recognition task. In Proc. of Evalita-09.
Samuel Broscheit, Simone Paolo Ponzetto, Yannick Ver-
sley, and Massimo Poesio. 2010. Extending BART to
provide a coreference resolution system for German.
In Proc. of LREC ?10.
Marc Finthammer and Irene Cramer. 2008. Explor-
ing and navigating: Tools for GermaNet. In Proc. of
LREC ?08.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev,
Gulsen Eryigit, Sandra Ku?bler, Svetoslav Marinov,
and Erwin Marsi. 2007. Maltparser: A language-
independent system for data-driven dependency pars-
ing. Natural Language Engineering, 13(2):95?135.
Slav Petrov, Leon Barett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proc. of COLING-ACL-06.
Massimo Poesio, Olga Uryupina, and Yannick Versley.
2010. Creating a coreference resolution system for
Italian. In Proc. of LREC ?10.
Helmut Schmid, Arne Fitschen, and Ulrich Heid. 2004.
SMOR: A German computational morphology cover-
ing derivation, composition and inflection. In Proc. of
LREC ?04.
Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong
Lim. 2001. A machine learning approach to corefer-
ence resolution of noun phrases. Computational Lin-
guistics (Special Issue on Computational Anaphora
Resolution), 27(4):521?544.
Yannick Versley, Simone Paolo Ponzetto, Massimo Poe-
sio, Vladimir Eidelman, Alan Jern, Jason Smith,
Xiaofeng Yang, and Alessandro Moschitti. 2008.
BART: A modular toolkit for coreference resolution.
In Proceedings of the Linguistic Coreference Work-
shop at the International Conference on Language Re-
sources and Evaluation (LREC-2008).
Zhibiao Wu and Martha Palmer. 1994. Verb semantics
and lexical selection. In Proc. of ACL-94, pages 133?
138.
Roberto Zanoli, Emiliano Pianta, and Claudio Giuliano.
2009. Named entity recognition through redundancy
driven classifier. In Proc. of Evalita-09.
107
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 148?152, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
SFS-TUE: Compound Paraphrasing with a Language Model and
Discriminative Reranking
Yannick Versley
SfS / SFB 833
University of Tu?bingen
versley@sfs.uni-tuebingen.de
Abstract
This paper presents an approach for gener-
ating free paraphrases of compounds (task 4
at SemEval 2013) by decomposing the train-
ing data into a collection of templates and
fillers and recombining/scoring these based on
a generative language model and discrimina-
tive MaxEnt reranking.
The system described in this paper achieved
the highest score (with a very small margin) in
the (default) isomorphic setting of the scorer,
for which it was optimized, at a disadvantage
to the non-isomorphic score.
1 Introduction
Compounds are an interesting phenomenon in nat-
ural language semantics as they normally realize a
semantic relation (between head and modifier noun)
that is both highly ambiguous as to the type of rela-
tion and usually nonambiguous as to the concepts it
relates (namely, those of the two nouns).
Besides inventory-based approaches, where the
relation is classified into a fixed number of relations,
many researchers have argued that the full variabil-
ity of the semantic relations inherent in compounds
is best captured with paraphrases: Lauer (1995) pro-
poses to use a preposition as a proxy for the meaning
of a compound. Finin (1980) and later Nakov (2008)
and others propose less restrictive schemes based on
paraphrasing verbs.
A previous SemEval task (task 9 in 2010; But-
nariu et al, 2009). The most successsful approaches
for this task such as Nulty and Costello (2010), Li
et al (2010), and Wubben (2010), or the subse-
quent approach of Wijaya and Gianfortoni (2011),
all make efficient use of both the training data and
general evidence from WordNet or statistics derived
from large corpora. The paper of Li et al men-
tions that solely inducing a global ranking of para-
phrasing verbs from the training data (looking which
verb is ranked higher in those cases where both were
considered for the same compound) yielded higher
scores than an unsupervised approach based on the
semantic resources, underlining the need to combine
training data and resources efficiently.
SemEval 2013 task 4 The present task on pro-
viding free paraphrases for noun compounds (Hen-
drickx et al, 2013) uses a dataset collected from Me-
chanical Turk workers asked to paraphrase a given
compound (without context). Prepositional, verbal,
and other paraphrases all occur in the data:
(1) a. bar for wine
b. bar that serves wine
c. bar where wine is sold
d. sweet vinegar made from wine
In the examples, the words of the compound (wine
bar and wine vinegar, respectively) are put in ital-
ics, and other content words in the paraphrase are
underlined.
It is clear that certain paraphrases (X for Y) will be
common across many compounds, whereas the ones
containing more lexical material will differ even be-
tween relatively similar compounds (consider wine
bar from the example, and liquor store, which al-
lows paraphrase c, but not paraphrase b).
148
2 General Approach
The approach chosen in the SFS-TUE system is
based on first retrieving a number of similar com-
pounds, then extracting a set of building blocks (pat-
terns and fillers) from these compounds, recombin-
ing these building blocks, and finally ranking the
list of potential paraphrases. The final list is post-
processed by keeping only one variant of each set
of paraphrases that only differ in a determiner (e.g.,
?strike from air? and ?strike from the air?) in order
to make a 1:1 mapping between system response and
gold standard possible.
As a first step, the system retrieves the most simi-
lar compounds from the training data.
This is achieved Lin?s wordnet similarity measure
(Lin, 1998) using the implementation in NLTK (Bird
et al, 2009). The similarity of two compounds X1Y1
and X2Y2 is calculated as
sC = min(sim(X1, X2), sim(Y1, Y2)) +
0.1 ? (sim(X1, X2) + sim(Y1, Y2))
which represents a compromise between requiring
that both modifier and head are approximately sim-
ilar, and still giving a small boost to pairs that have
very high modifier similarity but low head similar-
ity, or vice versa. For training, the target compound
is excluded from the most-similar compounds list so
that candidate construction is only based on actual
neighbours.
The paraphrases for the most similar compound
entries (such as 2a) are broken down into templates
(2b) and fillers (2c), by replacing modifier and head
by X and Y , respectively, and other content words
by their part-of-speech tag.
(2) a. bar that serves wine
b. X that VBZ Y
c. VBZ:serve
Conversely, template fillers consist of all the ex-
tracted content words, categorized by their part-of-
speech. (Part-of-speech tags were assigned using the
Stanford POS tagger: Toutanova et al, 2003).
Both paraphrase templates and template fillers are
weighted by the product of the similarity value sC
between the target compound and the neighbour, and
the total frequency of occurrence in that neighbour?s
type examples
Y of Y of X (159) / Y of the X (59) / Y of a X (47)
Y for Y for X (114) / Y for the X (33)
Y VBZ Y that VBZ X (91)/ Y which VBZ X (45)
Y VBG Y VBG X (90) / Y VBG the X/ Y VBG with X
Y VBN Y VBN for X (82) / Y VBN by X (52)
Y in Y in X (31)
Y on Y on X (38)
Table 1: Most frequent paraphrase pattern types and pat-
tern instances
paraphrases. (For example, if Mechanical Turk par-
ticipants named ?bar that sells wine? twice and ?bar
that serves wine? once, the total frequency of ?X
that VBZ Y ? would be three).
Paraphrase candidates are then constructed by
combining any paraphrase templates from a simi-
larity neighbour with any fillers matching the given
part-of-speech tag. The list of all candidates is cut
down to a shortlist of 512 paraphrase candidates.
These are subsequently ranked by assigning features
to each of the candidate paraphrases and scoring
them using weights learned in a maximum ranker
by optimizing a loss derived from the probability of
all candidates that have been mentioned at least two
times in the training set in proportion to the probabil-
ity of all candidates that are not part of the training
annotation for that compound at all. (Paraphrases
that were named only once are not used for the pa-
rameter estimation).
After scoring, determiners are removed from the
paraphrase string and duplicates are removed from
the list. The generated list is cut off to yield at most
60 items.
2.1 Data Sources
As sources of evidence in the fit (or lack thereof)
of a given verb (as a suspected template filler) with
the two target words of a compounds, we use data
derived from the fifth revision of the English Giga-
word1, tokenized, tagged and parsed with the RASP
parsing toolchain (Briscoe et al, 2006), and from
Google?s web n-gram dataset2.
1Robert Parker, David Graff, Junbo Kong, Ke Chen and
Kazuaki Maeda (2011): English Gigaword Fifth Edition.
LDC2011T07, Linguistic Data Consortium, Philadelphia.
2Thorsten Brants, Alex Franz (2006): Web 1T 5-gram Ver-
sion 1. LDC2006T13, Linguistic Data Consortium, Philadel-
149
To reproduce very general estimates of linguis-
tic plausibility, we built a four-gram language model
based on the combined text of the English Gigaword
and the British National Corpus (Burnard, 1995),
using the KenLM toolkit (Heafield, 2011). On the
one hand, free paraphrases are quite unrestricted,
which means that the language model helps also in
the case of more exotic paraphrases such as (1d)
in the first section. On the other hand, many of
the more specialized aspects of plausibility such as
preposition attachment or selectional preferences for
subjects and direct objects can be cast as modeling
(smoothed) probabilities for a certain class of short
surface strings, for which an n-gram model is a use-
ful first approximation.
Using the grammatical relations extracted by the
RASP toolkit, we created a database of plausible
verb-subject and verb-object combinations, defined
as having a positive pointwise mutual information
score.
In a similar fashion, we used a list of verbs and
the morphg morphological realizer (Minnen et al,
2001) to extract all occurrences of the patterns ?N
PREP N?, ?N PREP (DET) N? for noun-preposition-
noun combinations, and ?N that VBZ? as well as ?N
VBN by? for finding typical cases of an active or pas-
sive verb that modifies a given noun.
2.2 Ranking features
The following properties used to score each para-
phrase candidate (using weights learned by the Max-
Ent ranker):
? language model score lm
The score assigned by the 4-gram model
learned on the English Gigaword and the BNC.
? pattern type tp=type
The pattern type (usually the first two ?interest-
ing? tokens from the paraphrase template, i.e.,
filtering out determiners and auxiliaries). A list
of the most frequent pattern types can be found
in Table 1.
? pattern weight pat
The pattern weight as the sum of the (neighbour
similarity times number of occurrences) contri-
bution from each pattern template.
phia.
? linking preposition prep prep=type
This feature correlates occurring prepositions
(prep) to types of patterns, with the goal
of learning high feature weights for preposi-
tion/type combinations that fit well together.
The obvious example for this would be, e.g.,
that the of preposition pattern fits well with
Y of X paraphrases.
? absent preposition noprep=type
This feature is set when no X prep Y or similar
pattern could be found.
? subject preference (VBG, VBZ)
subj subj0, subj n that vbz
object preference (VBN)
obj dobj0, obj n vbn by
In cases of verbal paraphrases where the com-
pound head is the subject, we can directly
check for corpus evidence for the correspond-
ing subject-verb pattern. A similar check is
done for verb-object (or verb-patient) patterns
in the paraphrases that involve the head in a
passive construction.
? frequent/infrequent subject verb (VBG, VBZ)
subj verb, subj infrequent
Some verbs (belong, come, concern, consist,
contain, deal, give, have, involve, make, pro-
vide, regard, run, sell, show, use, work) oc-
cur frequent enough that we want to introduce
a (data-induced) bias towards or away from
them. Other verbs, which are more rare, are
treated as a single class in this regard (which
means that their goodness of fit is mostly rep-
resented through the language model and the
selectional preference models).
? frequent/infrequent object verb (VBN)
a similar distinction is made for a list of
verbs that often occur in passive form (ap-
pointed, associated, based, carried, caused,
conducted, designed, found, given, held, kept,
meant, needed, performed, placed, prepared,
produced, provided, related, taken)
? co-occurrence of filler with X (other patterns)
other POS cooc, other POS none
For pattern types where we cannot use one of
150
System isomorphic non-isom.
SFS 0.2313 0.1795
IIITH 0.2309 0.2584
MELODI I 0.1300 0.5485
MELODI II 0.1358 0.5360
of+for baseline 0.0472 0.8294
Table 2: Official evaluation results + simple baseline
the selectional preference models, we use a
model akin to Pado&Lapata?s (2007) syntax-
based model that provides association scores
based on syntactic dependency arc distance.
3 Evaluation Results
The official evaluation results for the task are sum-
marized in Table 2. Two evaluation scores were
used:
? Isomorphic scoring maps system paraphrases
to (unmapped) paraphrases from the reference
dataset, and requires systems to produce the
full set of paraphrases gathered from Mechani-
cal Turk workers in order to get a perfect score.
? Nonisomorphic scoring scores each system
paraphrase with respect to the best match from
the reference dataset, and averages these scores
over all system paraphrases. A system that
performs well in nonisomorphic scoring does
not need to produce all paraphrases, but will
get punished for producing non-reliable para-
phrases.
As apparent from the table, systems either score well
on the isomorphic score (producing a large number
of paraphrases in order to get good coverage of the
range of expressions in the reference) or on the non-
isomorphic score (producing a smaller number of
paraphrases that are highly ranked in the reference).
The difference is also apparent in the case of a hy-
pothetical system that produces ?Y for X? and and
?Y of X? as the paraphrase for any compound (e.g.
bar for wine and bar of wine for wine bar). Because
these paraphrases occur quite often as most frequent
responses, this would yield a high non-isomorphic
score, but an isomorphic score that is very low.
During system development, the relative quality
of system paraphrases for each compound was es-
timated using Maximum Average Precision (MAP)
Compound closest neighbour MAP Rmax
share holding withdrawal line 1.000 0.800
union power community life 1.000 0.750
truth value accounting treatment 1.000 0.750
amateur championship computer study 1.000 0.750
government authority unit manager 1.000 0.680
wine bar computer industry 0.000 0.040
mammoth task consumer benefit 0.000 0.040
obstacle course work area 0.000 0.040
operating system telephone system 0.000 0.000
deadweight burden divorce rate 0.000 0.000
Table 3: Best and worst compounds in cross-validation
on the training data
and the total achievable recall (Rmax) of the gen-
erated paraphrase list. Table 3 shows the MAP
score (for paraphrases that were listed at least two
times) and achievable recall (for all paraphrases).
These measures, unlike the official scores, do not
attempt to deal with paraphrase variants (e.g. dif-
ferent prepositions for a verbal paraphrase), but are
robust and simple enough to give an impression of
the quality of the system response.
As can be seen by looking at the achievable re-
call figures, it is not always the case that all refer-
ence paraphrases are in the list that is ranked by the
MaxEnt model. In the lower half of table 3, we see
that for these cases, the most-similar item selected
by the WordNet-based similarity measure is not very
close semantically; whether this is the only influ-
encing factor remains to be seen since some of the
best-ranked items in the upper half are also abstract
concepts with only-somewhat-close neighbours. Fu-
ture work would therefore have to cover both im-
provements to the similarity measure itself and to the
ranking mechanism used for the reranking of gener-
ated paraphrases.
Acknowledgments
The author?s work was funded as part of SFB
833 (?Constitution of Meaning?) by the Deutsche
Forschungsgemeinschaft (DFG).
References
Bird, S., Loper, E., and Klein, E. (2009). Natural
Language Processing with Python. O?Reilly Me-
dia Inc.
151
Briscoe, E., Carroll, J., and Watson, R. (2006). The
second release of the RASP system. In Proceed-
ings of the COLING/ACL 2006 Interactive Pre-
sentation Sessions.
Burnard, L., editor (1995). Users Reference Guide
British National Corpus Version 1.0. Oxford Uni-
versity Computing Service.
Butnariu, C., Kim, S. N., Nakov, P., Seaghdha,
D. O., Spakowicz, S., and Veale, T. (2009).
SemEval-2010 task 9: The interpretation of noun
compounds using paraphrasing verbs and prepo-
sition. In Proceedings of the NAACL HLT Work-
shop on Semantic Evaluations: Recent Achieve-
ments and Future Directions.
Finin, T. W. (1980). The semantic interpretation of
compound nominals. Report T-96, University of
Illinois, Coordinated Science Laboratory.
Heafield, K. (2011). KenLM: Faster and smaller
language model queries. In Proceedings of the
EMNLP 2011 Sixth Workshop on Statistical Ma-
chine Translation.
Hendrickx, I., Kozareva, Z., Nakov, P., Se?aghdha,
D. O., Szpakowicz, S., and Veale, T. (2013).
SemEval-2013 task 4: Free paraphrases of noun
compounds. In Proceedings of the International
Workshop on Semantic Evaluation, SemEval ?13.
Lauer, M. (1995). Corpus statistics meet the noun
compound: some empirical results. In Proceed-
ings of the 33rd Annual Meeting of the Associa-
tion for Computational Linguistics (ACL 1995).
Li, G., Lopez-Fernandez, A., and Veale, T. (2010).
Ucd-goggle: A hybrid system for noun compound
paraphrasing. In Proceedings of the 5th Interna-
tional Workshop on Semantic Evaluation.
Lin, D. (1998). An information-theoretic defini-
tion of similarity. In Proceedings of International
Conference on Machine Learning.
Minnen, G., Caroll, J., and Pearce, D. (2001). Ap-
plied morphological processing of English. Natu-
ral Language Engineering, 7(3):207?223.
Nakov, P. (2008). Noun compound interpretation
using paraphrasing verbs: Feasibility study. In
Dochev, D., Pistore, M., and Traverso, P., ed-
itors, Artificial Intelligence: Methodology, Sys-
tems, and Applications, volume 5253 of Lec-
ture Notes in Computer Science, pages 103?117.
Springer Berlin Heidelberg.
Nulty, P. and Costello, F. (2010). Ucd-pn: Selecting
general paraphrases using conditional probability.
In Proceedings of the 5th International Workshop
on Semantic Evaluation.
Pado?, S. and Lapata, M. (2007). Dependency-based
construction of semantic space models. Compu-
tational Linguistics, 33(2):161?199.
Toutanova, K., Klein, D., Manning, C. D., and
Singer, Y. (2003). Feature-rich part-of-speech
tagging with a cyclic dependency network. In
Proc. NAACL 2003, pages 252?259.
Wijaya, D. T. and Gianfortoni, P. (2011). ?nut
case: what does it mean??: understanding se-
mantic relationship between nouns in noun com-
pounds through paraphrasing and ranking the
paraphrases. In Proceedings of the 1st inter-
national workshop on Search and mining entity-
relationship data, SMER ?11, pages 9?14.
Wubben, S. (2010). Uvt: Memory-based pairwise
ranking of paraphrasing verbs. In Proceedings of
the 5th International Workshop on Semantic Eval-
uation.
152
Proceedings of the NAACL HLT 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages, pages 1?12,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Statistical Parsing of Morphologically Rich Languages (SPMRL)
What, How and Whither
Reut Tsarfaty
Uppsala Universitet
Djame? Seddah
Alpage (Inria/Univ. Paris-Sorbonne)
Yoav Goldberg
Ben Gurion University
Sandra Ku?bler
Indiana University
Marie Candito
Alpage (Inria/Univ. Paris 7)
Jennifer Foster
NCLT, Dublin City University
Yannick Versley
Universita?t Tu?bingen
Ines Rehbein
Universita?t Saarbru?cken
Lamia Tounsi
NCLT, Dublin City University
Abstract
The term Morphologically Rich Languages
(MRLs) refers to languages in which signif-
icant information concerning syntactic units
and relations is expressed at word-level. There
is ample evidence that the application of read-
ily available statistical parsing models to such
languages is susceptible to serious perfor-
mance degradation. The first workshop on sta-
tistical parsing of MRLs hosts a variety of con-
tributions which show that despite language-
specific idiosyncrasies, the problems associ-
ated with parsing MRLs cut across languages
and parsing frameworks. In this paper we re-
view the current state-of-affairs with respect
to parsing MRLs and point out central chal-
lenges. We synthesize the contributions of re-
searchers working on parsing Arabic, Basque,
French, German, Hebrew, Hindi and Korean
to point out shared solutions across languages.
The overarching analysis suggests itself as a
source of directions for future investigations.
1 Introduction
The availability of large syntactically annotated cor-
pora led to an explosion of interest in automati-
cally inducing models for syntactic analysis and dis-
ambiguation called statistical parsers. The devel-
opment of successful statistical parsing models for
English focused on the Wall Street Journal Penn
Treebank (PTB, (Marcus et al, 1993)) as the pri-
mary, and sometimes only, resource. Since the ini-
tial release of the Penn Treebank (PTB Marcus et
al. (1993)), many different constituent-based parsing
models have been developed in the context of pars-
ing English (e.g. (Magerman, 1995; Collins, 1997;
Charniak, 2000; Chiang, 2000; Bod, 2003; Char-
niak and Johnson, 2005; Petrov et al, 2006; Huang,
2008; Finkel et al, 2008; Carreras et al, 2008)).
At their time, each of these models improved the
state-of-the-art, bringing parsing performance on the
standard test set of the Wall-Street-Journal to a per-
formance ceiling of 92% F1-score using the PARS-
EVAL evaluation metrics (Black et al, 1991). Some
of these parsers have been adapted to other lan-
guage/treebank pairs, but many of these adaptations
have been shown to be considerably less successful.
Among the arguments that have been proposed
to explain this performance gap are the impact of
small data sets, differences in treebanks? annotation
schemes, and inadequacy of the widely used PARS-
EVAL evaluation metrics. None of these aspects in
isolation can account for the systematic performance
deterioration, but observed from a wider, cross-
linguistic perspective, a picture begins to emerge ?
that the morphologically rich nature of some of the
languages makes them inherently more susceptible
to such performance degradation. Linguistic factors
associated with MRLs, such as a large inventory of
word-forms, higher degrees of word order freedom,
and the use of morphological information in indi-
cating syntactic relations, makes them substantially
harder to parse with models and techniques that have
been developed with English data in mind.
1
In addition to these technical and linguistic fac-
tors, the prominence of English parsing in the litera-
ture reduces the visibility of research aiming to solve
problems particular to MRLs. The lack of stream-
lined communication among researchers working
on different MRLs often leads to a reinventing the
wheel syndrome. To circumvent this, the first work-
shop on Statistical Parsing of Morphologically Rich
Languages (SPMRL 2010) offers a platform for
this growing community to share their views of the
different problems and oftentimes similar solutions.
We identify three main types of challenges, each
of which raises many questions. Many of the ques-
tions are yet to be conclusively answered. The first
type of challenges has to do with the architectural
setup of parsing MRLs: What is the nature of the in-
put? Can words be represented abstractly to reflect
shared morphological aspects? How can we cope
with morphological segmentation errors propagated
through the pipeline? The second type concerns the
representation of morphological information inside
the articulated syntactic model: Should morpholog-
ical information be encoded at the level of PoS tags?
On dependency relations? On top of non-terminals
symbols? How should the integrated representations
be learned and used? A final genuine challenge
has to do with sound estimation for lexical probabil-
ities: Given the finite, and often rather small, set of
data, and the large number of morphological analy-
ses licensed by rich inflectional systems, how can we
analyze words unseen in the training data?
Many of the challenges reported here are mostly
irrelevant when parsing Section 23 of the PTB but
they are of primordial importance in other tasks, in-
cluding out-of-domain parsing, statistical machine
translation, and parsing resource-poor languages.
By synthesizing the contributions to the workshop
and bringing it to the forefront, we hope to advance
the state of the art of statistical parsing in general.
In this paper we therefore take the opportunity
to analyze the knowledge that has been acquired in
the different investigations for the purpose of iden-
tifying main bottlenecks and pointing out promising
research directions. In section 2, we define MRLs
and identify syntactic characteristics associated with
them. We then discuss work on parsing MRLs in
both the dependency-based and constituency-based
setup. In section 3, we review the types of chal-
lenges associated with parsing MRLs across frame-
works. In section 4, we focus on the contributions to
the SPMRL workshop and identify recurring trends
in the empirical results and conceptual solutions. In
section 5, we analyze the emerging picture from a
bird?s eye view, and conclude that many challenges
could be more faithfully addressed in the context of
parsing morphologically ambiguous input.
2 Background
2.1 What are MRLs?
The term Morphologically Rich Languages (MRLs)
is used in the CL/NLP literature to refer to languages
in which substantial grammatical information, i.e.,
information concerning the arrangement of words
into syntactic units or cues to syntactic relations, is
expressed at word level.
The common linguistic and typological wisdom is
that ?morphology competes with syntax? (Bresnan,
2001). In effect, this means that rich morphology
goes hand in hand with a host of nonconfigurational
syntactic phenomena of the kind discussed by Hale
(1983). Because information about the relations be-
tween syntactic elements is indicated in the form of
words, these words can freely change their positions
in the sentence. This is referred to as free word or-
der (Mithun, 1992). Information about the group-
ing of elements together can further be expressed by
reference to their morphological form. Such logical
groupings of disparate elements are often called dis-
continuous constituents. In dependency structures,
such discontinuities impose nonprojectivity. Finally,
rich morphological information is found in abun-
dance in conjunction with so-called pro-drop or zero
anaphora. In such cases, rich morphological infor-
mation in the head (or co-head) of the clause of-
ten makes it possible to omit an overt subject which
would be semantically impoverished.
English, the most heavily studied language within
the CL/NLP community, is not an MRL. Even
though a handful of syntactic features (such as per-
son and number) are reflected in the form of words,
morphological information is often secondary to
other syntactic factors, such as the position of words
and their arrangement into phrases. German, an
Indo-European language closely related to English,
already exhibits some of the properties that make
2
parsing MRLs problematic. The Semitic languages
Arabic and Hebrew show an even more extreme case
in terms of the richness of their morphological forms
and the flexibility in their syntactic ordering.
2.2 Parsing MRLs
Pushing the envelope of constituency parsing:
The Head-Driven models of the type proposed
by Collins (1997) have been ported to parsing
many MRLs, often via the implementation of Bikel
(2002). For Czech, the adaptation by Collins et al
(1999) culminated in an 80 F1-score.
German has become almost an archetype of the
problems caused by MRLs; even though German
has a moderately rich morphology and a moder-
ately free word order, parsing results are far from
those for English (see (Ku?bler, 2008) and references
therein). Dubey (2005) showed that, for German
parsing, adding case and morphology information
together with smoothed markovization and an ade-
quate unknown-word model is more important than
lexicalization (Dubey and Keller, 2003).
For Modern Hebrew, Tsarfaty and Sima?an (2007)
show that a simple treebank PCFG augmented with
parent annotation and morphological information as
state-splits significantly outperforms Head-Driven
markovized models of the kind made popular by
Klein and Manning (2003). Results for parsing
Modern Standard Arabic using Bikel?s implemen-
tation on gold-standard tagging and segmentation
have not improved substantially since the initial re-
lease of the treebank (Maamouri et al, 2004; Kulick
et al, 2006; Maamouri et al, 2008).
For Italian, Corazza et al (2004) used the Stan-
ford parser and Bikel?s parser emulation of Collins?
model 2 (Collins, 1997) on the ISST treebank, and
obtained significantly lower results compared to En-
glish. It is notable that these models were ap-
plied without adding morphological signatures, us-
ing gold lemmas instead. Corazza et al (2004) fur-
ther tried different refinements including parent an-
notation and horizontal markovization, but none of
them obtained the desired improvement.
For French, Crabbe? and Candito (2008) and Sed-
dah et al (2010) show that, given a corpus compara-
ble in size and properties (i.e. the number of tokens
and grammar size), the performance level, both for
Charniak?s parser (Charniak, 2000) and the Berke-
ley parser (Petrov et al, 2006) was higher for pars-
ing the PTB than it was for French. The split-merge-
smooth implementation of (Petrov et al, 2006) con-
sistently outperform various lexicalized and unlexi-
calized models for French (Seddah et al, 2009) and
for many other languages (Petrov and Klein, 2007).
In this respect, (Petrov et al, 2006) is considered
MRL-friendly, due to its language agnostic design.
The rise of dependency parsing: It is commonly
assumed that dependency structures are better suited
for representing the syntactic structures of free word
order, morphologically rich, languages, because this
representation format does not rely crucially on the
position of words and the internal grouping of sur-
face chunks (Mel?c?uk, 1988). It is an entirely differ-
ent question, however, whether dependency parsers
are in fact better suited for parsing such languages.
The CoNLL shared tasks on multilingual depen-
dency parsing in 2006 and 2007 (Buchholz and
Marsi, 2006; Nivre et al, 2007a) demonstrated that
dependency parsing for MRLs is quite challenging.
While dependency parsers are adaptable to many
languages, as reflected in the multiplicity of the lan-
guages covered,1 the analysis by Nivre et al (2007b)
shows that the best result was obtained for English,
followed by Catalan, and that the most difficult lan-
guages to parse were Arabic, Basque, and Greek.
Nivre et al (2007a) drew a somewhat typological
conclusion, that languages with rich morphology
and free word order are the hardest to parse. This
was shown to be the case for both MaltParser (Nivre
et al, 2007c) and MST (McDonald et al, 2005), two
of the best performing parsers on the whole.
Annotation and evaluation matter: An emerg-
ing question is therefore whether models that have
been so successful in parsing English are necessar-
ily appropriate for parsing MRLs ? but associated
with this question are important questions concern-
ing the annotation scheme of the related treebanks.
Obviously, when annotating structures for languages
with characteristics different than English one has to
face different annotation decisions, and it comes as
no surprise that the annotated structures for MRLs
often differ from those employed in the PTB.
1The shared tasks involved 18 languages, including many
MRLs such as Arabic, Basque, Czech, Hungarian, and Turkish.
3
For Spanish and French, it was shown by Cowan
and Collins (2005) and in (Arun and Keller, 2005;
Schluter and van Genabith, 2007), that restructuring
the treebanks? native annotation scheme to match
the PTB annotation style led to a significant gain in
parsing performance of Head-Driven models of the
kind proposed in (Collins, 1997). For German, a
language with four different treebanks and two sub-
stantially different annotation schemes, it has been
shown that a PCFG parser is sensitive to the kind of
representation employed in the treebank.
Dubey and Keller (2003), for example, showed
that a simple PCFG parser outperformed an emula-
tion of Collins? model 1 on NEGRA. They showed
that using sister-head dependencies instead of head-
head dependencies improved parsing performance,
and hypothesized that it is due to the flatness of
phrasal annotation. Ku?bler et al (2006) showed con-
siderably lower PARSEVAL scores on NEGRA (Skut
et al, 1998) relative to the more hierarchically struc-
tured Tu?Ba-D/Z (Hinrichs et al, 2005), again, hy-
pothesizing that this is due to annotation differences.
Related to such comparisons is the question of the
relevance of the PARSEVAL metrics for evaluating
parsing results across languages and treebanks. Re-
hbein and van Genabith (2007) showed that PARS-
EVAL measures are sensitive to annotation scheme
particularities (e.g. the internal node ratio). It was
further shown that different metrics (i.e. the Leaf-
ancestor path (Sampson and Babarczy, 2003) and
dependency based ones in (Lin, 1995)) can lead to
different performance ranking. This was confirmed
also for French by Seddah et al (2009).
The questions of how to annotate treebanks for
MRLs and how to evaluate the performance of the
different parsers on these different treebanks is cru-
cial. For the MRL parsing community to be able to
assess the difficulty of improving parsing results for
French, German, Arabic, Korean, Basque, Hindi or
Hebrew, we ought to first address fundamental ques-
tions including: Is the treebank sufficiently large
to allow for proper grammar induction? Does the
annotation scheme fit the language characteristics?
Does the use of PTB annotation variants for other
languages influence parsing results? Does the space-
delimited tokenization allow for phrase boundary
detection? Do the results for a specific approach
generalize to more than one language?
3 Primary Research Questions
It is firmly established in theoretical linguistics that
morphology and syntax closely interact through pat-
terns of case marking, agreement, clitics and various
types of compounds. Because of such close interac-
tions, we expect morphological cues to help parsing
performance. But in practice, when trying to incor-
porate morphological information into parsing mod-
els, three types of challenges present themselves:
Architecture and Setup: When attempting to
parse complex word-forms that encapsulate both
lexical and functional information, important archi-
tectural questions emerge, namely, what is the na-
ture of the input that is given to the parsing system?
Does the system attempt to parse sequences of words
or does it aim to assign structures to sequences of
morphological segments? If the former is the case,
how can we represent words abstractly so as to re-
flect shared morphological aspects between them?
If the latter is the case, how can we arrive at a good
enough morphological segmentation for the purpose
of statistical parsing, given raw input texts?
When working with morphologically rich lan-
guages such as Hebrew or Arabic, affixes may have
syntactically independent functions. Many parsing
models assume segmentation of the syntactically in-
dependent parts, such as prepositions or pronominal
clitics, prior to parsing. But morphological segmen-
tation requires disambiguation which is non-trivial,
due to case syncretism and high morphological am-
biguity exhibited by rich inflectional systems. The
question is then when should we disambiguate the
morphological analyses of input forms? Should we
do that prior to parsing or perhaps jointly with it?2
Representation and Modeling: Assuming that
the input to our system reflects morphological infor-
mation, one way or another, which types of morpho-
2Most studies on parsing MRLs nowadays assume the gold
standard segmentation and disambiguated morphological infor-
mation as input. This is the case, for instance, for the Arabic
parsing at CoNLL 2007 (Nivre et al, 2007a). This practice de-
ludes the community as to the validity of the parsing results
reported for MRLs in shared tasks. Goldberg et al (2009), for
instance, show a gap of up to 6pt F1-score between performance
on gold standard segmentation vs. raw text. One way to over-
come this is to devise joint morphological and syntactic disam-
biguation frameworks (cf. (Goldberg and Tsarfaty, 2008)).
4
logical information should we include in the parsing
model? Inflectional and/or derivational? Case infor-
mation and/or agreement features? How can valency
requirements reflected in derivational morphology
affect the overall syntactic structure? In tandem with
the decision concerning the morphological informa-
tion to include, we face genuine challenges concern-
ing how to represent such information in the syntac-
tic model, be it constituency-based or dependency-
based. Should we encode morphological informa-
tion at the level of PoS tags and/or on top of syn-
tactic elements? Should we decorate non-terminals
nodes and/or dependency arcs or both?
Incorporating morphology in the statistical model
is often even more challenging than the sum of
these bare decisions, because of the nonconfigu-
rational structures (free word order, discontinuous
constituents) for rich markings are crucial (Hale,
1983). The parsing models designed for English of-
ten focus on learning rigid word order, and they do
not take morphological information into account (cf.
developing parsers for German (Dubey and Keller,
2003; Ku?bler et al, 2006)). The more complex ques-
tion is therefore: what type of parsing model should
we use for parsing MRLs? shall we use a general
purpose implementation and attempt to amend it?
how? or perhaps we should devise a new model from
first principles, to address nonconfigurational phe-
nomena effectively? using what form of representa-
tion? is it possible to find a single model that can
effectively cope with different kinds of languages?
Estimation and Smoothing: Compared to En-
glish, MRLs tend to have a greater number of word
forms and higher out-of-vocabulary (OOV) rates,
due to the many feature combinations licensed by
the inflectional system. A typical problem associ-
ated with parsing MRLs is substantial lexical data
sparseness due to high morphological variation in
surface forms. The question is therefore, given our
finite, and often fairly small, annotated sets of data,
how can we guess the morphological analyses, in-
cluding the PoS tag assignment and various features,
of an OOV word? How can we learn the probabil-
ities of such assignments? In a more general setup,
this problem is akin to handling out-of-vocabulary
or rare words for robust statistical parsing, and tech-
niques for domain adaptation via lexicon enhance-
Constituency-Based Dependency-Based
Arabic (Attia et al, 2010) (Marton et al, 2010)?
Basque - (Bengoetxea and Gojenola, 2010)
English (Attia et al, 2010) -
French (Attia et al, 2010)
(Seddah et al, 2010)
(Candito and Seddah, 2010)? -
German (Maier, 2010) -
Hebrew (Tsarfaty and Sima?an, 2010) (Goldberg and Elhadad, 2010)?
Hindi - (Ambati et al, 2010a)?
(Ambati et al, 2010b)
Korean (Chung et al, 2010) -
Table 1: An overview of SPMRL contributions. (? report
results also for non-gold standard input)
ment (also explored for English and other morpho-
logically impoverished languages).
So, in fact, incorporating morphological informa-
tion inside the syntactic model for the purpose of
statistical parsing is anything but trivial. In the next
section we review the various approaches taken in
the individual contributions of the SPMRL work-
shop for addressing such challenges.
4 Parsing MRLs: Recurring Trends
The first workshop on parsing MRLs features 11
contributions for a variety of languages with a
range of different parsing frameworks. Table 1 lists
the individual contributions within a cross-language
cross-framework grid. In this section, we focus on
trends that occur among the different contributions.
This may be a biased view since some of the prob-
lems that exist for parsing MRLs may have not been
at all present, but it is a synopsis of where we stand
with respect to problems that are being addressed.
4.1 Architecture and Setup: Gold vs. Predicted
Morphological Information
While morphological information can be very infor-
mative for syntactic analysis, morphological anal-
ysis of surface forms is ambiguous in many ways.
In German, for instance, case syncretism (i.e. a sin-
gle surface form corresponding to different cases) is
pervasive, and in Hebrew and Arabic, the lack of vo-
calization patterns in written texts leads to multiple
morphological analyses for each space-delimited to-
ken. In real world situations, gold morphological in-
formation is not available prior to parsing. Can pars-
ing systems make effective use of morphology even
when gold morphological information is absent?
5
Several papers address this challenge by present-
ing results for both the gold and the automatically
predicted PoS and morphological information (Am-
bati et al, 2010a; Marton et al, 2010; Goldberg and
Elhadad, 2010; Seddah et al, 2010). Not very sur-
prisingly, all evaluated systems show a drop in pars-
ing accuracy in the non-gold settings.
An interesting trend is that in many cases, us-
ing noisy morphological information is worse than
not using any at all. For Arabic Dependency pars-
ing, using predicted CASE causes a substantial drop
in accuracy while it greatly improves performance
in the gold setting (Marton et al, 2010). For
Hindi Dependency Parsing, using chunk-internal
cues (i.e. marking non-recursive phrases) is benefi-
cial when gold chunk-boundaries are available, but
suboptimal when they are automatically predicted
(Ambati et al, 2010a). For Hebrew Dependency
Parsing with the MST parser, using gold morpholog-
ical features shows no benefit over not using them,
while using automatically predicted morphological
features causes a big drop in accuracy compared to
not using them (Goldberg and Elhadad, 2010). For
French Constituency Parsing, Seddah et al (2010)
and Candito and Seddah (2010) show that while
gold information for the part-of-speech and lemma
of each word form results in a significant improve-
ment, the gain is low when switching to predicted
information. Reassuringly, Ambati et al (2010a),
Marton et al (2010), and Goldberg and Elhadad
(2010) demonstrate that some morphological infor-
mation can indeed be beneficial for parsing even in
the automatic setting. Ensuring that this is indeed
so, appears to be in turn linked to the question of
how morphology is represented and incorporated in
the parsing model.
The same effect in a different guise appears in
the contribution of Chung et al (2010) concerning
parsing Korean. Chung et al (2010) show a sig-
nificant improvement in parsing accuracy when in-
cluding traces of null anaphors (a.k.a. pro-drop) in
the input to the parser. Just like overt morphology,
traces and null elements encapsulate functional in-
formation about relational entities in the sentence
(the subject, the object, etc.), and including them at
the input level provides helpful disambiguating cues
for the overall structure that represents such rela-
tions. However, assuming that such traces are given
prior to parsing is, for all practical purposes, infeasi-
ble. This leads to an interesting question: will iden-
tifying such functional elements (marked as traces,
overt morphology, etc) during parsing, while com-
plicating that task itself, be on the whole justified?
Closely linked to the inclusion of morphological
information in the input is the choice of PoS tag set
to use. The generally accepted view is that fine-
grained PoS tags are morphologically more informa-
tive but may be harder to statistically learn and parse
with, in particular in the non-gold scenario. Mar-
ton et al (2010) demonstrate that a fine-grained tag
set provides the best results for Arabic dependency
parsing when gold tags are known, while a much
smaller tag set is preferred in the automatic setting.
4.2 Representation and Modeling:
Incorporating Morphological Information
Many of the studies presented here explore the use
of feature representation of morphological informa-
tion for the purpose of syntactic parsing (Ambati et
al., 2010a; Ambati et al, 2010b; Bengoetxea and
Gojenola, 2010; Goldberg and Elhadad, 2010; Mar-
ton et al, 2010; Tsarfaty and Sima?an, 2010). Clear
trends among the contributions emerge concerning
the kind of morphological information that helps sta-
tistical parsing. Morphological CASE is shown to be
beneficial across the board. It is shown to help for
parsing Basque, Hebrew, Hindi and to some extent
Arabic.3 Morphological DEFINITENESS and STATE
are beneficial for Hebrew and Arabic when explic-
itly represented in the model. STATE, ASPECT and
MOOD are beneficial for Hindi, but only marginally
beneficial for Arabic. CASE and SUBORDINATION-
TYPE are the most beneficial features for Basque
transition-based dependency parsing.
A closer view into the results mentioned in the
previous paragraph suggests that, beyond the kind
of information that is being used, the way in which
morphological information is represented and used
by the model has substantial ramification as to
whether or not it leads to performance improve-
ments. The so-called ?agreement features? GEN-
DER, NUMBER, PERSON, provide for an interesting
case study in this respect. When included directly as
3For Arabic, CASE is useful when gold morphology infor-
mation is available, but substantially hurt results when it is not.
6
machine learning features, agreement features ben-
efit dependency parsing for Arabic (Marton et al,
2010), but not Hindi (dependency) (Ambati et al,
2010a; Ambati et al, 2010b) or Hebrew (Goldberg
and Elhadad, 2010). When represented as simple
splits of non-terminal symbols, agreement informa-
tion does not help constituency-based parsing per-
formance for Hebrew (Tsarfaty and Sima?an, 2010).
However, when agreement patterns are directly rep-
resented on dependency arcs, they contribute an im-
provement for Hebrew dependency parsing (Gold-
berg and Elhadad, 2010). When agreement is en-
coded at the realization level inside a Relational-
Realizational model (Tsarfaty and Sima?an, 2008),
agreement features improve the state-of-the-art for
Hebrew parsing (Tsarfaty and Sima?an, 2010).
One of the advantages of the latter study is that
morphological information which is expressed at the
level of words gets interpreted elsewhere, on func-
tional elements higher up the constituency tree. In
dependency parsing, similar cases may arise, that
is, morphological information might not be as use-
ful on the form on which it is expressed, but would
be more useful at a different position where it could
influence the correct attachment of the main verb
to other elements. Interesting patterns of that sort
occur in Basque, where the SUBORDINATIONTYPE
morpheme attaches to the auxiliary verb, though it
mainly influences attachments to the main verb.
Bengoetxea and Gojenola (2010) attempted two
different ways to address this, one using a trans-
formation segmenting the relevant morpheme and
attaching it to the main verb instead, and another
by propagating the morpheme along arcs, through
a ?stacking? process, to where it is relevant. Both
ways led to performance improvements. The idea of
a segmentation transformation imposes non-trivial
pre-processing, but it may be that automatically
learning the propagation of morphological features
is a promising direction for future investigation.
Another, albeit indirect, way to include morpho-
logical information in the parsing model is using
so-called latent information or some mechanism
of clustering. The general idea is the following:
when morphological information is added to stan-
dard terminal or non-terminal symbols, it imposes
restrictions on the distribution of these no-longer-
equivalent elements. Learning latent informa-
tion does not represent morphological information
directly, but presumably, the distributional restric-
tions can be automatically learned along with the
splits of labels symbols in models such as (Petrov
et al, 2006). For Korean (Chung et al, 2010),
latent information contributes significant improve-
ments. One can further do the opposite, namely,
merging terminals symbols for the purpose of ob-
taining an abstraction over morphological features.
When such clustering uses a morphological signa-
ture of some sort, it is shown to significantly im-
prove constituency-based parsing for French (Can-
dito and Seddah, 2010).
4.3 Representation and Modeling: Free Word
Order and Flexible Constituency Structure
Off-the-shelf parsing tools are found in abundance
for English. One problematic aspect of using them
to parse MRLs lies in the fact that these tools fo-
cus on the statistical modeling of configurational
information. These models often condition on the
position of words relative to one another (e.g. in
transition-based dependency parsing) or on the dis-
tance between words inside constituents (e.g. in
Head-Driven parsing). Many of the contributions to
the workshop show that working around existing im-
plementations may be insufficient, and we may have
to come up with more radical solutions.
Several studies present results that support the
conjecture that when free word-order is explicitly
taken into account, morphological information is
more likely to contribute to parsing accuracy. The
Relational-Realizational model used in (Tsarfaty
and Sima?an, 2010) allows for reordering of con-
stituents at a configuration layer, which is indepen-
dent of the realization patterns learned from the data
(vis-a`-vis case marking and agreement). The easy-
first algorithm of (Goldberg and Elhadad, 2010)
which allows for significant flexibility in the order of
attachment, allows the model to benefit from agree-
ment patterns over dependency arcs that are easier
to detect and attach first. The use of larger subtrees
in (Chung et al, 2010) for parsing Korean, within a
Bayesian framework, allows the model to learn dis-
tributions that take more elements into account, and
thus learn the different distributions associated with
morphologically marked elements in constituency
structures, to improve performance.
7
In addition to free word order, MRLs show higher
degree of freedom in extraposition. Both of these
phenomena can result in discontinuous structures.
In constituency-based treebanks, this is either an-
notated as additional information which has to be
recovered somehow (traces in the case of the PTB,
complex edge labels in the German Tu?Ba-D/Z), or
as discontinuous phrase structures, which cannot be
handled with current PCFG models. Maier (2010)
suggests the use of Linear Context-Free Rewriting
Systems (LCFRSs) in order to make discontinuous
structure transparent to the parsing process and yet
preserve familiar notions from constituency.
Dependency representation uses non-projective
dependencies to reflect discontinuities, which is
problematic to parse with models that assume pro-
jectivity. Different ways have been proposed to deal
with non-projectivity (Nivre and Nilsson, 2005; Mc-
Donald et al, 2005; McDonald and Pereira, 2006;
Nivre, 2009). Bengoetxea and Gojenola (2010)
discuss non-projective dependencies in Basque and
show that the pseudo-projective transformation of
(Nivre and Nilsson, 2005) improves accuracy for de-
pendency parsing of Basque. Moreover, they show
that in combination with other transformations, it
improves the utility of these other ones, too.
4.4 Estimation and Smoothing: Coping with
Lexical Sparsity
Morphological word form variation augments the
vocabulary size and thus worsens the problem of lex-
ical data sparseness. Words occurring with medium-
frequency receive less reliable estimates, and the
number of rare/unknown words is increased. One
way to cope with the one of both aspects of this
problem is through clustering, that is, providing an
abstract representation over word forms that reflects
their shared morphological and morphosyntactic as-
pects. This was done, for instance, in previous work
on parsing German. Versley and Rehbein (2009)
cluster words according to linear context features.
These clusters include valency information added to
verbs and morphological features such as case and
number added to pre-terminal nodes. The clusters
are then integrated as features in a discriminative
parsing model to cope with unknown words. Their
discriminative model thus obtains state-of-the-art re-
sults on parsing German.
Several contribution address similar challenges.
For constituency-based generative parsers, the sim-
ple technique of replacing word forms with more
abstract symbols is investigated by (Seddah et al,
2010; Candito and Seddah, 2010). For French, re-
placing each word form by its predicted part-of-
speech and lemma pair results in a slight perfor-
mance improvement (Seddah et al, 2010). When
words are clustered, even according to a very local
linear-context similarity measure, measured over a
large raw corpus, and when word clusters are used in
place of word forms, the gain in performance is even
higher (Candito and Seddah, 2010). In both cases,
the technique provides more reliable estimates for
in-vocabulary words, since a given lemma or cluster
appear more frequently. It also increases the known
vocabulary. For instance, if a plural form is un-
seen in the training set but the corresponding singu-
lar form is known, then in a setting of using lemmas
in terminal symbols, both forms are known.
For dependency parsing, Marton et al (2010) in-
vestigates the use of morphological features that in-
volve some semantic abstraction over Arabic forms.
The use of undiacritized lemmas is shown to im-
prove performance. Attia et al (2010) specifically
address the handling of unknown words in the latent-
variable parsing model. Here again, the technique
that is investigated is to project unknown words to
more general symbols using morphological clues. A
study on three languages, English, French and Ara-
bic, shows that this method helps in all cases, but
that the greatest improvement is obtained for Arabic,
which has the richest morphology among three.
5 Where we?re at
It is clear from the present overview that we are
yet to obtain a complete understanding concerning
which models effectively parse MRLs, how to an-
notate treebanks for MRLs and, importantly, how
to evaluate parsing performance across types of lan-
guages and treebanks. These foundational issues are
crucial for deriving more conclusive recommenda-
tions as to the kind of models and morphological
features that can lead to advancing the state-of-the-
art for parsing MRLs. One way to target such an
understanding would be to encourage the investiga-
tion of particular tasks, individually or in the context
8
of shared tasks, that are tailored to treat those prob-
lematic aspects of MRLs that we surveyed here.
So far, constituency-based parsers have been as-
sessed based on their performance on the PTB (and
to some extent, across German treebanks (Ku?bler,
2008)) whereas comparison across languages was
rendered opaque due to data set differences and
representation idiosyncrasies. It would be interest-
ing to investigate such a cross-linguistic compari-
son of parsers in the context of a shared task on
constituency-based statistical parsing, in additional
to dependency-based ones as reported in (Nivre et
al., 2007a). Standardizing data sets for a large
number of languages with different characteristics,
would require us, as a community, to aim for
constituency-representation guidelines that can rep-
resent the shared aspects of structures in different
languages, while at the same time allowing differ-
ences between them to be reflected in the model.
Furthermore, it would be a good idea to intro-
duce parsing tasks, for either constituent-based or
dependency-based setups, which consider raw text
as input, rather than morphologically segmented
and analyzed text. Addressing the parsing prob-
lem while facing the morphological disambiguation
challenge in its full-blown complexity would be il-
luminating and educating for at least two reasons:
firstly, it would give us a better idea of what is the
state-of-the-art for parsing MRLs in realistic scenar-
ios. Secondly, it might lead to profound insights
about the potentially successful ways to use mor-
phology inside a parser, which may differ from the
insights concerning the use of morphology in the
less realistic parsing scenarios, where gold morpho-
logical information is given.
Finally, to be able to perceive where we stand
with respect to parsing MRLs and how models fare
against one another across languages, it would be
crucial to arrive at evaluation metrics that capture
information that is shared among the different repre-
sentations, for instance, functional information con-
cerning predicate-argument relations. Using the dif-
ferent kinds of measures in the context of cross-
framework tasks will help us understand the util-
ity of the different evaluation metrics that have been
proposed and to arrive at a clearer picture of what it
is that we wish to compare, and how we can faith-
fully do so across models, languages and treebanks.
6 Conclusion
This paper presents the synthesis of 11 contributions
to the first workshop on statistical parsing for mor-
phologically rich languages. We have shown that
architectural, representational, and estimation issues
associated with parsing MRLs are found to be chal-
lenging across languages and parsing frameworks.
The use of morphological information in the non
gold-tagged input scenario is found to cause sub-
stantial differences in parsing performance, and in
the kind of morphological features that lead to per-
formance improvements.
Whether or not morphological features help pars-
ing also depends on the kind of model in which
they are embedded, and the different ways they are
treated within. Furthermore, sound statistical esti-
mation methods for morphologically rich, complex
lexica, turn out to be crucial for obtaining good pars-
ing accuracy when using general-purpose models
and algorithms. In the future we hope to gain better
understanding of the common pitfalls in, and novel
solutions for, parsing morphologically ambiguous
input, and to arrive at principled guidelines for se-
lecting the model and features to include when pars-
ing different kinds of languages. Such insights may
be gained, among other things, in the context of
more morphologically-aware shared parsing tasks.
Acknowledgements
The program committee would like to thank
NAACL for hosting the workshop and SIGPARSE
for their sponsorship. We further thank INRIA Al-
page team for their generous sponsorship. We are
finally grateful to our reviewers and authors for their
dedicated work and individual contributions.
References
Bharat Ram Ambati, Samar Husain, Sambhav Jain,
Dipti Misra Sharma, and Rajeev Sangal. 2010a. Two
methods to incorporate local morphosyntactic features
in Hindi dependency parsing. In Proceedings of the
NAACL/HLT Workshop on Statistical Parsing of Mor-
phologically Rich Languages (SPMRL 2010), Los An-
geles, CA.
Bharat Ram Ambati, Samar Husain, Joakim Nivre, and
Rajeev Sangal. 2010b. On the role of morphosyntactic
features in Hindi dependency parsing. In Proceedings
9
of the NAACL/HLT Workshop on Statistical Parsing of
Morphologically Rich Languages (SPMRL 2010), Los
Angeles, CA.
Abhishek Arun and Frank Keller. 2005. Lexicalization
in crosslinguistic probabilistic parsing: The case of
French. In Proceedings of the 43rd Annual Meeting of
the Association for Computational Linguistics, pages
306?313, Ann Arbor, MI.
Mohammed Attia, Jennifer Foster, Deirdre Hogan,
Joseph Le Roux, Lamia Tounsi, and Josef van Gen-
abith. 2010. Handling unknown words in statistical
latent-variable parsing models for Arabic, English and
French. In Proceedings of the NAACL/HLT Workshop
on Statistical Parsing of Morphologically Rich Lan-
guages (SPMRL 2010), Los Angeles, CA.
Kepa Bengoetxea and Koldo Gojenola. 2010. Applica-
tion of different techniques to dependency parsing of
Basque. In Proceedings of the NAACL/HLT Workshop
on Statistical Parsing of Morphologically Rich Lan-
guages (SPMRL 2010), Los Angeles, CA.
Daniel M. Bikel. 2002. Design of a multi-lingual,
parallel-processing statistical parsing engine. In Pro-
ceedings of the Second International Conference on
Human Language Technology Research, pages 178?
182. Morgan Kaufmann Publishers Inc. San Francisco,
CA, USA.
E. Black, S. Abney, D. Flickinger, C. Gdaniec, R. Gr-
ishman, P. Harrison, D. Hindle, R. Ingria, F. Jelinek,
J. Klavans, M. Liberman, M. Marcus, S. Roukos,
B. Santorini, and T. Strzalkowski. 1991. A procedure
for quantitatively comparing the syntactic coverage
of English grammars. In Proceedings of the DARPA
Speech and Natural Language Workshop, pages 306?
311, San Mateo (CA). Morgan Kaufman.
Rens Bod. 2003. An efficient implementation of a new
DOP model. In Proceedings of the tenth conference
on European chapter of the Association for Computa-
tional Linguistics, pages 19?26, Budapest, Hungary.
Joan Bresnan. 2001. Lexical-Functional Syntax. Black-
well, Oxford.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of the Tenth Conference on Computa-
tional Language Learning (CoNLL), pages 149?164,
New York, NY.
Marie Candito and Djame? Seddah. 2010. Parsing word
clusters. In Proceedings of the NAACL/HLT Workshop
on Statistical Parsing of Morphologically Rich Lan-
guages (SPMRL 2010), Los Angeles, CA.
Xavier Carreras, Michael Collins, and Terry Koo. 2008.
TAG, dynamic programming, and the perceptron for
efficient, feature-rich parsing. In Proceedings of the
Twelfth Conference on Computational Natural Lan-
guage Learning (CoNLL), pages 9?16, Manchester,
UK.
Eugene Charniak and Mark Johnson. 2005. Course-to-
fine n-best-parsing and maxent discriminative rerank-
ing. In Proceedings of the 43rd Annual Meeting of the
ACL, pages 173?180, Barcelona, Spain, June.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the 1st Annual Meeting of the
North American Chapter of the ACL (NAACL), Seattle.
David Chiang. 2000. Statistical parsing with an
automatically-extracted Tree Adjoining Grammar. In
Proceedings of the 38th Annual Meeting on Associ-
ation for Computational Linguistics, pages 456?463,
Hong Kong. Association for Computational Linguis-
tics Morristown, NJ, USA.
Tagyoung Chung, Matt Post, and Daniel Gildea. 2010.
Factors affecting the accuracy of Korean parsing. In
Proceedings of the NAACL/HLT Workshop on Sta-
tistical Parsing of Morphologically Rich Languages
(SPMRL 2010), Los Angeles, CA.
Michael Collins, Jan Hajic?, Lance Ramshaw, and
Christoph Tillmann. 1999. A statistical parser for
Czech. In Proceedings of the 37th Annual Meeting
of the ACL, volume 37, pages 505?512, College Park,
MD.
Michael Collins. 1997. Three Generative, Lexicalized
Models for Statistical Parsing. In Proceedings of the
35th Annual Meeting of the Association for Computa-
tional Linguistics, pages 16?23, Madrid, Spain.
Anna Corazza, Alberto Lavelli, Giogio Satta, and
Roberto Zanoli. 2004. Analyzing an Italian treebank
with state-of-the-art statistical parsers. In Proceedings
of the Third Third Workshop on Treebanks and Lin-
guistic Theories (TLT 2004), Tu?bingen, Germany.
Brooke Cowan and Michael Collins. 2005. Morphology
and reranking for the statistical parsing of Spanish. In
in Proceedins of EMNLP.
Benoit Crabbe? and Marie Candito. 2008. Expe?riences
d?analyse syntaxique statistique du franc?ais. In Actes
de la 15e`me Confe?rence sur le Traitement Automatique
des Langues Naturelles (TALN?08), pages 45?54, Avi-
gnon, France.
Amit Dubey and Frank Keller. 2003. Probabilistic pars-
ing for German using sister-head dependencies. In In
Proceedings of the 41st Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 96?103,
Ann Arbor, MI.
Amit Dubey. 2005. What to do when lexicalization fails:
parsing German with suffix analysis and smoothing.
In 43rd Annual Meeting of the Association for Compu-
tational Linguistics.
10
Jenny Rose Finkel, Alex Kleeman, and Christopher D.
Manning. 2008. Efficient, feature-based, conditional
random field parsing. In Proceedings of ACL.
Yoav Goldberg and Michael Elhadad. 2010. Easy-
first dependency parsing of Modern Hebrew. In Pro-
ceedings of the NAACL/HLT Workshop on Statistical
Parsing of Morphologically Rich Languages (SPMRL
2010), Los Angeles, CA.
Yoav Goldberg and Reut Tsarfaty. 2008. A single frame-
work for joint morphological segmentation and syntac-
tic parsing. In Proceedings of the 46nd Annual Meet-
ing of the Association for Computational Linguistics.
Yoav Goldberg, Reut Tsarfaty, Meni Adler, and Michael
Elhadad. 2009. Enhancing unlexicalized parsing per-
formance using a wide coverage lexicon, fuzzy tag-set
mapping, and em-hmm-based lexical probabilities. In
Proceedings of the 12th Conference of the European
Chapter of the Association for Computational Linguis-
tics, pages 327?335.
Kenneth L. Hale. 1983. Warlpiri and the grammar of
non-configurational languages. Natural Language and
Linguistic Theory, 1(1).
Erhard W. Hinrichs, Sandra Ku?bler, and Karin Naumann.
2005. A unified representation for morphological,
syntactic, semantic, and referential annotations. In
Proceedings of the ACL Workshop on Frontiers in Cor-
pus Annotation II: Pie in the Sky, pages 13?20, Ann
Arbor, MI.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of
ACL.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of ACL, pages
423?430.
Sandra Ku?bler, Erhard W. Hinrichs, and Wolfgang Maier.
2006. Is it really that difficult to parse German?
In Proceedings of the 2006 Conference on Empirical
Methods in Natural Language Processing, pages 111?
119, Sydney, Australia, July. Association for Compu-
tational Linguistics.
Sandra Ku?bler. 2008. The PaGe 2008 shared task on
parsing German. In Proceedings of the Workshop on
Parsing German, pages 55?63. Association for Com-
putational Linguistics.
Seth Kulick, Ryan Gabbard, and Mitchell Marcus. 2006.
Parsing the Arabic treebank: Analysis and improve-
ments. In Proceedings of TLT.
Dekang Lin. 1995. A dependency-based method for
evaluating broad-coverage parsers. In International
Joint Conference on Artificial Intelligence, pages
1420?1425, Montreal.
Mohamed Maamouri, Ann Bies, Tim Buckwalter, and
Wigdan Mekki. 2004. The Penn Arabic treebank:
Building a large-scale annotated Arabic corpus. In
Proceedings of NEMLAR International Conference on
Arabic Language Resources and Tools.
Mohamed Maamouri, Ann Bies, and Seth Kulick. 2008.
Enhanced annotation and parsing of the Arabic tree-
bank. In Proceedings of INFOS.
David M. Magerman. 1995. Statistical decision-tree
models for parsing. In Proceedings of the 33rd An-
nual Meeting on Association for Computational Lin-
guistics, pages 276?283, Cambridge, MA.
Wolfgang Maier. 2010. Direct parsing of discontin-
uous constituents in german. In Proceedings of the
NAACL/HLT Workshop on Statistical Parsing of Mor-
phologically Rich Languages (SPMRL 2010), Los An-
geles, CA.
Mitchell Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19(2):313?330.
Yuval Marton, Nizar Habash, and Owen Rambow. 2010.
Improving Arabic dependency parsing with lexical and
inflectional morphological features. In Proceedings of
the NAACL/HLT Workshop on Statistical Parsing of
Morphologically Rich Languages (SPMRL 2010), Los
Angeles, CA.
Ryan T. McDonald and Fernando C. N. Pereira. 2006.
Online learning of approximate dependency parsing
algorithms. In Proc. of EACL?06.
Ryan T. McDonald, Koby Crammer, and Fernando C. N.
Pereira. 2005. Online large-margin training of depen-
dency parsers. In Proc. of ACL?05, Ann Arbor, USA.
Igor Mel?c?uk. 1988. Dependency Syntax: Theory and
Practice. State University of New York Press.
Marianne Mithun. 1992. Is basic word order universal?
In Doris L. Payne, editor, Pragmatics of Word Order
Flexibility. John Benjamins, Amsterdam.
Joakim Nivre and Jens Nilsson. 2005. Pseudo-projective
dependency parsing. In Proceedings of the 43rd An-
nual Meeting of the Association for Computational
Linguistics (ACL), Ann Arbor, MI.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007a. The CoNLL 2007 shared task on depen-
dency parsing. In Proceedings of the CoNLL Shared
Task Session of EMNLP-CoNLL 2007, pages 915?932,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan McDon-
ald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret.
2007b. The CoNLL 2007 shared task on dependency
parsing. In Proceedings of the CoNLL Shared Task
Session of EMNLP-CoNLL 2007, pages 915?932.
11
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev,
Gu?ls?en Eryig?it, Sandra Ku?bler, Svetoslav Marinov,
and Erwin Marsi. 2007c. MaltParser: A language-
independent system for data-driven dependency pars-
ing. Natural Language Engineering, 13(2):95?135.
Joakim Nivre. 2009. Non-projective dependency parsing
in expected linear time. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Human Language Tech-
nologies 2007: The Conference of the North American
Chapter of the Association for Computational Linguis-
tics; Proceedings of the Main Conference.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and 44th Annual Meeting of the Association for Com-
putational Linguistics, Sydney, Australia, July. Asso-
ciation for Computational Linguistics.
Ines Rehbein and Josef van Genabith. 2007. Treebank
annotation schemes and parser evaluation for German.
In Proceedings of the 2007 Joint Conference on Em-
pirical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-
CoNLL), Prague, Czech Republic.
Geoffrey Sampson and Anna Babarczy. 2003. A test of
the leaf-ancestor metric for parse accuracy. Natural
Language Engineering, 9(04):365?380.
Natalie Schluter and Josef van Genabith. 2007. Prepar-
ing, restructuring, and augmenting a French Treebank:
Lexicalised parsers or coherent treebanks? In Proc. of
PACLING 07, Melbourne, Australia.
Djame? Seddah, Marie Candito, and Benoit Crabbe?. 2009.
Cross parser evaluation and tagset variation: A French
Treebank study. In Proceedings of the 11th Interna-
tion Conference on Parsing Technologies (IWPT?09),
pages 150?161, Paris, France, October. Association
for Computational Linguistics.
Djame? Seddah, Grzegorz Chrupa?a, Ozlem Cetinoglu,
Josef van Genabith, and Marie Candito. 2010.
Lemmatization and statistical lexicalized parsing of
morphologically-rich languages. In Proceedings of the
NAACL/HLT Workshop on Statistical Parsing of Mor-
phologically Rich Languages (SPMRL 2010), Los An-
geles, CA.
Wojciech Skut, Thorsten Brants, Brigitte Krenn, and
Hans Uszkoreit. 1998. A linguistically interpreted
corpus of German newspaper texts. In ESSLLI
Workshop on Recent Advances in Corpus Annotation,
Saarbru?cken, Germany.
Reut Tsarfaty and Khalil Sima?an. 2007. Three-
dimensional parametrization for parsing morphologi-
cally rich languages. In Proceedings of the 10th Inter-
national Conference on Parsing Technologies (IWPT),
pages 156?167.
Reut Tsarfaty and Khalil Sima?an. 2008. Relational-
Realizational parsing. In Proceedings of the 22nd In-
ternational Conference on Computational Linguistics,
pages 889?896.
Reut Tsarfaty and Khalil Sima?an. 2010. Model-
ing morphosyntactic agreement in constituency-based
parsing of Modern Hebrew. In Proceedings of the
NAACL/HLT Workshop on Statistical Parsing of Mor-
phologically Rich Languages (SPMRL 2010), Los An-
geles, CA.
Yannick Versley and Ines Rehbein. 2009. Scalable dis-
criminative parsing for german. In Proceedings of the
11th International Conference on Parsing Technolo-
gies (IWPT?09), pages 134?137, Paris, France, Octo-
ber. Association for Computational Linguistics.
12
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 12?23,
Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational Linguistics
Supervised Learning of German Qualia Relations
Yannick Versley
SFB 833
Universita?t Tu?bingen
versley@sfs.uni-tuebingen.de
Abstract
In the last decade, substantial progress has
been made in the induction of semantic rela-
tions from raw text, especially of hypernymy
and meronymy in the English language and
in the classification of noun-noun relations in
compounds or other contexts. We investigate
the question of learning qualia-like semantic
relations that cross part-of-speech boundaries
for German, by first introducing a hand-tagged
dataset of associated noun-verb pairs for this
task, and then provide classification results us-
ing a general framework for supervised classi-
fication of lexical relations.
1 Introduction
Ever since the introduction of wordnets (Miller
and Fellbaum, 1991) or more generally machine-
readable dictionaries containing semantic relations,
researchers have investigated ways to learn such ex-
amples automatically from large text corpora, or
generalize them from existing instances. Substan-
tial research exists on the learning of hyperonymy
relations (Hearst, 1992; Snow et al, 2005; Tjong
Kim Sang and Hofmann, 2009), meronymy relations
(Hearst, 1998; Berland and Charniak, 1999; Girju
et al, 2003) and selectional preferences (Erk et al,
2010; Bergsma et al, 2008; O? Se?aghdha, 2010).
Both lexicographic research (Chaffin and Her-
rmann, 1987; Morris and Hirst, 2004) and research
in cognitive psychology (Vigliocco et al, 2004;
McRae et al, 2005), argue that it is important to
consider relations beyond the classical inventory
of hyperonymy and meronymy relations; further-
more psychological research on priming (Hare et al,
2009) suggests different processing for different re-
lations, which would entail that cognitively plau-
sible modeling of human language should model
these relations explicitly rather than simply record-
ing untyped associations between concepts (as in the
?evocation? relation proposed for WordNet by Boyd-
Graber et al, 2006).
One set of suggestions for an extended inventory
of relations can be found in the telic and agentive
qualia relations of Pustejovsky (1991) which have
been shown to be useful in recognizing discourse re-
lations (Wellner et al, 2006), or metonymy/coercion
phenomena (Verspoor, 1997; Ru?d and Zarcone,
2011), and have the property of linking different
parts-of-speech groups, unlike meronymy and hy-
peronymy/troponymy.
The work we present in this paper consists of a
dataset of noun-verb associations for German con-
crete nouns, which we present in more detail in sec-
tion 3, and a state-of-the-art approach to the super-
vised classification of such cross-part-of-speech re-
lations using informative features from large collec-
tions of unannotated text, which we present in sec-
tion 4. Experimental results are discussed in section
6.
2 Related Work
Most of earlier work on discovering novel instances
of semantic relations was based on surface pattern
matching, as presented by Hearst (1998). In the do-
main of finding qualia relations, Cimiano and Wen-
deroth (2005) propose patterns such as ?. . . purpose
12
of X is . . . ? or ?. . . X is used to . . . ?, whereas they
argue that agentive qualia are best chosen from a
small, fixed inventory of verbs (e.g., make, bake,
create . . . ). Katrenko and Adriaans (2008a) addi-
tionally propose ?to Y a (new|complete) X? and ?a
(new|complete) X has been Y?d? as patterns for agen-
tive qualia.
Some of the more recent work starts out from
matches extracted by means of such a pattern, but
use supervised training data to learn semantic con-
straints that improve the precision by filtering the
extracted examples. Berland and Charniak (1999)
use some handcrafted rules to exclude abstract ob-
jects from the part-of relations they extract from a
corpus, and additionally rank pattern extractions by
collocation strength. Girju et al (2003) propose an
iterative refinement scheme based on taxonomic in-
formation from WordNet: In this learning approach,
general constraints using top-level semantic classes
(entity, abstraction, causal-agent) are passed to a de-
cision tree learner and iteratively refined until the se-
mantic constraints induced from the classes are no
longer ambiguous.
Katrenko and Adriaans (2008b, 2010) present ap-
proaches to learn semantic constraints for the use in
recognizing semantic relations between word tokens
(SemEval 2007 shared task, see Girju et al, 2009),
either in a graph-based generalization of Girju?s it-
erative refinement approach that is able to handle
sense ambiguities more gracefully, or by clustering
pairs of words by the joint similarity of both relation
arguments.
A complementary aspect is to improving recall
beyond the possibilities of a few hand-selected pat-
terns. Following Hearst (1998), Girju et al (2003)
show that it is possible to find usable patterns by
exploiting known positive examples and looking for
co-occurrences of these relation arguments in a cor-
pus. However, these patterns usually have low preci-
sion and/or very limited recall, meaning that a more
elaborate approach (such as Girju et al?s induction
of semantic constraints) is needed to make the best
use of them.
Yamada and Baldwin (2004) propose to use a
combination of templates typical of telic and agen-
tive qualia relations (X is worth Y ing, X deserves
Y ing, a well-Y ed X) and a statistical ranking com-
bining association and a classifier learned on pos-
itive and negative examples for that role. They
find that the combination of association statistic and
classification worked somewhat better than the tem-
plates alone.
One approach targeted at exploiting a greater
number of patterns for hyperonymy relations can be
found in the work of Snow et al (2005): they ex-
tract patterns consisting of the shortest path in the
dependency graph plus an optional satellite and use
the set of all found paths as features in a linear clas-
sifier. The resulting classifier for hyperonymy re-
lations outperforms single patterns both in terms of
precision and in terms of recall; a further improve-
ment can be achieved if the frequency of pattern in-
stances is binned instead of just occurrence or non-
occurrence being recorded.
Tjong Kim Sang and Hofmann (2009) investigate
the question whether it is necessary to use syntac-
tic (rather than surface) patterns for the hyperonym
classification approach of Snow et al They compare
a method of extracting features based on syntax as
in Snow et al?s approach with a surface-based al-
ternative where the string between two words, plus
optionally one word to the left or right side of the
word, is extracted. Tjong Kim Sang and Hofmann
argue that the benefit of the parser (additional recall
due to the better generalization capability of the syn-
tactic patterns) is mostly negated by parsing errors:
In some informative contexts that the system based
on POS patterns is able to find without problems,
parsing errors lead to a parse tree that does not ex-
hibit the intended (dependency path) pattern.
Several researchers have applied such pattern
classification approaches to a larger set of relations,
and have demonstrated that extracting a pattern dis-
tribution between occurrences and performing su-
pervised classification based on this distribution is
a promising solution for semantic relations that go
beyond hyperonymy.
O? Se?aghdha and Copestake (2007) use a super-
vised classification approach for noun-noun com-
pounds combining context features for each of the
single words with features characterizing the joint
occurrences of the two nouns that are part of the tar-
get compound. In their experiments, they found that
linear classification using informative (bag-of-words
and bag-of-triples) features in conjunction with fea-
tures aimed at the similarity of each word of the tar-
13
get pair yields good results. In particular, the results
of using a linear classifier with informative corpus-
based features that are quite close to those that can
be achieved using a (more accurate, but computa-
tionally quite expensive) string kernel or those that
O? Se?aghdha (2007) achieves using taxonomic infor-
mation from WordNet.
Turney (2008) presents a general approach for
classifying word pairs into semantic relations by ex-
tracting the strings occurring between the two words
of a pair (up to three words in-between, up to one
word on either side) and using a frequency-based
selection process to select sub-patterns where words
from the extracted context pattern may have been re-
placed by a wildcard. Using standard machine learn-
ing tools (a support vector machine with radial base
function kernel), he is able to reach results that are
close to those possible with previous more special-
ized approaches.
Similarly, Herdag?delen and Baroni (2009) tackle
a variety of problems in semantic relation classifi-
cation using a unified approach where frequent uni-
grams and bigrams are extracted from co-occurrence
contexts of the target word pair (in addition to fea-
tures extracted from general occurrence contexts of
each word). Herdag?delen and Baroni?s approach
uses a linear SVM (which is faster and better-suited
to large data sets in general than either kernelized
support vector machines or nearest-neighbour ap-
proaches) yet is able to reach competitive accuracy.
In contrast to approaches using generic machine
learning, O? Se?aghdha and Copestake (2009) and
Nakov and Kozareva (2011) model the similarities
between related word pairs more explicitly in terms
of distributional kernels (O? Se?aghdha and Copes-
take), or as a similarity metric between word pairs
(Nakov and Kozareva). Such approaches allow more
flexibility in the modeling of similarity and the com-
bination of lexical and relational similarity mea-
sures, but are less well-suited for scaling up to more
training data.1
Because of the need for sufficient training data,
purely supervised approaches to learning relations
1O? Se?aghdha and Copestake (2009) reports training times
of slightly more than one day for their most efficient method
whereas a ten-fold crossvalidation run using SVMperf ? see the
presentation on p. 6 ? takes under an hour, i.e., using linear
classification is more efficient by a factor of about 100.
in morphologically-rich languages are often lim-
ited to the classical relations found in wordnets.
Tjong Kim Sang and Hofmann (2009) use a Dutch
corpus and hyperonymy relations from the Dutch
Cornetto wordnet and mention relatively few dif-
ferences to approaches on English such as Snow
et al (2005). Kurc and Piasecki (2008) apply
the semi-supervised approach of Pantel and Pen-
nachiotti (2006) for learning hyperonymy relations,
but modify the patterns used to enforce morphosyn-
tactic agreement and accommodate a more flexi-
ble word order. Versley (2007) uses Web pattern
queries for finding hyperonymy relations and men-
tions the fact that greater morphological richness
and the smaller size of the German Web make the
use of Web queries more complex than for English.
Outside the realm of hyperonymy, Regneri (2006)
uses Web-based pattern search to classify verb-verb
associations into the semantic classes proposed for
English by Chklovski and Pantel (2004). Ru?d and
Zarcone (2011) perform a corpus study of patterns
indicative of telic and agentive qualia relations in a
German Web corpus, but perform no automatic clas-
sification.
In summary, the research of Tjong Kim Sang and
Hofmann (2009) seems to indicate that at least hy-
peronymy relations can be found using a shallow
pattern approach despite greater word order flex-
ibility of languages such as Dutch and German.
For cross-part-of-speech relations, such as telic and
agentive qualia, such a question has been unad-
dressed as of yet, which prompted us to create a
dataset that is suitable for such an investigation.
3 Material
In order to investigate general-domain Noun-Verb
relations in German, we first had to create an ap-
propriate dataset that captures a realistic notion of
the relationships that humans infer in a text. Exist-
ing datasets that explore this space (most of them
for English) use a variety of approaches: One ap-
proach starts from examples (such as the popular
analogy dataset for English introduced by Turney
and Littman, 2003); other approaches such as the
data collection for the SemEval task on identifying
relations between nominals (Girju et al, 2009; Hen-
drickx et al, 2010) start from common semantic re-
14
lations and use patterns to gather positive and nega-
tive examples by Web queries.
In our case, we started from noun-verb associa-
tions found in a sample of human-produced asso-
ciations to concrete noun stimuli (Melinger et al,
2006); starting from the original association data, we
excluded items that were produced by less than three
subjects and used the part-of-speech information at-
tached to the data to retrieve only the verb associates.
The classification scheme was motivated by exist-
ing generative lexicon research (Pustejovsky, 1991;
Lenci et al, 2003), but was modeled to achieve a
good fit to the associations present in the data rather
than to force a good fit to any particular theory.
? agentive relations exist between an artifact and
an event that creates or procures it (e.g. bread-
bake)
? the telic relations exist between an entity and
an event that is related to its purpose or (actual
or intended) role:
? telic-artifact holds between an artifact and
its intended usage (e.g. plane-fly)
? telic-role holds between a role (i.e., a pro-
fession, organizational position etc.) and
activities related to that role (e.g. cowboy-
ride)
? telic-bodypart holds between a body part
and its intended uses (e.g. eye-see)
? the behaviour group of relations hold between
an entity and events that are caused by it, but
are not necessarily intentional or related to a
role that it fulfills:
? behaviour-animate are typical activities
performed by animate entities that are un-
related to the role that they fulfill for hu-
mans (e.g., dog-bark)
? behaviour-artifact relates artifacts to (usu-
ally) unintended behaviour associated
with them (e.g., moped-rattle)
? behaviour-environment relates elements
of the environment to events that go on
around them (e.g., sun-shine)
? location relations hold between elements of the
environment and activities typically performed
in or at them (e.g., mountain-climb)
? grooming relations hold between artifacts and
activities that contribute to the readiness of an
artifact (or body part) for its intended use but
are not directly related to it (e.g., plant-water,
hair-dye)
In comparison to standard schemes such as SIM-
PLE (Lenci et al, 2003), we have extended the set of
telic and agentive qualia from the original generative
lexicon approach by supplementing it with relations
that describe the affordances of objects or guides the
interpretative linking of objects and events, namely
location for affordances of elements of the environ-
ment and grooming for object-related actions that
may not be necessary for a differently-built object
with that same function, and finally behaviour de-
scribes events that co-occur with objects but are usu-
ally not part of a human agent?s action plan.
As a refinement, we subdivided the telic qualia
and behaviour relations, in particular specifying any
telic relation with the reason a concrete object may
be relevant for goal-directed processing ? either by
teleological interpretation of body parts, by the cre-
ation of artifacts with a specific purpose, or the es-
tablishment of roles with social conventions sup-
porting certain types of actions.
Among the responses collected by Melinger et al
(2006), we found relatively few instances that were
genuinely ambiguous (Drachen - fliegen, which may
either be interpreted as ?kite/fly?, in which case it
would be a telic-artifact relation, or as ?dragon/fly?,
in which case it would be a behaviour-animate rela-
tion), but found that domestic animals (cows, horses,
dogs) have affordances such as horse-ride or dog-
bark that indicate they are conceptualized as instru-
ments serving a particular goal (which means that
the relation should be labeled as telic-artifact rather
than as behaviour-animate).
In the associated word pairs, we also found re-
lations such as Zwiebel-schneiden (?onion-cut?) or
Handtuch-duschen (?towel-shower?) where the ac-
tion is related to a thing?s purpose but not identical
to it (towels are used to dry yourself after showering,
and people acquire onions to eat them after having
cut them). Our initial annotation included a com-
bination between the qualia-like relations presented
here and an additional event-semantic relation link-
ing the elicited event and the intended affordance of
15
the object. However, the event relation was left out
of the dataset used in the experiments to avoid data
sparsity.
In our dataset with 641 items, the most fre-
quent relations are telic-artifact (425 instances),
behaviour-animate (94 instances), telic-role (35 in-
stances), telic-bodypart (24 instances). The other re-
lations have between 2 and 17 instances each (see
table 3). The relationship data is therefore heavily
skewed.
4 Classification Approach
Our classification approach is aimed at a practi-
cal toolkit for supervised classification of lexical-
semantic relations, similar in spirit to the BagPack
approach of Herdag?delen and Baroni (2009) but
adapted for the use in morphologically-rich lan-
guages, in particular German.
In addition to the surface-based unigram and bi-
gram features, we use features based on dependency
syntax, which is more robust against variation in
word order, and allows to reattach separable verb
prefixes.
4.1 Preprocessing
To see why a very shallow approach may be less use-
ful for German, let us consider a simple direct (ac-
cusative) object relation such as between aufessen
(eat up) and Kuchen (cake): this relation could be
realized in a variety of ways depending on clause
type and constituent order, as illustrated in example
(1).
(1) a. Peter isst den Kuchen auf.
Peter eats the cake up.
?Peter eats up the cake?.
b. Den Kuchen hat Peter aufgegessen.
The cakeacc has Peter eaten-up.
?Peter has eaten up the cake?.
c. . . . dass Peter den Kuchen aufisst.
. . . that Peter the cake up-eat.
?. . . that Peter eats up the cake?.
In German, clause type decides whether the verb
is in verb-second position (1a) or at the end of the
clause (1b,1c); additionally, as in (1a), prefixes of
verbs may be stranded at the end of a clause with the
verb in verb-second position.
In addition to morphological analysis, hence, reat-
tachment is necessary in such cases as (1a), and
parsing is necessary to reattach prefix and verb. In
cases such as (1b), word order variation also needs
to be taken into account in order to recover the direct
object relation, unlike in languages with less-flexible
word order.
As a text collection that furnishes contexts for the
words or word pairs that interest us, we use the web-
news corpus, a collection of online news articles col-
lected by Versley and Panchenko (2012). For the
processing of this 1.7 billion word corpus, we use
a pipeline that relies on deterministic dependency
parsing to provide complete dependency parses at a
speed that is suitable for the processing of Web-scale
corpora.
The parsing model is based on MALTParser, a
transition-based parser, and uses part-of-speech and
morphological information as input. Morphological
information is annotated using RFTagger (Schmid
and Laws, 2008), a state-of-the-art morphological
tagger based on decision trees and a large con-
text window (which allows it to model morpho-
logical agreement more accurately than a normal
trigram-based sequence tagger). While transition-
based parsers are quite fast in general, an SVM clas-
sifier (which is used in MALTParser by default) be-
comes slower with increasing training set. In con-
trast, using the MALTParser interface to LibLinear
by Cassel (2009), we were able to reach a much
larger speed of 55 sentences per second (against 0.4
sentences per second for a more feature-rich SVM-
based model that reaches state of the art perfor-
mance).
For lemmatization, we use the syntax-based
Tu?Ba-D/Z lemmatizer (Versley et al, 2010), which
uses a separate morphological analyzer and some
fallback heuristics. The SMOR morphology
(Schmid et al, 2004) serves to provide morpholog-
ical analyses for novel words, covering inflection,
derivation and composition processes. For unana-
lyzed novel words that are not covered by SMOR,
the lemmatizer falls back to surface-based guessing
heuristics. It uses morphological and syntactic in-
formation to provide more accurate lemmas; In ad-
dition to dependency structures, the morphological
tags from RFTagger as well as global frequency in-
formation are used.
16
4.2 Classification
For classification, we use the following learning
methods:
? For the SVMperf classifier, the set of possible
labels is decomposed into binary problems us-
ing the one-vs-all scheme (for each possible la-
bel, a classifier is trained that receives the in-
stances of this label as positive instances and
the others as negative instances). SVMperf al-
lows the training of models that either optimize
(an upper bound for) the accuracy (SVMacc)
or the f-measure (SVMF) of positive instances
(Joachims, 2005).
? The Maximum Entropy (MaxEnt) classifier
directly learns the multiclass decision. Here,
we used the AMIS package by Miyao and Tsu-
jii (2002).
All experiments are run in a ten-fold crossvalidation
setup where the data is split ten portions and each
portion (fold) is tagged using a classifier trained on
the remaining nine folds. This setup leads to de-
creased variation
As noted in section 6, SVMperf using optimization
for accuracy (i.e., a standard linear kernel SVM with
hinge loss and a one-versus-all reduction to handle
the multiclass problem) performs best on the two ag-
gregate measures that we used (accuracy and macro-
averaged F). Hence, most results we report in the
later part only use the standard SVM learner.
4.3 Features
The first group of surface-based features uses
a similar technique to Herdag?delen and Baroni
(2009): given the co-occurrences of two words X
and Y with at most 4 words in-between, we extract
frequent unigrams and bigrams. Because we can
maintain the sparsity of the resulting feature vector
(see section 5), we can use a larger list of 10 000
each of the most frequent unigrams and bigrams
(w12) alternatively to a list with only 2 000 entries
each (w12:2k). The lem12 feature uses the same
approach, but uses lemmas instead.
A second group of features uses a path-based
representation based on a modified version of the de-
pendency parse (where the main verb, and not the
auxiliary verb is the head of a clause and is con-
nected to both the subject and its other arguments).
In the path-based representation, we can extract
the (shortest) path between the two target words in
the dependency graph. The rel feature records the
complete path (labeled dependency edges as well
as lemmas of intervening nodes) between the target
words. In contrast, the sat feature records labeled
dependency edges as well as lemmas of the depen-
dents of one of the target words.
Because the rel feature yields relatively large
(and therefore sparse) strings, we also decompose
the dependency path in triples consisting of labeled
dependency edge in the path and the two nodes ad-
jacent to it (with the endpoints replaced by ?w1? and
?w2?, respectively) for the triples feature.
In order to emulate the feature extraction of Snow
et al (2005), we introduce a relsat feature, which
pairs the path (as in the rel feature) with one de-
pendent of either target word. The relsat feature
would be able to model patterns such as ?w1 and
other w2?, where a modifier (?other?) is not part of
the shortest dependency path between w1 and w2.
In addition, a feature based on GermaNet (Hen-
rich and Hinrichs, 2010) uses taxonomic informa-
tion: possible hypernyms of the noun and verb in
the pair are extracted, and are used by themselves
(e.g. ?noun is a hyponym of ?thing? ?, or ?verb is a
hyponym of ?communicate? ?) and in combinations
of up to two of these possible hypernym labels.
In addition to taxonomic information from Ger-
maNet, we use distributional similarity features
for single words. For the nouns, we use distribu-
tional features based on the co-occurrence of pre-
modifying adjectives, which Versley and Panchenko
(2012) found to work better than other grammatical-
relation-based collocates (attr1), while we use
Pado? and Lapata?s (2007) method of gathering and
weighting collocates based on distance in the depen-
dency graph for the verbs (pl2). Herdag?delen and
Baroni (2009) simply use a window-based approach
for gathering collocates, which we reimplemented as
a simpler way of capturing distributional similarity.
The resulting features are named w1 and w2.
17
Seine Tante backt ta?glich leckeren Kuchen
DET NSUBJ ADV
OBJA
AMOD
His aunt bakes daily luscious cake
?his aunt bakes luscious cake every day?
w12 Seinew2,w1 Tantew2,w1 w2 ta?glichw1
lem12 seinw2,w1 Tantew2,w1 w2 ta?glichw1
rel ?OBJA
sat w2ADV:ta?glich w1AMOD:lecker
w1NSUBJ:Tante
triples w1 ?OBJAw2
relsat ?OBJA/w2ADV:ta?glich
?OBJA/w1AMOD:lecker
?OBJA/w1NSUBJ:Tante
Due to the short path between w1 and w2, the triples and
rel features are not very different in the example. In case of
more complicated constructions, the triples approach would
yield multiple simpler features whereas rel would yield one
single complex string.
Figure 1: Kinds of features
5 Count Transformations
It is a well-known fact in distributional semantics
that raw observation counts for context items (be
they elements surrounding single word occurrences
or elements extracted from the occurences of two
words together) are incomparable for different target
words/target pairs (since their frequency can differ)
as well as for different context items. As a result, re-
searchers have proposed different approaches to pro-
duce transformed vectors using more sophisticated
association statistics (see Dumais, 1991, Weeds
et al, 2004, Turney and Pantel, 2010, inter alia).
In our case, we implemented L1 normalization
(which normalizes for target word frequency), a con-
servative estimate for pointwise mutual information
(which normalizes for the frequencies of both target
word and feature), and the G2 log-likelihood mea-
sure of Dunning (1993), which gives significance
scores (i.e., numbers that invariably grow both with
target and feature frequency, even if the association
strength ? the relation between actual occurrences
and those that would be expected when assuming no
association ? is constant). In both cases, very fre-
quent features would be emphasized in comparison
to medium- and low-frequency features.
In the realm of supervised learning, an additional
choice has to be made among learning methods that
can classify words or word pairs using large feature
vectors ? most commonly using nearest-neighbour
classification (Nakov and Kozareva, 2011), us-
ing custom kernels in support vector classification
(O? Se?aghdha and Copestake, 2009; Turney, 2008),
or by using appropriate techniques to represent the
feature vectors in linear classification.
In comparison to the former methods, linear clas-
sification scales better with the number of exam-
ples (where nearest-neigbour and kernel-based tech-
niques both show strongly superlinear behaviour)
and would be the method of choice for large-scale
classification.
Herdag?delen and Baroni (2009) propose to map
the values computed by association statistics by
computing mean and standard deviation of each fea-
ture and mapping the range [??2?, ?+2?] of asso-
ciation scores for that feature (seen over the values
of that feature for all target pairs) to the range [0, 1]
in the input for the classifier, clamping values out-
side that range to 0 or 1, respectively.
Unfortunately, the approach proposed by
Herdag?delen and Baroni has the property that an
association score of 0 is mapped to a non-zero
feature value for the classifier, which means that
feature vectors are no longer sparse (i.e., instead of
only storing non-zero values for context items that
are informative, values for all context items have to
be processed).
To keep the sparsity of the transformed counts,
we always use 0 as the lower bound of the mapping
(such that zero values stay zero values). In addi-
tion to the Herdagdelen and Baroni?s mean/variance-
based threshold, we investigated the following pos-
sibilities for fixing the upper bound:
? MI scale: use a constant upper bound of 1 on (a
conservative estimate of) the pointwise mutual
information.2
2To yield a conservative MI estimate, we use the discounting
factor introduced by Pantel and Lin (2002). The pointwise mu-
tual information value normalizes the frequency of both words
of a pair, hence all mutual information values are on a common
scale. A threshold of 1 in this case corresponds to two items oc-
18
baselines/single features Acc MacroF
random 0.463 0.090
telic-artifact 0.663 0.080
w12/L1-norm/AMIS 0.677 0.181
w12/L1-norm/SVMacc 0.715 0.212
w12/L1-norm/SVMF 0.674 0.120
w12/L1-norm 0.715 0.212
lem12/G2-quant 0.703 0.204
rel/L1-quant 0.722 0.154
sat/L1-norm 0.700 0.185
triples/L1-quant 0.741 0.192
triples/G2-norm 0.739 0.212
relsat/L1-quant 0.698 0.154
attr1+pl2/MI-thr 0.800 0.460
w1+w2/MI-thr 0.807 0.468
GermaNet, no combination 0.846 0.450
GermaNet, degree=2 0.851 0.516
Table 1: Trivial and single-feature baselines (using SVM-
acc unless noted otherwise)
? norm: use a value based on mean and standard
deviation of the occurring values for one given
feature (?+ 2?).
? quant: use a fixed quantile (99%) of all values
for a feature for the upper bound of the map-
ping interval.
In addition, to mapping feature values onto the
unit interval [0, 1], we investigated the usefulness of
making the features binary-valued by mapping all
values lower than the threshold to zero. While intu-
itively a continuous-valued feature should be more
informative, the high dimensionality of the feature
space may mean that noisy feature extraction ulti-
mately leads to a worse model in the continuous-
feature case.
6 Results and Discussion
Because of the skewed distribution, it is useful to
look not only at the overall accuracy (Acc) but also
at the macro-average of the F-measure of all rela-
tions (MacroF). The macro-averaged F-measure re-
flects the ability of the system to recognize all re-
curring together about exp(1) ? 2.7 times as often as would be
expected from the marginal distribution for that co-occurrence
relation.
combinations Acc MacroF
triples/G2-norm 0.739 0.212
triples+w12/G2-norm 0.733 0.206
triples+rel/G2-norm 0.725 0.190
triples+sat/G2-norm 0.738 0.200
triples+relsat/G2-norm 0.729 0.184
triples+w1+w2/MI-thr 0.816 0.469
triples+attr1+pl2/MI-thr 0.807 0.431
GermaNet 0.851 0.516
GermaNet+triples/G2-norm 0.853 0.482
GermaNet+triples/MI-thr 0.855 0.484
GermaNet+w12/G2-norm 0.855 0.496
GermaNet+w12/MI-thr 0.858 0.510
GWN+w12+triples/G2-norm 0.852 0.462
GWN+w12+triples/MI-thr 0.849 0.478
GermaNet+w1+w2/MI-thr 0.828 0.496
Table 2: Combination results (using SVMacc)
lations since it weighs all relation types equally, in-
stead of (implicitly) weighting by token count where
under-predicting rare relation types normally yields
a higher accuracy. As is evident from table 1, the
accuracy baseline for the most frequent label (telic-
artifact) is already quite high.
Looking at results with various scaling methods
and learners on single features (table 1), we found
that the SVMacc learner consistently yields better
accuracy and macro-averaged F-measure than the
other two learners. For the weighting functions, we
found that none of the measures was consistently
better than the others; results for the single features
in table 1 are reported for a weighting function that
works best for either accuracy or macro-averaged F-
measure using. (For space reasons, table 1 shows
numbers only for the w12 feature and L1-norm scal-
ing; other features and settings show a similar rela-
tion between the scores for different learners).
As in the investigation by O? Se?aghdha and Copes-
take (2007), dependency triples from the path be-
tween the two target words are the most effective
feature representation and yields both the great-
est accuracy value (with L1 scaling and quantile-
based setting of thresholds) and the greatest F-
measure macroaverage (with G2 scaling and setting
of thresholds based on average and standard devi-
ation). Combination of the triples feature with
19
agentive beh-anim beh-artif beh-body beh-env grooming location telic-artif telic-body telic-role
count 14 94 13 2 5 17 12 425 24 35
w12 0.105 0.513 0.000 0.000 0.000 0.000 0.154 0.834 0.214 0.255
triples 0.125 0.601 0.000 0.000 0.000 0.000 0.153 0.853 0.153 0.238
attr1+pl2 0.333 0.826 0.258 0.000 0.000 0.500 0.421 0.874 0.636 0.754
w1+w2 0.385 0.834 0.222 0.000 0.000 0.400 0.571 0.877 0.619 0.767
GermaNet 0.480 0.859 0.190 0.000 0.000 0.451 0.636 0.909 0.773 0.857
GWN+w12 0.400 0.857 0.133 0.000 0.333 0.384 0.600 0.916 0.600 0.873
Table 3: Results by relation
other features based on paired co-occurrences does
not lead to further improvements, especially with
those features that also express information from the
dependency path (rel,relsat).
In comparison, the accuracy of the GermaNet hy-
pernyms feature (which includes combinations of
the hypernyms of first and second word) is much
higher than the versions that do not make use of
hand-crafted taxonomic knowledge, which is sur-
prising since it uses only taxonomic and no rela-
tional information. The pairwise feature combina-
tion for GermaNet features yields another small im-
provement over these already very good results. Dis-
tributional information on single words, both the
strictly window-based w1+w2 feature and the one
that is based on more elaborated distributional mod-
eling (attr1+pl2) show quite good results that
show further (but relatively small) improvements
when combined with the triples feature.
The importance of taxonomic (or, alternatively,
distributional semantic) information for the task pro-
posed here - namely, the supervised classification of
qualia-like relations - partly mirrors results for the
supervised classification of relations between nomi-
nals, where O? Se?aghdha and Copestake (2007) find
that their best system for distributional similarity
based on the BNC performs at about the same level
as a (somewhat simpler) approach using WordNet-
based classification (O? Se?aghdha, 2007), with only
much more sophisticated approaches such as the one
of O? Se?aghdha and Copestake (2009), which also
makes use of a considerably larger textual basis to
improve results over the level of the WordNet-based
approach.
Another reason for the importance of taxonomic
information in this task may lie in the fact that the
different relations have relatively strong selectional
restrictions (for animate objects, roles/professions,
body parts, or artifacts on the noun side, and certain
types of actions or events on the verb side).
Looking at the results for each relation in table
3, we see that both telic-artifact and behaviour-
animate, the two relations with the largest counts,
are classified quite reliably, while behaviour-
bodypart and behaviour-environment, the two rela-
tions with very few examples, are never found by
the system. Among the other relations, taxonomi-
cal information for nouns and verbs seems to be in-
strumental for adequate classification of the groom-
ing relation and possibly also for location, telic-
bodypart and telic-role.
7 Summary
In this paper, we have presented a dataset contain-
ing cross-part-of-speech relations between concrete
nouns and human verb associates and demonstrated
a state-of-the-art approach for the supervised mul-
ticlass classification of the qualia relations in this
dataset.3 Our results show that taxonomic informa-
tion from GermaNet is much superior to all other
features, while corpus-based dependency triples are
still visibly superior to shallow surface-based fea-
tures.
Important questions for future research would in-
clude a more direct comparison to other languages
(ideally using a similar data set and information
sources) to tease apart the influences of word order,
taxonomic organization, and data sparsity, respec-
tively.
3The dataset and future corrected/improved versions, are
available on request. Please feel free to send an email to the
author if you want to use it or produce a create a version for
another language.
20
Acknowledgements The work described in this
paper was funded by the Deutsche Forschungsge-
meinschaft (DFG) as part of Collaborative Research
Centre (SFB) 833. The author gratefully acknowl-
edges Melike Heubach?s help with the annotation of
the dataset.
References
Bergsma, S., Lin, D., and Goebel, R. (2008).
Discriminative learning of selectional preference
from unlabeled text. In EMNLP 2008.
Berland, M. and Charniak, E. (1999). Finding parts
in very large corpora. In Proceedings of ACL-
1999.
Boyd-Graber, J., Fellbaum, C., Osherson, D., and
Schapire, R. (2006). Adding dense, weighted
connections to WordNet. In Proceedings of the
Global WordNet Conference.
Cassel, S. (2009). MaltParser and LIBLINEAR
- transition-based dependency parsing with lin-
ear classification for feature model optimization.
Master?s thesis, Uppsala University.
Chaffin, R. and Herrmann, D. J. (1987). Relation
element theory: A new account of the represen-
tation and processing of semantic relations. In
Gorfein, D. S., editor, Memory and Learning: the
Ebbinghaus Centennial Conference. Erlbaum.
Chklovski, T. and Pantel, P. (2004). VerbOcean:
Mining the web for fine-grained semantic verb re-
lations. In Proc. EMNLP 2004.
Cimiano, P. and Wenderoth, J. (2005). Automati-
cally learning qualia structures from the web. In
Proceedings of the ACL?05 Workshop on Deep
Lexical Acquisition.
Dumais, S. (1991). Improving the retrieval of infor-
mation from external sources. Behavior Research
Methods, Instruments and Computers, 23(2):229?
236.
Dunning, T. (1993). Accurate methods for the statis-
tics of surprise and coincidence. Computational
Linguistics, 19(1):61?74.
Erk, K., Pado?, S., and Pado?, U. (2010). A flexi-
ble, corpus-driven model of regular and inverse
selectional preferences. Computational Linguis-
tics, 36(4):723?761.
Girju, R., Badulescu, A., and Moldovan, D. (2003).
Learning semantic constraints for the automatic
discovery of part-whole relations. In NAACL-HLT
2003.
Girju, R., Nakov, P., Nastase, V., Szpakowicz, S.,
and Turney, P. (2009). Classification of semantic
relations between nominals. Language Resources
and Evaluation, 43(2):105?121.
Hare, M., Jones, M., Thomson, C., Kelly, S., and
McRae, K. (2009). Activating event knowledge.
Cognition, 111:151?167.
Hearst, M. (1992). Automatic acquisition of hy-
ponyms from large text corpora. In Proc. of the
14th International Conference on Computational
Linguistics (COLING 92).
Hearst, M. (1998). Automated discovery of word-
net relations. In WordNet: An Electronic Lexical
Database. MIT Press, Cambridge (MA), USA.
Hendrickx, I., Kim, S. N., Kozareva, Z., Nakov, P.,
Se?aghdha, D. O., Pado?, S., Pennacchiotti, M., Ro-
mano, L., and Szpakowicz, S. (2010). Semeval-
2010 task 8: Multi-way classification of semantic
relations between pairs of nominals. In SemEval
2010.
Henrich, V. and Hinrichs, E. (2010). GernEdiT -
the GermaNet editing tool. In Proceedings of the
Seventh Conference on International Language
Resources and Evaluation (LREC 2010), pages
2228?2235.
Herdag?delen, A. and Baroni, M. (2009). BagPack:
A general framework to represent semantic rela-
tions. In ACL09 Workshop on Geometric Models
of Natural Language Semantics (GEMS09).
Joachims, T. (2005). A support vector method for
multivariate performance measures. In Proceed-
ings of the International Conference on Machine
Learning (ICML).
Katrenko, S. and Adriaans, P. (2008a). Qualia struc-
tures and their impact on the concrete noun cat-
egorization task. In ESSLLI 2008 workshop on
Distributional Lexical Semantics.
Katrenko, S. and Adriaans, P. (2008b). Semantic
types of some generic relation arguments: Detec-
tion and evaluation. In ACL/HLT 2008.
21
Katrenko, S. and Adriaans, P. (2010). Finding con-
straints for semantic relations via clustering. In
CLIN 2010.
Kurc, R. and Piasecki, M. (2008). Automatic ac-
quisition of wordnet relations by the morpho-
syntactic patterns extracted from the corpora in
Polish. In Proceedings of the International Multi-
conference on Computer Science and Information
Technology - Third International Symposium Ad-
vances in Artificial Intelligence and Applications
(IMCSIT 2008).
Lenci, A., Calzolari, N., and Zampolli, A. (2003).
SIMPLE: Plurilingual semantic lexicons for nat-
ural language processing. Linguistica Compu-
tatazionale, 16?17:323?352.
McRae, K., Cree, G., Seidenberg, M., and Mc-
Norgan, C. (2005). Semantic feature produc-
tion norms for a large set of living and nonliv-
ing things. Behaviour Research Methods, 37:547?
559.
Melinger, A., Schulte im Walde, S., and Weber, A.
(2006). Characterizing response types and reveal-
ing noun ambiguity in german association norms.
In Workshop on Making Sense of Sense: Bringing
Psycholinguistics and Computational Linguistics
Together.
Miller, G. A. and Fellbaum, C. (1991). Semantic
networks of English. Cognition, 41:197?229.
Miyao, Y. and Tsujii, J. (2002). Maximum entropy
estimation for feature forests. In HLT 2002.
Morris, J. and Hirst, G. (2004). Non-classical lexical
semantic relations. In HLT/NAACL Workshop on
Computational Lexical Semantics.
Nakov, P. and Kozareva, Z. (2011). Combining re-
lational and attributional similarity for semantic
relation classification. In RANLP 2011.
O? Se?aghdha, D. and Copestake, A. (2009). Using
lexical and relational similarity to classify seman-
tic relations. In EACL 2009.
O? Se?aghdha, D. (2007). Annotating and learning
compound noun semantics. In Proceedings of the
ACL07 Student Research Workshop.
O? Se?aghdha, D. (2010). Latent variable models of
selectional preference. In ACL 2010.
O? Se?aghdha, D. and Copestake, A. (2007). Co-
occurrence contexts for noun compound interpre-
tation. In ACL 2007 Workshop on A Broader Per-
spective on Multiword Expressions.
O? Se?aghdha, D. and Copestake, A. (2009). Us-
ing lexical and relational similarity to classify se-
mantic relations. In 12th Conference of the Eu-
ropean Chapter of the Association for Computa-
tional Linguistics (EACL).
Pado?, S. and Lapata, M. (2007). Dependency-based
construction of semantic space models. Compu-
tational Linguistics, 33(2):161?199.
Pantel, P. and Lin, D. (2002). Discovering word
senses from text. In Proceedings of ACM Confer-
ence on Knowledge Discovery and Data Mining
(KDD-02), pages 613?619.
Pantel, P. and Pennachiotti, M. (2006). Espresso:
Leveraging generic patterns for automatically har-
vesting semantic relations. In COLING/ACL
2006.
Pustejovsky, J. (1991). The generative lexicon.
Computational Linguistics, 17(4):409?441.
Regneri, M. (2006). VerbOzean - maschinelles
Lernen von semantischen Relationen zwischen
deutschen Verben. Bachelorarbeit, Universita?t des
Saarlandes.
Ru?d, S. and Zarcone, A. (2011). Covert events and
qualia structures for German verbs. In Metonymy
2011.
Schmid, H., Fitschen, A., and Heid, U. (2004).
SMOR: A German computational morphology
covering derivation, composition and inflection.
In Proceedings of LREC 2004.
Schmid, H. and Laws, F. (2008). Estimation of con-
ditional probabilities with decision trees and an
application to fine-grained POS tagging. In COL-
ING 2008.
Snow, R., Jurafsky, D., and Ng, A. Y. (2005). Learn-
ing syntactic patterns for automatic hypernym dis-
covery. In NIPS 2005.
Tjong Kim Sang, E. and Hofmann, K. (2009). Lexi-
cal patterns or dependency patterns: Which is bet-
ter for hypernym extraction. In CoNLL-2009.
22
Turney, P. (2008). A uniform approach to analogies,
synonyms, antonyms and associations. In Coling
2008.
Turney, P. and Littman, M. (2003). Learning analo-
gies and semantic relations. Technical Report
ERB-1103, National Research Council, Institute
for Information Technology.
Turney, P. and Pantel, P. (2010). From frequency
to meaning: Vector space models of seman-
tics. Journal of Artificial Intelligence Research,
37:141?188.
Versley, Y. (2007). Using the Web to resolve
coreferent bridging in German newspaper text.
In Proceedings of GLDV-Fru?hjahrstagung 2007,
Tu?bingen. Narr.
Versley, Y., Beck, A. K., Hinrichs, E., and Telljo-
hann, H. (2010). A syntax-first approach to high-
quality morphological analysis and lemma disam-
biguation for the Tu?Ba-D/Z treebank. In Proceed-
ings of the 9th Conference on Treebanks and Lin-
guistic Theories (TLT9).
Versley, Y. and Panchenko, Y. (2012). Not just
bigger: Towards better-quality Web corpora. In
Proceedings of the 7th Web as Corpus Workshop
(WAC-7), pages 44?52.
Verspoor, C. M. (1997). Contextually-Dependent
Lexical Semantics. PhD thesis, University of Ed-
inburgh.
Vigliocco, G., Vinson, D., Lewis, W., and Garrett,
M. (2004). Representing the meanings of object
and action words: The featural and unitary se-
mantic space hypothesis. Cognitive Psychology,
48:422?488.
Weeds, J., Weir, D., and McCarthy, D. (2004). Char-
acterizing measures of lexical distributional simi-
larity. In CoLing 2004.
Wellner, B., Pustejovsky, J., Havasi, C., Rumshisky,
A., and Sauri, R. (2006). Classification of dis-
course coherence relations: An exploratory study
using multiple knowledge sources. In Proc. 7th
SIGdial Workshop on Discourse and Dialogue.
Yamada, I. and Baldwin, T. (2004). Automatic dis-
covery of telic and agentive roles from corpus
data. In Proceedings of the 18th Pacific Asia Con-
ference on Language, Information and Computa-
tion (PACLIC 18).
23
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 36?41,
Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational Linguistics
Using Nominal Compounds for Word Sense DiscriminationYannick Versley
University of Tu?bingen
Department of Linguisticsversley@sfs.uni-tuebingen.de Verena HenrichUniversity of Tu?bingenDepartment of Linguisticsverena.henrich@uni-tuebingen.deAbstract
In many morphologically rich languages, con-
ceptually independent morphemes are glued
together to form a new word (a compound)
with a meaning that is often at least in part pre-
dictable from the meanings of the contribut-
ing morphemes. Assuming that most com-
pounds express a subconcept of exactly one
sense of its nominal head, we use compounds
as a higher-quality alternative to simply using
general second-order collocate terms in the
task of word sense discrimination. We eval-
uate our approach using lexical entries from
the German wordnet GermaNet (Henrich and
Hinrichs, 2010).1 Introduction
In several morphologically rich languages such as
German and Dutch, compounds are usually written
as one word: In a process where nouns, verbs and
other prefixes combine with a head noun (called the
simplex when it occurs on its own), a novel word
can be formed which is typically interpretable by
considering its parts and the means of combination.
The process of compounding is both highly produc-
tive and subject to lexicalization (i.e., the creation
of non-transparent compounds that can only be in-
terpreted as a whole rather than as a combination
of parts). The analysis of compounds have been
subject to interest in machine translation as well as
in the semantic processing of morphologically rich
languages. The analysis of compounds is generally
challenging for many reasons. In particular, com-
pounds leave us with the dilemma of either model-
ing them as complete units, yielding a more accu-
rate picture for lexicalized compounds but creating
a more severe sparse data problem in general, or try-
ing to separate out their parts and ending up with
problems of wrongly split lexicalized compounds,
or of incurring mis-splits where spurious ambigui-
ties occur.
The purpose of this paper is to address the ques-
tion of whether semantic information of compound
occurrences can be used to learn more about the
sense distribution of the simplex head, with respect
to a text collection. Specifically, this paper focuses
on the task of word sense discrimination, where the
goal is to find different senses of a word without
assuming a hand-crafted lexical resource as train-
ing material (in contrast to word sense disambigua-
tion, where the exact sense inventory to be tagged
is known at training and inference time, and where
making effective use of a resource such as WordNet
(Miller and Fellbaum, 1991) or GermaNet (Henrich
and Hinrichs, 2010) is an important part of the prob-
lem to be solved).
While the present paper focuses on nominal com-
pounds in German, the method as such can also be
applied to other languages where compounds are
written as one word.2 Related Work
Automatic word sense discrimination (WSD) is a
task that consists of the automatic discovery of a
sense inventory for a word and of associated exam-
ples for each sense.
To evaluate systems performing word sense dis-
crimination, earlier research such as Schu?tze (1998)
36
uses either pseudowords ? two words that have been
artificially conflated to yield an ambiguous concept
such as wide range/consulting firm ? or use (ex-
pensive) manually annotated data. Subsequently,
the contexts of these occurrences are clustered into
groups that correspond to training examples for each
postulated sense.
A different approach to the idea of word sense
discrimination can be found in the work of Pantel
and Lin (2002): they retrieve a set of most-similar
items to the target word, and then cluster these sim-
ilar items according to distributional semantic prop-
erties. In Pantel and Lin?s approach, the output of
the word sense induction algorithm is not a group
of contexts with the target word that will be used to
represent a sense, but instead one or more words that
are (hopefully) related to one particular sense. The
contexts in which the related words occur could then
be used as positive examples for that particular sense
of the target word.
Pantel and Lin aim at a principled approach to
compare the soft-clustering approaches they pro-
pose, in conjunction with a fixed set of related
words. While the main interest of this paper lies
in comparing different methods for generating the
candidate set of related words, the exact clustering
method is only of marginal interest. In this paper, a
simpler hard clustering method is used and only the
assignment for the tight center of a cluster is consid-
ered since the non-central items can be different or
even incomparable for the different methods.3 Our Approach
Our approach to word sense discrimination is based
on the idea that different compounds that have the
same simplex word as their head (e.g. Blu?tenblatt
?petal?, and Revolverblatt ?tabloid rag?) are less am-
biguous than the simplex (Blatt ?leaf?, ?newspa-
per?) itself. This assumption is along the lines with
what the ?one sense per collocation? heuristic of
Yarowsky (1993) would predict.
Yarowsky noted that in a corpus of homo-
graphs/homophones/near-homographs, translation
distinctions, and pseudo-words, a single collocation
(such as ?foreign? or ?presidential?) is often enough
to disambiguate the occurrence of a near-homograph
such as aid/aide. While Yarowsky claims that most
of the problems of such an approach would be due
to absent or unseen collocates, it is easily imagin-
able that collocates such as old or big can occur with
multiple senses of a word.
In German, noun compounds usually involve at
least a minimum degree of lexicalization: In En-
glish, ?red flag? and ?red beet? are lexicalized (i.e.,
denote something more specific than the composi-
tional interpretation would suggest), but ?red rag?
or ?red box? are purely compositional. In German,
Rotwein ?red wine? is a compound, but the more
compositional roter Apfel/*Rotapfel ?red apple? is
not a compound and points to the fact that ?red ap-
ple? only has a compositional interpretation. Be-
cause of this minimal required degree of lexicaliza-
tion, we would expect that German nominal com-
pounds (as well as any compounds in a language that
has a similar distinction between affixating and non-
affixating compounds) are, for the largest portion,
compositional enough to be interpretable, but lexi-
calized enough that a compound is always specific
to only one sense of its head simplex.3.1 Finding Committees
The method of finding committees that form sense
clusters is illustrated in Figure 1 using the target
word example Blatt. To generate a candidate list
of related terms, our method first retrieves all words
(compounds) that have the target word as a suffix
(step 3 in Figure 1). This candidate set is then sorted
according to distributional similarity with the target
word and cut off after N items (step 2 in Figure 1) to
reduce the influence of spurious matches and non-
taxonomic compounds and to avoid too much noise
in the candidate list.
In order to evaluate the method of selecting com-
pounds as candidate words, we first cluster the set
of candidate words into as many clusters as there
are target word senses represented in the candidate
words (step 3 in Figure 1, again using the distribu-
tional similarity vectors of the words described in
the following subsection 3.2).
To avoid biasing our method towards any partic-
ular method of choosing the candidate words, we
simply assume that it is possible to produce a ?rea-
sonable? number of clusters. In the next step, the
most central items of each cluster (the ?committee?)
are determined, purely by closeness to the cluster?s
37
Figure 1: Steps in the clustering method
centroid and disregarding similarity with the target
word. The committee words are rendered in bold
face in the circles in Figure 1. The quality of the
approach is then evaluated according to whether the
committees form a suitable representation for the set
of senses that the target word possesses.
An advantage of only including compounds in the
candidate list of related terms, instead of all words,
is that the related words generated by such an ap-
proach are conceptually considerably closer to the
target word than those using all words as candidates:
Using all words, the top candidates include the co-
ordinate terms Frucht ?fruit? and Blu?te ?flower?, as
well as more faraway terms such as Tuch ?cloth? or
Haar ?hair?; using only compounds of the simplex,
the candidate list contains mostly hyponyms such as
Laubblatt ?leaf?, Titelblatt ?title page? or Notenblatt
?sheet of music?.3.2 Distributional Similarity and Clustering
Both for the initial selection of candidate words
(where the list is cut off after the top-N similar
terms) and for the subsequent clustering step, fre-
quency profiles from a large corpus are used to cre-
ate a semantic vector from the target word and each
(potential or actual) candidate word.
To construct these frequency profiles, the web-
news corpus of Versley and Panchenko (2012) is
used, which contains 1.7 billion words of text from
various German online newspapers. The text is
parsed using MALTParser (Hall et al, 2006) and
the frequency of collocates with the ATTR (premod-
ifying adjective) and OBJA (accusative object) re-
lations is recorded. Vectors are weighted using the
conservative pointwise mutual information estimate
from Pantel and Lin (2002). For selecting most-
similar words in candidate selection, we use a ker-
nel based on the Jensen-Shannon Divergence across
both grammatical relations, similar to the method
proposed by O? Se?aghdha and Copestake (2008).
The resulting vector representations of words are
then clustered using average-link hierarchical ag-
glomerative clustering using the CLUTO toolkit
(Zhao and Karypis, 2005), which uses cosine sim-
ilarity to assess the similarity of two vectors. In the
study of Pantel and Lin (2002), agglomerative clus-
tering was among the best-performing off-the-shelf
clustering methods.
As we initially found that many features that were
used in clustering were less relevant to the differ-
ent senses of the head word that were targeted, we
also introduce a method to enforce a focus on tar-
get word compatible aspects of those words. In the
basic approach (raw), the normal vector representa-
tion of each word is used. In the modified approach
(intersect), only the features that are relevant for the
target word are selected, by using for each feature
the minimum value of (i) that feature?s value in the
candidate word?s vector and (ii) that feature?s value
in the target word?s vector.3.3 Competing and Upper Baselines
To see how well our method performs in relation to
other approaches for finding related terms describ-
38
ing each sense of a synset, two lower baselines and
one upper baseline have been implemented.
One lower baseline uses general distributionally
similar items. This is an intelligent (but realistic)general baseline method ? close in spirit to Pantel
and Lin (2002). It simply consists in retrieving the
distributionally most similar words for the clustering
task. Effectively, this resembles our own method,
but without the compound filtering step.
The second lower baseline assumes that it should
always be possible to find one word that is related to
one of the senses (yielding poor coverage but trivi-
alizing the clustering problem). This trivial baseline
is called one-cluster.
The upper baseline (called profile) assumes that
it knows which senses of the word should be mod-
eled and that errors can only be introduced by the
clustering step not reproducing the original sense.
This baseline retrieves the synsets corresponding to
each sense of the word from GermaNet, and, among
the terms in the neighbouring synsets (synonyms,
hypernyms, hyponyms as well as sibling synsets),
select those that are both unambiguous (i.e., do not
have other synsets corresponding to that word) and
are distributionally most similar to the (ambiguous)
target word.4 Evaluation Framework
Our evaluation framework is based on retrieving a
set of words related to the target item (the candidate
set), and then using collocate vectors extracted from
a corpus to cluster the candidate set into multiple
subsets.
Once we have a clustering of the generated terms,
we want a quantitative evaluation of the clustering.
The underlying idea for this is that we would like to
have, for each sense of the target word, a cluster that
has one or several words describing it. (We should
not assume that it is always possible to find many
related words for a particular sense).4.1 Evaluation Data
As target items, we used a list of simplexes that
are most productive in terms of compounding, us-
ing a set of gold-standard compound splits that were
created by Henrich and Hinrichs (2011); candidate
words (both compounds and general neighbours)
Figure 2: Evaluation procedure for the committees of re-
lated words
were selected using a frequency list extracted from
the Tu?PP-D/Z corpus (Mu?ller, 2004). For the ex-
periments themselves, no information about correct
splits of the compounds was assumed and potential
compounds were simply retrieved as lemma forms
that have the target word as a suffix.
The subsets from clustering the candidate set
are then evaluated according to whether the most-
central related words in that cluster are related to the
same sense of the target word, and how many senses
of the target word are covered by the clusters.4.2 Evaluation Metric
Given the committee lists that are output by the can-
didate selection and output, we calculate an evalua-
tion score by creating a mapping between senses of
the target word and the committees that are the out-
put of the clustering algorithm, choosing that map-
ping according to a quality measure that describes
how well the committee members match that synset
(the precision of that possible pairing between a
committee and a sense of the target word), as shown
in figure 2. Each candidate word is assigned a sense
of the target word, either because it is a hyponym of
that sense (for the compound-based method) or be-
cause its path distance in GermaNet?s taxonomy is
less than four (for the general terms method). If a
candidate word is not near any of the target word?s
sense synsets, it is assigned no sense (and always
39
candidates num vectors score quality coverage
compound 5 intersect 0.399 0.882 0.468
compound 30 intersect 0.489 0.721 0.702
compound 100 intersect 0.419 0.586 0.769
general 5 intersect 0.433 0.882 0.510
general 30 intersect 0.528 0.696 0.784
general 100 intersect 0.573 0.650 0.896
compound 5 raw 0.406 0.898 0.468
compound 30 raw 0.479 0.712 0.702
compound 100 raw 0.422 0.591 0.769
general 5 raw 0.441 0.902 0.510
general 30 raw 0.526 0.694 0.784
general 100 raw 0.551 0.630 0.896
profile 10n intersect 0.737 0.781 0.945
profile 10n raw 0.753 0.801 0.946
one-cluster 1 ? 0.325 1.000 0.325
Table 1: Evaluation scores for the different methods and
baselines
counted wrong).1
Given a committee C of these (at most) k most-
central candidate words in a cluster, we can calculate
a measure P (C, s) = |w2C:sense(w)=s||C| that describes
how well this cluster corresponds to a given sense.
(Ideally, the committee would contain words only
related to one sense).
Using the Kuhn-Munkres algorithm (Kuhn,
1955), we compute a mapping between each rep-
resented synset s and a cluster Cs such thatP
s P (Cs, s) is maximized. The final score for one
target word is this sum divided by the total number
of synsets for the target word ? this means that a
method that yields a less representative set of can-
didate words will normally not get a better score,
unless the clusters are of higher enough quality, than
one that has candidate terms for each cluster.
In addition to the score metric, we calculated a
quality metric that divides the raw sum by the num-
ber of senses covered in the candidate set, and a
coverage metric that corresponds to the fraction of
senses covered by the candidate set in the first place
(see Table 1).5 Results and Discussion
Table 1 contains quantitative results for the differ-
ent methods and also evaluation statistics for some
1If a candidate word is not represented in GermaNet at all,
it is discarded before the committee-building step, so that all
committee words are in fact GermaNet-represented terms.
lower and upper baselines: Selecting exactly one re-
lated word as a candidate (and putting it in a clus-
ter of its own) would yield a quality of 1.0, since
that cluster is related to exactly one synset, but a
very poor coverage of 0.325. For the profile up-
per baseline, which takes related terms from Ger-
maNet and uses imperfect information only in clus-
tering, we see that our clustering approach is able
to reconstruct committees of sense-identical terms
out of the candidate list fairly well: given related
terms for each sense, distributional similarity yields
fairly good quality (0.801) and, unsurprisingly, near-
perfect coverage for all senses (0.946).
For the actual methods using compounds of a
word (compound rows in Table 1) or distribution-
ally similar words (general rows), we find that the
compound-based candidate selection only reaches
very limited coverage numbers and furthermore
gives the best results with a smaller number of can-
didate words (30 for compounds versus 100 for gen-
eral). Whether this effect is due to minority senses
being less productive in compounding or whether
compounds of the minority senses are not repre-
sented in GermaNet is left to be investigated in fu-
ture work.6 Conclusion
We used compounds in the selection of candidate
words for representing a target word?s senses in a
word sense discrimination approach. Because com-
pounds are less-frequent overall than the similar-
frequency coordinate terms that are retrieved in the
general baseline approach, the proposed approach
does less well in covering all senses encoded in the
gold standard and gets lower results in our evalua-
tion metric. In contrast to previous work by Pantel
and Lin, our evaluation approach allows a principled
comparison between approaches to generate candi-
date lemmas in such a task and would be applicable
also to other alternative methods to do so.Acknowledgements We would like to thank Anne
Brock, as well as several anonymous reviewers, for
helpful comments of an earlier version of this pa-
per. The research in this paper was partially funded
by the Deutsche Forschungsgemeinschaft as part of
Collaborative Research Centre (SFB) 833.
40
References
Agirre, E., Aldezabal, I., and Pociello, E. (2006).
Lexicalization and multiword expression in the
Basque WordNet. In Proceedings of the first In-
ternational WordNet Conference.
Bentivogli, L. and Pianta, E. (2004). Extending
WordNet with syntagmatic information. In Pro-
ceedings of the Second Global WordNet Confer-
ence (GWC 2004).
Hall, J., Nivre, J., and Nilsson, J. (2006). Discrim-
inative classifiers for deterministic dependency
parsing. In Proceedings of the COLING/ACL
2006 Main Conference Poster Sessions.
Henrich, V. and Hinrichs, E. (2010). GernEdiT - the
GermaNet editing tool. In LREC 2010.
Henrich, V. and Hinrichs, E. (2011). Determin-
ing immediate constituents of compounds in Ger-
maNet. In Proc. International Conference Re-
cent Advances in Language Processing (RANLP
2011).
Kuhn, H. W. (1955). The hungarian method for the
assignment problem. Naval Research Logistics
Quarterly, 2:83?97.
Miller, G. A. and Fellbaum, C. (1991). Semantic
networks of English. Cognition, 41:197?229.
Mu?ller, F. H. (2004). Stylebook for the Tu?bingen
partially parsed corpus of written German (Tu?PP-
D/Z). Technischer Bericht, Seminar fu?r Sprach-
wissenschaft, Universita?t Tu?bingen.
O? Se?aghdha, D. and Copestake, A. (2008). Semantic
classification with distributional kernels. In Pro-
ceedings of the 22nd International Conference on
Computational Linguistics (COLING 2008).
Pantel, P. and Lin, D. (2002). Discovering word
senses from text. In Proceedings of ACM Confer-
ence on Knowledge Discovery and Data Mining
(KDD-02).
Schu?tze, H. (1998). Automatic word sense discrimi-
nation. Computational Linguistics, 24(1):97?123.
Versley, Y. and Panchenko, Y. (2012). Not just
bigger: Towards better-quality Web corpora. In
Proceedings of the 7th Web as Corpus Workshop
(WAC-7).
Yarowsky, D. (1993). One sense per collocation. In
HUMAN LANGUAGE TECHNOLOGY: Proceed-
ings of a Workshop Held at Plainsboro.
Zhao, Y. and Karypis, G. (2005). Hierarchical clus-
tering algorithms for document datasets. Data
Mining and Knowledge Discovery, 10:141?168.
41
Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically Rich Languages, pages 146?182,
Seattle, Washington, USA, 18 October 2013. c?2013 Association for Computational Linguistics
Overview of the SPMRL 2013 Shared Task:
Cross-Framework Evaluation of Parsing Morphologically Rich Languages?
Djam? Seddaha, Reut Tsarfatyb, Sandra K?blerc,
Marie Canditod, Jinho D. Choie, Rich?rd Farkasf , Jennifer Fosterg, Iakes Goenagah,
Koldo Gojenolai, Yoav Goldbergj , Spence Greenk, Nizar Habashl, Marco Kuhlmannm,
Wolfgang Maiern, Joakim Nivreo, Adam Przepi?rkowskip, Ryan Rothq, Wolfgang Seekerr,
Yannick Versleys, Veronika Vinczet, Marcin Wolin?skiu,
Alina Wr?blewskav, Eric Villemonte de la Cl?rgeriew
aU. Paris-Sorbonne/INRIA, bWeizman Institute, cIndiana U., dU. Paris-Diderot/INRIA, eIPsoft Inc., f,tU. of Szeged,
gDublin City U., h,iU. of the Basque Country, jBar Ilan U., kStanford U., l,qColumbia U., m,oUppsala U., nD?sseldorf U.,
p,u,vPolish Academy of Sciences, rStuttgart U., sHeidelberg U., wINRIA
Abstract
This paper reports on the first shared task on
statistical parsing of morphologically rich lan-
guages (MRLs). The task features data sets
from nine languages, each available both in
constituency and dependency annotation. We
report on the preparation of the data sets, on
the proposed parsing scenarios, and on the eval-
uation metrics for parsing MRLs given dif-
ferent representation types. We present and
analyze parsing results obtained by the task
participants, and then provide an analysis and
comparison of the parsers across languages and
frameworks, reported for gold input as well as
more realistic parsing scenarios.
1 Introduction
Syntactic parsing consists of automatically assigning
to a natural language sentence a representation of
its grammatical structure. Data-driven approaches
to this problem, both for constituency-based and
dependency-based parsing, have seen a surge of inter-
est in the last two decades. These data-driven parsing
approaches obtain state-of-the-art results on the de
facto standard Wall Street Journal data set (Marcus et
al., 1993) of English (Charniak, 2000; Collins, 2003;
Charniak and Johnson, 2005; McDonald et al, 2005;
McClosky et al, 2006; Petrov et al, 2006; Nivre et
al., 2007b; Carreras et al, 2008; Finkel et al, 2008;
?Contact authors: djame.seddah@paris-sorbonne.fr,
reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu
Huang, 2008; Huang et al, 2010; Zhang and Nivre,
2011; Bohnet and Nivre, 2012; Shindo et al, 2012),
and provide a foundation on which many tasks oper-
ating on semantic structure (e.g., recognizing textual
entailments) or even discourse structure (coreference,
summarization) crucially depend.
While progress on parsing English ? the main
language of focus for the ACL community ? has in-
spired some advances on other languages, it has not,
by itself, yielded high-quality parsing for other lan-
guages and domains. This holds in particular for mor-
phologically rich languages (MRLs), where impor-
tant information concerning the predicate-argument
structure of sentences is expressed through word for-
mation, rather than constituent-order patterns as is the
case in English and other configurational languages.
MRLs express information concerning the grammati-
cal function of a word and its grammatical relation to
other words at the word level, via phenomena such
as inflectional affixes, pronominal clitics, and so on
(Tsarfaty et al, 2012c).
The non-rigid tree structures and morphological
ambiguity of input words contribute to the challenges
of parsing MRLs. In addition, insufficient language
resources were shown to also contribute to parsing
difficulty (Tsarfaty et al, 2010; Tsarfaty et al, 2012c,
and references therein). These challenges have ini-
tially been addressed by native-speaking experts us-
ing strong in-domain knowledge of the linguistic
phenomena and annotation idiosyncrasies to improve
the accuracy and efficiency of parsing models. More
146
recently, advances in PCFG-LA parsing (Petrov et al,
2006) and language-agnostic data-driven dependency
parsing (McDonald et al, 2005; Nivre et al, 2007b)
have made it possible to reach high accuracy with
classical feature engineering techniques in addition
to, or instead of, language-specific knowledge. With
these recent advances, the time has come for estab-
lishing the state of the art, and assessing strengths
and weaknesses of parsers across different MRLs.
This paper reports on the first shared task on sta-
tistical parsing of morphologically rich languages
(the SPMRL Shared Task), organized in collabora-
tion with the 4th SPMRL meeting and co-located
with the conference on Empirical Methods in Natural
Language Processing (EMNLP). In defining and exe-
cuting this shared task, we pursue several goals. First,
we wish to provide standard training and test sets for
MRLs in different representation types and parsing
scenarios, so that researchers can exploit them for
testing existing parsers across different MRLs. Sec-
ond, we wish to standardize the evaluation protocol
and metrics on morphologically ambiguous input,
an under-studied challenge, which is also present in
English when parsing speech data or web-based non-
standard texts. Finally, we aim to raise the awareness
of the community to the challenges of parsing MRLs
and to provide a set of strong baseline results for
further improvement.
The task features data from nine, typologically di-
verse, languages. Unlike previous shared tasks on
parsing, we include data in both dependency-based
and constituency-based formats, and in addition to
the full data setup (complete training data), we pro-
vide a small setup (a training subset of 5,000 sen-
tences). We provide three parsing scenarios: one in
which gold segmentation, POS tags, and morphologi-
cal features are provided, one in which segmentation,
POS tags, and features are automatically predicted
by an external resource, and one in which we provide
a lattice of multiple possible morphological analyses
and allow for joint disambiguation of the morpholog-
ical analysis and syntactic structure. These scenarios
allow us to obtain the performance upper bound of
the systems in lab settings using gold input, as well
as the expected level of performance in realistic pars-
ing scenarios ? where the parser follows a morpho-
logical analyzer and is a part of a full-fledged NLP
pipeline.
The remainder of this paper is organized as follows.
We first survey previous work on parsing MRLs (?2)
and provide a detailed description of the present task,
parsing scenarios, and evaluation metrics (?3). We
then describe the data sets for the nine languages
(?4), present the different systems (?5), and empiri-
cal results (?6). Then, we compare the systems along
different axes (?7) in order to analyze their strengths
and weaknesses. Finally, we summarize and con-
clude with challenges to address in future shared
tasks (?8).
2 Background
2.1 A Brief History of the SPMRL Field
Statistical parsing saw initial success upon the avail-
ability of the Penn Treebank (PTB, Marcus et al,
1994). With that large set of syntactically annotated
sentences at their disposal, researchers could apply
advanced statistical modeling and machine learning
techniques in order to obtain high quality structure
prediction. The first statistical parsing models were
generative and based on treebank grammars (Char-
niak, 1997; Johnson, 1998; Klein and Manning, 2003;
Collins, 2003; Petrov et al, 2006; McClosky et al,
2006), leading to high phrase-structure accuracy.
Encouraged by the success of phrase-structure
parsers for English, treebank grammars for additional
languages have been developed, starting with Czech
(Hajic? et al, 2000) then with treebanks of Chinese
(Levy and Manning, 2003), Arabic (Maamouri et
al., 2004b), German (K?bler et al, 2006), French
(Abeill? et al, 2003), Hebrew (Sima?an et al, 2001),
Italian (Corazza et al, 2004), Spanish (Moreno et al,
2000), and more. It quickly became apparent that
applying the phrase-based treebank grammar tech-
niques is sensitive to language and annotation prop-
erties, and that these models are not easily portable
across languages and schemes. An exception to that
is the approach by Petrov (2009), who trained latent-
annotation treebank grammars and reported good
accuracy on a range of languages.
The CoNLL shared tasks on dependency parsing
(Buchholz and Marsi, 2006; Nivre et al, 2007a) high-
lighted the usefulness of an alternative linguistic for-
malism for the development of competitive parsing
models. Dependency relations are marked between
input tokens directly, and allow the annotation of
147
non-projective dependencies that are parseable effi-
ciently. Dependency syntax was applied to the de-
scription of different types of languages (Tesni?re,
1959; Mel?c?uk, 2001), which raised the hope that in
these settings, parsing MRLs will further improve.
However, the 2007 shared task organizers (Nivre
et al, 2007a) concluded that: "[Performance] classes
are more easily definable via language characteris-
tics than via characteristics of the data sets. The
split goes across training set size, original data for-
mat [...], sentence length, percentage of unknown
words, number of dependency labels, and ratio of
(C)POSTAGS and dependency labels. The class
with the highest top scores contains languages with
a rather impoverished morphology." The problems
with parsing MRLs have thus not been solved by de-
pendency parsing, but rather, the challenge has been
magnified.
The first event to focus on the particular challenges
of parsing MRLs was a dedicated panel discussion
co-located with IWPT 2009.1 Work presented on
Hebrew, Arabic, French, and German made it clear
that researchers working on non-English parsing face
the same overarching challenges: poor lexical cover-
age (due to high level of inflection), poor syntactic
coverage (due to more flexible word ordering), and,
more generally, issues of data sparseness (due to
the lack of large-scale resources). Additionally, new
questions emerged as to the evaluation of parsers in
such languages ? are the word-based metrics used
for English well-equipped to capture performance
across frameworks, or performance in the face of
morphological complexity? This event provoked ac-
tive discussions and led to the establishment of a
series of SPMRL events for the discussion of shared
challenges and cross-fertilization among researchers
working on parsing MRLs.
The body of work on MRLs that was accumulated
through the SPMRL workshops2 and hosting ACL
venues contains new results for Arabic (Attia et al,
2010; Marton et al, 2013a), Basque (Bengoetxea
and Gojenola, 2010), Croatian (Agic et al, 2013),
French (Seddah et al, 2010; Candito and Seddah,
2010; Sigogne et al, 2011), German (Rehbein, 2011),
Hebrew (Tsarfaty and Sima?an, 2010; Goldberg and
1http://alpage.inria.fr/iwpt09/panel.en.
html
2See http://www.spmrl.org/ and related workshops.
Elhadad, 2010a), Hindi (Ambati et al, 2010), Ko-
rean (Chung et al, 2010; Choi and Palmer, 2011) and
Spanish (Le Roux et al, 2012), Tamil (Green et al,
2012), amongst others. The awareness of the model-
ing challenges gave rise to new lines of work on top-
ics such as joint morpho-syntactic processing (Gold-
berg and Tsarfaty, 2008), Relational-Realizational
Parsing (Tsarfaty, 2010), EasyFirst Parsing (Gold-
berg, 2011), PLCFRS parsing (Kallmeyer and Maier,
2013), the use of factored lexica (Green et al, 2013),
the use of bilingual data (Fraser et al, 2013), and
more developments that are currently under way.
With new models and data, and with lingering in-
terest in parsing non-standard English data, questions
begin to emerge, such as: What is the realistic per-
formance of parsing MRLs using today?s methods?
How do the different models compare with one an-
other? How do different representation types deal
with parsing one particular language? Does the suc-
cess of a parsing model on a language correlate with
its representation type and learning method? How to
parse effectively in the face of resource scarcity? The
first step to answering all of these questions is pro-
viding standard sets of comparable size, streamlined
parsing scenarios, and evaluation metrics, which are
our main goals in this SPMRL shared task.
2.2 Where We Are At: The Need for
Cross-Framework, Realistic, Evaluation
Procedures
The present task serves as the first attempt to stan-
dardize the data sets, parsing scenarios, and evalu-
ation metrics for MRL parsing, for the purpose of
gaining insights into parsers? performance across lan-
guages. Ours is not the first cross-linguistic task on
statistical parsing. As mentioned earlier, two previ-
ous CoNLL shared tasks focused on cross-linguistic
dependency parsing and covered thirteen different
languages (Buchholz and Marsi, 2006; Nivre et al,
2007a). However, the settings of these tasks, e.g.,
in terms of data set sizes or parsing scenarios, made
it difficult to draw conclusions about strengths and
weaknesses of different systems on parsing MRLs.
A key aspect to consider is the relation between
input tokens and tree terminals. In the standard sta-
tistical parsing setup, every input token is assumed
to be a terminal node in the syntactic parse tree (after
deterministic tokenization of punctuation). In MRLs,
148
morphological processes may have conjoined several
words into a single token. Such tokens need to be seg-
mented and their analyses need to be disambiguated
in order to identify the nodes in the parse tree. In
previous shared tasks on statistical parsing, morpho-
logical information was assumed to be known in ad-
vance in order to make the setup comparable to that
of parsing English. In realistic scenarios, however,
morphological analyses are initially unknown and are
potentially highly ambiguous, so external resources
are used to predict them. Incorrect morphological
disambiguation sets a strict ceiling on the expected
performance of parsers in real-world scenarios. Re-
sults reported for MRLs using gold morphological
information are then, at best, optimistic.
One reason for adopting this less-than-realistic
evaluation scenario in previous tasks has been the
lack of sound metrics for the more realistic scenario.
Standard evaluation metrics assume that the number
of terminals in the parse hypothesis equals the num-
ber of terminals in the gold tree. When the predicted
morphological segmentation leads to a different num-
ber of terminals in the gold and parse trees, standard
metrics such as ParsEval (Black et al, 1991) or At-
tachment Scores (Buchholz and Marsi, 2006) fail
to produce a score. In this task, we use TedEval
(Tsarfaty et al, 2012b), a metric recently suggested
for joint morpho-syntactic evaluation, in which nor-
malized tree-edit distance (Bille, 2005) on morpho-
syntactic trees allows us to quantify the success on
the joint task in realistic parsing scenarios.
Finally, the previous tasks focused on dependency
parsing. When providing both constituency-based
and dependency-based tracks, it is interesting to com-
pare results across these frameworks so as to better
understand the differences in performance between
parsers of different types. We are now faced with
an additional question: how can we compare pars-
ing results across different frameworks? Adopting
standard metrics will not suffice as we would be com-
paring apples and oranges. In contrast, TedEval is
defined for both phrase structures and dependency
structures through the use of an intermediate repre-
sentation called function trees (Tsarfaty et al, 2011;
Tsarfaty et al, 2012a). Using TedEval thus allows us
to explore both dependency and constituency parsing
frameworks and meaningfully compare the perfor-
mance of parsers of different types.
3 Defining the Shared-Task
3.1 Input and Output
We define a parser as a structure prediction function
that maps sequences of space-delimited input tokens
(henceforth, tokens) in a language to a set of parse
trees that capture valid morpho-syntactic structures
in that language. In the case of constituency parsing,
the output structures are phrase-structure trees. In de-
pendency parsing, the output consists of dependency
trees. We use the term tree terminals to refer to the
leaves of a phrase-structure tree in the former case
and to the nodes of a dependency tree in the latter.
We assume that input sentences are represented
as sequences of tokens. In general, there may be a
many-to-many relation between input tokens and tree
terminals. Tokens may be identical to the terminals,
as is often the case in English. A token may be
mapped to multiple terminals assigned their own POS
tags (consider, e.g., the token ?isn?t?), as is the case
in some MRLs. Several tokens may be grouped into
a single (virtual) node, as is the case with multiword
expressions (MWEs) (consider ?pomme de terre? for
?potatoe?). This task covers all these cases.
In the standard setup, all tokens are tree terminals.
Here, the task of a parser is to predict a syntactic
analysis in which the tree terminals coincide with the
tokens. Disambiguating the morphological analyses
that are required for parsing corresponds to selecting
the correct POS tag and possibly a set of morpho-
logical features for each terminal. For the languages
Basque, French, German, Hungarian, Korean, Polish,
and Swedish, we assume this standard setup.
In the morphologically complex setup, every token
may be composed of multiple terminals. In this case,
the task of the parser is to predict the sequence of tree
terminals, their POS tags, and a correct tree associ-
ated with this sequence of terminals. Disambiguating
the morphological analysis therefore requires split-
ting the tokens into segments that define the terminals.
For the Semitic languages Arabic and Hebrew, we
assume this morphologically complex setup.
In the multiword expression (MWEs) setup, pro-
vided here for French only, groupings of terminals
are identified as MWEs (non-terminal nodes in con-
stituency trees, marked heads in dependency trees).
Here, the parser is required to predict how terminals
are grouped into MWEs on top of predicting the tree.
149
3.2 Data Sets
The task features nine languages from six language
families, from Germanic languages (Swedish and
German) and Romance (French) to Slavic (Polish),
Koreanic (Korean), Semitic (Arabic, Hebrew), Uralic
(Hungarian), and the language isolate Basque.
These languages cover a wide range of morpho-
logical richness, with Arabic, Basque, and Hebrew
exhibiting a high degree of inflectional and deriva-
tional morphology. The Germanic languages, Ger-
man and Swedish, have greater degrees of phrasal
ordering freedom than English. While French is not
standardly classified as an MRL, it shares MRLs char-
acteristics which pose challenges for parsing, such as
a richer inflectional system than English.
For each contributing language, we provide two
sets of annotated sentences: one annotated with la-
beled phrase-structure trees, and one annotated with
labeled dependency trees. The sentences in the two
representations are aligned at token and POS levels.
Both representations reflect the predicate-argument
structure of the same sentence, but this information
is expressed using different formal terms and thus
results in different tree structures.
Since some of our native data sets are larger than
others, we provide the training set in two sizes: Full
containing all sentences in the standard training set
of the language, and 5k containing the number of
sentences that is equivalent in size to our smallest
training set (5k sentences). For all languages, the data
has been split into sentences, and the sentences are
parsed and evaluated independently of one another.
3.3 Parsing Scenarios
In the shared task, we consider three parsing scenar-
ios, depending on how much of the morphological
information is provided. The scenarios are listed
below, in increasing order of difficulty.
? Gold: In this scenario, the parser is provided
with unambiguous gold morphological segmen-
tation, POS tags, and morphological features for
each input token.
? Predicted: In this scenario, the parser is pro-
vided with disambiguated morphological seg-
mentation. However, the POS tags and mor-
phological features for each input segment are
unknown.
Scenario Segmentation PoS+Feat. Tree
Gold X X ?
Predicted X 1-best ?
Raw (1-best) 1-best 1-best ?
Raw (all) ? ? ?
Table 1: A summary of the parsing and evaluation sce-
narios. X depicts gold information, ? depicts unknown
information, to be predicted by the system.
? Raw: In this scenario, the parser is provided
with morphologically ambiguous input. The
morphological segmentation, POS tags, and
morphological features for each input token are
unknown.
The Predicted and Raw scenarios require predict-
ing morphological analyses. This may be done using
a language-specific morphological analyzer, or it may
be done jointly with parsing. We provide inputs that
support these different scenarios:
? Predicted: Gold treebank segmentation is given
to the parser. The POS tags assignment and mor-
phological features are automatically predicted
by the parser or by an external resource.
? Raw (1-best): The 1st-best segmentation and
POS tags assignment is predicted by an external
resource and given to the parser.
? Raw (all): All possible segmentations and POS
tags are specified by an external resource. The
parser selects jointly a segmentation and a tree.
An overview of all shown in table 1. For languages
in which terminals equal tokens, only Gold and Pre-
dicted scenarios are considered. For Semitic lan-
guages we further provide input for both Raw (1-
best) and Raw (all) scenarios. 3
3.4 Evaluation Metrics
This task features nine languages, two different repre-
sentation types and three different evaluation scenar-
ios. In order to evaluate the quality of the predicted
structures in the different tracks, we use a combina-
tion of evaluation metrics that allow us to compare
the systems along different axes.
3The raw Arabic lattices were made available later than the
other data. They are now included in the shared task release.
150
In this section, we formally define the different
evaluation metrics and discuss how they support sys-
tem comparison. Throughout this paper, we will be
referring to different evaluation dimensions:
? Cross-Parser Evaluation in Gold/Predicted
Scenarios. Here, we evaluate the results of dif-
ferent parsers on a single data set in the Gold
or Predicted setting. We use standard evalu-
ation metrics for the different types of anal-
yses, that is, ParsEval (Black et al, 1991)
on phrase-structure trees, and Labeled At-
tachment Scores (LAS) (Buchholz and Marsi,
2006) for dependency trees. Since ParsEval is
known to be sensitive to the size and depth of
trees (Rehbein and van Genabith, 2007b), we
also provide the Leaf-Ancestor metric (Samp-
son and Babarczy, 2003), which is less sensitive
to the depth of the phrase-structure hierarchy. In
both scenarios we also provide metrics to evalu-
ate the prediction of MultiWord Expressions.
? Cross-Parser Evaluation in Raw Scenarios.
Here, we evaluate the results of different parsers
on a single data set in scenarios where morpho-
logical segmentation is not known in advance.
When a hypothesized segmentation is not iden-
tical to the gold segmentation, standard evalua-
tion metrics such as ParsEval and Attachment
Scores break down. Therefore, we use TedEval
(Tsarfaty et al, 2012b), which jointly assesses
the quality of the morphological and syntactic
analysis in morphologically-complex scenarios.
? Cross-Framework Evaluation. Here, we com-
pare the results obtained by a dependency parser
and a constituency parser on the same set of sen-
tences. In order to avoid comparing apples and
oranges, we use the unlabeled TedEval metric,
which converts all representation types inter-
nally into the same kind of structures, called
function trees. Here we use TedEval?s cross-
framework protocol (Tsarfaty et al, 2012a),
which accomodates annotation idiosyncrasies.
? Cross-Language Evaluation. Here, we com-
pare parsers for the same representation type
across different languages. Conducting a com-
plete and faithful evaluation across languages
would require a harmonized universal annota-
tion scheme (possibly along the lines of (de
Marneffe and Manning, 2008; McDonald et al,
2013; Tsarfaty, 2013)) or task based evaluation.
As an approximation we use unlabeled TedEval.
Since it is unlabeled, it is not sensitive to label
set size. Since it internally uses function-trees,
it is less sensitive to annotation idiosyncrasies
(e.g., head choice) (Tsarfaty et al, 2011).
The former two dimensions are evaluated on the full
sets. The latter two are evaluated on smaller, compa-
rable, test sets. For completeness, we provide below
the formal definitions and essential modifications of
the evaluation software that we used.
3.4.1 Evaluation Metrics for Phrase Structures
ParsEval The ParsEval metrics (Black et al, 1991)
are evaluation metrics for phrase-structure trees. De-
spite various shortcomings, they are the de-facto stan-
dard for system comparison on phrase-structure pars-
ing, used in many campaigns and shared tasks (e.g.,
(K?bler, 2008; Petrov and McDonald, 2012)). As-
sume that G and H are phrase-structure gold and
hypothesized trees respectively, each of which is rep-
resented by a set of tuples (i, A, j) where A is a
labeled constituent spanning from i to j. Assume
that g is the same as G except that it discards the
root, preterminal, and terminal nodes, likewise for h
and H . The ParsEval scores define the accuracy of
the hypothesis in terms of the normalized size of the
intersection of the constituent sets.
Precision(g, h) = |g?h||h|
Recall(g, h) = |g?h||g|
F1(g, h) = 2?P?RP+R
We evaluate accuracy on phrase-labels ignoring any
further decoration, as it is in standard practices.
Evalb, the standard software that implements Par-
sEval,4 takes a parameter file and ignores the labels
specified therein. As usual, we ignore root and POS
labels. Contrary to the standard practice, we do take
punctuation into account. Note that, as opposed to the
official version, we used the SANCL?2012 version5
modified to actually penalize non-parsed trees.
4http://www.spmrl.org/
spmrl2013-sharedtask-metrics.html/#Evalb
5Modified by Petrov and McDonald (2012) to be less sensi-
tive to punctuation errors.
151
Leaf-Ancestor The Leaf-Ancestor metric (Samp-
son and Babarczy, 2003) measures the similarity be-
tween the path from each terminal node to the root
node in the output tree and the corresponding path
in the gold tree. The path consists of a sequence of
node labels between the terminal node and the root
node, and the similarity of two paths is calculated
by using the Levenshtein distance. This distance is
normalized by path length, and the score of the tree
is an aggregated score of the values for all terminals
in the tree (xt is the leaf-ancestor path of t in tree x).
LA(h, g) =
?
t?yield(g) Lv(ht,gt)/(len(ht)+len(gt))
|yield(g)|
This metric was shown to be less sensitive to dif-
ferences between annotation schemes in (K?bler et
al., 2008), and was shown by Rehbein and van Gen-
abith (2007a) to evaluate trees more faithfully than
ParsEval in the face of certain annotation decisions.
We used the implementation of Wagner (2012).6
3.4.2 Evaluation Metrics for Dependency
Structures
Attachment Scores Labeled and Unlabeled At-
tachment scores have been proposed as evaluation
metrics for dependency parsing in the CoNLL shared
tasks (Buchholz and Marsi, 2006; Nivre et al, 2007a)
and have since assumed the role of standard metrics
in multiple shared tasks and independent studies. As-
sume that g, h are gold and hypothesized dependency
trees respectively, each of which is represented by
a set of arcs (i, A, j) where A is a labeled arc from
terminal i to terminal j. Recall that in the gold and
predicted settings, |g| = |h| (because the number of
terminals determines the number of arcs and hence it
is fixed). So Labeled Attachment Score equals preci-
sion and recall, and it is calculated as a normalized
size of the intersection between the sets of gold and
parsed arcs.7
Precision(g, h) = |g?h||g|
Recall(g, h) = |g?h||h|
LAS(g, h) = |g?h||g| =
|g?h|
|h|
6The original version is available at
http://www.grsampson.net/Resources.
html, ours at http://www.spmrl.org/
spmrl2013-sharedtask-metrics.html/#Leaf.
7http://ilk.uvt.nl/conll/software.html.
3.4.3 Evaluation Metrics for Morpho-Syntactic
Structures
TedEval The TedEval metrics and protocols have
been developed by Tsarfaty et al (2011), Tsarfaty
et al (2012a) and Tsarfaty et al (2012b) for coping
with non-trivial evaluation scenarios, e.g., comparing
parsing results across different frameworks, across
representation theories, and across different morpho-
logical segmentation hypotheses.8 Contrary to the
previous metrics, which view accuracy as a normal-
ized intersection over sets, TedEval computes the ac-
curacy of a parse tree based on the tree-edit distance
between complete trees. Assume a finite set of (pos-
sibly parameterized) edit operations A = {a1....an},
and a cost function c : A ? 1. An edit script is the
cost of a sequence of edit operations, and the edit dis-
tance of g, h is the minimal cost edit script that turns
g into h (and vice versa). The normalized distance
subtracted from 1 provides the level of accuracy on
the task. Formally, the TedEval score on g, h is de-
fined as follows, where ted is the tree-edit distance,
and the |x| (size in nodes) discards terminals and root
nodes.
TedEval(g, h) = 1?
ted(g, h)
|g|+ |h|
In the gold scenario, we are not allowed to manipu-
late terminal nodes, only non-terminals. In the raw
scenarios, we can add and delete both terminals and
non-terminals so as to match both the morphological
and syntactic hypotheses.
3.4.4 Evaluation Metrics for
Multiword-Expression Identification
As pointed out in section 3.1, the French data set is
provided with tree structures encoding both syntactic
information and groupings of terminals into MWEs.
A given MWE is defined as a continuous sequence of
terminals, plus a POS tag. In the constituency trees,
the POS tag of the MWE is an internal node of the
tree, dominating the sequence of pre-terminals, each
dominating a terminal. In the dependency trees, there
is no specific node for the MWE as such (the nodes
are the terminals). So, the first token of a MWE is
taken as the head of the other tokens of the same
MWE, with the same label (see section 4.4).
8http://www.tsarfaty.com/unipar/
download.html.
152
To evaluate performance on MWEs, we use the
following metrics.
? R_MWE, P_MWE, and F_MWE are recall, pre-
cision, and F-score over full MWEs, in which
a predicted MWE counts as correct if it has the
correct span (same group as in the gold data).
? R_MWE +POS, R_MWE +POS, and F_MWE
+POS are defined in the same fashion, except
that a predicted MWE counts as correct if it has
both correct span and correct POS tag.
? R_COMP, R_COMP, and F_COMP are recall,
precision and F-score over non-head compo-
nents of MWEs: a non-head component of MWE
counts as correct if it is attached to the head of
the MWE, with the specific label that indicates
that it is part of an MWE.
4 The SPMRL 2013 Data Sets
4.1 The Treebanks
We provide data from nine different languages anno-
tated with two representation types: phrase-structure
trees and dependency trees.9 Statistics about size,
average length, label set size, and other character-
istics of the treebanks and schemes are provided in
Table 2. Phrase structures are provided in an ex-
tended bracketed style, that is, Penn Treebank brack-
eted style where every labeled node may be extended
with morphological features expressed. Dependency
structures are provided in the CoNLL-X format.10
For any given language, the dependency and con-
stituency treebanks are aligned at the token and ter-
minal levels and share the same POS tagset and mor-
phological features. That is, any form in the CoNLL
format is a terminal of the respective bracketed tree.
Any CPOS label in the CoNLL format is the pre-
terminal dominating the terminal in the bracketed
tree. The FEATS in the CoNLL format are repre-
sented as dash-features decorated on the respective
pre-terminal node in the bracketed tree. See Fig-
ure 1(a)?1(b) for an illustration of this alignment.
9Additionally, we provided the data in TigerXML format
(Brants et al, 2002) for phrase structure trees containing cross-
ing branches. This allows the use of more powerful parsing
formalisms. Unfortunately, we received no submissions for this
data, hence we discard them in the rest of this overview.
10See http://ilk.uvt.nl/conll/.
For ambiguous morphological analyses, we pro-
vide the mapping of tokens to different segmentation
possibilities through lattice files. See Figure 1(c) for
an illustration, where lattice indices mark the start
and end positions of terminals.
For each of the treebanks, we provide a three-way
dev/train/set split and another train set containing the
first 5k sentences of train (5k). This section provides
the details of the original treebanks and their anno-
tations, our data-set preparation, including prepro-
cessing and data splits, cross-framework alignment,
and the prediction of morphological information in
non-gold scenarios.
4.2 The Arabic Treebanks
Arabic is a morphologically complex language which
has rich inflectional and derivational morphology. It
exhibits a high degree of morphological ambiguity
due to the absence of the diacritics and inconsistent
spelling of letters, such as Alif and Ya. As a conse-
quence, the Buckwalter Standard Arabic Morpholog-
ical Analyzer (Buckwalter, 2004; Graff et al, 2009)
produces an average of 12 analyses per word.
Data Sets The Arabic data set contains two tree-
banks derived from the LDC Penn Arabic Treebanks
(PATB) (Maamouri et al, 2004b):11 the Columbia
Arabic Treebank (CATiB) (Habash and Roth, 2009),
a dependency treebank, and the Stanford version
of the PATB (Green and Manning, 2010), a phrase-
structure treebank. We preprocessed the treebanks
to obtain strict token matching between the treebanks
and the morphological analyses. This required non-
trivial synchronization at the tree token level between
the PATB treebank, the CATiB treebank and the mor-
phologically predicted data, using the PATB source
tokens and CATiB feature word form as a dual syn-
chronized pivot.
The Columbia Arabic Treebank The Columbia
Arabic Treebank (CATiB) uses a dependency repre-
sentation that is based on traditional Arabic grammar
and that emphasizes syntactic case relations (Habash
and Roth, 2009; Habash et al, 2007). The CATiB
treebank uses the word tokenization of the PATB
11The LDC kindly provided their latest version of the Arabic
Treebanks. In particular, we used PATB 1 v4.1 (Maamouri et al,
2005), PATB 2 v3.1 (Maamouri et al, 2004a) and PATB 3 v3.3.
(Maamouri et al, 2009)
153
Arabic Basque French German Hebrew Hungarian Korean Polish Swedish
train:
#Sents 15,762 7,577 14,759 40,472 8,146 23,010 6,578
#Tokens 589,220 96,368 443,113 719,532 170,141 351,184 68,424
Lex. Size 36,906 25,136 27,470 77,222 40,782 11,1540 22,911
Avg. Length 37.38 12.71 30.02 17.77 20.88 15.26 10.40
Ratio #NT/#Tokens 0.19 0.82 0.34 0.60 0.59 0.60 0.94
Ratio #NT/#Sents 7.40 10.50 10.33 10.70 12.38 9.27 9.84
#Non Terminals 22 12 32 25 16 8 34
#POS tags 35 25 29 54 16 1,975 29
#total NTs 116,769 79,588 152,463 433,215 100,885 213,370 64,792
Dep. Label Set Size 9 31 25 43 417 22 27
train5k:
#Sents 5,000 5,000 5,000 5,000 5,000 5,000 5,000 5,000 5,000
#Tokens 224,907 61,905 150,984 87,841 128,046 109,987 68,336 52,123 76,357
Lex. Size 19,433 18,405 15,480 17,421 15,975 29,009 29,715 18,632 14,110
Avg. Length 44.98 12.38 30.19 17.56 25.60 21.99 13.66 10.42 15.27
Ratio #NT/#Tokens 0.15 0.83 0.34 0.60 0.42 0.57 0.68 0.94 0.58
Ratio #NT/#Sents 7.18 10.33 10.32 10.58 10.97 12.57 9.29 9.87 8.96
#Non Terminals 22 12 29 23 60 16 8 34 8
#POS Tags 35 25 29 51 50 16 972 29 25
#total NTs 35,909 5,1691 51,627 52,945 54,856 62,889 46,484 49,381 44,845
Dep. Label Set Size 9 31 25 42 43 349 20 27 61
dev:
#Sents 1,985 948 1,235 5,000 500 1,051 2,066 821 494
#Tokens 73,932 13,851 38,820 76,704 11,301 29,989 30,480 8,600 9,341
Lex. Size 12,342 5,551 6,695 15,852 3,175 10,673 15,826 4,467 2,690
Avg. Length 37.24 14.61 31.43 15.34 22.60 28.53 14.75 10.47 18.90
Ratio #NT/#Tokens 0.19 0.74 0.33 0.63 0.47 047 0.63 0.94 0.48
Ratio #NT/#Sents 7.28 10.92 10.48 9.71 10.67 13.66 9.33 9.90 9.10
#Non Terminals 21 11 27 24 55 16 8 31 8
#POS Tags 32 23 29 50 47 16 760 29 24
#total NTs 14,452 10,356 12,951 48,560 5,338 14,366 19,283 8,132 4,496
Dep. Label Set Size 9 31 25 41 42 210 22 26 59
test:
#Sents 1959 946 2541 5000 716 1009 2287 822 666
#Tokens 73878 11457 75216 92004 16998 19908 33766 8545 10690
Lex. Size 12254 4685 10048 20149 4305 7856 16475 4336 3112
Avg. Length 37.71 12.11 29.60 18.40 23.74 19.73 14.76 10.39 16.05
Ratio #NT/#Tokens 0.19 0.83 0.34 0.60 0.47 0.62 0.61 0.95 0.57
Ratio #NT/#Sents 7.45 10.08 10.09 11.07 11.17 12.26 9.02 9.94 9.18
#Non Terminals 22 12 30 23 54 15 8 31 8
#POS Tags 33 22 30 52 46 16 809 27 25
#total NTs 14,610 9,537 25,657 55,398 8,001 12,377 20,640 8,175 6,118
Dep. Label Set Size 9 31 26 42 41 183 22 27 56
Table 2: Overview of participating languages and treebank properties. ?Sents? = number of sentences, ?Tokens? =
number of raw surface forms. ?Lex. size? and ?Avg. Length? are computed in terms of tagged terminals. ?NT? = non-
terminals in constituency treebanks, ?Dep Labels? = dependency labels on the arcs of dependency treebanks. ? A more
comprehensive table is available at http://www.spmrl.org/spmrl2013-sharedtask.html/#Prop.
154
(a) Constituency Tree
% % every line is a single tree in a bracketed Penn Treebank format
(ROOT (S (NP ( NNP-#pers=3|num=sing# John))(VP ( VB-#pers=3|num=sing# likes)(NP ( NNP-#pers=3|num=sing# Mary)))))
(b) Dependency Tree
%% every line describes a terminal: terminal-id form lemma CPOS FPOS FEATS Head Rel PHead PRel
1 John John NNP NNP pers=3|num=sing 2 sbj _ _
2 likes like VB VB pers=3|num=sing 0 root _ _
3 Mary Mary NNP NNP pers=3|num=sing 2 obj _ _
Input Lattice
0 1 2 3 4 5 6
1:AIF/NN
1:AIF/VB
1:AIF/NNT
2:LA/RB
3:NISH/VB
3:NISH/NN
4:L/PREP
4:LHSTIR/VB
4:HSTIR/VB
5:ZAT/PRP
%% every line describes a terminal: start-id end-id form lemma CPOS FPOS FEATS token-id
0 1 AIF AIF NN NN _ 1
0 1 AIF AIF NNT NNT _ 1
0 1 AIF AIF VB VB _ 1
1 2 LA LA RB RB _ 2
2 3 NISH NISH VB VB _ 3
2 3 NISH NISH NN NN _ 3
3 5 LHSTIR HSTIR VB VB _ 4
3 4 L L PREP PREP _ 4
4 5 HSTIR HSTIR VB VB _ 4
5 6 ZAT ZAT PRP PRP _ 5
Figure 1: File formats. Trees (a) and (b) are aligned constituency and dependency trees for a mockup English example.
Boxed labels are shared across the treebanks. Figure (c) shows an ambiguous lattice. The red part represents the yield
of the gold tree. For brevity, we use empty feature columns, but of course lattice arcs may carry any morphological
features, in the FEATS CoNLL format.
and employs a reduced POS tagset consisting of six
tags only: NOM (non-proper nominals including
nouns, pronouns, adjectives and adverbs), PROP
(proper nouns), VRB (active-voice verbs), VRB-
PASS (passive-voice verbs), PRT (particles such as
prepositions or conjunctions) and PNX (punctuation).
(This stands in extreme contrast with the Buckwalter
Arabic tagset (PATB official tagset) which is almost
500 tags.) To obtain these dependency trees, we used
the constituent-to-dependency tool (Habash and Roth,
2009). Additional CATiB trees were annotated di-
rectly, but we only use the portions that are converted
from phrase-structure representation, to ensure that
the constituent and dependency yields can be aligned.
The Stanford Arabic Phrase Structure Treebank
In order to stay compatible with the state of the art,
we provide the constituency data set with most of the
pre-processing steps of Green and Manning (2010),
as they were shown to improve baseline performance
on the PATB parsing considerably.12
To convert the original PATB to preprocessed
phrase-structure trees ? la Stanford, we first discard
all trees dominated by X, which indicates errors and
non-linguistic text. At the phrasal level, we collapse
unary chains with identical categories like NP? NP.
We finally remove all traces, but, unlike Green and
Manning (2010), we keep all function tags.
In the original Stanford instance, the pre-terminal
morphological analyses were mapped to the short-
ened Bies tag set provided with the treebank (where
Determiner markers, ?DT?, were added to definite
noun and adjectives, resulting in 32 POS tags). Here
we use the Kulick tagset (Kulick et al, 2006) for
12Both the corpus split and pre-processing code are available
with the Stanford parser at http://nlp.stanford.edu/
projects/arabic.shtml.
155
pre-terminal categories in the phrase-structure trees,
where the Bies tag set is included as a morphological
feature (stanpos) in our PATB instance.
Adapting the Data to the Shared Task We con-
verted the CATiB representation to the CoNLL rep-
resentation and added a ?split-from-previous? and
?split-from-next? markers as in LDC?s tree-terminal
fields.
A major difference between the CATiB treebank
and the Stanford treebank lies in the way they han-
dle paragraph annotations. The original PATB con-
tains sequences of annotated trees that belong to a
same discourse unit (e.g., paragraph). While the
CATiB conversion tool considers each sequence a
single parsing unit, the Stanford pre-processor treats
each such tree structure rooted at S, NP or Frag as
a tree spanning a single sentence. To be compati-
ble with the predicted morphology data which was
bootstrapped and trained on the CATiB interpretation,
we deterministically modified the original PATB by
adding pseudo XP root nodes, so that the Stanford
pre-proprecessor will generate the same tree yields
as the CATiB treebank.
Another important aspect of preprocessing (often-
delegated as a technicality in the Arabic parsing lit-
erature) is the normalization of token forms. Most
Arabic parsing work used transliterated text based on
the schemes proposed by Buckwalter (2002). The
transliteration schemes exhibit some small differ-
ences, but enough to increase the out-of-vocabulary
rate by a significant margin (on top of strictly un-
known morphemes). This phenomenon is evident in
the morphological analysis lattices (in the predicted
dev set there is a 6% OOV rate without normalization,
and half a point reduction after normalization is ap-
plied, see (Habash et al, 2009b; Green and Manning,
2010)). This rate is much lower for gold tokenized
predicted data (with an OOV rate of only 3.66%,
similar to French for example). In our data set, all
tokens are minimally normalized: no diacritics, no
normalization.13
Data Splits For the Arabic treebanks, we use the
data split recommended by the Columbia Arabic and
Dialect Modeling (CADiM) group (Diab et al, 2013).
13Except for the minimal normalization present in MADA?s
back-end tools. This script was provided to the participants.
The data of the LDC first three annotated Arabic Tree-
banks (ATB1, ATB2 and ATB3) were divided into
roughly a 10/80/10% dev/train/test split by word vol-
ume. When dividing the corpora, document bound-
aries were maintained. The train5k files are simply
the first 5,000 sentences of the training files.
POS Tagsets Given the richness of Arabic mor-
phology, there are multiple POS tag sets and tokeniza-
tion schemes that have been used by researchers, (see,
e.g., Marton et al (2013a)). In the shared task, we fol-
low the standard PATB tokenization which splits off
several categories of orthographic clitics, but not the
definite article Al+. On top of that, we consider three
different POS tag sets with different degrees of gran-
ularity: the Buckwalter tag set (Buckwalter, 2004),
the Kulick Reduced Tag set (Kulick et al, 2006), and
the CATiB tag set (Habash et al, 2009a), considering
that granularity of the morphological analyses may
affect syntactic processing. For more information see
Habash (2010).
Predicted Morphology To prepare input for the
Raw scenarios (?3.3), we used the MADA+TOKAN
system (Habash et al, 2009b). MADA is a system
for morphological analysis and disambiguation of
Arabic. It can predict the 1-best tokenization, POS
tags, lemmas and diacritization in one fell swoop.
The MADA output was also used to generate the
lattice files for the Raw-all scenario.
To generate input for the gold token / predicted
tag input scenario, we used Morfette (Chrupa?a et al,
2008), a joint lemmatization and POS tagging model
based on an averaged perceptron. We generated two
tagging models, one trained with the Buckwalter tag
set, and the other with the Kulick tag set. Both were
mapped back to the CATiB POS tag set such that all
predicted tags are contained in the feature field.14
4.3 The Basque Treebank
Basque is an agglutinative language with a high ca-
pacity to generate inflected wordforms, with free
constituent order of sentence elements with respect
to the main verb. Contrary to many other treebanks,
the Basque treebank was originally annotated with
dependency trees, which were later on converted to
constituency trees.
14A conversion script from the rich Buckwalter tagset to
CoNLL-like features was provided to the participants.
156
The Basque Dependency Treebank (BDT) is a
dependency treebank in its original design, due to
syntactic characteristics of Basque such as its free
word order. Before the syntactic annotation, mor-
phological analysis was performed, using the Basque
morphological analyzer of Aduriz et al (2000). In
Basque each lemma can generate thousands of word-
forms ? differing in morphological properties such
as case, number, tense, or different types of subordi-
nation for verbs. If only POS category ambiguity is
resolved, the analyses remain highly ambiguous.
For the main POS category, there is an average of
1.55 interpretations per wordform, which rises to 2.65
for the full morpho-syntactic information, resulting
in an overall 64% of ambiguous wordforms. The
correct analysis was then manually chosen.
The syntactic trees were manually assigned. Each
word contains its lemma, main POS category, POS
subcategory, morphological features, and the la-
beled dependency relation. Each form indicates mor-
phosyntactic features such as case, number and type
of subordination, which are relevant for parsing.
The first version of the Basque Dependency Tree-
bank, consisting of 3,700 sentences (Aduriz et al,
2003), was used in the CoNLL 2007 Shared Task on
Dependency Parsing (Nivre et al, 2007a). The cur-
rent shared task uses the second version of the BDT,
which is the result of an extension and redesign of the
original requirements, containing 11,225 sentences
(150,000 tokens).
The Basque Constituency Treebank (BCT) was
created as part of the CESS-ECE project, where the
main aim was to obtain syntactically annotated con-
stituency treebanks for Catalan, Spanish and Basque
using a common set of syntactic categories. BCT
was semi-automatically derived from the dependency
version (Aldezabal et al, 2008). The conversion pro-
duced complete constituency trees for 80% of the
sentences. The main bottlenecks have been sentence
connectors and non-projective dependencies which
could not be straightforwardly converted into projec-
tive tree structures, requiring a mechanism similar to
traces in the Penn English Treebank.
Adapting the Data to the Shared Task As the
BCT did not contain all of the original non-projective
dependency trees, we selected the set of 8,000 match-
ing sentences in both treebanks for the shared task.15
This implies that around 2k trees could not be gen-
erated and therefore were discarded. Furthermore,
the BCT annotation scheme does not contain attach-
ment for most of the punctuation marks, so those
were inserted into the BCT using a simple lower-left
attachment heuristic. The same goes for some con-
nectors that could not be aligned in the first phase.
Predicted Morphology In order to obtain pre-
dicted tags for the non-gold scenarios, we used the
following pipeline. First, morphological analysis as
described above was performed, followed by a dis-
ambiguation step. At that point, it is hard to obtain a
single interpretation for each wordform, as determin-
ing the correct interpretation for each wordform may
require knowledge of long-distance elements on top
of the free constituency order of the main phrasal el-
ements in Basque. The disambiguation is performed
by the module by Ezeiza et al (1998), which uses
a combination of knowledge-based disambiguation,
by means of Constraint Grammar (Karlsson et al,
1995; Aduriz et al, 1997), and a posterior statistical
disambiguation module, using an HMM.16
For the shared task data, we chose a setting that
disambiguates most word forms, and retains ? 97%
of the correct interpretations, leaving an ambiguity
level of 1.3 interpretations. For the remaining cases
of ambiguity, we chose the first interpretation, which
corresponds to the most frequent option. This leaves
open the investigation of more complex approaches
for selecting the most appropriate reading.17
4.4 The French Treebank
French is not a morphologically rich language per se,
though its inflectional system is richer than that of
English, and it also exhibits a limited amount of word
order variation occurring at different syntactic levels
including the word level (e.g. pre- or post-nominal
15We generated a 80/10/10 split, ? train/dev/test ? The first 5k
sentences of the train set were used as a basis for the train5k.
16Note that the statistical module can be parametrized accord-
ing to the level of disambiguation to trade off precision and
recall. For example, disambiguation based on the main cate-
gories (abstracting over morpho-syntactic features) maintains
most of the correct interpretations but still gives an output with
several interpretations per wordform.
17This is not an easy task. The ambiguity left is the hardest to
solve given that the knowledge-based and statistical disambigua-
tion processes have not been able to pick out a single reading.
157
adjective, pre- or post-verbal adverbs) and the phrase
level (e.g. possible alternations between post verbal
NPs and PPs). It also has a high degree of multi-
word expressions, that are often ambiguous with a
literal reading as a sequence of simple words. The
syntactic and MWE analysis shows the same kind of
interaction (though to a lesser extent) as morphologi-
cal and syntactic interaction in Semitic languages ?
MWEs help parsing, and syntactic information may
be required to disambiguate MWE identification.
The Data Set The French data sets were gener-
ated from the French Treebank (Abeill? et al, 2003),
which consists of sentences from the newspaper Le
Monde, manually annotated with phrase structures
and morphological information. Part of the treebank
trees are also annotated with grammatical function
tags for dependents of verbs. In the SPMRL shared
task release, we used only this part, consisting of
18,535 sentences,18 split into 14,759 sentences for
training, 1,235 sentences for development, and 2,541
sentences for the final evaluation.19
Adapting the Data to the Shared Task The con-
stituency trees are provided in an extended PTB
bracketed format, with morphological features at the
pre-terminal level only. They contain slight, auto-
matically performed, modifications with respect to
the original trees of the French treebank. The syntag-
matic projection of prepositions and complementiz-
ers was normalized, in order to have prepositions and
complementizers as heads in the dependency trees
(Candito et al, 2010).
The dependency representations are projective de-
pendency trees, obtained through automatic conver-
sion from the constituency trees. The conversion pro-
cedure is an enhanced version of the one described
by Candito et al (2010).
Both the constituency and the dependency repre-
sentations make use of coarse- and fine-grained POS
tags (CPOS and FPOS respectively). The CPOS are
the categories from the original treebank. The FPOS
18The process of functional annotation is still ongoing, the
objective of the FTB providers being to have all the 20000 sen-
tences annotated with functional tags.
19The first 9,981 training sentences correspond to the canoni-
cal 2007 training set. The development set is the same and the
last 1235 sentences of the test set are those of the canonical test
set.
are merged using the CPOS and specific morphologi-
cal information such as verbal mood, proper/common
noun distinction (Crabb? and Candito, 2008).
Multi-Word Expressions The main difference
with respect to previous releases of the bracketed
or dependency versions of the French treebank
lies in the representation of multi-word expressions
(MWEs). The MWEs appear in an extended format:
each MWE bears an FPOS20 and consists of a se-
quence of terminals (hereafter the ?components? of
the MWE), each having their proper CPOS, FPOS,
lemma and morphological features. Note though that
in the original treebank the only gold information
provided for a MWE component is its CPOS. Since
leaving this information blank for MWE components
would have provided a strong cue for MWE recog-
nition, we made sure to provide the same kind of
information for every terminal, whether MWE com-
ponent or not, by providing predicted morphological
features, lemma, and FPOS for MWE components
(even in the ?gold? section of the data set). This infor-
mation was predicted by the Morfette tool (Chrupa?a
et al, 2008), adapted to French (Seddah et al, 2010).
In the constituency trees, each MWE corresponds
to an internal node whose label is the MWE?s FPOS
suffixed by a +, and which dominates the component
pre-terminal nodes.
In the dependency trees, there is no ?node? for a
MWE as a whole, but one node (a terminal in the
CoNLL format) per MWE component. The first com-
ponent of a MWE is taken as the head of the MWE.
All subsequent components of the MWE depend on
the first one, with the special label dep_cpd. Further-
more, the first MWE component bears a feature mwe-
head equal to the FPOS of the MWE. For instance,
the MWE la veille (the day before) is an adverb, con-
taining a determiner component and a common noun
component. Its bracketed representation is (ADV+
(DET la) (NC veille)), and in the dependency repre-
sentation, the noun veille depends on the determiner
la, which bears the feature mwehead=ADV+.
Predicted Morphology For the predicted mor-
phology scenario, we provide data in which the
mwehead has been removed and with predicted
20In the current data, we did not carry along the lemma and
morphological features pertaining to the MWE itself, though this
information is present in the original trees.
158
FPOS, CPOS, lemma, and morphological features,
obtained by training Morfette on the whole train set.
4.5 The German Treebank
German is a fusional language with moderately free
word order, in which verbal elements are fixed in
place and non-verbal elements can be ordered freely
as long as they fulfill the ordering requirements of
the clause (H?hle, 1986).
The Data Set The German constituency data set
is based on the TiGer treebank release 2.2.21 The
original annotation scheme represents discontinuous
constituents such that all arguments of a predicate
are always grouped under a single node regardless of
whether there is intervening material between them
or not (Brants et al, 2002). Furthermore, punctua-
tion and several other elements, such as parentheses,
are not attached to the tree. In order to make the
constituency treebank usable for PCFG parsing, we
adapted this treebank as described shortly.
The conversion of TiGer into dependencies is a
variant of the one by Seeker and Kuhn (2012), which
does not contain empty nodes. It is based on the same
TiGer release as the one used for the constituency
data. Punctuation was attached as high as possible,
without creating any new non-projective edges.
Adapting the Data to the Shared Task For
the constituency version, punctuation and other
unattached elements were first attached to the tree.
As attachment target, we used roughly the respec-
tive least common ancestor node of the right and
left terminal neighbor of the unattached element (see
Maier et al (2012) for details), and subsequently, the
crossing branches were resolved.
This was done in three steps. In the first step, the
head daughters of all nodes were marked using a
simple heuristic. In case there was a daughter with
the edge label HD, this daughter was marked, i.e.,
existing head markings were honored. Otherwise, if
existing, the rightmost daughter with edge label NK
(noun kernel) was marked. Otherwise, as default, the
leftmost daughter was marked. In a second step, for
each continuous part of a discontinuous constituent,
a separate node was introduced. This corresponds
21This version is available from http://www.ims.
uni-stuttgart.de/forschung/ressourcen/
korpora/tiger.html
to the "raising" algorithm described by Boyd (2007).
In a third steps, all those newly introduced nodes
that did not cover the head daughter of the original
discontinuous node were deleted. For the second
and the third step, we used the same script as for the
Swedish constituency data.
Predicted Morphology For the predicted scenario,
a single sequence of POS tags and morphologi-
cal features has been assigned using the MATE
toolchain via a model trained on the train set via cross-
validation on the training set. The MATE toolchain
was used to provide predicted annotation for lem-
mas, POS tags, morphology, and syntax. In order to
achieve the best results for each annotation level, a
10-fold jackknifing was performed to provide realis-
tic features for the higher annotation levels. The pre-
dicted annotation of the 5k training set were copied
from the full data set.22
4.6 The Hebrew Treebank
Modern Hebrew is a Semitic language, characterized
by inflectional and derivational (templatic) morphol-
ogy and relatively free word order. The function
words for from/to/like/and/when/that/the are prefixed
to the next token, causing severe segmentation ambi-
guity for many tokens. In addition, Hebrew orthogra-
phy does not indicate vowels in modern texts, leading
to a very high level of word-form ambiguity.
The Data Set Both the constituency and the de-
pendency data sets are derived from the Hebrew
Treebank V2 (Sima?an et al, 2001; Guthmann et
al., 2009). The treebank is based on just over 6000
sentences from the daily newspaper ?Ha?aretz?, man-
ually annotated with morphological information and
phrase-structure trees and extended with head infor-
mation as described in Tsarfaty (2010, ch. 5). The
unlabeled dependency version was produced by con-
version from the constituency treebank as described
in Goldberg (2011). Both the constituency and depen-
dency trees were annotated with a set grammatical
function labels conforming to Unified Stanford De-
pendencies by Tsarfaty (2013).
22We also provided a predicted-all scenario, in which we
provided morphological analysis lattices with POS and mor-
phological information derived from the analyses of the SMOR
derivational morphology (Schmid et al, 2004). These lattices
were not used by any of the participants.
159
Adapting the Data to the Shared Task While
based on the same trees, the dependency and con-
stituency treebanks differ in their POS tag sets, as
well as in some of the morphological segmentation
decisions. The main effort towards the shared task
was unifying the two resources such that the two tree-
banks share the same lexical yields, and the same
pre-terminal labels. To this end, we took the layering
approach of Goldberg et al (2009), and included two
levels of POS tags in the constituency trees. The
lower level is lexical, conforming to the lexical re-
source used to build the lattices, and is shared by
the two treebanks. The higher level is syntactic, and
follows the tag set and annotation decisions of the
original constituency treebank.23 In addition, we uni-
fied the representation of morphological features, and
fixed inconsistencies and mistakes in the treebanks.
Data Split The Hebrew treebank is one of the
smallest in our language set, and hence it is provided
in only the small (5k) setting. For the sake of com-
parability with the 5k set of the other treebanks, we
created a comparable size of dev/test sets containing
the first and last 500 sentences respectively, where
the rest serve as the 5k training.24
Predicted Morphology The lattices encoding the
morphological ambiguity for the Raw (all) scenario
were produced by looking up the possible analyses
of each input token in the wide-coverage morpholog-
ical analyzer (lexicon) of the Knowledge Center for
Processing Hebrew (Itai and Wintner, 2008; MILA,
2008), with a simple heuristic for dealing with un-
known tokens. A small lattice encoding the possible
analyses of each token was produced separately, and
these token-lattices were concatenated to produce the
sentence lattice. The lattice for a given sentence may
not include the gold analysis in cases of incomplete
lexicon coverage.
The morphologically disambiguated input files for
the Raw (1-best) scenario were produced by run-
ning the raw text through the morphological disam-
23Note that this additional layer in the constituency treebank
adds a relatively easy set of nodes to the trees, thus ?inflating?
the evaluation scores compared to previously reported results.
To compensate, a stricter protocol than is used in this task would
strip one of the two POS layers prior to evaluation.
24This split is slightly different than the split in previous stud-
ies.
biguator (tagger) described in Adler and Elhadad
(2006; Goldberg et al (2008),Adler (2007). The
disambiguator is based on the same lexicon that is
used to produce the lattice files, but utilizes an extra
module for dealing with unknown tokens Adler et al
(2008). The core of the disambiguator is an HMM
tagger trained on about 70M unannotated tokens us-
ing EM, and being supervised by the lexicon.
As in the case of Arabic, we also provided data
for the Predicted (gold token / predicted morphol-
ogy) scenario. We used the same sequence labeler,
Morfette (Chrupa?a et al, 2008), trained on the con-
catenation of POS and morphological gold features,
leading to a model with respectable accuracy.25
4.7 The Hungarian Treebank
Hungarian is an agglutinative language, thus a lemma
can have hundreds of word forms due to derivational
or inflectional affixation (nominal declination and
verbal conjugation). Grammatical information is typ-
ically indicated by suffixes: case suffixes mark the
syntactic relationship between the head and its argu-
ments (subject, object, dative, etc.) whereas verbs
are inflected for tense, mood, person, number, and
the definiteness of the object. Hungarian is also char-
acterized by vowel harmony.26 In addition, there are
several other linguistic phenomena such as causa-
tion and modality that are syntactically expressed in
English but encoded morphologically in Hungarian.
The Data Set The Hungarian data set used in
the shared task is based on the Szeged Treebank,
the largest morpho-syntactic and syntactic corpus
manually annotated for Hungarian. This treebank
is based on newspaper texts and is available in
both constituent-based (Csendes et al, 2005) and
dependency-based (Vincze et al, 2010) versions.
Around 10k sentences of news domain texts were
made available to the shared task.27 Each word is
manually assigned all its possible morpho-syntactic
25POS+morphology prediction accuracy is 91.95% overall
(59.54% for unseen tokens). POS only prediction accuracy is
93.20% overall (71.38% for unseen tokens).
26When vowel harmony applies, most suffixes exist in two
versions ? one with a front vowel and another one with a back
vowel ? and it is the vowels within the stem that determine which
form of the suffix is selected.
27The original treebank contains 82,000 sentences, 1.2 million
words and 250,000 punctuation marks from six domains.
160
tags and lemmas and the appropriate one is selected
according to the context. Sentences were manu-
ally assigned a constituency-based syntactic struc-
ture, which includes information on phrase structure,
grammatical functions (such as subject, object, etc.),
and subcategorization information (i.e., a given NP
is subcategorized by a verb or an infinitive). The
constituency trees were later automatically converted
into dependency structures, and all sentences were
then manually corrected. Note that there exist some
differences in the grammatical functions applied to
the constituency and dependency versions of the tree-
bank, since some morpho-syntactic information was
coded both as a morphological feature and as dec-
oration on top of the grammatical function in the
constituency trees.
Adapting the Data to the Shared Task Origi-
nally, the Szeged Dependency Treebank contained
virtual nodes for elided material (ELL) and phonolog-
ically covert copulas (VAN). In the current version,
they have been deleted, their daughters have been
attached to the parent of the virtual node, and have
been given complex labels, e.g. COORD-VAN-SUBJ,
where VAN is the type of the virtual node deleted,
COORD is the label of the virtual node and SUBJ is
the label of the daughter itself. When the virtual node
was originally the root of the sentence, its daughter
with a predicative (PRED) label has been selected as
the new root of the sentence (with the label ROOT-
VAN-PRED) and all the other daughters of the deleted
virtual node have been attached to it.
Predicted Morphology In order to provide the
same POS tag set for the constituent and dependency
treebanks, we used the dependency POS tagset for
both treebank instances. Both versions of the tree-
bank are available with gold standard and automatic
morphological annotation. The automatic POS tag-
ging was carried out by a 10-fold cross-validation
on the shared task data set by magyarlanc, a natu-
ral language toolkit for processing Hungarian texts
(segmentation, morphological analysis, POS tagging,
and dependency parsing). The annotation provides
POS tags and deep morphological features for each
input token (Zsibrita et al, 2013).28
28The full data sets of both the constituency and de-
pendency versions of the Szeged Treebank are available at
4.8 The Korean Treebank
The Treebank The Korean corpus is generated by
collecting constituent trees from the KAIST Tree-
bank (Choi et al, 1994), then converting the con-
stituent trees to dependency trees using head-finding
rules and heuristics. The KAIST Treebank consists
of about 31K manually annotated constituent trees
from 97 different sources (e.g., newspapers, novels,
textbooks). After filtering out trees containing an-
notation errors, a total of 27,363 trees with 350,090
tokens are collected.
The constituent trees in the KAIST Treebank29 also
come with manually inspected morphological analy-
sis based on ?eojeol?. An eojeol contains root-forms
of word tokens agglutinated with grammatical affixes
(e.g., case particles, ending markers). An eojeol can
consist of more than one word token; for instance, a
compound noun ?bus stop? is often represented as
one eojeol in Korean, ????????????, which can be
broken into two word tokens,???? (bus) and????????
(stop). Each eojeol in the KAIST Treebank is sepa-
rated by white spaces regardless of punctuation. Fol-
lowing the Penn Korean Treebank guidelines (Han
et al, 2002), punctuation is separated as individual
tokens, and parenthetical notations surrounded by
round brackets are grouped into individual phrases
with a function tag (PRN in our corpus).
All dependency trees are automatically converted
from the constituent trees. Unlike English, which
requires complicated head-finding rules to find the
head of each phrase (Choi and Palmer, 2012), Ko-
rean is a head final language such that the rightmost
constituent in each phrase becomes the head of that
phrase. Moreover, the rightmost conjunct becomes
the head of all other conjuncts and conjunctions in
a coordination phrase, which aligns well with our
head-final strategy.
The constituent trees in the KAIST Treebank do
not consist of function tags indicating syntactic or
semantic roles, which makes it difficult to generate
dependency labels. However, it is possible to gener-
ate meaningful labels by using the rich morphology
in Korean. For instance, case particles give good
the following website: www.inf.u-szeged.hu/rgai/
SzegedTreebank, and magyarlanc is downloadable from:
www.inf.u-szeged.hu/rgai/magyarlanc.
29See Lee et al (1997) for more details about the bracketing
guidelines of the KAIST Treebank.
161
indications of what syntactic roles eojeols with such
particles should take. Given this information, 21
dependency labels were generated according to the
annotation scheme proposed by Choi (2013).
Adapting the Data to the Shared Task All details
concerning the adaptation of the KAIST treebank
to the shared task specifications are found in Choi
(2013). Importantly, the rich KAIST treebank tag set
of 1975 POS tag types has been converted to a list of
CoNLL-like feature-attribute values refining coarse
grained POS categories.
Predicted Morphology Two sets of automatic
morphological analyses are provided for this task.
One is generated by the HanNanum morphological
analyzer.30 The HanNanum morphological ana-
lyzer gives the same morphemes and POS tags as the
KAIST Treebank. The other is generated by the Se-
jong morphological analyzer.31 The Sejong morpho-
logical analyzer gives a different set of morphemes
and POS tags as described in Choi and Palmer (2011).
4.9 The Polish Treebank
The Data Set Sk?adnica is a constituency treebank
of Polish (Wolin?ski et al, 2011; S?widzin?ski and
Wolin?ski, 2010). The trees were generated with
a non-probabilistic DCG parser S?wigra and then
disambiguated and validated manually. The ana-
lyzed texts come from the one-million-token sub-
corpus of the National Corpus of Polish (NKJP,
(Przepi?rkowski et al, 2012)) manually annotated
with morpho-syntactic tags.
The dependency version of Sk?adnica is a re-
sult of an automatic conversion of manually disam-
biguated constituent trees into dependency structures
(Wr?blewska, 2012). The conversion was an entirely
automatic process. Conversion rules were based
on morpho-syntactic information, phrasal categories,
and types of phrase-structure rules encoded within
constituent trees. It was possible to extract dependen-
cies because the constituent trees contain information
about the head of the majority of constituents. For
other constituents, heuristics were defined in order to
select their heads.
30http://kldp.net/projects/hannanum
31http://www.sejong.or.kr
The version of Sk?adnica used in the shared task
comprises parse trees for 8,227 sentences.32
Predicted Morphology For the shared task Pre-
dicted scenario, an automatic morphological an-
notation was generated by the PANTERA tagger
(Acedan?ski, 2010).
4.10 The Swedish Treebank
Swedish is moderately rich in inflections, including
a case system. Word order obeys the verb second
constraint in main clauses but is SVO in subordinate
clauses. Main clause order is freer than in English
but not as free as in some other Germanic languages,
such as German. Also, subject agreement with re-
spect to person and number has been dropped in
modern Swedish.
The Data Set The Swedish data sets are taken
from the Talbanken section of the Swedish Treebank
(Nivre and Megyesi, 2007). Talbanken is a syntacti-
cally annotated corpus developed in the 1970s, orig-
inally annotated according to the MAMBA scheme
(Teleman, 1974) with a syntactic layer consisting
of flat phrase structure and grammatical functions.
The syntactic annotation was later automatically con-
verted to full phrase structure with grammatical func-
tions and from that to dependency structure, as de-
scribed by Nivre et al (2006).
Both the phrase structure and the dependency
version use the functional labels from the original
MAMBA scheme, which provides a fine-grained clas-
sification of syntactic functions with 65 different la-
bels, while the phrase structure annotation (which
had to be inferred automatically) uses a coarse set
of only 8 labels. For the release of the Swedish tree-
bank, the POS level was re-annotated to conform to
the current de facto standard for Swedish, which is
the Stockholm-Ume? tagset (Ejerhed et al, 1992)
with 25 base tags and 25 morpho-syntactic features,
which together produce over 150 complex tags.
For the shared task, we used version 1.2 of the
treebank, where a number of conversion errors in
the dependency version have been corrected. The
phrase structure version was enriched by propagating
morpho-syntactic features from preterminals (POS
32Sk?adnica is available from http://zil.ipipan.waw.
pl/Sklicense.
162
tags) to higher non-terminal nodes using a standard
head percolation table, and a version without crossing
branches was derived using the lifting strategy (Boyd,
2007).
Adapting the Data to the Shared Task Explicit
attribute names were added to the feature field and the
split was changed to match the shared task minimal
training set size.
Predicted Morphology POS tags and morpho-
syntactic features were produced using the Hun-
PoS tagger (Hal?csy et al, 2007) trained on the
Stockholm-Ume? Corpus (Ejerhed and K?llgren,
1997).
5 Overview of the Participating Systems
With 7 teams participating, more than 14 systems for
French and 10 for Arabic and German, this shared
task is on par with the latest large-scale parsing evalu-
ation campaign SANCL 2012 (Petrov and McDonald,
2012). The present shared task was extremely de-
manding on our participants. From 30 individuals or
teams who registered and obtained the data sets, we
present results for the seven teams that accomplished
successful executions on these data in the relevant
scenarios in the given the time frame.
5.1 Dependency Track
Seven teams participated in the dependency track.
Two participating systems are based on MaltParser:
MALTOPTIMIZER (Ballesteros, 2013) and AI:KU
(Cirik and S?ensoy, 2013). MALTOPTIMIZER uses
a variant of MaltOptimizer (Ballesteros and Nivre,
2012) to explore features relevant for the processing
of morphological information. AI:KU uses a combi-
nation of MaltParser and the original MaltOptimizer.
Their system development has focused on the inte-
gration of an unsupervised word clustering method
using contextual and morphological properties of the
words, to help combat sparseness.
Similarly to MaltParser ALPAGE:DYALOG
(De La Clergerie, 2013) also uses a shift-reduce
transition-based parser but its training and decoding
algorithms are based on beam search. This parser is
implemented on top of the tabular logic programming
system DyALog. To the best of our knowledge, this
is the first dependency parser capable of handling
word lattice input.
Three participating teams use the MATE parser
(Bohnet, 2010) in their systems: the BASQUETEAM
(Goenaga et al, 2013), IGM:ALPAGE (Constant et
al., 2013) and IMS:SZEGED:CIS (Bj?rkelund et al,
2013). The BASQUETEAM uses the MATE parser in
combination with MaltParser (Nivre et al, 2007b).
The system combines the parser outputs via Malt-
Blender (Hall et al, 2007). IGM:ALPAGE also uses
MATE and MaltParser, once in a pipeline architec-
ture and once in a joint model. The models are com-
bined via a re-parsing strategy based on (Sagae and
Lavie, 2006). This system mainly focuses on MWEs
in French and uses a CRF tagger in combination
with several large-scale dictionaries to handle MWEs,
which then serve as input for the two parsers.
The IMS:SZEGED:CIS team participated in both
tracks, with an ensemble system. For the depen-
dency track, the ensemble includes the MATE parser
(Bohnet, 2010), a best-first variant of the easy-first
parser by Goldberg and Elhadad (2010b), and turbo
parser (Martins et al, 2010), in combination with
a ranker that has the particularity of using features
from the constituent parsed trees. CADIM (Marton et
al., 2013b) uses their variant of the easy-first parser
combined with a feature-rich ensemble of lexical and
syntactic resources.
Four of the participating teams use exter-
nal resources in addition to the parser. The
IMS:SZEGED:CIS team uses external morpholog-
ical analyzers. CADIM uses SAMA (Graff et al,
2009) for Arabic morphology. ALPAGE:DYALOG
and IGM:ALPAGE use external lexicons for French.
IGM:ALPAGE additionally uses Morfette (Chrupa?a
et al, 2008) for morphological analysis and POS
tagging. Finally, as already mentioned, AI:KU clus-
ters words and POS tags in an unsupervised fashion
exploiting additional, un-annotated data.
5.2 Constituency Track
A single team participated in the constituency parsing
task, the IMS:SZEGED:CIS team (Bj?rkelund et al,
2013). Their phrase-structure parsing system uses a
combination of 8 PCFG-LA parsers, trained using a
product-of-grammars procedure (Petrov, 2010). The
50-best parses of this combination are then reranked
by a model based on the reranker by Charniak and
163
Johnson (2005).33
5.3 Baselines
We additionally provide the results of two baseline
systems for the nine languages, one for constituency
parsing and one for dependency parsing.
For the dependency track, our baseline system is
MaltParser in its default configuration (the arc-eager
algorithm and liblinear for training). Results marked
as BASE:MALT in the next two sections report the
results of this baseline system in different scenarios.
The constituency parsing baseline is based on the
most recent version of the PCFG-LA model of Petrov
et al (2006), used with its default settings and five
split/merge cycles, for all languages.34 We use this
parser in two configurations: a ?1-best? configura-
tion where all POS tags are provided to the parser
(predicted or gold, depending on the scenario), and
another configuration in which the parser performs
its own POS tagging. These baselines are referred to
as BASE:BKY+POS and BASE:BKY+RAW respec-
tively in the following results sections. Note that
even when BASE:BKY+POS is given gold POS tags,
the Berkeley parser sometimes fails to reach a perfect
POS accuracy. In cases when the parser cannot find a
parse with the provided POS, it falls back on its own
POS tagging for all tokens.
6 Results
The high number of submitted system variants and
evaluation scenarios in the task resulted in a large
number of evaluation scores. In the following evalu-
ation, we focus on the best run for each participant,
and we aim to provide key points on the different
dimensions of analysis resulting from our evaluation
protocol. We invite our interested readers to browse
the comprehensive representation of our results on
the official shared-task results webpages.35
33Note that a slight but necessary change in the configuration
of one of our metrics, which occurred after the system submis-
sion deadline, resulted in the IMS:SZEGED:CIS team to submit
suboptimal systems for 4 languages. Their final scores are ac-
tually slightly higher and can be found in (Bj?rkelund et al,
2013).
34For Semitic languages, we used the lattice based PCFG-LA
extension by Goldberg (2011).
35http://www.spmrl.org/
spmrl2013-sharedtask-results.html.
6.1 Gold Scenarios
This section presents the parsing results in gold sce-
narios, where the systems are evaluated on gold seg-
mented and tagged input. This means that the se-
quence of terminals, POS tags, and morphological
features are provided based on the treebank anno-
tations. This scenario was used in most previous
shared tasks on data-driven parsing (Buchholz and
Marsi, 2006; Nivre et al, 2007a; K?bler, 2008). Note
that this scenario was not mandatory. We thank our
participants for providing their results nonetheless.
We start by reviewing dependency-based parsing
results, both on the trees and on multi-word expres-
sion, and continue with the different metrics for
constituency-based parsing.
6.1.1 Dependency Parsing
Full Training Set The results for the gold parsing
scenario of dependency parsing are shown in the top
block of table 3.
Among the six systems, IMS:SZEGED:CIS
reaches the highest LAS scores, not only on aver-
age, but for every single language. This shows that
their approach of combining parsers with (re)ranking
provides robust parsing results across languages with
different morphological characteristics. The second
best system is ALPAGE:DYALOG, the third best sys-
tem is MALTOPTIMIZER. The fact that AI:KU is
ranked below the Malt baseline is due to their sub-
mission of results for 6 out of the 9 languages. Simi-
larly, CADIM only submitted results for Arabic and
ranked in the third place for this language, after the
two IMS:SZEGED:CIS runs. IGM:ALPAGE and
BASQUETEAM did not submit results for this setting.
Comparing LAS results across languages is prob-
lematic due to the differences between languages,
treebank size and annotation schemes (see section 3),
so the following discussion is necessarily tentative. If
we consider results across languages, we see that the
lowest results (around 83% for the best performing
system) are reached for Hebrew and Swedish, the
languages with the smallest data sets. The next low-
est result, around 86%, is reached for Basque. Other
languages reach similar LAS scores, around 88-92%.
German, with the largest training set, reaches the
highest LAS, 91.83%.
Interstingly, all systems have high LAS scores
on the Korean Treebank given a training set size
164
team Arabic Basque French German Hebrew Hungarian Korean Polish Swedish avg.
1) gold setting / full training set
IMS:SZEGED:CIS 89.83 86.68 90.29 91.83 83.87 88.06 89.59 89.58 83.97 88.19
ALPAGE:DYALOG 85.87 80.39 87.69 88.25 80.70 79.60 88.23 86.00 79.80 84.06
MALTOPTIMIZER 87.03 82.07 85.71 86.96 80.03 83.14 89.39 80.49 77.67 83.61
BASE:MALT 82.28 69.19 79.86 79.98 76.61 72.34 88.43 77.70 75.73 78.01
AI:KU 86.39 86.98 79.42 83.67 85.16 78.87 55.61
CADIM 85.56 9.51
2) gold setting / 5k training set
IMS:SZEGED:CIS 87.35 85.69 88.73 87.70 83.87 87.21 83.38 89.16 83.97 86.34
ALPAGE:DYALOG 83.25 79.11 85.66 83.88 80.70 78.42 81.91 85.67 79.80 82.04
MALTOPTIMIZER 85.30 81.40 84.93 83.59 80.03 82.37 83.74 79.79 77.67 82.09
BASE:MALT 80.36 67.13 78.16 76.64 76.61 71.27 81.93 76.64 75.73 76.05
AI:KU 84.98 83.47 79.42 82.84 84.37 78.87 54.88
CADIM 82.67 9.19
3) predicted setting / full training set
IMS:SZEGED:CIS 86.21 85.14 85.24 89.65 80.89 86.13 86.62 87.07 82.13 85.45
ALPAGE:DYALOG 81.20 77.55 82.06 84.80 73.63 75.58 81.02 82.56 77.54 79.55
MALTOPTIMIZER 81.90 78.58 79.00 82.75 73.01 79.63 82.65 79.89 75.82 79.25
BASE:MALT 80.36 70.11 77.98 77.81 69.97 70.15 82.06 75.63 73.21 75.25
AI:KU 72.57 82.32 69.01 78.92 81.86 76.35 51.23
BASQUETEAM 84.25 84.51 88.66 84.97 80.88 47.03
IGM:ALPAGE 85.86 9.54
CADIM 83.20 9.24
4) predicted setting / 5k training set
IMS:SZEGED:CIS 83.66 83.84 83.45 85.08 80.89 85.24 80.80 86.69 82.13 83.53
MALTOPTIMIZER 79.64 77.59 77.56 79.22 73.01 79.00 75.90 79.50 75.82 77.47
ALPAGE:DYALOG 78.65 76.06 80.11 73.07 73.63 74.48 73.79 82.04 77.54 76.60
BASE:MALT 78.48 68.12 76.54 74.81 69.97 69.08 74.87 75.29 73.21 73.37
AI:KU 71.23 79.16 69.01 78.04 81.30 76.35 50.57
BASQUETEAM 83.19 82.65 84.70 84.01 80.88 46.16
IGM:ALPAGE 83.60 9.29
CADIM 80.51 8.95
Table 3: Dependency parsing: LAS scores for full and 5k training sets and for gold and predicted input. Results in bold
show the best results per language and setting.
of approximately 23,000 sentences, which is a little
over half of the German treebank. For German, on
the other hand, only the IMS:SZEGED:CIS system
reaches higher LAS scores than for Korean. This
final observation indicates that more than treebank
size is important for comparing system performance
across treebanks. This is the reason for introducing
the reduced set scenario, in which we can see how the
participating system perform on a common ground,
albeit small.
5k Training Set The results for the gold setting
on the 5k train set are shown in the second block
of Table 3. Compared with the full training, we
see that there is a drop of around 2 points in this
setting. Some parser/language pairs are more sensi-
tive to data sparseness than others. CADIM, for in-
stance, exhibit a larger drop than MALTOPTIMIZER
on Arabic, and MALTOPTIMIZER shows a smaller
drop than IMS:SZEGED:CIS on French. On average,
among all systems that covered all languages, MALT-
OPTIMIZER has the smallest drop when moving to
5k training, possibly since the automatic feature opti-
mization may differ for different data set sizes.
Since all languages have the same number of sen-
tences in the train set, these results can give us limited
insight into the parsing complexity of the different
treebanks. Here, French, Arabic, Polish, and Korean
reach the highest LAS scores while Swedish reaches
165
Team F_MWE F_COMP F_MWE+POS
1) gold setting / full training set
AI:KU 99.39 99.53 99.34
IMS:SZEGED:CIS 99.26 99.39 99.21
MALTOPTIMIZER 98.95 98.99 0
ALPAGE:DYALOG 98.32 98.81 0
BASE:MALT 68.7 72.55 68.7
2) predicted setting / full training set
IGM:ALPAGE 80.81 81.18 77.37
IMS:SZEGED:CIS 79.45 80.79 70.48
ALPAGE:DYALOG 77.91 79.25 0
BASQUE-TEAM 77.19 79.81 0
MALTOPTIMIZER 70.29 74.25 0
BASE:MALT 67.49 71.01 0
AI:KU 0 0 0
3) predicted setting / 5k training set
IGM:ALPAGE 77.66 78.68 74.04
IMS:SZEGED:CIS 77.28 78.92 70.42
ALPAGE:DYALOG 75.17 76.82 0
BASQUETEAM 73.07 76.58 0
MALTOPTIMIZER 65.76 70.42 0
BASE:MALT 62.05 66.8 0
AI:KU 0 0 0
Table 4: Dependency Parsing: MWE results
the lowest one. Treebank variance depends not only
on the language but also on annotation decisions,
such as label set (Swedish, interestingly, has a rela-
tively rich one). A more careful comparison would
then take into account the correlation of data size,
label set size and parsing accuracy. We investigate
these correlations further in section 7.1.
6.1.2 Multiword Expressions
MWE results on the gold setting are found at
the top of Table 4. All systems, with the excep-
tion of BASE:MALT, perform exceedingly well in
identifying the spans and non-head components of
MWEs given gold morphology.36 These almost per-
fect scores are the consequence of the presence of
two gold MWE features, namely MWEHEAD and
PRED=Y, which respectively indicate the node span
of the whole MWE and its dependents, which do not
have a gold feature field. The interesting scenario is,
of course, the predicted one, where these features are
not provided to the parser, as in any realistic applica-
tion.
36Note that for the labeled measure F_MWE+POS, both
MALTOPTIMIZER and ALPAGE:DYALOG have an F-score of
zero, since they do not attempt to predict the MWE label at all.
6.1.3 Constituency Parsing
In this part, we provide accuracy results for phrase-
structure trees in terms of ParsEval F-scores. Since
ParsEval is sensitive to the non-terminals-per-word
ratio in the data set (Rehbein and van Genabith,
2007a; Rehbein and van Genabith, 2007b), and given
the fact that this ratio varies greatly within our data
set (as shown in Table 2), it must be kept in mind that
ParsEval should only be used for comparing parsing
performance over treebank instances sharing the ex-
act same properties in term of annotation schemes,
sentence length and so on. When comparing F-Scores
across different treebanks and languages, it can only
provide a rough estimate of the relative difficulty or
ease of parsing these kinds of data.
Full Training Set The F-score results for the gold
scenario are provided in the first block of Table 5.
Among the two baselines, BASE:BKY+POS fares
better than BASE:BKY+RAW since the latter selects
its own POS tags and thus cannot benefit from the
gold information. The IMS:SZEGED:CIS system
clearly outperforms both baselines, with Hebrew as
an outlier.37
As in the dependency case, the results are not
strictly comparable across languages, yet we can
draw some insights from them. We see consider-
able differences between the languages, with Basque,
Hebrew, and Hungarian reaching F-scores in the low
90s for the IMS:SZEGED:CIS system, Korean and
Polish reaching above-average F-scores, and Ara-
bic, French, German, and Swedish reaching F-scores
below the average, but still in the low 80s. The per-
formance is, again, not correlated with data set sizes.
Parsing Hebrew, with one of the smallest training
sets, obtains higher accuracy many other languages,
including Swedish, which has the same training set
size as Hebrew. It may well be that gold morphologi-
cal information is more useful for combatting sparse-
ness in languages with richer morphology (though
Arabic here would be an outlier for this conjecture),
or it may be that certain treebanks and schemes are
inherently harder to parser than others, as we investi-
gate in section 7.
For German, the language with the largest training
37It might be that the easy layer of syntactic tags benefits from
the gold POS tags provided. See section 4 for further discussion
of this layer.
166
team Arabic Basque French German Hebrew Hungarian Korean Polish Swedish avg.
1) gold setting / full training set
IMS:SZEGED:CIS 82.20 90.04 83.98 82.07 91.64 92.60 86.50 88.57 85.09 86.97
BASE:BKY+POS 80.76 76.24 81.76 80.34 92.20 87.64 82.95 88.13 82.89 83.66
BASE:BKY+RAW 79.14 69.78 80.38 78.99 87.32 81.44 73.28 79.51 78.94 78.75
2) gold setting / 5k training set
IMS:SZEGED:CIS 79.47 88.45 82.25 74.78 91.64 91.87 80.10 88.18 85.09 84.65
BASE:BKY+POS 77.54 74.06 78.07 71.37 92.20 86.74 72.85 87.91 82.89 80.40
BASE:BKY+RAW 75.22 67.16 75.91 68.94 87.32 79.34 60.40 78.30 78.94 74.61
3) predicted setting / full training set
IMS:SZEGED:CIS 81.32 87.86 81.83 81.27 89.46 91.85 84.27 87.55 83.99 85.49
BASE:BKY+POS 78.66 74.74 79.76 78.28 85.42 85.22 78.56 86.75 80.64 80.89
BASE:BKY+RAW 79.19 70.50 80.38 78.30 86.96 81.62 71.42 79.23 79.18 78.53
4) predicted setting / 5k training set
IMS:SZEGED:CIS 78.85 86.65 79.83 73.61 89.46 90.53 78.47 87.46 83.99 83.21
BASE:BKY+POS 74.84 72.35 76.19 69.40 85.42 83.82 67.97 87.17 80.64 77.53
BASE:BKY+RAW 74.57 66.75 75.76 68.68 86.96 79.35 58.49 78.38 79.18 74.24
Table 5: Constituent Parsing: ParsEval F-scores for full and 5k training sets and for gold and predicted input. Results in
bold show the best results per language and setting.
set and the highest scores in dependency parsing,
the F-scores are at the lower end. These low scores,
which are obtained despite the larger treebank and
only moderately free word-order, are surprising. This
may be due to case syncretism; gold morphological
information exhibits its own ambiguity and thus may
not be fully utilized.
5k Training Set Parsing results on smaller com-
parable test sets are presented in the second block
of Table 5. On average, IMS:SZEGED:CIS is less
sensitive than BASE:BKY+POS to the reduced size.
Systems are not equally sensitive to reduced training
sets, and the gaps range from 0.4% to 3%, with Ger-
man and Korean as outliers (Korean suffering a 6.4%
drop in F-score and German 7.3%). These languages
have the largest treebanks in the full setting, so it is
not surprising that they suffer the most. But this in
itself does not fully explain the cross-treebank trends.
Since ParsEval scores are known to be sensitive to
the label set sizes and the depth of trees, we provide
LeafAncestor scores in the following section.
6.1.4 Leaf-Ancestor Results
The variation across results in the previous subsec-
tion may have been due to differences across annota-
tion schemes. One way to neutralize this difference
(to some extent) is to use a different metric. We
evaluated the constituency parsing results using the
Leaf-Ancestor (LA) metric, which is less sensitive
to the number of nodes in a tree (Rehbein and van
Genabith, 2007b; K?bler et al, 2008). As shown in
Table 6, these results are on a different (higher) scale
than ParsEval, and the average gap between the full
and 5k setting is lower.
Full Training Set The LA results in gold setting
for full training sets are shown in the first block of Ta-
ble 6. The trends are similar to the ParsEval F-scores.
German and Arabic present the lowest LA scores
(in contrast to the corresponding F-scores, Arabic is
a full point below German for IMS:SZEGED:CIS).
Basque and Hungarian have the highest LA scores.
Hebrew, which had a higher F-score than Basque,
has a lower LA than Basque and is closer to French.
Korean also ranks worse in the LA analysis. The
choice of evaluation metrics thus clearly impacts sys-
tem rankings ? F-scores rank some languages suspi-
ciously high (e.g., Hebrew) due to deeper trees, and
another metric may alleviate that.
5k Training Set The results for the leaf-ancestor
(LA) scores in the gold setting for the 5k training set
are shown in the second block of Table 6. Across
167
team Arabic Basque French German Hebrew Hungarian Korean Polish Swedish avg.
1) gold setting / full training set
IMS:SZEGED:CIS 88.61 94.90 92.51 89.63 92.84 95.01 91.30 94.52 91.46 92.31
BASE:BKY+POS 87.85 91.55 91.74 88.47 92.69 92.52 90.82 92.81 90.76 91.02
BASE:BKY+RAW 87.05 89.71 91.22 87.77 91.29 90.62 87.11 90.58 88.97 89.37
2) gold setting / 5k training set
IMS:SZEGED:CIS 86.68 94.21 91.56 85.74 92.84 94.79 88.87 94.17 91.46 91.15
BASE:BKY+POS 86.26 90.72 89.71 84.11 92.69 92.11 86.75 92.91 90.76 89.56
BASE:BKY+RAW 84.97 88.68 88.74 83.08 91.29 89.94 81.82 90.31 88.97 87.53
3) predicted setting / full training set
IMS:SZEGED:CIS 88.45 94.50 91.79 89.32 91.95 94.90 90.13 94.11 91.05 91.80
BASE:BKY+POS 86.60 90.90 90.96 87.46 89.66 91.72 89.10 92.56 89.51 89.83
BASE:BKY+RAW 86.97 89.91 91.11 87.46 90.77 90.50 86.68 90.48 89.16 89.23
4) predicted setting / 5k training set
IMS:SZEGED:CIS 86.69 93.85 90.76 85.20 91.95 94.05 87.99 93.99 91.05 90.61
BASE:BKY+POS 84.76 89.83 89.18 83.05 89.66 91.24 84.87 92.74 89.51 88.32
BASE:BKY+RAW 84.63 88.50 89.00 82.69 90.77 89.93 81.50 90.08 89.16 87.36
Table 6: Constituent Parsing: Leaf-Ancestor scores for full and 5k training sets and for gold and predicted input.
parsers, IMS:SZEGED:CIS again has a smaller drop
than BASE:BKY+POS on the reduced size. German
suffers the most from the reduction of the training
set, with a loss of approximately 4 points. Korean,
however, which was also severely affected in terms
of F-scores, only loses 1.17 points in the LA score.
On average, the LA seem to reflect a smaller drop
when reducing the training set ? this underscores
again the impact of the choice of metrics on system
evaluation.
6.2 Predicted Scenarios
Gold scenarios are relatively easy since syntactically
relevant morphological information is disambiguated
in advance and is provided as input. Predicted scenar-
ios are more difficult: POS tags and morphological
features have to be automatically predicted, by the
parser or by external resources.
6.2.1 Dependency Parsing
Eight participating teams submitted dependency
results for this scenario. Two teams submitted for a
single language. Four teams covered all languages.
Full Training Set The results for the predicted
scenario in full settings are shown in the third
block of Table 3. Across the board, the re-
sults are considerably lower than the gold sce-
nario. Again, IMS:SZEGED:CIS is the best per-
forming system, followed by ALPAGE:DYALOG and
MALTOPTIMIZER. The only language for which
IMS:SZEGED:CIS is outperformed is French, for
which IGM:ALPAGE reaches higher results (85.86%
vs. 85.24%). This is due to the specialized treatment
of French MWEs in the IGM:ALPAGE system, which
is thereby shown to be beneficial for parsing in the
predicted setting.
If we compare the results for the predicted set-
ting and the gold one, given the full training set,
the IMS:SZEGED:CIS system shows small differ-
ences between 1.5 and 2 percent. The only ex-
ception is French, for which the LAS drops from
90.29% to 85.24% in the predicted setting. The
other systems show somewhat larger differences than
IMS:SZEGED:CIS, with the highest drops for Ara-
bic and Korean. The AI:KU system shows a similar
problem as IMS:SZEGED:CIS for French.
5k Training Set When we consider the predicted
setting for the 5k training set, in the last block of
Table 3, we see the same trends as comparing with
the full training set or when comparing to the gold
setting. Systems suffer from not having gold stan-
dard data, and they suffer from the small training set.
Interestingly, the loss between the different training
set sizes in the predicted setting is larger than in the
168
gold setting, but only marginally so, with a differ-
ence < 0.5. In other words, the predicted setting
adds a challenge to parsing, but it only minimally
compounds data sparsity.
6.2.2 Multiword Expressions Evaluation
In the predicted setting, shown in the second
block of table 4 for the full training set and in the
third block of the same table for the 5k training set,
we see that only two systems, IGM:ALPAGE and
IMS:SZEGED:CIS can predict the MWE label when
it is not present in the training set. IGM:ALPAGE?s
approach of using a separate classifier in combination
with external dictionaries is very successful, reach-
ing an F_MWE+POS score of 77.37. This is com-
pared to the score of 70.48 by IMS:SZEGED:CIS,
which predicts this node label as a side effect of
their constituent feature enriched dependency model
(Bj?rkelund et al, 2013). AI:KU has a zero score
for all predicted settings, which results from an erro-
neous training on the gold data rather than the pre-
dicted data.38
6.2.3 Constituency Parsing
Full Training Set The results for the predicted set-
ting with the full training set are shown in the third
block of table 5. A comparison with the gold setting
shows that all systems have a lower performance in
the predicted scenario, and the differences are in the
range of 0.88 for Arabic and 2.54 for Basque. It is
interesting to see that the losses are generally smaller
than in the dependency framework: on average, the
loss across languages is 2.74 for dependencies and
1.48 for constituents. A possible explanation can be
found in the two-dimensional structure of the con-
stituent trees, where only a subset of all nodes is
affected by the quality of morphology and POS tags.
The exception to this trend is Basque, for which the
loss in constituents is a full point higher than for de-
pendencies. Another possible explanation is that all
of our constituent parsers select their own POS tags
in one way or another. Most dependency parsers ac-
cept predicted tags from an external resource, which
puts an upper-bound on their potential performance.
5k Training Set The results for the predicted set-
ting given the 5k training set are shown in the bottom
38Unofficial updated results are to to be found in (Cirik and
S?ensoy, 2013)
block of table 5. They show the same trends as the
dependency ones: The results are slightly lower than
the results obtained in gold setting and the ones uti-
lizing the full training set.
6.2.4 Leaf Ancestor Metrics
Full Training Set The results for the predicted sce-
nario with a full training set are shown in the third
block of table 6. In the LA evaluation, the loss
in moving from gold morphology are considerably
smaller than in F-scores. For most languages, the
loss is less than 0.5 points. Exceptions are French
with a loss of 0.72, Hebrew with 0.89, and Korean
with 1.17. Basque, which had the highest loss in
F-scores, only shows a minor loss of 0.4 points. Also,
the average loss of 0.41 points is much smaller than
the one in the ParsEval score, 1.48.
5k Training Set The results for the predicted set-
ting given the 5k training set are shown in the last
block of table 6. These results, though considerably
lower (around 3 points), exhibit the exact same trends
as observed in the gold setting.
6.3 Realistic Raw Scenarios
The previous scenarios assume that input surface to-
kens are identical to tree terminals. For languages
such as Arabic and Hebrew, this is not always the
case. In this scenario, we evaluate the capacity of a
system to predict both morphological segmentation
and syntactic parse trees given raw, unsegmented
input tokens. This may be done via a pipeline as-
suming a 1-st best morphological analysis, or jointly
with parsing, assuming an ambiguous morpholog-
ical analysis lattice as input. In this task, both of
these scenarios are possible (see section 3). Thus,
this section presents a realistic evaluation of the par-
ticipating systems, using TedEval, which takes into
account complete morpho-syntactic parses.
Tables 7 and 8 present labeled and unlabeled
TedEval results for both constituency and depen-
dency parsers, calculated only for sentence of length
<= 70.39 We firstly observe that labeled TedEval
scores are considerably lower than unlabeled Ted-
Eval scores, as expected, since unlabeled scores eval-
uate only structural differences. In the labeled setup,
39TedEval builds on algorithms for calculating edit distance
on complete trees (Bille, 2005). In these algorithms, longer
sentences take considerably longer to evaluate.
169
Arabic Arabic Hebrew All
full training set 5k training set
Acc (x100) Ex (%) Acc (x100) Ex (%) Acc (x100) Ex (%) Avg. Soft Avg.
IMS:SZEGED:CIS (Bky) 83.34 1.63 82.54 0.67 56.47 0.67 69.51 69.51
IMS:SZEGED:CIS 89.12 8.37 87.82 5.56 86.08 8.27 86.95 86.95
CADIM 87.81 6.63 86.43 4.21 - - 43.22 86.43
MALTOPTIMIZER 86.74 5.39 85.63 3.03 83.05 5.33 84.34 84.34
ALPAGE:DYALOG 86.60 5.34 85.71 3.54 82.96 6.17 41.48 82.96
ALPAGE:DYALOG (RAW) - - - - 82.82 4.35 41.41 82.82
AI:KU - - - - 78.57 3.37 39.29 78.57
Table 7: Realistic Scenario: Tedeval Labeled Accuracy and Exact Match for the Raw scenario.
The upper part refers to constituency results, the lower part refers to dependency results
Arabic Arabic Hebrew All
full training set 5k training set
Acc (x100) Ex (%) Acc (x100) Ex (%) Acc (x100) Ex (%) Avg. Soft Avg.
IMS:SZEGED:CIS (Bky) 92.06 9.49 91.29 7.13 89.30 13.60 90.30 90.30
IMS:SZEGED:CIS 91.74 9.83 90.85 7.30 89.47 16.97 90.16 90.16
ALPAGE:DYALOG 89.99 7.98 89.46 5.67 88.33 12.20 88.90 88.90
MALTOPTIMIZER 90.09 7.08 89.47 5.56 87.99 11.64 88.73 88.73
CADIM 90.75 8.48 89.89 5.67 - - 44.95 89.89
ALPAGE:DYALOG (RAW) - - - - 87.61 10.24 43.81 87.61
AI:KU - - - - 86.70 8.98 43.35 86.70
Table 8: Realistic Scenario: Tedeval Unlabeled Accuracy and Exact Match for the Raw scenario.
Top upper part refers to constituency results, the lower part refers to dependency results.
the IMS:SZEGED:CIS dependency parser are the
best for both languages and data set sizes. Table 8
shows that their unlabeled constituency results reach
a higher accuracy than the next best system, their
own dependency results. However, a quick look at
the exact match metric reveals lower scores than for
its dependency counterparts.
For the dependency-based joint scenarios, there
is obviously an upper bound on parser performance
given inaccurate segmentation. The transition-based
systems, ALPAGE:DYALOG & MALTOPTIMIZER,
perform comparably on Arabic and Hebrew, with
ALPAGE:DYALOG being slightly better on both lan-
guages. Note that ALPAGE:DYALOG reaches close
results on the 1-best and the lattice-based input set-
tings, with a slight advantage for the former. This is
partly due to the insufficient coverage of the lexical
resource we use: many lattices do not contain the
gold path, so the joint prediction can only as be high
as the lattice predicted path allows.
7 Towards In-Depth Cross-Treebank
Evaluation
Section 6 reported evaluation scores across systems
for different scenarios. However, as noted, these re-
sults are not comparable across languages, represen-
tation types and parsing scenarios due to differences
in the data size, label set size, length of sentences and
also differences in evaluation metrics.
Our following discussion in the first part of this
section highlights the kind of impact that data set
properties have on the standard metrics (label set size
on LAS, non-terminal nodes per sentence on F-score).
Then, in the second part of this section we use the
TedEval cross-experiment protocols for comparative
evaluation that is less sensitive to representation types
and annotation idiosyncrasies.
7.1 Parsing Across Languages and Treebanks
To quantify the impact of treebank characteristics on
parsing parsing accuracy we looked at correlations
of treebank properties with parsing results. The most
highly correlated combinations we have found are
shown in Figures 2, 3, and 4 for the dependency track
and the constituency track (F-score and LeafAnces-
170
21/09/13 03:00SPMRL charts
Page 3 sur 3http://pauillac.inria.fr/~seddah/updated_official.spmrl_results.html
Correlation between label set size, treebank size, and mean LAS
FrP
FrP
GeP
GeP
HuP
HuP
SwP
ArP
ArP
ArG
ArG
BaP
BaP
FrG
FrG
GeG
GeG
HeP
HeG
HuG
HuG
PoP
PoP
PoG
PoG
SwG
BaG
BaG
KoP
KoP
KoG
KoG
10 50 100 500 1 000
72
74
76
78
80
82
84
86
88
90
treebank size / #labels
L
A
S
 
(
%
)
Figure 2: The correlation between treebank size, label set size, and LAS scores. x: treebank size / #labels ; y: LAS (%)
01/10/13 00:43SPMRL charts: all sent.
Page 1 sur 5file:///Users/djame2/=Boulot/=PARSING-FTB/statgram/corpus/SPMRL-S?/SPMRL_FINAL/RESULTS/OFFICIAL/official_ptb-all.spmrl_results.html
SPMRL Results charts (Parseval): Const. Parsing Track (gold tokens, all sent.)
(13/10/01 00:34:34
Arabic Basque French German Hebrew Hungarian Korean Polish Swedish Synthesis
pred/full pred/5k gold/full gold/5k Correlation charts
Correlation between treebank size (#Non terminal), number of sentences (#sent) and mean F1
Arabic
Basque
French
German
Hebrew
Hungarian
Korean
Polish
Swedish
ArP
ArG
BaP
BaG
FrP
FrG
GeP
GeG
HeP
HeG
HuP
HuG
KoP
KoG
PoP
PoG
SwP
SwG
8 9 10
72
74
76
78
80
82
84
86
88
90
92
treebank size (#Non terminal) / #sent
F
1
 
(
%
)
Figure 3: The correlation between the non terminals per sentence ratio and F-scores. x: #non terminal/ #sentence ; y:
F1 (%)
171
tor) respectively.
Figure 2 presents the LAS against the average num-
ber of tokens relative to the number of labels. The
numbers are averaged per language over all partici-
pating systems, and the size of the ?bubbles? is pro-
portional to the number of participants for a given
language setting. We provide ?bubbles? for all lan-
guages in the predicted (-P) and gold (-G) setting,
for both training set sizes. The lower dot in terms
of parsing scores always corresponds to the reduced
training set size.
Figure 2 shows a clear correlation between data-
set complexity and parsing accuracy. The simpler
the data set is (where ?simple" here translates into
large data size with a small set of labels), the higher
the results of the participating systems. The bubbles
reflects a diagonal that indicates correlation between
these dimensions. Beyond that, we see two interest-
ing points off of the diagonal. The Korean treebank
(pink) in the gold setting and full training set can be
parsed with a high LAS relative to its size and label
set. It is also clear that the Hebrew treebank (purple)
in the predicted version is the most difficult one to
parse, relative to our expectation about its complexity.
Since the Hebrew gold scenario is a lot closer to the
diagonal again, it may be that this outlier is due to the
coverage and quality of the predicted morphology.
Figure 340 shows the correlation of data complex-
ity in terms of the average number of non-terminals
per sentence, and parsing accuracy (ParsEval F-
score). Parsing accuracy is again averaged over all
participating systems for a given language. In this
figure, we see a diagonal similar to the one in figure 2,
where Arabic (dark blue) has high complexity of the
data (here interpreted as flat trees, low number of
non terminals per sentence) and low F-scores accord-
ingly. Korean (pink), Swedish (burgundy), Polish
(light green), and Hungarian (light blue) follow, and
then Hebrew (purple) is a positive outlier, possibly
due to an additional layer of ?easy" syntactic POS
nodes which increases tree size and inflates F-scores.
French (orange), Basque (red), and German (dark
green) are negative outliers, falling off the diago-
nal. German has the lowest F-score with respect to
40This figure was created from the IMS:SZEGED:CIS
(Const.) and our own PCFG-LA baseline in POS Tagged mode
(BASE:BKY+POS) so as to avoid the noise introduced by the
parser?s own tagging step (BASE:BKY+RAW).
what would be expected for the non-terminals per
sentence ratio, which is in contrast to the LAS fig-
ure where German occurs among the less complex
data set to parse. A possible explanation may be
the crossing branches in the original treebank which
were re-attached. This creates flat and variable edges
which might be hard predict accurately.
Figure 441 presents the correlation between parsing
accuracy in terms the LeafAncestor metrics (macro
averaged) and treebank complexity in terms of the
average number of non-terminals per sentence. As
in the correlation figures, the parsing accuracy is
averaged over the participanting systems for any lan-
guage. The LeafAncestor accuracy is calculated over
phrase structure trees, and we see a similar diago-
nal to the one in Figure 3 showing that flatter tree-
banks are harder (that is, are correlated with lower
averaged scores) But, its slope is less steep than for
the F-score, which confirms the observation that the
LeafAncestor metric is less sensitive than F-score to
the non-terminals-per-sentence ratio.
Similarly to Figure 3, German is a negative outlier,
which means that this treebank is harder to parse ? it
obtains lower scores on average than we would ex-
pect. As for Hebrew, it is much closer to the diagonal.
As it turns out, the "easy" POS layer that inflates the
scores does not affect the LA ratings as much.
7.2 Evaluation Across Scenarios, Languages
and Treebanks
In this section we analyze the results in cross-
scenario, cross-annotation, and cross-framework set-
tings using the evaluation protocols discussed in
(Tsarfaty et al, 2012b; Tsarfaty et al, 2011; Tsarfaty
et al, 2012a).
As a starting point, we select comparable sections
of the parsed data, based on system runs trained on
the small train set (train5k). For those, we selected
subsets containing the first 5,000 tree terminals (re-
specting sentence boundaries) of the test set. We only
used TedEval on sentences up to 70 terminals long,
and projectivized non-projective sentences in all sets.
We use the TedEval metrics to calculate scores on
both constituency and dependency structures in all
languages and all scenarios. Since the metric de-
fines one scale for all of these different cases, we can
41This figure was created under the same condition as the
F-score correlation in figure (Figure 3).
172
04/10/13 23:05SPMRL charts:
Page 1 sur 6file:///Users/djame2/=Boulot/=PARSING-FTB/statgram/corpus/SPMRL-SHAREDTASK/SPMRL_FINAL/RESULTS/TESTLEAF.spmrl_results.html
SPMRL Results charts (Parseval): Const. Parsing Track (gold tokens, )
(13/10/04 23:05:31
Arabic Basque French German Hebrew Hungarian Korean Polish Swedish
Synthesis
pred/full pred/5k gold/full gold/5k Correlation charts
Correlation between treebank size (#Non terminal), number of sentences (#sent) and mean Leaf Accuracy
ArP
ArG
BaP
BaG
FrP
FrG
GeP
HeP
HeG
HuP
HuG
KoP
KoG
PoP
PoG
SwP
SwG
GeG
8 9 10
74
76
78
80
82
84
86
88
90
92
94
treebank size (#Non terminal) / #sent
F
1
 
(
%
)
Figure 4: The correlation between the non terminals per sentence ratio and Leaf Accuracy (macro) scores. x: #non
terminal/ #sentence ; y: Acc.(%)
compare the performance across annotation schemes,
assuming that those subsets are representative of their
original source.42
Ideally, we would be using labeled TedEval scores,
as the labeled parsing task is more difficult, and la-
beled parses are far more informative than unlabeled
ones. However, most constituency-based parsers do
not provide function labels as part of the output, to
be compared with the dependency arcs. Furthermore,
as mentioned earlier, we observed a huge difference
between label set sizes for the dependency runs. Con-
sequently, labeled scores will not be as informative
across treebanks and representation types. We will
therefore only use labels across scenarios for the
same language and representation type.
42We choose this sample scheme for replicability. We first
tried sampling sentences, aiming at the same average sentence
length (20), but that seemed to create artificially difficult test sets
for languages as Polish and overly simplistic ones for French or
Arabic.
7.2.1 Cross-Scenario Evaluation: raw vs. gold
One novel aspect of this shared task is the evalu-
ation on non-gold segmentation in addition to gold
morphology. One drawback is that the scenarios are
currently not using the same metrics ? the metrics
generally applied for gold and predicted scenrios can-
not apply for raw. To assess how well state of the art
parsers perform in raw scenarios compared to gold
scenarios, we present here TedEval results comparing
raw and gold systems using the evaluation protocol
of Tsarfaty et al (2012b).
Table 9 presents the labeled and unlabeled results
for Arabic and Hebrew (in Full and 5k training set-
tings), and Table 10 presents unlabeled TedEval re-
sults (for all languages) in the gold settings. The
unlabeled TedEval results for the raw settings are
substantially lower then TedEval results on the gold
settings for both languages.
When comparing the unlabeled TedEval results for
Arabic and Hebrew on the participating systems, we
see a loss of 3-4 points between Table 9 (raw) and Ta-
ble 10 (gold). In particular we see that for the best per-
173
forming systems on Arabic (IMS:SZEGED:CIS for
both constituency and dependency), the gap between
gold and realistic scenarios is 3.4 and 4.3 points,
for the constituency and the dependency parser re-
spectively. These results are on a par with results
by Tsarfaty et al (2012b), who showed for different
settings, constituency and dependency based, that
raw scenarios are considerably more difficult to parse
than gold ones on the standard split of the Modern
Hebrew treebank.
For Hebrew, the performance gap between unla-
beled TedEval in raw (Table 9) and gold (Table 10)
is even more salient, with around 7 and 8 points of
difference between the scenarios. We can only specu-
late that such a difference may be due to the difficulty
of resolving Hebrew morpho-syntactic ambiguities
without sufficient syntactic information. Since He-
brew and Arabic now have standardized morpholog-
ically and syntactically analyzed data sets available
through this task, it will be possible to investigate
further how cross-linguistic differences in morpho-
logical ambiguity affect full-parsing accuracy in raw
scenarios.
This section compared the raw and gold parsing
results only on unlabeled TedEval metrics. Accord-
ing to what we have seen so far is expected that
for labeled TedEval metrics using the same protocol,
the gap between gold and raw scenario will be even
greater.
7.2.2 Cross-Framework Evaluation:
Dependency vs. Constituency
In this section, our focus is on comparing parsing
results across constituency and dependency parsers
based on the protocol of Tsarfaty et al (2012a) We
have only one submission from IMS:SZEGED:CIS
in the constituency track, and. from the same group,
a submission on the dependency track. We only com-
pare the IMS:SZEGED:CIS results on constituency
and dependency parsing with the two baselines we
provided. The results of the cross-framework evalua-
tion protocol are shown in Table 11.
The results comparing the two variants of the
IMS:SZEGED:CIS systems show that they are very
close for all languages, with differences ranging from
0.03 for German to 0.8 for Polish in the gold setting.
It has often been argued that dependency parsers
perform better than a constituency parser, but we
notice that when using a cross framework protocol,
such as TedEval, and assuming that our test set sam-
ple is representative, the difference between the in-
terpretation of both representation?s performance is
alleviated. Of course, here the metric is unlabeled, so
it simply tells us that both kind of parsing models are
equally able to provide similar tree structures. Said
differently, the gaps in the quality of predicting the
same underlying structure across representations for
MRLs is not as large as is sometimes assumed.
For most languages, the baseline constituency
parser performs better than the dependency base-
line one, with Basque and Korean as an exception,
and at the same time, the dependency version of
IMS:SZEGED:CIS performs slightly better than their
constituent parser for most languages, with the excep-
tion of Hebrew and Hungarian. It goes to show that,
as far as these present MRL results go, there is no
clear preference for a dependency over a constituency
parsing representation, just preferences among par-
ticular models.
More generally, we can say that even if the linguis-
tic coverage of one theory is shown to be better than
another one, it does not necessarily mean that the
statistical version of the formal theory will perform
better for structure prediction. System performance
is more tightly related to the efficacy of the learning
and search algorithms, and feature engineering on
top of the selected formalism.
7.2.3 Cross-Language Evaluation: All
Languages
We conclude with an overall outlook of the Ted-
Eval scores across all languages. The results on the
gold scenario, for the small training set and the 5k
test set are presented in Table 10. We concentrate
on gold scenarios (to avoid the variation in cover-
age of external morphological analyzers) and choose
unlabeled metrics as they are not sensitive to label
set sizes. We emphasize in bold, for each parsing
system (row in the table), the top two languages that
most accurately parsed by it (boldface) and the two
languages it performed the worse on (italics).
We see that the European languages German
and Hungarian are parsed most accurately in the
constituency-based setup, with Polish and Swedish
having an advantage in dependency parsing. Across
all systems, Korean is the hardest to parse, with Ara-
174
Arabic Hebrew AVG1 SOFT AVG Arabic Hebrew AVG2 SOFT AVG2
1) Constituency Evaluation
Labeled TedEval Unabeled TedEval
IMS:SZEGED:CIS (Bky) 83.59 56.43 70.01 70.01 92.18 88.02 90.1 90.1
2) Dependency Evaluation
Labeled TedEval Unabeled TedEval
IMS:SZEGED:CIS 88.61 84.74 86.68 86.68 91.41 88.58 90 90
ALPAGE:DYALOG 87.20 81.65 40.83 81.65 90.74 87.44 89.09 89.09
CADIM 87.99 - 44 87.99 91.22 - 45.61 91.22
MALTOPTIMIZER 86.62 81.74 43.31 86.62 90.26 87.00 45.13 90.26
ALPAGE:DYALOG (RAW) - 82.82 41.41 82.82 - 87.43 43.72 87.43
AI:KU - 77.8 38.9 77.8 - 85.87 42.94 85.87
Table 9: Labeled and Unlabeled TedEval Results for raw Scenarios, Trained on 5k sentences and tested on 5k terminals.
The upper part refers to constituency parsing and the lower part refers to dependency parsing.
Arabic Basque French German Hebrew Hungarian Korean Polish Swedish
1) Constituency Evaluation
IMS:SZEGED:CIS (Bky) 95.35 96.91 95.98 97.12 96.22 97.92 92.91 97.19 96.65
BASE:BKY+POS 95.11 94.69 95.08 97.01 95.85 97.08 90.55 96.99 96.38
BASE:BKY+RAW 94.58 94.32 94.72 96.74 95.64 96.15 87.08 95.93 95.90
2) Dependency Evaluation
IMS:SZEGED:CIS 95.76 97.63 96.59 96.88 96.29 97.56 94.62 98.01 97.22
ALPAGE:DYALOG 93.76 95.72 95.75 96.4 95.34 95.63 94.56 96.80 96.55
BASE:MALT 94.16 95.08 94.21 94.55 94.98 95.25 94.27 95.83 95.33
AI:KU - - 95.46 96.34 95.07 96.53 - 96.88 95.87
MALTOPTIMIZER 94.91 96.82 95.23 96.32 95.46 96.30 94.69 96.06 95.90
CADIM 94.66 - - - - - - - -
Table 10: Cross-Language Evaluation: Unlabeled TedEval Results in gold input scenario, On a 5k-sentences set set and
a 5k-terminals test set. The upper part refers to constituency parsing and the lower part refers to dependency parsing.
For each system we mark the two top scoring languages in bold and the two lowest scoring languages in italics.
team Arabic Basque French German Hebrew Hungarian Korean Polish Swedish
1) gold setting
IMS:SZEGED:CIS (Bky) 95.82 97.30 96.15 97.43 96.37 98.25 94.07 97.22 96.89
IMS:SZEGED:CIS 95.87 98.06 96.61 97.46 96.31 97.93 94.62 98.04 97.24
BASE:BKY+POS 95.61 95.25 95.48 97.31 96.03 97.53 92.15 96.97 96.66
BASE:MALT 94.26 95.76 94.23 95.53 95.00 96.09 94.27 95.90 95.35
2) predicted setting
IMS:SZEGED:CIS (Bky) 95.74 97.07 96.21 97.31 96.10 98.03 94.05 96.92 96.90
IMS:SZEGED:CIS 95.18 97.67 96.15 97.09 96.22 97.63 94.43 97.50 97.02
BASE:BKY+POS 95.03 95.35 97.12 95.36 97.20 91.34 96.92 96.25
BASE:MALT 95.49 93.84 95.39 94.41 95.72 93.74 96.04 95.09
Table 11: Cross Framework Evaluation: Unlabeled TedEval on generalized gold trees in gold scenario, trained on 5k
sentences and tested on 5k terminals.
bic, Hebrew and to some extent French following. It
appears that on a typological scale, Semitic and Asian
languages are still harder to parse than a range of Eu-
ropean languages in terms of structural difficulty and
complex morpho-syntactic interaction. That said,
note that we cannot tell why certain treebanks appear
more challenging to parse then others, and it is still
unclear whether the difficulty is inherent on the lan-
guage, in the currently available models, or because
of the annotation scheme and treebank consistency.43
43The latter was shown to be an important factor orthogonal
to the morphologically-rich nature of the treebank?s language
175
8 Conclusion
This paper presents an overview of the first shared
task on parsing morphologically rich languages. The
task features nine languages, exhibiting different lin-
guistic phenomena and varied morphological com-
plexity. The shared task saw submissions from seven
teams, and results produced by more than 14 different
systems. The parsing results were obtained in dif-
ferent input scenarios (gold, predicted, and raw) and
evaluated using different protocols (cross-framework,
cross-scenario, and cross-language). In particular,
this is the first time an evaluation campaign reports
on the execution of parsers in realistic, morphologi-
cally ambiguous, setting.
The best performing systems were mostly ensem-
ble systems combining multiple parser outputs from
different frameworks or training runs, or integrat-
ing a state-of-the-art morphological analyzer on top
of a carefully designed feature set. This is con-
sistent with previous shared tasks such as ConLL
2007 or SANCL?2012. However, dealing with am-
biguous morphology is still difficult for all systems,
and a promising approach, as demonstrated by AL-
PAGE:DYALOG, is to deal with parsing and morphol-
ogy jointly by allowing lattice input to the parser. A
promising generalization of this approach would be
the full integration of all levels of analysis that are
mutually informative into a joint model.
The information to be gathered from the results of
this shared task is vast, and we only scratched the
surface with our preliminary analyses. We uncov-
ered and documented insights of strategies that make
parsing systems successful: parser combination is
empirically proven to reach a robust performance
across languages, though language-specific strategies
are still a sound avenue for obtaining high quality
parsers for that individual language. The integration
of morphological analysis into the parsing needs to
be investigated thoroughly, and new approaches that
are morphologically aware need to be developed.
Our cross-parser, cross-scenario, and cross-
framework evaluation protocols have shown that, as
expected, more data is better, and that performance
on gold morphological input is significantly higher
than that in more realistic scenarios. We have shown
that gold morphological information is more help-
(Schluter and van Genabith, 2007)
ful to some languages and parsers than others, and
that it may also interact with successful identification
of multiword expressions. We have shown that dif-
ferences between dependency and constituency are
smaller than previously assumed and that properties
of the learning model and granularity of the output
labels are more influential. Finally, we observed
that languages which are typologically farthest from
English, such as Semitic and Asian languages, are
still amongst the hardest to parse, regardless of the
parsing method used.
Our cross-treebank, in-depth analysis is still pre-
liminary, owing to the limited time between the end
of the shared task and the deadline for publication
of this overview. but we nonetheless feel that our
findings may benefit researchers who aim to develop
parsers for diverse treebanks.44
A shared task is an inspection of the state of the
art, but it may also accelerate research in an area
by providing a stable data basis as well as a set of
strong baselines. The results produced in this task
give a rich picture of the issues associated with pars-
ing MRLs and initial cues towards their resolution.
This set of results needs to be further analyzed to be
fully understood, which will in turn contribute to new
insights. We hope that this shared task will provide
inspiration for the design and evaluation of future
parsing systems for these languages.
Acknowledgments
We heartily thank Miguel Ballesteros and Corentin
Ribeire for running the dependency and constituency
baselines. We warmly thank the Linguistic Data Con-
sortium: Ilya Ahtaridis, Ann Bies, Denise DiPersio,
Seth Kulick and Mohamed Maamouri for releasing
the Arabic Penn Treebank for this shared task and
for their support all along the process. We thank
Alon Itai and MILA, the knowledge center for pro-
cessing Hebrew, for kindly making the Hebrew tree-
bank and morphological analyzer available for us,
Anne Abeill? for allowing us to use the French tree-
bank, and Key-Sun Choi for the Kaist Korean Tree-
bank. We thank Grzegorz Chrupa?a for providing
the morphological analyzer Morfette, and Joachim
44The data set will be made available as soon as possible under
the license distribution of the shared-task, with the exception
of the Arabic data, which will continue to be distributed by the
LDC.
176
Wagner for his LeafAncestor implementation. We
finally thank ?zlem ?etinog?lu, Yuval Marton, Benoit
Crabb? and Benoit Sagot who have been nothing but
supportive during all that time.
At the end of this shared task (though watch out
for further updates and analyses), what remains to be
mentioned is our deep gratitude to all people involved,
either data providers or participants. Without all of
you, this shared task would not have been possible.
References
Anne Abeill?, Lionel Cl?ment, and Fran?ois Toussenel.
2003. Building a treebank for French. In Anne Abeill?,
editor, Treebanks. Kluwer, Dordrecht.
Szymon Acedan?ski. 2010. A Morphosyntactic Brill Tag-
ger for Inflectional Languages. In Advances in Natural
Language Processing, volume 6233 of Lecture Notes
in Computer Science, pages 3?14. Springer-Verlag.
Meni Adler and Michael Elhadad. 2006. An unsupervised
morpheme-based HMM for Hebrew morphological dis-
ambiguation. In Proceedings COLING-ACL, pages
665?672, Sydney, Australia.
Meni Adler, Yoav Goldberg, David Gabay, and Michael
Elhadad. 2008. Unsupervised lexicon-based resolution
of unknown words for full morphological analysis. In
Proceedings of ACL-08: HLT, pages 728?736, Colum-
bus, OH.
Meni Adler. 2007. Hebrew Morphological Disambigua-
tion: An Unsupervised Stochastic Word-based Ap-
proach. Ph.D. thesis, Ben-Gurion University of the
Negev.
Itziar Aduriz, Jos? Mar?a Arriola, Xabier Artola, A D?az
de Ilarraza, et al 1997. Morphosyntactic disambigua-
tion for Basque based on the constraint grammar for-
malism. In Proceedings of RANLP, Tzigov Chark, Bul-
garia.
Itziar Aduriz, Eneko Agirre, Izaskun Aldezabal, I?aki
Alegria, Xabier Arregi, Jose Maria Arriola, Xabier Ar-
tola, Koldo Gojenola, Aitor Maritxalar, Kepa Sarasola,
et al 2000. A word-grammar based morphological
analyzer for agglutinative languages. In Proceedings
of COLING, pages 1?7, Saarbr?cken, Germany.
Itziar Aduriz, Maria Jesus Aranzabe, Jose Maria Arriola,
Aitziber Atutxa, A Diaz de Ilarraza, Aitzpea Garmen-
dia, and Maite Oronoz. 2003. Construction of a
Basque dependency treebank. In Proceedings of the
2nd Workshop on Treebanks and Linguistic Theories
(TLT), pages 201?204, V?xj?, Sweden.
Zeljko Agic, Danijela Merkler, and Dasa Berovic. 2013.
Parsing Croatian and Serbian by using Croatian depen-
dency treebanks. In Proceedings of the Fourth Work-
shop on Statistical Parsing of Morphologically Rich
Languages (SPMRL), Seattle, WA.
I. Aldezabal, M.J. Aranzabe, A. Diaz de Ilarraza, and
K. Fern?ndez. 2008. From dependencies to con-
stituents in the reference corpus for the processing of
Basque. In Procesamiento del Lenguaje Natural, no
41 (2008), pages 147?154. XXIV edici?n del Congreso
Anual de la Sociedad Espa?ola para el Procesamiento
del Lenguaje Natural (SEPLN).
Bharat Ram Ambati, Samar Husain, Joakim Nivre, and
Rajeev Sangal. 2010. On the role of morphosyntactic
features in Hindi dependency parsing. In Proceedings
of the NAACL/HLT Workshop on Statistical Parsing of
Morphologically Rich Languages (SPMRL 2010), Los
Angeles, CA.
Mohammed Attia, Jennifer Foster, Deirdre Hogan,
Joseph Le Roux, Lamia Tounsi, and Josef van Gen-
abith. 2010. Handling unknown words in statistical
latent-variable parsing models for Arabic, English and
French. In Proceedings of the NAACL/HLT Workshop
on Statistical Parsing of Morphologically Rich Lan-
guages (SPMRL), Los Angeles, CA.
Miguel Ballesteros and Joakim Nivre. 2012. MaltOpti-
mizer: An optimization tool for MaltParser. In Pro-
ceedings of EACL, pages 58?62, Avignon, France.
Miguel Ballesteros. 2013. Effective morphological fea-
ture selection with MaltOptimizer at the SPMRL 2013
shared task. In Proceedings of the Fourth Workshop on
Statistical Parsing of Morphologically-Rich Languages,
pages 53?60, Seattle, WA.
Kepa Bengoetxea and Koldo Gojenola. 2010. Appli-
cation of different techniques to dependency parsing
of Basque. In Proceedings of the NAACL/HLT Work-
shop on Statistical Parsing of Morphologically Rich
Languages (SPMRL 2010), Los Angeles, CA.
Philip Bille. 2005. A survey on tree edit distance and re-
lated problems. Theoretical Computer Science, 337(1?
3):217?239, 6.
Anders Bj?rkelund, Ozlem Cetinoglu, Rich?rd Farkas,
Thomas Mueller, and Wolfgang Seeker. 2013.
(Re)ranking meets morphosyntax: State-of-the-art re-
sults from the SPMRL 2013 shared task. In Proceed-
ings of the Fourth Workshop on Statistical Parsing
of Morphologically-Rich Languages, pages 134?144,
Seattle, WA.
Ezra Black, Steven Abney, Dan Flickinger, Claudia
Gdaniec, Ralph Grishman, Philip Harrison, Donald
Hindle, Robert Ingria, Frederick Jelinek, Judith Kla-
vans, Mark Liberman, Mitchell Marcus, Salim Roukos,
Beatrice Santorini, and Tomek Strzalkowski. 1991. A
procedure for quantitatively comparing the syntactic
coverage of English grammars. In Proceedings of the
DARPA Speech and Natural Language Workshop 1991,
pages 306?311, Pacific Grove, CA.
177
Bernd Bohnet and Joakim Nivre. 2012. A transition-
based system for joint part-of-speech tagging and la-
beled non-projective dependency parsing. In Proceed-
ings of the EMNLP-CoNLL, pages 1455?1465, Jeju,
Korea.
Bernd Bohnet. 2010. Top accuracy and fast dependency
parsing is not a contradiction. In Proceedings of COL-
ING, pages 89?97, Beijing, China.
Adriane Boyd. 2007. Discontinuity revisited: An im-
proved conversion to context-free representations. In
Proceedings of the Linguistic Annotation Workshop,
Prague, Czech Republic.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang
Lezius, and George Smith. 2002. The TIGER treebank.
In Proceedings of the First Workshop on Treebanks
and Linguistic Theories (TLT), pages 24?41, Sozopol,
Bulgaria.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of CoNLL, pages 149?164, New York, NY.
Tim Buckwalter. 2002. Arabic morphological analyzer
version 1.0. Linguistic Data Consortium.
Tim Buckwalter. 2004. Arabic morphological analyzer
version 2.0. Linguistic Data Consortium.
Marie Candito and Djam? Seddah. 2010. Parsing word
clusters. In Proceedings of the NAACL/HLT Workshop
on Statistical Parsing of Morphologically Rich Lan-
guages (SPMRL 2010), Los Angeles, CA.
Marie Candito, Benoit Crabb?, and Pascal Denis. 2010.
Statistical French dependency parsing: Treebank con-
version and first results. In Proceedings of LREC, Val-
letta, Malta.
Xavier Carreras, Michael Collins, and Terry Koo. 2008.
TAG, dynamic programming, and the perceptron for ef-
ficient, feature-rich parsing. In Proceedings of CoNLL,
pages 9?16, Manchester, UK.
Eugene Charniak and Mark Johnson. 2005. Course-to-
fine n-best-parsing and maxent discriminative rerank-
ing. In Proceedings of ACL, pages 173?180, Barcelona,
Spain.
Eugene Charniak. 1997. Statistical parsing with a context-
free grammar and word statistics. In AAAI/IAAI, pages
598?603.
Eugene Charniak. 2000. A maximum entropy inspired
parser. In Proceedings of NAACL, pages 132?139, Seat-
tle, WA.
Jinho D. Choi and Martha Palmer. 2011. Statistical de-
pendency parsing in Korean: From corpus generation
to automatic parsing. In Proceedings of Second Work-
shop on Statistical Parsing of Morphologically Rich
Languages, pages 1?11, Dublin, Ireland.
Jinho D. Choi and Martha Palmer. 2012. Guidelines
for the Clear Style Constituent to Dependency Conver-
sion. Technical Report 01-12, University of Colorado
at Boulder.
Key-sun Choi, Young S. Han, Young G. Han, and Oh W.
Kwon. 1994. KAIST Tree Bank Project for Korean:
Present and Future Development. In In Proceedings
of the International Workshop on Sharable Natural
Language Resources, pages 7?14, Nara, Japan.
Jinho D. Choi. 2013. Preparing Korean data for the
shared task on parsing morphologically rich languages.
arXiv:1309.1649.
Grzegorz Chrupa?a, Georgiana Dinu, and Josef van Gen-
abith. 2008. Learning morphology with Morfette. In
Proceedings of LREC, Marrakech, Morocco.
Tagyoung Chung, Matt Post, and Daniel Gildea. 2010.
Factors affecting the accuracy of Korean parsing. In
Proceedings of the NAACL/HLT Workshop on Sta-
tistical Parsing of Morphologically Rich Languages
(SPMRL 2010), Los Angeles, CA.
Volkan Cirik and H?sn? S?ensoy. 2013. The AI-KU
system at the SPMRL 2013 shared task: Unsuper-
vised features for dependency parsing. In Proceed-
ings of the Fourth Workshop on Statistical Parsing of
Morphologically-Rich Languages, pages 68?75, Seat-
tle, WA.
Michael Collins. 2003. Head-driven statistical models for
natural language parsing. Computational Linguistics,
29(4):589?637.
Matthieu Constant, Marie Candito, and Djam? Seddah.
2013. The LIGM-Alpage architecture for the SPMRL
2013 shared task: Multiword expression analysis and
dependency parsing. In Proceedings of the Fourth
Workshop on Statistical Parsing of Morphologically-
Rich Languages, pages 46?52, Seattle, WA.
Anna Corazza, Alberto Lavelli, Giogio Satta, and Roberto
Zanoli. 2004. Analyzing an Italian treebank with
state-of-the-art statistical parsers. In Proceedings of
the Third Workshop on Treebanks and Linguistic Theo-
ries (TLT), T?bingen, Germany.
Benoit Crabb? and Marie Candito. 2008. Exp?riences
d?analyse syntaxique statistique du fran?ais. In Actes
de la 15?me Conf?rence sur le Traitement Automatique
des Langues Naturelles (TALN?08), pages 45?54, Avi-
gnon, France.
D?ra Csendes, J?nos Csirik, Tibor Gyim?thy, and Andr?s
Kocsor. 2005. The Szeged treebank. In Proceedings of
the 8th International Conference on Text, Speech and
Dialogue (TSD), Lecture Notes in Computer Science,
pages 123?132, Berlin / Heidelberg. Springer.
Eric De La Clergerie. 2013. Exploring beam-based
shift-reduce dependency parsing with DyALog: Re-
sults from the SPMRL 2013 shared task. In Proceed-
ings of the Fourth Workshop on Statistical Parsing of
178
Morphologically-Rich Languages, pages 81?89, Seat-
tle, WA.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The stanford typed dependencies repre-
sentation. In Proceedings of the workshop on Cross-
Framework and Cross-Domain Parser Evaluation.
Mona Diab, Nizar Habash, Owen Rambow, and Ryan
Roth. 2013. LDC Arabic treebanks and associated cor-
pora: Data divisions manual. Technical Report CCLS-
13-02, Center for Computational Learning Systems,
Columbia University.
Eva Ejerhed and Gunnel K?llgren. 1997. Stockholm
Ume? Corpus. Version 1.0. Department of Linguis-
tics, Ume? University and Department of Linguistics,
Stockholm University.
Eva Ejerhed, Gunnel K?llgren, Ola Wennstedt, and Mag-
nus ?str?m. 1992. The linguistic annotation system
of the Stockholm?Ume? Corpus project. Technical
Report 33, University of Ume?: Department of Linguis-
tics.
Nerea Ezeiza, I?aki Alegria, Jos? Mar?a Arriola, Rub?n
Urizar, and Itziar Aduriz. 1998. Combining stochastic
and rule-based methods for disambiguation in aggluti-
native languages. In Proceedings of COLING, pages
380?384, Montr?al, Canada.
Jenny Rose Finkel, Alex Kleeman, and Christopher D.
Manning. 2008. Efficient, feature-based, conditional
random field parsing. In Proceedings of ACL, pages
959?967, Columbus, OH.
Alexander Fraser, Helmut Schmid, Rich?rd Farkas, Ren-
jing Wang, and Hinrich Sch?tze. 2013. Knowledge
sources for constituent parsing of German, a morpho-
logically rich and less-configurational language. Com-
putational Linguistics, 39(1):57?85.
Iakes Goenaga, Koldo Gojenola, and Nerea Ezeiza. 2013.
Exploiting the contribution of morphological informa-
tion to parsing: the BASQUE TEAM system in the
SPRML?2013 shared task. In Proceedings of the Fourth
Workshop on Statistical Parsing of Morphologically-
Rich Languages, pages 61?67, Seattle, WA.
Yoav Goldberg and Michael Elhadad. 2010a. Easy-first
dependency parsing of Modern Hebrew. In Proceed-
ings of the NAACL/HLT Workshop on Statistical Pars-
ing of Morphologically Rich Languages (SPMRL 2010),
Los Angeles, CA.
Yoav Goldberg and Michael Elhadad. 2010b. An ef-
ficient algorithm for easy-first non-directional depen-
dency parsing. In Proceedings of HLT: NAACL, pages
742?750, Los Angeles, CA.
Yoav Goldberg and Reut Tsarfaty. 2008. A single frame-
work for joint morphological segmentation and syntac-
tic parsing. In Proceedings of ACL, Columbus, OH.
Yoav Goldberg, Meni Adler, and Michael Elhadad. 2008.
EM can find pretty good HMM POS-taggers (when
given a good start). In Proc. of ACL, Columbus, OH.
Yoav Goldberg, Reut Tsarfaty, Meni Adler, and Michael
Elhadad. 2009. Enhancing unlexicalized parsing per-
formance using a wide coverage lexicon, fuzzy tag-set
mapping, and EM-HMM-based lexical probabilities. In
Proceedings of EALC, pages 327?335, Athens, Greece.
Yoav Goldberg. 2011. Automatic syntactic processing of
Modern Hebrew. Ph.D. thesis, Ben Gurion University
of the Negev.
David Graff, Mohamed Maamouri, Basma Bouziri, Son-
dos Krouna, Seth Kulick, and Tim Buckwalter. 2009.
Standard Arabic Morphological Analyzer (SAMA) ver-
sion 3.1. Linguistic Data Consortium LDC2009E73.
Spence Green and Christopher D. Manning. 2010. Better
Arabic parsing: Baselines, evaluations, and analysis.
In Proceedings of COLING, pages 394?402, Beijing,
China.
Nathan Green, Loganathan Ramasamy, and Zden?k
?abokrtsk?. 2012. Using an SVM ensemble system for
improved Tamil dependency parsing. In Proceedings
of the ACL 2012 Joint Workshop on Statistical Pars-
ing and Semantic Processing of Morphologically Rich
Languages, pages 72?77, Jeju, Korea.
Spence Green, Marie-Catherine de Marneffe, and Christo-
pher D. Manning. 2013. Parsing models for identify-
ing multiword expressions. Computational Linguistics,
39(1):195?227.
Noemie Guthmann, Yuval Krymolowski, Adi Milea, and
Yoad Winter. 2009. Automatic annotation of morpho-
syntactic dependencies in a Modern Hebrew Treebank.
In Proceedings of the Eighth International Workshop on
Treebanks and Linguistic Theories (TLT), Groningen,
The Netherlands.
Nizar Habash and Ryan Roth. 2009. CATiB: The
Columbia Arabic Treebank. In Proceedings of ACL-
IJCNLP, pages 221?224, Suntec, Singapore.
Nizar Habash, Ryan Gabbard, Owen Rambow, Seth
Kulick, and Mitch Marcus. 2007. Determining case in
Arabic: Learning complex linguistic behavior requires
complex linguistic features. In Proceedings of EMNLP-
CoNLL, pages 1084?1092, Prague, Czech Republic.
Nizar Habash, Reem Faraj, and Ryan Roth. 2009a. Syn-
tactic Annotation in the Columbia Arabic Treebank. In
Proceedings of MEDAR International Conference on
Arabic Language Resources and Tools, Cairo, Egypt.
Nizar Habash, Owen Rambow, and Ryan Roth. 2009b.
MADA+TOKAN: A toolkit for Arabic tokenization,
diacritization, morphological disambiguation, POS tag-
ging, stemming and lemmatization. In Proceedings of
the Second International Conference on Arabic Lan-
guage Resources and Tools. Cairo, Egypt.
179
Nizar Habash. 2010. Introduction to Arabic Natural
Language Processing. Morgan & Claypool Publishers.
Jan Hajic?, Alena B?hmov?, Eva Hajic?ov?, and Barbora
Vidov?-Hladk?. 2000. The Prague Dependency Tree-
bank: A three-level annotation scenario. In Anne
Abeill?, editor, Treebanks: Building and Using Parsed
Corpora. Kluwer Academic Publishers.
P?ter Hal?csy, Andr?s Kornai, and Csaba Oravecz. 2007.
HunPos ? an open source trigram tagger. In Proceed-
ings of ACL, pages 209?212, Prague, Czech Republic.
Johan Hall, Jens Nilsson, Joakim Nivre, G?ls?en Eryig?it,
Be?ta Megyesi, Mattias Nilsson, and Markus Saers.
2007. Single malt or blended? A study in multilingual
parser optimization. In Proceedings of the CoNLL
Shared Task Session of EMNLP-CoNLL 2007, pages
933?939, Prague, Czech Republic.
Chung-hye Han, Na-Rae Han, Eon-Suk Ko, Martha
Palmer, and Heejong Yi. 2002. Penn Korean Treebank:
Development and evaluation. In Proceedings of the
16th Pacific Asia Conference on Language, Information
and Computation, Jeju, Korea.
Tilman H?hle. 1986. Der Begriff "Mittelfeld", Anmerkun-
gen ?ber die Theorie der topologischen Felder. In Ak-
ten des Siebten Internationalen Germanistenkongresses
1985, pages 329?340, G?ttingen, Germany.
Zhongqiang Huang, Mary Harper, and Slav Petrov. 2010.
Self-training with products of latent variable grammars.
In Proceedings of EMNLP, pages 12?22, Cambridge,
MA.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of ACL,
pages 586?594, Columbus, OH.
Alon Itai and Shuly Wintner. 2008. Language resources
for Hebrew. Language Resources and Evaluation,
42(1):75?98, March.
Mark Johnson. 1998. PCFG models of linguistic tree
representations. Computational Linguistics, 24(4):613?
632.
Laura Kallmeyer and Wolfgang Maier. 2013. Data-driven
parsing using probabilistic linear context-free rewriting
systems. Computational Linguistics, 39(1).
Fred Karlsson, Atro Voutilainen, Juha Heikkilae, and Arto
Anttila. 1995. Constraint Grammar: a language-
independent system for parsing unrestricted text. Wal-
ter de Gruyter.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of ACL, pages
423?430, Sapporo, Japan.
Sandra K?bler, Erhard W. Hinrichs, and Wolfgang Maier.
2006. Is it really that difficult to parse German? In Pro-
ceedings of EMNLP, pages 111?119, Sydney, Australia,
July.
Sandra K?bler, Wolfgang Maier, Ines Rehbein, and Yan-
nick Versley. 2008. How to compare treebanks. In
Proceedings of LREC, pages 2322?2329, Marrakech,
Morocco.
Sandra K?bler. 2008. The PaGe 2008 shared task on
parsing German. In Proceedings of the Workshop on
Parsing German, pages 55?63, Columbus, OH.
Seth Kulick, Ryan Gabbard, and Mitch Marcus. 2006.
Parsing the Arabic Treebank: Analysis and Improve-
ments. In Proceedings of the Treebanks and Linguistic
Theories Conference, pages 31?42, Prague, Czech Re-
public.
Joseph Le Roux, Benoit Sagot, and Djam? Seddah. 2012.
Statistical parsing of Spanish and data driven lemmati-
zation. In Proceedings of the Joint Workshop on Statis-
tical Parsing and Semantic Processing of Morphologi-
cally Rich Languages, pages 55?61, Jeju, Korea.
Kong Joo Lee, Byung-Gyu Chang, and Gil Chang Kim.
1997. Bracketing Guidelines for Korean Syntactic Tree
Tagged Corpus. Technical Report CS/TR-97-112, De-
partment of Computer Science, KAIST.
Roger Levy and Christopher D. Manning. 2003. Is it
harder to parse Chinese, or the Chinese treebank? In
Proceedings of ACL, Sapporo, Japan.
Mohamed Maamouri, Ann Bies, Tim Buckwalter, and
Hubert Jin. 2004a. Arabic Treebank: Part 2 v 2.0.
LDC catalog number LDC2004T02.
Mohamed Maamouri, Ann Bies, Tim Buckwalter, and
Wigdan Mekki. 2004b. The Penn Arabic Treebank:
Building a large-scale annotated Arabic corpus. In
NEMLAR Conference on Arabic Language Resources
and Tools, pages 102?109, Cairo, Egypt.
Mohamed Maamouri, Ann Bies, Tim Buckwalter, and
Hubert Jin. 2005. Arabic Treebank: Part 1 v 3.0. LDC
catalog number LDC2005T02.
Mohamed Maamouri, Ann Bies, Seth Kulick, Fatma Gad-
deche, Wigdan Mekki, Sondos Krouna, and Basma
Bouziri. 2009. The Penn Arabic Treebank part 3 ver-
sion 3.1. Linguistic Data Consortium LDC2008E22.
Wolfgang Maier, Miriam Kaeshammer, and Laura
Kallmeyer. 2012. Data-driven PLCFRS parsing re-
visited: Restricting the fan-out to two. In Proceedings
of the Eleventh International Conference on Tree Ad-
joining Grammars and Related Formalisms (TAG+11),
Paris, France.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn TreeBank. Computational
Linguistics, 19(2):313?330.
Andre Martins, Noah Smith, Eric Xing, Pedro Aguiar,
and Mario Figueiredo. 2010. Turbo parsers: Depen-
dency parsing by approximate variational inference. In
Proceedings of EMNLP, pages 34?44, Cambridge, MA.
180
Yuval Marton, Nizar Habash, and Owen Rambow. 2013a.
Dependency parsing of Modern Standard Arabic with
lexical and inflectional features. Computational Lin-
guistics, 39(1):161?194.
Yuval Marton, Nizar Habash, Owen Rambow, and Sarah
Alkhulani. 2013b. SPMRL?13 shared task system:
The CADIM Arabic dependency parser. In Proceed-
ings of the Fourth Workshop on Statistical Parsing of
Morphologically-Rich Languages, pages 76?80, Seat-
tle, WA.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Effective self-training for parsing. In Proceed-
ings of HLT:NAACL, pages 152?159, New York, NY.
Ryan T. McDonald, Koby Crammer, and Fernando C. N.
Pereira. 2005. Online large-margin training of depen-
dency parsers. In Proceedings of ACL, pages 91?98,
Ann Arbor, MI.
Ryan McDonald, Joakim Nivre, Yvonne Quirmbach-
Brundage, Yoav Goldberg, Dipanjan Das, Kuzman
Ganchev, Keith Hall, Slav Petrov, Hao Zhang, Oscar
Tackstrom, Claudia Bedini, Nuria Bertomeu Castello,
and Jungmee Lee. 2013. Universal dependency anno-
tation for multilingual parsing. In Proceedings of ACL,
Sofia, Bulgaria.
Igor Mel?c?uk. 2001. Communicative Organization in Nat-
ural Language: The Semantic-Communicative Struc-
ture of Sentences. J. Benjamins.
Knowledge Center for Processing Hebrew
MILA. 2008. Hebrew morphological analyzer.
http://mila.cs.technion.ac.il.
Antonio Moreno, Ralph Grishman, Susana Lopez, Fer-
nando Sanchez, and Satoshi Sekine. 2000. A treebank
of Spanish and its application to parsing. In Proceed-
ings of LREC, Athens, Greece.
Joakim Nivre and Be?ta Megyesi. 2007. Bootstrapping a
Swedish treeebank using cross-corpus harmonization
and annotation projection. In Proceedings of the 6th
International Workshop on Treebanks and Linguistic
Theories, pages 97?102, Bergen, Norway.
Joakim Nivre, Jens Nilsson, and Johan Hall. 2006. Tal-
banken05: A Swedish treebank with phrase structure
and dependency annotation. In Proceedings of LREC,
pages 1392?1395, Genoa, Italy.
Joakim Nivre, Johan Hall, Sandra K?bler, Ryan McDon-
ald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret.
2007a. The CoNLL 2007 shared task on dependency
parsing. In Proceedings of the CoNLL Shared Task Ses-
sion of EMNLP-CoNLL 2007, pages 915?932, Prague,
Czech Republic.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev,
G?ls?en Eryig?it, Sandra K?bler, Svetoslav Marinov,
and Erwin Marsi. 2007b. MaltParser: A language-
independent system for data-driven dependency pars-
ing. Natural Language Engineering, 13(2):95?135.
Slav Petrov and Ryan McDonald. 2012. Overview of the
2012 Shared Task on Parsing the Web. In Proceedings
of the First Workshop on Syntactic Analysis of Non-
Canonical Language (SANCL), a NAACL-HLT 2012
workshop, Montreal, Canada.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of COLING-
ACL, Sydney, Australia.
Slav Petrov. 2009. Coarse-to-Fine Natural Language
Processing. Ph.D. thesis, University of California at
Bekeley, Berkeley, CA.
Slav Petrov. 2010. Products of random latent variable
grammars. In Proceedings of HLT: NAACL, pages 19?
27, Los Angeles, CA.
Adam Przepi?rkowski, Miros?aw Ban?ko, Rafa? L. G?rski,
and Barbara Lewandowska-Tomaszczyk, editors. 2012.
Narodowy Korpus Jkezyka Polskiego. Wydawnictwo
Naukowe PWN, Warsaw.
Ines Rehbein and Josef van Genabith. 2007a. Eval-
uating Evaluation Measures. In Proceedings of the
16th Nordic Conference of Computational Linguistics
NODALIDA-2007, Tartu, Estonia.
Ines Rehbein and Josef van Genabith. 2007b. Treebank
annotation schemes and parser evaluation for German.
In Proceedings of EMNLP-CoNLL, Prague, Czech Re-
public.
Ines Rehbein. 2011. Data point selection for self-training.
In Proceedings of the Second Workshop on Statistical
Parsing of Morphologically Rich Languages, pages 62?
67, Dublin, Ireland.
Kenji Sagae and Alon Lavie. 2006. Parser combination
by reparsing. In Proceedings of HLT-NAACL, pages
129?132, New York, NY.
Geoffrey Sampson and Anna Babarczy. 2003. A test of
the leaf-ancestor metric for parse accuracy. Natural
Language Engineering, 9(04):365?380.
Natalie Schluter and Josef van Genabith. 2007. Prepar-
ing, restructuring, and augmenting a French Treebank:
Lexicalised parsers or coherent treebanks? In Proc. of
PACLING 07, Melbourne, Australia.
Helmut Schmid, Arne Fitschen, and Ulrich Heid. 2004.
SMOR: A German computational morphology covering
derivation, composition and inflection. In Proceedings
of LREC, Lisbon, Portugal.
Djam? Seddah, Grzegorz Chrupa?a, Ozlem Cetinoglu,
Josef van Genabith, and Marie Candito. 2010.
Lemmatization and statistical lexicalized parsing of
morphologically-rich languages. In Proceedings of the
First Workshop on Statistical Parsing of Morphologi-
cally Rich Languages (SPMRL), Los Angeles, CA.
Wolfgang Seeker and Jonas Kuhn. 2012. Making el-
lipses explicit in dependency conversion for a German
181
treebank. In Proceedings of LREC, pages 3132?3139,
Istanbul, Turkey.
Hiroyuki Shindo, Yusuke Miyao, Akinori Fujino, and
Masaaki Nagata. 2012. Bayesian symbol-refined tree
substitution grammars for syntactic parsing. In Pro-
ceedings of ACL, pages 440?448, Jeju, Korea.
Anthony Sigogne, Matthieu Constant, and Eric Laporte.
2011. French parsing enhanced with a word clustering
method based on a syntactic lexicon. In Proceedings
of the Second Workshop on Statistical Parsing of Mor-
phologically Rich Languages, pages 22?27, Dublin,
Ireland.
Khalil Sima?an, Alon Itai, Yoad Winter, Alon Altmann,
and Noa Nativ. 2001. Building a tree-bank of Modern
Hebrew text. Traitement Automatique des Langues,
42:347?380.
Marek S?widzin?ski and Marcin Wolin?ski. 2010. Towards
a bank of constituent parse trees for Polish. In Pro-
ceedings of Text, Speech and Dialogue, pages 197?204,
Brno, Czech Republic.
Ulf Teleman. 1974. Manual f?r grammatisk beskrivning
av talad och skriven svenska. Studentlitteratur.
Lucien Tesni?re. 1959. ?l?ments De Syntaxe Structurale.
Klincksieck, Paris.
Reut Tsarfaty and Khalil Sima?an. 2010. Modeling mor-
phosyntactic agreement in constituency-based parsing
of Modern Hebrew. In Proceedings of the First Work-
shop on Statistical Parsing of Morphologically Rich
Languages (SPMRL), Los Angeles, CA.
Reut Tsarfaty, Djame Seddah, Yoav Goldberg, Sandra
K?bler, Marie Candito, Jennifer Foster, Yannick Vers-
ley, Ines Rehbein, and Lamia Tounsi. 2010. Statistical
parsing for morphologically rich language (SPMRL):
What, how and whither. In Proceedings of the First
workshop on Statistical Parsing of Morphologically
Rich Languages (SPMRL), Los Angeles, CA.
Reut Tsarfaty, Joakim Nivre, and Evelina Andersson.
2011. Evaluating dependency parsing: Robust and
heuristics-free cross-framework evaluation. In Pro-
ceedings of EMNLP, Edinburgh, UK.
Reut Tsarfaty, Joakim Nivre, and Evelina Andersson.
2012a. Cross-framework evaluation for statistical pars-
ing. In Proceeding of EACL, Avignon, France.
Reut Tsarfaty, Joakim Nivre, and Evelina Andersson.
2012b. Joint evaluation for segmentation and parsing.
In Proceedings of ACL, Jeju, Korea.
Reut Tsarfaty, Djam? Seddah, Sandra K?bler, and Joakim
Nivre. 2012c. Parsing morphologically rich languages:
Introduction to the special issue. Computational Lin-
guistics, 39(1):15?22.
Reut Tsarfaty. 2010. Relational-Realizational Parsing.
Ph.D. thesis, University of Amsterdam.
Reut Tsarfaty. 2013. A unified morpho-syntactic scheme
of Stanford dependencies. In Proceedings of ACL,
Sofia, Bulgaria.
Veronika Vincze, D?ra Szauter, Attila Alm?si, Gy?rgy
M?ra, Zolt?n Alexin, and J?nos Csirik. 2010. Hungar-
ian Dependency Treebank. In Proceedings of LREC,
Valletta, Malta.
Joachim Wagner. 2012. Detecting Grammatical Errors
with Treebank-Induced Probabilistic Parsers. Ph.D.
thesis, Dublin City University.
Marcin Wolin?ski, Katarzyna G?owin?ska, and Marek
S?widzin?ski. 2011. A preliminary version of
Sk?adnica?a treebank of Polish. In Proceedings of
the 5th Language & Technology Conference, pages
299?303, Poznan?, Poland.
Alina Wr?blewska. 2012. Polish Dependency Bank. Lin-
guistic Issues in Language Technology, 7(1):1?15.
Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
Proceedings of ACL:HLT, pages 188?193, Portland,
OR.
J?nos Zsibrita, Veronika Vincze, and Rich?rd Farkas.
2013. magyarlanc: A toolkit for morphological and
dependency parsing of Hungarian. In Proceedings of
RANLP, pages 763?771, Hissar, Bulgaria.
182
First Joint Workshop on Statistical Parsing of Morphologically Rich Languages
and Syntactic Analysis of Non-Canonical Languages, pages 39?53 Dublin, Ireland, August 23-29 2014.
Experiments with Easy-first nonprojective constituent parsing
Yannick Versley
Department of Computational Linguistics
University of Heidelberg
versley@cl.uni-heidelberg.de
Abstract
Less-configurational languages such as German often show not just morphological variation but
also free word order and nonprojectivity. German is not exceptional in this regard, as other
morphologically-rich languages such as Czech, Tamil or Greek, offer similar challenges that
make context-free constituent parsing less attractive.
Advocates of dependency parsing have long pointed out that the free(r) word order and non-
projective phenomena are handled in a more straightforward way by dependency parsing. How-
ever, certain other phenomena in language, such as gapping, ellipses or verbless sentences, are
difficult to handle in a dependency formalism.
In this paper, we show that parsing of discontinuous constituents can be achieved using easy-first
parsing with online reordering, an approach that previously has only been used for dependencies,
and that the approach yields very fast parsing with reasonably accurate results that are close to
the state of the art, surpassing existing results that use treebank grammars. We also investigate
the question whether phenomena where dependency representations may be problematic ? in
particular, verbless clauses ? can be handled by this model.
1 Introduction
Automatic syntactic parsing has been fruitfully incorporated into sytems for information extraction
(Miyao et al., 2008), question answering, machine translation (Huang and Chiang, 2007), among others,
but we also see syntactic structures being used to communicate facts about language use in the digital
humanities or in investigations of the language of language learners. In all of these applications, we see
fruitful use both of constituent trees, and of dependency trees.
Depending on the application, different criteria may become important: on one hand, the ability to
produce structures that are (intuitively) compatible with semantic composition, or where arguments and
adjuncts are related to their predicate in the tree, which commonly requires dealing with nonprojectivity.
Such a formalism should also deal with a wide range of constructions including verbless clauses. Finally,
parsing speed is somewhat important for many application cases, and a parser that changes the tokeniza-
tion of the input or inserts additional ?null? tokens runs afoul many of the fundamental assumptions in
pipelines for semantic processing or information extraction.
If we look at the current three largest treebanks for German, namely the Hamburg Dependency Tree-
bank (Foth et al., 2014) with 101 000 sentences, the Tu?Ba-D/Z treebank (Telljohann et al., 2009) with
85 000 sentences or the Tiger treebank (Brants et al., 2002) with about 50 000 sentences, we see find a
continuum of the nonprojective single-parent dependencies of the HDT on one side and projective phrase
structures of Tu?Ba-D/Z, with Tiger straddling in the middle with a scheme that is neither projective nor
limited to dependencies, and which represents, we?ll argue, both the best and the worst of both worlds.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings
footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
39
Because of its expressivity, the Negra/Tiger scheme has also been used for other languages such as
Swedish Volk and Samuelsson (2004) as well as Georgian/Russian/Ukrainian (Kapanadze, 2012), and as
Early New High German (Pauly et al., 2012).
The Tiger scheme is arguably more expressive than either of the alternatives since it can capture
both elliptic clauses (which are difficult to represent in normal dependency schemes) and nonprojective
constructions (which have to be added as a second annotation layer in purely projective treebanks such
as Tu?Ba-D/Z). It also makes it the most difficult to provide good automatic tool support, in terms of
effective parsing components or of annotation tools, since parsing of discontinuous constituents has only
recently become practical.
The straightforward approach of Kallmeyer and Maier (2013) to use a treebank-derived linear context-
free rewriting system suffers from near-exponential observed time consumption in practice. Approaches
that use context-free grammar approximation such as the ones of Schmid (2006), Cai et al. (2011) or
van Cranenburgh and Bod (2013), still have cubic time complexity; especially in the latter case, it is not
clear whether techniques that allow fast PCFG parsing such as those of Bodenstab et al. (2011) would be
suitable for the subsequent steps with increased grammar complexity.
In this paper, we present a novel application of the easy-first parsing principle of Goldberg and Elhalad
(2010) to discontinuous constituent parsing, which performs fast enough for interactive use (about 40
sentences per second) while giving an acceptable accuracy that is within the range normally seen with
unmodified treebank grammars.
In the remainder of the paper, we will include a short discussion of the interrelation between con-
stituency and dependency relations of syntax, as well as relevant prior work in section 2, and discuss the
construction of the parser in section 3. Section 4 and following contain a discussion of quantitative re-
sults on the Tiger corpus, whereas the penultimate section contains a more detailed analysis of the parser
behaviour on constructions that are problematic for either dependency parsers or projective constituent
parsing.
2 Constituency and Dependency: Good friends?
Constituency and dependency structures are two formalisms that are frequently used for theory-neutral
description of syntactic structures. In constituent structures, usually influenced by some version of X-
bar theory (see Kornai and Pullum, 1990 for a discussion; most notably, phrases are supposed to be
projections of a head), whereas in dependency structures it is usually assumed that each word has exactly
one governor (except one or more words that are attached to a virtual root node).
The common subset of both can be described (in the words of Hockenmaier, 2007) as ?Heads, argu-
ments, modifiers, conjuncts?, which includes the grammatical function labels that are added in depen-
dency structures, and to varying extent in phrase structure treebanks. Nivre (2011) goes further and asks
whether we need constituents at all, since pure dependency parsing recovers arguments and adjuncts
while being generally faster (and, at least for results published on Czech and French which Nivre cites,
more accurate). Versley and Zinsmeister (2006) similarly argue that even ?deep? dependency relations
(including nonlocal ones) can be recovered from single-parent dependencies if subsequent disambigua-
tion steps identify the scope of conjunctions, argument sharing in coordination, passive identification,
and lexicalized control phenomena. However, verbless clauses as they may occur in coordination pose
a problem to the idea that every phrase is headed by a preterminal, or the equivalent assumption in
dependency grammar that every argument has a governing head word.
In constituent treebanks, the solution to this problem is rather simple: deviate from the descriptive-
Xbar schema outlined earlier on and introduce headless projections for these clauses. Dependency tree-
banks lack this additional degree of freedom, and the choice is usually to either attach the respective
nodes somewhere else (Bo?hmova et al., 2001; Foth, 2006) or introduce empty nodes that are the gover-
nors of the orphaned subtrees (Bosco and Lombardo, 2006; Vincze et al., 2010; Dipper et al., 2013).
In dependency parsing, good solutions for nonprojective edges have been found, including pseudopro-
jective parsing (Nivre and Nilsson, 2005), approximate weighted constraint solving (Koo et al., 2010), as
well as deterministic online reordering (Nivre, 2009), which also has been applied to easy-first decoding
40
strategies (Tratz and Hovy, 2011). Seeker et al. (2012) additionally employs an attach-inner opera-
tion which allows non-projective insertion into a structure that has already been built. Despite these
very reasonable solutions, the treatment of elliptic phrases, whether it is done using the somewhere-else
approach or by introducing empty nodes (see Seeker et al., 2012 and references therein) yields unin-
formative structures for subsequent processing components or even makes it necessary to re-engineer
subsequent processing stages for dealing with the newly introduced empty nodes, or (equally impracti-
cal) require the refactoring of annotated corpus resources to accommodate a new tokenization whenever
a null element is introduced or changed.
In constituency parsing, the problem of discontinuous constituents in parsing has, at least in German,
first been met with a proposals of raising degrees of complexity (among others, van Noord, 1991; Plaehn,
2000) and then silently been ignored both in the building of parsers and in their evaluation: researchers
from Dubey and Keller (2003) to the present day cite bracketing scores based on structures that would
make the reconstruction of ?Heads, arguments, modifiers, and conjuncts? ? usually ? rather difficult.
Only relatively recently has the problem of discontinuous constituent parsing been tackled head-on.
Kallmeyer and Maier (2013) propose an approach that extracts a treebank LCFRS grammar, which is
then used for probabilistic parsing, albeit with near-exponential time consumption. Maier et al. (2012)
present an approach to make parsing in this approach more efficient by flattening coherent structures in
a sentence to one single sentence node and thus eliminating scrambling as a source of discontinuities,
together with other transformations, which allows a time complexity of O(n6) and parsing times of about
2 minutes for a 40-word sentence. van Cranenburgh and Bod (2013) use a more practical approach that
first creates phrase candidates from the n-best list of a projective constituent parser, and uses these to
construct LCFRS items that do not necessarily correspond to grammar rules seen in the training set, but
which are then matched against a collection of tree fragments extracted from the training set.
There exists some work on transforming dependency structures into constituents that may help in the
recovery of discontinuous constituents: Hall and Nivre (2008) propose to encode information about node
labels in the dependency labels, whereas Carreras et al. (2008) show that an ILP-based combination of
finding dependencies and adding phrase projections and adjunctions to a dependency backbone works
well for constructing structures matching those of the Penn Treebank. Seddah (2010) found that similar
spinal structures can be used for the French Treebank.
3 Incremental parsing
In general, statistical parsing follows one of several general approaches: one is the approach of item-
based decoding, which is centered around the creation of a parse forest that implicitly stores a very large
number of possible trees, followed by either dynamic programming in the case of projective parsing (e.g.
(Collins, 2003)) or techniques that provide an approximate or exact solution to the intractable problem
in the case of nonprojective parsing with second-order factors (Koo et al., 2010). The second large group
of approaches is based on incremental structure building, including the approaches of Magerman (1995)
or Sagae and Lavie (2006) in the case of constituent parsing, or of Nivre (2003) and following in the case
of dependency parsing, with approaches such as Stolcke (1995) or Huang and Sagae (2010) occupying a
middle ground.
While the idea of head lexicalization has played a large role in projective constituent parsing, there are
rather few approaches that attempt to bridge the gap between dependency and constituency representa-
tions in a way that could be exploited for the efficient building of discontinuous constituent structures.
Among these, both the approaches of Hall and Nivre (2008) and of Carreras et al. (2008) could be
described in terms of a spinal transform: each terminal in the input string is assigned a set of governing
nodes that form its spine; parsing then consists of assigning a dependency structure among the terminal
nodes and of assigning spines and the relation to each other.
In the remainder of this section, we describe two approaches that we used to perform nonprojective
constituent parsing in expected linear time: one is relatively close to the approach of Hall and Nivre
(2008), but instead of assigning nodes to the first terminal of their yield, uses a strategy more like the
spinal tree adjoining grammr of Carreras et al. (2008). The other is an application of the principle
41
Zum einen
on the one hand
PP
add?
ist er
he is
S
add?
au?ergewo?hnlich
extraordinarily
ADJD
popula?r
popular
[i:i+1] AP
ADJD
[i] AP
Figure 1: Example for an intermediate state in EaFi, with the preferred action candidates for each position
of easy-first parsing, which has been used for unlabeled dependency parsing by Goldberg and Elhalad
(2010), and for non-projective labeled dependency parsing by Tratz and Hovy (2011), towards discon-
tinuous constituency parsing. Because computed feature vectors can be memorized and only have to be
recomputed in a small window around the last parser action, this latter approach, just as a left-to-right
transition-based parser, has an expected time consumption that is linear in the number of words to be
parsed.
3.1 ADG: Constituency-to-Dependency Reduction
Our baseline is an approach close in spirit to Hall and Nivre (2008): The tree with node labels is turned
into a dependency graph that encodes, on the governor edge of each terminal, a combination of (i) the
node labels on the spine of this node, and (ii) the level at which this node attaches to its parent?s spine.
We change two parameters of Hall and Nivre?s approach: on one hand, we do not use the first terminal
in the yield of a node as its representative but the head according to the head table that we also use to
assign the head in the easy-first parser. The reason for this is a practical one: using the head, we get
a distribution of 531 different spine/level combinations when we use the head, whereas we would get
about 1525 categories when we use the first terminal.
To ensure efficient parsing, this list is further pared down to 100 entries, with the remaining entries
being replaced by an UNK placeholder. In decoding, terminals with these entries are assigned the most
frequent combination of spine and parent category for the POS tags of the node and its governor, and the
topmost spine node with a matching category (or simply the topmost one) would be chosen.
The decoding algorithm and parameter settings for MaltParser were then determined using the Malt-
Optimizer software (Ballesteros and Nivre, 2012). The settings selected use the stack-projective algo-
rithm with head+path marking strategy for pseudoprojective parsing.1
Hall and Nivre?s approach is more complex than the approach presented here, and involves interleaving
of identifying dependency edges (using the nonprojective Covington parsing scheme) and the stepwise
determination of the topmost edge label, then the path of edge labels, and finally the path of constituent
labels and its attachment. However, we find that this approach of dependency reduction constitutes a
very reasonable intelligent baseline, and is able to perform at a similar speed than our approach.
3.2 EaFi: Easy-first Constituency Parsing
The main approach that we will present here constitutes an adaptation of the Easy-First approach to
nonprojective constituent parsing. The parser keeps track of a sequence of nodes, beginning with the
terminals that are output by the preprocessing consisting of morphological analyzer and lemmatization,
and at each point applies one of several actions:
? Reduce-Unary: one node is grouped under a unary node of a given category, with the restriction that
the corresponding unary rule must have been observed in the treebank. (Additionally, we collapse
any two nodes with the same category embedding each other, which sometimes occurs in the Tiger
treebank when several empty-headed phrases are assumed to embed each other in coordination).
1MaltParser is able to do direct nonprojective parsing using the reordering approaches of Nivre (2009) and Nivre et al.
(2009), however the pseudoprojective approach was selected in MaltOptimizer?s parameter selection.
42
Basic featureset
Unigram: n ? ni?2 . . . ni+3 CnPn CnWn CnMn CnLn
Left/Right Children: n ? ni,Lni,Rni+1,Lni+1,R CnPn
Bigram: m,n ? ni?1ni . . . ni+2ni+3 WmWn WmCn CmWn WmWn
Trigram: r,m,n ? ni?1nini+1 . . . ni+1ni+2ni+3
Medium featureset
Bigram+Child: m,n, r ? {nini+1ni+1,L;nini+1ni,R;
ni+1ni+2ni+1,R;nini?1ni,L} CmCnCr CmCnPr
Distance: ? ? {dist(ni, ni+1),gap(ni, ni+1)} ?Cm ?Cn ?Pm ?Pn
m,n = ni, ni+1 ?Fm ?Fn
Large featureset
Gap bigram: m,n ? ni?1ni+2, nini+2 WmWn WmCn CmWn WmWn
Bigram+2child: m,n ? ni?1ni . . . ni+2ni+3;C,D ? L,R CmCnCmCCnD CmCnCmCPnD
CmCnPmCCnD
Bigram-Dist m,n ? ni?1ni . . . ni+2ni+3 ?
? = dist(m,n) ?PmPn
Table 1: Features used: W=word, C=phrase label, M=head morph, P=head pos
? Reduce-Binary: two nodes are grouped under one node, with the head of the new node being
determined by a head table.
? Add-Left/Add-Right: one node is added as a child to a node on its left/to its right.
? Swap: if two nodes are in surface order (i.e., the head of the first node being left of the head of the
second node in the normal word ordering), they can be swapped.
In addition, we experimented with a retag action which allows the parser to change the tag of a word
that has been mistagged. While this has a positive effect on the parser?s accuracy for verb phrases, it also
results in a slight deterioration of other phrases, resulting in a very slight decline in performance.
To decide among different parsing actions, the parser uses a linear classifier with a pre-defined feature
set (POS, word form, morphological tag and lemma in a two-token window around the two nodes that
are being considered, the category and part-of-speech tag of the leftmost and rightmost dependent of
the nodes that are being considered; bigrams of words, categories, and one of each, in a window of
one around the two nodes being considered, and trigrams consisting of two category and one category,
part-of-speech tag, or word form within said window).
Weights are learned by performing online learning with early stopping, similar to the strategy em-
ployed by Collins and Roark (2004). We use the Adaptive Gradients method (Duchi et al., 2011) for
weight updates and averaging of the weight vector (in a fashion identical to the averaged perceptron).
We found that 5-10 epochs of training on Tiger were sufficient to get a mostly usable model, and used
15 epochs of training for the results reported in the later section. Considering that Goldberg and Elhalad
(2010) use a learning strategy that performs multiple perceptron updates until the constraint violation is
fixed, we also tried this strategy but did not achieve convergence.
3.3 Reordering Oracles for Constituents
The basic idea for reordering oracles in deterministic dependency parsing has been presented by Nivre
(2009). In the following, we present a straightforward adapation of the idea to constituent trees.
Given a set of terminals T = {w1, . . . ,wn} that is totally ordered by a relation <, an unordered tree
graph is a directed graph (NT ?T,?) with nonterminal (NT) and terminal nodes (T), where the transitive
hull ?? of the parent relation ? is acyclic, no node has a parent from T , and exactly one node, vroot, has
no parent.
An node ordering ? is consistent with ? whenever, for any node u and an descendant u? ?? u, and a
node v with an descendant v? ?? v, u ? v entails u? ? v?.
43
A tree cut of a tree is a sequence v1, . . . , vn that contains exactly one node from each path vroot, . . .wi
from the root to a terminal. Nivre?s insight, applied to constituent structures, is that sorting the terminals
in a ?-compatible order ? will allow us to use normal projective parsing techniques to find a sequence of
reductions that parses this tree, since any needed reduction would reduce one ?-ordered cut to another?-ordered cut. In the following, two orderings ?, ?? are considered equivalent iff they only differ on pairs
of nodes u, v where one is the ancestor of the other.
Two subtrees under nodes u and v with yields yield(u) = {u? ? T ?u? ?? u} and yield(v) = {v? ?
T ?v? ?? v} are separated by a surface ordering < whenever any two terminals u? of u and v? of v fulfill
u? < v?. Note that two nodes without gaps (i.e. block-degree one) either embed each other (in which
case u ?? v or v ?? u is the case) or they are separated by <. In a slight abuse of notation, we extend< from a total order of the terminals to a partial order of the nonterminals by writing u < v whenever u
and v are separated. For projective trees, this extension of < specifies exactly one total relation (modulo
equivalence), and which is also ?-compatible.
For trees that are non-projective, we can have the situation where two nodes u and v are overlapping
in that u has descendants u?, u?? and v has a descendant v? with u? < v? < u??. Then we cannot extend <
to an ordering of nodes that is ?-compatible. However, we can always find an ordering that respects <
locally such that, for two children u? and u?? of u, u? < u?? entails u? ? u??. Nivre proposes the sequence
assigned by an in-order traversal of the dependency tree. In our case, any function h ? NT ? (NT ? T )
that assigns a ?head? child to each node will do the same, with an extension h?(w) = w for all terminals
and h?(v) = h?(h(v)) otherwise, through u ? v ?? h?(u) ? h?(v).2
A transition sequence for parsing a tree is then a sequence consisting of reductions (leading from
a cut . . . vi, u?, . . . u??, vj , . . . with a contiguous subsequence of the children of u to the sequence
. . . vi, u, vj , . . . that contains u instead) and swaps (leading from a cut . . . vi, vj . . . that has h?(vi) and
h?(vj) ordered with respect to < but not with respect to ? to a cut . . . vj , vi . . . that is orders h?(vi) and
h?(vj) with respect to ? but not <).
Nivre (2009) defines an oracle for shift-reduce parsing that is swap-eager in that it always allows
swapping. In Nivre?s case, the oracle is deterministic and always performs the swapping before any
reduction.
Nivre et al. (2009) note that the swap-eager oracle performs too many swaps because it swaps groups
of words that are later reduced. They propose a swap-lazy algorithm that does not swap two nodes if
one of them is adjacent to another node that is within the same maximal projective subtree.
The perspective of parsing as a series of swap and reduce actions allows us to specify a strategy that
performs less reductions in some cases: Consider that we need to reorder the <-contiguous sequence of
terminals to the ?-contiguous sequence that is needed for reducing the tree to its final form. The number
of swaps performed, if we assume that we always swap adjacent constituents, is exactly equal to the
number of terminal pairs vi, vj that are <-ordered but not ?-ordered. Any reduction of the number of
swaps relative to this baseline will come from a group of nodes with heads vi1, . . . vik that are reduced to
their parent vi before being swapped with a node vj .
We can take advantage of this fact by using any node with blockdegree one as a barrier: no node that
is a descendant of this node can be swapped with a node that is not a descendant before the reduction
that results in the barrier node has been carried out. Because any projective subtree has all nodes as
barrier nodes, any pair of nodes whose swapping is delayed by the swap-lazy approach will be kept from
swapping by a barrier. Conversely, nodes with a block-degree of one can also occur higher-up in the tree
(e.g. as clause or sentence nodes), in which case they can act as a barrier even when their subtrees are
not projective.
4 Quantitative Evaluation
In order to evaluate our approach, we used the Tiger treebank, with the split used in the SPMRL?2013
shared task (about 40 000 training sentences and 5 000 development and test sentences each; see also
2Note that the concrete choice of h is quite arbitrary: we could take the actual head child, but also the first or last child of a
node.
44
` ? 30 ` ? 40
F1 F1 LA EX NP PP VP
EaFi: Preprocessing (large, barrier, noretag)
gold 77.95 76.64 92.17 41.71 75.0 82.8 56.6
marmot 75.51 73.97 91.08 38.48 72.7 81.3 48.3
pred 74.71 73.18 90.81 37.67 72.1 80.6 48.9
ADG, marmot preprocessing
marmot 73.42 72.24 90.95 33.77 68.0 77.4 52.1
EaFi: Train projective, evaluate on real data
gold 76.86 75.50 92.13 38.38 74.4 81.7 48.2
marmot 74.43 72.98 91.20 36.52 72.1 79.8 42.6
pred 73.75 72.32 90.72 35.55 71.8 79.2 42.4
EaFi: Train projective, evaluate on projective
gold 79.95 78.59 93.40 44.20 76.1 83.0 68.7
marmot 77.00 75.64 92.38 40.79 73.8 81.1 59.1
pred 76.25 74.94 91.87 39.60 73.5 80.5 58.4
Table 2: Results on SPMRL?13-dev (German, Tiger treebank) with varying preprocessing
` ? 30 ` ? 40
F1 F1 LA EX NP PP VP
EaFi: Feature set (barrier, noretag)
basic 70.26 68.60 89.03 34.03 69.1 77.1 40.1
medium 73.31 71.75 90.13 36.22 70.5 79.9 45.8
large 74.71 73.18 90.81 37.67 72.1 80.6 48.9
EaFi: Reordering (large, noretag)
eager 73.33 71.66 90.33 37.43 71.7 80.6 47.8
lazy 74.85 73.37 90.85 38.08 72.2 80.8 49.0
barrier 74.71 73.18 90.81 37.67 72.1 80.6 48.9
EaFi: Tag correction (large, barrier)
noretag 74.71 73.18 90.81 37.67 72.1 80.6 48.9
retag 74.62 73.16 90.83 37.51 71.5 80.3 49.4
Table 3: Results on SPMRL?13-dev (German, Tiger treebank) with pred preprocessing
45
Seddah et al., 2013 for a more extensive description), with the state-of-the-art preprocessing results
for part-of-speech and morphological tags3 which were produced by Bjo?rkelund et al. (2013) using the
MarMoT tagger (Mu?ller et al., 2013), in addition to the gold-standard preprocessing (gold) and automatic
predictions (pred) that are part of the official dataset of the SPMRL shared task.
We applied two transformations to the data, which are automatically reversed in the parser output:
one is adding NPs into PPs, which is also done by Seeker et al. (2012), and the other is that we make
parenthetical material subordinate to its embedding clause, as Maier et al. (2012) also advocate.
Evaluation was performed using the evaluator from the DISCODOP package of van Cranenburgh and
Bod (2013), excluding punctuation and the ROOT label added by disco-dop from the evaluation. Train-
ing was run for 15 epochs. Parsing the 5000 development sentences took about 90-120 seconds for
EAFI, which corresponds to 40-55 sentences per second (on a Core i7 2GHz) and is slightly faster than
MaltParser using the ADG-derived model and a LibLinear classifier.
In the results in table 2, we see the results for the dependency-to-constiuents approach, as well as for
the easy-first parsing with different reordering heuristics. As in Nivre et al. (2009), we notice that the
lazy strategy that keeps projective constituents together yields better results than the eager strategy which
allows moving right away. The overall results ? around 76.6% f-score on gold tags and 73.1% f-score on
predicted tags in sentences of 40 words and below ? indicate the promise of this approach, even though
they are significantly below the results of van Cranenburgh and Bod (2013) who achieve more than 78%
f-measure using predicted tags on a different split of the Tiger treebank. Van Cranenburgh?s approach is
about 15-20 times slower than ours, using 10 seconds for a 40-word sentence.
For informative purposes, we also included results for projective parsing in table 2, using a conversion
that first attaches punctuation and then projectivizes the tree by detaching non-head children.4 Compar-
ing the nonprojective parser and a variant that was trained on the projectivized version of the dataset,
we see that the projective parser is about 1-2 percent worse than the nonprojective one, corresponding
to our intuition that the reordering part improves the parsing on average. We also see that the projective
evaluation yields an estimate of parser performance that is substantially more optimistic than evaluating
on the original treebank.
4.1 Comparison with Related work
Tables 4 and 5 show previous results for discontinous constituent parsing on the Tiger and Negra tree-
banks. The current best results on the Tiger treebank have been achieved by van Cranenburgh and Bod
(2013), whose approach yields 78.8% Parseval F1 measure on the Tiger treebank in the split by Hall and
Nivre (2008), and 76.8% on the Negra treebank, in both cases with above 40% of exact matches among
the sentences of up to 40 words. Kallmeyer and Maier (2013) only report results on shorter sentences in
Negra for their approach using a modified treebank LCFRS. They achieve 75.6% on sentences of up to
30 words.
A recent approach that attempts to speed up discontinuous constituent parsing is the one by Angelov
and Ljunglo?f (2014), whose parser takes about 100 seconds for a length-40 sentence, which can be
reduced to 10 seconds for a length-40 sentence with an approximate search strategy. For sentences
between 5 and 60 tokens, their approach reaches an F1 score of 69.3%, which however deteriorates
quickly when approximate search is used, to 61.9% F1 in the latter case.
It is quite evident that pushing for more speed in these formalisms forcibly leads to a deterioration
in the quality of the results. As such, we think that the speed/quality tradeoff achieved in our system is
quite useful.
5 Qualitative Analysis
In the following, we will provide a categorization of the phenomena concerning verbless clauses on
one hand, and discontinuous constituents on the other. Table 6 contains a breakdown on these types of
3Data from http://www.cis.lmu.de/?muellets/marmot/marmot_spmrl.tar.bz2, version with file datesof June 13th 2014. See http://code.google.com/p/cistern/wiki/marmotSPMRL
4The SPMRL shared task dataset is idiosyncratic in that it deprojectivizes before attaching punctuation, which leads to a
result that is rather dissimilar to the original treebank.
46
` ? 30 ` ? 40
F1 EX F1 EX
Hall and Nivre (2008), golda ? ? 79.93 37.78
Hall and Nivre (2008), preda ? ? 75.33 32.63
van Cranenburgh and Bod (2013), preda ? ? 78.8 40.8
This work, golda 76.47 40.61 74.23 37.32
Maier (2010), LCFRS goldc 73.43 29.87 ? ?
Maier (2010), CFG goldc 75.57 31.80 ? ?
This work, goldb 77.95 43.81 76.64 41.71
This work, gold, eval w/ ROOTbc 81.13 43.81 79.80 41.71
a) Hall&Nivre split b) SPMRL split c) includes the ROOT node in the evaluation
Table 4: Previous results on the Tiger treebank
` ? 30 ` ? 40
F1 EX F1 EX
Maier (2010), LCFRS goldc 71.52 31.65 ? ?
Maier (2010), CFG goldc 74.04 33.43 ? ?
van Craenburgh (2012), LCFRS, gold ? ? 67.26 27.90
van Craenburgh (2012), Disco-DOP, gold ? ? 72.33 33.16
Maier et al. (2012) 74.5 ? ? ?
Kallmaier and Maier (2013), LCFRS, gold 75.75 ? ? ?
van Cranenburgh and Bod (2013), gold ? ? 76.8 40.5
c) includes the ROOT node in the evaluation
Table 5: Previous results on the NeGra treebank
phenomena according to whether they are:
? correctly parsed (+): when the incredients for the construction are present in the parse and they are
combined in a suitable fashion.
? missed (o): when the ingredients for the construction are present, but combined in another way ?
for example, an extraposition where the extraposed item is misattached
? broken (-): when the ingredients for the construction are not present and the parse has a completely
different structure.
Many of the same categories are discussed by Seeker and Kuhn (2012), who only discuss examples,
and by Maier et al. (2014), who published a list of sentence numbers for each phenomenon that is,
however, disjoint with the development portion considered here. Although the distinctions between
?missed? and ?broken? analyses are somewhat subjective, we think that it is still informative in the sense
that it helps to compare the relative difficulty of the problems involved.
5.1 Types of Verbless Clauses
In their conversion Seeker and Kuhn (2012) found 3 035 sentences that contain at least one empty node
in the Tiger treebank, or about one every 16 sentences. While this phenomenon may be more frequent in
spontaneously-produced text such as it may occur in user-generated content, it is still quite frequent.
Seeker et al. only distinguish among edge labels, followed by a guess on the clause type that they need
in order to place the inserted null element.
In this work, we will concentrate on verbless VP and S nodes, with rougly three categories:
The first consists of verbless copula clauses that mostly occur at top level,5 and where the most
obvious way to build a complete clause would be to add a be copula to the clause.
5sentences 40499, 41442, 41468, 41676, 41682, 41736, 41743, 42566, 42606, 42738
47
ADG/marm large/pred
+ o - + o -
copula clauses 3 4 3 2 5 3
gapping/ellipsis 0 4 6 0 4 6
parentheticals 1 6 4 0 4 6
extraposition 1 7 2 0 7 3
scrambling 3 2 4 3 1 5
topicalization 3 6 1 4 4 2
+) construction parsed ok, o) construction missed, -) broken parse
Table 6: Qualitative analysis: Counts for ok/missed/broken examples
A second group consists of clauses with gapping/ellipsis which occur in a coordinated structure, but
do not have a verb of their own.6 Such cases can occur with a final constituent in clause coordination as
well as with a non-final constituent in verb-last clauses:
(1) a. Die
the
Anstalt
institution
soll
shall
[Anfang
start
1998
1998
noch
still
1200
1200
Bescha?ftigte]
employees
und
and
[ein
one
Jahr
year
spa?ter
later
600
600
za?hlen].
count.
?The institution will count 1200 employees at the start of 1998 and one year later, 600?.
b. [Die
the
Zahl
number
der
of
Urlaubsreisen
holiday trips
im
in
Inland
interior
fiel
fell
laut
according to
Scho?rcher
Scho?rcher
um
by
zwei
two
Prozent]
percent
und
and
[damit
hence
nicht
not
mehr
anymore
so
as
stark
strong
wie
as
im
in the
Vorjahreszeitraum]
previous year period
.
?The national number of holiday trips fell by two percent according to Schorcher, and hence
not as strongly anymore as in the corresponding period from last year?.
Finally, we have parentheticals, which are rather rather similar to the examples listed under verbless
copula clauses, except that they occur as parenthetical material in a larger clause rather than by them-
selves.7
5.2 Types of Non-projectivity Phenomena
For the purpose of this paper, we will make a three-way distinction in the phenomena that create discon-
tinuities, according to the following questions:
? If we serialize the sorted (sub)tree, would the result yield a grammatical sequence? Or, to ask a
related question, would anything be missing if we kept only the continuous block of the head?
? If we flatten the tree by introducing a common ordering domain for multiple heads (which would
be the result of tree flattening as proposed by Uszkoreit, 1987 or of a common argument list as
advocated by Hinrichs and Nakazawa, 1989; flattening the sentence is also the solution used in the
German LFG grammar of Forst, 2007), would we have gotten rid of the problem?
Making these distinctions gives us three rather large categories that we can use to classify nonprojec-
tivity phenomena:
Extraposition8 is phenomenon where the sorted subtree would (usually) be grammatical, and where
the continuous part only would (usually) be acceptable:
6sentences 40698, 40788, 40836, 41003, 41174, 41218, 41356, 41399, 41544, 41665
7sentences 40698, 40749, 40861, 40894, 40899, 40924, 41219, 41267, 41437, 41443
8Sentences 40506, 40507, 40517, 40528, 40567, 40583, 40589, 40594, 40622, 40672
48
(2) a. Ele
Ele
hat
has
mir
me
[ein
a
Buch]
book
geschenkt
given
[u?ber
about
die
the
Savanne].
savannah.
?Ele gave me a book about the savannah?.
b. Ich
I
habe
have
[Ele]
Ele
ein
a
Buch
book
geschenkt
given
[und
and
Susi].
Susi.
?I gave a book to Ele and Susi?.
c. Heute
Today
ist
is
[der
the
Staubsauger]
vacuum cleaner
gekommen,
come,
[den
which
Du
you
bestellt
ordered
hast].
have.
?Today, the vacuum cleaner that you ordered came.?
Note that extraposition can be arbitrarily deep, as NPs can embed each other recursively:
(3) a. Heute
Today
ist
is
(die
the
Rechnung
invoice
fu?r
for
[den
the
Staubsauger])
vacuum cleanerj
gekommen,
come,
([den
whichj
Du
you
bestellt
ordered
hast]).
have.
?Today, the invoice for the vacuum cleaner you ordered came?
b. Ich
I
habe
have
(die
the
Rechnung
invoicei
fu?r
for
[den
the
Staubsauger])
vacuum cleaner
gefunden,
found,
(die
whichi
Du
you
vermi?t
missed
hast).
have.
?I found the invoice for the vacuum cleaner which you were missing?
Scrambling9 is the effect that occurs when two verbs that both have arguments are in the same clause,
and share the ordering domain in that clause, have crossing argument dependencies:
(4) . . . da?
. . . that
(dem
the
Kunden)
customer
[den
the
Ku?hlschrank]
fridge
bisher
until now
noch
yet
niemand
nobody
[zu
to
reparieren]
repair
zu
to
versuchen
try
(versprochen)
promised
hat.
has.
?that no one has promised the customer to try repairing the fridge.?
The example above is due to Becker et al. (1992), who claim that there is no bound on the distance over
which each element can scramble, nor a bound on the number of unbounded dependencies that can occur
in one sentence. Becker et al. further claim that no LCFRS can faithfully represent a sequence of m
verbs which are each preceded by one argument, where the arguments can be permuted freely.10
Finally, Topicalization11 or more generally V2-order phenomena are those where a part of the verb
clause (either an argument, or an argument of a phrase within the verb clause, or part of the verb clause
itself) is moved into clause-initial position.
(5) a. Ein
a
Buch
book
u?ber
about
die
the
Savanne
savannah
hat
has
Ele
Ele
mir
me
geschenkt.
given.
?Ele gave me a book about the savannah.?
b. U?ber
about
die
the
Savanne
savannah
hat
has
mir
me
Ele
Ele
ein
a
Buch
book
geschenkt.
given.
?Ele gave me a book about the savannah.?
c. Ein
a
Buch
book
geschenkt
given
hat
has
Ele
Ele
mir.
me.
?Give me a book, Ele did.?
9Sentences 40524, 40567, 40572, 40588, 40594, 40595, 40601, 40885, 40966, 41025
10The practical consequence is that any LCFRS extracted from a treebank will either underspecify the dependencies in such
a construction ? this is the flattening solution ? or yield rules with growing block-degree. van Cranenburgh (2012) shows that
the sentences with up to 25 words in Negra can be parsed with an LCFRS that leads to O(n9) time complexity when a suitable
binarization is used, where the original treebank grammar would mean a parsing complexity of O(n19). Maier et al. (2012)
point out that the observed time complexity of the arbitrary-block-degree parser used by Maier (2010) and Kallmeyer and Maier
(2013) is due to necessary bookkeeping, and that their variant with fixed block degree yields a polynomial time complexity.
11Sentences 40513, 40521, 40528, 40544, 40546, 40548, 40551, 40572, 40580, 40585
49
5.3 Analysis of Parser behaviour
Using the movement actions, the parser is able to correctly attach topicalized nodes in simple sentences,
and to sort out in most cases which nodes belong to the VP and which ones to the S node. In the presence
of complex sentence structure, the very local view on the sentence that the parser has quickly becomes
a hindrance. Extraposed material is attached correctly in the case of relative clauses, whereas infinitival
constructions (which can plausibly attach to the verb) are often missed, and clauses that are extraposed
modifiers of adverbs or adjectives are mostly missed. As with early treebank-based parsers, the presence
of multiple verbs (as in coherent constructions) can mislead the parser into assuming a more complex
structure than is actually present.
In general, verbless copula clauses, asyndetic coordination, and gapping/ellipsis, which are difficult
for dependency parsing, are also especially prone to confuse the very local view of the easy-first parser,
which is a rather anticlimactic, yet commonsensical conclusion.
In summary, simple material is often handled surprisingly well, whereas sentences with a complex
topological structure ? i.e., coordination, clauses embedded in a nominal phrase, or correlations, are
rather challenging for easy-first parsing. Parsing algorithms with more context such as Sartorio et al.
(2013) or an application of beam search might help in some of these cases.
6 Summary and Future Work
In this article, we presented a deterministic parser that uses an easy-first strategy to perform non-
projective constituent parsing in expected linear time, with results that perform in a similar range as
results for discontinuous treebank grammars, and provides a means to provide rather fast parsing in
cases where discontinuous structure is required. We introduced the barrier formulation as an alternative
to the lazy reordering of Nivre et al. (2009), which shows similar performance but which may reveal a
closer connection to formalisms with restricted discontinuities.
While all experiments and the phenomen-oriented analysis have been performed on German data, the
reordering oracle approach does not make any language-specific assumptions and constitutes a general
technique for deterministic parsing of discontinuous constituent trees.
Acknowledgements The author would like to thank the three anonymous reviewers for their valuable
comments, and Thomas Mu?ller for providing the Marmot-tagged version of the SPMRL dataset.
References
Angelov, Krasimir and Peter Ljunglo?f. 2014. Fast statistical parsing with multiple context-free grammars.
In Proceedings of EACL 2014.
Ballesteros, Miguel and Joakim Nivre. 2012. MaltOptimizer: A system for MaltParser optimization.
In Proceedings of the Eigth International Conference on Language Resources and Evaluation (LREC
2012).
Becker, Tilman, Owen Rambow, and Michael Niv. 1992. The derivational generative power, or, scram-
bling is beyond LCFRS. Technical report, University of Pennsylvania. A version of this paper was
presented at MOL3, Austin, Texas, November 1992.
Bjo?rkelund, Anders, O?zlem C?etinoglu, Richard Farkas, Thomas Mu?ller, and Wolfgang Seeker. 2013.
(re)ranking meets morphosyntax: State-of-the-art results from the SPMRL 2013 shared task. In Proc.
SPMRL 2013.
Bodenstab, Nathan, Aaron Dunlop, Keith Hall, and Brian Roark. 2011. Adaptive beam-width prediction
for efficient CYK parsing. In Proceedings of ACL/HLT 2011.
Bo?hmova, A., Jan Hajic?, Eva Hajic?ova?, and B. Hladka?. 2001. The Prague dependency treebank: Three-
level annotaion scenario. In Treebanks: Building and using syntactically annotated corpora, Kluwer
Academic Publishers, pages 103?127.
Bosco, Cristina and Vincenzo Lombardo. 2006. Comparing linguistic information in treebank annota-
tions. In LREC 2006.
50
Brants, Sabine, Stefanie Dipper, Silvia Hansen, Wolfgang Lezius, and George Smith. 2002. The TIGER
treebank. In Proc. TLT 2002.
Cai, Shu, David Chiang, and Yoav Goldberg. 2011. Language-independent parsing with empty elements.
In Proceedings of ACL 2011.
Carreras, Xavier, Michael Collins, and Terry Koo. 2008. TAG, dynamic programming, and the perceptron
for efficient, feature-rich parsing. In Proceedings of CoNLL.
Collins, Michael. 2003. Head-Driven statistical models for Natural Language parsing. Computational
Linguistics 29(4):589?637.
Collins, Michael and Brian Roark. 2004. Incremental parsing with the perceptron algorithm. In ACL-04.
Dipper, Stefanie, Anke Lu?deling, and Marc Resnicek. 2013. NoSta-D: A corpus of german non-standard
varieties. In Marcos Zampieri and Sascha Diwersy, editors, Non-standard DataSources in Corpus-
based Research, Shaker Verlag.
Dubey, Amit and Frank Keller. 2003. Probabilistic parsing for German using sister-head dependencies.
In ACL?2003.
Duchi, John, Elad Hazan, and Yoram Singer. 2011. Adaptive subgradient methods for online learning
and stochastic optimization. Journal of Machine Learning Research 12:2121?2159.
Forst, Martin. 2007. Filling statistics with linguistics - property design for the disambiguation of German
LFG parses. In ACL 2007 workshop on deep linguistic processing.
Foth, Kilian. 2006. Eine umfassende Dependenzgrammatik des Deutschen. Technical report, Fachbe-
reich Informatik, Universita?t Hamburg.
Foth, Kilian, Arne Ko?hn, Niels Beuck, and Wolfgang Menzel. 2014. Because size does matter: The
Hamburg dependency treebank. In Proceedings of the Language Resources and Evaluation Confer-
ence (LREC 2014).
Goldberg, Yoav and Michael Elhalad. 2010. An efficient algorithm for easy-first non-directional depen-
dency parsing. In Proceedings of NAACL-2010.
Hall, Johann and Joakim Nivre. 2008. Parsing discontinuous phrase structure with grammatical func-
tions. In Proceedings of the 6th International Conference on Natural Language Processing (GoTAL
2008).
Hinrichs, Erhard and Tsuneko Nakazawa. 1989. Flipped out: AUX in German. In Papers from the 25th
Annual Regional Meeting of the Chicago Linguistic Society.
Hockenmaier, Julia. 2007. CCG grammar extraction from treebanks: translation algorithms and applica-
tions. Presentation from the Treebank Workshop, 2007, Rochester NY.
Huang, Liang and David Chiang. 2007. Forest rescoring: Faster decoding with integrated language
models. In Proceedings of ACL 2007.
Huang, Liang and Kenji Sagae. 2010. Dynamic programming for linear-time incremental parsing. In
Proceedings of ACL 2010.
Kallmeyer, Laura and Wolfgang Maier. 2013. Data-driven parsing using probabilistic linear context-free
rewriting systems. Computational Linguistics 39:87?119.
Kapanadze, Oleg. 2012. Building parallel treebanks for the lesser-resourced languages. Technical report,
Tbilisi State University.
Koo, Terry, Alexander M. Rush, Michael Collins, Tommi Jaakkola, and David Sontag. 2010. Dual
decomposition for parsing with non-projective head automata. In Proceedings of EMNLP 2010.
Kornai, Andras and Geoff Pullum. 1990. The X-bar theory of phrase structure. Language 66:24?50.
Magerman, David M. 1995. Statistical decision-tree models for parsing. In ACL?1995.
Maier, Wolfgang. 2010. Direct parsing of discontinuous constituents in German. In Proceedings of the
NAACL-HLT First Workshop on Statistical Parsing of Morphologically Rich Languages.
51
Maier, Wolfgang, Miriam Kaeshammer, Peter Baumann, and Sandra Ku?bler. 2014. Discosuite ? a parser
test suite for German discontinuous structures. In Proceedings of LREC 2014.
Maier, Wolfgang, Miriam Kaeshammer, and Laura Kallmeyer. 2012. PLCFRS parsing revisited: Re-
stricting the fan-out to two. In Proceedings of the 11th International Workshop on Tree Adjoining
Grammar and Related Formalisms (TAG+11).
Miyao, Yusuke, Rune S?tre, Kenji Sagae, Takuya Matsuzaki, and Jun?ichi Tsujii. 2008. Task-oriented
evaluation of syntactic parsers and their representations. In ACL 2008.
Mu?ller, Thomas, Helmut Schmid, and Hinrich Schu?tze. 2013. Efficient higher-order CRFs for morpho-
logical tagging. In Proceedings fo EMNLP 2013.
Nivre, Joakim. 2003. An efficient algorithm for projective dependency parsing. In 8th International
Workshop on Parsing Technologies.
Nivre, Joakim. 2009. Non-projective dependency parsing in expected linear time. In Proc. Joint ACL-
AFNLP 2009.
Nivre, Joakim. 2011. Bare-bones dependency parsing - a case for occam?s razor? In Nodalida 2011.
Nivre, Joakim, Marco Kuhlmann, and Johan Hall. 2009. An improved oracle for dependency parsing
with online reordering. In Proceedings of the 11th International Conference on Parsing Technologies
(IWPT).
Nivre, Joakim and Jens Nilsson. 2005. Pseudo-projective dependency parsing. In Proceedings of ACL
2005.
Pauly, Dennis, Ulyana Senyuk, and Ulrike Demske. 2012. Strukturelle Mehrdeutigkeit in
fru?hneuhochdeutschen Texten. Journal for Language Technology and Computational Linguistics
27(2):65?82.
Plaehn, Oliver. 2000. Computing the most probable parse for a discontinuous phrase structure grammar.
In Proceedings of the 6th International Workshop on Parsing Technologies.
Sagae, Kenji and Alon Lavie. 2006. A best-first probabilistic shift-reduce parser. In Proceedings of the
Human Language Technology Conference of the NAACL (NAACL/HLT 2006).
Sartorio, Francesco, Giorgio Satta, and Joakim Nivre. 2013. A transition-based dependency parser using
a dynamic parsing strategy. In Proceedings of ACL 2013.
Schmid, Helmut. 2006. Trace prediction and recovery with unlexicalized PCFGs and slash features. In
Proceedings of COLING-ACL 2006.
Seddah, Djame?. 2010. Exploring the spinal-tig model for parsing French. In Proceedings of LREC 2010.
Seddah, Djame?, Reut Tsarfaty, Sandra Ku?bler, Marie Candito, Jinho D. Choi, Richa?rd Farkas, Jennifer
Foster, Iakes Goenaga, Koldo Gojenola Galletebeitia, Yoav Goldberg, Spence Green, Nizar Habash,
Marco Kuhlmann, Wolfgang Maier, Joakim Nivre, Adam Przepio?rkowski, Ryan Roth, Wolfgang
Seeker, Yannick Versley, Veronika Vincze, Marcin Wolin?ski, Alina Wro?blewska, and Eric Villemonte
de la Clergerie. 2013. Overview of the SPMRL 2013 Shared Task: A Cross-Framework Evaluation
of Parsing Morphologically Rich Languages. In Proceedings of the Fourth Workshop on Statistical
Parsing of Morphologically-Rich Languages. Seattle, WA, pages 146?182.
Seeker, Wolfgang, Richa?rd Farkas, Bernd Bohnet, Helmut Schmid, and Jonas Kuhn. 2012. Data-driven
dependency parsing with empty heads. In Proceedings of Coling 2012.
Seeker, Wolfgang and Jonas Kuhn. 2012. Making ellipses explicit in dependency conversion for a ger-
man treebank. In Proceedings of the Eight International Conference on Language Resources and
Evaluation (LREC?12).
Stolcke, Andreas. 1995. An efficient probabilistic context-free parsing algorithm that computes prefix
probabilities. Computational Linguistics 21(2):165?201.
52
Telljohann, Heike, Erhard W. Hinrichs, Sandra Ku?bler, Heike Zinsmeister, and Kathrin Beck. 2009.
Stylebook for the Tu?bingen Treebank of Written German (Tu?Ba-D/Z). Technical report, Seminar fu?r
Sprachwissenschaft, Universita?t Tu?bingen.
Tratz, Stephen and Eduard Hovy. 2011. A fast, accurate, non-projective, semantically-enriched parser. In
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing (EMNLP
2011).
Uszkoreit, Hans. 1987. Word order and constituent structure in German. Number 8 in CSLI Lecture
Notes. Center for the Study of Language and Information.
van Cranenburgh, Andreas. 2012. Efficient parsing with linear context-free rewriting systems. In EACL
2012.
van Cranenburgh, Andreas and Rens Bod. 2013. Discontinuous parsing with an efficient and accurate
DOP model. In Proceedings of the International Conference on Parsing Technologies (IWPT 2013).
van Noord, Geertjan. 1991. Head corner parsing for discontinuous constituency. In Proceedings of ACL
1991.
Versley, Yannick and Heike Zinsmeister. 2006. From dependency parsing to deep(er) semantics. In
Proceedings of the Fifth International Workshop on Treebanks and Linguistic Theories (TLT 2006).
Vincze, Veronika, Do?ra Szauter, Attila Alma?si adn Gyo?rgy Mo?ra, Zolta?n Alexin, and Ja?nos Csirik. 2010.
Hungarian dependency treebank. In Proceedings of the Seventh Conference on International Language
Resources and Evaluation (LREC 2010).
Volk, Martin and Yvonne Samuelsson. 2004. Bootstrapping parallel treebanks. In Proceedings of the
5th International Workshop on Linguistically Interpreted Corpora (LINC) at Coling 2004.
53
