Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 267?276, Prague, June 2007. c?2007 Association for Computational Linguistics
Exploiting Multi-Word Units in History-Based Probabilistic Generation
Deirdre Hogan, Conor Cafferkey, Aoife Cahill? and Josef van Genabith
National Centre for Language Technology
School of Computing, Dublin City University
Dublin 9, Ireland
dhogan,ccafferkey,josef@computing.dcu.ie
Abstract
We present a simple history-based model for
sentence generation from LFG f-structures,
which improves on the accuracy of previous
models by breaking down PCFG indepen-
dence assumptions so that more f-structure
conditioning context is used in the predic-
tion of grammar rule expansions. In addi-
tion, we present work on experiments with
named entities and other multi-word units,
showing a statistically significant improve-
ment of generation accuracy. Tested on sec-
tion 23 of the Penn Wall Street Journal Tree-
bank, the techniques described in this paper
improve BLEU scores from 66.52 to 68.82,
and coverage from 98.18% to 99.96%.
1 Introduction
Sentence generation, or surface realisation, is the
task of generating meaningful, grammatically cor-
rect and fluent text from some abstract semantic or
syntactic representation of the sentence. It is an im-
portant and growing field of natural language pro-
cessing with applications in areas such as transfer-
based machine translation (Riezler and Maxwell,
2006) and sentence condensation (Riezler et al,
2003). While recent work on generation in restricted
domains, such as (Belz, 2007), has shown promising
results there remains much room for improvement
particularly for broad coverage and robust genera-
tors, like those of Nakanishi et al (2005) and Cahill
? Now at the Institut fu?r Maschinelle Sprachverarbeitung,
Universita?t Stuttgart, Azenbergstrae 12, D-70174 Stuttgart,
Germany. aoife.cahill@ims.uni-stuttgart.de
and van Genabith (2006), which do not rely on hand-
crafted grammars and thus can easily be ported to
new languages.
This paper is concerned with sentence genera-
tion from Lexical-Functional Grammar (LFG) f-
structures (Kaplan, 1995). We present improve-
ments in previous LFG-based generation models
firstly by breaking down PCFG independence as-
sumptions so that more f-structure conditioning con-
text is included when predicting grammar rule ex-
pansions. This history-based approach has worked
well in parsing (Collins, 1999; Charniak, 2000) and
we show that it also improves PCFG-based genera-
tion.
We also present work on utilising named entities
and other multi-word units to improve generation
results for both accuracy and coverage. There has
been a limited amount of exploration into the use
of multi-word units in probabilistic parsing, for ex-
ample in (Kaplan and King, 2003) (LFG parsing)
and (Nivre and Nilsson, 2004) (dependency pars-
ing). We are not aware of any similar work on gen-
eration. In the LFG-based generation algorithm pre-
sented by Cahill and van Genabith (2006) complex
named entities (i.e. those consisting of more than
one word token) and other multi-word units can be
fragmented in the surface realization. We show that
the identification of such units may be used as a sim-
ple measure to constrain the generation model?s out-
put.
We take the generator of (Cahill and van Gen-
abith, 2006) as our baseline generator. When tested
on f-structures for all sentences from Section 23 of
the Penn Wall Street Journal (WSJ) treebank (Mar-
267
cus et al, 1993), the techniques described in this pa-
per improve BLEU score from 66.52 to 68.82. In
addition, coverage is increased from 98.18% to al-
most 100% (99.96%).
The remainder of the paper is structured as fol-
lows: in Section 2 we review related work on sta-
tistical sentence generation. Section 3 describes the
baseline generation model and in Section 4 we show
how the new history-based model improves over the
baseline. In Section 5 we describe the source of the
multi-word units (MWU) used in our experiments
and the various techniques we employ to make use
of these MWUs in the generation process. Section 6
gives experimental details and results.
2 Related Work on Statistical Generation
In (statistical) generators, sentences are generated
from an abstract linguistic encoding via the appli-
cation of grammar rules. These rules can be hand-
crafted grammar rules, such as those of (Langkilde-
Geary, 2002; Carroll and Oepen, 2005), created
semi-automatically (Belz, 2007) or, alternatively,
extracted fully automatically from treebanks (Ban-
galore and Rambow, 2000; Nakanishi et al, 2005;
Cahill and van Genabith, 2006).
Insofar as it is a broad coverage generator, which
has been trained and tested on sections of the WSJ
corpus, our generator is closer to the generators
of (Bangalore and Rambow, 2000; Langkilde-Geary,
2002; Nakanishi et al, 2005) than to those designed
for more restricted domains such as weather fore-
cast (Belz, 2007) and air travel domains (Ratna-
parkhi, 2000).
Another feature which characterises statistical
generators is the probability model used to select the
most probable sentence from among the space of all
possible sentences licensed by the grammar. One
generation technique is to first generate all possible
sentences, storing them in a word lattice (Langkilde
and Knight, 1998) or, alternatively, a generation for-
est, a packed represention of alternate trees proposed
by the generator (Langkilde, 2000), and then select
the most probable sequence of words via an n-gram
language model.
Increasingly syntax-based information is being
incorporated directly into the generation model. For
example, Carroll and Oepen (2005) describe a sen-
tence realisation process which uses a hand-crafted
HPSG grammar to generate a generation forest. A
selective unpacking algorithm allows the extraction
of an n-best list of realisations where realisation
ranking is based on a maximum entropy model. This
unpacking algorithm is used in (Velldal and Oepen,
2005) to rank realisations with features defined over
HPSG derivation trees. They achieved the best re-
sults when combining the tree-based model with an
n-gram language model.
Nakanishi et al (2005) describe a treebank-
extracted HPSG-based chart generator. Importing
techniques developed for HPSG parsing, they apply
a log linear model to a packed representation of all
alternative derivation trees for a given input. They
found that a model which included syntactic infor-
mation outperformed a bigram model as well as a
combination of bigram and syntax model.
The probability model described in this paper also
incorporates syntactic information, however, unlike
the discriminative HPSG models just described, it
is a generative history- and PCFG-based model.
While Belz (2007) and Humphreys et al (2001)
mention the use of contextual features for the rules
in their generation models, they do not provide de-
tails nor do they provide a formal probability model.
To the best of our knowledge this is the first paper
providing a probabilistic generative, history-based
generation model.
3 Surface Realisation from f-Structures
Cahill and van Genabith (2006) present a prob-
abilistic surface generation model for LFG (Ka-
plan, 1995). LFG is a constraint-based theory
of grammar, which analyses strings in terms of
c(onstituency)-structure and f(unctional)-structure
(Figure 1). C-structure is defined in terms of CFGs,
and f-structures are recursive attribute-value ma-
trices which represent abstract syntactic functions
(such as SUBJect, OBJect, OBLique, COMPlement
(sentential), ADJ(N)unct), agreement, control, long-
distance dependencies and some semantic informa-
tion (e.g. tense, aspect).
C-structures and f-structures are related in a pro-
jection architecture in terms of a piecewise corre-
spondence ?.1 The correspondence is indicated in
1Our formalisation follows (Kaplan, 1995).
268
S
?=?
NP VP
(? SUBJ)= ? ?=?
NNP V NP
?=? ?=? (? OBJ)= ?
Susan contacted PRP
(? PRED) = ?Susan? (? PRED) = ?contact? ?=?
(? NUM) = SG (? TENSE) = past
(? PERS) = 3 her
(? PRED) = ?pro?
(? NUM) = SG
(? PERS) = 3
f1:
?
?
?
?
?
?
PRED ?CONTACT?(?SUBJ)(?OBJ)??
SUBJ f2:
[
PRED ?SUSAN?
NUM SG
PERS 3
]
OBJ f2:
[
PRED ?PRO?
NUM SG
PERS 3
]
TENSE PAST
?
?
?
?
?
?
Figure 1: C- and f-structures with ? links for the sentence Susan contacted her.
terms of the curvy arrows pointing from c-structure
nodes to f-structure components in Figure 1. Given
a c-structure node ni, the corresponding f-structure
component fj is ?(ni). F-structures and the c-
structure/f-structure correspondence are described
in terms of functional annotations on c-structure
nodes (CFG grammar rules). An equation of the
form (?F) = ? states that the f-structure associated
with the mother of the current c-structure node (?)
has an attribute (grammatical function) (F), whose
value is the f-structure of the current node (?).
The up-arrows and down-arrows are shorthand for
?(M(ni)) = ?(ni) where ni is the c-structure node
annotated with the equation.2
Treebest := argmaxTreeP (Tree|F-Str) (1)
P (Tree|F-Str) :=
?
X ? Y in Tree
Feats = {ai|?vj(?(X))ai = vj}
P (X ? Y |X, Feats) (2)
The generation model of (Cahill and van Gen-
abith, 2006) maximises the probability of a tree
given an f-structure (Eqn. 1), and the string gener-
ated is the yield of the highest probability tree. The
generation process is guided by purely local infor-
mation in the input f-structure: f-structure annotated
CFG rules (LHS ? RHS) are conditioned on their
LHSs and on the set of features/attributes Feats =
{ai|?vj?(LHS)ai = vj}3 ?-linked to the LHS (Eqn.
2M is the mother function on CFG tree nodes.
3In words, Feats is the set of top level features/attributes
(those attributes ai for which there is a value vi) of the f-
structure ? linked to the LHS.
2). Table 1 shows a generation grammar rule and
conditioning features extracted from the example in
Figure 1. The probability of a tree is decomposed
into the product of the probabilities of the f-structure
annotated rules (conditioned on the LHS and local
Feats) contributing to the tree. Conditional proba-
bilities are estimated using maximum likelihood es-
timation.
grammar rule local conditioning features
S(?=?)? NP(?SUBJ=?) VP(?=?) S(?=?), {SUBJ,OBJ,PRED,TENSE}
Table 1: Example grammar rule (from Figure 1).
Cahill and van Genabith (2006) note that condi-
tioning f-structure annotated generation rules on lo-
cal features (Eqn. 2) can sometimes cause the model
to make inappropriate choices. Consider the follow-
ing scenario where in addition to the c-/f-structure in
Figure 1, the training set contains the c-/f-structure
displayed in Figure 2.
From Figures 1 and 2, the model learns (among
others) the generation rules and conditional proba-
bilities displayed in Tables 2 and 3.
F-Struct Feats Grammar Rules Prob
{SUBJ, OBJ, PRED} S(?=?) ? NP(?SUBJ=?) VP(?=?) 1
{SUBJ, OBJ, PRED} VP(?=?) ? V(?=?) NP(?OBJ=?) 1
{NUM, PER, GEN} NP(?SUBJ=?) ? NNP(?=?) 0.5
{NUM, PER, GEN} NP(?SUBJ=?) ? PRP(?=?) 0.5
{NUM, PER, GEN} NP(?OBJ=?) ? PRP(?=?) 1
Table 2: A sample of internal grammar rules ex-
tracted from Figures 1 and 2.
Given the input f-structure (for She
accepted) in Figure 3, (and assuming suit-
able generation rules for intransitive VPs and
accepted) the model would produce the inappro-
priate highest probability tree of Figure 4 with an
incorrect case for the pronoun in subject position.
269
S
?=?
NP VP
(? SUBJ)= ? ?=?
PRP V NP
?=? ?=? (? OBJ)= ?
She hired PRP
(? PRED) = ?pro? (? PRED) = ?hire? ?=?
(? NUM) = SG (? TENSE) = past
(? PERS) = 3 her
(? PRED) = ?pro?
(? NUM) = SG
(? PERS) = 3
f1 :
?
?
?
?
?
?
PRED ?HIRE?(?SUBJ)(?OBJ)??
SUBJ f2 :
[
PRED ?PRO?
NUM SG
PERS 3
]
OBJ f2 :
[
PRED ?PRO?
NUM SG
PERS 3
]
TENSE PAST
?
?
?
?
?
?
Figure 2: C- and f-structures with ? links for the sentence She hired her.
F-Struct Feats Grammar Rules Prob
{PRED=PRO,NUM=SG PER=3, GEN=FEM} PRP(?=?) ? she 0.33
{PRED=PRO,NUM=SG PER=3, GEN=FEM} PRP(?=?) ? her 0.66
Table 3: A sample of lexical item rules extracted
from Figures 1 and 2.
?
?
?
?
?
?
SUBJ
?
?
PRED pro
NUM sg
PERS 3
GEND fem
?
?
PRED accept
TENSE past
?
?
?
?
?
?
Figure 3: Input f-structure for She accepted.
To solve the problem, Cahill and van Gen-
abith (2006) apply an automatic generation gram-
mar transformation to their training data: they au-
tomatically label CFG nodes with additional case
information and the model now learns the new im-
proved generation rules of Tables 4 and 5. Note
how the additional case labelling subverts the prob-
lematic independence assumptions of the probabil-
ity model and communicates the fact that a subject
NP has to be realised as nominative case from the
S ? NP-nom VP production, via the intermediate
NP-nom ? PRP-nom, down to the lexical produc-
tion PRP-nom ? she. The labelling guarantees that,
given the example f-structure in Figure 3, the model
generates the correct string she accepted.
F-Struct Feats Grammar Rules
{SUBJ, OBJ, PRED} S(?=?) ? NP-nom(?SUBJ=?) VP(?=?)
{SUBJ, OBJ, PRED} VP(?=?) ? V(?=?) NP-acc(?OBJ=?)
{NUM, PER, GEN} NP-nom(?SUBJ=?) ? PRP-nom(?=?)
{NUM, PER, GEN} NP-nom(?SUBJ=?) ? NNP-nom(?=?)
{NUM, PER, GEN} NP-acc(?OBJ=?) ? PRP-acc(?=?)
Table 4: Internal grammar rules with case markings.
S
?=?
NP VP
(? SUBJ)= ? ?=?
PRP V
?=? ?=?
her accepted
(? PRED) = ?pro? (? PRED) = ?hire?
(? NUM) = SG (? TENSE) = past
(? PERS) = 3
Figure 4: Inappropriate output: her accepted.
F-Struct Feats Grammar Rules
{PRED=PRO,NUM=SG PER=3, GEN=FEM} PRP-nom(?=?) ? she
{PRED=PRO,NUM=SG PER=3, GEN=FEM} PRP-acc(?=?) ? her
Table 5: Lexical item rules with case markings
4 A History-Based Generation Model
The automatic generation grammar transform pre-
sented in (Cahill and van Genabith, 2006) provides
a solution to coarse-grained and (in fact) inappropri-
ate independence assumptions in the basic genera-
tion model. However, there is a sense in which the
proposed cure improves on the symptoms, but not
the cause of the problem: it weakens independence
assumptions by multiplying and hence increasing
the specificity of conditioning CFG category labels.
There is another option available to us, and that is
the option we will explore in this paper: instead of
applying a generation grammar transform, we will
improve the f-structure-based conditioning of the
generation rule probabilities. In the original model,
rules are conditioned on purely local f-structure con-
text: the set of features/attributes ?-linked to the
LHS of a grammar rule. As a direct consequence
of this, the conditioning (and hence the model) can-
not not distinguish between NP, PRP and NNP rules
270
appropriate to e.g. subject (SUBJ) or object con-
texts (OBJ) in a given input f-structure. However,
the required information can easily be incorporated
into the generation model by uniformly conditioning
generation rules on their parent (mother) grammati-
cal function, in addition to the local ?-linked feature
set. This additional conditioning has the effect of
making the choice of generation rules sensitive to
the history of the generation process, and, we argue,
provides a simpler, more uniform, general, intuitive
and natural probabilistic generation model obviating
the need for CFG-grammar transforms in the origi-
nal proposal of (Cahill and van Genabith, 2006).
In the new model, each generation rule is now
conditioned on the LHS rule CFG category, the set
of features ?-linked to LHS and the parent grammat-
ical function of the f-structure ?-linked to LHS. In a
given c-/f-structure pair, for a CFG node n, the par-
ent grammatical function of the f-structure ?-linked
to n is that grammatical function GF, which, if we
take the f-structure ?-linked to the mother M(n), and
apply it to GF, returns the f-structure ?-linked to n:
(?(M(n))GF) = ?(n).
The basic idea is best explained by way of an
example. Consider again Figure 1. The mother
grammatical function of the f-structure f2 asso-
ciated with node NP(?SUBJ=?) and its daughter
NNP(?=?) (via the ?=? functional annotation) is
SUBJ, as (?(M(n2))SUBJ) = ?(n2), or equivalently
(f1SUBJ) = f2.
Given Figures 1 and 2 as training set, the im-
proved model learns the generation rules (the mother
grammatical function of the outermost f-structure is
assumed to be a dummy TOP grammatical function)
of Tables 6 and 7.
F-Struct Feats Grammar Rules
{SUBJ, OBJ, PRED, TOP} S(?=?) ? NP(?SUBJ=?) VP(?=?)
{SUBJ, OBJ, PRED, TOP} VP(?=?) ? V(?=?) NP(?OBJ=?)
{NUM, PER, GEN, SUBJ} NP(?SUBJ=?) ? PRP(?=?)
{NUM, PER, GEN, OBJ} NP(?OBJ=?) ? PRP(?=?)
{NUM, PER, GEN, SUBJ} NP(?SUBJ=?) ? NNP(?=?)
Table 6: Grammar rules with extra feature extracted
from F-Structures.
Note, that for our example the effect of the uni-
form additional conditioning on mother grammat-
ical function has the same effect as the genera-
tion grammar transform of (Cahill and van Gen-
abith, 2006), but without the need for the gram-
F-Struct Feats Grammar Rules
{PRED=PRO,NUM=SG PER=3, GEN=FEM, SUBJ} PRP(?=?) ? she
{PRED=PRO,NUM=SG PER=3, GEN=FEM, OBJ} PRP(?=?) ? her
Table 7: Lexical item rules.
mar transform. Given the input f-structure in Fig-
ure 3, the model will generate the correct string
she accepted. In addition, uniform condition-
ing on mother grammatical function is more general
than the case-phenomena specific generation gram-
mar transform of (Cahill and van Genabith, 2006),
in that it applies to each and every sub-part of a
recursive input f-structure driving generation, mak-
ing available relevant generation history (context) to
guide local generation decisions.
The new history-based probabilistic generation
model is defined as:
P (Tree|F-Str) :=
?
X ? Y in Tree
Feats = {ai|?vj(?(X))ai = vj}
(?(M(X)))GF = ?(X)
P (X ? Y |X, Feats,GF) (3)
Note that the new conditioning feature, the f-
structure mother grammatical function, GF, is avail-
able from structure previously generated in the c-
structure tree. As such, it is part of the history of
the tree, i.e. it has already been generated in the top-
down derivation of the tree. In this way, the gen-
eration model resembles history-based models for
parsing (Black et al, 1992; Collins, 1999; Charniak,
2000). Unlike, say, the parent annotation for parsing
of (Johnson, 1998) the parent GF feature for a par-
ticular node expansion is not merely extracted from
the parent node in the c-structure tree, but is some-
times extracted from an ancestor node further up the
c-structure tree via intervening ?=? functional an-
notations.
Section 6 provides evaluation results for the new
model on section 23 of the Penn treebank.
5 Multi-Word Units
In another effort to improve generator accuracy over
the baseline model we explored the use of multi-
word units in generation. We expect that the identi-
fication of MWUs may be useful in imposing word-
order constraints and reducing the complexity of the
generation task. Take, for example, the following
271
??
?
?
?
?
?
?
APP
?
?
?
?
?
?
ADJUNCT
[
PRED ?New?
NUM sg
PERS 3
]
PRED ?York?
NUM sg
PERS 3
?
?
?
?
?
?
?
?
?
?
?
?
?
?
[
APP
[
PRED ?New York?
NUM sg
PERS 3
]]
?
?
?
?
?
?
?
?
APP
?
?
?
?
?
?
ADJUNCT
[
PRED ?New?/NE1 1
NUM sg
PERS 3
]
PRED ?York?/NE1 2
NUM sg
PERS 3
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Figure 5: Three different f-structure formats. From left to right: the original f-structure format; the MWU
chunk format; the MWU mark-up format.
two sentences which show the gold version of a sen-
tence followed by the version of the sentence pro-
duced by the generator:
Gold By this time , it was 4:30 a.m. in New York ,
and Mr. Smith fielded a call from a New York
customer wanting an opinion on the British
stock market , which had been having trou-
bles of its own even before Friday ?s New York
market break .
Test By this time , in New York , it was 4:30 a.m.
, and Mr. Smith fielded a call from New a
customer York , wanting an opinion on the
market British stock which had been having
troubles of its own even before Friday ?s New
York market break .
The gold version of the sentence contains a multi-
word unit, New York, which appears fragmented in
the generator output. If multi-word units were either
treated as one token throughout the generation pro-
cess, or, alternatively, if a constraint were imposed
on the generator such that multi-word units were al-
ways generated in the correct order, then this should
help improve generation accuracy. In Section 5.1
we describe the various techniques that were used
to incorporate multi-word units into the generation
process and in 5.2 we detail the different types and
sources of multi-word unit used in the experiments.
Section 6 provides evaluation results on test and de-
velopment sets from the WSJ treebank.
5.1 Incorporating MWUs into the Generation
Process
We carried out three types of experiment which, in
different ways, enabled the generation process to
respect the restrictions on word-order provided by
multi-word units. For the first experiments (type
1), the WSJ treebank training and test data were
altered so that multi-word units are concatenated
into single words (for example, New York becomes
New York). As in (Cahill and van Genabith, 2006) f-
structures are generated from the (now altered) tree-
bank and from this data, along with the treebank
trees, the PCFG-based grammar, which is used for
training the generation model, is extracted. Simi-
larly, the f-structures for the test and development
sets are created from Penn Treebank trees which
have been modified so that multi-word units form
single units. The leftmost and middle f-structures in
Figure 5 show an example of an original f-structure
format and a named-entity chunked format, respec-
tively. Strings output by the generator are then post-
processed so that the concatenated word sequences
are converted back into single words.
In the second experiment (type 2) only the test
data was altered with no concatenation of MWUs
carried out on the training data.
In the final experiments (type 3), instead of con-
catenating named entities, a constraint is introduced
to the generation algorithm which penalises the gen-
eration of sequences of words which violate the in-
ternal word order of named entities. The input is
marked-up in such a way that, although named en-
tities are no longer chunked together to form single
words, the algorithm can read which items are part
of named entities. See the rightmost f-structure in
Figure 5 for an example of an f-structure marked-
up in this way. The tag NE1 1, for example, indi-
cates that the sub-f-structure is part of a named iden-
tity with id number 1 and that the item corresponds
to the first word of the named entity. The baseline
generation algorithm, following Kay (1996)?s work
on chart generation, already contains the hard con-
straint that when combining two chart edges they
must cover disjoint sets of words. We added an ad-
ditional constraint which prevents edges from being
combined if this would result in the generation of
a string which contained a named entity which was
272
either incomplete or where the words in the named
entity were generated in the wrong order.
5.2 Types of MWUs used in Experiments
We carry out experiments with multi-word units
from three different sources. First, we use the output
of the maximum entropy-based named entity recog-
nition system of (Chieu and Ng, 2003). This sys-
tem identifies four types of named entity: person,
organisation, location, and miscellaneous. Addition-
ally we use a dictionary of candidate multi-word ex-
pressions based on a list from the Stanford Multi-
word Expression Project4. Finally, we also carry out
experiments with multi-word units extracted from
the BBN Pronoun Coreference and Entity Type Cor-
pus (Weischedel and Brunstein, 2005). This supple-
ments the Penn WSJ treebank?s one million words of
syntax-annotated Wall Street Journal text with addi-
tional annotations of 23 named entity types, includ-
ing nominal-type named entities such as person, or-
ganisation, location, etc. as well as numeric types
such as date, time, quantity and money. Since the
BBN corpus data is very comprehensive and is hand-
annotated we take this be be a gold standard, repre-
senting an upper bound for any gains that might be
made by identifying complex named entities in our
experiments.5 Table 8 gives examples of the various
types of MWUs identified by the three sources.
For our purposes we are not concerned with the
distinctions between different types of named enti-
ties; we are merely exploiting the fact that they may
be treated as atomic units in the generation model. In
all cases we disregard multi-word units that cross the
original syntactic bracketing of the WSJ treebank.
An overview of the various types of multi-word units
used in our experiments is presented in Table 9.
6 Experimental Evaluation
All experiments were carried out on the WSJ tree-
bank with sections 02-21 for training, section 24 for
development and section 23 for final test results. The
LFG annotation algorithm of (Cahill et al, 2004)
was used to produce the f-structures for develop-
ment, test and training sets.
4mwe.stanford.edu
5Although it is possible there are other types of MWUs that
may be more suitable to the task than the named entities identi-
fied by BBN, so further gains might be possible.
MWU type Examples
Names Martha Matthews
Yoshio Hatakeyama
Organisations Rolls-Royce Motor Cars Inc.
Washington State University
Locations New York City
New Zealand
Time expressions October 19th
two years ago
the 21st century
Quantities $2.7 million to $3 million
about 25 %
60 mph
Prepositional expressions in fact
at the time
on average
Table 8: Examples of some of the types of MWU
from the three different sources.
average number average length
(Chieu and Ng, 2003) 0.61 2.40
Stanford MWE Project 0.10 2.48
BBN Corpus 1.15 2.66
Table 9: Average number of MWUs per sentence
and average MWU length in the WSJ treebank
grouped by MWU source.
Table 10 shows the final results for section 23. For
each test we present BLEU score results as well as
String Edit Distance and coverage. We measure sta-
tistical significance using two different tests. First
we use a bootstrap resampling method, popular for
machine translation evaluations, to measure the sig-
nificance of improvements in BLEU scores, with a
resampling rate of 1000.6 We also calculated the
significance of an increase in String Edit Distance
by carrying out a paired t-test on the mean differ-
ence of the String Edit Distance scores. In Table 10,
? means significant at level 0.005. > means signif-
icant at level 0.05.
In Table 10, Baseline gives the results of the
generation algorithm of (Cahill and van Genabith,
2006). HB Model refers to the improved model
with the increased history context, as described in
Section 4. The results, where for example the
BLEU score rises from 66.52 to 67.24, show that
even increasing the conditioning context by a limited
6Scripts for running the bootstrapping method carried
out in our evaluation are available for download at projec-
tile.is.cs.cmu.edu/research/public/tools/bootStrap/tutorial.htm
273
Section 23 (2416 sentences)
Model BLEU StringEd Coverage BLEU Bootstrap Signif StringEd Paired T-Test
1. Baseline 66.52 68.69 98.18
2. HB Model 67.24 69.89 99.88 ? 1 ? 1
3. +MWU Best Automatic 67.81 70.36 99.92 ? 2 ? 2
4. MWU BBN 68.82 70.92 99.96 ? 3 > 3
Table 10: Results on Section 23 for all sentence lengths.
amount increases the accuracy of the system signif-
icantly for both BLEU and String Edit Distance. In
addition, coverage goes up from 98.18% to 99.88%.
+MWU Best Automatic displays our best results
using automatically identified named entities. These
were achieved using experiment type 2, described
in Section 5, with the MWUs produced by (Chieu
and Ng, 2003). Results displayed in Table 10 up
to this point are cumulative. The final row in Ta-
ble 10, MWU BBN, shows the best results with BBN
MWUs: the history-based model with BBN multi-
word units incorporated in a type 1 experiment.
We now discuss the various MWU experiments
in more detail. See Table 11 for a breakdown of
the MWU experiment results on the development
set, WSJ section 24. Our baseline for these exper-
iments is the history-based generator presented in
Section 4. For each experiment type described in
Section 5.1 we ran three experiments, varying the
source of MWUs. First, MWUs came from the auto-
matic NE recogniser of (Chieu and Ng, 2003), then
we added the MWUs from the Stanford list and fi-
nally we ran tests with MWUs extracted from the
BBN corpus.
Our first set of experiments (type 1), where both
training data and development set data were MWU-
chunked, produced the worst results for the automat-
ically chunked MWUs. BLEU score accuracy actu-
ally decreased for the automatically chunked MWU
experiments. In an error analysis of type 1 ex-
periments with (Chieu and Ng, 2003) concatenated
MWUs, we inspected those sentences where accu-
racy had decreased from the baseline. We found
that for over half (51.5%) of these sentences, the in-
put f-structures contained no multi-word units at all.
The problem for these sentences therefore lay with
the probabilistic grammar extracted from the MWU-
chunked training data. When the source of MWU
for the type 1 experiments was the BBN, however,
accuracy improved significantly over the baseline
and the result is the highest accuracy achieved over
all experiment types. One possible reason for the
low accuracy scores in the type 1 experiments with
the (Chieu and Ng, 2003) MWU chunked data could
be noisy MWUs which negatively affect the gram-
mar. For example, the named entity recogniser
of (Chieu and Ng, 2003) achieves an accuracy of
88.3% on section 23 of the Penn Treebank.
In order to avoid changing the grammar through
concatenation of MWU components (as in exper-
iment type 1) and thus risking side-effects which
cause some heretofore likely constructions become
less likely and vice versa, we ran the next set of ex-
periments (type 2) which leave the original grammar
intact and alter the input f-structures only. These
experiments were more successful overall and we
achieved an improvement over the baseline for both
BLEU and String Edit Distance scores with all
MWU types. As can be seen from Table 11 the
best score for automatically chunked MWUs are
with the (Chieu and Ng, 2003) MWUs. Accuracy
decreases marginally when we added the Stanford
MWUs. In our final set of experiments (type 3) al-
though the accuracy for all three types of MWUs
improves over the baseline, accuracy is a little be-
low the type 2 experiments.
It is difficult to compare sentence generators since
the information contained in the input varies greatly
between systems, systems are evaluated on different
test sets and coverage also varies considerably. In
order to compare our system with those of (Nakan-
ishi et al, 2005) and (Langkilde-Geary, 2002) we
report our best results with automatically acquired
MWUs for sentences of ? 20 words in length on
section 23: our system gets coverage of 100% and a
BLEU score of 71.39. For the same test set Nakan-
ishi et al (2005) achieved coverage of 90.75 and a
BLEU score of 77.33. Langkilde-Geary (2002) re-
274
Section 24 (1346 sentences)
Model MWUs BLEU StringEd Coverage
HB Model 65.85 69.93 99.93
type 1 (Chieu and Ng, 2003) 65.81 70.34 99.93
(training and test data chunked) +Stanford MWEs 64.81 69.67 99.93
BBN 67.24 71.46 99.93
type 2 (Chieu and Ng, 2003) 66.37 70.26 99.93
(test data chunked) +Stanford MWEs 66.28 70.21 99.93
BBN 66.84 70.74 99.93
type 3 (Chieu and Ng, 2003) 66.30 70.12 100
(internal generation constraint) +Stanford MWEs 66.07 70.02 99.93
BBN 66.45 70.14 99.93
Table 11: Results on Section 24, all sentence lengths.
ports 82.7% coverage and a BLEU score of 75.7%
on the same test set with the ?permute,no dir? type
input. Langkilde-Geary (2002) report results for ex-
periments with varying levels of linguistic detail in
the input given to the generator. As with Nakanishi
et al (2005) we find the ?permute,no dir? type of in-
put is most comparable to the level of information
contained in our input f-structures. Finally, the sym-
bolic generator of Callaway (2003) reports a Sim-
ple String Accuracy score of 88.84 and coverage of
98.7% on section 23 for all sentence lengths.
7 Conclusion and Future Work
We have presented techniques which improve the ac-
curacy of an already state-of-art surface generation
model. We found that a history-based model that
increases conditioning context in PCFG style rules
by simply including the grammatical function of the
f-structure parent, improves generator accuracy. In
the future we will experiment with increasing condi-
tioning context further and using more sophisticated
smoothing techniques to avoid sparse data problems
when conditioning is increased.
We have also demonstrated that automatically ac-
quired multi-word units can bring about moderate,
but significant, improvements in generator accuracy.
For automatically acquired MWUs, we found that
this could best be achieved by concatenating input
items when generating the f-structure input to the
generator, while training the input generation gram-
mar on the original (i.e. non-MWU concatenated)
sections of the treebank. Relying on the BBN cor-
pus as a source of multi-word units, we gave an up-
per bound to the potential usefulness of multi-word
units in generation and showed that automatically
acquired multi-word units, encouragingly, give re-
sults not far below the upper bound.
References
Srinivas Bangalore and Owen Rambow. 2000. Exploit-
ing a probabilistic hierarchical model for generation.
In Proceedings of the 18th COLING.
Anja Belz. 2007. Probabilistic generation of wether fore-
cast texts. In Proceedings of NAACL-HLT.
Ezra Black, Fred Jelinek, John Lafferty, David M. Mager-
man, Robert Mercer, and Salim Roukos. 1992. To-
wards history-based grammars: Using richer models
for probabilistic parsing. In Proceeding of the 5th
DARPA Speech and Language Workshop.
Aoife Cahill and Josef van Genabith. 2006. Robust
PCFG-based generation using automatically acquired
LFG approximations. In Proceedings of the 44th ACL.
Aoife Cahill, Michael Burke, Ruth O?Donovan, Josef van
Genabith, and Andy Way. 2004. Long-distance de-
pendency resolution in automatically acquired wide-
coverage PCFG-based LFG approximations. In Pro-
ceedings of the 42nd ACL.
Charles B. Callaway. 2003. Evaluating coverage for
large symbolic NLG grammars. In In Proceedings of
the 18th IJCAI.
John A. Carroll and Stephan Oepen. 2005. High ef-
ficiency realization for a wide-coverage unification
grammar. In Proceedings of IJCNLP.
Eugene Charniak. 2000. A maximum entropy-inspired
parser. In Proceedings of the 1st NAACL.
Hai Leong Chieu and Hwee Tou Ng. 2003. Named en-
tity recognition with a maximum entropy approach. In
Proceedings of the CoNLL.
275
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.
Kevin Humphreys, Mike Calcagno, and David Weise.
2001. Reusing a statistical language model for gen-
eration. In Proceedings of the 8th European Workshop
on Natural Language Generation (EWNLG).
Mark Johnson. 1998. PCFG models of linguistic tree
representations. Computational Linguistics, 24.
Ronald M. Kaplan and Tracy Holloway King. 2003.
Low-level mark-up and large-scale LFG grammar pro-
cessing. In Proceedings of the Lexical Functional
Grammar Conference.
Ron Kaplan. 1995. The formal architecture of
lexical-functional grammar. In Dalrymple, Kaplan,
Maxwell, and Zaenen, editors, Formal Issues in
Lexical-Functional Grammar, pages 7?27. CSLI Pub-
lications.
Martin Kay. 1996. Chart generation. In Proceedings of
the 34th ACL.
Irene Langkilde and Kevin Knight. 1998. Generation
that exploits corpus-based statistical knowledge. In
Proceedings of the 36th Annual Meeting of the Asso-
ciation for Computational Linguistics and the 17th In-
ternational Conference on Computational Linguistics
(ACL-COLING).
Irene Langkilde-Geary. 2002. An empirical verification
of coverage and correctness for a general-purpose sen-
tence generator. In Proceedings of the 2nd INLG.
Irene Langkilde. 2000. Forest-based statistical sentence
generation. In Proceedings of the 1st NAACL.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: the Penn Treebank. Computational
Linguistics, 19(2):313?330.
Hiroko Nakanishi, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic models for disambiguation of an
HPSG-based chart generator. In Proceedings of the
9th IWPT.
Joakim Nivre and Jens Nilsson. 2004. Multiword units
in syntactic parsing. In Workshop on Methodologies
and Evaluation of Multiword Units in Real-World Ap-
plications.
Adwait Ratnaparkhi. 2000. Trainable methods for sur-
face natural language generation. In Proceedings of
the 1st NAACL.
Stefan Riezler and John T. Maxwell. 2006. Grammat-
ical machine translation. In Proceedings of the 6th
NAACL.
Stefan Riezler, Tracy H. King, Richard Crouch, and An-
nie Zaenen. 2003. Statistical sentence condensation
using ambiguity packing and stochastic disambigua-
tion methods for lexical-functional grammar. In Pro-
ceedings of the 3rd NAACL.
Erik Velldal and Stephan Oepen. 2005. Maximum en-
tropy models for realization ranking. In Proceedings
of the MT-Summit.
Ralph Weischedel and Ada Brunstein, 2005. BBN pro-
noun coreference and entity type corpus. Technical
Report.
276
Large-Scale Induction and Evaluation of
Lexical Resources from the Penn-II and
Penn-III Treebanks
Ruth O?Donovan?
Dublin City University
Michael Burke??
Dublin City University
Aoife Cahill?
Dublin City University
Josef van Genabith??
Dublin City University
Andy Way??
Dublin City University
We present a methodology for extracting subcategorization frames based on an automatic
lexical-functional grammar (LFG) f-structure annotation algorithm for the Penn-II and
Penn-III Treebanks. We extract syntactic-function-based subcategorization frames (LFG
semantic forms) and traditional CFG category-based subcategorization frames as well as
mixed function/category-based frames, with or without preposition information for obliques
and particle information for particle verbs. Our approach associates probabilities with frames
conditional on the lemma, distinguishes between active and passive frames, and fully
reflects the effects of long-distance dependencies in the source data structures. In contrast
to many other approaches, ours does not predefine the subcategorization frame types extracted,
learning them instead from the source data. Including particles and prepositions, we extract
21,005 lemma frame types for 4,362 verb lemmas, with a total of 577 frame types and an
average of 4.8 frame types per verb. We present a large-scale evaluation of the complete
set of forms extracted against the full COMLEX resource. To our knowledge, this is
the largest and most complete evaluation of subcategorization frames acquired automatically
for English.
1. Introduction
In modern syntactic theories (e.g., lexical-functional grammar [LFG] [Kaplan and
Bresnan 1982; Bresnan 2001; Dalrymple 2001], head-driven phrase structure gram-
mar [HPSG] [Pollard and Sag 1994], tree-adjoining grammar [TAG] [Joshi 1988], and
combinatory categorial grammar [CCG] [Ades and Steedman 1982]), the lexicon is
the central repository for much morphological, syntactic, and semantic information.
? National Centre for Language Technology, School of Computing, Dublin City University, Glasnevin,
Dublin 9, Ireland. E-mail: {rodonovan,mburke,acahill,josef,away}@computing.dcu.ie.
? Centre for Advanced Studies, IBM, Dublin, Ireland.
Submission received: 19 March 2004; revised submission received: 18 December 2004; accepted for
publication: 2 March 2005.
? 2005 Association for Computational Linguistics
Computational Linguistics Volume 31, Number 3
Extensive lexical resources, therefore, are crucial in the construction of wide-coverage
computational systems based on such theories.
One important type of lexical information is the subcategorization requirements
of an entry (i.e., the arguments a predicate must take in order to form a grammatical
construction). Lexicons, including subcategorization details, were traditionally pro-
duced by hand. However, as the manual construction of lexical resources is time con-
suming, error prone, expensive, and rarely ever complete, it is often the case that the
limitations of NLP systems based on lexicalized approaches are due to bottlenecks in
the lexicon component. In addition, subcategorization requirements may vary across
linguistic domain or genre (Carroll and Rooth 1998). Manning (1993) argues that, aside
from missing domain-specific complementation trends, dictionaries produced by hand
will tend to lag behind real language use because of their static nature. Given these
facts, research on automating acquisition of dictionaries for lexically based NLP sys-
tems is a particularly important issue.
Aside from the extraction of theory-neutral subcategorization lexicons, there has
also been work in the automatic construction of lexical resources which comply
with the principles of particular linguistic theories such as LTAG, CCG, and HPSG
(Chen and Vijay-Shanker 2000; Xia 1999; Hockenmaier, Bierner, and Baldridge 2004;
Nakanishi, Miyao, and Tsujii 2004). In this article we present an approach to auto-
mating the process of lexical acquisition for LFG (i.e., grammatical-function-based sys-
tems). However, our approach also generalizes to CFG category-based approaches. In
LFG, subcategorization requirements are enforced through semantic forms specifying
which grammatical functions are required by a particular predicate. Our approach is
based on earlier work on LFG semantic form extraction (van Genabith, Sadler, and
Way 1999) and recent progress in automatically annotating the Penn-II and Penn-III
Treebanks with LFG f-structures (Cahill et al 2002; Cahill, McCarthy, et al 2004). Our
technique requires a treebank annotated with LFG functional schemata. In the early
approach of van Genabith, Sadler, and Way (1999), this was provided by manually
annotating the rules extracted from the publicly available subset of the AP Treebank to
automatically produce corresponding f-structures. If the f-structures are of high qual-
ity, reliable LFG semantic forms can be generated quite simply by recursively reading
off the subcategorizable grammatical functions for each local PRED value at each level of
embedding in the f-structures. The work reported in van Genabith, Sadler, and Way
(1999) was small scale (100 trees) and proof of concept and required considerable
manual annotation work. It did not associate frames with probabilities, discriminate
between frames for active and passive constructions, properly reflect the effects of
long-distance dependencies (LDDs), or include CFG category information. In this
article we show how the extraction process can be scaled to the complete Wall
Street Journal (WSJ) section of the Penn-II Treebank, with about one million words
in 50,000 sentences, based on the automatic LFG f-structure annotation algorithm
described in Cahill et al (2002) and Cahill, McCarthy, et al (2004). More recently
we have extended the extraction approach to the larger, domain-diverse Penn-III
Treebank. Aside from the parsed WSJ section, this version of the treebank contains
parses for a subsection of the Brown corpus (almost 385,000 words in 24,000 trees)
taken from a variety of text genres.1 In addition to extracting grammatical-function-
1 For the remainder of this work, when we refer to the Penn-II Treebank, we mean the parse-annotated WSJ,
and when we refer to the Penn-III Treebank, we mean the parse-annotated WSJ and Brown corpus
combined.
330
O?Donovan et al Large-Scale Induction and Evaluation of Lexical Resources
based subcategorization frames, we also include the syntactic categories of the predicate
and its subcategorized arguments, as well as additional details such as the prepositions
required by obliques and particles accompanying particle verbs. Our method discrim-
inates between active and passive frames, properly reflects LDDs in the source data
structures, assigns conditional probabilities to the semantic forms associated with each
predicate, and does not predefine the subcategorization frames extracted.
In Section 2 of this article, we briefly outline LFG, presenting typical lexical entries
and the encoding of subcategorization information. Section 3 reviews related work in
the area of automatic subcategorization frame extraction. Our methodology and its
implementation are presented in Section 4. In Section 5 we present results from the
extraction process. We evaluate the complete induced lexicon against the COMLEX
resource (Grishman, MacLeod, and Meyers 1994) and present the results in Section 6.
To our knowledge, this is by far the largest and most complete evaluation of subcat-
egorization frames automatically acquired for English. In Section 7, we examine the
coverage of our lexicon in regard to unseen data and the rate at which new lexical
entries are learned. Finally, in Section 8 we conclude and give suggestions for future
work.
2. Subcategorization in LFG
Lexical functional grammar (Kaplan and Bresnan 1982; Bresnan 2001; Dalrymple
2001) is a member of the family of constraint-based grammars. It posits minimally
two levels of syntactic representation:2 c(onstituent)-structure encodes details of sur-
face syntactic constituency, whereas f(unctional)-structure expresses abstract syntactic
information about predicate?argument?modifier relations and certain morphosyntactic
properties such as tense, aspect, and case. C-structure takes the form of phrase structure
trees and is defined in terms of CFG rules and lexical entries. F-structure is pro-
duced from functional annotations on the nodes of the c-structure and implemented
in terms of recursive feature structures (attribute?value matrices). This is exemplified
by the analysis of the string The inquiry soon focused on the judge (wsj 0267 72) using
the grammar in Figure 1, which results in the annotated c-structure and f-structure in
Figure 2.
The value of the PRED attribute in an f-structure is a semantic form ??gf1, gf2, . . . ,
gfn?, where ? is a lemma and gf a grammatical function. The semantic form provides
an argument list ?gf1,gf2, . . . ,gfn? specifying the governable grammatical functions (or
arguments) required by the predicate to form a grammatical construction. In Figure 1
the verb FOCUS requires a subject and an oblique object introduced by the preposition
on: FOCUS?(? SUBJ)(? OBLon)?. The argument list can be empty, as in the PRED value
for judge in Figure 1. According to Dalrymple (2001), LFG assumes the following uni-
versally available inventory of grammatical functions: SUBJ(ect), OBJ(ect), OBJ?, COMP,
XCOMP, OBL(ique)?, ADJ(unct), XADJ. OBJ? and OBL? represent families of grammatical
functions indexed by their semantic role, represented by the theta subscript. This list
of grammatical functions is divided into governable (subcategorizable) grammatical
functions (arguments) and nongovernable (nonsubcategorizable) grammatical func-
tions (modifiers/adjuncts), as summarized in Table 1.
2 LFGs may also involve morphological and semantic levels of representation.
331
Computational Linguistics Volume 31, Number 3
Figure 1
Sample LFG rules and lexical entries.
A number of languages allow the possibility of object functions in addition to the
primary OBJ, such as the second or indirect object in English. Oblique arguments are
realized as prepositional phrases in English. COMP, XCOMP, and XADJ are all clausal
functions which differ in the way in which they are controlled. A COMP is a closed
function which contains its own internal SUBJ:
The judge thinks [COMP that it will resume].
XCOMP and XADJ are open functions not requiring an internal SUBJ. The subject is
instead specified externally in the matrix phrase:
The judge wants [XCOMP to open an inquiry].
While many linguistic theories state subcategorization requirements in terms
of phrase structure (CFG categories), Dalrymple (2001) questions the viability and
universality of such an approach because of the variety of ways in which grammatical
functions may be realized at the language-specific constituent structure level. LFG
argues that subcategorization requirements are best stated at the f-structure level,
in functional rather than phrasal terms. This is because of the assumption that
abstract grammatical functions are primitive concepts as opposed to derivatives
332
O?Donovan et al Large-Scale Induction and Evaluation of Lexical Resources
Figure 2
C- and f-structures for Penn Treebank sentence wsj 0267 72, The inquiry soon focused on the judge.
of phrase structural position. In LFG, the subcategorization requirements of a
particular predicate are expressed by its semantic form: FOCUS?(? SUBJ)(? OBLon)? in
Figure 1.
The subcategorization requirements expressed by semantic forms are enforced at
f-structure level through completeness and coherence well-formedness conditions on
f-structure (Kaplan and Bresnan 1982):
An f-structure is locally complete iff it contains all the governable grammatical
functions that its predicate governs. An f-structure is complete iff it and all its
subsidiary f-structures are locally complete. An f-structure is locally coherent iff
all the governable grammatical functions that it contains are governed by a
local predicate. An f-structure is coherent iff it and all its subsidiary f-structures
are locally coherent. (page 211)
Consider again the f-structure in Figure 2. The semantic form associated with
the verb focus is FOCUS?(? SUBJ)(? OBLon)?. The f-structure is locally complete, as it
contains the SUBJ and an OBL with the preposition on specified by the semantic
form. The f-structure also satisfies the coherence condition, as it does not contain
any governable grammatical functions other than the SUBJ and OBL required by the
local PRED.
333
Computational Linguistics Volume 31, Number 3
Table 1
Governable and nongovernable grammatical functions in LFG.
Governable GFs Nongovernable GFs
SUBJ ADJ
OBJ XADJ
XCOMP
COMP
OBJ?
OBL?
Because of the specific form of the LFG lexicon, our extraction approach differs in
interesting ways from that of previous lexical extraction experiments. This contrast is
made evident in Sections 3 and 4.
3. Related Work
The encoding of verb subcategorization properties is an essential step in the
construction of computational lexicons for tasks such as parsing, generation, and
machine translation. Creating such a resource by hand is time consuming and error
prone, requires considerable linguistic expertise, and is rarely if ever complete. In
addition, a hand-crafted lexicon cannot be easily adapted to specific domains or
account for linguistic change. Accordingly, many researchers have attempted to
construct lexicons automatically, especially for English. In this section, we discuss
approaches to CFG-based subcategorization frame extraction as well as attempts to
induce lexical resources which comply with specific linguistic theories or express
information in terms of more abstract predicate-argument relations. The evaluation of
these approaches is discussed in greater detail in Section 6, in which we compare our
results with those reported elsewhere in the literature.
We will divide more-general approaches to subcategorization frame acquisition
into two groups: those which extract information from raw text and those which
use preparsed and hand-corrected treebank data as their input. Typically in the
approaches based on raw text, a number of subcategorization patterns are predefined,
a set of verb subcategorization frame associations are hypothesized from the data,
and statistical methods are applied to reliably select hypotheses for the final lexicon.
Brent (1993) relies on morphosyntactic cues in the untagged Brown corpus as indicators
of six predefined subcategorization frames. The frames do not include details of specific
prepositions. Brent used hypothesis testing on binomial frequency data to statistically
filter the induced frames. Ushioda et al (1993) run a finite-state NP parser on a
POS-tagged corpus to calculate the relative frequency of the same six subcategoriza-
tion verb classes. The experiment is limited by the fact that all prepositional phrases
are treated as adjuncts. Ushioda et al (1993) employ an additional statistical method
based on log-linear models and Bayes? theorem to filter the extra noise introduced by
the parser and were the first to induce relative frequencies for the extracted frames.
Manning (1993) attempts to improve on the approach of Brent (1993) by passing raw
text through a stochastic tagger and a finite-state parser (which includes a set of
simple rules for subcategorization frame recognition) in order to extract verbs and
the constituents with which they co-occur. He assumes 19 different subcategorization
334
O?Donovan et al Large-Scale Induction and Evaluation of Lexical Resources
frame definitions, and the extracted frames include details of specific prepositions.
The extracted frames are noisy as a result of parser errors and so are filtered using
the binomial hypothesis theory (BHT), following Brent (1993). Applying his technique
to approximately four million words of New York Times newswire, Manning acquired
4,900 verb-subcategorization frame pairs for 3,104 verbs, an average of 1.6 frames
per verb. Briscoe and Carroll (1997) predefine 163 verbal subcategorization frames,
obtained by manually merging the classes exemplified in the COMLEX (MacLeod,
Grishman, and Meyers 1994) and ANLT (Boguraev et al 1987) dictionaries and adding
around 30 frames found by manual inspection. The frames incorporate control informa-
tion and details of specific prepositions. Briscoe and Carroll (1997) refine the BHT with a
priori information about the probabilities of subcategorization frame membership and
use it to filter the induced frames. Recent work by Korhonen (2002) on the filtering
phase of this approach uses linguistic verb classes (based on Levin [1993]) for obtaining
more accurate back-off estimates for hypothesis selection. Carroll and Rooth (1998)
use a handwritten head-lexicalized, context-free grammar and a text corpus to
compute the probability of particular subcategorization patterns. The approach is
iterative with the aim of estimating the distribution of subcategorization frames
associated with a particular predicate. They perform a mapping between their frames
and those of the OALD, resulting in 15 frame types. These do not contain details of
specific prepositions.
More recently, a number of researchers have applied similar techniques to auto-
matically derive lexical resources for languages other than English. Schulte im Walde
(2002a, 2002b) uses a head-lexicalized probabilistic context-free grammar similar to
that of Caroll and Rooth (1998) to extract subcategorization frames from a large
German newspaper corpus from the 1990s. She predefines 38 distinct frame types,
which contain maximally three arguments each and are made up of a combination
of the following: nominative, dative, and accusative noun phrases; reflexive pro-
nouns; prepositional phrases; expletive es; subordinated nonfinite clauses; subordinated
finite clauses; and copula constructions. The frames may optionally contain details of
particular prepositional use. Unsupervised training is performed on a large German
newspaper corpus, and the resulting probabilistic grammar establishes the relevance of
different frame types to a specific lexical head. Because of computing time constraints,
Schulte im Walde limits sentence length for grammar training and parsing. Sentences
of length between 5 and 10 words were used to bootstrap the lexicalized grammar
model. For lexicalized training, sentences of length between 5 and 13 words were
used. The result is a subcategorization lexicon for over 14,000 German verbs. The
extensive evaluation carried out by Schulte im Walde will be discussed in greater detail
in Section 6.
Approaches using treebank-based data as a source for subcategorization infor-
mation, such as ours, do not predefine the frames to be extracted but rather learn them
from the data. Kinyon and Prolo (2002) describe a simple tool which uses fine-grained
rules to identify the arguments of verb occurrences in the Penn-II Treebank. This is
made possible by manual examination of more than 150 different sequences of syntactic
and functional tags in the treebank. Each of these sequences was categorized as a
modifier or argument. Arguments were then mapped to traditional syntactic functions.
For example, the tag sequence NP-SBJ denotes a mandatory argument, and its syntactic
function is subject. In general, argumenthood was preferred over adjuncthoood. As
Kinyon and Prolo (2002) does not include an evaluation, currently it is impossible to
say how effective their technique is. Sarkar and Zeman (2000) present an approach to
learn previously unknown frames for Czech from the Prague Dependency Bank (Hajic
335
Computational Linguistics Volume 31, Number 3
1998). Czech is a language with a freer word order than English and so configurational
information cannot be relied upon. In a dependency tree, the set of all dependents
of the verb make up a so-called observed frame, whereas a subcategorization frame
contains a subset of the dependents in the observed frame. Finding subcategorization
frames involves filtering adjuncts from the observed frame. This is achieved using three
different hypothesis tests: BHT, log-likelihood ratio, and t-score. The system learns 137
subcategorization frames from 19,126 sentences for 914 verbs (those which occurred
five times or more). Marinov and Hemming (2004) present preliminary work on the
automatic extraction of subcategorization frames for Bulgarian from the BulTreeBank
(Simov, Popova, and Osenova 2002). In a similar way to that of Sarkar and Zeman
(2000), Marinov and Hemming?s system collects both arguments and adjuncts. It then
uses the binomial log-likelihood ratio to filter incorrect frames. The BulTreebank trees
are annotated with HPSG-typed feature structure information and thus contain more
detail than the dependency trees. The work done for Bulgarian is small-scale, however,
as Marinov and Hemming are working with a preliminary version of the treebank with
580 sentences.
Work has been carried out on the extraction of formalism-specific lexical resources
from the Penn-II Treebank, in particular TAG, CCG, and HPSG. As these formalisms are
fully lexicalized with an invariant (LTAG and CCG) or limited (HPSG) rule component,
the extraction of a lexicon essentially amounts to the creation of a grammar. Chen and
Vijay-Shanker (2000) explore a number of related approaches to the extraction of a
lexicalized TAG from the Penn-II Treebank with the aim of constructing a statistical
model for parsing. The extraction procedure utilizes a head percolation table as intro-
duced by Magerman (1995) in combination with a variation of Collins?s (1997) approach
to the differentiation between complement and adjunct. This results in the construction
of a set of lexically anchored elementary trees which make up the TAG in question.
The number of frame types extracted (i.e., an elementary tree without a specific lexical
anchor) ranged from 2,366 to 8,996. Xia (1999) also presents a similar method for
the extraction of a TAG from the Penn Treebank. The extraction procedure consists
of three steps: First, the bracketing of the trees in the Penn Treebank is corrected and
extended based on the approaches of Magerman (1994) and Collins (1997). Then the
elementary trees are read off in a quite straightforward manner. Finally any invalid
elementary trees produced as a result of annotation errors in the treebank are filtered out
using linguistic heuristics. The number of frame types extracted by Xia (1999) ranged
from 3,014 to 6,099.
Hockenmaier, Bierner, and Baldridge (2004) outline a method for the automatic
extraction of a large syntactic CCG lexicon from the Penn-II Treebank. For each tree, the
algorithm annotates the nodes with CCG categories in a top-down recursive manner.
The first step is to label each node as either a head, complement, or adjunct based
on the approaches of Magerman (1994) and Collins (1997). Each node is subsequently
assigned the relevant category based on its constituent type and surface configuration.
The algorithm handles ?like? coordination and exploits the traces used in the treebank
in order to interpret LDDs. Unlike our approach, those of Xia (1999) and Hockenmaier,
Bierner, and Baldridge (2004) include a substantial initial correction and clean-up of the
Penn-II trees.
Miyao, Ninomiya, and Tsujii (2004) and Nakanishi, Miyao, and Tsujii (2004)
describe a methodology for acquiring an English HPSG from the Penn-II Treebank.
Manually defined heuristics are used to automatically annotate each tree in the treebank
with partially specified HPSG derivation trees: Head/argument/modifier distinctions
are made for each node in the tree based on Magerman (1994) and Collins (1997);
336
O?Donovan et al Large-Scale Induction and Evaluation of Lexical Resources
the whole tree is then converted to a binary tree; heuristics are applied to deal with
phenomena such as LDDs and coordination and to correct some errors in the tree-
bank, and finally an HPSG category is assigned to each node in the tree in accordance
with its CFG category. In the next phase of the process (externalization), HPSG lexical
entries are automatically extracted from the annotated trees through the application of
?inverse schemata.?
4. Methodology
The first step in the application of our methodology is the production of a tree-
bank annotated with LFG f-structure information. F-structures are attribute?value
structures which represent abstract syntactic information, approximating to ba-
sic predicate?argument?modifier structures. Most of the early work on automatic
f-structure annotation (e.g., van Genabith, Way, and Sadler 1999; Frank 2000; Sadler,
van Genabith, and Way 2000) was applied only to small data sets (fewer than 200
sentences) and was largely proof of concept. However, more recent work (Cahill et al
2002; Cahill, McCarthy, et al 2004) has presented efforts in evolving and scaling up
annotation techniques to the Penn-II Treebank (Marcus et al 1994), containing more
than 1,000,000 words and 49,000 sentences.
We utilize the automatic annotation algorithm of Cahill et al (2002) and Cahill,
McCarthy, et al (2004) to derive a version of Penn-II in which each node in each
tree is annotated with LFG functional annotations in the form of attribute-value struc-
ture equations. The algorithm uses categorial, configurational, local head, and Penn-II
functional and trace information. The annotation procedure is dependent on locating
the head daughter, for which an amended version of Magerman (1994) is used. The
head is annotated with the LFG equation ?=?. Linguistic generalizations are provided
over the left (the prefix) and the right (suffix) context of the head for each syntactic
category occurring as the mother nodes of such heads. To give a simple example, the
rightmost NP to the left of a VP head under an S is likely to be the subject of the sen-
tence (? SUBJ =?), while the leftmost NP to the right of the V head of a VP is most
probably the verb?s object (? OBJ =?). Cahill, McCarthy, et al (2004) provide four
classes of annotation principles: one for noncoordinate configurations, one for coor-
dinate configurations, one for traces (long-distance dependencies), and a final ?catch
all and clean up? phase.
The satisfactory treatment of long-distance dependencies by the annotation algo-
rithm is imperative for the extraction of accurate semantic forms. The Penn Treebank
employs a rich arsenal of traces and empty productions (nodes which do not realize
any lexical material) to coindex displaced material with the position where it should
be interpreted semantically. The algorithm of Cahill, McCarthy, et al (2004) translates
the traces into corresponding reentrancies in the f-structure representation by treating
null constituents as full nodes and recording the traces in terms of index=i f-structure
annotations (Figure 3). Passive movement is captured and expressed at f-structure level
using a passive:+ annotation. Once a treebank tree is annotated with feature structure
equations by the annotation algorithm, the equations are collected, and a constraint
solver produces an f-structure.
In order to ensure the quality of the semantic forms extracted by our method, we
must first ensure the quality of the f-structure annotations. The results of two different
evaluations of the automatically generated f-structures are presented in Table 2. Both
use the evaluation software and triple encoding presented in Crouch et al (2002). The
first of these is against the DCU 105, a gold-standard set of 105 hand-coded f-structures
337
Computational Linguistics Volume 31, Number 3
Figure 3
Use of reentrancy between TOPIC and COMP to capture long-distance dependency in Penn
Treebank sentence wsj 0008 2, Until Congress acts, the government hasn?t any authority to issue new
debt obligations of any kind, the Treasury said.
from Section 23 of the Penn Treebank as described in Cahill, McCarthy, et al (2004). For
the full set of annotations they achieve precision of over 96.5% and recall of over 96.6%.
There is, however, a risk of overfitting when evaluation is limited to a gold standard
of this size. More recently, Burke, Cahill, et al (2004a) carried out an evaluation of the
automatic annotation algorithm against the publicly available PARC 700 Dependency
Bank (King et al 2003), a set of 700 randomly selected sentences from Section 23
which have been parsed, converted to dependency format, and manually corrected
and extended by human validators. They report precision of over 88.5% and recall of
over 86% (Table 2). The PARC 700 Dependency Bank differs substantially from both
the DCU 105 f-structure bank and the automatically generated f-structures in regard to
Table 2
Results of f-structure evaluation.
DCU 105 PARC 700
Precision 96.52% 88.57%
Recall 96.62% 86.10%
F-score 96.57% 87.32%
338
O?Donovan et al Large-Scale Induction and Evaluation of Lexical Resources
the style of linguistic analysis, feature nomenclature, and feature geometry. Some, but
not all, of these differences are captured by automatic conversion software. A detailed
discussion of the issues inherent in this process and a full analysis of results is presented
in Burke, Cahill, et al (2004a). Results broken down by grammatical function for the
DCU 105 evaluation are presented in Table 3. OBL (prepositional phrase) arguments are
traditionally difficult to annotate reliably. The results show, however, that with respect
to obliques, the annotation algorithm, while slightly conservative (recall of 82%), is very
accurate: 96% of the time it annotates an oblique, the annotation is correct.
A high-quality set of f-structures having been produced, the semantic form ex-
traction methodology is applied. This is based on and substantially extends both the
granularity and coverage of an idea in van Genabith, Sadler, and Way (1999):
For each f-structure generated, for each level of embedding we determine the local
PRED value and collect the subcategorisable grammatical functions present at that level
of embedding. (page 72)
Consider the automatically generated f-structure in Figure 4 for tree wsj 0003 22
in the Penn-II and Penn-III Treebanks. It is crucial to note that in the automatically
generated f-structures the value of the PRED feature is a lemma and not a semantic
form. Exploiting the information contained in the f-structure and applying the
method described above, we recursively extract the following nonempty semantic
forms: impose([subj, obj, obl:on]), in([obj]), of([obj]), and on([obj]). In effect,
in both the approach of van Genabith, Sadler, and Way (1999) and our approach,
semantic forms are reverse-engineered from automatically generated f-structures
for treebank trees. The automatically induced semantic forms contain the following
subcategorizable syntactic functions:
SUBJ OBJ OBJ2 OBLprep OBL2 COMP XCOMP PART
PART is not a syntactic function in the strict sense, but we decided to capture the
relevant co-occurrence patterns of verbs and particles in the semantic forms. Just as
Table 3
Precision and recall on automatically generated f-structures by feature against the DCU 105.
Feature Precision Recall F-score
ADJUNCT 892/968 = 92 892/950 = 94 93
COMP 88/92 = 96 88/102 = 86 91
COORD 153/184 = 83 153/167 = 92 87
DET 265/267 = 99 265/269 = 99 99
OBJ 442/459 = 96 442/461 = 96 96
OBL 50/52 = 96 50/61 = 82 88
OBLAG 12/12 = 100 12/12 = 100 100
PASSIVE 76/79 = 96 76/80 = 95 96
RELMOD 46/48 = 96 46/50 = 92 94
SUBJ 396/412 = 96 396/414 = 96 96
TOPIC 13/13 = 100 13/13 = 100 100
TOPICREL 46/49 = 94 46/52 = 88 91
XCOMP 145/153 = 95 145/146 = 99 97
339
Computational Linguistics Volume 31, Number 3
Figure 4
Automatically generated f-structure and extracted semantic forms for the Penn-II Treebank
string wsj 0003 22, In July, the Environmental Protection Agency imposed a gradual ban on virtually
all uses of asbestos.
OBLprep includes the prepositional head of the PP, PART includes the actual particle
which occurs, for example, add([subj, obj, part:up]).
In the work presented here, we substantially extend and scale the approach of
van Genabith, Sadler, and Way (1999) in regard to coverage, granularity, and eval-
uation. First, we scale the approach to the full WSJ section of the Penn-II Treebank
and the parsed Brown corpus section of Penn-III, with a combined total of approx-
imately 75,000 trees. Van Genabith, Sadler, and Way (1999) was proof of concept on
100 trees. Second, in contrast to the approach of van Genabith, Sadler, and Way (1999)
(and many other approaches), our approach fully reflects long-distance dependencies,
indicated in terms of traces in the Penn-II and Penn-III Treebanks and correspond-
ing reentrancies at f-structure. Third, in addition to abstract syntactic-function-
based subcategorization frames, we also compute frames for syntactic function?CFG
category pairs, for both the verbal heads and their arguments, and also generate
340
O?Donovan et al Large-Scale Induction and Evaluation of Lexical Resources
Table 4
Conflation of Penn Treebank tags.
Conflated Category Penn Treebank Category
JJ JJ
JJR
JJS
N NN
NNS
NNP
NNPS
PRP
RB RB
RBR
RBS
V VB
VBD
VBG
VBN
VBP
VBZ
MD
pure CFG-based subcategorization frames. Fourth, in contrast to the approach of
van Genabith, Sadler, and Way (1999) (and many other approaches), our method differ-
entiates between frames for active and passive constructions. Fifth, in contrast to that of
van Genabith, Sadler, and Way (1999), our method associates conditional probabilities
with frames. Sixth, we evaluate the complete set of semantic forms extracted (not
just a selection) against the manually constructed COMLEX (MacLeod, Grishman, and
Meyers 1994) resource.
In order to capture CFG-based categorial information, we add a CAT feature to
the f-structures automatically generated from the Penn-II and Penn-III Treebanks. Its
value is the syntactic category of the lexical item whose lemma gives rise to the PRED
value at that particular level of embedding. This makes it possible to classify words
and their semantic forms based on their syntactic category and reduces the risk of
inaccurate assignment of subcategorization frame frequencies due to POS ambiguity,
distinguishing, for example, between the nominal and verbal occurrences of the lemma
fight. With this, the output for the verb impose in Figure 4 is impose(v,[subj, obj,
obl:on]). For some of our experiments, we conflate the different verbal (and other)
tags used in the Penn Treebanks to a single verbal marker (Table 4). As a further
extension, the extraction procedure reads off the syntactic category of the head of
each of the subcategorized syntactic functions: impose(v,[subj(n),obj(n),obl:on]).3
In this way, our methodology is able to produce surface syntactic as well as abstract
functional subcategorization details. Dalrymple (2001) argues that there are cases,
albeit exceptional ones, in which constraints on syntactic category are an issue in
subcategorization. In contrast to much of the work reviewed in Section 3, which limits
itself to the extraction of surface syntactic subcategorization details, our system can
provide this information as well as details of grammatical function.
3 We do not associate syntactic categories with OBLs as they are always PPs.
341
Computational Linguistics Volume 31, Number 3
Another way in which we develop and extend the basic extraction algorithm
is to deal with passive voice and its effect on subcategorization behavior. Consider
Figure 5: Not taking into account that the example sentence is a passive construction,
the extraction algorithm extracts outlaw([subj]). This is incorrect, as outlaw is a tran-
sitive verb and therefore requires both a subject and an object to form a gram-
matical sentence in the active voice. To cope with this problem, the extraction al-
gorithm uses the feature-value pair passive:+, which appears in the f-structure at
the level of embedding of the verb in question, to mark that predicate as occurring
in the passive: outlaw([subj],p). The annotation algorithm?s accuracy in recognizing
passive constructions is reflected by the f-score of 96% reported in Table 3 for the
PASSIVE feature.
The syntactic functions COMP and XCOMP refer to clausal complements with
different predicate control patterns as described in Section 2. However, as it stands,
neither of these functions betrays anything about the syntactic nature of the constructs
in question. Many lexicons, both automatically acquired and manually created, are
more fine grained in their approaches to subcategorized clausal arguments, differ-
entiating, for example, between a that-clause and a to + infinitive clause (Ushioda
et al 1993). With only a slight modification, our system, along with the details
provided by the automatically generated f-structures, allows us to extract frames
with an equivalent level of detail. For example, to identify a that-clause, we use
Figure 5
Automatically generated f-structure for the Penn-II Treebank string wsj 0003 23. By 1997, almost
all remaining uses of cancer-causing asbestos will be outlawed.
342
O?Donovan et al Large-Scale Induction and Evaluation of Lexical Resources
Table 5
Semantic forms for the verb accept.
Semantic form Occurrences Conditional probability
accept([subj, obj]) 122 0.813
accept ([subj]) 11 0.073
accept([subj, comp]) 5 0.033
accept([subj, obl:as]) 3 0.020
accept([subj, obj, obl:as]) 3 0.020
accept([subj, obj, obl:from]) 3 0.020
accept([subj, obj, obl:at]) 1 0.007
accept([subj, obj, obl:for]) 1 0.007
accept([subj, obj, xcomp]) 1 0.007
the feature-value pair that:+ at f-structure level to read off the following subcate-
gorization frame for the verb add: add([subj,comp(that)]). Using the feature-value pair
to inf:+, we can identify to + infinitive clauses, resulting in the following frame for
the verb want: want([subj,xcomp(to inf)]). We can also derive control information about
open complements. In Figure 5, the reentrant XCOMP subject is identical to the subject
of will in the matrix clause, which allows us to induce information about the nature
of the external control of the XCOMP (i.e., whether it is subject or object control).
In order to estimate the likelihood of the co-occurrence of a predicate with a partic-
ular argument list, we compute conditional probabilities for subcategorization frames
based on the number of token occurrences in the corpus:
P (ArgList|?) = count(??ArgList?)?n
i=1 count(??ArgListi?)
where ArgList1... ArgListn are the possible argument lists which can occur for ?. Be-
cause of variations in verbal subcategorization across domains, probabilities are also
useful for predicting the way in which verbs behave in certain contexts. In Section 6,
we use the conditional probabilities to filter possible error judgments by our system.
Tables 5?7 show, with varying levels of analysis, the attested semantic forms for the
verb accept with their associated conditional probabilities. The effect of differentiating
between the active and passive occurrences of verbs can be seen in the different con-
ditional probabilities associated with the intransitive frame ([subj]) of the verb accept
(shown in boldface type) in Tables 5 and 6.4 Table 7 shows the joint grammatical-
function/syntactic-category-based subcategorization frames.
5. Results
We extract semantic forms for 4,362 verb lemmas from Penn-III. Table 8 shows the
number of distinct semantic form types (i.e., lemma and argument list combination)
4 Given these, it is possible to condition frames on both lemma (?) and voice (v: active/passive):
P (ArgList|?, v) = count(??ArgList, v?)?n
i=1 count(??ArgListi, v?)
343
Computational Linguistics Volume 31, Number 3
Table 6
Semantic forms for the verb accept marked with p for passive use.
Semantic form Occurrences Conditional probability
accept([subj, obj]) 122 0.813
accept ([subj],p) 9 0.060
accept([subj, comp]) 5 0.033
accept([subj, obl:as],p) 3 0.020
accept([subj, obj, obl:as]) 3 0.020
accept([subj, obj, obl:from]) 3 0.020
accept ([subj]) 2 0.013
accept([subj, obj, obl:at]) 1 0.007
accept([subj, obj, obl:for]) 1 0.007
accept([subj, obj, xcomp]) 1 0.007
Table 7
Semantic forms for the verb accept including syntactic category for each grammatical function.
Semantic form Occurrences Conditional probability
accept([subj(n), obj(n)]) 116 0.773
accept([subj(n)]) 11 0.073
accept([subj(n), comp(that)]) 4 0.027
accept([subj(n), obj(n), obl:from]) 3 0.020
accept([subj(n), obl:as]) 3 0.020
Other 13 0.087
extracted. Discriminating obliques by associated preposition and recording particle
information, the algorithm finds a total of 21,005 semantic form types, 16,000 occurring
in active voice and 5,005 in passive voice. When the obliques are parameterized for
prepositions and particles are included for particle verbs, we find an average of 4.82
semantic form types per verb. Without the inclusion of details for individual preposi-
tions or particles, there was an average of 3.45 semantic form types per verb. Unlike
many of the researchers whose work is reviewed in Section 3, we do not predefine the
frames extracted by our system. Table 9 shows the numbers of distinct frame types
extracted from Penn-II, ignoring PRED values.5 We provide two columns of statistics,
one in which all oblique (PP) arguments are condensed into one OBL function and
all particle arguments are condensed into part, and the other in which we differen-
tiate among obl:to (e.g., give), obl:on (e.g., rely), obl:for (e.g., compensate), etc., and
likewise for particles. Collapsing obliques and particles into simple functions, we extract
38 frame types. Discriminating particles and obliques by preposition, we extract 577
frame types. Table 10 shows the same results for Penn-III, with 50 simple frame types
and 1,084 types when parameterized for prepositions and particles. We also show the
result of applying absolute thresholding techniques to the semantic forms induced.
Applying an absolute threshold of five occurrences, we still generate 162 frame types
5 To recap, if two verbs have the same subcategorization requirements (e.g., give([subj, obj, obj2]),
send([subj, obj, obj2])), then that frame [subj, obj, obj2] is counted only once.
344
O?Donovan et al Large-Scale Induction and Evaluation of Lexical Resources
Table 8
Number of semantic form types for Penn-III.
Without prepositions and particles With prepositions and particles
Semantic form types 15,166 21,005
Active 11,038 16,000
Passive 4,128 5,005
Table 9
Number of frame types for verbs for Penn-II.
Without prepositions With prepositions
and particles and particles
Number of frame types 38 577
Number of singletons 1 243
Number occurring twice 1 84
Number occurring five or fewer times 7 415
Number occurring more than five times 31 162
from Penn-II and 221 from Penn-III. Briscoe and Carroll (1997), by comparison, employ
163 distinct predefined frames.
6. Evaluation
Most of the previous approaches discussed in Section 3 have been evaluated to
different degrees. In general, a small number of frequently occurring verbs is selected,
and the subcategorization frames extracted for these verbs (from some quantity of
unseen test data) are compared to a gold standard. The gold standard is either manually
custom-made based on the test data or adapted from an existing external resource
such as the OALD (Hornby 1980) or COMLEX (MacLeod, Grishman, and Meyers
1994). There are advantages and disadvantages to both types of gold standard. While
it is time-consuming to manually construct a custom-made standard, the resulting
standard has the advantage of containing only the subcategorization frames exhibited
in the test data. Using an existing externally produced resource is quicker, but the gold
Table 10
Number of frame types for verbs for Penn-III.
Without prepositions With prepositions
and particles and particles
Number of frame types 50 1,084
Number of singletons 6 544
Number occurring twice 2 147
Number occurring five or fewer times 12 863
Number occurring more than five times 38 221
345
Computational Linguistics Volume 31, Number 3
standard may contain many more frames than those which occur in the data from which
the test lexicon is induced or, indeed, may omit relevant correct frames contained in
the data. As a result, systems generally score better against custom-made, manually
established gold standards.
Carroll and Rooth (1998) achieve an F-score of 77% against the OALD when they
evaluate a selection of 100 verbs with absolute frequency of greater than 500 each.
Their system recognizes 15 frames, and these do not contain details of subcategorized-
for prepositions. Still, to date this is the largest number of verbs used in any of the
evaluations of the systems for English described in Section 3. Sarkar and Zeman (2000)
evaluate 914 Czech verbs against a custom-made gold standard and record a token
recall of 88%. However, their evaluation does not examine the extracted subcatego-
rization frames but rather the argument?adjunct distinctions posited by their sys-
tem. The largest lexical evaluation we know of is that of Schulte im Walde (2002b)
for German. She evaluates 3,000 German verbs with a token frequency between
10 and 2,000 against the Duden (Dudenredaktion 2001). We will refer to this work
and the methods and results presented by Schulte im Walde again in Sections 6.2
and 6.3.
We carried out a large-scale evaluation of our automatically induced lexicon (2,993
active verb lemmas for Penn-II and 3,529 for Penn-III, as well as 1,422 passive verb
lemmas from Penn-II) against the COMLEX resource. To our knowledge this is the most
extensive evaluation ever carried out for English lexical extraction. We conducted a
number of experiments on the subcategorization frames extracted from Penn-II and
Penn-III which are described and discussed in Sections 6.2, 6.3, and 6.4. Finding a
common format for the gold standard and induced lexical entries is a nontrivial task.
To ensure that we did not bias the evaluation in favor of either resource, we carried
out two different mappings for the frames from Penn-II and Penn-III: COMLEX-LFG
Mapping I and COMLEX-LFG Mapping II. For each mapping we carried out six basic
experiments (and two additional ones for COMLEX-LFG Mapping II) for the active
subcategorization frames extracted. Within each experiment, the following factors were
varied: level of prepositional phrase detail, level of particle detail, relative threshold
(1% or 5%), and incorporation of an expanded set of directional prepositions. Using
the second mapping we also evaluated the automatically extracted passive frames and
experimented with absolute thresholds. Direct comparison of subcategorization frame
acquisition systems is difficult because of variations in the number of frames extracted,
the number of test verbs, the gold standards used, the size of the test data, and the
level of detail in the subcategorization frames (e.g., whether they are parameterized
for specific prepositions). Therefore, in order to establish a baseline against which to
compare our results, following Schulte in Walde (2002b), we assigned the two most
frequent frame types (transitive and intransitive) by default to each verb and compared
this ?artificial? lexicon to the gold standard. The section concludes with a full discussion
of the reported results.
6.1 COMLEX
We evaluate our induced semantic forms against COMLEX (MacLeod, Grishman, and
Meyers 1994), a computational machine-readable lexicon containing syntactic infor-
mation for approximately 38,000 English headwords. Its creators paid particular
attention to the encoding of more detailed subcategorization information than is avail-
able in either the OALD or the LDOCE (Proctor 1978), both for verbs and for nouns
346
O?Donovan et al Large-Scale Induction and Evaluation of Lexical Resources
Figure 6
Intersection between active-verb lemma types in COMLEX and the Penn-II-induced lexicon.
and adjectives which take complements (Grishman, MacLeod, and Meyers 1994). By
choosing to evaluate against COMLEX, we set our sights high: Our extracted semantic
forms are fine-grained, and COMLEX is considerably more detailed than the OALD
or LDOCE used for earlier evaluations. While our system can generate semantic forms
for any lemma (regardless of part of speech) which induces a PRED value, we have
thus far evaluated the automatic generation of subcategorization frames for verbs
only. COMLEX defines 138 distinct verb frame types without the inclusion of specific
prepositions or particles.
As COMLEX contains information other than subcategorization details, it was
necessary for us to extract the subcategorization frames associated with each verbal
lexicon entry. The following is a sample entry for the verb reimburse:
(VERB :ORTH ?reimburse? :SUBC ((NP-NP)
(NP-PP :PVAL (?for?))
(NP)))
Each entry is organized as a nested set of typed feature-value lists. The first symbol
(i.e., VERB) gives the part of speech. The value of the :ORTH feature is the base form
of the verb. Any entry with irregular morphological behavior will also include the
features :PLURAL, :PAST, and so on, with the relevant values. All verbs have a :SUBC
feature, and for our purposes, this is the most interesting feature. In the case of the
example above, the subcategorization values specify that reimburse can occur with two
object noun phrases (NP-NP), an object noun phrase followed by a prepositional phrase
headed by for (NP-PP :PVAL (?for?)) or just an object noun phrase (NP). (Note that the
details of the subject are not included in COMLEX frames.) What makes the COMLEX
resource particularly suitable for our evaluation is that each of the complement types
(NP-NP, NP-PP, and NP) which make up the value of the :SUBC feature is associated with
a formal frame definition which looks like the following:
(vp-frame np-np :cs ((np 2)(np 3))
:gs (:subject 1 :obj 2 :obj2 3)
:ex ?she asked him his name?)
The value of the :cs feature is the constituent structure of the subcategorization
frame, which lists the syntactic CF-PSG constituents in sequence (omitting the sub-
ject, again). The value of the :gs feature is the grammatical structure which indicates
the functional role played by each of the CF-PSG constituents. The elements of the
347
Computational Linguistics Volume 31, Number 3
Figure 7
Intersection between active-verb lemma types in COMLEX and the Penn-III-induced lexicon.
constituent structure are indexed, and these indices are referenced in the :gs field.
The index 1 always refers to the surface subject of the verb. This mapping between
constituent structure and functional structure makes the information contained in
COMLEX particularly suitable as an evaluation standard for the LFG semantic forms
which we induce.
We present the evaluation for the verbs which occur in an active context in the
treebank. COMLEX does not provide passive frames. For Penn-II, there are 2,993
verb lemmas (used actively) that both resources have in common. 2,669 verb lemmas
appear in COMLEX but not in the induced lexicon, and 416 verb lemmas (used actively)
appear in the induced lexicon but not in COMLEX (Figure 6). For Penn-III, COMLEX
and the induced lexicon share 3,529 verb lemmas (used actively). This is shown in
Figure 7. 6
6.2 COMLEX-LFG Mapping I and Penn-II
In order to carry out the evaluation, we have to find a common format for the expression
of subcategorization information between our induced LFG-style subcategorization
frames and those contained in COMLEX. The following are the common syntactic
functions: SUBJ, OBJ, OBJi, COMP, and PART. Unlike our system, COMLEX does not
distinguish an OBL from an OBJi, so we converted all the obliques in the induced frames
to OBJi. As in COMLEX, the value of i depends on the number of objects/obliques
already present in the semantic form. COMLEX does not differentiate between COMPs
and XCOMPs as our system does (control information is expressed in a different way:
see Section 6.3), so we conflate our two LFG categories to that of COMP. The process is
summarized in Table 11.
The manually constructed COMLEX entries provide a gold standard against which
we evaluate the automatically induced frames. We calculate the number of true pos-
itives (tps) (where our semantic forms and those from COMLEX are the same), the
number of false negatives ( fns) (those frames which appeared in COMLEX but were not
produced by our system), and the number of false positives ( fps) (those frames
6 Given these figures, one might begin to wonder about the value of automatic induction. First, COMLEX
does not rank frames by probabilities, which are essential in disambiguation. Second, the coverage of
COMLEX is not complete: 518 lemmas ?discovered? by the induction experiment are not listed in
COMLEX; see the error analysis in Section 6.5.
348
O?Donovan et al Large-Scale Induction and Evaluation of Lexical Resources
Table 11
Mapping I: Merging of COMLEX and LFG syntactic functions.
Our syntactic functions COMLEX syntactic functions Merged function
SUBJ Subject SUBJ
OBJ Object OBJ
OBJ2 Obj2 OBJi
OBL Obj3
OBL2 Obj4
COMP Comp COMP
XCOMP
PART Part PART
produced by our system which do not appear in COMLEX). We calculate precision,
recall, and F-score using the following standard equations:
recall =
tp
tp + fn
precision =
tp
tp + fp
f-score =
2 ? recall ? precision
recall + precision
We use the frequencies associated with each of our semantic forms in order to set
a relative threshold to filter the selection of semantic forms. For a threshold of 1% we
disregard any semantic forms with a conditional probability (i.e., given a lemma) of
less than or equal to 0.01. As some verbs occur less frequently than others, we think it
is important to use a relative rather than absolute threshold (as in Carroll and Rooth
[1998], for instance) in this way. We carried out the evaluation in a similar way to
Schulte im Walde?s (2002b) for German, the only experiment comparable in scale to
ours. Despite the obvious differences in approach and language, this allows us to make
some tentative comparisons between our respective results. The statistics shown in
Table 12 give the results of three different experiments with the relative threshold set
to 1%. As for all the results tables, the baseline statistics (simply assigning the most
frequent frames, in this case transitive and intransitive, to each lemma by default) are
in each case shown in the left column, and the results achieved by our induced lexicon
are presented in the right column. Distinguishing between complement and adjunct
prepositional phrases is a notoriously difficult aspect of automatic subcategorization
frame acquisition. For this reason, following the evaluation setup in Schulte im Walde
(2002b), the three experiments vary with respect to the amount of prepositional infor-
mation contained in the subcategorization frames.
Experiment 1. Here we excluded subcategorized prepositional-phrase arguments en-
tirely from the comparison. In a manner similar to that of Schulte im Walde (2002b), any
349
Computational Linguistics Volume 31, Number 3
Table 12
Results of Penn-II evaluation of active frames against COMLEX (relative threshold of 1%).
Precision Recall F-score
Mapping I Baseline Induced Baseline Induced Baseline Induced
Experiment 1 66.1% 75.2% 65.8% 69.1% 66.0% 72.0%
Experiment 2 71.5% 65.5% 64.3% 63.1% 67.7% 64.3%
Experiment 3 64.7% 71.8% 11.9% 16.8% 20.1% 27.3%
frames containing an OBL were mapped to the same frame type minus that argument.
For example, the frame [subj,obl:for] becomes [subj]. Using a relative threshold of
1% (Table 12), our results (precision of 75.2%, recall of 69.1%, and F-score of 72.0%)
are remarkably similar to those of Schulte im Walde (2002b), who reports precision of
74.53%, recall of 69.74%, and an f-score of 72.05%.
Experiment 2. Here we include subcategorized prepositional phrase arguments but
only in their simplest form; that is, they were not parameterized for particular prepo-
sitions. For example, the frame [subj,obl:for] is rewritten as [subj,obl]. Using a
relative threshold of 1% (Table 12), our results (precision of 65.5%, recall of 63.1%, and
F-score of 64.3%) compare favorably to those of Schulte im Walde (2002b), who recorded
precision of 60.76%, recall of 63.91%, and an F-score of 62.30%.
Experiment 3. Here we used semantic forms which contain details of specific prepo-
sitions for any subcategorized prepositional phrase (e.g., [subj,obl:for]). Using a rela-
tive threshold of 1% (Table 12), our precision figure (71.8%) is quite high (in comparison
to 65.52% as recorded by Schulte im Walde [2002b]). However our recall (16.8%) is very
low (compared to the 50.83% that Schulte im Walde [2002b] reports). Consequently our
F-score (27.3%) is also low (Schulte im Walde [2002b] records an F-score of 57.24%). The
reason for this is discussed in Section 6.2.1.
The statistics in Table 13 are the result of the second experiment, in which the
relative threshold was increased to 5%. The effect of such an increase is obvious in
that precision goes up (by as much as 5%) for each of the three evaluations while
recall goes down (by as much as 5.5%). This is to be expected, as a greater threshold
means that there are fewer semantic forms associated with each verb in the induced
lexicon, but they are more likely to be correct because of their greater frequency of
occurrence. The conditional probabilities we associate with each semantic form together
with thresholding can be used to customize the induced lexicon to the task for which
it is required, that is, whether a very precise lexicon is preferred to one with broader
Table 13
Results of Penn-II evaluation of active frames against COMLEX (relative threshold of 5%).
Precision Recall F-score
Mapping I Baseline Induced Baseline Induced Baseline Induced
Experiment 1 66.1% 80.2% 65.8% 63.6% 66.0% 70.9%
Experiment 2 71.5% 69.6% 64.3% 56.9% 67.7% 62.7%
Experiment 3 64.7% 76.7% 11.9% 13.9% 20.1% 23.5%
350
O?Donovan et al Large-Scale Induction and Evaluation of Lexical Resources
coverage. In Tables 12 and 13, the baseline is exceeded in all experiments with the
exception of Experiment 2. This can be attributed to Mapping I, in which OBLi becomes
OBJi (Table 11). Experiment 2 includes obliques without the specific preposition, mean-
ing that in this mapping, the frame [subj,obj:with] becomes [subj,obj]. Therefore,
the transitive baseline frame scores better than it should against the gold standard. A
more fine-grained LFG-COMLEX mapping in which this effect disappears is presented
in Section 6.3.
6.2.1 Directional Prepositions. Our recall statistic was particularly low in the case of
evaluation using details of prepositions (Experiment 3, Tables 12 and 13). This can be
accounted for by the fact that the creators of COMLEX have chosen to err on the side
of overgeneration in regard to the list of prepositions which may occur with a verb and
a subcategorization frame containing a prepositional phrase. This is particularly true
of directional prepositions. For COMLEX, a list of 31 directional prepositions (Table 14)
was prepared and assigned in its entirety by default to any verb which can potentially
appear with any directional preposition in order to save time and avoid the risk of
missing prepositions. Grishman, MacLeod, and Meyers (1994) acknowledge that this
can lead to a preposition list which is ?a little rich? for a particular verb, but this is
the approach they have chosen to take. In a subsequent experiment, we incorporated
this list of directional prepositions by default into our semantic form induction process
in the same way as the creators of COMLEX have done. Table 15 shows that doing
so results in a significant improvement in the recall statistic (45.1%), as would be
expected, with the new statistic being almost three times as good as the result re-
ported in Table 12 for Experiment 3 (16.8%). There is also an improvement in the
precision figure (from 71.8% to 86.9%). This is due to a substantial increase in the
number of true positives (from 5,612 to 14,675) compared with a stationary false pos-
itive figure (2,205 in both cases). The f-score increases from 27.3% to 59.4%.
6.3 COMLEX-LFG Mapping II and Penn-II
The COMLEX-LFG Mapping I presented above establishes a ?least common denomi-
nator? for the COMLEX and our LFG-inspired resources. More-fine-grained mappings
are possible: in order to ensure that the mapping from our semantic forms to the
COMLEX frames did not oversimplify the information in the automatically extracted
subcategorization frames, we conducted a further set of experiments in which we
converted the information in the COMLEX entries to the format of our extracted
semantic forms. We explicitly differentiated between OBLs and OBJs by automatically
Table 14
COMLEX directional prepositions.
about across along around
behind below beneath between
beyond by down from
in inside into off
on onto out out of
outside over past through
throughout to toward toward
up up to via
351
Computational Linguistics Volume 31, Number 3
Table 15
Penn-II evaluation of active frames against COMLEX using p-dir list (relative threshold of 1%).
Mapping I Precision Recall F-score
Experiment 3 86.9% 45.1% 59.4%
deducing whether a COMLEX OBJi was coindexed with an NP or a PP. Furthermore, as
can be seen in the following example, COMLEX frame definitions contain details of the
control patterns of sentential complements, encoded using the :features attribute. This
allows for automatic discrimination between COMPs and XCOMPs.
(vp-frame to-inf-sc :cs (vp 2 :mood to-infinitive :subject 1)
:features (:control subject)
:gs (:subject 1 :comp 2)
:ex ?I wanted to come?)
The mapping is summarized in Table 16. The results of the subsequent evaluation are
presented in Tables 17 and 18. We have added Experiments 2a and 3a. These are the
same as Experiments 2 and 3, except that they additionally include the specific particle
with each PART function. While the recall figures in Tables 17 and 18 are slightly lower
than those in Tables 12 and 13, changing the mapping in this way results in an increase
in precision in each case (by as much as 11.6%). The results of the lexical evaluation
are consistently better than the baseline, in some cases by almost 16% (Experiment 2,
threshold 5%). Notice that in contrast to Tables 12 and 13, in the more-fine-grained
COMLEX-LFG Mapping II presented here, all experiments exceed the baseline.
6.3.1 Directional Prepositions. The recall figures for Experiments 3 and 3a in Table 17
(24.0% and 21.5%) and Table 18 (19.7% and 17.4%) drop in a similar fashion to the results
seen in Tables 12 and 13. For this reason, we again incorporated the list of 31 directional
prepositions (Table 14) by default and reran Experiments 3 and 3a for a threshold of
1%. The results are presented in Table 19. The effect was as expected: The recall scores
for the two experiments increased to 40.8% and 35.4% (from 24.0% and 22.5%), and the
F-scores increased to 54.4% and 49.7% (from 35.9% and 33.0%).
6.3.2 Passive Evaluation. Table 20 presents the results of evaluating the extracted pas-
sive semantic forms for 1,422 verb lemmas shared by the induced lexicon and COMLEX.
Table 16
Mapping II: Merging of COMLEX and LFG syntactic functions.
Our syntactic functions COMLEX syntactic functions Merged function
SUBJ Subject SUBJ
OBJ Object OBJ
OBJ2 Obj2 OBJ2
OBL Obj3 OBL
OBL2 Obj4 OBL2
COMP Comp COMP
XCOMP Comp XCOMP
PART Part PART
352
O?Donovan et al Large-Scale Induction and Evaluation of Lexical Resources
Table 17
Results of Penn-II evaluation of active frames against COMLEX (relative threshold of 1%).
Precision Recall F-score
Mapping II Baseline Induced Baseline Induced Baseline Induced
Experiment 1 72.1% 79.0% 58.5% 59.6% 64.6% 68.0%
Experiment 2 65.2% 77.1% 37.4% 50.4% 47.5% 61.0%
Experiment 2a 65.2% 76.4% 32.7% 44.5% 43.6% 56.3%
Experiment 3 65.2% 75.9% 15.2% 24.0% 24.7% 35.9%
Experiment 3a 65.2% 71.0% 13.6% 21.5% 22.5% 33.0%
Table 18
Results of Penn-II evaluation of active frames against COMLEX (relative threshold of 5%).
Precision Recall F-score
Mapping II Baseline Induced Baseline Induced Baseline Induced
Experiment 1 72.1% 83.5% 58.5% 54.7% 64.6% 66.1%
Experiment 2 65.2% 81.4% 37.4% 44.8% 47.5% 57.8%
Experiment 2a 65.2% 80.9% 32.7% 39.0% 43.6% 52.6%
Experiment 3 65.2% 75.9% 15.2% 19.7% 24.7% 31.3%
Experiment 3a 65.2% 75.5% 13.6% 17.4% 22.5% 28.3%
We applied lexical-redundancy rules (Kaplan and Bresnan 1982) to automatically con-
vert the active COMLEX frames to their passive counterparts: For example, subjects are
demoted to optional by oblique agents, and direct objects become subjects. The resulting
precision was very high (from 72.3% to 80.2%), and there was the expected drop in recall
when prepositional details were included (from 54.7% to 29.3%).
Table 19
Penn-II evaluation of active frames against COMLEX using p-dir list (relative threshold of 1%).
Mapping II Precision Recall F-score
Experiment 3 81.7% 40.8% 54.4%
Experiment 3a 83.1% 35.4% 49.7%
Table 20
Results of Penn-II evaluation of passive frames (relative threshold of 1%).
Passive Precision Recall F-score
Experiment 2 80.2% 54.7% 65.1%
Experiment 2a 79.7% 46.2% 58.5%
Experiment 3 72.6% 33.4% 45.8%
Experiment 3a 72.3% 29.3% 41.7%
353
Computational Linguistics Volume 31, Number 3
6.3.3 Absolute Thresholds. Many of the previous approaches discussed in Section 3 use
a limited number of verbs for evaluation, based on the verbs? absolute frequency in the
corpus. We carried out a similar experiment. Table 21 shows the results of Experiment
2 for all verbs, for the verb lemmas with an absolute frequency greater than 100, and
for verbs with a frequency greater than 200. The use of an absolute threshold results
in an increase in precision (from 77.1% to 82.3% and 81.7%), an increase in recall (from
50.4% to 60.8% to 58.7%), and an overall increase in F-score (from 61.0% to 69.9%
and 68.4%).
6.4 Penn-III (Mapping-II)
Recently we have applied our methodology to the Penn-III Treebank, a more balanced
corpus resource with a number of text genres. Penn-III consists of the WSJ section from
Penn-II as well as a parse-annotated subset of the Brown corpus. The Brown corpus
comprises 24,242 trees compiled from a variety of text genres including popular lore,
general fiction, science fiction, mystery and detective fiction, and humor. It has been
shown (Roland and Jurafsky 1998) that the subcategorization tendencies of verbs vary
across linguistic domains. Our aim, therefore, is to increase the scope of the induced
lexicon not only in terms of the verb lemmas for which there are entries, but also in
terms of the frames with which they co-occur. The f-structure annotation algorithm was
extended with only minor amendments to cover the parsed Brown corpus. The most
important of these was the way in which we distinguish between oblique and adjunct.
We noted in Section 4 that our method of assigning an oblique annotation in Penn-II
was precise, albeit conservative. Because of a change of annotation policy in Penn-III,
the -CLR tag (indicating a close relationship between a PP and the local syntactic head),
information which we had previously exploited, is no longer used. For Penn-III the
algorithm annotates all PPs which do not carry a Penn adverbial functional tag (such
as -TMP or -LOC) and occur as the sisters of the verbal head of a VP as obliques.
In addition, the algorithm annotates as obliques PPs associated with -PUT (locative
complements of the verb put) or -DTV (second object in ditransitives) tags.
When evaluating the application of the lexical extraction system on Penn-III, we
carried out two sets of experiments, identical in each case to those described for Penn-II
in Section 6.3, including the use of relative (1% and 5%) rather than absolute thresholds.
For the first set of experiments we evaluated the lexicon induced from the parse-
annotated Brown corpus only. This evaluation was performed for 2,713 active-verb
lemmas using the more fine-grained Mapping-II. Tables 22 and 23 show that the results
generally exceed the baseline, in some cases by almost 10%, similar to those recorded
for Penn-II (Tables 17 and 18). While the precision is slightly lower than that re-
ported for the experiments in Tables 17 and 18, in particular for Experiments 2, 2a, 3,
Table 21
Penn-II evaluation of active frames against COMLEX using absolute thresholds (Experiment 2).
Threshold Precision Recall F-score
All 77.1% 50.4% 61.0%
Threshold 100 82.3% 60.8% 69.9%
Threshold 200 81.7% 58.7% 68.4%
354
O?Donovan et al Large-Scale Induction and Evaluation of Lexical Resources
Table 22
Results of Penn-III active frames (Brown Corpus only) COMLEX comparison (relative threshold
of 1%).
Precision Recall F-Score
Mapping II Baseline Induced Baseline Induced Baseline Induced
Experiment 1 73.2% 79.2% 60.1% 60.0% 66.0% 68.2%
Experiment 2 66.0% 70.5% 37.5% 50.5% 47.8% 58.9%
Experiment 2a 66.0% 71.3% 32.7% 44.5% 43.7% 54.8%
Experiment 3 66.0% 64.3% 15.2% 23.1% 24.8% 34.0%
Experiment 3a 66.0% 64.1% 13.5% 20.7% 22.4% 31.3%
and 3a, in which details of obliques are included, the recall in each of these experi-
ments is slightly higher than that recorded for Penn-II. We conjecture that the main
reason for this is that the amended approach to the annotation of obliques is slightly
less precise and conservative than the largely -CLR-tag-driven approach taken for
Penn-II. Consequently we record an increase in recall and a drop in precision. This
trend is repeated in the second set of experiments. In this instance, we combined the
lexicon extracted from the WSJ with that extracted from the parse-annotated Brown
corpus, and evaluated the resulting resource for 3,529 active-verb lemmas. The results
are shown in Tables 24 and 25. The results compare very positively against the baseline.
The precision scores are lower (by between 1.5% and 9.7%) than those reported for
Penn-II (Tables 17 and 18). There has however been a significant increase in recall (up to
8.7%) and an overall increase in F-score (by up to 4.4%).
6.5 Error Analysis and Discussion
The work presented in this section highlights a number of issues associated with the
evaluation of automatically induced subcategorization frames against an existing exter-
nal gold standard, in this case COMLEX. While this evaluation approach is arguably
less labor-intensive than the manual construction of a custom-made gold standard,
it does introduce a number of difficulties into the evaluation procedure. It is a
nontrivial task to convert both the gold standard and the induced resource to a common
Table 23
Results of Penn-III active frames (Brown corpus only) COMLEX comparison (relative threshold
of 5%).
Precision Recall F-score
Mapping II Baseline Induced Baseline Induced Baseline Induced
Experiment 1 73.2% 82.7% 60.1% 56.4% 66.0% 67.0%
Experiment 2 66.0% 74.6% 37.5% 46.1% 47.8% 57.0%
Experiment 2a 66.0% 76.0% 32.7% 40.0% 43.7% 52.4%
Experiment 3 66.0% 69.2% 15.2% 18.7% 24.8% 29.5%
Experiment 3a 66.0% 69.0% 13.5% 16.6% 22.4% 26.7%
355
Computational Linguistics Volume 31, Number 3
Table 24
Results of Penn-III active frames (Brown and WSJ) COMLEX comparison (relative threshold of
1%).
Precision Recall F-score
Mapping II Baseline Induced Baseline Induced Baseline Induced
Experiment 1 71.2% 77.4% 62.9% 66.2% 66.8% 71.4%
Experiment 2 64.5% 70.4% 40.0% 58.0% 49.3% 63.6%
Experiment 2a 64.5% 71.5% 35.1% 51.9% 45.5% 60.2%
Experiment 3 64.5% 66.2% 17.0% 27.4% 26.8% 38.8%
Experiment 3a 64.5% 66.0% 15.1% 24.8% 24.5% 36.0%
format in order to facilitate evaluation. In addition, as our results show, the choice
of common format and mapping to it can affect the results. In COMLEX-LFG Map-
ping I (Section 6.2), we found that mapping from the induced lexicon to COMLEX
resulted in higher recall scores than those achieved when we (effectively) reversed the
mapping (COMLEX-LFG Mapping II [Section 6.3]). The first mapping is essentially a
conflation of our more fine-grained LFG grammatical functions with the more generic
COMLEX functions, while the second mapping tries to maintain as many distinctions
as possible.
Another drawback to using an existing external gold standard such as COMLEX
to evaluate an automatically induced subcategorization lexicon is that the resources
are not necessarily constructed from the same source data. As noted above, it is well doc-
umented (Roland and Jurafsky 1998) that subcategorization frames (and their frequen-
cies) vary across domains. We have extracted frames from two sources (the WSJ and the
Brown corpus), whereas COMLEX was built using examples from the San Jose Mercury
News, the Brown corpus, several literary works from the Library of America, scientific
abstracts from the U.S. Department of Energy, and the WSJ. For this reason, it is likely
to contain a greater variety of subcategorization frames than our induced lexicon. It is
also possible that because of human error, COMLEX contains subcategorization frames
the validity of which are in doubt, for example, the overgeneration of subcategorized-for
directional prepositional phrases. This is because the aim of the COMLEX project was to
construct as complete a set of subcategorization frames as possible, even for infrequent
verbs. Lexicographers were allowed to extrapolate from the citations found, a procedure
Table 25
Results of Penn-III active frames (Brown and WSJ) COMLEX comparison (relative threshold of
5%).
Precision Recall F-score
Mapping II Baseline Induced Baseline Induced Baseline Induced
Experiment 1 71.2% 82.0% 62.9% 61.0% 66.8% 69.9%
Experiment 2 64.5% 74.3% 40.0% 53.5% 49.3% 62.2%
Experiment 2a 64.5% 76.4% 35.1% 45.1% 45.5% 56.7%
Experiment 3 64.5% 71.1% 17.0% 21.5% 26.8% 33.0%
Experiment 3a 64.5% 70.8% 15.1% 19.2% 24.5% 30.2%
356
O?Donovan et al Large-Scale Induction and Evaluation of Lexical Resources
which is bound to be less certain than the assignment of frames based entirely on exist-
ing examples. As a generalization, Briscoe (2001) notes that lexicons such as COMLEX
tend to demonstrate high precision but low recall. Briscoe and Carroll (1997) report
on manually analyzing an open-class vocabulary of 35,000 head words for predicate
subcategorization information and comparing the results against the subcategorization
details in COMLEX. Precision was quite high (95%), but recall was low (84%). This has
an effect on both the precision and recall scores of our system against COMLEX. In order
to ascertain the effect of using COMLEX as a gold standard for our induced lexicon,
we carried out some more-detailed error analysis, the results of which are summarized
in Table 26. We randomly selected 80 false negatives (fn) and 80 false positives (fp)
across a range of active frame types containing prepositional and particle detail taken
from Penn-III and manually examined them in order to classify them as ?correct? or
?incorrect.? Of the 80 fps, 33 were manually judged to be legitimate subcategorization
frames. For example, as Table 26 shows, there are a number of correct transitive verbs
([subj,obj]) in our automatically induced lexicon which are not included in COMLEX.
This examination was also useful in highlighting to us the frame types on which
the lexical extraction procedure was performing poorly, in our case, those containing
XCOMPs and those containing OBJ2S. Out of 80 fns, 14 were judged to be incorrect when
manually examined. These can be broken down as follows: one intransitive frame, three
ditransitive frames, three frames containing a COMP, and seven frames containing an
oblique were found to be invalid.
7. Lexical Accession Rates
In addition to evaluating the quality of our extracted semantic forms, we also examined
the rate at which they are induced. This can be expressed as a measure of the coverage
of the induced lexicon on new data. Following Hockenmaier, Bierner, and Baldridge
(2002), Xia (1999), and Miyao, Ninomiya, and Tsujii (2004), we extract a reference
lexicon from Sections 02?21 of the WSJ. We then compare this to a test lexicon from
Section 23. Table 27 shows the results of the evaluation of the coverage of an induced
lexicon for verbs only. There is a corresponding semantic form in the reference lexicon
for 89.89% of the verbs in Section 23. 10.11% of the entries in the test lexicon did not
appear in the reference lexicon. Within this group, we can distinguish between known
words, which have an entry in the reference lexicon, and unknown words, which do
not exist at all in the reference lexicon. In the same way we make the distinction
Table 26
Error analysis.
Frame type COMLEX: False negatives Induced: False positives
Correct Incorrect Correct Incorrect
[subj] 9 1 4 6
[subj, obj] 10 0 9 1
[subj, obj, obj2] 7 3 1 9
[.., xcomp, ..] 10 0 1 10
[.., comp, ..] 7 3 4 5
[.., obl, ..] 23 7 14 16
357
Computational Linguistics Volume 31, Number 3
Table 27
Coverage of induced lexicon (WSJ 02?21) on unseen data (WSJ 23) (verbs only).
Entries also in reference lexicon 89.89%
Entries not in reference lexicon 10.11%
Known words 7.85%
Known words, known frames 7.85%
Known words, unknown frames 0
Unknown words 2.32%
Unknown words, known frames 2.32%
Unknown words, unknown frames 0
between known frames and unknown frames. There are, therefore, four different cases
in which an entry may not appear in the reference lexicon. Table 27 shows that the
most common case is that of known verbs occurring with a different, although known,
subcategorization frame (7.85%).
The rate of accession may also be represented graphically. In Charniak (1996) and
Krotov et al (1998), it was observed that treebank grammars (CFGs extracted from
treebanks) are very large and grow with the size of the treebank. We were interested in
discovering whether the acquisition of lexical material from the same data displayed a
similar propensity. Figure 8 graphs the rate of induction of semantic form and CFG rule
types from Penn-III (the WSJ and parse-annotated Brown corpus combined). Because
of the variation in the size of sections between the Brown and the WSJ, we plotted
accession against word count. The first part of the graph (up to 1,004,414 words)
Figure 8
Comparison of accession rates for semantic form and CFG rule types for Penn-III (nonempty
frames) (WSJ followed by Brown).
358
O?Donovan et al Large-Scale Induction and Evaluation of Lexical Resources
represents the rate of accession from the WSJ, and the final 384,646 words are those
of the Brown corpus. The seven curves represent the following: The acquisition of
semantic form types (nonempty) for all syntactic categories with and without specific
preposition and particle information, the acquisition of semantic form types (non-
empty) for all verbs with and without specific preposition and particle information,
the number of lemmas associated with the extract semantic forms, and the acqui-
sition of CFG rule types. The curve representing the growth in the overall size of
the lexicon is similar in shape to that of the PCFG, while the rate of increase in
the number of verbal semantic forms (particularly when obliques and particles are
excluded) appears to slow more quickly. Figure 8 shows the effect of domain di-
versity from the Brown section in terms of increased growth rates for 1e+06 words
upward. Figure 9 depicts the same information, this time extracted from the Brown
section first followed by the WSJ. The curves are different, but similar trends are
represented. This time the effects of domain diversity for the Brown section are
discernible by comparing the absolute accession rate for the 0.4e+06 mark between
Figures 8 and 9.
Figure 10 shows the result when we abstract away from semantic forms (verb
frame combinations) to subcategorization frames and plot their rate of acces-
sion. The graph represents the growth rate of frame types for Penn-III (WSJ fol-
lowed by Brown and Brown followed by WSJ). The curve rises sharply initially
but gradually levels, practically flattening out, despite the increase in the number
of words. This reflects the information about Section 23 in Table 27, where we demon-
strate that although new verb frame combinations occur, all of the frame types in
Section 23 have been seen by the lexical extraction program in previous sections.
Figure 9
Comparison of accession rates for semantic form and CFG rule types for Penn-III (nonempty
frames) (Brown followed by WSJ).
359
Computational Linguistics Volume 31, Number 3
Figure 10
Accession rates for frame types (without prepositions and particles) for Penn-III.
Figure 11 shows that including information about prepositions and particles in the
frames results in an accession rate which continues to grow, albeit ever more slowly,
with the increase in size of the extraction data. This emphasizes the advantage of our
approach, which extracts frames containing such information without the limitation
of predefinition.
Figure 11
Accession rates for frame types for Penn-III.
360
O?Donovan et al Large-Scale Induction and Evaluation of Lexical Resources
8. Conclusions and Further Work
We have presented an algorithm for the extraction of semantic forms (or subcatego-
rization frames) from the Penn-II and Penn-III Treebanks, automatically annotated with
LFG f-structures. In contrast to many other approaches, ours does not predefine the sub-
categorization frames we extract. We have applied the algorithm to the WSJ sections of
Penn-II (50,000 trees) (O?Donovan et al 2004) and to the parse-annotated Brown corpus
of Penn-III (almost 25,000 additional trees). We extract syntactic-function-based subcat-
egorization frames (LFG semantic forms) and traditional CFG category-based frames, as
well as mixed-function-category-based frames. Unlike many other approaches to sub-
categorization frame extraction, our system properly reflects the effects of long-distance
dependencies. Also unlike many approaches, our method distinguishes between active
and passive frames. Finally, our system associates conditional probabilities with the
frames we extract. Making the distinction between the behavior of verbs in active and
passive contexts is particularly important for the accurate assignment of probabilities
to semantic forms. We carried out an extensive evaluation of the complete induced
lexicon against the full COMLEX resource. To our knowledge, this is the most extensive
qualitative evaluation of subcategorization extraction in English. The only evaluation of
a similar scale is that carried out by Schulte im Walde (2002b) for German. The results
reported here for Penn-II compare favorably against the baseline and, in fact, are an
improvement on those reported in O?Donovan et al (2004). The results for the larger,
more domain-diverse Penn-III lexicon are very encouraging, in some cases almost 15%
above the baseline. We believe our semantic forms are fine-grained, and by choosing
to evaluate against COMLEX, we set our sights high: COMLEX is considerably more
detailed than the OALD or LDOCE used for other earlier evaluations. Our error analysis
also revealed some interesting issues associated with using an external standard such as
COMLEX. In the future, we hope to evaluate the automatic annotations and extracted
lexicon against Propbank (Kingsbury and Palmer 2002).
Apart from the related approach of Miyao, Ninomiya, and Tsujii (2004), which
does not distinguish between argument and adjunct prepositional phrases, our
treebank and automatic f-structure annotation-based architecture for the automatic
acquisition of detailed subcategorization frames is quite unlike any of the architec-
tures presented in the literature. Subcategorization frames are reverse-engineered and
almost a byproduct of the automatic f-structure annotation algorithm. It is important
to realize that the induction of lexical resources is part of a larger project on the
acquisition of wide-coverage, robust, probabilistic, deep unification grammar resources
from treebanks Burke, Cahill, et al (2004b). We are already using the extracted seman-
tic forms in parsing new text with robust, wide-coverage probabilistic LFG grammar
approximations automatically acquired from the f-structure-annotated Penn-II tree-
bank, specifically in the resolution of LDDs, as described in Cahill, Burke, et al (2004).
We hope to be able to apply our lexical acquisition methodology beyond existing
parse-annotated corpora (Penn-II and Penn-III): New text is parsed by our probabilistic
LFG approximations into f-structures from which we can then extract further seman-
tic forms. The work reported here is part of the core components for bootstrapping
this approach.
In the shorter term, we intend to make the extracted subcategorization lexicons from
Penn-II and Penn-III available as a downloadable public-domain research resource.
We have also applied our more general unification grammar acquisition meth-
odology to the TIGER Treebank (Brants et al 2002) and Penn Chinese Treebank
(Xue, Chiou, and Palmer 2002), extracting wide-coverage, probabilistic LFG grammar
361
Computational Linguistics Volume 31, Number 3
approximations and lexical resources for German (Cahill et al 2003) and Chinese
(Burke, Lam, et al 2004). The lexical resources, however, have not yet been evaluated.
This, and much else, has to await further research.
Acknowledgments
The research reported here is partially
supported by Enterprise Ireland Basic
Research Grant SC/2001/186, an IRCSET
PhD fellowship award, and an IBM PhD
fellowship award. We are particularly
grateful to our anonymous reviewers, whose
insightful comments have helped to improve
this article considerably.
References
Ades, Anthony and Mark Steedman. 1982.
On the order of words. Linguistics and
Philosophy, 4(4):517? 558.
Boguraev, Branimir, Edward Briscoe,
John Carroll, David Carter, and
Claire Grover. 1987. The derivation of
a grammatically indexed lexicon from
the Longman Dictionary of Contemporary
English. In Proceedings of the 25th
Annual Meeting of the Association of
Computational Linguistics, pages 193?200,
Stanford, CA.
Brants, Sabine, Stefanie Dipper, Silvia
Hansen, Wolfgang Lezius, and George
Smith. 2002. The TIGER Treebank. In
Proceedings of the Workshop on Treebanks
and Linguistic Theories, Sozopol, Bulgaria.
Brent, Michael. 1993. From grammar to
lexicon: Unsupervised learning of lexical
syntax. Computational Linguistics,
19(2):203?222.
Bresnan, Joan. 2001. Lexical-Functional Syntax.
Blackwell, Oxford.
Briscoe, Edward. 2001. From dictionary to
corpus to self-organizing dictionary:
Learning valency associations in the face
of variation and change. In Proceedings of
Corpus Linguistics 2001, Lancaster, UK.
Briscoe, Edward and John Carroll. 1997.
Automatic extraction of subcategorization
from corpora. In Proceedings of the Fifth
ACL Conference on Applied Natural
Language Processing, pages 356?363,
Washington, DC.
Burke, Michael, Aoife Cahill, Ruth
O?Donovan, Josef van Genabith, and
Andy Way. 2004a. Evaluation of an
automatic annotation algorithm against
the PARC 700 Dependency Bank. In
Proceedings of the Ninth International
Conference on LFG, pages 101?121,
Christchurch, New Zealand.
Burke, Michael, Aoife Cahill, Ruth
O?Donovan, Josef van Genabith, and
Andy Way. 2004b. Treebank-based
acquisition of wide-coverage, probabilistic
LFG resources: Project overview, results
and evaluation. In Proceedings of the
Workshop ?Beyond Shallow Analyses?
Formalisms and Statistical Modelling
for Deep Analyses? at the First International
Joint Conference on Natural Language
Processing (IJCNLP-04), Hainan
Island, China.
Burke, Michael, Olivia Lam, Rowena
Chan, Aoife Cahill, Ruth O?Donovan,
Adams Bodomo, Josef van Genabith,
and Andy Way. 2004. Treebank-based
acquisition of a Chinese lexical-functional
grammar. In Proceedings of the 18th
Pacific Asia Conference on Language,
Information and Computation,
pages 161?172, Tokyo.
Cahill, Aoife, Michael Burke, Ruth
O?Donovan, Josef van Genabith, and Andy
Way. 2004. Long-distance dependency
resolution in automatically acquired
wide-coverage PCFG-based LFG
approximations. In Proceedings
of the 42nd Annual Meeting of the
Association of Computational Linguistics,
pages 320?327, Barcelona.
Cahill, Aoife, Martin Forst, Mairead
McCarthy, Ruth O?Donovan, Christian
Rohrer, Josef van Genabith, and Andy
Way. 2003. Treebank-based multilingual
unification-grammar development. In
Proceedings of the Workshop on Ideas and
Strategies for Multilingual Grammar
Development at the 15th ESS-LLI,
pages 17?24, Vienna.
Cahill, Aoife, Mairead McCarthy,
Michael Burke, Ruth O?Donovan,
Josef van Genabith, and Andy Way.
2004. Evaluating automatic F-structure
annotation for the Penn-II Treebank.
Journal of Research on Language and
Computation, 2(4):523?547.
Cahill, Aoife, Mairead McCarthy, Josef van
Genabith, and Andy Way. 2002. Parsing
text with a PCFG derived from Penn-II
with an automatic F-structure annotation
procedure. In Proceedings of the Seventh
International Conference on LFG, edited by
Miriam Butt and Tracy Holloway King.
CSLI Publications, Stanford, CA,
pages 76?95.
362
O?Donovan et al Large-Scale Induction and Evaluation of Lexical Resources
Carroll, Glenn and Mats Rooth. 1998. Valence
induction with a head-lexicalised PCFG. In
Proceedings of the Third Conference on
Empirical Methods in Natural Language
Processing, pages 36?45,
Granada, Spain.
Charniak, Eugene. 1996. Tree-bank
grammars. In AAAI-96: Proceedings of the
Thirteenth National Conference on Artificial
Intelligence. MIT Press, Cambridge, MA,
pages 1031?1036.
Chen, John and K. Vijay-Shanker. 2000.
Automated extraction of TAGs from the
Penn Treebank. In Proceedings of the 38th
Annual Meeting of the Association of
Computational Linguistics, pages 65?76,
Hong Kong.
Collins, Michael. 1997. Three generative
lexicalised models for statistical parsing. In
Proceedings of the 35th Annual Meeting of the
Association for Computational Linguistics,
pages 16?23, Madrid.
Crouch, Richard, Ron Kaplan, Tracy King,
and Stefan Riezler. 2002. A comparison
of evaluation metrics for a broad coverage
parser. In Proceedings of Workshop
?Beyond PARSEVAL? at Third International
Conference on Language Resources and
Evaluation, Las Palmas, Spain.
Dalrymple, Mary. 2001. Lexical Functional
Grammar. Volume 34 of Syntax and
Semantics. Academic Press, New York.
Dowty, David. 1982. Grammatical relations
and Montague grammar. In Pauline
Jacobson and Geoffrey Pullum, editors,
The Nature of Syntactic Representation.
Reidel, Dordrecht, The Netherlands,
pages 79?130.
Dudenredaktion, editor. 2001. DUDEN?Das
Stilworterbuch. [DUDEN?The Style
Dictionary]. Number 2 in Duden in zwo?lf
Banden [Duden in Twelve Volumes].
Dudenverlag, Mannheim, Germany.
Eckle, Judith. 1999. Linguistic Knowledge for
Automatic Lexicon Acquisition from German
Text Corpora. Ph.D. thesis, University of
Stuttgart, Germany.
Frank, Anette. 2000. Automatic F-structure
annotation of treebank trees. In Proceedings
of the Fifth International Conference on LFG,
Berkeley, CA, edited by Miriam Butt
and Tracy Holloway King. CSLI,
pages 139?160.
Grishman, Ralph, Catherine MacLeod, and
Adam Meyers. 1994. COMLEX syntax:
Building a computational lexicon. In
Proceedings of the 15th International
Conference on Computational Linguistics,
pages 268?272, Kyoto.
Hajic, Jan. 1998. Building a syntactically
annotated corpus: The Prague
Dependency Treebank. In Issues in Valency
and Meaning, edited by Eva Hajicova.
Karolinum, Prague, Czech Republic,
pages 106?132.
Hindle, Donald and Mats Rooth. 1993.
Ambiguity and lexical relations.
Computational Linguistics, 19(1):103?120.
Hockenmaier, Julia, Gann Bierner, and Jason
Baldridge. 2004. Extending the coverage of
a CCG system. Journal of Language and
Computation, 2(2):165?208.
Hornby, Albert, editor. 1980. Oxford Advanced
Learner?s Dictionary of Current English.
Oxford University Press, Oxford, UK.
Joshi, Aravind. 1988. Tree adjoining
grammars. In David Dowty, Lauri
Karttunen, and Arnold Zwicky, editors,
Natural Language Parsing. Cambridge
University Press, Cambridge,
pages 206?250.
Kaplan, Ronald and Joan Bresnan. 1982.
Lexical functional grammar: A formal
system for grammatical representation. In
Joan Bresnan, editor, The Mental
Representation of Grammatical Relations. MIT
Press, Cambridge, MA, pages 173?281.
King, Tracy Holloway, Richard Crouch,
Stefan Riezler, Mary Dalrymple, and
Ronald Kaplan. 2003. The PARC 700
Dependency Bank. In Proceedings of the
Fourth International Workshop on
Linguistically Interpreted Corpora, Budapest.
Kingsbury, Paul and Martha Palmer. 2002.
From Treebank to PropBank. In Proceedings
of the Third International Conference on
Language Resources and Evaluation
(LREC-2002), Las Palmas, Spain.
Kinyon, Alexandra and Carlos Prolo. 2002.
Identifying verb arguments and their
syntactic function in the Penn Treebank. In
Proceedings of the Third LREC Conference,
pages 1982?1987, Las Palmas, Spain.
Korhonen, Anna. 2002. Subcategorization
acquisition. As Technical Report
UCAM-CL-TR-530, Computer Laboratory,
University of Cambridge, UK.
Krotov, Alexander, Mark Hepple, Robert
Gaizauskas, and Yorick Wilks. 1998.
Compacting the Penn Treebank grammar.
In Proceedings of the 36th Annual Meeting of
the Association for Computational Linguistics
and 17th International Conference on
Computational Linguistics, pages 669?703,
Montreal.
Levin, Beth. 1993. English Verb Classes and
Alternations. University of Chicago Press,
Chicago.
363
Computational Linguistics Volume 31, Number 3
MacLeod, Catherine, Ralph Grishman, and
Adam Meyers. 1994. The Comlex Syntax
Project: The first year. In Proceedings of the
ARPA Workshop on Human Language
Technology, pages 669?703, Princeton.
Magerman, David. 1994. Natural Language
Parsing as Statistical Pattern Recognition.
Ph.D. thesis, Stanford University,
Stanford, CA.
Magerman, David. 1995. Statistical decision
tree models for parsing. In Proceedings of
the 33rd Annual Meeting for the Association
of Computational Linguistics, pages 276?283,
Cambridge, MA.
Manning, Christopher. 1993. Automatic
acquisition of a large subcategorisation
dictionary from corpora. In Proceedings of
the 31st Annual Meeting of the Association for
Computational Linguistics, pages 235?242,
Columbus, OH.
Marcus, Mitchell, Grace Kim, Mary Ann
Marcinkiewicz, Robert MacIntyre, Mark
Ferguson, Karen Katz, and Britta
Schasberger. 1994. The Penn Treebank:
Annotating predicate argument structure.
In Proceedings of the ARPA Human Language
Technology Workshop, Princeton.
Marinov, Svetoslav and Cecilia Hemming.
2004. Automatic Extraction of
Subcategorization Frames from the
Bulgarian Tree Bank. Unpublished
manuscript, Graduate School of Language
Technology, Go?teborg, Sweden.
Meyers, Adam, Catherine MacLeod, and
Ralph Grishman. 1996. Standardization of
the complement/adjunct distinction.
In Proceedings of the Seventh
EURALEX International Conference,
Go?teborg, Sweden.
Miyao, Yusuke, Takashi Ninomiya, and
Jun?ichi Tsujii. 2004. Corpus-oriented
grammar development for acquiring a
head-driven phrase structure grammar
from the Penn Treebank. In Proceedings of
the First International Joint Conference on
Natural Language Processing (IJCNLP-04),
pages 390?398, Hainan Island, China.
Nakanishi, Hiroko, Yusuke Miyao, and
Jun?ichi Tsujii. 2004. Using inverse
lexical rules to acquire a wide-coverage
lexicalized grammar. In Proceedings
of the Workshop ?Beyond Shallow
Analyses?Formalisms and Statistical
Modelling for Deep Analyses? at the First
International Joint Conference on Natural
Language Processing (IJCNLP-04), Hainan
Island, China.
O?Donovan, Ruth, Michael Burke,
Aoife Cahill, Josef van Genabith, and
Andy Way. 2004. Large-scale induction
and evaluation of lexical resources from
the Penn-II Treebank. In Proceedings
of the 42nd Annual Meeting of the
Association of Computational Linguistics,
pages 368?375, Barcelona.
Pollard, Carl and Ivan Sag. 1994.
Head-Driven Phrase Structure Grammar.
University of Chicago Press, Chicago.
Proctor, Paul, editor. 1978. Longman
Dictionary of Contemporary English.
Longman, London.
Roland, Douglas and Daniel Jurafsky.
1998. How verb subcategorization
frequencies are affected by corpus
choice. In Proceedings of the 36th
Annual Meeting of the Association
for Computational Linguistics and
17th International Conference on
Computational Linguistics,
pages 1117?1121, Montreal.
Sadler, Louisa, Josef van Genabith,
and Andy Way. 2000. Automatic
F-structure annotation from the
AP Treebank. In Proceedings of the
Fifth International Conference on LFG,
Berkeley, CA, edited by Miriam
Butt and Tracy Holloway King. CSLI,
pages 226?243.
Sarkar, Anoop and Daniel Zeman. 2000.
Automatic extraction of subcategorization
frames for Czech. In Proceedings of the 19th
International Conference on Computational
Linguistics, pages 691?697, Saarbru?cken,
Germany.
Schulte im Walde, Sabine. 2002a. A
subcategorisation lexicon for German
verbs induced from a lexicalised PCFG. In
Proceedings of the Third LREC Conference,
pages 1351?1357, Las Palmas, Spain.
Schulte im Walde, Sabine. 2002b. Evaluating
verb subcategorisation frames learned by a
German statistical grammar against
manual definitions in the Duden
Dictionary. In Proceedings of the 10th
EURALEX International Congress,
pages 187?197, Copenhagen.
Simov, Kiril, Gergana Popova, and Petya
Osenova. 2002. HPSG-based syntactic
treebank of Bulgarian (BulTreeBank). In
Andrew Wilson, Paul Rayson, and Tony
McEnery, editors, A Rainbow of Corpora:
Corpus Linguistics and the Languages of the
World. Lincon-Europa, Munich,
pages 135?142.
Ushioda, Akira, David Evans, Ted Gibson,
and Alex Waibel. 1993. The Automatic
acquisition of frequencies of verb
subcategorization frames from tagged
364
O?Donovan et al Large-Scale Induction and Evaluation of Lexical Resources
corpora. In SIGLEX ACL Workshop on the
Acquisition of Lexical Knowledge from Text,
pages 95?106, Columbus, OH.
van Genabith, Josef, Louisa Sadler, and
Andy Way. 1999. Data-driven compilation
of LFG semantic forms. In EACL-99
Workshop on Linguistically Interpreted
Corpora (LINC-99), pages 69?76, Bergen,
Norway.
van Genabith, Josef, Andy Way, and Louisa
Sadler. 1999. Semi-automatic generation of
F-structures from Treebanks. In
Proceedings of the Fourth International
Conference on Lexical-Functional Grammar,
Manchester, UK. Available at
http://cslipublications.stanford.edu/.
Wauschkuhn, Oliver. 1999. Automatische
Extraktion von Verbvalenzen aus deutschen
Textkorpora [Automatic Extraction of Verb
Valence from German Text Corpora]. PhD
thesis, University of Stuttgart, Germany.
Xia, Fei. 1999. Extracting tree adjoining
grammars from bracketed corpora.
In Fifth Natural Language Processing
Pacific Rim Symposium (NLPRS-99),
Beijing, China.
Xue, Nianwen, Fu-Dong Chiou, and Martha
Palmer. 2002. Building a large-scale
annotated Chinese corpus. In Proceedings
of the 19th International Conference on
Computational Linguistics (COLING 2002),
Taipei, Taiwan.
365

Wide-Coverage Deep Statistical Parsing
Using Automatic Dependency
Structure Annotation
Aoife Cahill?
Dublin City University
Michael Burke??,?
Dublin City University
IBM Center for Advanced Studies
Ruth O?Donovan??
Dublin City University
Stefan Riezler?
Palo Alto Research Center
Josef van Genabith??,?
Dublin City University
IBM Center for Advanced Studies
Andy Way??,?
Dublin City University
IBM Center for Advanced Studies
A number of researchers have recently conducted experiments comparing ?deep? hand-crafted
wide-coverage with ?shallow? treebank- and machine-learning-based parsers at the level of
dependencies, using simple and automatic methods to convert tree output generated by the
shallow parsers into dependencies. In this article, we revisit such experiments, this time using
sophisticated automatic LFG f-structure annotation methodologies with surprising results. We
compare various PCFG and history-based parsers to find a baseline parsing system that fits
best into our automatic dependency structure annotation technique. This combined system of
syntactic parser and dependency structure annotation is compared to two hand-crafted, deep
constraint-based parsers, RASP and XLE. We evaluate using dependency-based gold standards
? Now at the Institut fu?r Maschinelle Sprachverarbeitung, Universita?t Stuttgart, Germany. E-mail: aoife.
cahill@ims.uni-stuttgart.de.
?? National Centre for Language Technology, Dublin City University, Dublin 9, Ireland.
? IBM Dublin Center for Advanced Studies (CAS), Dublin 15, Ireland.
? Now at Google Inc., Mountain View, CA.
Submission received: 24 August 2005; revised submission received: 20 March 2007; accepted for publication:
2 June 2007.
? 2008 Association for Computational Linguistics
Computational Linguistics Volume 34, Number 1
and use the Approximate Randomization Test to test the statistical significance of the results.
Our experiments show that machine-learning-based shallow grammars augmented with so-
phisticated automatic dependency annotation technology outperform hand-crafted, deep, wide-
coverage constraint grammars. Currently our best system achieves an f-score of 82.73% against
the PARC 700 Dependency Bank, a statistically significant improvement of 2.18% over the
most recent results of 80.55% for the hand-crafted LFG grammar and XLE parsing system
and an f-score of 80.23% against the CBS 500 Dependency Bank, a statistically significant
3.66% improvement over the 76.57% achieved by the hand-crafted RASP grammar and parsing
system.
1. Introduction
Wide-coverage parsers are often evaluated against gold-standard CFG trees (e.g.,
Penn-II WSJ Section 23 trees) reporting traditional PARSEVALmetrics (Black et al 1991)
of labeled and unlabeled bracketing precision, recall and f-score measures, number of
crossing brackets, complete matches, and so forth. Although tree-based parser evalua-
tion provides valuable insights into the performance of grammars and parsing systems,
it is subject to a number of (related) drawbacks:
1. Bracketed trees do not always provide NLP applications with enough
information to carry out the required tasks: Many applications involve
a deeper analysis of the input in the form of semantically motivated
information such as deep dependency relations, predicate?argument
structures, or simple logical forms.
2. A number of alternative, but equally valid tree representations can
potentially be given for the same input. To give just a few examples: In
English, VPs containing modals and auxiliaries can be analyzed using
(predominantly) binary branching rules (Penn-II [Marcus et al 1994]), or
employ flatter analyses where modals and auxiliaries are sisters of the
main verb (AP treebank [Leech and Garside 1991]), or indeed do without
a designated VP constituent at all (SUSANNE [Sampson 1995]). Treebank
bracketing guidelines can use ?traditional? CFG categories such as S, NP,
and so on (Penn-II) or a maximal projection-inspired analysis with IPs
and DPs (Chinese Penn Treebank [Xue et al 2004]).
3. Because a tree-based gold standard for parser evaluation must adopt a
particular style of linguistic analysis (reflected in the geometry and
nomenclature of the nodes in the trees), evaluation of statistical parsers
and grammars that are derived from particular treebank resources (as
well as hand-crafted grammars/parsers) can suffer unduly if the gold
standard deviates systematically from the (possibly) equally valid style
of linguistic analysis provided by the parser.
Problems such as these have motivated research on more abstract, dependency-
based parser evaluation (e.g., Lin 1995; Carroll, Briscoe, and Sanfilippo 1998; Carroll
et al 2002; Clark and Hockenmaier 2002; King et al 2003; Preiss 2003; Kaplan et al
2004; Miyao and Tsujii 2004). Dependency-based linguistic representations are ap-
proximations of abstract predicate-argument-adjunct (or more basic head-dependent)
82
Cahill et al Statistical Parsing Using Automatic Dependency Structures
structures, providing a more normalized representation abstracting away from the
particulars of surface realization or CFG-tree representation, which enables meaningful
cross-parser evaluation.
A related contrast holds between shallow and deep grammars and parsers.1 In
addition to defining a language (as a set of strings), deep grammars relate strings to in-
formation/meaning, often in the form of predicate?argument structure, dependency re-
lations,2 or logical forms. By contrast, a shallow grammar simply defines a language and
may associate syntactic (e.g., CFG tree) representations with strings. Natural languages
do not always interpret linguistic material locally where the material is encountered
in the string (or tree). In order to obtain accurate and complete predicate?argument,
dependency, or logical form representations, a hallmark of deep grammars is that they
usually involve a long-distance dependency (LDD) resolution mechanism.
Traditionally, deep grammars are hand-crafted (cf. the ALVEY Natural Language
Tools [Briscoe et al 1987], the Core Language Engine [Alshawi and Pulman 1992], the
Alpino Dutch dependency parser [Bouma, van Noord, andMalouf 2000], the Xerox Lin-
guistic Environment [Butt et al 2002], the RASP dependency parser [Carroll and Briscoe
2002] and the LinGO English Resource Grammar [Flickinger 2000; Baldwin et al 2004]).
Wide-coverage, deep-grammar development, particularly in rich formalisms such as
LFG (Kaplan and Bresnan 1982; Bresnan 2001; Dalrymple 2001) and HPSG (Pollard
and Sag 1994), is knowledge-intensive, time-consuming and expensive, constituting
an instance of the (in-)famous ?knowledge acquisition bottleneck? familiar from other
areas in traditional, rule-based AI and NLP. Very few hand-crafted deep grammars
(Briscoe and Carroll 1993; Bouma, van Noord, and Malouf 2000; Riezler et al 2002)
have, in fact, been successfully scaled to unrestricted input.
The last 15 years have seen extensive efforts on treebank-based automatic gram-
mar acquisition using a variety of machine-learning techniques (e.g., Gaizauskas 1995;
Charniak 1996; Collins 1999; Johnson 1999; Charniak 2000; Bikel 2002; Bod 2003; Klein
and Manning 2003). These grammars are wide-coverage and robust and in contrast
to manual grammar development, machine-learning-based grammar acquisition in-
curs relatively low development cost. With few notable exceptions,3 however, these
treebank-induced wide-coverage grammars are shallow: They usually do not attempt
to resolve LDDs nor do they associate strings with meaning representations.
Over the last few years, addressing the knowledge acquisition bottleneck in deep
constraint-based grammar development, a growing body of research has emerged to au-
tomatically acquire wide-coverage deep grammars from treebank resources (TAG [Xia
1999], CCG [Hockenmaier and Steedman 2002], HPSG [Miyao, Ninomiya, and Tsujii
2003], LFG [Cahill et al 2002b, 2004]). To a first approximation, these approaches can
be classified as ?conversion?- or ?annotation?-based. TAG-based approaches convert
1 Our use of the terms ?shallow? and ?deep? parsers/grammars follows Kaplan et al (2004) where
a ?shallow parser? does not relate strings to meaning representations. This deviates from a more
common use of the terms where, for example, a ?shallow parser? refers to (often finite-state-based)
parsers (or chunkers) that may produce partial bracketings of input strings.
2 By dependency relations we mean deep, fine-grained, labeled dependencies that encode long-distance
dependencies and passive information, for example. These differ from the types of unlabeled
dependency relations in other work such as (McDonald and Pereira 2006).
3 Both Collins Model 3 (1999) and Johnson (2002) output CFG tree representations with traces. Collins
Model 3 performs LDD resolution for wh-relative clause constructions, Johnson (2002) for a wide range
of LDD phenomena in a post-processing approach based on Penn-II tree fragments linking displaced
material with where it is to be interpreted semantically. The work of Dienes and Dubey (2003) and
Levy and Manning (2004) is similar to that of Johnson (2002), recovering empty categories on top of
CFG-based parsers. None of them map strings into dependencies.
83
Computational Linguistics Volume 34, Number 1
treebank trees into (lexicalized) elementary or adjunct trees. CCG-based approaches
convert trees into CCG derivations fromwhich CCG categories can be extracted. HPSG-
and LFG-based grammar induction methods automatically annotate treebank trees
with (typed) attribute-value structure information for the extraction of constraint-based
grammars and lexical resources.
Two recent papers (Preiss 2003; Kaplan et al 2004) have started tying together
the research strands just sketched: They use dependency-based parser evaluation to
compare wide-coverage parsing systems using hand-crafted, deep, constraint-based
grammars with systems based on a simple version of treebank-based deep grammar
acquisition technology in the conversion paradigm. In the experiments, tree output
generated by Collins?s Model 1, 2, and 3 (1999) and Charniak?s (2000) parsers, for
example, are automatically translated into dependency structures and evaluated against
gold-standard dependency banks.
Preiss (2003) uses the grammatical relations and the CBS 500 Dependency Bank
described in Carroll, Briscoe, and Sanfilippo (1998) to compare a number of parsing
systems (Briscoe and Carroll 1993; Collins?s 1997 models 1 and 2; and Charniak 2000)
using a simple version of the conversion-based deep grammar acquisition process (i.e.,
reading off grammatical relations fromCFG parse trees produced by the treebank-based
shallow parsers). The article also reports on a task-based evaluation experiment to rank
the parsers using the grammatical relations as input to an anaphora resolution system.
Preiss concluded that parser ranking using grammatical relations reflected the absolute
ranking (between treebank-induced parsers) using traditional tree-based metrics, but
that the difference between the performance of the parsing algorithms narrowed when
they carried out the anaphora resolution task. Her results show that the hand-crafted
deep unification parser (Briscoe and Carroll 1993) outperforms the machine-learned
parsers (Collins 1997; Charniak 2000) on the f-score derived from weighted precision
and recall on grammatical relations.4 Kaplan et al (2004) compare their deep, hand-
crafted, LFG-based XLE parsing system (Riezler et al 2002) with Collins?s (1999) model
3 using a simple conversion-based approach, capturing dependencies from the tree
output of the machine-learned parser, and evaluating both parsers against the PARC
700 Dependency Bank (King et al 2003). They conclude that the hand-crafted, deep
grammar outperforms the state-of-the-art treebank-based shallow parser on the level of
dependency representation, at the price of a small decrease in parsing speed.
Both Preiss (2003) and Kaplan et al (2004) emphasize that they use rather basic
versions of the conversion-based deep grammar acquisition technology outlined herein.
In this article we revisit the experiments carried out by Preiss and Kaplan et al, this time
using the sophisticated and fine-grained treebank- and annotation-based, deep, probabilis-
tic LFG grammar acquisitionmethodology developed in Cahill et al (2002b), Cahill et al
(2004), O?Donovan et al (2004), and Burke (2006) with a number of surprising results:
1. Evaluating against the PARC 700 Dependency Bank (King et al 2003)
using a retrained version of Bikel?s (2002) parser, the best automatically
induced, deep LFG resources achieve an f-score of 82.73%. This is an
improvement of 3.13 percentage points over the previously best published
results established by Kaplan et al (2004) who use a hand-crafted,
wide-coverage, deep LFG and the XLE parsing system. This is also a
4 The numbers given are difficult to compare as the results for the Briscoe and Carroll (1993) parser were
captured for a richer set of grammatical relations than those for Collins (1997) and Charniak (2000).
84
Cahill et al Statistical Parsing Using Automatic Dependency Structures
statistically significant improvement of 2.18 percentage points over the
most recent improved results presented in this article for the XLE system.
2. Evaluating against the Carroll, Briscoe, and Sanfilippo (1998) CBS 500
gold-standard dependency bank using a retrained version of Bikel?s (2002)
parser, the best Penn-II treebank-based, automatically acquired, deep LFG
resources achieve an f-score of 80.23%. This is a statistically significant
improvement of 3.66 percentage points over Carroll and Briscoe (2002),
who use a hand-crafted, wide-coverage, deep, unification grammar and
the RASP parsing system.
Evaluation results on a reannotated version (Briscoe and Carroll 2006) of the PARC
700 Dependency Bank were recently published in Clark and Curran (2007), reporting
f-scores of 81.9% for the CCG parser, and 76.3% for RASP. As Briscoe and Carroll
(2006) point out, these evaluations are not directly comparable with the Kaplan et al
(2004) style evaluation against the original PARC 700 Dependency Bank, because the
annotation schemes are different.
The article is structured as follows: In Section 2, we outline the automatic LFG
f-structure annotation algorithm and the pipeline parsing architecture of Cahill et al
(2002b), Cahill et al (2004), and Burke (2006). In Section 3, we present our experiment
design. In Section 4, using the DCU 105 Dependency Bank as our development set, we
evaluate a number of treebank-induced LFG parsing systems against the automatically
generated Penn-II WSJ Section 22 Dependency Bank test set. We use the Approximate
Randomization Test (Noreen 1989) to test for statistical significance and choose the best
parsing system for the evaluations against the wide-coverage, hand-crafted RASP and
LFG grammars of Carroll and Briscoe (2002) and Kaplan et al (2004) using the CBS
500 and PARC 700 Dependency Banks in Section 5. In Section 6, we discuss results and
issues raised by our methodology, outline related and future research and conclude in
Section 7.
2. Methodology
In this section, we briefly outline LFG and present our automatic f-structure annotation
algorithm and parsing architecture. The parsing architecture enables us to integrate
PCFG- and history-based parsers, which allows us to compare these parsers at the level
of dependency structures, rather than just trees.
2.1 Lexical Functional Grammar
Lexical Functional Grammar (LFG) (Kaplan and Bresnan 1982; Bresnan 2001; Dalrymple
2001) is a constraint-based theory of grammar. It (minimally) posits two levels of
representation, c(onstituent)-structure and f(unctional)-structure. C-structure is rep-
resented by context-free phrase-structure trees, and captures surface grammatical
configurations such as word order. The nodes in the trees are annotated with functional
equations (attribute-value structure constraints, for example (?OBJ)=?) which are
resolved (in the case of well-formed strings) to produce an f-structure. F-structures
are recursive attribute-value matrices, representing abstract syntactic functions, which
85
Computational Linguistics Volume 34, Number 1
Figure 1
C- and f-structures for the sentence U.N. signs treaty.
approximate to basic predicate-argument-adjunct structures or dependency relations.5
Figure 1 shows the c- and f-structures for the string U.N. signs treaty. Each node in the
c-structure is annotated with f-structure equations, for example (? SUBJ)= ?. The
uparrows (?) point to the f-structure associated with the mother node, downarrows
(?) to that of the local node. In a complete parse tree, these ? and ? meta variables are
instantiated to unique tree node identifiers and a set of constraints (a set of terms in an
equality logic) is generated which (if satisfiable) generates an f-structure.
2.2 Automatic F-Structure Annotation Algorithm
Deep grammars can be induced from treebank resources if the treebank encodes
enough information to support the derivation of deep grammatical information, such
as predicate?argument structures, deep dependency relations, or logical forms. Many
second generation treebanks such as Penn-II provide information to support the compi-
lation of meaning representations, for example in the form of traces relating displaced
linguistic material to where it should be interpreted semantically. The f-structure anno-
tation algorithm exploits configurational and categorial information, as well as traces
and the Penn-II functional tag annotations (Table 1) to automatically associate Penn-II
CFG trees with LFG f-structure information.
Given a tree, such as the Penn-II-style tree in Figure 2, the algorithm will traverse
the tree and deterministically add f-structure equations to the phrasal and leaf nodes
of the tree, resulting in an f-structure annotated version of the tree. The annotations are
then collected and passed on to a constraint solver which generates an f-structure (if the
constraints are satisfiable). We use a simple graph-unification-based constraint solver
(Eisele and Do?rre 1986), extended to handle path, set-valued, disjunctive, and existential
constraints. Given parser output without Penn-II style annotations and traces, the same
algorithm is used to assign annotations to each node in the tree, whereas a separate
module is applied at the level of f-structure to resolve any long-distance dependencies
(see Section 2.3).
5 van Genabith and Crouch (1996, 1997) provide translations between f-structures, Quasi-Logical Forms
(QLFs), and Underspecified Discourse Representation Structures (UDRSs).
86
Cahill et al Statistical Parsing Using Automatic Dependency Structures
Table 1
A complete list of the Penn-II functional labels.
Tag Description
Form/function discrepancies
-ADV clausal and NP adverbials
-NOM non NPs that function as NPs
Grammatical role
-DTV dative
-LGS logical subjects in passives
-PRD non VP predicates
-PUT locative complement of put
-SBJ surface subject
-TPC topicalized and fronted constituents
-VOC vocatives
Adverbials
-BNF benefactive
-DIR direction and trajectory
-EXT extent
-LOC location
-MNR manner
-PRP purpose and reason
-TMP temporal phrases
Miscellaneous
-CLR closely related to verb
-CLF true clefts
-HLN headlines and datelines
-TTL titles
The f-structure annotation algorithm is described in detail in Cahill et al (2002a),
McCarthy (2003), Cahill et al (2004), and Burke (2006). In brief, the algorithm is modular
with four components (Figure 3), taking Penn-II trees as input and automatically adding
LFG f-structure equations to each node in the tree.
Lexical Information. Lexical information is generated automatically by macros for each
of the POS classes in Penn-II. To give a simple example, third-person plural noun
Penn-II POS-word sequences of the form NNS word are automatically associated with
the equations (?PRED) = word?, (?NUM) = pl and (?PERS) = 3rd, where word? is the
lemmatized word.
Left?Right Context Annotation. The Left?Right context annotation component identifies
the heads of Penn-II trees using a modified version of the head finding rules of
Magerman (1994). This partitions each local subtree (of depth one) into a local head, a
left context (left sisters), and a right context (right sisters). The contexts together with
information about the local mother and daughter categories and (if present) Penn-II
87
Computational Linguistics Volume 34, Number 1
Figure 2
Trees for the sentence U.N. signs treaty, the headline said before and after automatic f-structure
annotation, with the f-structure automatically produced.
Figure 3
F-structure annotation algorithm modules.
88
Cahill et al Statistical Parsing Using Automatic Dependency Structures
Table 2
Sample from an NP Annotation matrix.
Left context Head Right context
DT: (?SPEC DET)=? NN, NNS, NNP, NNPS, NP: RRC, SBAR: (?RELMOD)=?
CD: (?SPEC QUANT)=? ?=? PP: ??(?ADJUNCT)
ADJP, JJ, NN, NNP: ??(?ADJUNCT) NP: ??(?APP)
functional tag labels (Table 1) are used by the f-structure annotation algorithm. For each
Penn-II mother (i.e., phrasal) category an Annotation matrix expresses generalizations
about how to annotate immediate daughters dominated by the mother category relative
to their location in relation to the local head. To give a (much simplified) example,
the head finding rules for NPs state that the rightmost nominal (NN, NNS, NNP, . . . )
not preceded by a comma or ?-?6 is likely to be the local head. The Annotation ma-
trix for NPs states (inter alia) that heads are annotated ?=?, that DTs (determiners)
to the left of the head are annotated (? SPEC DET) = ?, NPs to the right of the head as
??(? APP) (appositions). Table 2 provides a sample extract from the NP Annotation
matrix. Figure 4 provides an example of the application of the NP and PP Annotation
matrices to a simple tree.
For each phrasal category, Annotation matrices are constructed by inspecting the
most frequent Penn-II rule types expanding the category such that the token occurrences
of these rule types cover more than 85% of all occurrences of expansions of that category
in Penn-II. For NP rules, for example, this means that we analyze the most frequent
102 rule types expanding NP, rather than the complete set of more than 6,500 Penn-II
NP rule types, in order to populate the NP Annotation matrix. Annotation matrices
generalize to unseen rule types as, in the case of NPs, these may also feature DTs to
the left of the local head and NPs to the right and similarly for rule types expanding
other categories.
Coordination. In order to support the modularity, maintainability, and extendability of
the annotation algorithm, the Left?Right Annotation matrices apply only to local trees
of depth one, which do not feature coordination. This keeps the statement of Annotation
matrices perspicuous and compact. The Penn-II treatment of coordination is (inten-
tionally) flat. The annotation algorithm has modules for like- and unlike-constituent
coordination. Coordinated constituents are elements of a COORD set and annotated ??
(? COORD). The Coordination module reuses the Left?Right context Annotation ma-
trices to annotate any remaining nodes in a local subtree containing a coordinating
conjunction. Figure 5 provides a VP-coordination example (with right-node-raising).
Catch-All and Clean-Up. The Catch-All and Clean-Up module provides defaults to cap-
ture remaining unannotated nodes (Catch-All) and corrects (Clean-Up) overgeneraliza-
tions resulting from the application of the Left?Right context Annotation matrices. The
Left?Right Annotation matrices are allowed a certain amount of overgeneralization as
this facilitates the perspicuous statement of generalizations and a separate statement of
exceptions, supporting the modularity and maintainability of the annotation algorithm.
PPs under VPs are a case in point. The VP Annotation matrix analyses PPs to the right
of the local VP head as adjuncts: ? ? (?ADJUNCT). The Catch-All and Clean-Up module
6 If the rightmost nominal is preceded by a comma or ?-?, it is likely to be an apposition to the head.
89
Computational Linguistics Volume 34, Number 1
Figure 4
Automatically annotated Penn-II tree (fragment) and f-structure (simplified) for Gerry Purdy,
director of marketing.
uses Penn-II functional tag (Table 1) information (if present), for example -CLR (closely
related to local head), to replace the original adjunct analysis by an oblique argument
analysis: (?OBL)=?. An example of this is provided by the PP-CLR in the left VP-conjunct
in Figure 5. In other cases, argument?adjunct distinctions are encoded configurationally
in Penn-II (without the use of -CLR tags). To give a simple example, the NP Anno-
tation matrix indiscriminately associates SBARs to the right of the local head with
(? RELMOD) = ?. However, some of these SBARs are actually arguments of the local
NP head and, unlike SBAR relative clauses which are Chomsky-adjoined to NP (i.e.,
relative clauses are daughters of an NP mother and sisters of a phrasal NP head), SBAR
arguments are sisters of non-phrasal NP heads.7 In such cases, the Catch-All and Clean-
Up module rewrites the original relative clause analysis into the correct complement
argument analysis (?COMP)=?. Figure 6 shows the COMP f-structure analyses for an
example NP containing an internal SBAR argument (rather than relative clause) node.
Traces. The Traces module translates traces and coindexed material in Penn-II trees
representing long-distance dependencies into corresponding reentrancies at f-structure.
Penn-II provides a rich arsenal of trace types to relate ?displaced? material to where it
7 Structural information of this kind is not encoded in the Annotation matrices; compare Table 2.
90
Cahill et al Statistical Parsing Using Automatic Dependency Structures
Figure 5
Automatically annotated Penn-II tree (fragment) and resulting f-structure for asked for and
received refunds.
should be interpreted semantically. The f-structure annotation algorithm coverswh- and
wh-less relative clause constructions, interrogatives, control and raising constructions,
right-node-raising, and general ICH (interpret constituent here) traces. Figure 5 gives an
example that shows the interplay between coordination, right-node-raising traces and
the corresponding automatically generated reentrancies at f-structure.
2.3 Parsing Architecture
The pipeline parsing architecture of Cahill et al (2004) and Cahill (2004) for parsing raw
text into LFG f-structures is shown in Figure 7. In this model, PCFGs or history-based
lexicalized parsers are extracted from the unannotated treebank and used to parse raw
text into trees. The resulting parse trees are then passed to the automatic f-structure
annotation algorithm to generate f-structures.8
Compared to full Penn-II treebank trees, the output of standard probabilistic
parsers is impoverished: Parsers do not normally output Penn-II functional tag an-
notations (Table 1) nor do they indicate/resolve long-distance dependencies, recorded
8 In the integratedmodel (Cahill et al 2004; Cahill 2004), we extract f-structure annotated PCFGs
(A-PCFGs) from the f-structure annotated treebank, where each non-terminal symbol in the grammar
has been augmented with LFG functional equations, such as NP[?OBJ=?] ? DT[?SPEC=?] NN[?=?].
We treat a non-terminal symbol followed by annotations as a monadic category for grammar extraction
and parsing. Parsing with A-PCFGs results in annotated parse trees, from which an f-structure can be
generated. In this article we only use the pipeline parsing architecture.
91
Computational Linguistics Volume 34, Number 1
Figure 6
Automatically annotated Penn-II tree (fragment) and f-structure for signs that managers
expect declines.
in terms of a fine-grained system of empty productions (traces) and coindexation in
the full Penn-II treebank trees. The f-structure annotation algorithm, as described in
Section 2.2, makes use of Penn-II functional tag information (if present) and relies on
traces and coindexation to capture LDDs in terms of corresponding reentrancies at
f-structure.
Penn-II functional labels are used by the annotation algorithm to discriminate
between adjuncts and (oblique) arguments. PP-sisters to a head verb are analyzed as
arguments iff they are labeled -CLR, -PUT, -DTV or -BNF, for example. Conversely,
functional labels (e.g., -TMP) are also used to analyze certain NPs as adjuncts, and
-LGS labels help to identify logical subjects in passive constructions. In the absence of
functional labels, the annotation algorithm will default to decisions based on simple
structural, configurational, and CFG-category information (and, for example, conserva-
tively analyze a PP sister to a head verb as an adjunct, rather than as an argument).
In Sections 3 and 4 we present a number of treebank-based parsers (in particular the
PCFGs and a version of Bikel?s history-based, lexicalized generative parser) trained to
output CFG categories with Penn-II functional tags. We achieve this through a simple
92
Cahill et al Statistical Parsing Using Automatic Dependency Structures
Figure 7
Treebank-based LFG parsing architecture.
masking and un-masking operation where functional tags are joined with their local
CFG category label to form a new (larger) set of (monadic) CFG category labels (e.g.,
PP-CLR goes to PP CLR) for training and parsing (for Bikel, the parser head-finding rules
are also adjusted to the expanded set of categories). After parsing, the Penn-II functional
tags are unmasked and available to the f-structure annotation algorithm.
The Traces component in the f-structure annotation algorithm (Figure 3) translates
LDDs represented in terms of traces and coindexation in the original Penn-II treebank
trees into corresponding reentrancies at f-structure. Most probabilistic treebank-based
parsers, however, do not indicate/resolve LDDs, and the Traces component of the an-
notation algorithm does not apply. Initially, the f-structures produced for parser output
trees in the architecture in Figure 7 are therefore LDD-unresolved: They are incomplete
(or proto) f-structures, where displaced material (e.g., the values of FOCUS, TOPIC, and
TOPICREL attributes [wh- and wh-less relative clauses, topicalization, and interrogative
constructions] at f-structure) is not yet linked to the appropriate argument grammati-
cal functions (or elements of adjunct sets) for the governing local PRED. A dedicated
LDD Resolution component in the architecture in Figure 7 turns parser output proto-
f-structures into fully LDD-resolved proper f-structures, without traces and coindexa-
tion in parse trees.
Consider the following fragment of a proper Penn-II treebank tree (Figure 8), where
the LDD between the WHNP in the relative clause and the embedded direct object
position of the verb reward is indicated in terms of the trace *T*-3 and its coindexation
with the antecedent WHNP-3. Note further that the control relation between the subject
of the verbs wanted and reward is similarly expressed in terms of traces (*T*-2) and
coindexation (NP-SBJ-2). From the treebank tree, the f-structure annotation algorithm
is able to derive a fully resolved f-structure where the LDD and the control relation are
captured in terms of corresponding reentrancies (Figure 9).
93
Computational Linguistics Volume 34, Number 1
Figure 8
Penn-II treebank tree with LDD indicated in terms of traces (empty productions) and
coindexation and f-structure annotations generated by the annotation algorithm.
Now consider the corresponding ?impoverished? (but otherwise correct) parser
output tree (Figure 10) for the same string: The parser output does not explicitly record
the control relation nor the LDD.
Given this parser output tree, prior to the LDD resolution component in the parsing
architecture (Figure 7), the f-structure annotation algorithmwould initially construct the
partial (proto-) f-structure in Figure 11, where the LDD indicated by the TOPICREL func-
tion is unresolved (i.e., the value of TOPICREL is not coindexedwith the OBJ grammatical
function of the embedded verb reward). The control relation (shared subject between
the two verbs in the relative clause) is in fact captured by the annotation algorithm in
terms of a default annotation (? SUBJ) = (? SUBJ) on sole argument VPs to the right of
head verbs (as often, even in the full Penn-II treebank trees, control relations are not
consistently captured through explicit argument traces).
In LFG, LDD resolution operates at the level of f-structure, using functional un-
certainty equations (regular expressions over paths in f-structure [Kaplan and Zaenen
1989] relating f-structure components in different parts of an f-structure), obviating
traces and coindexation in c-structure trees. For the example in Figure 10, a functional
uncertainty equation of the form (?TOPICREL) = (?[COMP|XCOMP]? [SUBJ|OBJ]) would
be associated with the WHNP daughter node of the SBAR relative clause. The equation
states that the value of the TOPICREL attribute is token-identical (re-entrant) with the
value of a SUBJ or OBJ function, reached through a path along any number (including
94
Cahill et al Statistical Parsing Using Automatic Dependency Structures
Figure 9
Fully LDD-resolved f-structure.
Figure 10
Impoverished parser output tree: LDDs not captured.
zero) of COMP or XCOMP attributes. This equation, together with subcategorization
frames (LFG semantic forms) for the local PREDs and the usual LFG completeness and
coherence conditions, resolve the partial proto-f-structure in Figure 11 into the fully
LDD-resolved proper f-structure in Figure 9.
95
Computational Linguistics Volume 34, Number 1
Figure 11
Proto-f-structure: LDDs not captured.
Following Cahill et al (2004), in our parsing architecture (Figure 7) we model
LFG LDD resolution using automatically induced finite approximations of functional-
uncertainty equations and subcategorization frames from the f-structure-annotated
Penn-II treebank (O?Donovan et al 2004) in an LDD resolution component. From the
fully LDD-resolved f-structures from the Penn-II training section treebank trees we
learn probabilistic LDD resolution paths (reentrancies in f-structure), conditional on
LDD type (Table 3), and subcategorization frames, conditional on lemma (and voice)
(Table 4). Table 3 lists the eight most probable TOPICREL paths (out of a total of 37
TOPICREL paths acquired). The totality of these paths constitutes a finite subset of the
reference language definde by the full functional uncertainty equation (?TOPICREL) =
(?[COMP|XCOMP]? [SUBJ|OBJ]). Given an unresolved LDD type (such as TOPICREL in
the parser output for the relative clause example in Figure 11), admissible LDD res-
olutions assert a reentrancy between the value of the LDD trigger (here, TOPICREL)
and a grammatical function (or adjunct set element) of an embedded local predicate,
subject to the conditions that (i) the local predicate can be reached from the LDD trigger
using the LDD path; (ii) the grammatical function terminates the LDD path; (iii) the
grammatical function is not already present (at the relevant level of embedding in
the local f-structure); and (vi) the local predicate subcategorizes for the grammatical
function in question.9 Solutions satisfying (i)?(iv) are ranked using the product of
LDD path and subcategorization frame probabilities and the highest ranked solution
(possibly involving multiple interacting LDDs for a single f-structure) is returned by
the algorithm (for details and comparison against alternative LDD resolution methods,
see Cahill et al 2004).10
For our example (Figure 11), the highest ranked LDD resolution is for LDD path
(?TOPICREL) = (? XCOMP OBJ) and the local subcat frame REWARD?? SUBJ, ? OBJ?. This
9 Conditions (i)?(iv) are suitably adapted for LDD resolutions terminating in adjunct sets.
10 In our experiments we do not use the limited LDD resolution for wh-phrases provided by Collins?s Model
3 parser as better results are achieved using the purely f-structure-based LDD resolution as shown in
Cahill et al (2004).
96
Cahill et al Statistical Parsing Using Automatic Dependency Structures
Table 3
Most frequent wh-TOPICREL paths.
wh-TOPICREL Probability wh-TOPICREL Probability
subj .7583 xcomp .0830
obj .0458 xcomp:obj .0338
xcomp:xcomp .0168 xcomp:subj .0109
comp .0097 comp:subj .0073
Table 4
Most frequent semantic forms for active and passive (p) occurrences of the verb want and
reward.
Semantic form Probability
want([subj,xcomp]) .6208
want([subj,obj]) .2496
want([subj,obj,xcomp]) .1008
want([subj]) .0096
want([subj,obj,obl]) .0048
want([subj,obj,part]),p) .5000
want([subj,obl]),p) .1667
want([subj,part]),p) .1667
want([subj]),p) .1667
reward([subj,obj]) .8000
reward([subj,obj,obl]) .2000
reward([subj]),p) 1.0000
(together with the subject control equation described previously) turns the parser-
output proto-f-structure (in Figure 11) into the fully LDD resolved f-structure in
(Figure 9).
The full pipeline parsing architecture with the LDD resolution (rather than the
Traces component for LDD resolved Penn-II treebank trees) component (and the LDD
path and subcategorization frame extraction) is given in Figure 7.
The pipeline architecture supports flexible integration of treebank-based PCFGs
or state-of-the-art, history-based, and lexicalized parsers (Collins 1999; Charniak 2000;
Bikel 2002) and enables dependency-based evaluation of such parsers.
3. Experiment Design
In our experiments we compare four history-based parsers for integration into the
pipeline parsing architecture described in Section 2.3:
 Collins?s 1999 Models 311
 Charniak?s 2000 maximum-entropy inspired parser12
11 Downloaded from ftp://ftp.cis.upenn.edu/pub/mcollins/PARSER.tar.gz.
12 Downloaded from ftp://ftp.cs.brown.edu/pub/nlparser/.
97
Computational Linguistics Volume 34, Number 1
 Bikel?s 2002 emulation of Collins Model 213
 a retrained version of Bikel?s (2002) parser which retains Penn-II functional
tags
Input for Collins?s and Bikel?s parsers was pre-tagged using the MXPOST POS tag-
ger (Ratnaparkhi 1996). Charniak?s parser provides its own POS tagger. The combined
system of best history-based parser and automatic f-structure annotation is compared
to two probabilistic parsing systems based on hand-crafted, wide-coverage, constraint-
based, deep grammars:
 the RASP parsing system (Carroll and Briscoe 2002)
 the XLE parsing system (Riezler et al 2002; Kaplan et al 2004)
Both hand-crafted grammars perform their own POS tagging, resolve LDDs, and
associate strings with dependency relations (in the form of grammatical relations or
LFG f-structures).
We evaluate the parsers against a number of gold-standard dependency banks.
We use the DCU 105 Dependency Bank (Cahill et al 2002a) as our development set
for the treebank-based LFG parsers. We use the f-structure annotation algorithm to
automatically generate a gold-standard test set from the original Section 22 treebank
trees (the f-structure annotated WSJ Section 22 Dependency Bank) to choose the best
treebank-based LFG parsing systems for the PARC 700 and CBS 500 experiments.
Following the experimental setup in Kaplan et al (2004), we use the Penn-II Section 23-
based PARC 700 Dependency Bank (King et al 2003) to evaluate the treebank-induced
LFG resources against the hand-crafted XLE grammar and parsing system of Riezler
et al (2002) and Kaplan et al Following Preiss (2003), we use the SUSANNE Based CBS
500 Dependency Bank (Carroll, Briscoe, and Sanfilippo 1998) to evaluate the treebank-
induced LFG resources against the hand-crafted RASP grammar and parsing system
(Carroll and Briscoe 2002) as well as against the XLE system (Riezler et al 2002).
For each gold standard, our experiment design is as follows: We parse automati-
cally tagged input14 sentences with the treebank- and machine-learning-based parsers
trained on WSJ Sections 02?21 in the pipeline architecture, pass the resulting parse
trees to our automatic f-structure annotation algorithm, collect the f-structure equations,
pass them to a constraint-solver which generates an f-structure, resolve long-distance
dependencies at f-structure following Cahill et al (2004) and convert the resulting LDD-
resolved f-structures into dependency representations using the formats and software
of Crouch et al (2002) (for the DCU 105, PARC 700, and WSJ Section 22 evaluations)
and the formats and software of Carroll, Briscoe, and Sanfilippo (1998) (for the CBS
500 evaluation). In the experiments we did not use any additional annotations such as
-A (for argument) that can be generated by some of the history-based parsers (Collins
1999) as the f-structure annotation algorithm is designed for Penn-II trees (which do
not contain such annotations). We also did not use the limited LDD resolution for wh-
relative clauses provided by Collins?s Model 3 as better results are achieved by LDD
13 This was developed at the University of Pennsylvania by Dan Bikel and is freely available to download
from http://www.cis.upenn.edu/?dbikel/software.html.
14 Tags were automatically assigned either by the parsers themselves or by the MXPOST tagger
(Ratnaparkhi 1996).
98
Cahill et al Statistical Parsing Using Automatic Dependency Structures
Table 5
Results of tree-based evaluation on all sentences WSJ section 23, Penn-II.
Parser Labeled
f-score (%)
PCFG 73.03
Parent-PCFG 78.05
Collins M3 88.33
Charniak 89.73
Bikel 88.32
Bikel+Tags 87.53
resolution on f-structure (Cahill et al 2004). A complete set of parameter settings for the
parsers is provided in the Appendix.
In order to evaluate the treebank-induced LFG resources against the PARC 700
and the CBS 500 dependency banks, a certain amount of automatic mapping is re-
quired to account for systematic differences in linguistic analysis, feature geometry,
and nomenclature at the level of dependencies. This is discussed in Sections 5.1 and
5.2. Throughout, we use the Approximate Randomization Test (Noreen 1989) to test the
statistical significance of the results.
4. Choosing a Treebank-Based LFG Parsing System
In this section, we choose the best treebank-based LFG parsing system for the compar-
isons with the hand-crafted XLE and RASP resources in Section 5. We use the DCU
105 Dependency Bank as our development set and carry out comparative evaluation
and statistical significance testing on the larger, automatically generated WSJ Section 22
Dependency Bank as a test set. The system based on Bikel?s (2002) parser retrained to
retain Penn-II functional tags (Table 1) achieves overall best results.
4.1 Tree-Based Evaluation against WSJ Section 23
For reference, we include the traditional CFG-tree-based comparison for treebank-
induced parsers. The parsers are trained on Sections 02 to 21 of the Penn-II Treebank and
tested on Section 23. The published results15 on these experiments for the history-based
parsers are given in Table 5. We also include figures for a PCFG and a Parent-PCFG (a
PCFG which has undergone the parent transformation [Johnson 1999]). These PCFGs
are induced following standard treebank preprocessing steps, including elimination of
empty nodes, but following Cahill et al (2004), they do include Penn-II functional tags
(Table 1), as these tags contain valuable information for the automatic f-structure anno-
tation algorithm (Section 2.2). These tags are removed for the tree-based evaluation.
The results show that the history-based parsers produce considerably better trees
than the more basic PCFGs (with and without parent transformations). Charniak?s
(2000) parser scores best with an f-score of 89.73% on all sentences in Section 23. The
15 Where there were no published results available for Section 23, we calculated them using the
downloadable versions of the parsers.
99
Computational Linguistics Volume 34, Number 1
vanilla PCFG achieves the lowest f-score of 73.03%, a difference of 16.7 percentage
points. The hand-crafted XLE and RASP grammars achieve around 80% coverage
(measured in terms of complete spanning parse) on Section 23 and use a variety of
(longest) fragments combining techniques to generate dependency representations for
the remaining 20% of Section 23 strings. By contrast, the treebank-induced PCFGs and
history-based parsers all achieve coverage of over 99.9%. Given that the history-based
parsers score considerably better than PCFGs on trees, we would also expect them to
produce dependency structures of substantially higher quality.
4.2 Using DCU 105 as a Development Set
The DCU 105 (Cahill et al 2002a) is a hand-crafted gold-standard dependency bank
for 105 sentences, randomly chosen from Section 23 of the Penn-II Treebank.16 This is a
relatively small gold standard, initially developed to evaluate the automatic f-structure
annotation algorithm. We parse the 105 tagged sentences into LFG f-structures with
each of the treebank-induced parsers in the pipeline parsing and f-structure annotation
architecture. The f-structures of the gold standard and the f-structures returned by the
parsing systems are converted into dependency triples following Crouch et al (2002)
and Riezler et al (2002) and we also use their software for evaluation. The following
dependency triples are produced by the f-structure in Figure 1:
subj(sign?0,U.N.?1)
obj(sign?0,treaty?2)
num(U.N.?1,sg)
pers(U.N.?1,3)
num(treaty?2,sg)
pers(treaty?3,3)
tense(sign?0,present)
We evaluate preds-only f-structures (i.e., where paths in f-structures end in a PRED
value: the predicate-argument-adjunct structure skeleton) and all grammatical func-
tions (GFs) including number, tense, person, and so on. The results are given in Table 6.
With one main exception, Tables 5 and 6 confirm the general expectation that
the better the trees produced by the parsers, the better the f-structures automatically
generated for those trees. The exception is Bikel+Tags. The automatic f-structure an-
notation algorithm will exploit Penn-II functional tag information if present to generate
appropriate f-structure equations (see Section 2.2). It will default to possibly less reliable
configurational and categorial information if Penn-II tags are not present in the trees.
In order to test whether the retention of Penn-II functional labels in the history-
based parser output will improve LFG f-structure-based dependency results, we use
Bikel?s (2002) training software,17 and retrain the parser on a version of the Penn-II
treebank (Sections 02 to 21) with the Penn-II functional tag labels (Table 1) annotated
in such a way that the resulting history-based parser will retain them (Section 2.3). The
retrained parser (Bikel+Tags) then produces CFG-trees with Penn-II functional labels
and these are used by the f-structure annotation algorithm. We evaluate the f-structure
dependencies against the DCU 105 (Table 6) and achieve an f-score of 82.92% preds-only
16 It is publicly available for download from: http://nclt.computing.dcu.ie/gold105.txt.
17 We use Bikel?s software rather than Charniak?s for this experiment as the former proved more stable
during the retraining phase.
100
Cahill et al Statistical Parsing Using Automatic Dependency Structures
Table 6
Treebank-induced parsers: results of dependency-based evaluation against DCU 105.
Parser Preds only All GFs
f-score (%) f-score (%)
PCFG 70.24 79.90
Parent-PCFG 75.84 83.58
Collins M3 77.84 85.08
Charniak 79.61 85.66
Bikel 79.39 86.56
Bikel+Tags 82.92 88.30
Table 7
Treebank induced parsers: breakdown by dependency relation of preds-only evaluation against
DCU 105.
Dep. Percent of total F-score (%)
Parent-PCFG Collins M3 Charniak Bikel Bikel+Tags
ADJUNCT 33.73 71 72 76 73 79
APP 0.68 61 0 55 70 65
COMP 2.31 60 61 66 61 73
COORD 5.73 64 73 77 67 76
DET 9.58 91 93 96 96 96
FOCUS 0.04 100 100 0 100 100
OBJ 16.42 82 84 86 85 90
OBJ2 0.07 80 57 50 57 50
OBL 2.17 58 24 27 23 63
OBL2 0.07 50 0 0 0 67
OBL AG 0.43 40 96 96 92 92
POSS 2.88 80 83 82 82 79
QUANT 1.85 70 67 69 70 70
RELMOD 1.78 50 78 67 78 73
SUBJ 14.74 80 81 83 85 85
TOPIC 0.46 85 87 96 96 89
TOPICREL 1.85 61 80 80 79 74
XCOMP 5.20 90 92 79 93 93
and 88.3% all GFs. A detailed breakdown by dependency is given in Table 7. The system
based on the retrained parser is nowmuch better able to identify oblique arguments and
overall preds-only accuracy has improved by 3.53% over the original Bikel experiment
and 3.31% over Charniak?s parser, even though Charniak?s parser performs more than
2% better on the tree-based scores in Table 5 and even though the retrained parser drops
0.79% against the original Bikel parser on the tree-based scores.18
Inspection of the results broken down by grammatical function (Table 7) for the
preds-only evaluation against the DCU 105 shows that just over one third of all depen-
dency triples in the gold standard are adjuncts. SUBJ(ects) and OBJ(ects) together make
up a further 30%.
18 The figures suggest that retraining Charniak?s parser to retain Penn-II functional tags is likely to produce
even better dependency scores than those achieved by Bikel?s retrained parser.
101
Computational Linguistics Volume 34, Number 1
Table 7 shows that the treebank-based LFG system using Collins?s Models 3 is
unable to identify APP(osition). This is due to Collins?s treatment of punctuation and
the fact that punctuation is often required to reliably identify apposition.19 None of
the original history-based parsers produced trees which enabled the annotation algo-
rithm to identify second oblique dependencies (OBL2), and they generally performed
considerably worse than Parent-PCFG when identifying OBL(ique) dependencies. This
is because the automatic f-structure annotation algorithm is cautious to the point of
undergeneralization when identifying oblique arguments. In many cases, the algorithm
relies on the presence of, for example, a -CLR Penn-II functional label (indicating that the
phrase is closely related to the verb), and the history-based (Collins M3, Charniak, and
Bikel) parsers do not produce these labels, whereas Parent-PCFG (as well as PCFG) are
trained to retain Penn-II functional labels. Parent-PCFG, by contrast, performs poorly
for oblique agents (OBL AG, agentive by-phrases in passive constructions), whereas the
history-based parsers are able to identify these with considerable accuracy. This is be-
cause Parent-PCFG often erroneously finds oblique agents, even when the preposition
is not by, as it never has enough context in which to distinguish by prepositional phrases
from other PPs. The history-based parsers produce trees from which the automatic
f-structure annotation algorithm can better identify RELMOD and TOPICREL dependen-
cies than Parent-PCFG. This, in turn, leads to improved long distance dependency
resolution which improves overall accuracy.
The DCU 105 development set is too small to support reliable statistical significance
testing of the performance ranking of the six treebank-based LFG parsing systems. In
order to carry out significance testing to select the best treebank-based LFG parsing
system for comparative evaluation against the hand-crafted deep XLE and RASP re-
sources, we move to a larger dependency-based evaluation data set: the gold-standard
dependency bank automatically generated fromWSJ Section 22.
4.3 Evaluation against WSJ Section 22 Dependencies
In an experimental setup similar to that of Hockenmaier and Steedman (2002),20 we
evaluate each parser against a large automatically generated gold standard. The gold-
standard dependency bank is automatically generated by annotating the original 1,700
treebank trees from WSJ Section 22 of the Penn-II Treebank with our f-structure an-
notation algorithm. We then evaluate the f-structures generated from the tree output
of the six parsers trained on Sections 02 to 21 resulting from parsing the Section 22
strings against the automatically produced f-structures for the original Section 22 Penn-II
treebank trees. The results are given in Table 8.
Compared to Table 6 for the DCU 105 gold standard, most scores are up, particularly
so for the history-based parsers. This trend is possibly due to the fact that the WSJ
19 The annotation algorithm relies on Penn-II-style punctuation patterns where an NP apposition follows a
nominal head separated by a comma ([NP [NP Bush ] , [NP the president ] ]), all three sisters of the same
mother node, while the trees produced by Collins?s parser attach the comma low in the tree ([NP [NP
Bush,] [NP the president ] ]). Although it would be trivial to carry out a tree transformation on the Collins
output to raise the punctuation to the expected level, we have not done this here.
20 This corresponds to experiments where the original Penn-II Section 23 treebank trees are automatically
converted into CCG derivations, which are then used as a gold standard to evaluate the CCG parser
trained on Sections 02?21. A similar methodology is used for the evaluation of treebank-based HPSG
resources (Miyao, Ninomiya, and Tsujii 2003) where Penn-II treebank trees are automatically annotated
with HPSG typed-feature structure information.
102
Cahill et al Statistical Parsing Using Automatic Dependency Structures
Table 8
Results of dependency-based evaluation against the automatically generated gold standard for
WSJ Section 22.
Parser Preds only All GFs
f-score (%) f-score (%)
PCFG 70.76 80.44
Parent-PCFG 74.92 83.04
Collins M3 79.30 86.00
Charniak 81.35 86.96
Bikel 81.40 87.00
Bikel+Tags 83.06 87.63
Section 22 gold standard is generated automatically from the original ?perfect? Penn-II
treebank trees using the automatic f-structure annotation algorithm, whereas the DCU
105 has been created manually without regard as to whether or not the f-structure
annotation algorithm could ever generate the f-structures, even given the ?perfect?
trees.
The LFG system based on Bikel?s retrained parser achieves the highest f-score of
83.06% preds-only and 87.63% all GFs. Parent-PCFG achieves an f-score of 74.92%
preds-only and 83.04% all GFs. Table 9 provides a breakdown by feature of the preds-
only evaluation.
Table 9 shows that, once again, the automatic f-structure annotation algorithm is
not able to identify any cases of apposition from the output of Collins?s Model 3 parser.
Apart from Bikel?s retrained parser, none of the history-based parsers are able to identify
Table 9
Breakdown by dependency of results of preds-only evaluation against the automatically
generated Section 22 gold standard.
Dep. Percent of total F-score (%)
Parent-PCFG Collins M3 Charniak Bikel Bikel+Tags
ADJUNCT 33.77 70 75 78 78 80
APP 0.74 61 0 77 77 71
COMP 1.35 60 72 70 70 80
COORD 5.11 74 78 82 82 81
DET 10.72 88 91 92 92 91
FOCUS 0.02 27 67 88 88 71
OBJ 16.17 80 85 87 87 88
OBJ2 0.07 15 30 32 32 71
OBL 1.92 50 19 21 21 73
OBL2 0.07 47 3 3 3 69
OBL AG 0.31 50 90 89 89 85
POSS 2.47 86 91 91 91 91
QUANT 2.12 89 89 93 93 92
RELMOD 1.84 51 71 72 72 69
SUBJ 15.45 75 80 81 81 82
TOPIC 0.44 81 85 84 84 76
TOPICREL 1.82 61 72 74 75 69
XCOMP 5.62 81 87 81 81 88
103
Computational Linguistics Volume 34, Number 1
Figure 12
Approximate Randomization Test for statistical significance testing.
OBJ2, OBL or OBL2 dependencies very well, although Parent-PCFG is able to produce
trees from which it is easier to identify obliques (OBL), because of the Penn-II functional
-CLR label. The automatic annotation algorithm is unable to identify RELMOD depen-
dencies satisfactorily from the trees produced by parsing with Parent-PCFG, although
the history-based parsers score reasonably well for this function. Whereas Charniak?s
parser is able to identify some dependencies better than Bikel?s retrained parser, overall
the system based on Bikel?s retrained parser performs better when evaluating against
the dependencies in WSJ Section 22.
In order to determine whether the results are statistically significant, we use the Ap-
proximate Randomization Test (Noreen 1989).21 This test is an example of a computer-
intensive statistical hypothesis test. Such tests are designed to assess result differences
with respect to a test statistic in cases where the sampling distribution of the test statistic
is unknown. Comparative evaluations of outputs of parsing systems according to test
statistics, such as differences in f-score, are examples of this situation. The test statistics
are computed by accumulating certain count variables over the sentences in the test
set. In the case of f-score, variable tuples consisting of the number of dependency-
relations in the parse for the system translation, the number of dependency-relations
in the parse for the reference translation, and the number of matching dependency-
relations between system and reference parse, are accumulated over the test set.
Under the null hypothesis, the compared systems are not different, thus any vari-
able tuple produced by one of the systems could just as likely have been produced by
the other system. So shuffling the variable tuples between the two systems with equal
probability, and recomputing the test statistic, creates an approximate distribution of
the test statistic under the null hypothesis. For a test set of S sentences there are 2S
different ways to shuffle the variable tuples between the two systems. Approximate
randomization produces shuffles by random assignments instead of evaluating all 2S
possible assignments. Significance levels are computed as the percentage of trials where
the pseudo statistic, that is the test statistic computed on the shuffled data, is greater
than or equal to the actual statistic, that is the test statistic computed on the test data. A
sketch of an algorithm for approximate randomization testing is given in Figure 12.
21 Applications of this test to natural language processing problems can be found in Chinchor et al (1993)
and Yeh (2000).
104
Cahill et al Statistical Parsing Using Automatic Dependency Structures
Table 10
Comparing parsers evaluated against Section 22 dependencies (preds-only): p-values for
approximate randomization test for 10,000,000 randomizations.
PCFG Parent-PCFG Collins M3 Charniak Bikel Bikel+Tags
PCFG - - - - - -
Parent-PCFG <.0001 - - - - -
Collins M3 <.0001 <.0001 - - - -
Charniak <.0001 <.0001 <.0001 - - -
Bikel <.0001 <.0001 <.0001 .0003 - -
Bikel+Tags <.0001 <.0001 <.0001 <.0001 <.0001 -
Table 10 gives the p-values (the smallest fixed level at which the null hypothesis can
be rejected) for comparing each parser against all of the other parsers. We test for sig-
nificance at the 95% level. Because we are doing a pairwise comparison of six systems,
giving 15 comparisons, the p-value needs to be below .0034 for there to be a significant
difference at the 95% level.22 For each parser, the values in the row corresponding to
that parser represent the p-values for those parsers that achieve a lower f-score than
that parser. This shows that the system based on Bikel?s retrained parser is significantly
better than those based on the other parsers with a statistical significance of >95%. For
the XLE and RASP comparisons, we will use the f-structure-annotation algorithm and
Bikel retrained-based LFG system.
5. Cross-Formalism Comparison of Treebank-Induced and Hand-Crafted Grammars
From the experiments in Section 4, we choose the treebank-based LFG system using
the retrained version of Bikel?s parser (which retains Penn-II functional tag labels) to
compare against parsing systems using deep, hand-crafted, constraint-based grammars
at the level of dependencies. We report on two experiments. In the first experiment
(Section 5.1), we evaluate the f-structure annotation algorithm and Bikel retrained
parser-based LFG system against the hand-crafted, wide-coverage LFG and XLE pars-
ing system (Riezler et al 2002; Kaplan et al 2004) on the PARC 700 Dependency Bank
(King et al 2003). In the second experiment (Section 5.2), we evaluate against the hand-
crafted, wide-coverage unification grammar and RASP parsing system of Carroll and
Briscoe (2002) on the CBS 500 Dependency Bank (Carroll, Briscoe, and Sanfilippo 1998).
5.1 Evaluation against PARC 700
The PARC 700 Dependency Bank (King et al 2003) provides dependency relations
(including LDD relations) for 700 sentences randomly selected from WSJ Section 23 of
the Penn-II Treebank. In order to evaluate the parsers, we follow the experimental setup
of Kaplan et al (2004) with a split of 560 dependency structures for the test set and 140
for the development set. The set of features (Table 12, later in this article) evaluated
in the experiment form a proper superset of preds-only, but a proper subset of all
22 Based on Cohen (1995, p. 190): ?e ? 1 ? (1 ? ?c )m, where m is the number of pairwise comparisons, ?e is
the experiment-wise error, and ?c is the per-comparison error.
105
Computational Linguistics Volume 34, Number 1
Figure 13
PARC 700 conversion software.
grammatical functions (preds-only ? PARC ? all GFs). This feature set was selected in
Kaplan et al because the features carry important semantic information. There are sys-
tematic differences between the PARC 700 dependencies and the f-structures generated
in our approach as regards feature geometry, feature nomenclature, and the treatment
of named entities. In order to evaluate against the PARC 700 test set, we automatically
map the f-structures produced by our parsers to a format similar to that of the PARC
700 Dependency Bank. This is done with conversion software in a post-processing stage
on the f-structure annotated trees (Figure 13).
The conversion software is developed on the 140-sentence development set of the
PARC 700, except for the Multi-Word Expressions section. Following the experimental
setup of Kaplan et al (2004), we mark up multi-word expression predicates based on
the gold-standard PARC 700 Dependency Bank.
Multi-Word Expressions The f-structure annotation algorithm analyzes the internal
structure of all noun phrases fully. In Figure 14, for example, BT is analyzed as
an ADJUNCT modifier of the head securities, whereas PARC 700 analyzes this and
other (more complex) named entities as multi-word expression predicates. The
conversion software transforms the output of the f-structure annotation algorithm
into the multi-word expression predicate format.
Feature Geometry In constructions such as Figure 2, the f-structure annotation algo-
rithm analyzes say as the main PRED with what is said as the value of a COMP
argument. In the PARC 700, these constructions are analyzed in such a way that
what is said/reported provides the top level f-structure whereas other material
(who reported, etc.) is analyzed in terms of ADJUNCTs modifying the top level
f-structure. A further systematic structural divergence is provided by the analysis
Figure 14
Named entity and OBL AG feature geometry mapping.
106
Cahill et al Statistical Parsing Using Automatic Dependency Structures
of passive oblique agent constructions (Figure 14): The f-structure annotation
algorithm generates a complex internal analysis of the oblique agent PP, whereas
the PARC analysis encodes a flat representation. The conversion software adjusts
the output of the f-structure annotation algorithm to the PARC-style encoding of
linguistic information.
Feature Nomenclature There are a number of systematic differences between feature
names used by the automatic annotation algorithm and PARC 700: For example,
DET is DET FORM in the PARC 700, COORD is CONJ, FOCUS is FOCUS INT. Nomen-
clature differences are treated in terms of a simple relabeling by the conversion
software.
Additional Features A number of features in the PARC 700 are not produced by the au-
tomatic annotation algorithm. These include: AQUANT for adjectival quantifiers,
MOD for NP-internal modifiers, and STMT TYPE for statement type (declarative,
interrogative, etc.). Additional features (and their values) are automatically gen-
erated by the mapping software, using categorial, configurational, and already
produced f-structure annotation information, extending the original annotation
algorithm.
XCOMP Flattening The automatic annotation algorithm treats both auxiliary and
modal verb constructions in terms of hierarchically cascading XCOMPs, whereas
in PARC 700 the temporal and aspectual information expressed by auxiliary verbs
is represented in terms of a flat analysis and features (Figure 15). The conversion
software automatically flattens the f-structures produced by the automatic anno-
tation algorithm into the PARC-style encoding.
For full details of the mapping, see Burke et al (2004).
In our parsing experiments, we used the most up-to-date version of the hand-
crafted, wide-coverage, deep LFG resources and XLE parsing system with improved
results over those reported in Kaplan et al (2004): This latest version achieves 80.55%
f-score, a 0.95 percentage point improvement on the previous 79.6%. The XLE parsing
system combines a large-scale, hand-crafted LFG for English and a statistical disam-
biguation component to choose the most likely analysis among those returned by
the symbolic parser. The statistical component is a log-linear model trained on 10,000
partially labeled structures from the WSJ. The results of the parsing experiments are
presented in Table 11. We also include a figure for the upper bound of each system.23
Using Bikel?s retrained parser, the treebank-based LFG system achieves an f-score of
82.73%, and the hand-crafted grammar and XLE-based system achieves an f-score
of 80.55%. The approximate randomization test produced a p-value of .0054 for this
pairwise comparison, showing that this result difference is statistically significant at
the 95% level. Evaluation results on a reannotated version (Briscoe and Carroll 2006) of
the PARC 700 Dependency Bank were recently published in Clark and Curran (2007),
reporting f-scores of 81.9% for the CCG parser, and 76.3% for RASP. As Briscoe and
23 The upper bound for the treebank-based LFG system is determined by taking the original Penn-II WSJ
Section 23 trees corresponding to the PARC 700 strings, automatically annotating them with the
f-structure annotation algorithm, and evaluating the f-structures against the PARC 700 dependencies. The
upper bound for the XLE system is determined by selecting the XLE parse that scores best against the
PARC 700 dependencies for each of the PARC 700 strings. It is interesting to note that the upper bound
for the treebank-based system is only 1.18 percentage points higher than that for the XLE system. Apart
from the two different methods for establishing the upper bounds, this is most likely due to the fact that
the mapping required for evaluating the treebank-based LFG system against PARC 700 is lossy (cf. the
discussion in Section 6).
107
Computational Linguistics Volume 34, Number 1
Figure 15
DCU 105 and PARC 700 analyses for the sentence Unlike 1987, interest rates have been falling
this year.
Carroll point out, these evaluations are not directly comparable with the Kaplan et al
(2004) style evaluation against the original PARC 700 Dependency Bank, because the
annotation schemes are different. Kaplan et al and our experiments use a fine-grained
feature set of 34 features (Table 12), while the Briscoe and Carroll scheme uses 17
features.
A breakdown by dependency relation for each system is given in Table 12. The
treebank-induced grammar system can better identify DET FORM, SUBORD FORM, and
Table 11
Results of evaluation against the PARC 700 Dependency Bank following the experimental setup
of Kaplan et al (2004).
Bikel+Tags XLE p-Value
F-score 82.73 80.55 .0054
Upper bound 86.83 85.65 -
108
Cahill et al Statistical Parsing Using Automatic Dependency Structures
Table 12
Breakdown by dependency relation of results of evaluation against PARC 700.
Dep. Percent of deps. F-score (%)
Bikel+Tags XLE
ADEGREE 6.17 80 82
ADJUNCT 14.32 68 66
AQUANT 0.06 78 61
COMP 1.23 80 74
CONJ 2.64 73 69
COORD FORM 1.20 83 90
DET FORM 4.61 97 91
FOCUS 0.02 0 36
MOD 2.74 74 67
NUM 19.82 91 89
NUMBER 1.42 89 83
NUMBER TYPE 2.10 94 86
OBJ 8.92 87 78
OBJ THETA 0.05 43 31
OBL 0.83 55 69
OBL AG 0.22 82 76
OBL COMPAR 0.07 38 56
PASSIVE 1.14 80 88
PCASE 0.25 79 68
PERF 0.41 89 90
POSS 0.98 88 80
PRECOORD FORM 0.03 0 91
PROG 0.97 89 81
PRON FORM 2.54 92 94
PRON INT 0.03 0 33
PRON REL 0.57 74 72
PROPER 3.56 83 93
PRT FORM 0.22 80 41
QUANT 0.34 77 80
STMT TYPE 5.23 87 80
SUBJ 8.51 78 78
SUBORD FORM 0.93 47 42
TENSE 5.02 95 90
TOPIC REL 0.57 56 73
XCOMP 2.29 80 78
PRT FORM dependencies24 and achieves higher f-scores for OBJ and POSS. However, the
hand-crafted parsing system can better identify FOCUS, OBL(ique) arguments, PRECO-
ORD FORM and TOPICREL relations.
5.2 Evaluation against CBS 500
We also compare the hand-crafted, deep, probabilistic unification grammar-based RASP
parsing system of Carroll and Briscoe (2002) to our treebank- and retrained Bikel
24 DET FORM, SUBORD FORM, and PRT FORM (and in general X FORM) dependencies record (semantically
relevant) surface forms in f-structure for X-type closed class categories.
109
Computational Linguistics Volume 34, Number 1
Figure 16
CBS 500 conversion software.
parser-based LFG system. The RASP parsing system is a domain-independent, robust
statistical parsing system for English, based on a hand-written, feature-based unification
grammar. A probabilistic parse selection model conditioned on the structural parse
context, degree of support for a subanalysis in the parse forest, and lexical informa-
tion (when available) chooses the most likely parses. For this experiment, we evaluate
against the CBS 500,25 developed by Carroll, Briscoe, and Sanfilippo (1998) in order to
evaluate a precursor of the RASP parsing resources. The CBS 500 contains dependency
structures (including some long distance dependencies26) for 500 sentences chosen at
random from the SUSANNE corpus (Sampson 1995), but subject to the constraint that
they are parsable by the parser in Carroll, Briscoe, and Sanfilippo. Aswith the PARC 700,
there are systematic differences between the f-structures produced by our methodology
and the dependency structures of the CBS 500. In order to be able to evaluate against
the CBS 500, we automatically map our f-structures into a format similar to theirs. We
did not split the data into a heldout and a test set when developing the mapping, so
that a comparison could be made with other systems that report evaluations against
the CBS 500. The following CBS 500-style grammatical relations are produced from the
f-structure in Figure 1:
(ncsubj sign U.N.)
(dobj sign treaty)
Some mapping is carried out (as in the evaluation against the PARC 700) on the f-
structure annotated trees, and the remaining mapping is carried out on the f-structures
(Figure 16). As with the PARC 700 mapping, all mappings are carried out automatically.
The following phenomena were dealt with on the f-structure annotated trees:
Auxiliary verbs (xcomp flattening) XCOMPS were flattened to promote the main verb
to the top level, while maintaining a list of auxiliary and modal verbs and their
relation to one another.
Treatment of topicalized sentences The predicate of the topicalized sentence became
the main predicate and any other top level material became an adjunct.
Multi-word expressions Multi-word expressions (such as according to) were not
marked up in the parser input, but captured in the annotated trees and the
annotations adjusted accordingly.
Treatment of the verbs be and become Our automatic annotation algorithm does not
treat the verbs be and become differently from any other verbs when they are used
transitively. This analysis conflicted with the CBS 500 analysis, so was changed to
match theirs.
25 This was downloaded from http://www.informatics.susx.ac.uk/research/nlp/carroll/greval.html.
26 The long distance dependencies include passive, wh-less relative clauses, control verbs, and so forth.
110
Cahill et al Statistical Parsing Using Automatic Dependency Structures
Table 13
Results of dependency evaluation against the CBS 500 (Carroll, Briscoe, and Sanfillipo 1998).
Bikel+Tags RASP p-Value
F-score 80.23 76.57 <.0001
The following are the main mappings carried out on the f-structures:
Encoding of Passive We treat passive as a feature in our automatic f-structure annota-
tion algorithm, whereas the CBS 500 triples encode this information indirectly.
Objects of Prepositional Phrases No dependency was generated for these objects, as
there was no corresponding dependency in the CBS 500 analyses.
Nomenclature Differences There were some trivial mappings to account for differ-
ences in nomenclature, for example OBL in our analyses became IOBJ in the
mapped dependencies.
Encoding of wh-less relative clauses These are encoded by means of reentrancies in
f-structure, but were encoded in a more indirect way in the mapped dependencies
to match the CBS 500 annotation format.
To carry out the experiments, we POS-tagged the tokenized CBS 500 sentences with
the MXPOST tagger (Ratnaparkhi 1996) and parsed the tag sequences with our Penn-II
and Bikel retrained-based LFG system. We use the evaluation software of Carroll,
Briscoe, and Sanfilippo (1998)27 to evaluate the grammatical relations produced by each
parser. The results are given in Table 13.
Our LFG system based on Bikel?s retrained parser achieves an f-score of 80.23%,
whereas the hand-crafted RASP grammar and parser achieves an f-score of 76.57%.
Crouch et al (2002) report that their XLE system achieves an f-score of 76.1% for the
same experiment. A detailed breakdown by dependency is given in Table 14. The
LFG system based on Bikel?s retrained parser is able to better identify MOD(ifier) de-
pendency relations, ARG MOD (the relation between a head and a semantic argument
which is syntactically realized as a modifier, for example by-phrases), IOBJ (indirect
object) and AUXiliary relations. RASP is able to better identify XSUBJ (clausal subjects
controlled from without), CSUBJ (clausal subjects), and COMP (clausal complement)
relations. Again we use the Approximate Randomization Test to test the parsing results
for statistical significance. The p-value for the test comparing our system using Bikel?s
retrained parser against RASP is <.0001. The treebank-based LFG system using Bikel?s
retrained parser is significantly better than the hand-crafted, deep, unification grammar-
based RASP parsing system with a statistical significance of >95%.
6. Discussion and Related Work
At the moment, we can only speculate as to why our treebank-based LFG resources
outperform the hand-crafted XLE and RASP grammars.
In Section 4, we observed that the treebank-induced LFG resources have consid-
erably wider coverage (>99.9% measured in terms of complete spanning parse) than
27 This was downloaded from http://www.informatics.susx.ac.uk/research/nlp/carroll/greval.html.
111
Computational Linguistics Volume 34, Number 1
Table 14
Breakdown by grammatical relation for results of evaluation against CBS 500.
Dep. Percent of deps. F-score (%)
Bikel+Tags RASP
DEPENDANT 100.00 80.23 76.57
MOD 48.96 80.30 75.29
NCMOD 30.42 85.37 72.98
XMOD 1.60 70.05 55.88
CMOD 2.61 75.60 53.08
DETMOD 14.06 95.85 91.97
ARG MOD 0.51 80.00 64.52
ARG 43.70 78.28 77.57
SUBJ 13.10 79.84 83.57
NCSUBJ 12.99 87.84 84.32
XSUBJ 0.06 0.00 88.89
CSUBJ 0.04 0.00 22.22
SUBJ OR DOBJ 18.22 81.21 83.84
COMP 12.38 76.73 71.87
OBJ 7.34 76.05 69.53
DOBJ 5.11 84.55 84.57
OBJ2 0.25 48.00 43.84
IOBJ 1.98 59.04 47.60
CLAUSAL 5.04 77.74 75.37
XCOMP 4.03 80.00 84.11
CCOMP 1.01 69.61 75.14
AUX 4.76 94.94 88.27
CONJ 2.06 68.84 69.09
the hand-crafted grammars (?80% for XLE and RASP grammars on unseen treebank
text). Both XLE and RASP use a number of (largest) fragment-combining techniques
to achieve full coverage. If coverage is a significant component in the performance
difference observed between the hand-crafted and treebank-induced resources, then
it is reasonable to expect that the performance difference is more pronounced with
increasing sentence length (with shorter sentences being simpler and more likely to
be within the coverage of the hand-crafted grammars). In other words, we expect the
hand-crafted, deep, precision grammars to do better on shorter sentences (more likely to
be within their coverage), whereas the treebank-induced grammars should show better
performance on longer strings (less likely to be within the coverage of the hand-crafted
grammars).
In order to test this hypothesis, we carried out a number of experiments:
First, we plotted the sentence length distribution for the 560 PARC 700 test sen-
tences and the 500 CBS 500 sentences (Figures 17 and 18). Both gold standards are
approximately normally distributed, with the CBS 500 distribution possibly showing
the effects of being chosen subject to the constraint that the strings are parsable by the
parser in Carroll, Briscoe, and Sanfilippo (1998). For each case we use the mean, ?, and
two standard deviations, 2?, to the left and right of themean to exclude sentence lengths
not supported by sufficient observations: For PARC 700, ? = 23.27, ?? 2? = 2.17, and
?+ 2? = 44.36; for CBS 500, ? = 17.27, ?? 2? = 1.59, and ?+ 2? = 32.96. Both the
PARC 700 and the CB 500 distributions are positively skewed. For the PARC 700, ?? 2?
is actually outside the observed data range, whereas for CB 500, ?? 2? almost coincides
112
Cahill et al Statistical Parsing Using Automatic Dependency Structures
Figure 17
Distribution of sentence frequency by sentence length in the PARC 700 test set with Bezier
interpolation. Vertical lines mark two standard deviations from the mean.
with the left border. It is therefore useful to further constrain the ?2? range by a
sentence count threshold of ? 5.28 This results in a sentence length range of 4?41 for
PARC 700 and 4?32 for CBS 500.
Second, in order to test whether fragment parses increase with sentence length, we
plotted the percentage of fragment parses over sentence length for the XLE parses of the
560-sentence test set of the PARC 700 (we did not do this for the CBS 500 as its strings
are selected to be fully parsable by RASP). Figure 19 shows that the number of fragment
parses tends to increase with sentence length.
Third, we plotted the average dependency f-score for the hand-crafted and the
treebank-induced resources against sentence lengths. Figure 20 shows the results for
PARC 700, Figure 21 for CBS 500.
In both cases, contrary to our (perhaps somewhat naive) assumption, the graphs
show that the treebank-induced resources outperform the hand-crafted resources
within (most of) the 4?41 and 4?32 sentence length bounds, with the results for the very
short and the very long strings outside those bounds not being supported by sufficient
data points.
In the parsing literature, results are often also provided for strings with lengths
?40. Below we give those results and statistical significance testing for the PARC 700
and CBS 500 (Tables 15 and 16). The results show that the Bikel retrained?based LFG
system achieves a higher dependency f-score on sentences of length ?40 than on all
sentences, whereas the XLE system achieves a slightly lower score on sentences of
length ?40. The Bikel-retrained system achieves an f-score of 83.18%, a statistically
28 Note that because the distributions in Figures 17 and 18 are Bezier interpolated, the constraint does not
guarantee that every sentence length within the range occurs five or more times.
113
Computational Linguistics Volume 34, Number 1
Figure 18
Distribution of sentence frequency by sentence length in the CB 500 test set with Bezier
interpolation. Vertical lines mark two standard deviations from the mean.
Figure 19
Percentage of fragment sentences for XLE parsing system per sentence length with Bezier
interpolation.
significant improvement of 2.67 percentage points over the XLE system on sentences of
length?40. Against the CBS 500, Bikel?s retrained system achieves a weighted f-score of
82.58%, a statistically significant improvement of 3.87 percentage points over the RASP
system which achieves a weighted f-score of 78.81% on sentences of length ?40.
114
Cahill et al Statistical Parsing Using Automatic Dependency Structures
Figure 20
Average f-score by sentence length for PARC 700 test set with Bezier interpolation.
Figure 21
Average f-score by sentence length for CB 500 test set with Bezier interpolation.
Finally, we test whether the strict preds-only dependency evaluation has an ef-
fect on the results for the PARC 700 experiments. Recall that following Kaplan et al
(2004) for the PARC 700 evaluation we used a set of semantically relevant grammatical
functions that is a superset of preds-only and a subset of all-GFs. A preds-only based
evaluation is ?stricter? and tends to produce lower scores as it directly reflects the effects
115
Computational Linguistics Volume 34, Number 1
Table 15
Evaluation and significance testing of sentences length ?40 against the PARC 700.
All sentence lengths Length ?40
f-score f-score
Bikel + Tags 82.73 83.18
XLE 80.55 80.51
p-value .0054 .0010
Table 16
Evaluation and significance testing of sentences length ?40 against the CBS 500.
All sentence lengths Length ?40
f-score f-score
Bikel + Tags 80.23 82.58
RASP 76.57 78.81
p-value <.0001 <.0001
Table 17
Preds-only evaluation against the PARC 700 Dependency Bank.
All GFs Preds only
f-score f-score
Bikel + Tags 82.73 77.40
XLE 80.55 74.31
of predicate?argument/adjunct misattachments in the resulting dependency represen-
tations (while local functions such as NUM(ber), for example, can score properly even
if the local predicate is misattached). Table 1729 below gives the results for preds-only
evaluation30 on the PARC 700 for all sentence lengths. The results show that the Bikel-
retrained treebank-based LFG resource achieves an f-score of 77.40%, 5.33 percentage
points lower than the score for all the PARC dependencies. The XLE system achieves
an f-score of 74.31%, 6.24 percentage points lower than the score for all the PARC
dependencies and a 3.09 percentage point drop against the results obtained by the
Bikel-retrained treebank-based LFG resources. The performance of the Bikel retrained?
based LFG system suffers less than the XLE system when preds-only dependencies are
evaluated.
It is important to note that in our f-structure annotation algorithm and treebank-
based LFG parsing architectures, we do not claim to provide fully adequate statistical
models. It is well known (Abney 1997) that PCFG- or history-based parser approxima-
tions to general constraint-based grammars can yield inconsistent probability models
29 We do not include a p-value here as the breakdown by function per sentence was not available to us for
the XLE data.
30 The dependency relations we include in preds-only evaluation are: ADJUNCT, AQUANT, COMP, CONJ,
COORD FORM, DET FORM, FOCUS INT, MOD, NUMBER, OBJ, OBJ THETA, OBL, OBL AG, OBL COMPAR,
POSS, PRON INT, PRON REL, PRT FORM, QUANT, SUBJ, SUBORD FORM, TOPIC REL, XCOMP.
116
Cahill et al Statistical Parsing Using Automatic Dependency Structures
due to loss of probability mass: The parser successfully returns the highest ranked
parse tree but the constraint solver cannot resolve the f-structure equations and the
probability mass associated with that tree is lost. Research on adequate probability
models for constraint-based grammars is important (Bouma, van Noord, and Malouf
2000; Miyao and Tsujii 2002; Riezler et al 2002; Clark and Curran 2004). In this context,
it is interesting to compare parser performance against upper bounds. For the PARC
700 evaluation, the upper bound for the XLE-based resources is 85.65%, against 86.83%
for the treebank-based LFG resources. XLE-based parsing currently achieves 94.05%
(f-score 80.55%) of its upper bound using a discriminative disambiguation method,
whereas the treebank-based LFG resource achieves 95.28% (f-score 82.73%) of its upper
bound.
Although this seems to indicate that the two disambiguationmodels achieve similar
results, the figures are actually very difficult to compare. In the case of the XLE, the
upper bound is established by unpacking all parses for a string and scoring the best
match against the gold standard (rather than letting the probability model select a
parse). By contrast, in the case of the treebank-based LFG resources, we use the original
?perfect? Penn-II treebank trees (rather than the trees produced by the parser), auto-
matically annotate those trees with the f-structure annotation algorithm, and score the
results against the PARC 700 (it is not feasible to generate all parses for a string, there
are simply too many for treebank-induced resources). The upper bound computed in
this fashion for the treebank-based LFG resource (86.83%) is relatively low. The main
reason is that the automatic mapping required to relate the f-structures generated by the
treebank-based LFG resources to the PARC 700 dependencies is lossy. This is indicated
by comparing the upper bound for the treebank-based LFG resources for the PARC 700
against the upper bound for the DCU 105 gold standard, where little or no mapping
(apart from the feature-structure to dependency-triple conversion) is required: Scoring
the f-structure annotations for the original treebank trees results in 86.83% against PARC
700 versus 96.80% against DCU 105.
Our discussion shows how delicate it can be to compare parsing systems and
their disambiguation models. Ultimately what is required is an evaluation strategy that
separates out and clearly distinguishes between the grammar, parsing algorithm, and
disambiguation model and is capable of assessing different combinations of these core
components. Of course, this will not always be possible andmoving towards it is part of
a much more extended research agenda, well beyond the scope of the research reported
in the present article. Our approach, and previous approaches, evaluate systems at
the highest level of granularity, that of the complete package: the combined grammar-
parser-disambiguation model. The results show that machine-learning-based resources
can outperform deep, state-of-the-art hand-crafted resources with respect to the quality
of dependencies generated.
Treebank-based, deep and wide-coverage constraint-based grammar acquisition
has become an important research topic: Starting with the seminal TAG-based work
of Xia (1999), there are now also HPSG-, CCG- and LFG-based approaches. Miyao,
Ninomiya, and Tsujii (2003) and Tsuruoka, Miyao, and Tsujii (2004) present re-
search on inducing Penn-II treebank-based HPSGs with log-linear probability models.
Hockenmaier and Steedman (2002) and Hockenmaier (2003) provide treebank-induced
CCG-basedmodels including LDD resolution. It would be interesting to conduct a com-
parative evaluation involving treebank-based HPSG, CCG, and LFG resources against
the PARC 700 and CBS 500 Dependency Banks to enable more comprehensive compar-
ison of machine-learning-based with hand-crafted, deep, wide-coverage resources such
as those used in the XLE or RASP parsing systems.
117
Computational Linguistics Volume 34, Number 1
Gildea and Hockenmaier (2003) and Miyao and Tsujii (2004) present PropBank
(Kingsbury, Palmer, andMarcus 2002)-based evaluations of their automatically induced
CCG and HPSG resources. PropBank-based evaluations provide valuable information
to rank parsing systems. Currently, however, PropBank-based evaluations are some-
what partial: They only represent (and hence score) verbal arguments and disregard a
raft of other semantically important dependencies (e.g., temporal and aspectual infor-
mation, dependencies within nominal clusters, etc.) as captured in the PARC 700 or CBS
500 Dependency Banks.31
7. Conclusions
Parser comparison is a non-trivial and time-consuming exercise. We extensively eval-
uated four machine-learning-based shallow parsers and two hand-crafted, wide-
coverage deep probabilistic parsers involving four gold-standard dependency banks,
using the Approximate Randomization Test (Noreen 1989) to test for statistical signifi-
cance. We used a sophisticated method for automatically producing deep dependency
relations from Penn-II-style CFG-trees (Cahill et al 2002b, 2004) to compare shallow
parser output at the level of dependency relation and revisit experiments carried out by
Preiss (2003) and Kaplan et al (2004).
Our main findings are twofold:
1. Using our CFG-tree-to-dependency annotation technology, together with treebank- and
machine-learning-based parsers, we can outperform hand-crafted, wide-coverage, deep,
probabilistic, constraint-based grammars and parsers.
This result is surprising for two reasons. First, it is established against two externally-
provided dependency banks (the PARC 700 and the CBS 500 gold standards), originally
designed using the hand-crafted, wide-coverage, probabilistic grammars for the XLE
(Riezler et al 2002) and RASP (Carroll and Briscoe 2002) parsing systems to evaluate
those systems. What is more, the SUSANNE corpus-based CBS 500 constitutes an
instance of domain variation for the Penn-II-trained LFG resources, likely to adversely
affect scores. Second, the treebank- and machine-learning-based LFG resources require
automatic mapping to relate f-structure output of the treebank-based parsing systems
to the representation format in the PARC 700 and CBS 500 Dependency Banks. These
mappings are partial and lossy: That is, they do not cover all of the systematic dif-
ferences between f-structure and dependency bank representations and introduce a
certain amount of error in what they are designed to capture, that is they both over-
and undergeneralize, again adversely affecting scores. Improvements of the mappings
should lead to a further improvement in the dependency scores.
2. Parser evaluation at the level of dependency representation still requires non-trivial
mappings between different dependency representation formats.
31 In a sense, PropBank does not yet provide a single agreed upon gold standard: Role information is
provided indirectly and an evaluation gold-standard has to be computed from this. In doing so, choices
have to be made as regards the representation of shared arguments, the analysis of coordinate structures,
and so forth, and it is not clear that the same choices are currently made for evaluations carried out by
different research groups.
118
Cahill et al Statistical Parsing Using Automatic Dependency Structures
Earlier we criticized tree-based parser evaluation on the grounds that equally valid
different tree-typologies can be associated with strings, and identified this as a major
obstacle to fair and unbiased parser evaluation. Dependency-based parser evaluation,
as it turns out, is not entirely free of this criticism either: There are significant systematic
differences between the PARC 700 dependency and the CBS 500 dependency represen-
tations; there are significant systematic differences between the LFG f-structures gener-
ated by the hand-crafted, wide-coverage grammars of Riezler et al (2002) and Kaplan
et al (2004) and those of the treebank-induced and f-structure annotation algorithm
based resources of Cahill et al (2004). These differences require careful implementation
of mappings if parsers are not to be unduly penalized for systematic and motivated
differences at the level of dependency representation. By and large, these differences
are, however, less pronounced than differences on CFG tree representations, making
dependency-based parser evaluation a worthwhile and rewarding exercise.
Summarizing our results, we find that against the DCU 105 development set, the
treebank- and f-structure annotation algorithm-based LFG system using Bikel?s parser
retrained to retain Penn-II functional tag labels performs best, achieving f-scores of
82.92% preds-only and 88.3% all grammatical functions. Against the automatically
generated WSJ Section 22 Dependency Bank, the system using Bikel?s retrained parser
achieves the highest results, achieving f-scores of 83.06% preds-only and 87.861% all
GFs. This is statistically significantly better than all other parsers. In order to evaluate
against the PARC 700 and CBS 500 gold standards, we automaticallymap the dependen-
cies produced by our treebank-based LFG system into a format compatible with the gold
standards. Against the PARC 700 Dependency Bank, the treebank-based LFG system
using Bikel?s retrained parser achieves an f-score of 82.73%, a statistically significant
improvement of 2.18% against the most up-to-date results of the hand-crafted XLE-
based parsing resources. Against the CBS 500, the treebank-based LFG system using
Bikel?s retrained parser achieved the highest f-score of 80.23%, a statistically significant
improvement of 3.66 percentage points on the highest previously published results
for the same experiment with the hand-crafted RASP resources in Carroll and Briscoe
(2002).
Appendix A. Parser Parameter Settings
This section provides a complete list of the parameter settings used for each of the
parsers described in this article.
Parser Parameters
Collins Model 3 We used the Collins parser with its default settings of a
beam size of 10,000 and where the values of the following
flags are set to 1: punctuation-flag, distaflag, distvflag,
and npbflag. Input was pre-tagged using the MXPOST
POS tagger (Ratnaparkhi 1996). We parse the input file
with all three models and use the scripts provided to
merge the outputs into the final parser output file. Note
that this file has been cleaned of all -A functional tags and
trace nodes.
Charniak We used the parser dated August 2005 and ran the
parser using the data provided in the download on pre-
tokenized sentences of length ?200. Input was automati-
cally tagged by the parser.
119
Computational Linguistics Volume 34, Number 1
Bikel Emulation of We used version 0.9.9b of the parser trained on the file
Collins Model 2 of observed events made available on the downloads
page. We used the collins.properties file and a maximum
heap size of 1,500 MB. Input was pre-tagged using the
MXPOST POS tagger (Ratnaparkhi 1996).
Bikel + Functional Tags We used version 0.9.9b of the parser trained on a version
of Sections 02?21 of the Penn-II treebank where functional
tags were not ignored by the parser. We updated the de-
fault head-finding rules to deal with the new categories.
We also trained on all sentences from the training set
(the default collins.properties file is set to ignore trees of
more than 500 tokens). Input was pre-tagged using the
MXPOST POS tagger (Ratnaparkhi 1996) and the maxi-
mum heap size was 1,500 MB.
RASP Weused a file of parser output provided through personal
communication with John Carroll. (Tagging is carried out
automatically by the parser.)
XLE We used a file of parser output provided by the Natural
Language Theory and Technology group at the Palo Alto
Research Center. (Tagging is carried out automatically by
the parser.)
Acknowledgments
We are grateful to our anonymous reviewers
whose insightful comments have improved
the article significantly. We would like to
thank John Carroll for discussion and help
with reproducing the RASP parsing results;
Michael Collins, Eugene Charniak, Dan
Bikel, and Helmut Schmid for making their
parsing engines available; and Ron Kaplan
and the team at PARC for discussion,
feedback, and support. Part of the research
presented here has been supported by
Science Foundation Ireland grants
04/BR/CS0370 and 04/IN/I527, Enterprise
Ireland Basic Research Grant SC/2001/0186,
an Irish Research Council for Science,
Engineering and Technology Ph.D.
studentship, an IBM Ph.D. studentship and
support from IBM?s Centre for Advanced
Studies (CAS) in Dublin.
References
Abney, Stephen. 1997. Stochastic
attribute-value grammars. Computational
Linguistics, 23(4):597?618.
Alshawi, Hiyan and Stephen Pulman, 1992.
Ellipsis, Comparatives, and Generation,
chapter 13. The MIT Press, Cambridge,
MA.
Baldwin, Timothy, Emily Bender, Dan
Flickinger, Ara Kim, and Stephan Oepen.
2004. Road-testing the English Resource
Grammar over the British National
Corpus. In Proceedings of the Fourth
International Conference on Language
Resources and Evaluation (LREC 2004),
pages 2047?2050, Lisbon, Portugal.
Bikel, Dan. 2002. Design of a multi-lingual,
parallel-processing statistical parsing
engine. In Proceedings of HLT 2002,
pages 24?27, San Diego, CA.
Black, Ezra, Steven Abney, Dan Flickenger,
Claudia Gdaniec, Ralph Grishman, Philip
Harrison, Donald Hindle, Robert Ingria,
Fred Jelineck, Judith Klavans, Mark
Liberman, Mitchell Marcus, Salim Roukos,
Beatrice Santorini, and Tomek
Strzalkowski. 1991. A procedure for
quantitatively comparing the syntactic
coverage of english grammars. In
Proceedings of the Speech and Natural
Language Workshop, pages 306?311, Pacific
Grove, CA.
Bod, Rens. 2003. An efficient implementation
of a new DOP model. In Proceedings of the
Tenth Conference of the European Chapter of
the Association for Computational Linguistics
(EACL?03), pages 19?26, Budapest,
Hungary.
Bouma, Gosse, Gertjan van Noord, and
Robert Malouf. 2000. Alpino:
Wide-coverage computational analysis of
dutch. In Walter Daelemans, Khalil
Sima?an, Jorn Veenstra, and Jakub Zavrel,
editors, Computational Linguistics in The
Netherlands 2000. Rodopi, Amsterdam,
pages 45?59.
120
Cahill et al Statistical Parsing Using Automatic Dependency Structures
Bresnan, Joan. 2001. Lexical-Functional Syntax.
Blackwell, Oxford, England.
Briscoe, Edward and John Carroll. 1993.
Generalized probabilistic LR parsing of
natural language (corpora) with
unification-based grammars. Computational
Linguistics, 19(1):25?59.
Briscoe, Ted and John Carroll. 2006.
Evaluating the accuracy of an
unlexicalized statistical parser on the
PARC DepBank. In Proceedings of the
COLING/ACL 2006 Main Conference Poster
Sessions, pages 41?48, Sydney, Australia.
Briscoe, Edward, Claire Grover, Bran
Boguraev, and John Carroll. 1987. A
formalism and environment for the
development of a large grammar of
English. In Proceedings of the 10th
International Joint Conference on AI,
pages 703?708, Milan, Italy.
Burke, Michael. 2006. Automatic Treebank
Annotation for the Acquisition of LFG
Resources. Ph.D. thesis, School of
Computing, Dublin City University,
Dublin, Ireland.
Burke, Michael, Aoife Cahill, Ruth
O?Donovan, Josef van Genabith, and Andy
Way. 2004. Evaluation of an automatic
annotation algorithm against the PARC
700 Dependency Bank. In Proceedings of the
Ninth International Conference on LFG,
pages 101?121, Christchurch, New Zealand.
Butt, Miriam, Helge Dyvik, Tracy Holloway
King, Hiroshi Masuichi, and Christian
Rohrer. 2002. The Parallel Grammar
Project. In Proceedings of COLING 2002,
Workshop on Grammar Engineering and
Evaluation, pages 1?7, Taipei, Taiwan.
Cahill, Aoife. 2004. Parsing with Automatically
Acquired, Wide-Coverage, Robust,
Probabilistic LFG Approximations. Ph.D.
thesis, School of Computing, Dublin City
University, Dublin, Ireland.
Cahill, Aoife, Michael Burke, Ruth
O?Donovan, Josef van Genabith, and Andy
Way. 2004. Long-distance dependency
resolution in automatically acquired
wide-coverage PCFG-based LFG
approximations. In Proceedings of the 42nd
Annual Meeting of the Association for
Computational Linguistics, pages 320?327,
Barcelona, Spain.
Cahill, Aoife, Maire?ad McCarthy, Josef van
Genabith, and Andy Way. 2002a.
Automatic annotation of the Penn
Treebank with LFG f-structure
information. In Proceedings of the LREC
Workshop on Linguistic Knowledge
Acquisition and Representation: Bootstrapping
Annotated Language Data, pages 8?15, Las
Palmas, Canary Islands, Spain.
Cahill, Aoife, Maire?ad McCarthy, Josef van
Genabith, and Andy Way. 2002b. Parsing
with PCFGs and automatic f-structure
annotation. In Proceedings of the Seventh
International Conference on LFG,
pages 76?95, Palo Alto, CA.
Carroll, John and Edward Briscoe. 2002.
High precision extraction of grammatical
relations. In Proceedings of the 19th
International Conference on Computational
Linguistics (COLING), pages 134?140,
Taipei, Taiwan.
Carroll, John, Edward Briscoe, and Antonio
Sanfilippo. 1998. Parser evaluation: A
survey and new proposal. In Proceedings of
the International Conference on Language
Resources and Evaluation, pages 447?454,
Granada, Spain.
Carroll, John, Anette Frank, Dekang Lin,
Detlef Prescher, and Hans Uszkoreit,
editors. 2002. HLT Workhop: ?Beyond
PARSEVAL ? Towards improved evaluation
measures for parsing systems?, Las Palmas,
Canary Islands, Spain.
Charniak, Eugene. 1996. Tree-bank
grammars. In Proceedings of the
Thirteenth National Conference on
Artificial Intelligence, pages 1031?1036,
Menlo Park, CA.
Charniak, Eugene. 2000. A maximum
entropy inspired parser. In Proceedings
of the First Annual Meeting of the North
American Chapter of the Association for
Computational Linguistics (NAACL 2000),
pages 132?139, Seattle, WA.
Chinchor, Nancy, Lynette Hirschman, and
David D. Lewis. 1993. Evaluating message
understanding systems: An analysis of the
Third Message Understanding Conference
(MUC-3). Computational Linguistics,
19(3):409?449.
Clark, Stephen and James Curran. 2004.
Parsing the WSJ using CCG and log-linear
models. In Proceedings of the 42nd Annual
Meeting of the Association for Computational
Linguistics (ACL-04), pages 104?111,
Barcelona, Spain.
Clark, Stephen and James Curran. 2007.
Formalism-independent parser evaluation
with CCG and DepBank. In Proceedings of
the 45th Annual Meeting of the Association
for Computational Linguistics (ACL 2007),
pages 248?255, Prague, Czech Republic
http://www.aclweb-org/anthology/P/
P07/P07-1032.
Clark, Stephen and Julia Hockenmaier. 2002.
Evaluating a wide-coverage CCG parser.
121
Computational Linguistics Volume 34, Number 1
In Proceedings of the LREC 2002 Beyond
Parseval Workshop, pages 60?66, Las
Palmas, Canary Islands, Spain.
Cohen, Paul R. 1995. Empirical Methods for
Artificial Intelligence. The MIT Press,
Cambridge, MA.
Collins, Michael. 1997. Three generative,
lexicalized models for statistical parsing.
In Proceedings of the 35th Annual Meeting of
the Association for Computational Linguistics,
pages 16?23, Madrid, Spain.
Collins, Michael. 1999. Head-Driven Statistical
Models for Natural Language Parsing. Ph.D.
thesis, University of Pennsylvania,
Philadelphia, PA.
Crouch, Richard, Ron Kaplan, Tracy Holloway
King, and Stefan Riezler. 2002. A
comparison of evaluation metrics for a
broad coverage parser. In Proceedings of
the LREC Workshop: Beyond PARSEVAL?
Towards Improved Evaluation Measures for
Parsing Systems, pages 67?74, Las Palmas,
Canary Islands, Spain.
Dalrymple, Mary. 2001. Lexical-Functional
Grammar. London, Academic Press.
Dienes, Pe?ter and Amit Dubey. 2003.
Antecedent recovery: Experiments with a
trace tagger. In Proceedings of the 2003
Conference on Empirical Methods in Natural
Language Processing, pages 33?40, Sapporo,
Japan.
Eisele, Andreas and Jochen Do?rre. 1986. A
lexical functional grammar system in
Prolog. In Proceedings of the 11th International
Conference on Computational Linguistics
(COLING 1986), pages 551?553, Bonn.
Flickinger, Dan. 2000. On building a more
efficient grammar by exploiting types.
Natural Language Engineering, 6(1):15?28.
Gaizauskas, Rob. 1995. Investigations into
the grammar underlying the Penn
Treebank II. Research Memorandum
CS-95-25, Department of Computer
Science, Univeristy of Sheffield, UK.
Gildea, Daniel and Julia Hockenmaier.
2003. Identifying semantic roles using
combinatory categorial grammar.
In Proceedings of the 2003 Conference
on Empirical Methods in Natural
Language Processing, pages 57?64,
Sapporo, Japan.
Hockenmaier, Julia. 2003. Parsing with
generative models of predicate?argument
structure. In Proceedings of the 41st Annual
Conference of the Association for
Computational Linguistics, pages 359?366,
Sapporo, Japan.
Hockenmaier, Julia and Mark Steedman.
2002. Generative models for statistical
parsing with combinatory categorial
grammar. In Proceedings of the 40th
Annual Meeting of the Association for
Computational Linguistics, pages 335?342,
Philadelphia, PA.
Johnson, Mark. 1999. PCFG models of
linguistic tree representations.
Computational Linguistics, 24(4):613?632.
Johnson, Mark. 2002. A simple
pattern-matching algorithm for
recovering empty nodes and their
antecedents. In Proceedings of the 40th
Annual Meeting of the Association for
Computational Linguistics, pages 136?143,
Philadelphia, PA.
Kaplan, Ron and Joan Bresnan. 1982.
Lexical functional grammar, a formal
system for grammatical representation.
In Joan Bresnan, editor, The Mental
Representation of Grammatical Relations.
MIT Press, Cambridge, MA,
pages 173?281.
Kaplan, Ron, Stefan Riezler, Tracy Holloway
King, John T. Maxwell, Alexander
Vasserman, and Richard Crouch. 2004.
Speed and accuracy in shallow and deep
stochastic parsing. In Proceedings of the
Human Language Technology Conference and
the 4th Annual Meeting of the North
American Chapter of the Association for
Computational Linguistics (HLT-NAACL?04),
pages 97?104, Boston, MA.
Kaplan, Ronald and Annie Zaenen. 1989.
Long-distance dependencies, constituent
structure and functional uncertainty. In
Mark Baltin and Anthony Kroch, editors,
Alternative Conceptions of Phrase Structure,
pages 17?42, University of Chicago Press,
Chicago.
King, Tracy Holloway, Richard Crouch,
Stefan Riezler, Mary Dalrymple, and Ron
Kaplan. 2003. The PARC 700 dependency
bank. In Proceedings of the EACL03: 4th
International Workshop on Linguistically
Interpreted Corpora (LINC-03), pages 1?8,
Budapest, Hungary.
Kingsbury, Paul, Martha Palmer, and
Mitch Marcus. 2002. Adding semantic
annotation to the Penn TreeBank. In
Proceedings of the Human Language
Technology Conference, pages 252?256,
San Diego, CA.
Klein, Dan and Christopher D. Manning.
2003. Accurate unlexicalized parsing. In
Proceedings of the 41st Annual Meeting of the
Association for Computational Linguistics,
pages 423?430, Sapporo, Japan.
Leech, Geoffrey and Roger Garside. 1991.
Running a grammar factory: On the
122
Cahill et al Statistical Parsing Using Automatic Dependency Structures
compilation of parsed corpora, or
?treebanks?. In Stig Johansson and
Anna-Brita Stenstro?m, editors, English
Computer Corpora: Selected Papers. Mouton
de Gruyter, Berlin, pages 15?32.
Levy, Roger and Christopher D. Manning.
2004. Deep dependencies from context-free
statistical parsers: Correcting the surface
dependency approximation. In Proceedings
of the 42nd Annual Meeting of the Association
for Computational Linguistics (ACL 2004),
pages 328?335, Barcelona, Spain.
Lin, Dekang. 1995. A dependency-based
method for evaluating broad-coverage
parsers. In Proceedings of the International
Joint Conference on AI, pages 1420?1427,
Montre?al, Canada.
Magerman, David. 1994. Natural Language
Parsing as Statistical Pattern Recognition.
Ph.D. thesis, Department of Computer
Science, Stanford University, CA.
Marcus, Mitchell, Grace Kim, Mary Ann
Marcinkiewicz, Robert MacIntyre,
Ann Bies, Mark Ferguson, Karen Katz,
and Britta Schasberger. 1994. The
Penn Treebank: Annotating predicate
argument structure. In Proceedings
of the ARPA Workshop on Human
Language Technology, pages 110?115,
Princeton, NJ.
McCarthy, Maire?ad. 2003. Design and
Evaluation of the Linguistic Basis of an
Automatic F-Structure Annotation Algorithm
for the Penn-II Treebank. Master?s thesis,
School of Computing, Dublin City
University, Dublin, Ireland.
McDonald, Ryan and Fernando Pereira. 2006.
Online learning of approximate
dependency parsing algorithms. In
Proceedings of the 11th Conference of the
European Chapter of the Association for
Computational Linguistics, pages 81?88,
Trento, Italy.
Miyao, Yusuke, Takashi Ninomiya, and
Jun?ichi Tsujii. 2003. Probabilistic modeling
of argument structures including non-local
dependencies. In Proceedings of the
Conference on Recent Advances in Natural
Language Processing (RANLP),
pages 285?291, Borovets, Bulgaria.
Miyao, Yusuke and Jun?ichi Tsujii. 2002.
Maximum entropy estimation for feature
forests. In Proceedings of Human Language
Technology Conference (HLT 2002),
pages 292?297, San Diego, CA.
Miyao, Yusuke and Jun?ichi Tsujii.
2004. Deep linguistic analysis
for the accurate identification of
predicate?argument relations. In
Proceedings of the 18th International
Conference on Computational Linguistics
(COLING 2004), pages 1392?1397,
Geneva, Switzerland.
Noreen, Eric W. 1989. Computer Intensive
Methods for Testing Hypotheses: An
Introduction. Wiley, New York.
O?Donovan, Ruth, Michael Burke, Aoife
Cahill, Josef van Genabith, and Andy
Way. 2004. Large-scale induction and
evaluation of lexical resources from the
Penn-II treebank. In Proceedings of the 42nd
Annual Meeting of the Association for
Computational Linguistics, pages 368?375,
Barcelona, Spain.
Pollard, Carl and Ivan Sag. 1994. Head-driven
Phrase Structure Grammar. CSLI
Publications, Stanford, CA.
Preiss, Judita. 2003. Using grammatical
relations to compare parsers. In Proceedings
of the Tenth Conference of the European
Chapter of the Association for Computational
Linguistics (EACL?03), pages 291?298,
Budapest, Hungary.
Ratnaparkhi, Adwait. 1996. A maximum
entropy part-of-speech tagger. In
Proceedings of the Empirical Methods in
Natural Language Processing Conference,
pages 133?142, Philadelphia, PA.
Riezler, Stefan, Tracy King, Ronald Kaplan,
Richard Crouch, John T. Maxwell,
and Mark Johnson. 2002. Parsing
theWall Street Journal using a
lexical-functional grammar and
discriminative estimation techniques.
In Proceedings of the 40th Annual
Conference of the Association for
Computational Linguistics (ACL-02),
pages 271?278, Philadelphia, PA.
Sampson, Geoffrey. 1995. English for the
Computer: The SUSANNE Corpus and
Analytic Scheme. Clarendon Press,
Oxford, England.
Tsuruoka, Yoshimasa, Yusuke Miyao,
and Jun?ichi Tsujii. 2004. Towards
efficient probabilistic HPSG parsing:
Integrating semantic and syntactic
preference to guide the parsing.
In Proceedings of IJCNLP-04 Workshop:
Beyond shallow analyses?Formalisms
and statistical modeling for deep
analyses, Hainan Island, China. [No
page numbers].
van Genabith, Josef and Richard Crouch.
1996. Direct and underspecified
interpretations of LFG f-structures. In 16th
International Conference on Computational
Linguistics (COLING 96), pages 262?267,
Copenhagen, Denmark.
123
Computational Linguistics Volume 34, Number 1
van Genabith, Josef and Richard
Crouch. 1997. On interpreting
f-structures as UDRSs. In Proceedings
of ACL-EACL-97, pages 402?409,
Madrid, Spain.
Xia, Fei. 1999. Extracting tree adjoining
grammars from bracketed corpora. In
Proceedings of the 5th Natural Language
Processing Pacific Rim Symposium
(NLPRS-99), pages 398?403, Beijing,
China.
Xue, Nianwen, Fei Xia, Fu-Dong Chiou, and
Martha Palmer. 2004. The Penn Chinese
treebank: Phrase structure annotation of a
large corpus. Natural Language Engineering,
10(4):1?30.
Yeh, Alexander. 2000. More accurate tests
for the statistical significance of result
differences. In Proceedings of the 18th
International Conference on Computational
Linguistics (COLING, 2000), pages 947?953,
Saarbru?cken, Germany.
124
Long-Distance Dependency Resolution in Automatically Acquired
Wide-Coverage PCFG-Based LFG Approximations
Aoife Cahill, Michael Burke, Ruth O?Donovan, Josef van Genabith, Andy Way
National Centre for Language Technology and School of Computing,
Dublin City University, Dublin, Ireland
{acahill,mburke,rodonovan,josef,away}@computing.dcu.ie
Abstract
This paper shows how finite approximations of
long distance dependency (LDD) resolution can be
obtained automatically for wide-coverage, robust,
probabilistic Lexical-Functional Grammar (LFG)
resources acquired from treebanks. We extract LFG
subcategorisation frames and paths linking LDD
reentrancies from f-structures generated automati-
cally for the Penn-II treebank trees and use them
in an LDD resolution algorithm to parse new text.
Unlike (Collins, 1999; Johnson, 2002), in our ap-
proach resolution of LDDs is done at f-structure
(attribute-value structure representations of basic
predicate-argument or dependency structure) with-
out empty productions, traces and coindexation in
CFG parse trees. Currently our best automatically
induced grammars achieve 80.97% f-score for f-
structures parsing section 23 of the WSJ part of the
Penn-II treebank and evaluating against the DCU
1051 and 80.24% against the PARC 700 Depen-
dency Bank (King et al, 2003), performing at the
same or a slightly better level than state-of-the-art
hand-crafted grammars (Kaplan et al, 2004).
1 Introduction
The determination of syntactic structure is an im-
portant step in natural language processing as syn-
tactic structure strongly determines semantic inter-
pretation in the form of predicate-argument struc-
ture, dependency relations or logical form. For a
substantial number of linguistic phenomena such
as topicalisation, wh-movement in relative clauses
and interrogative sentences, however, there is an im-
portant difference between the location of the (sur-
face) realisation of linguistic material and the loca-
tion where this material should be interpreted se-
mantically. Resolution of such long-distance de-
pendencies (LDDs) is therefore crucial in the de-
termination of accurate predicate-argument struc-
1Manually constructed f-structures for 105 randomly se-
lected trees from Section 23 of the WSJ section of the Penn-II
Treebank
ture, deep dependency relations and the construc-
tion of proper meaning representations such as log-
ical forms (Johnson, 2002).
Modern unification/constraint-based grammars
such as LFG or HPSG capture deep linguistic infor-
mation including LDDs, predicate-argument struc-
ture, or logical form. Manually scaling rich uni-
fication grammars to naturally occurring free text,
however, is extremely time-consuming, expensive
and requires considerable linguistic and computa-
tional expertise. Few hand-crafted, deep unification
grammars have in fact achieved the coverage and
robustness required to parse a corpus of say the size
and complexity of the Penn treebank: (Riezler et
al., 2002) show how a deep, carefully hand-crafted
LFG is successfully scaled to parse the Penn-II tree-
bank (Marcus et al, 1994) with discriminative (log-
linear) parameter estimation techniques.
The last 20 years have seen continuously increas-
ing efforts in the construction of parse-annotated
corpora. Substantial treebanks2 are now available
for many languages (including English, Japanese,
Chinese, German, French, Czech, Turkish), others
are currently under construction (Arabic, Bulgarian)
or near completion (Spanish, Catalan). Treebanks
have been enormously influential in the develop-
ment of robust, state-of-the-art parsing technology:
grammars (or grammatical information) automat-
ically extracted from treebank resources provide
the backbone of many state-of-the-art probabilis-
tic parsing approaches (Charniak, 1996; Collins,
1999; Charniak, 1999; Hockenmaier, 2003; Klein
and Manning, 2003). Such approaches are attrac-
tive as they achieve robustness, coverage and per-
formance while incurring very low grammar devel-
opment cost. However, with few notable exceptions
(e.g. Collins? Model 3, (Johnson, 2002), (Hocken-
maier, 2003) ), treebank-based probabilistic parsers
return fairly simple ?surfacey? CFG trees, with-
out deep syntactic or semantic information. The
grammars used by such systems are sometimes re-
2Or dependency banks.
ferred to as ?half? (or ?shallow?) grammars (John-
son, 2002), i.e. they do not resolve LDDs but inter-
pret linguistic material purely locally where it oc-
curs in the tree.
Recently (Cahill et al, 2002) showed how
wide-coverage, probabilistic unification grammar
resources can be acquired automatically from f-
structure-annotated treebanks. Many second gen-
eration treebanks provide a certain amount of
deep syntactic or dependency information (e.g. in
the form of Penn-II functional tags and traces)
supporting the computation of representations of
deep linguistic information. Exploiting this in-
formation (Cahill et al, 2002) implement an
automatic LFG f-structure annotation algorithm
that associates nodes in treebank trees with f-
structure annotations in the form of attribute-value
structure equations representing abstract predicate-
argument structure/dependency relations. From the
f-structure annotated treebank they automatically
extract wide-coverage, robust, PCFG-based LFG
approximations that parse new text into trees and
f-structure representations.
The LFG approximations of (Cahill et al, 2002),
however, are only ?half? grammars, i.e. like most
of their probabilistic CFG cousins (Charniak, 1996;
Johnson, 1999; Klein and Manning, 2003) they do
not resolve LDDs but interpret linguistic material
purely locally where it occurs in the tree.
In this paper we show how finite approxima-
tions of long distance dependency resolution can be
obtained automatically for wide-coverage, robust,
probabilistic LFG resources automatically acquired
from treebanks. We extract LFG subcategorisation
frames and paths linking LDD reentrancies from
f-structures generated automatically for the Penn-
II treebank trees and use them in an LDD resolu-
tion algorithm to parse new text. Unlike (Collins,
1999; Johnson, 2002), in our approach LDDs are
resolved on the level of f-structure representation,
rather than in terms of empty productions and co-
indexation on parse trees. Currently we achieve f-
structure/dependency f-scores of 80.24 and 80.97
for parsing section 23 of the WSJ part of the Penn-
II treebank, evaluating against the PARC 700 and
DCU 105 respectively.
The paper is structured as follows: we give a
brief introduction to LFG. We outline the automatic
f-structure annotation algorithm, PCFG-based LFG
grammar approximations and parsing architectures
of (Cahill et al, 2002). We present our subcategori-
sation frame extraction and introduce the treebank-
based acquisition of finite approximations of LFG
functional uncertainty equations in terms of LDD
paths. We present the f-structure LDD resolution
algorithm, provide results and extensive evaluation.
We compare our method with previous work. Fi-
nally, we conclude.
2 Lexical Functional Grammar (LFG)
Lexical-Functional Grammar (Kaplan and Bres-
nan, 1982; Dalrymple, 2001) minimally involves
two levels of syntactic representation:3 c-structure
and f-structure. C(onstituent)-structure represents
the grouping of words and phrases into larger
constituents and is realised in terms of a CF-
PSG grammar. F(unctional)-structure represents
abstract syntactic functions such as SUBJ(ect),
OBJ(ect), OBL(ique), closed and open clausal
COMP/XCOMP(lement), ADJ(unct), APP(osition)
etc. and is implemented in terms of recursive feature
structures (attribute-value matrices). C-structure
captures surface grammatical configurations, f-
structure encodes abstract syntactic information
approximating to predicate-argument/dependency
structure or simple logical form (van Genabith
and Crouch, 1996). C- and f-structures are re-
lated in terms of functional annotations (constraints,
attribute-value equations) on c-structure rules (cf.
Figure 1).
S
NP VP
U.N. V NP
signs treaty
[
SUBJ
[
PRED U.N.
]
PRED sign
OBJ
[
PRED treaty
]
]
S ? NP VP
?SUBJ=? ?=?
VP ? V NP
?=? ?OBJ=?
NP ? U.N V ? signs
?PRED=U.N. ?PRED=sign
Figure 1: Simple LFG C- and F-Structure
Uparrows point to the f-structure associated with the
mother node, downarrows to that of the local node.
The equations are collected with arrows instanti-
ated to unique tree node identifiers, and a constraint
solver generates an f-structure.
3 Automatic F-Structure Annotation
The Penn-II treebank employs CFG trees with addi-
tional ?functional? node annotations (such as -LOC,
-TMP, -SBJ, -LGS, . . . ) as well as traces and coin-
dexation (to indicate LDDs) as basic data structures.
The f-structure annotation algorithm of (Cahill et
3LFGs may also involve morphological and semantic levels
of representation.
al., 2002) exploits configurational, categorial, Penn-
II ?functional?, local head and trace information
to annotate nodes with LFG feature-structure equa-
tions. A slightly adapted version of (Magerman,
1994)?s scheme automatically head-lexicalises the
Penn-II trees. This partitions local subtrees of depth
one (corresponding to CFG rules) into left and right
contexts (relative to head). The annotation algo-
rithm is modular with four components (Figure 2):
left-right (L-R) annotation principles (e.g. leftmost
NP to right of V head of VP type rule is likely to be
an object etc.); coordination annotation principles
(separating these out simplifies other components
of the algorithm); traces (translates traces and coin-
dexation in trees into corresponding reentrancies in
f-structure ( 1 in Figure 3)); catch all and clean-up.
Lexical information is provided via macros for POS
tag classes.
L/R Context ? Coordination ? Traces ? Catch-All
Figure 2: Annotation Algorithm
The f-structure annotations are passed to a con-
straint solver to produce f-structures. Annotation
is evaluated in terms of coverage and quality, sum-
marised in Table 1. Coverage is near complete with
99.82% of the 48K Penn-II sentences receiving a
single, connected f-structure. Annotation quality is
measured in terms of precision and recall (P&R)
against the DCU 105. The algorithm achieves an
F-score of 96.57% for full f-structures and 94.3%
for preds-only f-structures.4
S
S-TPC- 1
NP
U.N.
VP
V
signs
NP
treaty
NP
Det
the
N
headline
VP
V
said
S
T- 1
?
?
?
?
?
?
?
TOPIC
[
SUBJ
[
PRED U.N.
]
PRED sign
OBJ
[
PRED treaty
]
]
1
SUBJ
[
SPEC the
PRED headline
]
PRED say
COMP 1
?
?
?
?
?
?
?
Figure 3: Penn-II style tree with LDD trace and cor-
responding reentrancy in f-structure
4Full f-structures measure all attribute-value pairs includ-
ing?minor? features such as person, number etc. The stricter
preds-only captures only paths ending in PRED:VALUE.
# frags # sent percent
0 85 0.176
1 48337 99.820
2 2 0.004
all preds
P 96.52 94.45
R 96.63 94.16
Table 1: F-structure annotation results for DCU 105
4 PCFG-Based LFG Approximations
Based on these resources (Cahill et al, 2002) de-
veloped two parsing architectures. Both generate
PCFG-based approximations of LFG grammars.
In the pipeline architecture a standard PCFG is
extracted from the ?raw? treebank to parse unseen
text. The resulting parse-trees are then annotated by
the automatic f-structure annotation algorithm and
resolved into f-structures.
In the integrated architecture the treebank
is first annotated with f-structure equations.
An annotated PCFG is then extracted where
each non-terminal symbol in the grammar
has been augmented with LFG f-equations:
NP[?OBJ=?] ? DT[?SPEC=?] NN[?=?] . Nodes
followed by annotations are treated as a monadic
category for grammar extraction and parsing.
Post-parsing, equations are collected from parse
trees and resolved into f-structures.
Both architectures parse raw text into ?proto? f-
structures with LDDs unresolved resulting in in-
complete argument structures as in Figure 4.
S
S
NP
U.N.
VP
V
signs
NP
treaty
NP
Det
the
N
headline
VP
V
said
?
?
?
?
?
TOPIC
[
SUBJ
[
PRED U.N.
]
PRED sign
OBJ
[
PRED treaty
]
]
SUBJ
[
SPEC the
PRED headline
]
PRED say
?
?
?
?
?
Figure 4: Shallow-Parser Output with Unresolved
LDD and Incomplete Argument Structure (cf. Fig-
ure 3)
5 LDDs and LFG FU-Equations
Theoretically, LDDs can span unbounded amounts
of intervening linguistic material as in
[U.N. signs treaty]1 the paper claimed . . . a source said []1.
In LFG, LDDs are resolved at the f-structure level,
obviating the need for empty productions and traces
in trees (Dalrymple, 2001), using functional uncer-
tainty (FU) equations. FUs are regular expressions
specifying paths in f-structure between a source
(where linguistic material is encountered) and a tar-
get (where linguistic material is interpreted seman-
tically). To account for the fronted sentential con-
stituents in Figures 3 and 4, an FU equation of the
form ? TOPIC = ? COMP* COMP would be required.
The equation states that the value of the TOPIC at-
tribute is token identical with the value of the final
COMP argument along a path through the immedi-
ately enclosing f-structure along zero or more COMP
attributes. This FU equation is annotated to the top-
icalised sentential constituent in the relevant CFG
rules as follows
S ? S NP VP
?TOPIC=? ?SUBJ=? ?=?
?TOPIC=?COMP*COMP
and generates the LDD-resolved proper f-structure
in Figure 3 for the traceless tree in Figure 4, as re-
quired.
In addition to FU equations, subcategorisation in-
formation is a crucial ingredient in LFG?s account
of LDDs. As an example, for a topicalised con-
stituent to be resolved as the argument of a local
predicate as specified by the FU equation, the local
predicate must (i) subcategorise for the argument in
question and (ii) the argument in question must not
be already filled. Subcategorisation requirements
are provided lexically in terms of semantic forms
(subcat lists) and coherence and completeness con-
ditions (all GFs specified must be present, and no
others may be present) on f-structure representa-
tions. Semantic forms specify which grammatical
functions (GFs) a predicate requires locally. For our
example in Figures 3 and 4, the relevant lexical en-
tries are:
V ? said ?PRED=say?? SUBJ, ? COMP?
V ? signs ?PRED=sign?? SUBJ, ? OBJ?
FU equations and subcategorisation requirements
together ensure that LDDs can only be resolved at
suitable f-structure locations.
6 Acquiring Lexical and LDD Resources
In order to model the LFG account of LDD resolu-
tion we require subcat frames (i.e. semantic forms)
and LDD resolution paths through f-structure. Tra-
ditionally, such resources were handcoded. Here we
show how they can be acquired from f-structure an-
notated treebank resources.
LFG distinguishes between governable (argu-
ments) and nongovernable (adjuncts) grammati-
cal functions (GFs). If the automatic f-structure
annotation algorithm outlined in Section 3 gen-
erates high quality f-structures, reliable seman-
tic forms can be extracted (reverse-engineered):
for each f-structure generated, for each level of
embedding we determine the local PRED value
and collect the governable, i.e. subcategoris-
able grammatical functions present at that level
of embedding. For the proper f-structure in
Figure 3 we obtain sign([subj,obj]) and
say([subj,comp]). We extract frames from
the full WSJ section of the Penn-II Treebank with
48K trees. Unlike many other approaches, our ex-
traction process does not predefine frames, fully
reflects LDDs in the source data-structures (cf.
Figure 3), discriminates between active and pas-
sive frames, computes GF, GF:CFG category pair-
as well as CFG category-based subcategorisation
frames and associates conditional probabilities with
frames. Given a lemma l and an argument list s, the
probability of s given l is estimated as:
P(s|l) := count(l, s)?n
i=1 count(l, si)
Table 2 summarises the results. We extract 3586
verb lemmas and 10969 unique verbal semantic
form types (lemma followed by non-empty argu-
ment list). Including prepositions associated with
the subcategorised OBLs and particles, this number
goes up to 14348. The number of unique frame
types (without lemma) is 38 without specific prepo-
sitions and particles, 577 with. F-structure anno-
tations allow us to distinguish passive and active
frames. Table 3 shows the most frequent seman-
tic forms for accept. Passive frames are marked
p. We carried out a comprehensive evaluation of
the automatically acquired verbal semantic forms
against the COMLEX Resource (Macleod et al,
1994) for the 2992 active verb lemmas that both re-
sources have in common. We report on the evalu-
ation of GF-based frames for the full frames with
complete prepositional and particle infomation. We
use relative conditional probability thresholds (1%
and 5%) to filter the selection of semantic forms
(Table 4). (O?Donovan et al, 2004) provide a more
detailed description of the extraction and evaluation
of semantic forms.
Without Prep/Part With Prep/Part
Lemmas 3586 3586
Sem. Forms 10969 14348
Frame Types 38 577
Active Frame Types 38 548
Passive Frame Types 21 177
Table 2: Verb Results
Semantic Form Occurrences Prob.
accept([obj,subj]) 122 0.813
accept([subj],p) 9 0.060
accept([comp,subj]) 5 0.033
accept([subj,obl:as],p) 3 0.020
accept([obj,subj,obl:as]) 3 0.020
accept([obj,subj,obl:from]) 3 0.020
accept([subj]) 2 0.013
accept([obj,subj,obl:at]) 1 0.007
accept([obj,subj,obl:for]) 1 0.007
accept([obj,subj,xcomp]) 1 0.007
Table 3: Semantic forms for the verb accept.
Threshold 1% Threshold 5%
P R F-Score P R F-Score
Exp. 73.7% 22.1% 34.0% 78.0% 18.3% 29.6%
Table 4: COMLEX Comparison
We further acquire finite approximations of FU-
equations. by extracting paths between co-indexed
material occurring in the automatically generated f-
structures from sections 02-21 of the Penn-II tree-
bank. We extract 26 unique TOPIC, 60 TOPIC-REL
and 13 FOCUS path types (with a total of 14,911 to-
ken occurrences), each with an associated probabil-
ity. We distinguish between two types of TOPIC-
REL paths, those that occur in wh-less constructions,
and all other types (c.f Table 5). Given a path p and
an LDD type t (either TOPIC, TOPIC-REL or FO-
CUS), the probability of p given t is estimated as:
P(p|t) := count(t, p)?n
i=1 count(t, pi)
In order to get a first measure of how well the ap-
proximation models the data, we compute the path
types in section 23 not covered by those extracted
from 02-21: 23/(02-21). There are 3 such path types
(Table 6), each occuring exactly once. Given that
the total number of path tokens in section 23 is 949,
the finite approximation extracted from 02-23 cov-
ers 99.69% of all LDD paths in section 23.
7 Resolving LDDs in F-Structure
Given a set of semantic forms s with probabilities
P(s|l) (where l is a lemma), a set of paths p with
P(p|t) (where t is either TOPIC, TOPIC-REL or FO-
CUS) and an f-structure f , the core of the algorithm
to resolve LDDs recursively traverses f to:
find TOPIC|TOPIC-REL|FOCUS:g pair; retrieve
TOPIC|TOPIC-REL|FOCUS paths; for each path p
with GF1 : . . . : GFn : GF, traverse f along GF1 : . . . :
GFn to sub-f-structure h; retrieve local PRED:l;
add GF:g to h iff
? GF is not present at h
wh-less TOPIC-REL # wh-less TOPIC-REL #
subj 5692 adjunct 1314
xcomp:adjunct 610 obj 364
xcomp:obj 291 xcomp:xcomp:adjunct 96
comp:subj 76 xcomp:subj 67
Table 5: Most frequent wh-less TOPIC-REL paths
02?21 23 23 /(02?21)
TOPIC 26 7 2
FOCUS 13 4 0
TOPIC-REL 60 22 1
Table 6: Number of path types extracted
? h together with GF is locally complete and co-
herent with respect to a semantic form s for l
rank resolution by P(s|l) ? P(p|t)
The algorithm supports multiple, interacting TOPIC,
TOPIC-REL and FOCUS LDDs. We use P(s|l) ?
P(p|t) to rank a solution, depending on how likely
the PRED takes semantic frame s, and how likely
the TOPIC, FOCUS or TOPIC-REL is resolved using
path p. The algorithm also supports resolution of
LDDs where no overt linguistic material introduces
a source TOPIC-REL function (e.g. in reduced rela-
tive clause constructions). We distinguish between
passive and active constructions, using the relevant
semantic frame type when resolving LDDs.
8 Experiments and Evaluation
We ran experiments with grammars in both the
pipeline and the integrated parsing architectures.
The first grammar is a basic PCFG, while A-PCFG
includes the f-structure annotations. We apply a
parent transformation to each grammar (Johnson,
1999) to give P-PCFG and PA-PCFG. We train
on sections 02-21 (grammar, lexical extraction and
LDD paths) of the Penn-II Treebank and test on sec-
tion 23. The only pre-processing of the trees that we
do is to remove empty nodes, and remove all Penn-
II functional tags in the integrated model. We evalu-
ate the parse trees using evalb. Following (Riezler et
al., 2002), we convert f-structures into dependency
triple format. Using their software we evaluate the
f-structure parser output against:
1. The DCU 105 (Cahill et al, 2002)
2. The full 2,416 f-structures automatically gen-
erated by the f-structure annotation algorithm
for the original Penn-II trees, in a CCG-style
(Hockenmaier, 2003) evaluation experiment
Pipeline Integrated
PCFG P-PCFG A-PCFG PA-PCFG
2416 Section 23 trees
# Parses 2416 2416 2416 2414
Lab. F-Score 75.83 80.80 79.17 81.32
Unlab. F-Score 78.28 82.70 81.49 83.28
DCU 105 F-Strs
All GFs F-Score (before LDD resolution) 79.82 79.24 81.12 81.20
All GFs F-Score (after LDD resolution) 83.79 84.59 86.30 87.04
Preds only F-Score (before LDD resolution) 70.00 71.57 73.45 74.61
Preds only F-Score (after LDD resolution) 73.78 77.43 78.76 80.97
2416 F-Strs
All GFs F-Score (before LDD resolution) 81.98 81.49 83.32 82.78
All GFs F-Score (after LDD resolution) 84.16 84.37 86.45 86.00
Preds only F-Score (before LDD resolution) 72.00 73.23 75.22 75.10
Preds only F-Score (after LDD resolution) 74.07 76.12 78.36 78.40
PARC 700 Dependency Bank
Subset of GFs following (Kaplan et al, 2004) 77.86 80.24 77.68 78.60
Table 7: Parser Evaluation
3. A subset of 560 dependency structures of the
PARC 700 Dependency Bank following (Ka-
plan et al, 2004)
The results are given in Table 7. The parent-
transformed grammars perform best in both archi-
tectures. In all cases, there is a marked improve-
ment (2.07-6.36%) in the f-structures after LDD res-
olution. We achieve between 73.78% and 80.97%
preds-only and 83.79% to 87.04% all GFs f-score,
depending on gold-standard. We achieve between
77.68% and 80.24% against the PARC 700 follow-
ing the experiments in (Kaplan et al, 2004). For
details on how we map the f-structures produced
by our parsers to a format similar to that of the
PARC 700 Dependency Bank, see (Burke et al,
2004). Table 8 shows the evaluation result broken
down by individual GF (preds-only) for the inte-
grated model PA-PCFG against the DCU 105. In
order to measure how many of the LDD reentran-
cies in the gold-standard f-structures are captured
correctly by our parsers, we developed evaluation
software for f-structure LDD reentrancies (similar
to Johnson?s (2002) evaluation to capture traces and
their antecedents in trees). Table 9 shows the results
with the integrated model achieving more than 76%
correct LDD reentrancies.
9 Related Work
(Collins, 1999)?s Model 3 is limited to wh-traces
in relative clauses (it doesn?t treat topicalisation,
focus etc.). Johnson?s (2002) work is closest to
ours in spirit. Like our approach he provides a fi-
nite approximation of LDDs. Unlike our approach,
however, he works with tree fragments in a post-
processing approach to add empty nodes and their
DEP. PRECISION RECALL F-SCORE
adjunct 717/903 = 79 717/947 = 76 78
app 14/15 = 93 14/19 = 74 82
comp 35/43 = 81 35/65 = 54 65
coord 109/143 = 76 109/161 = 68 72
det 253/264 = 96 253/269 = 94 95
focus 1/1 = 100 1/1 = 100 100
obj 387/445 = 87 387/461 = 84 85
obj2 0/1 = 0 0/2 = 0 0
obl 27/56 = 48 27/61 = 44 46
obl2 1/3 = 33 1/2 = 50 40
obl ag 5/11 = 45 5/12 = 42 43
poss 69/73 = 95 69/81 = 85 90
quant 40/55 = 73 40/52 = 77 75
relmod 26/38 = 68 26/50 = 52 59
subj 330/361 = 91 330/414 = 80 85
topic 12/12 = 100 12/13 = 92 96
topicrel 35/42 = 83 35/52 = 67 74
xcomp 139/160 = 87 139/146 = 95 91
OVERALL 83.78 78.35 80.97
Table 8: Preds-only results of PA-PCFG against the
DCU 105
antecedents to parse trees, while we present an ap-
proach to LDD resolution on the level of f-structure.
It seems that the f-structure-based approach is more
abstract (99 LDD path types against approximately
9,000 tree-fragment types in (Johnson, 2002)) and
fine-grained in its use of lexical information (sub-
cat frames). In contrast to Johnson?s approach, our
LDD resolution algorithm is not biased. It com-
putes all possible complete resolutions and order-
ranks them using LDD path and subcat frame prob-
abilities. It is difficult to provide a satisfactory com-
parison between the two methods, but we have car-
ried out an experiment that compares them at the
f-structure level. We take the output of Charniak?s
Pipeline Integrated
PCFG P-PCFG A-PCFG PA-PCFG
TOPIC
Precision (11/14) (12/13) (12/13) (12/12)
Recall (11/13) (12/13) (12/13) (12/13)
F-Score 0.81 0.92 0.92 0.96
FOCUS
Precision (0/1) (0/1) (0/1) (0/1)
Recall (0/1) (0/1) (0/1) (0/1)
F-Score 0 0 0 0
TOPIC-REL
Precision (20/34) (27/36) (34/42) (34/42)
Recall (20/52) (27/52) (34/52) (34/52)
F-Score 0.47 0.613 0.72 0.72
OVERALL 0.54 0.67 0.75 0.76
Table 9: LDD Evaluation on the DCU 105
Charniak -LDD res. +LDD res. (Johnson, 2002)
All GFs 80.86 86.65 85.16
Preds Only 74.63 80.97 79.75
Table 10: Comparison at f-structure level of LDD
resolution to (Johnson, 2002) on the DCU 105
parser (Charniak, 1999) and, using the pipeline
f-structure annotation model, evaluate against the
DCU 105, both before and after LDD resolution.
Using the software described in (Johnson, 2002) we
add empty nodes to the output of Charniak?s parser,
pass these trees to our automatic annotation algo-
rithm and evaluate against the DCU 105. The re-
sults are given in Table 10. Our method of resolv-
ing LDDs at f-structure level results in a preds-only
f-score of 80.97%. Using (Johnson, 2002)?s method
of adding empty nodes to the parse-trees results in
an f-score of 79.75%.
(Hockenmaier, 2003) provides CCG-based mod-
els of LDDs. Some of these involve extensive clean-
up of the underlying Penn-II treebank resource prior
to grammar extraction. In contrast, in our approach
we leave the treebank as is and only add (but never
correct) annotations. Earlier HPSG work (Tateisi
et al, 1998) is based on independently constructed
hand-crafted XTAG resources. In contrast, we ac-
quire our resources from treebanks and achieve sub-
stantially wider coverage.
Our approach provides wide-coverage, robust,
and ? with the addition of LDD resolution ? ?deep?
or ?full?, PCFG-based LFG approximations. Cru-
cially, we do not claim to provide fully adequate sta-
tistical models. It is well known (Abney, 1997) that
PCFG-type approximations to unification grammars
can yield inconsistent probability models due to
loss of probability mass: the parser successfully re-
turns the highest ranked parse tree but the constraint
solver cannot resolve the f-equations (generated in
the pipeline or ?hidden? in the integrated model)
and the probability mass associated with that tree is
lost. This case, however, is surprisingly rare for our
grammars: only 0.0018% (85 out of 48424) of the
original Penn-II trees (without FRAGs) fail to pro-
duce an f-structure due to inconsistent annotations
(Table 1), and for parsing section 23 with the in-
tegrated model (A-PCFG), only 9 sentences do not
receive a parse because no f-structure can be gen-
erated for the highest ranked tree (0.4%). Parsing
with the pipeline model, all sentences receive one
complete f-structure. Research on adequate prob-
ability models for unification grammars is impor-
tant. (Miyao et al, 2003) present a Penn-II tree-
bank based HPSG with log-linear probability mod-
els. They achieve coverage of 50.2% on section
23, as against 99% in our approach. (Riezler et
al., 2002; Kaplan et al, 2004) describe how a care-
fully hand-crafted LFG is scaled to the full Penn-II
treebank with log-linear based probability models.
They achieve 79% coverage (full parse) and 21%
fragement/skimmed parses. By the same measure,
full parse coverage is around 99% for our automat-
ically acquired PCFG-based LFG approximations.
Against the PARC 700, the hand-crafted LFG gram-
mar reported in (Kaplan et al, 2004) achieves an f-
score of 79.6%. For the same experiment, our best
automatically-induced grammar achieves an f-score
of 80.24%.
10 Conclusions
We presented and extensively evaluated a finite
approximation of LDD resolution in automati-
cally constructed, wide-coverage, robust, PCFG-
based LFG approximations, effectively turning the
?half?(or ?shallow?)-grammars presented in (Cahill
et al, 2002) into ?full? or ?deep? grammars. In
our approach, LDDs are resolved in f-structure, not
trees. The method achieves a preds-only f-score
of 80.97% for f-structures with the PA-PCFG in
the integrated architecture against the DCU 105
and 78.4% against the 2,416 automatically gener-
ated f-structures for the original Penn-II treebank
trees. Evaluating against the PARC 700 Depen-
dency Bank, the P-PCFG achieves an f-score of
80.24%, an overall improvement of approximately
0.6% on the result reported for the best hand-crafted
grammars in (Kaplan et al, 2004).
Acknowledgements
This research was funded by Enterprise Ireland Ba-
sic Research Grant SC/2001/186 and IRCSET.
References
S. Abney. 1997. Stochastic attribute-value gram-
mars. Computational Linguistics, 23(4):597?
618.
M. Burke, A. Cahill, R. O?Donovan, J. van Gen-
abith, and A. Way 2004. The Evaluation of
an Automatic Annotation Algorithm against the
PARC 700 Dependency Bank. In Proceedings
of the Ninth International Conference on LFG,
Christchurch, New Zealand (to appear).
A. Cahill, M. McCarthy, J. van Genabith, and A.
Way. 2002. Parsing with PCFGs and Auto-
matic F-Structure Annotation. In Miriam Butt
and Tracy Holloway King, editors, Proceedings
of the Seventh International Conference on LFG,
pages 76?95. CSLI Publications, Stanford, CA.
E. Charniak. 1996. Tree-Bank Grammars. In
AAAI/IAAI, Vol. 2, pages 1031?1036.
E. Charniak. 1999. A Maximum-Entropy-Inspired
Parser. Technical Report CS-99-12, Brown Uni-
versity, Providence, RI.
M. Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Uni-
versity of Pennsylvania, Philadelphia, PA.
M. Dalrymple. 2001. Lexical-Functional Gram-
mar. San Diego, CA; London Academic Press.
J. Hockenmaier. 2003. Parsing with Generative
models of Predicate-Argument Structure. In Pro-
ceedings of the 41st Annual Conference of the
Association for Computational Linguistics, pages
359?366, Sapporo, Japan.
M. Johnson. 1999. PCFG models of linguistic
tree representations. Computational Linguistics,
24(4):613?632.
M. Johnson. 2002. A simple pattern-matching al-
gorithm for recovering empty nodes and their
antecedents. In Proceedings of the 40th Annual
Meeting of the Association for Computational
Linguistics, pages 136?143, Philadelphia, PA.
R. Kaplan and J. Bresnan. 1982. Lexical Func-
tional Grammar, a Formal System for Grammat-
ical Representation. In The Mental Representa-
tion of Grammatical Relations, pages 173?281.
MIT Press, Cambridge, MA.
R. Kaplan, S. Riezler, T. H. King, J. T. Maxwell,
A. Vasserman, and R. Crouch. 2004. Speed and
accuracy in shallow and deep stochastic parsing.
In Proceedings of the Human Language Tech-
nology Conference and the 4th Annual Meeting
of the North American Chapter of the Associ-
ation for Computational Linguistics, pages 97?
104, Boston, MA.
T.H. King, R. Crouch, S. Riezler, M. Dalrymple,
and R. Kaplan. 2003. The PARC700 dependency
bank. In Proceedings of the EACL03: 4th Inter-
national Workshop on Linguistically Interpreted
Corpora (LINC-03), pages 1?8, Budapest.
D. Klein and C. Manning. 2003. Accurate Unlexi-
calized Parsing. In Proceedings of the 41st An-
nual Meeting of the Association for Computa-
tional Linguistics (ACL?02), pages 423?430, Sap-
poro, Japan.
C. Macleod, A. Meyers, and R. Grishman. 1994.
The COMLEX Syntax Project: The First Year.
In Proceedings of the ARPA Workshop on Human
Language Technology, pages 669-703, Princeton,
NJ.
D. Magerman. 1994. Natural Language Parsing as
Statistical Pattern Recognition. PhD thesis, Stan-
ford University, CA.
M. Marcus, G. Kim, M.A. Marcinkiewicz, R. Mac-
Intyre, A. Bies, M. Ferguson, K. Katz, and B.
Schasberger. 1994. The Penn Treebank: Anno-
tating Predicate Argument Structure. In Proceed-
ings of the ARPA Workshop on Human Language
Technology, pages 110?115, Princeton, NJ.
Y. Miyao, T. Ninomiya, and J. Tsujii. 2003. Proba-
bilistic modeling of argument structures includ-
ing non-local dependencies. In Proceedings of
the Conference on Recent Advances in Natural
Language Processing (RANLP), pages 285?291,
Borovets, Bulgaria.
R. O?Donovan, M. Burke, A. Cahill, J. van Gen-
abith, and A. Way. 2004. Large-Scale Induc-
tion and Evaluation of Lexical Resources from
the Penn-II Treebank. In Proceedings of the 42nd
Annual Conference of the Association for Com-
putational Linguistics (ACL-04), Barcelona.
S. Riezler, T.H. King, R. Kaplan, R. Crouch,
J. T. Maxwell III, and M. Johnson. 2002. Pars-
ing the Wall Street Journal using a Lexical-
Functional Grammar and Discriminative Estima-
tion Techniques. In Proceedings of the 40th An-
nual Conference of the Association for Compu-
tational Linguistics (ACL-02), pages 271?278,
Philadelphia, PA.
Y. Tateisi, K. Torisawa, Y. Miyao, and J. Tsujii.
1998. Translating the XTAG English Grammar
to HPSG. In 4th International Workshop on Tree
Adjoining Grammars and Related Frameworks,
Philadelphia, PA, pages 172?175.
J. van Genabith and R. Crouch. 1996. Direct
and Underspecified Interpretations of LFG f-
Structures. In Proceedings of the 16th Interna-
tional Conference on Computational Linguistics
(COLING), pages 262?267, Copenhagen.
Large-Scale Induction and Evaluation of Lexical Resources from the
Penn-II Treebank
Ruth O?Donovan, Michael Burke, Aoife Cahill, Josef van Genabith, Andy Way
National Centre for Language Technology and School of Computing
Dublin City University
Glasnevin
Dublin 9
Ireland
{rodonovan,mburke,acahill,josef,away}@computing.dcu.ie
Abstract
In this paper we present a methodology for ex-
tracting subcategorisation frames based on an
automatic LFG f-structure annotation algorithm
for the Penn-II Treebank. We extract abstract
syntactic function-based subcategorisation frames
(LFG semantic forms), traditional CFG category-
based subcategorisation frames as well as mixed
function/category-based frames, with or without
preposition information for obliques and particle in-
formation for particle verbs. Our approach does
not predefine frames, associates probabilities with
frames conditional on the lemma, distinguishes be-
tween active and passive frames, and fully reflects
the effects of long-distance dependencies in the
source data structures. We extract 3586 verb lem-
mas, 14348 semantic form types (an average of 4
per lemma) with 577 frame types. We present a
large-scale evaluation of the complete set of forms
extracted against the full COMLEX resource.
1 Introduction
Lexical resources are crucial in the construction
of wide-coverage computational systems based on
modern syntactic theories (e.g. LFG, HPSG, CCG,
LTAG etc.). However, as manual construction of
such lexical resources is time-consuming, error-
prone, expensive and rarely ever complete, it is of-
ten the case that limitations of NLP systems based
on lexicalised approaches are due to bottlenecks in
the lexicon component.
Given this, research on automating lexical acqui-
sition for lexically-based NLP systems is a partic-
ularly important issue. In this paper we present an
approach to automating subcategorisation frame ac-
quisition for LFG (Kaplan and Bresnan, 1982) i.e.
grammatical function-based systems. LFG has two
levels of structural representation: c(onstituent)-
structure, and f(unctional)-structure. LFG differ-
entiates between governable (argument) and non-
governable (adjunct) grammatical functions. Sub-
categorisation requirements are enforced through
semantic forms specifying the governable grammat-
ical functions required by a particular predicate (e.g.
FOCUS?(? SUBJ)(? OBLon)?). Our approach is
based on earlier work on LFG semantic form extrac-
tion (van Genabith et al, 1999) and recent progress
in automatically annotating the Penn-II treebank
with LFG f-structures (Cahill et al, 2004b). De-
pending on the quality of the f-structures, reliable
LFG semantic forms can then be generated quite
simply by recursively reading off the subcategoris-
able grammatical functions for each local pred
value at each level of embedding in the f-structures.
The work reported in (van Genabith et al, 1999)
was small scale (100 trees), proof of concept and
required considerable manual annotation work. In
this paper we show how the extraction process can
be scaled to the complete Wall Street Journal (WSJ)
section of the Penn-II treebank, with about 1 mil-
lion words in 50,000 sentences, based on the au-
tomatic LFG f-structure annotation algorithm de-
scribed in (Cahill et al, 2004b). In addition to ex-
tracting grammatical function-based subcategorisa-
tion frames, we also include the syntactic categories
of the predicate and its subcategorised arguments,
as well as additional details such as the prepositions
required by obliques, and particles accompanying
particle verbs. Our method does not predefine the
frames to be extracted. In contrast to many other
approaches, it discriminates between active and pas-
sive frames, properly reflects long distance depen-
dencies and assigns conditional probabilities to the
semantic forms associated with each predicate.
Section 2 reviews related work in the area of
automatic subcategorisation frame extraction. Our
methodology and its implementation are presented
in Section 3. Section 4 presents the results of our
lexical extraction. In Section 5 we evaluate the
complete extracted lexicon against the COMLEX
resource (MacLeod et al, 1994). To our knowl-
edge, this is the largest evaluation of subcategorisa-
tion frames for English. In Section 6, we conclude
and give suggestions for future work.
2 Related Work
Creating a (subcategorisation) lexicon by hand is
time-consuming, error-prone, requires considerable
linguistic expertise and is rarely, if ever, complete.
In addition, a system incorporating a manually con-
structed lexicon cannot easily be adapted to specific
domains. Accordingly, many researchers have at-
tempted to construct lexicons automatically, espe-
cially for English.
(Brent, 1993) relies on local morphosyntactic
cues (such as the -ing suffix, except where such a
word follows a determiner or a preposition other
than to) in the untagged Brown Corpus as proba-
bilistic indicators of six different predefined subcat-
egorisation frames. The frames do not include de-
tails of specific prepositions. (Manning, 1993) ob-
serves that Brent?s recognition technique is a ?rather
simplistic and inadequate approach to verb detec-
tion, with a very high error rate?. Manning feeds
the output from a stochastic tagger into a finite state
parser, and applies statistical filtering to the parsing
results. He predefines 19 different subcategorisation
frames, including details of prepositions. Applying
this technique to approx. 4 million words of New
York Times newswire, Manning acquires 4900 sub-
categorisation frames for 3104 verbs, an average of
1.6 per verb. (Ushioda et al, 1993) run a finite state
NP parser on a POS-tagged corpus to calculate the
relative frequency of just six subcategorisation verb
classes. In addition, all prepositional phrases are
treated as adjuncts. For 1565 tokens of 33 selected
verbs, they report an accuracy rate of 83%.
(Briscoe and Carroll, 1997) observe that in the
work of (Brent, 1993), (Manning, 1993) and (Ush-
ioda et al, 1993), ?the maximum number of distinct
subcategorization classes recognized is sixteen, and
only Ushioda et al attempt to derive relative subcat-
egorization frequency for individual predicates?. In
contrast, the system of (Briscoe and Carroll, 1997)
distinguishes 163 verbal subcategorisation classes
by means of a statistical shallow parser, a classifier
of subcategorisation classes, and a priori estimates
of the probability that any verb will be a member
of those classes. More recent work by Korhonen
(2002) on the filtering phase of this approach has
improved results. Korhonen experiments with the
use of linguistic verb classes for obtaining more ac-
curate back-off estimates for use in hypothesis se-
lection. Using this extended approach, the average
results for 45 semantically classified test verbs eval-
uated against hand judgements are precision 87.1%
and recall 71.2%. By comparison, the average re-
sults for 30 verbs not classified semantically are pre-
cision 78.2% and recall 58.7%.
Carroll and Rooth (1998) use a hand-written
head-lexicalised context-free grammar and a text
corpus to compute the probability of particular sub-
categorisation scenarios. The extracted frames do
not contain details of prepositions.
More recently, a number of researchers have
applied similar techniques to derive resources for
other languages, especially German. One of these,
(Schulte im Walde, 2002), induces a computational
subcategorisation lexicon for over 14,000 German
verbs. Using sentences of limited length, she ex-
tracts 38 distinct frame types, which contain max-
imally three arguments each. The frames may op-
tionally contain details of particular prepositional
use. Her evaluation on over 3000 frequently occur-
ring verbs against the German dictionary Duden -
Das Stilwo?rterbuch is similar in scale to ours and is
discussed further in Section 5.
There has also been some work on extracting
subcategorisation details from the Penn Treebank.
(Kinyon and Prolo, 2002) introduce a tool which
uses fine-grained rules to identify the arguments,
including optional arguments, of each verb occur-
rence in the Penn Treebank, along with their syn-
tactic functions. They manually examined the 150+
possible sequences of tags, both functional and cat-
egorial, in Penn-II and determined whether the se-
quence in question denoted a modifier, argument or
optional argument. Arguments were then mapped
to traditional syntactic functions. As they do not in-
clude an evaluation, currently it is impossible to say
how effective this technique is.
(Xia et al, 2000) and (Chen and Vijay-Shanker,
2000) extract lexicalised TAGs from the Penn Tree-
bank. Both techniques implement variations on
the approaches of (Magerman, 1994) and (Collins,
1997) for the purpose of differentiating between
complement and adjunct. In the case of (Xia et al,
2000), invalid elementary trees produced as a result
of annotation errors in the treebank are filtered out
using linguistic heuristics.
(Hockenmaier et al, 2002) outline a method for
the automatic extraction of a large syntactic CCG
lexicon from Penn-II. For each tree, the algorithm
annotates the nodes with CCG categories in a top-
down recursive manner. In order to examine the
coverage of the extracted lexicon in a manner simi-
lar to (Xia et al, 2000), (Hockenmaier et al, 2002)
compared the reference lexicon acquired from Sec-
tions 02-21 with a test lexicon extracted from Sec-
tion 23 of the WSJ. It was found that the reference
CCG lexicon contained 95.09% of the entries in the
test lexicon, while 94.03% of the entries in the test
TAG lexicon also occurred in the reference lexicon.
Both approaches involve extensive correction and
clean-up of the treebank prior to lexical extraction.
3 Our Methodology
The first step in the application of our methodology
is the production of a treebank annotated with LFG
f-structure information. F-structures are feature
structures which represent abstract syntactic infor-
mation, approximating to basic predicate-argument-
modifier structures. We utilise the automatic anno-
tation algorithm of (Cahill et al, 2004b) to derive
a version of Penn-II where each node in each tree
is annotated with an LFG functional annotation (i.e.
an attribute value structure equation). Trees are tra-
versed top-down, and annotation is driven by cate-
gorial, basic configurational, trace and Penn-II func-
tional tag information in local subtrees of mostly
depth one (i.e. CFG rules). The annotation proce-
dure is dependent on locating the head daughter, for
which the scheme of (Magerman, 1994) with some
changes and amendments is used. The head is anno-
tated with the LFG equation ?=?. Linguistic gen-
eralisations are provided over the left (the prefix)
and the right (suffix) context of the head for each
syntactic category occurring as the mother node of
such heads. To give a simple example, the rightmost
NP to the left of a VP head under an S is likely to
be its subject (? SUBJ =?), while the leftmost NP
to the right of the V head of a VP is most proba-
bly its object (? OBJ =?). (Cahill et al, 2004b)
provide four sets of annotation principles, one for
non-coordinate configurations, one for coordinate
configurations, one for traces (long distance depen-
dencies) and a final ?catch all and clean up? phase.
Distinguishing between argument and adjunct is an
inherent step in the automatic assignment of func-
tional annotations.
The satisfactory treatment of long distance de-
pendencies by the annotation algorithm is impera-
tive for the extraction of accurate semantic forms.
The Penn Treebank employs a rich arsenal of traces
and empty productions (nodes which do not re-
alise any lexical material) to co-index displaced ma-
terial with the position where it should be inter-
preted semantically. The algorithm of (Cahill et
al., 2004b) translates the traces into corresponding
re-entrancies in the f-structure representation (Fig-
ure 1). Passive movement is also captured and ex-
pressed at f-structure level using a passive:+ an-
notation. Once a treebank tree is annotated with
feature structure equations by the annotation algo-
rithm, the equations are collected and passed to a
constraint solver which produces the f-structures.
In order to ensure the quality of the seman-
S
S-TPC- 1
NP
U.N.
VP
V
signs
NP
treaty
NP
Det
the
N
headline
VP
V
said
S
T- 1
?
?
?
?
?
?
?
TOPIC
[
SUBJ
[
PRED U.N.
]
PRED sign
OBJ
[
PRED treaty
]
]
1
SUBJ
[
SPEC the
PRED headline
]
PRED say
COMP 1
?
?
?
?
?
?
?
Figure 1: Penn-II style tree with long distance depen-
dency trace and corresponding reentrancy in f-structure
tic forms extracted by our method, we must first
ensure the quality of the f-structure annotations.
(Cahill et al, 2004b) measure annotation quality
in terms of precision and recall against manually
constructed, gold-standard f-structures for 105 ran-
domly selected trees from section 23 of the WSJ
section of Penn-II. The algorithm currently achieves
an F-score of 96.3% for complete f-structures and
93.6% for preds-only f-structures.1
Our semantic form extraction methodology is
based on the procedure of (van Genabith et al,
1999): For each f-structure generated, for each
level of embedding we determine the local PRED
value and collect the subcategorisable grammat-
ical functions present at that level of embed-
ding. Consider the f-structure in Figure 1. From
this we recursively extract the following non-
empty semantic forms: say([subj,comp]),
sign([subj,obj]). In effect, in both (van
Genabith et al, 1999) and our approach seman-
tic forms are reverse engineered from automatically
generated f-structures for treebank trees. We ex-
tract the following subcategorisable syntactic func-
tions: SUBJ, OBJ, OBJ2, OBLprep, OBL2prep, COMP,
XCOMP and PART. Adjuncts (e.g. ADJ, APP etc)
are not included in the semantic forms. PART
is not a syntactic function in the strict sense but
we capture the relevant co-occurrence patterns of
verbs and particles in the semantic forms. Just
as OBL includes the prepositional head of the PP,
PART includes the actual particle which occurs e.g.
add([subj,obj,part:up]).
In the work presented here we substantially ex-
tend the approach of (van Genabith et al, 1999) as
1Preds-only measures only paths ending in PRED:VALUE so
features such as number, person etc are not included.
regards coverage, granularity and evaluation: First,
we scale the approach of (van Genabith et al, 1999)
which was proof of concept on 100 trees to the full
WSJ section of the Penn-II Treebank. Second, our
approach fully reflects long distance dependencies,
indicated in terms of traces in the Penn-II Tree-
bank and corresponding re-entrancies at f-structure.
Third, in addition to abstract syntactic function-
based subcategorisation frames we compute frames
for syntactic function-CFG category pairs, both for
the verbal heads and their arguments and also gen-
erate pure CFG-based subcat frames. Fourth, our
method differentiates between frames captured for
active or passive constructions. Fifth, our method
associates conditional probabilities with frames.
In contrast to much of the work reviewed in the
previous section, our system is able to produce sur-
face syntactic as well as abstract functional subcat-
egorisation details. To incorporate CFG details into
the extracted semantic forms, we add an extra fea-
ture to the generated f-structures, the value of which
is the syntactic category of the pred at each level
of embedding. Exploiting this information, the ex-
tracted semantic form for the verb sign looks as fol-
lows: sign(v,[subj(np),obj(np)]).
We have also extended the algorithm to deal with
passive voice and its effect on subcategorisation be-
haviour. Consider Figure 2: not taking voice into
account, the algorithm extracts an intransitive frame
outlaw([subj]) for the transitive outlaw. To
correct this, the extraction algorithm uses the fea-
ture value pair passive:+, which appears in the
f-structure at the level of embedding of the verb in
question, to mark that predicate as occurring in the
passive: outlaw([subj],p).
In order to estimate the likelihood of the cooc-
currence of a predicate with a particular argument
list, we compute conditional probabilities for sub-
categorisation frames based on the number of token
occurrences in the corpus. Given a lemma l and an
argument list s, the probability of s given l is esti-
mated as:
P(s|l) := count(l, s)?n
i=1 count(l, si)
We use thresholding to filter possible error judge-
ments by our system. Table 1 shows the attested
semantic forms for the verb accept with their as-
sociated conditional probabilities. Note that were
the distinction between active and passive not taken
into account, the intransitive occurrence of accept
would have been assigned an unmerited probability.
subj : spec : quant : pred : all
adjunct : 2 : pred : almost
adjunct : 3 : pred : remain
participle : pres
4 : obj : adjunct : 5 : pred : cancer-causing
pers : 3
pred : asbestos
num : sg
pform : of
pers : 3
pred : use
num : pl
passive : +
adjunct : 1 : obj : pred : 1997
pform : by
xcomp : subj : spec: quant : pred : all
adjunct : 2 : pred : almost
...
...
passive : +
xcomp : subj : spec: quant : pred : all
adjunct : 2 : pred : almost
...
...
passive : +
pred : outlaw
tense : past
pred : be
pred : will
modal : +
Figure 2: Automatically generated f-structure
for the string wsj 0003 23?By 1997, almost
all remaining uses of cancer-causing
asbestos will be outlawed.?
Semantic Form Frequency Probability
accept([subj,obj]) 122 0.813
- accept([subj],p) 9 0.060
accept([subj,comp]) 5 0.033
- accept([subj,obl:as],p) 3 0.020
accept([subj,obj,obl:as]) 3 0.020
accept([subj,obj,obl:from]) 3 0.020
- accept([subj]) 2 0.013
accept([subj,obj,obl:at]) 1 0.007
accept([subj,obj,obl:for]) 1 0.007
accept([subj,obj,xcomp]) 1 0.007
Table 1: Semantic Forms for the verb accept marked
with p for passive use.
4 Results
We extract non-empty semantic forms2 for 3586
verb lemmas and 10969 unique verbal semantic
form types (lemma followed by non-empty argu-
ment list). Including prepositions associated with
the OBLs and particles, this number rises to 14348,
an average of 4.0 per lemma (Table 2). The num-
ber of unique frame types (without lemma) is 38
without specific prepositions and particles, 577 with
(Table 3). F-structure annotations allow us to distin-
guish passive and active frames.
5 COMLEX Evaluation
We evaluated our induced (verbal) semantic forms
against COMLEX (MacLeod et al, 1994). COM-
2Frames with at least one subcategorised grammatical func-
tion.
Without Prep/Part With Prep/Part
Sem. Form Types 10969 14348
Active 8516 11367
Passive 2453 2981
Table 2: Number of Semantic Form Types
Without Prep/Part With Prep/Part
# Frame Types 38 577
# Singletons 1 243
# Twice Occurring 1 84
# Occurring max. 5 7 415
# Occurring > 5 31 162
Table 3: Number of Distinct Frames for Verbs (not in-
cluding syntactic category for grammatical function)
LEX defines 138 distinct verb frame types without
the inclusion of specific prepositions or particles.
The following is a sample entry for the verb
reimburse:
(VERB :ORTH ?reimburse? :SUBC ((NP-NP)
(NP-PP :PVAL (?for?))
(NP)))
Each verb has a :SUBC feature, specifying
its subcategorisation behaviour. For example,
reimburse can occur with two noun phrases
(NP-NP), a noun phrase and a prepositional phrase
headed by ?for? (NP-PP :PVAL (?for?)) or a single
noun phrase (NP). Note that the details of the subject
noun phrase are not included in COMLEX frames.
Each of the complement types which make up the
value of the :SUBC feature is associated with a for-
mal frame definition which looks as follows:
(vp-frame np-np :cs ((np 2)(np 3))
:gs (:subject 1 :obj 2 :obj2 3)
:ex ?she asked him his name?)
The value of the :cs feature is the constituent struc-
ture of the subcategorisation frame, which lists the
syntactic CF-PSG constituents in sequence. The
value of the :gs feature is the grammatical struc-
ture which indicates the functional role played by
each of the CF-PSG constituents. The elements of
the constituent structure are indexed, and referenced
in the :gs field. This mapping between constituent
structure and functional structure makes the infor-
mation contained in COMLEX suitable as an eval-
uation standard for the LFG semantic forms which
we induce.
5.1 COMLEX-LFG Mapping
We devised a common format for our induced se-
mantic forms and those contained in COMLEX.
This is summarised in Table 4. COMLEX does
not distinguish between obliques and objects so we
converted Obji to OBLi as required. In addition,
COMLEX does not explicitly differentiate between
COMPs and XCOMPs, but does encode control in-
formation for any Comps which occur, thus allow-
ing us to deduce the distinction automatically. The
manually constructed COMLEX entries provided us
with a gold standard against which we evaluated the
automatically induced frames for the 2992 (active)
verbs that both resources have in common.
LFG COMLEX Merged
SUBJ Subject SUBJ
OBJ Object OBJ
OBJ2 Obj2 OBJ2
OBL Obj3 OBL
OBL2 Obj4 OBL2
COMP Comp COMP
XCOMP Comp XCOMP
PART Part PART
Table 4: COMLEX and LFG Syntactic Functions
We use the computed conditional probabilities to set
a threshold to filter the selection of semantic forms.
As some verbs occur less frequently than others we
felt it was important to use a relative rather than ab-
solute threshold. For a threshold of 1%, we disre-
gard any frames with a conditional probability of
less than or equal to 0.01. We carried out the evalu-
ation in a similar way to (Schulte im Walde, 2002).
The scale of our evaluation is comparable to hers.
This allows us to make tentative comparisons be-
tween our respective results. The figures shown in
Table 5 are the results of three different kinds of
evaluation with the threshold set to 1% and 5%. The
effect of the threshold increase is obvious in that
Precision goes up for each of the experiments while
Recall goes down.
For Exp 1, we excluded prepositional phrases en-
tirely from the comparison, i.e. assumed that PPs
were adjunct material (e.g. [subj,obl:for] becomes
[subj]). Our results are better for Precision than for
Recall compared to Schulte im Walde (op cit.), who
reports Precision of 74.53%, Recall of 69.74% and
an F-score of 72.05%.
Exp 2 includes prepositional phrases but not
parameterised for particular prepositions (e.g.
[subj,obl:for] becomes [subj,obl]). While our fig-
ures for Recall are again lower, our results for
Precision are considerably higher than those of
Schulte im Walde (op cit.) who recorded Preci-
sion of 60.76%, Recall of 63.91% and an F-score
of 62.30%.
For Exp. 3, we used semantic forms which con-
tained details of specific prepositions for any sub-
categorised prepositional phrase. Our Precision fig-
ures are again high (in comparison to 65.52% as
recorded by (Schulte im Walde, 2002)). However,
Threshold 1% Threshold 5%
P R F-Score P R F-Score
Exp. 1 79.0% 59.6% 68.0% 83.5% 54.7% 66.1%
Exp. 2 77.1% 50.4% 61.0% 81.4% 44.8% 57.8%
Exp. 2a 76.4% 44.5% 56.3% 80.9% 39.0% 52.6%
Exp. 3 73.7% 22.1% 34.0% 78.0% 18.3% 29.6%
Exp. 3a 73.3% 19.9% 31.3% 77.6% 16.2% 26.8%
Table 5: COMLEX Comparison
our Recall is very low (compared to the 50.83% that
Schulte im Walde (op cit.) reports). Consequently
our F-score is also low (Schulte im Walde (op cit.)
records an F-score of 57.24%). Experiments 2a and
3a are similar to Experiments 2 and 3 respectively
except they include the specific particle associated
with each PART.
5.1.1 Directional Prepositions
There are a number of possible reasons for our
low recall scores for Experiment 3 in Table 5. It
is a well-documented fact (Briscoe and Carroll,
1997) that subcategorisation frames (and their fre-
quencies) vary across domains. We have extracted
frames from one domain (the WSJ) whereas COM-
LEX was built using examples from the San Jose
Mercury News, the Brown Corpus, several literary
works from the Library of America, scientific ab-
stracts from the U.S. Department of Energy, and
the WSJ. For this reason, it is likely to contain
a greater variety of subcategorisation frames than
our induced lexicon. It is also possible that due
to human error COMLEX contains subcategorisa-
tion frames, the validity of which may be in doubt.
This is due to the fact that the aim of the COMLEX
project was to construct as complete a set of subcat-
egorisation frames as possible, even for infrequent
verbs. Lexicographers were allowed to extrapo-
late from the citations found, a procedure which
is bound to be less certain than the assignment of
frames based entirely on existing examples. Our re-
call figure was particularly low in the case of eval-
uation using details of prepositions (Experiment 3).
This can be accounted for by the fact that COMLEX
errs on the side of overgeneration when it comes to
preposition assignment. This is particularly true of
directional prepositions, a list of 31 of which has
been prepared and is assigned in its entirety by de-
fault to any verb which can potentially appear with
any directional preposition. In a subsequent exper-
iment, we incorporate this list of directional prepo-
sitions by default into our semantic form induction
process in the same way as the creators of COM-
LEX have done. Table 6 shows the results of this
experiment. As expected there is a significant im-
Precision Recall F-Score
Experiment 3 81.7% 40.8% 54.4%
Experiment 3a 83.1% 35.4% 49.7%
Table 6: COMLEX Comparison using p-dir(Threshold
of 1%)
Passive Precision Recall F-Score
Experiment 2 80.2% 54.7% 65.1%
Experiment 2a 79.7% 46.2% 58.5%
Experiment 3 72.6% 33.4% 45.8%
Experiment 3a 72.3% 29.3% 41.7%
Table 7: Passive evaluation (Threshold of 1%)
provement in the recall figure, being almost double
the figures reported in Table 5 for Experiments 3
and 3a.
5.1.2 Passive Evaluation
Table 7 presents the results of our evaluation of
the passive semantic forms we extract. It was
carried out for 1422 verbs which occur with pas-
sive frames and are shared by the induced lexicon
and COMLEX. As COMLEX does not provide ex-
plicit passive entries, we applied Lexical Redun-
dancy Rules (Kaplan and Bresnan, 1982) to auto-
matically convert the active COMLEX frames to
their passive counterparts. For example, the COM-
LEX entry see([subj,obj]) is converted to
see([subj]). The resulting precision is very
high, a slight increase on that for the active frames.
The recall score drops for passive frames (from
54.7% to 29.3%) in a similar way to that for active
frames when prepositional details are included.
5.2 Lexical Accession Rates
As well as evaluating the quality of our extracted
semantic forms, we also examine the rate at which
they are induced. (Charniak, 1996) and (Krotov et
al., 1998) observed that treebank grammars (CFGs
extracted from treebanks) are very large and grow
with the size of the treebank. We were interested in
discovering whether the acquisition of lexical mate-
rial on the same data displays a similar propensity.
Figure 3 displays the accession rates for the seman-
tic forms induced by our method for sections 0?24
of the WSJ section of the Penn-II treebank. When
we do not distinguish semantic forms by category,
all semantic forms together with those for verbs dis-
play smaller accession rates than for the PCFG.
We also examined the coverage of our system in
a similar way to (Hockenmaier et al, 2002). We ex-
tracted a verb-only reference lexicon from Sections
02-21 of the WSJ and subsequently compared this
to a test lexicon constructed in the same way from
 0
 5000
 10000
 15000
 20000
 25000
 0  5  10  15  20  25
N
o.
 o
f S
Fs
/R
ul
es
WSJ Section
All SF Frames
All Verbs
All SF Frames, no category
All Verbs, no category
PCFG
Figure 3: Accession Rates for Semantic Forms and CFG
Rules
Entries also in reference lexicon: 89.89%
Entries not in reference lexicon: 10.11%
Known words: 7.85%
- Known words, known frames: 7.85%
- Known words, unknown frames: -
Unknown words: 2.32%
- Unknown words, known frames: 2.32%
- Unknown words, unknown frames: -
Table 8: Coverage of induced lexicon on unseen
data (Verbs Only)
Section 23. Table 8 shows the results of this ex-
periment. 89.89% of the entries in the test lexicon
appeared in the reference lexicon.
6 Conclusions
We have presented an algorithm and its implementa-
tion for the extraction of semantic forms or subcate-
gorisation frames from the Penn-II Treebank, auto-
matically annotated with LFG f-structures. We have
substantially extended an earlier approach by (van
Genabith et al, 1999). The original approach was
small-scale and ?proof of concept?. We have scaled
our approach to the entire WSJ Sections of Penn-
II (50,000 trees). Our approach does not predefine
the subcategorisation frames we extract as many
other approaches do. We extract abstract syntac-
tic function-based subcategorisation frames (LFG
semantic forms), traditional CFG category-based
frames as well as mixed function-category based
frames. Unlike many other approaches to subcate-
gorisation frame extraction, our system properly re-
flects the effects of long distance dependencies and
distinguishes between active and passive frames.
Finally our system associates conditional probabil-
ities with the frames we extract. We carried out an
extensive evaluation of the complete induced lexi-
con (not just a sample) against the full COMLEX
resource. To our knowledge, this is the most exten-
sive qualitative evaluation of subcategorisation ex-
traction in English. The only evaluation of a similar
scale is that carried out by (Schulte im Walde, 2002)
for German. Our results compare well with hers.
We believe our semantic forms are fine-grained and
by choosing to evaluate against COMLEX we set
our sights high: COMLEX is considerably more
detailed than the OALD or LDOCE used for other
evaluations.
Currently work is under way to extend the cov-
erage of our acquired lexicons by applying our
methodology to the Penn-III treebank, a more bal-
anced corpus resource with a number of text gen-
res (in addition to the WSJ sections). It is impor-
tant to realise that the induction of lexical resources
is part of a larger project on the acquisition of
wide-coverage, robust, probabilistic, deep unifica-
tion grammar resources from treebanks. We are al-
ready using the extracted semantic forms in parsing
new text with robust, wide-coverage PCFG-based
LFG grammar approximations automatically ac-
quired from the f-structure annotated Penn-II tree-
bank (Cahill et al, 2004a). We hope to be able to
apply our lexical acquisition methodology beyond
existing parse-annotated corpora (Penn-II and Penn-
III): new text is parsed by our PCFG-based LFG ap-
proximations into f-structures from which we can
then extract further semantic forms. The work re-
ported here is part of the core component for boot-
strapping this approach.
As the extraction algorithm we presented derives
semantic forms at f-structure level, it is easily ap-
plied to other, even typologically different, lan-
guages. We have successfully ported our automatic
annotation algorithm to the TIGER Treebank, de-
spite German being a less configurational language
than English, and extracted wide-coverage, proba-
bilistic LFG grammar approximations and lexical
resources for German (Cahill et al, 2003). Cur-
rently, we are migrating the technique to Spanish,
which has freer word order than English and less
morphological marking than German. Preliminary
results have been very encouraging.
7 Acknowledgements
The research reported here is supported by Enter-
prise Ireland Basic Research Grant SC/2001/186
and an IRCSET PhD fellowship award.
References
M. Brent. 1993. From Grammar to Lexicon: Unsu-
pervised Learning of Lexical Syntax. Computa-
tional Linguistics, 19(2):203?222.
E. Briscoe and J. Carroll. 1997. Automatic Extrac-
tion of Subcategorization from Corpora. In Pro-
ceedings of the 5th ACL Conference on Applied
Natural Language Processing, pages 356?363,
Washington, DC.
A. Cahill, M. Forst, M. McCarthy, R. O?Donovan,
C. Rohrer, J. van Genabith, and A. Way.
2003. Treebank-Based Multilingual Unification-
Grammar Development. In Proceedings of the
Workshop on Ideas and Strategies for Multilin-
gual Grammar Development at the 15th ESSLLI,
pages 17?24, Vienna, Austria.
A. Cahill, M. Burke, R. O?Donovan, J. van Gen-
abith, and A. Way. 2004a. Long-Distance De-
pendency Resolution in Automatically Acquired
Wide-Coverage PCFG-Based LFG Approxima-
tions. In Proceedings of the 42nd Annual Con-
ference of the Association for Computational Lin-
guistics (ACL-04), Barcelona, Spain.
A. Cahill, M. McCarthy, M. Burke, R. O?Donovan,
J. van Genabith, and A. Way. 2004b. Evaluating
Automatic F-Structure Annotation for the Penn-
II Treebank. Journal of Research on Language
and Computation.
G. Carroll and M. Rooth. 1998. Valence Induc-
tion with a Head-Lexicalised PCFG. In Proceed-
ings of the 3rd Conference on Empirical Meth-
ods in Natural Language Processing, pages 36?
45, Granada, Spain.
E. Charniak. 1996. Tree-bank Grammars. In AAAI-
96: Proceedings of the Thirteenth National Con-
ference on Artificial Intelligence, MIT Press,
pages 1031?1036, Cambridge, MA.
J. Chen and K. Vijay-Shanker. 2000. Automated
Extraction of TAGs from the Penn Treebank. In
Proceedings of the 38th Annual Meeting of the
Association of Computational Linguistics, pages
65?76, Hong Kong.
M. Collins. 1997. Three generative lexicalised
models for statistical parsing. In Proceedings of
the 35th Annual Meeting of the Association for
Computational Linguistics, pages 16?23.
J. Hockenmaier, G. Bierner, and J. Baldridge. 2002.
Extending the Coverage of a CCG System. Jour-
nal of Language and Computation, (2).
R. Kaplan and J. Bresnan. 1982. Lexical Func-
tional Grammar: A Formal System for Gram-
matical Representation. In Joan Bresnan, editor,
The Mental Representation of Grammatical Re-
lations, pages 206?250. MIT Press, Cambridge,
MA, Mannheim, 8th Edition.
A. Kinyon and C. Prolo. 2002. Identifying Verb Ar-
guments and their Syntactic Function in the Penn
Treebank. In Proceedings of the 3rd LREC Con-
ference, pages 1982?1987, Las Palmas, Spain.
A. Korhonen. 2002. Subcategorization Acquisition.
PhD thesis published as Techical Report UCAM-
CL-TR-530, Computer Laboratory, University of
Cambridge, UK.
A. Krotov, M. Hepple, R. Gaizauskas, and Y. Wilks.
1998. Compacting the Penn Treebank Grammar.
In Proceedings of COLING-ACL?98, pages 669?
703, Montreal, Canada.
C. MacLeod, R. Grishman, and A. Meyers. 1994.
The Comlex Syntax Project: The First Year. In
Proceedings of the ARPA Workshop on Human
Language Technology, pages 669?703, Prince-
ton, NJ.
D. Magerman. 1994. Natural Language Parsing
as Statistical Pattern Recognition. PhD Thesis,
Stanford University, CA.
C. Manning. 1993. Automatic Acquisition of a
Large Subcategorisation Dictionary from Cor-
pora. In Proceedings of the 31st Annual Meeting
of the Association for Computational Linguistics,
pages 235?242, Columbus, OH.
S. Schulte im Walde. 2002. Evaluating Verb Sub-
categorisation Frames learned by a German Sta-
tistical Grammar against Manual Definitions in
the Duden Dictionary. In Proceedings of the 10th
EURALEX International Congress, pages 187?
197, Copenhagen, Denmark.
A. Ushioda, D. Evans, T. Gibson, and A. Waibel.
1993. The Automatic Acquisition of Frequencies
of Verb Subcategorization Frames from Tagged
Corpora. In SIGLEX ACL Workshop on the Ac-
quisition of Lexical Knowledge from Text, pages
95?106, Columbus, OH.
J. van Genabith, A. Way, and L. Sadler. 1999. Data-
driven Compilation of LFG Semantic Forms. In
EACL-99 Workshop on Linguistically Interpreted
Corpora, pages 69?76, Bergen, Norway.
F. Xia, M. Palmer, and A. Joshi. 2000. A Uniform
Method of Grammar Extraction and its Applica-
tions. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP-2000), pages 53?62, Hong Kong.
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 497?504,
Sydney, July 2006. c?2006 Association for Computational Linguistics
QuestionBank: Creating a Corpus of Parse-Annotated Questions
John Judge1, Aoife Cahill1, and Josef van Genabith1,2
1National Centre for Language Technology and School of Computing,
Dublin City University, Dublin, Ireland
2IBM Dublin Center for Advanced Studies,
IBM Dublin, Ireland
{jjudge,acahill,josef}@computing.dcu.ie
Abstract
This paper describes the development of
QuestionBank, a corpus of 4000 parse-
annotated questions for (i) use in training
parsers employed in QA, and (ii) evalua-
tion of question parsing. We present a se-
ries of experiments to investigate the ef-
fectiveness of QuestionBank as both an
exclusive and supplementary training re-
source for a state-of-the-art parser in pars-
ing both question and non-question test
sets. We introduce a new method for
recovering empty nodes and their an-
tecedents (capturing long distance depen-
dencies) from parser output in CFG trees
using LFG f-structure reentrancies. Our
main findings are (i) using QuestionBank
training data improves parser performance
to 89.75% labelled bracketing f-score, an
increase of almost 11% over the base-
line; (ii) back-testing experiments on non-
question data (Penn-II WSJ Section 23)
shows that the retrained parser does not
suffer a performance drop on non-question
material; (iii) ablation experiments show
that the size of training material provided
by QuestionBank is sufficient to achieve
optimal results; (iv) our method for recov-
ering empty nodes captures long distance
dependencies in questions from the ATIS
corpus with high precision (96.82%) and
low recall (39.38%). In summary, Ques-
tionBank provides a useful new resource
in parser-based QA research.
1 Introduction
Parse-annotated corpora (treebanks) are crucial for
developing machine learning and statistics-based
parsing resources for a given language or task.
Large treebanks are available for major languages,
however these are often based on a specific text
type or genre, e.g. financial newspaper text (the
Penn-II Treebank (Marcus et al, 1993)). This can
limit the applicability of grammatical resources in-
duced from treebanks in that such resources un-
derperform when used on a different type of text
or for a specific task.
In this paper we present work on creating Ques-
tionBank, a treebank of parse-annotated questions,
which can be used as a supplementary training re-
source to allow parsers to accurately parse ques-
tions (as well as other text). Alternatively, the re-
source can be used as a stand-alone training corpus
to train a parser specifically for questions. Either
scenario will be useful in training parsers for use
in question answering (QA) tasks, and it also pro-
vides a suitable resource to evaluate the accuracy
of these parsers on questions.
We use a semi-automatic ?bootstrapping?
method to create the question treebank from raw
text. We show that a parser trained on the ques-
tion treebank alone can accurately parse ques-
tions. Training on a combined corpus consisting of
the question treebank and an established training
set (Sections 02-21 of the Penn-II Treebank), the
parser gives state-of-the-art performance on both
questions and a non-question test set (Section 23
of the Penn-II Treebank).
Section 2 describes background work and mo-
tivation for the research presented in this paper.
Section 3 describes the data we used to create
the corpus. In Section 4 we describe the semi-
automatic method to ?bootstrap? the question cor-
pus, discuss some interesting and problematic
phenomena, and show how the manual vs. auto-
matic workload distribution changed as work pro-
gressed. Two sets of experiments using our new
question corpus are presented in Section 5. In
Section 6 we introduce a new method for recover-
ing empty nodes and their antecedents using Lex-
ical Functional Grammar (LFG) f-structure reen-
497
trancies. Section 7 concludes and outlines future
work.
2 Background and Motivation
High quality probabilistic, treebank-based parsing
resources can be rapidly induced from appropri-
ate treebank material. However, treebank- and
machine learning-based grammatical resources re-
flect the characteristics of the training data. They
generally underperform on test data substantially
different from the training data.
Previous work on parser performance and do-
main variation by Gildea (2001) showed that by
training a parser on the Penn-II Treebank and test-
ing on the Brown corpus, parser accuracy drops by
5.7% compared to parsing the Wall Street Journal
(WSJ) based Penn-II Treebank Section 23. This
shows a negative effect on parser performance
even when the test data is not radically different
from the training data (both the Penn II and Brown
corpora consist primarily of written texts of Amer-
ican English, the main difference is the consider-
ably more varied nature of the text in the Brown
corpus). Gildea also shows how to resolve this
problem by adding appropriate data to the training
corpus, but notes that a large amount of additional
data has little impact if it is not matched to the test
material.
Work on more radical domain variance and on
adapting treebank-induced LFG resources to anal-
yse ATIS (Hemphill et al, 1990) question mate-
rial is described in Judge et al (2005). The re-
search established that even a small amount of ad-
ditional training data can give a substantial im-
provement in question analysis in terms of both
CFG parse accuracy and LFG grammatical func-
tional analysis, with no significant negative effects
on non-question analysis. Judge et al (2005) sug-
gest, however, that further improvements are pos-
sible given a larger question training corpus.
Clark et al (2004) worked specifically with
question parsing to generate dependencies for QA
with Penn-II treebank-based Combinatory Cate-
gorial Grammars (CCG?s). They use ?what? ques-
tions taken from the TREC QA datasets as the ba-
sis for a What-Question corpus with CCG annota-
tion.
3 Data Sources
The raw question data for QuestionBank comes
from two sources, the TREC 8-11 QA track
test sets1, and a question classifier training set
produced by the Cognitive Computation Group
(CCG2) at the University of Illinois at Urbana-
Champaign.3 We use equal amounts of data from
each source so as not to bias the corpus to either
data source.
3.1 TREC Questions
The TREC evaluations have become the standard
evaluation for QA systems. Their test sets con-
sist primarily of fact seeking questions with some
imperative statements which request information,
e.g. ?List the names of cell phone manufactur-
ers.? We included 2000 TREC questions in the
raw data from which we created the question tree-
bank. These 2000 questions consist of the test
questions for the first three years of the TREC QA
track (1893 questions) and 107 questions from the
2003 TREC test set.
3.2 CCG Group Questions
The CCG provide a number of resources for de-
veloping QA systems. One of these resources is
a set of 5500 questions and their answer types for
use in training question classifiers. The 5500 ques-
tions were stripped of answer type annotation, du-
plicated TREC questions were removed and 2000
questions were used for the question treebank.
The CCG 5500 questions come from a number
of sources (Li and Roth, 2002) and some of these
questions contain minor grammatical mistakes so
that, in essence, this corpus is more representa-
tive of genuine questions that would be put to a
working QA system. A number of changes in to-
kenisation were corrected (eg. separating contrac-
tions), but the minor grammatical errors were left
unchanged because we believe that it is necessary
for a parser for question analysis to be able to cope
with this sort of data if it is to be used in a working
QA system.
4 Creating the Treebank
4.1 Bootstrapping a Question Treebank
The algorithm used to generate the question tree-
bank is an iterative process of parsing, manual cor-
rection, retraining, and parsing.
1http://trec.nist.gov/data/qa.html
2Note that the acronym CCG here refers to Cognitive
Computation Group, rather than Combinatory Categorial
Grammar mentioned in Section 2.
3http://l2r.cs.uiuc.edu/ cogcomp/tools.php
498
Algorithm 1 Induce a parse-annotated treebank
from raw data
repeat
Parse a new section of raw data
Manually correct errors in the parser output
Add the corrected data to the training set
Extract a new grammar for the parser
until All the data has been processed
Algorithm 1 summarises the bootstrapping al-
gorithm. A section of raw data is parsed. The
parser output is then manually corrected, and
added to the parser?s training corpus. A new gram-
mar is then extracted, and the next section of raw
data is parsed. This process continues until all the
data has been parsed and hand corrected.
4.2 Parser
The parser used to process the raw questions prior
to manual correction was that of Bikel (2002)4 ,
a retrainable emulation of Collins (1999) model
2 parser. Bikel?s parser is a history-based parser
which uses a lexicalised generative model to parse
sentences. We used WSJ Sections 02-21 of the
Penn-II Treebank to train the parser for the first it-
eration of the algorithm. The training corpus for
subsequent iterations consisted of the WSJ ma-
terial and increasing amounts of processed ques-
tions.
4.3 Basic Corpus Development Statistics
Our question treebank was created over a period
of three months at an average annotation speed of
about 60 questions per day. This is quite rapid
for treebank development. The speed of the pro-
cess was helped by two main factors: the questions
are generally quite short (typically about 10 words
long), and, due to retraining on the continually in-
creasing training set, the quality of the parses out-
put by the parser improved dramatically during the
development of the treebank, with the effect that
corrections during the later stages were generally
quite small and not as time consuming as during
the initial phases of the bootstrapping process.
For example, in the first week of the project the
trees from the parser were of relatively poor qual-
ity and over 78% of the trees needed to be cor-
rected manually. This slowed the annotation pro-
cess considerably and parse-annotated questions
4Downloaded from http://www.cis.upenn.edu/?dbikel
/software.html#stat-parser
were being produced at an average rate of 40 trees
per day. During the later stages of the project this
had changed dramatically. The quality of trees
from the parser was much improved with less than
20% of the trees requiring manual correction. At
this stage parse-annotated questions were being
produced at an average rate of 90 trees per day.
4.4 Corpus Development Error Analysis
Some of the more frequent errors in the parser
output pertain to the syntactic analysis of WH-
phrases (WHNP, WHPP, etc). In Sections 02-21
of the Penn-II Treebank, these are used more often
in relative clause constructions than in questions.
As a result many of the corpus questions were
given syntactic analyses corresponding to relative
clauses (SBAR with an embedded S) instead of as
questions (SBARQ with an embedded SQ). Figure
1 provides an example.
SBAR
WHNP
WP
Who
S
VP
VBD
created
NP
DT
the
NN
Muppets
(a)
SBARQ
WHNP
WP
Who
SQ
VP
VBD
created
NP
DT
the
NNPS
Muppets
(b)
Figure 1: Example tree before (a) and after correc-
tion (b)
Because the questions are typically short, an er-
ror like this has quite a large effect on the accu-
racy for the overall tree; in this case the f-score
for the parser output (Figure 1(a)) would be only
60%. Errors of this nature were quite frequent
in the first section of questions analysed by the
parser, but with increased training material becom-
ing available during successive iterations, this er-
ror became less frequent and towards the end of
499
the project it was only seen in rare cases.
WH-XP marking was the source of a number of
consistent (though infrequent) errors during anno-
tation. This occurred mostly in PP constructions
containing WHNPs. The parser would output a
structure like Figure 2(a), where the PP mother of
the WHNP is not correctly labelled as a WHPP as
in Figure 2(b).
PP
IN
by
WHNP
WP$
whose
NN
authority
WHPP
IN
by
WHNP
WP$
whose
NN
authority
(a) (b)
Figure 2: WH-XP assignment
The parser output often had to be rearranged
structurally to varying degrees. This was common
in the longer questions. A recurring error in the
parser output was failing to identify VPs in SQs
with a single object NP. In these cases the verb
and the object NP were left as daughters of the
SQ node. Figure 3(a) illustrates this, and Figure
3(b) shows the corrected tree with the VP node in-
serted.
SBARQ
WHNP
WP
Who
SQ
VBD
killed
NP
Ghandi
SBARQ
WHNP
WP
Who
SQ
VP
VBD
killed
NP
Ghandi
(a) (b)
Figure 3: VP missing inside SQ with a single NP
On inspection, we found that the problem was
caused by copular constructions, which, accord-
ing to the Penn-II annotation guidelines, do not
feature VP constituents. Since almost half of the
question data contain copular constructions, the
parser trained on this data would sometimes mis-
analyse non-copular constructions or, conversely,
incorrectly bracket copular constructions using a
VP constituent (Figure 4(a)).
The predictable nature of these errors meant that
they were simple to correct. This is due to the par-
ticular context in which they occur and the finite
number of forms of the copular verb.
SBARQ
WHNP
WP
What
SQ
VP
VBZ
is
NP
a fear of shadows
SBARQ
WHNP
WP
What
SQ
VBZ
is
NP
a fear of shadows
(a) (b)
Figure 4: Erroneous VP in copular constructions
5 Experiments with QuestionBank
In order to test the effect training on the question
corpus has on parser performance, we carried out
a number of experiments. In cross-validation ex-
periments with 90%/10% splits we use all 4000
trees in the completed QuestionBank as the test
set. We performed ablation experiments to inves-
tigate the effect of varying the amount of question
and non-question training data on the parser?s per-
formance. For these experiments we divided the
4000 questions into two sets. We randomly se-
lected 400 trees to be held out as a gold standard
test set against which to evaluate, the remaining
3600 trees were then used as a training corpus.
5.1 Establishing the Baseline
The baseline we use for our experiments is pro-
vided by Bikel?s parser trained on WSJ Sections
02-21 of the Penn-II Treebank. We test on all 4000
questions in our question treebank, and also Sec-
tion 23 of the Penn-II Treebank.
QuestionBank
Coverage 100
F-Score 78.77
WSJ Section 23
Coverage 100
F-Score 82.97
Table 1: Baseline parsing results
Table 1 shows the results for our baseline eval-
uations on question and non-question test sets.
While the coverage for both tests is high, the
parser underperforms significantly on the question
test set with a labelled bracketing f-score of 78.77
compared to 82.97 on Section 23 of the Penn-II
Treebank. Note that unlike the published results
for Bikel?s parser in our evaluations we test on
Section 23 and include punctuation.
5.2 Cross-Validation Experiments
We carried out two cross-validation experiments.
In the first experiment we perform a 10-fold cross-
validation experiment using our 4000 question
500
treebank. In each case a randomly selected set of
10% of the questions in QuestionBank was held
out during training and used as a test set. In this
way parses from unseen data were generated for
all 4000 questions and evaluated against the Ques-
tionBank trees.
The second cross-validation experiment was
similar to the first, but in each of the 10 folds we
train on 90% of the 4000 questions in Question-
Bank and on all of Sections 02-21 of the Penn-II
Treebank.
In both experiments we also backtest each of the
ten grammars on Section 23 of the Penn-II Tree-
bank and report the average scores.
QuestionBank
Coverage 100
F-Score 88.82
Backtest on Sect 23
Coverage 98.79
F-Score 59.79
Table 2: Cross-validation experiment using the
4000 question treebank
Table 2 shows the results for the first cross-
validation experiment, using only the 4000 sen-
tence QuestionBank. Compared to Table 1, the re-
sults show a significant improvement of over 10%
on the baseline f-score for questions. However, the
tests on the non-question Section 23 data show not
only a significant drop in accuracy but also a drop
in coverage.
Questions
Coverage 100
F-Score 89.75
Backtest on Sect 23
Coverage 100
F-Score 82.39
Table 3: Cross-validation experiment using Penn-
II Treebank Sections 02-21 and 4000 questions
Table 3 shows the results for the second cross-
validation experiment using Sections 02-21 of the
Penn-II Treebank and the 4000 questions in Ques-
tionBank. The results show an even greater in-
crease on the baseline f-score than the experiments
using only the question training set (Table 2). The
non-question results are also better and are com-
parable to the baseline (Table 1).
5.3 Ablation Runs
In a further set of experiments we investigated the
effect of varying the amount of data in the parser?s
training corpus. We experiment with varying both
the amount of QuestionBank and Penn-II Tree-
bank data that the parser is trained on. In each
experiment we use the 400 question test set and
Section 23 of the Penn-II Treebank to evaluate
against, and the 3600 question training set de-
scribed above and Sections 02-21 of the Penn-II
Treebank as the basis for the parser?s training cor-
pus. We report on three experiments:
In the first experiment we train the parser using
only the 3600 question training set. We performed
ten training and parsing runs in this experiment,
incrementally reducing the size of the Question-
Bank training corpus by 10% of the whole on each
run.
The second experiment is similar to the first but
in each run we add Sections 02-21 of the Penn-II
Treebank to the (shrinking) training set of ques-
tions.
The third experiment is the converse of the sec-
ond, the amount of questions in the training set
remains fixed (all 3600) and the amount of Penn-
II Treebank material is incrementally reduced by
10% on each run.
 50
 60
 70
 80
 90
 100
 10 20 30 40 50 60 70 80 90 100
Co
ve
ra
ge
/F
-S
co
re
Percentage of 3600 questions in the training corpus
FScore Questions
FScore Section 23
Coverage Questions
Coverage Section 23
Figure 5: Results for ablation experiment reducing
3600 training questions in steps of 10%
Figure 5 graphs the coverage and f-score for
the parser in tests on the 400 question test set,
and Section 23 of the Penn-II Treebank in ten
parsing runs with the amount of data in the 3600
question training corpus reducing incrementally
on each run. The results show that training on only
a small amount of questions, the parser can parse
questions with high accuracy. For example when
trained on only 10% of the 3600 questions used
in this experiment, the parser successfully parses
all of the 400 question test set and achieves an f-
score of 85.59. However the results for the tests
on WSJ Section 23 are considerably worse. The
parser never manages to parse the full test set, and
the best score at 59.61 is very low.
Figure 6 graphs the results for the second abla-
501
 50
 60
 70
 80
 90
 100
 10 20 30 40 50 60 70 80 90 100
Co
ve
ra
ge
/F
-S
co
re
Percentage of 3600 questions in the training corpus
FScore Questions
FScore Section 23
Coverage Questions
Coverage Section 23
Figure 6: Results for ablation experiment using
PTB Sections 02-21 (fixed) and reducing 3600
questions in steps of 10%
 50
 60
 70
 80
 90
 100
 10 20 30 40 50 60 70 80 90 100
Co
ve
ra
ge
/F
-S
co
re
Percentage of PTB Stetcions 2-21 in the training corpus
FScore Questions
FScore Section 23
Coverage Questions
Coverage Section 23
Figure 7: Results for ablation experiment using
3600 questions (fixed) and reducing PTB Sections
02-21 in steps of 10%
tion experiment. The training set for the parser
consists of a fixed amount of Penn-II Treebank
data (Sections 02-21) and a reducing amount of
question data from the 3600 question training set.
Each grammar is tested on both the 400 question
test set, and WSJ Section 23. The results here
are significantly better than in the previous exper-
iment. In all of the runs the coverage for both test
sets is 100%, f-scores for the question test set de-
crease as the amount of question data in the train-
ing set is reduced (though they are still quite high.)
There is little change in the f-scores for the tests on
Section 23, the results all fall in the range 82.36 to
82.46, which is comparable to the baseline score.
Figure 7 graphs the results for the third abla-
tion experiment. In this case the training set is a
fixed amount of the question training set described
above (all 3600 questions) and a reducing amount
of data from Sections 02-21 of the Penn Treebank.
The graph shows that the parser performs consis-
tently well on the question test set in terms of both
coverage and accuracy. The tests on Section 23,
however, show that as the amount of Penn-II Tree-
bank material in the training set decreases, the f-
score also decreases.
6 Long Distance Dependencies
Long distance dependencies are crucial in the
proper analysis of question material. In English
wh-questions, the fronted wh-constituent refers to
an argument position of a verb inside the interrog-
ative construction. Compare the superficially sim-
ilar
1. Who1 [t1] killed Harvey Oswald?
2. Who1 did Harvey Oswald kill [t1]?
(1) queries the agent (syntactic subject) of the de-
scribed eventuality, while (2) queries the patient
(syntactic object). In the Penn-II and ATIS tree-
banks, dependencies such as these are represented
in terms of empty productions, traces and coindex-
ation in CFG tree representations (Figure 8).
SBARQ
WHNP-1
WP
Who
SQ
NP
*T*-1
VP
VBD
killed
NP
Harvey Oswald
(a)
SBARQ
WHNP-1
WP
Who
SQ
AUX
did
NP
Harvey Oswald
VP
VB
kill
NP
*T*-1
(b)
Figure 8: LDD resolved treebank style trees
With few exceptions5 the trees produced by cur-
rent treebank-based probabilistic parsers do not
represent long distance dependencies (Figure 9).
Johnson (2002) presents a tree-based method
for reconstructing LDD dependencies in Penn-
II trained parser output trees. Cahill et al
(2004) present a method for resolving LDDs
5Collins? Model 3 computes a limited number of wh-
dependencies in relative clause constructions.
502
SBARQ
WHNP
WP
Who
SQ
VP
VBD
killed
NP
Harvey Oswald
(a)
SBARQ
WHNP
WP
Who
SQ
AUX
did
NP
Harvey Oswald
VP
VB
kill
(b)
Figure 9: Parser output trees
at the level of Lexical-Functional Grammar f-
structure (attribute-value structure encodings of
basic predicate-argument structure or dependency
relations) without the need for empty productions
and coindexation in parse trees. Their method is
based on learning finite approximations of func-
tional uncertainty equations (regular expressions
over paths in f-structure) from an automatically f-
structure annotated version of the Penn-II treebank
and resolves LDDs at f-structure. In our work we
use the f-structure-based method of Cahill et al
(2004) to ?reverse engineer? empty productions,
traces and coindexation in parser output trees. We
explain the process by way of a worked example.
We use the parser output tree in Figure 9(a)
(without empty productions and coindexation) and
automatically annotate the tree with f-structure
information and compute LDD-resolution at the
level of f-structure using the resources of Cahill
et al (2004). This generates the f-structure an-
notated tree6 and the LDD resolved f-structure in
Figure 10.
Note that the LDD is indicated in terms of a
reentrancy 1 between the question FOCUS and the
SUBJ function in the resolved f-structure. Given
the correspondence between the f-structure and f-
structure annotated nodes in the parse tree, we
compute that the SUBJ function newly introduced
and reentrant with the FOCUS function is an argu-
ment of the PRED ?kill? and the verb form ?killed?
in the tree. In order to reconstruct the correspond-
ing empty subject NP node in the parser output
tree, we need to determine candidate anchor sites
6Lexical annotations are suppressed to aid readability.
SBARQ
WHNP
? FOCUS =?
WP
?=?
Who
SQ
?=?
VP
?=?
VBD
?=?
killed
NP
? OBJ =?
Harvey Oswald
(a)
?
?
?
FOCUS
[
PRED who
]
1
PRED ?kill?SUBJ OBJ??
OBJ
[
PRED ?Harvey Oswald?
]
SUBJ
[
PRED ?who?
]
1
?
?
?
(b)
Figure 10: Annotated tree and f-structure
for the empty node. These anchor sites can only be
realised along the path up to the maximal projec-
tion of the governing verb indicated by ?=? anno-
tations in LFG. This establishes three anchor sites:
VP, SQ and the top level SBARQ. From the auto-
matically f-structure annotated Penn-II treebank,
we extract f-structure annotated PCFG rules for
each of the three anchor sites whose RHSs contain
exactly the information (daughter categories plus
LFG annotations) in the tree in Figure 10 (in the
same order) plus an additional node (of whatever
CFG category) annotated ?SUBJ=?, located any-
where within the RHSs. This will retrieve rules of
the form
VP ? NP [? SUBJ =?] V BD[?=?] NP [? OBJ =?]
V P ? . . .
. . .
SQ ? NP [? SUBJ =?] V P [?=?]
SQ ? . . .
. . .
SBARQ ? . . .
. . .
each with their associated probabilities. We select
the rule with the highest probability and cut the
rule into the tree in Figure 10 at the appropriate
anchor site (as determined by the rule LHS). In our
case this selects SQ ? NP [? SUBJ=?]V P [?=?]
and the resulting tree is given in Figure 11. From
this tree, it is now easy to compute the tree with
the coindexed trace in Figure 8 (a).
In order to evaluate our empty node and coin-
dexation recovery method, we conducted two ex-
periments, one using 146 gold-standard ATIS
question trees and one using parser output on the
corresponding strings for the 146 ATIS question
trees.
503
SBARQ
WHNP-1
? FOCUS =?
WP
?=?
Who
SQ
?=?
NP
? SUBJ =?
-NONE-
*T*-1
VP
?=?
VBD
?=?
killed
NP
? OBJ =?
Harvey Oswald
Figure 11: Resolved tree
In the first experiment, we delete empty nodes
and coindexation from the ATIS gold standard
trees and and reconstruct them using our method
and the preprocessed ATIS trees. In the second
experiment, we parse the strings corresponding to
the ATIS trees with Bikel?s parser and reconstruct
the empty productions and coindexation. In both
cases we evaluate against the original (unreduced)
ATIS trees and score if and only if all of inser-
tion site, inserted CFG category and coindexation
match.
Parser Output Gold Standard Trees
Precision 96.77 96.82
Recall 38.75 39.38
Table 4: Scores for LDD recovery (empty nodes
and antecedents)
Table 4 shows that currently the recall of our
method is quite low at 39.38% while the accu-
racy is very high with precision at 96.82% on the
ATIS trees. Encouragingly, evaluating parser out-
put for the same sentences shows little change in
the scores with recall at 38.75% and precision at
96.77%.
7 Conclusions
The data represented in Figure 5 show that train-
ing a parser on 50% of QuestionBank achieves an
f-score of 88.56% as against 89.24% for training
on all of QuestionBank. This implies that while
we have not reached an absolute upper bound, the
question corpus is sufficiently large that the gain
in accuracy from adding more data is so small that
it does not justify the effort.
We will evaluate grammars learned from
QuestionBank as part of a working QA sys-
tem. A beta-release of the non-LDD-resolved
QuestionBank is available for download at
http://www.computing.dcu.ie/?
jjudge/qtreebank/4000qs.txt. The fi-
nal, hand-corrected, LDD-resolved version will be
available in October 2006.
Acknowledgments
We are grateful to the anonymous reviewers for
their comments and suggestions. This research
was supported by Science Foundation Ireland
(SFI) grant 04/BR/CS0370 and an Irish Research
Council for Science Engineering and Technology
(IRCSET) PhD scholarship 2002-05.
References
Daniel M. Bikel. 2002. Design of a multi-lingual, parallel-
processing statistical parsing engine. In Proceedings of
HLT 2002, pages 24?27, San Diego, CA.
Aoife Cahill, Michael Burke, Ruth O?Donovan, Josef van
Genabith, and Andy Way. 2004. Long-Distance De-
pendency Resolution in Automatically Acquired Wide-
Coverage PCFG-Based LFG Approximations. In Pro-
ceedings of ACL-04, pages 320?327, Barcelona, Spain.
Stephen Clark, Mark Steedman, and James R. Curran.
2004. Object-extraction and question-parsing using ccg.
In Dekang Lin and Dekai Wu, editors, Proceedings of
EMNLP-04, pages 111?118, Barcelona, Spain.
Michael Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University of
Pennsylvania, Philadelphia, PA.
Daniel Gildea. 2001. Corpus variation and parser perfor-
mance. In Lillian Lee and Donna Harman, editors, Pro-
ceedings of EMNLP, pages 167?202, Pittsburgh, PA.
Charles T. Hemphill, John J. Godfrey, and George R. Dod-
dington. 1990. The ATIS Spoken Language Systems pi-
lot corpus. In Proceedings of DARPA Speech and Natural
Language Workshop, pages 96?101, Hidden Valley, PA.
Mark Johnson. 2002. A simple pattern-matching algorithm
for recovering empty nodes and their antecedents. In Pro-
ceedings ACL-02, University of Pennsylvania, Philadel-
phia, PA.
John Judge, Aoife Cahill, Michael Burke, Ruth O?Donovan,
Josef van Genabith, and Andy Way. 2005. Strong Domain
Variation and Treebank-Induced LFG Resources. In Pro-
ceedings LFG-05, pages 186?204, Bergen, Norway, July.
Xin Li and Dan Roth. 2002. Learning question classifiers. In
Proceedings of COLING-02, pages 556?562, Taipei, Tai-
wan.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a Large Annotated Cor-
pus of English: The Penn Treebank. Computational Lin-
guistics, 19(2):313?330.
504
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 1033?1040,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Robust PCFG-Based Generation using Automatically Acquired LFG
Approximations
Aoife Cahill1 and Josef van Genabith1,2
1 National Centre for Language Technology (NCLT)
School of Computing, Dublin City University, Dublin 9, Ireland
2 Center for Advanced Studies, IBM Dublin, Ireland
{acahill,josef}@computing.dcu.ie
Abstract
We present a novel PCFG-based archi-
tecture for robust probabilistic generation
based on wide-coverage LFG approxima-
tions (Cahill et al, 2004) automatically
extracted from treebanks, maximising the
probability of a tree given an f-structure.
We evaluate our approach using string-
based evaluation. We currently achieve
coverage of 95.26%, a BLEU score of
0.7227 and string accuracy of 0.7476 on
the Penn-II WSJ Section 23 sentences of
length ?20.
1 Introduction
Wide coverage grammars automatically extracted
from treebanks are a corner-stone technology
in state-of-the-art probabilistic parsing. They
achieve robustness and coverage at a fraction of
the development cost of hand-crafted grammars. It
is surprising to note that to date, such grammars do
not usually figure in the complementary operation
to parsing ? natural language surface realisation.
Research on statistical natural language surface
realisation has taken three broad forms, differ-
ing in where statistical information is applied in
the generation process. Langkilde (2000), for ex-
ample, uses n-gram word statistics to rank alter-
native output strings from symbolic hand-crafted
generators to select paths in parse forest repre-
sentations. Bangalore and Rambow (2000) use
n-gram word sequence statistics in a TAG-based
generation model to rank output strings and ad-
ditional statistical and symbolic resources at in-
termediate generation stages. Ratnaparkhi (2000)
uses maximum entropy models to drive generation
with word bigram or dependency representations
taking into account (unrealised) semantic features.
Valldal and Oepen (2005) present a discriminative
disambiguation model using a hand-crafted HPSG
grammar for generation. Belz (2005) describes
a method for building statistical generation mod-
els using an automatically created generation tree-
bank for weather forecasts. None of these prob-
abilistic approaches to NLG uses a full treebank
grammar to drive generation.
Bangalore et al (2001) investigate the ef-
fect of training size on performance while using
grammars automatically extracted from the Penn-
II Treebank (Marcus et al, 1994) for generation.
Using an automatically extracted XTAG grammar,
they achieve a string accuracy of 0.749 on their
test set. Nakanishi et al (2005) present proba-
bilistic models for a chart generator using a HPSG
grammar acquired from the Penn-II Treebank (the
Enju HPSG). They investigate discriminative dis-
ambiguation models following Valldal and Oepen
(2005) and their best model achieves coverage of
90.56% and a BLEU score of 0.7723 on Penn-II
WSJ Section 23 sentences of length ?20.
In this paper we present a novel PCFG-based
architecture for probabilistic generation based on
wide-coverage, robust Lexical Functional Gram-
mar (LFG) approximations automatically ex-
tracted from treebanks (Cahill et al, 2004). In
Section 2 we briefly describe LFG (Kaplan and
Bresnan, 1982). Section 3 presents our genera-
tion architecture. Section 4 presents evaluation re-
sults on the Penn-II WSJ Section 23 test set us-
ing string-based metrics. Section 5 compares our
approach with alternative approaches in the litera-
ture. Section 6 concludes and outlines further re-
search.
2 Lexical Functional Grammar
Lexical Functional Grammar (LFG) (Kaplan and
Bresnan, 1982) is a constraint-based theory of
grammar. It (minimally) posits two levels of repre-
sentation, c(onstituent)-structure and f(unctional)-
structure. C-structure is represented by context-
free phrase-structure trees, and captures surface
1033
S
?=?
NP VP
(? SUBJ)= ? ?=?
NNP V SBAR
?=? ?=? (? COMP)= ?
They believe S
(? PRED) = ?pro? (? PRED) = ?believe? ?=?
(? NUM) = PL (? TENSE) = present
(? PERS) = 3 NP VP
(? SUBJ)= ? ?=?
NNP V
?=? ?=?
John resigned
(? PRED) = ?John? (? PRED) = ?resign?
(? NUM) = SG (? TENSE) = PAST
(? PERS) = 3
f1:
?
?
?
?
?
?
?
?
?
PRED ?BELIEVE?(?SUBJ)(?COMP)??
SUBJ f2:
[
PRED ?PRO?
NUM PL
PERS 3
]
COMP f3:
?
?
?
SUBJ f4:
[
PRED ?JOHN?
NUM SG
PERS 3
]
PRED RESIGN?(?SUBJ)??
TENSE PAST
?
?
?
TENSE PRESENT
?
?
?
?
?
?
?
?
?
Figure 1: C- and f-structures for the sentence They believe John resigned.
grammatical configurations such as word order.
The nodes in the trees are annotated with func-
tional equations (attribute-value structure con-
straints) which are resolved to produce an f-
structure. F-structures are recursive attribute-
value matrices, representing abstract syntactic
functions. F-structures approximate to basic
predicate-argument-adjunct structures or depen-
dency relations. Figure 1 shows the c- and f-
structures for the sentence ?They believe John re-
signed?.
3 PCFG-Based Generation for
Treebank-Based LFG Resources
Cahill et al (2004) present a method to au-
tomatically acquire wide-coverage robust proba-
bilistic LFG approximations1 from treebanks. The
method is based on an automatic f-structure an-
notation algorithm that associates nodes in tree-
bank trees with f-structure equations. For each
tree, the equations are collected and passed on to
a constraint solver which produces an f-structure
for the tree. Cahill et al (2004) present two
parsing architectures: the pipeline and the inte-
grated parsing architecture. In the pipeline ar-
chitecture, a PCFG (or a history-based lexicalised
generative parser) is extracted from the treebank
and used to parse unseen text into trees, the result-
ing trees are annotated with f-structure equations
by the f-structure annotation algorithm and a con-
straint solver produces an f-structure. In the in-
1The resources are approximations in that (i) they do not
enforce LFG completeness and coherence constraints and (ii)
PCFG-based models can only approximate LFG and similar
constraint-based formalisms (Abney, 1997).
tegrated architecture, first the treebank trees are
automatically annotated with f-structure informa-
tion, f-structure annotated PCFGs with rules of
the form NP(?OBJ=?)?DT(?=?) NN(?=?) are
extracted, syntactic categories followed by equa-
tions are treated as monadic CFG categories dur-
ing grammar extraction and parsing, unseen text is
parsed into trees with f-structure annotations, the
annotations are collected and a constraint solver
produces an f-structure.
The generation architecture presented here
builds on the integrated parsing architecture re-
sources of Cahill et al (2004). The generation
process takes an f-structure (such as the f-structure
on the right in Figure 1) as input and outputs the
most likely f-structure annotated tree (such as the
tree on the left in Figure 1) given the input f-
structure
argmaxTreeP (Tree|F-Str)
where the probability of a tree given an f-
structure is decomposed as the product of the
probabilities of all f-structure annotated produc-
tions contributing to the tree but where in addi-
tion to conditioning on the LHS of the produc-
tion (as in the integrated parsing architecture of
Cahill et al (2004)) each production X ? Y is
now also conditioned on the set of f-structure fea-
tures Feats ?-linked2 to the LHS of the rule. For
an f-structure annotated tree Tree and f-structure
F-Str with ?(Tree)=F-Str:3
2? links LFG?s c-structure to f-structure in terms of many-
to-one functions from tree nodes into f-structure.
3? resolves the equations in Tree into F-Str (if satisfiable)
in terms of the piece-wise function ?.
1034
Conditioning F-Structure Features Grammar Rules Probability
{PRED, SUBJ, COMP, TENSE} VP(?=?) ? VBD(?=?) SBAR(?COMP=?) 0.4998
{PRED, SUBJ, COMP, TENSE} VP(?=?) ? VBP(?=?) SBAR(?COMP=?) 0.0366
{PRED, SUBJ, COMP, TENSE} VP(?=?) ? VBD(?=?) , S(?COMP=?) 6.48e-6
{PRED, SUBJ, COMP, TENSE} VP(?=?) ? VBD(?=?) S(?COMP=?) 3.88e-6
{PRED, SUBJ, COMP, TENSE} VP(?=?) ? VBP(?=?) , SBARQ(?COMP=?) 7.86e-7
{PRED, SUBJ, COMP, TENSE} VP(?=?) ? VBD(?=?) SBARQ(?COMP=?) 1.59e-7
Table 1: Example VP Generation rules automatically extracted from Sections 02?21 of the Penn-II
Treebank
P (Tree|F-Str) :=
?
X ? Y in Tree
?(X) = Feats
P (X ? Y |X,Feats) (1)
P (X ? Y |X,Feats) = P (X ? Y,X, Feats)P (X,Feats) = (2)
P (X ? Y, Feats)
P (X,Feats) ?
#(X ? Y, Feats)
#(X ? . . . , F eats) (3)
and where probabilities are estimated using a
simple MLE and rule counts (#) from the auto-
matically f-structure annotated treebank resource
of Cahill et al (2004). Lexical rules (rules ex-
panding preterminals) are conditioned on the full
set of (atomic) feature-value pairs ?-linked to the
RHS. The intuition for conditioning rules in this
way is that local f-structure components of the in-
put f-structure drive the generation process. This
conditioning effectively turns the f-structure an-
notated PCFGs of Cahill et al (2004) into prob-
abilistic generation grammars. For example, in
Figure 1 (where ?-links are represented as ar-
rows), we automatically extract the rule S(?=?) ?
NP(?SUBJ=?) VP(?=?) conditioned on the feature
set {PRED,SUBJ,COMP,TENSE}. The probability
of the rule is then calculated by counting the num-
ber of occurrences of that rule (and the associated
set of features), divided by the number of occur-
rences of rules with the same LHS and set of fea-
tures. Table 1 gives example VP rule expansions
with their probabilities when we train a grammar
from Sections 02?21 of the Penn Treebank.
3.1 Chart Generation Algorithm
The generation algorithm is based on chart gen-
eration as first introduced by Kay (1996) with
Viterbi-pruning. The generation grammar is first
converted into Chomsky Normal Form (CNF). We
recursively build a chart-like data structure in a
bottom-up fashion. In contrast to packing of lo-
cally equivalent edges (Carroll and Oepen, 2005),
in our approach if two chart items have equiva-
lent rule left-hand sides and lexical coverage, only
the most probable one is kept. Each grammatical
function-labelled (sub-)f-structure in the overall f-
structure indexes a (sub-)chart. The chart for each
f-structure generates the most probable tree for
that f-structure, given the internal set of condition-
ing f-structure features and its grammatical func-
tion label. At each level, grammatical function in-
dexed charts are initially unordered. Charts are
linearised by generation grammar rules once the
charts themselves have produced the most prob-
able tree for the chart. Our example in Figure 1
generates the following grammatical function in-
dexed, embedded and (at each level of embedding)
unordered (sub-)chart configuration:
SUBJ f :2
COMP f :3
SUBJ f :4TOP f :1
For each local subchart, the following algorithm
is applied:
Add lexical rules
While subchart is Changing
Apply unary productions
Apply binary productions
Propagate compatible rules
3.2 A Worked Example
As an example, we step through the construc-
tion of the COMP-indexed chart at level f3 of
the f-structure in Figure 1. For lexical rules,
we check the feature set at the sub-f-structure
level and the values of the features. Only fea-
tures associated with lexical material are consid-
ered. The SUBJ-indexed sub-chart f4 is con-
structed by first adding the rule NNP(?=?) ?
John(?PRED=?John?,?NUM=pl,?PERS=3). If more
than one lexical rule corresponds to a particular set
of features and values in the f-structure, we add all
rules with different LHS categories. If two or more
1035
rules with equal LHS categories match the feature
set, we only add the most probable one.
Unary productions are applied if the RHS of the
unary production matches the LHS of an item al-
ready in the chart and the feature set of the unary
production matches the conditioning feature set of
the local sub-f-structure. In our example, this re-
sults in the rule NP(?SUBJ=?) ? NNP(?=?), con-
ditioned on {NUM, PERS, PRED}, being added to
the sub-chart at level f4 (the probability associated
with this item is the probability of the rule multi-
plied by the probability of the previous chart item
which combines with the new rule). When a rule
is added to the chart, it is automatically associated
with the yield of the rule, allowing us to propa-
gate chunks of generated material upwards in the
chart. If two items in the chart have the same LHS
(and the same yield independent of word order),
only the item with the highest probability is kept.
This Viterbi-style pruning ensures that processing
is efficient.
At sub-chart f4 there are no binary rules that
can be applied. At this stage, it is not possible
to add any more items to the sub-chart, therefore
we propagate items in the chart that are compat-
ible with the sub-chart index SUBJ. In our ex-
ample, only the rule NP(?SUBJ=?) ? NNP(?=?)
(which yields the string John) is propagated to the
next level up in the overall chart for consideration
in the next iteration. If the yield of an item be-
ing propagated upwards in the chart is subsumed
by an element already at that level, the subsumed
item is removed. This results in efficiently treat-
ing the well known problem originally described
in Kay (1996), where one unnecessarily retains
sub-optimal strings. For example, generating the
string ?The very tall strong athletic man?, one
does not want to keep variations such as ?The very
tall man?, or ?The athletic man?, if one can gener-
ate the entire string. Our method ensures that only
the most probable tree with the longest yield will
be propagated upwards.
The COMP-indexed chart at level f3 of the f-
structure is constructed in a similar fashion. First
the lexical rule V(?=?) ? resigned is added.
Next, conditioning on {PRED, SUBJ, TENSE}, the
unary rule VP(?=?) ? V(?=?) (with yield re-
signed) is added. We combine the new VP(?=?)
rule with the NP(?SUBJ=?) already present from
the previous iteration to enable us to add the rule
S(?=?) ? NP(?SUBJ=?) VP(?=?), conditioned
on {PRED, SUBJ, TENSE}. The yield of this rule
is John resigned. Next, conditioning on the same
feature set, we add the rule SBAR(?comp=?) ?
S(?=?) with yield John resigned to the chart. It is
not possible to add any more new rules, so at this
stage, only the SBAR(?COMP=?) rule with yield
John resigned is propagated up to the next level.
The process continues until at the outermost
level of the f-structure, there are no more rules to
be added to the chart. At this stage, we search for
the most probable rule with TOP as its LHS cate-
gory and return the yield of this rule as the output
of the generation process. Generation fails if there
is no rule with LHS TOP at this level in the chart.
3.3 Lexical Smoothing
Currently, the only smoothing in the system ap-
plies at the lexical level. Our backoff uses
the built-in lexical macros4 of the automatic f-
structure annotation algorithm of Cahill et al
(2004) to identify potential part-of-speech cate-
gories corresponding to a particular set of features.
Following Baayen and Sproat (1996) we assume
that unknown words have a probability distribu-
tion similar to hapax legomena. We add a lexical
rule for each POS tag that corresponds to the f-
structure features at that level to the chart with a
probability computed from the original POS tag
probability distribution multiplied by a very small
constant. This means that lexical rules seen during
training have a much higher probability than lexi-
cal rules added during the smoothing phase. Lexi-
cal smoothing has the advantage of boosting cov-
erage (as shown in Tables 3, 4, 5 and 6 below) but
slightly degrades the quality of the strings gener-
ated. We believe that the tradeoff in terms of qual-
ity is worth the increase in coverage.
Smoothing is not carried out when there is no
suitable phrasal grammar rule that applies during
the process of generation. This can lead to the gen-
eration of partial strings, since some f-structure
components may fail to generate a corresponding
string. In such cases, generation outputs the con-
catenation of the strings generated by the remain-
ing components.
4 Experiments
We train our system on WSJ Sections 02?21 of
the Penn-II Treebank and evaluate against the raw
4The lexical macros associate POS tags with sets of fea-
tures, for example the tag NNS (plural noun) is associated
with the features ?PRED=$LEMMA and ?NUM=pl.
1036
S. length ? 20 ? 25 ? 30 ? 40 all
Training 16667 23597 29647 36765 39832
Test 1034 1464 1812 2245 2416
Table 2: Number of training and test sentences per
sentence length
strings from Section 23. We use Section 22 as our
development set. As part of our evaluation, we ex-
periment with sentences of varying length (20, 25,
30, 40, all), both in training and testing. Table 2
gives the number of training and test sentences for
each sentence length. In each case, we use the au-
tomatically generated f-structures from Cahill et
al. (2004) from the original Section 23 treebank
trees as f-structure input to our generation experi-
ments. We automatically mark adjunct and coor-
dination scope in the input f-structure. Notice that
these automatically generated f-structures are not
?perfect?, i.e. they are not guaranteed to be com-
plete and coherent (Kaplan and Bresnan, 1982): a
local f-structure may contain material that is not
supposed to be there (incoherence) and/or may be
missing material that is supposed to be there (in-
completeness). The results presented below show
that our method is robust with respect to the qual-
ity of the f-structure input and will always attempt
to generate partial output rather than fail. We con-
sider this an important property as pristine gen-
eration input cannot always be guaranteed in re-
alistic application scenarios, such as probabilistic
transfer-based machine translation where genera-
tion input may contain a certain amount of noise.
4.1 Pre-Training Treebank Transformations
During the development of the generation system,
we carried out error analysis on our development
set WSJ Section 22 of the Penn-II Treebank. We
identified some initial pre-training transformations
to the treebank that help generation.
Punctuation: Punctuation is not usually en-
coded in f-structure representations. Because our
architecture is completely driven by rules con-
ditioned by f-structure information automatically
extracted from an f-structure annotated treebank,
its placement of punctuation is not principled.
This led to anomalies such as full stops appear-
ing mid sentence and quotation marks appearing
in undesired locations. One partial solution to this
was to reduce the amount of punctuation that the
system trained on. We removed all punctuation
apart from commas and full stops from the train-
ing data. We did not remove any punctuation from
the evaluation test set (Section 23), but our system
will ever only produce commas and full stops. In
the evaluation (Tables 3, 4, 5 and 6) we are pe-
nalised for the missing punctuation. To solve the
problem of full stops appearing mid sentence, we
carry out a punctuation post-processing step on all
generated strings. This removes mid-sentence full
stops and adds missing full stops at the end of gen-
erated sentences prior to evaluation. We are work-
ing on a more appropriate solution allowing the
system to generate all punctuation.
Case: English does not have much case mark-
ing, and for parsing no special treatment was en-
coded. However, when generating, it is very
important that the first person singular pronoun
is I in the nominative case and me in the ac-
cusative. Given the original grammar used in pars-
ing, our generation system was not able to distin-
guish nominative from accusative contexts. The
solution we implemented was to carry out a gram-
mar transformation in a pre-processing step, to au-
tomatically annotate personal pronouns with their
case information. This resulted in phrasal and lex-
ical rules such as NP(?SUBJ) ? PRP?nom(?=?)
and PRP?nom(?=?) ? I and greatly improved the
accuracy of the pronouns generated.
4.2 String-Based Evaluation
We evaluate the output of our generation system
against the raw strings of Section 23 using the
Simple String Accuracy and BLEU (Papineni et
al., 2002) evaluation metrics. Simple String Accu-
racy is based on the string edit distance between
the output of the generation system and the gold
standard sentence. BLEU is the weighted average
of n-gram precision against the gold standard sen-
tences. We also measure coverage as the percent-
age of input f-structures that generate a string. For
evaluation, we automatically expand all contracted
words. We only evaluate strings produced by the
system (similar to Nakanishi et al (2005)).
We conduct a total of four experiments. The
parameters we investigate are lexical smoothing
(Section 3.3) and partial output. Partial output
is a robustness feature for cases where a sub-f-
structure component fails to generate a string and
the system outputs a concatenation of the strings
generated by the remaining components, rather
than fail completely.
1037
Sentence length of Evaluation Section 23 Sentences of length:
Training Data Metric ? 20 ? 25 ? 30 ? 40 all
? 20 BLEU 0.6812 0.6601 0.6373 0.6013 0.5793
String Accuracy 0.7274 0.7052 0.6875 0.6572 0.6431
Coverage 96.52 95.83 94.59 93.76 93.92
? 25 BLEU 0.6915 0.6800 0.6696 0.6396 0.6233
String Accuracy 0.7262 0.7095 0.6983 0.6731 0.6618
Coverage 96.52 95.83 94.59 93.76 93.92
? 30 BLEU 0.6979 0.6881 0.6792 0.6576 0.6445
String Accuracy 0.7317 0.7169 0.7075 0.6853 0.6749
Coverage 97.97 97.95 97.41 97.15 97.31
? 40 BLEU 0.7045 0.6951 0.6852 0.6715 0.6605
String Accuracy 0.7349 0.7212 0.7074 0.6881 0.6788
Coverage 98.45 98.36 98.01 97.82 97.93
all BLEU 0.7077 0.6974 0.6859 0.6734 0.6651
String Accuracy 0.7373 0.7221 0.7087 0.6894 0.6808
Coverage 98.65 98.5 98.12 97.95 98.05
Table 3: Generation +partial output +lexical smoothing
Sentence length of Evaluation Section 23 Sentences of length:
Training Data Metric ? 20 ? 25 ? 30 ? 40 all
all BLEU 0.6253 0.6097 0.5887 0.5730 0.5590
String Accuracy 0.6886 0.6688 0.6513 0.6317 0.6207
Coverage 91.20 91.19 90.84 90.33 90.11
Table 4: Generation +partial output -lexical smoothing
Varying the length of the sentences included in
the training data (Tables 3 and 5) shows that re-
sults improve (both in terms of coverage and string
quality) as the length of sentence included in the
training data increases.
Tables 3 and 5 give the results for the exper-
iments including lexical smoothing and varying
partial output. Table 3 (+partial, +smoothing)
shows that training on sentences of all lengths and
evaluating all strings (including partial outputs),
our system achieves coverage of 98.05%, a BLEU
score of 0.6651 and string accuracy of 0.6808. Ta-
ble 5 (-partial, +smoothing) shows that coverage
drops to 89.49%, BLEU score increases to 0.6979
and string accuracy to 0.7012, when the system
is trained on sentences of all lengths. Similarly,
for strings ?20, coverage drops from 98.65% to
95.26%, BLEU increases from 0.7077 to 0.7227
and String Accuracy from 0.7373 to 0.7476. In-
cluding partial output increases coverage (by more
than 8.5 percentage points for all sentences) and
hence robustness while slightly decreasing quality.
Tables 3 (+partial, +smoothing) and 4 (+partial,
-smoothing) give results for the experiments in-
cluding partial output but varying lexical smooth-
ing. With no lexical smoothing (Table 4), the
system (trained on all sentence lengths) produces
strings for 90.11% of the input f-structures and
achieves a BLEU score of 0.5590 and string ac-
curacy of 0.6207. Switching off lexical smooth-
ing has a negative effect on all evaluation met-
rics (coverage and quality), because many more
strings produced are now partial (since for PRED
values unseen during training, no lexical entries
are added to the chart).
Comparing Tables 5 (-partial, +smoothing)
and 6 (-partial, -smoothing), where the system
does not produce any partial outputs and lexi-
cal smoothing is varied, shows that training on
all sentence lengths, BLEU score increases from
0.6979 to 0.7147 and string accuracy increases
from 0.7012 to 0.7192. At the same time, cover-
age drops dramatically from 89.49% (Table 5) to
47.60% (Table 6).
Comparing Tables 4 and 6 shows that while par-
tial output almost doubles coverage, this comes
at a price of a severe drop in quality (BLEU
score drops from 0.7147 to 0.5590). On the other
hand, comparing Tables 5 and 6 shows that lexical
smoothing achieves a similar increase in coverage
with only a very slight drop in quality.
5 Discussion
Nakanishi et al (2005) achieve 90.56% cover-
age and a BLEU score of 0.7723 on Section 23
1038
Sentence length of Evaluation Section 23 Sentences of length:
Training Data Metric ? 20 ? 25 ? 30 ? 40 all
? 20 BLEU 0.7326 0.7185 0.7165 0.7082 0.7052
String Accuracy 0.76 0.7428 0.7363 0.722 0.7175
Coverage 85.49 81.56 77.26 71.94 69.08
? 25 BLEU 0.7300 0.7235 0.7218 0.7118 0.7077
String Accuracy 0.7517 0.7382 0.7315 0.7172 0.7116
Coverage 89.65 87.77 84.38 80.31 78.56
? 30 BLEU 0.7207 0.7125 0.7107 0.6991 0.6946
String Accuracy 0.747 0.7336 0.7275 0.711 0.7045
Coverage 93.23 92.14 89.74 86.59 85.18
? 40 BLEU 0.7221 0.7140 0.7106 0.7016 0.6976
String Accuracy 0.746 0.7331 0.7236 0.7072 0.7001
Coverage 94.58 93.85 91.89 89.62 88.33
all BLEU 0.7227 0.7145 0.7095 0.7011 0.6979
String Accuracy 0.7476 0.7331 0.7239 0.7077 0.7012
Coverage 95.26 94.40 92.55 90.69 89.49
Table 5: Generation -partial output +lexical smoothing
Sentence length of Evaluation Section 23 Sentences of length:
Training Data Metric ? 20 ? 25 ? 30 ? 40 all
all BLEU 0.7272 0.7237 0.7201 0.7160 0.7147
String Accuracy 0.7547 0.7436 0.7361 0.7237 0.7192
Coverage 61.99 57.38 53.64 47.60 47.60
Table 6: Generation -partial output -lexical smoothing
sentences, restricted to length ?20 for efficiency
reasons. Langkilde-Geary?s (2002) best system
achieves 82.8% coverage, a BLEU score of 0.924
and string accuracy of 0.945 against Section 23
sentences of all lengths. Callaway (2003) achieves
98.7% coverage and a string accuracy of 0.6607
on sentences of all lengths. Our best results for
sentences of length ? 20 are coverage of 95.26%,
BLEU score of 0.7227 and string accuracy of
0.7476. For all sentence lengths, our best results
are coverage of 89.49%, a BLEU score of 0.6979
and string accuracy of 0.7012.
Using hand-crafted grammar-based genera-
tion systems (Langkilde-Geary, 2002; Callaway,
2003), it is possible to achieve very high results.
However, hand-crafted systems are expensive to
construct and not easily ported to new domains or
other languages. Our methodology, on the other
hand, is based on resources automatically acquired
from treebanks and easily ported to new domains
and languages, simply by retraining on suitable
data. Recent work on the automatic acquisition
of multilingual LFG resources from treebanks for
Chinese, German and Spanish (Burke et al, 2004;
Cahill et al, 2005; O?Donovan et al, 2005) has
shown that given a suitable treebank, it is possi-
ble to automatically acquire high quality LFG re-
sources in a very short space of time. The genera-
tion architecture presented here is easily ported to
those different languages and treebanks.
6 Conclusion and Further Work
We present a new architecture for stochastic LFG
surface realisation using the automatically anno-
tated treebanks and extracted PCFG-based LFG
approximations of Cahill et al (2004). Our model
maximises the probability of a tree given an f-
structure, supporting a simple and efficient imple-
mentation that scales to wide-coverage treebank-
based resources. An improved model would
maximise the probability of a string given an f-
structure by summing over trees with the same
yield. More research is required to implement
such a model efficiently using packed representa-
tions (Carroll and Oepen, 2005). Simple PCFG-
based models, while effective and computationally
efficient, can only provide approximations to LFG
and similar constraint-based formalisms (Abney,
1997). Research on discriminative disambigua-
tion methods (Valldal and Oepen, 2005; Nakanishi
et al, 2005) is important. Kaplan and Wedekind
(2000) show that for certain linguistically interest-
ing classes of LFG (and PATR etc.) grammars,
generation from f-structures yields a context free
language. Their proof involves the notion of a
1039
?refinement? grammar where f-structure informa-
tion is compiled into CFG rules. Our probabilis-
tic generation grammars bear a conceptual similar-
ity to Kaplan and Wedekind?s ?refinement? gram-
mars. It would be interesting to explore possible
connections between the treebank-based empirical
work presented here and the theoretical constructs
in Kaplan and Wedekind?s proofs.
We presented a full set of generation experi-
ments on varying sentence lengths training on Sec-
tions 02?21 of the Penn Treebank and evaluat-
ing on Section 23 strings. Sentences of length
?20 achieve coverage of 95.26%, BLEU score
of 0.7227 and string accuracy of 0.7476 against
the raw Section 23 text. Sentences of all lengths
achieve coverage of 89.49%, BLEU score of
0.6979 and string accuracy of 0.7012. Our method
is robust and can cope with noise in the f-structure
input to generation and will attempt to produce
partial output rather than fail.
Acknowledgements
We gratefully acknowledge support from Science
Foundation Ireland grant 04/BR/CS0370 for the
research reported in this paper.
References
Stephen Abney. 1997. Stochastic Attribute-Value Gram-
mars. Computational Linguistics, 23(4):597?618.
Harald Baayen and Richard Sproat. 1996. Estimating lexi-
cal priors for low-frequency morphologically ambiguous
forms. Computational Linguistics, 22(2):155?166.
Srinivas Bangalore and Owen Rambow. 2000. Exploit-
ing a probabilistic hierarchical model for generation. In
Proceedings of COLING 2000, pages 42?48, Saarbrcken,
Germany.
Srinivas Bangalore, John Chen, and Owen Rambow. 2001.
Impact of quality and quantity of corpora on stochastic
generation. In Proceedings of EMNLP 2001, pages 159?
166.
Anja Belz. 2005. Statistical generation: Three methods com-
pared and evaluated. In Proceedings of the 10th European
Workshop on Natural Language Generation (ENLG? 05),
pages 15?23, Aberdeen, Scotland.
Michael Burke, Olivia Lam, Rowena Chan, Aoife Cahill,
Ruth O?Donovan, Adams Bodomo, Josef van Genabith,
and Andy Way. 2004. Treebank-Based Acquisition of a
Chinese Lexical-Functional Grammar. In Proceedings of
the 18th Pacific Asia Conference on Language, Informa-
tion and Computation, pages 161?172, Tokyo, Japan.
Aoife Cahill, Michael Burke, Ruth O?Donovan, Josef van
Genabith, and Andy Way. 2004. Long-Distance De-
pendency Resolution in Automatically Acquired Wide-
Coverage PCFG-Based LFG Approximations. In Pro-
ceedings of ACL-04, pages 320?327, Barcelona, Spain.
Aoife Cahill, Martin Forst, Michael Burke, Mairead Mc-
Carthy, Ruth O?Donovan, Christian Rohrer, Josef van
Genabith, and Andy Way. 2005. Treebank-based acquisi-
tion of multilingual unification grammar resources. Jour-
nal of Research on Language and Computation; Special
Issue on ?Shared Representations in Multilingual Gram-
mar Engineering?, pages 247?279.
Charles B. Callaway. 2003. Evaluating coverage for large
symbolic NLG grammars. In Proceedings of the Eigh-
teenth International Joint Conference on Artificial Intelli-
gence, pages 811?817, Acapulco, Mexico.
John Carroll and Stephan Oepen. 2005. High efficiency real-
ization for a wide-coverage unification grammar. In Pro-
ceedings of IJCNLP05, pages 165?176, Jeju Island, Ko-
rea.
Ron Kaplan and Joan Bresnan. 1982. Lexical Functional
Grammar, a Formal System for Grammatical Representa-
tion. In Joan Bresnan, editor, The Mental Representation
of Grammatical Relations, pages 173?281. MIT Press,
Cambridge, MA.
Ron Kaplan and Juergen Wedekind. 2000. LFG Generation
produces Context-free languages. In Proceedings of COL-
ING 2000, pages 141?148, Saarbruecken, Germany.
Martin Kay. 1996. Chart Generation. In Proceedings of the
34th Annual Meeting of the Association for Computational
Linguistics, pages 200?204, Santa Cruz, CA.
Irene Langkilde-Geary. 2002. An empirical verification of
coverage and correctness for a general-purpose sentence
generator. In Second International Natural Language
Generation Conference, pages 17?24, Harriman, NY.
Irene Langkilde. 2000. Forest-based statistical sentence gen-
eration. In Proceedings of NAACL 2000, pages 170?177,
Seattle, WA.
Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz,
Robert MacIntyre, Ann Bies, Mark Ferguson, Karen Katz,
and Britta Schasberger. 1994. The Penn Treebank: An-
notating Predicate Argument Structure. In Proceedings
of the ARPA Workshop on Human Language Technology,
pages 110?115, Princton, NJ.
Hiroko Nakanishi, Yusuke Miyao, and Jun?ichi Tsujii. 2005.
Probabilistic models for disambiguation of an HPSG-
based chart generator. In Proceedings of the International
Workshop on Parsing Technology, Vancouver, Canada.
Ruth O?Donovan, Aoife Cahill, Josef van Genabith, and
Andy Way. 2005. Automatic Acquisition of Spanish LFG
Resources from the CAST3LB Treebank. In Proceedings
of LFG 05, pages 334?352, Bergen, Norway.
Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing
Zhu. 2002. BLEU: a Method for Automatic Evaluation of
Machine Translation. In Proceedings of ACL 2002, pages
311?318, Philadelphia, PA.
Adwait Ratnaparkhi. 2000. Trainable methods for natu-
ral language generation. In Proceedings of NAACL 2000,
pages 194?201, Seattle, WA.
Erik Valldal and Stephan Oepen. 2005. Maximum En-
tropy Models for Realization Reranking. In Proceedings
of the 10th Machine Translation Summit, pages 109?116,
Phuket, Thailand.
1040
Proceedings of the 5th Workshop on Important Unresolved Matters, pages 65?72,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Pruning the Search Space of a Hand-Crafted Parsing System with a
Probabilistic Parser
Aoife Cahill
Dublin City University
acahill@computing.dcu.ie
Tracy Holloway King
PARC
thking@parc.com
John T. Maxwell III
PARC
maxwell@parc.com
Abstract
The demand for deep linguistic analysis
for huge volumes of data means that it is
increasingly important that the time taken
to parse such data is minimized. In the
XLE parsing model which is a hand-crafted,
unification-based parsing system, most of
the time is spent on unification, searching
for valid f-structures (dependency attribute-
value matrices) within the space of the many
valid c-structures (phrase structure trees).
We carried out an experiment to determine
whether pruning the search space at an ear-
lier stage of the parsing process results in
an improvement in the overall time taken to
parse, while maintaining the quality of the
f-structures produced. We retrained a state-
of-the-art probabilistic parser and used it to
pre-bracket input to the XLE, constraining
the valid c-structure space for each sentence.
We evaluated against the PARC 700 Depen-
dency Bank and show that it is possible to
decrease the time taken to parse by ?18%
while maintaining accuracy.
1 Introduction
When deep linguistic analysis of massive data is re-
quired (e.g. processing Wikipedia), it is crucial that
the parsing time be minimized. The XLE English
parsing system is a large-scale, hand-crafted, deep,
unification-based system that processes raw text
and produces both constituent-structures (phrase
structure trees) and feature-structures (dependency
attribute-value matrices). A typical breakdown of
parsing time of XLE components is Morphology
(1.6%), Chart (5.8%) and Unifier (92.6%).
The unification process is the bottleneck in the
XLE parsing system. The grammar generates many
valid c-structure trees for a particular sentence: the
Unifier then processes all of these trees (as packed
structures), and a log-linear disambiguation module
can choose the most probable f-structure from the
resulting valid f-structures. For example, the sen-
tence ?Growth is slower.? has 84 valid c-structure
trees according to the current English grammar;1
however once the Unifier has processed all of these
trees (in a packed form), only one c-structure and
f-structure pair is valid (see Figure 1). In this in-
stance, the log-linear disambiguation does not need
to choose the most probable result.
The research question we pose is whether the
search space can be pruned earlier before unifi-
cation takes place. Bangalore and Joshi (1999),
Clark and Curran (2004) and Matsuzaki et al (2007)
show that by using a super tagger before (CCG and
HPSG) parsing, the space required for discrimini-
tive training is drastically reduced. Supertagging
is not widely used within the LFG framework, al-
though there has been some work on using hypertags
(Kinyon, 2000). Ninomiya et al (2006) propose a
method for faster HPSG parsing while maintaining
accuracy by only using the probabilities of lexical
entry selections (i.e. the supertags) in their discrim-
initive model. In the work presented here, we con-
1For example, is can be a copula, a progressive auxiliary or
a passive auxiliary, while slower can either be an adjective or an
adverb.
65
centrate on reducing the number of c-structure trees
that the Unifier has to process, ideally to one tree.
The hope was that this would speed up the parsing
process, but how would it affect the quality of the f-
structures? This is similar to the approach taken by
Cahill et al (2005) who do not use a hand-crafted
complete unification system (rather an automatically
acquired probabilistic approximation). They parse
raw text into LFG f-structures by first parsing with a
probabilistic CFG parser to choose the most proba-
ble c-structure. This is then passed to an automatic
f-structure annotation algorithm which deterministi-
cally generates one f-structure for that tree.
The most compact way of doing this would be to
integrate a statistical component to the parser that
could rank the c-structure trees and only pass the
most likely forward to the unification process. How-
ever, this would require a large rewrite of the sys-
tem. So, we first wanted to investigate a ?cheaper?
alternative to determine the viability of the pruning
strategy; this is the experiment reported in this pa-
per. This is implemented by stipulating constituent
boundaries in the input string, so that any c-structure
that is incompatible with these constraints is invalid
and will not be processed by the Unifier. This was
done to some extent in Riezler et al (2002) to au-
tomatically generate training data for the log-linear
disambiguation component of XLE. Previous work
obtained the constituent constraints (i.e. brackets)
from the gold-standard trees in the Penn-II Tree-
bank. However, to parse novel text, gold-standard
trees are unavailable.
We used a state-of-the-art probabilistic parser to
provide the bracketing constraints to XLE. These
parsers are accurate (achieving accuracy of over
90% on Section 23 WSJ text), fast, and robust.
The idea is that pre-parsing of the input text by a
fast and accurate parser can prune the c-structure
search space, reducing the amount of work done by
the Unifier, speed up parsing and maintain the high
quality of the f-structures produced.
The structure of this paper is as follows: Section
2 introduces the XLE parsing system. Section 3 de-
scribes a baseline experiment and based on the re-
sults suggests retraining the Bikel parser to improve
results (Section 4). Section 5 describes experiments
on the development set, from which we evaluate the
most successful system against the PARC 700 test
CS 1: ROOT
Sadj[fin]
S[fin]
NP
NPadj
NPzero
N
^ growth
VPall[fin]
VPcop[fin]
Vcop[fin]
is
AP[pred]
A
slower
PERIOD
.
"Growth is slower."
'be<[68:slow]>[23:growth]'PRED
'growth'PRED23SUBJ
'slow<[23:growth]>'PRED
[23:growth]SUBJ
'more'PRED-1ADJUNCT68
XCOMP
47
Figure 1: C- and F-Structure for ?Growth is slower.?
set (Section 6). Finally, Section 7 concludes.
2 Background
In this section we introduce Lexical Functional
Grammar, the grammar formalism underlying the
XLE, and briefly describe the XLE parsing system.
2.1 Lexical Functional Grammar
Lexical Functional Grammar (LFG) (Kaplan and
Bresnan, 1982) is a constraint-based theory of gram-
mar. It (minimally) posits two levels of repre-
sentation, c(onstituent)-structure and f(unctional)-
structure. C-structure is represented by context-
free phrase-structure trees, and captures surface
grammatical configurations such as word order.
The nodes in the trees are annotated with func-
tional equations (attribute-value structure con-
straints) which are resolved to produce an f-
structure. F-structures are recursive attribute-value
matrices, representing abstract syntactic functions.
F-structures approximate basic predicate-argument-
adjunct structures or dependency relations. Fig-
ure 1 shows the c- and f-structure for the sentence
?Growth is slower.?.
66
Parser Output: (S1 (S (NP (NN Growth)) (VP (AUX is) (ADJP (JJR slower))) (. .)))
Labeled: \[S1 \[S Growth \[VP is \[ADJP slower\] \].\] \]
Unlabeled:\[ \[ Growth \[ is \[ slower\] \].\] \]
Figure 2: Example of retained brackets from parser output to constrain the XLE parser
2.2 The XLE Parsing System
The XLE parsing system is a deep-grammar-based
parsing system. The experiments reported in this
paper use the English LFG grammar constructed
as part of the ParGram project (Butt et al, 2002).
This system incorporates sophisticated ambiguity-
management technology so that all possible syn-
tactic analyses of a sentence are computed in
an efficient, packed representation (Maxwell and
Kaplan, 1993). In accordance with LFG the-
ory, the output includes not only standard context-
free phrase-structure trees (c-structures) but also
attribute-value matrices (f-structures) that explic-
itly encode predicate-argument relations and other
meaningful properties. The f-structures can be de-
terministically mapped to dependency triples with-
out any loss of information, using the built-in or-
dered rewrite system (Crouch et al, 2002). XLE se-
lects the most probable analysis from the potentially
large candidate set by means of a stochastic disam-
biguation component based on a log-linear proba-
bility model (Riezler et al, 2002) that works on the
packed representations. The underlying parsing sys-
tem also has built-in robustness mechanisms that al-
low it to parse strings that are outside the scope of
the grammar as a list of fewest well-formed ?frag-
ments?. Furthermore, performance parameters that
bound parsing and disambiguation can be tuned for
efficient but accurate operation. These parameters
include at which point to timeout and return an error,
the amount of stack memory to allocate, the num-
ber of new edges to add to the chart and at which
point to start skimming (a process that guarantees
XLE will finish processing a sentence in polynomial
time by only carrying out a bounded amount of work
on each remaining constituent after a time threshold
has passed). For the experiments reported here, we
did not fine-tune these parameters due to time con-
straints; so default values were arbitrarily set and the
same values used for all parsing experiments.
3 Baseline experiments
We carried out a baseline experiment with two
state-of-the-art parsers to establish what effect pre-
bracketing the input to the XLE system has on the
quality and number of the solutions produced. We
used the Bikel () multi-threaded, head-driven chart-
parsing engine developed at the University of Penn-
sylvania. The second parser is that described in
Charniak and Johnson (2005). This parser uses a
discriminative reranker that selects the most proba-
ble parse from the 50-best parses returned by a gen-
erative parser based on Charniak (2000).
We evaluated against the PARC 700 Dependency
Bank (King et al, 2003) which provides gold-
standard analyses for 700 sentences chosen at ran-
dom from Section 23 of the Penn-II Treebank. The
Dependency Bank was bootstrapped by parsing the
700 sentences with the XLE English grammar, and
then manually correcting the output. The data is di-
vided into two sets, a 140-sentence development set
and a test set of 560 sentences (Kaplan et al, 2004).
We took the raw strings from the 140-sentence
development set and parsed them with each of the
state-of-the-art probabilistic parsers. As an upper
bound for the baseline experiment, we use the brack-
ets in the original Penn-II treebank trees for the 140
development set.
We then used the brackets from each parser out-
put (or original treebank trees) to constrain the XLE
parser. If the input to the XLE parser is bracketed,
the parser will only generate c-structures that respect
these brackets (i.e., only c-structures with brackets
that are compatible with the input brackets are con-
sidered during the unification stage). Figure 2 gives
an example of retained brackets from the parser out-
put. We do not retain brackets around PRN (paren-
thetical phrase) or NP nodes as their structure often
differed too much from XLE analyses of the same
phrases. We passed pre-bracketed strings to the XLE
and evaluated the output f-structures in terms of de-
pendency triples against the 140-sentence subset of
67
Non-Fragment Fragment
Penn-XLE Penn-XLE Penn-XLE Penn-XLE
(lab.) (unlab.) (lab.) (unlab.)
Total XLE parses (/140) 0 89 140 140
F-Score of subset 0 84.11 53.92 74.87
Overall F-Score 0 58.91 53.92 74.87
Table 1: Upper-bound results for original Penn-II trees
Non-Fragment Fragment
XLE Bikel-XLE Bikel-XLE XLE Bikel-XLE Bikel-XLE
(lab.) (unlab.) (lab.) (unlab.)
Total XLE Parses (/140) 119 0 84 135 140 140
F-Score of Subset 81.57 0 84.23 78.72 54.37 73.71
Overall F-Score 72.01 0 55.06 76.13 54.37 *73.71
XLE CJ-XLE CJ-XLE XLE CJ-XLE CJ-XLE
(lab.) (unlab.) (lab.) (unlab.)
Total XLE Parses (/140) 119 0 86 135 139 139
F-Score of Subset 81.57 0 86.57 78.72 53.96 75.64
Overall F-Score 72.01 0 58.04 76.13 53.48 *74.98
Table 2: Bikel (2002) and Charniak and Johnson (2005) out-of-the-box baseline results
the PARC 700 Dependency Bank.
The results of the baseline experiments are given
in Tables 1 and 2. Table 1 gives the upper bound
results if we use the gold standard Penn treebank
to bracket the input to XLE. Table 2 compares the
XLE (fragment and non-fragment) grammar to the
system where the input is pre-parsed by each parser.
XLE fragment grammars provide a back-off when
parsing fails: the grammar is relaxed and the parser
builds a fragment parse of the well-formed chunks.
We compare the parsers in terms of total number
of parses (out of 140) and the f-score of the sub-
set of sentences successfully parsed. We also com-
bine these scores to give an overall f-score, where
the system scores 0 for each sentence it could not
parse. When testing for statistical significance be-
tween systems, we compare the overall f-score val-
ues. Figures marked with an asterisk are not statisti-
cally significantly different at the 95% level.2
The results show that using unlabeled brackets
achieves reasonable f-scores with the non-fragment
grammar. Using the labeled bracketing from the out-
put of both parsers causes XLE to always fail when
parsing. This is because the labels in the output of
parsers trained on the Penn-II treebank differ con-
siderably from the labels on c-structure trees pro-
2We use the approximate randomization test (Noreen, 1989)
to test for significance.
duced by XLE. Interestingly, the f-scores for both
the CJ-XLE and Bikel-XLE systems are very sim-
ilar to the upper bounds. The gold standard upper
bound is not as high as expected because the Penn
trees used to produce the gold bracketed input are
not always compatible with the XLE-style trees. As
a simple example, the tree in Figure 1 differs from
the parse tree for the same sentence in the Penn
Treebank (Figure 3). The most obvious difference
is the labels on the nodes. However, even in this
small example, there are structural differences, e.g.
the position of the period. In general, the larger the
tree, the greater the difference in both labeling and
structure between the Penn trees and the XLE-style
trees. Therefore, the next step was to retrain a parser
to produce trees with structures the same as XLE-
style trees and with XLE English grammar labels on
the nodes. For this experiment we use the Bikel ()
parser, as it is more suited to being retrained on a
new treebank annotation scheme.
4 Retraining the Bikel parser
We retrained the Bikel parser so that it produces
trees like those outputted by the XLE parsing sys-
tem (e.g. Figure 1). To do this, we first created a
training corpus, and then modified the parser to deal
with this new data.
Since there is no manually-created treebank of
68
SNP
NN
Growth
VP
VBZ
is
ADJP-PRD
JJR
slower
?
?
Figure 3: Penn Treebank tree for ?Growth is slower.?
XLE-style trees, we created one automatically from
sections 02-21 of the Penn-II Treebank. We took the
raw strings from those sections and marked up NP
and SBAR constituents using the brackets from the
gold standard Penn treebank. The NP constituents
are labeled, and the SBAR unlabeled (i.e. the SBAR
constituents are forced to exist in the XLE parse, but
the label on them is not constrained to be SBAR).
We also tagged verbs, adjectives and nouns, based
on the gold standard POS tags.
We parsed the 39,832 marked-up sentences in the
standard training corpus and used the XLE disam-
biguation module to choose the most probable c-
and f-structure pair for each sentence. Ideally we
would have had an expert choose these. We au-
tomatically extracted the c-structure trees produced
by the XLE and performed some automatic post-
processing.3 This resulted in an automatically cre-
ated training corpus of 27,873 XLE-style trees. The
11,959 missing trees were mainly due to the XLE
parses not being compatible with the bracketed in-
put, but sometimes due to time and memory con-
straints.
Using the automatically-created training corpus
of XLE-style trees, we retrained the Bikel parser on
this data. This required adding a new language mod-
ule (?XLE-English?) to the Bikel parser, and regen-
erating head-finding rules for the XLE-style trees.
5 Experiments
Once we had a retrained version of the Bikel parser
that parses novel text into XLE-style trees, we car-
ried out a number of experiments on our develop-
ment set in order to establish the optimum settings
3The postprocessing included removing morphological in-
formation and the brackets from the original markup.
All Sentences
XLE Bikel-XLE
Non-fragment grammar
Labeled brackets
Total Parsing Time 964 336
Total XLE Parses (/140) 119 77
F-Score of Subset 81.57 86.11
Overall F-Score 72.01 52.84
Non-fragment grammar
Unlabeled brackets
Total Parsing Time 964 380
Total XLE Parses (/140) 119 89
F-Score of Subset 81.57 85.62
Overall F-Score 72.01 59.34
Fragment grammar
Labeled brackets
Total Parsing Time 1143 390
Total XLE Parses (/140) 135 140
F-Score of Subset 78.72 71.86
Overall F-Score 76.13 71.86
Fragment grammar
Unlabeled brackets
Total Parsing Time 1143 423
Total XLE Parses (/140) 135 140
F-Score of Subset 78.72 74.51
Overall F-Score 76.13 *74.51
Table 3: Bikel-XLE Initial Experiments
for the evaluation against the PARC 700 test set.
5.1 Pre-bracketing
We automatically pre-processed the raw strings from
the 140-sentence development set. This made sys-
tematic changes to the tokens so that the retrained
Bikel parser can parse them. The changes included
removing quotes, converting a and an to a, con-
verting n?t to not, etc. We parsed the pre-processed
strings with the new Bikel parser.
We carried out four initial experiments, experi-
menting with both labeled and unlabeled brackets
and XLE fragment and non-fragment grammars. Ta-
ble 3 gives the results for these experiments. We
compare the parsers in terms of time, total number
of parses (out of 140), the f-score of the subset of
sentences successfully parsed and the overall f-score
if the system achieves a score of 0 for all sentences
it does not parse. The time taken for the Bikel-XLE
system includes the time taken for the Bikel parser
to parse the sentences, as well as the time taken for
XLE to process the bracketed input.
Table 3 shows that using the non-fragment gram-
mar, the Bikel-XLE system performs better on the
69
subset of sentences parsed than XLE system alone,
though the results are not statistically significantly
better overall, since the coverage is much lower. The
number of bracketed sentences that can be parsed
by XLE increases if the brackets are unlabeled.
The table also shows that the XLE system performs
much better than Bikel-XLE when using the frag-
ment grammars. Although the Bikel-XLE system is
quite a bit faster, there is a drop in f-score; however
this is not statistically significant when the brackets
are unlabeled.
5.2 Pre-tagging
We performed some error analysis on the output of
the Bikel-XLE system and noticed that a consider-
able number of errors were due to mis-tagging. So,
we pre-tagged the input to the Bikel parser using the
MXPOST tagger (Ratnaparkhi, 1996). The results
for the non-fragment grammars are presented in Ta-
ble 4. Pre-tagging with MXPOST, however, does
not result in a statistically significantly higher re-
sult than parsing untagged input, although more sen-
tences can be parsed by both systems. Pre-tagging
also adds an extra time overhead cost.
No pretags MXPOST tags
XLE Bikel-XLE Bikel-XLE
Unlabeled
Total Parsing Time 964 380 493
# XLE Parses (/140) 119 89 92
F-Score of Subset 81.57 85.62 84.98
Overall F-Score 72.01 59.34 *61.11
Labeled
Total Parsing Time 964 336 407
# XLE Parses (/140) 119 77 80
F-Score of Subset 81.57 86.11 85.87
Overall F-Score 72.01 52.84 *54.91
Table 4: MXPOST pre-tagged, Non-fragment gram-
mar
5.3 Pruning
The Bikel parser can be customized to allow differ-
ent levels of pruning. The above experiments were
carried out using the default level. We carried out
experiments with three levels of pruning.4 The re-
4The default level of pruning starts at 3.5, has a maximum of
4 and relaxes constraints when parsing fails. Level 1 pruning is
the same as the default except the constraints are never relaxed.
Level 2 pruning has a start value of 3.5 and a maximum value
of 3.5. Level 3 pruning has a start and maximum value of 3.
sults are given in Table 5 for the experiment with
labeled brackets and the non-fragment XLE gram-
mar. More pruning generally results in fewer and
lower-quality parses. The biggest gain is with prun-
ing level 1, where the number and quality of brack-
eted sentences that can be parsed with XLE remains
the same as with the default level. This is because
Bikel with pruning level 1 does not relax the con-
straints when parsing fails and does not waste time
parsing sentences that cannot be parsed in bracketed
form by XLE.
Default L1 L2 L3
Total Parsing Time 336 137 137 106
# XLE Parses (/140) 77 77 76 75
F-Score of Subset 86.11 86.11 86.04 85.87
Overall F-Score 52.84 *52.84 *52.43 *52.36
Table 5: Pruning with Non-fragment grammar, La-
beled brackets, Levels default-3
5.4 Hybrid systems
Although pre-parsing with Bikel results in faster
XLE parsing time and high-quality f-structures
(when examining only the quality of the sentences
that can be parsed by the Bikel-XLE system), the
coverage of this system remains poor, therefore the
overall f-score remains poor. One solution is to build
a hybrid two-pass system. During the first pass all
sentences are pre-parsed by Bikel and the bracketed
output is parsed by the XLE non-fragment gram-
mar. In the second pass, the sentences that were
not parsed during the first pass are parsed with the
XLE fragment grammar. We carried out a number
of experiments with hybrid systems and the results
are given in Table 6.
The results show that again labeled brackets re-
sult in a statistically significant increase in f-score,
although the time taken is almost the same as the
XLE fragment grammar alone. Coverage increases
by 1 sentence. Using unlabeled brackets results in
3 additional sentences receiving parses, and parsing
time is improved by ?12%; however the increase in
f-score is not statistically significant.
Table 7 gives the results for hybrid systems with
pruning using labeled brackets. The more pruning
that the Bikel parser does, the faster the system,
but the quality of the f-structures begins to deteri-
70
XLE Bikel-XLE hybrid Bikel-XLE hybrid
(frag) (labeled) (unlabeled)
Total Parsing Time 1143 1121 1001
Total XLE Parses (/140) 135 136 138
F-Score of Subset 78.72 79.85 79.51
Overall F-Score 76.13 77.61 *78.28
Table 6: Hybrid systems compared to the XLE fragment grammar alone
XLE Bikel-XLE hybrid Bikel-XLE hybrid Bikel-XLE hybrid
(frag) (level 1) (level 2) (level 3)
Total Parsing Time 1143 918 920 885
Total XLE Parses (/140) 135 136 136 136
F-Score of Subset 78.72 79.85 79.79 79.76
Overall F-Score 76.13 77.61 77.55 77.53
Table 7: Hybrid systems with pruning compared to the XLE fragment grammar alone
orate. The best system is the Bikel-XLE hybrid sys-
tem with labeled brackets and pruning level 1. This
system achieves a statistically significant increase in
f-score over the XLE fragment grammar alone, de-
creases the time taken to parse by almost 20% and
increases coverage by 1 sentence. Therefore, we
chose this system to perform our final evaluation
against the PARC 700 Dependency Bank.
6 Evaluation against the PARC 700
We evaluated the system that performs best on the
development set against the 560-sentence test set of
the PARC 700 Dependency Bank. The results are
given in Table 8. The hybrid system achieves an
18% decrease in parsing time, a slight improvement
in coverage of 0.9%, and a 1.12% improvement in
overall f-structure quality.
XLE Bikel-XLE hybrid
(frag) (labeled, prune 1)
Total Parsing Time 4967 4077
Total XLE Parses (/560) 537 542
F-Score of Subset 80.13 80.63
Overall F-Score 77.04 78.16
Table 8: PARC 700 evaluation of the Hybrid system
compared to the XLE fragment grammar alone
7 Conclusions
We successfully used a state-of-the-art probabilistic
parser in combination with a hand-crafted system to
improve parsing time while maintaining the quality
of the output produced. Our hybrid system consists
of two phases. During phase one, pre-processed, to-
kenized text is parsed with a retrained Bikel parser.
We use the labeled brackets in the output to constrain
the c-structures generated by the XLE parsing sys-
tem. In the second phase, we use the XLE fragment
grammar to parse any remaining sentences that have
not received a parse in the first phase.
Given the slight increase in overall f-score per-
formance, the speed up in parsing time (?18%) can
justify more complicated processing architecture for
some applications.5 The main disadvantage of the
current system is that the input to the Bikel parser
needs to be tokenized, whereas XLE processes raw
text. One solution to this is to use a state-of-the-art
probabilistic parser that accepts untokenized input
(such as Charniak and Johnson, 2005) and retrain it
as described in Section 4.
Kaplan et al (2004) compared time and accuracy
of a version of the Collins parser tuned to maximize
speed and accuracy to an earlier version of the XLE
parser. Although the XLE parser was more accu-
rate, the parsing time was a factor of 1.49 slower
(time converting Collins trees to dependencies was
not counted in the parse time; time to produce f-
structures from c-structures was counted in the XLE
parse time). The hybrid system here narrows the
speed gap while maintaining greater accuracy.
The original hope behind using the brackets to
constrain the XLE c-structure generation was that
5For example, in massive data applications, if the parsing
task takes 30 days, reducing this by 18% saves more than 5
days.
71
the brackets would force the XLE to choose only
one tree. However, the brackets were sometimes
ambiguous, and sometimes more than one valid tree
was found. In the final evaluation against the PARC
700 test set, the average number of optimal solutions
was 4.05; so the log-linear disambiguation mod-
ule still had to chose the most probable f-structure.
However, this is considerably less to choose from
than the average of 341 optimal solutions produced
by the XLE fragment grammar for the same sen-
tences when unbracketed.
Based on the results of this experiment we have
integrated a statistical component into the XLE
parser itself. With this architecture the packed c-
structure trees are pruned before unification with-
out needing to preprocess the input text. The XLE
c-structure pruning results in a ?30% reduction in
parse time on the Wikipedia with little loss in preci-
sion. We hope to report on this in the near future.
Acknowledgments
The research in this paper was partly funded by Sci-
ence Foundation Ireland grant 04/BR/CS0370.
References
Srinivas Bangalore and Aravind K. Joshi. 1999. Su-
pertagging: An approach to alsmost parsing. Com-
putational Linguistics, 25(2):237?265.
Dan Bikel. Design of a Multi-lingual, Parallel-processing
Statistical Parsing Engine. In Proceedings of HLT,
YEAR = 2002, pages = 24?27, address = San Diego,
CA,.
Miriam Butt, Helge Dyvik, Tracy Holloway King, Hi-
roshi Masuichi, and Christian Rohrer. 2002. The Par-
allel Grammar Project. In Proceedings of Workshop
on Grammar Engineering and Evaluation, pages 1?7,
Taiwan.
Aoife Cahill, Martin Forst, Michael Burke, Mairead Mc-
Carthy, Ruth O?Donovan, Christian Rohrer, Josef van
Genabith, and Andy Way. 2005. Treebank-based
acquisition of multilingual unification grammar re-
sources. Journal of Research on Language and Com-
putation, pages 247?279.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-Fine n-Best Parsing and MaxEnt Discriminative
Reranking. In Proceedings of ACL, pages 173?180,
Ann Arbor, Michigan.
Eugene Charniak. 2000. A maximum entropy inspired
parser. In Proceedings of NAACL, pages 132?139,
Seattle, WA.
Stephen Clark and James R. Curran. 2004. The Impor-
tance of Supertagging for Wide-Coverage CCG Pars-
ing . In Proceedings of COLING, pages 282?288,
Geneva, Switzerland, Aug 23?Aug 27. COLING.
Richard Crouch, Ron Kaplan, Tracy Holloway King, and
Stefan Riezler. 2002. A comparison of evaluation
metrics for a broad coverage parser. In Proceedings of
the LREC Workshop: Beyond PARSEVAL, pages 67?
74, Las Palmas, Canary Islands, Spain.
Ron Kaplan and Joan Bresnan. 1982. Lexical Functional
Grammar, a Formal System for Grammatical Repre-
sentation. In Joan Bresnan, editor, The Mental Repre-
sentation of Grammatical Relations, pages 173?281.
MIT Press, Cambridge, MA.
Ron Kaplan, Stefan Riezler, Tracy Holloway King,
John T. Maxwell, Alexander Vasserman, and Richard
Crouch. 2004. Speed and Accuracy in Shallow and
Deep Stochastic Parsing. In Proceedings of HLT-
NAACL, pages 97?104, Boston, MA.
Tracy Holloway King, Richard Crouch, Stefan Riezler,
Mary Dalrymple, and Ron Kaplan. 2003. The PARC
700 dependency bank. In Proceedings of LINC, pages
1?8, Budapest, Hungary.
Alexandra Kinyon. 2000. Hypertags. In Proceedings of
COLING, pages 446?452, Saarbru?cken.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.
2007. Efficient HPSG Parsing with Supertagging and
CFG-filtering. In Proceedings of IJCAI, pages 1671?
1676, India.
John T. Maxwell and Ronald M. Kaplan. 1993. The
interface between phrasal and functional constraints.
Computational Linguistics, 19(4):571?590.
Takashi Ninomiya, Takuya Matsuzaki, Yoshimasa Tsu-
ruoka, Yusuke Miyao, and Jun?ichi Tsujii. 2006.
Extremely Lexicalized Models for Accurate and Fast
HPSG Parsing. In Proceedings of EMNLP, pages
155?163, Australia.
Eric W. Noreen. 1989. Computer Intensive Methods
for Testing Hypotheses: An Introduction. Wiley, New
York.
Adwait Ratnaparkhi. 1996. A Maximum Entropy Part-
Of-Speech Tagger. In Proceedings of EMNLP, pages
133?142, Philadelphia, PA.
Stefan Riezler, Tracy King, Ronald Kaplan, Richard
Crouch, John T. Maxwell, and Mark Johnson. 2002.
Parsing the Wall Street Journal using a Lexical-
Functional Grammar and Discriminative Estimation
Techniques. In Proceedings of ACL, pages 271?278,
Philadelphia, PA.
72
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 112?120,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Human Evaluation of a German Surface Realisation Ranker
Aoife Cahill
Institut fu?r Maschinelle Sprachverarbeitung (IMS)
University of Stuttgart
70174 Stuttgart, Germany
aoife.cahill@ims.uni-stuttgart.de
Martin Forst
Palo Alto Research Center
3333 Coyote Hill Road
Palo Alto, CA 94304, USA
mforst@parc.com
Abstract
In this paper we present a human-based
evaluation of surface realisation alterna-
tives. We examine the relative rankings of
naturally occurring corpus sentences and
automatically generated strings chosen by
statistical models (language model, log-
linear model), as well as the naturalness of
the strings chosen by the log-linear model.
We also investigate to what extent preced-
ing context has an effect on choice. We
show that native speakers do accept quite
some variation in word order, but there are
also clearly factors that make certain real-
isation alternatives more natural.
1 Introduction
An important component of research on surface
realisation (the task of generating strings for a
given abstract representation) is evaluation, espe-
cially if we want to be able to compare across sys-
tems. There is consensus that exact match with
respect to an actually observed corpus sentence is
too strict a metric and that BLEU score measured
against corpus sentences can only give a rough im-
pression of the quality of the system output. It is
unclear, however, what kind of metric would be
most suitable for the evaluation of string realisa-
tions, so that, as a result, there have been a range of
automatic metrics applied including inter alia ex-
act match, string edit distance, NIST SSA, BLEU,
NIST, ROUGE, generation string accuracy, gener-
ation tree accuracy, word accuracy (Bangalore et
al., 2000; Callaway, 2003; Nakanishi et al, 2005;
Velldal and Oepen, 2006; Belz and Reiter, 2006).
It is not always clear how appropriate these met-
rics are, especially at the level of individual sen-
tences. Using automatic evaluation metrics cannot
be avoided, but ideally, a metric for the evaluation
of realisation rankers would rank alternative real-
isations in the same way as native speakers of the
language for which the surface realisation system
is developed, and not only globally, but also at the
level of individual sentences.
Another major consideration in evaluation is
what to take as the gold standard. The easiest op-
tion is to take the original corpus string that was
used to produce the abstract representation from
which we generate. However, there may well be
other realisations of the same input that are as
suitable in the given context. Reiter and Sripada
(2002) argue that while we should take advantage
of large corpora in NLG, we also need to take care
that we do not introduce errors by learning from
incorrect data present in corpora.
In order to better understand what makes good
evaluation data (and metrics), we designed and im-
plemented an experiment in which human judges
evaluated German string realisations. The main
aims of this experiment were: (i) to establish how
much variation in German word order is accept-
able for human judges, (ii) to find an automatic
evaluation metric that mirrors the findings of the
human evaluation, (iii) to provide detailed feed-
back for the designers of the surface realisation
ranking model and (iv) to establish what effect
preceding context has on the choice of realisation.
In this paper, we concentrate on points (i) and (iv).
The remainder of the paper is structured as fol-
lows: In Section 2 we outline the realisation rank-
ing system that provided the data for the experi-
ment. In Section 3 we outline the design of the
experiment and in Section 4 we present our find-
ings. In Section 5 we relate this to other work and
finally we conclude in Section 6.
2 A Realisation Ranking System for
German
We take the realisation ranking system for German
described in Cahill et al (2007) and present the
output to human judges. One goal of this series
of experiments is to examine whether the results
112
based on automatic evaluation metrics published
in that paper are confirmed in an evaluation by hu-
mans. Another goal is to collect data that will al-
low us and other researchers1 to explore more fine-
grained and reliable automatic evaluation metrics
for realisation ranking.
The system presented by Cahill et al (2007)
ranks the strings generated by a hand-crafted
broad-coverage Lexical Functional Grammar
(Bresnan, 2001) for German (Rohrer and Forst,
2006) on the basis of a given input f-structure.
In these experiments, we use f-structures from
their held-out and test sets, of which 96% can
be associated with surface realisations by the
grammar. F-structures are attribute-value ma-
trices representing grammatical functions and
morphosyntactic features; roughly speaking,
they are predicate-argument structures. In LFG,
f-structures are assumed to be a crosslinguistically
relatively parallel syntactic representation level,
alongside the more surface-oriented c-structures,
which are context-free trees. Figure 1 shows
the f-structure2 associated with TIGER Corpus
sentence 8609, glossed in (1), as well as the 4
string realisations that the German LFG generates
from this f-structure. The LFG is reversible,
i.e. the same grammar is used for parsing as for
generation. It is a hand-crafted grammar, and
has been carefully constructed to only parse (and
therefore generate) grammatical strings.3
(1) Williams
Williams
war
was
in
in
der
the
britischen
British
Politik
politics
a?u?erst
extremely
umstritten.
controversial.
?Williams was extremely controversial in British
politics.?
The ranker consists of a log-linear model that
is based on linguistically informed structural fea-
tures as well as a trigram language model, whose
1The data is available for download from
http://www.ims.uni-stuttgart.de/projekte/pargram/geneval/data/
2Note that only grammatical functions are displayed;
morphosyntactic features are omitted due to space con-
straints. Also note that the discourse function TOPIC was
ignored in generation.
3A ranking mechanism based on so-called optimality
marks can lead to a certain ?asymmetry? between parsing and
generation in the sense that not all sentences that can be as-
sociated with a certain f-structure are necessarily generated
from this same f-structure. E.g. the sentence Williams war
a?u?erst umstritten in der britischen Politik. can be parsed
into the f-structure in Figure 1, but it is not generated because
an optimality mark penalizes the extraposition of PPs to the
right of a clause. Only few optimality marks were used in the
process of generating the data for our experiments, so that the
bias they introduce should not be too noticeable.
score is integrated into the model simply as an ad-
ditional feature. The log-linear model is trained on
corpus data, in this case sentences from the TIGER
Corpus (Brants et al, 2002), for which f-structures
are available; the observed corpus sentences are
considered as references whose probability is to
be maximised during the training process.
The output of the realisation ranker is evalu-
ated in terms of exact match and BLEU score,
both measured against the actually observed cor-
pus sentences. In addition to the figures achieved
by the ranker, the corresponding figures achieved
by the employed trigram language model on its
own are given as a baseline, and the exact match
figure of the best possible string selection is given
as an upper bound.4 We summarise these figures
in Table 1.
Exact Match BLEU score
Language model 27% 0.7306
Log-linear model 37% 0.7939
Upper bound 62% ?
Table 1: Results achieved by trigram LM ranker
and log-linear model ranker in Cahill et al (2007)
By means of these figures, Cahill et al (2007)
show that a log-linear model based on structural
features and a language model score performs con-
siderably better realisation ranking than just a lan-
guage model. In our experiments, presented in de-
tail in the following section, we examine whether
human judges confirm this and how natural and/or
acceptable the selection performed by the realisa-
tion ranker under consideration is for German na-
tive speakers.
3 Experiment Design
The experiment was divided into three parts. Each
part took between 30 and 45 minutes to complete,
and participants were asked to leave some time
(e.g. a week) between each part. In total, 24 par-
ticipants completed the experiment. All were na-
tive German speakers (mostly from South-Western
Germany) and almost all had a linguistic back-
ground. Table 2 gives a breakdown of the items
in each part of the experiment.5
4The observed corpus sentence can be (re)generated from
the corresponding f-structure for only 62% of the sentences
used, usually because of differences in punctuation. Hence
this exact match upper bound. An upper bound in terms
of BLEU score cannot be computed because BLEU score is
computed on entire corpora rather than individual sentences.
5Experiments 3a and 3b contained the same items as ex-
periments 1a and 1b.
113
"Williams war in der britischen Politik ?u?erst umstritten."
'sein<[378:umstritten]>[1:Williams]'PRED
'Williams'PRED1SUBJ
'umstritten<[1:Williams]>'PRED
[1:Williams]SUBJ
'?u?erst'PRED274ADJUNCT378
XCOMP-PRED
'in<[115:Politik]>'PRED
'Politik'PRED
'britisch<[115:Politik]>'PRED
[115:Politik]SUBJ171ADJUNCT
'die'PREDDETSPEC115
OBJ
88
ADJUNCT
[1:Williams]TOPIC65
Williams war in der britischen Politik a?u?erst umstritten.
In der britischen Politik war Williams a?u?erst umstritten.
?Au?erst umstritten war Williams in der britischen Politik.
?Au?erst umstritten war in der britischen Politik Williams.
Figure 1: F-structure associated with (1) and strings generated from it.
Exp 1a Exp 1b Exp 2
Num. items 44 52 41
Avg. sent length 14.4 12.1 9.4
Table 2: Statistics for each experiment part
3.1 Part 1
The aim of part 1 of the experiment was twofold.
First, to identify the relative rankings of the sys-
tems evaluated in Cahill et al (2007) according to
the human judges, and second to evaluate the qual-
ity of the strings as chosen by the log-linear model
of Cahill et al (2007). To these ends, part 1 was
further subdivided into two tasks: 1a and b.
Task 1a: During the first task, participants were
presented with alternative realisations for an input
f-structure (but not shown the original f-structure)
and asked to rank them in order of how natural
sounding they were, 1 being the best and 3 be-
ing the worst.6 Each item contained three alter-
natives, (i) the original string found in TIGER, (ii)
the string chosen as most likely by the trigram lan-
guage model, and (iii) the string chosen as most
likely by the log-linear model. Only items where
each system chose a different alternative were cho-
sen from the evaluation data of Cahill et al (2007).
The three alternatives were presented in random
order for each item, and the items were presented
in random order for each participant. Some items
were presented randomly to participants more than
6Joint rankings were not allowed, i.e. the participants
were forced to make strict ranking decisions, and in hindsight
this may have introduced some noise into the data.
once as a sanity check, and in total for Part 1a, par-
ticipants made 52 ranking judgements on 44 items.
Figure 2 shows a screen shot of what the partici-
pant was presented with for this task.
Task 1b: In the second task of part 1, partic-
ipants were presented with the string chosen by
the log-linear model as being the most likely and
asked to evaluate it on a scale from 1 to 5 on how
natural sounding it was, 1 being very unnatural
or marked and 5 being completely natural. Fig-
ure 3 shows a screen shot of what the participant
saw during the experiment. Again some random
items were presented to the participant more than
once, and the items themselves were presented in
random order. In total, the participants made 58
judgements on 52 items.
3.2 Part 2
In the second part of the experiment, participants
were presented between 4 and 8 alternative sur-
face realisations for an input f-structure, as well
as some preceding context. This preceding con-
text was automatically determined using informa-
tion from the export release of the TIGER treebank
and was not hand-checked for relevance.7 The par-
ticipants were then asked to choose the realisation
that they felt fit best given the preceding sentences.
7The export release of the TIGER treebank includes an
article ID for each sentence. Unfortunately, this is not com-
pletely reliable for determining relevant context, since an ar-
ticle can also contain several short news snippets which are
completely unrelated. Paragraph boundaries are not marked.
This leads to some noise, which unfortunately is difficult to
measure objectively
114
Figure 2: Screenshot of Part 1a of the Experiment
Figure 3: Screenshot of Part 1b of the Experiment
Total Average
Rank 1 Rank 2 Rank 3 Rank
Original String 817 366 65 1.40
LL String 303 593 352 2.04
LM String 128 289 831 2.56
Table 3: Task 1a: Ranks for each system
The items were presented in random order, and the
list of alternatives were presented in random order
to each participant. Some items were randomly
presented more than once, resulting in 50 judge-
ments on 41 items. Figure 4 shows a screen shot
of what the participant saw.
3.3 Part 3
Part 3 of the experiment was identical to Part 1,
except that now, rather than the participants being
presented with sentences in isolation, they were
given some preceding context. The context was
determined automatically, in the same way as in
Part 2. The items themselves were the same as in
Part 1. The aim of this part of the experiment was
to see what effect preceding context had on judge-
ments.
4 Results
In this section we present the result and analysis
of the experiments outlined above.
4.1 How good were the strings?
The data collected in Experiment 1a showed the
overall human relative ranking of the three sys-
tems. We calculate the total numbers of each
rank for each system. Table 3 summarises the re-
sults. The original string is the string found in the
Figure 5: Task 1b: Naturalness scores for strings
chosen by log-linear model, 1=worst
TIGER Corpus, the LM String is the string cho-
sen as being most likely by the trigram language
model and the LL String is the string chosen as
being most likely by the log-linear model.
Table 3 confirms the overall relative rankings
of the three systems as determined using BLEU
scores. The original TIGER strings are ranked best
(average 1.4), the strings chosen by the log-linear
model are ranked better than the strings chosen by
the language model (average 2.65 vs 2.04).
In Experiment 1b, the aim was to find out how
acceptable the strings chosen by the log-linear
model were, although they were not the same as
the original string. Figure 5 summarises the data.
The graph shows that the majority of strings cho-
sen by the log-linear model ranked very highly on
the naturalness scale.
4.2 Did the human judges agree with the
original authors?
In Experiment 2, the aim was to find out how of-
ten the human judges chose the same string as the
original author (given alternatives generated by the
LFG grammar). Most items had between 4 and 6
alternative strings. In 70% of all items, the human
judges chose the same string as the original au-
thor. However, the remaining 30% of the time, the
human judges picked an alternative as being the
115
Figure 4: Screenshot of Part 2 of the Experiment
most fitting in the given context.8 This suggests
that there is quite some variation in what native
German speakers will accept, but that this varia-
tion is by no means random, as indicated by 70%
of choices being the same string as the original au-
thor?s.
Figure 6 shows for each bin of possible alterna-
tives, the percentage of items with a given num-
ber of choices made. For example, for the items
with 4 possible alternatives, over 70% of the time,
the judges chose between only 2 of them. For the
items with 5 possible alternatives, in 10% of those
items the human judges chose only 1 of those al-
ternatives; in 30% of cases, the human judges all
chose the same 2 solutions, and for the remain-
ing 60% they chose between only 3 of the 5 pos-
sible alternatives. These figures indicate that al-
though judges could not always agree on one best
string, often they were only choosing between 2 or
3 of the possible alternatives. This suggests that,
on the one hand, native speakers do accept quite
some variation, but that, on the other hand, there
are clearly factors that make certain realisation al-
ternatives more preferable than others.
Figure 6: Exp 2: Number of Alternatives Chosen
8Recall that almost all strings presented to the judges were
grammatical.
The graph in Figure 6 shows that only in two
cases did the human judges choose from among
all possible alternatives. In one case, there were 4
possible alternatives and in the other 6. The origi-
nal sentence that had 4 alternatives is given in (2).
The four alternatives that participants were asked
to choose from are given in Table 4, with the fre-
quency of each choice. The original sentence that
had 6 alternatives is given in (3). The six alterna-
tives generated by the grammar and the frequen-
cies with which they were chosen is given in Table
5.
(2) Die
The
Brandursache
cause of fire
blieb
remained
zuna?chst
initially
unbekannt.
unknown.
?The cause of the fire remained unknown initially.?
Alternative Freq.
Zuna?chst blieb die Brandursache unbekannt. 2
Die Brandursache blieb zuna?chst unbekannt. 24
Unbekannt blieb die Brandursache zuna?chst. 1
Unbekannt blieb zuna?chst die Brandursache. 1
Table 4: The 4 alternatives given by the grammar
for (2) and their frequencies
Tables 4 and 5 tell different stories. On the one
hand, although each of the 4 alternatives was cho-
sen at least once from Table 4, there is a clear pref-
erence for one string (and this is also the origi-
nal string from the TIGER Corpus). On the other
hand, there is no clear preference9 for any one of
the alternatives in Table 5, and, in fact, the alterna-
tive that was selected most frequently by the par-
ticipants is not the original string. Interestingly,
out of the 41 items presented to participants, the
original string was chosen by the majority of par-
ticipants in 36 cases. Again, this confirms the
hypothesis that there is a certain amount of ac-
ceptable variation for native speakers but there are
clear preferences for certain strings over others.
9Although it is clear that alternative 2 is dispreferred.
116
(3) Die
The
Unternehmensgruppe
group of companies
Tengelmann
Tengelmann
fo?rdert
assists
mit
with
einem
a
sechsstelligen
6-figure
Betrag
sum
die
the
Arbeit
work
im
in
brandenburgischen
of-Brandenburg
Biospha?renreservat
biosphere reserve
Schorfheide.
Schorfheide.
?The Tengelmann group of companies is supporting the work at the biosphere reserve in Schorfheide, Brandenburg,
with a 6-figure sum.?
Alternative Freq.
Mit einem sechsstelligen Betrag fo?rdert die Unternehmensgruppe Tengelmann die Arbeit im brandenburgischen
Biospha?renreservat Schorfheide. 7
Mit einem sechsstelligen Betrag fo?rdert die Arbeit im brandenburgischen Biospha?renreservat Schorfheide
die Unternehmensgruppe Tengelmann. 1
Die Arbeit im brandenburgischen Biospha?renreservat Schorfheide fo?rdert die Unternehmensgruppe Tengelmann
mit einem sechsstelligen Betrag. 4
Die Arbeit im brandenburgischen Biospha?renreservat Schorfheide fo?rdert mit einem sechsstelligen Betrag
die Unternehmensgruppe Tengelmann. 5
Die Unternehmensgruppe Tengelmann fo?rdert die Arbeit im brandenburgischen Biospha?renreservat Schorfheide
mit einem sechsstelligen Betrag. 5
Die Unternehmensgruppe Tengelmann fo?rdert mit einem sechsstelligen Betrag die Arbeit im brandenburgischen
Biospha?renreservat Schorfheide. 5
Table 5: The 6 alternatives given by the grammar for (3) and their frequencies
4.3 Effects of context
As explained in Section 3.1, Part 3 of our exper-
iment was identical to Part 1, except that the par-
ticipants could see some preceding context. The
aim of this part was to investigate to what extent
discourse factors influence the way in which hu-
man judges evaluate the output of the realisation
ranker. In Task 3a, we expected the original strings
to be ranked (even) higher in context than out of
context; consequently, the ranks of the realisations
selected by the log-linear and the language model
would have to go down. With respect to Task 3b,
we had no particular expectation, but were just in-
terested in seeing whether some preceding context
would affect the evaluation results for the strings
selected as most probable by the log-linear model
ranker in any way.
Table 6 summarises the results of Task 3a. It
shows that, at least overall, our expectation that the
original corpus sentences would be ranked higher
within context than out of context was not borne
out. Actually, they were ranked a bit lower than
they were when presented in isolation, and the
only realisations that are ranked slightly higher
overall are the ones selected by the trigram LM.
The overall results of Task 3b are presented in
Figure 7. Interestingly, although we did not ex-
pect any particular effect of preceding context on
the way the participants would rate the realisa-
tions selected by the log-linear model, the natu-
ralness scores were higher in the condition with
context (Task 3b) than in the one without context
Total Average
Rank 1 Rank 2 Rank 3 Rank
Original String 810 365 71 1.41
(-7) (-1) (+6) (+0.01)
LL String 274 615 357 2.07
(-29) (+22) (+5) (+0.03)
LM String 162 266 818 2.53
(+34) (-23) (-13) (-0.03)
Table 6: Task 3a: Ranks for each system (com-
pared to ranks in Task 1a)
(Task 1b). One explanation might be that sen-
tences in some sort of default order are generally
rated higher in context than out of context, simply
because the context makes sentences less surpris-
ing.
Since, contrary to our expectations, we could
not detect a clear effect of context in the overall re-
sults of Task 3a, we investigated how the average
ranks of the three alternatives presented for indi-
vidual items differ between Task 1a and Task 3a.
An example of an original corpus sentence which
many participants ranked higher in context than in
isolation is given in (4a.). The realisations selected
by the the log-linear model and the trigram LM are
given in (4b.) and (4c.) respectively, and the con-
text shown to the participants is given above these
alternatives. We believe that the context has this
effect because it prepares the reader for the struc-
ture with the sentence-initial predicative partici-
ple entscheidend; usually, these elements appear
rather in clause-final position.
In contrast, (5a) is an example of a corpus
117
(4) -2 Betroffen
Concerned
sind
are
die
the
Antibabypillen
contraceptive pills
Femovan,
Femovan,
Lovelle,
Lovelle,
[...]
[...],
und
and
Dimirel.
Dimirel.
-1 Das
The
Bundesinstitut
federal institute
schlie?t
excludes
nicht
not
aus, da?
that
sich die
the
Thrombose-Warnung
thrombosis warning
als
as
grundlos
unfounded
erweisen
turn out
ko?nnte.
could.
a. Entscheidend
Decisive
sei
is
die
the
[...]
[...]
abschlie?ende
final
Bewertung,
evaluation,
sagte
said
Ju?rgen
Ju?rgen
Beckmann
Beckmann
vom
of the
Institut
institute
dem
the
ZDF.
ZDF.
b. Die [...] abschlie?ende Bewertung sei entscheidend, sagte Ju?rgen Beckmann vom Institut dem ZDF.
c. Die [...] abschlie?ende Bewertung sei entscheidend, sagte dem ZDF Ju?rgen Beckmann vom Institut.
(5) -2 Im
In the
konkreten
concrete
Fall
case
darf
may
der
the
Kurde
Kurd
allerdings
however
trotz
despite
der
the
Entscheidung
decision
der
of the
Bundesrichter
federal judges
nicht
not
in
to
die
the
Tu?rkei
Turkey
abgeschoben
deported
werden,
be
weil
because
ihm
him
dort
there
nach
according to
den
the
Feststellungen
conclusions
der
of the
Vorinstanz
court of lower instance
politische
political
Verfolgung
persecution
droht.
threatens.
-1 Es
It
besteht
exists
Abschiebeschutz
deportation protection
nach
according to
dem
the
Ausla?ndergesetz.
foreigner law.
a. Der
The
9.
9th
Senat
senate
[...]
[...]
a?u?erte
expressed
sich
itself
in
in
seiner
its
Entscheidung
decision
nicht
not
zur
to the
Verfassungsgema??heit
constitutionality
der
of the
Drittstaatenregelung.
third-country rule.
b. In seiner Entscheidung a?u?erte sich der 9. Senat [...] nicht zur Verfassungsgema??heit der Drittstaatenregelung.
c. Der 9. Senat [...] a?u?erte sich in seiner Entscheidung zur Verfassungsgema??heit der Drittstaatenregelung nicht.
Figure 7: Tasks 1b and 3b: Naturalness scores
for strings chosen by log-linear model, presented
without and with context
sentence which our participants tended to rank
lower in context than in isolation. Actually, the
human judges preferred the realisation selected
by the trigram LM to the original sentence and
the realisation chosen by the log-linear model in
both conditions, but this preference was even re-
inforced when context was available. One expla-
nation might be that the two preceding sentences
are precisely about the decision to which the ini-
tial phrase of variant (5b) refers, which ensures a
smooth flow of the discourse.
4.4 Inter-Annotator Agreement
We measure two types of annotator agreement.
First we measure how well each annotator agrees
with him/herself. This is done by evaluating what
percentage of the time an annotator made the same
choice when presented with the same item choices
(recall that as described in Section 3, a number of
items were presented randomly more than once to
each participant). The results are given in Table 7.
The results show that in between 70% and 74% of
cases, judges make the same decision when pre-
sented with the same data. We found this to be a
surprisingly low number and think that it is most
likely due to the acceptable variation in word or-
der for speakers. Another measure of agreement
is how well the individual participants agree with
each other. In order to establish this, we cal-
culate an average Spearman?s correlation coeffi-
cient (non-parametric Pearson?s correlation coef-
ficient) between each participant for each experi-
ment. The results are summarised in Table 8. Al-
though these figures indicate a high level of inter-
annotator agreement, more tests are required to es-
tablish exactly what these figures mean for each
experiment.
5 Related Work
The work that is most closely related to what is
presented in this paper is that of Velldal (2008). In
118
Experiment Agreement (%)
Part 1a 77.43
Part 1b 71.05
Part 2 74.32
Part 3a 72.63
Part 3b 70.89
Table 7: How often did a participant make the
same choice?
Experiment Spearman coefficient
Part 1a 0.62
Part 1b 0.60
Part 2 0.58
Part 3a 0.61
Part 3b 0.51
Table 8: Inter-Annotator Agreement for each ex-
periment
his thesis several models of realisation ranking are
presented and evaluated against the original cor-
pus text. Chapter 8 describes a small human-based
experiment, where 7 native English speakers rank
the output of 4 systems. One system is the orig-
inal text, another is a randomly chosen baseline,
another is a string chosen by a log-linear model
and the fourth is one chosen by a language model.
Joint rankings were allowed. The results presented
in Velldal (2008) mirror our findings in Exper-
iments 1a and 3a, that native speakers rank the
original strings higher than the log-linear model
strings which are ranked higher than the language
model strings. In both cases, the log-linear mod-
els include the language model score as a feature
in the log-linear model. Nakanishi et al (2005) re-
port that they achieve the best BLEU scores when
they do not include the language model score in
their log-linear model, but they also admit that
their language model was not trained on enough
data.
Belz and Reiter (2006) carry out a comparison
of automatic evaluation metrics against human do-
main experts and human non-experts in the do-
main of weather forecast statements. In their eval-
uations, the NIST score correlated more closely
than BLEU or ROUGE to the human judgements.
They conclude that more than 4 reference texts are
needed for automatic evaluation of NLG systems.
6 Conclusion and Outlook to Future
Work
In this paper, we have presented a human-based
experiment to evaluate the output of a realisation
ranking system for German. We evaluated the
original corpus text, and strings chosen by a lan-
guage model and a log-linear model. We found
that, at a global level, the human judgements mir-
rored the relative rankings of the three system ac-
cording to the BLEU score. In terms of natural-
ness, the strings chosen by the log-linear model
were generally given 4 or 5, indicating that al-
though the log-linear model might not choose the
same string as the original author had written, the
strings it was choosing were mostly very natural
strings.
When presented with all alternatives generated
by the grammar for a given input f-structure, the
human judges chose the same string as the origi-
nal author 70% of the time. In 5 out of 41 cases,
the majority of judges chose a string other than
the original string. These figures show that native
speakers accept some variation in word order, and
so caution should be exercised when using corpus-
derived reference data. The observed acceptable
variation was often linked to information struc-
tural considerations, and further experiments will
be carried out to investigate this relationship be-
tween word order and information structure.
In examining the effect of preceding context, we
found that overall context had very little effect. At
the level of individual sentences, however, clear
tendencies were observed, but there were some
sentences which were judged better in context and
others which were ranked lower. This again indi-
cates that corpus-derived reference data should be
used with caution.
An obvious next step is to examine how well
automatic metrics correlate with the human judge-
ments collected, not only at an individual sen-
tence level, but also at a global level. This can be
done using statistical techniques to correlate the
human judgements with the scores from the auto-
matic metrics. We will also examine the sentences
that were consistently judged to be of poor quality,
so that we can provide feedback to the developers
of the log-linear model in terms of possible addi-
tional features for disambiguation.
Acknowledgments
We are extremely grateful to all of our participants
for taking part in this experiment. This work was
partly funded by the Collaborative Research Cen-
tre (SFB 732) at the University of Stuttgart.
119
References
Srinivas Bangalore, Owen Rambow, and Steve Whit-
taker. 2000. Evaluation metrics for generation. In
Proceedings of the First International Natural Lan-
guage Generation Conference (INLG2000), pages
1?8, Mitzpe Ramon, Israel.
Anja Belz and Ehud Reiter. 2006. Comparing auto-
matic and human evaluation of NLG systems. In
Proceedings of the 11th Conference of the European
Chapter of the Association for Computational Lin-
guistics, pages 313?320, Trento, Italy.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolf-
gang Lezius, and George Smith. 2002. The TIGER
treebank. In Proceedings of the Workshop on Tree-
banks and Linguistic Theories, Sozopol, Bulgaria.
Joan Bresnan. 2001. Lexical-Functional Syntax.
Blackwell, Oxford.
Aoife Cahill, Martin Forst, and Christian Rohrer. 2007.
Stochastic Realisation Ranking for a Free Word Or-
der Language. In Proceedings of the Eleventh Eu-
ropean Workshop on Natural Language Generation,
pages 17?24, Saarbru?cken, Germany, June. DFKI
GmbH. Document D-07-01.
Charles Callaway. 2003. Evaluating Coverage for
Large Symbolic NLG Grammars. In Proceedings
of the 18th International Joint Conference on Artifi-
cial Intelligence (IJCAI 2003), pages 811?817, Aca-
pulco, Mexico.
Hiroko Nakanishi, Yusuke Miyao, and Jun?ichi Tsu-
jii. 2005. Probabilistic models for disambiguation
of an HPSG-based chart generator. In Proceedings
of IWPT 2005.
Ehud Reiter and Somayajulu Sripada. 2002. Should
Corpora Texts Be Gold Standards for NLG? In Pro-
ceedings of INLG-02, pages 97?104, Harriman, NY.
Christian Rohrer and Martin Forst. 2006. Improving
coverage and parsing quality of a large-scale LFG
for German. In Proceedings of the Language Re-
sources and Evaluation Conference (LREC-2006),
Genoa, Italy.
Erik Velldal and Stephan Oepen. 2006. Statistical
ranking in tactical generation. In Proceedings of the
2006 Conference on Empirical Methods in Natural
Language Processing, Sydney, Australia.
Erik Velldal. 2008. Empirical Realization Ranking.
Ph.D. thesis, University of Oslo.
120
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 817?825,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Incorporating Information Status into Generation Ranking
Aoife Cahill and Arndt Riester
Institut fu?r Maschinelle Sprachverarbeitung (IMS)
University of Stuttgart
70174 Stuttgart, Germany
{aoife.cahill,arndt.riester}@ims.uni-stuttgart.de
Abstract
We investigate the influence of informa-
tion status (IS) on constituent order in Ger-
man, and integrate our findings into a log-
linear surface realisation ranking model.
We show that the distribution of pairs of IS
categories is strongly asymmetric. More-
over, each category is correlated with mor-
phosyntactic features, which can be au-
tomatically detected. We build a log-
linear model that incorporates these asym-
metries for ranking German string reali-
sations from input LFG F-structures. We
show that it achieves a statistically signif-
icantly higher BLEU score than the base-
line system without these features.
1 Introduction
There are many factors that influence word order,
e.g. humanness, definiteness, linear order of gram-
matical functions, givenness, focus, constituent
weight. In some cases, it can be relatively straight-
forward to automatically detect these features (i.e.
in the case of definiteness, this is a syntactic prop-
erty). The more complex the feature, the more dif-
ficult it is to automatically detect. It is common
knowledge that information status1 (henceforth,
IS) has a strong influence on syntax and word or-
der; for instance, in inversions, where the subject
follows some preposed element, Birner (1994) re-
ports that the preposed element must not be newer
in the discourse than the subject. We would like
to be able to use information related to IS in the
automatic generation of German text. Ideally, we
would automatically annotate text with IS labels
and learn from this data. Unfortunately, however,
to date, there has been little success in automati-
cally annotating text with IS.
1We take information status to be a subarea of information
structure; the one dealing with varieties of givenness but not
with contrast and focus in the strictest sense.
We believe, however, that despite this shortcom-
ing, we can still take advantage of some of the in-
sights gained from looking at the influence of IS
on word order. Specifically, we look at the prob-
lem from a more general perspective by comput-
ing an asymmetry ratio for each pair of IS cate-
gories. Results show that there are a large num-
ber of pairs exhibiting clear ordering preferences
when co-occurring in the same clause. The ques-
tion then becomes, without being able to auto-
matically detect these IS category pairs, can we,
nevertheless, take advantage of these strong asym-
metric patterns in generation. We investigate the
(automatically detectable) morphosyntactic char-
acteristics of each asymmetric IS pair and inte-
grate these syntactic asymmetric properties into
the generation process.
The paper is structured as follows: Section 2
outlines the underlying realisation ranking system
for our experiments. Section 3 introduces infor-
mation status and Section 4 describes how we ex-
tract and measure asymmetries in information sta-
tus. In Section 5, we examine the syntactic charac-
teristics of the IS asymmetries. Section 6 outlines
realisation ranking experiments to test the integra-
tion of IS into the system. We discuss our findings
in Section 7 and finally we conclude in Section 8.
2 Generation Ranking
The task we are considering is generation rank-
ing. In generation (or more specifically, surface
realisation) ranking, we take an abstract represen-
tation of a sentence (for example, as produced by
a machine translation or automatic summarisation
system), produce a number of alternative string
realisations corresponding to that input and use
some model to choose the most likely string. We
take the model outlined in Cahill et al (2007), a
log-linear model based on the Lexical Functional
Grammar (LFG) Framework (Kaplan and Bres-
nan, 1982). LFG has two main levels of represen-
817
CS 1: ROOT:1458
CProot[std]:1451
DP[std]:906
DPx[std]:903
D[std]:593
die:34
NP:738
N[comm]:693
Beh?rden:85
Cbar:1448
Cbar-flat:1436
V[v,fin]:976
Vx[v,fin]:973
warnten:117
PP[std]:2081
PPx[std]:2072
P[pre]:1013
vor:154
DP[std]:1894
DPx[std]:1956
NP:1952
AP[std,+infl]:1946
APx[std,+infl]:1928
A[+infl]:1039
m?glichen:185
N[comm]:1252
Nachbeben:263
PERIOD:397
.:389
"Die Beh?rden warnten vor m?glichen Nachbeben."
'warnen<[34:Beh?rde], [263:Nachbeben]>'PRED
'Beh?rde'PRED
'die'PREDDETSPEC
CASE nom, NUM pl, PERS 334
SUBJ
'vor<[263:Nachbeben]>'PRED
'Nachbeben'PRED
'm?glich<[263:Nachbeben]>'PRED [263:Nachbeben]SUBJ
attributiveATYPE185
ADJUNCT
CASE dat, NUM pl, PERS 3263
OBJ
154
OBL
MOOD indicative, TENSE pastTNS-ASP
[34:Beh?rde]TOPIC117
Figure 1: An example C(onstituent) and F(unctional) Structure pair for (1)
tation, C(onstituent)-Structure and F(unctional)-
Structure. C-Structure is a context-free tree rep-
resentation that captures characteristics of the sur-
face string while F-Structure is an abstract repre-
sentation of the basic predicate-argument structure
of the string. An example C- and F-Structure pair
for the sentence in (1) is given in Figure 1.
(1) Die
the
Beho?rden
authorities
warnten
warned
vor
of
mo?glichen
possible
Nachbeben.
aftershocks
?The authorities warned of possible aftershocks.?
The input to the generation system is an F-
Structure. A hand-crafted, bi-directional LFG of
German (Rohrer and Forst, 2006) is used to gener-
ate all possible strings (licensed by the grammar)
for this input. As the grammar is hand-crafted,
it is designed only to parse (and therefore) gen-
erate grammatical strings.2 The task of the reali-
sation ranking system is then to choose the most
likely string. Cahill et al (2007) describe a log-
linear model that uses linguistically motivated fea-
tures and improves over a simple tri-gram lan-
guage model baseline. We take this log-linear
model as our starting point.3
2There are some rare instances of the grammar parsing
and therefore also generating ungrammatical output.
3Forst (2007) presents a model for parse disambiguation
that incorporates features such as humanness, definiteness,
linear order of grammatical functions, constituent weight.
Many of these features are already present in the Cahill et
al. (2007) model.
An error analysis of the output of that system
revealed that sometimes ?unnatural? outputs were
being selected as most probable, and that often
information structural effects were the cause of
subtle differences in possible alternatives. For
instance, Example (3) appeared in the original
TIGER corpus with the 2 preceding sentences (2).
(2) Denn ausdru?cklich ist darin der rechtliche Ma?stab
der Vorinstanz, des Sa?chsischen Oberverwaltungs-
gerichtes, besta?tigt worden. Und der besagt: Die
Beteiligung am politischen Strafrecht der DDR, der
Mangel an kritischer Auseinandersetzung mit to-
talita?ren U?berzeugungen rechtfertigen den Ausschluss
von der Dritten Gewalt.
?Because, the legal benchmark has explicitly been con-
firmed by the lower instance, the Saxonian Higher Ad-
ministrative Court. And it indicates: the participation
in the political criminal law of the GDR as well as
deficits regarding the critical debate on totalitarian con-
victions justify an expulsion from the judiciary.?
(3) Man
one
hat
has
aus
out of
der
the
Vergangenheitsaufarbeitung
coming to terms with the past
gelernt.
learnt
?People have learnt from dealing with the past mis-
takes.?
The five alternatives output by the grammar are:
a. Man hat aus der Vergangenheitsaufarbeitung gelernt.
b. Aus der Vergangenheitsaufarbeitung hat man gelernt.
c. Aus der Vergangenheitsaufarbeitung gelernt hat man.
d. Gelernt hat man aus der Vergangenheitsaufarbeitung.
e. Gelernt hat aus der Vergangenheitsaufarbeitung man.
818
The string chosen as most likely by the system of
Cahill et al (2007) is Alternative (b). No mat-
ter whether the context in (2) is available or the
sentence is presented without any context, there
seems to be a preference by native speakers for
the original string (a). Alternative (e) is extremely
marked4 to the point of being ungrammatical. Al-
ternative (c) is also very marked and so is Alterna-
tive (d), although less so than (c) and (e). Alter-
native (b) is a little more marked than the original
string, but it is easier to imagine a preceding con-
text where this sentence would be perfectly appro-
priate. Such a context would be, e.g. (4).
(4) Vergangenheitsaufarbeitung und Abwiegeln sind zwei
sehr unterschiedliche Arten, mit dem Geschehenen
umzugehen.
?Dealing with the mistakes or playing them down are
two very different ways to handle the past.?
If we limit ourselves to single sentences, the
task for the model is then to choose the string that
is closest to the ?default? expected word order (i.e.
appropriate in the most number of contexts). In
this work, we concentrate on integrating insights
from work on information status into the realisa-
tion ranking process.
3 Information Status
The concept of information status (Prince, 1981;
Prince, 1992) involves classifying NP/PP/DP ex-
pressions in texts according to various ways of
their being given or new. It replaces and specifies
more clearly the often vaguely used term given-
ness. The process of labelling a corpus for IS can
be seen as a means of discourse analysis. Different
classification systems have been proposed in the
literature; see Riester (2008a) for a comparison of
several IS labelling schemes and Riester (2008b)
for a new proposal based on criteria from presup-
position theory. In the work described here, we
use the scheme of Riester (2008b). His main theo-
retic assumption is that IS categories (for definites)
should group expressions according to the contex-
tual resources in which their presuppositions find
an antecedent. For definites, the set of main cate-
gory labels found in Table 1 is assumed.
The idea of resolution contexts derives from
the concept of a presupposition trigger (e.g. a
definite description) as potentially establishing an
4By marked, we mean that there are relatively few or spe-
cialised contexts in which this sentence is acceptable.
Context resource IS label
discourse D-GIVEN
context
encyclopedic/ ACCESSIBLE-GENERAL
knowledge
context
environment/ SITUATIVE
situative
context
bridging BRIDGING
context (scenario)
accommodation ACCESSIBLE-
(no context) DESCRIPTION
Table 1: IS classification for definites
anaphoric relation (van der Sandt, 1992) to an en-
tity being available by some means or other. But
there are some expressions whose referent cannot
be identified and needs to be accommodated, com-
pare (5).
(5) [die monatelange Fu?hrungskrise der Hamburger
Sozialdemokraten]ACC-DESC
?the leadership crisis lasting for months among the
Hamburg Social Democrats?
Examples like this one have been mentioned
early on in the literature (e.g. Hawkins (1978),
Clark and Marshall (1981)). Nevertheless, label-
ing schemes so far have neglected this issue, which
is explicitly incorporated in the system of Riester
(2008b).
The status of an expression is ACCESSIBLE-
GENERAL (or unused, following Prince (1981))
if it is not present in the previous discourse but
refers to an entity that is known to the intended
recipent. There is a further differentiation of the
ACCESSIBLE-GENERAL class into generic (TYPE)
and non-generic (TOKEN) items.
An expression is D-GIVEN (or textually evoked)
if and only if an antecedent is available in the
discourse context. D-GIVEN entities are subdi-
vided according to whether they are repetitions of
their antecedent, short forms thereof, pronouns or
whether they use new linguistic material to add in-
formation about an already existing discourse ref-
erent (label: EPITHET). Examples representing a
co-reference chain are shown in (6).
(6) [Angela Merkel]ACC-GEN (first mention) . . . [An-
gela Merkel]D-GIV-REPEATED (second mention) . . .
[Merkel]D-GIV-SHORT . . . [she]D-GIV-PRONOUN . . .
[herself]D-GIV-REFLEXIVE . . . [the Hamburg-born
politician]D-GIV-EPITHET
Indexicals (referring to entities in the environ-
ment context) are labeled as SITUATIVE. Definite
819
items that can be identified within a scenario con-
text evoked by a non-coreferential item receive the
label BRIDGING; compare Example (7).
(7) In
in
Sri Lanka
Sri Lanka
haben
have
tamilische
Tamil
Rebellen
rebels
erstmals
for the first time
einen
an
Luftangriff
airstrike
[gegen
against
die
the
Streitkra?fte]BRIDG
armed forces
geflogen.
flown.
?In Sri Lanka, Tamil rebels have, for the first time, car-
ried out an airstrike against the armed forces.?
In the indefinite domain, a simple classification
along the lines of Table 2 is proposed.
Type IS label
unrelated to context NEW
part-whole relation PARTITIVE
to previous entity
other (unspecified) INDEF-REL
relation to context
Table 2: IS classification for indefinites
There are a few more subdivisions. Table 3,
for instance, contains the labels BRIDGING-CON-
TAINED and PARTITIVE-CONTAINED, going back
to Prince?s (1981:236) ?containing inferrables?.
The entire IS label inventory used in this study
comprises 19 (sub)classes in total.
4 Asymmetries in IS
In order to find out whether IS categories are un-
evenly distributed within German sentences we
examine a corpus of German radio news bulletins
that has been manually annotated for IS (496 an-
notated sentences in total) using the scheme of
Riester (2008b).5
For each pair of IS labels X and Y we count
how often they co-occur in the corpus within a sin-
gle clause. In doing so, we distinguish the num-
bers for ?X preceding Y ? (=A) and ?Y preceding
X? (= B). The larger group is referred to as the
dominant order. Subsequently, we compute a ratio
indicating the degree of asymmetry between the
two orders. If, for instance, the dominant pattern
occurs 20 times (A) and the reverse pattern only 5
times (B), the asymmetry ratio B/A is 0.25.6
5The corpus was labeled by two independent annotators
and the results were compared by a third person who took
the final decision in case of disagreement. An evaluation as
regards inter-coder agreement is currently underway.
6Even if some of the sentences we are learning from are
marked in terms of word order, the ratios allow us to still learn
the predominant order, since the marked order should occur
much less frequently and the ratio will remain low.
Dominant order (: ?before?) B/A Total
D-GIV-PROINDEF-REL 0 19
D-GIV-PROD-GIV-CAT 0.1 11
D-GIV-RELNEW 0.11 31
D-GIV-PROSIT 0.13 17
ACC-DESCINDEF-REL 0.14 24
ACC-DESCACC-GEN-TY 0.19 19
D-GIV-EPIINDEF-REL 0.2 12
D-GIV-REPNEW 0.21 23
D-GIV-PROACC-GEN-TY 0.22 11
ACC-GEN-TOACC-GEN-TY 0.24 42
D-GIV-PROACC-DESC 0.24 46
EXPLNEW 0.25 30
D-GIV-RELD-GIV-EPI 0.25 15
BRIDG-CONTPART-CONT 0.25 15
ACC-DESCEXPL 0.29 27
D-GIV-PROD-GIV-REP 0.29 18
D-GIV-PRONEW 0.29 88
D-GIV-RELACC-DESC 0.3 26
SITEXPL 0.31 17
D-GIV-PROBRIDG-CONT 0.31 21
D-GIV-PROD-GIV-SHORT 0.32 29
. . . . . .
ACC-DESCACC-GEN-TO 0.91 201
SITBRIDG 0.92 23
EXPLACC-DESC 1 12
Table 3: Asymmetric pairs of IS labels
Table 3 gives the top asymmetry pairs down to
a ratio of about 1:3 as well as, down at the bottom,
the pairs that are most evenly distributed. This
means that the top pairs exhibit strong ordering
preferences and are, hence, unevenly distributed
in German sentences. For instance, the ordering
D-GIVEN-PRONOUN before INDEF-REL (top line),
shown in Example (8), occurs 19 times in the ex-
amined corpus while there is no example in the
corpus for the reverse order.7
(8) [Sie]D-GIV-PRO
she
wu?rde
would
auch
also
[bei
at
verringerter
reduced
Anzahl]INDEF-REL
number
jede
every
vernu?nftige
sensible
Verteidigungsplanung
defence planning
sprengen.
blast
?Even if the numbers were reduced it would blow every
sensible defence planning out of proportion.?
5 Syntactic IS Asymmetries
It seems that IS could, in principle, be quite bene-
ficial in the generation ranking task. The problem,
of course, is that we do not possess any reliable
system of automatically assigning IS labels to un-
known text and manual annotations are costly and
time-consuming. As a substitute, we identify a list
7Note that we are not claiming that the reverse pattern is
ungrammatical or impossible, we just observe that it is ex-
tremely infrequent.
820
of morphosyntactic characteristics that the expres-
sions can adopt and investigate how these are cor-
related to our inventory of IS categories.
For some IS labels there is a direct link between
the typical phrases that fall into that IS category,
and the syntactic features that describe it. One
such example is D-GIVEN-PRONOUN, which al-
ways corresponds to a pronoun, or EXPL which
always corresponds to expletive items. Such syn-
tactic markers can easily be identified in the LFG
F-structures. On the other hand, there are many
IS labels for which there is no clear cut syntac-
tic class that describes its typical phrases. Ex-
amples include NEW, ACCESSIBLE-GENERAL or
ACCESSIBLE-DESCRIPTION.
In order to determine whether we can ascertain
a set of syntactic features that are representative
of a particular IS label, we design an inventory of
syntactic features that are found in all types of IS
phrases. The complete inventory is given in Table
5. It is a much easier task to identify these syntac-
tic characteristics than to try and automatically de-
tect IS labels directly, which would require a deep
semantic understanding of the text. We automati-
cally mark up the news corpus with these syntactic
characteristics, giving us a corpus both annotated
for IS and syntactic features.
We can now identify, for each IS label, what the
most frequent syntactic characteristics of that la-
bel are. Some examples and their frequencies are
given in Table 4.
Syntactic feature Count
D-GIVEN-PRONOUN
PERS PRON 39
DA PRON 25
DEMON PRON 19
GENERIC PRON 11
NEW
SIMPLE INDEF 113
INDEF ATTR 53
INDEF NUM 32
INDEF PPADJ 26
INDEF GEN 25
. . .
Table 4: Syntactic characteristics of IS labels
Combining the most frequent syntactic charac-
teristics with the asymmetries presented in Table 3
gives us Table 6.8
8For reasons of space, we are only showing the very top
of the table.
6 Generation Ranking Experiments
Using the augmented set of IS asymmetries,
we design new features to be included into the
original model of Cahill et al (2007). For each
IS asymmetry, we extract all precedence patterns
of the corresponding syntactic features. For
example, from the first asymmetry in Table 6, we
extract the following features:
PERS PRON precedes INDEF ATTR
PERS PRON precedes SIMPLE INDEF
DA PRON precedes INDEF ATTR
DA PRON precedes SIMPLE INDEF
DEMON PRON precedes INDEF ATTR
DEMON PRON precedes SIMPLE INDEF
GENERIC PRON precedes INDEF ATTR
GENERIC PRON precedes SIMPLE INDEF
We extract these patterns for all of the asym-
metric pairs in Table 3 (augmented with syntac-
tic characteristics) that have a ratio >0.4. The
patterns we extract need to be checked for incon-
sistencies because not all of them are valid. By
inconsistencies, we mean patterns of the type X
precedes X, Y precedes Y, and any pat-
tern where the variant X precedes Y as well
as Y precedes X is present. These are all auto-
matically removed from the list of features to give
a total of 130 new features for the log-linear rank-
ing model.
We train the log-linear ranking model on 7759
F-structures from the TIGER treebank. We gen-
erate strings from each F-structure and take the
original treebank string to be the labelled exam-
ple. All other examples are viewed as unlabelled.
We tune the parameters of the log-linear model on
a small development set of 63 sentences, and carry
out the final evaluation on 261 unseen sentences.
The ranking results of the model with the addi-
tional IS-inspired features are given in Table 7.
Exact
Model BLEU Match
(%)
Cahill et al (2007) 0.7366 52.49
New Model (Model 1) 0.7534 54.40
Table 7: Ranking Results for new model with IS-
inspired syntactic asymmetry features.
We evaluate the string chosen by the log-linear
model against the original treebank string in terms
of exact match and BLEU score (Papineni et al,
821
Syntactic feature Type
Definites
Definite descriptions
SIMPLE DEF simple definite descriptions
POSS DEF simple definite descriptions with a possessive determiner
(pronoun or possibly genitive name)
DEF ATTR ADJ definite descriptions with adjectival modifier
DEF GENARG definite descriptions with a genitive argument
DEF PPADJ definite descriptions with a PP adjunct
DEF RELARG definite descriptions including a relative clause
DEF APP definite descriptions including a title or job description
as well as a proper name (e.g. an apposition)
Names
PROPER combinations of position/title and proper name (without article)
BARE PROPER bare proper names
Demonstrative descriptions
SIMPLE DEMON simple demonstrative descriptions
MOD DEMON adjectivally modified demonstrative descriptions
Pronouns
PERS PRON personal pronouns
EXPL PRON expletive pronoun
REFL PRON reflexive pronoun
DEMON PRON demonstrative pronouns (not: determiners)
GENERIC PRON generic pronoun (man ? one)
DA PRON ?da?-pronouns (darauf, daru?ber, dazu, . . . )
LOC ADV location-referring pronouns
TEMP ADV,YEAR Dates and times
Indefinites
SIMPLE INDEF simple indefinites
NEG INDEF negative indefinites
INDEF ATTR indefinites with adjectival modifiers
INDEF CONTRAST indefinites with contrastive modifiers
(einige ? some, andere ? other, weitere ? further, . . . )
INDEF PPADJ indefinites with PP adjuncts
INDEF REL indefinites with relative clause adjunct
INDEF GEN indefinites with genitive adjuncts
INDEF NUM measure/number phrases
INDEF QUANT quantified indefinites
Table 5: An inventory of interesting syntactic characteristics in IS phrases
Label 1 (+ features) Label 2 (+ features) B/A Total
D-GIVEN-PRONOUN INDEF-REL 0 19
PERS PRON 39 INDEF ATTR 23
DA PRON 25 SIMPLE INDEF 17
DEMON PRON 19
GENERIC PRON 11
D-GIVEN-PRONOUN D-GIVEN-CATAPHOR 0.1 11
PERS PRON 39 SIMPLE DEF 13
DA PRON 25 DA PRON 10
DEMON PRON 19
GENERIC PRON 11
D-GIVEN-REFLEXIVE NEW 0.11 31
REFL PRON 54 SIMPLE INDEF 113
INDEF ATTR 53
INDEF NUM 32
INDEF PPADJ 26
INDEF GEN 25
...
Table 6: IS asymmetric pairs augmented with syntactic characteristics
822
2002). We achieve an improvement of 0.0168
BLEU points and 1.91 percentage points in exact
match. The improvement in BLEU is statistically
significant (p < 0.01) using the paired bootstrap
resampling significance test (Koehn, 2004).
Going back to Example (3), the new model
chooses a ?better? string than the Cahill et al
(2007) model. The new model chooses the orig-
inal string. While the string chosen by the Cahill
et al (2007) system is also a perfectly valid sen-
tence, our empirical findings from the news corpus
were that the default order of generic pronoun be-
fore definite NP were more frequent. The system
with the new features helped to choose the original
string, as it had learnt this asymmetry.
Was it just the syntax?
The results in Table 7 clearly show that the new
model is beneficial. However, we want to know
how much of the improvement gained is due to
the IS asymmetries, and how much the syntactic
asymmetries on their own can contribute. To this
end, we carry out a further experiment where we
calculate syntactic asymmetries based on the au-
tomatic markup of the corpus, and ignore the IS
labels completely. Again we remove any incon-
sistent asymmetries and only choose asymmetries
with a ratio of higher than 0.4. The top asymme-
tries are given in Table 8.
Dominant order (: ?before?) B/A Total
BAREPROPERINDEF NUM 0 33
DA PRONINDEF NUM 0 16
DEF PPADJTEMP ADV 0 15
SIMPLE INDEFINDEF QUANT 0 14
PERS PRONINDEF ATTR 0 12
DEF PPADJEXPL PRON 0 12
GENERIC PRONINDEF ATTR 0 12
REFL PRONYEAR 0 11
INDEF PPADJINDEF NUM 0.02 57
DEF APPBAREPROPER 0.03 34
BAREPROPERTEMP ADV 0.04 26
TEMP ADVINDEF NUM 0.04 25
PROPERINDEF GEN 0.05 20
DEF GENARGINDEF ATTR 0.06 18
. . . . . .
Table 8: Purely syntactic asymmetries
For each asymmetry, we create a new feature X
precedes Y. This results in a total of 66 fea-
tures. Of these 30 overlap with the features used
in the above experiment. We do not include the
features extracted in the first attempt in this exper-
iment. The same training procedure is carried out
and we test on the same heldout test set of 261 sen-
tences. The results are given in Table 9. Finally,
we combine the two lists of features and evaluate,
these results are also presented in Table 9.
Exact
Model BLEU Match
(%)
Cahill et al (2007) 0.7366 52.49
Model 1 0.7534 54.40
Synt.-asym.-based Model 0.7419 54.02
Combination 0.7437 53.64
Table 9: Results for ranking model with purely
syntactic asymmetry features
They show that although the syntactic asymme-
tries alone contribute to an improvement over the
baseline, the gain is not as large as when the syn-
tactic asymmetries are constrained to correspond
to IS label asymmetries (Model 1).9 Interest-
ingly, the combination of the lists of features does
not result in an improvement over Model 1. The
difference in BLEU score between the model of
Cahill et al (2007) and the model that only takes
syntactic-based asymmetries into account is not
statistically significant, while the difference be-
tween Model 1 and this model is statistically sig-
nificant (p < 0.05).
7 Discussion
In the work described here, we concentrate only on
taking advantage of the information that is read-
ily available to us. Ideally, we would like to be
able to use the IS asymmetries directly as features,
however, without any means of automatically an-
notating new text with these categories, this is im-
possible. Our experiments were designed to test,
whether we can achieve an improvement in the
generation of German text, without a fully labelled
corpus, using the insight that at least some IS cate-
gories correspond to morphosyntactic characteris-
tics that can be easily identified. We do not claim
to go beyond this level to the point where true IS
labels would be used, rather we attempt to pro-
vide a crude approximation of IS using only mor-
phosyntactic information. To be able to fully auto-
matically annotate text with IS labels, one would
need to supplement the morphosyntactic features
9The difference may also be due to the fewer features used
in the second experiment. However, this emphasises, that
the asymmetries gleaned from syntactic information alone are
not strong enough to be able to determine the prevailing order
of constituents. When we take the IS labels into account, we
are honing in on a particular subset of interesting syntactic
asymmetries.
823
with information about anaphora resolution, world
knowledge, ontologies, and possibly even build
dynamic discourse representations.
We would also like to emphasise that we are
only looking at one sentence at a time. Of course,
there are other inter-sentential factors (not relying
on external resources) that play a role in choosing
the optimal string realisation, for example paral-
lelism or the position of the sentence in the para-
graph or text. Given that we only looked at IS fac-
tors within a sentence, we think that such a sig-
nificant improvement in BLEU and exact match
scores is very encouraging. In future work, we will
look at what information can be automatically ac-
quired to help generation ranking based on more
than one sentence.
While the experiments presented this paper are
limited to a German realisation ranking system,
there is nothing in the methodology that precludes
it from being applied to another language. The IS
annotation scheme is language-independent, and
so all one needs to be able to apply this to another
language is a corpus annotated with IS categories.
We extracted our IS asymmetry patterns from a
small corpus of spoken news items. This corpus
contains text of a similar domain to the TIGER
treebank. Further experiments are required to de-
termine how domain specific the asymmetries are.
Much related work on incorporating informa-
tion status (or information structure) into language
generation has been on spoken text, since infor-
mation structure is often encoded by means of
prosody. In a limited domain setting, Prevost
(1996) describes a two-tiered information struc-
ture representation. During the high level plan-
ning stage of generation, using a small knowl-
edge base, elements in the discourse are automat-
ically marked as new or given. Contrast and fo-
cus are also assigned automatically. These mark-
ings influence the final string generated. We are
focusing on a broad-coverage system, and do not
use any external world-knowledge resources. Van
Deemter and Odijk (1997) annotate the syntac-
tic component from which they are generating
with information about givenness. This informa-
tion is determined by detecting contradictions and
parallel sentences. Pulman (1997) also uses in-
formation about parallelism to predict word or-
der. In contrast, we only look at one sentence
when we approximate information status, future
work will look at cross sentential factors. Endriss
and Klabunde (2000) describe a sentence planner
for German that annotates the propositional in-
put with discourse-related features in order to de-
termine the focus, and thus influence word order
and accentuation. Their system, again, is domain-
specific (generating monologue describing a film
plot) and requires the existence of a knowledge
base. The same holds for Yampolska (2007), who
presents suggestions for generating information
structure in Russian and Ukrainian football re-
ports, using rules to determine parallel structures
for the placement of contrastive accent, following
similar work by Theune (1997). While our paper
does not address the generation of speech / accen-
tuation, it is of course conceivable to employ the
IS annotated radio news corpus from which we de-
rived the label asymmetries (and which also exists
in a spoken and prosodically annotated version) in
a similar task of learning the correlations between
IS labels and pitch accents. Finally, Bresnan et
al. (2007) present work on predicting the dative
alternation in English using 14 features relating to
information status which were manually annotated
in their corpus. In our work, we manually annotate
a small corpus in order to learn generalisations.
From these we learn features that approximate the
generalisations, enabling us to apply them to large
amounts of unseen data without further manual an-
notation.
8 Conclusions
In this paper we presented a novel method of in-
cluding IS into the task of generation ranking.
Since automatic annotation of IS labels them-
selves is not currently possible, we approximate
the IS categories by their syntactic characteristics.
By calculating strong asymmetries between pairs
of IS labels, and establishing the most frequent
syntactic characteristics of these asymmetries, we
designed a new set of features for a log-linear
ranking model. In comparison to a baseline model,
we achieve statistically significant improvement in
BLEU score. We showed that these improvements
were not only due to the effect of purely syntac-
tic asymmetries, but that the IS asymmetries were
what drove the improved model.
Acknowledgments
This work was funded by the Collaborative Re-
search Centre (SFB 732) at the University of
Stuttgart.
824
References
Betty J. Birner. 1994. Information Status and Word
Order: an Analysis of English Inversion. Language,
70(2):233?259.
Joan Bresnan, Anna Cueni, Tatiana Nikitina, and
R. Harald Baayen. 2007. Predicting the Dative Al-
ternation. Cognitive Foundations of Interpretation,
pages 69?94.
Aoife Cahill, Martin Forst, and Christian Rohrer. 2007.
Stochastic Realisation Ranking for a Free Word Or-
der Language. In Proceedings of the Eleventh Eu-
ropean Workshop on Natural Language Generation,
pages 17?24, Saarbru?cken, Germany. DFKI GmbH.
Herbert H. Clark and Catherine R. Marshall. 1981.
Definite Reference and Mutual Knowledge. In Ar-
avind Joshi, Bonnie Webber, and Ivan Sag, editors,
Elements of Discourse Understanding, pages 10?63.
Cambridge University Press.
Kees van Deemter and Jan Odijk. 1997. Context
Modeling and the Generation of Spoken Discourse.
Speech Communication, 21(1-2):101?121.
Cornelia Endriss and Ralf Klabunde. 2000. Planning
Word-Order Dependent Focus Assignments. In Pro-
ceedings of the First International Conference on
Natural Language Generation (INLG), pages 156?
162, Morristown, NJ. Association for Computa-
tional Linguistics.
Martin Forst. 2007. Disambiguation for a Linguis-
tically Precise German Parser. Ph.D. thesis, Uni-
versity of Stuttgart. Arbeitspapiere des Instituts
fu?r Maschinelle Sprachverarbeitung (AIMS), Vol.
13(3).
John A. Hawkins. 1978. Definiteness and Indefinite-
ness: A Study in Reference and Grammaticality Pre-
diction. Croom Helm, London.
Ron Kaplan and Joan Bresnan. 1982. Lexical Func-
tional Grammar, a Formal System for Grammatical
Representation. In Joan Bresnan, editor, The Men-
tal Representation of Grammatical Relations, pages
173?281. MIT Press, Cambridge, MA.
Philipp Koehn. 2004. Statistical Significance Tests for
Machine Translation Evaluation. In Dekang Lin and
Dekai Wu, editors, Proceedings of the Conference
on Empirical Methods in Natural Language Pro-
cessing (EMNLP 2004), pages 388?395, Barcelona.
Association for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic
Evaluation of Machine Translation. In Proceedings
of the 40th Annual Meeting of the Association for
Computational Linguistics (ACL 2002), pages 311?
318, Philadelphia, PA.
Scott Prevost. 1996. An Information Structural Ap-
proach to Spoken Language Generation. In Pro-
ceedings of the 34th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL 1996),
pages 294?301, Morristown, NJ.
Ellen F. Prince. 1981. Toward a Taxonomy of Given-
New Information. In P. Cole, editor, Radical Prag-
matics, pages 233?255. Academic Press, New York.
Ellen F. Prince. 1992. The ZPG Letter: Subjects, Def-
initeness and Information Status. In W. C. Mann
and S. A. Thompson, editors, Discourse Descrip-
tion: Diverse Linguistic Analyses of a Fund-Raising
Text, pages 295?325. Benjamins, Amsterdam.
Stephen G. Pulman. 1997. Higher Order Unification
and the Interpretation of Focus. Linguistics and Phi-
losophy, 20:73?115.
Arndt Riester. 2008a. A Semantic Explication of ?In-
formation Status? and the Underspecification of the
Recipients? Knowledge. In Atle Gr?nn, editor, Pro-
ceedings of Sinn und Bedeutung 12, University of
Oslo.
Arndt Riester. 2008b. The Components of Focus
and their Use in Annotating Information Struc-
ture. Ph.D. thesis, University of Stuttgart. Ar-
beitspapiere des Instituts fu?r Maschinelle Sprachver-
arbeitung (AIMS), Vol. 14(2).
Christian Rohrer and Martin Forst. 2006. Improving
Coverage and Parsing Quality of a Large-Scale LFG
for German. In Proceedings of the Language Re-
sources and Evaluation Conference (LREC 2006),
Genoa, Italy.
Rob van der Sandt. 1992. Presupposition Projection as
Anaphora Resolution. Journal of Semantics, 9:333?
377.
Marie?t Theune. 1997. Goalgetter: Predicting Con-
trastive Accent in Data-to-Speech Generation. In
Proceedings of the 35th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL/EACL
1997), pages 519?521, Madrid. Student paper.
Nadiya Yampolska. 2007. Information Structure in
Natural Language Generation: an Account for East-
Slavic Languages. Term paper. Universita?t des Saar-
landes.
825
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 97?100,
Suntec, Singapore, 4 August 2009.
c?2009 ACL and AFNLP
Correlating Human and Automatic Evaluation of a German Surface
Realiser
Aoife Cahill
Institut f?ur Maschinelle Sprachverarbeitung (IMS)
University of Stuttgart
70174 Stuttgart, Germany
aoife.cahill@ims.uni-stuttgart.de
Abstract
We examine correlations between native
speaker judgements on automatically gen-
erated German text against automatic eval-
uation metrics. We look at a number of
metrics from the MT and Summarisation
communities and find that for a relative
ranking task, most automatic metrics per-
form equally well and have fairly strong
correlations to the human judgements.
In contrast, on a naturalness judgement
task, the General Text Matcher (GTM) tool
correlates best overall, although in gen-
eral, correlation between the human judge-
ments and the automatic metrics was quite
weak.
1 Introduction
During the development of a surface realisation
system, it is important to be able to quickly and au-
tomatically evaluate its performance. The evalua-
tion of a string realisation system usually involves
string comparisons between the output of the sys-
tem and some gold standard set of strings. Typi-
cally automatic metrics from the fields of Machine
Translation (e.g. BLEU) or Summarisation (e.g.
ROUGE) are used, but it is not clear how success-
ful or even appropriate these are. Belz and Reiter
(2006) and Reiter and Belz (2009) describe com-
parison experiments between the automatic eval-
uation of system output and human (expert and
non-expert) evaluation of the same data (English
weather forecasts). Their findings show that the
NIST metric correlates best with the human judge-
ments, and all automatic metrics favour systems
that generate based on frequency. They conclude
that automatic evaluations should be accompanied
by human evaluations where possible. Stent et al
(2005) investigate a number of automatic evalua-
tion methods for generation in terms of adequacy
and fluency on automatically generated English
paraphrases. They find that the automatic metrics
are reasonably good at measuring adequacy, but
not good measures of fluency, i.e. syntactic cor-
rectness.
In this paper, we carry out experiments to corre-
late automatic evaluation of the output of a surface
realisation ranking system for German against hu-
man judgements. We particularly look at correla-
tions at the individual sentence level.
2 Human Evaluation Experiments
The data used in our experiments is the output of
the Cahill et al (2007) German realisation rank-
ing system. That system is couched within the
Lexical Functional Grammar (LFG) grammatical
framework. LFG has two levels of representa-
tion, C(onstituent)-Structure which is a context-
free tree representation and F(unctional)-Structure
which is a recursive attribute-value matrix captur-
ing basic predicate-argument-adjunct relations.
Cahill et al (2007) use a large-scale hand-
crafted grammar (Rohrer and Forst, 2006) to gen-
erate a number of (almost always) grammatical
sentences given an input F-Structure. They show
that a linguistically-inspired log-linear ranking
model outperforms a simple baseline tri-gram lan-
guage model trained on the Huge German Corpus
(HGC), a corpus of 200 million words of newspa-
per and other text.
Cahill and Forst (2009) describe a number of
experiments where they collect judgements from
native speakers about the three systems com-
pared in Cahill et al (2007): (i) the original
corpus string, (ii) the string chosen by the lan-
guage model, and (iii) the string chosen by the
linguistically-inspired log-linear model.
1
We only
take the data from 2 of those experiments since
the remaining experiments would not provide any
1
In all cases, the three strings were different.
97
informative correlations. In the first experiment
that we consider (A), subjects are asked to rank
on a scale from 1?3 (1 being the best, 3 being
the worst) the output of the three systems (joint
rankings were not permitted). In the second ex-
periment (B), subjects were asked to rank on a
scale from 1?5 (1 being the worst, 5 being the
best) how natural sounding the string chosen by
the log-linear model was. The goal of experiment
B was to determine whether the log-linear model
was choosing good or bad alternatives to the orig-
inal string. Judgements on the data were collected
from 24 native German speakers. There were 44
items in Experiment A with an average sentence
length of 14.4, and there were 52 items in Exper-
iment B with an average sentence length of 12.1.
Each item was judged by each native speaker at
least once.
3 Correlation with Automatic Metrics
We examine the correlation between the human
judgements and a number of automatic metrics:
BLEU (Papineni et al, 2001) calculates the number of n-
grams a solution shares with a reference, adjusted by a
brevity penalty. Usually the geometric mean for scores
up to 4-gram are reported.
ROUGE (Lin, 2004) is an evaluation metric designed to eval-
uate automatically generated summaries. It comprises
a number of string comparison methods including n-
gram matching and skip-ngrams. We use the default
ROUGE-L longest common subsequence f-score mea-
sure.
2
GTM General Text Matching (Melamed et al, 2003) calcu-
lates word overlap between a reference and a solution,
without double counting duplicate words. It places less
importance on word order than BLEU.
SED Levenshtein (String Edit) distance
WER Word Error Rate
TER Translation Error Rate (Snover et al, 2006) computes
the number of insertions, deletions, substitutions and
shifts needed to match a solution to a reference.
Most of these metrics come from the Machine
Translation field, where the task is arguably sig-
nificantly different. In the evaluation of a surface
realisation system (as opposed to a complete gen-
eration system), typically the choice of vocabulary
is limited and often the task is closer to word re-
ordering. Many of the MT metrics have methods
2
Preliminary experiments with the skip n-grams per-
formed worse than the default parameters.
Experiment A Experiment B
GOLD LM LL LL
human A (rank 1?3) 1.4 2.55 2.05
human B (scale 1?5) 3.92
BLEU 1.0 0.67 0.72 0.79
ROUGE-L 1.0 0.85 0.78 0.85
GTM 1.0 0.55 0.60 0.74
SED 1.0 0.54 0.61 0.71
WER 0.0 48.04 39.88 28.83
TER 0.0 0.16 0.14 0.11
DEP 100 82.60 87.50 93.11
WDEP 1.0 0.70 0.82 0.90
Table 1: Average scores of each metric for Exper-
iment A data
Sentence Corpus
corr p-value corr p-value
BLEU -0.615 <0.001 -1 0.3333
ROUGE-L -0.644 <0.001 -0.5 1
GTM -0.643 <0.001 -1 0.3333
SED -0.628 <0.001 -1 0.3333
WER 0.623 <0.001 1 0.3333
TER 0.608 <0.001 1 0.3333
Table 2: Correlation between human judgements
for experiment A (rank 1?3) and automatic metrics
for attempting to account for different but equiva-
lent translations of a given source word, typically
by integrating a lexical resource such as WordNet.
Also, these metrics were mostly designed to eval-
uate English output, so it is not clear that they will
be equally appropriate for other languages, espe-
cially freer word order ones, such as German.
The scores given by each metric for the data
used in both experiments are presented in Table 1.
For the Experiment A data, we use the Spearman
rank correlation coefficient to measure the corre-
lation between the human judgements and the au-
tomatic scorers. The results are presented in Table
2 for both the sentence and the corpus level corre-
lations, we also present p-values for statistical sig-
nificance. Since we only have judgements on three
systems, the corpus correlation is not that informa-
tive. Interestingly, the ROUGE-L metric is the only
one that does not rank the output of the three sys-
tems in the same order as the judges. It ranks the
strings chosen by the language model higher than
the strings chosen by the log-linear model. How-
ever, at the level of the individual sentence, the
ROUGE-L metric correlates best with the human
judgements. The GTM metric correlates at about
the same level, but in general there seems to be
little difference between the metrics.
For the Experiment B data we use the Pearson
correlation coefficient to measure the correlation
between the human judgements and the automatic
98
Sentence
Correlation P-Value
BLEU 0.095 0.5048
ROUGE-L 0.207 0.1417
GTM 0.424 0.0017
SED 0.168 0.2344
WER -0.188 0.1817
TER -0.024 0.8646
Table 3: Correlation between human judgements
for experiment B (naturalness scale 1?5) and au-
tomatic metrics
metrics. The results are given in Table 3. Here
we only look at the correlation at the individual
sentence level, since we are looking at data from
only one system. For this data, the GTM met-
ric clearly correlates most closely with the human
judgements, and it is the only metric that has a sta-
tistically significant correlation. BLEU and TER
correlate particularly poorly, with correlation co-
efficients very close to zero.
3.1 Syntactic Metrics
Recently, there has been a move towards more
syntactic, rather than purely string based, evalu-
ation of MT output and summarisation (Hovy et
al., 2005; Owczarzak et al, 2008). The idea is to
go beyond simple string comparisons and evaluate
at a deeper linguistic level. Since most of the work
in this direction has only been carried out for En-
glish so far, we apply the idea rather than a specific
tool to the data. We parse the data from both ex-
periments with a German dependency parser (Hall
and Nivre, 2008) trained on the TIGER Treebank
(with sentences 8000-10000 heldout for testing).
This parser achieves 91.23% labelled accuracy on
the 2000-sentence test set.
To calculate the correlation between the human
judgements and the dependency parser, we parse
the original strings as well as the strings chosen
by the log-linear and language models. The stan-
dard evaluation procedure relies on both strings
being identical to calculate (un-)labelled depen-
dency accuracy, and so we map the dependen-
cies produced by the parser into sets of triples
as used in the evaluation software of Crouch et
al. (2002) where each dependency is represented
as deprel(head,word) and each word is in-
dexed with its position in the original string.
3
We
compare the parses for both experiments against
3
This is a 1-1 mapping, and the indexing ensures that du-
plicate words in a sentence are not confused.
Experiment A Experiment B
corr p-value corr p-value
Dependencies -0.640 <0.001 0.186 0.1860
Unweighted Deps -0.657 <0.001 0.290 0.03686
Table 4: Correlation between dependency-based
evaluation and human judgements
the parses of the original strings. We calculate
both a weighted and unweighted dependency f-
score, as given in Table 1. The unweighted f-score
is calculated by taking the average of the scores
for each dependency type, while the weighted f-
score weighs each average score by its frequency
in the test corpus. We calculate the Spearman
and Pearson correlation coefficients as before; the
results are given in Table 4. The results show
that the unweighted dependencies correlate more
closely (and statistically significantly) with the hu-
man judgements than the weighted ones. This sug-
gests that the frequency of a dependency type does
not matter as much as its overall correctness.
4 Discussion
The large discrepancy between the absolute corre-
lation coefficients for Experiment A and B can be
explained by the fact that they are different tasks.
Experiment A ranks 3 strings relative to one an-
other, while Experiment B measures the natural-
ness of the string. We would expect automatic
metrics to be better at the first task than the sec-
ond, as it is easier to rank systems relative to each
other than to give a system an absolute score.
Disappointingly, the correlation between the de-
pendency parsing metric and the human judge-
ments was no higher than the simple GTM string-
based metric (although it did outperform all other
automatic metrics). This does not correspond to
related work on English Summarisation evalua-
tion (Owczarzak, 2009) which shows that a met-
ric based on an automatically induced LFG parser
for English achieves comparable or higher correla-
tion with human judgements than ROUGE and Ba-
sic Elements (BE).
4
Parsers of German typically
do not achieve as high performance as their En-
glish counterparts, and further experiments includ-
ing alternative parsers are needed to see if we can
improve performance of this metric.
The data used in our experiments was almost
always grammatically correct. Therefore the task
4
The GTM metric was not compared in that paper
99
of an evaluation system is to score more natural
sounding strings higher than marked or unnatural
ones. In this respect, our findings mirror those of
Stent et al (2005) for English data, that the au-
tomatic metrics do not correlate well with human
judges on syntactic correctness.
5 Conclusions
We presented data that examined the correla-
tion between native speaker judgements and au-
tomatic evaluation metrics on automatically gen-
erated German text. We found that for our first
experiment, all metrics were correlated to roughly
the same degree (with ROUGE-L achieving the
highest correlation at an individual sentence level
and the GTM tool not far behind). At a corpus
level all except ROUGE were in agreement with
the human judgements. In the second experiment,
the General Text Matcher Tool had the strongest
correlation. We carried out an experiment to test
whether a more sophisticated syntax-based evalua-
tion metric performed better than the more simple
string-based ones. We found that while the un-
weighted dependency evaluation metric correlated
with the human judgements more strongly than al-
most all metrics, it did not outperform the GTM
tool. The correlation between the human judge-
ments and the automatic evaluation metrics was
much higher for the relative ranking task than for
the naturalness task.
Acknowledgments
This work was funded by the Collaborative Re-
search Centre (SFB 732) at the University of
Stuttgart. We would like to thank Martin Forst,
Alex Fraser and the anonymous reviewers for their
helpful feedback. Furthermore, we would like
to thank Johan Hall, Joakim Nivre and Yannick
Versely for their help in retraining the MALT de-
pendency parser with our data set.
References
Anja Belz and Ehud Reiter. 2006. Comparing auto-
matic and human evaluation of NLG systems. In
Proceedings of EACL 2006, pages 313?320, Trento,
Italy.
Aoife Cahill and Martin Forst. 2009. Human Eval-
uation of a German Surface Realisation Ranker. In
Proceedings of EACL 2009, pages 112?120, Athens,
Greece, March.
Aoife Cahill, Martin Forst, and Christian Rohrer. 2007.
Stochastic Realisation Ranking for a Free Word Or-
der Language. In Proceedings of ENLG-07, pages
17?24, Saarbr?ucken, Germany, June.
Richard Crouch, Ron Kaplan, Tracy Holloway King,
and Stefan Riezler. 2002. A comparison of evalu-
ation metrics for a broad coverage parser. In Pro-
ceedings of the LREC Workshop: Beyond PARSE-
VAL, pages 67?74, Las Palmas, Spain.
Johan Hall and Joakim Nivre. 2008. A dependency-
driven parser for German dependency and con-
stituency representations. In Proceedings of
the Workshop on Parsing German, pages 47?54,
Columbus, Ohio, June.
Eduard Hovy, Chin yew Lin, and Liang Zhou. 2005.
Evaluating duc 2005 using basic elements. In Pro-
ceedings of DUC-2005.
Chin-Yew Lin. 2004. Rouge: A package for auto-
matic evaluation of summaries. In Stan Szpakowicz
Marie-Francine Moens, editor, Text Summarization
Branches Out: Proceedings of the ACL-04 Work-
shop, pages 74?81, Barcelona, Spain, July.
I. Dan Melamed, Ryan Green, and Joseph P. Turian.
2003. Precision and recall of machine translation.
In Proceedings of NAACL-03, pages 61?63, NJ,
USA.
Karolina Owczarzak, Josef van Genabith, and Andy
Way. 2008. Evaluating machine translation with
LFG dependencies. Machine Translation, 21:95?
119.
Karolina Owczarzak. 2009. DEPEVAL(summ):
Dependency-based Evaluation for Automatic Sum-
maries. In Proceedings of ACL-IJCNLP 2009, Sin-
gapore.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a method for automatic
evaluation of machine translation. In Proceedings
of ACL-02, pages 311?318, NJ, USA.
Ehud Reiter and Anja Belz. 2009. An Investigation
into the Validity of Some Metrics for Automatically
Evaluating Natural Language Generation Systems.
Computational Linguistics, 35.
Christian Rohrer and Martin Forst. 2006. Improving
Coverage and Parsing Quality of a Large-Scale LFG
for German. In Proceedings of LREC 2006, Genoa,
Italy.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and Ralph Weischedel. 2006. A
study of translation error rate with targeted human
annotation. In Proceedings of AMTA 2006, pages
223?231.
Amanda Stent, Matthew Marge, and Mohit Singhai.
2005. Evaluating evaluation methods for generation
in the presense of variation. In Proceedings of CI-
CLING, pages 341?351.
100
Coling 2010: Poster Volume, pages 1426?1434,
Beijing, August 2010
Cross-Lingual Induction for Deep Broad-Coverage Syntax: A Case
Study on German Participles
Sina Zarrie? Aoife Cahill Jonas Kuhn Christian Rohrer
Institut fu?r Maschinelle Sprachverarbeitung (IMS), University of Stuttgart
{zarriesa,cahillae,jonas.kuhn,rohrer}@ims.uni-stuttgart.de
Abstract
This paper is a case study on cross-lingual
induction of lexical resources for deep,
broad-coverage syntactic analysis of Ger-
man. We use a parallel corpus to in-
duce a classifier for German participles
which can predict their syntactic category.
By means of this classifier, we induce a
resource of adverbial participles from a
huge monolingual corpus of German. We
integrate the resource into a German LFG
grammar and show that it improves pars-
ing coverage while maintaining accuracy.
1 Introduction
Parallel corpora are currently exploited in a wide
range of induction scenarios, including projection
of morphologic (Yarowsky et al, 2001), syntactic
(Hwa et al, 2005) and semantic (Pado? and Lap-
ata, 2009) resources. In this paper, we use cross-
lingual data to learn to predict whether a lexi-
cal item belongs to a specific syntactic category
that cannot easily be learned from monolingual re-
sources. In an application test scenario, we show
that this prediction method can be used to obtain
a lexical resource that improves deep, grammar-
based parsing.
The general idea of cross-lingual induction is
that linguistic annotations or structures, which are
not available or explicit in a given language, can
be inferred from another language where these an-
notations or structures are explicit or easy to ob-
tain. Thus, this technique is very attractive for
cheap acquisition of broad-coverage resources, as
is proven by the approaches cited above. More-
over, this induction process can be attractive for
the induction of deep (and perhaps specific) lin-
guistic knowledge that is hard to obtain in a mono-
lingual context. However, this latter perspective
has been less prominent in the NLP community
so far.
This paper investigates a cross-lingual induc-
tion method based on an exemplary problem aris-
ing in the deep syntactic analysis of German. This
showcase is the syntactic flexibility of German
participles, being morphologically ambiguous be-
tween verbal, adjectival and adverbial readings,
and it is instructive for several reasons: first, the
phenomenon is a notorious problem for linguistic
analysis and annotation of German, such that stan-
dard German resources do not represent the under-
lying analysis. Second, in Zarrie? et al (2010),
we showed that integrating the phenomenon of
adverbial participles in a naive way into a broad-
coverage grammar of German leads to significant
parsing problems, due to spurious ambiguities.
Third, it is completely straightforward to detect
adverbial participles in cross-lingual data since in
other languages, e.g. English or French, adverbs
are often morphologically marked.
In this paper, we use instances of adverbially
translated participles in a parallel corpus to boot-
strap a classifier that is able to identify an ad-
verbially used participle based on its monolingual
syntactic context. In contrast to what is commonly
assumed, we show that it is possible to detect ad-
verbial participles using only a relatively narrow
context window. This classifier enables us to iden-
tify an occurence of an adverbial participle inde-
pendently of its translation in a parallel corpus,
going far beyond the induction methodology in
Zarrie? et al (2010). By means of the participle
classifier, we can extract new types of adverbial
participles from a larger corpus of German news-
paper text and substantially augment the size of
the resource extracted only on Europarl data. Fi-
nally, we integrate this new resource into the Ger-
man LFG grammar and show that it improves cov-
erage without negatively affecting performance.
1426
The paper is structured as follows: in Sec-
tion 2, we describe the linguistic and computa-
tional problems related to the parsing of adver-
bial participles in German. Section 3 introduces
the general idea of using the translation data to
find instances of different participle categories. In
Section 4, we illustrate the training of the clas-
sifier, evaluating the impact of the context win-
dow and the quality of the training data obtained
from cross-lingual text. In Section 5, we apply the
classifier to new, monolingual data and describe
the extension of the resource for adverbial partici-
ples. Section 6 evaluates the extended resource by
means of parsing experiments using the German
LFG grammar.
2 The Problem
In German, past perfect participles are ambiguous
with respect to their morphosyntactic category. As
in other languages, they can be used as part of
the verbal complex (Example (1-a)) or as adjec-
tives (Example (1-b)). Since German adjectives
can generally undergo conversion into adverbs,
participles can also be used adverbially (Example
(1-c)). The verbal and adverbial participle forms
are morphologically identical.
(1) a. Sie haben das Experiment wiederholt.
?They have repeated the experiment.?
b. Das wiederholte Experiment war erfolgreich.
?The repeated experiment was succesful.?
c. Sie haben das Experiment wiederholt abge-
brochen.
?They cancelled the experiment repeatedly.?
Moreover, German adjectival modifiers can be
generally used as predicatives that can be either
selected by a verb (Example (2-a)) or that can oc-
cur as free predicatives (Example (2-b)).
(2) a. Er scheint begeistert von dem Experiment.
?He seems enthusiastic about the experiment.?
b. Er hat begeistert experimentiert.
?He has experimented enthusiastic.?
Since predicative adjectives are not inflected,
the surface form of a German participle is ambigu-
ous between a verbal, predicative or adverbial use.
2.1 Participles in the German LFG
In order to account for sentences like (1-c), an in-
tuitive approach would be to generally allow for
adverb conversion of participles in the grammar.
However, in Zarrie? et al (2010), we show that
such a rule can have a strong negative effect on
the overall performance of the parsing system, de-
spite the fact that it produces the desired syntac-
tic and semantic analysis for specific sentences.
This problem was illustrated using a German LFG
grammar (Rohrer and Forst, 2006) constructed as
part of the ParGram project (Butt et al, 2002).
The grammar is implemented in the XLE, a gram-
mar development environment which includes a
very efficient LFG parser and a stochastic dis-
ambiguation component which is based on a log-
linear probability model (Riezler et al, 2002).
In Zarrie? et al (2010), we found that the
naive implementation of adverbial participles in
the German LFG, i.e. in terms of a general gram-
mar rule that allows for participles-adverb conver-
sion, leads to spurious ambiguities that mislead
the disambiguation component of the grammar.
Moreover, the rule increases the number of time-
outs, i.e. sentences that cannot be parsed in a pre-
defined amount of time (20 seconds). Therefore,
we observe a drop in parsing accuracy although
grammar coverage is improved. As a solution, we
induced a lexical resource of adverbial participles
based on their adverbial translations in a paral-
lel corpus. This resource, comprising 46 partici-
ple types, restricts the adverb conversion such that
most of the spurious ambiguities are eliminated.
To assess the impact of specific rules in a broad-
coverage grammar, possibly targeting medium-to-
low frequency phenomena, we have established a
fine-grained evaluation methodology. The chal-
lenge posed by these low-frequent phenomena is
typically two-fold: on the one hand, if one takes
into account the disambiguation component of the
grammar and pursues an evaluation of the most
probable parses on a general test set, the new
grammr rule cannot be expected to show a positive
effect since the phenomenon is not likely to occur
very often in the test set. On the other hand, if one
is interested in a linguistically precise grammar,
it is very unsatisfactory to reduce grammar cov-
erage to statistically frequent phenomena. There-
fore, we combined a coverage-oriented evaluation
on specialised testsuites with a quantitative evalu-
ation including disambiguation, making sure that
1427
the increased coverage does not lead to an overall
drop in accuracy. The evaluation methodolgy will
also be applied to evaluate the impact of the new
participle resource, see Section 6.
2.2 The Standard Flat Analysis of Modifiers
The fact that German adjectival modifiers can gen-
erally undergo conversion into adverbs without
overt morphological marking is a notorious prob-
lem for the syntactic analysis of German: there
are no theoretically established tests to distinguish
predicative adjectives and adverbials, see Geuder
(2004). For this reason, the standard German tag
set assigns a uniform tag (?ADJD?) to modifiers
that are morphologically ambiguous between an
adjectival and adverbial reading. Moreover, in
the German treebank TIGER (Brants et al, 2002)
the resulting syntactic differences between the two
readings are annotated by the same flat structure
that does not disambiguate the sentence.
Despite certain theoretical problems related to
the analysis of German modifiers, their interpre-
tation in real corpus sentences is often unambigu-
ous for native speakers. As an example, consider
example (3) from the TIGER treebank. In the
sentence, the participle unterschrieben (signed)
clearly functions as a predicative modifier of the
sentence?s subject. The other, theoretically possi-
ble reading where the participle would modify the
verb send is semantically not acceptable. How-
ever, in TIGER, the participle is analysed as an
ADJD modifier attached under the VP node which
is the general analysis for adjectival and adverbial
modifiers.
(3) Die
It
sollte
should
unterschrieben
signed
an
to
die
the
Leitung
administration
zuru?ckgesandt
sent back
werden.
be.
?It should be sent back signed to the administation.?
Sentence (4) (also taken from TIGER) illus-
trates the case of an adverbial participle. In this
example, the reading where angemessen (ade-
quately) modifies the main verb is the only one
that is semantically plausible. In the treebank, the
participle is tagged as ADJD and analysed as a
modifier in the VP.
(4) Der
The
menschliche
human
Geist
mind
la??t
lets
sich
itself
rechnerisch
computationally
nicht
not
angemessen
adequately
simulieren.
simulate.
?The human mind cannot be adequately simulated in a
computational way.?
The flat annotation strategy adopted for modi-
fiers in the standard German tag set and in the tree-
bank TIGER entails that instances of adverbs (and
adverbial participles) cannot be extracted from au-
tomatically tagged, or parsed, text. Therefore,
it would be very hard to obtain training mate-
rial from German resources to train a system that
automatically identifies adverbially used partici-
ples. However, the intuition corroborated by the
examples presented in this section is that the struc-
tures can actually be disambiguated in many cor-
pus sentences.
In the following sections, we show how we ex-
ploit parallel text to obtain training material for
learning to predict occurences of adverbial par-
ticiples, without any manual effort. Moreover, by
means of this technique, we can substantially ex-
tend the grammatical resource for adverbial par-
ticiples compared to the resource that can be di-
rectly extracted from the parallel text.
3 Participles in the Parallel Corpus
The intuition of the cross-lingual induction ap-
proach is that adverbial participles can easily be
extracted from parallel corpora since in other lan-
guages (such as English or French) adverbs are
often morphologically marked and easily labelled
by statistical PoS taggers. As an example, con-
sider sentence (5) extracted from Europarl, where
the German participle versta?rkt is translated by an
English adverb (increasingly).
(5) a. Nicht
Not
ohne
without
Grund
reason
sprechen
speak
wir
we
versta?rkt
increasingly
vom
of a
Europa
Europe
der
of the
Regionen.
Regions.
b. It is not without reason that we increasingly speak
in terms of a Europe of the Regions.
The idea is to project specific morphological
information about adverbs which is overt in lan-
guages like English onto German where adverbs
cannot be directly extracted from tagged data.
While this idea might seem intuitively straightfor-
1428
ward, we also know that translation pairs in paral-
lel data are not always lingusitically parallel, and
as a consequence, word-alignment is not always
reliable. To assess the impact of non-parallelism
in adverbial translations of German participles,
we manually annotated a sample of 300 transla-
tions. This data also constitutes the basis for the
experiments reported in Section 4.
3.1 Data
Our experiments are based on the same data as in
(Zarrie? et al, 2010). For convenience, we pro-
vide a short description here.
We limit our investigations to non-lexicalised
participles occuring in the Europarl corpus and
not yet recorded as adverbs in the lexicon of the
German LFG grammar (5054 participle types in
total). Given the participle candidates, we ex-
tract the set of sentences that exhibit a word align-
ment between a German participle and an English,
French or Dutch adverb. The word alignments
have been obtained with GIZA++. The extrac-
tion yields 27784 German-English sentence pairs
considering all alignment links, and 5191 sen-
tence pairs considering only bidirectional align-
ments between a participle and an English adverb.
3.2 Systematic Non-Parallelism
For data exploration and evaluation, we anno-
tated 300 participle alignments out of the 5191
German-English sentences (with a bidirectional
participle-adverb alignment). We distinguish the
following annotation categories: (i) parallel trans-
lation, adverb information can be projected, (ii)
incorrect alignment, (iii) correct alignment, but
translation is a multi-word expression, (iv) correct
alignment, but translation is a paraphrase (possi-
bly involving a translation shift).
Parallel Cases In our annotated sample of En-
glish adverb - German participle pairs, 43%1 of
the translation instances are parallel in the sense
that the overt adverb information from the English
side can be projected onto the German participle.
This means that if we base the induction technique
1The diverging figures we report in Zarrie? et al (2010)
were due to a small bug in the script and it does not affect the
overall interpretation of the data.
on word-alignments alone, its precision would be
relatively low.
Non-Parallel Cases Taking a closer look at the
non-parallel cases in our sample (57% of the
translation pairs), we find that 47% of this set are
due to incorrect word alignments. The remain-
ing 53% thus reflect regular cases of non-parallel
translations. A typical configuration which makes
up 30% of the the non-parallel cases is exempli-
fied in (6) where the German main verb vorlegen
is translated by the English multiword expression
put forward.
(6) a. Wir haben eine Reihe von Vorschla?gen vorgelegt.
b. We have put forward a number of proposals.
An example for the general paraphrase or trans-
lation shift category is given in Sentence (7).
Here, the translational correspondence between
gekommen (arrived) and the adverb now is due
to language-specific, idiomatic realisations of an
identical underlying semantic concept. The para-
phrase translations make up 23% of the non-
parallel cases in the annotated sample.
(7) a. Die
That
Zeit
time
ist
is
noch
yet
nicht
not
gekommen
arrived.
.
b. That time is not now .
Furthermore, it is noticeable that the cross-
lingual approach seems to inherently factor out
the ambiguity between predicative and adverbial
participles. In our annotated sample, there are no
predicative participles that have been translated by
an English adverb.
3.3 Filtering Mechanisms
The data analysis in the previous section, show-
ing only 43% of parallel cases in English adverb
translations for German participles, mainly con-
firms other studies in annotation projection which
find that translational correspondences only allow
for projection of linguistic analyses in a more or
less limited proportion (Yarowsky et al, 2001;
Hwa et al, 2005; Mihalcea et al, 2007).
In previous studies on annotation projection,
quite distinct filtering methods have been pro-
posed: in Yarowsky et al (2001), projection er-
rors are mainly attributed to word alignment er-
rors and filtered based on translation probabilities.
1429
Hwa et al (2005) find that errors in the projec-
tion of syntactic relations are also due to system-
atic grammatical divergences between languages
and propose correcting these errors by means of
specific, manually designed filters. Bouma et al
(2008) make similar observations to Hwa et al
(2005), but try to replace manual correction rules
by filters from additional languages.
In Zarrie? et al (2010), we compared a num-
ber of filtering techniques on our participle data.
The 300 annotated translation instances are used
as a test set for evaluation. In particular, we
have established that a combination of syntactic
dependency-based filters and multilingual filters
can very accurately separate non-parallel transla-
tions from parallel ones where the adverb infor-
mation can be projected. In Section 4, we show
that these filtering techniques are also very useful
for removing noise from the training material that
we use to build a classifier.
4 Bootstrapping a German Participle
Classifier from Crosslingual Data
In the previous section, we have seen that German
adverbial participles can be easily found in cross-
lingual text by looking at their translations in a
language that morphologically marks adverbials.
In previous work, we exploited this observation
by directly extracting types of adverbial partici-
ples based on word alignment links and the filter-
ing mechanisms mentioned in Section 3. How-
ever, this method is very closely tied to data in
the parallel corpus, which only comprises around
5000 participle-adverb translations in total, which
results in 46 types of adverbial participles after fil-
tering. Thus, we have no means of telling whether
we would discover new types of adverbial partici-
ples in other corpora, from different domains to
Europarl. As this corpus is rather small and genre
specific, it even seems very likely that one could
find additional adverbial participles in a bigger
corpus. Moreover, we cannot be sure that certain
adverbial participles have systematically diverg-
ing translations in other languages, due to cross-
lingual lexicalisation differences. Generally, it is
not clear whether we have learned something gen-
eral about the syntactic phenomenon of adverbial
participles in German or whether we have just ex-
tracted a small, corpus-dependent subset of the
class of adverbial participles.
In this section, we use instances of adverbially
translated participles as training material for a
classifier that learns to predict adverbial partici-
ples based on their monolingual syntactic context.
Thus, we exploit the translations in the parallel
corpus as a means of obtaining ?annotated? or dis-
ambiguated training data without any manual ef-
fort. During training, we only consider the mono-
lingual context of the participle, such that the fi-
nal application of the classifier is not dependent
on cross-lingual data anymore.
4.1 Context-based Identification of
Adverbial Participles
Given the general linguistic problems related to
adverbial participles (see Section 2), one could
assume that it is very difficult to identify them
in a given context. To assess the general dif-
ficulty of this syntactic problem, we run a first
experiment comparing a grammar-based identifi-
cation method against a classifier that only con-
siders relatively narrow morpho-syntactic context.
For evaluation, we use the 300 annotated partici-
ple instances described in Section 3. This test
set divides into 172 negative instances, i.e. non-
adverbial participles, and 128 positive instances.
We report accuracy of the identification method,
as well as precision and recall relating to the num-
ber of correctly predicted adverbial participles.
For the grammar-based identification, we use
the German LFG which integrates the lexical
resource for adverbial participles established in
(Zarrie? et al, 2010). We parse the 300 Europarl
sentences and check whether the most probable
parse proposed by the grammar analyses the re-
spective participle as an adverb or not. The gram-
mar obtains a complete parse for 199 sentences
out of the test set and we only consider these in
the evaluation. The results are given in Table 1.
The high precision and accuracy of the
grammar-based identification of adverbial partici-
ples suggests that in a lot of sentences, the adver-
bial analysis is the only possible reading, i.e. the
only analysis that makes the sentence grammati-
cal. But of course, we have substantially restricted
the adverb participle-conversion in the grammar,
1430
Training Data Precision Recall Accuracy
Grammar 97.3 90.12 94.97
Classifier Unigram 87.10 84.38 87.92
Classifier Bigram 88.28 88.28 89.93
Classifier Trigram 89.60 87.5 90.27
Table 1: Evaluation on 300 participle instances
from Europarl
so that it does not propose adverbial analyses for
participles that are very unlikely to function as
modifiers of verbs.
For the classifier-based identification, we use
the adverbially translated participle tokens in our
Europarl data (5191 tokens in total) as training
material. We remove the 300 test instances from
this training set, and then divide it into a set of
positive and negative instances. To do this, we
use the filtering mechanisms already proposed in
Zarrie? et al (2010). These filters apply on the
type level, such that we first identify the positive
types (46 total) and then use all instances of these
types in the 4891 sentences as positive instances
of adverbial participles (1978 instances). The re-
maining sentences are used as negative instances.
For the training of the classifier, we use
maximum-entropy classification, which is also
commonly used for the general task of tagging
(Ratnaparkhi, 1996). In particular, we use the
open source TADM tool for parameter estimation
(Malouf, 2002). The tags of the words surround-
ing the participles are used as features in the clas-
sification task. We explore different sizes of the
context window, where the trigram window is the
most succesful (see Table 1). Beyond the trigram
window, the results of the classifier start decreas-
ing again, probably because of too many mislead-
ing features. Generally, this experiment shows
that the grammar-based identification is more pre-
cise, but that the classifier still performs surpris-
ingly well. Compared to the results from the
grammar-based identification, the high accuracy
of the classifier suggests that even the narrow syn-
tactic contexts of adverbial vs. non-adverbial par-
ticiples are quite distinct.
4.2 Designing Training Data for Participle
Classification
There are several questions related to the design
of the training data that we use to build our clas-
sifier. First, it is not clear how many negative
instances are helpful for learning the adverbial -
non-adverbial distinction. In the above experi-
ment, we simply use the instances that do not pass
the cross-lingual filters. In this section, we exper-
iment with an augmented set of negative instances
that was also obtained by extracting German par-
ticiple that are bi-directionally aligned to an En-
glish participle in Europarl. This is based on the
assumption that these participles are very likely
to be verbal. Second, it is not clear whether we
really need the filtering mechanisms proposed in
Zarrie? et al (2010) and whether we could im-
prove the classifier by training it on a larger set
of positive instances. Therefore, we also experi-
ment with two further sets of positive instances:
one where we used all participles (not necessarily
bidirectionally) aligned to an adverb, one where
we only use the bidirectional alignments. The re-
sults obtained for the different sizes of positive
and negative instance sets are given in Table 2.
The picture that emerges from the results in Ta-
ble 2 is very clear: the stricter the filtering of the
training material (i.e. the positive instances) is,
the better the performance of the classifier. The
fact that we (potentially) loose certain positive in-
stances in the filtering does not negatively impact
on the classifier which substantially benefits from
the fact that noise gets removed. Moreover, we
find that if the training material is appropriately
filtered, adding further negative instances does not
help improving the accuracy. By contrast, if we
train on a noisy set of positive instances, the clas-
sifier benefits from a larger set of negative in-
stances. However, the positive effect that we get
from augmenting the non-filtered training data is
still weaker than the positive effect we get from
the filtering.
5 Induction of Adverbial Participles on
Monolingual Data
Given the classifier from Section 4 that predicts
the syntactic category of a participle instance
1431
Training Data Pos. Instances Neg. Instances Precision Recall Accuracy
Non-Filtered Instances (all alignments) 27.184 10.000 43.10 100 43.10
Non-Filtered Instances (all alignments) 27.184 50.000 74.38 92.97 83.22
Non-Filtered Instances (symm. alignments) 4891 10.000 78.08 89.06 84.56
Non-Filtered Instances (symm. alignments) 4891 50.000 82.31 83.59 85.23
Filtered Instances 1978 10.000 91.60 85.16 90.27
Filtered Instances 1978 50.000 90.83 77.34 86.91
Table 2: Evaluation on 300 participle instances from Europarl
based on its monolingual syntactic context, we
can now detect new instances or types of adver-
bial participles in any PoS-tagged German corpus.
In this section, we investigate whether the classi-
fier can be used to augment the resource of ad-
verbial participles directly induced from Europarl
with new types.
5.1 Data Extraction
We run our extraction experiment on the Huge
German Corpus (HGC), a corpus of 200 million
words of newspaper and other text. This corpus
has been tagged with TreeTagger (Schmid, 1994).
For each of the 5054 participle candidates, we ex-
tract all instances from the HGC which have not
been tagged as finite verbs (at most 2000 tokens
per participle). For each participle token, we also
extract its syntactic context in terms of the 3 pre-
ceding and the 3 following tags. For classification,
we use only those participles that have more than
50 instances in the corpus (2953 types).
In contrast to the cross-lingual filtering mech-
anisms developed in Zarrie? et al (2010) which
operate on the type-level, the classifier makes a
prediction for every token of a given participle
candidate. Thus, for each of the participle can-
didates, we obtain a percentage of instances that
have been classified as adverbs. As we would ex-
pect, the percentage of adverbial instances is very
low for most of the participles in our candidate set:
for 75% of the 2953 types, the percentage is below
5%. This result confirms our initial intuition that
the property of being used as an adverb is strongly
lexically restricted to a certain class of participles.
5.2 Evaluation
Since we know that the classifier has an accu-
racy of 90% on the Europarl data, we only con-
sider participles as candidates for adverbs where
the classifier predicted more than 14% adverbial
instances. This leaves us with a set of 210 partici-
ples, which comprises 13 of the original 46 par-
ticiples extracted from Europarl, meaning we have
discovered 197 new adverbial participle types.
We performed a manual evaluation of 50 ran-
domly selected types out of the set of 197 new
participle types. Therefore, we looked at the in-
stances and their context which the classifier pre-
dicted to be adverbial. If there was at least one ad-
verbial instance among these, the participle type
was evaluated as correctly annotated by the clas-
sifier. By this means, we find that 76% of the par-
ticiples were correctly classified.
This evaluation suggests that the accuracy of
our classifier which we trained and tested on Eu-
roparl data is lower on the HGC data. The rea-
son for this drop in performance will be explained
in the following Section 5.3. However, assuming
an accuracy of 76%, we have discovered 150 new
types of adverbial participles. We argue that this is
a very satisfactory result given that we have not in-
vested any manual effort into the annotation or ex-
traction of adverbial participles. This results also
makes clear that the previous resource we induced
on Europarl data, comprising only 46 participle
types, was a very limited one.
5.3 Error Analysis
Taking a closer look at the 12 participle candi-
dates that the classifier incorrectly labels as adver-
bial, we observe that their adverbially classified
instances are mostly instances of a predicative use.
This means that our Europarl training data does
not contain enough evidence to learn the distinc-
tion between adverbial and predicative participles.
This is not surprising since the set of negative
instances used for training the classifier mainly
comprises verbal instances of participles. More-
over, the syntactic contexts and constructions in
which some predicatives and adverbials are used
1432
Grammar Prec. Rec. F-Sc. Time
in sec
46 Part-Adv 84.12 78.2 81.05 665
243 Part-Adv 84.12 77.67 80.76 665
Table 3: Evaluation on 371 TIGER sentences
are very similar. Thus, in future work, we will
have to include more data on predicatives (which
is more difficult to obtain) and analyse the syntac-
tic contexts in more detail.
6 Assessing the Impact of Resource
Coverage on Grammar-based Parsing
In this section, we evaluate the classifier-based in-
duction of adverbial participles from a grammar-
based perspective. We integrate the entire set of
induced adverbial participles (46 from Europarl
and 197 from the HGC) into the German LFG
grammar. As a consequence, the grammar al-
lows the adverb conversion for 243 lexical par-
ticiple types. We use the evaluation methodolgy
explained in Section 2.
First, we conduct an accuracy-oriented evalua-
tion on the standard TIGER test set. We compare
against the German LFG that only integrates the
small participle resource from Europarl. The re-
sults are given in Table 3. The difference between
the 46 Part-Adv and 243 Part-Adv resource is not
statistically signficant. Thus, the larger participle
resource has no overall negative effect on the pars-
ing performance. As established by an automatic
upperbound evaluation in Zarrie? et al (2010),
we cannot not expect to find a positive effect in
this evaluation because the phenomenon does not
occur in the standard test set.
To show that the augmented resource indeed
improves the coverage of the grammar, we built
a specialised testsuite of 1044 TIGER sentences
that contain an instance of a participle from the
resource. Since this testsuite comprises sen-
tences from the training set, we can only report
a coverage-oriented evaluation here, see Table 4.
The 243 Part-Adv increases the coverage by 8%
on the specialised testsuite.
Moreover, we manually evaluated 20 sentences
covered by the 243-Part-Adv grammar and not
by 46-Part-Adv as to whether they contain a cor-
rectly analysed adverbial participle. In two sen-
Grammar Parsed
Sent.
Starred
Sent.
Time-
outs
Time
in sec
No Part-Adv 665 315 64 3033
46 Part-Adv 710 269 65 3118
243 Part-Adv 767 208 69 3151
Table 4: Performance on the specialised TIGER
test set (1044 sentences)
tences, the grammar obtained an adverbial analy-
sis for clearly predicative modifiers, based on the
enlarged resource. In three different sentences, it
was difficult to decide whether the participle acts
as an adverb or a predicative. In the remaining 15
sentences, the grammar established the the correct
analysis of a clearly adverbially used participle.
7 Conclusion
We have proposed a cross-lingual induction
method to automatically obtain data on adverbial
participles in German. We exploited this cross-
lingual data as training material for a classifier that
learns to predict the syntactic category of a partici-
ple from its monolingual syntactic context. Since
this category is usually not annotated in German
resources and hard to describe in theory, the find-
ing that adverbial participles can be predicted rel-
atively precisely is of general interest for theo-
retic and computational approaches to the syntac-
tic analysis of German.
We showed that, in order to obtain an accurate
participle classifier, the quality of the training ma-
terial induced from the parallel corpus is of crucial
importance. By applying the filtering techniques
from Zarrie? et al (2010), the accuracy of the
classifier increases between 5% and 7%. In future
work, we plan to include more data on predicative
participles to learn a more accurate distinction be-
tween predicative and adverbial participles.
Finally, we used the participle classifier to ex-
tract a lexical resource of adverbial participles for
the German LFG grammar. In comparison to the
relatively small resource of 46 types that can be
directly induced from Europarl, we discovered a
large number of new participle types (197 types
in total). In a parsing experiment, we showed that
this much bigger resource does not negatively im-
pact on parsing performance and improves gram-
mar coverage.
1433
References
Bouma, Gerlof, Jonas Kuhn, Bettina Schrader, and
Kathrin Spreyer. 2008. Parallel LFG Grammars
on Parallel Corpora: A Base for Practical Trian-
gulation. In Butt, Miriam and Tracy Holloway
King, editors, Proceedings of the LFG08 Confer-
ence, pages 169?189, Sydney, Australia. CSLI Pub-
lications, Stanford.
Brants, Sabine, Stefanie Dipper, Silvia Hansen, Wolf-
gang Lezius, and George Smith. 2002. The TIGER
Treebank. In Proceedings of the Workshop on Tree-
banks and Linguistic Theories.
Butt, Miriam, Helge Dyvik, Tracy Holloway King, Hi-
roshi Masuichi, and Christian Rohrer. 2002. The
Parallel Grammar Project.
Geuder, Wilhelm. 2004. Depictives and transpar-
ent adverbs. In Austin, J. R., S. Engelbrecht,
and G. Rauh, editors, Adverbials. The Interplay of
Meaning, Context, and Syntactic Structure, pages
131?166. Benjamins.
Hwa, Rebecca, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrapping
parsers via syntactic projection across parallel texts.
Nat. Lang. Eng., 11(3):311?325.
Malouf, Robert. 2002. A comparison of algorithms
for maximum entropy parameter estimation. In Pro-
ceedings of the Sixth Conference on Natural Lan-
guage Learning (CoNLL-2002), pages 49?55.
Mihalcea, Rada, Carmen Banea, and Jan Wiebe.
2007. Learning multilingual subjective language
via cross-lingual projections. In Proceedings of
the Association for Computational Linguistics (ACL
2007), pages 976?983, Prague.
Pado?, Sebastian and Mirella Lapata. 2009. Cross-
lingual annotation projection of semantic roles.
Journal of Artificial Intelligence Research, 36:307?
340.
Ratnaparkhi, Adwait. 1996. A maximum entropy
model for part-of-speech tagging. In Proceedings
of EMNLP 96, pages 133?142.
Riezler, Stefan, Tracy Holloway King, Ronald M. Ka-
plan, Richard Crouch, John T. Maxwell, and Mark
Johnson. 2002. Parsing the Wall Street Journal
using a Lexical-Functional Grammar and Discrim-
inative Estimation Techniques . In Proceedings of
ACL 2002.
Rohrer, Christian and Martin Forst. 2006. Improving
coverage and parsing quality of a large-scale LFG
for German. In Proceedings of LREC-2006.
Schmid, Helmut. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of In-
ternational Conference on New Methods in Lan-
guage Processing.
Yarowsky, David, Grace Ngai, and Richard Wicen-
towski. 2001. Inducing multilingual text analy-
sis tools via robust projection across aligned cor-
pora. In Proceedings of HLT 2001, First Interna-
tional Conference on Human Language Technology
Research.
Zarrie?, Sina, Aoife Cahill, Jonas Kuhn, and Christian
Rohrer. 2010. A Cross-Lingual Induction Tech-
nique for German Adverbial Participles. In Pro-
ceedings of the 2010 Workshop on NLP and Lin-
guistics: Finding the Common Ground, ACL 2010,
pages 34?42, Uppsala, Sweden.
1434
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1363?1373,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Can characters reveal your native language? A language-independent
approach to native language identification
Radu Tudor Ionescu

, Marius Popescu

, Aoife Cahill
?

University of Bucharest
Department of Computer Science
14 Academiei, Bucharest, Romania
raducu.ionescu@gmail.com
popescunmarius@gmail.com
?
Educational Testing Service
660 Rosedale Rd
Princeton, NJ 08541, USA
acahill@ets.org
Abstract
A common approach in text mining tasks
such as text categorization, authorship
identification or plagiarism detection is to
rely on features like words, part-of-speech
tags, stems, or some other high-level lin-
guistic features. In this work, an approach
that uses character n-grams as features is
proposed for the task of native language
identification. Instead of doing standard
feature selection, the proposed approach
combines several string kernels using mul-
tiple kernel learning. Kernel Ridge Re-
gression and Kernel Discriminant Analy-
sis are independently used in the learning
stage. The empirical results obtained in all
the experiments conducted in this work in-
dicate that the proposed approach achieves
state of the art performance in native lan-
guage identification, reaching an accuracy
that is 1.7% above the top scoring system
of the 2013 NLI Shared Task. Further-
more, the proposed approach has an im-
portant advantage in that it is language in-
dependent and linguistic theory neutral. In
the cross-corpus experiment, the proposed
approach shows that it can also be topic
independent, improving the state of the art
system by 32.3%.
1 Introduction
Using words as basic units is natural in textual
analysis tasks such as text categorization, author-
ship identification or plagiarism detection. Per-
haps surprisingly, recent results indicate that meth-
ods handling the text at the character level can
also be very effective (Lodhi et al., 2002; Sander-
son and Guenter, 2006; Popescu and Dinu, 2007;
Grozea et al., 2009; Popescu, 2011; Popescu and
Grozea, 2012). By disregarding features of natu-
ral language such as words, phrases, or meaning,
an approach that works at the character level has
an important advantage in that it is language inde-
pendent and linguistic theory neutral. This paper
presents a state of the art machine learning system
for native language identification that works at the
character level. The proposed system is inspired
by the system of Popescu and Ionescu (2013), but
includes some variations and improvements. A
major improvement is that several string kernels
are combined via multiple kernel learning (Shawe-
Taylor and Cristianini, 2004). Despite the fact that
the (histogram) intersection kernel is very popular
in computer vision (Maji et al., 2008; Vedaldi and
Zisserman, 2010), it has never been used before in
text mining. In this work, the intersection kernel is
used for the first time in a text categorization task,
alone and in combination with other kernels. The
intersection kernel lies somewhere in the middle
between the kernel that takes into account only the
presence of n-grams and the kernel based on the
frequency of n-grams (p-spectrum string kernel).
Two kernel classifiers are proposed for the
learning task, namely Kernel Ridge Regression
(KRR) and Kernel Discriminant Analysis (KDA).
The KDA classifier is able to avoid the class-
masking problem (Hastie and Tibshirani, 2003),
which may often arise in the context of native
language identification. Several experiments are
conducted to evaluate the performance of the ap-
proach proposed in this work. While multiple ker-
nel learning seems to produce a more robust sys-
tem, the two kernel classifiers obtained mixed re-
sults in the experiments. Overall, the empirical re-
sults indicate that the approach proposed in this
paper achieves state of the art performance in na-
tive language identification, while being both lan-
1363
guage independent and linguistic theory neutral.
Furthermore, the approach based on string kernels
does not need any expert knowledge of words or
phrases in the language.
The paper is organized as follows. Related
work is presented in Section 2. Section 3 presents
several similarity measures for strings, including
string kernels and Local Rank Distance. The
learning methods used in the experiments are de-
scribed in Section 4. Section 5 presents details
about the experiments. Finally, the conclusions are
drawn in Section 6.
2 Related Work
2.1 Native Language Identification
The goal of automatic native language identifica-
tion (NLI) is to determine the native language of
a language learner, based on a piece of writing in
a foreign language. This can provide useful in-
formation in forensic linguistic tasks (Estival et
al., 2007) or could be used in an educational set-
ting to provide contrastive feedback to language
learners. Most research has focused on identify-
ing the native language of English language learn-
ers, though there have been some efforts recently
to identify the native language of writing in other
languages (Malmasi and Dras, 2014).
In general most approaches to NLI have used
multi-way classification with SVMs or similar
models along with a range of linguistic features.
The seminal paper by Koppel et al. (2005) intro-
duced some of the best-performing features: char-
acter, word and part-of-speech n-grams along with
features inspired by the work in the area of second-
language acquisition such as spelling and gram-
matical errors. In 2013, Tetreault et al. (2013) or-
ganized the first shared task in the field. This al-
lowed researchers to compare approaches for the
first time on a specifically designed NLI corpus
that was much larger than previously available
data sets. In the shared task, 29 teams submit-
ted results for the test set, and one of the most
successful aspects of the competition was that it
drew submissions from teams working in a variety
of research fields. The submitted systems utilized
a wide range of machine learning approaches,
combined with several innovative feature contri-
butions. The best performing system achieved an
overall accuracy of 83.6% on the 11-way classifi-
cation of the test set, although there was no signif-
icant difference between the top teams.
2.2 Methods that Work at the Character
Level
In recent years, methods of handling text at
the character level have demonstrated impres-
sive performance levels in various text analy-
sis tasks (Lodhi et al., 2002; Sanderson and
Guenter, 2006; Popescu and Dinu, 2007; Grozea
et al., 2009; Popescu, 2011; Popescu and Grozea,
2012). Lodhi et al. (2002) used string kernels
for document categorization with very good re-
sults. String kernels were also successfully used in
authorship identification (Sanderson and Guenter,
2006; Popescu and Dinu, 2007; Popescu and
Grozea, 2012). For example, the system described
in (Popescu and Grozea, 2012) ranked first in most
problems and overall in the PAN 2012 Traditional
Authorship Attribution tasks.
Using string kernels makes the corresponding
learning method completely language indepen-
dent, because the texts will be treated as sequences
of symbols (strings). Methods working at the
word level or above very often restrict their feature
space according to theoretical or empirical princi-
ples. For instance, they select only features that re-
flect various types of spelling errors or only some
type of words, such as function words. These fea-
tures prove to be very effective for specific tasks,
but it is possible that other good features also ex-
ist. String kernels embed the texts in a very large
feature space, given by all the substrings of length
p, and leave it to the learning algorithm to select
important features for the specific task, by highly
weighting these features. It is important to note
that this approach is also linguistic theory neutral,
since it disregards any features of natural language
such as words, phrases, or meaning. On the other
hand, a method that considers words as features
cannot be completely language independent, since
the definition of a word is necessarily language-
specific. For example, a method that uses only
function words as features is not completely lan-
guage independent because it needs a list of func-
tion words which is specific to a language. When
features such as part-of-speech tags are used, as
in the work of Jarvis et al. (2013), the method re-
lies on a part-of-speech tagger which might not be
available (yet) for some languages. Furthermore,
a way to segment a text into words is not an easy
task for some languages, such as Chinese.
Character n-grams are used by some of the sys-
tems developed for native language identification.
1364
In work where feature ablation results have been
reported, the performance with only character n-
gram features was modest compared to other types
of features (Tetreault et al., 2012). Initially, most
work limited the character features to unigrams,
bigrams and trigrams, perhaps because longer n-
grams were considered too expensive to compute
or unlikely to improve performance. However,
some of the top systems in the 2013 NLI Shared
Task were based on longer character n-grams,
up to 9-grams (Jarvis et al., 2013; Popescu and
Ionescu, 2013). The results presented in this work
are obtained using a range of 5?8 n-grams. Com-
bining all 5?8 n-grams would generate millions
of features, which are indeed expensive to com-
pute and represent. The key to avoiding the com-
putation of such a large number of features lies
in using the dual representation provided by the
string kernel. String kernel similarity matrices can
be computed much faster and are extremely useful
when the number of samples is much lower than
the number of features.
3 Similarity Measures for Strings
3.1 String Kernels
The kernel function gives kernel methods the
power to naturally handle input data that is not
in the form of numerical vectors, e.g. strings.
The kernel function captures the intuitive notion
of similarity between objects in a specific domain
and can be any function defined on the respec-
tive domain that is symmetric and positive definite.
For strings, many such kernel functions exist with
various applications in computational biology and
computational linguistics (Shawe-Taylor and Cris-
tianini, 2004).
Perhaps one of the most natural ways to mea-
sure the similarity of two strings is to count how
many substrings of length p the two strings have
in common. This gives rise to the p-spectrum ker-
nel. Formally, for two strings over an alphabet ?,
s, t ? ?
?
, the p-spectrum kernel is defined as:
k
p
(s, t) =
?
v??
p
num
v
(s) ? num
v
(t),
where num
v
(s) is the number of occurrences of
string v as a substring in s.
1
The feature map de-
1
Note that the notion of substring requires contiguity.
Shawe-Taylor and Cristianini (2004) discuss the ambiguity
between the terms substring and subsequence across differ-
ent domains: biology, computer science.
fined by this kernel associates a vector of dimen-
sion |?|
p
containing the histogram of frequencies
of all its substrings of length p (p-grams) with each
string.
A variant of this kernel can be obtained if the
embedding feature map is modified to associate a
vector of dimension |?|
p
containing the presence
bits (instead of frequencies) of all its substrings of
length p with each string. Thus, the character p-
grams presence bits kernel is obtained:
k
0/1
p
(s, t) =
?
v??
p
in
v
(s) ? in
v
(t),
where in
v
(s) is 1 if string v occurs as a substring
in s, and 0 otherwise.
In computer vision, the (histogram) intersec-
tion kernel has successfully been used for object
class recognition from images (Maji et al., 2008;
Vedaldi and Zisserman, 2010). In this paper, the
intersection kernel is used for the first time as a
kernel for strings. The intersection string kernel is
defined as follows:
k
?
p
(s, t) =
?
v??
p
min{num
v
(s), num
v
(t)},
where num
v
(s) is the number of occurrences of
string v as a substring in s.
For the p-spectrum kernel, the frequency of a p-
gram has a very significant contribution to the ker-
nel, since it considers the product of such frequen-
cies. On the other hand, the frequency of a p-gram
is completely disregarded in the p-grams presence
bits kernel. The intersection kernel lies some-
where in the middle between the p-grams presence
bits kernel and p-spectrum kernel, in the sense that
the frequency of a p-gram has a moderate contri-
bution to the intersection kernel. More precisely,
the following inequality that describes the relation
between the three kernels holds:
k
0/1
p
(s, t) ? k
?
p
(s, t) ? k
p
(s, t).
What is actually more interesting is that the inter-
section kernel assigns a high score to a p-gram if it
has a high frequency in both strings, since it con-
siders the minimum of the two frequencies. The
p-spectrum kernel assigns a high score even when
the p-gram has a high frequency in only one of
the two strings. Thus, the intersection kernel cap-
tures something about the correlation between the
p-gram frequencies in the two strings, which may
lead to a more sensitive similarity between strings.
1365
Normalized versions of these kernels ensure a
fair comparison of strings of different lengths:
?
k
p
(s, t) =
k
p
(s, t)
?
k
p
(s, s) ? k
p
(t, t)
,
?
k
0/1
p
(s, t) =
k
0/1
p
(s, t)
?
k
0/1
p
(s, s) ? k
0/1
p
(t, t)
,
?
k
?
p
(s, t) =
k
?
p
(s, t)
?
k
?
p
(s, s) ? k
?
p
(t, t)
.
Taking into account p-grams of different length
and summing up the corresponding kernels, new
kernels, termed blended spectrum kernels, can be
obtained.
The string kernel implicitly embeds the texts
in a high dimensional feature space. Then, a
kernel-based learning algorithm implicitly assigns
a weight to each feature, thus selecting the fea-
tures that are important for the discrimination task.
For example, in the case of text categorization
the learning algorithm enhances the features rep-
resenting stems of content words (Lodhi et al.,
2002), while in the case of authorship identifica-
tion the same learning algorithm enhances the fea-
tures representing function words (Popescu and
Dinu, 2007).
3.2 Local Rank Distance
A recently introduced distance measure, termed
Local Rank Distance (Ionescu, 2013), comes from
the idea of better adapting rank distance (Dinu,
2003) to string data, in order to capture a bet-
ter similarity between strings, such as DNA se-
quences or text. Local Rank Distance (LRD) has
already shown promising results in computational
biology (Ionescu, 2013) and native language iden-
tification (Popescu and Ionescu, 2013).
In order to describe LRD, the following nota-
tions are defined. Given a string x over an al-
phabet ?, and a character a ? ?, the length of
x is denoted by |x|. Strings are considered to
be indexed starting from position 1, that is x =
x[1]x[2] ? ? ?x[|x|]. Moreover, x[i : j] denotes its
substring x[i]x[i+ 1] ? ? ?x[j ? 1].
Local Rank Distance is inspired by rank dis-
tance (Dinu, 2003), the main differences being
that it uses p-grams instead of single charac-
ters, and that it matches each p-gram in the first
string with the nearest equal p-gram in the second
string. Given a fixed integer p ? 1, a thresh-
old m ? 1, and two strings x and y over ?,
the Local Rank Distance between x and y, de-
noted by ?
LRD
(x, y), is defined through the fol-
lowing algorithmic process. For each position i in
x (1 ? i ? |x|?p+1), the algorithm searches for
that position j in y (1 ? j ? |y|? p+ 1) such that
x[i : i+p] = y[j : j+p] and |i? j| is minimized.
If j exists and |i ? j| < m, then the offset |i ? j|
is added to the Local Rank Distance. Otherwise,
the maximal offset m is added to the Local Rank
Distance. An important remark is that LRD does
not impose any mathematically developed global
constraints, such as matching the i-th occurrence
of a p-gram in x with the i-th occurrence of that
same p-gram in y. Instead, it is focused on the lo-
cal phenomenon, and tries to pair equal p-grams at
a minimum offset. To ensure that LRD is a (sym-
metric) distance function, the algorithm also has
to sum up the offsets obtained from the above pro-
cess by exchanging x and y. LRD can be formally
defined as follows.
Definition 1 Let x, y ? ?
?
be two strings, and let
p ? 1 and m ? 1 be two fixed integer values. The
Local Rank Distance between x and y is defined
as:
?
LRD
(x, y) = ?
left
(x, y) + ?
right
(x, y),
where ?
left
(x, y) and ?
right
(x, y) are defined as
follows:
?
left
(x, y) =
|x|?p+1
?
i=1
min{|i? j| such that
1 ? j ? |y| ? p+ 1 and
x[i : i+ p] = y[j : j + p]} ? {m},
?
right
(x, y) =
|y|?p+1
?
j=1
min{|j ? i| such that
1 ? i ? |x| ? p+ 1 and
y[j : j + p] = x[i : i+ p]} ? {m}.
Interestingly, the search for matching p-grams is
limited within a window of fixed size. The size of
this window is determined by the maximum offset
parameter m. This parameter must be set a priori
and should be proportional to the size of the alpha-
bet, the p-grams, and to the lengths of the strings.
The following example offers a better under-
standing of how LRD actually works. LRD is
computed between two strings using 2-grams.
Example 1 Given two strings x = abcaa and
y = cabca, a fixed maximal offset m = 3, and
1366
a fixed size of p-grams p = 2, ?
left
and ?
right
are computed as follows:
?
left
(x, y) = |1? 2|+ |2? 3|
+ |3? 4|+ 3 = 6,
?
right
(x, y) = |1? 3|+ |2? 1|
+ |3? 2|+ |4? 3| = 5.
By summing up the two partial sums, Local Rank
Distance is obtained
?
LRD
(x, y) = ?
left
(x, y) + ?
right
(x, y) = 11.
The maximum LRD value between two strings
can be computed as the product between the max-
imum offset m and the number of pairs of com-
pared p-grams. Thus, LRD can be normalized
to a value in the [0, 1] interval. By normalizing,
LRD becomes a dissimilarity measure. LRD can
be also used as a kernel, since kernel methods are
based on similarity. The classical way to transform
a distance or dissimilarity measure into a simi-
larity measure is by using the Gaussian-like ker-
nel (Shawe-Taylor and Cristianini, 2004):
?
k
LRD
p
(s, t) = e
?
?
LRD
(s, t)
2?
2
,
where s and t are two strings and p is the p-grams
length. The parameter ? is usually chosen so that
values of
?
k(s, t) are well scaled. In the above
equation, ?
LRD
is already normalized to a value
in the [0, 1] interval to ensure a fair comparison of
strings of different length.
4 Learning Methods
Kernel-based learning algorithms work by embed-
ding the data into a Hilbert feature space, and
searching for linear relations in that space. The
embedding is performed implicitly, that is by spec-
ifying the inner product between each pair of
points rather than by giving their coordinates ex-
plicitly. More precisely, a kernel matrix that con-
tains the pairwise similarities between every pair
of training samples is used in the learning stage
to assign a vector of weights to the training sam-
ples. Let ? denote this weight vector. In the test
stage, the pairwise similarities between a test sam-
ple x and all the training samples are computed.
Then, the following binary classification function
assigns a positive or a negative label to the test
sample:
g(x) =
n
?
i=1
?
i
? k(x, x
i
),
where x is the test sample, n is the number of
training samples, X = {x
1
, x
2
, ..., x
n
} is the set
of training samples, k is a kernel function, and ?
i
is the weight assigned to the training sample x
i
.
In the primal form, the same binary classification
function can be expressed as:
g(x) = ?w, x?,
where ??, ?? denotes the scalar product, x ? R
m
is
the test sample represented as a vector of features,
and w ? R
m
is a vector of feature weights that can
be computed as follows:
w =
n
?
i=1
?
i
? x
i
,
given that the kernel function k can be expressed
as a scalar product between samples.
The advantage of using the dual representation
induced by the kernel function becomes clear if
the dimension of the feature space m is taken
into consideration. Since string kernels are based
on character n-grams, the feature space is indeed
very high. For instance, using 5-grams based only
on the 26 letters of the English alphabet will re-
sult in a feature space of 26
5
= 11, 881, 376 fea-
tures. However, in the experiments presented in
this work the feature space includes 5-grams along
with 6-grams, 7-grams and 8-grams. As long as
the number of samples n is not greater than the
number of features m, it is more efficient to use
the dual representation given by the kernel matrix.
This fact is also known as the kernel trick (Shawe-
Taylor and Cristianini, 2004).
Various kernel methods differ in the way they
learn to separate the samples. In the case of binary
classification problems, kernel-based learning al-
gorithms look for a discriminant function, a func-
tion that assigns +1 to examples belonging to one
class and ?1 to examples belonging to the other
class. For the NLI experiments, two binary kernel
classifiers are used, namely the SVM (Cortes and
Vapnik, 1995), and the KRR. Support Vector Ma-
chines try to find the vector of weights that defines
the hyperplane that maximally separates the im-
ages in the Hilbert space of the training examples
1367
belonging to the two classes. Kernel Ridge Re-
gression selects the vector of weights that simulta-
neously has small empirical error and small norm
in the Reproducing Kernel Hilbert Space gener-
ated by the kernel function. More details about
SVM and KRR can be found in (Shawe-Taylor and
Cristianini, 2004). The important fact is that the
above optimization problems are solved in such a
way that the coordinates of the embedded points
are not needed, only their pairwise inner products
which in turn are given by the kernel function.
SVM and KRR produce binary classifiers, but
native language identification is usually a multi-
class classification problem. There are many ap-
proaches for combining binary classifiers to solve
multi-class problems. Typically, the multi-class
problem is broken down into multiple binary clas-
sification problems using common decomposing
schemes such as: one-versus-all and one-versus-
one. There are also kernel methods that take the
multi-class nature of the problem directly into ac-
count, e.g. Kernel Discriminant Analysis. The
KDA classifier is able to improve accuracy by
avoiding the masking problem (Hastie and Tib-
shirani, 2003). In the case of multi-class native
language identification, the masking problem may
appear when non-native English speakers have ac-
quired, as the second language, a different lan-
guage to English. For example, an essay written in
English produced by a French native speaker that
is also proficient in German, could be identified as
either French or German.
5 Experiments
5.1 Data Sets Description
In this paper, experiments are carried out on three
datasets: a modified version of the ICLEv2 cor-
pus (Granger et al., 2009), the ETS Corpus of
Non-Native Written English, or TOEFL11 (Blan-
chard et al., 2013), and the TOEFL11-Big corpus
as used by Tetreault et al. (2012). A summary of
the corpora is given in Table 1.
Corpus Languages Documents
ICLE 7 770
TOEFL11 11 12, 100
TOEFL11-Big 11 87, 502
Table 1: Summary of corpora used in the experi-
ments.
The ICLEv2 is a corpus of essays written by
highly-proficient non-native college-level students
of English. For many years this was the standard
corpus used in the task of native language identi-
fication. However, the corpus was originally col-
lected for the purpose of corpus linguistic inves-
tigations, and because of this contains some id-
iosyncrasies that make it problematic for the task
of NLI (Brooke and Hirst, 2012). Therefore, a
modified version of the corpus that has been nor-
malized as much as possible for topic and charac-
ter encoding (Tetreault et al., 2012) is used. This
version of the corpus contains 110 essays each for
7 native languages: Bulgarian, Chinese, Czech,
French, Japanese, Russian and Spanish.
The ETS Corpus of Non-Native Written English
(TOEFL11) was first introduced by Tetreault et al.
(2012) and extended for the 2013 Native Language
Identification Shared Task (Tetreault et al., 2013).
It was designed to overcome many of the short-
comings identified with using the ICLEv2 corpus
for this task. The TOEFL11 corpus contains a
balanced distribution of essays per prompt (topic)
per native language. It also contains information
about the language proficiency of each writer. The
corpus contains essays written by speakers of the
following 11 languages: Arabic, Chinese, French,
German, Hindi, Italian, Japanese, Korean, Span-
ish, Telugu and Turkish. For the shared task, the
12, 100 essays were split into 9, 900 for training,
1, 100 for development and 1, 100 for testing.
Tetreault et al. (2012) present a corpus,
TOEFL11-Big, to investigate the performance of
their NLI system on a very large data set. This
data set contains the same languages as TOEFL11,
but with no overlap in content. It contains a total
of over 87 thousand essays written to a total of
76 different prompts. The distribution of L1 per
prompt is not as even as for TOEFL11, though all
topics are represented for all L1s.
5.2 Parameter Tuning and Implementation
Choices
In the string kernels approach proposed in this
work, documents or essays from this corpus are
treated as strings. Therefore, the notions of string
or document is used interchangeably throughout
this work. Because the approach works at the char-
acter level, there is no need to split the texts into
words, or to do any NLP-specific preprocessing.
The only editing done to the texts was the replac-
ing of sequences of consecutive space characters
1368
(space, tab, new line, and so on) with a single
space character. This normalization was needed in
order to prevent the artificial increase or decrease
of the similarity between texts, as a result of differ-
ent spacing. All uppercase letters were converted
to the corresponding lowercase ones.
A series of preliminary experiments were con-
ducted in order to select the best-performing learn-
ing method. In these experiments the string ker-
nel was fixed to the p-spectrum normalized ker-
nel of length 5 (
?
k
5
), because the goal was to se-
lect the best learning method, and not to find the
best kernel. The following learning methods were
evaluated: one-versus-one SVM, one-versus-all
SVM, one-versus-one KRR, one-versus-all KRR,
and KDA. A 10-fold cross-validation procedure
was carried out on the TOEFL11 training set to
evaluate the classifiers. The preliminary results in-
dicate that the one-versus-all KRR and the KDA
classifiers produce the best results. Therefore,
they are selected for the remaining experiments.
Another set of preliminary experiments were
performed to determine the range of n-grams that
gives the most accurate results on a 10-fold cross-
validation procedure carried out on the TOEFL11
training set. All the n-grams in the range 2-10
were evaluated. Furthermore, experiments with
different blended kernels were conducted to see
whether combining n-grams of different lengths
could improve the accuracy. The best results were
obtained when all the n-grams with the length in
the range 5-8 were used. Other authors (Bykh
and Meurers, 2012; Popescu and Ionescu, 2013)
also report better results by using n-grams with
the length in a range, rather than using n-grams
of fixed length. Consequently, the results reported
in this work are based on blended string kernels
based on 5-8 n-grams.
Some preliminary experiments were also per-
formed to establish the type of kernel to be used,
namely the blended p-spectrum kernel (
?
k
5?8
), the
blended p-grams presence bits kernel (
?
k
0/1
5?8
), the
blended p-grams intersection kernel (
?
k
?
5?8
), or the
kernel based on LRD (
?
k
LRD
5?8
.). These different
kernel representations are obtained from the same
data. The idea of combining all these kernels is
natural when one wants to improve the perfor-
mance of a classifier. When multiple kernels are
combined, the features are actually embedded in
a higher-dimensional space. As a consequence,
the search space of linear patterns grows, which
helps the classifier to select a better discriminant
function. The most natural way of combining two
kernels is to sum them up. Summing up kernels
or kernel matrices is equivalent to feature vector
concatenation. Another option is to combine ker-
nels by kernel alignment (Cristianini et al., 2001).
Instead of simply summing kernels, kernel align-
ment assigns weights for each of the two kernels
based on how well they are aligned with the ideal
kernel Y Y
?
obtained from training labels. The ker-
nels were evaluated alone and in various combina-
tions. The best kernels are the blended p-grams
presence bits kernel and the blended p-grams in-
tersection kernel. The best kernel combinations
include the blended p-grams presence bits kernel,
the blended p-grams intersection kernel and the
kernel based on LRD. Since the kernel based on
LRD is slightly slower than the other string ker-
nels, the kernel combinations that include it were
only evaluated on the TOEFL11 corpus and on the
ICLE corpus.
5.3 Experiment on TOEFL11 Corpus
This section describes the results on the TOEFL11
corpus. Thus, results for the 2013 Closed NLI
Shared Task are also included. In the closed shared
task the goal is to predict the native language of
testing examples, restricted to learning only from
the training and the development data. The ad-
ditional information from prompts or the English
language proficiency level were not used in the
proposed approach.
The regularization parameters were tuned on the
development set. In this case, the systems were
trained on the entire training set. A 10-fold cross-
validation (CV) procedure was done on the train-
ing and the development sets. The folds were pro-
vided along with the TOEFL11 corpus. Finally,
the results of the proposed systems are also re-
ported on the NLI Shared Task test set. For test-
ing, the systems were trained on both the training
set and the development set. The results are sum-
marized in Table 2.
The results presented in Table 2 show that string
kernels can reach state of the art accuracy levels
for this task. Overall, it seems that KDA is able
to obtain better results than KRR. The intersection
kernel alone is able to obtain slightly better results
than the presence bits kernel. The kernel based on
LRD gives significantly lower accuracy rates, but
it is able to improve the performance when it is
1369
Method Development 10-fold CV Test
Ensemble model (Tetreault et al., 2012) - 80.9% -
KRR and string kernels (Popescu and Ionescu, 2013) - 82.6% 82.7%
SVM and word features (Jarvis et al., 2013) - 84.5% 83.6%
KRR and
?
k
0/1
5?8
85.4% 82.5% 82.0%
KRR and
?
k
?
5?8
84.9% 82.2% 82.6%
KRR and
?
k
LRD
5?8
78.7% 77.1% 77.5%
KRR and
?
k
0/1
5?8
+
?
k
LRD
5?8
85.7% 82.6% 82.7%
KRR and
?
k
?
5?8
+
?
k
LRD
5?8
84.9% 82.2% 82.0%
KRR and
?
k
0/1
5?8
+
?
k
?
5?8
85.5% 82.6% 82.5%
KRR and a
1
?
k
0/1
5?8
+ a
2
?
k
?
5?8
85.5% 82.6% 82.5%
KDA and
?
k
0/1
5?8
86.2% 83.6% 83.6%
KDA and
?
k
?
5?8
85.2% 83.5% 84.6%
KDA and
?
k
LRD
5?8
79.7% 78.5% 79.2%
KDA and
?
k
0/1
5?8
+
?
k
LRD
5?8
87.1% 84.0% 84.7%
KDA and
?
k
?
5?8
+
?
k
LRD
5?8
85.8% 83.4% 83.9%
KDA and
?
k
0/1
5?8
+
?
k
?
5?8
86.4% 84.1% 85.0%
KDA and a
1
?
k
0/1
5?8
+ a
2
?
k
?
5?8
86.5% 84.1% 85.3%
KDA and
?
k
0/1
5?8
+
?
k
?
5?8
+
?
k
LRD
5?8
87.0% 84.1% 84.8%
Table 2: Accuracy rates on TOEFL11 corpus of various classification systems based on string kernels
compared with other state of the art approaches. The best accuracy rates on each set of experiments are
highlighted in bold. The weights a
1
and a
2
from the weighted sums of kernels are computed by kernel
alignment.
combined with the blended p-grams presence bits
kernel. In fact, most of the kernel combinations
give better results than each of their components.
The best kernel combination is that of the pres-
ence bits kernel and the intersection kernel. Re-
sults are quite similar when they are combined ei-
ther by summing them up or by kernel alignment.
The best performance on the test set (85.3%) is ob-
tained by the system that combines these two ker-
nels via kernel alignment and learns using KDA.
This system is 1.7% better than the state of the art
system of Jarvis et al. (2013) based on SVM and
word features, this being the top scoring system in
the NLI 2013 Shared Task. It is also 2.6% better
than the state of the art system based on string ker-
nels of Popescu and Ionescu (2013). On the cross
validation procedure, there are three systems that
reach the accuracy rate of 84.1%. All of them are
based on KDA and various kernel combinations.
The greatest accuracy rate of 84.1% reported for
the cross validation procedure is 3.2% above the
state of the art system of Tetreault et al. (2012) and
0.4% below the top scoring system of Jarvis et al.
(2013). The empirical results obtained in this ex-
periment demonstrate that the approach proposed
in this paper can reach state of the art accuracy
levels. It is worth mentioning that a significance
test performed by the organizers of the NLI 2013
Shared Task showed that the top systems that par-
ticipated in the competition are not essentially dif-
ferent. Further experiments on the ICLE corpus
and on the TOEFL11-Big corpus are conducted to
determine whether the approach proposed in this
paper is significantly better than other state of the
art approaches.
5.4 Experiment on ICLE Corpus
The results on the ICLE corpus using a 5-fold
cross validation procedure are summarized in Ta-
ble 3. To adequately compare the results with a
state of the art system, the same 5-fold cross val-
idation procedure used by Tetreault et al. (2012)
was also used in this experiment. Table 3 shows
that the results obtained by the presence bits kernel
and by the intersection kernel are systematically
better than the state of the art system of Tetreault
et al. (2012). While both KRR and KDA produce
accuracy rates that are better than the state of the
art accuracy rate, it seems that KRR is slightly bet-
ter in this experiment. Again, the idea of com-
bining kernels seems to produce more robust sys-
tems. The best systems are based on combin-
ing the presence bits kernel either with the kernel
based on LRD or the intersection kernel. Over-
all, the reported accuracy rates are higher than the
state of the art accuracy rate. The best perfor-
mance (91.3%) is achieved by the KRR classifier
based on combining the presence bits kernel with
1370
Method 5-fold CV
Ensemble model (Tetreault et al., 2012) 90.1%
KRR and
?
k
0/1
5?8
91.2%
KRR and
?
k
?
5?8
90.5%
KRR and
?
k
LRD
5?8
81.8%
KRR and
?
k
0/1
5?8
+
?
k
LRD
5?8
91.3%
KRR and
?
k
?
5?8
+
?
k
LRD
5?8
90.1%
KRR and
?
k
0/1
5?8
+
?
k
?
5?8
90.9%
KRR and
?
k
0/1
5?8
+
?
k
?
5?8
+
?
k
LRD
5?8
90.6%
KDA and
?
k
0/1
5?8
90.5%
KDA and
?
k
?
5?8
90.5%
KDA and
?
k
LRD
5?8
82.3%
KDA and
?
k
0/1
5?8
+
?
k
LRD
5?8
90.8%
KDA and
?
k
?
5?8
+
?
k
LRD
5?8
90.4%
KDA and
?
k
0/1
5?8
+
?
k
?
5?8
91.0%
KDA and
?
k
0/1
5?8
+
?
k
?
5?8
+
?
k
LRD
5?8
90.8%
Table 3: Accuracy rates on ICLE corpus of vari-
ous classification systems based on string kernels
compared with a state of the art approach. The ac-
curacy rates are reported for the same 5-fold CV
procedure as in (Tetreault et al., 2012). The best
accuracy rate is highlighted in bold.
the kernel based on LRD. This represents an 1.2%
improvement over the state of the art accuracy rate
of Tetreault et al. (2012). Two more systems are
able to obtain accuracy rates greater than 91.0%.
These are the KRR classifier based on the presence
bits kernel (91.2%) and the KDA classifier based
on the sum of the presence bits kernel and the in-
tersection kernel (91.0%). The overall results on
the ICLE corpus show that the string kernels ap-
proach can reach state of the art accuracy levels.
It is worth mentioning the purpose of this experi-
ment was to use the same approach determined to
work well in the TOEFL11 corpus. To serve this
purpose, the range of n-grams was not tuned on
this data set. Furthermore, other classifiers were
not tested in this experiment. Nevertheless, better
results can probably be obtained by adding these
aspects into the equation.
5.5 Cross-corpus Experiment
In this experiment, various systems based on KRR
or KDA are trained on the TOEFL11 corpus and
tested on the TOEFL11-Big corpus. The kernel
based on LRD was not included in this experiment
since it is more computationally expensive. There-
fore, only the presence bits kernel and the intersec-
tion kernel were evaluated on the TOEFL11-Big
corpus. The results are summarized in Table 4.
The same regularization parameters determined to
Method Test
Ensemble model (Tetreault et al., 2012) 35.4%
KRR and
?
k
0/1
5?8
66.7%
KRR and
?
k
?
5?8
67.2%
KRR and
?
k
0/1
5?8
+
?
k
?
5?8
67.7%
KRR and a
1
?
k
0/1
5?8
+ a
2
?
k
?
5?8
67.7%
KDA and
?
k
0/1
5?8
65.6%
KDA and
?
k
?
5?8
65.7%
KDA and
?
k
0/1
5?8
+
?
k
?
5?8
66.2%
KDA and a
1
?
k
0/1
5?8
+ a
2
?
k
?
5?8
66.2%
Table 4: Accuracy rates on TOEFL11-Big corpus
of various classification systems based on string
kernels compared with a state of the art approach.
The systems are trained on the TOEFL11 corpus
and tested on the TOEFL11-Big corpus. The best
accuracy rate is highlighted in bold. The weights
a
1
and a
2
from the weighted sums of kernels are
computed by kernel alignment.
work well on the TOEFL11 development set were
used.
The most interesting fact is that all the proposed
systems are at least 30% better than the state of the
art system. Considering that the TOEFL11-Big
corpus contains 87 thousand samples, the 30% im-
provement is significant without any doubt. Div-
ing into details, it can be observed that the results
obtained by KRR are higher than those obtained
by KDA. However, both methods perform very
well compared to the state of the art. Again, kernel
combinations are better than each of their individ-
ual kernels alone.
It is important to mention that the significant
performance increase is not due to the learning
method (KRR or KDA), but rather due to the string
kernels that work at the character level. It is not
only the case that string kernels are language in-
dependent, but for the same reasons they can also
be topic independent. Since the topics (prompts)
from TOEFL11 are different from the topics from
TOEFL11-Big, it becomes clear that a method
that uses words as features is strongly affected,
since the distribution of words per topic can be
completely different. But mistakes that reveal the
native language can be captured by character n-
grams that can appear more often even in differ-
ent topics. The results indicate that this is also
the case of the approach based on string kernels,
which seems to be more robust to such topic vari-
ations of the data set. The best system has an ac-
curacy rate that is 32.3% better than the state of
1371
the art system of Tetreault et al. (2012). Overall,
the empirical results indicate that the string ker-
nels approach can achieve significantly better re-
sults than other state of the art approaches.
6 Conclusions
A language-independent approach to native lan-
guage identification was presented in this paper.
The system works at the character level, mak-
ing the approach completely language indepen-
dent and linguistic theory neutral. The results ob-
tained in all the three experiments were very good.
The best system presented in this work is based on
combining the intersection and the presence string
kernels by kernel alignment and on deciding the
class label either with KDA or KRR. The best sys-
tem is 1.7% above the top scoring system of the
2013 NLI Shared Task. Furthermore, it has an im-
pressive generalization capacity, achieving results
that are 30% higher than the state of the art method
in the cross-corpus experiment.
Despite the fact that the approach based on
string kernels performed so well, it remains to be
further investigated why this is the case and why
such a simple approach can compete with far more
complex approaches that take words, lemmas,
syntactic information, or even semantics into ac-
count. It seems that there are generalizations to the
kinds of mistakes that certain non-native English
speakers make that can be captured by n-grams
of different lengths. Interestingly, using a range
of n-grams generates a large number of features
including (but not limited to) stop words, stems
of content words, word suffixes, entire words, and
even n-grams of short words. Rather than doing
feature selection before the training step, which
is the usual NLP approach, the kernel classifier
selects the most relevant features during training.
With enough training samples, the kernel classi-
fier does a better job of selecting the right features
from a very high feature space. This may be one
reason for why the string kernel approach works
so well. To gain additional insights into why this
technique is working well, the features selected
by the classifier as being more discriminating can
be analyzed in future work. This analysis would
also offer some information about localized lan-
guage transfer effects, since the features used by
the proposed model are n-grams of lengths 5 to
8. As mentioned before, the features captured by
the model typically include stems, function words,
word prefixes and suffixes, which have the poten-
tial to generalize over purely word-based features.
These features would offer insights into two kinds
of language transfer effects, namely word choice
(lexical transfer) and morphological differences.
Acknowledgments
The authors would like to thank Beata Beigman
Klebanov, Nitin Madnani and Xinhao Wang from
ETS for their helpful comments and suggestions.
The author also thank the anonymous reviewers
for their valuable insights which lead to improve-
ments in the presentation of this work.
References
Daniel Blanchard, Joel Tetreault, Derrick Higgins,
Aoife Cahill, and Martin Chodorow. 2013.
TOEFL11: A Corpus of Non-Native English. Tech-
nical report, Educational Testing Service Research
Report No. RR?13?24.
Julian Brooke and Graeme Hirst. 2012. Robust, Lex-
icalized Native Language Identification. Proceed-
ings of COLING 2012, pages 391?408, December.
Serhiy Bykh and Detmar Meurers. 2012. Native Lan-
guage Identification using Recurring n-grams ? In-
vestigating Abstraction and Domain Dependence.
Proceedings of COLING 2012, pages 425?440, De-
cember.
Corinna Cortes and Vladimir Vapnik. 1995. Support-
Vector Networks. Machine Learning, 20(3):273?
297.
Nello Cristianini, John Shawe-Taylor, Andr?e Elisseeff,
and Jaz S. Kandola. 2001. On kernel-target align-
ment. Proceedings of NIPS, pages 367?373, De-
cember.
Liviu P. Dinu. 2003. On the classification and aggre-
gation of hierarchies with different constitutive ele-
ments. Fundamenta Informaticae, 55(1):39?50.
Dominique Estival, Tanja Gaustad, Son-Bao Pham,
Will Radford, and Ben Hutchinson. 2007. Author
profiling for English emails. Proceedings of PA-
CLING, pages 263?272.
Sylviane Granger, Estelle Dagneaux, and Fanny Me-
unier. 2009. The International Corpus of
Learner English: Handbook and CD-ROM, version
2. Presses Universitaires de Louvain, Louvain-la-
Neuve, Belgium.
Cristian Grozea, Christian Gehl, and Marius Popescu.
2009. ENCOPLOT: Pairwise Sequence Matching
in Linear Time Applied to Plagiarism Detection. In
3rd PAN Workshop. Uncovering Plagiarism, Author-
ship, and Social Software Misuse, page 10.
1372
Trevor Hastie and Robert Tibshirani. 2003. The El-
ements of Statistical Learning. Springer, corrected
edition, July.
Radu Tudor Ionescu. 2013. Local Rank Distance.
Proceedings of SYNASC, pages 221?228.
Scott Jarvis, Yves Bestgen, and Steve Pepper. 2013.
Maximizing classification accuracy in native lan-
guage identification. Proceedings of the Eighth
Workshop on Innovative Use of NLP for Building
Educational Applications, pages 111?118, June.
Moshe Koppel, Jonathan Schler, and Kfir Zigdon.
2005. Automatically Determining an Anonymous
Author?s Native Language. Proceedings of ISI,
pages 209?217.
Huma Lodhi, Craig Saunders, John Shawe-Taylor,
Nello Cristianini, and Christopher J. C. H. Watkins.
2002. Text classification using string kernels. Jour-
nal of Machine Learning Research, 2:419?444.
Subhransu Maji, Alexander C. Berg, and Jitendra Ma-
lik. 2008. Classification using intersection kernel
support vector machines is efficient. Proceedings of
CVPR.
Shervin Malmasi and Mark Dras. 2014. Chinese Na-
tive Language Identification. Proceedings of EACL,
2:95?99, April.
Marius Popescu and Liviu P. Dinu. 2007. Kernel meth-
ods and string kernels for authorship identification:
The federalist papers case. Proceedings of RANLP,
September.
Marius Popescu and Cristian Grozea. 2012. Ker-
nel methods and string kernels for authorship analy-
sis. CLEF (Online Working Notes/Labs/Workshop),
September.
Marius Popescu and Radu Tudor Ionescu. 2013. The
Story of the Characters, the DNA and the Native
Language. Proceedings of the Eighth Workshop on
Innovative Use of NLP for Building Educational Ap-
plications, pages 270?278, June.
Marius Popescu. 2011. Studying translationese at the
character level. Proceedings of RANLP, pages 634?
639, September.
Conrad Sanderson and Simon Guenter. 2006. Short
text authorship attribution via sequence kernels,
markov chains and author unmasking: An investiga-
tion. Proceedings of EMNLP, pages 482?491, July.
John Shawe-Taylor and Nello Cristianini. 2004. Ker-
nel Methods for Pattern Analysis. Cambridge Uni-
versity Press.
Joel Tetreault, Daniel Blanchard, Aoife Cahill, and
Martin Chodorow. 2012. Native Tongues, Lost and
Found: Resources and Empirical Evaluations in Na-
tive Language Identification. Proceedings of COL-
ING 2012, pages 2585?2602, December.
Joel Tetreault, Daniel Blanchard, and Aoife Cahill.
2013. A report on the first native language identifi-
cation shared task. Proceedings of the Eighth Work-
shop on Innovative Use of NLP for Building Educa-
tional Applications, pages 48?57, June.
Andrea Vedaldi and Andrew Zisserman. 2010. Effi-
cient additive kernels via explicit feature maps. Pro-
ceedings of CVPR, pages 3539?3546.
1373
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 664?674,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Modeling Inflection and Word-Formation in SMT
Alexander Fraser? Marion Weller? Aoife Cahill? Fabienne Cap?
?Institut fu?r Maschinelle Sprachverarbeitung ?Educational Testing Service
Universita?t Stuttgart Princeton, NJ 08541
D?70174 Stuttgart, Germany USA
{fraser,wellermn,cap}@ims.uni-stuttgart.de acahill@ets.org
Abstract
The current state-of-the-art in statistical
machine translation (SMT) suffers from is-
sues of sparsity and inadequate modeling
power when translating into morphologi-
cally rich languages. We model both in-
flection and word-formation for the task
of translating into German. We translate
from English words to an underspecified
German representation and then use linear-
chain CRFs to predict the fully specified
German representation. We show that im-
proved modeling of inflection and word-
formation leads to improved SMT.
1 Introduction
Phrase-based statistical machine translation
(SMT) suffers from problems of data sparsity
with respect to inflection and word-formation
which are particularly strong when translating to
a morphologically rich target language, such as
German. We address the problem of inflection
by first translating to a stem-based representation,
and then using a second process to inflect these
stems. We study several models for doing
this, including: strongly lexicalized models,
unlexicalized models using linguistic features,
and models combining the strengths of both of
these approaches. We address the problem of
word-formation for compounds in German, by
translating from English into German word parts,
and then determining whether to merge these
parts to form compounds.
We make the following new contributions: (i)
we introduce the first SMT system combining
inflection prediction with synthesis of portman-
teaus and compounds. (ii) For inflection, we com-
pare the mostly unlexicalized prediction of lin-
guistic features (with a subsequent surface form
generation step) versus the direct prediction of
surface forms, and show that both approaches
have complementary strengths. (iii) We com-
bine the advantages of the prediction of linguis-
tic features with the prediction of surface forms.
We implement this in a CRF framework which
improves on a standard phrase-based SMT base-
line. (iv) We develop separate (but related) pro-
cedures for inflection prediction and dealing with
word-formation (compounds and portmanteaus),
in contrast with most previous work which usu-
ally either approaches both problems as inflec-
tional problems, or approaches both problems as
word-formation problems.
We evaluate on the end-to-end SMT task of
translating from English to German of the 2009
ACL workshop on SMT. We achieve BLEU score
increases on both the test set and the blind test set.
2 Overview of the translation process for
inflection prediction
The work we describe is focused on generaliz-
ing phrase-based statistical machine translation to
better model German NPs and PPs. We particu-
larly want to ensure that we can generate novel
German NPs, where what we mean by novel is
that the (inflected) realization is not present in the
parallel German training data used to build the
SMT system, and hence cannot be produced by
our baseline (a standard phrase-based SMT sys-
tem). We first present our system for dealing with
the difficult problem of inflection in German, in-
cluding the inflection-dependent phenomenon of
portmanteaus. Later, after performing an exten-
sive analysis of this system, we will extend it
664
to model compounds, a highly productive phe-
nomenon in German (see Section 8).
The key linguistic knowledge sources that we
use are morphological analysis and generation of
German based on SMOR, a morphological ana-
lyzer/generator of German (Schmid et al 2004)
and the BitPar parser, which is a state-of-the-art
parser of German (Schmid, 2004).
2.1 Issues of inflection prediction
In order to ensure coherent German NPs, we
model linguistic features of each word in an NP.
We model case, gender, and number agreement
and whether or not the word is in the scope of
a determiner (such as a definite article), which
we label in-weak-context (this linguistic feature
is necessary to determine the type of inflection of
adjectives and other words: strong, weak, mixed).
This is a diverse group of features. The number
of a German noun can often be determined given
only the English source word. The gender of a
German noun is innate and often difficult to deter-
mine given only the English source word. Case
is a function of the slot in the subcategorization
frame of the verb (or preposition). There is agree-
ment in all of these features in an NP. For instance
the number of an article or adjective is determined
by the head noun, while the type of inflection of an
adjective is determined by the choice of article.
We can have a large number of surface forms.
For instance, English blue can be translated as
German blau, blaue, blauer, blaues, blauen. We
predict which form is correct given the context.
Our system can generate forms not seen in the
training data. We follow a two-step process: in
step-1 we translate to blau (the stem), in step-2 we
predict features and generate the inflected form.1
2.2 Procedure
We begin building an SMT system by parsing the
German training data with BitPar. We then extract
morphological features from the parse. Next, we
lookup the surface forms in the SMOR morpholog-
ical analyzer. We use the morphological features
in the parse to disambiguate the set of possible
SMOR analyses. Finally, we output the ?stems?
of the German text, with the addition of markup
taken from the parse (discussed in Section 2.3).
1E.g., case=nominative, gender=masculine, num-
ber=singular, in-weak-context=true; inflected: blaue.
We then build a standard Moses system trans-
lating from English to German stems. We obtain
a sequence of stems and POS2 from this system,
and then predict the correct inflection using a se-
quence model. Finally we generate surface forms.
2.3 German Stem Markup
The translation process consists of two major
steps. The first step is translation of English
words to German stems, which are enriched with
some inflectional markup. The second step is
the full inflection of these stems (plus markup)
to obtain the final sequence of inflected words.
The purpose of the additional German inflectional
markup is to strongly improve prediction of in-
flection in the second step through the addition of
markup to the stems in the first step.
In general, all features to be predicted are
stripped from the stemmed representation because
they are subject to agreement restrictions of a
noun or prepositional phrase (such as case of
nouns or all features of adjectives). However, we
need to keep all morphological features that are
not dependent on, and thus not predictable from,
the (German) context. They will serve as known
input for the inflection prediction model. We now
describe this markup in detail.
Nouns are marked with gender and number: we
consider the gender of a noun as part of its stem,
whereas number is a feature which we can obtain
from English nouns.
Personal pronouns have number and gender an-
notation, and are additionally marked with nom-
inative and not-nominative, because English pro-
nouns are marked for this (except for you).
Prepositions are marked with the case their ob-
ject takes: this moves some of the difficulty in pre-
dicting case from the inflection prediction step to
the stem translation step. Since the choice of case
in a PP is often determined by the PP?s meaning
(and there are often different meanings possible
given different case choices), it seems reasonable
to make this decision during stem translation.
Verbs are represented using their inflected surface
form. Having access to inflected verb forms has a
positive influence on case prediction in the second
2We use an additional target factor to obtain the coarse
POS for each stem, applying a 7-gram POS model. Koehn
and Hoang (2007) showed that the use of a POS factor only
results in negligible BLEU improvements, but we need ac-
cess to the POS in our inflection prediction models.
665
input decoder output inflected merged
in
in<APPR><Dat> in
im
die<+ART><Def> dem
contrast Gegensatz<+NN><Masc><Sg>Gegensatz Gegensatz
to zu<APPR><Dat> zu
zur
the die<+ART><Def> der
animated lebhaft<+ADJ><Pos> lebhaften lebhaften
debate Debatte<+NN><Fem><Sg> Debatte Debatte
Table 1: Re-merging of prepositions and articles after
inflection to form portmanteaus, in dem means in the.
step through subject-verb agreement.
Articles are reduced to their stems (the stem itself
makes clear the definite or indefinite distinction,
but lemmatizing involves removing markings of
case, gender and number features).
Other words are also represented by their stems
(except for words not covered by SMOR, where
surface forms are used instead).
3 Portmanteaus
Portmanteaus are a word-formation phenomenon
dependent on inflection. As we have discussed,
standard phrase-based systems have problems
with picking a definite article with the correct
case, gender and number (typically due to spar-
sity in the language model, e.g., a noun which
was never before seen in dative case will often
not receive the correct article). In German, port-
manteaus increase this sparsity further, as they
are compounds of prepositions and articles which
must agree with a noun.
We adopt the linguistically strict definition of
the term portmanteau: the merging of two func-
tion words.3 We treat this phenomena by split-
ting the component parts during training and re-
merging during generation. Specifically for
German, this requires splitting the words which
have German POS tag APPRART into an APPR
(preposition) and an ART (article). Merging is re-
stricted, the article must be definite, singular4 and
the preposition can only take accusative or dative
case. Some prepositions allow for merging with
an article only for certain noun genders, for exam-
ple the preposition inDative is only merged with
the following article if the following noun is of
masculine or neuter gender. The definite article
3Some examples are: zum (to the) = zu (to) + dem (the)
[German], du (from the) = de (from) + le (the) [French] or al
(to the) = a (to) + el (the) [Spanish].
4This is the reason for which the preposition + article in
Table 2 remain unmerged.
must be inflected before making a decision about
whether to merge a preposition and the article into
a portmanteau. See Table 1 for examples.
4 Models for Inflection Prediction
We present 5 procedures for inflectional predic-
tion using supervised sequence models. The first
two procedures use simple N-gram models over
fully inflected surface forms.
1. Surface with no features is presented with an
underspecified input (a sequence of stems), and
returns the most likely inflected sequence.
2. Surface with case, number, gender is a hybrid
system giving the surface model access to linguis-
tic features. In this system prepositions have addi-
tionally been labeled with the case they mark (in
both the underspecified input and the fully spec-
ified output the sequence model is built on) and
gender and number markup is also available.
The rest of the procedures predict morpholog-
ical features (which are input to a morphological
generator) rather than surface words. We have de-
veloped a two-stage process for predicting fully
inflected surface forms. The first stage takes a
stem and predicts morphological features for that
stem, based on the surrounding context. The aim
of the first stage is to take a stem and predict
four morphological features: case, gender, num-
ber and type of inflection. We experiment with
a number of models for doing this. The sec-
ond stage takes the stems marked with morpho-
logical features (predicted in the first stage) and
uses a morphological generator to generate the
full surface form. For the second stage, a modified
version of SMOR (Schmid et al 2004) is used,
which, given a stem annotated with morphologi-
cal features, generates exactly one surface form.
We now introduce our first linguistic feature
prediction systems, which we call joint sequence
models (JSMs). These are standard language
models, where the ?word? tokens are not repre-
sented as surface forms, but instead using POS
and features. In testing, we supply the input as a
sequence in underspecified form, where some of
the features are specified in the stem markup (for
instance, POS=Noun, gender=masculine, num-
ber=plural), and then use Viterbi search to find the
most probable fully specified form (for instance,
POS=Noun, gender=masculine, number=plural,
666
output decoder input prediction output prediction inflected forms gloss
haben<VAFIN> haben-V haben-V haben have
Zugang<+NN><Masc><Sg> NN-Sg-Masc NN-Masc.Acc.Sg.in-weak-context=false Zugang access
zu<APPR><Dat> APPR-zu-Dat APPR-zu-Dat zu to
die<+ART><Def> ART-in-weak-context=true ART-Neut.Dat.Pl.in-weak-context=true den the
betreffend<+ADJ><Pos> ADJA ADJA-Neut.Dat.Pl.in-weak-context=true betreffenden respective
Land<+NN><Neut><Pl> NN-Pl-Neut NN-Neut.Dat.Pl.in-weak-context=true La?ndern countries
Table 2: Overview: inflection prediction steps using a single joint sequence model. All words except verbs and
prepositions are replaced by their POS tags in the input. Verbs are inflected in the input (?haben?, meaning
?have? as in ?they have?, in the example). Prepositions are lexicalized (?zu? in the example) and indicate which
case value they mark (?Dat?, i.e., Dative in the example).
case=nominative, in-weak-context=true).5
3. Single joint sequence model on features. We
illustrate the different stages of the inflection pre-
diction when using a joint sequence model. The
stemmed input sequence (cf. Section 2.3) contains
several features that will be part of the input to
the inflection prediction. With the exception of
verbs and prepositions, the representation for fea-
ture prediction is based on POS-tags.
As gender and number are given by the heads
of noun phrases and prepositional phrases, and
the expected type of inflection is set by articles,
the model has sufficient information to compute
values for these features and there is no need to
know the actual words. In contrast, the prediction
of case is more difficult as it largely depends on
the content of the sentence (e.g. which phrase is
object, which phrase is subject). Assuming that
verbs and prepositions indicate subcategorization
frames, the model is provided crucial information
for the prediction of case by keeping verbs (recall
that verbs are produced by the stem translation
system in their inflected form) and prepositions
(the prepositions also have case markup) instead
of replacing them with their tags.
After having predicted a single label with val-
ues for all features, an inflected word form for the
stem and the features is generated. The prediction
steps are illustrated in Table 2.
4. Using four joint sequence models (one for
each linguistic feature). Here the four linguistic
feature values are predicted separately. The as-
sumption that the different linguistic features can
be predicted independently of one another is a rea-
5Joint sequence models are a particularly simple HMM.
Unlike the HMMs used for POS-tagging, an HMM as used
here only has a single emission possibility for each state,
with probability 1. The states in the HMM are the fully
specified representation. The emissions of the HMM are the
stems+markup (the underspecified representation).
sonable linguistic assumption to make given the
additional German markup that we use. By split-
ting the inflection prediction problem into 4 com-
ponent parts, we end up with 4 simpler models
which are less sensitive to data sparseness.
Each linguistic feature is modeled indepen-
dently (by a JSM) and has a different input rep-
resentation based on the previously described
markup. The input consists of a sequence of
coarse POS tags, and for those stems that are
marked up with the relevant feature, this feature
value. Finally, we combine the predicted fea-
tures together to produce the same final output as
the single joint sequence model, and then generate
each surface form using SMOR.
5. Using four CRFs (one for each linguistic fea-
ture). The sequence models already presented are
limited to the n-gram feature space, and those that
predict linguistic features are not strongly lexi-
calized. Toutanova et al(2008) uses an MEMM
which allows the integration of a wide variety of
feature functions. We also wanted to experiment
with additional feature functions, and so we train
4 separate linear chain CRF6 models on our data
(one for each linguistic feature we want to pre-
dict). We chose CRFs over MEMMs to avoid the
label bias problem (Lafferty et al 2001).
The CRF feature functions, for each German
word wi, are in Table 3. The common feature
functions are used in all models, while each of the
4 separate models (one for each linguistic feature)
includes the context of only that linguistic feature.
We use L1 regularization to eliminate irrelevant
feature functions, the regularization parameter is
optimized on held out data.
6We use the Wapiti Toolkit (Lavergne et al 2010) on 4
x 12-Core Opteron 6176 2.3 GHz with 256GB RAM to train
our CRF models. Training a single CRF model on our data
was not tractable, so we use one for each linguistic feature.
667
Common lemmawi?5...wi+5 , tagwi?7...wi+7
Case casewi?5...wi+5
Gender genderwi?5...wi+5
Number numberwi?5...wi+5
in-weak-context in-weak-contextwi?5...wi+5
Table 3: Feature functions used in CRF models (fea-
ture functions are binary indicators of the pattern).
5 Experimental Setup
To evaluate our end-to-end system, we perform
the well-studied task of news translation, us-
ing the Moses SMT package. We use the En-
glish/German data released for the 2009 ACL
Workshop on Machine Translation shared task on
translation.7 There are 82,740 parallel sentences
from news-commentary09.de-en and 1,418,115
parallel sentences from europarl-v4.de-en. The
monolingual data contains 9.8 M sentences.8
To build the baseline, the data was tokenized
using the Moses tokenizer and lowercased. We
use GIZA++ to generate alignments, by running
5 iterations of Model 1, 5 iterations of the HMM
Model, and 4 iterations of Model 4. We sym-
metrize using the ?grow-diag-final-and? heuris-
tic. Our Moses systems use default settings. The
LM uses the monolingual data and is trained as
a five-gram9 using the SRILM-Toolkit (Stolcke,
2002). We run MERT separately for each sys-
tem. The recaser used is the same for all systems.
It is the standard recaser supplied with Moses,
trained on all German training data. The dev set
is wmt-2009-a and the test set is wmt-2009-b, and
we report end-to-end case sensitive BLEU scores
against the unmodified reference SGML file. The
blind test set used is wmt-2009-blind (all lines).
In developing our inflection prediction sys-
tems (and making such decisions as n-gram order
used), we worked on the so-called ?clean data?
task, predicting the inflection on stemmed refer-
ence sentences (rather than MT output). We used
the 2000 sentence dev-2006 corpus for this task.
Our contrastive systems consist of two steps,
the first is a translation step using a similar
Moses system (except that the German side is
stemmed, with the markup indicated in Sec-
7http://www.statmt.org/wmt09/translation-task.html
8However, we reduced the monolingual data (only) by
retaining only one copy of each unique line, which resulted
in 7.55 M sentences.
9Add-1 smoothing for unigrams and Kneser-Ney
smoothing for higher order n-grams, pruning defaults.
tion 2.3), and the second is inflection prediction
as described previously in the paper. To derive
the stem+markup representation we first parse
the German training data and then produce the
stemmed representation. We then build a sys-
tem for translating from English words to Ger-
man stems (the stem+markup representation), on
the same data (so the German side of the parallel
data, and the German language modeling uses the
stem+markup representation). Likewise, MERT
is performed using references which are in the
stem+markup representation.
To train the inflection prediction systems, we
use the monolingual data. The basic surface form
model is trained on lowercased surface forms,
the hybrid surface form model with features is
trained on lowercased surface forms annotated
with markup. The linguistic feature prediction
systems are trained on the monolingual data pro-
cessed as described previously (see Table 2).
Our JSMs are trained using the SRILM Toolkit.
We use the SRILM disambig tool for predicting
inflection, which takes a ?map? that specifies the
set of fully specified representations that each un-
derspecified stem can map to. For surface form
models, it specifies the mapping from stems to
lowercased surface forms (or surface forms with
markup for the hybrid surface model).
6 Results for Inflection Prediction
We build two different kinds of translation sys-
tem, the baseline and the stem translation system
(where MERT is used to train the system to pro-
duce a stem+markup sequence which agrees with
the stemmed reference of the dev set). In this sec-
tion we present the end-to-end translation results
for the different inflection prediction models de-
fined in Section 4, see Table 4.
If we translate from English into a stemmed
German representation and then apply a unigram
stem-to-surface-form model to predict the surface
form, we achieve a BLEU score of 9.97 (line 2).
This is only presented for comparison.
The baseline10 is 14.16, line 1. We compare
this with a 5-gram sequence model11 that predicts
10This is a better case-sensitive score than the baselines
on wmt-2009-b in experiments by top-performers Edinburgh
and Karlsruhe at the shared task. We use Moses with default
settings.
11Note that we use a different set, the ?clean data? set, to
determine the choice of n-gram order, see Section 7. We use
668
surface forms without access to morphological
features, resulting in a BLEU score of 14.26. In-
troducing morphological features (case on prepo-
sitions, number and gender on nouns) increases
the BLEU score to 14.58, which is in the same
range as the single JSM system predicting all lin-
guistic features at once.
This result shows that the mostly unlexicalized
single JSM can produce competitive results with
direct surface form prediction, despite not having
access to a model of inflected forms, which is the
desired final output. This strongly suggests that
the prediction of morphological features can be
used to achieve additional generalization over di-
rect surface form prediction. When comparing the
simple direct surface form prediction (line 3) with
the hybrid system enriched with number, gender
and case (line 4), it becomes evident that feature
markup can also aid surface form prediction.
Since the single JSM has no access to lexical
information, we used a language model to score
different feature predictions: for each sentence of
the development set, the 100 best feature predic-
tions were inflected and scored with a language
model. We then optimized weights for the two
scores LM (language model on surface forms)
and FP (feature prediction, the score assigned by
the JSM). This method disprefers feature predic-
tions with a top FP-score if the inflected sen-
tence obtains a bad LM score and likewise dis-
favors low-ranked feature prediction with a high
LM score. The prediction of case is the most
difficult given no lexical information, thus scor-
ing different prediction possibilities on inflected
words is helpful. An example is when the case of
a noun phrase leads to an inflected phrase which
never occurs in the (inflected) language model
(e.g., case=genitive vs. case=other). Applying
this method to the single JSM leads to a negligible
improvement (14.53 vs. 14.56). Using the n-best
output of the stem translation system did not lead
to any improvement.
The comparison between different feature pre-
diction models is also illustrative. Performance
decreases somewhat when using individual joint
sequence models (one for each linguistic feature)
compared to one single model (14.29, line 6).
The framework using the individual CRFs for
a 5-gram for surface forms and a 4-gram for JSMs, and the
same smoothing (Kneser-Ney, add-1 for unigrams, default
pruning).
1 baseline 14.16
2 unigram surface (no features) 9.97
3 surface (no features) 14.26
4 surface (with case, number, gender features) 14.58
5 1 JSM morphological features 14.53
6 4 JSMs morphological features 14.29
7 4 CRFs morphological features, lexical information 14.72
Table 4: BLEU scores (detokenized, case sensitive) on
the development test set wmt-2009-b
each linguistic feature performs best (14.72, line
7). The CRF framework combines the advantages
of surface form prediction and linguistic feature
prediction by using feature functions that effec-
tively cover the feature function spaces used by
both forms of prediction. The performance of the
CRF models results in a statistically significant
improvement12 (p < 0.05) over the baseline. We
also tried CRFs with bilingual features (projected
from English parses via the alignment output by
Moses), but obtained only a small improvement of
0.03, probably because the required information
is transferred in our stem markup (also a poor im-
provement beyond monolingual features is con-
sistent with previous work, see Section 8.3). De-
tails are omitted due to space.
We further validated our results by translating
the blind test set from wmt-2009, which we have
never looked at in any way. Here we also had
a statistically significant difference between the
baseline and the CRF-based prediction, the scores
were 13.68 and 14.18.
7 Analysis of Inflection-based System
Stem Markup. The first step of translating
from English to German stems (with the markup
we previously discussed) is substantially easier
than translating directly to inflected German (we
see BLEU scores on stems+markup that are over
2.0 BLEU higher than the BLEU scores on in-
flected forms when running MERT). The addition
of case to prepositions only lowered the BLEU
score reached by MERT by about 0.2, but is very
helpful for prediction of the case feature.
Inflection Prediction Task. Clean data task re-
sults13 are given in Table 5. The 4 CRFs outper-
form the 4 JSMs by more than 2%.
12We used Kevin Gimpel?s implementation of pairwise
bootstrap resampling with 1000 samples.
1326,061 of 55,057 tokens in our test set are ambiguous.
We report % surface form matches for ambiguous tokens.
669
Model Accuracy
unigram surface (no features) 55.98
surface (no features) 86.65
surface (with case, number, gender features) 91.24
1 JSM morphological features 92.45
4 JSMs morphological features 92.01
4 CRFs morphological features, lexical information 94.29
Table 5: Comparing predicting surface forms directly
with predicting morphological features.
training data 1 model 4 models
7.3 M sentences 92.41 91.88
1.5 M sentences 92.45 92.01
100000 sentences 90.20 90.64
1000 sentences 83.72 86.94
Table 6: Accuracy for different training data sizes of
the single and the four separate joint sequence models.
As we mentioned in Section 4, there is a spar-
sity issue at small training data sizes for the sin-
gle joint sequence model. This is shown in Ta-
ble 6. At the largest training data sizes, model-
ing all 4 features together results in the best pre-
dictions of inflection. However using 4 separate
models is worth this minimal decrease in perfor-
mance, since it facilitates experimentation with
the CRF framework for which the training of a
single model is not currently tractable.
Overall, the inflection prediction works well for
gender, number and type of inflection, which are
local features to the NP that normally agree with
the explicit markup output by the stem transla-
tion system (for example, the gender of a com-
mon noun, which is marked in the stem markup,
is usually successfully propagated to the rest of
the NP). Prediction of case does not always work
well, and could maybe be improved through hier-
archical labeled-syntax stem translation.
Portmanteaus. An example of where the sys-
tem is improved because of the new handling of
portmanteaus can be seen in the dative phrase
im internationalen Rampenlicht (in the interna-
tional spotlight), which does not occur in the par-
allel data. The accusative phrase in das interna-
tionale Rampenlicht does occur, however in this
case there is no portmanteau, but a one-to-one
mapping between in the and in das. For a given
context, only one of accusative or dative case is
valid, and a strongly disfluent sentence results
from the incorrect choice. In our system, these
two cases are handled in the same way (def-article
international Rampenlicht). This allows us to
generalize from the accusative example with no
portmanteau and take advantage of longer phrase
pairs, even when translating to something that will
be inflected as dative and should be realized as a
portmanteau. The baseline does not have this ca-
pability. It should be noted that the portmanteau
merging method described in Section 3 remerges
all occurrences of APPR and ART that can techni-
cally form a portmanteau. There are a few cases
where merging, despite being grammatical, does
not lead to a good result. Such exceptions require
semantic interpretation and are difficult to capture
with a fixed set of rules.
8 Adding Compounds to the System
Compounds are highly productive in German and
lead to data sparsity. We split the German com-
pounds in the training data, so that our stem trans-
lation system can now work with the individual
words in the compounds. After we have trans-
lated to a split/stemmed representation, we deter-
mine whether to merge words together to form a
compound. Then we merge them to create stems
in the same representation as before and we per-
form inflection and portmanteau merging exactly
as previously discussed.
8.1 Details of Splitting Process
We prepare the training data by splitting com-
pounds in two steps, following the technique of
Fritzinger and Fraser (2010). First, possible split
points are extracted using SMOR, and second, the
best split points are selected using the geometric
mean of word part frequencies.
compound word parts gloss
Inflationsrate Inflation Rate inflation rate
auszubrechen aus zu brechen out to break (to break out)
Training data is then stemmed as described in
Section 2.3. The formerly modifying words of the
compound (in our example the words to the left
of the rightmost word) do not have a stem markup
assigned, except for two cases: i) they are nouns
themselves or ii) they are particles separated from
a verb. In these cases, former modifiers are rep-
resented identically to their individual occurring
counterparts, which helps generalization.
8.2 Model for Compound Merging
After translation, compound parts have to be
resynthesized into compounds before inflection.
Two decisions have to be taken: i) where to
670
merge and ii) how to merge. Following the work
of Stymne and Cancedda (2011), we implement
a linear-chain CRF merging system using the
following features: stemmed (separated) surface
form, part-of-speech14 and frequencies from the
training corpus for bigrams/merging of word and
word+1, word as true prefix, word+1 as true suf-
fix, plus frequency comparisons of these. The
CRF is trained on the split monolingual data. It
only proposes merging decisions, merging itself
uses a list extracted from the monolingual data
(Popovic et al 2006).
8.3 Experiments
We evaluated the end-to-end inflection system
with the addition of compounds.15 As in the in-
flection experiments described in Section 5, we
use a 5-gram surface LM and a 7-gram POS
LM, but for this experiment, they are trained on
stemmed, split data. The POS LM helps com-
pound parts and heads appear in correct order.
The results are in Table 7. The BLEU score of the
CRF on test is 14.04, which is low. However the
system produces 19 compound types which are
in the reference but not in the parallel data, and
therefore not accessible to other systems. We also
observe many more compounds in general. The
100-best inflection rescoring technique previously
discussed reached 14.07 on the test set. Blind
test results with CRF prediction are much better,
14.08, which is a statistically significant improve-
ment over the baseline (13.68) and approaches the
result we obtained without compounds (14.18).
Correctly generated compounds are single words
which usually carry the same information as mul-
tiple words in English, and are hence likely un-
derweighted by BLEU. We again see many in-
teresting generalizations. For instance, take the
case of translating English miniature cameras to
the German compound Miniaturkameras. minia-
ture camera or miniature cameras does not occur
in the training data, and so there is no appropri-
ate phrase pair in any system (baseline, inflec-
tion, or inflection&compound-splitting). How-
ever, our system with compound splitting has
learned from split composita that English minia-
14Compound modifiers get assigned a special tag based on
the POS of their former heads, e.g., Inflation in the example
is marked as a non-head of a noun.
15We found it most effective to merge word parts during
MERT (so MERT uses the same stem references as before).
1 1 JSM morphological features 13.94
2 4 CRFs morphological features, lexical information 14.04
Table 7: Results with Compounds on the test set
ture can be translated as German Miniatur- and
gets the correct output.
9 Related Work
There has been a large amount of work on trans-
lating from a morphologically rich language to
English, we omit a literature review here due to
space considerations. Our work is in the opposite
direction, which primarily involves problems of
generation, rather than problems of analysis.
The idea of translating to stems and then in-
flecting is not novel. We adapted the work of
Toutanova et al(2008), which is effective but lim-
ited by the conflation of two separate issues: word
formation and inflection.
Given a stem such as brother, Toutanova et. al?s
system might generate the ?stem and inflection?
corresponding to and his brother. Viewing and
and his as inflection is problematic since a map-
ping from the English phrase and his brother to
the Arabic stem for brother is required. The situ-
ation is worse if there are English words (e.g., ad-
jectives) separating his and brother. This required
mapping is a significant problem for generaliza-
tion. We view this issue as a different sort of prob-
lem entirely, one of word-formation (rather than
inflection). We apply a ?split in preprocessing and
resynthesize in postprocessing? approach to these
phenomena, combined with inflection prediction
that is similar to that of Toutanova et. al. The
only work that we are aware of which deals with
both issues is the work of de Gispert and Marin?o
(2008), which deals with verbal morphology and
attached pronouns. There has been other work
on solving inflection. Koehn and Hoang (2007)
introduced factored SMT. We use more complex
context features. Fraser (2009) tried to solve the
inflection prediction problem by simply building
an SMT system for translating from stems to in-
flected forms. Bojar and Kos (2010) improved on
this by marking prepositions with the case they
mark (one of the most important markups in our
system). Both efforts were ineffective on large
data sets. Williams and Koehn (2011) used uni-
fication in an SMT system to model some of the
671
agreement phenomena that we model. Our CRF
framework allows us to use more complex con-
text features.
We have directly addressed the question as to
whether inflection should be predicted using sur-
face forms as the target of the prediction, or
whether linguistic features should be predicted,
along with the use of a subsequent generation
step. The direct prediction of surface forms is
limited to those forms observed in the training
data, which is a significant limitation. How-
ever, it is reasonable to expect that the use of
features (and morphological generation) could
also be problematic as this requires the use of
morphologically-aware syntactic parsers to anno-
tate the training data with such features, and addi-
tionally depends on the coverage of morpholog-
ical analysis and generation. Despite this, our
research clearly shows that the feature-based ap-
proach is superior for English-to-German SMT.
This is a striking result considering state-of-the-
art performance of German parsing is poor com-
pared with the best performance on English pars-
ing. As parsing performance improves, the per-
formance of linguistic-feature-based approaches
will increase.
Virpioja et al(2007), Badr et al(2008), Luong
et al(2010), Clifton and Sarkar (2011), and oth-
ers are primarily concerned with using morpheme
segmentation in SMT, which is a useful approach
for dealing with issues of word-formation. How-
ever, this does not deal directly with linguistic fea-
tures marked by inflection. In German these lin-
guistic features are marked very irregularly and
there is widespread syncretism, making it difficult
to split off morphemes specifying these features.
So it is questionable as to whether morpheme seg-
mentation techniques are sufficient to solve the in-
flectional problem we are addressing.
Much previous work looks at the impact of us-
ing source side information (i.e., feature func-
tions on the aligned English), such as those
of Avramidis and Koehn (2008), Yeniterzi and
Oflazer (2010) and others. Toutanova et. al.?s
work showed that it is most important to model
target side coherence and our stem markup also
allows us to access source side information. Us-
ing additional source side information beyond the
markup did not produce a gain in performance.
For compound splitting, we follow Fritzinger
and Fraser (2010), using linguistic knowledge en-
coded in a rule-based morphological analyser and
then selecting the best analysis based on the ge-
ometric mean of word part frequencies. Other
approaches use less deep linguistic resources
(e.g., POS-tags Stymne (2008)) or are (almost)
knowledge-free (e.g., Koehn and Knight (2003)).
Compound merging is less well studied. Popovic
et al(2006) used a simple, list-based merging ap-
proach, merging all consecutive words included
in a merging list. This approach resulted in too
many compounds. We follow Stymne and Can-
cedda (2011), for compound merging. We trained
a CRF using (nearly all) of the features they used
and found their approach to be effective (when
combined with inflection and portmanteau merg-
ing) on one of our two test sets.
10 Conclusion
We have shown that both the prediction of sur-
face forms and the prediction of linguistic features
are of interest for improving SMT. We have ob-
tained the advantages of both in our CRF frame-
work, and also integrated handling of compounds,
and an inflection-dependent word formation phe-
nomenon, portmanteaus. We validated our work
on a well-studied large corpora translation task.
Acknowledgments
The authors wish to thank the anonymous review-
ers for their comments. Aoife Cahill was partly
supported by Deutsche Forschungsgemeinschaft
grant SFB 732. Alexander Fraser, Marion Weller
and Fabienne Cap were funded by Deutsche
Forschungsgemeinschaft grant Models of Mor-
phosyntax for Statistical Machine Translation.
The research leading to these results has received
funding from the European Community?s Seventh
Framework Programme (FP7/2007-2013) under
grant agreement Nr. 248005. This work was sup-
ported in part by the IST Programme of the Euro-
pean Community, under the PASCAL2 Network
of Excellence, IST-2007-216886. This publica-
tion only reflects the authors? views. We thank
Thomas Lavergne and Helmut Schmid.
References
Eleftherios Avramidis and Philipp Koehn. 2008. En-
riching Morphologically Poor Languages for Statis-
tical Machine Translation. In Proceedings of ACL-
672
08: HLT, pages 763?770, Columbus, Ohio, June.
Association for Computational Linguistics.
Ibrahim Badr, Rabih Zbib, and James Glass. 2008.
Segmentation for English-to-Arabic statistical ma-
chine translation. In Proceedings of ACL-08: HLT,
Short Papers, pages 153?156, Columbus, Ohio,
June. Association for Computational Linguistics.
Ondr?ej Bojar and Kamil Kos. 2010. 2010 Failures in
English-Czech Phrase-Based MT. In Proceedings
of the Joint Fifth Workshop on Statistical Machine
Translation and MetricsMATR, pages 60?66, Upp-
sala, Sweden, July. Association for Computational
Linguistics.
Ann Clifton and Anoop Sarkar. 2011. Combin-
ing morpheme-based machine translation with post-
processing morpheme prediction. In Proceed-
ings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Lan-
guage Technologies, pages 32?42, Portland, Ore-
gon, USA, June. Association for Computational
Linguistics.
Adria` de Gispert and Jose? B. Marin?o. 2008. On the
impact of morphology in English to Spanish statisti-
cal MT. Speech Communication, 50(11-12):1034?
1046.
Alexander Fraser. 2009. Experiments in Morphosyn-
tactic Processing for Translating to and from Ger-
man. In Proceedings of the Fourth Workshop on
Statistical Machine Translation, pages 115?119,
Athens, Greece, March. Association for Computa-
tional Linguistics.
Fabienne Fritzinger and Alexander Fraser. 2010. How
to Avoid Burning Ducks: Combining Linguistic
Analysis and Corpus Statistics for German Com-
pound Processing. In Proceedings of the Fifth
Workshop on Statistical Machine Translation, pages
224?234. Association for Computational Linguis-
tics.
Philipp Koehn and Hieu Hoang. 2007. Factored
Translation Models. In Proceedings of the 2007
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL), pages 868?
876, Prague, Czech Republic, June. Association for
Computational Linguistics.
Philipp Koehn and Kevin Knight. 2003. Empirical
methods for compound splitting. In EACL ?03:
Proceedings of the 10th conference of the European
chapter of the Association for Computational Lin-
guistics, pages 187?193, Morristown, NJ, USA. As-
sociation for Computational Linguistics.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of the International
Conference on Machine Learning, pages 282?289.
Morgan Kaufmann, San Francisco, CA.
Thomas Lavergne, Olivier Cappe?, and Franc?ois Yvon.
2010. Practical very large scale CRFs. In Proceed-
ings the 48th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 504?513.
Association for Computational Linguistics, July.
Minh-Thang Luong, Preslav Nakov, and Min-Yen
Kan. 2010. A Hybrid Morpheme-Word Represen-
tation for Machine Translation of Morphologically
Rich Languages. In Proceedings of the 2010 Con-
ference on Empirical Methods in Natural Language
Processing, pages 148?157, Cambridge, MA, Octo-
ber. Association for Computational Linguistics.
Maja Popovic, Daniel Stein, and Hermann Ney. 2006.
Statistical Machine Translation of German Com-
pound Words. In Proceedings of FINTAL-06, pages
616?624, Turku, Finland. Springer Verlag, LNCS.
Helmut Schmid, Arne Fitschen, and Ulrich Heid.
2004. SMOR: A German Computational Morphol-
ogy Covering Derivation, Composition, and Inflec-
tion. In 4th International Conference on Language
Resources and Evaluation.
Helmut Schmid. 2004. Efficient Parsing of Highly
Ambiguous Context-Free Grammars with Bit Vec-
tors. In Proceedings of Coling 2004, pages 162?
168, Geneva, Switzerland, Aug 23?Aug 27. COL-
ING.
Andreas Stolcke. 2002. SRILM - An Extensible Lan-
guage Modeling Toolkit. In International Confer-
ence on Spoken Language Processing.
Sara Stymne and Nicola Cancedda. 2011. Produc-
tive Generation of Compound Words in Statistical
Machine Translation. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, pages
250?260, Edinburgh, Scotland UK, July. Associa-
tion for Computational Linguistics.
Sara Stymne. 2008. German Compounds in Factored
Statistical Machine Translation. In Proceedings of
GOTAL-08, pages 464?475, Gothenburg, Sweden.
Springer Verlag, LNCS/LNAI.
Kristina Toutanova, Hisami Suzuki, and Achim
Ruopp. 2008. Applying Morphology Generation
Models to Machine Translation. In Proceedings of
ACL-08: HLT, pages 514?522, Columbus, Ohio,
June. Association for Computational Linguistics.
Sami Virpioja, Jaakko J. Va?yrynen, Mathias Creutz,
and Markus Sadeniemi. 2007. Morphology-aware
statistical machine translation based on morphs in-
duced in an unsupervised manner. In PROC. OF
MT SUMMIT XI, pages 491?498.
Philip Williams and Philipp Koehn. 2011. Agree-
ment constraints for statistical machine translation
into German. In Proceedings of the Sixth Workshop
on Statistical Machine Translation, pages 217?226,
Edinburgh, Scotland, July. Association for Compu-
tational Linguistics.
Reyyan Yeniterzi and Kemal Oflazer. 2010. Syntax-
to-Morphology Mapping in Factored Phrase-Based
673
Statistical Machine Translation from English to
Turkish. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 454?464, Uppsala, Sweden, July. Asso-
ciation for Computational Linguistics.
674
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 767?776,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
To what extent does sentence-internal realisation reflect discourse
context? A study on word order
Sina Zarrie? Jonas Kuhn
Institut fu?r maschinelle Sprachverarbeitung
University of Stuttgart, Germany
zarriesa,jonas@ims.uni-stuttgart.de
Aoife Cahill
Educational Testing Service
Princeton, NJ 08541, USA
acahill@ets.org
Abstract
We compare the impact of sentence-
internal vs. sentence-external features on
word order prediction in two generation
settings: starting out from a discrimina-
tive surface realisation ranking model for
an LFG grammar of German, we enrich
the feature set with lexical chain features
from the discourse context which can be
robustly detected and reflect rough gram-
matical correlates of notions from theoreti-
cal approaches to discourse coherence. In a
more controlled setting, we develop a con-
stituent ordering classifier that is trained
on a German treebank with gold corefer-
ence annotation. Surprisingly, in both set-
tings, the sentence-external features per-
form poorly compared to the sentence-
internal ones, and do not improve over
a baseline model capturing the syntactic
functions of the constituents.
1 Introduction
The task of surface realization, especially in a rel-
atively free word order language like German, is
only partially determined by hard syntactic con-
straints. The space of alternative realizations that
are strictly speaking grammatical is typically con-
siderable. Nevertheless, for any given choice of
lexical items and prior discourse context, only a
few realizations will come across as natural and
will contribute to a coherent text. Hence, any NLP
application involving a non-trivial generation step
is confronted with the issue of soft constraints on
grammatical alternatives in one way or another.
There are countless approaches to modelling
these soft constraints, taking into account their
interaction with various aspects of the discourse
context (givenness or salience of particular refer-
ents, prior mentioning of particular concepts).
Since so many factors are involved and there is
further interaction with subtle semantic and prag-
matic differentiations, lexical choice, stylistics
and presumably processing factors, theoretical ac-
counts making reliable predictions for real cor-
pus examples have for a long time proven elusive.
As for German, only quite recently, a number of
corpus-based studies (Filippova and Strube, 2007;
Speyer, 2005; Dipper and Zinsmeister, 2009) have
made some good progress towards a coherence-
oriented account of at least the left edge of the
German clause structure, the Vorfeld constituent.
What makes the technological application of
theoretical insights even harder is that for most
relevant factors, automatic recognition cannot be
performed with high accuracy (e.g., a coreference
accuracy in the 70?s means there is a good deal
of noise) and for the higher-level notions such
as the information-structural focus, interannotator
agreement on real corpus data tends to be much
lower than for core-grammatical notions (Poesio
and Artstein, 2005; Ritz et al 2008).
On the other hand, many of the relevant dis-
course factors are reflected indirectly in proper-
ties of the sentence-internal material. Most no-
tably, knowing the shape of referring expressions
narrows down many aspects of givenness and
salience of its referent; pronominal realizations
indicate givenness, and in German there are even
two variants of the personal pronoun (er and der)
for distinguishing salience. So, if the genera-
tion task is set in such a way that the actual lex-
ical choice, including functional categories such
as determiners, is fully fixed (which is of course
not always the case), one can take advantage of
767
these reflexes. This explains in part the fairly high
baseline performance of n-gram language mod-
els in the surface realization task. And the effect
can indeed be taken much further: the discrimi-
native training experiments of Cahill and Riester
(2009) show how effective it is to systematically
take advantage of asymmetry patterns in the mor-
phosyntactic reflexes of the discourse notion of
information status (i.e., using a feature set with
well-chosen purely sentence-bound features).
These observations give rise to the question: in
the light of the difficulty in obtaining reliable dis-
course information on the one hand and the effec-
tiveness of exploiting the reflexes of discourse in
the sentence-internal material on the other ? can
we nevertheless expect to gain something from
adding sentence-external feature information?
We propose two scenarios for adressing this
question: first, we choose an approximative ac-
cess to context information and relations between
discourse referents ? lexical reiteration of head
words, combined with information about their
grammatical relation and topological positioning
in prior sentences. We apply these features in a
rich sentence-internal surface realisation ranking
model for German. Secondly, we choose a more
controlled scenario: we train a constituent order-
ing classifier based on a feature model that cap-
tures properties of discourse referents in terms of
manually annotated coreference relations. As we
get the same effect in both setups ? the sentence-
external features do not improve over a baseline
that captures basic morphosyntactic properties of
the constituents ? we conclude that sentence-
internal realisation is actually a relatively accurate
predictor of discourse context, even more accurate
than information that can be obtained from coref-
erence and lexical chain relations.
2 Related Work
In the generation literature, most works on ex-
ploiting sentence-external discourse information
are set in a summarisation or content ordering
framework. Barzilay and Lee (2004) propose an
account for constraints on topic selection based on
probabilistic content models. Barzilay and Lapata
(2008) propose an entity grid model which repre-
sents the distribution of referents in a discourse
for sentence ordering. Karamanis et al(2009)
use Centering-based metrics to assess coherence
in an information ordering system. Clarke and La-
pata (2010) have improved a sentence compres-
sion system by capturing prominence of phrases
or referents in terms of lexical chain information
inspired by Morris and Hirst (1991) and Center-
ing (Grosz et al 1995). In their system, discourse
context is represented in terms of hard constraints
modelling whether a certain constituent can be
deleted or not.
In the linearisation or surface realisation do-
main, there is a considerable body of work ap-
proximating information structure in terms of
sentence-internal realisation (Ringger et al 2004;
Filippova and Strube, 2009; Velldal and Oepen,
2005; Cahill et al 2007). Cahill and Riester
(2009) improve realisation ranking for German ?
which mainly deals with word order variation ? by
representing precedence patterns of constituents
in terms of asymmetries in their morphosyntac-
tic properties. As a simple example, a pattern ex-
ploited by Cahill and Riester (2009) is the ten-
dency of definite elements tend to precede indef-
inites, which, on a discourse level, reflects that
given entities in a sentence tend to precede new
entities.
Other work on German surface realisation has
highlighted the role of the initial position in the
German sentence, the so-called Vorfeld (or ?pre-
field?). Filippova and Strube (2007) show that
once the Vorfeld (i.e. the constituent that precedes
the finite verb) is correctly determined, the pre-
diction of the order in the Mittelfeld (i.e. the con-
stituents that follow the finite verb) is very easy.
Cheung and Penn (2010) extend the approach
of Filippova and Strube (2007) and augment a
sentence-internal constituent ordering model with
sentence-external features inspired from the en-
tity grid model proposed by Barzilay and Lapata
(2008).
3 Motivation
While there would be many ways to construe
or represent discourse context (e.g. in terms of
the global discourse or information structure), we
concentrate on capturing local coherence through
the distribution of discourse referents in a text.
These discourse referents basically correspond to
the constituents that our surface realisation model
has to put in the right order. As the order of refer-
ents or constituents is arguably influenced by the
information structure of a sentence given the pre-
vious text, our main assumption was that infor-
768
(1) a. Kurze Zeit spa?ter erkla?rte ein Anrufer bei Nachrichtenagenturen in Pakistan , die Gruppe Gamaa bekenne sich.
Shortly after, a caller declared at the news agencies in Pakistan, that the group Gamaa avowes itself.
b. Diese Gruppe wird fu?r einen Gro?teil der Gewalttaten verantwortlich gemacht , die seit dreieinhalb Jahren in
A?gypten veru?bt worden sind .
This group is made responsible for most of the violent acts that have been committed in Egypt in the last three and
a half years.
(2) a. Belgien wu?nscht, dass sich WEU und NATO daru?ber einigen.
Belgium wants that WEU and NATO agree on that.
b. Belgien sieht in der NATO die beste milita?rische Struktur in Europa .
Belgium sees the best military structure of Europe in the NATO.
(3) a. Frauen vom Land ka?mpften aktiv darum , ein Staudammprojekt zu verhindern.
Women from the countryside fighted actively to block the dam project.
b. Auch in den Sta?dten fa?nden sich immer mehr Frauen in Selbsthilfeorganisationen zusammen.
Also in the cities, more and more women team up in self-help organisations.
mation about the prior mentioning of a referent
would be helpful for predicting the position of this
referent in a sentence.
The idea that the occurence of discourse refer-
ents in a text is a central aspect of discourse struc-
ture has been systematically pursued by Centering
Theory (Grosz et al 1995). Its most important
notions are related to the realisation of discourse
referents (i.e. described as ?centers?) and the way
the centers are arranged in a sequence of utter-
ances to make this sequence a coherent discourse.
Another important concept is the ?ranking? of dis-
course referents which basically determines the
prominence of a referent in a certain sentence and
is driven by several factors (e.g. their grammati-
cal function). For free word order languages like
German, word order has been proposed as one of
the factors that account for the ranking (Poesio et
al., 2004). In a similar spirit, Morris and Hirst
(1991) have proposed that chains of (related) lex-
ical items in a text are an important indicator of
text structure.
Our main hypothesis was that it is possible to
exploit these intuitions from Centering Theory
and the idea of lexical chains for word order pre-
diction. Thus, we expected that it would be easier
to predict the position of a referent in a sentence
if we have not only given its realisation in the cur-
rent utterance but also its prominence in the previ-
ous discourse. Especially, we expected this intu-
ition to hold for cases where the morpho-syntactic
realisation of a constituent does not provide many
clues. This is illustrated in Examples (1) and (2)
which both exemplify the reiteration of a lexical
item in two subsequent sentences, (reiteration is
one type of lexical chain discussed in Morris and
Hirst (1991)). In Example (1), the second instance
of the noun ?group? is modified by a demonstra-
tive pronoun such that its ?known? and prominent
discourse status is overt in the morpho-syntactic
realisation. In Example (2), both instances of
?Belgium? are realised as bare proper nouns with-
out an overt morphosyntactic clue indicating their
discourse status.
Beyond the simple presence of reitered items in
sequences of sentences, we expected that it would
be useful to look at the position and syntactic
function of the previous mentions of a discourse
referent. In Example (1), the reiterated item is first
introduced in an embedded sentence and realised
in the Vorfeld in the second utterance. In terms
of centering, this transition would correspond to
a topic shift. In Example (2), both instances are
realised in the Vorfeld, such that the topic of the
first sentence is carried over to the next.
In Example (3), we illustrate a further type of
lexical reiteration. In this case, two identical head
nouns are realised in subsequent sentences, even
though they refer to two different discourse refer-
ents. While this type of lexical chain is described
as ?reiteration without identity of referents? by
Morris and Hirst (1991), it would not be captured
in Centering since this is not a case of strict coref-
erence. On the other hand, lexical chains do not
capture types of reiterated discourse referents that
have distinct morpho-syntactic realisations, e.g.
nouns and pronouns.
Originally, we had the hypothesis that strict
corefence information is more useful and accurate
for word order prediction than rather loose lexi-
cal chains which conflate several types of referen-
tial and lexical relations. However, the advantage
of chains, especially chains of reiteration, is that
they can be easily detected in any corpus text and
769
that they might capture ?topics? of sentences be-
yond the identity of referents. Thus, we started
out from the idea of lexical chains and added cor-
responding features in a statistical ranking model
for surface realisation of German (Section 4). As
this strategy did not work out, we wanted to assess
whether an ideal coreference annotation would be
helpful at all for predicting word order. In a sec-
ond experiment, we use a corpus which is manu-
ally annotated for coreference (Section 5).
4 Experiment 1: Realisation Ranking
with Lexical Chains
In this Section, we present an experiment that in-
vestigates sentence-external context in a surface
realisation task. The sentence-external context is
represented in terms of lexical chain features and
compared to sentence-internal models which are
based on morphosyntactic features. The experi-
ment thus targets a generation scenario where no
coreference information is available and aims at
assessing whether relatively naive context infor-
mation is also useful.
4.1 System Description
We carry out our first experiment in a regener-
ation set-up with two components: a) a large-
scale hand-crafted Lexical Functional Grammar
(LFG) for German (Rohrer and Forst, 2006), used
to parse and regenerate a corpus sentence, b)
a stochastic ranker that selects the most appro-
priate regenerated sentence in context according
to an underlying, linguistically motivated feature
model. In contrast to fully statistical linearisation
methods, our system first generates the full set
of sentences that correspond to the grammatically
well-formed realisations of the intermediate syn-
tactic representation.1 This representation is an
f-structure, which underspecifies the order of con-
stituents and, to some extent, their morphological
realisation, such that the output sentences contain
all possible combinations of word order permu-
tations and morphological variants. Depending
on the length and structure of the original corpus
sentence, the set of regenerated sentences can be
huge (see Cahill et al(2007) for details on regen-
erating the German treebank TIGER).
1There are occasional mistakes in the grammar which
sometimes lead to ungrammatical strings being generated,
but this is rare.
The realisation ranking component is an SVM
ranking model implemented with SVMrank,
a Support Vector Machine-based learning tool
(Joachims, 2006). During training, each sentence
is annotated with a rank and a set of features ex-
tracted from the F-structure, its surface string and
external resources (e.g. a language model). If
the sentence matches the original corpus string,
its rank will be highest, the assumption being that
the original sentence corresponds to the optimal
realisation in context. The output of generation,
the top-ranked sentence, is evaluated against the
original corpus sentence.
4.2 The Feature Models
As the aim of this experiment is to better un-
derstand the nature of sentence-internal features
reflecting discourse context and compare them
to sentence-external ones, we build several fea-
ture models which capture different aspects of the
constituents in a given sentence. The sentence-
internal features describe the morphosyntacic re-
alisation of constituents, for instance their func-
tion (?subject?, ?object?), and can be straightfor-
wardly extracted from the f-structure. These fea-
tures are then combined into discriminative prece-
dence features, for instance ?subject-precedes-
object?. We implement the following types of
morphosyntactic features:
? syntactic function (arguments and adjuncts)
? modification (e.g. nouns modified by relative
clauses, genitive etc.)
? syntactic category (e.g. adverbs, proper
nouns, phrasal arguments)
? definiteness for nouns
? number and person for nominal elements
? types of pronouns (e.g. demonstrative, re-
flexive)
? constituent span and number of embedded
nodes in the tree
In addition, we also include language model
scores in our ranking model. In Section 4.4,
we report on results for several subsets of these
features where ?BaseSyn? refers to a model that
only includes the syntactic function features and
?FullMorphSyn? includes all features mentioned
above.
For extracting the lexical chains, we check for
any overlapping nouns in the n sentences previ-
ous to the current one being generated. We check
770
Rank Sentence and Features
% Diese Gruppe wird fu?r einen Gro?teil der Gewalttaten verantwortlich gemacht.
% This group is for a major part of the violent acts responsible made.
1 subject-<-pp-object, demonstrative-<-indefinite, overlap-<-no-overlap, overlap-in-vorfeld, lm:-7.89
% Fu?r einen Gro?teil der Gewalttaten wird diese Gruppe verantwortlich gemacht.
% For a major part of the violent acts is this group responsible made.
3 pp-object-<-subject, indefinite-<-demonstrative, no-overlap-<-overlap, no-overlap-in-vorfeld, lm:-10.33
% Verantwortlich gemacht wird diese Gruppe fu?r einen Gro?teil der Gewalttaten.
% Responsible made is this group for a major part of the violent acts.
3 subject-<-pp-object, demonstrative-<-indefinite, overlap-<-no-overlap, lm:-9.41
Figure 1: Made-up training example for realisation ranking with precedence features
proper and common nouns, considering full and
partial overlaps as shown in Examples (1) and
(2), where the (a) example is the previous sen-
tence in the corpus. For each overlap, we record
the following properties: (i) function in the previ-
ous sentence, (ii) position in the previous sentence
(e.g. Vorfeld), (iii) distance between sentences,
(iv) total number of overlaps.
These overlap features are then also
combined in terms of precedence, e.g.
?has subject overlap:3-precedes-no overlap?,
meaning that in the current sentence a noun
that was previously mentioned in a subject 3
sentences ago precedes a noun that was not
mentioned before.
In Figure 1, we give an example of a set of gen-
eration alternatives and their (partial) feature rep-
resentation for the sentence (1-b). Precedence is
indicated by ?<?.
Basically, our sentence-external feature model
is built on the intuition that lexical chains or over-
laps approximate discourse status in a way which
is similar to sentence-internal morphosyntactic
properties. Thus, we would expect that overlaps
indicate givenness, salience or prominence and
that asymmetries between overlapping and non-
overlapping entities are helpful in the ranking.
4.3 Data
All our models are trained on 7,039 sentences
(subdivided into 1259 texts) from the TIGER
Treebank of German newspaper text (Brants et al
2002). We tune the parameters of our SVM model
on a development set of 55 sentences and report
the final results for our unseen test set of 240 sen-
tences. Table 1 shows how many sentences in our
training, development and test sets have at least
one textually overlapping phrase in the previous
1?10 sentences.
We choose the TIGER treebank, which has no
# Sentences % Sentences with overlap
in context Training Dev Test
1 20.96 23.64 20.42
2 35.42 40.74 35.00
3 45.58 50.00 53.33
4 52.66 53.70 58.75
5 57.45 58.18 64.58
6 61.42 57.41 68.75
7 64.58 61.11 70.83
8 67.05 62.96 72.08
9 69.20 64.81 74.17
10 71.16 70.37 75.83
Table 1: The percentage of sentences that have at least
one overlapping entity in the previous n sentences
coreference annotation, since we already have a
number of resources available to match the syn-
tactic analyses produced by our grammar against
the analyses in the treebank. Thus, in our regen-
eration system, we parse the sentences with the
grammar, and choose the parsed f-structures that
are compatible with the manual annotation in the
TIGER treebank as is done in Cahill et al(2007).
This compatibility check eliminates noise which
would be introduced by generating from incorrect
parses (e.g. incorrect PP-attachments typically re-
sult in unnatural and non-equivalent surface reali-
sations).
For comparing the string chosen by the mod-
els against the original corpus sentence, we use
BLEU, NIST and exact match. Exact match is
a strict measure that only credits the system if it
chooses the exact same string as the original cor-
pus string. BLEU and NIST are more relaxed
measures that compare the strings on the n-gram
level. Finally, we report accuracy scores for the
Vorfeld position (VF) corresponding to the per-
centage of sentences generated with a correct Vor-
feld.
771
Sc BLEU NIST Exact VF
0 0.766 11.885 50.19 64.0
1 0.765 11.756 49.78 64.0
2 0.765 11.886 50.01 64.1
3 0.765 11.885 50.08 63.8
4 0.761 11.723 49.43 63.2
5 0.765 11.884 49.71 64.2
6 0.768 11.892 50.42 64.6
7 0.765 11.885 50.01 64.5
8 0.764 11.884 49.78 64.3
9 0.765 11.888 49.82 63.6
10 0.764 11.889 49.7 63.5
Table 2: Tenfold-crossvalidation for feature model
FullMorphSyn and different context windows (Sc)
Model BLEU VF
Language Model 0.702 51.2
Language Model + Context Sc = 5 0.715 54.3
BaseSyn 0.757 62.0
BaseSyn + Context Sc = 5 0.760 63.0
FullMorphSyn 0.766 64.0
FullMorphSyn + Context Sc = 5 0.763 64.2
Table 3: Evaluation for different feature models; ?Lan-
guage Model?: ranking based on language model
scores, ?BaseSyn?: precedence between constituent
functions, ?FullMorphSyn?: entire set of sentence-
internal features.
4.4 Results
In Table 2, we report the performance of the full
sentence-internal feature model combined with
context windows from zero to ten. The scores
have been obtained from tenfold-crossvalidation.
For none of the context windows, the model out-
performs the baseline with a zero context which
has no sentence-external features. In Table 3,
we compare the performance of several feature
models corresponding to subsets of the features
used so far which are combined with sentence-
external features respectively. We note that the
function precedence features (i.e. the ?BaseSyn?
model) are very powerful, leading to a major im-
provement compared to a language model. The
sentence-external features lead to an improvement
when combined with the language-model based
ranking. However, this improvement is leveled
out in the BaseSyn model.
On the one hand, the fact that the lexical chain
features improve a language-model based ranking
suggests these features are, to some extent, pre-
dictive for certain patterns of German word order.
On the other hand, the fact that they don?t improve
over an informed sentence-internal baseline sug-
gests that these patterns are equally well captured
by morphosyntactic features. However, we cannot
exclude the possibility that the chain features are
too noisy as they conflate several types of lexical
and coreferential relations. This will be adressed
in the following experiment.
5 Experiment 2: Constituent Ordering
with Centering-inspired Features
We now look at a simpler generation setup where
we concentrate on the ordering of constituents in
the German Vorfeld and Mittelfeld. This strat-
egy has also been adopted in previous investiga-
tions of German word order: Filippova and Strube
(2007) show that once the German Vorfeld is cor-
rectly chosen, the prediction accuracy for the Mit-
telfeld (the constituents following the finite verb)
is in the 90s.
In order to eliminate noise introduced from po-
tentially heterogeneous chain features, we look at
coreference features and, again, compare them to
sentence-internal morphosyntactic features. We
target a generation scenario where coreference in-
formation is available. The aim is to establish an
upper bound concerning the quality improvement
for word order prediction by recurring to manual
corefence annotation.
5.1 Data and Setup
We carry out the constituent ordering experiment
on the Tu?ba-D/Z treebank (v5) of German news-
paper articles (Telljohann et al 2006). It com-
prises about 800k tokens in 45k sentences. We
choose this corpus because it is not only annotated
with syntactic analyses but also with coreference
relations (Naumann, 2006). The syntactic annota-
tion format differs from the TIGER treebank used
in the previous experiment, for instance, it ex-
plicitely represents the Vorfeld and Mittelfeld as
phrasal nodes in the tree. This format is very con-
venient for the extraction of constituents in the re-
spective positions.
The Tu?ba-D/Z coreference annotation distin-
guishes several relations between discourse ref-
erents, most importantly ?coreferential relation?
and ?anaphoric relation? where the first denotes
a relation between noun phrases that refer to the
same entity, and the latter refers to a link between
a pronoun and a contextual antecedent, see Nau-
mann (2006) for further detail. We expected the
coreferential relation to be particularly useful, as
772
it cannot always be read off the morphosyntac-
tic realisation of a noun phrase, whereas pronouns
are almost always used in an anaphoric relation.
The constituent ordering model is implemented
as a classifier that is given a set of constituents
and predicts the constituent that is most likely to
be realised in the Vorfeld.
The set of candidate constituents is determined
from the tree of the original corpus sentence. We
will assume that all constituents under a Vorfeld
and Mittelfeld node can be freely reordered. Thus,
we do not check whether the word order variants
we look at are actually grammatical assuming that
most of them are. In this sense, this experiment
is close to fully statistical generation approaches.
As a further simplification, we do not look at mor-
phological generation variants of the constituents
or their head verb.
The classifier is implemented with SVMrank
again. In contrast to the previous experiment
where we learned to rank sentences, the classi-
fier now learns to rank constituents. The con-
stituents have been extracted using the tool de-
scribed in Bouma (2010). The final data set com-
prises 48.513 candidate sets of freely orderable
constituents.
5.2 Centering-inspired Feature Model
To compare the discourse context model against a
sentence-based model, we implemented a number
of sentence-internal features that are very similar
to the features used in the previous experiment.
Since we extract them from the syntactic annota-
tion instead of f-structures, some labels and fea-
ture names will be different, however, the design
of the sentence-internal model is identical to the
previous one in Section 4.
The sentence-external features differ in some
aspects from Section 4, since we extract coref-
erence relations of several types (see (Naumann,
2006) for the anaphoric relations annotated in the
Tueba-D/Z). For each type of coreference link,
we extract the following properties: (i) function
of the antecedent, (ii) position of the antecedent,
(iii) distance between sentences, (iv) type of rela-
tion. We also distinguish coreference links anno-
tated for the whole phrase (?head link?) and links
that are annotated for an element embedded by the
constituent (?contained link?). The two types are
illustrated in Examples (4) and (5). Note that both
cases would not have been captured in the lexical
# VF # MF
Backward Center 3.5% 5.1%
Forward Center 6.8% 6.8%
Coref Link 30.5% 23.4%
Table 4: Backward and forward centers and their posi-
tions
chain model since there is no lexical overlap be-
tween the realisations of the discourse referents.
These types of coreference features implicitly
carry the information that would also be consid-
ered in a Centering formalisation of discourse
context. In addition to these, we designed features
that explicitly describe centers as these might
have a higher weight. In line with Clarke and
Lapata (2010), we compute backward (CB) and
forward centers (CF ) in the following way:
1. Extract all entities from the current sentence
and the previous sentence.
2. Rank the entities of the previous sentence ac-
cording to their function (subject < direct
object < indirect object ...).
3. Find the highest ranked entity in the previous
sentence that has a link to an entity in the
current sentence, this entity is the CB of the
sentence.
In the same way, we mark entities as forward
centers that are ranked highest in the current sen-
tence and have a link to an entity in the following
sentence.2 In Table 4, we report the percentage of
sentences that have backward and forward centers
in the Vorfeld or Mittelfeld. While the percentage
of sentences that realise a backward center is quite
low, the overall proportion of sentences contain-
ing some type of coreference link is in a dimen-
sion such that the learner could definitely pick up
some predictive patterns. Going by the relative
frequencies, coreferential constituents have a bias
towards appearing in the Vorfeld rather than in the
Mittelfeld.
5.3 Results
First, we build three coreference-based con-
stituent classifiers on their entire training set and
compare them to their sentence-internal baseline.
The most simple baseline records the category of
2In Centering, all entities in a given utterance can be seen
as forward centers, however we thought that this implemen-
tation would be more useful.
773
(4) a. Die Rechnung geht an die AWO.
The bill goes to the AWO.
b. [Hintergrund der gegenseitigen Vorwu?rfe in der Arbeiterwohlfahrt] sind offenbar scharfe Konkurrenzen zwischen
Bremern und Bremerhavenern.
Apparently, [the background of the mutual accusations at the labour welfare] are rivalries between people from
Bremen and Bremerhaven.
(5) a. Dies ist die Behauptung, mit der Bremens Ha?fensenator die Skeptiker davon u?berzeugt hat, [...].
This is the claim, which Bremen?s harbour senator used to convince doubters, [...].
b. Fu?r diese Behauptung hat Beckmeyer bisher keinen Nachweis geliefert. So far, Beckmeyer has not given a prove of
this claim.
Model VF
ConstituentLength + HeadPos 47.48%
ConstituentLength + HeadPos + Coref 51.30%
BaseSyn 54.82%
BaseSyn + Coref 56.21%
FullMorphSyn 57.24%
FullMorphSyn + Coref 57.40%
Table 5: Results from Vorfeld classification, training
and evaluation on entire treebank
the constituent head and the number of words that
the constituent spans. Additionally, in parallel to
the experiment in Section 4, we build a ?BaseSyn?
model which has the syntactic function features,
and a ?FullMorphSyn? model which comprises
the entire set of sentence-internal features. To
each of these baseline, we add the coreference
features. The results are reported in Table 5.
In this experiment, we find an effect of
the sentence-external features over the simple
sentence-internal baselines. However, in the fully
spelled-out, sentence-internal model, the effect
is, again, minimal. Moreover, for each base-
line, we obtain higher improvements by adding
further sentence-internal features than by adding
sentence-external ones the accuracy of the sim-
ple baseline (47.48%) improves by 7.34 points
through adding function features (the accuracy
of BaseSyn is 54.82%) and by only 3.48 points
through adding coreference features.
We run a second experiment in order to so see
whether the better performance of the sentence-
internal features is related to their coverage. We
build and evaluate the same set of classifiers on
the subset of sentences that contain at least one
coreference link for one of its constituents (see
Table 4 for the distribution of coreference links
in our data). The results are given in Table 6. In
this experiment, the coreference features improve
over all sentence-internal baselines including the
?FullMorphSyn? model.
Model VF
ConstituentLength + HeadPos 46.61%
ConstituentLength + HeadPos + Coref 52.23%
BaseSyn 54.63%
BaseSyn + Coref 56.67%
FullMorphSyn 55.36%
FullMorphSyn + Coref 57.93%
Table 6: Results from Vorfeld classification, training
and evaluation on sentences that contain a coreference
link
5.4 Discussion
The results presented in this Section consis-
tently complete the picture that emerged from
the experiments in Section 4. Even if we have
high quality information about discourse con-
text in terms of relations between referents, a
non-trivial sentence-internal model for word or-
der prediction can be hardly improved. This
suggests that sentence-internal approximations of
discourse context provide a fairly good way of
dealing with local coherence in a linearisation
task. It is also interesting that the sentence-
external features improve over simple baselines,
but get leveled out in rich sentence-internal fea-
ture models. From this, we conclude that the
sentence-external features we implemented are to
some extent predictive for word order, but that
they can be covered by sentence-internal features
as well.
Our second evaluation concentrating on the
sentences that have coreference information
shows that the better performance of the sentence-
internal features is also related to their cover-
age. These results confirm our initial intuition
that coreference information can add to the pre-
dictive power of the morpho-syntactic features in
certain contexts. This positive effect disappears
when sentences with and without coreferential
constituents are taken together. For future work,
it would be promising to investigate whether the
774
positive impact of coreference features can be
strengthened if the coreference annotation scheme
is more exhaustive, including, e.g., bridging and
event anaphora.
6 Conclusion
We have carried out a number of experiments that
show that sentence-internal models for word order
are hardly improved by features which explicitely
represent the preceding context of a sentence in
terms of lexical and referential relations between
discourse entities. This suggests that sentence-
internal realisation implicitly carries a lot of im-
formation about discourse context. On average,
the morphosyntactic properties of constituents in
a text are better approximates of their discourse
status than actual coreference relations.
This result feeds into a number of research
questions concerning the representation of dis-
course and its application in generation systems.
Although we should certainly not expect a com-
putational model to achieve a perfect accuracy in
the constituent ordering task ? even humans only
agree to a certain extent in rating word order vari-
ants (Belz and Reiter, 2006; Cahill, 2009) ? the
average accuracy in the 60?s for prediction of Vor-
feld occupance is still moderate. An obvious di-
rection would be to further investigate more com-
plex representations of discourse that take into ac-
count the relations between utterances, such as
topic shifts. Moreover, it is not clear whether the
effects we find for linearisation in this paper carry
over to other levels of generation such as tacti-
cal generation where syntactic functions are not
fully specified. In a broader perspective, our re-
sults underline the need for better formalisations
of discourse that can be translated into features for
large-scale applications such as generation.
Acknowledgments
This work was funded by the Collaborative Re-
search Centre (SFB 732) at the University of
Stuttgart.
References
Regina Barzilay and Mirella Lapata. 2008. Modeling
local coherence: An entity-based approach. Com-
putational Linguistics, 34:1?34.
Regina Barzilay and Lillian Lee. 2004. Catching the
drift: Probabilistic content models with applications
to generation and summarization. In Proceedings of
HLT-NAACL 2004, Boston,MA.
Anja Belz and Ehud Reiter. 2006. Comparing auto-
matic and human evaluation of NLG systems. In
Proceedings of EACL 2006, pages 313?320, Trento,
Italy.
Gerlof Bouma. 2010. Syntactic tree queries in prolog.
In Proceedings of the Fourth Linguistic Annotation
Workshop, ACL 2010.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolf-
gang Lezius, and George Smith. 2002. The TIGER
Treebank. In Proceedings of the Workshop on Tree-
banks and Linguistic Theories.
Aoife Cahill and Arndt Riester. 2009. Incorporat-
ing information status into generation ranking. In
Proceedings of the Joint Conference of the 47th An-
nual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing
of the AFNLP, pages 817?825, Suntec, Singapore,
August. Association for Computational Linguistics.
Aoife Cahill, Martin Forst, and Christian Rohrer.
2007. Stochastic Realisation Ranking for a Free
Word Order Language. In Proceedings of the
Eleventh European Workshop on Natural Language
Generation, pages 17?24, Saarbru?cken, Germany.
DFKI GmbH.
Aoife Cahill. 2009. Correlating human and automatic
evaluation of a german surface realiser. In Proceed-
ings of the ACL-IJCNLP 2009 Conference Short Pa-
pers, pages 97?100, Suntec, Singapore, August. As-
sociation for Computational Linguistics.
Jackie C.K. Cheung and Gerald Penn. 2010. Entity-
based local coherence modelling using topological
fields. In Proceedings of the 48th Annual Meeting
of the Association for Computational Linguistics
(ACL 2010). Association for Computational Lin-
guistics.
James Clarke and Mirella Lapata. 2010. Discourse
constraints for document compression. Computa-
tional Linguistics, 36(3):411?441.
Stefanie Dipper and Heike Zinsmeister. 2009. The
role of the German Vorfeld for local coherence. In
Christian Chiarcos, Richard Eckart de Castilho, and
Manfred Stede, editors, Von der Form zur Bedeu-
tung: Texte automatisch verarbeiten/From Form to
Meaning: Processing Texts Automatically, pages
69?79. Narr, Tu?bingen.
Katja Filippova and Michael Strube. 2007. The ger-
man vorfeld and local coherence. Journal of Logic,
Language and Information, 16:465?485.
Katja Filippova and Michael Strube. 2009. Tree Lin-
earization in English: Improving Language Model
Based Approaches. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, Companion Volume:
Short Papers, pages 225?228, Boulder, Colorado,
June. Association for Computational Linguistics.
775
Barbara J. Grosz, Aravind Joshi, and Scott Weinstein.
1995. Centering: A framework for modeling the
local coherence of discourse. Computational Lin-
guistics, 21(2):203?225.
Thorsten Joachims. 2006. Training linear SVMs in
linear time. In Proceedings of the ACM Conference
on Knowledge Discovery and Data Mining (KDD),
pages 217?226.
Nikiforos Karamanis, Massimo Poesioand Chris Mel-
lish, and Jon Oberlander. 2009. Evaluating center-
ing for information ordering using corpora. Com-
putational Linguistics, 35(1).
Jane Morris and Graeme Hirst. 1991. Lexical cohe-
sion, the thesaurus, and the structure of text. Com-
putational Linguistics, 17(1):21?225.
Karin Naumann. 2006. Manual for the annotation of
in-document referential relations. Technical report,
Seminar fu?r Sprachwissenschaft, Abt. Computerlin-
guistik, Universita?t Tu?bingen.
Massimo Poesio and Ron Artstein. 2005. The relia-
bility of anaphoric annotation, reconsidered: Taking
ambiguity into account. In Proc. of ACL Workshop
on Frontiers in Corpus Annotation.
Massimo Poesio, Rosemary Stevenson, Barbara di Eu-
genio, and Janet Hitzeman. 2004. Centering: A
parametric theory and its instantiations. Computa-
tional Linguistics, 30(3):309?363.
Eric K. Ringger, Michael Gamon, Robert C. Moore,
David Rojas, Martine Smets, and Simon Corston-
Oliver. 2004. Linguistically Informed Statisti-
cal Models of Constituent Structure for Ordering
in Sentence Realization. In Proceedings of the
2004 International Conference on Computational
Linguistics, Geneva, Switzerland.
Julia Ritz, Stefanie Dipper, and Michael Go?tze. 2008.
Annotation of information structure: An evaluation
across different types of texts. In Proceedings of the
the 6th LREC conference.
Christian Rohrer and Martin Forst. 2006. Improv-
ing Coverage and Parsing Quality of a Large-Scale
LFG for German. In Proceedings of the Fifth In-
ternational Conference on Language Resources and
Evaluation (LREC), Genoa, Italy.
Augustin Speyer. 2005. Competing constraints on
vorfeldbesetzung in german. In Proceedings of the
Constraints in Discourse Workshop, pages 79?87.
Heike Telljohann, Erhard Hinrichs, Sandra Ku?bler,
and Heike Zinsmeister. 2006. Stylebook for the
tu?bingen treebank of written german (tu?ba-d/z).
revised version. Technical report, Seminar fu?r
Sprachwissenschaft, Universita?t Tu?bingen.
Erik Velldal and Stephan Oepen. 2005. Maximum
entropy models for realization ranking. In Proceed-
ings of the 10th Machine Translation Summit, pages
109?116, Thailand.
776
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 579?587,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
How to Produce Unseen Teddy Bears:
Improved Morphological Processing of Compounds in SMT
Fabienne Cap, Alexander Fraser
CIS, University of Munich
{cap|fraser}@cis.uni-muenchen.de
Marion Weller
IMS, University of Stuttgart
wellermn@ims.uni-stuttgart.de
Aoife Cahill
Educational Testing Service
acahill@ets.org
Abstract
Compounding in morphologically rich
languages is a highly productive process
which often causes SMT approaches to
fail because of unseen words. We present
an approach for translation into a com-
pounding language that splits compounds
into simple words for training and, due
to an underspecified representation, allows
for free merging of simple words into
compounds after translation. In contrast to
previous approaches, we use features pro-
jected from the source language to predict
compound mergings. We integrate our ap-
proach into end-to-end SMT and show that
many compounds matching the reference
translation are produced which did not ap-
pear in the training data. Additional man-
ual evaluations support the usefulness of
generalizing compound formation in SMT.
1 Introduction
Productive processes like compounding or inflec-
tion are problematic for traditional phrase-based
statistical machine translation (SMT) approaches,
because words can only be translated as they have
occurred in the parallel training data. As paral-
lel training data is limited, it is desirable to ex-
tract as much information from it as possible. We
present an approach for compound processing in
SMT, translating from English to German, that
splits compounds prior to training (in order to ac-
cess the individual words which together form the
compound) and recombines them after translation.
While compound splitting is a well-studied task,
compound merging has not received as much at-
tention in the past. We start from Stymne and Can-
cedda (2011), who used sequence models to pre-
dict compound merging and Fraser et al. (2012)
who, in addition, generalise over German inflec-
tion. Our new contributions are: (i) We project
features from the source language to support com-
pound merging predictions. As the source lan-
guage input is fluent, these features are more re-
liable than features derived from target language
SMT output. (ii) We reduce compound parts to
an underspecified representation which allows for
maximal generalisation. (iii) We present a detailed
manual evaluation methodology which shows that
we obtain improved compound translations.
We evaluated compound processing both on
held-out split data and in end-to-end SMT. We
show that using source language features increases
the accuracy of compound generation. Moreover,
we find more correct compounds than the base-
lines, and a considerable number of these com-
pounds are unseen in the training data. This is
largely due to the underspecified representation we
are using. Finally, we show that our approach im-
proves upon the previous work.
We discuss compound processing in SMT in
Section 2, and summarise related work in Sec-
tion 3. In Section 4 we present our method for
splitting compounds and reducing the component
words to an underspecified representation. The
merging to obtain German compounds is the sub-
ject of Section 5. We evaluate the accuracy of
compound prediction on held-out data in Section 6
and in end-to-end SMT experiments in Section 7.
We conclude in Section 8.
2 Dealing with Compounds in SMT
In German, two (or more) single words (usually
nouns or adjectives) are combined to form a
compound which is considered a semantic unit.
The rightmost part is referred to as the head while
all other parts are called modifiers. EXAMPLE (1)
lists different ways of joining simple words into
compounds: mostly, no modification is required
(A) or a filler letter is introduced (B). More rarely,
a letter is deleted (C), or transformed (D).
579
Werkzeug
Handel
Obst
Kiste
fruit
box
trading
tool Handelswerkzeug
ObstkisteWerkzeugkiste
Obsthandel fruittrading
tool
boxKiste
Werkzeug
Obst
Handel
splitting training
splitting training
testing re?combination
testing re?combination
Figure 1: Compound processing in SMT allows the synthesis of compounds unseen in the training data.
EXAMPLE (1)
(A) Haus+Boot = Hausboot (?house boat?)
(B) Ort+s+Zeit = Ortszeit (?local time?)
(C) Kirche-e+Turm = Kirchturm (?church tower?)
(D) Kriterium+Liste = Kriterienliste (?criteria list?)
German compounds are highly productive,
1
and
traditional SMT approaches often fail in the face
of such productivity. Therefore, special process-
ing of compounds is required for translation into
German, as many compounds will not (e.g. Haus-
boot, ?house boat?) or only rarely have been seen
in the training data.
2
In contrast, most compounds
consist of two (or more) simple words that occur
more frequently in the data than the compound as
a whole (e.g. Haus (7,975) and Boot (162)) and of-
ten, these compound parts can be translated 1-to-
1 into simple English words. Figure 1 illustrates
the basic idea of compound processing in SMT:
imagine, ?Werkzeug? (?tool?) occurred only as a
modifier of e.g. ?Kiste? (?box?) in the training
data, but the test set contains ?tool? as a simple
word or as the head of a compound. Splitting com-
pounds prior to translation model training enables
better access to the component translations and al-
lows for a high degree of generalisation. At test-
ing time, the English text is translated into the split
German representation, and only afterwards, some
sequences of simple words are (re-)combined into
(possibly unseen) compounds where appropriate.
This merging of compounds is much more chal-
lenging than the splitting, as it has to be applied
to disfluent MT output: i.e., compound parts may
not occur in the correct word order and even if they
do, not all sequences of German words that could
form a compound should be merged.
3 Related Work
Compound processing for translation into a com-
pounding language includes both compound split-
1
Most newly appearing words in German are compounds.
2
~30% of the word types and ~77% of the compound
types we identified in our training data occurred ? 3 times.
ting and merging, we thus report on previous ap-
proaches for both of these tasks.
In the past, there have been numerous attempts
to split compounds, all improving translation qual-
ity when translating from a compounding to a non-
compounding language. Several compound split-
ting approaches make use of substring corpus fre-
quencies in order to find the optimal split points of
a compound (e.g. Koehn and Knight (2003), who
allowed only ?(e)s? as filler letters). Stymne et al.
(2008) use Koehn and Knight?s technique, include
a larger list of possible modifier transformations
and apply POS restrictions on the substrings, while
Fritzinger and Fraser (2010) use a morphological
analyser to find only linguistically motivated sub-
strings. In contrast, Dyer (2010) presents a lattice-
based approach to encode different segmentations
of words (instead of finding the one-best split).
More recently, Macherey et al. (2011) presented
a language-independent unsupervised approach in
which filler letters and a list of words not to be split
(e.g., named entities) are learned using phrase ta-
bles and Levenshtein distance.
In contrast to splitting, the merging of com-
pounds has received much less attention in the
past. An early approach by Popovi?c et al. (2006)
recombines compounds using a list of compounds
and their parts. It thus never creates invalid Ger-
man compounds, but on the other hand it is limited
to the coverage of the list. Moreover, in some con-
texts a merging in the list may still be wrong, cf.
EXAMPLE (3) in Section 5 below. The approach
of Stymne (2009) makes use of a factored model,
with a special POS-markup for compound mod-
ifiers, derived from the POS of the whole com-
pound. This markup enables sound mergings of
compound parts after translation if the POS of the
candidate modifier (X-Part) matches the POS of
the candidate compound head (X): Inflations|N-
Part + Rate|N = Inflationsrate|N (?inflation rate?).
In Stymne and Cancedda (2011) the factored ap-
580
Gas|Traum      8.34Gastraum        3.74Gast|Raum    8.59
4) Disambiguation
...
...
...
Amerikanische Medien ...
Tim Baumeister besiegt ...
Der Gastraum des ...
0) Original Text
...
(S(NP(ADJA Amerikanische) (NN Medien)...))
...(S(NP(PN(NE Tim)(NE Baumeister))(VV besiegt)...))
...
(S(NP(ART Der) (NN Gastraum) (ART des)...))
1) Bitpar Parsed Text
> amerikanische
> Gastraum
> BaumeisterBau<NN>Meister<+NN>Baumeister<+NPROP>
Gast<NN>Raum<+NN>Gas<NN>Traum<+NN>
amerikanisch<+ADJ>
3) SMOR Analysis
amerikanische Medien ...ADJA NN
NETim Baumeister besiegt ...NE VV
ARTNNARTder Gastraum des ...
...
...
...
2) True Casing
Figure 2: Compound splitting pipeline 1) The original text is parsed with BITPAR to get unambiguous POS tags,
2) The original text is then true-cased using the most frequent casing for each word and BITPAR tags are added,
3) All words are analysed with SMOR, analyses are filtered using BITPAR tags (only bold-faced analyses are kept),
4) If several splitting options remain, the geometric mean of the word (part) frequencies is used to disambiguate them.
proach was extended to make use of a CRF se-
quence labeller (Lafferty et al., 2001) in order
to find reasonable merging points. Besides the
words and their POS, many different target lan-
guage frequency features were defined to train the
CRF. This approach can even produce new com-
pounds unseen in the training data, provided that
the modifiers occurred in modifier position of a
compound and heads occurred as heads or even as
simple words with the same inflectional endings.
However, as former compound modifiers were left
with their filler letters (cf. ?Inflations?), they can
not be generalised to compound heads or simple
words, nor can inflectional variants of compound
heads or simple words be created (e.g. if ?Rate?
had only been observed in nominative form in the
training data, the genitive ?Raten? could not be
produced). The underspecified representation we
are using allows for maximal generalisation over
word parts independent of their position of oc-
currence or inflectional realisations. Moreover,
their experiments were limited to predicting com-
pounds on held-out data; no results were reported
for using their approach in translation. In Fraser
et al. (2012) we re-implemented the approach of
Stymne and Cancedda (2011), combined it with
inflection prediction and applied it to a transla-
tion task. However, compound merging was re-
stricted to a list of compounds and parts. Our
present work facilitates more independent com-
bination. Toutanova et al. (2008) and Weller et
al. (2013) used source language features for target
language inflection, but to our knowledge, none of
these works applied source language features for
compound merging.
4 Step 1: Underspecified Representation
In order to enhance translation model accuracy,
it is reasonable to have similar degrees of mor-
phological richness between source and target lan-
guage. We thus reduce the German target lan-
guage training data to an underspecified represen-
tation: we split compounds, and lemmatise all
words (except verbs). All occurrences of simple
words, former compound modifiers or heads have
the same representation and can thus be freely
merged into ?old? and ?new? compounds after
translation, cf. Figure 1 above. So that we can later
predict the merging of simple words into com-
pounds and the inflection of the words, we store
all of the morphological information stripped from
the underspecified representation.
Note that erroneous over-splitting might make
the correct merging of compounds difficult
3
(or even impossible), due to the number of
correct decisions required. For example, it
requires only 1 correct prediction to recom-
bine ?Niederschlag|Menge? into ?Niederschlags-
menge? (?amount of precipitation?) but 3 for
the wrong split into ?nie|der|Schlag|Menge?
(?never|the|hit|amount?). We use the compound
splitter of Fritzinger and Fraser (2010), who have
shown that using a rule-based morphological anal-
yser (SMOR, Schmid et al. (2004)) drastically re-
duced the number of erroneous splits when com-
pared to the frequency-based approach of Koehn
and Knight (2003). However, we adapted it to
work on tokens: some words can, depending on
their context, either be interpreted as named enti-
ties or common nouns, e.g., ?Dinkelacker? (a Ger-
man beer brand or ?spelt|field?).
4
We parsed the
training data and use the parser?s decisions to iden-
tify proper names, see ?Baumeister? in Figure 2.
After splitting, we use SMOR to reduce words to
lemmas, keeping morphological features like gen-
der or number, and stripping features like case, as
illustrated for ?
?
Olexporteure? (?oil exporters?):
3
In contrast, they may not hurt translation quality in the
other direction, where phrase-based SMT is likely to learn
the split words as a phrase and thus recover from that error.
4
Note that Macherey et al. (2011) blocked splitting of
words which can be used as named entities, independent of
context, which is less general than our solution.
581
No. Feature Description Example
Experiment
SC T TR
1SC surface form of the word string: Arbeit<+NN><Fem><Sg> X X
2SC main part of speech of the word (from the parser) string: +NN X X
3SC word occurs in a bigram with the next word frequency: 0 X X
4SC word combined to a compound with the next word frequency: 10,000 X X X
5SC word occurs in modifier position of a compound frequency: 100,000 X X
6SC word occurs in a head position of a compound frequency: 10,000 X X
7SC word occurs in modifier position vs. simplex string: P>W (P= 5SC, W= 100,000) X
8SC word occurs in head position vs. simplex string: S<W (S= 6SC, W= 100,000) X
7SC+ word occurs in modifier position vs. simplex ratio: 10 (10**ceil(log10(5SC/W))) X X
8SC+ word occurs in head position vs. simplex ratio: 1 (10**ceil(log10(6SC/W))) X X
9N different head types the word can combine with number: 10,000 X X
Table 1: Target language CRF features for compound merging. SC = features taken from Stymne and Cancedda
(2011), SC+ = improved versions, N = new feature. Experiments: SC = re-implementation of Stymne and Cancedda (2011),
T= use full Target feature set, TR = use Target features, but only a Reduced set.
EXAMPLE (2)
?l<+NN><Neut><Sg> Exporteur<+NN> <Masc><Pl>
?l<NN>Exporteur<+NN><Masc><Nom><Pl>compound
headmodifier
While the former compound head (?Exporteure?)
automatically inherits all morphological features
of the compound as a whole, the features of the
modifier need to be derived from SMOR in an ad-
ditional step. We need to ensure that the repre-
sentation of the modifier is identical to the same
word when it occurs independently in order to ob-
tain full generalisation over compound parts.
5 Step 2: Compound Merging
After translation from English into the underspec-
ified German representation, post-processing is re-
quired to transform the output back into fluent,
morphologically fully specified German. First,
compounds need to be merged where appropriate,
e.g., ?Hausboote? (?house boats?):
Haus<+NN><Neut><Sg> + Boot<+NN><Neut><Pl>
? Haus<NN>Boot<+NN><Neut><Pl> (merged)
and second, all words need to be inflected:
Haus<NN>Boot<+NN><Neut><Acc><Pl>
? Hausbooten (inflected)
5.1 Target Language Features
To decide which words should be combined, we
follow Stymne and Cancedda (2011) who used
CRFs for this task. The features we derived from
the target language to train CRF models are listed
in Table 1. We adapted features No. 1-8 from
Stymne and Cancedda (2011). Then, we modi-
fied two features (7+8) and created a new feature
indicating the productivity of a modifier (9N).
5.2 Projecting Source Language Features
We also use new features derived from the English
source language input, which is coherent and flu-
ent. This makes features derived from it more reli-
able than the target language features derived from
disfluent SMT output. Moreover, source language
features might support or block merging decisions
in unclear cases, i.e., where target language fre-
quencies are not helpful, either because they are
very low or they have roughly equal frequency dis-
tributions when occurring in a compound (as mod-
ifier or head) vs. as a simple word.
In Table 2, we list three types of features:
1. Syntactic features: different English noun
phrase patterns that are aligned to German
compound candidate words (cf. 10E-13E)
2. The POS tag of the English word (cf. 14E)
3. Alignment features, derived from word
alignments (cf. 15E-18E)
The examples given in Table 2 (10E-13E) show
that English compounds often have 1-to-1 corre-
spondences to the parts of a German compound.
Knowing that two consecutive German simple
words are aligned to two English words of the
same noun phrase is a strong indicator that the
German words should be merged:
EXAMPLE (3)
should be merged:
ein erh?ohtes verkehrs aufkommen sorgt f?ur chaos
?an increased traffic volume causes chaos?
(S...(NP(DT An)(VN increased)(NN traffic)(NN volume))..)))
should not be merged:
f?ur die finanzierung des verkehrs aufkommen
?pay for the financing of transport?
(VP(V pay)(PP(IN for)(NP(NP(DT the)(NN financing))
(PP(IN of)(NP(NN transport)..))
In the compound reading of ?verkehr + aufkom-
men?, the English parse structure indicates that
the words aligned to ?verkehr? (?traffic?) and
582
No. Feature Description Type
10E
word and next word are aligned from a noun phrase in the English source sentence:
(NP(NN traffic)(NN accident))? Verkehr (?traffic?) + Unfall (?accident?)
true/false
11E
word and next word are aligned from a gerund construction in the English source sentence:
(NP(VBG developing)(NNS nations))? Entwicklung (?development?) + L?ander (?countries?)
true/false
12E
word and next word are aligned from a genitive construction in the English source sentence:
(NP(NP(DT the)(NN end))(PP(IN of)(NP(DT the)(NN year))? Jahr (?year?) + Ende(?end?)
true/false
13E
word and next word are aligned from an adjective noun construction in the English source sentence:
(NP (ADJ protective)(NNS measures))? Schutz (?protection?) + Ma?nahmen (?measures?)
true/false
14E print the POS of the corresponding aligned English word string
15E
word and next word are aligned 1-to-1 from the same word in the English source sentence, e.g.,
beef
?
?
Rind(?cow?)
Fleisch(?meat?)
true/false
16E like 15E, but the English word contains a dash, e.g., Nobel ? Prize
?
?
Nobel(?Nobel?)
Preis(?prize?)
true/false
17E like 15E, but also considering 1-to-n and n-to-1 links true/false
18E like 16E, but also considering 1-to-n and n-to-1 links true/false
Table 2: List of new source language CRF features for compound merging.
?aufkommen? (?volume?), are both nouns and
part of one common noun phrase, which is a strong
indicator that the two words should be merged
in German. In contrast, the syntactic relation-
ship between ?pay? (aligned to ?aufkommen?)
and ?transport? (aligned to ?verkehr?) is more dis-
tant
5
: merging is not indicated.
We also use the POS of the English words to
learn (un)usual combinations of POS, indepen-
dent of their exact syntactic structure (14E). Re-
consider EXAMPLE (3): NN+NN is a more com-
mon POS pair for compounds than V+NN.
Finally, the alignment features (15E-18E) pro-
mote the merging into compounds whose align-
ments indicate that they should not have been split
in the first place (e.g., Rindfleisch, 15E).
5.3 Compound Generation and Inflection
So far, we reported on how to decide which sim-
ple words are to be merged into compounds, but
not how to recombine them. Recall from EXAM-
PLE (1) that the modifier of a compound some-
times needs to be transformed, before it can be
combined with the head word (or next modifier),
e.g., ?Ort?+?Zeit? = ?Ortszeit? (?local time?).
We use SMOR to generate compounds from a
combination of simple words. This allows us to
create compounds with modifiers that never oc-
curred as such in the training data. Imagine that
?Ort? occurred only as compound head or as a
single word in the training data. Using SMOR, we
are still able to create the correct form of the mod-
ifier, including the required filler letter: ?Orts?.
This ability distinguishes our approach from pre-
5
Note that ?f?ur etwas aufkommen? (lit. ?for sth. arise?,
idiom.: ?to pay for sth.?) is an idiomatic expression.
vious approaches: Stymne and Cancedda (2011)
do not reduce modifiers to their base forms
6
(they
can only create new compounds when the modifier
occurred as such in the training data) and Fraser et
al. (2012) use a list for merging.
Finally, we use the system described in Fraser
et al. (2012) to inflect the entire text.
6 Accuracy of Compound Prediction
We trained CRF models on the parallel training
data (~40 million words)
7
of the EACL 2009
workshop on statistical machine translation
8
us-
ing different feature (sub)sets, cf. the ?Exper-
iment? column in Table 1 above. We exam-
ined the reliability of the CRF compound predic-
tion models by applying them to held-out data:
1. split the German wmt2009 tuning data set
2. remember compound split points
3. predict merging with CRF models
4. combine predicted words into compounds
5. calculate f-scores on how properly the
compounds were merged
Table 3 lists the CRF models we trained, together
with their compound merging accuracies on held-
out data. It can be seen that using more features
(SC?T?ST) is favourable in terms of precision
and overall accuracy and the positive impact of us-
ing source language features is clearer when only
reduced feature sets are used (TR vs. STR).
However, these accuracies only somewhat cor-
relate with SMT performance: while being trained
and tested on clean, fluent German language, the
6
They account for modifier transformations by using char-
acter n-gram features (cf.EXAMPLE (1)).
7
However, target language feature frequencies are derived
from the monolingual training data, ~146 million words.
8
http://www.statmt.org/wmt09
583
exp to be all correct wrong wrong not merging
precision recall f-score
merged merged merged merged merged wrong
SC 1,047 997 921 73 121 3 92.38% 88.13% 90.21%
T 1,047 979 916 59 128 4 93.56% 87.40% 90.38%
ST 1,047 976 917 55 126 4 93.95% 87.58% 90.66%
TR 1,047 893 836 52 204 5 93.62% 80.00% 86.27%
STR 1,047 930 866 58 172 6 93.12% 82.95% 87.74%
Table 3: Compound production accuracies of CRF models on held-out data: SC: re-implementation of Stymne
and Cancedda (2011); T: all target language features, including a new one (cf. Table 1); ST = all Source and Target language
features; TR: only a reduced set of target language features; STR: TR, plus all source language features given in Table 2.
exp BLEU SCORES #compounds found
mert.log BLEU RTS all ref new new*
RAW 14.88 14.25 1.0054 646 175 n.a. n.a.
UNSPLIT 15.86 14.74 0.9964 661 185 n.a. n.a.
SC 15.44 14.45 0.9870 882 241 47 8
T 15.56 14.32 0.9634 845 251 47 8
ST 15.33 14.51 0.9760 820 248 46 9
TR 15.24 14.26 0.9710 753 234 44 5
STR 15.37 14.61 0.9884 758 239 43 7
#compounds in reference text: 1,105 1,105 396 193
Table 4: SMT results. Tuning scores (mert.log) are on merged but uninflected data (except RAW).
RTS: length ratio; all: #compounds produced; ref: reference matches; new: unknown to parallel data; new*: unknown to
target language data. bold face indicates statistical significance wrt. the RAW baseline, SC, T and TR.
models will later be applied to disfluent SMT out-
put and might thus lead to different results there.
Stymne and Cancedda (2011) dealt with this by
noisifying the CRF training data: they translated
the whole data set using an SMT system that was
trained on the same data set. This way, the train-
ing data was less fluent than in its original format,
but still of higher quality than SMT output of un-
seen data. In contrast, we left the training data as
it was, but strongly reduced the feature set for CRF
model training (e.g., no more use of surface words
and POS tags, cf. TR and STR in Table 3) instead.
7 Translation Performance
We integrated our compound processing pipeline
into an end-to-end SMT system. Models were
trained with the default settings of the Moses SMT
toolkit, v1.0 (Koehn et al., 2007) using the data
from the EACL 2009 workshop on statistical ma-
chine translation. All compound processing sys-
tems are trained and tuned identically, except us-
ing different CRF models for compound predic-
tion. All training data was split and reduced
to the underspecified representation described in
Section 4. We used KenLM (Heafield, 2011) with
SRILM (Stolcke, 2002) to train a 5-gram language
model based on all available target language train-
ing data. For tuning, we used batch-mira with ?-
safe-hope? (Cherry and Foster, 2012) and ran it
separately for every experiment. We integrated the
CRF-based merging of compounds into each itera-
tion of tuning and scored each output with respect
to an unsplit and lemmatised version of the tuning
reference. Testing consists of:
1. translation into the split, underspecified
German representation
2. compound merging using CRF models
to predict recombination points
3. inflection of all words
7.1 SMT Results
We use 1,025 sentences for tuning and 1,026 sen-
tences for testing. The results are given in Table 4.
We calculate BLEU scores (Papineni et al., 2002)
and compare our systems to a RAW baseline (built
following the instructions of the shared task) and a
baseline very similar to Fraser et al. (2012), using
a lemmatised representation of words for decod-
ing, re-inflecting them after translation, but with-
out compound processing (UNSPLIT). Table 4
shows that only UNSPLIT and STR (source lan-
guage and a reduced set of target language fea-
tures) are significantly
9
improving over the RAW
baseline. They also significantly outperform all
other systems, except ST (full source and target
language feature set). The difference between STR
(14.61) and the UNSPLIT baseline (14.74) is not
statistically significant.
9
We used pair-wise bootstrap resampling with sample size
1000 and p-value 0.05, from: http://www.ark.cs.cmu.edu/MT
584
group ID example reference english UNSPLIT STR
lexically 1a: perfect match Inflationsrate Inflationsrate inflation rate 185 239
matches 1b: inflection wrong Rohstoffpreisen Rohstoffpreise raw material prices 40 44
the 2a: merging wrong Anwaltsbewegung Anw?altebewegung lawyers movement 5 9
reference 2b: no merging Polizei Chef Polizeichef police chief 101 54
correct 3a: compound Zentralbanken Notenbank central banks 92 171
translation 3b: no compound pflanzliche
?
Ole Speise?ol vegetable oils 345 291
wrong 4a: compound Haushaltsdefizite Staatshaushalts state budget 12 42
translation 4b: no compound Ansporn Linien Nebenlinien spur lines 325 255
Total number of compounds in reference text: 1,105 1,105
Table 5: Groups for detailed manual compound evaluation and results for UNSPLIT and STR.
reference English source UNSPLIT baseline STR
Teddyb?aren teddy bear 4b
Teddy tragen
1a
Teddyb?aren
(Teddy, to bear) (teddy bear)
Emissionsreduktion emissions reduction 3b
Emissionen Reduzierung
3a
Emissionsverringerung
(emissions, reducing) (emission decrease)
Geldstrafe fine 4b
sch?onen
3a
Bu?geld
(fine/nice) (monetary fine)
Tischtennis table tennis 2b
Tisch Tennis
4a
Spieltischtennis
(table, tennis) (play table tennis)
Kreditkartenmarkt credit-card market 2b
Kreditkarte Markt
4a
Kreditmarkt
(credit-card, market) (credit market)
Rotationstempo rotation rate 2b
Tempo Rotation
4a
Temporotation
(rate, rotation) (rate rotation)
Table 6: Examples of the detailed manual compound analysis for UNSPLIT and STR.
Compound processing leads to improvements at
the level of unigrams and as BLEU is dominated
by four-gram precision and length penalty, it does
not adequately reflect compound related improve-
ments. We thus calculated the number of com-
pounds matching the reference for each experi-
ment and verified whether these were known to
the training data. The numbers in Table 4 show
that all compound processing systems outperform
both baselines in terms of finding more exact refer-
ence matches and also more compounds unknown
to the training data. Note that STR finds less ref-
erence matches than e.g. T or ST, but it also pro-
duces less compounds overall, i.e. it is more pre-
cise when producing compounds.
However, as compounds that are correctly com-
bined but poorly inflected are not counted, this is
only a lower bound on true compounding perfor-
mance. We thus performed two additional manual
evaluations and show that the quality of the com-
pounds (Section 7.2), and the human perception of
translation quality is improving (Section 7.3).
7.2 Detailed Evaluation of Compounds
This evaluation focuses on how compounds in the
the reference text have been translated.
10
We:
10
In another evaluation, we investigated the 519 com-
pounds that our system produced but which did not match
the reference: 367 were correct translations of the English,
1. manually identify compounds in German
reference text (1,105 found)
2. manually perform word alignment of these
compounds to the English source text
3. project these English counterparts of com-
pounds in the reference text to the decoded
text using the ??print-alignment-info? flag
4. manually annotate the resulting tuples, us-
ing the categories given in Table 5
The results are given in the two rightmost columns
of Table 5: besides a higher number of reference
matches (cf. row 1a), STR overall produces more
compounds than the UNSPLIT baseline, cf. rows
2a, 3a and 4a. Indirectly, this can also be seen from
the low numbers of STR in category 2b), where
the UNSPLIT baseline produces much more (101
vs. 54) translations that lexically match the refer-
ence without being a compound. While the 171
compounds of STR of category 3a) show that our
system produces many compounds that are correct
translations of the English, even though not match-
ing the reference (and thus not credited by BLEU),
the compounds of categories 2a) and 4a) contain
examples where we either fail to reproduce the
correct compound or over-generate compounds.
We give some examples in Table 6: for ?teddy
bear?, the correct German word ?Teddyb?aren? is
87 contained erroneous lexemes and 65 were over-mergings.
585
missing in the parallel training data and instead
of ?B?ar? (?bear?), the baseline selected ?tragen?
(?to bear?). Extracting all words containing the
substring ?b?ar? (?bear?) from the original parallel
training data and from its underspecified split
version demonstrates that our approach is able
to access all occurrences of the word. This leads
to higher frequency counts and thus enhances
the probabilities for correct translations. We can
generalise over 18 different word types containing
?bear? (e.g. ?polar bears?, ?brown bears?, ?bear
skin?, ?bear fur?) to obtain only 2:
occurrences in raw training data: B?ar (19), B?aren
(26), B?arendienst (42), B?arenfarmen (1), B?arenfell (2),
B?arengalle(1), B?arenhaut (1), B?arenmarkt (1), Braunb?ar
(1), Braunb?aren (3), Braunb?arengebiete (1), Braunb?ar-
Population (1), Eisb?aren(18), Eisb?arenpopulation (2),
Eisb?arenpopulationen (1), Schwarzb?ar (1), Schwarzb?aren (1)
?b?ar? occurring in underspecified split data:
B?ar<+NN><Masc><Sg> (94)
B?ar<+NN><Masc><Pl> (29)
?Emissionsverringerung? (cf. Table 6) is a typ-
ical example of group 3a): a correctly translated
compound that does not lexically match the ref-
erence, but which is semantically very similar to
the reference. The same applies for ?Bu?geld?,
a synonym of ?Geldstrafe?, for which the UN-
SPLIT baseline selected ?sch?onen? (?fine, nice?)
instead. Consider also the wrong compound pro-
ductions, e.g. ?Tischtennis? is combined with
the verb ?spielen? (?to play?) into ?Spieltischten-
nis?. In contrast, ?Kreditmarkt? dropped the mid-
dle part ?Karte? (?card?), and in the case of ?Tem-
porotation?, the head and modifier of the com-
pound are switched.
7.3 Human perception of translation quality
We presented sentences of the UNSPLIT baseline
and of STR in random order to two native speak-
ers of German and asked them to rank the sen-
tences according to preference. In order to pre-
vent them from being biased towards compound-
bearing sentences, we asked them to select sen-
tences based on their native intuition, without re-
vealing our focus on compound processing.
Sentences were selected based on source lan-
guage sentence length: 10-15 words (178 sen-
tences), of which either the reference or our
system had to contain a compound (95 sen-
tences). After removing duplicates, we ended up
with 84 sentences to be annotated in two subse-
(a) Fluency: without reference sentence
? = 0.3631
person 1
STR UNSPLIT equal
p
e
r
s
o
n
2
STR 24 6 7 37
UNSPLIT 5 16 9 30
equal 6 2 9 17
35 24 25 84
(b) Adequacy: with reference sentence
? = 0.4948
person 1
STR UNSPLIT equal
p
e
r
s
o
n
2
STR 23 4 5 32
UNSPLIT 4 21 7 32
equal 5 3 12 20
32 28 24 84
Table 7: Human perception of translation quality.
quent passes: first, without being given the refer-
ence sentence (approximating fluency), then, with
the reference sentence (approximating adequacy).
The results are given in Table 7. Both annotators
preferred more sentences of our system overall,
but the difference is clearer for the fluency task.
8 Conclusion
Compounds require special attention in SMT, es-
pecially when translating into a compounding lan-
guage. Compared with the baselines, all of our ex-
periments that included compound processing pro-
duced not only many more compounds matching
the reference exactly, but also many compounds
that did not occur in the training data. Taking
a closer look, we found that some of these new
compounds could only be produced due to the un-
derspecified representation we are using, which al-
lows us to generalise over occurrences of simple
words, compound modifiers and heads. Moreover,
we demonstrated that features derived from the
source language are a valuable source of informa-
tion for compound prediction: experiments were
significantly better compared with contrastive ex-
periments without these features. Additional man-
ual evaluations showed that compound processing
leads to improved translations where the improve-
ment is not captured by BLEU.
Acknowledgements
This work was supported by Deutsche For-
schungsgemeinschaft grants Models of Mor-
phosyntax for Statistical Machine Translation
(Phase 2) and Distributional Approaches to Se-
mantic Relatedness. We thank the anonymous re-
viewers for their comments and the annotators.
586
References
Colin Cherry and George Foster. 2012. Batch Tun-
ing Strategies for Statistical Machine Translation. In
HLT-NAACL?12: Proceedings of the Human Lan-
guage Technology Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics, volume 12, pages 34?35. Association
for Computational Linguistics.
Chris Dyer. 2010. A Formal Model of Ambiguity and
its Applications in Machine Translation. Phd disser-
tation, University of Maryland, USA.
Alexander Fraser, Marion Weller, Aoife Cahill, and Fa-
bienne Cap. 2012. Modeling Inflection and Word
Formation in SMT. In EACL?12: Proceedings of the
13th Conference of the European Chapter of the As-
sociation for Computational Linguistics, pages 664?
674. Association for Computational Linguistics.
Fabienne Fritzinger and Alexander Fraser. 2010. How
to Avoid Burning Ducks: Combining Linguistic
Analysis and Corpus Statistics for German Com-
pound Processing. In Proceedings of the Fifth Work-
shop on Statistical Machine Translation, pages 224?
234. Association for Computational Linguistics.
Kenneth Heafield. 2011. KenLM: Faster and Smaller
Language Model Queries. In Proceedings of the
Sixth Workshop on Statistical Machine Translation,
Edinburgh, UK, July. Association for Computational
Linguistics.
Philipp Koehn and Kevin Knight. 2003. Empirical
Methods for Compound Splitting. In EACL ?03:
Proceedings of the 10th Conference of the European
Chapter of the Association for Computational Lin-
guistics, pages 187?193, Morristown, NJ, USA. As-
sociation for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
In ACL?07: Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguis-
tics, Demonstration Session, pages 177?180. Asso-
ciation for Computational Linguistics.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional Random Fields: Prob-
abilistic Models for Segmenting and Labeling Se-
quence Data. In ICML?01: Proceedings of the 18th
International Conference on Machine Learning.
Klaus Macherey, Andrew M. Dai, David Talbot,
Ashok C. Popat, and Franz Och. 2011. Language-
independent Compound Splitting with Morpholog-
ical Operations. In ACL ?11: Proceedings of the
49th annual meeting of the Association for Compu-
tational Linguistics, pages 1395?1404. Association
for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: A Method for Automatic
Evaluation of Machine Translation. In ACL?02:
Proceedings of the 40th Annual Meeting of the As-
sociation for Computational Linguistics, pages 311?
318. Association for Computational Linguistics.
Maja Popovi?c, Daniel Stein, and Hermann Ney. 2006.
Statistical Machine Translation of German Com-
pound Words. In FinTAL?06: Proceedings of the
5th International Conference on Natural Language
Processing, pages 616?624. Springer Verlag.
Helmut Schmid, Arne Fitschen, and Ulrich Heid.
2004. SMOR: A German Computational Morphol-
ogy Covering Derivation, Composition and Inflec-
tion. In LREC ?04: Proceedings of the 4th Confer-
ence on Language Resources and Evaluation, pages
1263?1266.
Andreas Stolcke. 2002. SRILM ? an Extensible Lan-
guage Modelling Toolkit. In ICSLN?02: Proceed-
ings of the international conference on spoken lan-
guage processing, pages 901?904.
Sara Stymne and Nicola Cancedda. 2011. Productive
Generation of Compound Words in Statistical Ma-
chine Translation. In EMNLP?11: Proceedings of
the 6th Workshop on Statistical Machine Transla-
tion and Metrics MATR of the conference on Em-
pirical Methods in Natural Language Processing,
pages 250?260. Association for Computational Lin-
guistics.
Sara Stymne, Maria Holmqvist, and Lars Ahrenberg.
2008. Effects of Morphological Analysis in Transla-
tion between German and English. In ACL?08: Pro-
ceedings of the 3rd workshop on statistical machine
translation of the 46th annual meeting of the Associ-
ation for Compuational Linguistics, pages 135?138.
Association for Computational Linguistics,.
Sara Stymne. 2009. A Comparison of Merging Strate-
gies for Translation of German Compounds. In
EACL ?09: Proceedings of the Student Research
Workshop of the 12th conference of the European
Chapter of the Association for Computational Lin-
guistics, pages 61?69. Association for Computa-
tional Linguistics.
Kristina Toutanova, Hisami Suzuki, and Achim Ruopp.
2008. Applying Morphology Generation Models to
Machine Translation. In ACL?08: Proceedings of
the 46th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, pages 514?522. Association for Computational
Linguistics.
Marion Weller, Alexander Fraser, and Sabine
Schulte im Walde. 2013. Using Subcatego-
rization Knowledge to Improve Case Prediction for
Translation to German. In ACL?13: Proceedings
of the 51st Annual Meeting of the Association
for Computational Linguistics, pages 593?603.
Association for Computational Linguistics.
587
Proceedings of NAACL-HLT 2013, pages 507?517,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Robust Systems for Preposition Error Correction Using Wikipedia Revisions
Aoife Cahill?, Nitin Madnani?, Joel Tetreault? and Diane Napolitano?
? Educational Testing Service, 660 Rosedale Road, Princeton, NJ 08541, USA
{acahill, nmadnani, dnapolitano}@ets.org
? Nuance Communications, Inc., 1198 E. Arques Ave, Sunnyvale, CA 94085, USA
Joel.Tetreault@nuance.com
Abstract
We show that existing methods for training
preposition error correction systems, whether
using well-edited text or error-annotated cor-
pora, do not generalize across very differ-
ent test sets. We present a new, large error-
annotated corpus and use it to train systems
that generalize across three different test sets,
each from a different domain and with differ-
ent error characteristics. This new corpus is
automatically extracted from Wikipedia revi-
sions and contains over one million instances
of preposition corrections.
1 Introduction
One of the main themes that has defined the field of
automatic grammatical error correction has been the
availability of error-annotated learner data to train
and test a system. Some errors, such as determiner-
noun number agreement, are easily corrected us-
ing rules and regular expressions (Leacock et al,
2010). On the other hand, errors involving the usage
of prepositions and articles are influenced by sev-
eral factors including the local context, the prior dis-
course and semantics. These errors are better han-
dled by statistical models which potentially require
millions of training examples.
Most statistical approaches to grammatical error
correction have used one of the following training
paradigms: 1) training solely on examples of cor-
rect usage (Han et al, 2006); 2) training on exam-
ples of correct usage and artificially generated er-
rors (Rozovskaya and Roth, 2010); and 3) training
on examples of correct usage and real learner er-
rors (Dahlmeier and Ng, 2011; Dale et al, 2012).
The latter two methods require annotated corpora of
errors, and while they have shown great promise,
manually annotating grammatical errors in a large
enough corpus of learner writing is often a costly
and time-consuming endeavor.
In order to efficiently and automatically acquire a
very large corpus of annotated learner errors, we in-
vestigate the use of error corrections extracted from
Wikipedia revision history. While Wikipedia re-
vision history has shown promise for other NLP
tasks including paraphrase generation (Max and
Wisniewski, 2010; Nelken and Yamangil, 2008) and
spelling correction (Zesch, 2012), this resource has
not been used for the task of grammatical error cor-
rection.
To evaluate the usefulness of Wikipedia revision
history for grammatical error correction, we address
the task of correcting errors in preposition selection
(i.e., where the context licenses the use of a prepo-
sition, but the writer selects the wrong one). We
first train a model directly on instances of correct
and incorrect preposition usage extracted from the
Wikipedia revision data. We also generate artificial
errors using the confusion distributions derived from
this data. We compare both of these approaches to
models trained on well-edited text and evaluate each
on three test sets with a range of different character-
istics. Each training paradigm is applied to multiple
data sources for comparison. With these multiple
evaluations, we address the following research ques-
tions:
1. Across multiple test sets, which data source
507
is more useful for correcting preposition er-
rors: a large amount of well-edited text, a large
amount of potentially noisy error-annotated
data (either artificially generated or automati-
cally extracted) or a smaller amount of higher
quality error-annotated data?
2. Given error-annotated data, is it better to train
on the corrections directly or to use the con-
fusion distributions derived from these correc-
tions for generating artificial errors in well-
edited text?
3. What is the impact of having a mismatch in the
error distributions of the training and test sets?
2 Related Work
In this section, we only review work in preposi-
tion error correction in terms of the three training
paradigms and refer the reader to Leacock et al
(2010) for a more comprehensive review of the field.
2.1 Training on Well-Edited Text
Early approaches to error detection and correction
did not have access to large amounts of error-
annotated data to train statistical models and thus,
systems were trained on millions of well-edited ex-
amples from news text instead (Gamon et al, 2008;
Tetreault and Chodorow, 2008; De Felice and Pul-
man, 2009). Feature sets usually consisted of n-
grams around the preposition, POS sequences, syn-
tactic features and semantic information. Since the
model only had knowledge of correct usage, an error
was flagged if the system?s prediction for a particu-
lar preposition context differed from the preposition
the writer used.
2.2 Artificial Errors
The issue with training solely on correct usage was
that the systems had no knowledge of typical learner
errors. Ideally, a system would be trained on ex-
amples of correct and incorrect usage, however, for
many years, such error-annotated corpora were not
available. Instead, several researchers generated ar-
tificial errors based on the error distributions derived
from the error-annotated learner corpora available at
the time. Izumi et al (2003) was the first to evaluate
a model trained on incorrect usage as well as artifi-
cial errors for the task of correcting several different
error types, including prepositions. However, with
limited training data, system performance was quite
poor. Rozovskaya and Roth (2010) evaluated dif-
ferent ways of generating artificial errors and found
that a system trained on artificial errors could outper-
form the more traditional training paradigm of using
only well-edited texts. Most recently, Imamura et al
(2012) showed that performance could be improved
by training a model on artificial errors and address-
ing domain adaptation for the task of Japanese par-
ticle correction.
2.3 Error-Annotated Learner Corpora
Recently, error-annotated learner data has become
more readily and publicly available allowing models
to be trained on both examples of correct usage as
well typical learner errors. Han et al (2010) showed
that a preposition error detection and correction sys-
tem trained on 100,000 annotated preposition errors
from the Chungdahm Corpus of Korean Learner En-
glish (in addition to 1 million examples of correct
usage) outperformed a model trained only on 5 mil-
lion examples of correct usage. Gamon (2010) and
Dahlmeier and Ng (2011) showed that combining
models trained separately on examples of correct
and incorrect usage could also improve the perfor-
mance of a preposition error correction system.
3 Mining Wikipedia Revisions for
Grammatical Error Corrections
3.1 Related Work
Many NLP researchers have taken advantage of the
wealth of information available in Wikipedia revi-
sions. Dutrey et al (2011) define a typology of mod-
ifications found in the French Wikipedia (WiCo-
PaCo). They show that the kinds of edits made range
from specific lexical changes to more general rewrite
edits. Similar types of edits are found in the En-
glish Wikipedia. The data extracted from Wikipedia
revisions has been used for a wide variety of tasks
including spelling correction (Max and Wisniewski,
2010; Zesch, 2012), lexical error detection (Nelken
and Yamangil, 2008), sentence compression (Ya-
mangil and Nelken, 2008), paraphrase generation
(Max and Wisniewski, 2010; Nelken and Yamangil,
2008), lexical simplification (Yatskar et al, 2010)
and entailment (Zanzotto and Pennacchiotti, 2010;
508
(1) [Wiki clean] In addition, sometimes it is also left to stand overnight (at? in) the refrigerator.
(2) [Wiki clean] Also none of the witnesses present (of? on) those dates supports Ranneft?s claims.
(3) [Wiki dirty] . . . cirque has a permanent production (to? at) the Mirage, love.
(4) [Wiki dirty] In the late 19th century Vasilli Andreyev a salon violinist took up the balalaika in his
performances for French tourists (in? to) Petersburg.
Figure 1: Example sentences with preposition errors extracted from Wikipedia revisions. The second preposition is
assumed to be the correction.
Cabrio et al, 2012). To our knowledge, no one has
previously extracted data for training a grammatical
error detection system from Wikipedia revisions.
3.2 Extracting Preposition Correction Data
from Wikipedia Revisions
As the source of our Wikipedia revisions, we used an
XML snapshot of Wikipedia generated in July 2011
containing 8,735,890 articles and 288,583,063 revi-
sions.1 We then used the following process to ex-
tract preposition errors and their corresponding cor-
rections from this snapshot:
Step 1: Extract the plain text versions of all revi-
sions of all articles using the Java Wikipedia
Library (Ferschke et al, 2011).
Step 2: For each Wikipedia article, compare each
revision with the revision immediately preced-
ing it using an efficient diff algorithm.2
Step 3: Compute all 1-word edit chains for the arti-
cle, i.e., sequences of related edits derived from
all revisions of the same article. For example,
say revision 10 of an article inserts the preposi-
tion of into a sentence and revision 12 changes
that preposition to on. Assuming that no other
revisions change this sentence, the correspond-
ing edit chain would contain the following 3 el-
ements: ?of?on. The extracted chains con-
tain the full context on either side of the 1-word
edit, up to the automatically detected sentence
boundaries.
Step 4: (a) Ignore any circular chains, i.e., where
the first element in the edit chain is the same as
the last element. (b) Collapse all non-circular
1http://dumps.wikimedia.org/enwiki/
2http://code.google.com/p/google-diff-match-patch/
chains, i.e., only retain the first and the last ele-
ments in a chain. Both these decisions are mo-
tivated by the assumption that the intermediate
links in the chain are unreliable for training an
error correction system since a Wikipedia con-
tributor modified them.
Step 5 : From all remaining 2-element chains, find
those where a preposition is replaced with an-
other preposition. If the preposition edit is the
only edit in the sentence, we convert the chain
into a sentence pair and label it clean. If there
are other 1-word edits but not within 5 words of
the preposition edit on either side, we label the
sentence somewhat clean. Otherwise, we label
it dirty. The motivation is that the presence of
other nearby edits make the preposition correc-
tion less reliable when used in isolation, due to
the possible dependencies between corrections.
All extracted sentences were part-of-speech tagged
using the Stanford Tagger (Toutanova et al, 2003).
Using the above process, we are able to extract ap-
proximately 2 million sentences containing preposi-
tions errors and their corrections. Some examples
of the sentences we extracted are given in Figure 1.
Example (4) shows an example of a bad correction.
4 Corpora
We use several corpora for training and testing our
preposition error correction system. The proper-
ties of each are outlined in Table 1, organized by
paradigm. For each corpus we report the total num-
ber of prepositions used for training, as well as the
number and percentage of preposition corrections.
4.1 Well-edited Text
We train our system on two well-edited corpora.
The first is the same corpus used by Tetreault and
509
Corpus Total # Preps # Corrected Preps
Well-edited Text
Wikipedia Snapshot (10m sents) 26,069,860 0 (0%)
Lexile/SJM 6,719,077 0 (0%)
Artificially Generated
Errors
Wikipedia Snapshot 26,127,464 2,844,227 (10.9%)
Lexile/SJM 6,723,206 792,195 (11.8%)
Naturally Occurring
Errors
Wikipedia Revisions All 7,125,317 1,027,643 (20.6%)
Wikipedia Revisions ?Clean 3,001,900 381,644 (12.7%)
Wikipedia Revisions Clean 1,978,802 266,275 (14.4%)
Lang-8 129,987 53,493 (41.2%)
NUCLE Train 72,741 922 (1.3%)
Test Corpora
NUCLE Test 9,366 125 (1.3%)
FCE 33,243 2,900 (8.7%)
HOO 2011 Test 1,703 81 (4.8%)
Table 1: Corpora characteristics
Chodorow (2008), comprising roughly 1.8 million
sentences from the San Jose Mercury News Corpus3
and roughly 1.8 million sentences from grades 11
and 12 of the MetaMetrics Lexile Corpus. Our sec-
ond corpus is a random sample of 10 million sen-
tences containing at least one preposition from the
June 2012 snapshot of English Wikipedia Articles.4
4.2 Artificially Generated Errors
Similar to Foster and Andersen (2009) and Ro-
zovskaya and Roth (2010), we artificially introduce
preposition errors into well-edited corpora (the two
described above). We do this based on a distribu-
tion of possible confusions and train a model that
is aware of the corrections. The two sets of con-
fusion distributions we used were derived based on
the errors extracted from Wikipedia revisions and
Lang-8 respectively (discussed in Section 4.3). For
each corrected preposition pi in the revision data,
we calculated P (pi|pj), where pj is each of the pos-
sible original prepositions that were confused with
pi. Then, for each sentence in the well-edited text,
all prepositions are extracted. A preposition is ran-
domly selected (without replacement) and changed
based on the distribution of possible confusions
(note that the original preposition is also included
in the distribution, usually with a high probabil-
3The San Jose Mercury News is available from the Linguis-
tic Data Consortium (catalog number LDC93T3A).
4We used a newer version of the Wikipedia text for the well-
edited text, since we assume that more recent versions of the
text will be most grammatical, and therefore closer to well-
edited.
ity, meaning that there is a strong preference not to
change the preposition). If a preposition is changed
to something other than the original preposition, all
remaining prepositions in the sentence are left un-
changed.
4.3 Naturally Occurring Errors
We have a number of corpora that contain annotated
preposition errors. Note that we are only considering
incorrectly selected prepositions, we do not consider
missing or extraneous.
NUCLE The NUS Corpus of Learner English (NU-
CLE)5 contains one million words of learner
essay text, manually annotated with error tags
and corrections. We use the same training, dev
and test splits as Dahlmeier and Ng (2011).
FCE The CLC FCE Dataset6 is a collection of
1,244 exam scripts written by learners of En-
glish as part of the Cambridge ESOL First Cer-
tificate in English (Yannakoudakis et al, 2011).
It includes demographic metadata about the
candidate, a grade for each essay and manually-
annotated error corrections.
Wikipedia We use three versions of the preposi-
tion errors extracted from the Wikipedia revi-
sions as described in Section 3.2. The first in-
cludes corrections where the preposition was
the only word corrected in the entire sentence
5http://bit.ly/nuclecorpus
6http://ilexir.co.uk/applications/clc-fce-dataset/
510
(clean). The second contains all clean cor-
rections, as well as all corrections where there
were no other edits within a five-word span on
either side of the preposition (?clean). The
third contains all corrections regardless of any
other changes in the surrounding context (all).
Lang-8 The Lang-8 website contains journals writ-
ten by language learners, where native speakers
highlight and correct errors on a sentence-by-
sentence basis. As a result, it contains typical
grammatical mistakes made by language learn-
ers, which can be easily downloaded. We auto-
matically extract 75,622 sentences with prepo-
sition errors and corrections from the first mil-
lion journal entries.7
HOO 2011 We take the test set from the HOO 2011
shared task (Dale and Kilgarriff, 2011) and ex-
tract all examples of preposition selection er-
rors. The texts are fragments of ACL papers
that have been manually annotated for gram-
matical errors.8
It is important to note that the three test sets we use
are from entirely different domains: exam scripts
from non-native English speakers (FCE), essays by
highly proficient college students in Singapore (NU-
CLE) and ACL papers (HOO). In addition, they have
a different number of total prepositions as well as er-
roneous prepositions.
5 Preposition Error Correction
Experiments
We use the preposition error correction model de-
scribed in Tetreault and Chodorow (2008)9 to eval-
uate the many ways of using Wikipedia error cor-
rections as described in the Section 4. We use this
system since it has been recreated for other work
(Dahlmeier and Ng, 2011; Tetreault et al, 2010) and
is similar in methodology to Gamon et al (2008)
7Tajiri et al (2012) extract a corpus of English verb phrases
corrected for tense/aspect errors from Lang-8. They kindly pro-
vided us with their scripts to carry out the scraping of Lang-8.
8The results of the HOO 2011 shared task were not reported
at level of preposition selection error, therefore it is not possible
to compare the results presented in this paper with those results.
9Note that in that work, the model was evaluated in terms of
preposition error detection rather than correction, however the
model itself does not change.
and De Felice and Pulman (2009). In short, the
method models the problem of preposition error cor-
rection (for replacement errors) as a 36-way classifi-
cation problem using a multinomial logistic regres-
sion model.10 The system uses 25 lexical, syntac-
tic and n-gram features derived from the contexts of
each preposition training instance.
We modified the training paradigm of Tetreault
and Chodorow (2008) so that a model could be
trained on examples of correct usage as well as ac-
tual errors. We did this by adding a new feature
specifying the writer?s original preposition (as in
Han et al (2010) and Dahlmeier and Ng (2011)).
5.1 Results
We train a preposition correction system using each
of the three data paradigms and test on the FCE,
NUCLE and HOO 2011 test corpora. For each
preposition in the test corpus, we record whether
the system predicted that it should be changed,
and if so, what it should be changed to. We then
compare the prediction to the annotation in the test
corpus. We report results in terms of f-score, where
precision and recall are calculated as follows:11
Precision = Number of correct preposition correctionsTotal number of corrections suggested
Recall = Number of correct preposition correctionsTotal number of corrections in test set
Note that due to the high volume of unchanged
prepositions in the test corpus, we obtain very high
accuracies, which are not indicative of true perfor-
mance, and are not included in our results.
The results of our experiments are presented in
Table 2.12 The first part of the table shows the f-
scores of preposition error correction systems that
10We use liblinear (Fan et al, 2008) with the L1-regularized
logistic regression solver and default parameters.
11As Chodorow et al (2012) note, it is not clear how to han-
dle cases where the system predicts a preposition that is neither
the same as the writer preposition nor the correct preposition.
We count these cases as false positives.
12No thresholds were used in the systems that were trained
on well-edited text. Traditionally, thresholds are applied so as
to only predict a correction when the system is highly confident.
This has the effect of increasing precision at the cost of recall,
and sometimes leads to an overall improved f-score. Here we
take the prediction of the system, regardless of the confidence,
reflecting a lower-bound of this method.
511
Data Source Paradigm CLC-FCE NUCLE HOO2011
N=33,243 N=9,366 N=1,703
Without
Wikipedia
Revisions
(nonWikiRev)
Wikipedia Snapshot Well-edited Text 24.43? 5.02? 12.36?
Lexile/SJM Well-edited Text 24.73? 4.29? 9.73?
Wikipedia Snapshot Artificial Errors (Lang-8) 42.15? 19.91? 28.75
Lexile/SJM Artificial Errors (Lang-8) 45.36 18.00? 25.15
Lang-8 Error-annotated Text 38.22? 8.18? 24.00
NUCLE train Error-annotated Text 5.38? 20.14 4.82?
With
Wikipedia
Revisions
(WikiRev)
Wikipedia Snapshot Artificial Errors (Wiki) 31.17? 24.52 28.30
Lexile/SJM Artificial Errors (Wiki) 34.35? 23.38 32.76
Wikipedia Revisions All Error-annotated Text 33.59? 26.39 36.84
Wikipedia Revisions ?Clean Error-annotated Text 29.68? 22.13 36.04
Wikipedia Revisions Clean Error-annotated Text 28.09? 21.74 28.30
Table 2: Preposition selection error correction results (f-score). The systems with scores in bold are statistically
significantly better than all systems marked with an asterisk (p < 0.01). Confidence intervals were obtained using
bootstrap resampling with 50,000 replicates.
one might be able to train with publicly available
data excluding the Wikipedia revisions that we have
extracted. We refer to these systems as nonWikiRev
systems. The second part of the table shows the f-
scores of systems trained on the Wikipedia revisions
data ? either directly on the annotated errors or on
the artificial errors produced using the confusion dis-
tributions derived from these annotated errors. We
refer to this second set of systems as WikiRev sys-
tems. The nonWikiRev systems perform inconsis-
tently, heavily dependent on the characteristics of
the test set in question. On the other hand, it is
obvious that the WikiRev systems ? while not al-
ways outperforming the best nonWikiRev systems
? generalize much better across the three test sets.
In fact, for the NUCLE test set, the best WikiRev
system performs as well as the nonWikiRev system
trained on data from the same domain and with iden-
tical error characteristics as the test set. The distri-
butions of errors in the three test sets are not sim-
ilar, and therefore, the stability in performance of
the WikiRev systems cannot be attributed to the hy-
pothesis that the WikiRev training data error distri-
butions are more similar to the test data than any of
the other training corpora. Therefore, we claim that
if a preposition error correction system is to be de-
ployed on data for which the error characteristics are
not known in advance, i.e. most real-world scenar-
ios, training the system using Wikipedia revisions is
likely to be the most robust option.
6 Discussion
We examine the results of our experiments in light
of the research questions we posed in Section 1.
6.1 Which Data Source is More Useful?
We wanted to know whether it was better to have
a smaller corpus of carefully annotated corrections,
or a much larger (but automatically generated, and
therefore noisier) error-annotated corpus. We also
wanted to compare this scenario to training on large
amounts of well-edited text. From our experiments,
it is clear that the composition of the test set plays
a major role in answering this question. On a test
set with few corrections (NUCLE), training on well-
edited text (and without using thresholds) performs
particularly poorly. On the other hand, when eval-
uating on the FCE test set which contains far more
errors, training on well-edited text performs reason-
ably well (though statistically significantly worse
than training on all of the Wikipedia errors). Sim-
ilarly, training on the smaller, high-quality NU-
CLE corpus and evaluating on the NUCLE test set
achieves good results, however training on NUCLE
and testing on FCE achieves the lowest f-score of all
our systems on that test set.
Figure 2 shows the learning curves obtained by
increasing the size of the training data for two
of the test sets.13 Although one might assume
13For space reasons, the graph for HOO2011 is omitted. Also
note that the results in Table 2 may not appear in the graph,
512
Wiki (All)Wiki (Clean)Lang-8NUCLELexile (artificial via Wiki)Lexile (artificial via Lang-8)
F-s
cor
e
0
10
20
30
40
50
log(training data size in thousands of instances)1 2 3 4
Wiki (All)Wiki (Clean)Lang-8NUCLELexile (artificial via Wiki)Lexile (artificial via Lang-8)
F-s
cor
e
0
5
10
15
20
25
1 2 3 4
(a) NUCLE
(b) FCE
Figure 2: The effect of varying the size of the training corpus
that Wikipedia-clean would be more reliable than
Wikipedia-all, the cleanness of the Wikipedia data
seems to make very little difference, probably be-
cause the data extracted in the dirty contexts is not
as noisy as we expected. Interestingly, it also seems
that additional data would lead to further improve-
ments for models trained on artificial errors in Lexile
data and for those trained on all of the automatically
extracted Wikipedia errors.
Another interesting aspect of Figure 2 is that
since we were sampling at specific data points which did not
correspond exactly to the total sizes of the training corpora.
training on the Lang-8 data shows a very steep rising
trend. This suggests that automatically-scraped data
that is highly targeted towards language learners is
very useful in correcting preposition errors in texts
where they are reasonably frequent.
6.2 Natural or Artificially Generated Errors?
Table 2 shows that training on artificially generated
errors via Wikipedia revisions performs fairly con-
sistently across test corpora. While using Lang-8
for artificial error generation is also quite promis-
ing for FCE, it does not generalize across test sets.
513
Wiki (All)Wiki (Clean)Lang-8Lexile (artificial via Wiki)Lexile (artificial via Lang-8)
F-s
cor
e
0
10
20
30
40
50
Percentage of Errors in Training Data0 5 10 15 20 25 30 35 40 45 50 55
Wiki (All)Wiki (Clean)Lang-8Lexile (artificial via Wiki)Lexile (artificial via Lang-8)
F-s
cor
e
5
10
15
20
25
30
0 5 10 15 20 25 30 35 40 45 50 55
(a) NUCLE
(b) FCE
Figure 3: The effect of varying the percentage of errors in the training corpus
On FCE it achieves the highest results, on NUCLE
it performs statistically significantly worse than the
best system, and on HOO 2011 it achieves a lower
(though not statistically significant) result than the
best system. This highlights that extracting errors
from Wikipedia is useful in two ways: (1) training a
system on the errors alone works well and (2) gener-
ating artificial errors in well-edited corpora of differ-
ent domains and training a system on that also works
well. It also indicates that if the system were to be
applied to a specific domain, applying the confusion
distributions to a domain specific corpus ? if avail-
able ? would likely yield the best results.
6.3 Mismatching Distributions
The proportion of errors in the training and test data
plays an important role in the performance of any
preposition error correction system. This is clearly
evident by comparing system performances across
the three test sets which have fairly different compo-
sitions. FCE contains a much higher proportion of
errors than NUCLE, and HOO falls somewhere in
between. Interestingly, the system trained on Lang-
8 data (which contains the highest proportion of er-
514
rors among all training corpora) performs best on
the FCE data. On the other hand, the same sys-
tem performs poorly on NUCLE test which contains
far fewer errors. In this instance, the system learns
to predict an incorrect preposition too often. We
see a similar pattern with the system trained on the
NUCLE training data. It performs poorly on FCE
which contains many errors, but well on NUCLE
test which contains a similar proportion of errors.
In order to better understand the relationship be-
tween the percentage of errors in the training data
and system performance, we vary the percentage of
errors in each training corpus from 1-50% and test
on the unchanged FCE and NUCLE test corpora.
For each training corpus, we reduce the size to be
twice the size of the total number of errors.14 Keep-
ing this size constant, we then artificially change the
percentage of errors. Note that because the total size
of the corpus has changed, the results in Table 2 may
not appear in the graph. Figure 3 shows the effect on
f-score when the data composition is changed. For
both test sets, there is a peak after which increas-
ing the proportion of errors in the training corpus is
detrimental. For NUCLE test with its low number
of preposition errors, this peak is very pronounced.
For FCE, it is more of a gentle degradation in per-
formance, but the pattern is clear. Also noteworthy
is the fact that the degradation for models trained on
artificial errors is less steep suggesting that they may
be more stable across test sets.
In general, these results indicate that when
building a preposition error detection using error-
annotated data, the characteristics of the data to
which the system will be applied should play a vital
role in how the system is to be trained. Our results
show that the WikiRev systems are robust across
test sets, however if the exact distribution of errors
in the data is known in advance, other models may
perform better.
7 Conclusion
Although previous approaches to preposition er-
ror correction using either well-edited text or small
hand-annotated corrections performed well on some
specific test set, they did not generalize well across
14We omit the NUCLE train corpus from this comparison,
because it contains too few errors to obtain a meaningful result.
very different test sets. In this paper, we present
work that automatically extracts preposition error
corrections from Wikipedia Revisions and uses it
to build robust error correction systems. We show
that this data is useful for two purposes. Firstly, a
model trained directly on the corrections performs
well across test sets. Secondly, models trained on ar-
tificial errors generated from the distribution of con-
fusions in the Wikipedia data perform equally well.
The distribution of confusions can also be applied to
other well-edited corpora in different domains, pro-
viding a very powerful method of automatically gen-
erating error corpora. The results of our experiments
also highlight the importance of the distribution of
expected errors in the test set. Models that perform
well on one kind of distribution may not necessar-
ily work on a completely different one, as evident
in the performances of the systems trained on either
Lang-8 or NUCLE. In general, the WikiRev mod-
els perform well across distributions. We also con-
ducted some preliminary system combination exper-
iments and found that while they yielded promising
results, further investigation is necessary. We have
also made the Wikipedia preposition correction cor-
pus available for download.15
In future work, we will examine whether the
results we obtain for English generalize to other
Wikipedia languages. We also plan to extract multi-
word corrections for other types of errors and to ex-
amine the usefulness of including error contexts in
our confusion distributions (e.g., preposition confu-
sions following verbs versus those following nouns).
Acknowledgments
The authors would like to thank Daniel Dahlmeier,
Torsten Zesch, Mamoru Komachi, Tajiri Toshikazu,
Tomoya Mizumoto and Yuji Matsumoto for provid-
ing scripts and data that enabled us to carry out
this research. We would also like to thank Martin
Chodorow and the anonymous reviewers for their
helpful suggestions and comments.
References
Elena Cabrio, Bernardo Magnini, and Angelina Ivanova.
2012. Extracting Context-Rich Entailment Rules from
15http://bit.ly/etsprepdata
515
Wikipedia Revision History. In Proceedings of the 3rd
Workshop on the People?s Web Meets NLP: Collabora-
tively Constructed Semantic Resources and their Ap-
plications to NLP, pages 34?43, Jeju, Republic of Ko-
rea, July. Association for Computational Linguistics.
Martin Chodorow, Markus Dickinson, Ross Israel, and
Joel Tetreault. 2012. Problems in Evaluating Gram-
matical Error Detection Systems. In Proceedings of
COLING 2012, pages 611?628, Mumbai, India, De-
cember. The COLING 2012 Organizing Committee.
Daniel Dahlmeier and Hwee Tou Ng. 2011. Grammat-
ical Error Correction with Alternating Structure Op-
timization. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies, pages 915?923, Port-
land, Oregon, USA, June. Association for Computa-
tional Linguistics.
Robert Dale and Adam Kilgarriff. 2011. Helping Our
Own: The HOO 2011 Pilot Shared Task. In Pro-
ceedings of the Generation Challenges Session at the
13th European Workshop on Natural Language Gener-
ation, pages 242?249, Nancy, France, September. As-
sociation for Computational Linguistics.
Robert Dale, Ilya Anisimoff, and George Narroway.
2012. HOO 2012: A Report on the Preposition
and Determiner Error Correction Shared Task. In
Proceedings of the Seventh Workshop on Building
Educational Applications Using NLP, pages 54?62,
Montre?al, Canada, June. Association for Computa-
tional Linguistics.
Rachele De Felice and Stephen G. Pulman. 2009. Auto-
matic detection of preposition errors in learner writing.
CALICO Journal, 26(3):512?528.
Camille Dutrey, Houda Bouamor, Delphine Bernhard,
and Aure?lien Max. 2011. Local modifications and
paraphrases in Wikipedias revision history. SEPLN
journal(Revista de Procesamiento del Lenguaje Nat-
ural), 46:51?58.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A li-
brary for large linear classification. Journal of Ma-
chine Learning Research, 9:1871?1874.
Oliver Ferschke, Torsten Zesch, and Iryna Gurevych.
2011. Wikipedia Revision Toolkit: Efficiently Access-
ing Wikipedia?s Edit History. In Proceedings of the
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies.
System Demonstrations.
Jennifer Foster and Oistein Andersen. 2009. Gen-
ERRate: Generating Errors for Use in Grammatical
Error Detection. In Proceedings of the Fourth Work-
shop on Innovative Use of NLP for Building Educa-
tional Applications, pages 82?90, Boulder, Colorado,
June. Association for Computational Linguistics.
Michael Gamon, Jianfeng Gao, Chris Brockett, Alex
Klementiev, William B. Dolan, Dmitriy Belenko, and
Lucy Vanderwende. 2008. Using Contextual Speller
Techniques and Language Modeling for ESL Error
Correction. In Proceedings of the International Joint
Conference on Natural Language Processing (IJC-
NLP), pages 449?456, Hyderabad, India.
Michael Gamon. 2010. Using Mostly Native Data to
Correct Errors in Learners? Writing. In Human Lan-
guage Technologies: The 2010 Annual Conference of
the North American Chapter of the Association for
Computational Linguistics, pages 163?171, Los An-
geles, California, June. Association for Computational
Linguistics.
Na-Rae Han, Martin Chodorow, and Claudia Leacock.
2006. Detecting errors in English article usage by
non-native speakers. Natural Language Engineering,
12(2):115?129.
Na-Rae Han, Joel Tetreault, Soo-Hwa Lee, and Jin-
Young Ha. 2010. Using Error-Annotated ESL Data
to Develop an ESL Error Correction System. In Pro-
ceedings of the Seventh International Conference on
Language Resources and Evaluation (LREC), Malta.
Kenji Imamura, Kuniko Saito, Kugatsu Sadamitsu, and
Hitoshi Nishikawa. 2012. Grammar Error Correc-
tion Using Pseudo-Error Sentences and Domain Adap-
tation. In Proceedings of the 50th Annual Meeting
of the Association for Computational Linguistics (Vol-
ume 2: Short Papers), pages 388?392, Jeju Island, Ko-
rea, July. Association for Computational Linguistics.
Emi Izumi, Kiyotaka Uchimoto, Toyomi Saiga, Thepchai
Supnithi, and Hitoshi Isahara. 2003. Automatic Error
Detection in the Japanese Learners? English Spoken
Data. In The Companion Volume to the Proceedings
of 41st Annual Meeting of the Association for Compu-
tational Linguistics, pages 145?148, Sapporo, Japan,
July. Association for Computational Linguistics.
Claudia Leacock, Martin Chodorow, Michael Gamon,
and Joel Tetreault. 2010. Automated Grammatical
Error Detection for Language Learners. Synthesis
Lectures on Human Language Technologies. Morgan
Claypool.
Aure?lien Max and Guillaume Wisniewski. 2010. Mining
Naturally-occurring Corrections and Paraphrases from
Wikipedia?s Revision History. In Nicoletta Calzo-
lari (Conference Chair), Khalid Choukri, Bente Mae-
gaard, Joseph Mariani, Jan Odijk, Stelios Piperidis,
Mike Rosner, and Daniel Tapias, editors, Proceed-
ings of the Seventh conference on International Lan-
guage Resources and Evaluation (LREC?10), Valletta,
Malta, may. European Language Resources Associa-
tion (ELRA).
Rami Nelken and Elif Yamangil. 2008. Mining
Wikipedias Article Revision History for Training
516
Computational Linguistics Algorithms. In Proceed-
ings of the 1st AAAI Workshop on Wikipedia and Arti-
ficial Intelligence, pages 31?36, Chicago, IL.
Alla Rozovskaya and Dan Roth. 2010. Generating Con-
fusion Sets for Context-Sensitive Error Correction.
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, pages 961?
970, Cambridge, MA, October. Association for Com-
putational Linguistics.
Toshikazu Tajiri, Mamoru Komachi, and Yuji Mat-
sumoto. 2012. Tense and Aspect Error Correction
for ESL Learners Using Global Context. In Proceed-
ings of the 50th Annual Meeting of the Association for
Computational Linguistics (ACL), Short Papers, pages
198?202, Jeju Island, Korea.
Joel R. Tetreault and Martin Chodorow. 2008. The
Ups and Downs of Preposition Error Detection in
ESL Writing. In Proceedings of the 22nd Interna-
tional Conference on Computational Linguistics (Col-
ing 2008), pages 865?872, Manchester, UK.
Joel Tetreault, Jennifer Foster, and Martin Chodorow.
2010. Using Parse Features for Preposition Selection
and Error Detection. In Proceedings of the ACL 2010
Conference Short Papers, pages 353?358, Uppsala,
Sweden, July. Association for Computational Linguis-
tics.
Kristina Toutanova, Dan Klein, Christopher D. Manning,
and Yoram Singer. 2003. Feature-rich Part-of-speech
Tagging with a Cyclic Dependency Network. In Pro-
ceedings of NAACL, pages 173?180.
Elif Yamangil and Rani Nelken. 2008. Mining
Wikipedia Revision Histories for Improving Sentence
Compression. In Proceedings of ACL-08: HLT, Short
Papers, pages 137?140, Columbus, Ohio, June. Asso-
ciation for Computational Linguistics.
Helen Yannakoudakis, Ted Briscoe, and Ben Medlock.
2011. A New Dataset and Method for Automati-
cally Grading ESOL Texts. In Proceedings of the
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies,
pages 180?189, Portland, Oregon, USA, June. Associ-
ation for Computational Linguistics.
Mark Yatskar, Bo Pang, Cristian Danescu-Niculescu-
Mizil, and Lillian Lee. 2010. For the sake of simplic-
ity: Unsupervised extraction of lexical simplifications
from Wikipedia. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Linguis-
tics, pages 365?368, Los Angeles, California, June.
Association for Computational Linguistics.
Fabio Massimo Zanzotto and Marco Pennacchiotti.
2010. Expanding textual entailment corpora from
Wikipedia using co-training. In Proceedings of the
2nd Workshop on The People?s Web Meets NLP: Col-
laboratively Constructed Semantic Resources, pages
28?36, Beijing, China, August. Coling 2010 Organiz-
ing Committee.
Torsten Zesch. 2012. Measuring Contextual Fitness Us-
ing Error Contexts Extracted from the Wikipedia Revi-
sion History. In Proceedings of the 13th Conference of
the European Chapter of the Association for Compu-
tational Linguistics, pages 529?538, Avignon, France,
April. Association for Computational Linguistics.
517
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1007?1017,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Underspecifying and Predicting Voice for Surface Realisation Ranking
Sina Zarrie?, Aoife Cahill and Jonas Kuhn
Institut fu?r maschinelle Sprachverarbeitung
Universita?t Stuttgart, Germany
{sina.zarriess,aoife.cahill,jonas.kuhn}@ims.uni-stuttgart.de
Abstract
This paper addresses a data-driven surface
realisation model based on a large-scale re-
versible grammar of German. We investigate
the relationship between the surface realisa-
tion performance and the character of the in-
put to generation, i.e. its degree of underspec-
ification. We extend a syntactic surface reali-
sation system, which can be trained to choose
among word order variants, such that the can-
didate set includes active and passive variants.
This allows us to study the interaction of voice
and word order alternations in realistic Ger-
man corpus data. We show that with an ap-
propriately underspecified input, a linguisti-
cally informed realisation model trained to re-
generate strings from the underlying semantic
representation achieves 91.5% accuracy (over
a baseline of 82.5%) in the prediction of the
original voice.
1 Introduction
This paper1 presents work on modelling the usage
of voice and word order alternations in a free word
order language. Given a set of meaning-equivalent
candidate sentences, such as in the simplified En-
glish Example (1), our model makes predictions
about which candidate sentence is most appropriate
or natural given the context.
(1) Context: The Parliament started the debate about the state
budget in April.
a. It wasn?t until June that the Parliament approved it.
b. It wasn?t until June that it was approved by the Parliament.
c. It wasn?t until June that it was approved.
We address the problem of predicting the usage of
linguistic alternations in the framework of a surface
1This work has been supported by the Deutsche Forschungs-
gemeinschaft (DFG; German Research Foundation) in SFB 732
Incremental specification in context, project D2 (PIs: Jonas
Kuhn and Christian Rohrer).
realisation ranking system. Such ranking systems
are practically relevant for the real-world applica-
tion of grammar-based generators that usually gen-
erate several grammatical surface sentences from a
given abstract input, e.g. (Velldal and Oepen, 2006).
Moreover, this framework allows for detailed exper-
imental studies of the interaction of specific linguis-
tic features. Thus it has been demonstrated that for
free word order languages like German, word or-
der prediction quality can be improved with care-
fully designed, linguistically informed models cap-
turing information-structural strategies (Filippova
and Strube, 2007; Cahill and Riester, 2009).
This paper is situated in the same framework, us-
ing rich linguistic representations over corpus data
for machine learning of realisation ranking. How-
ever, we go beyond the task of finding the correct or-
dering for an almost fixed set of word forms. Quite
obviously, word order is only one of the means at
a speaker?s disposal for expressing some content in
a contextually appropriate form; we add systematic
alternations like the voice alternation (active vs. pas-
sive) to the picture. As an alternative way of pro-
moting or demoting the prominence of a syntactic
argument, its interaction with word ordering strate-
gies in real corpus data is of high theoretical interest
(Aissen, 1999; Aissen, 2003; Bresnan et al, 2001).
Our main goals are (i) to establish a corpus-based
surface realisation framework for empirically inves-
tigating interactions of voice and word order in Ger-
man, (ii) to design an input representation for gen-
eration capturing voice alternations in a variety of
contexts, (iii) to better understand the relationship
between the performance of a generation ranking
model and the type of realisation candidates avail-
able in its input. In working towards these goals,
this paper addresses the question of evaluation. We
conduct a pilot human evaluation on the voice al-
1007
ternation data and relate our findings to our results
established in the automatic ranking experiments.
Addressing interactions among a range of gram-
matical and discourse phenomena on realistic corpus
data turns out to be a major methodological chal-
lenge for data-driven surface realisation. The set of
candidate realisations available for ranking will in-
fluence the findings, and here, existing surface re-
alisers vary considerably. Belz et al (2010) point
out the differences across approaches in the type of
syntactic and semantic information present and ab-
sent in the input representation; and it is the type of
underspecification that determines the number (and
character) of available candidate realisations and,
hence, the complexity of the realisation task.
We study the effect of varying degrees of under-
specification explicitly, extending a syntactic gen-
eration system by a semantic component capturing
voice alternations. In regeneration studies involving
underspecified underlying representations, corpus-
oriented work reveals an additional methodological
challenge. When using standard semantic represen-
tations, as common in broad-coverage work in se-
mantic parsing (i.e., from the point of view of analy-
sis), alternative variants for sentence realisation will
often receive slightly different representations: In
the context of (1), the continuation (1-c) is presum-
ably more natural than (1-b), but with a standard
sentence-bounded semantic analysis, only (1-a) and
(1-b) would receive equivalent representations.
Rather than waiting for the availability of robust
and reliable techniques for detecting the reference of
implicit arguments in analysis (or for contextually
aware reasoning components), we adopt a relatively
simple heuristic approach (see Section 3.1) that ap-
proximates the desired equivalences by augmented
representations for examples like (1-c). This way
we can overcome an extremely skewed distribution
in the naturally occurring meaning-equivalent active
vs. passive sentences, a factor which we believe jus-
tifies taking the risk of occasional overgeneration.
The paper is structured as follows: Section 2 situ-
ates our methodology with respect to other work on
surface realisation and briefly summarises the rele-
vant theoretical linguistic background. In Section 3,
we present our generation architecture and the de-
sign of the input representation. Section 4 describes
the setup for the experiments in Section 5. In Section
6, we present the results from the human evaluation.
2 Related Work
2.1 Generation Background
The first widely known data-driven approach to
surface realisation, or tactical generation, (Langk-
ilde and Knight, 1998) used language-model n-
gram statistics on a word lattice of candidate re-
alisations to guide a ranker. Subsequent work ex-
plored ways of exploiting linguistically annotated
data for trainable generation models (Ratnaparkhi,
2000; Marciniak and Strube, 2005; Belz, 2005, a.o.).
Work on data-driven approaches has led to insights
into the importance of linguistic features for sen-
tence linearisation decisions (Ringger et al, 2004;
Filippova and Strube, 2009). The availability of dis-
criminative learning techniques for the ranking of
candidate analyses output by broad-coverage gram-
mars with rich linguistic representations, originally
in parsing (Riezler et al, 2000; Riezler et al, 2002),
has also led to a revival of interest in linguistically
sophisticated reversible grammars as the basis for
surface realisation (Velldal and Oepen, 2006; Cahill
et al, 2007). The grammar generates candidate
analyses for an underlying representation and the
ranker?s task is to predict the contextually appropri-
ate realisation.
The work that is most closely related to ours is
Velldal (2008). He uses an MRS representation
derived by an HPSG grammar that can be under-
specified for information status. In his case, the
underspecification is encoded in the grammar and
not directly controlled. In multilingually oriented
linearisation work, Bohnet et al (2010) generate
from semantic corpus annotations included in the
CoNLL?09 shared task data. However, they note that
these annotations are not suitable for full generation
since they are often incomplete. Thus, it is not clear
to which degree these annotations are actually un-
derspecified for certain paraphrases.
2.2 Linguistic Background
In competition-based linguistic theories (Optimal-
ity Theory and related frameworks), the use of
argument alternations is construed as an effect
of markedness hierarchies (Aissen, 1999; Aissen,
2003). Argument functions (subject, object, . . . ) on
1008
the one hand and the various properties that argu-
ment phrases can bear (person, animacy, definite-
ness) on the other are organised in markedness hi-
erarchies. Wherever possible, there is a tendency to
align the hierarchies, i.e., use prominent functions to
realise prominently marked argument phrases. For
instance, Bresnan et al (2001) find that there is a sta-
tistical tendency in English to passivise a verb if the
patient is higher on the person scale than the agent,
but an active is grammatically possible.
Bresnan et al (2007) correlate the use of the En-
glish dative alternation to a number of features such
as givenness, pronominalisation, definiteness, con-
stituent length, animacy of the involved verb argu-
ments. These features are assumed to reflect the dis-
course acessibility of the arguments.
Interestingly, the properties that have been used
to model argument alternations in strict word or-
der languages like English have been identified as
factors that influence word order in free word or-
der languages like German, see Filippova and Strube
(2007) for a number of pointers. Cahill and Riester
(2009) implement a model for German word or-
der variation that approximates the information sta-
tus of constituents through morphological features
like definiteness, pronominalisation etc. We are not
aware of any corpus-based generation studies inves-
tigating how these properties relate to argument al-
ternations in free word order languages.
3 Generation Architecture
Our data-driven methodology for investigating fac-
tors relevant to surface realisation uses a regen-
eration set-up2 with two main components: a) a
grammar-based component used to parse a corpus
sentence and map it to all its meaning-equivalent
surface realisations, b) a statistical ranking compo-
nent used to select the correct, i.e. contextually most
appropriate surface realisation. Two variants of this
set-up that we use are sketched in Figure 1.
We generally use a hand-crafted, broad-coverage
LFG for German (Rohrer and Forst, 2006) to parse
a corpus sentence into a f(unctional) structure3
and generate all surface realisations from a given
2Compare the bidirectional competition set-up in some
Optimality-Theoretic work, e.g., (Kuhn, 2003).
3The choice among alternative f-structures is done with a
discriminative model (Forst, 2007).
Sntx
SVM Ranker
Snta1 Snta2 ... Sntam
LFG grammar
FSa
LFG grammar
Snti
Snty
SVM Ranker
Sntb1 Snta1 Snta2 ... Sntbn
LFG Grammar
FSa FSb
Reverse Sem. Rules
SEM
Sem. Rules
FS1
LFG Grammar
Snti
Figure 1: Generation pipelines
f-structure, following the generation approach of
Cahill et al (2007). F-structures are attribute-
value matrices representing grammatical functions
and morphosyntactic features; their theoretical mo-
tivation lies in the abstraction over details of sur-
face realisation. The grammar is implemented in the
XLE framework (Crouch et al, 2006), which allows
for reversible use of the same declarative grammar
in the parsing and generation direction.
To obtain a more abstract underlying representa-
tion (in the pipeline on the right-hand side of Fig-
ure 1), the present work uses an additional seman-
tic construction component (Crouch and King, 2006;
Zarrie?, 2009) to map LFG f-structures to meaning
representations. For the reverse direction, the mean-
ing representations are mapped to f-structures which
can then be mapped to surface strings by the XLE
generator (Zarrie? and Kuhn, 2010).
For the final realisation ranking step in both
pipelines, we used SVMrank, a Support Vector
Machine-based learning tool (Joachims, 1996). The
ranking step is thus technically independent from the
LFG-based component. However, the grammar is
used to produce the training data, pairs of corpus
sentences and the possible alternations.
The two pipelines allow us to vary the degree to
which the generation input is underspecified. An f-
structure abstracts away from word order, i.e. the
candidate set will contain just word order alterna-
tions. In the semantic input, syntactic function and
voice are underspecified, so a larger set of surface
realisation candidates is generated. Figure 2 illus-
trates the two representation levels for an active and
1009
a passive sentence. The subject of the passive and
the object of the active f-structure are mapped to the
same role (patient) in the meaning representation.
3.1 Issues with ?naive? underspecification
In order to create an underspecified voice represen-
tation that does indeed leave open the realisation op-
tions available to the speaker/writer, it is often not
sufficient to remove just the syntactic function in-
formation. For instance, the subject of the active
sentence (2) is an arbitrary reference pronoun man
?one? which cannot be used as an oblique agent in
a passive, sentence (2-b) is ungrammatical.
(2) a. Man
One
hat
has
den
the
Kanzler
chancellor
gesehen.
seen.
b. *Der
The
Kanzler
chancellor
wurde
was
von
by
man
one
gesehen.
seen.
So, when combined with the grammar, the mean-
ing representation for (2) in Figure 2 contains im-
plicit information about the voice of the original cor-
pus sentence; the candidate set will not include any
passive realisations. However, a passive realisation
without the oblique agent in the by-phrase, as in Ex-
ample (3), is a very natural variant.
(3) Der
The
Kanzler
chancellor
wurde
was
gesehen.
seen.
The reverse situation arises frequently too: pas-
sive sentences where the agent role is not overtly
realised. Given the standard, ?analysis-oriented?
meaning representation for Sentence (4) in Figure
2, the realiser will not generate an active realisation
since the agent role cannot be instantiated by any
phrase in the grammar. However, depending on the
exact context there are typically options for realis-
ing the subject phrase in an active with very little
descriptive content.
Ideally, one would like to account for these phe-
nomena in a meaning representation that under-
specifies the lexicalisation of discourse referents,
and also captures the reference of implicit argu-
ments. Especially the latter task has hardly been
addressed in NLP applications (but see Gerber and
Chai (2010)). In order to work around that problem,
we implemented some simple heuristics which un-
derspecify the realisation of certain verb arguments.
These rules define: 1. a set of pronouns (generic and
neutral pronouns, universal quantifiers) that corre-
spond to ?trivial? agents in active and implicit agents
Active Passive
2-role trans. 71% (82%) 10% (2%)
1-role trans. 11% (0%) 8% (16%)
Table 1: Distribution of voices in SEMh (SEMn)
in passive sentences; 2. a set of prepositional ad-
juncts in passive sentences that correspond to sub-
jects in active sentence (e.g. causative and instru-
mental prepositions like durch ?by means of?); 3.
certain syntactic contexts where special underspec-
ification devices are needed, e.g. coordinations or
embeddings, see Zarrie? and Kuhn (2010) for ex-
amples. In the following, we will distinguish 1-role
transitives where the agent is ?trivial? or implicit
from 2-role transitives with a non-implicit agent.
By means of the extended underspecification rules
for voice, the sentences in (2) and (3) receive an
identical meaning representation. As a result, our
surface realiser can produce an active alternation for
(3) and a passive alternation for (2). In the follow-
ing, we will refer to the extended representations as
SEMh (?heuristic semantics?), and to the original
representations as SEMn (?naive semantics?).
We are aware of the fact that these approximations
introduce some noise into the data and do not always
represent the underlying referents correctly. For in-
stance, the implicit agent in a passive need not be
?trivial? but can correspond to an actual discourse
referent. However, we consider these heuristics as
a first step towards capturing an important discourse
function of the passive alternation, namely the dele-
tion of the agent role. If we did not treat the passives
with an implicit agent on a par with certain actives,
we would have to ignore a major portion of the pas-
sives occurring in corpus data.
Table 1 summarises the distribution of the voices
for the heuristic meaning representation SEMh on
the data-set we will introduce in Section 4, with
the distribution for the naive representation SEMn
in parentheses.
4 Experimental Set-up
Data To obtain a sizable set of realistic corpus ex-
amples for our experiments on voice alternations, we
created our own dataset of input sentences and rep-
resentations, instead of building on treebank exam-
ples as Cahill et al (2007) do. We extracted 19,905
sentences, all containing at least one transitive verb,
1010
f-structure
Example (2)
2
6
6
6
6
4
PRED ?see < (? SUBJ)(? OBJ) >?
SUBJ
?
PRED ?one?
?
OBJ
?
PRED ?chancellor?
?
TOPIC
?
?one?
?
PASS ?
3
7
7
7
7
5
f-structure
Example (3)
2
6
6
4
PRED ?see < NULL (? SUBJ) >?
SUBJ
?
PRED ?chancellor?
?
TOPIC
?
?chancellor?
?
PASS +
3
7
7
5
semantics
Example (2)
HEAD (see)
PAST (see)
ROLE (agent,see,one)
ROLE (patient,see,chancellor)
semantics
Example (3)
HEAD (see)
PAST (see)
ROLE (agent,see,implicit)
ROLE (patient,see,chancellor)
Figure 2: F-structure pair for passive-active alternation
from the HGC, a huge German corpus of newspa-
per text (204.5 million tokens). The sentences are
automatically parsed with the German LFG gram-
mar. The resulting f-structure parses are transferred
to meaning representations and mapped back to f-
structure charts. For our generation experiments,
we only use those f-structure charts that the XLE
generator can map back to a set of surface realisa-
tions. This results in a total of 1236 test sentences
and 8044 sentences in our training set. The data loss
is mostly due to the fact the XLE generator often
fails on incomplete parses, and on very long sen-
tences. Nevertheless, the average sentence length
(17.28) and number of surface realisations (see Ta-
ble 2) are higher than in Cahill et al (2007).
Labelling For the training of our ranking model,
we have to tell the learner how closely each surface
realisation candidate resembles the original corpus
sentence. We distinguish the rank categories: ?1?
identical to the corpus string, ?2? identical to the
corpus string ignoring punctuation, ?3? small edit
distance (< 4) to the corpus string ignoring punc-
tuation, ?4? different from the corpus sentence. In
one of our experiments (Section 5.1), we used the
rank category ?5? to explicitly label the surface real-
isations derived from the alternation f-structure that
does not correspond to the parse of the original cor-
pus sentence. The intermediate rank categories ?2?
and ?3? are useful since the grammar does not al-
ways regenerate the exact corpus string, see Cahill
et al (2007) for explanation.
Features The linguistic theories sketched in Sec-
tion 2.2 correlate morphological, syntactic and se-
mantic properties of constituents (or discourse ref-
erents) with their order and argument realisation. In
our system, this correlation is modelled by a combi-
nation of linguistic properties that can be extracted
from the f-structure or meaning representation and
of the surface order that is read off the sentence
string. Standard n-gram features are also used as
features.4 The feature model is built as follows:
for every lemma in the f-structure, we extract a set
of morphological properties (definiteness, person,
pronominal status etc.), the voice of the verbal head,
its syntactic and semantic role, and a set of infor-
mations status features following Cahill and Riester
(2009). These properties are combined in two ways:
a) Precedence features: relative order of properties
in the surface string, e.g. ?theme < agent in pas-
sive?, ?1st person < 3rd person?; b) ?scale align-
ment? features (ScalAl.): combinations of voice and
role properties with morphological properties, e.g.
?subject is singular?, ?agent is 3rd person in active
voice? (these are surface-independent, identical for
each alternation candidate).
The model for which we present our results is
based on sentence-internal features only; as Cahill
and Riester (2009) showed, these feature carry a
considerable amount of implicit information about
the discourse context (e.g. in the shape of referring
expressions). We also implemented a set of explic-
itly inter-sentential features, inspired by Centering
Theory (Grosz et al, 1995). This model did not im-
prove over the intra-sentential model.
Evaluation Measures In order to assess the gen-
eral quality of our generation ranking models, we
4The language model is trained on the German data release
for the 2009 ACL Workshop on Machine Translation shared
task, 11,991,277 total sentences.
1011
FS SEMn SEMh
Avg. # strings 36.7 68.2 75.8
Random Match 16.98 10.72 7.28
LM
Match 15.45 15.04 11.89
BLEU 0.68 0.68 0.65
NIST 13.01 12.95 12.69
Ling. Model
Match 27.91 27.66 26.38
BLEU 0.764 0.759 0.747
NIST 13.18 13.14 13.01
Table 2: Evaluation of Experiment 1
use several standard measures: a) exact match:
how often does the model select the original cor-
pus sentence, b) BLEU: n-gram overlap between
top-ranked and original sentence, c) NIST: modifi-
cation of BLEU giving more weight to less frequent
n-grams. Second, we are interested in the model?s
performance wrt. specific linguistic criteria. We re-
port the following accuracies: d) Voice: how often
does the model select a sentence realising the correct
voice, e) Precedence: how often does the model gen-
erate the right order of the verb arguments (agent and
patient), and f) Vorfeld: how often does the model
correctly predict the verb arguments to appear in the
sentence initial position before the finite verb, the
so-called Vorfeld. See Sections 5.3 and 6 for a dis-
cussion of these measures.
5 Experiments
5.1 Exp. 1: Effect of Underspecified Input
We investigate the effect of the input?s underspecifi-
cation on a state-of-the-art surface realisation rank-
ing model. This model implements the entire fea-
ture set described in Section 4 (it is further analysed
in the subsequent experiments). We built 3 datasets
from our alternation data: FS - candidates generated
from the f-structure; SEMn - realisations from the
naive meaning representations; SEMh - candidates
from the heuristically underspecified meaning rep-
resentation. Thus, we keep the set of original cor-
pus sentences (=the target realisations) constant, but
train and test the model on different candidate sets.
In Table 2, we compare the performance of the
linguistically informed model described in Section 4
on the candidates sets against a random choice and
a language model (LM) baseline. The differences in
BLEU between the candidate sets and models are
FS SEMn SEMh SEMn?
A
ll
Tr
an
s. Voice Acc. 100 98.06 91.05 97.59
Voice Spec. 100 22.8 0 0
Majority BL 82.4 98.1
2-
ro
le
Tr
an
s. Voice Acc. 100 97.7 91.8 97.59
Voice Spec. 100 8.33 0 0
Majority BL 88.5 98.1
1-
ro
le
Tr
an
s. Voice Acc. 100 100 90.0 -
Voice Spec. 100 100 0 -
Majority BL 53.9 -
Table 3: Accuracy of Voice Prediction by Ling. Model in
Experiment 1
statistically significant.5 In general, the linguistic
model largely outperforms the LM and is less sen-
sitive to the additional confusion introduced by the
SEMh input. Its BLEU score and match accuracy
decrease only slightly (though statistically signifi-
cantly).
In Table 3, we report the performance of the lin-
guistic model on the different candidate sets with re-
spect to voice accuracy. Since the candidate sets dif-
fer in the proportion of items that underspecify the
voice (see ?Voice Spec.? in Table 3), we also report
the accuracy on the SEMn? test set, which is a sub-
set of SEMn excluding the items where the voice is
specified. Table 3 shows that the proportion of active
realisations for the SEMn? input is very high, and
the model does not outperform the majority baseline
(which always selects active). In contrast, the SEMh
model clearly outperforms the majority baseline.
Example (4) is a case from our development set
where the SEMn model incorrectly predicts an ac-
tive (4-a), and the SEMh correctly predicts a passive
(4-b).
(4) a. 26
26
kostspielige
expensive
Studien
studies
erwa?hnten
mentioned
die
the
Finanzierung.
funding.
b. Die
The
Finanzierung
funding
wurde
was
von
by
26
26
kostspieligen
expensive
Studien
studies
erwa?hnt.
mentioned.
This prediction is according to the markedness hier-
archy: the patient is singular and definite, the agent
5According to a bootstrap resampling test, p < 0.05
1012
Features Match BLEU Voice Prec. VF
Prec. 16.3 0.70 88.43 64.1 59.1
ScalAl. 10.4 0.64 90.37 58.9 56.3
Union 26.4 0.75 91.50 80.2 70.9
Table 4: Evaluation of Experiment 2
is plural and indefinite. Counterexamples are possi-
ble, but there is a clear statistical preference ? which
the model was able to pick up.
On the one hand, the rankers can cope surpris-
ingly well with the additional realisations obtained
from the meaning representations. According to the
global sentence overlap measures, their quality is
not seriously impaired. On the other hand, the de-
sign of the representations has a substantial effect
on the prediction of the alternations. The SEMn
does not seem to learn certain preferences because
of the extremely imbalanced distribution in the in-
put data. This confirms the hypothesis sketched in
Section 3.1, according to which the degree of the
input?s underspecification can crucially change the
behaviour of the ranking model.
5.2 Exp. 2: Word Order and Voice
We examine the impact of certain feature types on
the prediction of the variation types in our data. We
are particularly interested in the interaction of voice
and word order (precedence) since linguistic theo-
ries (see Section 2.2) predict similar information-
structural factors guiding their use, but usually do
not consider them in conjunction.
In Table 4, we report the performance of ranking
models trained on the different feature subsets intro-
duced in Section 4. The union of the features corre-
sponds to the model trained on SEMh in Experiment
1. At a very broad level, the results suggest that the
precedence and the scale alignment features interact
both in the prediction of voice and word order.
The most pronounced effect on voice accuracy
can be seen when comparing the precedence model
to the union model. Adding the surface-independent
scale alignment features to the precedence features
leads to a big improvement in the prediction of word
order. This is not a trivial observation since a) the
surface-independent features do not discriminate be-
tween the word orders and b) the precedence fea-
tures are built from the same properties (see Sec-
tion 4). Thus, the SVM learner discovers depen-
dencies between relative precedence preferences and
abstract properties of a verb argument which cannot
be encoded in the precedence alone.
It is worth noting that the precedence features im-
prove the voice prediction. This indicates that wher-
ever the application context allows it, voice should
not be specified at a stage prior to word order. Ex-
ample (5) is taken from our development set, illus-
trating a case where the union model predicted the
correct voice and word order (5-a), and the scale
alignment model top-ranked the incorrect voice and
word order. The active verb arguments in (5-b) are
both case-ambigous and placed in the non-canonical
order (object < subject), so the semantic relation can
be easily misunderstood. The passive in (5-a) is un-
ambiguous since the agent is realised in a PP (and
placed in the Vorfeld).
(5) a. Von
By
den
the
deutschen
German
Medien
media
wurden
were
die
the
Ausla?nder
foreigners
nur
only
erwa?hnt,
mentioned,
wenn
when
es
there
Zoff
trouble
gab.
was.
b. Wenn
When
es
there
Zoff
trouble
gab,
was,
erwa?hnten
mentioned
die
the
Ausla?nder
foreigners
nur
only
die
the
deutschen
German
Medien.
media.
Moreover, our results confirm Filippova and
Strube (2007) who find that it is harder to predict
the correct Vorfeld occupant in a German sentence,
than to predict the relative order of the constituents.
5.3 Exp. 3: Capturing Flexible Variation
The previous experiment has shown that there is a
certain inter-dependence between word order and
voice. This experiment addresses this interaction
by varying the way the training data for the ranker
is labelled. We contrast two ways of labelling the
sentences (see Section 4): a) all sentences that are
not (nearly) identical to the reference sentence have
the rank category ?4?, irrespective of their voice (re-
ferred to as unlabelled model), b) the sentences that
do not realise the correct voice are ranked lower than
sentences with the correct voice (?4? vs. ?5?), re-
ferred to as labelled model. Intuitively, the latter
way of labelling tells the ranker that all sentences
in the incorrect voice are worse than all sentences
in the correct voice, independent of the word order.
Given the first labelling strategy, the ranker can de-
cide in an unsupervised way which combinations of
word order and voice are to be preferred.
1013
Top 1 Top 1 Top 1 Top 2 Top 3
Model Match BLEU NIST Voice Prec. Prec.+Voice Prec.+Voice Prec.+Voice
Labelled, no LM 21.52 0.73 12.93 91.9 76.25 71.01 78.35 82.31
Unlabelled, no LM 26.83 0.75 13.01 91.5 80.19 74.51 84.28 88.59
Unlabeled + LM 27.35 0.75 13.08 91.5 79.6 73.92 79.74 82.89
Table 5: Evaluation of Experiment 3
In Table 5, it can be seen that the unlabelled model
improves over the labelled on all the sentence over-
lap measures. The improvements are statistically
significant. Moreover, we compare the n-best ac-
curacies achieved by the models for the joint pre-
diction of voice and argument order. The unla-
belled model is very flexible with respect to the word
order-voice interaction: the accuracy dramatically
improves when looking at the top 3 sentences. Ta-
ble 5 also reports the performance of an unlabelled
model that additionally integrates LM scores. Sur-
prisingly, these scores have a very small positive ef-
fect on the sentence overlap features and no positive
effect on the voice and precedence accuracy. The
n-best evaluations even suggest that the LM scores
negatively impact the ranker: the accuracy for the
top 3 sentences increases much less as compared to
the model that does not integrate LM scores.6
The n-best performance of a realisation ranker is
practically relevant for re-ranking applications such
as Velldal (2008). We think that it is also concep-
tually interesting. Previous evaluation studies sug-
gest that the original corpus sentence is not always
the only optimal realisation of a given linguistic in-
put (Cahill and Forst, 2010; Belz and Kow, 2010).
Humans seem to have varying preferences for word
order contrasts in certain contexts. The n-best evalu-
ation could reflect the behaviour of a ranking model
with respect to the range of variations encountered
in real discourse. The pilot human evaluation in the
next Section deals with this question.
6 Human Evaluation
Our experiment in Section 5.3 has shown that the ac-
curacy of our linguistically informed ranking model
dramatically increases when we consider the three
6(Nakanishi et al, 2005) also note a negative effect of in-
cluding LM scores in their model, pointing out that the LM was
not trained on enough data. The corpus used for training our
LM might also have been too small or distinct in genre.
best sentences rather than only the top-ranked sen-
tence. This means that the model sometimes predicts
almost equal naturalness for different voice realisa-
tions. Moreover, in the case of word order, we know
from previous evaluation studies, that humans some-
times prefer different realisations than the original
corpus sentences. This Section investigates agree-
ment in human judgements of voice realisation.
Whereas previous studies in generation mainly
used human evaluation to compare different sys-
tems, or to correlate human and automatic evalua-
tions, our primary interest is the agreement or cor-
relation between human rankings. In particular, we
explore the hypothesis that this agreement is higher
in certain contexts than in others. In order to select
these contexts, we use the predictions made by our
ranking model.
The questionnaire for our experiment comprised
24 items falling into 3 classes: a) items where the
3 best sentences predicted by the model have the
same voice as the original sentence (?Correct?), b)
items where the 3 top-ranked sentences realise dif-
ferent voices (?Mixed?), c) items where the model
predicted the incorrect voice in all 3 top sentences
(?False?). Each item is composed of the original
sentence, the 3 top-ranked sentences (if not identical
to the corpus sentence) and 2 further sentences such
that each item contains different voices. For each
item, we presented the previous context sentence.
The experiment was completed by 8 participants,
all native speakers of German, 5 had a linguistic
background. The participants were asked to rank
each sentence on a scale from 1-6 according to its
naturalness and plausibility in the given context. The
participants were explicitly allowed to use the same
rank for sentences they find equally natural. The par-
ticipants made heavy use of this option: out of the
192 annotated items, only 8 are ranked such that no
two sentences have the same rank.
We compare the human judgements by correlat-
1014
ing them with Spearman?s ?. This measure is con-
sidered appropriate for graded annotation tasks in
general (Erk and McCarthy, 2009), and has also
been used for analysing human realisation rankings
(Velldal, 2008; Cahill and Forst, 2010). We nor-
malise the ranks according to the procedure in Vell-
dal (2008). In Table 6, we report the correlations
obtained from averaging over all pairwise correla-
tions between the participants and the correlations
restricted to the item and sentence classes. We used
bootstrap re-sampling on the pairwise correlations to
test that the correlations on the different item classes
significantly differ from each other.
The correlations in Table 6 suggest that the agree-
ment between annotators is highest on the false
items, and lowest on the mixed items. Humans
tended to give the best rank to the original sentence
more often on the false items (91%) than on the oth-
ers. Moreover, the agreement is generally higher on
the sentences realising the correct voice.
These results seem to confirm our hypothesis that
the general level of agreement between humans dif-
fers depending on the context. However, one has to
be careful in relating the effects in our data solely to
voice preferences. Since the sentences were chosen
automatically, some examples contain very unnatu-
ral word orders that probably guided the annotators?
decisions more than the voice. This is illustrated
by Example (6) showing two passive sentences from
our questionnaire which differ only in the position of
the adverb besser ?better?. Sentence (6-a) is com-
pletely implausible for a native speaker of German,
whereas Sentence (6-b) sounds very natural.
(6) a. Durch
By
das
the
neue
new
Gesetz
law
sollen
should
besser
better
Eigenheimbesitzer
house owners
geschu?tzt
protected
werden.
be.
b. Durch
By
das
the
neue
new
Gesetz
law
sollen
should
Eigenheimbesitzer
house owners
besser
better
geschu?tzt
protected
werden.
be.
This observation brings us back to our initial point
that the surface realisation task is especially chal-
lenging due to the interaction of a range of semantic
and discourse phenomena. Obviously, this interac-
tion makes it difficult to single out preferences for a
specific alternation type. Future work will have to
establish how this problem should be dealt with in
Items
All Correct Mixed False
?All? sent. 0.58 0.6 0.54 0.62
?Correct? sent. 0.64 0.63 0.56 0.72
?False? sent. 0.47 0.57 0.48 0.44
Top-ranked
corpus sent.
84% 78% 83% 91%
Table 6: Human Evaluation
the design of human evaluation experiments.
7 Conclusion
We have presented a grammar-based generation ar-
chitecture which implements the surface realisation
of meaning representations abstracting from voice
and word order. In order to be able to study voice
alternations in a variety of contexts, we designed
heuristic underspecification rules which establish,
for instance, the alternation relation between an ac-
tive with a generic agent and a passive that does
not overtly realise the agent. This strategy leads
to a better balanced distribution of the alternations
in the training data, such that our linguistically
informed generation ranking model achieves high
BLEU scores and accurately predicts active and pas-
sive. In future work, we will extend our experiments
to a wider range of alternations and try to capture
inter-sentential context more explicitly. Moreover, it
would be interesting to carry over our methodology
to a purely statistical linearisation system where the
relation between an input representation and a set of
candidate realisations is not so clearly defined as in
a grammar-based system.
Our study also addressed the interaction of dif-
ferent linguistic variation types, i.e. word order
and voice, by looking at different types of linguis-
tic features and exploring different ways of labelling
the training data. However, our SVM-based learn-
ing framework is not well-suited to directly assess
the correlation between a certain feature (or fea-
ture combination) and the occurrence of an alterna-
tion. Therefore, it would be interesting to relate our
work to the techniques used in theoretical papers,
e.g. (Bresnan et al, 2007), where these correlations
are analysed more directly.
1015
References
Judith Aissen. 1999. Markedness and subject choice in
optimality theory. Natural Language and Linguistic
Theory, 17(4):673?711.
Judith Aissen. 2003. Differential Object Marking:
Iconicity vs. Economy. Natural Language and Lin-
guistic Theory, 21:435?483.
Anja Belz and Eric Kow. 2010. Comparing rating
scales and preference judgements in language evalu-
ation. In Proceedings of the 6th International Natural
Language Generation Conference (INLG?10).
Anja Belz, Mike White, Josef van Genabith, Deirdre
Hogan, and Amanda Stent. 2010. Finding common
ground: Towards a surface realisation shared task.
In Proceedings of the 6th International Natural Lan-
guage Generation Conference (INLG?10).
Anja Belz. 2005. Statistical generation: Three meth-
ods compared and evaluated. In Proceedings of Tenth
European Workshop on Natural Language Generation
(ENLG-05), pages 15?23.
Bernd Bohnet, Leo Wanner, Simon Mill, and Alicia
Burga. 2010. Broad coverage multilingual deep sen-
tence generation with a stochastic multi-level realizer.
In Proceedings of the 23rd International Conference
on Computational Linguistics (COLING 2010), Bei-
jing, China.
Joan Bresnan, Shipra Dingare, and Christopher D. Man-
ning. 2001. Soft Constraints Mirror Hard Constraints:
Voice and Person in English and Lummi. In Proceed-
ings of the LFG ?01 Conference.
Joan Bresnan, Anna Cueni, Tatiana Nikitina, and Harald
Baayen. 2007. Predicting the Dative Alternation. In
G. Boume, I. Kraemer, and J. Zwarts, editors, Cogni-
tive Foundations of Interpretation. Amsterdam: Royal
Netherlands Academy of Science.
Aoife Cahill and Martin Forst. 2010. Human Evaluation
of a German Surface Realisation Ranker. In Proceed-
ings of the 12th Conference of the European Chapter
of the ACL (EACL 2009), pages 112 ? 120, Athens,
Greece. Association for Computational Linguistics.
Aoife Cahill and Arndt Riester. 2009. Incorporating
Information Status into Generation Ranking. In Pro-
ceedings of the 47th Annual Meeting of the ACL, pages
817?825, Suntec, Singapore, August. Association for
Computational Linguistics.
Aoife Cahill, Martin Forst, and Christian Rohrer. 2007.
Stochastic realisation ranking for a free word order
language. In Proceedings of the Eleventh European
Workshop on Natural Language Generation, pages
17?24, Saarbru?cken, Germany, June. DFKI GmbH.
Document D-07-01.
Dick Crouch and Tracy Holloway King. 2006. Se-
mantics via F-Structure Rewriting. In Miriam Butt
and Tracy Holloway King, editors, Proceedings of the
LFG06 Conference.
Dick Crouch, Mary Dalrymple, Ron Kaplan, Tracy King,
John Maxwell, and Paula Newman. 2006. XLE Docu-
mentation. Technical report, Palo Alto Research Cen-
ter, CA.
Katrin Erk and Diana McCarthy. 2009. Graded Word
Sense Assignment. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing, pages 440 ? 449, Singapore.
Katja Filippova and Michael Strube. 2007. Generat-
ing constituent order in German clauses. In Proceed-
ings of the 45th Annual Meeting of the Association for
Computational Linguistics (ACL 07), Prague, Czech
Republic.
Katja Filippova and Michael Strube. 2009. Tree lin-
earization in English: Improving language model
based approaches. In Companion Volume to the Pro-
ceedings of Human Language Technologies Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics (NAACL-HLT 09,
short)., Boulder, Colorado.
Martin Forst. 2007. Filling Statistics with Linguistics
? Property Design for the Disambiguation of German
LFG Parses. In ACL 2007 Workshop on Deep Linguis-
tic Processing, pages 17?24, Prague, Czech Republic,
June. Association for Computational Linguistics.
Matthew Gerber and Joyce Chai. 2010. Beyond nom-
bank: A study of implicit argumentation for nominal
predicates. In Proceedings of the ACM Conference on
Knowledge Discovery and Data Mining (KDD).
Barbara J. Grosz, Aravind Joshi, and Scott Weinstein.
1995. Centering: A framework for modeling the lo-
cal coherence of discourse. Computational Linguis-
tics, 21(2):203?225.
Thorsten Joachims. 1996. Training linear svms in linear
time. In M. Butt and T. H. King, editors, Proceedings
of the ACM Conference on Knowledge Discovery and
Data Mining (KDD), CSLI Proceedings Online.
Jonas Kuhn. 2003. Optimality-Theoretic Syntax?A
Declarative Approach. CSLI Publications, Stanford,
CA.
Irene Langkilde and Kevin Knight. 1998. Generation
that exploits corpus-based statistical knowledge. In
Proceedings of the ACL/COLING-98, pages 704?710,
Montreal, Quebec.
Tomasz Marciniak and Michael Strube. 2005. Using
an annotated corpus as a knowledge source for lan-
guage generation. In Proceedings of Workshop on Us-
ing Corpora for Natural Language Generation, pages
19?24, Birmingham, UK.
Hiroko Nakanishi, Yusuke Miyao, and Junichi Tsujii.
2005. Probabilistic models for disambiguation of an
1016
HPSG-based chart generator. In Proceedings of IWPT
2005.
Adwait Ratnaparkhi. 2000. Trainable methods for sur-
face natural language generation. In Proceedings of
NAACL 2000, pages 194?201, Seattle, WA.
Stefan Riezler, Detlef Prescher, Jonas Kuhn, and Mark
Johnson. 2000. Lexicalized stochastic modeling of
constraint-based grammars using log-linear measures
and EM training. In Proceedings of the 38th Annual
Meeting of the Association for Computational Linguis-
tics (ACL?00), Hong Kong, pages 480?487.
Stefan Riezler, Dick Crouch, Ron Kaplan, Tracy King,
John Maxwell, and Mark Johnson. 2002. Parsing the
Wall Street Journal using a Lexical-Functional Gram-
mar and discriminative estimation techniques. In Pro-
ceedings of the 40th Annual Meeting of the Associa-
tion for Computational Linguistics (ACL?02), Pennsyl-
vania, Philadelphia.
Eric K. Ringger, Michael Gamon, Robert C. Moore,
David Rojas, Martine Smets, and Simon Corston-
Oliver. 2004. Linguistically Informed Statistical
Models of Constituent Structure for Ordering in Sen-
tence Realization. In Proceedings of the 2004 In-
ternational Conference on Computational Linguistics,
Geneva, Switzerland.
Christian Rohrer and Martin Forst. 2006. Improving
coverage and parsing quality of a large-scale LFG for
German. In Proceedings of LREC-2006.
Erik Velldal and Stephan Oepen. 2006. Statistical rank-
ing in tactical generation. In Proceedings of the 2006
Conference on Empirical Methods in Natural Lan-
guage Processing, Sydney, Australia.
Erik Velldal. 2008. Empirical Realization Ranking.
Ph.D. thesis, University of Oslo, Department of Infor-
matics.
Sina Zarrie? and Jonas Kuhn. 2010. Reversing F-
structure Rewriting for Generation from Meaning Rep-
resentations. In Proceedings of the LFG10 Confer-
ence, Ottawa.
Sina Zarrie?. 2009. Developing German Semantics on
the basis of Parallel LFG Grammars. In Proceed-
ings of the 2009 Workshop on Grammar Engineering
Across Frameworks (GEAF 2009), pages 10?18, Sun-
tec, Singapore, August. Association for Computational
Linguistics.
1017
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 145?150,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
ParaQuery: Making Sense of Paraphrase Collections
Lili Kotlerman
Bar-Ilan University
Israel
lili.dav@gmail.com
Nitin Madnani and Aoife Cahill
Educational Testing Service
Princeton, NJ, USA
{nmadnani,acahill}@ets.org
Abstract
Pivoting on bilingual parallel corpora is a
popular approach for paraphrase acquisi-
tion. Although such pivoted paraphrase
collections have been successfully used to
improve the performance of several dif-
ferent NLP applications, it is still difficult
to get an intrinsic estimate of the qual-
ity and coverage of the paraphrases con-
tained in these collections. We present
ParaQuery, a tool that helps a user inter-
actively explore and characterize a given
pivoted paraphrase collection, analyze its
utility for a particular domain, and com-
pare it to other popular lexical similarity
resources ? all within a single interface.
1 Introduction
Paraphrases are widely used in many Natural Lan-
guage Processing (NLP) tasks, such as informa-
tion retrieval, question answering, recognizing
textual entailment, text simplification etc. For ex-
ample, a question answering system facing a ques-
tion ?Who invented bifocals and lightning rods??
could retrieve the correct answer from the text
?Benjamin Franklin invented strike termination
devices and bifocal reading glasses? given the in-
formation that ?bifocal reading glasses? is a para-
phrase of ?bifocals? and ?strike termination de-
vices? is a paraphrase of ?lightning rods?.
There are numerous approaches for automati-
cally extracting paraphrases from text (Madnani
and Dorr, 2010). We focus on generating para-
phrases by pivoting on bilingual parallel corpora
as originally suggested by Bannard and Callison-
Burch (2005). This technique operates by attempt-
ing to infer semantic equivalence between phrases
in the same language by using a second language
as a bridge. It builds on one of the initial steps used
to train a phrase-based statistical machine transla-
tion system. Such systems rely on phrase tables ?
a tabulation of correspondences between phrases
in the source language and phrases in the target
language. These tables are usually extracted by in-
ducing word alignments between sentence pairs in
a parallel training corpus and then incrementally
building longer phrasal correspondences from in-
dividual words and shorter phrases. Once such a
tabulation of bilingual correspondences is avail-
able, correspondences between phrases in one lan-
guage may be inferred simply by using the phrases
in the other language as pivots, e.g., if both ?man?
and ?person? correspond to ?personne? in French,
then they can be considered paraphrases. Each
paraphrase pair (rule) in a pivoted paraphrase col-
lection is defined by a source phrase e1, the target
phrase e2 that has been inferred as its paraphrase,
and a probability score p(e2|e1) obtained from the
probability values in the bilingual phrase table.1
Pivoted paraphrase collections have been suc-
cessfully used in different NLP tasks including
automated document summarization (Zhou et al,
2006), question answering (Riezler et al, 2007),
and machine translation (Madnani, 2010). Yet, it
is still difficult to get an estimate of the intrinsic
quality and coverage of the paraphrases contained
in these collections. To remedy this, we propose
ParaQuery ? a tool that can help explore and ana-
lyze pivoted paraphrase collections.
2 ParaQuery
In this section we first briefly describe how to set
up ParaQuery (?2.1) and then demonstrate its use
in detail for interactively exploring and character-
izing a paraphrase collection, analyzing its util-
ity for a particular domain, and comparing it with
other word-similarity resources (?2.2). Detailed
documentation will be included in the tool.
1There may be other values associated with each pair, but
we ignore them for the purposes of this paper.
145
2.1 Setting up
ParaQuery operates on pivoted paraphrase collec-
tions and can accept collections generated using
any set of tools that are preferred by the user, as
long as the collection is stored in a pre-defined
plain-text format containing the source and target
phrases, the probability values, as well as informa-
tion on pivots (optional but useful for pivot-driven
analysis, as shown later). This format is com-
monly used in the machine translation and para-
phrase generation community. In this paper, we
adapt the Thrax and Joshua (Ganitkevitch et al,
2012) toolkits to generate a pivoted paraphrase
collection using the English-French EuroParl par-
allel corpus, which we use as our example col-
lection for demonstrating ParaQuery. Once a piv-
oted collection is generated, ParaQuery needs to
convert it into an SQLite database against which
queries can be run. This is done by issuing the
index command at the ParaQuery command-line
interface (described in ?2.2.1).
2.2 Exploration and Analysis
In order to provide meaningful exploration and
analysis, we studied various scenarios in which
paraphrase collections are used, and found that the
following issues typically interest the developers
and users of such collections:
1. Semantic relations between the paraphrases
in the collection (e.g. synonymy, hyponymy)
and their frequency.
2. The frequency of inaccurate paraphrases,
possible ways of de-noising the collection,
and the meaningfulness of scores (better
paraphrases should be scored higher).
3. The utility of the collection for a specific do-
main, i.e. whether domain terms of interest
are present in the collection.
4. Comparison of different collections based on
the above dimensions.
We note that paraphrase collections are used in
many tasks with different acceptability thresholds
for semantic relations, noisy paraphrases etc. We
do not intend to provide an exhaustive judgment
of paraphrase quality, but instead allow users to
characterize a collection, enabling an analysis of
the aforesaid issues and providing information for
them to decide whether a given collection is suit-
able for their specific task and/or domain.
2.2.1 Command line interface
ParaQuery allows interactive exploration and
analysis via a simple command line interface, by
processing user issued queries such as:
show <query>: display the rules which satisfy
the conditions of the given query.
show count <query>: display the number of
such rules.
explain <query>: display information about the
pivots which yielded each of these rules.
analyze <query>: display statistics about these
rules and save a report to an output file.
The following information is stored in the
SQLite database for each paraphrase rule:2
? The source and the target phrases, and the
probability score of the rule.
? Are the source and the target identical?
? Do the source and the target have the same
part of speech?3
? Length of the source and the target, and the
difference in their lengths.
? Number of pivots and the list of pivots.
? Are both the source and the target found in
WordNet (WN)? If yes, the WN relation be-
tween them (synonym, derivation, hypernym,
hyponym, co-hyponym, antonym, meronym,
holonym, pertainym) or the minimal dis-
tance, if they are not connected directly.
Therefore, all of the above can be used, alone or
in combination, to constrain the queries and de-
fine the rule(s) of interest. Figure 1 presents sim-
ple queries processed by the show command: the
first query displays top-scoring rules with ?man?
as their source phrase, while the second adds re-
striction on the rules? score. By default, the tool
displays the 10 best-scoring rules per query, but
this limit can be changed as shown. For each
rule, the corresponding score and semantic rela-
tion/distance is displayed.
2Although some of this information is available in the
paraphrase collection that was indexed, the remaining is auto-
matically computed and injected into the database during the
indexing process. Indexing the French-pivoted paraphrase
collection (containing 3,633,015 paraphrase rules) used in
this paper took about 6 hours.
3We use the simple parts of speech provided by WordNet
(nouns, verbs, adjectives and adverbs).
146
The queries provide a flexible way to define and
work with the rule set of interest, starting from fil-
tering low-scoring rules till extracting specific se-
mantic relations or constraining on the number of
pivots. Figure 2 presents additional examples of
queries. The tool also enables filtering out target
terms with a recurrent lemma, as illustrated in the
same figure. Note that ParaQuery also contains a
batch mode (in addition to the interactive mode il-
lustrated so far) to automatically extract the output
for a set of queries contained in a batch script.
Figure 1: Examples of the show command and the
probability constraint.
2.2.2 Analyzing pivot information
It is well known that pivoted paraphrase collec-
tions contain a lot of noisy rules. To understand
the origins of such rules, an explain query can be
used, which displays the pivots that yielded each
paraphrase rule, and the probability share of each
pivot in the final probability score. Figure 3 shows
an example of this command.
We see that noisy rules can originate from stop-
word pivots, e.g. ?l?. It is common to filter rules
containing stop-words, yet perhaps it is also im-
portant to exclude stop-word pivots, which was
never considered in the past. We can use Para-
Query to further explore whether discarding stop-
word pivots is a good idea. Figure 4 presents
a more complex query showing paraphrase rules
that were extracted via a single pivot ?l?. We see
that the top 5 such rules are indeed noisy, indicat-
ing that perhaps all of the 5,360 rules satisfying
the query can be filtered out.
2.2.3 Analysis of rule sets
In order to provide an overall analysis of a rule set
or a complete collection, ParaQuery includes the
Figure 2: Restricting the output of the show com-
mand using WordNet relations and distance, and
the unique lemma constraint.
Figure 3: An example of the explain command.
analyze command. Figure 5 shows the typical in-
formation provided by this command. In addition,
a report is generated to a file, including the anal-
ysis information for the whole rule set and for its
three parts: top, middle and bottom, as defined by
the scores of the rules in the set. The output to the
file is more detailed and expands on the informa-
tion presented in Figure 5. For example, it also
includes, for each part, rule samples and score dis-
tributions for each semantic relation and different
WordNet distances.
The information contained in the report can be
147
Figure 4: Exploring French stop-word pivots using the pivots condition of the show command.
Figure 5: An example of the analyze command (full output not shown for space reasons).
148
TOP BOTTOM
finest? better approach? el
outdoors? external effect? parliament
unsettled? unstable comment? speak up
intelligentsia? intelligence propose? allotted
caretaker? provisional prevent? aimed
luckily? happily energy? subject matter
Table 1: A random sample of undefined relation
rules from our collection?s top and bottom parts.
easily used for generating graphs and tables. For
example, Figure 6 shows the distribution of se-
mantic relations in the three parts of our exam-
ple paraphrase collection. The figure character-
izes the collection in terms of semantic relations
it contains and illustrates the fact that the scores
agree with their desired behavior: (1) the collec-
tion?s top-scoring part contains significantly more
synonyms than its middle and bottom parts, (2)
similar trends hold for derivations and hypernyms,
which are more suitable for paraphrasing than co-
hyponyms and other relations not defined in Word-
Net (we refer to these relations as undefined rela-
tions), (3) such undefined relations have the high-
est frequency in the collection?s bottom part, and
are least frequent in its top part. Among other
conclusions, the figure shows, that discarding the
lower-scoring middle and bottom parts of the col-
lection would allow retaining almost all the syn-
onyms and derivations, while filtering out most of
the co-hyponyms and a considerable number of
undefined relations.
Yet from Figure 6 we see that undefined rela-
tions constitute the majority of the rules in the col-
lection. To better understand this, random rule
samples provided in the analysis output can be
used, as shown in Table 1. From this table, we see
that the top-part rules are indeed mostly valid for
paraphrasing, unlike the noisy bottom-part rules.
The score distributions reported as part of the anal-
ysis can be used to further explore the collec-
tion and set sound thresholds suitable for different
tasks and needs.
2.2.4 Analysis of domain utility
One of the frequent questions of interest is
whether a given collection is suitable for a specific
domain. To answer this question, ParaQuery al-
lows the user to run the analysis from ?2.2.3 over
rules whose source phrases belong to a specific
domain, by means of the analyze <query> us-
ing <file> command. The file can hold either a
list of domain terms or a representative domain
text, from which frequent terms and term collo-
cations will be automatically extracted, presented
to the user, and utilized for analysis. The analysis
includes the coverage of the domain terms in the
paraphrase collection, and can also be restricted to
top-K rules per source term, a common practice in
many NLP applications. We do not show an exam-
ple of this command due to space considerations.
2.2.5 Comparison with other collections
The output of the analyze command can also be
used to compare different collections, either in
general or for a given domain. Although Para-
Query is designed for pivoted paraphrase collec-
tions, it allows comparing them to non-pivoted
paraphrase collections as well. Next we present an
example of such a comparative study, performed
using ParaQuery via several analyze commands.
Table 2 compares three different collections:
the French pivoted paraphrase collection, a dis-
tributional similarity resource (Kotlerman et al,
2010) and a Wikipedia-based resource (Shnarch et
al., 2009). The table shows the collection sizes,
as well as the number of different (unique) source
phrases in them and, correspondingly, the average
number of target phrases per source. From the
table we can see that the distributional similarity
resource contains a lot of general language terms
found in WordNet, while the Wikipedia resource
includes only a small amount of such terms. A
sample of rules from the Wikipedia collection ex-
plains this behavior, e.g. ?Yamaha SR500 ? mo-
torcycle?. The table provides helpful information
to decide which collection is (more) suitable for
specific tasks, such as paraphrase recognition and
generation, query expansion, automatic generation
of training data for different supervised tasks, etc.
3 Conclusions and Future Work
We presented ParaQuery?a tool for interactive
exploration and analysis of pivoted paraphrase
collections?and showed that it can be used to
estimate the intrinsic quality and coverage of the
paraphrases contained in these collections, a task
that is still somewhat difficult. ParaQuery can also
be used to answer the questions that users of such
collections are most interested in. We plan to re-
lease ParaQuery under an open-source license, in-
cluding our code for generating paraphrase col-
lections that can then be indexed and analyzed by
149
0%
20%
40%
60%
80% Top Middle Bottom
22% 11% 8% 4% 8% 0%
45%
6% 1% 4% 4%
17%
0%
68%
1% 0% 1% 1% 11% 0%
86%
Synonym Derivation Hypernym Hyponym Co-hyponym Antonym Undefined
Figure 6: Distribution of semantic relations in the top, middle and bottom parts of the example collection.
The parts are defined by binning the scores of the rules in the collection.
Collection Size (rules) In WordNet Unique Src Avg. Tgts per Src davg for UR
Pivoted (FR) 3,633,015 757,994 (21%) 188,898 16.064 2.567
Dist.Sim. 7,298,321 3,252,967 (45%) 113,444 64.334 6.043
Wikipedia 7,880,962 295,161 (4%) 2,727,362 2.890 8.556
Table 2: Comparing the French-pivoted paraphrase collection to distributional-similarity based and
Wikipedia-based similarity collections, in terms of total size, percentage of rules in WordNet, number
of unique source phrases, average number of target phrases per source phrase, and the average WordNet
distance between the two sides of the undefined relation (UR) rules.
ParaQuery. We also plan to include pre-generated
paraphrase collections in the release so that users
of ParaQuery can use it immediately.
In the future, we plan to use this tool for analyz-
ing the nature of pivoted paraphrases. The quality
and coverage of these paraphrases is known to de-
pend on several factors, including (a) the genre of
the bilingual corpus, (b) the word-alignment algo-
rithm used during bilingual training, and (c) the
pivot language itself. However, there have been
no explicit studies designed to measure such vari-
ations. We believe that ParaQuery is perfectly
suited to conducting such studies and moving the
field of automated paraphrase generation forward.
Acknowledgments
This work was partially supported by the European Commu-
nity?s Seventh Framework Programme (FP7/2007-2013) un-
der grant agreement no. 287923 (EXCITEMENT).
References
Colin Bannard and Chris Callison-Burch. 2005. Paraphras-
ing with Bilingual Parallel Corpora. In Proceedings of
ACL, pages 597?604.
Juri Ganitkevitch, Yuan Cao, Jonathan Weese, Matt Post, and
Chris Callison-Burch. 2012. Joshua 4.0: Packing, PRO,
and Paraphrases. In Proceedings of WMT, pages 283?291.
Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan
Zhitomirsky-Geffet. 2010. Directional Distributional
Similarity for Lexical Inference. Natural Language En-
gineering, 16(4):359?389.
Nitin Madnani and Bonnie J. Dorr. 2010. Generating
Phrasal and Sentential Paraphrases: A Survey of Data-
driven Methods. Computational Linguistics, 36(3):341?
387.
Nitin Madnani. 2010. The Circle of Meaning: From Trans-
lation to Paraphrasing and Back. Ph.D. thesis, Depart-
ment of Computer Science, University of Maryland Col-
lege Park.
Stefan Riezler, Alexander Vasserman, Ioannis Tsochan-
taridis, Vibhu O. Mittal, and Yi Liu. 2007. Statistical
Machine Translation for Query Expansion in Answer Re-
trieval. In Proceedings of ACL, pages 464?471.
Eyal Shnarch, Libby Barak, and Ido Dagan. 2009. Extracting
lexical reference rules from Wikipedia. In Proceedings of
ACL-IJCNLP, pages 450?458.
Liang Zhou, Chin-Yew Lin, Dragos Stefan Muntenau, and
Eduard Hovy. 2006. ParaEval: Using Paraphrases to
Evaluate Summaries Automatically. In Proceedings of
HLT-NAACL, pages 447?454.
150
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 174?180,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Predicting Grammaticality on an Ordinal Scale
Michael Heilman Aoife Cahill Nitin Madnani Melissa Lopez Matthew Mulholland
Educational Testing Service
Princeton, NJ, USA
{mheilman,acahill,nmadnani,mlopez002,mmulholland}@ets.org
Joel Tetreault
Yahoo! Research
New York, NY, USA
tetreaul@yahoo-inc.com
Abstract
Automated methods for identifying
whether sentences are grammatical
have various potential applications (e.g.,
machine translation, automated essay
scoring, computer-assisted language
learning). In this work, we construct a
statistical model of grammaticality using
various linguistic features (e.g., mis-
spelling counts, parser outputs, n-gram
language model scores). We also present
a new publicly available dataset of learner
sentences judged for grammaticality on
an ordinal scale. In evaluations, we
compare our system to the one from Post
(2011) and find that our approach yields
state-of-the-art performance.
1 Introduction
In this paper, we develop a system for the task
of predicting the grammaticality of sentences, and
present a dataset of learner sentences rated for
grammaticality. Such a system could be used, for
example, to check or to rank outputs from systems
for text summarization, natural language genera-
tion, or machine translation. It could also be used
in educational applications such as essay scoring.
Much of the previous research on predicting
grammaticality has focused on identifying (and
possibly correcting) specific types of grammati-
cal errors that are typically made by English lan-
guage learners, such as prepositions (Tetreault and
Chodorow, 2008), articles (Han et al, 2006), and
collocations (Dahlmeier and Ng, 2011). While
some applications (e.g., grammar checking) rely
on such fine-grained predictions, others might be
better addressed by sentence-level grammaticality
judgments (e.g., machine translation evaluation).
Regarding sentence-level grammaticality, there
has been much work on rating the grammatical-
ity of machine translation outputs (Gamon et al,
2005; Parton et al, 2011), such as the MT Quality
Estimation Shared Tasks (Bojar et al, 2013, ?6),
but relatively little on evaluating the grammatical-
ity of naturally occurring text. Also, most other re-
search on evaluating grammaticality involves arti-
ficial tasks or datasets (Sun et al, 2007; Lee et al,
2007; Wong and Dras, 2010; Post, 2011).
Here, we make the following contributions.
? We develop a state-of-the-art approach for
predicting the grammaticality of sentences on
an ordinal scale, adapting various techniques
from the previous work described above.
? We create a dataset of grammatical and un-
grammatical sentences written by English
language learners, labeled on an ordinal scale
for grammaticality. With this unique data set,
which we will release to the research com-
munity, it is now possible to conduct realis-
tic evaluations for predicting sentence-level
grammaticality.
2 Dataset Description
We created a dataset consisting of 3,129 sentences
randomly selected from essays written by non-
native speakers of English as part of a test of
English language proficiency. We oversampled
lower-scoring essays to increase the chances of
finding ungrammatical sentences. Two of the au-
thors of this paper, both native speakers of English
with linguistic training, annotated the data. We
refer to these annotators as expert judges. When
making judgments of the sentences, they saw the
previous sentence from the same essay as context.
These two authors were not directly involved in
development of the system in ?3.
Each sentence was annotated on a scale from
1 to 4 as described below, with 4 being the most
174
grammatical. We use an ordinal rather than bi-
nary scale, following previous work such as that of
Clark et al (2013) and Crocker and Keller (2005)
who argue that the distinction between grammati-
cal and ungrammatical is not simply binary. Also,
for practical applications, we believe that it is use-
ful to distinguish sentences with minor errors from
those with major errors that may disrupt communi-
cation. Our annotation scheme was influenced by
a translation rating scheme by Coughlin (2003).
Every sentence judged on the 1?4 scale must be
a clause. There is an extra category (?Other?) for
sentences that do not fit this criterion. We exclude
instances of ?Other? in our experiments (see ?4).
4. Perfect The sentence is native-sounding. It has
no grammatical errors, but may contain very mi-
nor typographical and/or collocation errors, as in
Example (1).
(1) For instance, i stayed in a dorm when i
went to collge.
3. Comprehensible The sentence may contain
one or more minor grammatical errors, includ-
ing subject-verb agreement, determiner, and mi-
nor preposition errors that do not make the mean-
ing unclear, as in Example (2).
(2) We know during Spring Festival, Chinese
family will have a abundand family banquet
with family memebers.
?Chinese family?, which could be corrected to
?Chinese families?, ?each Chinese family?, etc.,
would be an example of a minor grammatical er-
ror involving determiners.
2. Somewhat Comprehensible The sentence
may contain one or more serious grammatical
errors, including missing subject, verb, object,
etc., verb tense errors, and serious preposition
errors. Due to these errors, the sentence may
have multiple plausible interpretations, as in
Example (3).
(3) I can gain the transportations such as buses
and trains.
1. Incomprehensible The sentence contains so
many errors that it would be difficult to correct,
as in Example (4).
(4) Or you want to say he is only a little boy do
not everything clearly?
The phrase ?do not everything? makes the sen-
tence practically incomprehensible since the sub-
ject of ?do? is not clear.
O. Other/Incomplete This sentence is incom-
plete. These sentences, such as Example (5), ap-
pear in our corpus due to the nature of timed tests.
(5) The police officer handed the
This sentence is cut off and does not at least in-
clude one clause.
We measured interannotator agreement on a
subset of 442 sentences that were independently
annotated by both expert annotators. Exact agree-
ment was 71.3%, unweighted ? = 0.574, and
Pearson?s r = 0.759.
1
For our experiments, one
expert annotator was arbitrarily selected, and for
the doubly-annotated sentences, only the judg-
ments from that annotator were retained.
The labels from the expert annotators are dis-
tributed as follows: 72 sentences are labeled 1;
538 are 2; 1,431 are 3; 978 are 4; and 110 are ?O?.
We also gathered 5 additional judgments using
Crowdflower.
2
For this, we excluded the ?Other?
category and any sentences that had been marked
as such by the expert annotators. We used 100
(3.2%) of the judged sentences as ?gold? data in
Crowdflower to block contributors who were not
following the annotation guidelines. For those
sentences, only disagreements within 1 point of
the expert annotator judgment were accepted. In
preliminary experiments, averaging the six judg-
ments (1 expert, 5 crowdsourced) for each item
led to higher human-machine agreement. For all
experiments reported later, we used this average
of six judgments as our gold standard.
For our experiments (?4), we randomly split the
data into training (50%), development (25%), and
testing (25%) sets. We also excluded all instances
labeled ?Other?. These are relatively uncommon
and less interesting to this study. Also, we believe
that simpler, heuristic approaches could be used to
identify such sentences.
We use ?GUG? (?Grammatical? versus ?Un-
Grammatical?) to refer to this dataset. The dataset
is available for research at https://github.
com/EducationalTestingService/
gug-data.
1
The reported agreement values assume that ?Other?
maps to 0. For the sentences where both labels were in the
1?4 range (n = 424), Pearson?s r = 0.767.
2
http://www.crowdflower.com
175
3 System Description
This section describes the statistical model (?3.1)
and features (?3.2) used by our system.
3.1 Statistical Model
We use `
2
-regularized linear regression (i.e., ridge
regression) to learn a model of sentence grammat-
icality from a variety of linguistic features.
34
To tune the `
2
-regularization hyperparameter ?,
the system performs 5-fold cross-validation on the
data used for training. The system evaluates ? ?
10
{?4,...,4}
and selects the one that achieves the
highest cross-validation correlation r.
3.2 Features
Next, we describe the four types of features.
3.2.1 Spelling Features
Given a sentence with with n word tokens, the
model filters out tokens containing nonalpha-
betic characters and then computes the num-
ber of misspelled words n
miss
(later referred to
as num misspelled), the proportion of mis-
spelled words
n
miss
n
, and log(n
miss
+ 1) as fea-
tures. To identify misspellings, we use a freely
available spelling dictionary for U.S. English.
5
3.2.2 n-gram Count and Language Model
Features
Given each sentence, the model obtains the counts
of n-grams (n = 1 . . . 3) from English Gigaword
and computes the following features:
6
?
?
s?S
n
log(count(s) + 1)
?S
n
?
3
We use ridge regression from the scikit-learn
toolkit (Pedregosa et al, 2011) v0.23.1 and the
SciKit-Learn Laboratory (http://github.com/
EducationalTestingService/skll).
4
Regression models typically produce conservative pre-
dictions with lower variance than the original training data.
So that predictions better match the distribution of labels in
the training data, the system rescales its predictions. It saves
the mean and standard deviation of the training data gold
standard (M
gold
and SD
gold
, respectively) and of its own
predictions on the training data (M
pred
and SD
pred
, respec-
tively). During cross-validation, this is done for each fold.
From an initial prediction y?, it produces the final prediction:
y?
?
=
y??M
pred
SD
pred
? SD
gold
+M
gold
. This transformation does
not affect Pearson?s r correlations or rankings, but it would
affect binarized predictions.
5
http://pythonhosted.org/pyenchant/
6
We use the New York Times (nyt), the Los Ange-
les Times-Washington Post (ltw), and the Washington Post-
Bloomberg News (wpb) sections from the fifth edition of En-
glish Gigaword (LDC2011T07).
? max
s?S
n
log(count(s) + 1)
? min
s?S
n
log(count(s) + 1)
where S
n
represents the n-grams of order n from
the given sentence. The model computes the fol-
lowing features from a 5-gram language model
trained on the same three sections of English Gi-
gaword using the SRILM toolkit (Stolcke, 2002):
? the average log-probability of the
given sentence (referred to as
gigaword avglogprob later)
? the number of out-of-vocabulary words in the
sentence
Finally, the system computes the average
log-probability and number of out-of-vocabulary
words from a language model trained on a col-
lection of essays written by non-native English
speakers
7
(?non-native LM?).
3.2.3 Precision Grammar Features
Following Wagner et al (2007) and Wagner et
al. (2009), we use features extracted from preci-
sion grammar parsers. These grammars have been
hand-crafted and designed to only provide com-
plete syntactic analyses for grammatically cor-
rect sentences. This is in contrast to treebank-
trained grammars, which will generally provide
some analysis regardless of grammaticality. Here,
we use (1) the Link Grammar Parser
8
and (2)
the HPSG English Resource Grammar (Copestake
and Flickinger, 2000) and PET parser.
9
We use a binary feature, complete link,
from the Link grammar that indicates whether at
least one complete linkage can be found for a sen-
tence. We also extract several features from the
HPSG analyses.
10
They mostly reflect information
about unification success or failure and the associ-
ated costs. In each instance, we use the logarithm
of one plus the frequency.
7
This did not overlap with the data described in ?2 and
was a subset of the data released by Blanchard et al (2013).
8
http://www.link.cs.cmu.edu/link/
9
http://moin.delph-in.net/PetTop
10
The complete list of relevant statistics used as features
is: trees, unify cost succ, unify cost fail,
unifications succ, unifications fail,
subsumptions succ, subsumptions fail,
words, words pruned, aedges, pedges,
upedges, raedges, rpedges, medges. During
development, we observed that some of these features vary
for some inputs, probably due to parsing search timeouts. On
10 preliminary runs with the development set, this variance
had minimal effects on correlations with human judgments
(less than 0.00001 in terms of r).
176
rour system 0.668
? non-native LM (?3.2.2) 0.665
? HPSG parse (?3.2.3) 0.664
? PCFG parse (?3.2.4) 0.662
? spelling (?3.2.1) 0.643
? gigaword LM (?3.2.2) 0.638
? link parse (?3.2.3) 0.632
? gigaword count (?3.2.2) 0.630
Table 1: Pearson?s r on the development set, for
our full system and variations excluding each fea-
ture type. ?? X? indicates the full model without
the ?X? features.
3.2.4 PCFG Parsing Features
We find phrase structure trees and basic depen-
dencies with the Stanford Parser?s English PCFG
model (Klein and Manning, 2003; de Marneffe et
al., 2006).
11
We then compute the following:
? the parse score as provided by the Stan-
ford PCFG Parser, normalized for sentence
length, later referred to as parse prob
? a binary feature that captures whether the top
node of the tree is sentential or not (i.e. the
assumption is that if the top node is non-
sentential, then the sentence is a fragment)
? features binning the number of dep rela-
tions returned by the dependency conversion.
These dep relations are underspecified for
function and indicate that the parser was un-
able to find a standard relation such as subj,
possibly indicating a grammatical error.
4 Experiments
Next, we present evaluations on the GUG dataset.
4.1 Feature Ablation
We conducted a feature ablation study to iden-
tify the contributions of the different types of fea-
tures described in ?3.2. We compared the perfor-
mance of the full model with all of the features
to models with all but one type of feature. For
this experiment, all models were estimated from
the training set and evaluated on the development
set. We report performance in terms of Pearson?s
r between the averaged 1?4 human labels and un-
rounded system predictions.
The results are shown in Table 1. From these
results, the most useful features appear to be the
n-gram frequencies from Gigaword and whether
the link parser can fully parse the sentence.
4.2 Test Set Results
In this section, we present results on the held-out
test set for the full model and various baselines,
summarized in Table 2. For test set evaluations,
we trained on the combination of the training and
development sets (?2), to maximize the amount of
training data for the final experiments.
We also trained and evaluated on binarized ver-
sions of the ordinal GUG labels: a sentence was
labeled 1 if the average judgment was at least 3.5
(i.e., would round to 4), and 0 otherwise. Evaluat-
ing on a binary scale allows us to measure how
well the system distinguishes grammatical sen-
tences from ungrammatical ones. For some ap-
plications, this two-way distinction may be more
relevant than the more fine-grained 1?4 scale. To
train our system on binarized data, we replaced the
`
2
-regularized linear regression model with an `
2
-
regularized logistic regression and used Kendall?s
? rank correlation between the predicted probabil-
ities of the positive class and the binary gold stan-
dard labels as the grid search metric (?3.1) instead
of Pearson?s r.
For the ordinal task, we report Pearson?s r be-
tween the averaged human judgments and each
system. For the binary task, we report percentage
accuracy. Since the predictions from the binary
and ordinal systems are on different scales, we in-
clude the nonparametric statistic Kendall?s ? as a
secondary evaluation metric for both tasks.
We also evaluated the binary system for the or-
dinal task by computing correlations between its
estimated probabilities and the averaged human
scores, and we evaluated the ordinal system for the
binary task by binarizing its predictions.
12
We compare our work to a modified version of
the publicly available
13
system from Post (2011),
which performed very well on an artificial dataset.
To our knowledge, it is the only publicly available
system for grammaticality prediction. It is very
11
We use the Nov. 12, 2013 version of the Stanford Parser.
12
We selected a threshold for binarization from a grid of
1001 points from 1 to 4 that maximized the accuracy of bina-
rized predictions from a model trained on the training set and
evaluated on the binarized development set. For evaluating
the three single-feature baselines discussed below, we used
the same approach except with grid ranging from the min-
imum development set feature value to the maximum plus
0.1% of the range.
13
The Post (2011) system is available at https://
github.com/mjpost/post2011judging.
177
Ordinal Task Binary Task
r Sig.
r
? % Acc. Sig.
%Acc.
?
our system 0.644 0.479 79.3 0.419
our system
logistic
0.616 * 0.484 80.7 0.428
Post 0.321 * 0.225 75.5 * 0.195
Post
logistic
0.259 * 0.181 74.4 * 0.181
complete link 0.386 * 0.335 74.8 * 0.302
gigaword avglogprob 0.414 * 0.290 76.7 * 0.280
num misspelled -0.462 * -0.370 74.8 * -0.335
Table 2: Human-machine agreement statistics for our system, the system from Post (2011), and simple
baselines, computed from the averages of human ratings in the testing set (?2). ?*? in a Sig. column
indicates a statistically significant difference from ?our system? (p < .05, see text for details). A majority
baseline for the binary task achieves 74.8% accuracy. The best results for each metric are in bold.
different from our system since it relies on par-
tial tree-substitution grammar derivations as fea-
tures. We use the feature computation components
of that system but replace its statistical model. The
system was designed for use with a dataset consist-
ing of 50% grammatical and 50% ungrammatical
sentences, rather than data with ordinal or continu-
ous labels. Additionally, its classifier implementa-
tion does not output scores or probabilities. There-
fore, we used the same learning algorithms as for
our system (i.e., ridge regression for the ordinal
task and logistic regression for the binary task).
14
To create further baselines for comparison,
we selected the following features that represent
ways one might approximate grammaticality if a
comprehensive model was unavailable: whether
the link parser can fully parse the sentence
(complete link), the Gigaword language
model score (gigaword avglogprob),
and the number of misspelled tokens
(num misspelled). Note that we expect
the number of misspelled tokens to be negatively
correlated with grammaticality. We flipped the
sign of the misspelling feature when computing
accuracy for the binary task.
To identify whether the differences in perfor-
mance for the ordinal task between our system and
each of the baselines are statistically significant,
we used the BC
a
Bootstrap (Efron and Tibshirani,
1993) with 10,000 replications to compute 95%
confidence intervals for the absolute value of r for
our system minus the absolute value of r for each
of the alternative methods. For the binary task, we
14
In preliminary experiments, we observed little difference
in performance between logistic regression and the original
support vector classifier used by the system from Post (2011).
used the sign test to test for significant differences
in accuracy. The results are in Table 2.
5 Discussion and Conclusions
In this paper, we developed a system for predict-
ing grammaticality on an ordinal scale and cre-
ated a labeled dataset that we have released pub-
licly (?2) to enable more realistic evaluations in
future research. Our system outperformed an ex-
isting state-of-the-art system (Post, 2011) in eval-
uations on binary and ordinal scales. This is the
most realistic evaluation of methods for predicting
sentence-level grammaticality to date.
Surprisingly, the system from Post (2011) per-
formed quite poorly on the GUG dataset. We spec-
ulate that this is due to the fact that the Post sys-
tem relies heavily on features extracted from au-
tomatic syntactic parses. While Post found that
such a system can effectively distinguish gram-
matical news text sentences from sentences gen-
erated by a language model, measuring the gram-
maticality of real sentences from language learn-
ers seems to require a wider variety of features,
including n-gram counts, language model scores,
etc. Of course, our findings do not indicate that
syntactic features such as those from Post (2011)
are without value. In future work, it may be pos-
sible to improve grammaticality measurement by
integrating such features into a larger system.
Acknowledgements
We thank Beata Beigman Klebanov, Yoko Futagi,
Su-Youn Yoon, and the anonymous reviewers for
their helpful comments. We also thank Jennifer
Foster for discussions about this work and Matt
Post for making his system publicly available.
178
References
Daniel Blanchard, Joel Tetreault, Derrick Higgins,
Aoife Cahill, and Martin Chodorow. 2013.
TOEFL11: A Corpus of Non-Native English. Tech-
nical report, Educational Testing Service.
Ond?rej Bojar, Christian Buck, Chris Callison-Burch,
Christian Federmann, Barry Haddow, Philipp
Koehn, Christof Monz, Matt Post, Radu Soricut, and
Lucia Specia. 2013. Findings of the 2013 Work-
shop on Statistical Machine Translation. In Pro-
ceedings of the Eighth Workshop on Statistical Ma-
chine Translation, pages 1?44, Sofia, Bulgaria, Au-
gust. Association for Computational Linguistics.
Alexander Clark, Gianluca Giorgolo, and Shalom Lap-
pin. 2013. Towards a statistical model of grammat-
icality. In Proceedings of the 35th Annual Confer-
ence of the Cognitive Science Society, pages 2064?
2069.
Ann Copestake and Dan Flickinger. 2000. An
open-source grammar development environment
and broad-coverage English grammar using HPSG.
In Proceedings of the 2nd International Confer-
ence on Language Resources and Evaluation (LREC
2000), Athens, Greece.
Deborah Coughlin. 2003. Correlating automated and
human assessments of machine translation quality.
In Proceedings of MT Summit IX, pages 63?70.
Matthew W. Crocker and Frank Keller. 2005. Prob-
abilistic grammars as models of gradience in lan-
guage processing. In Gradience in Grammar: Gen-
erative Perspectives. University Press.
Daniel Dahlmeier and Hwee Tou Ng. 2011. Correcting
Semantic Collocation Errors with L1-induced Para-
phrases. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing, pages 107?117, Edinburgh, Scotland, UK., July.
Association for Computational Linguistics.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating Typed
Dependency Parses from Phrase Structure Parses. In
LREC 2006, pages 449?454.
B. Efron and R. Tibshirani. 1993. An Introduction to
the Bootstrap. Chapman and Hall/CRC, Boca Ra-
ton, FL.
Michael Gamon, Anthony Aue, and Martine Smets.
2005. Sentence-level MT evaluation without refer-
ence translations: Beyond language modeling. In
Proceedings of EAMT, pages 103?111. Springer-
Verlag.
Na-Rae Han, Martin Chodorow, and Claudia Leacock.
2006. Detecting errors in English article usage by
non-native speakers. Natural Language Engineer-
ing, 12(2):115?129.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate Unlexicalized Parsing. In Proceedings of the
41st Annual Meeting of the Association for Com-
putational Linguistics, pages 423?430, Sapporo,
Japan, July. Association for Computational Linguis-
tics.
John Lee, Ming Zhou, and Xiaohua Liu. 2007. De-
tection of Non-Native Sentences Using Machine-
Translated Training Data. In Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Compu-
tational Linguistics; Companion Volume, Short Pa-
pers, pages 93?96, Rochester, New York, April. As-
sociation for Computational Linguistics.
Kristen Parton, Joel Tetreault, Nitin Madnani, and Mar-
tin Chodorow. 2011. E-rating machine translation.
In Proceedings of the Sixth Workshop on Statistical
Machine Translation, pages 108?115, Edinburgh,
Scotland, July. Association for Computational Lin-
guistics.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-
sos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. 2011. Scikit-learn: Machine Learn-
ing in Python. Journal of Machine Learning Re-
search, 12:2825?2830.
Matt Post. 2011. Judging Grammaticality with Tree
Substitution Grammar Derivations. In Proceedings
of the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 217?222, Portland, Oregon, USA,
June. Association for Computational Linguistics.
Andreas Stolcke. 2002. SRILM - An Extensible Lan-
guage Modeling Toolkit. In 7th International Con-
ference on Spoken Language Processing.
Guihua Sun, Xiaohua Liu, Gao Cong, Ming Zhou,
Zhongyang Xiong, John Lee, and Chin-Yew Lin.
2007. Detecting Erroneous Sentences using Auto-
matically Mined Sequential Patterns. In Proceed-
ings of the 45th Annual Meeting of the Association
of Computational Linguistics, pages 81?88, Prague,
Czech Republic, June. Association for Computa-
tional Linguistics.
Joel R. Tetreault and Martin Chodorow. 2008. The
Ups and Downs of Preposition Error Detection in
ESL Writing. In Proceedings of the 22nd Inter-
national Conference on Computational Linguistics
(Coling 2008), pages 865?872, Manchester, UK,
August. Coling 2008 Organizing Committee.
Joachim Wagner, Jennifer Foster, and Josef van Gen-
abith. 2007. A Comparative Evaluation of Deep
and Shallow Approaches to the Automatic Detec-
tion of Common Grammatical Errors. In Proceed-
ings of the 2007 Joint Conference on Empirical
179
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 112?121, Prague, Czech Republic,
June. Association for Computational Linguistics.
Joachim Wagner, Jennifer Foster, and Josef van Gen-
abith. 2009. Judging grammaticality: Experi-
ments in sentence classification. CALICO Journal,
26(3):474?490.
Sze-Meng Jojo Wong and Mark Dras. 2010. Parser
Features for Sentence Grammaticality Classifica-
tion. In Proceedings of the Australasian Language
Technology Association Workshop 2010, pages 67?
75, Melbourne, Australia, December.
180
Coling 2008: Proceedings of the workshop on Grammar Engineering Across Frameworks, pages 33?40
Manchester, August 2008
Speeding up LFG Parsing Using C-Structure Pruning
Aoife Cahill? John T. Maxwell III? Paul Meurer? Christian Rohrer? Victoria Rose?n?
?IMS, University of Stuttgart, Germany, {cahillae, rohrer}@ims.uni-stuttgart.de
?Palo Alto Research Center, 3333 Coyote Hill Road, Palo Alto, CA 94304, maxwell@parc.com
?Unifob Aksis, Bergen, Norway, paul.meurer@aksis.uib.no
?Unifob Aksis and University of Bergen, Norway, victoria@uib.no
Abstract
In this paper we present a method for
greatly reducing parse times in LFG pars-
ing, while at the same time maintaining
parse accuracy. We evaluate the method-
ology on data from English, German and
Norwegian and show that the same pat-
terns hold across languages. We achieve
a speedup of 67% on the English data and
49% on the German data. On a small
amount of data for Norwegian, we achieve
a speedup of 40%, although with more
training data we expect this figure to in-
crease.
1 Introduction
Efficient parsing of large amounts of natural lan-
guage is extremely important for any real-world
application. The XLE Parsing System is a large-
scale, hand-crafted, deep, unification-based sys-
tem that processes raw text and produces both
constituent structures (phrase structure trees) and
feature structures (dependency attribute-value ma-
trices). A typical breakdown of parsing time
of XLE components with the English grammar
is Morphology (1.6%), Chart (5.8%) and Unifier
(92.6%). It is clear that the major bottleneck in
processing is in unification. Cahill et al (2007)
carried out a preliminary experiment to test the
theory that if fewer c-structures were passed to
the unifier, overall parsing times would improve,
while the accuracy of parsing would remain sta-
ble. Their experiments used state-of-the-art prob-
abilistic treebank-based parsers to automatically
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
mark certain constituents on the input sentences,
limiting the number of c-structures the XLE pars-
ing system would build. They achieved an 18%
speedup in parse times, while maintaining the ac-
curacy of the output f-structures. The experiments
presented in Cahill et al (2007) used the XLE sys-
tem as a black box and did not make any changes to
it. However, the results were encouraging enough
for a c-structure pruning mechanism to be fully in-
tegrated into the XLE system.
The paper is structured as follows: we present
the pruning model that has been integrated into the
XLE system (Section 2), and how it can be ap-
plied successfully to more than one language. We
present experiments for English (Section 3), Ger-
man (Section 4) and Norwegian (Section 5) show-
ing that for both German and English, a significant
improvement in speed is achieved, while the qual-
ity of the f-structures remains stable. For Norwe-
gian a speedup is also achieved, but more training
data is required to sustain the accuracy of the f-
structures. In Section 7 we present an error anal-
ysis on the German data. We then relate the work
presented in this paper to similar efficient parsing
strategies (Section 8) before concluding in Section
9.
2 XLE and the C-Structure Pruning
Mechanism
The XLE system is designed to deal with large
amounts of data in a robust manner. There are
several mechanisms which facilitate this, including
fragmenting and skimming. Fragmenting is called
when the grammar is unable to provide a complete
parse for the input sentence, and a fragment anal-
ysis of largest possible chunks is built. Skimming
is called when too much time or memory has been
used by XLE. Any constituents that have not been
33
fully processed are ?skimmed?, which means that
the amount of work carried out in processing the
constituent is limited. This guarantees that XLE
will finish processing the sentence in polynomial
time.
XLE uses a chart-based mechanism for build-
ing parses, and has been complemented with a c-
structure pruning mechanism to speed up parsing
time. During pruning, subtrees at a particular cell
in the chart are pruned if their probabilities are not
higher than a certain threshold. The chart pruner
uses a simple stochastic CFG model. The proba-
bility of a tree is the product of the probabilities
of each of the rules used to form the tree, includ-
ing the rules that lead to lexical items (such as N
? dog). The probability of a rule is basically the
number of times that that particular form of the
rule occurs in the training data divided by the num-
ber of times the rule?s category occurs in the train-
ing data, plus a smoothing term. This is similar
to the pruning described in Charniak and Johnson
(2005) where edges in a coarse-grained parse for-
est are pruned to allow full evaluation with fine-
grained categories.
The pruner prunes at the level of individual con-
stituents in the chart. It calculates the probabil-
ities of each of the subtrees of a constituent and
compares them. The probability of each subtree
is compared with the best subtree probability for
that constituent. If a subtree?s probability is lower
than the best probability by a given factor, then the
subtree is pruned. In practice, the threshold is the
natural logarithm of the factor used. So a value of
5 means that a subtree will be pruned if its prob-
ability is about a factor of 150 less than the best
probability.
If two different subtrees have different num-
bers of morphemes under them, then the proba-
bility model is biased towards the subtree that has
fewer morphemes (since there are fewer probabil-
ities multiplied together). XLE counteracts this by
normalizing the probabilities based on the differ-
ence in length.
To illustrate how this works, we give the follow-
ing example. The string Fruit flies like bananas has
two different analyses. Figures 1 and 2 give their
analyses along with hypothetical probabilities for
each rule.
These two analyses come together at the S con-
stituent that spans the whole sentence. The proba-
bility of the first analysis is 8.4375E-14. The prob-
S
NP
N
Fruit
N
flies
VP
V
like
NP
N
bananas
S ? NP VP 0.5000
NP ? N N 0.1500
N ? Fruit 0.0010
N ? flies 0.0015
VP ? V NP 0.2000
V ? like 0.0050
NP ? N 0.5000
N ? bananas 0.0015
8.4375E-14
Figure 1: Analysis (1) for the string Fruit flies like
bananas with hypothetical probabilities
S
NP
N
Fruit
VP
V
flies
PP
P
like
NP
N
bananas
S ? NP VP 0.5000
NP ? N 0.5000
N ? Fruit 0.0010
V ? flies 0.0025
VP ? V PP 0.1000
P ? like 0.0500
PP ? P NP 0.9000
NP ? bananas 0.0015
4.21875E-12
Figure 2: Analysis (2) for the string Fruit flies like
bananas with hypothetical probabilities
ability of the second analysis is 4.21875E-12. This
means that the probability of the second analysis
is 50 times higher than the probability of the first
analysis. If the threshold is less than the natural
logarithm of 50 (about 3.9), then the subtree of the
first analysis will be pruned from the S constituent.
3 Experiments on English
We carried out a number of parsing experiments to
test the effect of c-structure pruning, both in terms
of time and accuracy. We trained the c-structure
pruning algorithm on the standard sections of Penn
Treebank Wall Street Journal Text (Marcus et al,
1994). The training data consists of the original
WSJ strings, marked up with some of the Penn
34
Treebank constituent information. We marked up
NPs and SBARs as well as adjective and verbal
POS categories. This is meant to guide the train-
ing process, so that it does learn from parses that
are not compatible with the original treebank anal-
ysis. We evaluated against the PARC 700 Depen-
dency Bank (King et al, 2003), splitting it into 140
sentences as development data and the remaining
unseen 560 for final testing (as in Kaplan et al
(2004)). We experimented with different values
of the pruning cutoff on the development set; the
results are given in Table 1.
The results show that the lower the cutoff value,
the quicker the sentences can be parsed. Using
a cutoff of 4, the development sentences can be
parsed in 100 CPU seconds, while with a cutoff
of 10, the same experiment takes 182 seconds.
With no cutoff, the experiment takes 288 CPU sec-
onds. However, this increase in speed comes at a
price. The number of fragment parses increases,
i.e. there are more sentences that fail to be analyzed
with a complete spanning parse. With no pruning,
the number of fragment parses is 23, while with
the most aggressive pruning factor of 4, there are
39 fragment parses. There are also many more
skimmed sentences with no c-structure pruning,
which impacts negatively on the results. The ora-
cle f-score with no pruning is 83.07, but with prun-
ing (at all thresholds) the oracle f-score is higher.
This is due to less skimming when pruning is acti-
vated, since the more subtrees that are pruned, the
less likely the XLE system is to run over the time
or memory limits needed to trigger skimming.
Having established that a cutoff of 5 performs
best on the development data, we carried out the
final evaluation on the 560-sentence test set using
this cutoff. The results are given in Table 2. There
is a 67% speedup in parsing the 560 sentences, and
the most probable f-score increases significantly
from 79.93 to 82.83. The oracle f-score also in-
creases, while there is a decrease in the random f-
score. This shows that we are throwing away good
solutions during pruning, but that overall the re-
sults improve. Part of this again is due to the fact
that with no pruning, skimming is triggered much
more often. With a pruning factor of 5, there are
no skimmed sentences. There is also one sentence
that timed out with no pruning, which also lowers
the most probable and oracle f-scores.
Pruning Level None 5
Total Time 1204 392
Most Probable F-Score 79.93 82.83
Oracle F-Score 84.75 87.79
Random F-Score 75.47 74.31
# Fragment Parses 96 91
# Time Outs 1 0
# Skimmed Sents 33 0
Table 2: Results of c-structure pruning experi-
ments on English test data
4 Experiments on German
We carried out a similar set of experiments on
German data to test whether the methodology de-
scribed above ported to a language other than En-
glish. In the case of German, the typical time of
XLE components is: Morphology (22.5%), Chart
(3.5%) and Unifier (74%). As training data we
used the TIGER corpus (Brants et al, 2002). Set-
ting aside 2000 sentences for development and
testing, we used the remaining 48,474 sentences as
training data. In order to create the partially brack-
eted input required for training, we converted the
original TIGER graphs into Penn-style trees with
empty nodes and retained bracketed constituents of
the type NP, S, PN and AP. The training data was
parsed by the German ParGram LFG (Rohrer and
Forst, 2006). This resulted in 25,677 full parses,
21,279 fragmented parses and 1,518 parse fail-
ures.1 There are 52,959 features in the final prun-
ing model.
To establish the optimal pruning settings for
German, we split the 2,000 saved sentences into
371 development sentences and 1495 test sen-
tences for final evaluation. We evaluated against
the TiGer Dependency Bank (Forst et al, 2004)
(TiGerDB), a dependency-based gold standard for
German parsers that encodes grammatical rela-
tions similar to, though more fine-grained than,
the ones in the TIGER Treebank as well as mor-
phosyntactic features. We experimented with the
same pruning levels as in the English experiments.
The results are given in Table 3.
The results on the development set show a sim-
ilar trend to the English results. A cutoff of 4 re-
sults in the fastest system, however at the expense
1The reason there are more fragment parses than, for ex-
ample, the results reported in Rohrer and Forst (2006) is that
the bracketed input constrains the parser to only return parses
compatible with the bracketed input. If there is no solution
compatible with the brackets, then a fragment parse is re-
turned.
35
Pruning Level None 4 5 6 7 8 9 10
Oracle F-Score 83.07 84.50 85.47 85.75 85.57 85.57 85.02 84.10
Time (CPU seconds) 288 100 109 123 132 151 156 182
# Time Outs 0 0 0 0 0 0 0
# Fragments 23 39 36 31 29 27 27 24
# Skimmed Sents 8 0 0 1 1 1 1 3
Table 1: Results of c-structure pruning experiments on English development data
Pruning Level None 4 5 6 7 8 9 10
Oracle F-Score 83.69 83.45 84.02 82.86 82.82 82.95 83.03 82.81
Time (CPU seconds) 1313 331 465 871 962 1151 1168 1163
# Time Outs 6 0 0 5 5 5 5 6
# Fragments 65 104 93 81 74 73 73 68
Table 3: Results of c-structure pruning experiments on German development data
Pruning Level None 5
Total Time 3300 1655
Most Probable F-Score 82.63 82.73
Oracle F-Score 84.96 84.79
Random F-Score 73.58 73.72
# Fragment Parses 324 381
# Time Outs 2 2
Table 4: Results of c-structure pruning experi-
ments on German test data
of accuracy. A cutoff of 5 seems to provide the
best tradeoff between time and accuracy. Again,
most of the gain in oracle f-score is due to fewer
timeouts, rather than improved f-structures. In the
German development set, a cutoff of 5 leads to a
speedup of over 64% and a small increase in or-
acle f-score of 0.33 points. Therefore, for the fi-
nal evaluation on the unseen test-set, we choose a
cutoff of 5. The results are given in Table 4. We
achieve a speedup of 49% and a non-significant in-
crease in most probable f-score of 0.094. The time
spent by the system on morphology is much higher
for German than for English. If we only take the
unification stage of the process into account, the
German experiments show a speedup of 65.5%.
5 Experiments on Norwegian
As there is no treebank currently available for Nor-
wegian, we were unable to train the c-structure
pruning mechanism for Norwegian in the same
way as was done for English and German. There
is, however, some LFG-parsed data that has been
completely disambiguated using the techniques
described in Rose?n et al (2006). In total there
are 937 sentences from various text genres includ-
ing Norwegian hiking guides, Sophie?s World and
the Norwegian Wikipedia. We also use this dis-
ambiguated data as a gold standard for evaluation.
The typical time of XLE components with the Nor-
wegian grammar is: Morphology (1.6%), Chart
(11.2%) and Unifier (87.2%).
From the disambiguated text, we can automati-
cally extract partially bracketed sentences as input
to the c-structure pruning training method. We can
also extract sentences for training that are partially
disambiguated, but these cannot be used as part of
the test data. To do this, we extract the bracketed
string for each solution. If all the solutions pro-
duce the same bracketed string, then this is added
to the training data. This results in an average of
4556 features. As the data set is small, we do not
split it into development, training and test sections
as was done for English and German. Instead we
carry out a 10-fold cross validation over the entire
set. The results for each pruning level are given in
Table 5.
The results in Table 5 show that the pattern that
held for English and German does not quite hold
for Norwegian. While, as expected, the time taken
to parse the test set is greatly reduced when using
c-structure pruning, there is also a negative impact
on the quality of the f-structures. One reason for
this is that there are now sentences that could pre-
viously be parsed, and that now no longer can be
parsed, even with a fragment grammar.2 With c-
structure pruning, the number of fragment parses
increases for all thresholds, apart from 10. It is
also difficult to compare the Norwegian experi-
ment to the English and German, since the gold
standard is constrained to only consist of sentences
that can be parsed by the grammar. Theoretically
the oracle f-score for the experiment with no prun-
2With an extended fragment grammar, this would not hap-
pen.
36
Pruning Level None 4 5 6 7 8 9 10
Oracle F-Score 98.76 94.45 95.60 96.40 96.90 97.52 98.00 98.33
Time (CPU seconds) 218.8 106.2 107.4 109.3 112 116.2 124 130.7
# Time Outs 0 0 0 0 0 0 0 0
# Parse Failures 0.2 5.7 3.9 2 3.2 4.2 4.6 4.2
# Fragments 1.3 7.7 6.5 4.7 2.8 1.8 1.5 1.2
Table 5: Results of c-structure pruning 10-fold cross validation experiments on Norwegian data
55
60
65
70
75
80
85
90
95
None 4 5 6 7 8 9 10
Figure 3: The lower-bound results for each of the
10 cross validation runs across the thresholds
ing should be 100. The slight drop is due to a
slightly different morphological analyzer used in
the final experiments that treats compound nouns
differently. A threshold of 10 gives the best results,
with a speedup of 40% and a drop in f-score of 0.43
points. It is difficult to choose the ?best? thresh-
old, as the amount of training data is probably not
enough to get an accurate picture of the data. For
example, Figure 3 shows the lower-bound results
for each of the 10 runs. It is difficult to see a clear
pattern for all the runs, indicating that the amount
of training data is probably not enough for a reli-
able experiment.
6 Size of Training Data Corpus
The size of the Norwegian training corpus is con-
siderably smaller than the training corpora for En-
glish or German, so the question remains how
much training data we need in order for the c-
structure pruning to deliver reliable results. In or-
der to establish a rough estimate for the size of
training corpus required, we carried out an experi-
ment on the German TIGER training corpus.
We randomly divided the TIGER training cor-
pus into sets of 500 sentences. We plot the learn-
ing curve of the c-structure pruning mechanism in
Figure 4, examining the effect of increasing the
size of the training corpus on the oracle f-score on
the development set of 371 sentences. The curve
shows that, for the German data, the highest oracle
f-score of 84.98 was achieved with a training cor-
pus of 32,000 sentences. Although the curve fluc-
tuates, the general trend is that the more training
data, the better the oracle f-score.3
7 Error Analysis
Given that we are removing some subtrees during
parsing, it can sometimes happen that the desired
analysis gets pruned. We will take German as an
example, and look at some of these cases.
7.1 Separable particles vs pronominal
adverbs
The word dagegen (?against it?) can be a separable
prefix (VPART) or a pronominal adverb (PADV).
The verb protestieren (?to protest?) does not take
dagegen as separable prefix. The verb stimmen
(?to agree?) however does. If we parse the sen-
tence in (1) with the verb protestieren and activate
pruning, we do not get a complete parse. If we
parse the same sentence with stimmen as in (2) we
do get a complete parse. If we replace dagegen
by dafu?r, which in the current version of the Ger-
man LFG can only be a pronominal adverb, the
sentence in (3) gets a parse. We also notice that
if we parse a sentence, as in (4), where dagegen
occurs in a position where our grammar does not
allow separable prefixes to occur, we get a com-
plete parse for the sentence. These examples show
that the pruning mechanism has learned to prune
the separable prefix reading of words that can be
both separable prefixes and pronominal adverbs.
(1) Sie
they
protestieren
protest
dagegen.
against-it
?They protest against it.?
(2) Sie
they
stimmen
vote
dagegen.
against-it
?They vote against it.?
3Unexpectedly, the curve begins to decline after 32,000
sentences. However, the differences in f-score are not statis-
tically significant (using the approximate randomization test).
Running the same experiment with a different random seed
results in a similarly shaped graph, but any decline in f-score
when training on more data was not statistically significant at
the 99% level.
37
32000, 84.97698
84
84.1
84.2
84.3
84.4
84.5
84.6
84.7
84.8
84.9
85
50
0
20
00
35
00
50
00
65
00
80
00
95
00
11
00
0
12
50
0
14
00
0
15
50
0
17
00
0
18
50
0
20
00
0
21
50
0
23
00
0
24
50
0
26
00
0
27
50
0
29
00
0
30
50
0
32
00
0
33
50
0
35
00
0
36
50
0
38
00
0
39
50
0
41
00
0
42
50
0
44
00
0
45
50
0
47
00
0
48
50
0
Number of Training Sentences
F-
Sc
o
re
Figure 4: The effect of increasing the size of the training data on the oracle f-score
(3) Er
he
protestiert
protests
dafu?r.
for-it
?He protests in favour of it.?
(4) Dagegen
against-it
protestiert
protests
er.
he
?Against it, he protests.?
7.2 Derived nominal vs non-derived nominal
The word Morden can be the dative plural of the
noun Mord (?murder?) or the nominalized form of
the verb morden (?to murder?). With c-structure
pruning activated (at level 5), the nominalized
reading, as in (6), gets pruned, whereas the dative
plural reading is not (5). At pruning level 6, both
readings are assigned a full parse. We see simi-
lar pruning of nominalized readings as in (7). If
we look in more detail at the raw counts for re-
lated subtrees gathered from the training data, we
see that the common noun reading for Morden oc-
curs 156 times, while the nominalized reading only
occurs three times. With more training data, the c-
structure pruning mechanism could possibly learn
when to prune correctly in such cases.
(5) Er
he
redet
speaks
von
of
Morden.
murders
?He speaks of murders.?
(6) Das
the
Morden
murdering
will
wants
nicht
not
enden.
end
?The murdering does not want to end.?
(7) Das
the
Arbeiten
working
endet.
ends
?The operation ends.?
7.3 Personal pronouns which also function as
determiners
There are a number of words in German that can
function both as personal pronouns and determin-
ers. If we take, for example, the word ihr, which
can mean ?her?, ?their?, ?to-her?, ?you-pl? etc.,
the reading as a determiner gets pruned as well as
some occurrences as a pronoun. In example (8),
we get a complete parse for the sentence with the
dative pronoun reading of ihr. However, in ex-
ample (9), the determiner reading is pruned and
we fail to get a complete parse. In example (10),
we also fail to get a complete parse, but in exam-
ple (11), we do get a complete parse. There is a
parameter we can set that sets a confidence value
in certain tags. So, for example, we set the con-
fidence value of INFL-F BASE[det] (the tag given
to the determiner reading of personal pronouns) to
be 0.5, which says that we are 50% confident that
the tag INFL-F BASE[det] is correct. This results in
38
examples 8, 9 and 11 receiving a complete parse,
with the pruning threshold set to 5.
(8) Er
he
gibt
gives
es
it
ihr.
her
?He gives it to her.?
(9) Ihr
her/their
Auto
car
fa?hrt.
drives
?Her/Their car drives.
(10) Ihr
you(pl)
kommt.
come
?You come.?
(11) Er
he
vertraut
trusts
ihr.
her
?He trusts her.?
7.4 Coordination of Proper Nouns
Training the German c-structure pruning mecha-
nism on the TIGER treebank resulted in a pecu-
liar phenomenon when parsing coordinated proper
nouns. If we parse four coordinated proper nouns
with c-structure pruning activated as in (12), we
get a complete parse. However, as soon as we add
a fifth proper noun as in (13), we get a fragment
parse. This is only the case with proper nouns,
since the sentence in (14) which coordinates com-
mon nouns gets a complete parse. Interestingly, if
we coordinate n proper nouns plus one common
noun, we also get a complete parse. The reason for
this is that proper noun coordination is less com-
mon than common noun coordination in our train-
ing set.
(12) Hans, Fritz, Emil und Maria singen.
?Hans, Fritz, Emil and Maria sing.?
(13) Hans, Fritz, Emil, Walter und Maria sin-
gen.
?Hans, Fritz, Emil, Walter and Maria sing.?
(14) Hunde, Katzen, Esel, Pferde und Affen
kommen.
?Dogs, cats, donkeys, horses and apes
come.?
(15) Hans, Fritz, Emil, Walter, Maria und
Kinder singen.
?Hans, Fritz, Emil, Walter, Maria and chil-
dren sing.?
We ran a further experiment to test what effect
adding targeted training data had on c-structure
pruning. We automatically extracted a specialized
corpus of 31,845 sentences from the Huge Ger-
man Corpus. This corpus is a collection of 200
million words of newspaper and other text. The
sentences we extracted all contained examples of
proper noun coordination and had been automati-
cally chunked. Training on this sub-corpus as well
as the original TIGER training data did have the
desired effect of now parsing example (13) with
c-structure pruning activated.
8 Related Work
Ninomiya et al (2005) investigate beam threshold-
ing based on the local width to improve the speed
of a probabilistic HPSG parser. In each cell of a
CYK chart, the method keeps only a portion of the
edges which have higher figure of merits compared
to the other edges in the same cell. In particular,
each cell keeps the edges whose figure of merit is
greater than ?
max
- ?, where ?
max
is the high-
est figure of merit among the edges in the chart.
The term ?beam thresholding? is a little confusing,
since a beam search is not necessary ? instead, the
CYK chart is pruned directly. For this reason, we
prefer the term ?chart pruning? instead.
Clark and Curran (2007) describe the use of
a supertagger with a CCG parser. A supertag-
ger is like a tagger but with subcategorization in-
formation included. Chart pruners and supertag-
gers are conceptually complementary, since chart
pruners prune edges with the same span and the
same category, whereas supertaggers prune (lexi-
cal) edges with the same span and different cate-
gories. Ninomiya et al (2005) showed that com-
bining a chunk parser with beam thresholding pro-
duced better results than either technique alone. So
adding a supertagger should improve the results
described in this paper.
Zhang et al (2007) describe a technique to
selectively unpack an HPSG parse forest to ap-
ply maximum entropy features and get the n-best
parses. XLE already does something similar when
it applies maximum entropy features to get the
n-best feature structures after having obtained a
packed representation of all of the valid feature
structures. The current paper shows that pruning
the c-structure chart before doing (packed) unifica-
tion speeds up the process of getting a packed rep-
resentation of all the valid feature structures (ex-
cept the ones that may have been pruned).
39
9 Conclusions
In this paper we have presented a c-structure prun-
ing mechanism which has been integrated into the
XLE LFG parsing system. By pruning the number
of c-structures built in the chart, the next stage of
processing, the unifier, has considerably less work
to do. This results in a speedup of 67% for En-
glish, 49% for German and 40% for Norwegian.
The amount of training data for Norwegian was
much less than that for English or German, there-
fore further work is required to fully investigate
the effect of c-structure pruning. However, the re-
sults, even from the small training data, were en-
couraging and show the same general patterns as
English and German. We showed that for the Ger-
man training data, 32,000 sentences was the opti-
mal number in order to achieve the highest oracle
f-score. There remains some work to be done in
tuning the parameters for the c-structure pruning,
as our error analysis shows. Of course, with sta-
tistical methods one can never be guaranteed that
the correct parse will be produced; however we can
adjust the parameters to account for known prob-
lems. We have shown that the c-structure pruning
mechanism described is an efficient way of reduc-
ing parse times, while maintaining the accuracy of
the overall system.
Acknowledgements
The work presented in this paper was supported
by the COINS project as part of the linguistic
Collaborative Research Centre (SFB 732) at the
University of Stuttgart and by the Norwegian Re-
search Council through the LOGON and TREPIL
projects.
References
Brants, Sabine, Stefanie Dipper, Silvia Hansen, Wolf-
gang Lezius, and George Smith. 2002. The TIGER
Treebank. In Proceedings of the Workshop on Tree-
banks and Linguistic Theories, Sozopol, Bulgaria.
Cahill, Aoife, Tracy Holloway King, and John T.
Maxwell III. 2007. Pruning the Search Space of
a Hand-Crafted Parsing System with a Probabilistic
Parser. In ACL 2007 Workshop on Deep Linguistic
Processing, pages 65?72, Prague, Czech Republic,
June. Association for Computational Linguistics.
Charniak, Eugene and Mark Johnson. 2005. Coarse-
to-Fine n-Best Parsing and MaxEnt Discriminative
Reranking. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguis-
tics (ACL?05), pages 173?180, Ann Arbor, Michi-
gan, June. Association for Computational Linguis-
tics.
Clark, Stephen and James R. Curran. 2007. Wide-
Coverage Efficient Statistical Parsing with CCG and
Log-Linear Models. Computational Linguistics,
33(4):493?552.
Forst, Martin, Nu?ria Bertomeu, Berthold Crysmann,
Frederik Fouvry, Silvia Hansen-Schirra, and Valia
Kordoni. 2004. Towards a dependency-based gold
standard for German parsers ? The TiGer Depen-
dency Bank. In Proceedings of the COLING Work-
shop on Linguistically Interpreted Corpora (LINC
?04), Geneva, Switzerland.
Kaplan, Ronald M., John T. Maxwell, Tracy H. King,
and Richard Crouch. 2004. Integrating Finite-state
Technology with Deep LFG Grammars. In Pro-
ceedings of the ESSLLI 2004 Workshop on Combin-
ing Shallow and Deep Processing for NLP, Nancy,
France.
King, Tracy Holloway, Richard Crouch, Stefan Riezler,
Mary Dalrymple, and Ronald M. Kaplan. 2003. The
PARC 700 Dependency Bank. In Proceedings of the
EACL Workshop on Linguistically Interpreted Cor-
pora (LINC ?03), Budapest, Hungary.
Marcus, Mitchell P., Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1994. Building a Large Annotated
Corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2):313?330.
Ninomiya, Takashi, Yoshimasa Tsuruoka, Yusuke
Miyao, and Jun?ichi Tsujii. 2005. Efficacy of Beam
Thresholding, Unification Filtering and Hybrid Pars-
ing in Probabilistic HPSG Parsing. In Proceed-
ings of the Ninth International Workshop on Pars-
ing Technology, pages 103?114, Vancouver, British
Columbia, October. Association for Computational
Linguistics.
Rohrer, Christian and Martin Forst. 2006. Improving
Coverage and Parsing Quality of a Large-scale LFG
for German. In Proceedings of the Language Re-
sources and Evaluation Conference (LREC-2006),
Genoa, Italy.
Rose?n, Victoria, Paul Meurer, and Koenraad de Smedt.
2006. Towards a Toolkit Linking Treebanking and
Grammar Development. In Hajic, Jan and Joakim
Nivre, editors, Proceedings of the Fifth Workshop
on Treebanks and Linguistic Theories, pages 55?66,
December.
Zhang, Yi, Stephan Oepen, and John Carroll. 2007.
Efficiency in Unification-Based N-Best Parsing. In
Proceedings of the Tenth International Conference
on Parsing Technologies, pages 48?59, Prague,
Czech Republic, June. Association for Computa-
tional Linguistics.
40
Proceedings of the 2010 Workshop on NLP and Linguistics: Finding the Common Ground, ACL 2010, pages 34?42,
Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational Linguistics
A Cross-Lingual Induction Technique for German Adverbial Participles
Sina Zarrie? Aoife Cahill Jonas Kuhn Christian Rohrer
Institut fu?r Maschinelle Sprachverarbeitung (IMS)
University of Stuttgart
Stuttgart, Germany
{zarriesa,cahillae,jonas.kuhn,rohrer}@ims.uni-stuttgart.de
Abstract
We provide a detailed comparison of
strategies for implementing medium-to-
low frequency phenomena such as Ger-
man adverbial participles in a broad-
coverage, rule-based parsing system. We
show that allowing for general adverb con-
version of participles in the German LFG
grammar seriously affects its overall per-
formance, due to increased spurious am-
biguity. As a solution, we present a
corpus-based cross-lingual induction tech-
nique that detects adverbially used par-
ticiples in parallel text. In a grammar-
based evaluation, we show that the auto-
matically induced resource appropriately
restricts the adverb conversion to a limited
class of participles, and improves parsing
quantitatively as well as qualitatively.
1 Introduction
In German, past perfect participles are ambigu-
ous with respect to their morphosyntactic cate-
gory. As in other languages, they can be used
as part of the verbal complex (example (1-a)) or
as adjectives (example (1-b)). Since German ad-
jectives can generally undergo conversion into ad-
verbs, participles can also be used adverbially (ex-
ample (1-c)). All three participle forms in (1) are
morphologically identical.
(1) a. Das Experiment hat ihn begeistert.
?The experiment has enthused him.?
b. Er scheint von dem Experiment begeistert.
?He seems enthusiastic about the experiment.?
c. Er hat begeistert experimentiert.
?He has experimented in an enthusiastic way? or:
?He was enthusiastic when he experimented.?
This paper adresses the question of how to deal
with medium-to-low frequency phenomena such
as adverbial participles in a broad-coverage, rule-
based parsing system. In order to account for sen-
tences like (1-c), an intuitive approach would be to
generally allow for adverb conversion of partici-
ples in the grammar. However, on the basis of the
German LFG grammar (Rohrer and Forst, 2006),
we show that such a rule can have a strong negative
on the overall performance of the parsing system,
despite the fact that it produces the desired syntac-
tic and semantic analysis for specific sentences.
This trade-off between large-scale, statistical
and theoretically precise coverage is often en-
countered in engineering broad-coverage and, at
the same time, linguistically motivated parsing
systems: adding the analysis for a specific phe-
nomenon does not necessarily improve the overall
quality of the system since the rule might overgen-
erate and interact with completely different phe-
nomena in unpredicted ways.
In principle, there are two ways of dealing with
such an overgeneration problem in a grammar-
based framework: First, one could hand-craft
word lists or other linguistic constraints that re-
strict the adverb conversion to a certain set of par-
ticiples. Second, one could try to mine corpora for
this particular type of adverbs and integrate this
automatically induced knowledge into the gram-
mar (i.e. by means of pre-tagged input, word lists,
etc.). In the case of adverbial participles, both
ways are prone with difficulties. To our knowl-
edge, there has not been much theoretical work on
the linguistic properties of the participle adverb
conversion. Moreover, since the distinction be-
tween (predicative) adjectives and adverbs is the-
oretically hard to establish, the standard tag set
for German and, in consequence, annotated cor-
pora for German do not explicitly capture this phe-
nomenon. Thus, available statistical taggers and
parsers for German usually conflate the syntactic
structures underlying (1-b) and (1-c).
In this paper, we present a corpus-based ap-
proach to restricting the overgenerating adverb
conversion for participles in German, exploiting
34
parallel corpora and cross-lingual NLP induc-
tion techniques. Since adverbs are often overtly
marked in other languages (i.e. the ly-suffix in
English), adverbial participles can be straightfor-
wadly detected on word-aligned parallel text. We
describe the ingretation of the automatically in-
duced resource of adverbial participles into the
German LFG, and provide a detailed evaluation of
its effect on the grammar, see Section 5.
While the use of parallel resources is rather
familiar in a wide range of NLP domains, such
as statistical machine translation (Koehn, 2005)
or annotation projection (Yarowsky et al, 2001),
our work shows that they can be exploited for
very specific problems that arise in deep linguis-
tic analysis (see Section 4). In this way, high-
precision, data-oriented induction techniques can
clearly improve rule-based system development
through combining the benefits of high empirical
accuracy and little manual effort.
2 A Broad-Coverage LFG for German
Lexical Functional Grammar (LFG) (Bresnan,
2000) is a constraint-based theory of grammar. It
posits two levels of representation, c(onstituent)-
structure and f(unctional)- structure. C-structure
is represented by contextfree phrase-structure
trees, and captures surface grammatical configu-
rations. F-structures approximate basic predicate-
argument and adjunct structures.
The experiments reported in this paper use the
German LFG grammar constructed as part of the
ParGram project (Butt et al, 2002). The grammar
is implemented in the XLE, a grammar develop-
ment environment which includes a very efficient
LFG parser. Within the spectrum of appraoches
to natural language parsing, XLE can be consid-
ered a hybrid system combining a hand-crafted
grammar with a number of automatic ambiguity
management techniques: (i) c-structure pruning
where, based on information from statstically ob-
tained parses, some trees are ruled out before f-
structure unification (Cahill et al, 2007), (ii) an
Optimaly Theory-style constraint mechanism for
filtering and ranking competing analyses (Frank
et al, 2001), and (iii) a stochastic disambiguation
component which is based on a log-linear proba-
bility model (Riezler et al, 2002) and works on
the packed representations.
The German LFG grammar integrates a mor-
phological component which is a variant of
DMOR1 (Becker, 2001). This means that the (in-
ternal) lexicon does not comprise entries for sur-
face word forms, but entries for specific morpho-
logical tags, see (Dipper, 2003).
3 Participles in the German LFG
3.1 Analysis
The morphosyntactic ambiguity of German par-
ticiples presents a notorious difficulty for theoreti-
cal and computational analysis. The reason is that
adjectives (i.e. adjectival participles) do not only
occur as attributive modifiers (shown in (1-a)), but
can also be used as predicatives (see (2-b)). These
predicatives have exactly the same form as ver-
bal or adverbial participles (compare the three sen-
tences in (2)). Predicatives do appear either as ar-
guments of verbs like seem or as free adjuncts such
that they are not even syntactically distinguishable
from adverbs. The sentence in (2-c) is thus am-
biguous as to whether the participle is an adverb
modifying the main verb, or a predicative which
modifies the subject. Especially in the case of
modifiers refering to a psychological state, the two
underlying readings are hard to tell apart (Geuder,
2004). It is due to the lack of reliable semantic
tests that the standard German tag set (Schiller et
al., 1995) assigns the tag ?ADJD? to predicative
adjectives as well as adverbs.
(2) a. Das Experiment hat ihn begeistert.
?The experiment has enthused him.?
b. Er scheint von dem Experiment begeistert.
?He seems enthusiastic about the experiment.?
c. Er hat begeistert experimentiert.
?He has experimented in an enthusiastic way? or:
?He was enthusiastic when he experimented.?
For performance reasons, the German LFG does
not cover free predicatives at the moment. In the
context of our crosslingual induction approach,
the distinction between predicatives and adverbs
is rather straigtforward since we base our experi-
ments on languages that have morphologically dis-
tinct forms for these categories. In the follow-
ing, we will thus limit the discussion to adverbial
participles and ignore the complexities related to
predicative participles.
In the German LFG, the treatment of a given
participle form is closely tight to the morphologi-
cal analysis encoded in DMOR. In particular, ad-
verbial participles can have different degrees of
lexicalisation. For bestimmt (probably) in (3-a),
which is completely lexicalised, the morphology
35
proposes two analyses: (i) a participle tag of the
verbal lemma bestimmen (determine) and (ii) an
adverb tag for the lemma bestimmt. In this case,
the LFG parsing algorithm will figure out which
morphological analysis yields a syntactically well-
formed analysis. For gezielt (purposeful) in (3-b),
DMOR outputs, besides the participle analysis, an
adjective tag for the lemma. However, the gram-
mar can turn it into an adverb by a general ad-
verb conversion rule for adjectives. The difficult
case for the German LFG grammar is illustrated in
(3-c) by means of the adverbial participle wieder-
holt (repeatedly). This participle is neither lexi-
calised as an adverb nor as an adjective, but it still
can be used as an adverb.
(3) a. Bestimmt
Probably
ist
is
dieser
the
Mann
man
sehr
very
traurig.
sad.
b. Der
The
Mann
man
hat
has
gezielt
acted
gehandelt.
purposefully.
c. Der
The
Mann
man
hat
has
wiederholt
repeatedly
geweint.
cried.
To cover sentences like (3-c), the grammar
needs to include a rule that allows adverb conver-
sion for participles. Unfortunately, this rule is very
costly in terms of the overall performance of the
grammar, as is shown in the following section.
3.2 Assessing the Effect of Participle
Ambiguity on the German LFG
In this section, we want to illustrate the effect of
one specific grammar rule, i.e. the rule that gener-
ally allows for conversion of participles into ad-
verbs. We perform a contrastive evaluation of
two versions of the grammar: (i) the No-Part-Adv
version which does not allow for adverb conver-
sion (except for the lexicalised participles from
DMOR), (ii) the All-Part-Adv version which al-
lows every participle to be analysed as adverb.
Otherwise, the two versions of the grammar are
completely identical.
The comparison between the All-Part-Adv and
No-Part-Adv grammar version pursues two major
goals: On the one hand, we want to assess their
overall quantitative performance on representative
gold standard data, as it is common practice for
statistical parsing systems. On the other hand, we
are interested in getting a detailed picture of the
quality of the grammar for parsing adverbial par-
ticiples. These two goals do not necessarily go to-
gether since we know that the phenomenon is not
very frequent in the data which we use for evalu-
ation. Therefore, we do not only report accuracy
on gold standard data in the following, but also fo-
cus on error analysis and describe ways of qualti-
tatively assessing the grammar performance.
For evaluation, we use the TIGER treebank
(Brants et al, 2002). We report grammar per-
formance on the development set which consists
of the first 5000 TIGER sentences, and statistical
accuracy on the standard heldout set which com-
prises 371 sentences.
Quantitative Evaluation We first want to assess
the quantitative impact of the phenomenon of ad-
verbial participles in our evaluation data. We parse
the heldout set storing all possible analyses ob-
tained by both grammars, in order to compare the
upperbound score that the both versions can op-
timally achieve (i.e. independently of the disam-
biguation quality). Then, we run the XLE eval-
uation in the ?oracle? mode which means that the
disambiguation compares all system analyses for a
given sentence to its gold analysis, and chooses the
best system analysis for computing accuracy. The
upperbound f-score for both grammar versions is
almost identical (at about 83.6%). This suggests
that the phenomenon of adverbial participles does
not occur in the heldout set.
If we run the grammar versions on a larger
set of sentences, the difference in coverage be-
comes more obvious. In Table 1, we report the
absolute number of parsed sentences, starred sen-
tences (only receiving a partial or fragment parse),
and the timeouts 1 on our standard TIGER devel-
opment set. Not very surprisingly, the coverage
of the All-Part-Adv version seems to be broader.
However, this does not necessarily mean that the
40 additionally covered sentences all exhibit ad-
verbial participles (see below). Moreover, Table 2
gives a first indication of the fact that the extended
coverage comes at a price: the All-Part-Adv ver-
sion massively increases the number of ambigui-
ties per sentence. Related to this, in the All-Part-
Adv version, the number of timeouts increases by
16% and parsing speed goes down by 6% com-
pared to the No-Part-Adv version.
To assess the effect of the massively increased
ambiguity rate and the bigger proportion of time-
outs in All-Part-Adv, we perform a statistical eval-
uation of the two versions of the grammar against
the heldout set, i.e. we compute f-score based
1Sentences whose parsing can not be finished in prede-
fined amount of time, the maximally allowed parse time is
set to 20 seconds.
36
Grammar Parsed
Sent.
Starred
Sent.
Time-
outs
Time
in sec
No-Part-Adv 4301 608 90 6853
All-Part-Adv 4339 555 105 7265
Table 1: Coverage-based evaluation on the TIGER
development set (sentences 1-5000), 4999 sen-
tences total
Sent. Av. ambiguities per sent. Av.
length No-Part-Adv All-Part-Adv Incr.
1-10 2.95 3.3 11%
11-20 24.99 36.09 44%
21-30 250.4 343.76 37%
31-40 1929.06 2972.847 54%
41-50 173970.0 663310.4 429%
Table 2: Average number of ambiguities per sen-
tence
on the parses that the XLE disambiguation selects
as the most probable parse. Both versions use
the same disambiguation model which results in
a slightly biased comparison but still reflects the
effect of increased ambiguity on the disambigua-
tion component. In Table 3, we can see that the
All-Part-Adv version performs significantly worse
than the grammar version which does not cap-
ture adverbial participles. The spurious ambigu-
ities and timeouts produced in All-Part-Adv have
such a strong negative impact on the disambigua-
tion component that it can not be outweighed by
the extended coverage of the grammar.
Qualitative Evaluation The fact that the All-
Part-Adv version generally increases parse ambi-
guity suggests that it produces a lot of undesired
analyses for constructions not related to adverbial
participles. To assess this assumption, we drew a
random sample of 20 sentences out of the addi-
tionally covered 41 sentences and checked manu-
ally whether these contained an adverbial partici-
ple: Only 40% of these sentences are actually cor-
rectly analysed. In all other cases, the grammar
lacks an analysis for a completely different phe-
Grammar Prec. Rec. F-Sc. Time
in sec
All-Part-Adv 83.80 76.71 80.1 666.55
No-Part-Adv 84.25 78.3 81.17 632.21
Table 3: Evaluation on the TIGER heldout set, 371
sentences total
nomenon (mostly related to coordination), but ob-
tains an (incorrect) analysis on the basis of the ad-
verb conversion rule.
As an example, Figure 1 presents two c-
structure analyses for the sentence in (4) in the
All-Part-Adv grammar. In the second c-structure
(CS2), the participle kritisiert (criticised) is anal-
ysed as adverb modifing the main verb haben
(have). This results in a very strange underlying f-
structure, meaning something like the Greens pos-
sess the SPD in a criticising manner.
(4) Die
The
Gru?nen
Greens
haben
have
die
the
SPD
SPD
kritisiert.
criticised.
?The Greens have criticised the SPD?
3.3 Interim Conclusion
This section has illustrated an exemplary dilemma
for parsing systems that aim broad-coverage and
linguisitically motivated analyses at the same time.
Since these systems need to explicitly address and
represent ambiguities that purely statistical sys-
tems are able to conflate or ignore, their perfor-
mance is not automatically improved by adding
a specific rule for a specific phenomenon. Inter-
estingly, the negative consequences affecting the
quantitative (statistical) as well as the qualitative
(linguistic) dimension of the grammar seem to be
closely related: The overgenerating adverb con-
version rule empirically leads to linguistically un-
motivated analyses which causes problems for the
disambiguation component. In the rest of the pa-
per, we show how the adverbial analysis of partici-
ples can be reasonably constrained on the basis of
a lexical resource induced from a parallel corpus.
4 Cross-Lingual Induction of Adverbial
Participles
The intuition of the cross-lingual induction ap-
proach is that adverbial participles can be easily
extracted from parallel corpora since in other lan-
guages (such as English or French) adverbs are
often morphologically marked and easily labelled
by statistical PoS taggers. As an example, con-
sider the sentence in (5), extracted from Europarl,
where the German participle versta?rkt is translated
by unambiguous adverbs in English and French
(increasingly and davantage).
(5) a. Nach der Osterweiterung stehen die Zeichen
versta?rkt auf Liberalisierung.
b. Following enlargement towards the east, the emphasis
is increasingly on liberalisation.
37
CS 1: ROOT:2543
CProot[std]:2536
DP[std]:984
DPx[std]:981
D[std]:616
die:34
NP:773
N[comm]:717
NAdj:714
Gr?nen:85
Cbar:2506
Vaux[haben,fin]:1054
haben:159
VP[v,part]:2080
DP[std]:1856
DPx[std]:2321
D[std]:1180
die:204
NP:1720
N[comm]:284
SPD:257
VC[v,part]:2009
V[v,part]:1593
Vx[v,part]:1590
kritisiert:348
PERIOD:418
.:410
CS 2: ROOT:2543
CProot[std]:2536
DP[std]:984
DPx[std]:981
D[std]:616
die:34
NP:773
N[comm]:717
NAdj:714
Gr?nen:85
Cbar:2506
V[v,fin]:2494
Vx[v,fin]:2491
haben:159
DP[std]:1856
DPx[std]:2321
D[std]:1180
die:204
NP:1720
N[comm]:284
SPD:257
ADVP[std]:1493
V[v,-infl]:1491
Vx[v,-infl]:1488
kritisiert:348
PERIOD:418
.:410
Figure 1: Two c-structures for sentence (4), obtained by the grammar All-Part-Adv - CS1 is correct, CS2
is semantically very strange
c. Apre`s l? e?largissement a` l? Est, la tendance sera da-
vantage a` la libe?ralisation.
In the following, we describe experiments on
Europarl where we automatically extract and fil-
ter adverbially translated German participles.
4.1 Data
We base our experiments on the German, En-
glish, French and Dutch part of the Europarl cor-
pus. We automatically word-aligned the German
part to each of the others with the GIZA++ tool
(Och and Ney, 2003). Note that, due to diver-
gences in sentence alignment and tokenisation,
the three word-alignments are not completely syn-
chronised. Moreover, each of the 4 languages has
been automatically PoS tagged using the TreeTag-
ger (Schmid, 1994). In addition, the German and
English parts have been parsed with MaltParser
(Nivre et al, 2006).
Since we want to limit our investigation to those
participles that are not already recorded as lexi-
calised adjective or adverb in the DMOR morphol-
ogy, we first have to generate the set of participle
candidates from the tagged Europarl data. We ex-
tract all distinct words (types) from the German
part that have been either tagged as ADJD (pred-
icative or adverbial modifier), 6089 types in total,
or as VVPP (past perfect participle), 5469 types
in total. We intersect this set of potential partici-
ples with the set of DMOR participles that only
have a verbal lemma. The resulting intersection
(5054 types in total) constitutes the set of all Ger-
man participles in Europarl that are not recorded
as lexicalised in the DMOR morphology .
Given the participle candidates, we now ex-
tract the set of sentences that exhibit a word
alignment between a German participle and an
English, French or Dutch adverb. The extrac-
tion yields 5191 German-English sentence pairs,
2570 German-French, and 4129 German-Dutch
sentence pairs. The German-English pairs com-
prise 1070 types of potentially adverbial partici-
ples. The types found in the German-French and
German-Dutch part form a proper subset of the
types extracted from the German-English pairs.
Thus, the additional languages will not increase
the recall of the induction. However, we will show
that they are extremely useful for filtering incor-
rect or uninteresting participle alignments.
For data exploration and evaluation, we anno-
tated 300 participle alignments out of the 5191
German-English sentences as to whether the En-
glish adverbial really points to an adverbial par-
ticiple on the German side (and/or the word-
alignment was correct). Throughout the entire set
of annotated sentences, this ratio between the par-
allel cases (where an English adverbial correctly
indicates a German adverbial) and all adverbially
translated participles is at about 30%. This means
that if we base the induction on word-alignments
alone, its precision would be relatively low.
The remaining 60% translation pairs do not only
reflect word alignment errors, but also cases where
we find a proper participle in the German sentence
that has a correct adverbial translation for other
reasons. A typical configuration is exemplified in
(6) where the German main verb vorlegen is trans-
lated as the verb-adverb combination put forward.
(6) a. Wir haben eine Reihe von Vorschla?gen vorgelegt.
b. We have put forward a number of proposals.
These sentence pairs are cases of free or para-
38
Figure 2: Type/token ratio for adverbial participles
phrasing translations. Ideally, we want our induc-
tion method to filter such type of configurations.
The 300 annotated sentences comprise 121 to-
ken instances of German adverbially used partici-
ples that have an adverbial translation in English.
However, these 121 tokens reduce to 24 partici-
ple types. The graph in Figure 2 displays the
type/token-ratio for an increasing number of in-
stances in our gold standard. The curve exponen-
tially decays from about 10 tokens onward and
suggests that from about 30 tokens onward, the
number of unseen types is relatively low. This can
be interpreted as evidence in favour of the hypoth-
esis that the number of adverbially used participles
is actually fairly limited and can be integrated into
the grammar in terms of a hard-coded resource.
4.2 Filtering
The data analysis in the previous section has
shown that approximately one third of the English
adverb alignments actually point to an adverbial
participle on the German side. This means that we
have to rigorously filter the data that we extract on
the basis of word-alignments in order to obtain a
high quality resource for our grammar. In this sec-
tion, we will investigate several filtering methods
and evaluate them on our annotated sentence pairs.
Frequency-based filtering As a first attempt,
we filtered the non-parallel cases in our set of
participle-adverb translations by means of the rel-
ative frequency of the adverb translations. For
each participle candidate, we counted the number
of tokens that exhibit an adverbial alignment on
the English side, and divided this number by its
total number of occurrences in the German Eu-
roparl. The best f-score of the ADV-FREQ filter
(see Table 4) is achieved by the 0.05 threshold, but
generally, the precision of the frequency filters is
too low for high-quality resource induction. The
reason for the poor performance of the frequency-
based filters seems to be that some German verbs
are systematically translated as verb - adverb com-
binations as in (6). For these participles, the rel-
ative frequency of adverbial alignments is not a
good indicator for their adverbial use in German.
Multilingual Filtering Similar to filters used
in annotation projection where noisy word-
alignments are ?cleaned? with the help of addi-
tional languages (Bouma et al, 2008), we have
implemented a filter that only selects those par-
ticiples as adverbials which also exhibit a certain
amount of adverbial translations in the French and
Dutch Europarl. We count the total number of
adverbial translations of a given participle on the
French side and divide it by the number of English
adverbial translations. For French, the best f-score
is achieved at a threshold of >0.1 (filter FR). For
Dutch, the best f-score is achieved at a threshold
of >0.05 (filter NL). The exact precision and re-
call values are given in Table 4.
Syntax-based Filtering The intuition behind
the filters presented in this section is that adver-
bial translations which are due to cross-lingual di-
vergences can be identified on the basis of their
syntactic contexts. Information about these con-
texts can be extracted from the dependency anal-
yses produced by MaltParser for the German and
English data. On the German side, we want to ex-
clude those participle instances for which the Ger-
man parser has found an auxiliary head, since this
configuration points to a normal partciple context
in German. The filter is called G-HEAD in Table
4. It filters all types which have an auxiliary head
in more than 40% of their adverbial translation
configurations. On the English side, we exclude
all translations where the adverb has a verbal head
which is also aligned to the German partciple. The
filter is called E-HEAD in Table 4. It excludes all
participle types which exhibit the E-HEAD con-
figuration in more than 50% of the cases.
39
filter prec. rec. f-sc.
ADV-FREQ 0.38 0.75 0.51
FR 0.48 0.76 0.58
NL 0.33 0.73 0.45
G-HEAD 0.65 0.8 0.71
E-HEAD 0.4 0.8 0.53
COMBINED-1 0.61 0.8 0.69
COMBINED-2 0.86 0.76 0.81
Table 4: Performance of filters on the set of gold
adverbial participle types
Combined Token-level Filtering So far, we
have shown that multilingual and syntactic in-
formation is useful to filter non-parallel partici-
ple translations. We have found that the pre-
cision of the syntactic filters can still be in-
creased by combining it with the multilingual fil-
ters. COMBINED-1 in Table 4 refers to the filter
which only includes those participle types which
have at least one adverbial translation on the En-
glish target side such that (i) the adverbial trans-
lation is paralleled on the French or Dutch target
side for the same German participle token and (ii)
the German participle token does not have an aux-
iliary head. If we combine this token-level filter-
ing with the syntactic type-level filtering G-HEAD
and E-HEAD (the filter called COMBINED-2 in
Table 4), the precision increases by about 25%
with little loss in recall.
4.3 Analysis
Based on the filtering techniques described in the
previous section, we can finally induce a list of 46
German adverbial participles from Europarl. The
fact that this participle class seems fairly delimited
in our data raises the theoretical question whether
the adverb conversion is licensed by any linguistic,
i.e. lexical-semantic, properties of these partici-
ples. However, we observe that the automatically
induced list comprises very diverse types of ad-
verbs, as well as very distinct types of underlying
verbs. Thus, besides adverbs that clearly modify
events (see sentence (5)), we also found adverbs
that are more likely to modify adjectives (sentence
(7-a)), or propositions (sentence (7-b)).
(7) a. Es ist eine verdammt gefa?hrliche Situation.
?It is a damned dangerous situation.?
b. Wir machen einen Bericht u?ber den Bericht des Rech-
nungshofes , zugegeben.
?We are drafting a report about the report of the Court
of Auditors , admittedly.?
A more fine-grained classification and analysis
of adverbial participles is left for future research.
5 Grammar-based Evaluation
The resource of participles licensing adverbial use,
whose induction was described in the previous
section, can be straightforwardly integrated into
the German LFG. By explicitly enumerating the
participles in the adverb lexicon, the grammar can
apply the standard adverb macros to them. To as-
sess the effect of the filtering, we built two new
versions of the grammar: (i) Euro-Part-Adv, its ad-
verb lexicon comprises all adverbially translated
participles found in Europarl (1091 types) and (ii)
Filt-Part-Adv, its adverb lexicon comprises only
the syntactically and multilingually filtered par-
ticiples found in Europarl (46 types).
Although we have seen in section 3.2 that adver-
bial participles do not seem to occur in the TIGER
heldout set, we also know that it is important to
assess the effect of ambiguity rate on the overall
grammar performance. Therefore, we computed
the accuracy of the most probable parses produced
by the Euro-Part-Adv and Filt-Part-Adv on the
heldout set. As is shown in Table 5, the Euro-Part-
Adv performs significantly worse than Filt-Part-
Adv. This suggests that the non-filtered participle
resource is not constrained enough and still pro-
duces a lot of spurious ambiguites that mislead the
disambiguation component. The coverage values
in Table 6 further corroborate the observation that
the unfiltered participle resource behaves similar
to the unrestricted adverb conversion in All-Part-
Adv (see Section 3.2). The coverage of the filtered
vs. the unfiltered version on the development set is
identical, however the timeouts in Euro-Part-Adv
increase by 17% and parsing time by 8%.
By contrast, there is no significant difference
in f-score between the No-Part-Adv version pre-
sented in Section 3.2 and the Filt-Part-Adv ver-
sion. Thus, we can, at least, assume that the fil-
tered participles resources has restricted the mas-
sive overgeneration caused by the general adverb
conversion rule such that the overall performance
of the original grammar is not negatively affected.
To evaluate the participle resource as to whether
it could have a positive qualtitative effect on pars-
ing TIGER at all, we built a specialised test-
suite which comprises only sentences containing
a non-lexicalised participle, which has an adver-
bial translation in Europarl and is tagged as ADJD
40
Grammar Prec. Rec. F-Sc. Time
in sec
Euro-Part-Adv 82.32 75.78 78.91 701
Filt-Part-Adv 84.12 78.2 81.05 665
Table 5: Evaluation on the TIGER heldout set, 371
sentences total
Grammar Parsed
Sent.
Starred
Sent.
Time-
outs
Time
in sec
Euro-Part-Adv 4304 588 107 7359
Filt-Part-Adv 4304 604 91 6791
Table 6: Performance on the TIGER development
set (sentences 1-5000), 4999 sentences total
in TIGER. The sentences were extracted from the
whole TIGER corpus yielding a set of 139 sen-
tences. In this quality-oriented evaluation, we
only contrast the No-Part-Adv version with the
filtered Filt-Part-Adv version since the unfiltered
version leads to worse overall performance. As
can be seen in Table 7, the No-Part-Adv can only
completely cover 36% of the specialised testsuite
which is much lower than its average complete
coverage on the development set (86%). This sug-
gests that a substantial number of the extracted
ADJD participles are actually used as adverbial in
the specialised testsuite.
Similar to the qualitative evaluation procedure
in 3.2, we manually evaluated a random sample of
20 sentences covered by Filt-Part-Adv and not by
No-Part-Adv as to whether they contain an adver-
bial participle that has been correctly recognised.
This was the case for 90% of the sentences, the
remaining 2 sentences were cases of secondary
predications. An example of a relatively simple
TIGER sentence that the grammar could not cover
in the No-Part-Adv version is given in (8).
(8) Die Anti-Baby-Pillen stehen im Verdacht , vermehrt
Thrombosen auszulo?sen.
?The birth control pill is suspected to increasingly cause
thromboses.?
We also manually checked a random sample of
Grammar Parsed
Sent.
Starred
Sent.
Time-
outs
Time
in sec
No-Part-Adv 50 77 12 427
Filt-Part-Adv 92 39 8 366
Table 7: Performance on the specialised TIGER
test set, 139 sentences total
20 sentences that the Filt-Part-Adv grammar could
not cover, in order to see whether the grammar sys-
tematically misses certain cases of adverbial par-
ticiples. In this second random sample, the per-
centage of sentences containing a true adverbial
participle was again 90%. The grammar could
not correctly analyse these because of their spe-
cial syntax that is not covered by the general ad-
verb macro (or, of course, because of difficult con-
structions not related to adverbial participles). An
example for such a case is given in (9).
(9) Transitreisen junger Ma?nner vom Gaza-Streifen ins
Westjordanland und umgekehrt sind nicht gestattet.
?Transit travels from the Gaza Strip to the West Bank and
vice versa are not allowed for young men.?
The high proportion of true adverbial participle
instances in our specific testsuite suggests that the
data we induced from Europarl largely carries over
to TIGER (despite genre differences, for instance)
and constitutes a generally useful resource. Thus,
we can not only say that the filtered participle re-
source has no negative effect on the overall per-
formance of the German LFG, but also extends its
coverage for a less frequent phenomenon in a lin-
guistically precise way.
6 Conclusion
We have proposed an empirical account for detect-
ing adverbial participles in German. Since this
category is usually not annotated in German re-
sources and hard to describe in theory, we based
our method on multilingual parallel data. This
data suggests that only a fairly limited class of par-
ticiples actually undergo the conversion to adverbs
in free text. We have described a set of linguisti-
cally motivated filters which are necessary to in-
duce a high-precision resource for adverbial par-
ticiples from parallel data. This resource has been
integrated into the German LFG grammar. In con-
trast to the version of the grammar which does not
restrict the participle - adverb conversion, the re-
stricted version produces less spurious ambigui-
ties which leads to better f-score on gold standard
data. Moreover, by manually evaluating a spe-
cialised data set, we have established that the re-
stricted version also extends the coverage and pro-
duces the correct analyses which can be used for
further linguistic study.
41
References
Tanja Becker. 2001. DMOR: Handbuch. Technical
report, IMS, University of Stuttgart.
Gerlof Bouma, Jonas Kuhn, Bettina Schrader, and
Kathrin Spreyer. 2008. Parallel LFG Grammars
on Parallel Corpora: A Base for Practical Trian-
gulation. In Miriam Butt and Tracy Holloway
King, editors, Proceedings of the LFG08 Confer-
ence, pages 169?189, Sydney, Australia. CSLI Pub-
lications, Stanford.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolf-
gang Lezius, and George Smith. 2002. The tiger
treebank. In Proceedings of the Workshop on Tree-
banks and Linguistic Theories.
Joan Bresnan. 2000. Lexical-Functional Syntax.
Blackwell, Oxford.
Miriam Butt, Helge Dyvik, Tracy Holloway King, Hi-
roshi Masuichi, and Christian Rohrer. 2002. The
Parallel Grammar Project.
Aoife Cahill, John T. Maxwell III, Paul Meurer, Chris-
tian Rohrer, and Victoria Rose?n. 2007. Speeding
up LFG Parsing using C-Structure Pruning . In Col-
ing 2008: Proceedings of the workshop on Grammar
Engineering Across Frameworks, pages 33 ? 40.
Stefanie Dipper. 2003. Implementing and Document-
ing Large-Scale Grammars ? German LFG. Ph.D.
thesis, Universita?t Stuttgart, IMS.
Anette Frank, Tracy Holloway King, Jonas Kuhn, and
John T. Maxwell. 2001. Optimality Theory Style
Constraint Ranking in Large-Scale LFG Grammars
. In Peter Sells, editor, Formal and Empirical Issues
in Optimality Theoretic Syntax, page 367?397. CSLI
Publications.
Wilhelm Geuder. 2004. Depictives and transparent ad-
verbs. In J. R. Austin, S. Engelbrecht, and G. Rauh,
editors, Adverbials. The Interplay of Meaning, Con-
text, and Syntactic Structure, pages 131?166. Ben-
jamins.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In MT Summit 2005.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006.
Maltparser: A data driven parser-generator for de-
pendency parsing. In Proc. of LREC-2006.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Stefan Riezler, Tracy Holloway King, Ronald M. Ka-
plan, Richard Crouch, John T. Maxwell, and Mark
Johnson. 2002. Parsing the Wall Street Journal us-
ing a Lexical-Functional Grammar and Discrimina-
tive Estimation Techniques . In Proceedings of ACL
2002.
Christian Rohrer and Martin Forst. 2006. Improving
coverage and parsing quality of a large-scale LFG
for German. In Proceedings of LREC-2006.
Anne Schiller, Simone Teufel, and Christine Thielen.
1995. Guidelines fuer das Tagging deutscher Tex-
tkorpora mit STTS. Technical report, IMS, Univer-
sity of Stuttgart.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of
International Conference on New Methods in Lan-
guage Processing.
David Yarowsky, Grace Ngai, and Richard Wicen-
towski. 2001. Inducing multilingual text analysis
tools via robust projection across aligned corpora. In
Proceedings of HLT 2001, First International Con-
ference on Human Language Technology Research.
42
Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 232?236,
Seoul, South Korea, 5-6 July 2012. c?2012 Association for Computational Linguistics
Automatically Acquiring Fine-Grained
Information Status Distinctions in German
Aoife Cahill
Educational Testing Service,
660 Rosedale Road,
Princeton, NJ 08541, USA
acahill@ets.org
Arndt Riester
Institute for Natural Language Processing (IMS)
Pfaffenwaldring 5b
70569 Stuttgart, Germany
arndt.riester@ims.uni-stuttgart.de
Abstract
We present a model for automatically predict-
ing information status labels for German refer-
ring expressions. We train a CRF on manually
annotated phrases, and predict a fine-grained
set of labels. We achieve an accuracy score of
69.56% on our most detailed label set, 76.62%
when gold standard coreference is available.
1 Introduction
The automatic identification of information status
(Prince, 1981; 1992), i.e. categorizing discourse en-
tities into different classes on the given-new scale,
has recently been identified as an important issue in
natural language processing (Nissim, 2006; Rahman
and Ng, 2011; 2012). It is widely acknowledged that
information status and, more generally, information
structure,1 is reflected in word order, in the form of
referring expressions as well as in prosody. In com-
putational linguistics, the ability to automatically la-
bel text with information status, therefore, could be
of great benefit to many applications, including sur-
face realization, text-to-speech synthesis, anaphora
resolution, summarization, etc.
The task of automatically labeling text with infor-
mation status, however, is a difficult one. Part of
1Information structure is usually taken to describe clause-
internal divisions into focus-background, topic-comment, or
theme-rheme, which are in turn defined in terms of contex-
tual factors such as given-new information, salience, contrast
and alternatives, cf. Steedman and Kruijff-Korbayova? (2003),
Krifka (2007). Information status is the subfield of information
structure which exclusively deals with the given-new distinction
and which is normally confined to referring expressions.
the difficulty arises from the fact that, to a certain
degree, such labeling requires world knowledge and
semantic comprehension of the text, but another ob-
stacle is simply that theoretical notions of informa-
tion status are not used consistently in the literature.
In this paper we outline a system, trained on a
small amount of data, that achieves encouraging
results on the task of automatically labeling tran-
scribed German radio news data with fine-grained
information status labels.
2 Learning information status
A simpler variant of the task is anaphoricity de-
tection (discourse-new detection) (Bean and Riloff,
1999; Ng and Cardie, 2002; Uryupina, 2003; Denis
and Baldridge, 2007; Zhou and Kong, 2011), which
divides discourse entities into anaphoric (given) and
new. Identifying discourse-new expressions in texts
is helpful as a precursor to coreference resolution,
since, by definition, there is no need to identify an-
tecedents for new entities.
In the linguistic literature, referring expressions
have been distinguished in much more detail, and
there is reason to believe that this could also provide
useful information for NLP applications. Nissim
(2006) and Rahman and Ng (2011) developed meth-
ods to automatically identify three different classes:
OLD, MEDIATED and NEW expressions. This classi-
fication, which is described in Nissim et al (2004),
has been used for annotating the Switchboard dialog
corpus (Calhoun et al, 2010), on which both studies
are based. Most recently, Rahman and Ng (2012)
extend their automatic prediction system to a more
fine-grained set of 16 subtypes.
232
Old. The class of OLD entities in Nissim et al
(2004) is not limited to full-fledged anaphors like in
Example (1a) but also includes cases of generic and
first/second person pronouns like in (1b), which may
or may not possess a previous mention.
(1) a. Shares in General Electric rose as investors
bet that the US company would take more
lucrative engine orders for the A380.
b. I wonder where this comes from.
Mediated. The group of MEDIATED entities mainly
has two subtypes: (2a) shows an expression which
has not been mentioned before but which is depen-
dent on previous context. Such items have also been
called bridging anaphors (Poesio and Vieira, 1998).
(2b) contains a phrase which is generally known but
does not depend on the discourse context.
(2) a. Tomorrow, the Shenzhou 8 spacecraft will
be in a position to attempt the docking.
b. They hope that he will be given the right to
remain in the Netherlands.
New. The label NEW, following Nissim et al (2004:
1024), applies ?to entities that have not yet been in-
troduced in the dialog and that the hearer cannot in-
fer from previously mentioned entities.?2 Two kinds
of expressions which fall into this category are unfa-
miliar definites (3a) and (specific) indefinites (3b).
(3) a. The man who shot a policeman yesterday
has not been caught yet.
b. Klose scored a penalty in the 80th minute.
Based on work described in Nissim (2006), Rahman
and Ng (2011) develop a machine learning approach
to information-status determination. They develop a
support vector machine (SVM) model from the an-
notated Switchboard dialogs in order to predict the
three possible classes. In an extension of this work,
Rahman and Ng (2012) compare a rule-based sys-
tem to a classifier with features based on the rules to
predict 16 subtypes of the three basic types. On this
extended label set on the dialog data, they achieve
accuracy of 86.4% with gold standard coreference
and 78.7% with automatically detected coreference.
3 Extending Information Status prediction
The work we present here is most similar to that
of Rahman and Ng (2012), however, our work dif-
2Note that this definition fails to exclude cases like (2b).
fers from theirs in a number of important respects.
We (i) experiment with a different information status
classification, derived from Riester et al (2010), (ii)
use (morpho-)syntactic and functional features auto-
matically extracted from a deep linguistic parser in
our CRF sequence model, (iii) test our approach on
a different language (German), (iv) show that high
accuracy can be achieved with a limited number of
training examples, and (v) that the approach works
on a different genre (transcribed radio news bulletins
which contain complex embedded phrases like an
offer to the minority Tamil population of Sri Lanka,
not typically found in spoken dialog).
The annotation scheme by Riester et al (2010)
divides referring items differently to Nissim et al
(2004). Arguments are provided in the former pa-
per and in Baumann and Riester (to appear). As it
stands, the scheme provides too many labels for our
purpose. As a compromise, we group them in seven
classes: GIVEN, SITUATIVE, BRIDGING, UNUSED,
NEW, GENERIC and EXPLETIVE.
Given. Givenness is a central notion in informa-
tion structure theory. Schwarzschild (1999) de-
fines givenness of individual-type entities in terms
of coreference. If desired, GIVEN items can be sub-
classified, e.g. whether they are pronouns or full
noun phrases, and whether the latter are repetitions
or short forms of earlier material, or whether they
consist of lexically new material (epithets).
Situative. 1st and 2nd person pronouns, locative and
temporal adverbials, usually count as deictic expres-
sions since they refer to elements in the utterance sit-
uation. We therefore count them as a separate class.
SITUATIVE entities may, but need not, corefer.
Bridging. Bridging anaphors, as in (2a) above, have
received much attention, see e.g. Asher and Las-
carides (1998) or Poesio and Vieira (1998). Al-
though they are discourse-new, they share properties
with coreference anaphors since they depend on the
discourse context. They represent a class which can
be easily identified by human annotators but are dif-
ficult to capture by automatic techniques.
Unused. In manual annotation practice, it is very of-
ten impossible to decide whether an entity is hearer-
known, since this depends on who we assume the
hearer to be; and even if we agree on a recipient, we
may still be mistaken about their knowledge. For ex-
ample, Wolfgang Bosbach, deputy chairman of the
233
Countable Boolean Descriptive
# Words in phrase* Phrase contains a compound noun Adverbial type, e.g. locative
# Predicative phrases Phrase contains coordination Determiner type, e.g. definite *
# DPs and NPs in phrase Phrase contains time expression Left/Right-most POS tag of phrase
# top category children Phrase contains < 2, 5 or 10 words Highest syntactic node label
# Labels/titles Phrase does not have a complete parse that dominates the phrase
# Depth of syntactic phrase Phrase is a pronoun Grammatical function, e.g. SUBJ *
# Cardinal numbers Phrase contains more than 1 DP Type of pronoun, e.g. demonstrative
# Depth of syntactic phrase and 1 NP (i.e. phrase contains Syntactic shape, e.g. apposition with
ignoring unary branching an embedded argument) a determiner and attributive modifier
# Apposition phrases Head noun appears (partly or completely) Head noun type, e.g. common *
# Year phrases in previous 10 sentences * Head noun number, e.g. singular
Table 1: Features of the CRF prediction model (* indicates feature used in baseline model)
CDU parliamentary group may be known to parts
of a German audience but not to other people.
We address this by collecting both hearer-known
and hearer-unknown definite expressions into one
class UNUSED. This does not rule out further sub-
classification (known/unknown) or the possibility of
using machine learning techniques to identify this
distinction, see Nenkova et al (2005). The fact that
Rahman and Ng (2011) report the highest confusion
rate between NEW and MEDIATED entities may have
its roots in this issue.
New. Only (specific) indefinites are labeled NEW.
Generic. An issue which is not dealt with in Nissim
et al (2004) are GENERIC expressions as in Lions
have manes. Reiter and Frank (2010) discuss the
task of identifying generic items in a manner sim-
ilar to the learning tasks presented above, using a
Bayesian network. We believe it makes sense to in-
tegrate genericity detection into information-status
prediction.3
4 German data
Our work is based on the DIRNDL radio news cor-
pus of Eckart et al (2012) which has been hand-
annotated with information status labels. We choose
a selection of 6668 annotated phrases (1420 sen-
tences). This is an order of magnitude smaller than
the annotated Switchboard corpus of Calhoun et al
(2010). We parse each sentence with the German
Lexical Functional Grammar of Rohrer and Forst
(2006) using the XLE parser in order to automati-
3Note that in coreference annotation it is an open question
whether two identical generic terms should count as coreferent.
cally extract (morpho-)syntactic and functional fea-
tures for our model.
5 Prediction Model for Information Status
Cahill and Riester (2009) show that there are asym-
metries between pairs of information status labels
contained in sentences, i.e. certain classes of expres-
sions tend to precede certain other classes. We there-
fore treat the prediction of IS labels as a sequence
labeling task.4 We train a CRF using wapiti
(Lavergne et al, 2010), with the features outlined in
Table 1. We also include a basic ?coreference? fea-
ture, similar to the lexical features of Rahman and
Ng (2011), that fires if there is some lexical overlap
of nouns (or compound nouns) in the preceding 10
sentences. The original label set described in Riester
et al (2010) contains 21 labels. Here we work with
a subset of maximally 12 labels, but also consider
smaller subsets of labels and carry out a mapping to
the Nissim (2006) label set (Table 2).5 We run a 10-
fold cross-validation experiment and report average
prediction accuracy. The results are given in Table
3a. As an informed baseline, we run the same cross-
validation experiment with a subset of features that
roughly correspond to the features of Nissim (2006).
Our models perform statistically significantly better
than the baseline (p < 0.001, using the approximate
randomization test) for all label sets.
4Preliminary experimental evidence showed that the CRF
performed slightly better than a simple multiclass logistic re-
gression model (e.g. compare 72.19 to 72.43 in Table 3a).
5Unfortunately, due to underlying theoretical differences, it
is impossible to map between the Riester label set and the ex-
tended label set used in Rahman and Ng (2012).
234
Total Riester 1 Riester 2 Riester 3 Nissim ?06
462 GIVEN- GIVEN-
GIVEN OLD
PRONOUN PRONOUN
143 GIVEN- GIVEN-REFLEXIVE REFLEXIVE
427 GIVEN-EPITHET
169 GIVEN- GIVEN-REPEATED NOUN
204 GIVEN-SHORT
265 SITUATIVE SITUATIVE SITUATIVE
449 BRIDGING BRIDGING BRIDGING
MEDIATED1271 UNUSED- UNUSED- UNUSEDKNOWN KNOWN
1227 UNUSED- UNUSED-
NEWUNKNOWN UNKNOWN1282 NEW NEW NEW
632 GENERIC GENERIC GENERIC
96 EXPLETIVE EXPLETIVE EXPLETIVE OTHER
Table 2: Varying the granularity of the label sets
As expected, the less fine-grained a label set, the
easier it is to predict the labels. It remains for fu-
ture work to show the effect of different label set
granularities in practical applications. We approx-
imate gold standard coreference information from
the manually annotated labels (e.g. all GIVEN la-
bel types are by their nature coreferent), and carry
out an experiment with gold-standard approximation
of coreference marking. These results are also re-
ported in Table 3a. Here we see a clear performance
difference in the effect of gold-standard corefer-
ence on the Riester label set (increasing around 6-
10%), compared to the Nissim label set (decreasing
slightly). This is an artifact of the way the mapping
was carried out, deriving the gold standard corefer-
ence information from the Riester label set. There is
not a one-to-one mapping between OLD and GIVEN,
and, in the Riester label set, coreferential entities
that are labeled as SITUATIVE (deictic terms) are not
recognized as such.
The feature set in Table 1 reflects the morpho-
syntactic properties of the phrases to be labeled.
Sometimes world knowledge is required in order
to be able to accurately predict a label; for exam-
ple, to know that the pope can be categorized as
UNUSED-KNOWN, because it can occur discourse-
initially, whereas the priest must usually be cate-
gorized as GIVEN. The BRIDGING relationship is
also difficult to capture without some world knowl-
edge. For example, to infer that the waitress can
be categorized as BRIDGING in the context of the
restaurant requires information that links the two
concepts. Rahman and Ng (2012) also note this and
include features based on FrameNet, WordNet and
the ReVerb corpus for English.
For German, we address this issue by introducing
two further types of features into our model based on
the GermaNet resource (Hamp and Feldweg, 1997).
The first type is based on the GermaNet synset of
the head noun in the phrase and its distance from the
root node (the assumption is that entities closer to
root are more generic than those further away). The
second include the sum and maximum of the Lin
semantic relatedness measures (Lin, 1998) of how
similar the head noun of the phrase is to the other
nouns in current and immediately preceding sen-
tence surrounding the phrase (calculated with Ger-
maNet Pathfinder; Finthammer and Cramer, 2008).
The results are given in Table 3b. Here we see a
consistent increase in performance of around 4% for
each label set over the model that does not include
the GermaNet features. Again, we see the same de-
crease in performance on the Nissim label set when
using gold standard coreference information.
Label Set Accuracy Gold Baseline
coref. feats.
Riester 1 65.49 72.49 57.25
Riester 2 67.21 76.88 58.82
Riester 3 72.43 82.22 64.20
Nissim ?06 76.24 74.06 71.70
(a) Only morpho-syntactic features
Label Set Accuracy Gold coreference
Riester 1 69.56 76.62
Riester 2 71.99 79.86
Riester 3 75.82 84.76
Nissim ?06 79.61 78.46
(b) Morpho-syntactic + GermaNet features
Table 3: Cross validation accuracy results
6 Conclusion
In this paper we presented a model for automatically
labeling German text with fine-grained information
status labels. The results reported here show that we
can achieve high accuracy prediction on a complex
text type (transcribed radio news), even with a lim-
ited amount of data.
235
References
Nicholas Asher and Alex Lascarides. 1998. Bridging.
Journal of Semantics, 15(1):83?113.
Stefan Baumann and Arndt Riester. to appear. Ref-
erential and Lexical Givenness: Semantic, Prosodic
and Cognitive Aspects. In G. Elordieta and P. Prieto,
editors, Prosody and Meaning. Mouton de Gruyter,
Berlin.
David L. Bean and Ellen Riloff. 1999. Corpus-Based
Identification of Non-Anaphoric Noun Phrases. In
Proceedings of ACL, pages 373?380, College Park,
MD.
Aoife Cahill and Arndt Riester. 2009. Incorporating
Information Status into Generation Ranking. In Pro-
ceedings of ACL-IJCNLP, pages 817?825, Singapore.
Sasha Calhoun, Jean Carletta, Jason Brenier, Neil Mayo,
Dan Jurafsky, Mark Steedman, and David Beaver.
2010. The NXT-Format Switchboard Corpus: A
Rich Resource for Investigating the Syntax, Seman-
tics, Pragmatics and Prosody of Dialogue. Language
Resources and Evaluation, 44(4):387?419.
Pascal Denis and Jason Baldridge. 2007. Global Joint
Determination of Anaphoricity and Coreference Res-
olution Usinger Integer Programming. In Proceedings
of ACL-HLT, Rochester, NY.
Kerstin Eckart, Arndt Riester, and Katrin Schweitzer.
2012. A Discourse Information Radio News Database
for Linguistic Analysis. In C. Chiarcos et al, edi-
tors, Linked Data in Linguistics, pages 65?76, Berlin.
Springer.
Marc Finthammer and Irene Cramer. 2008. Exploring
and Navigating: Tools for GermaNet. In Proceedings
of LREC, Marrakech, Morocco.
Birgit Hamp and Helmut Feldweg. 1997. GermaNet ? a
Lexical-Semantic Net for German. In Proceedings of
the ACL Workshop Automatic Information Extraction
and Building of Lexical Semantic Resources for NLP
Applications, pages 9?15.
Manfred Krifka. 2007. Basic Notions of Information
Structure. In C. Fe?ry and M. Krifka, editors, The No-
tions of Information Structure, pages 57?68. Univer-
sita?tsverlag Potsdam.
Thomas Lavergne, Olivier Cappe?, and Franc?ois Yvon.
2010. Practical Very Large Scale CRFs. In Proceed-
ings of ACL, pages 504?513.
Dekang Lin. 1998. An Information-Theoretic Definition
of Similarity. In International Conference on Machine
Learning, pages 296?304.
Ani Nenkova, Advaith Siddharthan, and Kathleen McK-
eown. 2005. Automatically Learning Cognitive Sta-
tus for Multi-Document Summarization of Newswire.
In Proceedings of HLT/EMNLP, pages 241?248, Van-
couver.
Vincent Ng and Claire Cardie. 2002. Identifying
Anaphoric and Non-Anaphoric Noun Phrases to Im-
prove Coreference Resolution. In Proceedings of
COLING, pages 730?736, Taipei, Taiwan.
Malvina Nissim, Shipra Dingare, Jean Carletta, and Mark
Steedman. 2004. An Annotation Scheme for Infor-
mation Status in Dialogue. In Proceedings of LREC,
Lisbon.
Malvina Nissim. 2006. Learning Information Status of
Discourse Entities. In Proceedings of EMNLP, pages
94?102, Sydney.
Massimo Poesio and Renata Vieira. 1998. A Corpus-
Based Investigation of Definite Description Use.
Computational Linguistics, 24(2).
Ellen F. Prince. 1981. Toward a Taxonomy of Given-
New Information. In P. Cole, editor, Radical Prag-
matics, pages 233?255. Academic Press, New York.
Ellen F. Prince. 1992. The ZPG Letter: Subjects, Def-
initeness and Information Status. In W. Mann and
S. Thompson, editors, Discourse Description, pages
295?325. Benjamins, Amsterdam.
Altaf Rahman and Vincent Ng. 2011. Learning the Infor-
mation Status of Noun Phrases in Spoken Dialogues.
In Proceedings of EMNLP, pages 1069?1080, Edin-
burgh.
Altaf Rahman and Vincent Ng. 2012. Learning the Fine-
Grained Information Status of Discourse Entities. In
Proceedings of EACL 2012, Avignon, France.
Nils Reiter and Anette Frank. 2010. Identifying Generic
Noun Phrases. In Proceedings of ACL, pages 40?49,
Uppsala, Sweden.
Arndt Riester, David Lorenz, and Nina Seemann. 2010.
A Recursive Annotation Scheme for Referential In-
formation Status. In Proceedings of LREC, Valletta,
Malta.
Christian Rohrer and Martin Forst. 2006. Improving
Coverage and Parsing Quality of a Large-Scale LFG
for German. In Proceedings of LREC, Genoa, Italy.
Roger Schwarzschild. 1999. GIVENness, AvoidF, and
other Constraints on the Placement of Accent. Natural
Language Semantics, 7(2):141?177.
Mark Steedman and Ivana Kruijff-Korbayova?. 2003.
Discourse Structure and Information Structure. Jour-
nal of Logic, Language and Information, 12:249?259.
Olga Uryupina. 2003. High-precision Identification of
Discourse New and Unique Noun Phrases. In Pro-
ceedings of the ACL Student Workshop, pages 80?86,
Sapporo.
Guodong Zhou and Fang Kong. 2011. Learning Noun
Phrase Anaphoricity in Coreference Resolution via
Label Propagation. Journal of Computer Science and
Technology, 26(1).
236
The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 233?241,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Precision Isn?t Everything:
A Hybrid Approach to Grammatical Error Detection
Michael Heilman and Aoife Cahill and Joel Tetreault
Educational Testing Service
660 Rosedale Road
Princeton, NJ 08541, USA
{mheilman,acahill,jtetreault}@ets.org
Abstract
Some grammatical error detection methods,
including the ones currently used by the Edu-
cational Testing Service?s e-rater system (At-
tali and Burstein, 2006), are tuned for pre-
cision because of the perceived high cost
of false positives (i.e., marking fluent En-
glish as ungrammatical). Precision, however,
is not optimal for all tasks, particularly the
HOO 2012 Shared Task on grammatical er-
rors, which uses F-score for evaluation. In this
paper, we extend e-rater?s preposition and de-
terminer error detection modules with a large-
scale n-gram method (Bergsma et al, 2009)
that complements the existing rule-based and
classifier-based methods. On the HOO 2012
Shared Task, the hybrid method performed
better than its component methods in terms of
F-score, and it was competitive with submis-
sions from other HOO 2012 participants.
1 Introduction
The detection of grammatical errors is a challenging
problem that, arguably, requires the use of both lin-
guistic knowledge (e.g., in the form of rules or com-
plex features) and large corpora for statistical learn-
ing. Additionally, grammatical error detection can
be applied in various scenarios (e.g., automated es-
say scoring, writing assistance, language learning),
many of which may benefit from task-specific adap-
tation or tuning. For example, one might want to
take a different approach when detecting errors for
the purpose of providing feedback than when de-
tecting errors to evaluate the quality of writing in
an essay. Thus, it seems desirable to take a flexible
approach to grammatical error detection that incor-
porates multiple, complementary techniques.
In this paper, we extend the preposition and de-
terminer error detection modules currently used in
the Educational Testing Service?s e-rater automated
essay scoring system (Attali and Burstein, 2006) for
the HOO 2012 Shared Task on grammatical errors
(?2). We refer to this set of modules from e-rater as
our ?base system? (?3). While the base system uses
statistical methods to learn models of grammatical
English, it also leverages substantial amounts of lin-
guistic knowledge in the form of various hand-coded
filters and complex syntactic features. The base sys-
tem is also tuned for high precision at the expense
of recall in order to avoid a high rate of potentially
costly false positives (i.e., frequent marking of cor-
rect English sentences as ungrammatical).
We apply the pre-existing base system without
modifications but complement it with a large-scale
n-gram method (?5) based on work by Bergsma et
al. (2009). The n-gram method employs very little
linguistic knowledge and instead relies almost ex-
clusively upon corpus statistics. We also tune the
resulting hybrid system with labeled training data
in order to maximize the primary evaluation met-
ric used in the HOO 2012 Shared Task: balanced
F-score, or F1 (?6). We find that the tuned hybrid
system improves upon the recall and F-score of the
base system. Also, in the HOO 2012 Shared Task,
the hybrid system achieved results that were com-
petitive with other submitted grammatical error de-
tection systems (?7).
233
2 Task Definition
In this section, we provide a brief overview of the
HOO 2012 Shared Task (Dale et al, 2012). The
task focuses on prepositions and determiners only,
distinguishing the following error types: preposition
selection errors (coded ?RT? in the data), extraneous
prepositions (?UT?), missing prepositions (?MT?),
determiner selection errors (?RD?), extraneous de-
terminers (?UD?), and missing determiners (?MD?).
For training and testing data, the shared task uses
short essays from an examination for speakers of En-
glish as a foreign language. The data includes gold
standard human annotations identifying preposition
and determiner errors. These errors are represented
as edits that transform an ungrammatical text into
a grammatical one. Edits consist of start and end
offsets into the original text and a correction string
that should replace the original text at the speci-
fied offsets. The offsets differ by error type: word
selection errors include just the word, extraneous
word errors include an extra space after the word so
that a blank will result in an appropriate amount of
whitespace, and missing word errors specify spans
of length zero.1
There are three subtasks: detection, recognition,
and correction. Each is evaluated according to pre-
cision, recall, and F-score according to a set of
gold standard edits produced by human annotation.
While the correction subtask requires both correct
character offsets and appropriate corrections, the de-
tection and recognition subtasks only consider the
offsets. Detection and recognition are essentially the
same, except that detection allows for loose match-
ing of offsets, which permits mismatches between
the extraneous use (e.g., UT) and word selection
(e.g., RT) error types. For our submission to the
shared task, we chose to tune for the detection sub-
task, and we also chose to avoid the correction task
entirely since the interface to the pre-existing base
system did not give us access to possible corrections.
1The offsets for extraneous word errors prior to punctuation,
a relatively rare occurrence, include a space before the word
rather than after it. Our script for converting our system?s output
into the HOO 2012 format did not account for this, which may
have decreased recognition performance slightly.
3 Base System
As our base system, we repurpose a complex sys-
tem designed to automatically score student essays
(both native and non-native and across a wide range
of competency levels). The system is also used to
give feedback to essay writers, so precision is fa-
vored over recall. There are three main modules in
the essay-scoring system whose purpose it is to de-
tect preposition and determiner errors (as they are
defined in that system). Many of the details have
been reported previously (Chodorow and Leacock,
2000; Han et al, 2004; Han et al, 2006; Chodorow
et al, 2007; Tetreault and Chodorow, 2008), so here
we will only give brief summaries of these modules.
It is important to note that this system was run
without modification. That is, no training of new
models or tuning was carried out specifically for the
shared task. In addition, for the two statistical mod-
ules, we only had access to the final, boolean deci-
sions about whether an error is present or not at a
particular location in text. That is, we did not have
access to confidence scores, and so task-specific tun-
ing for F-score was not an option.
3.1 Preposition Error Detection
The base system detects incorrect and extraneous
prepositions (Chodorow et al, 2007; Tetreault and
Chodorow, 2008). Tetreault and Chodorow (2008)
reports approximately 84% precision and 19% re-
call on both error types combined when evaluating
the system on manually annotated non-native text.
3.1.1 Incorrect Prepositions
The module to detect incorrectly used preposi-
tions consists of a multiclass logistic regression (i.e.,
?Maximum Entropy?) model of grammatical usage,
along with heuristic pre- and post- filters. The mod-
ule works by extracting a set of features from the
?context? around a preposition, generating a distri-
bution over possible prepositions using the model of
grammatical usage, and then flagging an error if the
difference in probability between the text?s original
preposition and an alternative preposition exceeds a
certain threshold. The probability for any correction
also needs to exceed another minimum threshold.
For this work, we used the pre-existing, manually-
set thresholds.
234
A pre-filter prevents any contexts that contain
spelling errors from being submitted to the logistic
regression model. The motivation for this is that the
NLP components that provide the features for the
model are unreliable on such data, and since the sys-
tems favors precision over recall, no attempt is made
to correct prepositions where the system cannot rely
on the accuracy of those features.
The logistic regression model of correct preposi-
tion usage is trained on approximately 82 million
words from the San Jose Mercury News2 and texts
for 11th to 12th grade reading levels from the Meta-
Metrics Lexile corpus, resulting in 7 million prepo-
sition contexts. The model uses 25 types of features:
words and part-of-speech tags around the existing
preposition, head verb (or noun) in the preceding
VP (or NP), head noun in the following NP, among
others. NPs and VPs were detected using chunking
rather than full parsing, as the performance of statis-
tical parsers on erroneous text was deemed to be too
poor.
A post-filter rules out certain candidates based on
the following heuristics: (1) if the suggested correc-
tion is an antonym of the original preposition (e.g.,
from vs to), it is discarded; (2) any correction of the
benefactive for is discarded when the head noun of
the following NP is human (detected as a WordNet
hyponym of person or group).
3.1.2 Extraneous Prepositions
Heuristics are applied to detect common occur-
rences of extraneous prepositions in two scenar-
ios: (1) accidentally repeated prepositions (e.g., with
with) and (2) insertion of unnecessary prepositions
in plural quantifier constructions (e.g., some of peo-
ple).
3.2 Determiner Error Detection
There are two separate components that detect er-
rors related to determiners. The first is a filter-based
model that detects determiner errors involving num-
ber and person agreement. The second is a statistical
system that supplements the rule-based system and
detects article errors.
2The San Jose Mercury News is available from the Linguis-
tic Data Consortium (catalog number LDC93T3A).
3.2.1 Filter-based system
The filter-based system combines unsupervised
detection of a set of possible errors (Chodorow and
Leacock, 2000) with hand-crafted filters designed
to reduce this set to the largest subset of correctly
flagged errors and the smallest possible number
of false positives. Chodorow and Leacock (2000)
found that low-frequency bigrams (sequences of two
lexical categories with a negative log-likelihood) are
quite reliable predictors of grammatical errors. Text
is tagged and chunked, and filters that detect likely
cases of NP-internal agreement violations are ap-
plied. These filters will mark, for example, a sin-
gular determiner followed by a plural noun head and
vice versa, or a number disagreement between a nu-
meral and the noun it modifies. This system has
the ability to take advantage of linguistic knowledge,
which contributes to its ability to detect errors with
high precision.
3.2.2 Statistical model
In addition to the hand-crafted filters described
above, there is a statistical component that detects
incorrect, missing and extraneous articles (Han et
al., 2004; Han et al, 2006). This component con-
sists of a multiclass logistic regression that selects
an appropriate article for every NP from a, an, the,
or . This model is trained on 31.5 million words
of diverse genres from the MetaMetrics Lexile cor-
pus (from 10th to 12th grade reading levels), or 8
million NP contexts. Again, NPs were determined
by chunking. The model includes various features:
words and POS tags around and within the NP, NP
head information including the countability of the
head noun (estimated automatically from large cor-
pora), etc.
In a cross-validation experiment, the model
achieved approximately 83% accuracy on well-
edited text. In an experiment evaluated on non-
native learner text, the model achieved approxi-
mately 85% agreement with human annotators.
4 Task-Specific Heuristic Filtering
There is not a one-to-one mapping between the def-
initions of determiner and preposition errors as used
in the HOO data set and the definitions used in our
base system. For example, our base system marks
235
errors involving every, many and other quantifiers as
determiner errors, while these are not marked in the
current HOO 2012 Shared Task data.
To ensure that our system was aligned with the
HOO 2012 Shared Task, we automatically extracted
lists of the most frequently occurring determiners
and prepositions in the HOO training data. Any RT,
UT, RD or UD edit predicted for a word not in those
lists is automatically discarded. In the training data,
this resulted in the removal of 4 of the 463 RT errors
and 98 of the 361 RD errors detected by the base
system.
5 Large-scale n-Gram Models
In order to complement the high-precision base sys-
tem and increase recall, we incorporate a large
scale n-gram model into our full system. Specifi-
cally, we adapt the SUMLM method from Bergsma
et al (2009). SUMLM creates confusion sets for
each preposition token in an input text and uses the
Google Web 1T 5-gram Corpus to score each item
in the confusion set.3 We extend SUMLM to sup-
port determiners, extraneous use errors, and missing
word errors.
Consider the case of preposition selection errors.
For a preposition token at position i in an input sen-
tence w, we compute the following score for each
possible alternative v, using Eq. 1.4
s(w, i, v) =
?
n=2...5
?
x?G(w,i,n,v)
log(count(x))
|G(w, i, n, v)|
(1)
The function G(w, i, n, v) returns the set of n-
grams in w that include the word at position i and
3The Google Web 1T 5-gram Corpus is available from the
Linguistic Data Consortium (catalog number LDC2006T13).
We plan to test other corpora for n-gram counts in future work.
4The n-gram approach considers all of the following words
to be prepositions: to, of, in, for, on, with, at, by, as, from, about,
up, over, into, down, between, off, during, under, through,
around, among, until, without, along, within, outside, toward,
inside, upon, except, onto, towards, besides, beside, and under-
neath. It considers all of the following words to be determiners:
a, an, and the. The sets of possible prepositions and determiners
for the base system are not exactly the same. Part of speech tags
are not used in the n-gram system except to identify insertion
points for missing prepositions and determiners.
replace that word, wi, with v. For example, if w =
Mary and John went at the store to buy milk, n = 4,
i = 4, and v = to, then G(w, i, n, v) returns the
following 4-grams:
? and John went to
? John went to the
? went to the store
? to the store to
The expression log(count(x)) is the natural loga-
rithm of the number of times the n-gram x occurred
in the corpus.5 |G(w, i, n, v)| is the number of n-
gram count lookups, used to normalize the scores.
Note that this normalization factor is not included in
the original SumLM. When v is an alternative prepo-
sition not near the beginning or end of a sentence,
|G(w, i, n, v)| = 14 since there are 14 n-gram count
lookups in the numerator. Or, for example, if i = 0,
indicating that the preposition occurs at the begin-
ning of the sentence, |G(w, i, n, v)| = 4.6
Next, we compute the ratio of the score of each
alternative to the score for the original, using Eq. 2.
r(w, i, v) =
s(w, i, v)
s(w, i, wi)
(2)
We then identify the best scoring alternative, re-
quiring that its score be higher than the original (i.e.,
r(w, i, v) > 1). The procedure is the same for deter-
miners, except, of course, that the set of alternatives
includes determiners rather than prepositions.
To extend the method from Bergsma et al (2009)
for extraneous prepositions and determiners, we
simply set v to be a blank and sum over j = 3 . . . 5
instead. |G(w, i, n, v)|will then be 12 instead of 14,
since bigrams from the original sentence, which be-
come unigrams when replacing wi with a blank, are
excluded.
To identify positions at which to flag selection or
extraneous use errors, we simply scan for words that
match an item in our sets of possible prepositions
and determiners. To extend the method for missing
5We use the TrendStream system (Flor, 2012) to retrieve n-
gram counts efficiently.
6Our n-gram counts do not include start- or end-of-sentence
symbols. Also, all n-grams are case-normalized with numbers
replaced by a special symbol.
236
Algorithm 1 tune(W, y, y? ?, ?min):
The hill-climbing algorithm for optimizing the n-
gram method?s penalty parameters q. W consists
of the training set texts. y? is a set of candidate edits.
y is a set of gold standard edits. ? is an initial step
size, and ?min is a minimum step size.
qallbest ? 0
scoreallbest ? eval(qallbest,W,y, y?)
while ? > ?min do
scorebest ? ??
qbest ? qallbest
for qtmp ? perturb(qbest, ?) do
scoretmp ? eval(qtmp,W,y, y?)
if scoretmp > scorebest then
qbest ? qtmp
scorebest ? scoretmp
end if
end for
if scorebest > scoreallbest then
qallbest ? qbest
scoreallbest ? scorebest
else
?? 0.5 ? ?
end if
end while
return qallbest
word errors, however, we apply a set of heuristics to
identify potential insertion points.7
6 Tuning
The n-gram approach in ?5 generates a large num-
ber of possible edits of different types. In this sec-
tion, we describe how we filter edits using their
scores and how we combine them with edits from
the base system (?3).
As described above, for an alternative v to be con-
sidered as a candidate edit, the value of r(w, i, v) in
Eq. 2 must be greater than a threshold of 1, indicat-
ing that the alternative scores higher than the origi-
nal word. However, we observed low precision dur-
ing development when including all candidate ed-
its and decided to penalize the ratios. Bergsma et
al. (2009) discuss raising the threshold, which has
7The heuristics are based on those used in Gamon (2010)
(personal communication).
a similar effect. Preliminary experiments indicated
that different edits (e.g., extraneous preposition edits
and preposition selection edits) should have differ-
ent penalties, and we also want to avoid edits with
overlapping spans. Thus, for each location with one
or more candidate edits, we select the best according
to Equation 3 and filter out the rest.
v? = argmax
v
r(w, i, v)? penalty(wi, v) (3)
penalty(wi, v) is a function that takes the current
word wi and the alternative v and returns one of 6
values: qRT for preposition selection, qUT for extra-
neous prepositions, qMT for missing prepositions,
qRD for determiner selection, qUD for extraneous
determiners, and qMD for missing determiners.
If the value for r(w, i, v?)?penalty(wi, v?) does
not exceed 1, we exclude it from the output.
We tune the vector q of all the penalties to op-
timize our objective function (F-score, see ?7) on
the training set using the hill-climbing approach de-
scribed in Algorithm 1. The algorithm initializes
the parameter vector to all zeros, and then itera-
tively evaluates candidate parameter vectors that re-
sult from taking positive and negative steps of size
? in each direction (steps with negative penalties
are skipped). The best step is taken if it improves
the current score, according to the eval function,
which returns the training set F-score after filtering
based on the current parameters.8 This process pro-
ceeds until there is no improvement. Then, the step
size ? is halved, and the whole process is repeated.
The algorithm proceeds as such until the step size
becomes lower than a specified minimum ?min.
When merging edits from the base system and the
n-gram approach, the hybrid system always prefers
edits from the base system if any edit spans overlap,
equivalent to including them in Eq. 3 and assigning
them a penalty of ??.9 Note that the set of pre-
dicted edits y passed as input to the tune algorithm
8Our implementation of the tuning algorithm uses the HOO
2012 Shared Task?s evalfrag.py module to evaluate the F-
score for the error detection subtask.
9If the base system produces overlapping edits, we keep
them all. If there are overlapping edits from the n-gram sys-
tem that have the same highest value for the penalized score in
Equation 3 and do not overlap with any base system edits, we
keep them all.
237
texts
edits
edits
edits
parameters
parameters
gold edits
base system -gram systemn
filteringtuning 
heuristic filtering
training testing
Figure 1: The architecture of the hybrid system. Different
steps are discussed in different parts of the paper: ?base
system? in ?3, ?n-gram system? in ?5, ?heuristic filter-
ing? in ?4, and ?tuning? and ?filtering? in ?6.
includes edits from both the base and n-gram meth-
ods.
Figure 1 illustrates the processes of training and
of producing test output from the hybrid system.
7 Results
Table 1 presents results for the HOO 2012 detec-
tion subtask, including errors of all types. The re-
sults here, reproduced from Dale et al (2012), are
prior to applying participant-suggested revisions to
the set of gold standard edits.10 We include four
variations of our approach: the base system (?3, la-
beled ?base?); the n-gram system (?5, labeled ?n-
gram?) by itself, tuned without edits from the base
system; the hybrid system, tuned with edits from the
base system (?hybrid?); and a variation of the hy-
10After submitting our predictions for the shared task, we
noticed a few minor implementation mistakes in our code re-
lated to the conversion of edits from the base system (?3) and
the task-specific heuristic filtering (?4). We corrected them and
retrained our system. The detection F-scores for the original
and corrected implementations were as follows: 26.45% (orig-
inal) versus 26.23% (corrected) for the base system, 30.70%
(original) versus 30.45% (corrected) for the n-gram system,
35.65% (original) versus 35.24% (corrected) for the hybrid sys-
tem, and 31.82% (original) versus 31.45% (corrected) for the
hybridindep system. Except for this footnote, all results in this
paper are for the original system.
run P R F
base 0 52.63 17.66 26.45
n-gram ? 25.87 37.75 30.70
hybrid 1 33.59 37.97 35.65
hybridindep 2 24.88 44.15 31.82
UI 8 37.22 43.71 40.20
Table 1: Precision, recall, and F-score for the combined
preposition and determiner error detection subtask for
various methods, before participant-suggested revisions
to the gold standard were applied. All values are percent-
ages. Official run numbers are shown in the ?run? col-
umn. The ?n-gram? run was not part of our official sub-
mission. For comparison, ?UI? is the submission, from
another team, that achieved the highest detection F-score
in the HOO 2012 Shared Task.
brid system (?hybridindep?) with the penalties tuned
independently, rather than jointly, to maximize F-
score for detection of each error type. For compari-
son, we also include the best performing run for the
detection subtask in terms of F-score (labeled ?UI?).
We observe that the base and n-gram systems ap-
pear to complement each other well for this task: the
base system achieved 26.45% F-score, and the n-
gram system achieved 30.70%, while the hybrid sys-
tem, with penalties tuned jointly, achieved 35.65%.
Table 2 shows further evidence that the two systems
have complementary performance. We calculate the
overlap between each system?s edits and the gold
standard. We see that only a small number of edits
are predicted by both systems (38 in total, 18 cor-
rect and 20 incorrect), and that the base system pre-
dicts 62 correct edits that the n-gram method does
not predict, and similarly the n-gram method pre-
dicts 92 correct edits that the base system does not
predict. The table also verifies that the base system
exhibits high precision (only 68 false positives in to-
tal) while the n-gram system is tuned for higher re-
call (286 false positives).
Not surprisingly, when the n-gram method?s
penalties were tuned independently (?hybridindep?)
rather than jointly, the overall score was lower, at
31.82% F-score. However, tuning independently
might be desirable if one were concerned with
performance on specific error types or if macro-
averaged F-score were the objective.
The hybrid system performed quite competitively
238
(1) All models had a UD very strange long shoes made from black skin . . .
(2) I think it is a great idea to organise this sort of festival because most of UT people enjoy it.
Figure 2: Examples of errors detected by the base system and missed by the n-gram models.
(3) We have to buy for UT some thing.
(4) I am  MD good diffender.
Figure 3: Examples of errors detected by the n-gram system and missed by the base model.
? gold /? gold
? base /? base ? base /? base
? n-gram 18 92 20 266
/? n-gram 62 276 48 ?
Table 2: The numbers of edits that overlap in the hybrid
system?s output and the gold standard for the test set. The
hybrid system?s output is broken down by whether edits
came from the base system (?3) or the n-gram method
(?5). The empty cell corresponds to hypothetical edits
that were in neither the gold standard or the system?s out-
put (e.g., edits missed by annotators), which we cannot
count.
compared to the other HOO 2012 submissions,
achieving the 3rd best results out of 14 teams for
the detection and recognition subtasks. The per-
formance of the ?UI? system was somewhat higher,
however, at 40.20% F-score compared to the hybrid
system?s 35.65%. We speculate that our hybrid sys-
tem?s performance could be improved somewhat if
we also tuned the base system for the task.
8 Error Analysis
It is illustrative to look at some examples of edits
that the base system correctly detects but the n-gram
model does not, and vice versa. Figure 2 shows ex-
amples of errors detected by the base system, but
missed by the n-gram system. Example (1) illus-
trates that the n-gram model has no concept of syn-
tactic structure. The base system, on the other hand,
carries out simple processing including POS tagging
and chunking, and is therefore aware of at least some
longer-distance dependencies (e.g., a . . . shoes). Ex-
ample (2) shows the effectiveness of the heuris-
tics based on quantifier constructions mentioned in
?3.1.2. These heuristics were developed by devel-
opers familiar with the kinds of errors that language
learners frequently make, and are therefore more tar-
geted than the general n-gram method.
Figure 3 shows examples of errors detected by the
n-gram system but missed by the base system. Ex-
ample (3) shows an example of where the base sys-
tem does not detect the extraneous preposition be-
cause it only searches for these in certain quantifier
constructions. Example (4) contains a spelling error,
which confuses the determiner error detection sys-
tem. It has not seen the misspelling often enough to
be able to reliably judge whether it needs an article
or not before it, and so errs on the side of caution.
When diffender is correctly spelled as defender, the
base system does detect that there is a missing article
in the sentence.
There were a small number of cases where dialect
caused a mismatch between our system?s error pre-
dictions and the gold standard. For example, an ho-
tel is not marked as an error in the gold standard
since it is correct in many dialects. However, it was
always corrected to a hotel by our system. Our sys-
tem also often corrected determiners before the noun
camp, since in American Standard English it is more
usual to talk about going to Summer Camp rather
than going to a/the Summer Camp.
Although the task was to detect preposition and
determiner errors in isolation, there was sometimes
interference from other errors in the sentence. This
impacted the task in two ways. Firstly, in a sentence
239
with multiple errors, it was sometimes possible to
correct it in multiple ways, not all of which involved
preposition or determiner errors. For example, you
could correct the phrase a women by either chang-
ing the a to the, deleting the a entirely or replacing
women with woman. The last change would not fall
under the category of determiner error, and so there
was sometimes a mismatch between the corrections
predicted by the system and the gold standard cor-
rections. Secondly, the presence of multiple errors
impacted the task when a gold standard correction
depended on another error in the same sentence be-
ing corrected in a particular way. For example, you
could correct I?m really excited to read the book. as
I?m really excited about reading the book., however
if you add the preposition about without correcting
to read this correction results in the sentence becom-
ing even more ungrammatical than the original.11
9 Conclusion
In this paper, we have described a hybrid system
for grammatical error detection that combines a pre-
existing base system, which leverages detailed lin-
guistic knowledge and produces high-precision out-
put, with a large-scale n-gram approach, which re-
lies almost exclusively on simple counting of n-
grams in a massive corpus. Though the base system
was not tuned at all for the HOO 2012 Shared Task,
it performed well in the official evaluation. The two
methods also complemented each other well: many
of the predictions from one did not appear in the out-
put of the other, and the F-score of the hybrid system
was considerably higher than the scores for the indi-
vidual methods.
Acknowledgments
We thank Martin Chodorow for discussions about
the base system, Daniel Blanchard for help with run-
ning the base system, Nitin Madnani for discussions
about the paper and for its title, and Michael Flor for
the TrendStream system.
11Many of these cases were addressed in the revised version
of the gold standard data, however we feel that the issue is a
more general one and deserves consideration in the design of
future tasks.
References
Yigal Attali and Jill Burstein. 2006. Automated Es-
say Scoring with e-rater V.2. Journal of Technol-
ogy, Learning, and Assessment, 4(3). Available from
http://www.jtla.org.
Shane Bergsma, Dekang Lin, and Randy Goebel. 2009.
Web-Scale N-gram Models for Lexical Disambigua-
tion. In Proceedings of the 21st international joint
conference on Artifical intelligence, IJCAI?09, pages
1507?1512, Pasadena, California. Morgan Kaufmann
Publishers Inc.
Martin Chodorow and Claudia Leacock. 2000. An Un-
supervised Method for Detecting Grammatical Errors.
In Proceedings of the First Meeting of the North Amer-
ican Chapter of the Association for Computational
Linguistics (NAACL), pages 140?147, Seattle, Wash-
ington. Association for Computational Linguistics.
Martin Chodorow, Joel Tetreault, and Na-Rae Han. 2007.
Detection of Grammatical Errors Involving Preposi-
tions. In Proceedings of the Fourth ACL-SIGSEM
Workshop on Prepositions, pages 25?30, Prague,
Czech Republic. Association for Computational Lin-
guistics.
Robert Dale, Ilya Anisimoff, and George Narroway.
2012. HOO 2012: A Report on the Preposition and
Determiner Error Correction Shared Task. In Proceed-
ings of the Seventh Workshop on Innovative Use of
NLP for Building Educational Applications, Montreal,
Canada. Association for Computational Linguistics.
Michael Flor. 2012. A fast and flexible archi-
tecture for very large word n-gram datasets.
Natural Language Engineering, pages 1?33.
doi:10.1017/S135132491100034.
Michael Gamon. 2010. Using Mostly Native Data to
Correct Errors in Learners? Writing. In Human Lan-
guage Technologies: The 2010 Annual Conference of
the North American Chapter of the Association for
Computational Linguistics, pages 163?171, Los An-
geles, California. Association for Computational Lin-
guistics.
Na-Rae Han, Martin Chodorow, and Claudia Leacock.
2004. Detecting Errors in English Article Usage with
a Maximum Entropy Classifier Trained on a Large, Di-
verse Corpus. In Proceedings of the 4th International
Conference on Language Resources and Evaluation
(LREC 2004), pages 1625?1628, Lisbon, Portugal.
Na-Rae Han, Martin Chodorow, and Claudia Leacock.
2006. Detecting errors in English article usage by
non-native speakers. Natural Language Engineering,
12:115?129. doi:10.1017/S1351324906004190.
Joel R. Tetreault and Martin Chodorow. 2008. The
Ups and Downs of Preposition Error Detection in
240
ESL Writing. In Proceedings of the 22nd Interna-
tional Conference on Computational Linguistics (Col-
ing 2008), pages 865?872, Manchester, UK. Coling
2008 Organizing Committee.
241
Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 48?57,
Atlanta, Georgia, June 13 2013. c?2013 Association for Computational Linguistics
A Report on the First Native Language Identification Shared Task
Joel Tetreault?, Daniel Blanchard? and Aoife Cahill?
? Nuance Communications, Inc., 1198 E. Arques Ave, Sunnyvale, CA 94085, USA
Joel.Tetreault@nuance.com
? Educational Testing Service, 660 Rosedale Road, Princeton, NJ 08541, USA
{dblanchard, acahill}@ets.org
Abstract
Native Language Identification, or NLI, is the
task of automatically classifying the L1 of a
writer based solely on his or her essay writ-
ten in another language. This problem area
has seen a spike in interest in recent years
as it can have an impact on educational ap-
plications tailored towards non-native speak-
ers of a language, as well as authorship pro-
filing. While there has been a growing body
of work in NLI, it has been difficult to com-
pare methodologies because of the different
approaches to pre-processing the data, differ-
ent sets of languages identified, and different
splits of the data used. In this shared task, the
first ever for Native Language Identification,
we sought to address the above issues by pro-
viding a large corpus designed specifically for
NLI, in addition to providing an environment
for systems to be directly compared. In this
paper, we report the results of the shared task.
A total of 29 teams from around the world
competed across three different sub-tasks.
1 Introduction
One quickly growing subfield in NLP is the task
of identifying the native language (L1) of a writer
based solely on a sample of their writing in an-
other language. The task is framed as a classifica-
tion problem where the set of L1s is known a priori.
Most work has focused on identifying the native lan-
guage of writers learning English as a second lan-
guage. To date this topic has motivated several pa-
pers and research projects.
Native Language Identification (NLI) can be use-
ful for a number of applications. NLI can be used in
educational settings to provide more targeted feed-
back to language learners about their errors. It
is well known that speakers of different languages
make different kinds of errors when learning a lan-
guage (Swan and Smith, 2001). A writing tutor
system which can detect the native language of the
learner will be able to tailor the feedback about the
error and contrast it with common properties of the
learner?s language. In addition, native language is
often used as a feature that goes into authorship pro-
filing (Estival et al, 2007), which is frequently used
in forensic linguistics.
Despite the growing interest in this field, devel-
opment has been encumbered by two issues. First
is the issue of data. Evaluating an NLI system re-
quires a corpus containing texts in a language other
than the native language of the writer. Because of
a scarcity of such corpora, most work has used the
International Corpus of Learner English (ICLEv2)
(Granger et al, 2009) for training and evaluation
since it contains several hundred essays written by
college-level English language learners. However,
this corpus is quite small for training and testing
statistical systems which makes it difficult to tell
whether the systems that are developed can scale
well to larger data sets or to different domains.
Since the ICLE corpus was not designed with the
task of NLI in mind, the usability of the corpus for
this task is further compromised by idiosyncrasies
in the data such as topic bias (as shown by Brooke
and Hirst (2011)) and the occurrence of characters
which only appear in essays written by speakers of
certain languages (Tetreault et al, 2012). As a result,
it is hard to draw conclusions about which features
48
actually perform best. The second issue is that there
has been little consistency in the field in the use of
cross-validation, the number of L1s, and which L1s
are used. As a result, comparing one approach to
another has been extremely difficult.
The first Shared Task in Native Language Identifi-
cation is intended to better unify this community and
help the field progress. The Shared Task addresses
the two deficiencies above by first using a new cor-
pus (TOEF11, discussed in Section 3) that is larger
than the ICLE and designed specifically for the task
of NLI and second, by providing a common set of
L1s and evaluation standards that everyone will use
for this competition, thus facilitating direct compar-
ison of approaches. In this report we describe the
methods most participants used, the data they eval-
uated their systems on, the three sub-tasks involved,
the results achieved by the different teams, and some
suggestions and ideas about what we can do for the
next iteration of the NLI shared task.
In the following section, we provide a summary
of the prior work in Native Language Identification.
Next, in Section 3 we describe the TOEFL11 cor-
pus used for training, development and testing in this
shared task. Section 4 describes the three sub-tasks
of the NLI Shared Task as well as a review of the
timeline. Section 5 lists the 29 teams that partici-
pated in the shared task, and introduce abbreviations
that will be used throughout this paper. Sections 6
and 7 describe the results of the shared task and a
separate post shared task evaluation where we asked
teams to evaluate their system using cross-validation
on a combination of the training and development
data. In Section 8 we provide a high-level view of
the common features and machine learning methods
teams tended to use. Finally, we offer conclusions
and ideas for future instantiations of the shared task
in Section 9.
2 Related Work
In this section, we provide an overview of some of
the common approaches used for NLI prior to this
shared task. While a comprehensive review is out-
side the scope of this paper, we have compiled a
bibliography of related work in the field. It can be
downloaded from the NLI Shared Task website.1
To date, nearly all approaches have treated the
task of NLI as a supervised classification problem
where statistical models are trained on data from the
different L1s. The work of Koppel et al (2005) was
the first in the field and they explored a multitude
of features, many of which are employed in several
of the systems in the shared tasks. These features
included character and POS n-grams, content and
function words, as well as spelling and grammati-
cal errors (since language learners have tendencies
to make certain errors based on their L1 (Swan and
Smith, 2001)). An SVM model was trained on these
features extracted from a subsection of the ICLE
corpus consisting of 5 L1s.
N-gram features (word, character and POS) have
figured prominently in prior work. Not only are they
easy to compute, but they can be quite predictive.
However, there are many variations on the features.
Past reseach efforts have explored different n-gram
windows (though most tend to focus on unigrams
and bigrams), different thresholds for how many n-
grams to include as well as whether to encode the
feature as binary (presence or absence of the partic-
ular n-gram) or as a normalized count.
The inclusion of syntactic features has been a fo-
cus in recent work. Wong and Dras (2011) explored
the use of production rules from two parsers and
Swanson and Charniak (2012) explored the use of
Tree Substitution Grammars (TSGs). Tetreault et
al. (2012) also investigated the use of TSGs as well
as dependency features extracted from the Stanford
parser.
Other approaches to NLI have included the use of
Latent Dirichlet Analysis to cluster features (Wong
et al, 2011), adaptor grammars (Wong et al, 2012),
and language models (Tetreault et al, 2012). Ad-
ditionally, there has been research into the effects of
training and testing on different corpora (Brooke and
Hirst, 2011).
Much of the aforementioned work takes the per-
spective of optimizing for the task of Native Lan-
guage Identification, that is, what is the best way of
modeling the problem to get the highest system ac-
curacy? The problem of Native Language Identifica-
1http://nlisharedtask2013.org/bibliography-of-related-
work-in-nli
49
tion is also of interest to researchers in Second Lan-
guage Acquisition where they seek to explain syn-
tactic transfer in learner language (Jarvis and Cross-
ley, 2012).
3 Data
The dataset for the task was the new TOEFL11
corpus (Blanchard et al, 2013). TOEFL11 con-
sists of essays written during a high-stakes college-
entrance test, the Test of English as a Foreign Lan-
guage (TOEFL R?). The corpus contains 1,100 es-
says per language sampled as evenly as possible
from 8 prompts (i.e., topics) along with score lev-
els (low/medium/high) for each essay. The 11 na-
tive languages covered by our corpus are: Ara-
bic (ARA), Chinese (CHI), French (FRE), German
(GER), Hindi (HIN), Italian (ITA), Japanese (JAP),
Korean (KOR), Spanish (SPA), Telugu (TEL), and
Turkish (TUR).
The TOEFL11 corpus was designed specifically
to support the task of native language identifica-
tion. Because all of the essays were collected
through ETS?s operational test delivery system for
the TOEFL R? test, the encoding and storage of all
texts in the corpus is consistent. Furthermore, the
sampling of essays was designed to ensure approx-
imately equal representation of native languages
across topics, insofar as this was possible.
For the shared task, the corpus was split into
three sets: training (TOEFL11-TRAIN), development
(TOEFL11-DEV), and test (TOEFL11-TEST). The
train corpus consisted of 900 essays per L1, the de-
velopment set consisted of 100 essays per L1, and
the test set consisted of another 100 essays per L1.
Although the overall TOEFL11 corpus was sampled
as evenly as possible with regard to language and
prompts, the distribution for each language is not ex-
actly the same in the training, development and test
sets (see Tables 1a, 1b, and 1c). In fact, the distri-
bution is much closer between the training and test
sets, as there are several languages for which there
are no essays for a given prompt in the development
set, whereas there are none in the training set, and
only one, Italian, for the test set.
It should be noted that in the first instantiation of
the corpus, presented in Tetreault et al (2012), we
used TOEFL11 to denote the body of data consisting
of TOEFL11-TRAIN and TOEFL11-DEV. However,
in this shared task, we added 1,100 sentences for a
test set and thus use the term TOEFL11 to now de-
note the corpus consisting of the TRAIN, DEV and
TEST sets. We expect the corpus to be released
through the the Linguistic Data Consortium in 2013.
4 NLI Shared Task Description
The shared task consisted of three sub-tasks. For
each task, the test set was TOEFL11-TEST and only
the type of training data varied from task to task.
? Closed-Training: The first and main task
was the 11-way classification task using only
the TOEFL11-TRAIN and optionally TOEFL11-
DEV for training.
? Open-Training-1: The second task allowed
the use of any amount or type of training data
(as is done by Brooke and Hirst (2011)) exclud-
ing any data from the TOEFL11, but still evalu-
ated on TOEFL11-TEST.
? Open-Training-2: The third task allowed the
use of TOEFL11-TRAIN and TOEFL11-DEV
combined with any other additional data. This
most closely reflects a real-world scenario.
Additionally, each team could submit up to 5 dif-
ferent systems per task. This allowed a team to ex-
periment with different variations of their core sys-
tem.
The training data was released on January 14,
with the development data and evaluation script re-
leased almost one month later on February 12. The
train and dev data contained an index file with the L1
for each essay in those sets. The previously unseen
and unlabeled test data was released on March 11
and teams had 8 days to submit their system predic-
tions. The predictions for each system were encoded
in a CSV file, where each line contained the file ID
of a file in TOEFL11-TEST and the corresponding
L1 prediction made by the system. Each CSV file
was emailed to the NLI organizers and then evalu-
ated against the gold standard.
5 Teams
In total, 29 teams competed in the shared task com-
petition, with 24 teams electing to write papers de-
scribing their system(s). The list of participating
50
Lang. P1 P2 P3 P4 P5 P6 P7 P8
ARA 113 113 113 112 112 113 112 112
CHI 113 113 113 112 112 113 112 112
FRE 128 128 76 127 127 60 127 127
GER 125 125 125 125 125 26 125 124
HIN 132 132 132 71 132 38 132 131
ITA 142 70 122 141 141 12 141 131
JAP 108 114 113 113 113 113 113 113
KOR 113 113 113 112 112 113 112 112
SPA 124 120 38 124 123 124 124 123
TEL 139 139 139 41 139 26 139 138
TUR 132 132 72 132 132 37 132 131
Total 1369 1299 1156 1210 1368 775 1369 1354
(a) Training Set
Lang. P1 P2 P3 P4 P5 P6 P7 P8
ARA 12 13 13 13 14 7 14 14
CHI 14 14 0 15 15 14 13 15
FRE 17 18 0 14 19 0 13 19
GER 15 15 16 10 13 0 15 16
HIN 16 17 17 0 17 0 16 17
ITA 18 0 0 30 31 0 21 0
JAP 0 14 15 14 15 14 14 14
KOR 15 8 15 2 13 15 16 16
SPA 7 0 0 21 7 21 21 23
TEL 16 17 17 0 17 0 16 17
TUR 22 4 0 22 7 0 22 23
Total 152 120 93 141 168 71 181 174
(b) Dev Set
Lang. P1 P2 P3 P4 P5 P6 P7 P8
ARA 13 11 12 14 10 13 12 15
CHI 13 14 13 13 7 14 14 12
FRE 13 14 11 15 14 8 11 14
GER 15 14 16 16 12 2 12 13
HIN 13 13 14 15 7 15 10 13
ITA 13 19 16 16 15 0 11 10
JAP 8 14 12 11 10 15 14 16
KOR 12 12 8 14 12 14 13 15
SPA 10 13 16 14 4 12 15 16
TEL 10 10 11 14 13 15 11 16
TUR 15 9 18 16 8 6 13 15
Total 135 143 147 158 112 114 136 155
(c) Test Set
Table 1: Number of essays per language per prompt in each data set
teams, along with their abbreviations, can be found
in Table 2.
6 Shared Task Results
This section summarizes the results of the shared
task. For each sub-task, we have tables listing the
51
Team Name Abbreviation
Bobicev BOB
Chonger CHO
CMU-Haifa HAI
Cologne-Nijmegen CN
CoRAL Lab @ UAB COR
CUNI (Charles University) CUN
cywu CYW
dartmouth DAR
eurac EUR
HAUTCS HAU
ItaliaNLP ITA
Jarvis JAR
kyle, crossley, dai, mcnamara KYL
LIMSI LIM
LTRC IIIT Hyderabad HYD
Michigan MIC
MITRE ?Carnie? CAR
MQ MQ
NAIST NAI
NRC NRC
Oslo NLI OSL
Toronto TOR
Tuebingen TUE
Ualberta UAB
UKP UKP
Unibuc BUC
UNT UNT
UTD UTD
VTEX VTX
Table 2: Participating Teams and Team Abbrevia-
tions
top submission for each team and its performance
by overall accuracy and by L1.2
Table 3 shows results for the Closed sub-task
where teams developed systems that were trained
solely on TOEFL11-TRAIN and TOEFL11-DEV. This
was the most popular sub-task with 29 teams com-
peting and 116 submissions in total for the sub-task.
Most teams opted to submit 4 or 5 runs.
The Open sub-tasks had far fewer submissions.
Table 4 shows results for the Open-1 sub-task where
teams could train systems using any training data ex-
cluding TOEFL11-TRAIN and TOEFL11-DEV. Three
teams competed in this sub-task for a total of 13 sub-
2For those interested in the results of all submissions, please
contact the authors.
missions. Table 5 shows the results for the third sub-
task ?Open-2?. Four teams competed in this task for
a total of 15 submissions.
The challenge for those competing in the Open
tasks was finding enough non-TOEFL11 data for
each L1 to train a classifier. External corpora com-
monly used in the competition included the:
? ICLE: which covered all L1s except for Ara-
bic, Hindi and Telugu;
? FCE: First Certificate in English Corpus
(Yannakoudakis et al, 2011): a collection of
essay written for an English assessment exam,
which covered all L1s except for Arabic, Hindi
and Telugu
? ICNALE: International Corpus Network of
Asian Learners of English (Ishikawa, 2011):
a collection of essays written by Chinese,
Japanese and Korean learners of English along
with 7 other L1s with Asian backgrounds.
? Lang8: http://www.lang8.com: a social net-
working service where users write in the lan-
guage they are learning, and get corrections
from users who are native speakers of that lan-
guage. Shared Task participants such as NAI
and TOR scraped the website for all writng
samples from English language learners. All
of the L1s in the shared task are represented on
the site, though the Asian L1s dominate.
The most challenging L1s to find data for seemed
to be Hindi and Telugu. TUE used essays written
by Pakastani students in the ICNALE corpus to sub-
stitute for Hindi. For Telugu, they scraped mate-
rial from bilingual blogs (English-Telugu) as well
as other material for the web. TOR created cor-
pora for Telugu and Hindi by scraping news articles,
tweets which were geolocated in the Hindi and Tel-
ugu speaking areas, and translations of Hindi and
Telugu blogs using Google Translate.
We caution directly comparing the results of the
Closed sub-task to the Open ones. In the Open-1
sub-task most teams had smaller training sets than
used in the Closed competition which automatically
puts them at a disadvantage, and in some cases there
52
L1 F-Score
Team
Name
Run Overall
Acc.
ARA CHI FRE GER HIN ITA JPN KOR SPA TEL TUR
JAR 2 0.836 0.785 0.856 0.860 0.893 0.775 0.905 0.854 0.813 0.798 0.802 0.854
OSL 2 0.834 0.816 0.850 0.874 0.912 0.792 0.873 0.828 0.806 0.783 0.792 0.840
BUC 5 0.827 0.840 0.866 0.853 0.931 0.736 0.873 0.851 0.812 0.779 0.760 0.796
CAR 2 0.826 0.859 0.847 0.810 0.921 0.762 0.877 0.825 0.827 0.768 0.802 0.790
TUE 1 0.822 0.810 0.853 0.806 0.897 0.768 0.883 0.842 0.776 0.772 0.824 0.812
NRC 4 0.818 0.804 0.845 0.848 0.916 0.745 0.903 0.818 0.790 0.788 0.755 0.790
HAI 1 0.815 0.804 0.842 0.835 0.903 0.759 0.845 0.825 0.806 0.776 0.789 0.784
CN 2 0.814 0.778 0.845 0.848 0.882 0.744 0.857 0.812 0.779 0.787 0.784 0.827
NAI 1 0.811 0.814 0.829 0.828 0.876 0.755 0.864 0.806 0.789 0.757 0.793 0.802
UTD 2 0.809 0.778 0.846 0.832 0.892 0.731 0.866 0.846 0.819 0.715 0.784 0.784
UAB 3 0.803 0.820 0.804 0.822 0.905 0.724 0.850 0.811 0.736 0.777 0.792 0.786
TOR 1 0.802 0.754 0.827 0.827 0.878 0.722 0.850 0.820 0.808 0.747 0.784 0.798
MQ 4 0.801 0.800 0.828 0.789 0.885 0.738 0.863 0.826 0.780 0.703 0.782 0.802
CYW 1 0.797 0.769 0.839 0.782 0.833 0.755 0.842 0.815 0.770 0.741 0.828 0.788
DAR 2 0.781 0.761 0.806 0.812 0.870 0.706 0.846 0.788 0.776 0.730 0.723 0.767
ITA 1 0.779 0.738 0.775 0.832 0.873 0.711 0.860 0.788 0.742 0.708 0.762 0.780
CHO 1 0.775 0.764 0.835 0.798 0.888 0.721 0.816 0.783 0.670 0.688 0.786 0.758
HAU 1 0.773 0.731 0.820 0.806 0.897 0.686 0.830 0.832 0.763 0.703 0.702 0.736
LIM 4 0.756 0.737 0.760 0.788 0.886 0.654 0.808 0.775 0.756 0.712 0.701 0.745
COR 5 0.748 0.704 0.806 0.783 0.898 0.670 0.738 0.794 0.739 0.616 0.730 0.741
HYD 1 0.744 0.680 0.778 0.748 0.839 0.693 0.788 0.781 0.735 0.613 0.770 0.754
CUN 1 0.725 0.696 0.743 0.737 0.830 0.714 0.838 0.676 0.670 0.680 0.697 0.684
UNT 3 0.645 0.667 0.682 0.635 0.746 0.558 0.687 0.676 0.620 0.539 0.667 0.609
BOB 4 0.625 0.513 0.684 0.638 0.751 0.612 0.706 0.647 0.549 0.495 0.621 0.608
KYL 1 0.590 0.589 0.603 0.643 0.634 0.554 0.663 0.627 0.569 0.450 0.649 0.507
UKP 2 0.583 0.592 0.560 0.624 0.653 0.558 0.616 0.631 0.565 0.456 0.656 0.489
MIC 3 0.430 0.419 0.386 0.411 0.519 0.407 0.488 0.422 0.384 0.400 0.500 0.396
EUR 1 0.386 0.500 0.390 0.277 0.379 0.487 0.522 0.441 0.352 0.281 0.438 0.261
VTX 5 0.319 0.367 0.298 0.179 0.297 0.159 0.435 0.340 0.370 0.201 0.410 0.230
Table 3: Results for closed task
L1 F-Score
Team
Name
Run Overall
Acc.
ARA CHI FRE GER HIN ITA JPN KOR SPA TEL TUR
TOR 5 0.565 0.410 0.776 0.692 0.754 0.277 0.680 0.660 0.650 0.653 0.190 0.468
TUE 2 0.385 0.114 0.502 0.420 0.430 0.167 0.611 0.485 0.348 0.385 0.236 0.314
NAI 2 0.356 0.329 0.450 0.331 0.423 0.066 0.511 0.426 0.481 0.314 0.000 0.207
Table 4: Results for open-1 task
L1 F-Score
Team
Name
Run Overall
Acc.
ARA CHI FRE GER HIN ITA JPN KOR SPA TEL TUR
TUE 1 0.835 0.798 0.876 0.844 0.883 0.777 0.883 0.836 0.794 0.846 0.826 0.818
TOR 4 0.816 0.770 0.861 0.840 0.900 0.704 0.860 0.834 0.800 0.816 0.804 0.790
HYD 1 0.741 0.677 0.782 0.755 0.829 0.693 0.784 0.777 0.728 0.613 0.766 0.744
NAI 3 0.703 0.676 0.695 0.708 0.846 0.618 0.830 0.677 0.610 0.663 0.726 0.688
Table 5: Results for open-2 task
53
was a mismatch in the genre of corpora (for exam-
ple, tweets by Telugu speakers are different in com-
position than essays written by Telugu speakers).
TUE and TOR were the only two teams to partic-
ipate in all three sub-tasks, and their Open-2 sys-
tems outperformed their respective best systems in
the Closed and Open-1 sub-tasks. This suggests, un-
surprisingly, that adding more data can benefit NLI,
though quality and genre of data are also important
factors.
7 Cross Validation Results
Upon completion of the competition, we asked the
participants to perform 10-fold cross-validation on a
data set consisting of the union of TOEFL11-TRAIN
and TOEFL11-DEV. This was the same set of data
used in the first work to use any of the TOEFL11
data (Tetreault et al, 2012), and would allow another
point of comparison for future NLI work. For direct
comparison with Tetreault et al (2012), we provided
the exact folds used in that work.
The results of the 10-fold cross-validation are
shown in Table 6. Two teams had systems that per-
formed at 84.5 or better, which is just slightly higher
than the best team performance on the TOEFL11-
TEST data. In general, systems that performed well
in the main competition also performed similarly
(in terms of performance and ranking) in the cross-
validation experiment. Please note that we report
results as they are reported in the respective papers,
rounding to just one decimal place where possible.
8 Discussion of Approaches
With so many teams competing in the shared task
competition, we investigated whether there were any
commonalities in learning methods or features be-
tween the teams. In this section, we provide a coarse
grained summary of the common machine learning
methods teams employed as well as some of the
common features. Our summary is based on the in-
formation provided in the 24 team reports.
While there are many machine learning algo-
rithms to choose from, the overwhelming majority
of teams used Support Vector Machines. This may
not be surprising given that most prior work has also
used SVMs. Tetreault et al (2012) showed that one
could achieve even higher performance on the NLI
Team Accuracy
CN 84.6
JAR 84.5
OSL 83.9
BUC 82.6
MQ 82.5
TUE 82.4
CAR 82.2
NAI 82.1
Tetreault et al (2012) 80.9
HAU 79.9
LIM 75.9
CUN 74.2
UNT 63.8
MIC 63
Table 6: Results for 10-fold cross-validation on
TOEFL11-TRAIN + TOEFL11-DEV
task using ensemble methods for combining classi-
fiers. Four teams also experimented with different
ways of using ensemble methods. Three teams used
Maximum Entropy methods for their modeling. Fi-
nally, there were a few other teams that tried differ-
ent methods such as Discriminant Function Analysis
and K-Nearest Neighbors. Possibly the most distinct
method employed was that of string kernels by the
BUC team (who placed third in the closed compe-
tition). This method only used character level fea-
tures. A summary of the machine learning methods
is shown in Table 7.
A summary of the common features used across
teams is shown in Table 8. It should be noted that
the table does not detail the nuanced differences in
how the features are realized. For example, in the
case of n-grams, some teams used only the top k
most frequently n-grams while others used all of the
n-grams available. If interested in more information
about the particulars of a system and its feature, we
recommend reading the team?s summary report.
The most common features were word, character
and POS n-gram features. Most teams used n-grams
ranging from unigrams to trigrams, in line with prior
literature. However several teams used higher-order
n-grams. In fact, four of the top five teams (JAR,
OSL, CAR, TUE) generally used at least 4-grams,
54
Machine Learning Teams
SVM CN, UNT, MQ, JAR, TOR, ITA, CUN, TUE, COR, NRC, HAU, MIC, CAR
MaxEnt / logistic regression LIM, HAI, CAR
Ensemble MQ, ITA, NRC, CAR
Discriminant Function Analysis KYL
String Kernels / LRD BUC
PPM BOB
k-NN VTX
Table 7: Machine Learning algorithms used in Shared Task
and some, such as OSL and JAR, went as high 7 and
9 respectively in terms of character n-grams.
Syntactic features, which were first evaluated in
Wong and Dras (2011) and Swanson and Char-
niak (2012) were used by six teams in the competi-
tion, with most using dependency parses in different
ways. Interestingly, while Wong and Dras (2011)
showed some of the highest performance scores on
the ICLE corpus using parse features, only two of
the six teams which used them placed in the top ten
in the Closed sub-task.
Spelling features were championed by Koppel et
al. (2005) and in subsequent NLI work, however
only three teams in the competition used them.
There were several novel features that teams tried.
For example, several teams tried skip n-grams, as
well as length of words, sentences and documents;
LIM experimented with machine translation; CUN
had different features based on the relative frequen-
cies of the POS and lemma of a word; HAI tried
several new features based on passives and context
function; and the TUE team tried a battery of syn-
tactic features as well as text complexity measures.
9 Summary
We consider the first edition of the shared task a
success as we had 29 teams competing, which we
consider a large number for any shared task. Also
of note is that the task brought together researchers
not only from the Computational Linguistics com-
munity, but also those from other linguistics fields
such as Second Language Acquisition.
We were also delighted to see many teams build
on prior work but also try novel approaches. It is
our hope that finally having an evaluation on a com-
mon data set will allow researchers to learn from
each other on what works well and what does not,
and thus the field can progress more rapidly. The
evaluation scripts are publicly available and we ex-
pect that the data will become available through the
Linguistic Data Consortium in 2013.
For future editions of the NLI shared task, we
think it would be interesting to expand the scope of
NLI from identifying the L1 of student essays to be
able to identify the L1 of any piece of writing. The
ICLE and TOEFL11 corpora are both collections of
academic writing and thus it may be the case that
certain features or methodologies generalize better
to other writing genres and domains. For those in-
terested in robust NLI approaches, please refer to the
TOR team shared task report as well as Brooke and
Hirst (2012).
In addition, since the TOEFL11 data contains pro-
ficiency level one could include an evaluation by
proficiency level as language learners make differ-
ent types of errors and may even have stylistic differ-
ences in their writing as their proficiency progresses.
Finally, while this may be in the periphery of the
scope of an NLI shared task, one interesting evalua-
tion is to see how well human raters can fare on this
task. This would of course involve knowledgeable
language instructors who have years of experience
in teaching students from different L1s. Our think-
ing is that NLI might be one task where computers
would outperform human annotators.
Acknowledgments
We would like to thank Derrick Higgins and mem-
bers of Educational Testing Service for assisting us
in making the TOEFL11 essays available for this
shared task. We would also like to thank Patrick
Houghton for assisting the shared task organizers.
55
Feature Type Teams
Word N-Grams 1 CN, UNT, JAR, TOR, KYL, ITA, CUN, BOB, OSL, TUE, UAB,
CYW, NAI, NRC, MIC, CAR
2 CN, UNT, JAR, TOR, KYL, ITA, CUN, BOB, OSL, TUE, COR,
UAB, CYW, NAI, NRC, HAU, MIC, CAR
3 UNT, MQ, JAR, KYL, CUN, COR, HAU, MIC, CAR
4 JAR, KYL, CAR
5 CAR
POS N-grams 1 CN, UNT, JAR, TOR, ITA, LIM, CUN, BOB, TUE, HAI, CAR
2 CN, UNT, JAR, TOR, ITA, LIM, CUN, BOB, TUE, COR, HAI,
NAI, NRC, MIC, CAR
3 CN, UNT, JAR, TOR, LIM, CUN, TUE, COR, HAI, NAI, NRC,
CAR
4 CN, JAR, TUE, HAI, NRC, CAR
5 TUE, CAR
Character N-Grams 1 CN, UNT, MQ, JAR, TOR, LIM, BOB, OSL, HAI, CAR
2 CN, UNT, MQ, JAR, TOR, ITA, LIM, BOB, OSL, COR, HAI, NAI,
HAU, MIC, CAR
3 CN, UNT, MQ, JAR, TOR, LIM, BOB, OSL, VTX, COR, HAI,
NAI, NRC, HAU, MIC, CAR
4 CN, JAR, LIM, BOB, OSL, HAI, HAU, MIC, CAR
5 CN, JAR, BOB, OSL, HAU, CAR
6 CN, JAR, OSL,
7 JAR, OSL
8-9 JAR
Function N-Grams MQ, UAB
Syntactic Features Dependencies MQ, TOR, ITA, TUE, NAI, NRC
TSG MQ, TOR, NAI,
CF Productions TOR,
Adaptor Grammars MQ
Spelling Features LIM,CN, HAI
Table 8: Common Features used in Shared Task
In addition, thanks goes to the BEA8 Organizers
(Joel Tetreault, Jill Burstein and Claudia Leacock)
for hosting the shared task with their workshop. Fi-
nally, we would like to thank all the teams for partic-
ipating in this first shared task and making it a suc-
cess. Their feedback, patience and enthusiasm made
organizing this shared task a great experience.
References
Daniel Blanchard, Joel Tetreault, Derrick Higgins, Aoife
Cahill, and Martin Chodorow. 2013. TOEFL11: A
Corpus of Non-Native English. Technical report, Ed-
ucational Testing Service.
Julian Brooke and Graeme Hirst. 2011. Native language
detection with ?cheap? learner corpora. In Conference
of Learner Corpus Research (LCR2011), Louvain-la-
Neuve, Belgium. Presses universitaires de Louvain.
Julian Brooke and Graeme Hirst. 2012. Robust, Lexical-
ized Native Language Identification. In Proceedings
of COLING 2012, pages 391?408, Mumbai, India, De-
cember. The COLING 2012 Organizing Committee.
Dominique Estival, Tanja Gaustad, Son Bao Pham, Will
Radford, and Ben Hutchinson. 2007. Author profiling
for English emails. In Proceedings of the 10th Con-
ference of the Pacific Association for Computational
Linguistics, pages 263?272, Melbourne, Australia.
Sylviane Granger, Estelle Dagneaux, and Fanny Meunier.
2009. The International Corpus of Learner English:
Handbook and CD-ROM, version 2. Presses Universi-
taires de Louvain, Louvain-la-Neuve, Belgium.
Shin?ichiro Ishikawa. 2011. A New Horizon in Learner
Corpus Studies: The Aim of the ICNALE Projects. In
G. Weir, S. Ishikawa, and K. Poonpon, editors, Cor-
56
pora and Language Technologies in Teaching, Learn-
ing and Research. University of Strathclyde Publish-
ing.
Scott Jarvis and Scott Crossley, editors. 2012. Approach-
ing Language Transfer Through Text Classification:
Explorations in the Detection-based Approach, vol-
ume 64. Multilingual Matters Limited, Bristol, UK.
Moshe Koppel, Jonathan Schler, and Kfir Zigdon. 2005.
Determining an author?s native language by mining a
text for errors. In Proceedings of the eleventh ACM
SIGKDD international conference on Knowledge dis-
covery in data mining, pages 624?628, Chicago, IL.
ACM.
Michael Swan and Bernard Smith, editors. 2001.
Learner English: A teacher?s guide to interference and
other problems. Cambridge University Press, 2 edi-
tion.
Benjamin Swanson and Eugene Charniak. 2012. Na-
tive Language Detection with Tree Substitution Gram-
mars. In Proceedings of the 50th Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 2: Short Papers), pages 193?197, Jeju Island, Ko-
rea, July. Association for Computational Linguistics.
Joel Tetreault, Daniel Blanchard, Aoife Cahill, and Mar-
tin Chodorow. 2012. Native tongues, lost and
found: Resources and empirical evaluations in native
language identification. In Proceedings of COLING
2012, pages 2585?2602, Mumbai, India, December.
The COLING 2012 Organizing Committee.
Sze-Meng Jojo Wong and Mark Dras. 2011. Exploiting
Parse Structures for Native Language Identification.
In Proceedings of the 2011 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1600?1610, Edinburgh, Scotland, UK., July. Associa-
tion for Computational Linguistics.
Sze-Meng Jojo Wong, Mark Dras, and Mark Johnson.
2011. Topic Modeling for Native Language Identifi-
cation. In Proceedings of the Australasian Language
Technology Association Workshop 2011, pages 115?
124, Canberra, Australia, December.
Sze-Meng Jojo Wong, Mark Dras, and Mark Johnson.
2012. Exploring Adaptor Grammars for Native Lan-
guage Identification. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 699?709, Jeju Island, Korea,
July. Association for Computational Linguistics.
Helen Yannakoudakis, Ted Briscoe, and Ben Medlock.
2011. A New Dataset and Method for Automati-
cally Grading ESOL Texts. In Proceedings of the
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies,
pages 180?189, Portland, Oregon, USA, June. Associ-
ation for Computational Linguistics.
57
Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 300?305,
Atlanta, Georgia, June 13 2013. c?2013 Association for Computational Linguistics
Detecting Missing Hyphens in Learner Text
Aoife Cahill?, Martin Chodorow?, Susanne Wolff? and Nitin Madnani?
? Educational Testing Service, 660 Rosedale Road, Princeton, NJ 08541, USA
{acahill, swolff, nmadnani}@ets.org
? Hunter College and the Graduate Center, City University of New York, NY 10065, USA
martin.chodorow@hunter.cuny.edu
Abstract
We present a method for automatically de-
tecting missing hyphens in English text. Our
method goes beyond a purely dictionary-based
approach and also takes context into account.
We evaluate our model on artificially gener-
ated data as well as naturally occurring learner
text. Our best-performing model achieves
high precision and reasonable recall, making
it suitable for inclusion in a system that gives
feedback to language learners.
1 Introduction
While errors of punctuation are not as frequent, nor
often as serious, as some of the other typical mis-
takes that learners make, they are nevertheless an
important consideration for students aiming to im-
prove the overall quality of their writing. In this pa-
per we focus on the error of missing hyphens. The
following example is a typical mistake made by a
student writer:
(1) Schools may have more after school sports.
In this case the tokens after and school should be hy-
phenated as they modify the noun sports. However,
in Example (2) a hyphen between after and school
would be incorrect, since in this instance after func-
tions as as the head of a prepositional phrase modi-
fying went.
(2) I went to the dentist after school today.
These examples illustrate that purely dictionary-
based approaches to detecting missing hyphens are
not likely to be sophisticated enough to differentiate
the contexts in which a hyphen is required. In addi-
tion, learner text frequently contains other grammat-
ical and spelling errors, further complicating auto-
matic error detection. Example (3) contains an error
father like instead of father likes to. This causes dif-
ficulty for automated hyphenation systems because
like is a frequent suffix of hyphenated words and
play can function as a noun.
(3) My father like play basketball with me.
In this paper, we propose a classifier-based approach
to automatically detecting missing hyphen errors.
The goal of our system is to detect missing hyphen
errors and provide feedback to language learners.
Therefore, we place more importance on the preci-
sion of the system than recall. We train our model on
features that take the context of a pair of words into
account, as well as other discriminative features. We
present a number of evaluations on both artificially
generated errors and naturally occurring learner er-
rors and show that our classifiers achieve high preci-
sion and reasonable recall.
2 Related Work
The task of detecting missing hyphens is related to
previous work on detecting punctuation errors. One
of the classes of errors in the Helping Our Own
(HOO) 2011 shared task (Dale and Kilgarriff, 2011)
was punctuation. Comma errors are the most fre-
quent kind of punctuation error made by learners. Is-
rael et al (2012) present a model for detecting these
kinds of errors in learner texts. They train CRF mod-
els on sentences from unedited essays written by
high-level college students and show that they per-
forms well on detecting errors in learner text. As
300
far as we are aware, the HOO 2011 system descrip-
tion of Rozovskaya et al (2011) is the only work to
specifically reference hyphen errors. They use rules
derived from frequencies in the training corpus to
determine whether a hyphen was required between
two words separated by white space.
The task of detecting missing hyphens is related
to the task of inserting punctuation into the output of
unpunctuated text (for example, the output of speech
recognition, automatic generation, machine transla-
tion, etc.). Systems that are built on the output of
speech recognition can obviously take features like
prosody into account. In our case, we are deal-
ing only with written text. Gravano et al (2009)
present an n-gram-based model for automatically
adding punctuation and capitalization to the output
of an ASR system, without taking any of the speech
signal information into account. They conclude that
more training data, rather than wider n-gram con-
texts leads to a greater improvement in accuracy.
3 Baselines
We implement three baseline systems which we will
later compare to our classification approach. The
first baseline is a na??ve heuristic that predicts a miss-
ing hyphen between bigrams that appear hyphenated
in the Collins Dictionary.1 As a somewhat less-
na??ve baseline, we implement a heuristic that pre-
dicts a missing hyphen between bigrams that occur
hyphenated more than 1,000 times in Wikipedia. A
third baseline is a heuristic that predicts a missing
hyphen between bigrams where the probability of
the hyphenated form as estimated from Wikipedia
is greater than 0.66, meaning that the hyphenated
bigram is twice as likely as the non-hyphenated bi-
gram. This baseline is similar to the approach taken
by Rozovskaya et al (2011), except that the proba-
bilities are estimated from a much larger corpus.
4 System Description
Using the features in Table 1, we build a logis-
tic regression model which assigns a probability to
the likelihood of a hyphen occurring between two
words, wi and wi+1. As we are primarily interested
in using this system for giving feedback to language
learners, we require very high precision. Therefore,
1LDC catalog number LDC93T1
Tokens wi?1, wi, wi+1, wi+2
Stems si?1, si, si+1, si+2
Tags ti?1, ti, ti+1, ti+2
Bigrams wi?wi+1, si?si+1, ti?ti+1
Dict Does the hyphenated form appear in
the Collins dictionary?
Prob What is the probability of the word
bigram appearing hyphenated in
Wikipedia?
Distance Distance to following and preced-
ing verb, noun
Verb/Noun Is there a verb/noun preced-
ing/following this bigram
Table 1: Features used in all models. Positive in-
stances are those where there was a hyphen between
wi and wi+1 in the data. Stems are generated using
NLTK?s implementation of the Lancaster Stemmer,
and tags are obtained from the Stanford Parser.
we only predict a missing hyphen error when the
probability of the prediction is >0.99.
We experiment with two different sources of
training data, in addition to their combination. We
first train on well-edited text, using almost 1.8 mil-
lion sentences from the San Jose Mercury News cor-
pus.2 For training, hyphenated words are automati-
cally split (i.e. well-known becomes well known).
The positive examples for the classifier are all bi-
grams where a hyphen was removed. Negative ex-
amples consist of bigrams where there was no hy-
phen in the training data. Since this is over 99% of
the data, we randomly sample 3% of the negative
examples for training. We also restrict the negative
examples to only the most likely contexts, where a
context is defined as a part-of-speech bigram. A list
of possible contexts in which hyphens occur is ex-
tracted from the entire training set. Only contexts
that occur more than 20 times are selected during
training. All contexts are evaluated during testing.
Table 2 lists some of the most frequent contexts with
examples of when they should be hyphenated and
when they should remain unhyphenated.
The second data source for training the model
comes from pairs of revisions from Wikipedia ar-
ticles. Following Cahill et al (2013), we automati-
cally extract a corpus of error annotations for miss-
2LDC catalog number LDC93T3A.
301
Context Hyphenated Unhyphenated
NN NN terrific truck-stop
waitress
a quake insurance
surcharge
CD CD Twenty-two thou-
sand
the 126 million
Americans
JJ NN an early-morning
blaze
an entire practice
session
CD NN a two-year contract about 600 tank cars
NN VBN a court-ordered
program
a letter delivered to-
day
Table 2: Some frequent likely POS contexts for hy-
phenation, with examples from the Brown corpus.
ing hyphens. This is done by extracting the plain
text from every revision to every article and com-
paring adjacent pairs of revisions. For each article,
chains of errors are detected, using the surrounding
text to identify them. When a chain begins and ends
with the same form, it is ignored. Only the first and
last points in an error chain are retained for train-
ing. An example chain is the following: It has been
an ancient {focal point ? location ? focal point
? focal-point} of trade and migration., where we
would extract the correction focal point ? focal-
point. In total, we extract a corpus of 390,298 sen-
tences containing missing hyphen error annotations.
Finally, we combine both data sources.
5 Evaluating on Artificial Data
Since there are large corpora of well-edited text
readily available, it is easy to evaluate on artifi-
cial data. For testing, we take 24,243 sentences
from the Brown corpus and automatically remove
hyphens from the 2,072 hyphenated words (but not
free-standing dashes). Each system makes a predic-
tion for all bigrams about whether a hyphen should
appear between the pair of words. We measure the
performance of each system in terms of precision, P,
(how many of the missing hyphen errors predicted
by the system were true errors), recall, R, (how many
of the artificially removed hyphens the system de-
tected as errors) and f-score, F, (the harmonic mean
of precision and recall). The results are given in
Table 3, and also include the raw number of true
positives, TP, detected by each system. The results
show that the baseline using Wikipedia probabilities
obtains the highest precision, however with low re-
call. The classifiers trained on newswire text and the
TP P R F
Baseline
Collins dict 397 40.5 19.2 26.0
Wiki Counts-1000 359 39.1 17.3 24.0
Wiki Probs-0.66 811 85.5 39.1 53.7
Classifier
SJM-trained 1097 82.0 52.9 64.3
Wiki-revision-trained 1061 72.8 51.2 60.1
Combined 1106 80.9 53.4 64.3
Table 3: Results of evaluating on the Brown Corpus
with hyphens removed
combined news and Wikipedia revision text achieve
the highest overall f-score. Figure (1a) shows the
Precision Recall curves for the Wikipedia baselines
and the three classifiers. The curves mirror the re-
sults in the table, showing that the classifier trained
on the newswire text, and the classifier trained on the
combined data perform best. The Wikipedia counts
baseline performs worst.
6 Evaluating on Learner Text
We carry out two evaluations of our system on
learner text. We first evaluate on the missing hyphen
errors contained in the CLC-FCE (Yannakoudakis et
al., 2011). This corpus contains 1,244 exam scripts
written by learners of English as part of the Cam-
bridge ESOL First Certificate in English. In total,
there are 173 instances of missing hyphen errors.
The results are given in Table 4, and the precision
recall curves are displayed in Figure (1b).
The results show that the classifiers consistently
achieve high precision on this data set. This is as
expected, given the high threshold set. Looking at
the curves, it seems that a slightly lower threshold in
this case may lead to better results. The curves show
that the combined classifier is performing slightly
better than the other two classifiers. The baselines
are clearly not performing as well on this dataset.
While the overall size of the CLC-FCE data set
is quite large, the low frequency of this kind of er-
ror means that the evaluation was carried out on a
relatively small number of examples. For this rea-
son, the reliability of the results may be called into
question. There is, for instance, a striking difference
between the f-scores for the Collins Dictionary base-
302
0.0 0.2 0.4 0.6 0.8
0.0
0.2
0.4
0.6
0.8
1.0
Recall
Prec
ision
combinedsjmwikibaseline?wiki?countsbaseline?wiki?probs
(a) Brown Corpus
0.2 0.4 0.6 0.8 1.00
.0
0.2
0.4
0.6
0.8
1.0
Recall
Prec
ision
combinedsjmwikibaseline?wiki?countsbaseline?wiki?probs
(b) CLC-FCE Corpus
Figure 1: Precision Recall curves for the Wikipedia baselines and the three classifiers.
TP P R F
Baseline
Collins dict 131 64.5 75.7 69.7
Wiki Counts-1000 141 73.1 81.5 77.0
Wiki Probs-0.66 36 92.3 20.8 34.0
Classifier
SJM-trained 60 84.5 34.7 49.2
Wiki-revision-trained 71 98.6 41.0 58.0
Combined 66 98.5 38.2 55.0
Table 4: Results of evaluating on the CLC-FCE
dataset
line on the Brown corpus (26.0) and on the learner
data (69.7). Inspection of the 131 true positives for
the learner data reveal that 87 of these are cases of a
single type, the word ?make-up?, which students of-
ten wrote without a hyphen in response to a prompt
about a fashion and leisure show. Since the hyphen-
ated form was in the Collins Dictionary, the base-
line system was credited with detection of this error.
However, when the 87 occurrences of ?make up? are
removed from the data set, the values of precision,
recall and f-score for the Collins Dictionary baseline
fall to 37.9, 51.2, and 42.9, respectively. This points
to a problem for system evaluation that is more gen-
303
eral than the low frequency of an error type, such
as missing hyphens. The more general problem is
that of non-independence among errors, which oc-
curs when an individual writer contributes multiple
times to an error count or when a particular prompt
gives rise to many occurrences of the same error, as
in the current case of ?make-up?.
Despite the problem of non-independent errors, a
more accurate picture of system performance may
nonetheless emerge with more evidence. Therefore,
we evaluate system precision on a data set of 1,000
student GRE and TOEFL essays written by both na-
tive and nonnative speakers, across a wide range of
proficiency levels and prompts. The essays, drawn
from 295 prompts, ranged in length from 1 to 50
sentences, with an average of 378 words per essay.
We manually inspect a random sample of 100 in-
stances where each system detected a missing hy-
phen. Two native-English speakers judged the cor-
rectness of the predictions using the Chicago Man-
ual of Style as a guide.3 Inter-annotator agreement
on the binary classification task for 600 items was
0.79?, showing high agreement. The results are
given in Table 5.
Total Judge-1 Judge 2
Predictions Precision Precision
Baseline
Collins dict 416 11 8
Wiki Counts 2185 20 21
Wiki Probs 224 54 52
Classifier
SJM-trained 421 62 69
Wiki-revision 577 43 41
Combined 450 60 62
Table 5: Precision results on 1000 student responses,
estimated by randomly sampling 100 hyphen predic-
tions of each system and manually evaluating them.
The results show that the first two baseline sys-
tems do not perform well on this essay data. This
is mainly because they do not take context into ac-
count. Many of the errors made by these systems in-
volved verb + preposition bigrams, as in Examples
(4) and (5). Restricting the detection by probability
clearly improves precision, but at the cost of recall
3http://www.chicagomanualofstyle.org
(only 224 total instances of missing hyphen errors
detected, the lowest of all 6 systems). In the man-
ual evaluation, the system trained on the SJM corpus
achieves the highest precision, though all precision
figures are lower than the previous evaluations. Ex-
ample (6) is a typical example of the kinds of false
positives made by the classifier models.
(4) If these men were required to step-down after a
limited number of years, the damage would be
contained.
(5) These families may even choose to eat at-home
than outside.
(6) The wellness program will save money in the
long-term.
Future work will explore additional features that
may help improve performance. A more thorough
study will also be carried out to fully understand the
differences in performance of the classifiers across
corpora. Another direction to explore in future work
is the related task of identifying extraneous hyphens
in learner text. These are even less frequent than
missing hyphens (87 annotated cases in the CLC-
FCE corpus), but we believe a similar classification
approach could be successful.
7 Conclusion
In this paper we presented a model for automatically
detecting missing hyphen errors in learner text. We
experimented with two kinds of training data, one
well-edited text, and the other an automatically ex-
tracted corpus of error annotations. When evaluat-
ing on artificially generated errors in otherwise well-
edited text, the classifiers generally performed bet-
ter than the baseline systems. When evaluating on
the small number of missing hyphen errors in the
CLC-FCE corpus, the word-based models did well,
though the classifiers also achieved consistently high
precision. A precision-only evaluation on a sample
of learner essays resulted in overall lower scores, but
the classifier trained on well-edited text performed
best. In general, the classifiers outperform the base-
line, especially in terms of precision, showing that
taking context into account when detecting these
kinds of errors is important.
304
References
Aoife Cahill, Nitin Madnani, Joel Tetreault, and Diane
Napolitano. 2013. Robust Systems for Preposition
Error Correction Using Wikipedia Revisions. In Pro-
ceedings of the Conference of the North American
Chapter of the Association for Computational Linguis-
tics: Human Language Technologies, Atlanta, GA.
Robert Dale and Adam Kilgarriff. 2011. Helping Our
Own: The HOO 2011 Pilot Shared Task. In Pro-
ceedings of the Generation Challenges Session at the
13th EuropeanWorkshop on Natural Language Gener-
ation, pages 242?249, Nancy, France, September. As-
sociation for Computational Linguistics.
Agustin Gravano, Martin Jansche, and Michiel Bacchi-
ani. 2009. Restoring punctuation and capitalization in
transcribed speech. In Acoustics, Speech and Signal
Processing, 2009. ICASSP 2009. IEEE International
Conference on, pages 4741?4744. IEEE.
Ross Israel, Joel Tetreault, and Martin Chodorow. 2012.
Correcting Comma Errors in Learner Essays, and
Restoring Commas in Newswire Text. In Proceed-
ings of the 2012 Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, pages 284?
294, Montre?al, Canada, June. Association for Compu-
tational Linguistics.
Alla Rozovskaya, Mark Sammons, Joshua Gioja, and
Dan Roth. 2011. University of Illinois System in
HOO Text Correction Shared Task. In Proceedings
of the Generation Challenges Session at the 13th Eu-
ropean Workshop on Natural Language Generation,
pages 263?266, Nancy, France, September. Associa-
tion for Computational Linguistics.
Helen Yannakoudakis, Ted Briscoe, and Ben Medlock.
2011. A New Dataset and Method for Automati-
cally Grading ESOL Texts. In Proceedings of the
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies,
pages 180?189, Portland, Oregon, USA, June. Associ-
ation for Computational Linguistics.
305
Proceedings of the Ninth Workshop on Innovative Use of NLP for Building Educational Applications , pages 79?88,
Baltimore, Maryland USA, June 26, 2014.
c?2014 Association for Computational Linguistics
An Explicit Feedback System for Preposition Errors
based on Wikipedia Revisions
Nitin Madnani and Aoife Cahill
Educational Testing Service
660 Rosedale Road
Princeton, NJ 08541, USA
{nmadnani,acahill}@ets.org
Abstract
This paper presents a proof-of-concept
tool for providing automated explicit feed-
back to language learners based on data
mined from Wikipedia revisions. The tool
takes a sentence with a grammatical er-
ror as input and displays a ranked list of
corrections for that error along with evi-
dence to support each correction choice.
We use lexical and part-of-speech con-
texts, as well as query expansion with a
thesaurus to automatically match the er-
ror with evidence from the Wikipedia revi-
sions. We demonstrate that the tool works
well for the task of preposition selection
errors, evaluating against a publicly avail-
able corpus.
1 Introduction
A core feature of learning to write is receiving
feedback and making revisions based on that feed-
back (Biber et al., 2011; Lipnevich and Smith,
2008; Truscott, 2007; Rock, 2007). In the field of
second language acquisition, the main focus has
been on explicit or direct feedback vs. implicit
or indirect feedback. In writing, explicit or direct
feedback involves a clear indication of the location
of an error as well as the correction itself, or, more
recently, a meta-linguistic explanation (of the un-
derlying grammatical rule). Implicit or indirect
written feedback indicates that an error has been
made at a location, but it does not provide a cor-
rection.
The work in this paper describes a novel tool
for presenting language learners with explicit
feedback based on human-authored revisions in
Wikipedia. Here we describe the proof-of-concept
tool that provides explicit feedback on one specific
category of grammatical errors, preposition selec-
tion. We restrict the scope of the tool in order to
be able to carry out a focused study, but expect
that our findings presented here will also general-
ize to other error types. The task of preposition se-
lection errors has been well studied (Tetreault and
Chodorow, 2008; De Felice and Pulman, 2009;
Tetreault et al., 2010; Rozovskaya and Roth, 2010;
Dahlmeier and Ng, 2011; Seo et al., 2012; Cahill
et al., 2013), and the availability of public, anno-
tated corpora containing such errors provides easy
access to evaluation data.
Our tool takes a sentence with a grammatical
error as input, and returns a ranked list of possi-
ble corrections. The tool makes use of frequency
of correction in edits to Wikipedia articles (as
recorded in the Wikipedia revision history) to cal-
culate the rank order. In addition to the ranked list
of suggestions, the tool also provides evidence for
each correction based on the actual changes made
between different versions of Wikipedia articles.
The tool uses the notion of ?context similarity? to
determine whether a particular edit to a Wikipedia
article can provide evidence of a correction in a
given context.
Specifically, this paper makes the following
contributions:
1. We build a tool to provide explicit feedback
for preposition selection errors in the form of
ranked lists of suggested corrections.
2. We use evidence from human-authored cor-
rections for each suggested correction on a
list.
3. We conduct a detailed examination of how
the performance of the tool is affected by
varying the type and size of contextual infor-
mation and by the use of query expansion.
The remainder of this paper is organized as fol-
lows: ?2 describes related work and ?3 outlines
potential approaches for using Wikipedia revision
data in a feedback tool. ?4 outlines the core system
79
for generating feedback and ?5 presents an empir-
ical evaluation of this system. In ?6 we describe a
method for enhancing the system using query ex-
pansions. We discuss our findings and some future
work in ?7 and, finally, conclude in ?8.
2 Related Work
Attali (2004) examines the general effect of feed-
back in the Criterion system (Burstein et al., 2003)
and finds that students presented with feedback are
able to improve the overall quality of their writ-
ing, as measured by an automated scoring system.
This study does not investigate different kinds of
feedback, but rather looks at the issue of whether
feedback in general is useful for students. Shermis
et al. (2004) look at groups of students who used
Criterion and students who did not and compare
their writing performance as measured by high-
stakes state assessment. They found that, in gen-
eral, the students who made use of Criterion and
its feedback improved their writing skills. They
analyze the distributions of the individual gram-
mar and style error types and found that Criterion
helped reduce the number of repeated errors, par-
ticularly for mechanics (e.g. spelling and punctu-
ation errors). Chodorow et al. (2010) describe a
small study in which Criterion provided feedback
about article errors to students writing an essay for
a college-level course. They find, similarly to At-
tali (2004), that the number of article errors was
reduced in the final revised version of the essay.
Gamon et al. (2009) describe ESL Assistant ?
a web-based proofreading tool designed for lan-
guage learners who are native speakers of East-
Asian languages. They used a decision-tree ap-
proach to detect and offer suggestions for poten-
tial article and preposition errors. They also al-
lowed the user to compare the various suggestions
by showing results of corresponding web searches.
Chodorow et al. (2010) also describe a small study
where ESL Assistant was used to offer sugges-
tions for potential grammatical errors to web users
while they were composing email messages. They
reported that users were able to make effective use
of the explicit feedback for that task. The tool had
been offered as a web service but has since been
discontinued.
Our tool is similar to ESL Assistant in that both
produce a list of possible corrections. The main
difference between the tools is that ours automat-
ically derives the ranked list of correction sugges-
tions from a very large corpus of annotated errors,
rather than performing a web search on all pos-
sible alternatives in the context. The advantage
of using an error-annotated corpus is that it con-
tains implicit information about frequent confu-
sion pairs (e.g. ?at? instead of ?in?) that are in-
dependent of the frequency of the preposition and
the current context.
Milton and Cheng (2010) describe a toolkit for
helping Chinese learners of English become more
independent writers. The toolkit gives the learners
access to online resources including web searches,
online concordance tools, and dictionaries. Users
are provided with snapshots of the word or struc-
ture in context. In Milton (2006), 500 revisions
to 323 journal entries were made using an earlier
version of this tool. Around 70 of these revisions
had misinterpreted the evidence presented or were
careless mistakes; the remaining revisions resulted
in more natural sounding sentences.
3 Wikipedia Revisions
Our goal is to build a tool that can provide explicit
feedback about errors to writers. We take advan-
tage of the recently released Wikipedia preposi-
tion error corpus (Cahill et al., 2013) and design
our tool based on this large corpus containing sen-
tences annotated for preposition errors and their
corrections. The corpus was produced automati-
cally by mining a total of 288 million revisions for
8.8 million articles present in a Wikipedia XML
snapshot from 2011. The Wikipedia error corpus,
as we refer to in the rest of the paper, contains
2 million sentences annotated with preposition er-
rors and their respective corrections.
There are two possible approaches to building
an explicit feedback tool for preposition errors
based on this corpus:
1. Classifier-based. We could train a classi-
fier on the Wikipedia error corpus to predict
the correct preposition in a given context, as
Cahill et al. (2013) did. Although this would
allow us to suggest corrections for contexts
that are unseen in the Wikipedia data, the
suggestions would likely be quite noisy given
the inherent difficulty of a classification prob-
lem with a large number of classes.
1
In addi-
tion, this approach would not facilitate pro-
1
Cahill et al. (2013) used a list of 36 prepositions as
classes.
80
viding evidence for each correction to the
user.
2. Corpus-based. We could use the Wikipedia
error corpus directly for feedback. Al-
though this means that suggestions can only
be generated for contexts occurring in the
Wikipedia data, it also means that all sug-
gestion would be grounded in actual revisions
made by other humans on Wikipedia.
We believe that anchoring suggestions to
human-authored corrections affords greater util-
ity to a language learner, in line with the current
practice in lexicography that emphasizes authen-
tic usage examples (Collins COBUILD learner?s
dictionary, Sketch Engine (Kilgarriff et al., 2004)).
Therefore, in this paper, we choose the second ap-
proach to build our tool.
4 Methodology
In order to use the Wikipedia error corpus directly
for feedback, we first index the sentences in the
corpus using the following fields:
? The incorrect preposition.
? The correct preposition.
? The words, bigrams, and trigrams before (and
after) the preposition error (indexed sepa-
rately).
? The part-of-speech tags, tag bigrams, and tag
trigrams before (and after) the error (indexed
separately).
? The title and URL of the Wikipedia article in
which the sentence occurred.
? The ID of the article revision containing the
preposition error.
? The ID of the article revision in which the
correction was made.
Once the index is constructed, eliciting explicit
feedback is straightforward. The input to the sys-
tem is a tokenized sentence with a marked up
preposition error (e.g. from an automated prepo-
sition error detection system). For each input sen-
tence, the Wikipedia index is then searched with
the identified preposition error and the words (or
n-grams) present in its context. The index returns
a list of the possible corrections occurring in the
given context. The tool then counts how often
each possible preposition is returned as a possible
correction and orders its suggestions from most
frequent to least frequent. In addition, the tool also
displays five randomly chosen sentences from the
index as evidence for each correction in order to
help the learner make a better choice. The tool
can use either the lexical n-grams (n=1,2,3) or the
part-of-speech n-grams (n=1,2,3) around the error
for the contextualized search of the Wikipedia in-
dex.
Figure 1 shows a screenshot of the tool in oper-
ation. The input sentence is entered into the text
box at the top, with the preposition error enclosed
in asterisks. In this case, the tool is using parts-of-
speech on either side of the error for context. By
default, the tool shows the top five possible correc-
tions as a bar chart, sorted according to how many
times the erroneous preposition was changed to
the correction in the Wikipedia revision index. In
this example, the preposition of with the left con-
text of <DT, NNS> and the right context of <DT,
NN> was changed to the preposition in 242 times
in the Wikipedia revisions. When the user clicks
on a bar, the box on the top shows the sentence
with the change and the gray box on the right
shows 5 (randomly chosen) actual sentences from
Wikipedia where the change represented by the
bar was made.
If parts-of-speech are chosen as context, the tool
uses WebSockets to send the sentence to the Stan-
ford Tagger (Toutanova et al., 2003) in the back-
ground and compute its part-of-speech tags before
searching the index.
5 Evaluation
In order to determine how well the tool performs
at suggesting corrections, we used sentences con-
taining preposition errors from the CLC FCE
dataset. The CLC FCE Dataset is a collection of
1,244 exam scripts written by learners of English
as part of the Cambridge ESOL First Certificate in
English (Yannakoudakis et al., 2011). Our evalua-
tion set consists of 3,134 sentences, each contain-
ing a single preposition error.
We evaluate the tool on two criteria:
? Coverage. We define coverage as the pro-
portion of errors for which the tool is able to
suggest any corrections.
? Accuracy. The obvious definition of accu-
81
F
i
g
u
r
e
1
:
A
s
c
r
e
e
n
s
h
o
t
o
f
t
h
e
t
o
o
l
s
u
g
g
e
s
t
i
n
g
t
h
e
t
o
p
5
c
o
r
r
e
c
t
i
o
n
s
f
o
r
a
s
e
n
t
e
n
c
e
u
s
i
n
g
t
w
o
p
a
r
t
s
-
o
f
-
s
p
e
e
c
h
o
n
e
i
t
h
e
r
s
i
d
e
o
f
t
h
e
m
a
r
k
e
d
e
r
r
o
r
a
s
c
o
n
t
e
x
t
.
T
h
e
c
o
r
r
e
c
t
i
o
n
s
a
r
e
d
i
s
p
l
a
y
e
d
i
n
r
a
n
k
e
d
f
a
s
h
i
o
n
a
s
a
h
i
s
t
o
g
r
a
m
a
n
d
c
l
i
c
k
i
n
g
o
n
o
n
e
d
i
s
p
l
a
y
s
t
h
e
?
c
o
r
r
e
c
t
e
d
?
s
e
n
t
e
n
c
e
a
b
o
v
e
a
n
d
t
h
e
c
o
r
r
e
s
p
o
n
d
i
n
g
e
v
i
d
e
n
c
e
f
r
o
m
W
i
k
i
p
e
d
i
a
r
e
v
i
s
i
o
n
s
o
n
t
h
e
l
e
f
t
.
82
Context Found Missed Blank MRR
words1 889 (28.4%) 356 (11.4%) 1889 (60.3%) .522
words2 55 ( 1.8%) 22 ( 0.7%) 3057 (97.5%) .619
words3 16 ( 0.5%) 5 ( 0.2%) 3113 (99.3%) .762
tags1 2821 (90.0%) 241 ( 7.7%) 72 ( 2.3%) .419
tags2 1896 (60.5%) 718 (22.9%) 520 (16.6%) .390
tags3 661 (21.1%) 633 (20.2%) 1840 (58.7%) .325
Table 1: A detailed breakdown of the Found, Missing and Blank classes along with the Mean Reciprocal
Rank (MRR) values, for different types (words, tags) and sizes (1, 2, or 3 around the error) of
contextual information used in the search.
racy would be the proportion of errors for
which the tool?s best suggestion is the cor-
rect one. However, since the tool returns
a ranked list of suggestions, it is important
to award partial credit for errors where the
tool made a correct suggestion but it was not
ranked at the top. Therefore, we use the Mean
Reciprocal Rank (MRR), a standard metric
used for evaluating ranked retrieval systems
(Voorhees, 1999). MRR is computed as fol-
lows:
MRR =
1
|S|
|S|
?
i=1
1
R
i
where S denotes the set of sentences for
which ranked lists of suggestions are gener-
ated and R
i
denotes the rank of the true cor-
rection in the list of suggestions the tool re-
turns for sentence i. A higher MRR is better
since that means that the tool ranked the true
correction closer to the top of the list.
To conduct the evaluation on the FCE dataset,
we run each of the sentences through the tool and
extract the top 5 suggestions for each error anno-
tated in the sentence.
2
At this point, each error
instance input to the tool can be classified as one
of three classes:
1. Found. The true correction for the error was
found in the ranked list of suggestions made
by the tool.
2. Missing. The true correction for the error
was not found in the ranked list of sugges-
tions.
3. Blank. The tool did not return any sugges-
tions for the error.
2
In this paper, we separate the tasks of error detection and
correction and use the gold standard as an oracle to detect er-
rors and then use our system to propose and rank corrections.
First, we examine the distribution of the three
classes across the types and sizes of the contextual
information used to conduct the search. Table 1
shows, for each context type and size, a detailed
breakdown of the distribution of the three classes
along with the mean reciprocal rank (MRR) val-
ues.
3
We observe that, with words as contexts, us-
ing larger contexts certainly produces more accu-
rate results (as indicated by the larger MRR val-
ues). However, we also observe that employing
larger contexts reduces coverage (as indicated by
the decreasing percentage of Found sentences and
by the the increasing percentage of the Blank sen-
tences).
With part-of-speech tags, we observe that al-
though using larger tag contexts can find correc-
tions for a significantly larger number of sentences
as compared to similar-sized word contexts (as in-
dicated by the larger percentages of Found sen-
tences), doing so yields overall worse MRR val-
ues. This is primarily due to the fact that with
larger part-of-speech contexts the system produces
more suggestions that never contain the true cor-
rection, i.e., an increasing percentage of Missed
sentences. The most likely reason is that signifi-
cantly reducing the vocabulary size by using part-
of-speech tags introduces a lot of noise.
Figure 2 shows the distribution of the rank R
of the true correction in the list of suggestions.
4
The figure uses a rank of 10+ to denote all ranks
greater than 10 to conserve space. We observe
similar trends in the figure as in Table 1 ? us-
ing larger word contexts yield higher accuracies
but significantly lower coverage and using larger
3
We do not include Blank sentences when computing the
MRR values.
4
Note that in this figure, the bar for R = 0 includes both
sentences where no ranked list was produced (Blank) and
those where the true correction was not produced as a sug-
gestion at all (Missing).
83
wo
rds
1
wo
rds
2
wo
rds
3
tag
s1
tag
s2
tag
s3
0
100
0
200
0
300
0 0
100
0
200
0
300
0
0
1
2
3
4
5
6
7
8
9
10
10+
0
1
2
3
4
5
6
7
8
9
10
10+
0
1
2
3
4
5
6
7
8
9
10
10+
Ra
nk 
of t
rue
 co
rrec
tion
 (R)
Number of FCE sentences
Cla
ss
Bla
nk
Mi
ssin
g
Fou
nd
F
i
g
u
r
e
2
:
T
h
e
d
i
s
t
r
i
b
u
t
i
o
n
o
f
t
h
e
r
a
n
k
t
h
a
t
t
h
e
t
r
u
e
c
o
r
r
e
c
t
i
o
n
h
a
s
i
n
t
h
e
l
i
s
t
o
f
s
u
g
g
e
s
t
i
o
n
s
f
o
r
t
h
e
F
C
E
s
e
n
t
e
n
c
e
s
,
a
c
r
o
s
s
e
a
c
h
c
o
n
t
e
x
t
t
y
p
e
a
n
d
s
i
z
e
u
s
e
d
.
84
tag contexts yield lower accuracies and lower cov-
erage, even though the coverage is significantly
larger than that of the correspondingly sized word
context.
6 Query Expansion
The results in the previous section indicate that al-
though we could use part-of-speech tags as con-
texts to improve the coverage of the tool (as indi-
cated by the number of Found sentences), doing
so leads to a significant reduction in accuracy, as
indicated by the lower MRR values.
In the field of information retrieval, a common
practice is to expand the query with words similar
to words in the query in order to increase the like-
lihood of finding documents relevant to the query
(Sp?arck-Jones and Tait, 1984). In this section, we
examine whether we can use a similar technique
to improve the coverage of the tool.
We employ a simple query expansion technique
for the cases where no results would otherwise be
returned by the tool. For these cases, we first ob-
tain a list of K words similar to the two words
around the error from a distributional thesaurus
(Lin, 1998), ranked by similarity. We then gener-
ate a list of additional queries by combining these
two ranked lists of similar words. We then run
each query in the list against the Wikipedia index
until one of them yields results. Note that since
we are using a word-based thesaurus, this expan-
sion technique can only increase coverage when
applied to the words1 condition, i.e., single word
contexts. We investigate K = 1, 2, 5, or 10 expan-
sions for each of the context words.
Table 2 shows the a detailed breakdown of the
distribution of the three classes and the MRR val-
ues with query expansion integrated into the tool
for sentences where it would generally produce no
output. Each row corresponds to a different value
of K ? the number of expansions used per context
word ? is varied. Note that K = 0 corresponds to
the condition where query expansion is not used.
From the table, we observe that using query ex-
pansion indeed seems to increase the coverage of
the tool as indicated by the increasing percentage
of Found sentences and decreasing percentage of
Blank sentences. However, we also find that using
query expansion yields worse MRR values, again
because of the increasing percentage of Missed
sentences. This represents a traditional trade-off
scenario where accuracy can be traded off for an
increase in coverage, depending on the desired op-
erating characteristics.
7 Discussion and Future Work
There are several issues that merit further discus-
sion and possibly provide future extensions to the
work described in this paper.
? Need for an extrinsic evaluation. Although
our intrinsic evaluation clearly shows that the
tool has reasonably good coverage as well
as accuracy on publicly available data con-
taining preposition errors, it does not provide
any evidence that the explicit feedback pro-
vided by the tool is useful to English lan-
guage learners in a classroom setting. In the
future, we plan to conduct a controlled study
in a classroom setting that measures, for ex-
ample, whether the students that see the im-
proved feedback from the tool learn more
or better than those who either see no feed-
back at all or those who see only implicit
feedback. Biber et al. (2011) review sev-
eral previously published studies on the ef-
fects of feedback on writing development in
classrooms. Although the number of studies
that were included in the analysis is small,
some patterns did emerge. In general, stu-
dents improve their writing when they re-
ceive feedback, however greater gains are
made when they are presented with com-
ments rather than direct location and correc-
tion of errors. It is unclear how students
would react to a ranked list of suggestions
for a particular error at a given location. An
interesting finding was that L2-English stu-
dents showed greater improvements in writ-
ing when they received either feedback from
peers or computer-generated feedback than
when they received feedback from teachers.
? Assuming a single true correction. Our
evaluation setup assumes that the single cor-
rection provided as part of the FCE data set is
the only correct preposition for a given sen-
tence. However, it is well known in the gram-
matical error detection community that this is
not always the case. Most usage errors such
as preposition selection errors are a matter of
degree rather than simple rule violations such
as number agreement. As a consequence, it
is common for two native English speakers
85
Context K Found Missed Blank MRR
words1 0 889 (28.4%) 356 (11.4%) 1889 (60.3%) .522
words1 1 932 (29.7%) 417 (13.3%) 1785 (57.0%) .513
words1 2 1033 (33.0%) 550 (17.6%) 1551 (49.5%) .493
words1 5 1118 (35.7%) 691 (22.1%) 1325 (42.3%) .476
words1 10 1160 (37.0%) 780 (24.9%) 1194 (38.1%) .465
Table 2: A detailed breakdown of the Found, Missing and Blank classes along with the Mean Reciprocal
Rank (MRR) values, for different number of query expansions (K).
to have different judgments of usage. In fact,
this is exactly why the tool is designed to re-
turn a ranked list of suggestions rather than
a single suggestion. Therefore, it is possible
that our intrinsic evaluation is underestimat-
ing the performance of the tool.
? Practical considerations for deployment.
In this study, we used the gold standard er-
ror annotations for detecting preposition er-
rors before querying the tool for suggestions.
Such a setup allowed us to separate the prob-
lems of error detection and the generation
of feedback and likely gives an upper bound
on performance. Using a fully automatic
error detection system will likely introduce
additional noise into the pipeline, however,
we believe that tuning the detection system
for higher precision could mitigate that ef-
fect. Another useful idea would be to use the
classifier-based approach (see ?3) as a backup
for the corpus-based approach for providing
suggestions, i.e., using the classifier to pre-
dict the suggested corrections when no cor-
rections can be found in the Wikipedia revi-
sions.
? Using other types of expansions. In this pa-
per, we used a very simple method of gener-
ating query expansions ? a distributional the-
saurus. However, in the future, it may be
worth exploring other distributional similar-
ity methods such as Brown clusters (Brown
et al., 1992; Miller et al., 2004; Liang, 2005)
or word2vec (Mikolov et al., 2013).
8 Conclusions
In this paper, we presented our work on build-
ing a proof-of-concept tool that can provide au-
tomated explicit feedback for preposition errors.
We used an existing, error-annotated preposition
corpus produced by mining Wikipedia revisions
(Cahill et al., 2013) to not only provide a ranked
list of suggestions for any given preposition error
but also to produce human-authored evidence for
each suggested correction. The tool can use either
words or part-of-speech tags around the error as
context. We evaluated the tool in terms of both
accuracy and coverage and found that: (1) using
larger context window sizes for words increases
accuracy but reduces coverage due to sparsity (2)
using part-of-speech tags leads to increased cov-
erage compared to using words as contexts but
decreases accuracy. We also experimented with
query expansion for single words around the er-
ror and found that it led to an increase in cover-
age with only a slight decrease in accuracy; using
a larger set of expansions added more noise. In
general, we find that the approach of using a large
error-annotated corpus to provide explicit feed-
back to writers performs reasonably well in terms
of providing ranked lists of alternatives. It remains
to be seen how useful this tool is in a practical sit-
uation.
Acknowledgments
We would like to thank Beata Beigman Klebanov,
Michael Heilman, Jill Burstein, and the anony-
mous reviewers for their helpful comments about
the paper. We also thank Ani Nenkova, Chris
Callison-Burch, Lyle Ungar and their students at
the University of Pennsylvania for their feedback
on this work.
References
Yigal Attali. 2004. Exploring the Feedback and Re-
vision Features of Criterion. Paper presented at
the National Council on Measurement in Education
(NCME), Educational Testing Service, Princeton,
NJ.
Douglas Biber, Tatiana Nekrasova, and Brad Horn.
2011. The Effectiveness of Feedback for L1-English
and L2-Writing Development: A Meta-Analysis.
86
Research Report RR-11-05, Educational Testing
Service, Princeton, NJ.
Peter F. Brown, Vincent J. Della Pietra, Peter V.
de Souza, Jennifer C. Lai, and Robert L. Mercer.
1992. Class-Based n-gram Models of Natural Lan-
guage. Computational Linguistics, 18(4):467?479.
Jill Burstein, Martin Chodorow, and Claudia Leacock.
2003. Criterion online essay evaluation: An applica-
tion for automated evaluation of student essays. In
Proceedings of IAAI, pages 3?10, Acapulco, Mex-
ico.
Aoife Cahill, Nitin Madnani, Joel Tetreault, and Di-
ane Napolitano. 2013. Robust Systems for Prepo-
sition Error Correction Using Wikipedia Revisions.
In Proceedings of NAACL, pages 507?517, Atlanta,
GA, USA.
Martin Chodorow, Michael Gamon, and Joel Tetreault.
2010. The Utility of Article and Preposition Er-
ror Correction Systems for English Language Learn-
ers: Feedback and Assessment. Language Testing,
27(3):419?436.
Daniel Dahlmeier and Hwee Tou Ng. 2011. Gram-
matical Error Correction with Alternating Structure
Optimization. In Proceedings of ACL-HLT, pages
915?923, Portland, Oregon, USA.
Rachele De Felice and Stephen G. Pulman. 2009.
Automatic detection of preposition errors in learner
writing. CALICO Journal, 26(3):512?528.
Michael Gamon, Claudia Leacock, Chris Brockett,
William B Dolan, Jianfeng Gao, Dmitriy Belenko,
and Alexandre Klementiev. 2009. Using Statistical
Techniques and Web Search to Correct ESL Errors.
CALICO Journal, 26(3):491?511.
Adam Kilgarriff, Pavel Rychly, Pavel Smrz, and David
Tugwell. 2004. The Sketch Engine. In Proceedings
of EURALEX, pages 105?116.
Percy Liang. 2005. Semi-supervised Learning for Nat-
ural Language. Master?s thesis, Massachusetts Insti-
tute of Technology.
Dekang Lin. 1998. Automatic Retrieval and
Clustering of Similar Words. In Proceedings of
ACL-COLING, pages 768?774, Montreal, Quebec,
Canada.
Anastasiya A. Lipnevich and Jeffrey K. Smith. 2008.
Response to Assessment Feedback: The Effects of
Grades, Praise, and Source of Information. Re-
search Report RR-08-30, Educational Testing Ser-
vice, Princeton, NJ.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S.
Corrado, and Jeffrey Dean. 2013. Distributed Rep-
resentations of Words and Phrases and their Com-
positionality. In Proceedings of NIPS, pages 3111?
3119.
Scott Miller, Jethran Guinness, and Alex Zamanian.
2004. Name Tagging with Word Clusters and
Discriminative Training. In Proceedings of HLT-
NAACL, pages 337?342, Boston, MA, USA.
John Milton and Vivying SY Cheng. 2010. A Toolkit
to Assist L2 Learners Become Independent Writers.
In Proceedings of the NAACL Workshop on Compu-
tational Linguistics and Writing: Writing Processes
and Authoring Aids, pages 33?41, Los Angeles, CA,
USA.
John Milton. 2006. Resource-rich Web-based Feed-
back: Helping learners become Independent Writ-
ers. Feedback in second language writing: Contexts
and issues, pages 123?139.
JoAnn Leah Rock. 2007. The Impact of Short-
Term Use of Criterion on Writing Skills in Ninth
Grade. Research Report RR-07-07, Educational
Testing Service, Princeton, NJ.
Alla Rozovskaya and Dan Roth. 2010. Training
Paradigms for Correcting Errors in Grammar and
Usage. In Proceedings of NAACL-HLT, pages 154?
162, Los Angeles, California.
Hongsuck Seo, Jonghoon Lee, Seokhwan Kim, Kyu-
song Lee, Sechun Kang, and Gary Geunbae Lee.
2012. A Meta Learning Approach to Grammatical
Error Correction. In Proceedings of ACL (short pa-
pers), pages 328?332, Jeju Island, Korea.
Mark D. Shermis, Jill C. Burstein, and Leonard Bliss.
2004. The Impact of Automated Essay Scoring on
High Stakes Writing Assessments. In Annual Meet-
ing of the National Council on Measurement in Ed-
ucation.
Karen Sp?arck-Jones and J. I. Tait. 1984. Automatic
Search Term Variant Generation. Journal of Docu-
mentation, 40(1):50?66.
Joel R. Tetreault and Martin Chodorow. 2008. The
Ups and Downs of Preposition Error Detection in
ESL Writing. In Proceedings of COLING, pages
865?872, Manchester, UK.
Joel Tetreault, Jennifer Foster, and Martin Chodorow.
2010. Using Parse Features for Preposition Selec-
tion and Error Detection. In Proceedings of ACL
(short papers), pages 353?358, Uppsala, Sweden.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of NAACL, pages 173?180, Edmon-
ton, Canada.
John Truscott. 2007. The Effect of Error Correction
on Learners? Ability to Write Accurately. Journal
of Second Language Writing, 16(4):255?272.
Ellen M. Voorhees. 1999. The TREC-8 Question An-
swering Track Report. In Proceedings of the Text
REtrieval Conference (TREC), volume 99, pages
77?82.
87
Helen Yannakoudakis, Ted Briscoe, and Ben Medlock.
2011. A New Dataset and Method for Automatically
Grading ESOL Texts. In Proceedings of the ACL:
HLT, pages 180?189, Portland, OR, USA.
88
First Joint Workshop on Statistical Parsing of Morphologically Rich Languages
and Syntactic Analysis of Non-Canonical Languages, pages 66?73 Dublin, Ireland, August 23-29 2014.
Self-Training for Parsing Learner Text
Aoife Cahill, Binod Gyawali and James V. Bruno
Educational Testing Service,
660 Rosedale Road,
Princeton, NJ 08541,
USA
{acahill, bgyawali, jbruno}@ets.org
Abstract
We apply the well-known parsing technique of self-training to a new type of text: language-
learner text. This type of text often contains grammatical and other errors which can cause
problems for traditional treebank-based parsers. Evaluation on a small test set of student data
shows improvement over the baseline, both by training on native or non-native text. The main
contribution of this paper adds additional support for the claim that the new self-trained parser
has improved over the baseline by carrying out a qualitative linguistic analysis of the kinds of
differences between two parsers on non-native text. We show that for a number of linguistically
interesting cases, the self-trained parser is able to provide better analyses, despite the sometimes
ungrammatical nature of the text.
1 Introduction
The vast majority of treebank-based parsing research assumes that the text to be parsed is well-formed.
In this paper, we are concerned with parsing text written by non-native speakers of English into phrase
structure trees, as a precursor for applications in automated scoring and error detection. Non-native text
often contains grammatical errors ranging in severity from minor collocational differences to extremely
garbled strings that are difficult to interpret. These kinds of errors are known to cause difficulty for
automated analyses (De Felice and Pulman, 2007; Lee and Knutsson, 2008).
We explore a previously documented technique for adapting a state-of-the-art parser to be able to bet-
ter parse learner text: domain adaptation using self-training. Self-training is a semi-supervised learning
technique that relies on some labeled data to train an initial model, and then uses large amounts of unla-
beled data to iteratively improve that model. Self-training was first successfully applied in the newspaper
parsing domain by McClosky et al. (2006) who used the Penn Treebank WSJ as their labeled data and un-
labeled data from the North American News Text corpus. Previous attempts (Charniak, 1997; Steedman
et al., 2003) had not shown encouraging results, and McClosky et al. (2006) hypothesize that the gain
they saw was due to the two-phase nature of the BLLIP parser used in their experiments. In a follow-up
study (McClosky et al., 2008) they find that one major factor leading to successful self-training is when
the process sees known words in new combinations.
2 Related Work
Foster et al. (2011) compare edited newspaper text and unedited forum posts in a self-training parsing
experiment, evaluating on a treebank of informal discussion forum entries about football. They find that
both data sources perform about equally well on their small test set overall, but that the underlying gram-
mars learned from the two sources were different. Ott and Ziai (2010) apply an out-of-the-box German
dependency parser to learner text and analyze the impact on down-stream semantic interpretation. They
find that core functions such as subject and object can generally be reliably detected, but that when there
are key elements (e.g. main verbs) missing from the sentence that the parses are less reliable. They
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
66
also found that less-severe grammatical errors such as agreement did not tend to cause problems for the
parser.
An alternative approach to parsing learner text is to modify the underlying dependency scheme used in
parsing to account for any grammatical errors. This can be useful because it is not always clear what the
syntactic analysis of ungrammatical text should be, given some scheme designed for native text. Dick-
inson and Ragheb (2009) present such a modified scheme for English, designed for annotating syntactic
dependencies over a modified POS tagset. Dickinson and Lee (2009) retrain a Korean dependency parser,
but rather than adding additional unlabeled data as we do, they modify the original annotated training
data. The modifications are specifically targeted to be able to detect errors relating to Korean postpo-
sitional particles. They show that the modified parser can be useful in detecting those kinds of particle
errors and in their conclusion suggest self-training as an alternative approach to parsing of learner text.
A similar alternative approach is to directly integrate error detection into the parsing process (Menzel
and Schro?der, 1999; Vandeventer Faltin, 2003).
3 Self-training a new parser
We first describe the data that we use for both training and evaluating our parsers, and then we describe
our experiments and results.
We take the standard portion of the Penn Treebank (sections 02?21) as our seed labeled data. We
then compare two different unlabeled training data sets. The first data set consists of 480,000 sentences
of newspaper text extracted from the LA Times portion of the North American News Corpus (NANC).
The second is a corpus of non-native written English text randomly sampled from a large dataset of
student essays. It consists of 480,900 sentences from 33,637 essays written as part of a test of English
proficiency, usually administered to non-native college-level students. The essays have been written to
422 different prompts (topics) and so cover a wide range of vocabulary and usage. Each essay has been
assigned a proficiency level (high, medium, low) by a trained human grader. 17.5% of the sentences were
from low proficiency essays, 42% from medium proficiency and 40.5% from high proficiency essays.
In order to determine the optimal number of self-training iterations and carry out our final evaluations
we use a small corpus of manually treebanked sentences. The corpus consists of 1,731 sentences written
by secondary level students which we randomly split into a development set (865 sentences) and a test set
(866 sentences). The native language of the students is unknown, but it is likely that many spoke English
as their first language. In addition, this corpus had originally been developed for another purpose and
therefore contains modifications that are not ideal for our experiments. The main changes are that spelling
and punctuation errors were corrected before the trees were annotated (and we do not have access to the
original text). Although the treebanked corpus does not align perfectly with our requirements, we believe
that it is a more useful evaluation data set than any other existing treebanked corpus.
We used the Charniak and Johnson (2005) (BLLIP) parser1 to perform the self training experiments.
Our experiment is setup as follows: first we train a baseline model on the Penn Treebank WSJ data
(sections 02-21). Then, iteratively, sentences are selected from the unlabeled data sets, parsed by the
parser, and combined with the previously annotated data to retrain the parser. The parser also requires
development data, for which we use section 22 of the WSJ data. After each iteration we evaluate the
parser using our 865-sentence development set. Parser evaluation was done using the EVALB2 tool and
we report the performance in terms of F1 score.
There are two main parameters in our self-training setup: the size of the unlabeled data set added at
each iteration and the weight given to the original labeled data.3 In preliminary experiments, we found
that a block size of 40,000 sentences per each iteration and a weight of 5 on the original labeled data
performed best. Given our training data, and a block size of 40K, this results in 12 iterations. In each
iteration, the training data consists of the PTB data repeated 5 times, plus the parsed output of previous
blocks of unlabeled data.
1https://github.com/BLLIP/bllip-parser
2http://nlp.cs.nyu.edu/evalb/
3Note that this approach differs to that outlined in McClosky et al. (2006) who only perform one self-training iteration. It is
more similar to the approach described in Reichart and Rappoport (2007).
67
The results of our experiments are as shown in Figure 1. Iteration 0 corresponds to the baseline parser
while iterations 1?12 are the self trained parsers. We see that the F1 score of the baseline parser is
80.9%.4 The self trained parsers have higher accuracies compared to the baseline parser starting at the
first iteration. The highest score training on non-native text (82.3%) was achieved on the 11th iteration,
and the highest score training on newspaper text (81.8%) was achieved on the 8th iteration. Both of these
results are statistically significantly better than the baseline parser only trained on WSJ text.5 The graph
also shows that the non-native training results in slightly higher overall f-scores than the parser trained
on the native data after iteration 5, however these differences are not statistically significant.
81.
5
81.
7582
82.
2582.
5
F 1 S c o r
F1?
sco
re?o
f?ea
ch?
iter
atio
n
No
n?N
ativ
e?E
ngl
ish
NA
NC
80.
7581
81.
25
0
1
2
3
4
5
6
7
8
9
10
11
12
r e ( % )
Ite
rat
ion
?nu
mb
er
Figure 1: Performance of parsers after each iteration. Parsers used WSJ Section 22 as development data
and were evaluated on the student response development data.
The final evaluation was carried out by evaluating on the student test corpus of 866 sentences, using
the parsing model that performed best on the student dev corpus. The parser trained on native text
achieved an f-score of 82.4% and the parser trained on the non-native text achieved an f-score of 82.6%.
This difference is not statistically significant and is a similar finding to Foster et al. (2011). In another
experiment, we found that if the development data used during self-training is similar to the test data, we
see even smaller differences between the two different kinds of training data.6
4 Analysis
We carry out a qualitative analysis of the differences in parses between the original parser and one of the
best-performing self-trained ones, trained on non-native text. We randomly sample 5 essays written by
non-native speakers (but not overlapping with the data used to self-train the parser). Table 1 shows the
number of sentences and the number of parse trees that differ, according to each proficiency level.
Proficiency # Essays # Sentences # Words # Differing Parses % Differing Parses
High 2 30 694 12 40
Mid 1 22 389 12 54
Low 2 17 374 8 47
Totals 5 69 1457 32 46
Table 1: Descriptive Statistics for Essays in the Qualitative Sample
4Note that these overall f-scores are considerably lower than current state-of-the-art for newspaper text, indicating that this
set of student texts are considerably different.
5Significance testing was carried out using Dan Bikel?s Randomized Parsing Evaluation Comparator script for comparing
evalb output files. We performed 1000 random shuffles and tested for p-values < 0.01.
6These data sets were all quite small, however, so further investigation is required to fully assess this finding.
68
Figure 2 reports the number of differences by proficiency level. It is important to note that these
differences only included ones that were considered to be independent (e.g. a change in POS tag that
necessitated a change in constituent label was only counted once). We note a trend in which the self-
trained parser produces better parses than the baseline more often; however, at the highest proficiency
level the baseline parser produces better parses more often. In some applications it might be possible to
take the proficiency level into account before running the parser. However for many applications this will
present a challenge since the parser output plays a role in predicting the proficiency level. A possible
alternative would be to approximate proficiency using frequencies of spelling and other grammatical
errors that can be automatically detected without relying on parser output and use this information to
decide which version of the parser to use.
10 14 7
5 4
3
4 3 9
10%20%
30%40%
50%60%
70%80%
90%100%
Indep
enden
t Diffe
rence
s
0% Low(Total = 19) Mid(Total = 21) High(Total = 19)
I
Proficiency Level
Better Self-Trained Better Unclear Better Original
Figure 2: Unrelated Differences by Proficiency Level.
We systematically examine each of the 32 pairs of differing parse trees in the sample and manually
categorize the differences. Figure 3 shows the 5 most frequent types of differences, their breakdown
by proficiency level, as well as the results of a subjective evaluation on which parse was better. These
judgements were made by one of the authors of this paper who is a trained linguist.
3 3 5 4 2 4 5 4
2
2
2
1 1 1
1
2
5
1
2
3
1 1 2 10
2
4
6
8
10
12
14
Low Mid High Low Mid High Low Mid High Low Mid High Low Mid High
Attachment Site(Total = 22) POS Tag(Total = 15) Sentential Components(Total = 10)
POS of Misspelled Terminal(Total = 6)
Headedness(Total = 5)
Better Self-Trained Better Unclear Better Original
Figure 3: Parse Tree Differences by Proficiency Level.
The differences in Figure 3 are defined as follows. Attachment Site: the same constituent is attached
to different nodes in each parse; POS Tag: the same terminal bears a different POS tag in each parse,
where the terminal exists in our dictionary of known English7 words; Sentential Components: One parse
groups a set of constituents exhaustively into an S-node, while the other does not; POS of misspelled
terminal: the same terminal bears a different POS tag in each parse, where the terminal has been flagged
as a misspelling; Headedness: a terminal heads a maximal projection of a different syntactic category in
one parse but not the other, (e.g. a VP headed by a nominal).
7We use the python package enchant with US spelling dictionaries to carry out spelling error detection.
69
We characterized the differences according to whether the better output was produced by the original
parser, the self trained parser, or if it was not clear that either parse was better than the other. Attachment
Site differences were evaluated according to whether or not they were attached to the constituent they
modified; POS Tag differences were evaluated according to the Penn Treebank Guidelines (Santorini,
1995); Sentential Components differences were evaluated according to whether or not the terminals
should indeed form a clausal constituent, infinitive, or gerund; POS of Misspelled Terminal differences
were evaluated according to the evaluator?s perception of the writer?s intended target. We note that
the most abundant differences are in Attachment Site, that the biggest improvements resulting from self-
training are in the recognition of Sentential Components and in the identification of the POS of Misspelled
Terminals, and that the biggest degradation is in Headedness.
4.1 General Difference Patterns
Using the categories defined during the manual analysis of the 5 essays, we develop rules to automatically
detect these kinds of differences in a large dataset. We expect that the automatic rules will identify more
differences than the linguist, however we hope to see the same general patterns. We apply our rules to an
additional set of data consisting of roughly 10,000 sentences written by non-native speakers of English.
Table 2 shows the number of sentences for which the parsers found different parses at each proficiency
level, and Table 3 gives the totals for each of the five difference categories described above.
Proficiency # Essays # Sentences # Words # Differing Parses % Differing Parses
High 256 4178 266543 2214 53
Mid 285 4168 263685 2364 57
Low 149 1657 93466 971 59
Totals 690 10003 623694 5549 55
Table 2: Descriptive Statistics for Essays in the Larger Sample
Difference Total Low Medium High
Attachment Site 7805 1331 3474 3000
POS Tag 6827 1205 3238 2384
Sentential Components 4103 778 1786 1539
POS of Misspelled Terminal 2040 346 894 800
Headedness 1357 353 568 436
Table 3: Total number of differences detected automatically by proficiency level
We see that the proportion of sentences with different parses is similar to the 5-essay sample and also
that the relative ordering of the five difference categories is identical. This at least indicates that the
5-essay sample does not differ largely in its general properties from a larger set.
4.2 Illustrative Observations
We highlight some of the most interesting differences between the baseline parser and the self-trained
parser, using examples from our 5-essay sample described above.
Ambiguity of subordinating conjunctions: Figure 4 shows an example from a lower proficiency
essay that contains multiple interacting differences, primarily stemming from the fact that the POS tag
for a subordinating conjunction is the same as the POS tag for a regular preposition according to the
Penn Treebank guidelines (Santorini, 1995). The original parser (4a) treats it as a preposition: it is
dominated by PP and takes NP as a complement. The self-trained parser (4b) correctly treats because
as a subordinating conjunction: it is dominated by SBAR and takes S as a complement. In addition,
the original parser identified suffer as the main verb in the sentence. The self-trained parser correctly
analyzes this as part of the dependent clause, however this results in no main verb being identified and
an overall FRAGMENT analysis. Since it is unclear what the original intention of the writer was, this
fragment analysis could be more useful for identifying grammatical errors and giving feedback.
70
S. . . PP
IN
because
NP
DT
the
NN
world
VP
VBP
suffer
. . .
(a) Original Parser
FRAG
. . . SBAR
IN
because
S
NP
DT
the
NN
world
VP
VBP
suffer
. . .
(b) Self-Trained Parser
Figure 4: Parses for Especaily, in this time, because the world suffer, the economy empress.
Ambiguity of to: Figure 5 exemplifies a difference related to the analysis of infinitives. Here we can
see that the original parser analyzed the to phrase as a PP (c.f. afraid of) whereas the self-trained parser
analyzes it as an infinitival. We believe that the infinitival interpretation is slightly more likely (with a
missing verb do), though of course it is difficult to say for sure what the intended meaning is. Here there
are two interacting difference types: Sentential Components and Headedness. In the self-trained parse,
anything is an NN that heads a VP, whereas it is an NN that appropriately heads an NP in the original
parse. However, it is important to note that the self-trained parse treats to anything as an infinitive: a TO
dominated by a VP, which is dominated by a unary-branching S. The original parse treats to anything as
a regular PP. The fact that the self-trained parse contains a set of terminals exhaustively dominated by
an S-node that does not exist in the original parse constitutes a Sentential Components difference. We
believe that it is more useful to correctly identify infinitives and gerunds as sentential constituents, even
at the cost of an XP that is apparently headed by an inappropriate terminal (VP headed by NN).
S
. . . AdjP
JJ
afraid
PP
TO
to
NP
NN
anything
PP
during your life
(a) Original Parser
S
. . . AdjP
JJ
afraid
S
VP
TO
to
VP
NN
anything
PP
during your life
(b) Self-Trained Parser
Figure 5: Parses for If you have this experience, you will do not afraid to anything during your life.
Attachment ambiguity: We turn now to Figure 6. The main difference has to do with the attachment
of the phrase that you think it worth: the SBAR is attached to the VP in the original parse (as a clausal
complement) and to the NP in the self-trained parse (as a relative clause). This example also shows that
a change in POS-tag can have a significant impact on the final parse tree.
5 Future Work and Conclusions
We have shown that it is possible to apply self-training techniques in order to adapt a state-of-the-art
parser to be able to better parse English language learner text. We experimented with training the parser
on native text as well as non-native text. In an evaluation on student data (not necessarily language-
71
VP
VBN
used
PP
IN
in
NP
DT
the
NN
thing
SBAR
IN
that
S
you think it worth
(a) Original Parser
VP
VBN
used
PP
IN
in
NP
NP
DT
the
NN
thing
SBAR
WHNP
WDT
that
S
you think it worth
(b) Self-Trained Parser
Figure 6: Parses for So I support that the money should be used in the thing that you think it worth.
learner data) we found that both training sets performed at about the same level, but that both significantly
out-performed the baseline parser trained only on WSJ text.
We carry out an in-depth study on a small data set of 5 learner essays and define a set of difference
categories in order to describe the parse-tree differences from a linguistic perspective. We implement
rules to automatically detect these parse-tree differences and show that the general proportions of errors
found in the small data set are similar to that of a larger data set. We highlight some of the most interesting
improvements of the parser, and we show that despite various grammatical errors present in sentences,
the self-trained parser is, in general, able to assign better analyses than the baseline parser.
Of course, the self-trained parser does sometimes choose a parse that is less appropriate than the
baseline one. In particular, we noticed that this happened most frequently for the highest proficiency
essays. Further investigation is required to be able to better understand the reasons for this. In future
work, the most informative evaluation of the self-trained parser would be in a task-based setting. We
plan to investigate whether the self-trained parser improves the overall performance of tasks such as
automated essay scoring or automated error detection, which internally rely on parser output.
References
Eugene Charniak and Mark Johnson. 2005. Coarse-to-Fine n-Best Parsing and MaxEnt Discriminative Reranking.
In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL?05), pages
173?180, Ann Arbor, Michigan, June. Association for Computational Linguistics.
Eugene Charniak. 1997. Statistical parsing with a context-free grammar and word statistics. In Proceedings of the
Fourteenth National Conference on Artificial Intelligence, pages 598?603, Menlo Park, CA. AAAI Press/MIT
Press.
Rachele De Felice and Stephen Pulman. 2007. Automatically Acquiring Models of Preposition Use. In Pro-
ceedings of the Fourth ACL-SIGSEM Workshop on Prepositions, pages 45?50, Prague, Czech Republic, June.
Association for Computational Linguistics.
Markus Dickinson and Chong Min Lee. 2009. Modifying corpus annotation to support the analysis of learner
language. CALICO Journal, 26(3):545?561.
Markus Dickinson and Marwa Ragheb. 2009. Dependency Annotation for Learner Corpora. In Proceedings of
the Eighth Workshop on Treebanks and Linguistic Theories (TLT-8), pages 59?70, Milan, Italy.
Jennifer Foster, O?zlem C?etinog?lu, Joachim Wagner, and Josef van Genabith. 2011. Comparing the Use of Edited
and Unedited Text in Parser Self-Training. In Proceedings of the 12th International Conference on Parsing
Technologies, pages 215?219, Dublin, Ireland. Association for Computational Linguistics.
72
John Lee and Ola Knutsson. 2008. The Role of PP Attachment in Preposition Generation. In Proceedings
of CICLing 2008, 9th International Conference on Intelligent Text Processing and Computational Linguistics,
pages 643?654, Haifa, Israel.
David McClosky, Eugene Charniak, and Mark Johnson. 2006. Effective Self-Training for Parsing. In Proceedings
of the Human Language Technology Conference of the NAACL, Main Conference, pages 152?159, New York
City, USA, June. Association for Computational Linguistics.
David McClosky, Eugene Charniak, and Mark Johnson. 2008. When is Self-Training Effective for Parsing? In
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 561?568,
Manchester, UK, August. Coling 2008 Organizing Committee.
Wolfgang Menzel and Ingo Schro?der. 1999. Error diagnosis for language learning systems. ReCALL, 11:20?30.
Niels Ott and Ramon Ziai. 2010. Evaluating dependency parsing performance on German learner language. In
Proceedings of the Ninth Workshop on Treebanks and Linguistic Theories (TLT-9), pages 175?186.
Roi Reichart and Ari Rappoport. 2007. Self-Training for Enhancement and Domain Adaptation of Statistical
Parsers Trained on Small Datasets. In Proceedings of the 45th Annual Meeting of the Association of Computa-
tional Linguistics, pages 616?623, Prague, Czech Republic, June. Association for Computational Linguistics.
Beatrice Santorini. 1995. Part-of-speech tagging guidelines for the Penn Treebank Project (3rd revision). Techni-
cal Report, Department of Computer and Information Science, University of Pennsylvania.
Mark Steedman, Miles Osborne, Anoop Sarkar, Stephen Clark, Rebecca Hwa, Julia Hockenmaier, Paul Ruhlen,
Steven Baker, and Jeremiah Crim. 2003. Bootstrapping statistical parsers from small datasets. In Proceedings
of EACL 03, pages 331?228.
Anne Vandeventer Faltin. 2003. Syntactic Error Diagnosis in the context of Computer Assisted Language Learn-
ing. Ph.D. thesis, Universite? de Gene`ve.
73
