Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 852?860,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
SMS based Interface for FAQ Retrieval
Govind Kothari
IBM India Research Lab
gokothar@in.ibm.com
Sumit Negi
IBM India Research Lab
sumitneg@in.ibm.com
Tanveer A. Faruquie
IBM India Research Lab
ftanveer@in.ibm.com
Venkatesan T. Chakaravarthy
IBM India Research Lab
vechakra@in.ibm.com
L. Venkata Subramaniam
IBM India Research Lab
lvsubram@in.ibm.com
Abstract
Short Messaging Service (SMS) is popu-
larly used to provide information access to
people on the move. This has resulted in
the growth of SMS based Question An-
swering (QA) services. However auto-
matically handling SMS questions poses
significant challenges due to the inherent
noise in SMS questions. In this work we
present an automatic FAQ-based question
answering system for SMS users. We han-
dle the noise in a SMS query by formu-
lating the query similarity over FAQ ques-
tions as a combinatorial search problem.
The search space consists of combinations
of all possible dictionary variations of to-
kens in the noisy query. We present an ef-
ficient search algorithm that does not re-
quire any training data or SMS normaliza-
tion and can handle semantic variations in
question formulation. We demonstrate the
effectiveness of our approach on two real-
life datasets.
1 Introduction
The number of mobile users is growing at an
amazing rate. In India alone a few million sub-
scribers are added each month with the total sub-
scriber base now crossing 370 million. The any-
time anywhere access provided by mobile net-
works and portability of handsets coupled with the
strong human urge to quickly find answers has fu-
eled the growth of information based services on
mobile devices. These services can be simple ad-
vertisements, polls, alerts or complex applications
such as browsing, search and e-commerce. The
latest mobile devices come equipped with high
resolution screen space, inbuilt web browsers and
full message keypads, however a majority of the
users still use cheaper models that have limited
screen space and basic keypad. On such devices,
SMS is the only mode of text communication.
This has encouraged service providers to build in-
formation based services around SMS technology.
Today, a majority of SMS based information ser-
vices require users to type specific codes to re-
trieve information. For example to get a duplicate
bill for a specific month, say June, the user has
to type DUPBILLJUN. This unnecessarily con-
straints users who generally find it easy and intu-
itive to type in a ?texting? language.
Some businesses have recently allowed users to
formulate queries in natural language using SMS.
For example, many contact centers now allow cus-
tomers to ?text? their complaints and requests for
information over SMS. This mode of communica-
tion not only makes economic sense but also saves
the customer from the hassle of waiting in a call
queue. Most of these contact center based services
and other regular services like ?AQA 63336?1 by
Issuebits Ltd, GTIP2 by AlienPant Ltd., ?Tex-
perts?3 by Number UK Ltd. and ?ChaCha?4 use
human agents to understand the SMS text and re-
spond to these SMS queries. The nature of tex-
ting language, which often as a rule rather than ex-
ception, has misspellings, non-standard abbrevia-
tions, transliterations, phonetic substitutions and
omissions, makes it difficult to build automated
question answering systems around SMS technol-
ogy. This is true even for questions whose answers
are well documented like a FAQ database. Un-
like other automatic question answering systems
that focus on generating or searching answers, in
a FAQ database the question and answers are al-
ready provided by an expert. The task is then
to identify the best matching question-answer pair
for a given query.
In this paper we present a FAQ-based ques-
tion answering system over a SMS interface. Our
1http://www.aqa.63336.com/
2http://www.gtip.co.uk/
3http://www.texperts.com/
4http://www.chacha.com/
852
system allows the user to enter a question in
the SMS texting language. Such questions are
noisy and contain spelling mistakes, abbrevia-
tions, deletions, phonetic spellings, translitera-
tions etc. Since mobile handsets have limited
screen space, it necessitates that the system have
high accuracy. We handle the noise in a SMS
query by formulating the query similarity over
FAQ questions as a combinatorial search prob-
lem. The search space consists of combinations
of all possible dictionary variations of tokens in
the noisy query. The quality of the solution, i.e.
the retrieved questions is formalized using a scor-
ing function. Unlike other SMS processing sys-
tems our model does not require training data or
human intervention. Our system handles not only
the noisy variations of SMS query tokens but also
semantic variations. We demonstrate the effective-
ness of our system on real-world data sets.
The rest of the paper is organized as follows.
Section 2 describes the relevant prior work in this
area and talks about our specific contributions.
In Section 3 we give the problem formulation.
Section 4 describes the Pruning Algorithm which
finds the best matching question for a given SMS
query. Section 5 provides system implementation
details. Section 6 provides details about our exper-
iments. Finally we conclude in Section 7.
2 Prior Work
There has been growing interest in providing ac-
cess to applications, traditionally available on In-
ternet, on mobile devices using SMS. Examples
include Search (Schusteritsch et al, 2005), access
to Yellow Page services (Kopparapu et al, 2007),
Email 5, Blog 6 , FAQ retrieval 7 etc. As high-
lighted earlier, these SMS-based FAQ retrieval ser-
vices use human experts to answer questions.
There are other research and commercial sys-
tems which have been developed for general ques-
tion and answering. These systems generally
adopt one of the following three approaches:
Human intervention based, Information Retrieval
based, or Natural language processing based. Hu-
man intervention based systems exploit human
communities to answer questions. These sys-
tems 8 are interesting because they suggest simi-
lar questions resolved in the past. Other systems
5http://www.sms2email.com/
6http://www.letmeparty.com/
7http://www.chacha.com/
8http://www.answers.yahoo.com/
like Chacha and Askme9 use qualified human ex-
perts to answer questions in a timely manner. The
information retrieval based system treat question
answering as an information retrieval problem.
They search large corpus of text for specific text,
phrases or paragraphs relevant to a given question
(Voorhees, 1999). In FAQ based question answer-
ing, where FAQ provide a ready made database of
question-answer, the main task is to find the clos-
est matching question to retrieve the relevant an-
swer (Sneiders, 1999) (Song et al, 2007). The
natural language processing based system tries to
fully parse a question to discover semantic struc-
ture and then apply logic to formulate the answer
(Molla et al, 2003). In another approach the ques-
tions are converted into a template representation
which is then used to extract answers from some
structured representation (Sneiders, 2002) (Katz et
al., 2002). Except for human intervention based
QA systems most of the other QA systems work
in restricted domains and employ techniques such
as named entity recognition, co-reference resolu-
tion, logic form transformation etc which require
the question to be represented in linguistically cor-
rect format. These methods do not work for SMS
based FAQ answering because of the high level of
noise present in SMS text.
There exists some work to remove noise from
SMS (Choudhury et al, 2007) (Byun et al, 2007)
(Aw et al, 2006) (Kobus et al, 2008). How-
ever, all of these techniques require aligned cor-
pus of SMS and conventional language for train-
ing. Building this aligned corpus is a difficult task
and requires considerable human effort. (Acharya
et al, 2008) propose an unsupervised technique
that maps non-standard words to their correspond-
ing conventional frequent form. Their method can
identify non-standard transliteration of a given to-
ken only if the context surrounding that token is
frequent in the corpus. This might not be true in
all domains.
2.1 Our Contribution
To the best of our knowledge we are the first to
handle issues relating to SMS based automatic
question-answering. We address the challenges
in building a FAQ-based question answering sys-
tem over a SMS interface. Our method is unsu-
pervised and does not require aligned corpus or
explicit SMS normalization to handle noise. We
propose an efficient algorithm that handles noisy
9http://www.askmehelpdesk.com/
853
lexical and semantic variations.
3 Problem Formulation
We view the input SMS S as a sequence of tokens
S = s1, s2, . . . , sn. Let Q denote the set of ques-
tions in the FAQ corpus. Each question Q ? Q
is also viewed as a sequence of terms. Our goal
is to find the question Q? from the corpus Q that
best matches the SMS S. As mentioned in the in-
troduction, the SMS string is bound to have mis-
spellings and other distortions, which needs to be
taken care of while performing the match.
In the preprocessing stage, we develop a Do-
main dictionary D consisting of all the terms that
appear in the corpusQ. For each term t in the dic-
tionary and each SMS token si, we define a simi-
larity measure ?(t, si) that measures how closely
the term t matches the SMS token si. We say that
the term t is a variant of si, if ?(t, si) > 0; this is
denoted as t ? si. Combining the similarity mea-
sure and the inverse document frequency (idf) of t
in the corpus, we define a weight function ?(t, si).
The similarity measure and the weight function are
discussed in detail in Section 5.1.
Based on the weight function, we define a scor-
ing function for assigning a score to each question
in the corpus Q. The score measures how closely
the question matches the SMS string S. Consider
a question Q ? Q. For each token si, the scor-
ing function chooses the term from Q having the
maximum weight; then the weight of the n chosen
terms are summed up to get the score.
Score(Q) =
n?
i=1
[
max
t:t?Q and t?si
?(t, si)
]
(1)
Our goal is to efficiently find the question Q? hav-
ing the maximum score.
4 Pruning Algorithm
We now describe algorithms for computing the
maximum scoring question Q?. For each token
si, we create a list Li consisting of all terms from
the dictionary that are variants of si. Consider a
token si. We collect all the variants of si from the
dictionary and compute their weights. The vari-
ants are then sorted in the descending order of
their weights. At the end of the process we have n
ranked lists. As an illustration, consider an SMS
query ?gud plc buy 10s strng on9?. Here, n = 6
and six lists of variants will be created as shown
Figure 1: Ranked List of Variations
in Figure 1. The process of creating the lists is
speeded up using suitable indices, as explained in
detail in Section 5.
Now, we assume that the lists L1, L2, . . . , Ln
are created and explain the algorithms for com-
puting the maximum scoring question Q?. We de-
scribe two algorithms for accomplishing the above
task. The two algorithms have the same function-
ality i.e. they compute Q?, but the second algo-
rithm called the Pruning algorithm has a better
run time efficiency compared to the first algorithm
called the naive algorithm. Both the algorithms re-
quire an index which takes as input a term t from
the dictionary and returns Qt, the set of all ques-
tions in the corpus that contain the term t. We
call the above process as querying the index on
the term t. The details of the index creation is dis-
cussed in Section 5.2.
Naive Algorithm: In this algorithm, we scan
each list Li and query the index on each term ap-
pearing in Li. The returned questions are added to
a collection C. That is,
C =
n?
i=1
?
?
?
t?Li
Qt
?
?
The collection C is called the candidate set. No-
tice that any question not appearing in the candi-
date set has a score 0 and thus can be ignored. It
follows that the candidate set contains the maxi-
mum scoring question Q?. So, we focus on the
questions in the collection C, compute their scores
and find the maximum scoring question Q?. The
scores of the question appearing in C can be com-
puted using Equation 1.
The main disadvantage with the naive algorithm
is that it queries each term appearing in each list
and hence, suffers from high run time cost. We
next explain the Pruning algorithm that avoids this
pitfall and queries only a substantially small subset
of terms appearing in the lists.
Pruning Algorithm: The pruning algorithm
854
is inspired by the Threshold Algorithm (Fagin et
al., 2001). The Pruning algorithm has the prop-
erty that it queries fewer terms and ends up with
a smaller candidate set as compared to the naive
algorithm. The algorithm maintains a candidate
set C of questions that can potentially be the max-
imum scoring question. The algorithm works in
an iterative manner. In each iteration, it picks
the term that has maximum weight among all the
terms appearing in the lists L1, L2, . . . , Ln. As
the lists are sorted in the descending order of the
weights, this amounts to picking the maximum
weight term amongst the first terms of the n lists.
The chosen term t is queried to find the setQt. The
set Qt is added to the candidate set C. For each
question Q ? Qt, we compute its score Score(Q)
and keep it along with Q. The score can be com-
puted by Equation 1 (For each SMS token si, we
choose the term from Q which is a variant of si
and has the maximum weight. The sum of the
weights of chosen terms yields Score(Q)). Next,
the chosen term t is removed from the list. Each
iteration proceeds as above. We shall now develop
a thresholding condition such that when it is sat-
isfied, the candidate set C is guaranteed to contain
the maximum scoring questionQ?. Thus, once the
condition is met, we stop the above iterative pro-
cess and focus only on the questions in C to find
the maximum scoring question.
Consider end of some iteration in the above pro-
cess. Suppose Q is a question not included in C.
We can upperbound the score achievable by Q, as
follows. At best, Q may include the top-most to-
ken from every list L1, L2, . . . , Ln. Thus, score of
Q is bounded by
Score(Q) ?
n?
i=0
?(Li[1]).
(Since the lists are sorted Li[1] is the term having
the maximum weight in Li). We refer to the RHS
of the above inequality as UB.
Let Q? be the question in C having the maximum
score. Notice that if Q? ? UB, then it is guaranteed
that any question not included in the candidate set
C cannot be the maximum scoring question. Thus,
the condition ?Q? ? UB? serves as the termination
condition. At the end of each iteration, we check
if the termination condition is satisfied and if so,
we can stop the iterative process. Then, we simply
pick the question in C having the maximum score
and return it. The algorithm is shown in Figure 2.
In this section, we presented the Pruning algo-
Procedure Pruning Algorithm
Input: SMS S = s1, s2, . . . , sn
Output: Maximum scoring question Q?.
Begin
Construct lists L1, L2, . . . , Ln //(see Section 5.3).
// Li lists variants of si in descending
//order of weight.
Candidate list C = ?.
repeat
j? = argmaxi?(Li[1])
t? = Lj? [1]
// t? is the term having maximum weight among
// all terms appearing in the n lists.
Delete t? from the list Lj? .
Query the index and fetch Qt?
// Qt? : the set of all questions inQ
//having the term t?
For each Q ? Qt?
Compute Score(Q) and
add Q with its score into C
UB =
?n
i=1 ?(Li[1])
Q? = argmaxQ?CScore(Q).
if Score(Q?) ? UB, then
// Termination condition satisfied
Output Q? and exit.
forever
End
Figure 2: Pruning Algorithm
rithm that efficiently finds the best matching ques-
tion for the given SMS query without the need to
go through all the questions in the FAQ corpus.
The next section describes the system implemen-
tation details of the Pruning Algorithm.
5 System Implementation
In this section we describe the weight function,
the preprocessing step and the creation of lists
L1, L2, . . . , Ln.
5.1 Weight Function
We calculate the weight for a term t in the dic-
tionary w.r.t. a given SMS token si. The weight
function is a combination of similarity measure
between t and si and Inverse Document Frequency
(idf) of t. The next two subsections explain the
calculation of the similarity measure and the idf in
detail.
5.1.1 Similarity Measure
Let D be the dictionary of all the terms in the cor-
pus Q. For term t ? D and token si of the SMS,
the similarity measure ?(t, si) between them is
855
?(t, si) =
?
????
????
LCSRatio(t,si)
EditDistanceSMS(t,si)
if t and si share same
starting character *
0 otherwise
(2)
where LCSRatio(t, si) =
length(LCS(t,si))
length(t) and LCS(t, si) is
the Longest common subsequence between t and si.
* The rationale behind this heuristic is that while typing a SMS, people
typically type the first few characters correctly. Also, this heuristic helps limit
the variants possible for a given token.
The Longest Common Subsequence Ratio
(LCSR) (Melamed, 1999) of two strings is the ra-
tio of the length of their LCS and the length of the
longer string. Since in SMS text, the dictionary
term will always be longer than the SMS token,
the denominator of LCSR is taken as the length of
the dictionary term. We call this modified LCSR
as the LCSRatio.
Procedure EditDistanceSMS
Input: term t, token si
Output: Consonant Skeleton Edit distance
Begin
return LevenshteinDistance(CS(si), CS(t)) + 1
// 1 is added to handle the case where
// Levenshtein Distance is 0
End
Consonant Skeleton Generation (CS)
1. remove consecutive repeated characters
// (call? cal)
2. remove all vowels
//(waiting ? wtng, great? grt)
Figure 3: EditDistanceSMS
The EditDistanceSMS shown in Figure 3
compares the Consonant Skeletons (Prochasson et
al., 2007) of the dictionary term and the SMS to-
ken. If the consonant keys are similar, i.e. the Lev-
enshtein distance between them is less, the simi-
larity measure defined in Equation 2 will be high.
We explain the rationale behind using the
EditDistanceSMS in the similarity measure
?(t, si) through an example. For the SMS
token ?gud? the most likely correct form is
?good?. The two dictionary terms ?good? and
?guided? have the same LCSRatio of 0.5 w.r.t
?gud?, but the EditDistanceSMS of ?good? is
1 which is less than that of ?guided?, which has
EditDistanceSMS of 2 w.r.t ?gud?. As a result
the similarity measure between ?gud? and ?good?
will be higher than that of ?gud? and ?guided?.
5.1.2 Inverse Document Frequency
If f number of documents in corpus Q contain a
term t and the total number of documents in Q is
N, the Inverse Document Frequency (idf) of t is
idf(t) = log
N
f
(3)
Combining the similarity measure and the idf
of t in the corpus, we define the weight function
?(t, si) as
?(t, si) = ?(t, si) ? idf(t) (4)
The objective behind the weight function is
1. We prefer terms that have high similarity
measure i.e. terms that are similar to the
SMS token. Higher the LCSRatio and lower
the EditDistanceSMS , higher will be the
similarity measure. Thus for example, for a
given SMS token ?byk?, similarity measure
of word ?bike? is higher than that of ?break?.
2. We prefer words that are highly discrimi-
native i.e. words with a high idf score.
The rationale for this stems from the fact
that queries, in general, are composed of in-
formative words. Thus for example, for a
given SMS token ?byk?, idf of ?bike? will
be more than that of commonly occurring
word ?back?. Thus, even though the similar-
ity measure of ?bike? and ?back? are same
w.r.t. ?byk?, ?bike? will get a higher weight
than ?back? due to its idf.
We combine these two objectives into a single
weight function multiplicatively.
5.2 Preprocessing
Preprocessing involves indexing of the FAQ cor-
pus, formation of Domain and Synonym dictionar-
ies and calculation of the Inverse Document Fre-
quency for each term in the Domain dictionary.
As explained earlier the Pruning algorithm re-
quires retrieval of all questions Qt that contains a
given term t. To do this efficiently we index the
FAQ corpus using Lucene10. Each question in the
FAQ corpus is treated as a Document; it is tok-
enized using whitespace as delimiter and indexed.
10http://lucene.apache.org/java/docs/
856
The Domain dictionaryD is built from all terms
that appear in the corpus Q.
The weight calculation for Pruning algorithm
requires the idf for a given term t. For each term t
in the Domain dictionary, we query the Lucene in-
dexer to get the number of Documents containing
t. Using Equation 3, the idf(t) is calculated. The
idf for each term t is stored in a Hashtable, with t
as the key and idf as its value.
Another key step in the preprocessing stage is
the creation of the Synonym dictionary. The Prun-
ing algorithm uses this dictionary to retrieve se-
mantically similar questions. Details of this step is
further elaborated in the List Creation sub-section.
The Synonym Dictionary creation involves map-
ping each word in the Domain dictionary to it?s
corresponding Synset obtained from WordNet11.
5.3 List Creation
Given a SMS S, it is tokenized using white-spaces
to get a sequence of tokens s1, s2, . . . , sn. Digits
occurring in SMS token (e.g ?10s? , ?4get?) are re-
placed by string based on a manually crafted digit-
to-string mapping (?10? ? ?ten?). A list Li is
setup for each token si using terms in the domain
dictionary. The list for a single character SMS to-
ken is set to null as it is most likely to be a stop
word . A term t from domain dictionary is in-
cluded in Li if its first character is same as that of
the token si and it satisfies the threshold condition
length(LCS(t, si)) > 1.
Each term t that is added to the list is assigned a
weight given by Equation 4.
Terms in the list are ranked in descending or-
der of their weights. Henceforth, the term ?list?
implies a ranked list.
For example the SMS query ?gud plc 2 buy 10s
strng on9? (corresponding question ?Where is a
good place to buy tennis strings online??), is to-
kenized to get a set of tokens {?gud?, ?plc?, ?2?,
?buy?, ?10s?, ?strng?, ?on9?}. Single character to-
kens such as ?2? are neglected as they are most
likely to be stop words. From these tokens cor-
responding lists are setup as shown in Figure 1.
5.3.1 Synonym Dictionary Lookup
To retrieve answers for SMS queries that are
semantically similar but lexically different from
questions in the FAQ corpus we use the Synonym
dictionary described in Section 5.2. Figure 4 illus-
trates some examples of such SMS queries.
11http://wordnet.princeton.edu/
Figure 4: Semantically similar SMS and questions
Figure 5: Synonym Dictionary LookUp
For a given SMS token si, the list of variations
Li is further augmented using this Synonym dic-
tionary. For each token si a fuzzy match is per-
formed between si and the terms in the Synonym
dictionary and the best matching term from the
Synonym dictionary, ? is identified. As the map-
pings between the Synonym and the Domain dic-
tionary terms are maintained, we obtain the corre-
sponding Domain dictionary term ? for the Syn-
onym term ? and add that term to the list Li. ? is
assigned a weight given by
?(?, si) = ?(?, si) ? idf(?) (5)
It should be noted that weight for ? is based on
the similarity measure between Synonym dictio-
nary term ? and SMS token si.
For example, the SMS query ?hw2 countr quik
srv?( corresponding question ?How to return a
very fast serve??) has two terms ?countr? ?
?counter? and ?quik? ? ?quick? belonging to
the Synonym dictionary. Their associated map-
pings in the Domain dictionary are ?return? and
?fast? respectively as shown in Figure 5. During
the list setup process the token ?countr? is looked
857
up in the Domain dictionary. Terms from the Do-
main dictionary that begin with the same character
as that of the token ?countr? and have a LCS > 1
such as ?country?,?count?, etc. are added to the
list and assigned a weight given by Equation 4.
After that, the token ?countr? is looked up in the
Synonym dictionary using Fuzzy match. In this
example the term ?counter? from the Synonym
dictionary fuzzy matches the SMS token. The Do-
main dictionary term corresponding to the Syn-
onym dictionary term ?counter? is looked up and
added to the list. In the current example the cor-
responding Domain dictionary term is ?return?.
This term is assigned a weight given by Equation
5 and is added to the list as shown in Figure 5.
5.4 FAQ retrieval
Once the lists are created, the Pruning Algorithm
as shown in Figure 2 is used to find the FAQ ques-
tionQ? that best matches the SMS query. The cor-
responding answer to Q? from the FAQ corpus is
returned to the user.
The next section describes the experimental
setup and results.
6 Experiments
We validated the effectiveness and usability of
our system by carrying out experiments on two
FAQ data sets. The first FAQ data set, referred
to as the Telecom Data-Set, consists of 1500 fre-
quently asked questions, collected from a Telecom
service provider?s website. The questions in this
data set are related to the Telecom providers prod-
ucts or services. For example queries about call
rates/charges, bill drop locations, how to install
caller tunes, how to activate GPRS etc. The sec-
ond FAQ corpus, referred to as the Yahoo DataSet,
consists of 7500 questions from three Yahoo!
Answers12 categories namely Sports.Swimming,
Sports.Tennis, Sports.Running.
To measure the effectiveness of our system, a
user evaluation study was performed. Ten human
evaluators were asked to choose 10 questions ran-
domly from the FAQ data set. None of the eval-
uators were authors of the paper. They were pro-
vided with a mobile keypad interface and asked to
?text? the selected 10 questions as SMS queries.
Through that exercise 100 relevant SMS queries
per FAQ data set were collected. Figure 6 shows
sample SMS queries. In order to validate that the
system was able to handle queries that were out of
12http://answers.yahoo.com/
Figure 6: Sample SMS queries
Data Set Relevant Queries Irrelevant Queries
Telecom 100 50
Yahoo 100 50
Table 1: SMS Data Set.
the FAQ domain, we collected 5 irrelevant SMS
queries from each of the 10 human-evaluators for
both the data sets. Irrelevant queries were (a)
Queries out of the FAQ domain e.g. queries re-
lated to Cricket, Billiards, activating GPS etc (b)
Absurd queries e.g. ?ama ameyu tuem? (sequence
of meaningless words) and (c) General Queries
e.g. ?what is sports?. Table 1 gives the number
of relevant and irrelevant queries used in our ex-
periments.
The average word length of the collected SMS
messages for Telecom and Yahoo datasets was 4
and 7 respectively. We manually cleaned the SMS
query data word by word to create a clean SMS
test-set. For example, the SMS query ?h2 mke a
pdl bke fstr? was manually cleaned to get ?how
to make pedal bike faster?. In order to quantify
the level of noise in the collected SMS data, we
built a character-level language model(LM)13 us-
ing the questions in the FAQ data-set (vocabulary
size is 44 characters) and computed the perplex-
ity14 of the language model on the noisy and the
cleaned SMS test-set. The perplexity of the LM on
a corpus gives an indication of the average num-
ber of bits needed per n-gram to encode the cor-
pus. Noise will result in the introduction of many
previously unseen n-grams in the corpus. Higher
number of bits are needed to encode these improb-
able n-grams which results in increased perplexity.
From Table 2 we can see the difference in perplex-
ity for noisy and clean SMS data for the Yahoo
and Telecom data-set. The high level of perplexity
in the SMS data set indicates the extent of noise
present in the SMS corpus.
To handle irrelevant queries the algorithm de-
scribed in Section 4 is modified. Only if the
Score(Q?) is above a certain threshold, it?s answer
is returned, else we return ?null?. The threshold
13http://en.wikipedia.org/wiki/Language model
14bits = log2(perplexity)
858
Cleaned SMS Noisy SMS
Yahoo bigram 14.92 74.58trigram 8.11 93.13
Telecom bigram 17.62 59.26trigram 10.27 63.21
Table 2: Perplexity for Cleaned and Noisy SMS
Figure 7: Accuracy on Telecom FAQ Dataset
was determined experimentally.
To retrieve the correct answer for the posed
SMS query, the SMS query is matched against
questions in the FAQ data set and the best match-
ing question(Q?) is identified using the Pruning al-
gorithm. The system then returns the answer to
this best matching question to the human evalua-
tor. The evaluator then scores the response on a bi-
nary scale. A score of 1 is given if the returned an-
swer is the correct response to the SMS query, else
it is assigned 0. The scoring procedure is reversed
for irrelevant queries i.e. a score of 0 is assigned
if the system returns an answer and 1 is assigned
if it returns ?null? for an ?irrelevant? query. The
result of this evaluation on both data-sets is shown
in Figure 7 and 8.
Figure 8: Accuracy on Yahoo FAQ Dataset
In order to compare the performance of our sys-
tem, we benchmark our results against Lucene?s
15 Fuzzy match feature. Lucene supports fuzzy
searches based on the Levenshtein Distance, or
Edit Distance algorithm. To do a fuzzy search
15http://lucene.apache.org
we specify the ? symbol at the end of each to-
ken of the SMS query. For example, the SMS
query ?romg actvt? on the FAQ corpus is refor-
mulated as ?romg? 0.3 actvt? 0.3?. The param-
eter after the ? specifies the required similarity.
The parameter value is between 0 and 1, with a
value closer to 1 only terms with higher similar-
ity will be matched. These queries are run on the
indexed FAQs. The results of this evaluation on
both data-sets is shown in Figure 7 and 8. The
results clearly demonstrate that our method per-
forms 2 to 2.5 times better than Lucene?s Fuzzy
match. It was observed that with higher values
of similarity parameter (? 0.6, ? 0.8), the num-
ber of correctly answered queries was even lower.
In Figure 9 we show the runtime performance of
the Naive vs Pruning algorithm on the Yahoo FAQ
Dataset for 150 SMS queries. It is evident from
Figure 9 that not only does the Pruning Algorithm
outperform the Naive one but also gives a near-
constant runtime performance over all the queries.
The substantially better performance of the Prun-
ing algorithm is due to the fact that it queries much
less number of terms and ends up with a smaller
candidate set compared to the Naive algorithm.
Figure 9: Runtime of Pruning vs Naive Algorithm
for Yahoo FAQ Dataset
7 Conclusion
In recent times there has been a rise in SMS based
QA services. However, automating such services
has been a challenge due to the inherent noise in
SMS language. In this paper we gave an efficient
algorithm for answering FAQ questions over an
SMS interface. Results of applying this on two
different FAQ datasets shows that such a system
can be very effective in automating SMS based
FAQ retrieval.
859
References
Rudy Schusteritsch, Shailendra Rao, Kerry Rodden.
2005. Mobile Search with Text Messages: Design-
ing the User Experience for Google SMS. CHI,
Portland, Oregon.
Sunil Kumar Kopparapu, Akhilesh Srivastava and Arun
Pande. 2007. SMS based Natural Language Inter-
face to Yellow Pages Directory, In Proceedings of
the 4th International conference on mobile technol-
ogy, applications, and systems and the 1st Interna-
tional symposium on Computer human interaction
in mobile technology, Singapore.
Monojit Choudhury, Rahul Saraf, Sudeshna Sarkar, Vi-
jit Jain, and Anupam Basu. 2007. Investigation and
Modeling of the Structure of Texting Language, In
Proceedings of IJCAI-2007 Workshop on Analytics
for Noisy Unstructured Text Data, Hyderabad.
E. Voorhees. 1999. The TREC-8 question answering
track report.
D. Molla. 2003. NLP for Answer Extraction in Tech-
nical Domains, In Proceedings of EACL, USA.
E. Sneiders. 2002. Automated question answering
using question templates that cover the conceptual
model of the database, In Proceedings of NLDB,
pages 235?239.
B. Katz, S. Felshin, D. Yuret, A. Ibrahim, J. Lin, G.
Marton, and B. Temelkuran. 2002. Omnibase: Uni-
form access to heterogeneous data for question an-
swering, Natural Language Processing and Infor-
mation Systems, pages 230?234.
E. Sneiders. 1999. Automated FAQ Answering: Con-
tinued Experience with Shallow Language Under-
standing, Question Answering Systems. Papers from
the 1999 AAAI Fall Symposium. Technical Report
FS-99?02, November 5?7, North Falmouth, Mas-
sachusetts, USA, AAAI Press, pp.97?107
W. Song, M. Feng, N. Gu, and L. Wenyin. 2007.
Question similarity calculation for FAQ answering,
In Proceeding of SKG 07, pages 298?301.
Aiti Aw, Min Zhang, Juan Xiao, and Jian Su. 2006.
A phrase-based statistical model for SMS text nor-
malization, In Proceedings of COLING/ACL, pages
33?40.
Catherine Kobus, Franois Yvon and Graldine Damnati.
2008. Normalizing SMS: are two metaphors bet-
ter than one?, In Proceedings of the 22nd Inter-
national Conference on Computational Linguistics,
pages 441?448 Manchester.
Jeunghyun Byun, Seung-Wook Lee, Young-In Song,
Hae-Chang Rim. 2008. Two Phase Model for SMS
Text Messages Refinement, Association for the Ad-
vancement of Artificial Intelligence. AAAI Workshop
on Enhanced Messaging
Ronald Fagin , Amnon Lotem , Moni Naor. 2001.
Optimal aggregation algorithms for middleware, In
Proceedings of the 20th ACM SIGMOD-SIGACT-
SIGART symposium on Principles of database sys-
tems.
I. Dan Melamed. 1999. Bitext maps and alignment via
pattern recognition, Computational Linguistics.
E. Prochasson, Christian Viard-Gaudin, Emmanuel
Morin. 2007. Language Models for Handwritten
Short Message Services, In Proceedings of the 9th
International Conference on Document Analysis and
Recognition.
Sreangsu Acharya, Sumit Negi, L. V. Subramaniam,
Shourya Roy. 2008. Unsupervised learning of mul-
tilingual short message service (SMS) dialect from
noisy examples, In Proceedings of the second work-
shop on Analytics for noisy unstructured text data.
860
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1468?1476, Dublin, Ireland, August 23-29 2014.
Single Document Keyphrase Extraction Using Label Information
Sumit Negi
IBM Research
Delhi, India
sumitneg@in.ibm.com
Abstract
Keyphrases have found wide ranging application in NLP and IR tasks such as document sum-
marization, indexing, labeling, clustering and classification. In this paper we pose the problem
of extracting label specific keyphrases from a document which has document level metadata as-
sociated with it namely labels or tags (i.e. multi-labeled document). Unlike other, supervised
or unsupervised, methods for keyphrase extraction our proposed methods utilizes both the doc-
ument?s text and label information for the task of extracting label specific keyphrases. We pro-
pose two models for this purpose both of which model the problem of extracting label specific
keyphrases as a random walk on the document?s text graph. We evaluate and report the quality
of the extracted keyphrases on a popular multi-label text corpus.
1 Introduction
The use of graphs to model and solve various problems arising in Natural Language Processing have
lately become very popular. Graph theoretical methods or graph based approaches have been success-
fully applied for a varied set of NLP tasks such as Word Sense Disambiguation, Text Summarization,
Topic detection etc. One of the earliest and most prominent work in this area has been the TextRank (Mi-
halcea and Tarau, 2004) method - an unsupervised graph-based ranking model for extracting keyphrases
and ?key? sentences from natural language text. This unsupervised method extracts prominent terms,
phrases and sentences from text. The TextRank models the text as a graph where, depending on the end
application, text units of various sizes and characteristics can be added as vertices e.g. open class words,
collocations, sentences etc. Similarly, based on the application, connections can be drawn between these
vertices e.g. lexical or semantic relation, contextual overlap etc. To identify ?central? or ?key? text units
in this text graph, TextRank runs the PageRank algorithm on this constructed graph. The ranking over
vertices (text units), which indicates their centrality and importance, is obtained by finding the stationary
distribution of the random walk on the text graph.
In this paper, we consider the problem of extracting label specific keyphrases from a document which
has document level metadata associated with it namely labels (i.e. multi-labeled document). To elabo-
rate, consider a document as shown in Figure 1. This document has been assigned to two categories as
indicated by the labels ?Air Pollution? and ?Plant Physiology?. Running TextRank on this article yields
top ranked key-phrases such as ?calibrated instrument?, ?polluting gases?, ?industrial development?
etc. These keyphrases, though central to the article, are not specific to any of the labels that have been
assigned to the article. For instance, one would associate keyphrases such as ?carbon monoxide ?, ?air
pollutants? to be more relevant to the ?Air Pollution? label and keyphrases such as ?stomatal movement?,
?cell defense? to be more closely associated with the ?Plant Physiology? label. The objective of this pa-
per is to explore extensions to TextRank for extracting label-specific keyphrases from a multi-labeled
document. Such label-specific keyphrases can be useful for a number of practical applications namely:
highlighting such terms within the body of a document could provide a label-specific (topic-focussed)
view of the document thus facilitating fast browsing and reading of the document, such key terms could
also be useful for generating topic-driven or label-specific summaries and in multifaceted search.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1468
Figure 1: Label specific keyphrases (best viewed in color). Note that there could be keyphrases that are
common to both labels. Due to space restrictions only a snippet of the document is shown.
The rest of the paper is organized as following. We discuss related work and provide an overview
of our approach in Section 2. Details of the proposed method is discussed in Section 3 followed by
evaluation in Section 4. Future work and conclusion is presented in Section 5.
2 Related Work
The methods for keyphrase (or keyword) extraction can be roughly categorized into either unsupervised
or supervised. Unsupervised methods usually involve assigning a saliency score to each candidate phrase
by considering various features. Popular work in this area include the use of point-wise KL-divergence
between multiple language models for scoring both phrase-ness and informativeness of candidate phrases
(Tomokiyo and Hurst, 2003), use of TF-IDF weighting (A. Hulth, 2003) etc. Supervised machine learn-
ing algorithms have been proposed to classify a candidate phrase into either keyphrase or not using
features such as the frequency of occurrence, POS information, and location of the phrase in the docu-
ment. All the above methods only make use the document text for generating keyphrases and cannot be
used (as-is) for generating label-specific keyphrases.
One possible method for extracting label-specific keyphrases from a document could be based on
post-processing the output of the TextRank algorithm in the following way (1) Identify a set of label
specific features f
cand
l
(unigram terms) that are strongly correlated with the label. This could be done
by applying feature selection methods (Forman, 2003), (Forman, 2003) on a multi-label text corpus (we
discuss this step in more detail in a later section). For instance, f
cand
air_pollution
={?pollutant?,?gases?,...}
(2) Run the TextRank algorithm on the document d to generate a list of keyphrases keyphrase
d
(3) Filter
the resultant list keyphrase
d
based on lexical or semantic match with the label specific features f
cand
l
to
generate keyphrase
l
d
or label-l specific keyphrase for document d.
This approach suffers from the following limitations (a) The keyphrase list generated in Step (2) i.e.
keyphrase
d
might be dominated by keyphrases which have little to do with label l. Post processing
this list (Step 3) using f
cand
l
might result in only very few keyphrases in keyphrase
l
d
. (b) The label
specific features f
cand
l
, which are derived from corpus level statistics
1
, might not be the best indicator of
the keyphrase-ness of a term in the document. (c) Moreover, consider a scenario where a document is
associated with more than one label. Consider the previous example where the document is associated
with two labels ?Air Pollution? and ?Plant Physiology?. When extracting keyphrases specific to the
label/category ?Air Pollution? from document d one would expect that the extracted keyphrases are
closer to the Air Pollution label/category and distant from other labels associated with document d i.e.
?Plant Physiology?. It is not evident how this can be modeled in this approach. In this paper we propose
an approach that models the problem of finding label-specific keyphrases in a document as a random
walk on the document?s text-graph. Two approaches are proposed namely PTR: Personalized TextRank
and TRDMS: TextRank using Ranking on Data Manifolds with Sinks.
1
Using feature selection methods
1469
PTR: Personalized TextRank : In this setting the PageRank algorithm, which is the underpinning of
the TextRank keyphrase extraction algorithm, is replaced with the personalized page rank (Haveliwala,
2002) algorithm. By using the label specific features f
cand
l
as the personalization vector we are able to
bias the walk on the underlying text graph towards terms relevant to the label. We discuss this approach
in more detail in Section 3.3. Even though using a label specific transport or personalization vector
helps bias the walk towards terms specific to that label, terms relevant to labels other than l continue to
influence the walk. The Personalized TextRank method offers no elegant solution which would penalize
terms unrelated to l while simultaneously preferring terms relevant to label l.
To achieve both these goals in one model we propose the TRDMS: TextRank using Ranking on Data
Manifolds with Sinks approach. We model the problem of identifying label specific keyphrases in a given
document as a random walk over the document?s weighted text graph with sink and query nodes
2
. Rank-
ing on data manifolds was first proposed by (Zhou et al., 2004) and has been used for multi-document
summarization (Wan et al., 2007), image retrieval (He et al., 2004) etc. An intuitive description of the
ranking algorithm is described as follows. A weighted network is constructed first, where nodes rep-
resent all the data and query points, and an edge is put between two nodes if they are ?close?. Query
nodes are then initialized with a positive ranking score, while the nodes to be ranked are assigned a zero
initial score. All the nodes, except the sink nodes, then propagate their ranking scores to their neighbor
via the weighted network. The propagation process is repeated until a global state is achieved, and all
the nodes except the query nodes are ranked according to their final scores. Manifold ranking gives
high rank to nodes that are close to the query nodes on the manifold (relevance) and that have strong
centrality (importance). Sink nodes, whose ranking is fixed to the minimum (zero) during the ranking
process, do not spread any ranking score to their neighbors thus penalizing the nodes that are connected
to them. To use this method for extracting label-(l) specific keyphrases , f
cand
l
are modeled as query
nodes while features associated with labels other than l are modeled as sink nodes. This approach is
inspired by the work done by (Cheng et al., 2011) for query recommendation and update summarization.
Section 3.4 discusses this method in more detail. To summarize, to the best of our knowledge we are
the first to propose the problem of extracting label specific keyphrases from a multi-labeled document.
Our modifications to TextRank for achieving this task are novel. Moreover, our idea of of using Ranking
on Data Manifolds on the document-level text graphs for extracting label specific keyphrases is a new
contribution.
3 Generating Label Specific Keyphrases
3.1 Notation
In this section we introduce notations which we use throughout the paper. Let D represent a multi-label
document corpus and = be the set of all possible labels which could be associated with documents in D.
A document from this corpus is denoted by d and the set of labels associated with document d is denoted
by `, where d ?D and `?=. The text graph for document d is denoted byG
d
andM denotes the number
of vertices inG
d
. We describe how this text graph is constructed in Section 3.2. Features specific to label
l, which are extracted from the corpus D, are represented as f
cand
l
, where l ? =. Section 3.5 describes
how these label specific features are extracted from a multi-label document corpus.
3.2 Building the Text Graph
For a given document d the text graph G
d
is built in the following way. All open-class, unigram tokens
occurring in d are treated as vertices. Two vertices are connected if their corresponding lexical units
co-occur within a window of maximum N words, where N is set to 10 for all our experiments. As in-
dicated by (Mihalcea and Tarau, 2004) co-occurrence links express relations between syntactic elements
and represent cohesion indicators for a given text. Note that the methods described in Section 3.3 and
Section 3.4 provide a score/rank for each vertex (unigram term) in the graph. To generate keyphrases (n-
grams) from these candidate terms the following post-processing is performed on the top ranked terms.
Vertices are sorted in reverse order of their score and the top K vertices in the ranking are retained
2
Nodes correspond to terms in a text graph
1470
Figure 2: (a) Label specific features f
cand
l
(b) Personalized TextRank - walk biased towards terms related
to f
cand
plant_physiology
(shown in red color). (c) TextRank using Ranking on Data Manifold with Sinks: walk
biased towards terms related to f
cand
plant_physiology
, while simultaneously penalizing terms that are related
to f
cand
air_pollution
. The sink points, which are shown in black color, are vertices whose ranking scores are
fixed at the minimum score (zero in our case) during the ranking process. Hence, the sink points will
never spread any ranking score to their neighbors. Arrows indicate diffusion of ranking scores (Figure
best viewed in color)
for post-processing. Let this ranked list be represented as <T
K
>. During post-processing, all terms se-
lected as potential keywords are marked in the text, and sequence of adjacent keywords are collapsed
into a multi-word keyphrase. For example, in the text calibrated instruments are used to measure, if
the unigram terms calibrated and instruments are selected as potential/candidate terms by the PTR or
TRDMS method, since they are adjacent they are collapsed into one single keyphrase ?calibrated in-
struments?. This heuristic is implemented as a function which is referred as kphrase
gen
(<T
K
>,d). This
function takes as input the ranked term list <T
K
> and the document text d and returns the collapsed set
of keyphrases. A similar approach was adopted in the TextRank (Mihalcea and Tarau, 2004) work.
3.3 PTR: Personalized TextRank
For extracting label-l specific keyphrases from document d we modify the TextRank (Mihalcea and
Tarau, 2004) algorithm. We replace the PageRank algorithm used in the TextRank method with the Per-
sonalized Page Rank (Haveliwala, 2002) algorithm. PageRank gives a stationary distribution of a random
walk which, at each step, with a certain probability  jumps to a random node, and with probability 1-
follows a randomly chosen outgoing edge from the current node. More formally, let G
d
denotes the text
graph of document d with M vertices where d
i
denotes the out degree of node w
i
, then p = Lp + (1-)v.
Where p is the page rank vector, L is a M ?M transition probability matrix with L
ji
=
1
d
i
. In the page
rank equation v is a stochastic normalized vector whose element values are all
1
M
. This assigns equal
probabilities to all nodes in the graph in case of random jumps. In the personalized page rank formulation
the vector v can be non-uniform and can assign stronger probabilities to certain kind of nodes effectively
biasing the PageRank vector. In the PTR approach v is modeled to capture the evidence that is available
for label l in document d. Doing so biases the walk towards terms that are more specific to label l in the
document. This is achieved by considering vertices (terms) that are common between the label l feature
vector i.e. f
cand
l
and the text graph for document d i.e. G
d
. More precisely, for a label l associated with
a document d, let V
l
d
denote the intersection of the set V
d
with f
cand
l
, i.e. V
l
d
= V
d
? f
cand
l
, where V
d
denote the vertex set for the text graph G
d
3
and l ? `. In this way V
l
d
indicates the evidence we have
for label l in the text graph G
d
. To illustrate this point consider Figure 2. The label specific features for
label Plant Physiology is shown in Figure 2 (a) denoted as f
cand
plant?physiology
. The term colored in red
3
G
d
is the text graph built for document d using the method outlined in Section 3.2.
1471
indicates the term that is common between f
cand
plant?physiology
and G
d
i.e. V
plant?physiology
d
Having identified the nodes (V
l
d
) which should be allocated stronger probabilities in v the next step
is to devise a mechanism to determine these probabilities. We experiment with four approaches. In the
first approach, referred to as seed_nodes_only, we allocate all the probability mass in v uniformly to
the nodes in V
l
d
, all other nodes i.e. nodes 6? V
l
d
are assigned zero probability. In the second approach, re-
ferred to as the seed_and_eta approach, we keep aside a small fraction ? of the probability mass, which
is distributed uniformly to all the nodes 6? V
l
d
, the rest of the probability mass i.e. 1-?
4
is uniformly dis-
tributed to all nodes ? V
l
d
. The third approach, referred to as non_uniform_seed_only, is similar to the
seed_nodes_only approach except that in this case the probability mass in v is not allocated uniformly
to the nodes in V
l
d
. Probability mass is allocated to the nodes in proportion to their importance, as indi-
cated by the weights allocated to the feature in f
cand
l
by the feature selection method used. As we discuss
in Section 3.5 the feature selection methods, which are used for generating label specific feature f
cand
l
,
compute weights for individual features in f
cand
l
. These weights (e.g mutual information score, t-score)
indicate the strength of association between the feature and the label. In the non_uniform_seed_only
approach we allocate probability mass to nodes in V
l
d
in proportion to their feature weights. Finally, in
the non_uniform_eta approach we distribute the probability mass i.e. 1-? amongst the V
l
d
in propor-
tion to their feature weights. The left probability mass of ? is distributed uniformly amongst other nodes
6? V
l
d
. Performance of these different configurations are evaluated in Section 4.1.
One shortcoming of the PTR approach is that it does not provides a clean mechanism to integrate
features from labels other than l which are associated with the document d. The motivation of doing so
is to on one hand bias the walk on the text graph towards terms in f
cand
l
while simultaneously penal-
izing terms which are in F
cand
= ?
k 6=l and k?`
f
cand
k
5
. As shown in Figure 2 (b) not incorporating this
information results in a leakage of scores (indicated using arrows) to nodes not relevant to label l (e.g.
gases, sulphur etc) . In the next section we describe the TRDMS or TextRank using Ranking on Data
Manifold with Sinks approach which allows us to simultaneously consider both f
cand
l
and F
cand
in the
same model.
3.4 TRDMS: TextRank using Ranking on Data Manifold with Sinks
Algorithm 1: Algorithm for generating label-l specific keyphrases for document d
Data: Document d, label-l specific unigram features f
cand
l
, unigram features for label categories other than l
represented as F
cand
= ?
k 6=land k?`
f
cand
k
Result: label-l specific keyphrases from document d
1. Build a Text Graph G
d
for document d as discussed in Section 3.2. Let w
i
indicate the vertices in G
d
;
2. Construct an affinity matrix A, where A
ij
= sim(w
i
,w
j
) if there is an edge linking w
i
, w
j
in G
d
. sim(w
i
,w
j
)
indicates similarity between vertices w
i
, w
j
;
3. Symmetrically, normalize A as S = D
?1/2
AD
?1/2
. D is a diagonal matrix matrix with its (i,i)-element equal
to the sum of the i-th row A;
4. while (!converge(p)) do
Iterate p(t+ 1) = ?SIp(t) + (1-?)y ;
/* where 0 <? <1 and I is an indicator diagonal matrix with it?s (i,i)-element equal to 0 if w
i
? V
?l
d
and 1
otherwise.*/
end
5. Sort the vertices w
q
? V
q
in descending order of their scores p[q]. Let this ranked list be represented as
<T
K
>;
6. kphrase
l
d
= kphrase
gen
(<T
K
>,d) , where kphrase
l
d
is the label-l specific keyphrase list for document d;
7. return kphrase
l
d
;
In this section we describe the TextRank using Ranking on Data Manifold with Sinks approach that
allows us to simultaneously consider both f
cand
l
and F
cand
when extracting label l specific keyphrases
from document?s d text graph. For ease of exposition we repeat a few notations and introduce some new
ones. Let V
d
denote the vertex set for the text graph G
d
. Vertices for the text graph G
d
are represented
by w
i
where i ? [1..M], M is the number of vertices i.e. M=|V
d
|. As introduce earlier, V
l
d
denotes the
4
Please note v is a stochastic normalized vector whose elements sum to 1. In our experiments we set ?=0.2
5
Where ` indicates the label set associated with document d
1472
intersection of the set V
d
with f
cand
l
, i.e. V
l
d
= V
d
? f
cand
l
. V
l
d
indicates the evidence we have for label l
in the text graphG
d
, where l ? `. These vertices are also referred to as query nodes in the ranking on data
manifold literature. Let V
?l
d
denote the intersection of the set V
d
with F
cand
, where F
cand
= ?
k 6=l and k?`
f
cand
k
i.e. all the unigram features associated with label categories other than l
6
. These vertices are also
referred to as sink nodes in the ranking on data manifold literature. All other vertices are indicated by
V
q
d
, where V
q
d
= V
d
\ (V
?l
d
? V
l
d
) denote the set of points to be ranked. Let p:V ?< denote the ranking
function which assigns a ranking score p
i
to each vertex w
i
in G
d
. One can view p as a vector i.e. p =
[p
1
,....,p
M
]. A binary vector y = [y
1
,....,y
M
] is defined in which y
i
= 1 if w
i
? V
l
d
otherwise y
i
= 0.
Algorithm 1 gives a detailed outline of the TRDMS method. This algorithm is based on the algorithm
proposed by (Cheng et al., 2011) for ranking on data manifold with sink points. To generate label-l
specific keyphrase for document d the algorithm considers document d, label-l specific unigram features
f
cand
l
, and unigram features for labels other than l represented as F
cand
. It begins by first building a text
graph G
d
. After this an affinity matrix A is constructed. This is shown in Step 2. The affinity matrix
A, which captures the similarity between vertices (terms in the text graph) w
i
and w
j
, is built using
WordNet. We use the popular WordNet::Similarity (Pedersen et al., 2004) package which measures the
semantic similarity and relatedness between a pairs of concepts. After symmetrically normalizing A
(Step 3) and initializing the query and sink nodes the scores are propagated till convergence (Step 4).
The routine converge(p) checks for convergence by comparing the value of p between two consecutive
iterations. If there is little or no change in p the routine return true. To generate n-gram keyphrases
we follow the approach described in Section 3.2. In Step 6 of Algorithm 1 the kphrase
gen
7
routine is
invoked. In order to choose top-k, label-l specific keyphrases for document d one can select the first k
elements of the kphrase
l
d
list.
3.5 Generating label specific features from a multi-label corpus
As discussed in previous sections the label specific features f
cand
l
play an important role in the overall
ranking process. When searching for label-l specific keyphrases, the unigram features f
cand
l
helps bias
the walk on the document?s text graph towards terms that are relevant and central to label l. We also
saw that by considering F
cand
i.e. unigram features belonging to label categories other than l
8
as sink
nodes prevents leakage of the ranking score to terms not relevant or central to l. We show through
experiments in Section 4 that this improves the quality of label-l specific keyphrases extracted from
document d. In order to generate label specific features from a multi-label corpus D we adopt the
problem transformation approach commonly used in multi-label learning. In this approach the multi-
label corpus D is transformed into | = | single-label data sets, where = is the set of labels associated
with corpus D. Post this transformation any single-label feature selection method can be used to extract
label l specific features from these single-label data sets. For our setup we experiment with unigram
features selected using mutual information and chi-squared based feature selection methods.
4 Experiment
In order to assess the quality of the label-specific keyphrases generated by our system we conduct a
manual evaluation of the generated output. Details of this evaluation are provided in Section 4.1. For
our experiments we use a subset of the multi-label corpus EUR-Lex
9
. The EUR-Lex text collection is
a collection of documents about European Union law. It contains many different types of documents,
including treaties, legislation, case-law and legislative proposals, which are labeled with EUROVOC
descriptors. A document in this data-set could be associated with multiple EUROVOC descriptors
10
. The
data set that was downloaded contained 16k documents documents and 3,993 EUROVOC descriptors.
6
We do not assume that f
cand
l
? F
cand
= ?
7
Details of this routine are provided in Section 3.2
8
In cases where the document is associated with more than one label or category
9
http://www.ke.tu-darmstadt.de/resources/eurlex
10
We treat these as labels
1473
Method Precision
avg
Recall
avg
F-measure
avg
TPP
baseline
0.163 0.194 0.177
PTR
seed_nodes_only
0.169 0.213 0.188
PTR
seed_and_eta
0.199 0.223 0.210
PTR
non_uniform_seed_only
0.203 0.231 0.216
PTR
non_uniform_eta
0.237 0.257 0.247
TRDMS 0.397 0.387 0.392
Table 1: Keyphrase Extraction Results
We removed labels that were under represented
11
in this data set. We refer to this data set as the EUR?
Lex
filtered
data set. We randomly selected 100 documents from the EUR ? Lex
filtered
data set. Two
criteria were considered when selecting these documents (a) Each document should be associated with
at least 2 but not more than 3 labels (b) The size of the evidence set i.e. |V
l
d
| where V
l
d
= V
d
? f
l
cand
is
at least 10% of |V
d
|, where V
d
represents the vertex set of the text graph associated with d. The resulting
data set is referred to as the EUR ? Lex
keyphrase
filtered
data set. The reason for enforcing these two criteria
is the following. Ensuring that a document in EUR ? Lex
keyphrase
filtered
has at least 2 labels allows us to
experiment with sink nodes i.e. F
cand
. As we discuss in Section 4.1 for each label associated with a
document, a human evaluator was asked to generates a label specific list of keyphrases. For example,
if a document is associated with 3 labels, three label specific keyphrase list had to be generated by the
human evaluator. Allowing documents with more than 3 labels makes this process tedious. The reason
for putting restriction (b) when building the EUR ? Lex
keyphrase
filtered
is explained in Section 4.1.1. For
generating label-l specific features we use the approach described in Section 3.5. For our experiments
mutual information based feature selection method was used with a feature size of 250 i.e. |f
cand
l
| =
250.
4.1 Label-specific Keyphrase Evaluation
Two graduate students were asked to manually extract label-specific keyphrases for each document in the
EUR ? Lex
keyphrase
filtered
data set. At most 10 keyphrases could be assigned to each document-label pair.
This results in a total of 1721 keyphrases. The Kappa statistics for measuring inter-agreement among the
annotation was 0.81. Any annotation conflicts between the two subjects was resolved by a third graduate
student. For evaluation, the automatically extracted label-specific keyphrases for a given document were
compared with the manually extracted/annotated keyphrases. Before comparing the keyphrase, the words
in the keyphrase were converted to their corresponding base form using word stemming. We calculate
three evaluation metrics namely Precision, Recall and F-measure for each document-label pair. Precision
(P) =
count
correct
count
system
, Recall (R) =
count
correct
count
human
and F-measure (F) =
2PR
P+R
, where count
correct
is the total
number of correct keyphrases extracted by our method, count
system
is the total number of automatically
extracted keyphrases and count
human
is the total number of keyphrases labeled by the human annotators.
These metrics are calculated for each document-label pair in the EUR?Lex
keyphrase
filtered
data set and then
averaged to obtain Precision
avg
, Recall
avg
and F ?measure
avg
. These results are shown in Table 1
We compare the performance of our system against the TextRank with Post-Processing: TPP
baseline
baseline which was explained in Section 2. Briefly, in this setup to identify label-l specific keyphrases in
document d, we run TextRank on document d and filter the generated keyphrase list based on f
cand
l
i.e.
label l specific features. In all setups the document text graph is built in the same fashion i.e. N = 10
and co-occurrence relationship is used to draw edges between nodes in the text graph. For generating the
affinity matrix A, which is used in the TRDMS method, the res semantic similarity method is used
12
. To
reiterate, when generating label-l specific keyphrases for document d the PTR method only uses f
cand
l
,
whereas the TRDMS method uses both f
cand
l
(as query nodes) and F
cand
= ?
k 6=l and k?`
f
cand
k
(where
11
Any label which occurred less than 10% times in the data set was removed. The documents associated with these labels
were also removed from the data set
12
We experimented with other semantic similarity measures such as lin and jcn. The res measure gave us the best results
1474
` is the set of labels associated with document d) i.e. all the unigram features associated with label cat-
egories other than l (as sink nodes). One can observe from Table 1 that for PTR the non_uniform_eta
configuration gives the best result. Overall the TRDMS approach significantly outperforms all PTR con-
figurations and our baseline. This validates our belief that one can significantly improve the quality of
extracted keyphrase by not only considering label-l specific features i.e. f
cand
l
but also features associ-
ated with label categories other than l. When we analyzed the performance of TRDMS at the document
level we observed that the keyphrase extraction metrics for documents which had strongly correlated la-
bels e.g. ?tariff_quota? and ?import_license? was 9-11% lower than the reported average scores. On the
contrary, keyphrase extraction metrics for documents which had labels that had no or weak correlation
e.g. ?aid_contract? and ?import_license? was 3-5% higher than the reported average scores. One reason
for this could be the substantial overlap between f
cand
l
and F
cand
for highly correlated labels. This large
overlap results in the query nodes being considered as sink nodes which negatively impacts the score
propagation in the underlying text graph.
Figure 3: Impact of evidence set size on F-measure ( best viewed in color)
4.1.1 Impact of evidence set size (|V
l
d
|) on keyphrase generation results
To recap, elements in set V
l
d
indicate the evidence we have for label l in the text graph of document d
i.e. G
d
. In order to investigate how the size of the evidence set i.e. |V
l
d
| impacts the performance of our
system the following simulation was carried out. In different setups we randomly drop out elements from
V
l
d
so that the size of the resulting evidence set ranges from 2% to 10% of |V
d
|, where |V
d
| represents the
vertex set size of text graph G
d
. We plot the impact this has on the F-measure in Figure 3. One observes
that when the evidence set size is in the range 2-4% the gains over the TPP
baseline
baseline (0.177) are
low to modest. As the evidence set size increases the gains over the baseline increases substantially.
5 Conclusion and Future Work
In this paper we presented the problem of extracting label specific keyphrases from a document. We
pose the problem of extracting such keyphrases from a document as a random walk on a document?s
text graph. The methods proposed in this paper utilizes the label specific features, which are strongly
associated with the label, to bias the walk towards terms that are more relevant to the label. We show
through experiments that when generating label-l specific keyphrases it helps to consider both label-l
specific features and features associated with labels other than l. As future work we would like to further
assess the quality of the generated keyphrases by using these keyphrases for generating topic (or label)
focused document summaries.
1475
References
Takashi Tomokiyo and Matthew Hurst. 2003. A language model approach to keyphrase extraction. Proceedings
of the ACL 2003 workshop on Multiword expressions: analysis, acquisition and treatment.
A. Hulth. 2003. Improved Automatic Keyword Extraction Given More Linguistic Knowledge. Proceedings of the
Conference on Empirical Methods in Natural Language Processing.
Yutaka Matsuo and Mitsuru Ishizuka. 2004. Keyword Extraction from a Single Document using Word Co-
occurrence Statistical Information. International Journal on Artificial Intelligence Tools.
Peter D. Turney. 2000. Learning Algorithms for Keyphrase Extraction. Information Retrieval.
Eibe Frank and W. Gordon Paynter and Ian H. Witten and Carl Gutwin and Craig G. Nevill-Manning. 1999.
Domain-Specific Keyphrase Extraction. IJCAI ?99: Proceedings of the Sixteenth International Joint Conference
on Artificial Intelligence.
Peter D. Turney. 2003. Coherent Keyphrase Extraction via Web Mining. IJCAI ?03: Proceedings of the Sixteenth
International Joint Conference on Artificial Intelligence.
Min Song and Il-Yeol Song and Xiaohua Hu. 2003. KPSpotter: a flexible information gain-based keyphrase
extraction system. Fifth International Workshop on Web Information and Data Management.
Olena Medelyan and Ian H. Witten. 2006. Thesaurus based automatic keyphrase indexing. JCDL ?06: Proceed-
ings of the 6th ACM/IEEE-CS joint conference on Digital libraries.
George Forman. 2003. An Extensive Empirical Study of Feature Selection Metrics for Text Classification. The
Journal of Machine Learning Research.
George Forman. 2004. A pitfall and solution in multi-class feature selection for text classification. International
Conference on Machine Learning.
Rada Mihalcea and Paul Tarau. 2004. TextRank: Bringing Order into Texts. Proceedings of EMNLP-04 and the
2004 Conference on Empirical Methods in Natural Language Processing.
Rada Mihalcea, Paul Tarau and Elizabeth Figa. 2004. PageRank on Semantic Networks, with Application to Word
Sense Disambiguation. COLING.
Rada Mihalcea, Paul Tarau and Elizabeth Figa. 2004. PageRank on Semantic Networks, with Application to Word
Sense Disambiguation. COLING.
Dengyong Zhou and Jason Weston and Arthur Gretton and Olivier Bousquet and Bernhard Sch?u?lkopf. 2004.
Ranking on Data Manifolds. Advances in Neural Information Processing Systems.
Ted Pedersen and Siddharth Patwardhan and Jason Michelizzi. 2004. WordNet::Similarity: Measuring the Relat-
edness of Concepts. Association for Computational Linguistics.
Rada Mihalcea and Paul Tarau. 2005. A language independent algorithm for single and multiple document
summarization. In Proceedings of IJCNLP?
?
A
?
Z2005.
Xiaojun Wan. 2007. TimedTextRank: adding the temporal dimension to multi-document summarization. SIGIR.
Taher H. Haveliwala. 2002. Topic-sensitive PageRank. Proceedings of the Eleventh International World Wide
Web Conference.
Xue-Qi Cheng and Pan Du and Jiafeng Guo and Xiaofei Zhu and Yixin Chen. 2011. Ranking on Data Manifold
with Sink Points. IEEE Transactions on Knowledge and Data Engineering.
Jingrui He and Mingjing Li and Hong-Jiang Zhang and Hanghang Tong and Changshu Zhang. 2004. Manifold-
ranking Based Image Retrieval. Proceedings of the 12th Annual ACM International Conference on Multimedia.
Xiaojun Wan and Jianwu Yang and Jianguo Xiao. 2007. Manifold-Ranking Based Topic-Focused Multi-Document
Summarization. IJCAI.
1476
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 87?96,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Handling Noisy Queries In Cross Language FAQ Retrieval
Danish Contractor Govind Kothari Tanveer A. Faruquie
L. Venkata Subramaniam Sumit Negi
IBM Research India
Vasant Kunj, Institutional Area
New Delhi, India
{dcontrac,govkotha,ftanveer,lvsubram,sumitneg}@in.ibm.com
Abstract
Recent times have seen a tremendous growth
in mobile based data services that allow peo-
ple to use Short Message Service (SMS) to
access these data services. In a multilin-
gual society it is essential that data services
that were developed for a specific language
be made accessible through other local lan-
guages also. In this paper, we present a ser-
vice that allows a user to query a Frequently-
Asked-Questions (FAQ) database built in a lo-
cal language (Hindi) using Noisy SMS En-
glish queries. The inherent noise in the SMS
queries, along with the language mismatch
makes this a challenging problem. We handle
these two problems by formulating the query
similarity over FAQ questions as a combina-
torial search problem where the search space
consists of combinations of dictionary varia-
tions of the noisy query and its top-N transla-
tions. We demonstrate the effectiveness of our
approach on a real-life dataset.
1 Introduction
There has been a tremendous growth in the number
of new mobile subscribers in the recent past. Most
of these new subscribers are from developing coun-
tries where mobile is the primary information de-
vice. Even for users familiar with computers and the
internet, the mobile provides unmatched portability.
This has encouraged the proliferation of informa-
tion services built around SMS technology. Several
applications, traditionally available on Internet, are
now being made available on mobile devices using
SMS. Examples include SMS short code services.
Short codes are numbers where a short message in
a predesignated format can be sent to get specific
information. For example, to get the closing stock
price of a particular share, the user has to send a
message IBMSTOCKPR. Other examples are search
(Schusteritsch et al, 2005), access to Yellow Page
services (Kopparapu et al, 2007), Email 1, Blog 2 ,
FAQ retrieval 3 etc. The SMS-based FAQ retrieval
services use human experts to answer SMS ques-
tions.
Recent studies have shown that instant messag-
ing is emerging as the preferred mode of commu-
nication after speech and email.4 Millions of users
of instant messaging (IM) services and short mes-
sage service (SMS) generate electronic content in a
dialect that does not adhere to conventional gram-
mar, punctuation and spelling standards. Words are
intentionally compressed by non-standard spellings,
abbreviations and phonetic transliteration are used.
Typical question answering systems are built for use
with languages which are free from such errors. It
is difficult to build an automated question answer-
ing system around SMS technology. This is true
even for questions whose answers are well docu-
mented like in a Frequently-Asked-Questions (FAQ)
database. Unlike other automatic question answer-
ing systems that focus on searching answers from
a given text collection, Q&A archive (Xue et al,
2008) or the Web (Jijkoun et al, 2005), in a FAQ
database the questions and answers are already pro-
1http://www.sms2email.com/
2http://www.letmeparty.com/
3http://www.chacha.com/
4http://www.whyconverge.com/
87
Figure 1: Sample SMS queries with Hindi FAQs
vided by an expert. The main task is then to iden-
tify the best matching question to retrieve the rel-
evant answer (Sneiders, 1999) (Song et al, 2007).
The high level of noise in SMS queries makes this a
difficult problem (Kothari et al, 2009). In a multi-
lingual setting this problem is even more formidable.
Natural language FAQ services built for users in one
language cannot be accessed in another language.
In this paper we present a FAQ-based question an-
swering system over a SMS interface that solves this
problem for two languages. We allow the FAQ to be
in one language and the SMS query to be in another.
Multi-lingual question answering and information
retrieval has been studied in the past (Sekine and
Grishman, 2003)(Cimiano et al, 2009). Such sys-
tems resort to machine translation so that the search
can be performed over a single language space. In
the two language setting, it involves building a ma-
chine translation system engine and using it such
that the question answering system built for a sin-
gle language can be used.
Typical statistical machine translation systems
use large parallel corpora to learn the translation
probabilities (Brown et al, 2007). Traditionally
such corpora have consisted of news articles and
other well written articles. Since the translation sys-
tems are not trained on SMS language they perform
very poorly when translating noisy SMS language.
Parallel corpora comprising noisy sentences in one
language and clean sentences in another language
are not available and it would be hard to build such
large parallel corpora to train a machine translation
system. There exists some work to remove noise
from SMS (Choudhury et al, 2007) (Byun et al,
2008) (Aw et al, 2006) (Neef et al, 2007) (Kobus
et al, 2008). However, all of these techniques re-
quire an aligned corpus of SMS and conventional
language for training. Such data is extremely hard
to create. Unsupervised techniques require huge
amounts of SMS data to learn mappings of non-
standard words to their corresponding conventional
form (Acharyya et al, 2009).
Removal of noise from SMS without the use of
parallel data has been studied but the methods used
are highly dependent on the language model and the
degree of noise present in the SMS (Contractor et
al., 2010). These systems are not very effective if
the SMSes contain grammatical errors (or the sys-
tem would require large amounts of training data in
the language model to be able to deal with all pos-
sible types of noise) in addition to misspellings etc.
Thus, the translation of a cleaned SMS, into a second
language, will not be very accurate and it would not
give good results if such a translated SMS is used to
query an FAQ collection.
Token based noise-correction techniques (such as
those using edit-distance, LCS etc) cannot be di-
rectly applied to handle the noise present in the SMS
query. These noise-correction methods return a list
of candidate terms for a given noisy token (E.g.
?gud? ? > ?god?,?good?,?guide? ) . Considering all
these candidate terms and their corresponding trans-
lations drastically increase the search space for any
multi-lingual IR system. Also , naively replacing the
noisy token in the SMS query with the top matching
candidate term gives poor performance as shown by
our experiments. Our algorithm handles these and
related issues in an efficient manner.
In this paper we address the challenges arising
when building a cross language FAQ-based ques-
tion answering system over an SMS interface. Our
method handles noisy representation of questions in
a source language to retrieve answers across target
languages. The proposed method does not require
hand corrected data or an aligned corpus for explicit
SMS normalization to mitigate the effects of noise.
It also works well with grammatical noise. To the
best of our knowledge we are the first to address
issues in noisy SMS based cross-language FAQ re-
trieval. We propose an efficient algorithm that can
handle noise in the form of lexical and semantic cor-
ruptions in the source language.
2 Problem formulation
Consider an input SMS Se in a source language
e. We view Se as a sequence of n tokens Se =
s1, s2, . . . , sn. As explained in the introduction, the
input is bound to have misspellings and other lexical
and semantic distortions. Also let Qh denote the set
88
of questions in the FAQ corpus of a target language
h. Each question Qh ? Qh is also viewed as a se-
quence of tokens. We want to find the question Q?h
from the corpus Qh that best matches the SMS Se.
The matching is assisted by a source dictionary
De consisting of clean terms in e constructed from
a general English dictionary and a domain dictio-
nary of target language Dh built from all the terms
appearing in Qh. For a token si in the SMS in-
put, term te in dictionary De and term th in dictio-
nary Dh we define a cross-lingual similarity mea-
sure ?(th, te, si) that measures the extent to which
term si matches th using the clean term te. We con-
sider th a cross lingual variant of si if for any te the
cross language similarity measure ?(th, te, si) > .
We denote this as th ? si.
We define a weight function ?(th, te, si) using the
cross lingual similarity measure and the inverse doc-
ument frequency (idf) of th in the target language
FAQ corpus. We also define a scoring function to as-
sign a score to each question in the corpusQh using
the weight function. Consider a question Qh ? Qh.
For each token si, the scoring function chooses the
term from Qh having the maximum weight using
possible clean representations of si; then the weight
of the n chosen terms are summed up to get the
score. The score measures how closely the question
in FAQ matches the noisy SMS string Se using the
composite weights of individual tokens.
Score(Qh) =
n?
i=1
max
th?Qh,te?De & th?si
?(th, te, si)
Our goal is to efficiently find the question Q?h having
the maximum score.
3 Noise removal from queries
In order to process the noisy SMS input we first have
to map noisy tokens in Se to the possible correct lex-
ical representations. We use a similarity measure to
map the noisy tokens to their clean lexical represen-
tations.
3.1 Similarity Measure
For a term te ? De and token si of the SMS input
Se, the similarity measure ?(te, si) between them is
?(te, si) =
?
???????
???????
LCSRatio(te,si)
EditDistanceSMS(te,si)
if te and si share
same starting
character *
0 otherwise
(1)
Where LCSRatio(te, si) =
length(LCS(te,si))
length(te)
and LCS(te, si)
is the Longest common subsequence between te and si.
* The intuition behind this measure is that people typically type the
first few characters of a word in an SMS correctly. This way we limit
the possible variants for a particular noisy token
The Longest Common Subsequence Ratio (LC-
SRatio) (Melamed et al, 1999) of two strings is the
ratio of the length of their LCS and the length of the
longer string. Since in the SMS scenario, the dictio-
nary term will always be longer than the SMS token,
the denominator of LCSRatio is taken as the length
of the dictionary term.
The EditDistanceSMS (Figure 2) compares the
Consonant Skeletons (Prochasson et al, 2007) of the
dictionary term and the SMS token. If the Leven-
shtein distance between consonant skeletons is small
then ?(te, si) will be high. The intuition behind us-
ing EditDistanceSMS can be explained through
an example. Consider an SMS token ?gud? whose
most likely correct form is ?good?. The longest
common subsequence for ?good? and ?guided? with
?gud? is ?gd?. Hence the two dictionary terms
?good? and ?guided? have the same LCSRatio of 0.5
w.r.t ?gud?, but the EditDistanceSMS of ?good?
is 1 which is less than that of ?guided?, which has
EditDistanceSMS of 2 w.r.t ?gud?. As a result the
similarity measure between ?gud? and ?good? will
be higher than that of ?gud? and ?guided?. Higher
the LCSRatio and lower the EditDistanceSMS ,
higher will be the similarity measure. Hence, for
a given SMS token ?byk?, the similarity measure of
word ?bike? is higher than that of ?break?.
4 Cross lingual similarity
Once we have potential candidates which are the
likely disambiguated representations of the noisy
89
Procedure EditDistanceSMS(te, si)
Begin
return LevenshteinDistance(CS(si), CS(te)) + 1
End
Procedure CS (t): // Consonant Skeleton Generation
Begin
Step 1. remove consecutive repeated characters in t
// (fall? fal)
Step 2. remove all vowels in t
//(painting ? pntng, threat? thrt)
return t
End
Figure 2: EditDistanceSMS
term, we map these candidates to appropriate terms
in the target language. We use a statistical dictionary
to achieve this cross lingual mapping.
4.1 Statistical Dictionary
In order to build a statistical dictionary we use
the statistical translation model proposed in (Brown
et al, 2007). Under IBM model 2 the transla-
tion probability of source language sentence e? =
{t1e, . . . , t
j
e, . . . , t
m
e } and a target language sentence
h? = {t1h, . . . , t
i
h, . . . , t
l
e} is given by
Pr(h?|e?) = ?(l|m)
l?
i=1
m?
j=0
?(tih|t
j
e)a(j|i,m, l).
(2)
Here the word translation model ?(th|te) gives the
probability of translating the source term to target
term and the alignment model a(j|i,m, l) gives the
probability of translating the source term at position
i to a target position j. This model is learnt using an
aligned parallel corpus.
Given a clean term tie in source language we get
all the corresponding terms T = {t1h, . . . , t
k
h, . . .}
from the target language such that word translation
probability ?(tkh|t
i
e) > ?. We rank these terms ac-
cording to the probability given by the word trans-
lation model ?(th|te) and consider only those tar-
get terms that are part of domain dictionary i.e.
tkh ? D
h.
4.2 Cross lingual similarity measure
For each term si in SMS input query, we find all
the clean terms te in source dictionary De for which
similarity measure ?(te, si) > ?. For each of these
term te, we find the cross lingual similar terms Tte
using the word translation model. We compute the
cross lingual similarity measure between these terms
as
?(si, te, th) = ?(te, si).?(th, te) (3)
The measure selects those terms in target lan-
guage that have high probability of being translated
from a noisy term through one or more valid clean
terms.
4.3 Cross lingual similarity weight
We combine the idf and the cross lingual similarity
measure to define the cross lingual weight function
?(th, te, si) as
?(th, te, si) = ?(th, te, si).idf(th) (4)
By using idf we give preference to terms that are
highly discriminative. This is necessary because
queries are distinguished from each other using in-
formative words. For example for a given noisy
token ?bck? if a word translation model produces
a translation output ?wapas? (as in came back) or
?peet? or ?qamar? (as in back pain) then idf will
weigh ?peet? more as it is relatively more discrim-
inative compared to ?wapas? which is used fre-
quently.
5 Pruning and matching
In this section we describe our search algorithm and
the preprocessing needed to find the best question
Q?h for a given SMS query.
5.1 Indexing
Our algorithm operates at a token level and its corre-
sponding cross lingual variants. It is therefore nec-
essary to be able to retrieve all questions Qhth that
contain a given target language term th. To do this
efficiently we index the questions in FAQ corpus us-
ing Lucene5. Each question in FAQ is treated as a
document. It is tokenized using whitespace as de-
limiter before indexing.
5http://lucene.apache.org/java/docs/
90
The cross lingual similarity weight calculation re-
quires the idf for a given term th. We query on this
index to determine the number of documents f that
contain th. The idf of each term in Dh is precom-
puted and stored in a hashtable with th as the key.
The cross lingual similarity measure calculation re-
quires the word translation probability for a given
term te. For every te in dictionary De, we store
Tte in a hashmap that contains a list of terms in the
target language along with their statistically deter-
mined translation probability ?(th|te) > ?, where
th ? Dh.
Since the query and the FAQs use terms from dif-
ferent languages, the computation of IDF becomes a
challenge (Pirkola, 1998) (Oard et al, 2007). Prior
work uses a bilingual dictionary for translations for
calculating the IDF. We on the other hand rely on
a statistical dictionary that has translation probabil-
ities. Applying the method suggested in the prior
work on a statistical dictionary leads to errors as the
translations may themselves be inaccurate.
We therefore calculate IDFs for target language
term (translation) and use it in the weight measure
calculation. The method suggested by Oard et al
(Oard et al, 2007) is more useful in retrieval tasks
for multiple documents, while in our case we need
to retrieve a specific document (FAQ).
5.2 List Creation
Given an SMS input string Se, we tokenize it on
white space and replace any occurrence of digits to
their string based form (e.g. 4get, 2day) to get a se-
ries of n tokens s1, s2, . . . , sn. A list Lei is created
for each token si using terms in the monolingual dic-
tionary De. The list for a single character SMS to-
ken is set to null as it is most likely to be a stop word.
A term te from De is included in Lei if it satisfies the
threshold condition
?(te, si) > ? (5)
The threshold value ? is determined experimen-
tally. For every te ? Lei we retrieve Tte and then
retrieve the idf scores for every th ? Tte . Using the
word translation probabilities and the idf score we
compute the cross lingual similarity weight to create
a new list Lhi . A term th is included in the list only
if
?(th|te) > 0.1 (6)
This probability cut-off is used to prevent poor
quality translations from being included in the list.
If more than one term te has the same transla-
tion th, then th can occur more than once in a given
list. If this happens, then we remove repetitive oc-
currences of th and assign it a weight equal to the
maximum weight amongst all occurrences in the list,
multiplied by the number of times it occurs. The
terms th in Lhi are sorted in decreasing order of their
similarity weights. Henceforth, the term ?list? im-
plies a sorted list.
For example given a SMS query ?hw mch ds it cst
to stdy in india? as shown in Fig. 3, for each token
we create a list of possible correct dictionary words
by dictionary look up. Thus for token ?cst? we get
dictionary words lik ?cost, cast, case, close?. For
each dictionary word we get a set of possible words
in Hindi by looking at statistical translation table.
Finally we merged the list obtained to get single list
of Hindi words. The final list is ranked according to
their similarity weights.
5.3 Search algorithm
Given Se containing n tokens, we create n sorted
lists Lh1 , L
h
2 , . . . , L
h
n containing terms from the do-
main dictionary and sorted according to their cross
lingual weights as explained in the previous section.
A naive approach would be to query the index using
each term appearing in all Lhi to build a Collection
set C of questions. The best matching question Q?h
will be contained in this collection. We compute the
score of each question in C using Score(Q) and the
question with highest score is treated as Q?h. How-
ever the naive approach suffers from high runtime
cost.
Inspired by the Threshold Algorithm (Fagin et
al., 2001) we propose using a pruning algorithm
that maintains a much smaller candidate set C of
questions that can potentially contain the maximum
scoring question. The algorithm is shown in Fig-
ure 4. The algorithm works in an iterative manner.
In each iteration, it picks the term that has maxi-
mum weight among all the terms appearing in the
lists Lh1 , L
h
2 , . . . , L
h
n. As the lists are sorted in the
descending order of the weights, this amounts to
picking the maximum weight term amongst the first
terms of the n lists. The chosen term th is queried to
find the set Qth . The set Qth is added to the candi-
91
Figure 3: List creation
date set C. For each question Q ? Qth , we compute
its score Score(Q) and keep it along with Q. After
this the chosen term th is removed from the list and
the next iteration is carried out. We stop the iterative
process when a thresholding condition is met and fo-
cus only on the questions in the candidate set C. The
thresholding condition guarantees that the candidate
set C contains the maximum scoring question Q?h.
Next we develop this thresholding condition.
Let us consider the end of an iteration. Sup-
pose Q is a question not included in C. At
best, Q will include the current top-most tokens
Lh1 [1], L
h
2 [1], . . . , L
h
n[1] from every list. Thus, the
upper bound UB on the score of Q is
Score(Q) ?
n?
i=0
?(Lhi [1]).
Let Q? be the question in C having the maximum
score. Notice that if Q? ? UB, then it is guaranteed
that any question not included in the candidate set C
cannot be the maximum scoring question. Thus, the
condition ?Q? ? UB? serves as the termination cri-
terion. At the end of each iteration, we check if the
termination condition is satisfied and if so, we can
stop the iterative process. Then, we simply pick the
question in C having the maximum score and return
it.
Procedure Search Algorithm
Input: SMS S = s1, s2, . . . , sn
Output: Maximum scoring question Q?h.
Begin
?si, construct Lei for which ?(si, te) > 
// Li lists variants of si
Construct lists Lh1 , L
h
2 , . . . , L
h
n //(see Section 5.2).
// Lhi lists cross lingual variants of si in decreasing
//order of weight.
Candidate list C = ?.
repeat
j? = argmaxi?(L
h
i [1])
t?h = L
h
j? [1]
// t?h is the term having maximum weight among
// all terms appearing in the n lists.
Delete t?h from the list L
h
j? .
Retrieve Qt?h using the index
// Qt?h : the set of all questions in Q
h
//having the term t?h
For each Q ? Qt?h
Compute Score(Q) and
add Q with its score into C
UB =
?n
i=1 ?(L
h
i [1])
Q? = argmaxQ?CScore(Q).
if Score(Q?) ? UB, then
// Termination condition satisfied
Output Q? and exit.
forever
End
Figure 4: Search Algorithm with Pruning
6 Experiments
To evaluate our system we used noisy English SMS
queries to query a collection of 10, 000 Hindi FAQs.
These FAQs were collected from websites of vari-
ous government organizations and other online re-
sources. These FAQs are related to railway reser-
vation, railway enquiry, passport application and
health related issues. For our experiments we asked
6 human evaluators, proficient in both English and
Hindi, to create English SMS queries based on the
general topics that our FAQ collection dealt with.
We found 60 SMS queries created by the evaluators,
had answers in our FAQ collection and we desig-
nated these as the in-domain queries. To measure
the effectiveness of our system in handling out of
domain queries we used a total of 380 SMSes part of
which were taken from the NUS corpus (How et al,
92
whch metro statn z nr pragati maidan ?
dus metro goes frm airpot 2 new delhi rlway statn?
is dere any special metro pas 4 delhi uni students?
whn is d last train of delhi metro?
whr r d auto stands N delhi?
Figure 5: Sample SMS queries
2005) and the rest from the ?out-of-domain? queries
created by the human evaluators. Thus the total SMS
query data size was 440. Fig 5 shows some of the
sample queries.
Our objective was to retrieve the correct Hindi
FAQ response given a noisy English SMS query. A
given English SMS query was matched against the
list of indexed FAQs and the best matching FAQ was
returned by the Pruning Algorithm described in Sec-
tion 5. A score of 1 was assigned if the retrieved
answer was indeed the response to the posed SMS
query else we assigned a score of 0. In case of out
of domain queries a score of 1 was assigned if the
output was NULL else we assigned a score of 0.
6.1 Translation System
We used the Moses toolkit (Koehn et al, 2007) to
build an English-Hindi statistical machine transla-
tion system. The system was trained on a collec-
tion of 150, 000 English and Hindi parallel sentences
sourced from a publishing house. The 150, 000 sen-
tences were on a varied range of subjects such as
news, literature, history etc. Apart from this the
training data also contained an aligned parallel cor-
pus of English and Hindi FAQs. The FAQs were
collected from government websites on topics such
as health, education, travel services etc.
Since an MT system trained solely on a collection
of sentences would not be very accurate in translat-
ing questions, we trained the system on an English-
Hindi parallel question corpus. As it was difficult
to find a large collection of parallel text consisting
of questions, we created a small collection of par-
allel questions using 240 FAQs and multiplied them
to create a parallel corpus of 50, 000 sentences. This
set was added to the training data and this helped fa-
miliarize the language model and phrase tables used
by the MT systems to questions. Thus in total the
MT system was trained on a corpus of 200, 000 sen-
tences.
Experiment 1 and 2 form the baseline against
which we evaluated our system. For our experi-
ments the lexical translation probabilities generated
by Moses toolkit were used to build the word trans-
lation model. In Experiment 1 the threshold ? de-
scribed in Equation 5 is set to 1. In Experiment 2
and 3 this is set to 0.5. The Hindi FAQ collection
was indexed using Lucene and a domain dictionary
Dh was created from the Hindi words in the FAQ
collection.
6.2 System Evaluation
We perform three sets of experiments to show how
each stage of the algorithm contributes in improving
the overall results.
6.2.1 Experiment 1
For Experiment 1 the threshold ? in Equation 5
is set to 1 i.e. we consider only those tokens in the
query which belong to the dictionary. This setup il-
lustrates the case when no noise handling is done.
The results are reported in Figure 6.
6.2.2 Experiment 2
For Experiment 2 the noisy SMS query was
cleaned using the following approach. Given a noisy
token in the SMS query it?s similarity (Equation 1)
with each word in the Dictionary is calculated. The
noisy token is replaced with the Dictionary word
with the maximum similarity score. This gives us
a clean English query.
For each token in the cleaned English SMS query,
we create a list of possible Hindi translations of the
token using the statistical translation table. Each
Hindi word was assigned a weight according to
Equation 4. The Pruning algorithm in Section 5 was
then applied to get the best matching FAQ.
6.2.3 Experiment 3
In this experiment, for each token in the noisy En-
glish SMS we obtain a list of possible English vari-
ations. For each English variation a corresponding
set of Hindi words from the statistical translation ta-
ble was obtained. Each Hindi word was assigned
a weight according to Equation 4. As described in
Section 5.2, all Hindi words obtained from English
variations of a given SMS token are merged to create
93
Experiment 1 Experiment 2 Experiment 3
MRR Score 0.41 0.68 0.83
Table 1: MRR Scores
F1 Score
Expt 1 (Baseline 1) 0.23
Expt 2 (Baseline 2) 0.68
Expt 3 (Proposed Method) 0.72
Table 2: F1 Measure
a list of Hindi words sorted in terms of their weight.
The Pruning algorithm as described in Section 5 was
then applied to get the best matching FAQ.
We evaluated our system using two different cri-
teria. We used MRR (Mean reciprocal rank) and
the best matching accuracy. Mean reciprocal rank
is used to evaluate a system by producing a list of
possible responses to a query, ordered by probabil-
ity of correctness. The reciprocal rank of a query
response is the multiplicative inverse of the rank of
the first correct answer. The mean reciprocal rank
is the average of the reciprocal ranks of results for a
sample of queries Q.
MRR = 1/|Q|
Q?
i=1
1/ranki (7)
Best match accuracy can be considered as a spe-
cial case of MRR where the size of the ranked list is
1. As the SMS based FAQ retrieval system will be
used via mobile phones where screen size is a ma-
jor constraint it is crucial to have the correct result
on the top. Hence in our settings the best match ac-
curacy is a more relevant and stricter performance
evaluation measure than MRR.
Table 1 compares the MRR scores for all three
experiments. Our method reports the highest MRR
of 0.83. Figure 6 shows the performance using the
strict evaluation criterion of the top result returned
being correct.
We also experimented with different values of
the threshold for Score(Q) (Section 5.3). The ROC
curve for various threshold is shown in Figure 7. The
result for both in-domain and out-of-domain queries
for the three experiments are shown in Figure 6 for
Score(Q) = 8. The F1 Score for experiments 1, 2 and
3 are shown in Table 2.
Figure 6: Comparison of results
Figure 7: ROC Curve for Score(Q)
6.3 Measuring noise level in SMS queries
In order to quantify the level of noise in the col-
lected SMS data, we built a character-level language
model(LM) using the questions in the FAQ data-set
(vocabulary size is 70) and computed the perplexity
of the language model on the noisy and the cleaned
SMS test-set. The perplexity of the LM on a cor-
pus gives an indication of the average number of bits
needed per n-gram to encode the corpus. Noise re-
Cleaned SMS Noisy SMS
English FAQ collection
bigram 16.64 55.19
trigram 9.75 69.41
Table 3: Perplexity for Cleaned and Noisy SMS
94
sults in the introduction of many previously unseen
n-grams in the corpus. Higher number of bits are
needed to encode these improbable n-grams which
results in increased perplexity. From Table 3 we can
see the difference in perplexity for noisy and clean
SMS data for the English FAQ data-set. Large per-
plexity values for the SMS dataset indicates a high
level of noise.
For each noisy SMS query e.g. ?hw 2 prvnt ty-
phd? we manually created a clean SMS query ?how
to prevent typhoid?. A character level language
model using the questions in the clean English FAQ
dataset was created to quantify the level of noise in
our SMS dataset. We computed the perplexity of the
language model on clean and noisy SMS queries.
7 Conclusion
There has been a tremendous increase in information
access services using SMS based interfaces. How-
ever, these services are limited to a single language
and fail to scale for multilingual QA needs. The
ability to query a FAQ database in a language other
than the one for which it was developed is of great
practical significance in multilingual societies. Au-
tomatic cross-lingual QA over SMS is challenging
because of inherent noise in the query and the lack
of cross language resources for noisy processing. In
this paper we present a cross-language FAQ retrieval
system that handles the inherent noise in source lan-
guage to retrieve FAQs in a target language. Our sys-
tem does not require an end-to-end machine transla-
tion system and can be implemented using a sim-
ple dictionary which can be static or constructed
statistically using a moderate sized parallel corpus.
This side steps the problem of building full fledged
translation systems but still enabling the system to
be scaled across multiple languages quickly. We
present an efficient algorithm to search and match
the best question in the large FAQ corpus of tar-
get language for a noisy input question. We have
demonstrated the effectiveness of our approach on a
real life FAQ corpus.
References
Sreangsu Acharyya, Sumit Negi, L Venkata Subrama-
niam, Shourya Roy. 2009. Language independent
unsupervised learning of short message service di-
alect. International Journal on Document Analysis
and Recognition, pp. 175-184.
Aiti Aw, Min Zhang, Juan Xiao, and Jian Su. 2006. A
phrase-based statistical model for SMS text normaliza-
tion. In Proceedings of COLING-ACL, pp. 33-40.
Peter F. Brown, Vincent J.Della Pietra, Stephen A. Della
Pietra, Robert. L. Mercer 1993. The Mathematics of
Statistical Machine Translation: Parameter Estimation
Computational Linguistics, pp. 263-311.
Jeunghyun Byun, Seung-Wook Lee, Young-In Song,
Hae-Chang Rim. 2008. Two Phase Model for SMS
Text Messages Refinement. AAAI Workshop on En-
hanced Messaging.
Monojit Choudhury, Rahul Saraf, Vijit Jain, Animesh
Mukherjee, Sudeshna Sarkar, Anupam Basu. 2007.
Investigation and modeling of the structure of texting
language. International Journal on Document Analy-
sis and Recognition, pp. 157-174.
Philipp Cimiano, Antje Schultz, Sergej Sizov, Philipp
Sorg, Steffen Staab. 2009. Explicit versus latent con-
cept models for cross-language information retrieval.
In Proceeding of IJCAI, pp. 1513-1518.
Danish Contractor, Tanveer A. Faruquie, L. Venkata Sub-
ramaniam. 2010. Unsupervised cleansing of noisy
text. In Proceeding of COLING 2010: Posters, pp.
189-196.
R. Fagin, A. Lotem, and M. Naor. 2001. Optimal aggre-
gation algorithms for middleware. In Proceedings of
the 20th ACM SIGMOD-SIGACT-SIGART symposium
on Principles of database systems, pp. 102-113.
Yijue How and Min-Yen Kan. 2005. Optimizing pre-
dictive text entry for short message service on mobile
phones. In M. J. Smith and G. Salvendy (Eds.) Proc. of
Human Computer Interfaces International,Lawrence
Erlbaum Associates
Valentin Jijkoun and Maarten de Rijke. 2005. Retrieving
answers from frequently asked questions pages on the
web. In Proceedings of the Tenth ACM Conference on
Information and Knowledge Management,CIKM, pp.
76-83.
Catherine Kobus, Francois Yvon and Grraldine Damnati.
2008. Normalizing SMS: Are two metaphors better
than one? In Proceedings of COLING, pp. 441-448.
Philipp Koehn, Hieu Hoang, Alexandra Birch Mayne,
Christopher Callison-Burch, Marcello Federico,
Nicola Bertoldi, Brooke Cowan, Wade Shen, Chris-
tine Moran, Richard Zens, Chris Dyer, Ondrej Bojar,
Alexandra Constantin, Evan Herbst 2007. Moses:
Open source toolkit for statistical machine translation.
Annual Meeting of the Association for Computation
Linguistics (ACL), Demonstration Session .
Sunil Kumar Kopparapu, Akhilesh Srivastava and Arun
Pande. 2007. SMS based Natural Language Interface
95
to Yellow Pages Directory. In Proceedings of the 4th
international conference on mobile technology, appli-
cations, and systems and the 1st international sympo-
sium on Computer human interaction in mobile tech-
nology, pp. 558-563 .
Govind Kothari, Sumit Negi, Tanveer Faruquie, Venkat
Chakravarthy and L V Subramaniam 2009. SMS
based Interface for FAQ Retrieval. Annual Meeting
of the Association for Computation Linguistics (ACL).
I. D. Melamed. 1999. Bitext maps and alignment via pat-
tern recognition. Computational Linguistics, pp. 107-
130.
Guimier de Neef, Emilie, Arnaud Debeurme, and
Jungyeul Park. 2007. TILT correcteur de SMS : Eval-
uation et bilan quantitatif. In Actes de TALN, pp. 123-
132.
Douglas W. Oard, Funda Ertunc. 2002. Translation-
Based Indexing for Cross-Language Retrieval In Pro-
ceedings of the ECIR, pp. 324-333.
A. Pirkola 1998. The Effects of Query Structure
and Dictionary Setups in Dictionary-Based Cross-
Language Information Retrieval SIGIR ?98: Proceed-
ings of the 21st Annual International ACM SIGIR Con-
ference on Research and Development in Information
Retrieval , pp. 55-63.
E. Prochasson, C. Viard-Gaudin, and E. Morin. 2007.
Language models for handwritten short message ser-
vices. In Proceedings of the 9th International Confer-
ence on Document Analysis and Recognition, pp. 83-
87.
Rudy Schusteritsch, Shailendra Rao, Kerry Rodden.
2005. Mobile Search with Text Messages: Designing
the User Experience for Google SMS. In Proceedings
of ACM SIGCHI, pp. 1777-1780.
Satoshi Sekine, Ralph Grishman. 2003. Hindi-English
cross-lingual question-answering system. ACM Trans-
actions on Asian Language Information Processing,
pp. 181-192.
E. Sneiders. 1999. Automated FAQ Answering: Contin-
ued Experience with Shallow Language Understand-
ing Question Answering Systems. Papers from the
1999 AAAI Fall Symposium. Technical Report FS-99-
02, AAAI Press, pp. 97-107.
W. Song, M. Feng, N. Gu, and L. Wenyin. 2007. Ques-
tion similarity calculation for FAQ answering. In Pro-
ceeding of SKG 07, pp. 298-301.
X. Xue, J. Jeon, and W.B Croft. 2008. Retrieval Models
for Question and Answer Archives. In Proceedings of
SIGIR, pp. 475-482.
96
