Language Resources and Localisation 
Reinhard SCH
Localisation Research
Department of Computer Science an
University of L
Limerick, Ir
Reinhard.Schal
 
 
Abstract 
Localisation is one of the fastest growing 
industrial sectors in the digital world. Since 
the mid-eighties, the role of localisation has 
developed and changed dramatically. 
Localisation has been redefined as the 
provision of services and technologies for the 
management of multilinguality across the 
global information flow. This paper discusses 
the need for easily accessible dedicated 
language resources for localisation, provides a 
practical example of what can be achieved 
with appropriate language resources in the 
context of localisation and proposes a strategy 
to acquire, maintain and make them easily 
accessible. 
1 Introduction 
Since its emergence in the mid 1980s, 
localisation has largely been defined as the 
linguistic and cultural adaptation of products for 
specific locales. Over the past 20 years, 
Localisation has become one of the engines driving 
the development of the multilingual information 
society and probably the first industrial sector 
where language resources have been used widely 
and consistently on large-scale commercial 
projects. 
Localisation professionals must prepare very 
large amounts of digital content simultaneously for 
different markets in acceptable quality and at 
affordable costs. This is only possible with the 
support of language resources, such as written and 
spoken corpora, translation memories and 
terminology databases, as well as the appropriate 
software tools for the acquisition, preparation, 
collection, management, customisation and use of 
these resources. 
In the following paragraphs, we will provide an 
overview of the current state of the localisation 
industry and its requirements, and focus on those 
aspects of the localisation process where the 
successful use of language resources is crucial for ?LER 
 Centre (LRC)  
d Information Systems (CSIS) 
imerick 
eland 
er@ul.ie 
the timely delivery of multilingual digital content. 
We will describe some of the most widely used 
language resources in localisation. Finally, we will 
describe major current research efforts relating to 
language resources and localisation, and highlight 
the opportunity for the establishment of a 
Language Resources Centre for Localisation. 
2 The changing face of localisation 
In this section, we will provide some 
background information which will help to explain 
the reasons for the changing needs of the 
localisation industry, particularly in relation to 
language resources. 
2.1 Growing language populations create 
commercial opportunities 
Any content made available on the digital 
networks (e.g. the internet) becomes instantly 
available to millions of people across the globe. To 
make this content accessible, however, it needs to 
be localised. In today?s cyberspace, posting digital 
content in just one language is not sufficient 
anymore. 
Of a total online language population of 680 
million in September 2003, only 35.5% spoke 
English as a mother tongue, but 25% spoke 
Chinese, Japanese or Korean (CJK) and another 
Source: Global Reach (global-reach.biz/globstats)  
Figure 1: Online language population
25% Spanish, German, French, Italian and 
Portuguese. 
In this scenario, localisation is a pre-requisite for 
the provision of equal access to the digital 
information society independently of an 
individual?s cultural and linguistic background or 
their geographical location and, at the same time, 
offers enormous business potential. In addition, it 
is evident that localised digital content 
(applications and systems) is a pre-requisite for the 
preservation of linguistic and cultural diversity in 
the digital world. 
According to US-analysts Allied Business 
Intelligence, the world-wide market for translation 
and software or web localisation is growing from 
US$11 billion (1999) to US$20 billion this year. 
However, although much digital content is 
created in Asia and Europe, 95% of localised 
digital content still originates in the USA. 
 Localisation
Vectors of scalability and growth
Geography / Languages
Content
Medium of delivery
Europe
Documents
Manuals
Asia
Global
CD-ROM
Online
Pure Internet-based
General
technical
Any
content
Culture
Symbols
Rights
Values
 
Figure 3: Vectors of scalability and growth 
2.2 Localisation services ? redefined 
While politicians all over the world want to 
make Information Society Technologies (IST) 
available and accessible in the language and locale 
of the people they represent, software and digital 
content publishers need to respond to the demands 
of their customers by supporting a wide variety of 
local languages and cultures in their products. 
 
Since its emergence in the mid-1980s, the 
localisation industry has taken on the task of 
responding to these requirements of business and 
politics. Initially seen as just one of many service 
suppliers to the general IT sector, it is now taking 
on a more independent role translating the global 
digital content challenge into new business and 
social opportunities. It is the localisation industry 
that can enable the open, pluralist, user-friendly 
and inclusive multilingual and cross-cultural 
information society. 
The dramatic change in the role and the funtion 
of localisation has happened in parallel with the 
development and changes of the IT and content 
publishing sectors in general, now all converging 
in the digital world. 
 
The businesses of computing (hardware, 
software and services), communications 
(telephony, cable and satellite), and content 
(publishing, entertainment, advertising) are coming 
together to create the new digital media industry. 
New media publishing on the Internet combines 
computing, entertainment, broadcasting, music and 
video production. 
The issues faced by a wide variety of formerly 
independent, unconnected traditional content 
publishing industries joining in the digital world 
include the need to handle, control and translate 
larger amounts of text than ever before into an ever 
increasing variety of languages in parallel with the 
development of the original version, within a tight 
budget and according to strict quality guidelines as 
well as the need to adapt ? not just translate ? 
their products to the culture and locale of the target 
market. 
IT provides the framework for the convergence 
of these activities. The localisation industry 
provides the framework for the convergence of the 
multilingual aspects of these activities. 
Localisation becomes the catalyst for electronic 
multilingual production and publishing. 
On the background of these developments, the 
concept of localisation is being redefined as the 
provision of services and technologies for the 
management of multilinguality across the global 
information flow. 
Timely and cost effective delivery of high 
quality digital content to the global marketplace 
has become the major growth area for the 
localisation industry. It opened the relatively 
narrow software localisation industry to a wider 
range of players who are broadening traditional 
roles within the software localisation industry. 
Yet, the localisation industry does not have 
access to a robust infrastructure comprising 
Multilingual software = Multilingual information society
Preservation of
linguistic diversity,
cultural heritage
Development  of
business growth,
competitive advantage
Inclusion
of all countries and sections of society 
in the global village
Political focus Business focus
Figure 2: Multilingual software and Infosociety 
language data and tools, which are a prerequisite 
for the timely and cost effective creation and 
deployment of multilingual, cross-cultural and 
multimodal digital content. There is an urgent need 
for the development of the structural basis to make 
a sustained internationalisation and localisation 
effort possible, especially for less widely spoken 
languages. 
2.3 Localisation research 
The new role of the localisation industry also 
creates new opportunities and requirements for 
research and development. 
How can the business requirement for a 
reduction in production cost, combined with fast 
throughput time and high quality be satisfied in the 
context of localisation? 
One answer to this question is by access to 
adequate language resources, tools and standards. 
Re-use of already translated material, translation 
recycling, can not only speed-up translation, it can 
also at least help to ensure a more consistent use of 
terminology and thus increase the quality of the 
translation. 
Access to adequate terminology tools and 
resources can also help to achieve better 
translations at a lower cost. 
To make appropriate use of these technologies 
and resources, their role must be assessed in the 
context of localisation. 
3 Language resources and localisation 
Language data or resources have been defined as 
a set of speech or language data and descriptions in 
machine readable form, used e.g. for building, 
improving or evaluating natural language and 
speech algorithms or systems, or, as core resources 
for the software localisation and language services 
industries, for language studies, electronic 
publishing, international transactions, subject-area 
specialists and end users. 
Examples of language resources are written and 
spoken corpora, computational lexicons, 
terminology databases, speech collection and 
processing, etc. Basic software tools are also 
important for the acquisition, preparation, 
collection, management, customisation and use of 
these language resources and other resources. (see: 
http://www.elra.info/article.php3?id_article=35, 
consulted May 2004) 
Currently, there are no large collections of 
language data relevant to localisation easily 
accessible. While individual digital content 
developers, especially large multinationals, have 
very large collections of multilingual language data 
available and use them in a highly effective and 
efficient way, these data collections are not 
available to the localisation community in general. 
Large multinational content publishers, however, 
have shown how the efficient use of language 
resources can help to achieve astonishing 
production results leading to what some have 
described as the translation factory. 
? Start: mid-eighties
? Packaged softward -> multimedia -> content
? Ireland: the world centre (certainly the European centre)
? 95% of source orginates in the USA
? International market more important for pubishers than domestic markets
? MS: >60%, >US$5b ., >1,000 projects/year, Ireland: US$1.9b revenue (2001)
R e v e n u e  p e r  S e r v ic e - S e g m e n t
( in  U S $ m )
0
5 0 0
1 0 0 0
1 5 0 0
2 0 0 0
2 5 0 0
3 0 0 0
2 0 0 0 2 0 0 1 2 0 0 2 2 0 0 3 2 0 0 4 2 0 0 5
S o f tw a r e  L o c a lis a t io n W e b  L o c a lis a t io n T r a n s la t io n In te r p r e ta t io n
Source: IDC, Worldwidc Globalization and Localization Services Market Forecast and Analysis, 2000-2005, 2001, www.idc.com
Figure 4: Revenue per service segment 
3.1.1 Case study 
Tony Jewtushenko, Tools Manager with 
Oracle?s Worldwide Translation Group, presented 
the following example of the use of language 
resources in localisation at the LRC?s 2003 Annual 
Localisation Conference. 
Project constraints 
? 4m wordcount software strings; 
? 30 languages simultaneous release; 
? 13k localisable files; 
? Localisation group in Dublin; 5,000 people 
world-wide distributed development team. 
Objectives 
? 24/7, 100% automated process ? no 
exceptions 
? Translation in parallel with development 
? Translation begins at code check-in 
? Translation ?on demand? ? no more ?big 
project? model 
Solution: the translation factory metrics 
? Current throughput: 100,000 language 
check-ins per month 
? 2 million files per month 
? 98% of words leverage 
? Average time to process a file: 45 seconds 
? Fully scalable ?add-a-box model? 
? Simpship of all 30 languages 
? International version testing before US 
release 
? Reduced no. of release engineers (20->2) 
resulting in US$20m saving per year 
? Positive ROI within 1 year 
It is important to keep in mind that Oracle is one 
of the world?s leading digital publishers and runs 
one of the most sophisticated internationalisation 
and localisation operations in the world. Oracle is 
also centrally involved in the development of two 
key standards under the umbrella of OASIS, the 
XML-based Localisation Exchange Format 
(XLIFF) and the Translation Web Services Group 
(TWS), which, combined, have the potential to 
fundamentally change not just the way localisation 
is done by Oracle, but by every digital publisher 
bringing its contents to the global market. 
3.1.2 Support for SMEs and researchers 
There are no good reasons why the work of other 
organisations, such as small and medium sized 
enterprises (SME?s) and research organisations, 
should not benefit from the intelligent use of 
linguistic resources, including language data, tools 
and standards. While large organisation catering 
for the main language markets have access to the 
finances necessary for the development and 
maintenance of this linguistic infrastructure, 
smaller organisations and those catering for 
financially less significant markets will need the 
support of a shared and widely supported 
infrastructure. 
This infrastructure would need to cover: 
? Multimodal digital content in source and 
target languages; 
? Monolingual and multilingual 
terminology; 
? Translation memories. 
3.2 Linguistic tools 
Linguistic tools are seen by the Localisation 
Industry Standards Assocication (LISA) generally 
as still an emerging sector (Localization Industry 
Primer) although, according to LISA, enormous 
progress has been made over the past years in the 
area and a number of productivity enhancing tools 
are now in use, without which the localisation 
industry as we know it today would not be able to 
operate. 
3.2.1 Current situation 
The issues, which are addressed by linguistic 
tools and technologies answer some of the central 
questions asked by localisation professionals 
around terminology handling and update 
processing. 
Terminology handling 
? Where can translators find standard 
terminology in multiple languages? 
? How can multilingual terminology be 
processed so that it can be made readily 
available and easily accessible? 
? Are there feasible models and mechanisms 
to maintain and constantly update 
multilingual terminologys so that 
modifications can be made accessible to 
translators instantaneously? 
? How can changes in previously agreed 
terminology be automatically integrated in 
already translated text? 
? How can translated texts automatically be 
checked for the correct and consistent use 
of terminology? 
Update processing  
? How can versions of the same source 
material be compared against each other 
automatically? 
? How can overlaps be identified, marked 
and analysed? 
? How can source and target language 
content be compared and aligned? 
? How can already translated text fragments 
(exact or fuzzy matches) automatically be 
used for the generation of a new target 
version? 
? How can the limitations of Translation 
Memory Systems be overcome?  
The linguistic tools and technologies most 
widely needed and developed for use in the 
internationalisation and localisation effort include: 
? Terminology management systems, which 
aid the collection and use of specialised 
vocabularies; 
? Translation memories, which are designed 
to facilitate the reuse of previous 
translations; 
? Machine translation, which provides actual 
linguistic analysis and conversion of texts 
from source language into the desired 
target language; 
? User interface and user assistance visual 
translation environments, which aid 
translators to interactively work with 
compiled and uncompiled resource files in 
a variety of formats; 
? Language data analysis tools, which 
rapidly compare and analyse old and new 
source material; 
? Sophisticated matching tools leveraging 
material from previous projects; 
? Natural language parsers; 
? Extract-and-Insert tools; 
? Parsers for natural language digital content 
in compiled sources. 
While large and sophisticated localisation 
operations have easy access to relevant linguistic 
third party technologies and in-house tools, smaller 
operations often do not. Reviewing the large 
variety of sophisticated tools and technologies 
available on the market, they often shy away from 
the purchase and implementation of tools because 
of the perceived high-risk factor attached to their 
deployment. 
3
 to 
a on 
d of 
t on 
a  of 
t the 
L nd 
S ed 
E
ns, 
s on 
A of 
L
t
c
d
t
i
p
s
s
t
i
i
3
e
a
A
p
i
w
t
p
d
s
o
w
 
3.3.1 Current situation 
A large number of standards relevant to 
linguistic resources in the context of localisation 
have been published by a number of organisations 
identified by LISA as being involved in the 
development of standards. Among these are: 
International Standards Organisation (ISO) ? 
This is a network of national standards institutes 
from 140 countries working in partnership with 
international organizations, governments, industry, 
business and consumer representatives. The ISO 
sees itself as a bridge between public and private 
sectors. .2.2 Impact 
A support infrastructure must be put into place
llow smaller players involved in localisati
irect and online access to the widest variety 
ools and technologies and detailed informati
bout these. A first step in the implementation
his infrastructure has been the establishment of 
ocalisation Tools and Technology Laboratory a
howcased (LOTS) as part of the European-fund
LECT project. 
Cooperating with leading industry associatio
uch as the Globalisation and Localisati
ssociation (GALA) and The Institute 
ocalisation Professionals (TILP), and building on 
he expertise available within the ELECT 
onsortium, a sophisticated online library with 
etailed background information on each of the 
ools available was prepared and published. 
The detailed LOTS-sponsored reviews of 
ndividual tools and technologies will allow 
otential users of linguistic tools and technologies, 
pecifically those working in small and medium 
ized enterprises, to base their decision on which 
ool to use for their particular localisation needs on 
ndependent, well-researched and easily accessible 
nformation. 
.3 Standards 
Standards played a central role in the 
stablishment of the localisation industry?s first 
ssociation, the Localisation Industry Standards 
ssociation (LISA). Very early on, localisation 
rofessionals recognised that the successful 
mplementation of widely recognised standards 
ould lower the cost of localisation, shorten the 
ime necessary for the successful completion of 
rojects and increase the quality of the products 
elivered to international audiences. 
The following graphic visualises the role 
tandards for linguistic resources play in the 
verall localisation process (source: i18n Inc.: 
ww.i18n.ca): 
? Technical Committee 37 - Terminology 
and other language resources 
? ISO 639 - Language Codes 
? Terminology Data Categories - ISO 12620 
? MARTIF - ISO 12200 - Machine-readable 
terminology interchange format 
? Terminology Work - ISO 704 
? Vocabulary - ISO 1087-1 - Part 1: Theory 
and Application 
? Vocabulary - ISO 1087-2 - Part 2: 
Computer Applications 
? Terminological Markup Framework - ISO 
DIS 16642 
? ISO639-1 : New ISO standard for the 
identification of languages names 
Localisation Industry Standards Association 
(LISA) ? This organisation has published a variety 
of standards relevant to the use of language 
resources in localistion: 
? TMX the exchange standard for 
translation memory data between tools 
and/or translation vendors aiming at little 
or no loss of critical data during the 
process. 
? TBX the open XML-based standard 
format for terminological data. 
? OLIF the XML-compliant standard for 
terminology offering support for natural 
language processing (NLP) systems, such 
as machine translation, by providing 
coverage of a wide and detailed range of 
linguistic features. 
 
Figure 5: Standards in the localisation 
OASIS ? This international, not-for-profit 
consortium designs and develops industry standard 
specifications for interoperability based on XML. 
Two of these have been developed specifically 
with localisation in mind: 
? XLIFF ? the XML-based Localisation 
Interchange File Format 
? TWS ? the Translation Vendor Web 
Services Standard 
The Free Standards Group Open 
Internationalization Initiative (Openi18n.org) ? 
This non-profit initiative aims to accelerate the use 
and acceptance of open source technologies 
through the application, development and 
promotion of interoperability standards. 
Termnet ? The International Network for 
Terminology promotes co-operation in the field of 
terminology internationally, so as to stimulate the 
development of the terminology and knowledge 
market, as well as terminology proper. Termnet 
publishes terminologically relevant data in both 
printed and computerised forms and thus makes it 
accessible to a large circle of users.  
Unicode ? The Unicode Consortium is a non-
profit organization founded to develop, extend and 
promote the use of the Unicode Standard, which 
specifies the representation of text in modern 
software products and standards. The membership 
of the consortium represents a broad spectrum of 
corporations and organisations in the computer and 
information processing industry. Membership in 
the Unicode Consortium is open to organisations 
and individuals anywhere in the world who support 
the Unicode Standard and wish to assist in its 
extension and implementation. Unicode?s most 
visible activities include the holding of the 
Internationalisation and Unicode conference twice 
a year. 
WC3 ? This consortium develops interoperable 
technologies (specifications, guidelines, software, 
and tools) to lead the Web to its full potential as a 
forum for information, commerce, communication, 
and collective understanding. 
3.3.2 Impact 
Currently, there is not central repository of 
standards relevant to the development and 
maintenance of linguistic resources for localisation 
comprising language data, tools and standards 
which is easily accessible to the localisation 
community. Furthermore, and equally important, 
no independent organisation or consortium is 
currently set up to demonstrate the effective and 
efficient use of linguistic resources in a localisation 
environment following industry-standard 
approaches and using state-of-the-art technologies. 
4 The Localisation Tools, Technologies and 
Resources Laboratory 
4.1 The Rationale 
The establishment of the Localisation Tools and 
Technologies Laboratory and Showcase (LOTS) as 
part of the European-funded ELECT project was 
the first attempt to make a repository of language 
resources covering linguistic data, tools and 
standards available and easily accessible. 
Although LOTS is located at the Localisation 
Research Centre in Limerick, it is also available 
online via www.electonline.org. 
In one location, LOTS provides different user 
groups with access to the widest possible range of 
tools and corresponding resources. 
Figure 6: The LOTS online desktop 
? Students, Trainers can quickly get an 
overview of tools, technologies and 
resources relevant to localisation. 
? Researchers can experiment with state-
of-the-art technologies and resources 
comparing the results of their efforts 
with commercial offerings. 
? Professionals can test whether particular 
applications are appropriate to cover 
their specific needs. 
? The LRC uses the facilities available in 
LOTS to verify standards and 
interoperability issues. 
4.2 Tools and Technologies 
LOTS was established with the support of the 
localisation tools and technology developers. All 
the resources available to LOTS have been given 
to the LRC by their owners free-of-charge. 
Twenty-four companies, resprenting the majority 
of localisation tools and technologies worldwide, 
have so far contributed to LOTS. 
 
Figure 7: The LOTS laboratory 
The LRC has estimated that the laboratory 
contains tools and technologies worth more than  
?350,000. 
These are accessible on a large number of PCs 
running in LOTS under a variety of operating 
systems in more than a dozen different languages. 
They are also available online on the LOTS 
server which brings LOTS directly to the desktop 
of users anywhere in the world. 
4.3 Resources 
In addition to the tools and technologies, the 
resources available on the LOTS server and in the 
laboratory include the most widely used file 
formats and standards. 
While tools and technologies are well covered in 
LOTS, the coverage of corresponding resources as 
used by these tools could be improved. 
The LRC is working with content and 
technology developers on agreements which will 
allow a wider deployment of authentic source 
material in a wider range of languages and file 
formats. 
5 Language Resource Centre for Localisation 
Although with LOTS the LRC has established 
the foundations for a language resources centre for 
localisation, a consortium representing the main 
actors in localisation, based in a number of 
European countries, including the accession 
countries, will need to be established to develop 
and maintain such a repository long-term. This 
consortium will also need to develop long-lasting 
relations with the leading industry associations and 
standards bodies, e.g. the Globalisation and 
Localisation Association, LISA, OASIS, Unicode, 
W3C and The Institute of Localisation 
Professionals (TILP). 
The centre, proposed to be located at the LRC, 
will provide access to 
? Linguistic resources 
? Methodologies and guidelines 
? Localisation scenarios 
Each of these components will be explained 
more in detail in the following sections. 
5.1 Repository of linguistic resource 
The centre will establish, develop and maintain a 
repository of linguistic resources for localisation 
physically based at the LRC but accessible online 
over the Internet covering: 
? A large variety of multimodal authentic 
digital content in source and target 
languages; 
? Mono- and multilingual terminology; 
? Translation memories; 
? Linguistic tools and technologies used for 
the automatic processing of digital content; 
? Guidelines and standards for the 
development and processing of digital 
content to be localised. 
Together, the efforts in these areas will deliver 
the structural basis for a sustained 
internationalisation and localisation effort, 
especially for less widely spoken languages where 
market forces often provide insufficient incentives. 
5.2 Methodologies and guidelines 
The centre will provde access to methodologies 
and guidelines for the verification of standards 
compliance and interoperability verification for 
linguistic resources in a multilingual, multicultural 
and multimodal localisation environment. This 
work will be based on industry-standard 
approaches and be guided by established principles 
and procedures. The reports will be sourced in 
cooperation with relevant associations and 
standards bodies and public deliverables of 
European Union funded projects. 
5.3 Localisation scenarios 
The centre will, in consultation with the wider 
localisation community, build a laboratory-based, 
automated localisation environment mirroring real-
world, authentic localisation scenarios. This 
environment will be used to showcase, verify and 
demonstrate best practice in localisation making 
use of the linguistic resources available through the 
centre. 
Results from each of these three areas of activity 
will be made available to the wider digital content 
and localisation communities using different 
dissemination strategies. It is envisaged that the 
development of a market place for these linguistic 
resources will guarantee the sustainability of the 
effort. 
6 Conclusion 
We have shown how the needs and requirements 
of localisation have developed over the years. The 
enormous pressure on localisation providers to 
produces language versions of original material 
simultaneously with the production and publication 
of the original can only be addressed making 
efficient and effective use of customised language 
resources covering linguistic data, tools and 
technologies, and appropriate standards. 
Access to language resources for SMEs and the 
research community can be realised within a 
widely supported Language Resource Centre for 
Localisation, built on the foundations of the 
Localisation Tools and Technology Laboratory 
(LOTS) at the LRC. 
The establishment of the Language Resources 
Centre for Localisation is a five-year project. 
During this time, the LRC plans to create a 
sustainable, accessible and financially viable 
linguistic infrastructure for the internationalisation 
and localisation communities. 
Its overall aims are to: 
? Pool together linguistic infrastructure 
resources for the localisation industry, 
including digital content, monolingual and 
multilingual terminology, translation 
memories, tools and technologies, as well 
as relevant standards. 
? Establish a linguistic resources support 
network within the localisation industry 
covering digital content publishers, service 
providers, technology developers, 
standards bodies and standards verification 
initiatives. 
? Provide convenient access to relevant 
linguistic resources for content developers, 
service providers, as well as suppliers of 
localisation services and solutions. 
? Develop methodologies for the verification 
of standards compliance. 
? Implement localisation scenarios in a 
laboratory environment aimed at 
demonstrating state-of-the-art, best 
practice localisation technology and 
process solutions, and at verifying relevant 
localisation standards. 
? Work towards the establishment of a 
market place and a viable linguistic 
resources provider network for the 
localisation industry. 
Activities during years 1-2 will focus on the 
establishment of a repository of linguistic 
resources. During this period, the LRC will build a 
core group of partners supporting its development. 
This core group will be backed by the LRC?s 
Contact Group. 
Activities during years 3-5 will see the 
implementation of a financially viable standards 
testing and interoperability verification system at 
the LRC. While few commercial organisations 
have managed to make a profit to sustain this kind 
of activity in other industrial sectors, there is no 
doubt that the operation of such a centre is 
financially viable long-term in a not-for-profit 
environment such as that provided by the LRC at 
the University of Limerick. 
7 Acknowledgements 
We would like to acknowledge the support of the 
European Union for the European Localisation 
Exchange Centre (ELECT) under its eContent 
Programme, contract 52005. 
References  
Electonline ? the online resource for the 
localisation community. www.electonline.org, 
last visited 10 June 2004. 
Language resources: the ELRA definition. 
http://www.elra.info/article.php3?id_article=35, 
last visited 20 May 2004. 
Localization Industry Primer. LISA. 
www.hltcentral.org/usr_docs/call_docs/ 
eContent/call1/LISA%20Primer.pdf, last visited 
13 May 2004. 
Sch?ler, R., Michael Carl, Andy Way. 2002. 
Toward a Hybrid Integrated Translation 
Environment. In: ?Proceedings of the Fifth 
Biennial Conference of the Association for 
Machine Translation in the Americas Conference 
(AMTA)?, Tiburon, California, 08-12 October 
2002.  
Sch?ler, R. 2002. The European Localisation 
Exchange Centre (Keynote). In: Proceedings of 
the Twenty-first International Unicode 
Conference, Unicode Localization and the Web: 
The Global Connection, Dublin, Ireland, 16-17 
May 2002. 
Sch?ler, R. 2002. The XLIFF Standard. Panel 
Session with Ian Dunlop (Novell), Tony 
Jewtushenko (Oracle), Wojtek Kosinski and 
Peter Reynolds (Bowne), in: Proceedings of the 
Twenty-first International Unicode Conference, 
Unicode Localization and the Web: The Global 
Connection, Dublin, Ireland, 16-17 May 2002.  
Standards in the localisation process. Graphical 
representation. www.i18n.ca, last visited 10 May 
2004. 
Proceedings of the NAACL HLT Workshop on Software Engineering, Testing, and Quality Assurance for Natural Language Processing, pages 47?55,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Web Service Integration for Next Generation Localisation 
David Lewis, Stephen Curran,  
Kevin Feeney, Zohar Etzioni, 
John Keeney 
Andy Way Reinhard Sch?ler 
Centre for Next Generation Localisation 
Knowledge and Data Engineering 
Group 
School of Computing Centre for Localisation  
Research 
Trinity College Dublin, Ireland Dublin City Universit 
y, Ireland 
University of Limerick,  
Ireland 
{Dave.Lewis|Stephen.curran|K
evin.Feeney|etzioniz|John.Ke
eney}@cs.tcd.ie 
away@computing.dcu.ie 
 
Reinhard.Schaler@ul.ie 
 
 
Abstract 
Developments in Natural Language Processing technol-
ogies promise a variety of benefits to the localization 
industry, both in its current form in performing bulk 
enterprise-based localization and in the future in sup-
porting personalized web-based localization on increa-
singly user-generated content. As an increasing variety 
of natural language processing services become availa-
ble, it is vital that the localization industry employs the 
flexible software integration techniques that will enable 
it to make best use of these technologies. To date how-
ever, the localization industry has been slow reap the 
benefits of modern integration technologies such as web 
service integration and orchestration. Based on recent 
integration experiences, we examine how the localiza-
tion industry can best exploit web-based integration 
technologies in developing new services and exploring 
new business models  
? Introduction 
Research and development of natural language 
processing technologies are leading to a variety of 
advances in areas such as text analytics and ma-
chine translation that have a range of commercial 
applications. The Localization Industry in particu-
lar, is strategically well placed to make good use of 
these advances as it faces the challenge of localiz-
ing accelerating volumes of digital content that is 
being targeted at increasingly global markets of 
this content. It needs to exploit the benefits of NLP 
technologies to reduce the cost of translation and 
minimise the time to market of this digital content.  
Furthermore, where the localization industry best 
learns how to efficiently and flexibly employ  NLP 
technologies in the localization of digital content it 
will be ideally placed to develop new services and 
exploit new business opportunities offered by the 
WWW. In particular, today?s localization tech-
niques are not able to keep pace with the WWW?s 
ability to dynamically compose and personalize 
existing content and to support rapid development 
of large volumes of user generated content. To 
meet this challenge, localization processes must 
effectively employ NLP to move from manually 
centered, professional batch activities to highly 
automated, highly participative continuous activi-
ties. To do this, the technologies of the WWW 
need to be employed to dynamically combine NLP 
technologies and leverage different levels of hu-
man linguistic abilities and knowledge to best ac-
complish the task at hand.   
In this paper we examine how this vision, which 
we term Next Generation Localization, can be sup-
ported by current web-based, service-oriented 
software integration techniques such as web ser-
vice integration and orchestration. Based on recent 
integration experience we review the current issues 
in using open interoperability standards and web 
services to the integration of commercial localiza-
tion platforms and NLP software. We then describe 
some generic definitions for NLP web services and 
how these provide flexibility in developing new 
localization service compositions. Finally, we out-
line the major software integration challenges fac-
ing the localization industry and describe how 
these are being addressed at Ireland?s Centre for 
Next Generation Localization (CNGL). 
47
? Next Generation Localization 
Traditional localization technologies and 
workflows are no longer able to cope with the es-
calating growth in volume. Traditional localization 
methods are not adequate to manage, localize and 
personalize unpredictable, on-line, multilingual, 
digital content. Machine Translation (MT) needs to 
be integrated into translation and post-editing 
workflows together with human translators. Novel 
machine-learning-based language technologies can 
automatically provide metadata annotations (la-
bels) to localization input in order to automate lo-
calization standardization and management. 
 
 
 
 
Figure 1: Example use of Web Service Orchestration in 
a Localisation Workflow 
 
For Next Generation Localisation to be 
achieved, the individual components need to be 
interoperable and easily reconfigurable. The com-
plexity of the resulting systems poses substantial 
software engineering challenges and crucially re-
quires detailed user requirement studies, technical 
and user interface standards, as well as support for 
rapid prototyping and formative evaluation early 
on in the software lifecycle. Blueprints for an in-
dustrial environment for Next Generation Localisa-
tion, which we term a Localisation Factory, are 
needed to guide the development of localisation 
services systems integrating advanced language, 
digital content and localisation management tech-
nologies. However, in order to successfully 
achieve the goal of technical interoperability these 
services crucially needs to be supplemented by 
standardised localisation processes and workflows 
for the Localisation Factory. Figure 1 gives an 
overview of a typical localisation workflow, that 
would be used for translating the content such as 
the use manual for a product, into multiple lan-
guages for different target markets. Typically this 
involves segmenting the content into sentences, 
looking up previously translated sentences from a 
Translation Memory (MT), before passing untrans-
lated segments to a Machine Translation (TM) ser-
vice to generate further candidate translations. 
Next, the job is passed to professional translators, 
who can accept automated translations or provide 
their own translations. Current practice in perform-
ing such workflows uses localisation platforms 
such as SDL?s Idiom WorldServer to integrate 
Translation Memory databases, Machine Transla-
tion packages and the routing of jobs to translators 
who typically work remotely under the manage-
ment of a localisation service provision agency.  
The localization industry has already underta-
ken a number of separate standardization activities 
to support interoperability between different locali-
sation applications. The Localisation Industry 
Standards Association (LISA ? www.lisa.org) has 
developed various localisation standards: 
? Translation Memory Exchange (TMX) for ex-
changing TM database content.  Many TM tool 
providers have implemented support for TMX 
in their products. 
? Term Base eXchange (TBX): XML Terminol-
ogy Exchange Standard. An XML linking 
standard, called Term Link, is also being in-
vestigated.  
? Segmentation Rules eXchange (SRX), for ex-
changing the rule by which content is original-
ly segmented. There has been very little sup-
port to date for SRX because segmentation is 
the main component that distinguished TM 
tools.  Segmentation has direct consequences 
for the level of reuse of a TM.  A TM's value is 
significantly reduced without the segmentation 
rules that were used to build it.   
? Global information management Metrics eX-
change (GMX): A partially populated family 
of standards of globalization and localization-
related metrics  
The Organization for the Advancement of Struc-
tured Information Standards (OASIS ? www.oasis-
open.org), which produces e-business standards 
has had a number of initiatives: 
? XML Localisation Interchange File Format 
(XLIFF):  XLIFF is the most common open 
standard for the exchange of localisable con-
48
tent and localisation process information be-
tween tools in a workflow.  Many tool provid-
ers have implemented support for XLIFF in 
their products. 
? Trans-WS  for automating the translation and 
localization process as a Web service.  There 
has not been much adoption of this standard.  
Work on the development and maintenance of 
the standard seems to be at a stand-still.  
? Open Architecture for XML Authoring and 
Localization: A recently started group looking 
at linking many existing localisation standards 
The W3C, which develops many web stan-
dards, has an Internationalisation Activity 
(www.w3.org/International) working on enabling 
the use Web technologies with different languages, 
scripts, and cultures. Specific standardisation in-
cludes the Internationalisation Tag Set to support 
internationalisation of XML Schema/DTDs. 
To date, therefore, standard localisation proc-
esses and workflows addressing common interop-
erability issues have not yet been widely adopted. 
Outside of proprietary scenarios, digital publishers 
and service providers cannot integrate their proc-
esses and technologies and cannot provide inde-
pendent performance measures. This implies lost 
business opportunities for many and missed oppor-
tunities for significant performance improvement 
for most of the stakeholders. We now examine 
how web services may help improve this situation. 
? Service Oriented Localization Integra-
tion 
The Centre for Next Generation Localisation 
[cngl] is developing a number of systems in order 
to investigate the issues that arise in integrating 
centralized workflows with community-based 
value creation. It aims to make full use of Service-
Oriented Architecture [erl]. This advocates 
software integration through well defined 
functional interfaces that can be invoked remotely, 
typically using the Web?s HTTP protocol with 
input and output parameters encoded in XML. The 
W3C have standardized an XML format, The Web 
Service Description Language (WSDL), for 
describing and exchanging such service 
definitions. Web services can be composed into 
more complicated applications using explicit 
control and data flow models that can be directly 
executed by workflow engines. This allows new 
workflow applications to be defined declaratively 
and immediately executed, thus greatly reducing 
the integration costs of developing new workflows 
and increasing the flexibility to modify existing 
ones. Such web-service based service composition 
is known as Web Service Orchestration. OASIS 
has standardized web service orchestration 
language called the Business Process Execution 
Language (BPEL), which has resulted in the 
development of several commercial execution 
platform and BPEL workflow definition tools, 
which support workflow definition through drag-
and drop interfaces. In CNGL, web services and 
web service orchestration are used  for integrating 
components and operating workflows between 
potential partners in the commercial localization 
value chain. This provides a high degree of 
flexibility in integrating the different language 
technologies and localization products into 
different workflow configurations for the project, 
while avoiding reliance on any single proprietary 
platform. As an initial exploration of this space a 
system integration trial was undertaken. The use of 
BPEL for integrating NLP software has previously 
been used in the LanguageGrid project, but is a 
purely in support of academic research integration. 
Our work aimed flexibility instantiate commercial 
localisation workflow using NLP software 
wrapped in services that are orchestrated using 
BPEL, while, as indicated in Figure 1, still 
integrating with commercial localisation workflow 
tools. This exploration also included extending the 
human element of the localisation workflow by 
soliciting translations from a body of volunteer 
translators. This is seen as more appropriate if the 
required translation is not time constrained and it 
often forms part of a customer relationship 
strategy. Quality management may require 
involvement of volunteer post-editors, and 
incomplete or poor translations may ultimately still 
need to be referred to professional translators. 
Thus our workflows can be configured to oper-
ate in parallel to provide alternative translations. In 
the professional localization workflow, after the 
MT stage, the candidate translation would be re-
turned to the SDL Worldserver platform via which 
professional translators and post-editors are able to 
complete the task. In the crowd-sourcing variation, 
this manual step is instead performed by passing 
the job to a similar application implemented as a 
49
plug-in to the Drupal collaborative content man-
agement system. 
Our implementation uses the XLIFF format as a 
standard for encapsulating the various transforma-
tions that happen to a resource as it passes through 
the localisation process. It should be noted, how-
ever, that support for XLIFF is partial at best in 
most localisation tools. Where the standard is sup-
ported, there are often different, specific flavours 
used, and embedded elements within the XLIFF 
can be lost as the resource passes through various 
stages in the process.  Another problem with in-
corporating current tools in our service-oriented 
framework is that some of them, such as IBM?s 
UIMA, are designed to function in a batch mode ? 
which does not map cleanly to services.  Neverthe-
less, despite a range of practical problems, it was 
in general possible to engineer service front-ends 
for most of these tools so that they can be inte-
grated into a composable service infrastructure. In 
the following section we proceed to detail the de-
sign of the generic web services we defined for this 
system and discuss the option undertaken in their 
implementation. 
3.1 Web Service Definitions 
The OASIS TWS working group remains the 
only real attempt to define web-services to support 
the localization process.  However, TWS has a li-
mited scope.  Rather than aiming to support the 
dynamic composition of language services into 
flexible localization workflows, it concentrates on 
supporting the negotiation of ?jobs? between ser-
vice providers.  It is primarily intended to support 
the efficient out-sourcing of localization and trans-
lation jobs and it does not address the composition 
of language-services to form automated 
workflows.   
Therefore, in order to deploy web-services to 
support such composition, there is little standardi-
sation to rely on.  Thus, a first step in addressing 
the problem is to design a set of web-services and 
their interfaces suitable for the task.   In designing 
these services, it is worthwhile to recall the general 
goals of service-oriented architectures; the services 
should be designed to be as flexible and general as 
possible and they should neither be tightly coupled 
to one another, nor to the overall system which 
they are part of.  Furthermore, in keeping with the 
general trends in service designs [foster], variabili-
ty in service behavior should generally be sup-
ported through the passed data-structures rather 
than through different function signatures.  
Bearing these design goals in mind, we can be-
gin to analyse the basic requirements of localisa-
tion with a view to translating these requirements 
into concrete service definitions.  However, in or-
der to further simplify this task, we adopt certain 
assumptions about the data-formats that will be 
deployed.  Firstly, we assume that UTF-8 is the 
universal character encoding scheme in use across 
our services.  Secondly, we assume that XLIFF is 
employed as the standard format for exchanging 
localization data between different parts of the lo-
calisation process.  
XLIFF is primarily focused on describing a re-
source in terms of source segments and target seg-
ments.  Essentially, it assumes the following mod-
el: a localization job can be divided up into a set of 
translatable resources.  Each of these resources is 
represented as an XLIFF file.  Each resource can 
be further sub-divided into a sequence of translata-
ble segments (which may be defined by an SRX 
configuration). Each of these source segments can 
be associated with a number of target segments, 
which represent the source segment translated into 
a target language.  Finally, XLIFF also supports 
the association of various pieces of meta-data with 
each resource or with the various elements into 
which the resource is sub-divided.  
This simple basic structure allows us to define a 
very simple set of general web-services, each of 
which serves to transform the XLIFF in some way.  
These three basic classes of services transform the 
XLIFF inputs in the following ways: 
1. Addition of target segments.   
2. Sorting of target candidates 
3. Addition of meta-data.  
 
Thus, we adopt these service-types as the set of 
basic, general service interfaces that our services 
will implement.  They allow us to apply a wide 
range of useful language-technology processes to 
localization content through an extremely simple 
set of service interfaces.  To give some examples 
of how concrete services map onto these basic in-
terfaces: 
? A machine translation service is a manifesta-
tion of type 1.  It adds translations, as target 
segments, for  source segments  in the XLIFF 
file 
50
? A translation memory leveraging service is, 
similarly, implemented as a service of type 1. 
It can be considered as a special case of a 
translation service. 
? Our basic service-design supports the applica-
tion of multiple TM and MT services to each 
XLIFF file, potentially producing multiple 
translation candidates for each source segment.  
There are various situations where there is a 
need to order these candidates ? for example to 
choose which one will actually be used in the 
final translation, or to present a sorted list to a 
human user to allow them to most convenient-
ly select the candidate that is most likely to be 
selected by them.  These services can be im-
plemented using the common type 2 interface.  
? A wide range of text analytics service can be 
implemented as services of type 3.  For exam-
ple, domain identification, language identifica-
tion and various tagging services are all instan-
tiations of this type.  
Although these service types are generic, in terms 
of the transformations that they apply to the XLIFF 
content, they may be very different in terms of 
their management and configuration.  Thus, it is 
neither possible nor desirable to devise generic 
management interfaces ? these interfaces need to 
be tailored to the particular requirements of each 
specific service.  Thus, each service really consists 
of two specifications ? an implementation of the 
generic interface which allows the service to be 
easily integrated as a standard component into a 
workflow that transforms the XLIFF content, and a 
specific interface that defines how the service can 
be configured and managed.  The following section 
provides several examples of specific services and 
their management interfaces.  
Although XLIFF provides significant support for 
management of the transformation of resources as 
they proceed through the localisation workflow, it 
is not a universal solution. It is an inherently re-
source-oriented standard and it is thus not well 
suited for the aggregation of meta-data that has 
broader scope than that of the translatable resource.  
For example, in the course of a localisation 
workflow, we may wish to store state information 
relating to the user, the project, the workflow itself 
or various other entities that are not expressible as 
XLIFF resources. Therefore, a service-oriented 
localization workflow has a need for a service 
which allows the setting and retrieving of such me-
ta-data. The following section also includes a basic 
outline of a service which can provide such func-
tionality across the localization workflow.  
Finally, it should be pointed out that BPEL 
does not provide a universal solution to the prob-
lem of constructing workflows.  It is primarily de-
signed to facilitate the orchestration of automated 
web-services and does not map well to human 
processes. This has been acknowledged in the pro-
posed BPEL4People extension and the incorpora-
tion of better support for human tasks is also a key 
motivating factor for the development of the 
YAWL workflow specification language ? a BPEL 
alternative [vanderaalst].  To overcome this limita-
tion, we have designed a general purpose service 
which allows components to query the state of hu-
man tasks within the workflow ? this allows 
workflows to be responsive to the progress of hu-
man tasks (e.g. by redirecting a task that is taking 
too long).   
3.2 An MT Web Service 
As part of our work within CNGL in the devel-
opment of a Localisation Factory we have engi-
neered a web service capable of leveraging transla-
tions from multiple automated translation compo-
nents.  The service operates by taking in an XLIFF 
document, iterating the segments of the document 
and getting a translation from each of the transla-
tion components for each segment.  These transla-
tions are attached to the segment within the XLIFF 
and the service returns the final XLIFF document 
back to the client.  The service can be configured 
to use any permutation of the automated translation 
components depending on the workflow in which 
the service finds itself operating.  Some translation 
components may be inappropriate in a given 
workflow context and may be removed.  The ser-
vice also allows for the weighting of translations 
coming from different translation components so 
that certain translations are preferred above others. 
The service implementation leverages transla-
tion from two open web based translation systems 
Microsoft Live Translator [mslive] and Yahoo Ba-
belfish [babelfish].  Microsoft Live Translator can 
be accessed through a web service interface.  Ya-
hoo Babelfish has no web service interface so get-
ting back translations is implemented through a 
screen-scraping technique on the HTML document 
returned.   
51
The service also makes use of MaTrEx [ma-
trex], a hybrid statistical/example-based machine 
translation system developed by our partner uni-
versity Dublin City University. MaTreX makes use 
of the open-source Moses decoder [moses]. Trans-
lation models are created using MaTreX and are 
passed to the Moses decoder which performs that 
translation from source to target language. We took 
the Moses decoder and wrapped it in a web ser-
vice.  The web service pipes segments for transla-
tion to Moses which responds with translations.  
This translation model is produced based on 
aligned source and target corpora of content repre-
sentative of the content passing through the 
workflow. 
Finally we have taken a translation memory 
product LanguageExchange from Alchemy, an 
industrial partner within the project, and added that 
to the list of automated translation components 
available to our service.  This allows any previous 
human translations to be leveraged during the au-
tomated translation process. 
The service is engineered using Business 
Process Execution Language (BPEL) to orchestrate 
the calling of the various translation components 
that compose the service.  BPEL allows those 
managing the service to easily compose a particu-
lar configuration of the service.  Translation com-
ponents can be easily added or removed from the 
service.  The tool support around BPEL means that 
the user does not need a background in program-
ming to  develop a particular configuration of the 
components. 
One problem we encountered implementing the 
MT service as a wrapper around existing compo-
nents was that they are unable to handle internal 
markup within the segments.  Segments passing 
through a localisation workflow are likely to con-
tain markup to indicate particular formatting of the 
text.  The machine translation components are only 
able to handle free text and the markup is not pre-
served during translation. Another problem en-
countered in using free web services over the In-
ternet was that implementations did not encourage 
volume invocations, with source IP addresses re-
questing high volumes being blacklisted. 
 
3.3 A Text Analytics  Web Service 
We have implemented a generic text-
categorization service to provide text-analytic sup-
port for localization workflows.  It takes an XLIFF 
file as input and produces an XLIFF file as output, 
transforming it by adding meta-data (a type 3 
transform). The meta-data can be added either on a 
file-basis or on a segment basis, depending on the 
requirements of the workflow as expressed in the 
service?s configuration. The service provides a 
simple and generic XLIFF transformation as part 
of the localization workflow, while the manage-
ment interface provides flexible configurability.  
The management interface is designed in order 
to support multiple text analytic engines, each of 
which can support multiple categorization schema 
at once.  Our implementation uses two text en-
gines, the open source TextCat package [textcat] 
and IBM?s Fragma software [fragma].  The follow-
ing operations are provided by the service:  
 
Operation createSchema: The createSchema 
function creates a new categorisation schema based 
on a provided set of training data, which can op-
tionally be provided by an RSS feed for ongoing 
training data updates.  
Operation getEngines: This returns a list (en-
coded in XML) of the categorisation engines that 
are available to the Service. This allows the client 
to specify that a specific categorisation engine be 
used in subsequent requests. 
Operation viewSchema: This returns a list of the 
categories contained within a schema (and the de-
tails of the engine that was used to create it). 
Operation addData: This operation adds a piece 
of training data to a categorisation schema - i.e. it 
allows components to tell the service that a piece 
of text has a known category of categoryID accord-
ing to the schema with schemaID. 
Operation categorise: This provides a categorisa-
tion of text provided as an XLIFF segment, accord-
ing to a specified schema taken form the list sup-
ported by the service. 
3.4 A Crowd-sourcing Web Service 
In order to allow the localization workflow to in-
corporate crowd-sourcing, by which we mean col-
laborative input from a volunteer web-based user-
community, we have designed and implemented a 
web-service interface. This interface is designed to 
52
allow stages in the localization job to be handed 
off to such a community.  From the point of view 
of the workflow, the important thing is that the 
localisation requirements can be adequately speci-
fied and that the status of the job can be ascer-
tained by other elements in the workflow ? allow-
ing them to react to the progress (or lack thereof) 
in the task and, for example, to allow the job to be 
redirected to another process when it is not pro-
gressing satisfactorily.  
Our service design is focused on supporting 
crowd-sourcing, but it is intended to extend it to 
offer general-purpose support for the integration of 
human-tasks into a BPEL workflow.  It serves as a 
testbed and proof of concept for the development 
of a generic localization human task interface. The 
initial specification has been derived from the 
TWS specification [tws], but incorporates several 
important changes. Firstly, it is greatly simplified 
by removing all the quote-related functions and 
replacing them with the RequestJob and SubmitJob 
functions and combining all of the job control 
functions into a single updateJob function and 
combining the two job list functions into one. 
TWS, as a standard focused on support for lo-
calization outsourcing ? hence the concentration on 
negotiating ?quotes? between partners.  Our re-
quirements are quite different ? we cannot assume 
that there is any price, or even any formal agree-
ment which governs crowd-sourcing.  Indeed, in 
general, a major problem with TWS which hin-
dered its uptake is that it assumed a particular 
business model ? in practice localization jobs are 
not so automated, nor so quick that automated 
price negotiation is a particularly desired feature.  
Such information can be incorporated into a Job 
Description data structure, but a generic human-
task interface should not assume any particular 
business model ? hence the significant changes 
between our API and that of TWS.  Nevertheless, 
there is much clear and well-structured thinking 
contained in the TWS standard ? how best to de-
scribe language pairs, jobs and various other com-
monly referenced ideas in a localization workflow.  
By using TWS as a base, we can take advantage of 
all of that work rather than designing our own da-
ta-structures from scratch. The main operation are 
as follows: 
Operation requestJob: The JobDescription input 
parameter is an XML format which contains de-
tails of the job that is being requested. The returned 
datatype is the details of the job that is offered by 
the service. These are not necessarily the same. For 
example, the requested job might contain several 
language pairs, but the returned description might 
not contain all of these language pairs as some of 
those requested might not be available in the ser-
vice. Generally, it can be assumed that the service 
will make its ?best effort? to fulfill the require-
ments and the returned data will be as close as it 
can get to the requirements submitted.  
Operation submitJob: This operation works ex-
actly as the one above, except for the fact that it 
submits the job to the service with the particular 
JobDescription required and receives back the 
JobDescription that will actually be carried out.  
Operation retrieveJobList: This accepts a Job-
Description  input parameter, an XML format 
which contains a ?filter? on the various active jobs. 
The operation will return a list of all of the jobs 
which match that specified in the JobdDescription 
argument.  
Operation updateJob: A JobDescription input 
parameter is an XML format which contains a de-
scription of the various changes to the job that are 
being requested. The function will return a descrip-
tion which details the new, updated state of the job 
(note that the service does not have to follow all 
the requested changes and might ignore them).  
Operation retrieveJob:  A JobDescription input 
parameter is an XML format which contains a ?fil-
ter? on the various jobs. The operation returns a 
URI from which the client can retrieve the loca-
lised content corresponding to the filters. 
Operation associateResource: This functions as-
sociates a resource (TM / Glossary / etc) with a 
particular job. The returned value is the URI of the 
resource (which may be different than the passed 
ResURI). The types of resource supported will 
need to be decided upon.  
? Future Work: Translation Quality 
The next challenge to applying these techniques 
to workable industrial workflows is to fully ad-
dress the metrology of such workflows. The cur-
rent approach does not support the instrumentation 
of web services to provide quality measurements. 
Further, such quality measures need to be provided 
in a way that is relevant to the quality of the 
workflow as a whole and the business-driven key 
performance indicators which it aims to support.  
53
However, the integration of translation quality 
metrics across different forms of workflow and 
different industrial workflow components and lin-
guistic technologies has been widely identified as 
requiring considerable further investigation. Even 
the most basic metric used in commercial 
workflow, the word count against which transla-
tion effort is estimated, is calculated differently by 
different workflow systems. This particular case 
has already been addressed by LISA though its 
proposal for Global information management Me-
trics eXchange (GMX) [gmx].  
It is hardly surprising, therefore, that closing the 
gap between the metrics typically used by MT sys-
tem developers and what is needed to support the 
use of MT in commercial localization workflows is 
likely to be even more challenging. For example, 
metrics such as BLEU [bleu] are well-understood 
by MT developers used to participating in large-
scale open MT evaluations such as NIST; a BLEU 
score of 0.8 (say) means either that one?s MT sys-
tem is extremely good, or that the task is quite 
simple, or both, or even that there are a large num-
ber of reference translations against which the sys-
tem output is being compared. On the other hand, a 
score of 0.2 means that the quality is poor, that 
there is probably only one reference translation 
against which candidate translations are being eva-
luated, or that the task is a very complex one.  
However, neither score means anything (much) 
to a potential user. In the localization industry, 
Translation Memory is much more widely used, 
and there users and vendors use a different metric, 
namely fuzzy match score, i.e. how closely a pre-
viously translated source sentence matches the cur-
rent input string. Users typically ?know? that a 
score of around 70% fuzzy match is useful, whe-
reas for a lower scored sentence it is likely to be 
quicker to translate this from scratch.   
One of our research goals in the CNGL is to 
bring these two communities closer together by 
developing a translation quality metric that speaks 
to both sets of people, developers and users. One 
step in the right direction might be the Translation 
Edit Rate metric [ter], which measures the number 
of editing commands (deletions, substitutions, and 
insertions) that need to be carried out in order to 
transform the MT output into the reference transla-
tion(s). This is being quite widely used in the MT 
community (cf. the Global Autonomous Language 
Exploitation (GALE) project) by MT developers, 
and speaks a language that users understand well. 
User studies will very much inform the directions 
that such research will take, but there are reasons 
to believe that the gap can be bridged.   
Supposing then that such hurdles can be over-
come, broadly speaking, the quality of a translation 
process might be dependent on multiple factors, 
each of which could be measured both intrinsically 
and extrinsically, including; 
? Source and destination languages 
? Content domain 
? Diversity of vocabulary  
? Repetitiveness of text 
? Length and complexity of sentences 
? Availability of relevant translation memories 
? The cost and time incurred per translated word 
 
Often control of quality of the translation process 
can be impacted most directly by the quality of the 
human translators and the degree of control exerted 
over the source text. Different levels of linguistic 
quality assurance may be undertaken and post-
editors (who are often more experienced translators 
and therefore more expensive) are involved in 
handling incomplete or missing translations. How-
ever, even in professional translation environ-
ments, translation quality is regarded as relatively 
subjective and exact measurement of the quality of 
translation is therefore problematic. 
? Conclusion 
In this paper we have discussed some the chal-
lenges faced in taking a web service integration 
and orchestration approach to the development of 
next generation localization workflows. Based on 
our experiences of using these approaches to inte-
grate both existing localization products and cut-
ting edge research prototypes in MT , TA and 
crowd-sourcing, new, innovative localisation 
workflows can be rapidly assembled. The maturity 
of the BPEL standard and the design of general 
purpose, reusable web service interfaces are key to 
this success.  
 
Acknowledgments: This research is supported 
by the Science Foundation Ireland (Grant 
07/CE/I1142) as part of the Centre for Next Gener-
ation Localisation (www.cngl.ie) at Trinity College 
Dublin. 
54
References  
[babelfish] Yahoo Babelfish Machine Translation 
http://babelfish.yahoo.com/ 6th Feb 2009 
 [drupal] Drupal Content Management System 
http://www.drupal.org 6th Feb 2009 
[bleu] Kishore Papineni, Salim Roukos, Todd Ward and 
Wei-Jing Zhu. 2002. In 40th Annual Meeting of the 
Association for Computational Linguistics, Philadel-
phia, PA., pp.311?318. 
[bpel] Web Services Business Process Execution Lan-
guage Version 2.0, OASIS Standard, 11 April 2007, 
Downloaded from http://docs.oasis-
open.org/wsbpel/2.0/OS/wsbpel-v2.0-0S.html 6th 
Feb 2009 
[erl] Erl, Thomas, Service-oriented Architecture: Con-
cepts, Technology, and Design. Upper Saddle River: 
Prentice Hall  2005 
[foster] Foster, I., Parastatidis, S., Watson, P., and 
Mckeown, M. 2008. How do I model state?: Let me 
count the ways. Commun. ACM 51, 9 (Sep. 2008), 
34-41. 
[fragma] Alexander Troussov, Mayo Takeuchi, 
D.J.McCloskey, 
http://atroussov.com/uploads/TSD2004_LangID_wor
d_fragments.pdf 6th Feb 2009 
[gmx] Global Information Management Metrics Vo-
lume (GMX-V) 1.0 Specification Version 1.0, 26 
February 2007, downloaded from http://www.xml-
intl.com/docs/specification/GMX-V.html on 6th Feb 
2009 
[langexchange] Alchemy Language Exchange 
http://www.alchemysoftware.ie/products/alchemy_la
nguage_exchange.html 6th Feb 2009 
[matrex] MaTrEx Machine Translation - John Tinsley, 
Yanjun Ma, Sylwia Ozdowska, Andy Way. 
http://doras.dcu.ie/559/1/Tinsleyetal_WMT08.pdf  
[moses] Moses decoder http://www.statmt.org/moses/ 
9th March 2009 
[mslive] Microsoft Live Translator 
http://www.windowslivetranslator.com/ 6th Feb 2009 
[ter] Matt Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study of 
Translation Edit Rate with Targeted Human Annota-
tion. In Proceedings of the 7th Conference of the As-
sociation for Machine Translation in the Americas, 
Cambridge, MA., pp.223?231. 
 [textcat] Java Text Categorisation 
http://textcat.sourceforge.net/ 6th Feb 2009 
 [tbx] Termbase eXchange Format 
http://www.lisa.org/Term-Base-eXchange.32.0.html 
6th March 2009 
 [tmx] Translation Memory eXchange 
http://www.lisa.org/Translation-Memory-e.34.0.html 
6th March 2009 
[tws] Translation Web Services Specification: 
http://www.oasis-
open.org/committees/download.php/24350/trans-ws-
spec-1.0.3.html 
[vanderaalst] Van Der Aalst, W.M.P. Ter Hofstede, 
A.H.M. ?YAWL: Yet another workflow language? In-
formation Systems, Volume 30, Issue 4, June 2005, 
Pages 245-275   
[xliff] XML Localisation Interchange File Format 
http://docs.oasis-open.org/xliff/v1.2/os/xliff-
core.html 6th March 2009 
55
