Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 64?69,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Improvements to Monolingual English Word Sense Disambiguation?
Weiwei Guo
Computer Science Department
Columbia University
New York, NY, 10115, USA
wg2162@cs.columbia.edu
Mona T. Diab
Center for Computational Learning Systems
Columbia University
New York, NY 10115, USA
mdiab@ccls.columbia.edu
Abstract
Word Sense Disambiguation remains one of
the most complex problems facing compu-
tational linguists to date. In this paper we
present modification to the graph based state
of the art algorithm In-Degree. Our modifi-
cations entail augmenting the basic Lesk sim-
ilarity measure with more relations based on
the structure of WordNet, adding SemCor ex-
amples to the basic WordNet lexical resource
and finally instead of using the LCH similarity
measure for computing verb verb similarity in
the In-Degree algorithm, we use JCN. We re-
port results on three standard data sets using
three different versions of WordNet. We re-
port the highest performing monolingual un-
supervised results to date on the Senseval 2 all
words data set. Our system yields a perfor-
mance of 62.7% using WordNet 1.7.1.
1 Introduction
Despite the advances in natural language process-
ing (NLP), Word Sense Disambiguation (WSD) is
still considered one of the most challenging prob-
lems in the field. Ever since the field?s inception,
WSD has been perceived as one of the central prob-
lems in NLP as an enabling technology that could
potentially have far reaching impact on NLP appli-
cations in general. We are starting to see the be-
ginnings of a positive effect of WSD in NLP appli-
cations such as Machine Translation (Carpuat and
Wu, 2007; Chan et al, 2007). Advances in re-
search on WSD in the current millennium can be
attributed to several key factors: the availability of
large scale computational lexical resources such as
?The second author has been partially funded by DARPA
GALE project. We would also like to thank the useful com-
ments rendered by three anonymous reviewers.
WordNets (Fellbaum, 1998; Miller, 1990), the avail-
ability of large scale corpora, the existence and dis-
semination of standardized data sets over the past 10
years through the different test beds of SENSEVAL
and SEMEVAL competitions,1 devising more robust
computing algorithms to handle large scale data sets,
and simply advancement in hardware machinery.
In this paper, we address the problem of WSD of
all the content words in a sentence. In this frame-
work, the task is to associate all tokens with their
contextually relevant meaning definitions from some
computational lexical resource. We present an en-
hancement on an existing graph based algorithm, In-
Degree, as described in (Sinha and Mihalcea, 2007).
Like the previous work, our algorithm is unsuper-
vised. We show significant improvements over pre-
vious state of the art performance on several exist-
ing data sets, SENSEVAL2, SENSEVAL3 and SE-
MEVAL.
2 Word Sense Disambiguation
The definition of WSD has taken on several different
meanings in recent years. In the latest SEMEVAL
(2007) workshop, there were 18 tasks defined, sev-
eral of which were on different languages, however
we notably recognize the widening of the defini-
tion of the task of WSD. In addition to the tradi-
tional all words and lexical sample tasks, we note
new tasks on word sense discrimination (no sense
inventory is needed, the different senses are merely
distinguished), lexical substitution using synonyms
of words as substitutes, as well as meaning defini-
tions obtained from different languages namely us-
ing words in translation.
Our paper is about the classical all words task of
WSD. In this task, all the content bearing words in a
running text are disambiguated from a static lexical
1http://www.semeval.org
64
resource. For example a sentence such as I walked
by the bank and saw many beautiful plants there.
will have the verbs walked, saw, the nouns bank,
plants, the adjectives many, beautiful, and the ad-
verb there, be disambiguated from a standard lexi-
cal resource. Hence using WordNet,2 walked will be
assigned the meaning to use one?s feet to advance;
advance by steps, saw will be assigned the meaning
to perceive by sight or have the power to perceive
by sight, the noun bank will be assigned the mean-
ing sloping land especially the slope beside a body
of water and so on.
3 Related Works
Many systems over the years have been used for the
task. A thorough review of the current state of the
art is in (Navigli, 2009). Several techniques have
been used to tackle the problem ranging from rule
based/knowledge based approaches to unsupervised
and supervised machine learning approaches. To
date, the best approaches that solve the all words
WSD task are supervised as illustrated in the dif-
ferent SenseEval and SEMEVAL All Words tasks
(M. Palmer and Dang, 2001; Snyder and Palmer,
2004; Pradhan et al, 2007).
In this paper, we present an unsupervised ap-
proach to the all words WSD problem relying on
WordNet similarity measures. We will review only
three of the most relevant related research due to
space limitations. We acknowledge the existence of
many research papers that tackled the problem using
unsupervised approaches.
Firstly, in work by (Pedersen and Patwardhan,
2005), the authors investigate different word simi-
larity measures as a means of disambiguating words
in context. They compare among different similar-
ity measures. They show that using an extension on
the Lesk similarity measure (Lesk, 1986) between
the target words and their contexts and the contexts
of those of the WordNet synset entries (Gloss Over-
lap), outperforms all the other similarity measures.
Their approach is unsupervised. They exploit the
different relations in WordNet. They also go beyond
the single word overlap, they calculate the overlap
in n-grams. They report results on the English Lex-
ical sample task from Senseval 2 which comprised
2http://wordnet.princeton.edu
nouns, verbs and adjectives. The majority of the
words in this set is polysemous. They achieve an
F-measure of 41.2% on nouns, 21.2% on verbs, and
25.1% on adjectives.
The second related work to ours is the work by
(Mihalcea, 2005). Mihalcea (2005) introduced a
graph based unsupervised technique for all word
sense disambiguation. Similar to the previous study,
the author relied on the similarity of the WordNet
entry glosses using the Lesk similarity measure. The
study introduces a graph based sequence model of
the problem. All the open class words in a sentence
are linked via an undirected graph where all the pos-
sible senses are listed. Then dependency links are
drawn between all the sense pairs. Weights on the
arcs are determined based on the semantic similar-
ity using the Lesk measure. The algorithm is basi-
cally to walk the graph and find the links with the
highest possible weights deciding on the appropri-
ate sense for the target words in question. This algo-
rithm yields an overall F-score of 54.2% on the Sen-
seval 2 all words data set and an F-score of 64.2%
on nouns alone.
Finally, the closest study relevant to the current
paper yields state of the art performance is an un-
supervised approach described in (Sinha and Mihal-
cea, 2007). In this work, the authors combine dif-
ferent semantic similarity measures with different
graph based algorithms as an extension to work in
(Mihalcea, 2005). The authors proposed a graph-
based WSD algorithm. Given a sequence of words
W = {w1, w2...wn}, each word wi with several
senses {si1, si2...sim}. A graph G = (V,E) is defined
such that there exists a vertex v for each sense. Two
senses of two different words may be connected by
an edge e, depending on their distance. That two
senses are connected suggests they should have in-
fluence on each other, so normally a maximum al-
lowable distance is set. They explore 4 different
graph based algorithms. The highest yielding algo-
rithm in their work is the In-Degree algorithm
combining different WordNet similarity measures
depending on POS. They used the Jiang and Conrath
(JCN) (Jiang and Conrath., 1997) similarity mea-
sure within nouns, the Leacock & Chodorow (LCH)
(Leacock and Chodorow, 1998) similarity measure
within verbs, and the Lesk (Lesk, 1986) similarity
measure within adjectives and within adverbs and
65
across different POS tags. They evaluate their work
against the Senseval 2 all words task. They tune the
parameters of their algorithm ? specifically the nor-
malization ratio for some of these measures ? based
on the Senseval 3 data set. They report a state of the
art unsupervised system that yields an overall per-
formance of 57.2%.
4 Our Approach
In this paper, we extend the (Sinha and Mihalcea,
2007) work (hence forth SM07) in some interesting
ways. We focus on the In-Degree graph based
algorithm as it was the best performer in the SM07
work. The In-Degree algorithm presents the prob-
lem as a weighted graph with senses as nodes and
similarity between senses as weights on edges. The
In-Degree of a vertex refers to the number of edges
incident on that vertex. In the weighted graph, the
In-Degree for each vertex is calculated by summing
the weights on the edges that are incident on it.
After all the In-Degree values for each sense is
computed, the sense with maximum value is cho-
sen as the final sense for that word. SM07 com-
bine different similarity measures. They show that
best combination is JCN for noun pairs and LCH
for verb pairs, and Lesk for within adjectives and
within adverbs and also across different POS, for ex-
ample comparing senses of verbs and nouns. Since
different similarity measures use different similarity
scales, SM07 did not directly use the value returned
from the similarity metrics. Instead, the values were
normalized. Lesk value is observed in a range from
0 to an arbitrary value, so values larger than 240
were set to 1, and the rest is mapped to an interval
[0,1]. Similarily JCN and LCH were normalized to
the interval from [0,1].3
In this paper, we use the basic In-Degree algo-
rithm while applying some modifications to the ba-
sic similarity measures exploited and the WordNet
lexical resource. Similar to the original In-Degree
algorithm, we produce a probabilistic ranked list of
senses. Our modifications are described as follows:
JCN for Verb-Verb Similarity In our implemen-
tation of the In-Degree algorithm, we use the JCN
similarity measure for both Noun-Noun similarity
3These values were decided on based on calibrations on the
SENSEVAL 3 data set.
calculation similar to SM07. In addition, instead of
using LCH for Verb-Verb similarity, we use JCN for
Verb Verb similarity based on our empirical obser-
vation on SENSEVAL 3 data, JCN yields better per-
formance than when employing LCH among verbs.
Expand Lesk Following the intuition in (Peder-
sen and Patwardhan, 2005) ? henceforth (PEA05)
? we expand the basic Lesk similarity measure to
take into account the glosses for all the relations
for the synsets on the contextual words and com-
pare them with the glosses of the target word senses,
hence going beyond the is-a relation. The idea is
based on the observation that WordNet senses are
too fine-grained, therefore the neighbors share a lot
of semantic meanings. To find similar senses, we
use the relations: hypernym, hyponym, similar at-
tributes, similar verb group, pertinym, holonym, and
meronyms.4 The algorithm assumes that the words
in the input are POS tagged. It is worth noting the
differences between our algorithm and the PEA05
algorithm, though we take our cue from it. In
PEA05, the authors retrieve all the relevant neigh-
bors to form a large bag of words for both the target
sense and the surrounding sense and they specifi-
cally focus on the Lesk similarity measure. In our
current work, we employ the neighbors in a dis-
ambiguation strategy using different similarity mea-
sures one pair at a time.
This algorithm takes as input a target sense and a
sense pertaining to a word in the surrounding con-
text, and returns a sense similarity score. It is worth
noting that we do not apply the WN relations ex-
pansion to the target sense. It is only applied to the
contextual word. We experimented with expanding
both the contextual sense and the target sense and
we found that the unreliability of some of the rela-
tions is detrimental to the algorithm?s performance.
Hence we decided empirically to expand only the
contextual word.
We employ the same normalization values used in
SM07 for the different similarity measures. Namely
for the Lesk and Expand-Lesk we use the same cut
off value of 240, accordingly, if the Lesk or Expand-
Lesk similarity value returns 0 <= 240 it is con-
4We have run experiments varying the number of relations to
employ and they all yielded relatively similar results. Hence in
this paper, we report results using all the relations listed above.
66
verted to a real number in the interval [0,1], any sim-
ilarity over 240 is by default mapped to a 1. For
JCN, similar to SM07, the values are from 0.04 to
0.2, we mapped them to the interval [0,1]. It is worth
noting that we did not run any calibration studies be-
yond the what was reported in SM07.
SemCor Expansion of WordNet A basic part of
our approach relies on using the Lesk algorithm. Ac-
cordingly, the availability of glosses associated with
the WordNet entries is extremely beneficial. There-
fore, we expand the number of glosses available
in WordNet by using the SemCor data set, thereby
adding more examples to compare. The SemCor
corpus is a corpus that is manually sense tagged
(Miller, 1990). In this expansion, depending on the
version of WordNet, we use the sense-index file in
the WordNet Database to convert the SemCor data to
the appropriate version sense annotations. We aug-
ment the sense entries for the different POS Word-
Net databases with example usages from SemCor.
The augmentation is done as a look up table external
to WordNet proper since we did not want to dabble
with the WordNet offsets. We set a cap of 30 addi-
tional examples per synset. Many of the synsets had
no additional examples. A total of 26875 synsets in
WordNet 1.7.1 and a total of 25940 synsets are aug-
mented with SemCor examples.5
5 Experiments and Results
5.1 Data
We experiment with all the standard data sets,
namely, Senseval 2 (SV2) (M. Palmer and Dang,
2001), Senseval 3 (SV3) (Snyder and Palmer, 2004),
and SEMEVAL (SM) (Pradhan et al, 2007) English
All Words data sets. We used the true POS tag sets
in the test data as rendered in the Penn Tree Bank.
We exclude the data points that have a tag of ?U?
in the gold standard since our system does not al-
low for an unknown option (i.e. it has to produce a
sense tag). We present our results on 3 versions of
WordNet (WN), 1.7.1 for ease of comparison with
previous systems, 2.1 for SEMEVAL data, and 3.0
in order to see whether the trends in performance
hold across WN versions.
5It is worth noting that some example sentences are repeated
across different synsets and POS since the SemCor data is an-
notated as an All-Words tagged data set.
5.2 Evaluation Metrics
We use the scorer2 software to report fine-
grained (P)recision and (R)ecall and (F)-measure on
the different data sets.
5.3 Baselines
We consider here the two different baselines. 1. A
random baseline (RAND) is the most appropriate
baseline for an unsupervised approach. We consider
the first sense baseline to be a supervised baseline
since it depends crucially on SemCor in ranking the
senses within WordNet.6 It is worth pointing out that
our algorithm is still an unsupervised algorithm even
though we use SemCor to augment WordNet since
we do not use any annotated data in our algorithm
proper. 2. The SM07 baseline which we consider
our true baseline.
5.4 Experimental Conditions
We explore 4 different experimental conditions:
JCN-V which uses JCN instead of LCH for verb-
verb similarity comparison, we consider this our
base condition; +ExpandL is adding the Lesk Ex-
pansion to the base condition; +SemCor adds the
SemCor expansion to the base condition; and finally
+ExpandL SemCor, adds the latter both conditions
simultaneously.
5.5 Results
Table 1 illustrates the obtained results on the three
data sets reporting only overall F-measure. The cov-
erage for SV2 is 98.36% losing some of the verb and
adverb target words. The coverage for SV3 is 99.7%
and that of SM is 100%. These results are on the
entire data set as described in Table ??. Moreover,
Table 2 presents the detailed results for the Senseval
2 data set using WN 1.7.1 since it is the most stud-
ied data set and for ease of comparison with previ-
ous studies. We break the results down by POS tag
(N)oun, (V)erb, (A)djective, and Adve(R)b.
6From an application standpoint, we do not find the first
sense baseline to be of interest since it introduces a strong level
of uniformity ? removing semantic variability ? that is not de-
sirable. Even if the frist sense achieves higher results in these
data sets, it is an artifact of the size of the data and the very
limited number of documents under investigation.
67
Condition SV2-WN171 SV2-WN30 SV3-WN171 SV3-WN30 SM-WN2.1 SM-WN30
RAND 39.9 41.8 32.9 33.4 25.4
SM07 59.7 59.8 54 53.8 40.4 40.8
JCN-V 60.2 60.2 55.9 55.5 44.1 45.5
+ExpandL 60.9 60.6 55.7 55.5 43.7 45.1
+SemCor 62.04 62.2 59.7 60.3 46.8 46.8
+ExpandL SemCor 62.7 62.9 59.5 59.6 45.9 45.7
Table 1: F-measure % for all experimental conditions on all data sets
Condition N V A R
RAND 43.7 21 41.2 57.4
SM07 68.7 33.01 65.2 63.1
JCN-V 68.7 35.46 65.2 63.1
+ExpandL 70 35.86 65.6 62.8
+SemCor 68.3 37.86 68.6 68.75
+ExpandL SemCor 69.5 38.66 68.2 69.15
Table 2: F-measure results per POS tag per condition for SV2 using WN 1.7.1.
6 Discussion
Our overall results on all the data sets clearly out-
perform the baseline as well as state of the art per-
formance using an unsupervised system (SM07) in
overall accuracy across all the data sets. Our im-
plementation of SM07 is slightly higher than those
reported in (Sinha and Mihalcea, 2007), 57.12% is
probably due to the fact that we do not consider the
items tagged as ?U? and also we resolve some of
the POS tag mismatches between the gold set and
the test data. We note that for the SV2 data set our
coverage is not 100% due to some POS tag mis-
matches that could not have been resolved automat-
ically. These POS tag problems have to do mainly
with multiword expressions and the like.
In observing the performance of the overall sys-
tem, we note that using JCN for verbs clearly outper-
forms using the LCH similarity measure across the
board on all data sets as illustrated in Table 1. Us-
ing SemCor to augment WordNet examples seems to
have the biggest impact on SV3 and SM compared
to ExpandL. This may be attributed to the fact that
the percentage of polysemous words in the latter two
sets is much higher than it is for SV2. Combining
SemCor with ExpandL yields the best results for the
SV2 data sets. There seems to be no huge notable
difference between the three versions of WN, though
WN3.0 seems to yield slightly higher results maybe
due to higher consistency in the overall structure
when comparing WN1.7.1, WN2.1, and WN3.0. We
do recognize that we can?t directly compare the var-
ious WordNets except to draw conclusions on struc-
tural differences remotely. It is also worth noting
that less words in WN3.0 used SemCor expansions.
Observing the results yielded per POS in Table
2, ExpandL seems to have the biggest impact on
the Nouns only. This is understandable since the
nouns hierachy has the most dense relations and the
most consistent ones. SemCor augmentation of WN
seemed to benefit all POS significantly except for
nouns. In fact the performance on the nouns deteri-
orated from the base condition JCN-V from 68.7 to
68.3%. This maybe due to inconsistencies in the an-
notations of nouns in SemCor or the very fine granu-
larity of the nouns in WN. We know that 72% of the
nouns, 74% of the verbs, 68.9% of the adjectives,
and 81.9% of the adverbs directly exploited the use
of SemCor augmented examples. Combining Sem-
Cor and ExpandL seems to have a positive impact
on the verbs and adverbs, but not on the nouns and
adjectives. These trends are not held consistently
across data sets. For example, we see that SemCor
augmentation helps both all POS tag sets over using
ExpandL alone or when combined with SemCor. In
order to analyze this further, we explore the perfor-
mance on the polysemous POS only in all the data
sets. We note that the same trend persists, SemCor
augmentation has a negative impact on the SV2 data
set in both WN 1.7.1. and WN 3.0. yet it benefits
all POS in the other data sets, namely SV3 WN1.7.1
and SV3 WN3.0, SM WN2.1 and SM WN3.0.
We did some basic data analysis on the items we
are incapable of capturing. Several of them are cases
68
of metonymy in examples such as ?the English are
known...?, the sense of English here is clearly in ref-
erence to the people of England, however, our WSD
system preferred the language sense of the word. If
it had access to syntactic/semantic role we would as-
sume it could capture that this sense of the word
entails volition for example. Other types of errors
resulted from the lack of a method to help identify
multiwords.
7 Conclusions and Future Directions
In this paper, we presented improvements on state
of the art monolingual all words WSD using a well
established graph based algorithm coupled with en-
hancements on basic similarity measures. We also
explored the impact of augmenting WordNet with
more gloss examples from a hand annotated re-
source as a means of improving WSD performance.
We present the best results to date for an unsuper-
vised approach on standard data sets: Senseval 2
(62.7%) using WN1.7.1, and Senseval 3 (59.7%) us-
ing WN1.7.1. In the future, we would like to explore
the incorporation of multiword chunks, document
level lexical chains, and syntactic features in the
modeling of the Lesk overlap measure. We would
like to further explore why ExpandL conditions did
not yield the expected high performance across the
different POS tags. Moreover, we are still curious
as to why SemCor expansion did not help the nouns
performance in SV2 conditions specifically.
References
Marine Carpuat and Dekai Wu. 2007. Improving statisti-
cal machine translation using word sense disambigua-
tion. In Proceedings of the 2007 Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP-CoNLL), pages 61?72, Prague, Czech Re-
public, June. Association for Computational Linguis-
tics.
Yee Seng Chan, Hwee Tou Ng, and David Chiang. 2007.
Word sense disambiguation improves statistical ma-
chine translation. In Proceedings of the 45th Annual
Meeting of the Association of Computational Linguis-
tics, pages 33?40, Prague, Czech Republic, June. As-
sociation for Computational Linguistics.
Christiane Fellbaum. 1998. ?wordnet: An electronic lex-
ical database?. MIT Press.
J. Jiang and D. Conrath. 1997. Semantic similarity based
on corpus statistics and lexical taxonomy. In Proceed-
ings of the International Conference on Research in
Computational Linguistics, Taiwan.
C. Leacock and M. Chodorow. 1998. Combining lo-
cal context and wordnet sense similarity for word
sense identification. In WordNet, An Electronic Lex-
ical Database. The MIT Press.
M. Lesk. 1986. Automatic sense disambiguation using
machine readable dictionaries: How to tell a pine cone
from an ice cream cone. In In Proceedings of the SIG-
DOC Conference, Toronto, June.
S. Cotton L. Delfs M. Palmer, C. Fellbaum and H. Dang.
2001. English tasks: all-words and verb lexical sam-
ple. In In Proceedings of ACL/SIGLEX Senseval-2,
Toulouse, France, June.
Rada Mihalcea. 2005. Unsupervised large-vocabulary
word sense disambiguation with graph-based algo-
rithms for sequence data labeling. In Proceedings of
Human Language Technology Conference and Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 411?418, Vancouver, British Columbia,
Canada, October. Association for Computational Lin-
guistics.
George A. Miller. 1990. Wordnet: a lexical database for
english. In Communications of the ACM, pages 39?41.
Roberto Navigli. 2009. Word sense disambiguation:
a survey. In ACM Computing Surveys, pages 1?69.
ACM Press.
Banerjee Pedersen and Patwardhan. 2005. Maximiz-
ing semantic relatedness to perform word sense disam-
biguation. In University of Minnesota Supercomputing
Institute Research Report UMSI 2005/25, Minnesotta,
March.
Sameer Pradhan, Edward Loper, Dmitriy Dligach, and
Martha Palmer. 2007. Semeval-2007 task-17: En-
glish lexical sample, srl and all words. In Proceed-
ings of the Fourth International Workshop on Seman-
tic Evaluations (SemEval-2007), pages 87?92, Prague,
Czech Republic, June. Association for Computational
Linguistics.
Ravi Sinha and Rada Mihalcea. 2007. Unsupervised
graph-based word sense disambiguation using mea-
sures of word semantic similarity. In Proceedings of
the IEEE International Conference on Semantic Com-
puting (ICSC 2007), Irvine, CA.
Benjamin Snyder and Martha Palmer. 2004. The english
all-words task. In Rada Mihalcea and Phil Edmonds,
editors, Senseval-3: Third International Workshop on
the Evaluation of Systems for the Semantic Analysis of
Text, pages 41?43, Barcelona, Spain, July. Association
for Computational Linguistics.
69
Committed Belief Annotation and Tagging
Mona T. Diab Lori Levin
CCLS LTI
Columbia U. CMU
mdiab@cs.columbia.edu lsl@cs.cmu.edu
Teruko Mitamura Owen Rambow
LTI CCLS
CMU Columbia U.
teruko+@cs.cmu.edu rambow@ccls.columbia.edu
Vinodkumar Prabhakaran Weiwei Guo
CS CS
Columbia U. Columbia U.
Abstract
We present a preliminary pilot study of
belief annotation and automatic tagging.
Our objective is to explore semantic mean-
ing beyond surface propositions. We aim
to model people?s cognitive states, namely
their beliefs as expressed through linguis-
tic means. We model the strength of their
beliefs and their (the human) degree of
commitment to their utterance. We ex-
plore only the perspective of the author of
a text. We classify predicates into one of
three possibilities: committed belief, non
committed belief, or not applicable. We
proceed to manually annotate data to that
end, then we build a supervised frame-
work to test the feasibility of automati-
cally predicting these belief states. Even
though the data is relatively small, we
show that automatic prediction of a belief
class is a feasible task. Using syntactic
features, we are able to obtain significant
improvements over a simple baseline of
23% F-measure absolute points. The best
performing automatic tagging condition is
where we use POS tag, word type fea-
ture AlphaNumeric, and shallow syntac-
tic chunk information CHUNK. Our best
overall performance is 53.97% F-measure.
1 Introduction
As access to large amounts of textual informa-
tion increases, there is a strong realization that
searches and processing purely based on surface
words is highly limiting. Researchers in infor-
mation retrieval and natural language processing
(NLP) have long used morphological and (in a
more limited way) syntactic analysis to improve
access and processing of text; recently, interest has
grown in relating text to more abstract representa-
tions of its propositional meaning, as witnessed by
work on semantic role labeling, word sense disam-
biguation, and textual entailment. However, there
are more levels to ?meaning? than just proposi-
tional content. Consider the following examples,
and suppose we find these sentences in the New
York Times:1
(1) a. GM will lay off workers.
b. A spokesman for GM said GM will lay off
workers.
c. GM may lay off workers.
d. The politician claimed that GM will lay
off workers.
e. Some wish GM would lay of workers.
f. Will GM lay off workers?
g. Many wonder if GM will lay off workers.
If we are searching text to find out whether GM
will lay off workers, all of the sentences in (1) con-
1In this paper, we concentrate on written communication,
and we use the terms reader and writer. However, nothing in
the approach precludes applying it to spoken communication.
tain the proposition LAYOFF(GM,WORKERS).
However, the six sentences clearly allow us very
different inferences about whether GM will lay off
workers or not. Supposing we consider the Times
a trustworthy news source, we would be fairly cer-
tain with (1a) and (1b). (1c) suggests the Times is
not certain about the layoffs, but considers them
possible. When reading (1d), we know that some-
one else thinks that GM will lay off workers, but
that the Times does not necessarily share this be-
lief. (1e), (1f), and (1g) do not tell us anything
about whether anyone believes whether GM will
lay off workers.
In order to tease apart what is happening, we
need to refine a simple IR-ish view of text as a
repository of propositions about the world. We use
two theories to aid us. The first theory is that in ad-
dition to facts about the world (GM will or will not
lay off workers), we have facts about people?s cog-
nitive states, and these cognitive states relate their
bearer to the facts in the world. (Though perhaps
there are only cognitive states, and no facts about
the world.) Following the literature in Artificial
Intelligence (Cohen and Levesque, 1990), we can
model cognitive state as beliefs, desires, and inten-
tions. In this paper, we are only interested in be-
liefs (and in distinguishing them from desires and
intentions). The second theory is that communi-
cation is intention-driven, and understanding text
actually means understanding the communicative
intention of the writer. Furthermore, communica-
tive intentions are intentions to affect the reader?s
cognitive state ? his or her beliefs, desires, and/or
intentions. This view has been worked out in the
text generation and dialog community more than
in the text understanding community (Mann and
Thompson, 1987; Hovy, 1993; Moore, 1994).
In this paper we are interested in exploring the
following: we would like to recognize what the
text wants to make us believe about various peo-
ple?s cognitive states, including the speaker?s. As
mentioned, we are only interested in people?s be-
lief. In this view, the result of text processing is
not a list of facts about the world, but a list of facts
about different people?s cognitive states.
This paper is part of an on-going research effort.
The goals of this paper are to summarize a pilot
annotation effort, and to present the results of ini-
tial experiments in automatically extracting facts
about people?s beliefs from open domain running
text.
2 Belief Annotation
We have developed a manual for annotating be-
lief, which we summarize here. For more de-
tailed information, we refer to the cited works. In
general, we are interested in the writer?s intention
as to making us believe that various people have
certain beliefs, desires, and intentions. We sim-
plify the annotation in two ways: we are only in-
teretsed in beliefs, and we are only interested in
the writer?s beliefs. This is not because we think
this is the only interesting information in text, but
we do this in order to obtain a manageable anno-
tation in our pilot study. Specifically, we annotate
whether the writer intends the reader to interpret
a stated proposition as the writer?s strongly held
belief, as a proposition which the writer does not
believe strongly (but could), or as a proposition
towards which the writer has an entirely differ-
ent cognitive attitude, such as desire or intention.
We do not annotate subjectivity (Janyce Wiebe and
Martin, 2004; Wilson and Wiebe, 2005), nor opin-
ion (for example: (Somasundaran et al, 2008)):
the nature of the proposition (opinion and type of
opinion, statement about interior world, external
world) is not of interest. Thus, this work is or-
thogonal to the extensive literature on opinion de-
tection. And we do not annotate truth: real-world
(encyclopedic) truth is not relevant.
We have three categories:
? Committed belief (CB): the writer indicates
in this utterance that he or she believes the
proposition. For example, GM has laid off
workers, or, even stronger, We know that GM
has laid off workers.
A subcase of committed belief concerns
propositions about the future, such as GM
will lay off workers. People can have equally
strong beliefs about the future as about the
past, though in practice probably we have
stronger beliefs about the past than about the
future.
? Non-committed belief (NCB): the writer
identifies the propositon as something which
he or she could believe, but he or she hap-
pens not to have a strong belief in. There are
two subcases. First, there are cases in which
the writer makes clear that the belief is not
strong, for example by using a modal auxil-
iary:2 GM may lay off workers. Second, in
reported speech, the writer is not signaling to
us what he or she believes about the reported
speech: The politician claimed that GM will
lay off workers. However, sometimes, we can
use the speech act verb to infer the writer?s
attitude,3 and we can use our own knowledge
2The annotators must distinguish epistemic and deontic
uses of modals.
3Some languages may also use grammatical devices; for
to infer the writer?s beliefs; for example, in
A GM spokesman said that GM will lay off
workers, we can assume that the writer be-
lieves that GM intends to lay off workers, not
just the spokesman. However, this is not part
of the annotation, and all reported speech is
annotated as NCB. Again, the issue of tense
is orthogonal.
? Not applicable (NA): for the writer, the
proposition is not of the type in which he or
she is expressing a belief, or could express a
belief. Usually, this is because the proposi-
tion does not have a truth value in this world
(be it in the past or in the future). This covers
expressions of desire (Some wish GM would
lay of workers), questions (Will GM lay off
workers? or Many wonder if GM will lay
off workers, and expressions of requirements
(GM is required to lay off workers or Lay off
workers!).
This sort of annotation is part of an annotation
of all ?modalities? that a text may express. We
only annotate belief. A further complication is
that these modalities can be nested: one can ex-
press a belief about someone else?s belief, and one
may be strong and the other weak (I believe John
may believe that GM will lay off workers). At this
phase, we only annotate from the perspective of
the writer, i.e. what the writer of the text that is
being annotated believes.
The annotation units (annotatables) are, con-
ceptually, propositions as defined by PropBank
(Kingsbury et al, 2002). In practice, annotators
are asked to identify full lexical verbs (whether
in main or embedded clauses, whether finite or
non-finite). In predicative constructions (John is a
doctor/in the kitchen/drunk), we ask them to iden-
tify the nominal, prepositional, or adjectival head
rather than the form of to be, in order to also han-
dle small clauses (I think [John an idiot]).
The interest of the annotation is clear: we want
to be able to determine automatically from a given
text what beliefs we can ascribe to the writer,
and with what strengths he or she holds them.
Across languages, many different linguistic means
are used to denote this attitude towards an uttered
proposition, including syntax, lexicon, and mor-
phology. To our knowledge, no systematic empir-
ical study exists for English, and this annotation is
a step towards that goal.
example, in German, the choice between indicative mood and
subjunctive mood in reported speech can signal the writer?s
attitude.
3 Related Work
The work of Roser et al (2006) is, in many re-
spects, very similar to ours. In particular, they are
concerned with extracting information about peo-
ple?s beliefs and the strength of these beliefs from
text. However, their annotation is very different
from ours. They extend the TimeML annotation
scheme to include annotation of markers of belief
and strength of belief. For example, in the sen-
tence The Human Rights Committee regretted that
discrimination against women persisted in prac-
tice, TimeML identifies the events associated with
the verbs regret and persist, and then the extension
to the annotation adds the mark that there is a ?fac-
tive? link between the regret event and the persist
event, i.e., if we regret something, then we assume
the truth of that something. In contrast, in our
annotation, we directly annotate events with their
level of belief. In this example, we would annotate
persist as being a committed belief of the Human
Rights Committee (though in this paper we only
report on beliefs attributed to the writer). This dif-
ference is important, as in the annotation of Roser
et al (2006), the annotator must analyze the situ-
ation and find evidence for the level of belief at-
tributed to an event. As a result, we cannot use
the annotation to discover how natural language
expresses level of belief. Our annotation is more
primitively semantic: we ask the annotators sim-
ply to annotate meaning (does X believe the event
takes place), as opposed to annotating the linguis-
tic structures which express meaning. As a conse-
quence of the difference in annotation, we cannot
compare our automatic prediction results to theirs.
Other related works explored belief systems in
an inference scenario as opposed to an intentional-
ity scenario. In work by (Ralf Krestel and Bergler,
2007; Krestel et al, 2008), the authors explore
belief in the context of news media exploring re-
ported speech where they track newspaper text
looking for elements indicating evidentiality. The
notion of belief is more akin to finding statements
that support or negate specific events with differ-
ent degrees of support. This is different from our
notion of committed belief in this work, since we
seek to make explicit the intention of the author or
the speaker.
4 Our Approach
4.1 Data
We create a relatively small corpus of English
manually annotated for the three categories: CB,
NCB, NA. The data covers different domains and
genres from newswire, to blog data, to email cor-
respondence, to letter correspondence, to tran-
scribed dialogue data. The data comprises 10K
words of running text. 70% of the data was dou-
bly annotated comprising 6188 potentially anno-
tatable tokens. Hence we had a 4 way manual clas-
sification in essence between NONE, CB, NCB,
and NA. Most of the confusions between NONE
and CB from both annotators, for 103 tokens.
The next point of disagreement was on NCB and
NONE for 48 tokens.They disagreed on NCB and
CB for 32 of the tokens. In general the interanno-
tator agreements were high as they agreed 95.8%
of the time on the annotatable and the exact belief
classification.4 Here is an example of a disagree-
ment between the two annotators, The Iraqi gov-
ernment has agreed to let Rep Tony Hall visit the
country next week to assess a humanitarian cri-
sis that has festered since the Gulf War of 1991
Hall?s office said Monday. One annotator deemed
?agreed? a CB while the other considered it an
NCB.
4.2 Automatic approach
Once we had the data manually annotated and re-
vised, we wanted to explore the feasibility of au-
tomatically predicting belief states based on lin-
guistic features. We apply a supervised learning
framework to the problem of both identifying and
classifying a belief annotatable token in context.
This is a three way classification task where an
annotatable token is tagged as one of our three
classes: Committed Belief (CB), Non Committed
Belief (NCB), and Not Applicable (NA). We adopt
a chunking approach to the problem using an In-
side Outside Beginning (IOB) tagging framework
for performing the identification and classification
of belief tokens in context. For chunk tagging,
we use YamCha sequence labeling system.5 Yam-
Cha is based on SVM technology. We use the de-
fault parameter settings most importantly the ker-
nels are polynomial degree 2 with a c value of 0.5.
We label each sentence with standard IOB tags.
Since this is a ternary classification task, we have
7 different tags: B-CB (Beginning of a commit-
ted belief chunk), I-CB (Inside of a committed be-
lief chunk), B-NCB (Beginning of non commit-
ted belief chunk), I-NCB (Inside of a non com-
mitted belief chunk), B-NA (Beginning of a not
applicable chunk), I-NA (Inside a not applicable
chunk), and O (Outside a chunk) for the cases
that are not annotatable tokens. As an example
of the annotation, a sentence such as Hall said
he wanted to investigate reports from relief agen-
cies that a quarter of Iraqi children may be suffer-
4This interannotator agreement number includes the
NONE category.
5http://www.tado-chasen.com/yamcha
ing from chronic malnutrition. will be annotated
as follows: {Hall O said B-CB he O wanted B-
NCB to B-NA investigate I-NA reports O from O
relief O agencies O that O a O quarter O of O
Iraqi O children O may O be O suffering B-NCB
from O chronic O malnutrition O.}
We experiment with some basic features and
some more linguistically motivated ones.
CXT: Since we adopt a sequence labeling
paradigm, we experiment with different window
sizes for context ranging from ?/+2 tokens after
and before the token of interest to ?/+5.
NGRAM: This is a character n-gram feature,
explicity representing the first and last character
ngrams of a word. In this case we experiment with
up to ?/+4 characters of a token. This feature
allows us to capture implicitly the word inflection
morphology.
POS: An important feature is the Part-of-Speech
(POS) tag of the words. Most of the annotatables
are predicates but not all predicates in the text are
annotatables. We obtain the POS tags from the
TreeTagger POS tagger tool which is trained on
the Penn Treebank.6
ALPHANUM: This feature indicates whether
the word has a digit in it or not or if it is a non
alphanumeric token.
VerbType: We classify the verbs as to whether
they are modals (eg. may, might, shall, will,
should, can, etc.), auxilliaries (eg. do, be, have),7
or regular verbs. Many of our annotatables occur
in the vicinity of modals and auxilliaries. The list
of modals and auxilliaries is deterministic.
Syntactic Chunk (CHUNK): This feature ex-
plicitly models the syntactic phrases in which our
tokens occur. The possible phrases are shallow
syntactic representations that we obtain from the
TreeTagger chunker:8 ADJC (Adjective Chunk),
ADVC (Adverbial Chunk), CONJC (Conjunc-
tional Chunk), INTJ (Interjunctional Chunk), LST
(numbers 1, 2,3 etc), NC (Noun Chunk), PC
(Prepositional Chunk), PRT (off,out,up etc), VC
(Verb Chunk).
5 Experiments and Results
5.1 Conditions
Since the data is very small, we tested our au-
tomatic annotation using 5 fold cross validation
6http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/
7We realize in some of the grammar books auxilliaries
include modal verbs.
8http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/
where 10% of the data is set aside as development
data, then 70% is used for training and 20% for
testing. The reported results are averaged over the
5 folds for the Test data for each of our experimen-
tal conditions.
Our baseline condition is using the tokenized
words only with no other features (TOK). We em-
pirically establish that a context size of ?/+3
yields the best results in the baseline condition as
evaluated on the development data set. Hence all
the results are yielded from a CXT of size 3.
The next conditions present the impact of
adding a single feature at a time and then combin-
ing them. It is worth noting that the results reflect
the ability of the classifier to identify a token that
could be annotatable and also classify it correctly
as one of the possible classes.
5.2 Evaluation Metrics
We use F?=1 (F-measure) as the harmonic mean
between (P)recision and (R)ecall. All the pre-
sented results are the F-measure. We report the
results separately for the three classes CB, NCB,
and NA as well as the overall global F measure for
any one condition averaged over the 5 folds of the
TEST data set.
5.3 Results
In Table 1 we present the results yielded per con-
dition including the baseline TOK and presented
for the three different classes as well as the overall
F-measure.
All the results yielded by our experiments
outperform the baseline TOK. We highlight
the highest performing conditions in Ta-
ble 1: TOK+AlphaNum+POS +CHUNK,
TOK+AN+POS and TOK+POS. Even though
all the features independently outperform the
baseline TOK in isolation, POS is the single most
contributing feature. The least contributing factor
independently is the AlphaNumeric feature AN.
However combining AN with character Ngram
NG yields better results than using each of them
independently. We note that adding NG to any
other feature combination is not helpful, in fact
it seems to add noise rather than signal to the
learning process in the presence of more sophis-
ticated features such as POS or syntactic chunk
information. Adding the verbtype VT explicitly
as a feature is not helpful for all categories, it
seems most effective with CB. As mentioned
earlier we deterministically considered all modal
verbs to be modal. This might not be the case
for all modal auxilliaries since some of them
are used epistemically while others deontically,
hence our feature could be introducing an element
of noise. Adding syntactic chunk information
helps boost the results by a small margin from
53.5 to 53.97 F-measure. All the results seem to
suggest the domination of the POS feature and it?s
importance for such a tagging problem. In general
our performance on CB is the highest, followed
by NA then we note that NCB is the hardest
category to predict. Examining the data, NCB
has the lowest number of occurrence instances
in this data set across the board in the whole
data set and accordingly in the training data,
which might explain the very low performance.
Also in our annotation effort, it was the hardest
category to annotate since the annotation takes
more than the sentential context into account.
Hence a typical CB verb such as ?believe? in the
scope of a reporting predicate such as ?say? as
in the following example Mary said he believed
the suspect with no qualms. The verb believed
should be tagged NCB however in most cases it
is tagged as a CB. Our syntactic feature CHUNK
helps a little but it does not capture the overall
dependencies in the structure. We believe that
representing deeper syntactic structure should
help tremendously as it will model these relatively
longer dependencies.
We also calculated a confusion matrix for the
different classes. The majority of the errors are
identification errors where an annotatable is con-
sidered an O class as opposed to one of the 3 rel-
evant classes. This suggests that identifying the
annotatable words is a harder task than classifica-
tion into one of the three classes, which is consis-
tent with our observation from the interannotator
disagreements where most of their disagreements
were on the annotatable tokens, though a small
overall number of tokens, 103 tokens out of 6188,
it was the most significant disagreement category.
We find that for the TOK+POS condition, CBs are
mistagged as un-annotatable O 55% of the time.
We find most of the confusions between NA and
CB, and NCB and CB, both cases favoring a CB
tag.
6 Conclusion
We presented a preliminary pilot study of belief
annotation and automatic tagging. Even though
the data is relatively tiny, we show that automatic
prediction of a belief class is a feasible task. Us-
ing syntactic features, we are able to obtain signif-
icant improvements over a simple baseline of 23%
F-measure absolute points. The best performing
automatic tagging condition is where we use POS
tag, word type feature AlphaNumeric, and shallow
syntactic chunk information CHUNK. Our best
overall performance is 53.97% F-measure.
CB NA NCB Overall F
TOK 25.12 41.18 13.64 30.3
TOK+NG 33.18 42.29 5 34.25
TOK+AN 30.43 44.57 12.24 33.92
TOK+AN+NG 37.17 42.46 9.3 36.61
TOK+POS 54.8 59.23 13.95 53.5
TOK+NG+POS 43.15 50.5 22.73 44.35
TOK+AN+POS 54.79 58.97 22.64 53.54
TOK+NG+AN+POS 43.09 54.98 18.18 45.91
TOK+POS+CHUNK 55.45 57.5 15.38 52.77
TOK+POS+VT+CHUNK 53.74 57.14 14.29 51.43
TOK+AN+POS+CHUNK 55.89 59.59 22.58 53.97
TOK+AN+POS+VT+CHUNK 56.27 58.87 12.9 52.89
Table 1: Final results averaged over 5 folds of test data using different features and their combinations:
NG is NGRAM, AN is AlphaNumeric, VT is verbtype
In the future we are looking at ways of adding
more sophisticated deep syntactic and semantic
features using lexical chains from discourse struc-
ture. We will also be exploring belief annotation in
Arabic and Urdu on a parallel data collection since
these languages express evidentiality in ways that
differ linguistically from English. Finally we will
explore ways of automatically augmenting the la-
beled data pool using active learning.
Acknowledgement
This work was supported by grants from the Hu-
man Language Technology Center of Excellence.
Any opinions, findings, and conclusions or recom-
mendations expressed in this material are those of
the authors and do not necessarily reflect the views
of the sponsor.
References
Philip R. Cohen and Hector J. Levesque. 1990. Ratio-
nal interaction as the basis for communication. In
Jerry Morgan Philip Cohen and James Allen, edi-
tors, Intentions in Communication. MIT Press.
Eduard H. Hovy. 1993. Automated discourse gener-
ation using discourse structure relations. Artificial
Intelligence, 63:341?385.
Rebecca Bruce Matthew Bell Janyce Wiebe,
Theresa Wilson and Melanie Martin. 2004.
Learning subjective language. In Computational
Linguistics, Volume 30 (3).
Paul Kingsbury, Martha Palmer, and Mitch Marcus.
2002. Adding semantic annotation to the Penn Tree-
Bank. In Proceedings of the Human Language Tech-
nology Conference, San Diego, CA.
Ralf Krestel, Sabine Bergler, and Rene? Witte. 2008.
Minding the Source: Automatic Tagging of Re-
ported Speech in Newspaper Articles. In European
Language Resources Association (ELRA), editor,
Proceedings of the Sixth International Language Re-
sources and Evaluation (LREC 2008), Marrakech,
Morocco, May 28?30.
William C. Mann and Sandra A. Thompson. 1987.
Rhetorical Structure Theory: A theory of text orga-
nization. Technical Report ISI/RS-87-190, ISI.
Johanna Moore. 1994. Participating in Explanatory
Dialogues. MIT Press.
Rene? Witte Ralf Krestel and Sabine Bergler. 2007.
Processing of Beliefs extracted from Reported
Speech in Newspaper Articles. In International
Conference on Recent Advances in Natural Lan-
guage Processing (RANLP 2007), Borovets, Bul-
garia, September 27?29.
Saur?? Roser, Marc Verhagen, and James Pustejovsky.
2006. Annotating and Recognizing Event Modality
in Text. In FLAIRS 2006, editor, In Proceedings
of the 19th International FLAIRS Conference, Mel-
bourne Beach, Florida, May 11-13.
Swapna Somasundaran, Janyce Wiebe, and Josef Rup-
penhofer. 2008. Discourse level opinion interpre-
tation. In Proceedings of the 22nd International
Conference on Computational Linguistics (Coling
2008), pages 801?808, Manchester, UK, August.
Coling 2008 Organizing Committee.
Theresa Wilson and Janyce Wiebe. 2005. Annotat-
ing attributions and private states. In Proceedings of
the Workshop on Frontiers in Corpus Annotations II:
Pie in the Sky, pages 53?60, Ann Arbor, Michigan,
June. Association for Computational Linguistics.
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 486?496, Dublin, Ireland, August 23-29 2014.
Fast Tweet Retrieval with Compact Binary Codes
Weiwei Guo
?
Wei Liu
?
Mona Diab
?
?
Computer Science Department, Columbia University, New York, NY, USA
?
IBM T. J. Watson Research Center, Yorktown Heights, NY, USA
?
Department of Computer Science, George Washington University, Washington, D.C., USA
weiwei@cs.columbia.edu weiliu@us.ibm.com mtdiab@gwu.edu
Abstract
The most widely used similarity measure in the field of natural language processing may be co-
sine similarity. However, in the context of Twitter, the large scale of massive tweet data inevitably
makes it expensive to perform cosine similarity computations among tremendous data samples.
In this paper, we exploit binary coding to tackle the scalability issue, which compresses each data
sample into a compact binary code and hence enables highly efficient similarity computations via
Hamming distances between the generated codes. In order to yield semantics sensitive binary
codes for tweet data, we design a binarized matrix factorization model and further improve it in
two aspects. First, we force the projection directions employed by the model nearly orthogonal to
reduce the redundant information in their resulting binary bits. Second, we leverage the tweets?
neighborhood information to encourage similar tweets to have adjacent binary codes. Evaluated
on a tweet dataset using hashtags to create gold labels in an information retrieval scenario, our
proposed model shows significant performance gains over competing methods.
1 Introduction
Twitter is rapidly gaining worldwide popularity, with 500 million active users generating more than
340 million tweets daily
1
. Massive-scale tweet data which is freely available on the Web contains rich
linguistic phenomena and valuable information, therefore making it one of most favorite data sources
used by a variety of Natural Language Processing (NLP) applications. Successful examples include
first story detection (Petrovic et al., 2010), local event detection (Agarwal et al., 2012), Twitter event
discovery (Benson et al., 2011) and summarization (Chakrabarti and Punera, 2011), etc.
In these NLP applications, one of core technical components is tweet similarity computing to search
for the desired tweets with respect to some sample tweets. For example, in first story detection (Petrovic
et al., 2010), the purpose is to find an incoming tweet that is expected to report a novel event not revealed
by the previous tweets. This is done by measuring cosine similarity between the incoming tweet and
each previous tweet.
One obvious issue is that cosine similarity computations among tweet data will become very slow once
the scale of tweet data grows drastically. In this paper, we investigate the problem of searching for most
similar tweets given a query tweet. Specifically, we propose a binary coding approach to render com-
putationally efficient tweet comparisons that should benefit practical NLP applications, especially in the
face of massive data scenarios. Using the proposed approach, each tweet is compressed into short-length
binary bits (i.e., a compact binary code), so that tweet comparisons can be performed substantially faster
through measuring Hamming distances between the generated compact codes. Crucially, Hamming
distance computation only involves very cheap NOR and popcount operations instead of floating-point
operations needed by cosine similarity computation.
Compared to other genres of data, similarity search in tweet data is very challenging due to the short
nature of Twitter messages, that is, a tweet contains too little information for traditional models to extract
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1
http://en.wikipedia.org/wiki/Twitter
486
Symbol Definition
n Number of tweets in the corpus.
d Dimension of a tweet vector, i.e., the vocabulary size.
x
i
The sparse tf-idf vector corresponding to the i-th tweet in the corpus.
?
x
i
The vector subtracted by the mean ? of the tweet corpus:
?
x
i
= x
i
??.
X,
?
X The tweet corpus in a matrix format, and the zero-centered tweet data.
r The number of binary coding functions, i.e., the number of latent topics.
f
k
The k-th binary coding function.
Table 1: Symbols used in binary coding.
latent topical semantics. For instance, in our collected dataset, there exist only 11 words per tweet on
average. We address the sparsity issue pertaining to tweet data by converting our previously proposed
topic modelWeighted Textual Matrix Factorization (WTMF) (Guo and Diab, 2012) to a binarized version.
WTMF maps a tweet to a low-dimensional semantic vector which can easily be transformed to a binary
code by virtue of a sign function. We consider WTMF a good baseline for the task of tweet retrieval, as
it has achieved state-of-the-art performance among unsupervised systems on two benchmark short-text
datasets released by Li et al. (2006) and Agirre et al. (2012).
In this paper, we improve WTMF in two aspects. The first drawback of the WTMF model is that it
focuses on exhaustively encoding the local context, and hence introduces some overlapping information
that is reflected in its associated projections. In order to remove the redundant information and meanwhile
discover more distinct topics, we employ a gradient descent method to make the projection directions
nearly orthogonal.
The second aspect is to enrich each tweet by its neighbors. Because of the short context, most tweets
do not contain sufficient information of an event, as noticed by previous work (Agarwal et al., 2012; Guo
et al., 2013). Ideally, we would like to learn a model such that the tweets related to the same event are
mapped to adjacent binary codes. We fulfill this purpose by augmenting each tweet in a given training
dataset with its neighboring tweets within a temporal window, and assuming that these neighboring
(or similar) tweets are triggered by the same event. We name the improved model Orthogonal Matrix
Factorization with Neighbors (OrMFN).
In our experiments, we use Twitter hashtags to create the gold (i.e., groundtruth) labels, where tweets
with the same hashtag are considered semantically related, hence relevant. We collect a tweet dataset
which consists of 1.35 million tweets over 3 months where each tweet has exactly one hashtag. The
experimental results show that our proposed model OrMFN significantly outperforms competing binary
coding methods.
2 Background and Related Work
2.1 Preliminaries
We first introduce some notations used in this paper to formulate our problem. Suppose that we are
given a dataset of n tweets and the size of the vocabulary is d. A tweet is represented by all the words it
contains. We use notationx ? R
d
to denote a sparse d-dimensional tf-idf vector corresponding to a tweet,
where each word stands for a dimension. For ease of notation, we represent all n tweets in a matrix X =
[x
1
,x
2
, ? ? ? ,x
n
] ? R
d?n
. For binary coding, we seek r binarization functions
{
f
k
: R
d
? {1,?1}
}
r
k=1
so that a tweet x
i
is encoded into an r-bit binary code (i.e., a string of r binary bits). Table 1 illustrates
the symbols used in this paper for notation.
Hamming Ranking: In the paper we evaluate the quality of binary codes in terms of Hamming ranking.
Given a query tweet, all data items are ranked in an ascending order according to the Hamming distances
between their binary codes and the query?s binary code, where a Hamming distance is the number of
bit positions in which bits of two codes differ. Compared with cosine similarity, computing Hamming
distance can be substantially efficient. This is because fixed-length binary bits enable very cheap logic
operations for Hamming distance computation, whereas real-valued vectors require floating-point op-
487
erations for cosine similarity computation. Since logic operations are much faster than floating-point
operations, Hamming distance computation is typically much faster than cosine similarity computation
2
2.2 Binary Coding
Early explorations of binary coding focused on using random permutations or random projections to ob-
tain binary coding functions (aka, hash functions), such as Min-wise Hashing (MinHash) (Broder et al.,
1998) and Locality-Sensitive Hashing (LSH) (Indyk and Motwani, 1998). MinHash and LSH are gen-
erally considered data-independent approaches, as their coding functions are generated in a randomized
fashion. In the context of Twitter, the simple LSH scheme proposed in (Charikar, 2002) is of particular
interest. Charikar proved that the probability of two data points colliding is proportional to the angle
between them, and then employed a random projection w ? R
d
to construct a binary coding function:
f(x) = sgn
(
w
>
x
)
=
{
1, if w
>
x > 0,
?1, otherwise.
(1)
The current held view is that data-dependent binary coding can lead to better performance. A data-
dependent coding scheme typically includes two steps: 1) learning a series of binary coding functions
with a small amount of training data; 2) applying the learned functions to larger scale data to produce
binary codes.
In the context of tweet data, Latent Semantic Analysis (LSA) (Landauer and Dumais, 1997) can di-
rectly be used for data-dependent binary coding. LSA reduces the dimensionality of the data in X by
performing singular value decomposition (SVD) over X: X = U?V
>
. Let
?
X be the zero-centered data
matrix, where each tweet vector x
i
is subtracted by the mean vector ?, resulting in
?
x
i
= x
i
? ?. The r
coding functions are then constructed by using the r eigenvectors u
1
,u
2
, ? ? ? ,u
r
associated with the r
largest eigenvalues, that is, f
k
(x) = sgn
(
u
>
k
?
x
)
= sgn
(
u
>
k
(x ? ?)
)
(k = 1, ? ? ? , r). The goal of using
zero-centered data
?
X is to balance 1 bits and ?1 bits.
Iterative Quantization (ITQ) (Gong and Lazebnik, 2011) is another popular unsupervised binary cod-
ing approach. ITQ attempts to find an orthogonal rotation matrix R ? R
r?r
to minimize the squared
quantization error: ?B?RV ?
2
F
, whereB ? {1,?1}
r?n
contains the binary codes of all data, V ? R
r?n
contains the LSA-projected and zero-centered vectors, and ? ? ?
F
denotes Frobenius norm. After R is
optimized, the binary codes are simply obtained by B = sgn(RV ).
Much recent work learns nonlinear binary coding functions, including Spectral Hashing (Weiss et
al., 2008), Anchor Graph Hashing (Liu et al., 2011), Bilinear Hashing (Liu et al., 2012b), Kernelized
LSH (Kulis and Grauman, 2012), etc. Concurrently, supervised information defined among training data
samples was incorporated into coding function learning such as Minimal Loss Hashing (Norouzi and
Fleet, 2011) and Kernel-Based Supervised Hashing (Liu et al., 2012a). Our proposed method falls into
the category of unsupervised, linear, data-dependent binary coding.
2.3 Applications in NLP
The NLP community has successfully applied LSH in several tasks such as first story detection (Petrovic
et al., 2010), and paraphrase retrieval for relation extraction (Bhagat and Ravichandran, 2008), etc. This
paper shows that our proposed data-dependent binary coding approach is superior to data-independent
LSH in terms of the quality of generated binary codes.
Subercaze et al. (2013) proposed a binary coding approach to encode user profiles for recommenda-
tions. Compared to (Subercaze et al., 2013) in which a data unit is a whole user profile consisting of all
his/her Twitter posts, we tackle a more challenging problem, since our data units are extremely short ?
namely, a single tweet.
2
We recognize that different hardware exploiting techniques such as GPU or parallelization accelerate cosine similarity.
However, they don?t change the inherent nature of the data representation. They can be equally applied to Hamming distance
and we anticipate significant speed gains. We relegate this exploration of different implementations of Hamming distance to
future work.
488
X	 ? P	 ? Q
T	 ??	 ? ?	 ?
Figure 1: Weighted Textual Matrix Factorization. The d ? n matrix X is approximated by the product
of a d? r matrix P and an n? r matrix Q. Note in the figure we used the transpose of the Q matrix.
3 Weighted Textual Matrix Factorization
The WTMF model proposed by Guo and Diab (2012) is designed to extract latent semantic vectors for
short textual data. The low-dimensional semantic vectors can be used to represent the tweets in the
original high-dimensional space. WTMF achieved state-of-the-art unsupervised performance on two
short text similarity datasets, which can be attributed to the fact that WTMF carefully handles missing
words (the missing words of a text are the words with 0 values in a data vector x).
Assume that there are r latent dimensions/topics in the data, the matrix X is approximated by the
product of a d?r matrix P and an n?r matrixQ, as in Figure 1. Accordingly, a tweet x
j
is represented
by an r-dimensional vector Q
j,?
; similarly, a word w
i
is generalized by the r-dimensional vector P
i,?
(the
ith row in matrix P ). The matrix factorization scheme has an intuitive explanation: the inner-product
of a word profile vector P
i,?
and a tweet profile vector Q
j,?
is to approximate the TF-IDF value X
ij
:
P
i,?
>
Q
j,?
? X
ij
(as illustrated by the shaded parts in Figure 1).
Intuitively, X
ij
= 0 suggests that the latent topics of the text x
j
are not relevant to the word w
i
.
Note that 99% of the cells in X are 0 because of the short contexts, which significantly diminishes the
contribution of the observed words to the searching of optimal P andQ. To reduce the impact of missing
words, a small weight w
m
is assigned to each 0 cell of X in the objective function:
?
i
?
j
W
ij
(
P
i,?
>
Q
j,?
?X
ij
)
2
+ ?||P ||
2
2
+ ?||Q||
2
2
,
W
i,j
=
{
1, if X
ij
6= 0,
w
m
, if X
ij
= 0.
(2)
where ? is the regularization parameter. Alternating Least Squares (Srebro and Jaakkola, 2003) is used
to iteratively compute the latent semantic vectors in P and Q:
P
i,?
=
(
Q
>
?
W
(i)
Q+ ?I
)
?1
Q
>
?
W
(i)
X
>
i,?
,
Q
j,?
=
(
P
>
?
W
(j)
P + ?I
)
?1
P
>
?
W
(j)
X
?,j
(3)
where
?
W
(i)
= diag(W
i,?
) is a n ? n diagonal matrix containing the i-th row of the weight matrix W .
Similarly,
?
W
(j)
= diag(W
?,j
) is a d? d diagonal matrix containing the j-th column of W .
As in Algorithm 1 line 6-9, P andQ are computed iteratively, i.e., in a iteration each P
i,?
(i = 1, ? ? ? , d)
is calculated based on Q, then each Q
j,?
(j = 1, ? ? ? , n) is calculated based on P . This can be computed
efficiently since: (1) all P
i,?
share the same Q
>
Q; similarly all Q
j,?
share the same P
>
P ; (2) X is very
sparse. More details can be found in (Steck, 2010).
Adapting WTMF to binary coding is straightforward. Following LSA, we use the matrix P to linearly
project tweets into low-dimensional vectors, and then apply the sign function. The k-th binarization
function uses the k-th column of the P matrix (P
?,k
) as follows
f
k
(x) = sgn (P
?,k
?
x) =
{
1, if P
?,k
?
x > 0,
?1, otherwise.
(4)
4 Removing Redundant Information
It is worth noting that there are two explanations of the d? r matrix P . The rows of P , denoted by P
i,?
,
may be viewed as the collection of r-dimensional latent profiles of words, which we observe frequently
489
Algorithm 1: OrMF
1 Procedure P = OrMF(X,W, ?, n itr, ?)
2 n words, n docs? size(X);
3 randomly initialize P,Q;
4 itr ? 1;
5 while itr < n itr do
6 for j ? 1 to n docs do
7 Q
j,?
=
(
P
>
?
W
(j)
P + ?I
)
?1
P
>
?
W
(j)
X
?,j
8 for i? 1 to n words do
9 P
i,?
=
(
Q
>
?
W
(i)
Q+ ?I
)
?1
Q
>
?
W
(i)
X
>
i,?
10 c = mean(diag(P
>
P ));
11 P ? P ? ?P (P
>
P ? cI);
12 itr ? itr + 1;
in the WTMF model. Meanwhile, columns of P are projection vectors, denoted by P
?,k
, which are
similar to eigenvectors U obtained by LSA. The projection vector P
?,k
is employed to multiply to a zero
centered data vector
?
x to generate a binary string: sgn(P
?,k
>
?
x). In this section, we focus on the property
of the P matrix columns.
As in equation 3, each row in matrices P and Q is iteratively optimized to approximate the data:
P
i,?
>
Q
j,?
? X
ij
. While it does a good job at preserving the existence/relevance of each word in a short
text, it might encode repetitive information by means of the dimensionality reduction or the projection
vectors P
?,k
(the columns of P ). For example, the first dimension P
?,1
may be 90% about the politics
topic and 10% about the economics topic, and the second dimension P
?,2
is 95% on economics and 5%
on technology topics, respectively.
Ideally we would like the dimensions to be uncorrelated, so that more distinct topics of data could
be captured. One way to ensure the uncorrelatedness is to force P to be orthogonal, i.e., P
>
P = I . It
implies P
?,j
>
P
?,k
= 0 if k 6= j.
4.1 Implementation of Orthogonal Projections
To produce nearly orthogonal projections in the current framework, we could add a regularizer ?(P
>
P?
I)
2
with the weight ? in the objective function of the WTMF model (equation 6). However, in practice
this method does not lead to the convergence of P . This is mainly caused by the phenomenon that any
word profile P
i,?
becomes dependent of all other word profiles after an iteration.
Therefore, we adopt a simpler method, gradient descent, in which P is updated by taking a small step
in the direction of the negative gradient of (P
>
P ? I)
2
. It is also worth noting that (P
>
P ? I)
2
requires
each projection P
?,k
to be a unit vector because of P
?,k
>
P
?,k
= 1, which is infeasible when the nonzero
values in X are large. Therefore, we multiply the matrix I by a coefficient c, which is calculated from
the mean of the diagonal of P
>
P in the current iteration. The following two lines are added at the end
of an iteration:
c? mean(diag(P
>
P )),
P ?P ? ?P (P
>
P ? cI).
(5)
This procedure is presented in Algorithm 1. Accordingly, the magnitude of P is not affected. The step
size ? is fixed to 0.0001. We refer to this model as Orthogonal Matrix Factorization (OrMF).
5 Exploiting Nearest Neighbors for Tweets
We observe that tweets triggered by the same event do not have very high cosine similarity scores among
them. This is caused by the inherent short length of tweets such that usually a tweet only describes one
490
aspect of an event (Agarwal et al., 2012; Guo et al., 2013). Our objective is to find the relevant tweets
given a tweet, and then learn a model that assigns similar binary bits to these relevant tweets.
5.1 Modeling Neighboring Tweets
Given a tweet, we treat its nearest neighbors in a temporal window as its most relevant tweets. We
assume that the other aspects of an event can be found in its nearest neighbors. Accordingly, we extract
t neighbors for a tweet from 10,000 most chronologically close tweets. In this current implementation,
we set t = 5.
Under the weighted matrix factorization framework, we extend each tweet by its t nearest neighbors.
Specifically, for each tweet, we incorporate additional words from its neighboring tweets. The values
of the new words are averaged. Moreover, these new words are treated differently by assigning a new
weight w
n
to them, since we believe that the new words are not as informative as the original words in
the tweet.
We present an illustrative example of how to use neighbors to extend the tweets. Let x
1
be a tweet
with the following words (the numbers after the colon are TF-IDF values):
x
1
= {obama:5.5, medicare:8.3, website:3.8}
which has two nearest neighbors:
x
27
= {obama:5.5, medicare:8.3, website:3.8, down:5.4}
x
356
= {obama:5.5, medicare:8.3, website:3.8, problem:7.0}
Then there are two additional words added in x
1
whose values are averaged. The new data vector x
?
1
is:
x
?
1
= {obama:5.5, medicare:8.3, website:3.8, down:2.7, problem:3.5}
Therefore, the algorithm is run on the new neighbor-augmented data matrix, denoted by X
?
, and the
weight matrix W becomes
W
i,j
=
?
?
?
1, if X
?
ij
6= 0 &j is an original word,
w
n
, if X
?
ij
6= 0, &j is from neighbor tweets,
w
m
, if X
?
ij
= 0.
(6)
This model is referred to as Orthogonal Matrix Factorization with Neighbors (OrMFN).
5.2 Binary coding without Neighbors
It is important to point out that the data used by OrMFN, X
?
, could be a very small subset of the whole
dataset. Therefore we only need to find neighbors for a small portion of the data. After the P matrix
is learned, the neighborhood information is implicitly encoded in the matrix P , and we still apply the
same binarization function sgn(P
?,k
>
?
x) on the whole dataset (in large scale) without neighborhood
information. We randomly sample 200,000 tweets for OrMFN to learn P ; neighbors are extracted only
for these 200,000 tweets (note that the neighbors are from the 200,000 tweets as well), and then we use
the learned P to generate binary codes for the whole dataset 1.35 million tweets without searching for
their nearest neighbors.
3
Our scheme has a clear advantage: the binary coding remains very efficient. During binarization for
any data, there is no need to compare 10,000 most recent tweets to find nearest neighbors, which could
be time-consuming. An opposite example is the method presented in (Guo et al., 2013), where t most
nearest neighbor tweets were extracted, and a tweet profile Q
j,?
was explicitly forced to be similar to its
neighbors? profiles. However, for each new data, the approach proposed in (Guo et al., 2013) requires
computing its nearest neighbors.
6 Experiments
6.1 Tweet Data
We crawled English tweets spanning three months from October 5th 2013 to January 5th 2014 using the
Twitter API.
4
We cleaned the data such that each hashtag appears at least 100 times in the corpus, and
3
When generating the binary codes for the 200,000 tweets, these tweets are not augmented with neighbor words.
4
https://dev.twitter.com
491
each word appears at least 10 times. This data collection consists of 1,350,159 tweets, 15 million word
tokens, 30,608 unique words, and 3,214 unique hashtags.
One of main reasons to use hashtags is to enhance accessing topically similar tweets (Efron, 2010).
In a large-scale data setting, it is impossible to manually identify relevant tweets. Therefore, we use
Twitter hashtags to create groundtruth labels, which means that tweets marked by the same hashtag
as the query tweet are considered relevant. Accordingly, in our experiments all hashtags are removed
from the original data corpus. We chose a subset of hashtags from the most frequent hashtags to create
groundtruth labels: we manually removed some tags from the subset that are not topic-related (e.g.,
#truth, #lol) or are ambiguous; we also removed all the tags that are referring to TV series (the relevant
tweets can be trivially obtained by named entity matching). The resulting subset contains 18 hashtags.
5
100 tweets are randomly selected as queries (test data) for each of the 18 hashtags. The median
number of relevant tweets per query is 5,621. The small size of gold standard makes the task relatively
challenging. We need to identify 5,621 (0.42% of the whole dataset) tweets out of 1.35 million tweets.
200,000 tweets are randomly selected (not including the 1,800 queries) as training data for the data
dependent models to learn binarization functions.
6
The functions are subsequently applied on all the
1.35 million tweets, including the 1,800 query tweets.
6.2 Evaluation
We evaluate a model by the search quality: given a tweet as query, we would like to rank the relevant
tweets as high as possible. Following previous work (Weiss et al., 2008; Liu et al., 2011), we use mean
precision among top 1000 returned list (MP@1000) to measure the ranking quality. Let pre@k be the
precision among top k return data, then MP@1000 is the average value of pre@1, pre@2...pre@1000.
Obviously MP gives more reward on the systems that can rank relevant data in the top places, e.g., if
the highest ranked tweet is a relevant tweet, then all the precision values (pre@2, pre@3, pre@4...) are
increased. We also calculate the precision and recall curve at varying values of top k returned list.
6.3 Methods
We evaluate the proposed unsupervised binary coding models OrMF and OrMFN, whose performance is
compared against 5 other unsupervised methods, LSH, SH, LSA, ITQ, and WTMF. All the binary coding
functions except LSH are learned on the 200,000 tweet set. All the methods have the same form of binary
coding functions: sgn(P
?,k
>
?
x), where they differ only in the projection vector P
?,k
. The retrieved tweets
are ranked according to their Hamming distance to the query, where Hamming distance is the number of
different bit positions between the binary codes of a tweet and the query.
For ITQ and SH, we use the code provided by the authors. Note that the dense matrix
?
X
?
X
>
is
impossible to compute due the large vocabulary, therefore we replace it by sparse matrix XX
>
. For the
three matrix factorization based methods (WTMF, OrMF, OrMFN) we run 10 iterations. The regularizer
? in equation 6 is fixed at 20 as in (Guo and Diab, 2012). A small set of 500 tweets is selected from
the training set as tuning set to choose the missing word weight w
m
in the baseline WTMF, and then its
value is fixed for OrMF and OrMFN. The same 500 tweets tuning set is used to choose the neighbor word
weight w
n
. In fact these models are very stable, consistently outperforming the baselines regardless of
different values of w
m
and w
n
, as later shown in Figure 4 and 5.
We also present the results of cosine similarity on the original word space (COSINE) as an upper
bound of the binary coding methods. We implemented an efficient algorithm for COSINE, which is the
algorithm 1 in (Petrovic et al., 2010). It firstly normalizes each data to a unit vector, then cosine similarity
is calculated by traversing only once the tweets via inverted word index.
6.4 Results
Table 2 summarizes the ranking performance measured by MP@1000 (the mean precision at top 1000
returned list). Figures 2 and 3 illustrate the corresponding precision and recall curve for the Hamming
5
The tweet dataset and their associated list of hashtags will be available upon request.
6
Although we use the word ?training?, the hashtags are never seen by the models. The training data is used for the models
to learn the word co-occurrence, and construct binary coding functions.
492
Models Parameters r=64 r=96 r=128
LSH ? 19.21% 21.84% 23.75%
SH ? 18.29% 19.32% 19.95%
LSA ? 21.04% 22.07% 22.67%
ITQ ? 20.8% 22.06% 22.86%
WTMF w
m
= 0.1 26.64% 29.39% 30.38%
OrMF w
m
= 0.1 27.7% 30.48% 31.26%
OrMFN w
m
= 0.1, w
n
= 0.5 29.73% 31.73% 32.55%
COSINE ? 33.68%
Table 2: Mean precision among top 1000 returned list
0 200 400 600 800 1000
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
Pre
cisio
n
# of samples
 
 LSH
SH
LSA
ITQ
WTMF
OrMF
OrMFN
(a) r = 64
0 200 400 600 800 1000
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
Pre
cisio
n
# of samples
 
 
LSH
SH
LSA
ITQ
WTMF
OrMF
OrMFN
(b) r = 96
0 200 400 600 800 1000
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
Pre
cisio
n
# of samples
 
 
LSH
SH
LSA
ITQ
WTMF
OrMF
OrMFN
(c) r = 128
Figure 2: Hamming ranking: precision curve under top 1000 returned list
distance ranking. The number of r binary coding functions corresponds to the number of dimensions in
the 6 data-dependent models LSA, SH, ITQ, WTMF, OrMF and OrMFN. The missing words weight w
m
is fixed as 0.1 based on the tuning set in the three weighted matrix factorization based models WTMF,
OrMF and OrMFN. The neighbor word weight w
n
is chosen as 0.5 for OrMFN. Later in Section 6.4.1
we show that the performance is robust using varying values of w
m
and w
n
.
As the number of bits increases, all binary coding models yield better results. This is understandable
since the binary bits really record very tiny bits of information from each tweet, and more bits, the more
they are able to capture more semantic information.
SH has the worst MP@1000 performance. The reason might be it is designed for vision data where
the data vector is relatively dense. ITQ yields comparable results to LSA in terms of MP@1000, yet the
recall curve in Figure 3b,c clearly shows the superiority of ITQ over LSA.
WTMF outperforms LSA by a large margin (around 5% to 7%) through properly modeling missing
words, which is also observed in (Guo and Diab, 2012). Although WTMF already reaches a very high
MP@1000 performance level, OrMF can still achieve around 1% improvement over WTMF, which can
be attributed to orthogonal projections that captures more distinct topics. At last, leveraging neighbor-
hood information, OrMFN is the best performing model (around 1% improvement over OrMF). The
trend holds consistently across all conditions. The precision and recall curves in Figures 2 and 3 confirm
the trend observed in Table 2 as well.
All the binary coding models yield worse performance than COSINE baseline. This is expected, as the
binary bits are employed to gain efficiency at the cost of accuracy: the 128 bits significantly compress
the data losing a lot of nuanced information, whereas in the high dimensional word space 128 bits can
be only used to record two words (32 bits for two word indices and 32 bits for two TF-IDF values). We
manually examined the ranking list. We found in the binary coding models, there exist a lot of ties (128
bits only result in 128 possible Hamming distance values), whereas the COSINE baseline can correctly
rank them by detecting the subtle difference signaled by the real-valued TF-IDF values.
493
0 2 4 6 8 10 12
x 104
0
0.05
0.1
0.15
0.2
0.25
0.3
Rec
all
# of samples
 
 LSH
SH
LSA
ITQ
WTMF
OrMF
OrMFN
(a) r = 64
0 2 4 6 8 10 12
x 104
0
0.05
0.1
0.15
0.2
0.25
0.3
Rec
all
# of samples
 
 LSH
SH
LSA
ITQ
WTMF
OrMF
OrMFN
(b) r = 96
0 2 4 6 8 10 12
x 104
0
0.05
0.1
0.15
0.2
0.25
0.3
Rec
all
# of samples
 
 LSH
SH
LSA
ITQ
WTMF
OrMF
OrMFN
(c) r = 128
Figure 3: Hamming ranking: recall curve under top 100,000 returned list
0.05 0.08 0.1 0.15 0.20.25
0.26
0.27
0.28
0.29
0.3
0.31
0.32
0.33
0.34
MP@
1000
wm
 
 WTMFOrMFOrMFN
(a) r = 64
0.05 0.08 0.1 0.15 0.20.25
0.26
0.27
0.28
0.29
0.3
0.31
0.32
0.33
0.34
MP@
1000
wm
 
 WTMFOrMFOrMFN
(b) r = 96
0.05 0.08 0.1 0.15 0.20.25
0.26
0.27
0.28
0.29
0.3
0.31
0.32
0.33
0.34
MP@
1000
wm
 
 
WTMFOrMFOrMFN
(c) r = 128
Figure 4: Weighted matrix factorization based models: MP@1000 vs. missing word weight w
m
6.4.1 Analysis
We are interested in whether other values of w
m
and w
n
can generate good results ? in other words,
whether the performance is robust to the two parameter values. Accordingly, we present their im-
pact on MP@1000 in Figure 4 and 5. In Figure 4, the missing word weight w
m
is chosen from
{0.05, 0.08, 0.1, 0.15, 0.2}, where in OrMFN the neighbor weight w
n
is fixed as 0.5. The figure in-
dicates we can achieve even better MP@1000 around 33.2% when selecting the optimal w
m
= 0.05.
In general, the curves for all the code length are very smooth; the chosen value of w
m
does not have a
negative impact, e.g., the gain from OrMF over WTMF is always positive.
Figure 5 demonstrates the impact of varying the values of neighbor word weight w
n
from
{0, 0.25, 0.5, 0.75, 1} on OrMFN tested in different r conditions. Note that when w
n
= 0 indicating
that no neighbor information is exploited, the OrMFN model is simply reduced to the OrMF model.
Based on the Figure illustration we can conclude that integrating neighboring word information always
yields a positive effect, since any value of w
n
> 0 yields a performance gain over w
n
= 0 which is
OrMF.
6.5 Computation Cost
The data-dependent models involve 2 steps: 1) learning coding functions from a small dataset, and 2)
binary coding for the large scale whole dataset.
7
In real-time scenarios, the time is only spent on the
2nd step that involves no matrix factorization. The computation cost of binary coding for all models
(LSH, ITQ, LSA, WTMF, OrMF and OrMFN) are roughly the same: sgn(P
?,k
>
?
x). Note that P
?,k
>
?
x =
P
?,k
>
x? P
?,k
>
? where x is a very sparse vector (with 11 non-zeros values on average) and P
?,k
>
? can
be precomputed. On the other hand, calculating Hamming distance on binary codes is also very fast
using the logic operations.
7
Learning the binarization functions can be always done on a small dataset, for example in this paper all the data dependent
models are run on the 200,000 tweets, hence it performs very fast. In addition, in the OrMFN model, there is no need to find
nearest neighbors for the whole dataset in the 2nd step (the binary coding step).
494
0 0.2 0.4 0.6 0.8 10.27
0.28
0.29
0.3
0.31
0.32
0.33
0.34
MP
@1
00
0
wn
 
 
r=64
r=96
r=128
Figure 5: OrMFN model: MP@1000 vs. neighbor word weight w
n
7 Conclusion
In this paper, we proposed a novel unsupervised binary coding model which provides efficient similarity
search in massive tweet data. The proposed model, OrMFN, improves an existing matrix factorization
model through learning nearly orthogonal projection directions and leveraging the neighborhood infor-
mation hidden in tweet data. We collected a dataset whose groundtruth labels are created from Twitter
hashtags. Our experiments conducted on this dataset showed significant performance gains of OrMFN
over the competing methods.
Acknowledgements
We thank Boyi Xie and three anonymous reviewers for their valuable comments. This project is sup-
ported by the DARPA DEFT Program.
References
Puneet Agarwal, Rajgopal Vaithiyanathan, Saurabh Sharma, and Gautam Shroff. 2012. Catching the long-tail: Ex-
tracting local news events from twitter. In Proceedings of the Sixth International AAAI Conference on Weblogs
and Social Media.
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonzalez-Agirre. 2012. Semeval-2012 task 6: A pilot on
semantic textual similarity. In First Joint Conference on Lexical and Computational Semantics (*SEM).
Edward Benson, Aria Haghighi, and Regina Barzilay. 2011. Event discovery in social media feeds. In Proceedings
of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies.
Rahul Bhagat and Deepak Ravichandran. 2008. Large scale acquisition of paraphrases for learning surface pat-
terns. In Proceedings of ACL-08: HLT.
Andrei Z Broder, Moses Charikar, Alan M Frieze, and Michael Mitzenmacher. 1998. Min-wise independent
permutations. In Proceedings of the Thirtieth Annual ACM Symposium on Theory of Computing.
Deepayan Chakrabarti and Kunal Punera. 2011. Event summarization using tweets. In Proceedings of the Fifth
International AAAI Conference on Weblogs and Social Media.
Moses S. Charikar. 2002. Similarity estimation techniques from rounding algorithms. In Proceedings of the
Thiry-fourth Annual ACM Symposium on Theory of Computing.
Miles Efron. 2010. Information search and retrieval in microblogs. In Journal of the American Society for
Information Science and Technology.
Yunchao Gong and Svetlana Lazebnik. 2011. Iterative quantization: A procrustean approach to learning binary
codes. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.
495
Weiwei Guo and Mona Diab. 2012. Modeling sentences in the latent space. In Proceedings of the 50th Annual
Meeting of the Association for Computational Linguistics.
Weiwei Guo, Hao Li, Heng Ji, and Mona Diab. 2013. Linking tweets to news: A framework to enrich online
short text data in social media. In Proceedings of the 51th Annual Meeting of the Association for Computational
Linguistics.
Piotr Indyk and Rajeev Motwani. 1998. Approximate nearest neighbors: towards removing the curse of dimen-
sionality. In Proceedings of the thirtieth annual ACM symposium on Theory of computing.
Brian Kulis and Kristen Grauman. 2012. Kernelized locality-sensitive hashing. IEEE Transactions On Pattern
Analysis and Machine Intelligence, 34(6):1092?1104.
Thomas K. Landauer and Susan T. Dumais. 1997. A solution to plato?s problem: The latent semantic analysis
theory of acquisition, induction and representation of knowledge. In Psychological review.
Yuhua Li, David McLean, Zuhair A. Bandar, James D. O?Shea, and Keeley Crockett. 2006. Sentence similarity
based on semantic nets and corpus statistics. IEEE Transaction on Knowledge and Data Engineering, 18.
Wei Liu, Jun Wang, Sanjiv Kumar, and Shih-Fu Chang. 2011. Hashing with graphs. In Proceedings of the 28th
International Conference on Machine Learning.
Wei Liu, Jun Wang, Rongrong Ji, Yu-Gang Jiang, and Shih-Fu Chang. 2012a. Supervised hashing with kernels.
In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.
Wei Liu, Jun Wang, Yadong Mu, Sanjiv Kumar, and Shih-Fu Chang. 2012b. Compact hyperplane hashing with
bilinear functions. In Proceedings of the 29th International Conference on Machine Learning.
Mohammad Norouzi and David J. Fleet. 2011. Minimal loss hashing for compact binary codes. In Proceedings
of the 28th International Conference on Machine Learning.
Sasa Petrovic, Miles Osborne, and Victor Lavrenko. 2010. Streaming first story detection with application to
twitter. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the
Association for Computational Linguistics.
Nathan Srebro and Tommi Jaakkola. 2003. Weighted low-rank approximations. In Proceedings of the Twentieth
International Conference on Machine Learning.
Harald Steck. 2010. Training and testing of recommender systems on data missing not at random. In Proceedings
of the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.
Julien Subercaze, Christophe Gravier, and Frederique Laforest. 2013. Towards an expressive and scalable twitter?s
users profiles. In IEEE/WIC/ACM International Joint Conferences on Web Intelligence and Intelligent Agent
Technologies.
Yair Weiss, Antonio Torralba, and Rob Fergus. 2008. Spectral hashing. In Advances in Neural Information
Processing Systems.
496
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 552?561,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Semantic Topic Models: Combining Word Distributional Statistics and
Dictionary Definitions
Weiwei Guo
Department of Computer Science,
Columbia University,
weiwei@cs.columbia.edu
Mona Diab
Center for Computational Learning Systems,
Columbia University,
mdiab@ccls.columbia.edu
Abstract
In this paper, we propose a novel topic
model based on incorporating dictionary
definitions. Traditional topic models treat
words as surface strings without assuming
predefined knowledge about word mean-
ing. They infer topics only by observing
surface word co-occurrence. However, the
co-occurred words may not be semanti-
cally related in a manner that is relevant
for topic coherence. Exploiting dictionary
definitions explicitly in our model yields
a better understanding of word semantics
leading to better text modeling. We exploit
WordNet as a lexical resource for sense
definitions. We show that explicitly mod-
eling word definitions helps improve per-
formance significantly over the baseline
for a text categorization task.
1 Introduction
Latent Dirichlet Allocation (LDA) (Blei et al,
2003) serves as a data-driven framework in model-
ing text corpora. The statistical model allows vari-
able extensions to integrate linguistic features such
as syntax (Griffiths et al, 2005), and has been ap-
plied in many areas.
In LDA, there are two factors which determine
the topic of a word: the topic distribution of the
document, and the probability of a topic to emit
this word. This information is learned in an unsu-
pervised manner to maximize the likelihood of the
corpus. However, this data-driven approach has
some limitations. If a word is not observed fre-
quently enough in the corpus, then it is likely to
be assigned the dominant topic in this document.
For example, the word grease (a thick fatty oil) in
a political domain document should be assigned
the topic chemicals. However, since it is an in-
frequent word, LDA cannot learn its correct se-
mantics from the observed distribution, the LDA
model will assign it the dominant document topic
politics. If we look up the semantics of the word
grease in a dictionary, we will not find any of its
meanings indicating the politics topic, yet there is
ample evidence for the chemical topic. Accord-
ingly, we hypothesize that if we know the seman-
tics of words in advance, we can get a better in-
dication of their topics. Therefore, in this paper,
we test our hypothesis by exploring the integration
of word semantics explicitly in the topic modeling
framework.
In order to incorporate word semantics from
dictionaries, we recognize the need to model
sense-topic distribution rather than word-topic dis-
tribution, since dictionaries are constructed at the
sense level. We use WordNet (Fellbaum, 1998)
as our lexical resource of choice. The notion of
a sense in WordNet goes beyond a typical word
sense in a traditional dictionary since a WordNet
sense links senses of different words that have
similar meanings. Accordingly, the sense for the
first verbal entry for buy and for purchase will
have the same sense id (and same definition) in
WordNet, while they could have different mean-
ing definitions in a traditional dictionary such as
the Merriam Webster Dictionary or LDOCE. In
our model, a topic will first emit a WordNet sense,
then the sense will generate a word. This is in-
spired by the intuition that words are instantiations
of concepts.
The paper is organized as follows: In Sections 2
and 3, we describe our models based on WordNet.
In Section 4, experiment results on text catego-
rization are presented. Moreover, we analyze both
qualitatively and quantitatively the contribution of
modeling definitions (by teasing out the contribu-
tion of explicit sense modeling in a word sense dis-
ambiguation task). Related work is introduced in
Section 5. We conclude in Section 6 by discussing
some possible future directions.552
d
(a)
d
T S
sense
n ?/N
sen
d
s
(b)
Figure 1: (a) LDA: Latent Dirichlet Allocation
(Blei et al, 2003). (b) STM: Semantic topic
model. The dashed arrows indicate the distribu-
tions (? and ?) and nodes (z) are not influenced by
the values of pointed nodes.
2 Semantic Topic Model
2.1 Latent Dirichlet Allocation
We briefly introduce LDA where Collapsed Gibbs
Sampling (Griffiths and Steyvers, 2004) is used
for inference. In figure 1a, given a corpus with
D documents, LDA will summarize each docu-
ment as a normalized T -dimension topic mixture
?. Topic mixture ? is drawn from a Dirichlet distri-
bution Dir(?) with a symmetric prior ?. ? con-
tains T multinomial distribution, each represent-
ing the probability of a topic z generating word w
p(w|z). ? is drawn from a Dirichlet distribution
Dir(?) with prior ?.
In Collapsed Gibbs Sampling, the distribution
of a topic for the word wi = w based on values of
other data is computed as:
P (zi = z|z?i,w) ?
n(d)?i,z + ?
n(d)?i + T?
? n
w
?i,z + ?
n?i,z +W?
(1)
In this equation, n(d)?i,z is a count of how many
words are assigned topic z in document d, exclud-
ing the topic of the ith word; nw?i,z is a count of
how many words = w are assigned topic z, also
excluding the topic of the ith word. Hence, the
first fraction is the proportion of the topic in this
document p(z|?). The second fraction is the prob-
ability of topic z emitting wordw. After the topics
become stable, all the topics in a document con-
struct the topic mixture ?.
2.2 Applying Word Sense Disambiguation
Techniques
We add a sense node between the topic node and
the word node based on two linguistic observa-
tions: a) Polysemy: many words have more than
one meaning. A topic is more directly relevant to
a word meaning (sense) than to a word due to pol-
ysemy; b) Synonymy: different words may share
the same sense. WordNet explicitly models syn-
onymy by linking synonyms to the same sense. In
WordNet, each sense has an associated definition.
It is worth noting that we model the sense-word
relation differently from (Boyd-Graber and Blei,
2007), where in their model words are generated
from topics, then senses are generated from words.
In our model, we assume that during the genera-
tive process, the author picks a concept relevant to
the topic, then thinks of a best word that represents
that concept. Hence the word choice is dependent
on the relatedness of the sense and its fit to the
document context.
In standard topic models, the topic of a word
is sampled from the document level topic mixture
?. The underlying assumption is that all words in a
document constitute the context of the target word.
However, it is not the case in real world corpora.
Titov and McDonald (2008) find that using global
topic mixtures can only extract global topics in on-
line reviews (e.g., Creative Labs MP3 players and
iPods) and ignores local topics (product features
such as portability and battery). They design the
Multi-grain LDA where the local topic of a word
is only determined by topics of surrounding sen-
tences. In word sense disambiguation (WSD), an
even narrower context is taken into consideration,
for instance in graph based WSD models (Mihal-
cea, 2005), the choice of a sense for a word only
depends on a local window whose size equals the
length of the sentence. Later in (Sinha and Mihal-
cea, 2007; Guo and Diab, 2010; Li et al, 2010),
people use a fixed window size containing around
12 neighbor words for WSD.
Accordingly, we adopt the WSD inspired local
window strategy in our model. However, we do553
not employ the complicated schema in (Titov and
McDonald, 2008). We simply hypothesize that the
surrounding words are semantically related to the
considered word, and they construct a local slid-
ing window for that target word. For a document
d with Nd words, we represent it as Nd local win-
dows ? a window is created for each word. The
model is illustrated in the left rectangle in figure
1b. The window size is fixed for each word: it
contains /2 preceding words, and /2 following
words. Therefore, a word in the original document
will have  copies, existing in +1 local windows.
Similarly, there are  + 1 pairs of topics/senses
assigned for each word in the original document.
Each window has a distribution ?i over topics. ?i
will emit the topics of words in the window.
This approach enables us to exploit different
context sizes without restricting it to the sentence
length, and hence spread topic information across
sentence boundaries.
2.3 Integrating Definitions
Intuitively, a sense definition reveals some prior
knowledge on the topic domain: the definition of
sense [crime, offense, offence] indicates a legal
topic; the definition of sense [basketball] indicates
a sports topic, etc. Therefore, during inference, we
want to choose a topic/sense pair for each word,
such that the topic is supported by the context ?
and the sense definition also matches that topic.
Given that words used in the sense definitions
are strongly relevant to the sense/concept, we set
out to find the topics of those definition words, and
accordingly assign the sense sen itself these top-
ics. We treat a sense definition as a document and
perform Gibbs sampling on it. We normalize def-
inition length by a variable ?. Therefore, before
the topic model sees the actual documents, each
sense s has been sampled ? times. The ? topics
are then used as a ?training set?, so that given a
sense, ? has some prior knowledge of which topic
it should be sampled from.
Consider the sense [party, political party] with
a definition ?an organization to gain political
power? of length 6 when ? = 12. If topic
model assigns politics topic to the words ?orga-
nization political power?, then sense [party, polit-
ical party] will be sampled from politics topic for
3 ? ?/definitionLength = 6 times.
We refer to the proposed model as Semantic
Topic Model (figure 1b). For each window vi in
the document set, the model will generate a distri-
bution of topics ?i. It will emit the topics of + 1
words in the window. For a word wij in window
vi, a sense sij is drawn from the topic, and then sij
generates the word wi. Sense-topic distribution ?
contains T multinomial distributions over all pos-
sible senses in the corpus drawn from a symmetric
Dirichlet distribution Dir(?). From WordNet we
know the set of words W (s) that have a sense s
as an entry. A sense s can only emit words from
W (s). Hence, for each sense s, there is a multi-
nomial distribution ?s over W (s). All ? are drawn
from symmetric Dir(?).
On the definition side, we use a different prior
?s to generate a topic mixture ?. Aside from gen-
erating si, zi will deterministically generate the
current sense sen for ?/Nsen times (Nsen is the
number of words in the definition of sense sen),
so that sen is sampled ? times in total.
The formal procedure of generative process is
the following:
For the definition of sense sen:
? choose topic mixture ? ? Dir(?s).
? for each word wi:
? choose topic zi ?Mult(?).
? choose sense si ?Mult(?zi).
? deterministically choose sense sen ?
Mult(?zi) for ?/Nsen times.
? choose word wi ?Mult(?si).
For each window vi in a document:
? choose local topic mixture ?i ? Dir(?d).
? for each word wij in vi:
? choose topic zij ?Mult(?i).
? choose sense sij ?Mult(?zij ).
? choose word wij ?Mult(?sij ).
2.4 Using WordNet
Since definitions and documents are in different
genre/domains, they have different distributions
on senses and words. Besides, the definition sets
contain topics from all kinds of domains, many of
which are irrelevant to the document set. Hence
we prefer ? and ? that are specific for the doc-
ument set, and we do not want them to be ?cor-
rupted? by the text in the definition set. There-
fore, as in figure 1b, the dashed lines indicate that
when we estimate ? and ?, the topic/sense pair and
sense/word pairs in the definition set are not con-
sidered.
WordNet senses are connected by relations such
as synonymy, hypernymy, similar attributes, etc.554
We observe that neighboring sense definitions are
usually similar and are in the same topic domain.
Hence, we represent the definition of a sense as
the union of itself with its neighboring sense def-
initions pertaining to WordNet relations. In this
way, the definition gets richer as it considers more
data for discovering reliable topics.
3 Inference
We still use Collapsed Gibbs Sampling to find la-
tent variables. Gibbs Sampling will initialize all
hidden variables randomly. In each iteration, hid-
den variables are sequentially sampled from the
distribution conditioned on all the other variables.
In order to compute the conditional probability
P (zi = z, si = s|z?i, s?i,w) for a topic/sense
pair, we start by computing the joint probability
P (z, s,w) = P (z)P (s|z)P (w|s). Since the gen-
erative processes are not exactly the same for def-
initions and documents, we need to compute the
joint probability differently. We use a type spe-
cific subscript to distinguish them: Ps(?) for sense
definitions and Pd(?) for documents.
Let sen be a sense. Integrating out ? we have:
Ps(z) =
(
?(T?s)
?(?s)T
)S S?
sen=1
?
z ?(n
(sen)
z + ?s)
?(n(sen) + T?) (2)
where n(sen)z means the number of times a word
in the definition of sen is assigned to topic z, and
n(sen) is the length of the definition. S is all the
potential senses in the documents.
We have the same formula of P (s|z) and
P (w|s) for definitions and documents. Similarly,
let nz be the number of words in the documents
assigned to topic z, and nsz be the number of times
sense s assigned to topic z. Note that when s
appears in the superscript surrounded by brackets
such as n(s)z , it denotes the number of words as-
signed to topics z in the definition of sense s. By
integrating out ? we obtain the second term:
P (s|z) =
(
?(S?)
?(?)S
)T T?
z=1
?
s ?(nsz + n
(s)
z ?/n(s) + ?)
?(nz +
?
s? n
(s?)
z ?/n(s?) + S?)
(3)
At last, assume ns denotes the number of sense
s in the documents, and nws denotes the number of
sense s to generate the word w, then integrating
out ? we have:
P (w|s) =
S?
s=1
?(|W (s)|?)
?(?)|W (s)|
?W (s)
w ?(nws + ?)
?(ns + |W (s)|?)
(4)
With equation 2-4, we can compute the condi-
tional probability Ps(zi = z, si = s|z?i, s?i,w)
for a sense-topic pair in the sense definition. Let
seni be the sense definition containing word wi,
then we have:
Ps(zi = z, si = s|z?i, s?i,w) ?
n(seni)?i,z + ?s
n(seni)?i + T?s
nsz + n(s
?)
?i,z?/n(s
?) + ?
nz +
?
s? n
(s?)
?i,z?/n(s?) + S?
nws + ?
ns + |W (s)|?
(5)
The subscript ?i in expression n?i denotes
the number of certain events excluding word wi.
Hence the three fractions in equation 5 correspond
to the probability of choosing z from ?sen, choos-
ing s from z and choosingw from s. Also note that
our model defines s that can only generate words
in W (s), therefore for any word w /? W (s), the
third fraction will yield a 0.
The probability for documents is similar to that
for definitions except that there is a topic mixture
for each word, which is estimated by the topics in
the window. Hence Pd(z) is estimated as:
Pd(z) =
?
i
?(T?d)
?(?d)T
?
z ?(n
(vi)z + ?d)
?(n(vi) + T?d)
(6)
Thus, the conditional probability for documents
can be estimated by cancellation terms in equation
6, 3, and 4:
Pd(zij = z, sij = s|z?ij, s?ij,w) ?
n(vi)?ij,z + ?d
n(vi)?ij + T?d
ns?ij,z + n(s
?)
z ?/n(s
?) + ?
n?ij,z +
?
s? n
(s?)
z ?/n(s?) + S?
nw?ij,s + ?
n?ij,s + |W (s)|?
(7)
3.1 Approximation
In current model, each word appears in + 1 win-
dows, and will be generated  + 1 times, so there
will be  + 1 pairs of topics/senses sampled for
each word, which requires a lot of additional com-
putation (proportional to context size ). On the
other hand, it can be imagined that the set of val-
ues {zij , sij |j ? /2 ? i ? j + /2} in dif-
ferent windows vi should roughly be the same,
since they are hidden values for the same wordwj .
Therefore, to reduce computation complexity dur-
ing Gibbs sampling, we approximate the values of
{zij , sij | i 6= j} by the topic/sense (zjj , sjj) that
are generated from window vj . That is, in Gibbs
sampling, the algorithm does not actually sample
the values of {zij , sij , | i 6= j}; instead, it directly
assumes the sampled values are zjj , sjj .555
4 Experiments and Analysis
Data: We experiment with several datasets,
namely, the Brown Corpus (Brown), New York
Times (NYT) from the American National Cor-
pus, Reuters (R20) and WordNet definitions. In a
preprocessing step, we remove all the non-content
words whose part of speech tags are not one of
the following set {noun, adjective, adverb, verb}.
Moreover, words that do not have a valid lemma in
WordNet are removed. For WordNet definitions,
we remove stop words hence focusing on relevant
content words.
Corpora statistics after each step of preprocess-
ing is presented in Table 1. The column WN token
lists the number of word#pos tokens after prepro-
cessing. Note that now we treat word#pos as a
word token. The column word types shows cor-
responding word#pos types, and the total number
of possible sense types is listed in column sense
types. The DOCs size for WordNet is the total
number of senses defined in WordNet.
Experiments: We design two tasks to test our
models: (1) text categorization task for evaluat-
ing the quality of values of topic nodes, and (2) a
WSD task for evaluating the quality of the values
of the sense nodes, mainly as a diagnostic tool tar-
geting the specific aspect of sense definitions in-
corporation and distinguish that component?s con-
tribution to text categorization performance. We
compare the performance of four topic models.
(a) LDA: the traditional topic model proposed in
(Blei et al, 2003) except that it uses Gibbs Sam-
pling for inference. (b) LDA+def: is LDA with
sense definitions. However they are not explic-
itly modeled; rather they are treated as documents
and used as augmented data. (c) STM0: the topic
model with an additional explicit sense node in the
model, but we do not model the sense definitions.
And finally (d) STMn is the full model with defi-
nitions explicitly modeled. In this setting n is the
? value. We experiment with different ? values
in the STM models, and investigate the semantic
scope of words/senses by choosing different win-
dow size . We report mean and standard deviation
based on 10 runs.
It is worth noting that a larger window size
 suggests documents have larger impact on the
model (?, ?) than definitions, since each document
word has  copies. This is not a desirable property
when we want to investigate the weight of defi-
nitions by choosing different ? values. Accord-
ingly, we only use zjj , sjj , wjj to estimate ?, ?, so
that the impact of documents is fixed. This makes
more sense, in that after the approximation in sec-
tion 3.1, there is no need to use {zij , sij , | i 6= j}
(they have the same values as zjj , sjj).
4.1 Text Categorization
We believe our model can generate more ?correct?
topics by looking into dictionaries. In topic mod-
els, each word is generalized as a topic and each
document is summarized as the topic mixture ?,
hence it is natural to evaluate the quality of in-
ferred topics in a text categorization task. We fol-
low the classification framework in (Griffiths et
al., 2005): first run topic models on each dataset
individually without knowing label information
to achieve document level topic mixtures, then we
employ Naive Bayes and SVM (both implemented
in the WEKA Toolkit (Hall et al, 2009)) to per-
form classification on the topic mixtures. For all
document, the features are the percentage of top-
ics. Similar to (Griffiths et al, 2005), we assess in-
ferred topics by the classification accuracy of 10-
fold cross validation on each dataset.
We evaluate our models on three datasets in the
cross validation manner: The Brown corpus which
comprises 500 documents grouped into 15 cate-
gories (same set used in (Griffiths et al, 2005));
NYT comprising 800 documents grouped into the
16 most frequent label categories; Reuters (R20)
comprising 8600 documents labeled with the most
frequent 20 categories. In R20, combination of
categories is treated as separate category labels,
so money, interest and interest are considered
different labels.
For the three datasets, we use the Brown cor-
pus only as a tuning set to decide on the topic
model parameters for all of our experimentation,
and use the optimized parameters directly on NYT
and R20 without further optimization.
4.1.1 Classification Results
Searching ? and  on Brown: The classification
accuracy on the Brown corpus with different  and
? values using Naive Bayes and SVM are pre-
sented in figure 2a and 2b. In this section, the
number of topics T is set to 50. The possible
 values in the horizontal axis are 2, 10, 20, 40,
all. The possible ? values are 0, 1, 2. Note that
 = all means that no local window is used, and
? = 0 means definitions are not used. The hyper-556
Corpus DOCs size orig tokens content tokens WN tokens word types sense types
Brown 500 1022393 580882 547887 27438 46645
NYT 800 743665 436988 393120 19025 37631
R20 8595 901691 450935 417331 9930 24834
SemCor 352 676546 404460 352563 28925 45973
WordNet 117659 1447779 886923 786679 42080 60567
Table 1: Corpus statistics
0 10 20 30 40 50 all40
45
50
55
60
65
70
window size
accur
acy%
 
 LDALDA+defSTM0STM1STM2
(a) Naive Bayes on Brown
0 10 20 30 40 50 all40
45
50
55
60
65
70
75
window size
accur
acy%
 
 LDALDA+defSTM0STM1STM2
(b) SVM on Brown
0 10 20 30 40 50 all55
60
65
70
75
80
window size
accur
acy%
 
 STM0STM1STM2
(c) SVM on NYT
Figure 2: Classification accuracy at different parameter settings
parameters are tuned as ?d = 0.1, ?s = 0.01, ? =
0.01, ? = 0.1.
From figure 2, we observe that results using
SVM have the same trend as Naive Bayes except
that the accuracies are roughly 5% higher for SVM
classifier. The results of LDA and LDA+def sug-
gest that simply treating definitions as documents
in an augmented data manner does not help. Com-
paring SMT0 with LDA in the same  values, we
find that explicitly modeling the sense node in the
model greatly improves the classification results.
The reason may be that words in LDA are inde-
pendent isolated strings, while in STM0 they are
connected by senses.
STM2 prefers smaller window sizes ( less than
40). That means two words with a distance larger
than 40 are not necessarily semantically related or
share the same topic. This  number also corre-
lates with the optimal context window size of 12
reported in WSD tasks (Sinha and Mihalcea, 2007;
Guo and Diab, 2010).
Classification results: Table 2 shows the results
of our models using best tuned parameters of  =
10, ? = 2 on 3 datasets. We present three base-
lines in Table 2: (1) WEKA uses WEKA?s classi-
fiers directly on bag-of-words without topic mod-
eling. The values of features are simply term fre-
quency. (2) WEKA+FS performs feature selection
using information gain before applying classifica-
tion. (3) LDA, is the traditional topic model. Note
that Griffiths et al?s (2005) implementation of
LDA achieve 51% on Brown corpus using Naive
Bayes . Finally the Table illustrates the results
obtained using our proposed models STM0 (?=0)
and STM2 (? = 2).
It is worth noting that R20 (compared to NYT)
is a harder condition for topic models. This is
because fewer words (10000 distinct words ver-
sus 19000 in NYT) are frequently used in a large
training set (8600 documents versus 800 in NYT),
making the surface word feature space no longer
as sparse as in the NYT or Brown corpus, which
implies simply using surface words without con-
sidering the words distributional statistics ? topic
modeling ? is good enough for classification. In
(Blei et al, 2003) figure 10b they also show worse
text categorization results over the SVM baseline
when more than 15% of the training labels of
Reuters are available for the SVM classifiers, indi-
cating that LDA is less necessary with large train-
ing data. In our investigation, we report results
on SVM classifiers trained on the whole Reuters
training set. In our experiments, LDA fails to cor-
rectly classify nearly 10% of the Reuters docu-
ments compared to the WEKA baseline, however
STM2 can still achieve significantly better accu-
racy (+4%) in the SVM classification condition.
Table 2 illustrates that despite the difference be-
tween NYT, Reuters and Brown (data size, genre,
domains, category labels), exploiting WSD tech-
niques (namely using a local window size cou-
pled with explicitly modeling a sense node) yields557
Brown NYT R20
NB SVM NB SVM NB SVM
WEKA 48 47.8 57 54.1 72.4 82.9
WEKA+FS 50 47.2 56.9 55.1 72.9 83.4
LDA 47.8?4.3 53.9?3.8 48.5?5.5 53.8?3.5 61.0?3.3 72.5?2.5
STM0 68.6?3.5 70.7?3.9 66.7?3.8 74.2?4.0 72.7?3.5 85.2?0.9
STM2 69.3?3.3 75.4?3.7 74.6?3.3 79.3?2.5 73?3.7 86.9?1.2
Table 2: Classification results on 3 datasets using hyperparameters tuned on Brown.
0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 1650
60
70
80
90
100
acc
ura
cy%
 
 
STM0
STM2
1.Sports  2.Politics  3.National News  4.Entertainment  5.International News6.Society  7.Business  8.Miscellaneous  9.Finance  10.Culture  11.Science12.Health  13.Law  14.Technology  15.Religion  16.Environment
Figure 3: SVM accuracy on each category of NYT
significantly better results than all three baselines
including LDA. Furthermore, explicit definition
modeling as used in STM2 yields the best perfor-
mance consistently overall.
Finally, in Figure 2c we show the SVM clas-
sification results on NYT in different parame-
ter settings. We find that the NYT classifica-
tion accuracy trend is consistent with that on the
Brown corpus for each parameter setting of  ?
{2, 10, 20, 40, all} and ? ? {0, 1, 2}. This further
proves the robustness of STMn.
4.2 Analysis on the Impact of Modeling
Definitions
4.2.1 Qualitative Analysis
To understand why definitions are helpful in text
categorization, we analyze the SVM performance
of STM0 and STM2 ( = 10) on each cate-
gory of NYT dataset (figure 3). We find STM2
outperforms STM0 in all categories. However,
the largest gain is observed in Society, Miscel-
laneous, Culture, Technology. For Technology,
we should credit WordNet definitions, since Tech-
nology may contain many infrequent technical
terms, and STM0 cannot generalize the meaning
of words only by distributional information due to
their low frequency usage. However in some other
domains, fewer specialized words are repeatedly
used, hence STM0 can do as well as STM2.
For the other 3 categories, we hypothesize that
these documents are likely to be a mixture of mul-
tiple topics. For example, a Culture news could
contain topics pertaining to religion, history, art;
while a Society news about crime could relate to
law, family, economics. In this case, it is very
important to sample a true topic for each word,
so that ML algorithms can distinguish the Cul-
ture documents from the Religion ones by the pro-
portion of topics. Accordingly, adding definitions
should be very helpful, since it specifically defines
the topic of a sense, and shields it from the influ-
ence of other ?incorrect/irrelevant? topics.
4.2.2 Quantitative Analysis with Word Sense
Disambiguation
A side effect of our model is that it sense disam-
biguates all words. As a means of analyzing and
gaining some insight into the exact contribution of
explicitly incorporating sense definitions (STMn)
versus simply a sense node (STM0) in the model,
we investigate the quality of the sense assignments
in our models. We believe that the choice of the
correct sense is directly correlated with the choice
of a correct topic in our framework. Accord-
ingly, a relative improvement of STMn over STM0
(where the only difference is the explicit sense def-
inition modeling) in WSD task is an indicator of
the impact of using sense definitions in the text
categorization task.
WSD Data: We choose the all-words WSD task in
which an unsupervised WSD system is required to
disambiguate all the content words in documents.
Our models are evaluated against the SemCor
dataset. We prefer SemCor to all-words datasets
available in Senseval-3 (Snyder and Palmer, 2004)
or SemEval-2007 (Pradhan et al, 2007), since
it includes many more documents than either set
(350 versus 3) and therefore allowing more reli-
able results. Moreover, SemCor is also the dataset
used in (Boyd-Graber et al, 2007), where a Word-
Net based topic model for WSD is introduced. The558
Total Noun Adjective Adverb Verb
sense annotated words 225992 86996 31729 18947 88320
polysemous words 187871 70529 21989 11498 83855
TF-IDF - 0.422 0.300 0.153 0.182
Table 3: Statistics of SemCor per POS
statistics of SemCor is listed in table 3.
We use hyperparameters tuned from the text cat-
egorization task: ?d=0.1, ?s=0.01, ?=0.01, ?=1,
T=50, and try different values of  ? {10, 20, 40}
and ? ? {0, 2, 10}. The Brown corpus and Word-
Net definitions corpus are used as augmented data,
which means the dashed line in figure 1c will be-
come bold. Finally, we choose the most frequent
answer for each word in the last 10 iterations of a
Gibbs Sampling run as the final sense choice.
WSD Results: Disambiguation per POS results
are presented in table 4. We only report results
on polysemous words. We can see that modeling
definitions (STM2 and STM10) improves perfor-
mance significantly over STM0?s across the board
per POS and overall. The fact that STMn picks
more correct senses helps explain why STMn clas-
sifies more documents correctly than STM0. Also
it is interesting to see that unlike in the text cate-
gorization task, larger values of ? generate better
WSD results. However, the window size , does
not make a significant difference, yet we note that
=10 is still the optimal value, similar to our ob-
servation in the text categorization task.
STM10 achieves similar results as in LDAWN
(Boyd-Graber et al, 2007) which was specifically
designed for WSD. LDAWN needs a fine grained
hypernym hierarchy to perform WSD, hence they
can only disambiguate nouns. They report differ-
ent performances under various parameter setting.
We cite their best performance of 38% accuracy
on nouns as a comparison point to our best perfor-
mance for nouns of 38.5%.
An interesting feature of STM10 is that it
performs much better in nouns than adverbs and
verbs, compared to a random baseline in Table
4. This is understandable since topic information
content is mostly borne by nouns and adjectives,
while adverbs and verbs tend to be less informa-
tive about topics (e.g., even, indicate, take), and
used more across different domain documents.
Hence topic models are weaker in their ability
to identify clear cues for senses for verbs and
adverbs. In support of our hypothesis about the
POS distribution, we compute the average TF-IDF
scores for each POS (shown in Table 3 according
to the equation illustrated below). The average
TF-IDF clearly indicate the positive skewness of
the nouns and adjectives (high TF-IDF) correlates
with the better WSD performance.
TF-IDF(pos) =
?
i
?
d TF-IDF(wi,d)
# of wi,d
where wi,d ? pos.
At last, we notice that the most frequent sense
baseline performs much better than our models.
This is understandable since: (1) most frequent
sense baseline can be treated as a supervised
method in the sense that the sense frequency is
calculated based on the sense choice as present
in sense annotated data; (2) our model is not de-
signed for WSD, therefore it discards a lot of in-
formation when choosing the sense: in our model,
the choice of a sense si is only dependent on two
facts: the corresponding topic zi and word wi,
while in (Li et al, 2010; Banerjee and Pedersen,
2003), they consider all the senses and words in
the context words.
5 Related work
Various topic models have been developed for
many applications. Recently there is a trend
of modeling document dependency (Dietz et al,
2007; Mei et al, 2008; Daume, 2009). How-
ever, topics are only inferred based on word co-
occurrence, while word semantics are ignored.
Boyd-Graber et al (2007) are the first to inte-
grate semantics into the topic model framework.
They propose a topic model based on WordNet
noun hierarchy for WSD. A word is assumed to be
generated by first sampling a topic, then choosing
a path from the root node of hierarchy to a sense
node corresponding to that word. However, they
only focus on WSD. They do not exploit word def-
initions, neither do they report results on text cat-
egorization.
Chemudugunta et al (2008) also incorporate a
sense hierarchy into a topic model. In their frame-
work, a word may be directly generated from a
topic (as in standard topic models), or it can be559
Total Noun Adjective Adverb Verb
random 22.1 26.2 27.9 32.2 15.8
most frequent sense 64.7 74.7 77.5 74.0 59.6
STM0  = 10 24.1?1.4 29.3?4.3 28.7?1.1 34.1?3.1 17.1?1.6
 = 20 24?1.3 30.2?3.3 29.1?1.4 34.9?3.1 15.9?0.7
 = 40 24?2.4 28.4?4.3 28.7?1.1 36.4?4.7 17.3?2.4
STM2  = 10 27.5?1.1 36.1?3.8 34.0?1.2 33.4?1.8 17.8?1.4
 = 20 25.7?1.3 32.0?4.2 33.5?0.7 34.2?3.4 17.3?0.7
 = 40 26.1?1.3 32.5?3.9 33.6?0.9 34.2?3.4 17.5?1.4
STM10  = 10 28.8?1.1 38.5?2.3 34.7?0.8 34.0?3.3 18.4?1.2
 = 20 27.7?1.0 36.8?2.2 34.5?0.7 33.0?3.1 17.6?0.7
 = 40 28.1?1.5 38.4?3.1 34.0?1.0 35.1?5.4 17.0?0.9
Table 4: Disambiguation results per POS on polysemous words.
generated by choosing a sense path in the hierar-
chy. Note that no topic information is on the sense
path. If a word is generated from the hierarchy,
then it is not assigned a topic. Their models based
on different dictionaries improve perplexity.
Recently, several systems have been proposed
to apply topic models to WSD. Cai et al (2007)
incorporate topic features into a supervised WSD
framework. Brody and Lapata (2009) place the
sense induction in a Baysian framework by assum-
ing each context word is generated from the target
word?s senses, and a context is modeled as a multi-
nomial distribution over the target word?s senses
rather than topics. Li et al (2010) design sev-
eral systems that use latent topics to find a most
likely sense based on the sense paraphrases (ex-
tracted from WordNet) and context. Their WSD
models are unsupervised and outperform state-of-
art systems.
Our model borrows the local window idea from
word sense disambiguation community. In graph-
based WSD systems (Mihalcea, 2005; Sinha and
Mihalcea, 2007; Guo and Diab, 2010), a node is
created for each sense. Two nodes will be con-
nected if their distance is less than a predefined
value; the weight on the edge is a value returned
by sense similarity measures, then the PageR-
ank/Indegree algorithm is applied on this graph to
determine the appropriate senses.
6 Conclusion and Future Work
We presented a novel model STM that combines
explicit semantic information and word distribu-
tion information in a unified topic model. STM
is able to capture topics of words more accurately
than traditional LDA topic models. In future work,
we plan to model the WordNet sense network. We
believe that WordNet senses are too fine-grained,
hence we plan to use clustered senses, instead of
current WN senses, in order to avail the model of
more generalization power.
Acknowledgments
This research was funded by the Ofce of the Direc-
tor of National Intelligence (ODNI), Intelligence
Advanced Research Projects Activity (IARPA),
through the U.S. Army Research Lab. All state-
ments of fact, opinion or conclusions contained
herein are those of the authors and should not be
construed as representing the ofcial views or poli-
cies of IARPA, the ODNI or the U.S. Government.
References
Satanjeev Banerjee and Ted Pedersen. 2003. Extended
gloss overlaps as a measure of semantic relatedness.
In Proceedings of the 18th International Joint Con-
ference on Artificial Intelligence, pages 805?810.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. Journal of Ma-
chine Learning Research, 3:993?1022.
Jordan Boyd-Graber and David M. Blei. 2007. Putop:
turning predominant senses into a topic model for
word sense disambiguation. In Proceedings of the
4th International Workshop on Semantic Evalua-
tions, pages 277?281.
Jordan Boyd-Graber, David M. Blei, and Xiaojin Zhu.
2007. A topic model for word sense disambiguation.
In Proceedings of the 2007 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP-CoNLL), pages 1024?1033.
Samuel Brody and Mirella Lapata. 2009. Bayesian
word sense induction. In Proceedings of the 12th
Conference of the European Chapter of the ACL,
pages 103?111.
Jun Fu Cai, Wee Sun Lee, and Yee Whye Teh. 2007.
Improving word sense disambiguation using topic
features. In Proceedings of 2007 Joint Confer-
ence on Empirical Methods in Natural Language560
Processing and Computational Natural Language
Learning, pages 1015?1023.
Chaitanya Chemudugunta, Padhraic Smyth, and Mark
Steyvers. 2008. Combining concept hierarchies
and statistical topic models. In Proceedings of the
17th ACM conference on Information and knowl-
edge management, pages 1469?1470.
Hal Daume. 2009. Markov random topic fields. In
Proceedings of the ACL-IJCNLP Conference, pages
293?296.
Laura Dietz, Steffen Bickel, and Tobias Scheffer. 2007.
Unsupervised prediction of citation influence. In
Proceedings of the 24th international conference on
Machine learning, pages 233?240.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
Academy of Sciences, 101:5228?5235.
Thomas L. Griffiths, Mark Steyvers, David M. Blei,
and Joshua B. Tenenbaum. 2005. Integrating top-
ics and syntax. In Advances in Neural Information
Processing Systems.
Weiwei Guo and Mona Diab. 2010. Combining or-
thogonal monolingual and multilingual sources of
evidence for all words wsd. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 1542?1551.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: an update.
SIGKDD Explor. Newsl., 11:10?18.
Linlin Li, Benjamin Roth, and Caroline Sporleder.
2010. Topic models for word sense disambiguation
and token-based idiom detection. In Proceedings of
the 48th Annual Meeting of the Association for Com-
putational Linguistics, pages 1138?1147.
Qiaozhu Mei, Deng Cai, Duo Zhang, and Chengxiang
Zhai. 2008. Topic modeling with network regu-
larization. In Proceedings of the 17th international
conference on World Wide Web, pages 101?110.
Rada Mihalcea. 2005. Unsupervised large-vocabulary
word sense disambiguation with graph-based algo-
rithms for sequence data labeling. In Proceedings
of the Joint Conference on Human Language Tech-
nology and Empirical Methods in Natural Language
Processing, pages 411?418.
Sameer S. Pradhan, Edward Loper, Dmitriy Dligach,
and Martha Palmer. 2007. Semeval-2007 task 17:
English lexical sample, srl and all words. In Pro-
ceedings of the 4th International Workshop on Se-
mantic Evaluations, pages 87?92. ACL.
Ravi Sinha and Rada Mihalcea. 2007. Unsupervised
graph-based word sense disambiguation using mea-
sures of word semantic similarity. In Proceedings
of the IEEE International Conference on Semantic
Computing, pages 363?369.
Benjamin Snyder and Martha Palmer. 2004. The en-
glish all-words task. In Senseval-3: Third Interna-
tional Workshop on the Evaluation of Systems for the
Semantic Analysis of Text, pages 41?43. ACL.
Ivan Titov and Ryan McDonald. 2008. Modeling
online reviews with multi-grain topic models. In
Proceedings of the 17th international conference on
World Wide Web, pages 111?120.
561
Proceedings of NAACL-HLT 2013, pages 739?745,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Improving Lexical Semantics for Sentential Semantics: Modeling Selectional
Preference and Similar Words in a Latent Variable Model
Weiwei Guo
Department of Computer Science
Columbia University
weiwei@cs.columbia.edu
Mona Diab
Department of Computer Science
George Washington University
mtdiab@gwu.edu
Abstract
Sentence Similarity [SS] computes a similar-
ity score between two sentences. The SS task
differs from document level semantics tasks
in that it features the sparsity of words in a
data unit, i.e. a sentence. Accordingly it is
crucial to robustly model each word in a sen-
tence to capture the complete semantic picture
of the sentence. In this paper, we hypoth-
esize that by better modeling lexical seman-
tics we can obtain better sentential semantics.
We incorporate both corpus-based (selectional
preference information) and knowledge-based
(similar words extracted in a dictionary) lex-
ical semantics into a latent variable model.
The experiments show state-of-the-art perfor-
mance among unsupervised systems on two
SS datasets.
1 Introduction
Sentence Similarity [SS] is emerging as a crucial
step in many NLP tasks that focus on sentence level
semantics such as word sense disambiguation (Guo
and Diab, 2010; Guo and Diab, 2012a), summariza-
tion (Zhou et al, 2006), text coherence (Lapata and
Barzilay, 2005), tweet clustering (Sankaranarayanan
et al, 2009; Jin et al, 2011), etc. SS operates in a
very small context, on average 11 words per sen-
tence in Semeval-2012 dataset (Agirre et al, 2012),
resulting in inadequate evidence to generalize to ro-
bust sentential semantics.
Weighted Textual Matrix Factorization [WTMF]
(Guo and Diab, 2012b) is a latent variable model that
outperforms Latent Semantic Analysis [LSA] (Deer-
wester et al, 1990) and Latent Dirichelet Allocation
[LDA] (Blei et al, 2003) models by a large margin in
the SS task, yielding state-of-the-art performance on
the LI06 (Li et al, 2006) SS dataset. However, all of
these models make harsh simplifying assumptions
on how a token is generated: (1) in LSA/WTMF, a
token is generated by the inner product of the word
latent vector and the document latent vector; (2) in
LDA, all the tokens in a document are sampled from
the same document level topic distribution. Under
this framework, they ignore rich linguistic phenom-
ena such as inter-word dependency, semantic scope
of words, etc. This is a result of simply using docu-
ment IDs as features to represent a word.
Modeling quality lexical semantics in latent vari-
able models does not draw enough attention in the
community, since people usually apply dimension
reduction techniques for documents, which have
abundant words for extracting the document level
semantics. However, in the SS setting, it is crucial to
make good use of each word, given the limited num-
ber of words in a sentence. We believe a reasonable
word generation story will avoid introducing noise
in sentential semantics, encouraging robust lexical
semantics which can further boost the sentential se-
mantics. In this paper, we explicitly encode lexical
semantics, both corpus-based and knowledge-based
information, in the WTMF model, by which we are
able to achieve even better results in SS task.
The additional corpus-based information we ex-
ploit is selectional preference semantics (Resnik,
1997), a feature already existing in the data yet ig-
nored by most latent variable models. Selectional
preference focuses on the admissible arguments for
a word, thus capturing more nuanced semantics than
the sentence IDs (when applied to a corpus of sen-
tences as opposed to documents). Consider the fol-
lowing example:
739
Figure 1: matrix factorization
Many analysts say the global Brent crude oil bench-
mark price, currently around $111 a barrel ...
In WTMF/LSA/LDA, a word will receive semantics
from all the other words in a sentence, hence, the
word oil, in the above example, will be assigned the
incorrect finance topic that reflects the sentence level
semantics. Moreover, the problem worsens for ad-
jectives, adverbs and verbs, which have a much nar-
rower semantic scope than the whole sentence. For
example, the verb say should only be associated with
analyst (only receiving semantics from analyst), as
it is not related to other words in the sentence. In
contrast, oil, according to its selectional preference,
should be associated with crude indicating the re-
source topic. We believe modeling selectional pref-
erence capturing local evidence completes the se-
mantic picture for words, hence further rendering
better sentential semantics. To our best knowledge,
this is the first work to model selectional preference
for sentence/document semantics.
We also integrate knowledge-based semantics
in the WTMF framework. Knowledge-based se-
mantics, a human-annotated clean resource, is an
important complement to corpus-based noisy co-
occurrence information. We extract similar word
pairs from Wordnet (Fellbaum, 1998). Leveraging
these pairs, an infrequent word such as purchase
can exploit robust latent vectors from its synonyms
such as buy. Similar words pairs can be seamlessly
modeled in WTMF, since in the matrix factorization
framework a latent vector profile is explicitly created
for each word, while in LDA all the data structures
are designed for documents/sentences. We construct
a graph to connect words according to the extracted
similar word pairs, to encourage similar words to
share similar latent vector profiles. We will refer to
our proposed novel model as WTMF+PK.
2 Weighted Textual Matrix Factorization
Our previous work (Guo and Diab, 2012b) models
the sentences in the weighted matrix factorization
framework (Figure 1). The corpus is stored in an
M ?N matrix X , with each cell containing the TF-
IDF values of words. The rows of X are M distinct
words and columns are N sentences. As in Figure
1, X is approximated by the product of a K ?M
matrix P and a K?N matrix Q. Accordingly, each
sentence sj is represented by a K dimensional la-
tent vector Q?,j . Similarly a word wi is generalized
by P?,i. P and Q is optimized by minimize the ob-
jective function:
?
i
?
j
Wij (P?,i ?Q?,j ?Xij)
2 + ?||P ||22 + ?||Q||
2
2
Wi,j =
{
1, if Xij 6= 0
wm, if Xij = 0
(1)
where ? is a regularization term. Missing tokens are
modeled by assigning a different weightwm for each
0 cell in the matrix X . We can see the inner product
of a word vector P?,i and a sentence vector Q?,j is
used to approximate the cell Xij .
The graphical model of WTMF is illustrated in
Figure 2a. A wi/sj node is a latent vector P?,i/Q?,j ,
corresponding to a word/sentence, respectively. A
shaded node is a non-zero cell in X , representing
an observed token in a sentence. For simplicity, the
missing tokens and weights are not shown in the
graph.
3 Corpus-based Semantics: Selectional
Preference
In this paper, we focus on selectional preference that
reflects the association of two words: if two words
form a bigram, then the two words should share
similar latent dimensions. In the previous example,
crude and oil form a bigram, and they share the re-
source topic. In our framework, this is implemented
by adding extra columns in X , so that each addi-
tional column corresponds to a bigram, treating each
bigram as a pseudo-sentence for the two words. The
graphical model is illustrated in Figure 2b. There-
fore, oil will receive more resource topic from crude
through the bigram crude oil, instead of only finance
topic from the sentence as a whole.
Each non-zero cell in the new columns of X , i.e.
an observed token in a bigram (pseudo-sentence), is
given a different weight:
Wi,j =
?
?
?
1, if Xij 6= 0 and j is a sentence index
? ? freq(j), if Xij 6= 0 and j is a bigram index
wm, if Xij = 0
740
w1	 ? w2	 ? w3	 ? w4	 ? w5	 ?
s1	 ? s2	 ?
b2	 ?(c)	 ?
b1	 ?
w1	 ? w2	 ? w3	 ? w4	 ? w5	 ?
s1	 ? s2	 ?
(a)	 ? (b)	 ?
w1	 ? w2	 ? w3	 ? w4	 ? w5	 ?
s1	 ? s2	 ?
b2	 ?b1	 ?
Figure 2: WTMF+PK model (WTMF + corpus-based Selectional [P]references semantics + [K]nowledge-based
semantics): a w/s/b node represents a word/sentence/bigram, respectively
freq(j) denotes the frequency of bigram j appear-
ing in the corpus, hence the strength of association is
differentiated such that higher weights are assigned
on the more probable bigrams. The coefficient ? is
the importance of selectional preference. A larger
? indicates that we trust the selectional preference
over the global sentential semantics.
4 Knowledge-based Semantics: Similar
Word Pairs
We first extract synonym pairs from WordNet, which
are words associated with the same sense, synset.
We further expand the set by exploiting the relations
defined in WordNet. For the extracted words, we
consider the first sense of each word, and if it is con-
nected to other senses by any of the WordNet defined
relations (hypernym, similar words, etc.), then we
treat the words associated with the other senses as
similar words. In total, we are able to discover 80K
pairs of similar words for the 46K distinct words in
our corpus.
Given a pair of similar words wi1/wi2 , we want
the two corresponding latent vectors P?,i1/P?,i2 to be
as close as possible, namely the cosine similarity to
be close to 1. Accordingly, a term is added in equa-
tion 1 for each similar word pair wi1/wi2 :
? ? (P?,i1 ? P?,i2 ? |P?,i1 ||P?,i2 |)
2 (2)
|P?,i| denotes the length of the vector P?,i. The co-
efficient ?, analogous to ?, denotes the importance
of the knowledge-based evidence. The Figure 2c
shows the final WTMF+PK model.
5 Inference
In (Guo and Diab, 2012b) we use Alternating Least
Square [ALS] for inference, which is to set the
derivative of equation 1 for P/Q to 0 and iteratively
compute P/Q by fixing the other matrix (Srebro and
Jaakkola, 2003). However, it is no longer applicable
with the new term (equation 2) involving the length
of word vectors |P?,i|. Therefore we approximate the
objective function by treating the vector length |P?,i|
as fixed values during the ALS iterations:
Q?,j =
(
PW? (j)P> + ?I
)?1
PW? (j)X?,j
P?,i =
(
QW? (i)Q> + ?I + ?P?,s(i)P
>
?,s(i)
)?1
(
QW? (i)X>i,? + ?LiP?,s(i)Ls(i)
)
(3)
where P?,s(i) are the latent vectors of similar words
of word i; the length of these vectors in the current
iteration are stored in Ls(i) (similarly Li is the cur-
rent length of P?,i) (cf. (Steck, 2010; Guo and Diab,
2012b) for optimization details).
6 Experimental Setting
We build the model WTMF+PK on the same cor-
pora as used in our previous work (Guo and Diab,
2012b), comprising the following: Brown corpus
(each sentence is treated as a document), sense def-
initions from Wiktionary and Wordnet (only defini-
tions without target words and usage examples). We
follow the preprocessing steps in (Guo and Diab,
2012c): tokenization, pos-tagging, lemmatization
and further merge lemmas. The corpus is used for
building matrix X .
The evaluation datasets are LI06 dataset and
Semeval-2012 STS [STS12] (Agirre et al, 2012)
dataset. LI06 consists of 30 sentence pairs (dic-
tionary definitions). For STS12,1 the training data
(2000 pairs) are used as the tuning set for setting the
1A detailed description of the data sets is provided in (Agirre
et al, 2012).
741
parameters of our models. This data comprises msr-
par, msr-vid, smt-eur. Once the models are tuned,
we evaluate them on the STS12 test data that com-
prises 3150 sentence pairs from msr-par, msr-vid,
smt-eur, smt-news, On-WN. It is worth noting that
smt-news and On-WN are not part of the tuning data.
We use cosine similarity to measure the similarity
scores between two sentences. Pearson correlation
between the system?s answer and gold standard sim-
ilarity scores is used as the evaluation metric.
We include three baselines LSA, LDA and
WTMF using the setting described in (Guo and
Diab, 2012b). We run Gibbs Sampling based LDA
for 2000 iterations and average the model over the
last 10 iterations. For WTMF, we run 20 iterations
and fix the missing words weight at wm = 0.01 with
a regularization coefficient set at ? = 20, which is
the best condition found in (Guo and Diab, 2012b).
7 Experiments
Table 1 summarizes the results at dimension K =
100 (the dimension of latent vectors). To remove
randomness, each reported number is the averaged
results of 10 runs. Based on the STS tuning set,
we experiment with different values for the selec-
tional preference weight (? = {0, 1, 2}), and like-
wise for the similar word pairs weight varying the ?
value as follows ? = {0, 0.1, 0.3, 0.5, 0.7}. The per-
formance on STS12 tuning and test dataset as well
as on the LI06 dataset are illustrated in Figures 3a,
3b and 3d. The parameters of model 6 in Table 1
(? = 2, ? = 0.3) are the chosen values based on
tuning set performance.
7.1 Evaluation on the STS12 datasets
Table 1 shows WTMF is already a very strong base-
line: it outperforms LSA and LDA by a large mar-
gin. Same as in (Guo and Diab, 2012b), LSA per-
formance degrades dramatically when trained on a
corpus of sentence sized documents, yielding results
worse than the surface words baseline 31% (Agirre
et al, 2012). Using corpus-based selectional prefer-
ence semantics alone (model 4 WTMF+P in Table
1) boosts the performance of WTMF by +1.17% on
the test set, while using knowledge-based semantics
alone (model 5 WTMF+K) improves the over the
WTMF results by an absolute +2.31%. Combining
them (model 6 WTMF+PK) yields the best results,
with an absolute increase of +3.39%, which sug-
gests that the two sources of semantic evidence are
useful, but more importantly, they are complemen-
tary for each other.
Table 1 also presents the performance on each in-
dividual dataset. The gain on each individual source
is not as much as the overall gain, which suggests
part of the overall gain comes from the correct rank-
ing of intra-source pairs. Note that WTMF+PK im-
proves all individual datasets except smt-eur. This
may be caused by too many overlapping words in
the sentence pairs in smt-eur, while our approach
focuses on extracting similarity between different
words.
Observing the performance using different values
of weights in figure 3a and 3b, we can conclude
that the selectional preference and similar word pairs
yield very promising results. The trends hold in
different parameter conditions with a consistent im-
provement. Figure 3c illustrates the impact of di-
mension K = {50, 75, 100, 125, 150} on WTMF
and WTMF+PK. Generally a larger K leads to a
higher Pearson correlation, but the improvement is
tiny when K ? 100 (0.1% increase).
Compared to all the unsupervised systems that
participated in Semeval STS 2012 task, WTMF+PK
yields state-of-the-art performance (70.70%).2 In
(Guo and Diab, 2012c) we also apply WTMF (K =
100) on STS12, achieving a correlation of 69.5%.
However, additional data is incorporated in the train-
ing corpora: (1) STS12 tuning set; (2) for WordNet
and Wiktionary data, the target words are also in-
cluded in the definitions (hence synonym pairs were
used); (3) the usage examples of target words were
also appended to the definitions.3 While trained with
this experimental setting, our model WTMF+PK
(? = 2, ? = 0.3,K = 100) is able to reach an even
higher correlation of 72.0%.
2WTMF+PK is an unsupervised system, since the gold stan-
dard similarly scores are never used in the objective function.
Moreover, even without a tuning set, a non-zero value of ? or ?
will always improve the baseline WTMF according to figure 3a
and 3b.
3We do not adopt this corpora schema, since some defini-
tions are test set sentences in On-WN, thereby adding target
words and usage examples introduces additional information
for some of the test set sentences
742
Models Parameters STS12 tune STS12 test msr-par msr-vid On-WN smt-eur smt-news LI06
1. LSA - 21.67% 24.41% 27.18% 9.91% 50.93% 27.86% 19.73% 63.77%
2. LDA ? = 0.05, ? = 0.05 71.10% 63.18% 29.15% 76.73% 62.81% 47.81% 27.2% 83.71%
3. WTMF - 71.41% 67.31% 44.00% 82.59% 70.78% 50.89% 37.77% 89.81%
4. WTMF+P ? = 2, ? = 0 72.94% 68.48% 46.21% 83.29% 70.61% 49.54% 39.50% 90.16%
5. WTMF+K ? = 0, ? = 0.3 73.84% 69.64% 45.04% 83.04% 70.40% 49.88% 41.66% 90.11%
6. WTMF+PK ? = 2, ? = 0.3 75.29% 70.70% 46.77% 83.90% 71.03% 49.77% 40.48% 90.17%
Table 1: Evaluation Results using Pearson Correlation on STS12 and LI06
0 0.1 0.3 0.5 0.771
72
73
74
75
cor
rela
tion
%
?
 
 
?=0?=1?=2
(a) STS12 tuning set (K = 100)
0 0.1 0.3 0.5 0.767
68
69
70
71
?
 
 
?=0?=1?=2
(b) STS12 test set (K = 100)
50 75 100 125 15066
67
68
69
70
71
K
 
 
WTMFWTMF+PK, ?=2, ?=0.3
(c) STS12 test set
0 0.1 0.3 0.5 0.789.5
90
90.5
91
?
 
 ?=0?=1?=2
(d) LI06 (K = 100)
Figure 3: Pearson correlation at different parameter settings
7.2 Evaluation on the LI06 dataset
Figure 3d presents the results obtained on the LI06
data set at different weight values for the corpus-
based selectional preference semantics ? and for the
knowledge-based semantics ?. Our previous exper-
iments (Guo and Diab, 2012b) show that WTMF
is the state-of-the-art model on LI06. With lexi-
cal semantics explicitly modeled, WTMF+PK yields
better results than WTMF (see Table 1). It should
be noted that LI06 prefers a smaller similar word
pair weight ( a ? = 0.1 yields the best perfor-
mance around of 90.75%), yet in almost all condi-
tions WTMF+PK outperforms WTMF as shown in
Figure 3d.
8 Related Work
SS has progressed immensely in recent years, espe-
cially with the establishment of the Semantic Tex-
tual Similarity task in SEMEVAL 2012. Early work
in SS focused on word pair similarity in the high di-
mensional space (Li et al, 2006; Liu et al, 2007;
Islam and Inkpen, 2008; Tsatsaronis et al, 2010; Ho
et al, 2010), where co-occurrence information was
not efficiently exploited. Researchers (O?Shea et al,
2008) find LSA does not yield good performance. In
(Guo and Diab, 2012b; Guo and Diab, 2012c), we
show the superiority of the latent space approach in
WTMF. In this paper, we improve the WTMF model
and achieve state-of-the-art Pearson correlation on
two standard SS datasets.
There are latent variable models designed for lex-
ical semantics, such as word senses (Boyd-Graber
et al, 2007; Guo and Diab, 2011), function words
(Griffiths et al, 2005), selectional preference (Ritter
et al, 2010), synonyms and antonyms (Yih et al,
2012), etc. However little improvement is shown
on document/sentence level semantics: (Ritter et al,
2010) and (Yih et al, 2012) focus on selectional
preference and antonym identification, respectively;
in (Griffiths et al, 2005) the LDA performance de-
grades in the text categorization task including the
modeling of function words. Rather, we concentrate
on nuanced lexical semantics phenomena that could
benefit sentential semantics.
9 Conclusion
We incorporate corpus-based (selectional prefer-
ence) and knowledge-based (similar word pairs) lex-
ical semantics into a latent variable model. Our
system yields state-of-the-art unsupervised perfor-
mance on two most popular and standard SS
datasets.
10 Acknowledgment
This work is supported by the IARPA SCIL pro-
gram.
743
References
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. Semeval-2012 task 6: A
pilot on semantic textual similarity. In First Joint
Conference on Lexical and Computational Semantics
(*SEM).
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. Journal of Machine
Learning Research, 3.
Jordan Boyd-Graber, David Blei, and Xiaojin Zhu. 2007.
A topic model for word sense disambiguation. In Pro-
ceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning.
Scott Deerwester, Susan T Dumais, George W Furnas,
Thomas K Landauer, and Richard Harshman. 1990.
Indexing by latent semantic analysis. Journal of the
American Society for Information Science.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
Thomas L. Griffiths, Mark Steyvers, David M. Blei, and
Joshua B. Tenenbaum. 2005. Integrating topics and
syntax. In Advances in Neural Information Processing
Systems.
Weiwei Guo and Mona Diab. 2010. Combining orthogo-
nal monolingual and multilingual sources of evidence
for all words wsd. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics.
Weiwei Guo and Mona Diab. 2011. Semantic topic mod-
els: Combining word distributional statistics and dic-
tionary definitions. In Proceedings of the 2011 Con-
ference on Empirical Methods in Natural Language
Processing.
Weiwei Guo and Mona Diab. 2012a. Learning the latent
semantics of a concept by its definition. In Proceed-
ings of the 50th Annual Meeting of the Association for
Computational Linguistics.
Weiwei Guo and Mona Diab. 2012b. Modeling sen-
tences in the latent space. In Proceedings of the 50th
Annual Meeting of the Association for Computational
Linguistics.
Weiwei Guo and Mona Diab. 2012c. Weiwei: A simple
unsupervised latent semantics based approach for sen-
tence similarity. In First Joint Conference on Lexical
and Computational Semantics (*SEM).
Chukfong Ho, Masrah Azrifah Azmi Murad, Rabiah Ab-
dul Kadir, and Shyamala C. Doraisamy. 2010. Word
sense disambiguation-based sentence similarity. In
Proceedings of the 23rd International Conference on
Computational Linguistics.
Aminul Islam and Diana Inkpen. 2008. Semantic
text similarity using corpus-based word similarity and
string similarity. ACM Transactions on Knowledge
Discovery from Data, 2.
Ou Jin, Nathan N. Liu, Kai Zhao, Yong Yu, and Qiang
Yang. 2011. Transferring topical knowledge from
auxiliary long texts for short text clustering. In Pro-
ceedings of the 20th ACM international conference on
Information and knowledge management.
Mirella Lapata and Regina Barzilay. 2005. Automatic
evaluation of text coherence: Models and representa-
tions. In Proceedings of the 19th International Joint
Conference on Artificial Intelligence.
Yuhua Li, Davi d McLean, Zuhair A. Bandar, James D.
O?Shea, and Keeley Crockett. 2006. Sentence similar-
ity based on semantic nets and corpus statistics. IEEE
Transaction on Knowledge and Data Engineering, 18.
Xiao-Ying Liu, Yi-Ming Zhou, and Ruo-Shi Zheng.
2007. Sentence similarity based on dynamic time
warping. In The International Conference on Seman-
tic Computing.
James O?Shea, Zuhair Bandar, Keeley Crockett, and
David McLean. 2008. A comparative study of two
short text semantic similarity measures. In Proceed-
ings of the Agent and Multi-Agent Systems: Technolo-
gies and Applications, Second KES International Sym-
posium (KES-AMSTA).
Philip Resnik. 1997. Selectional preference and sense
disambiguation. In Proceedings of the ACL SIGLEX
Workshop on Tagging Text with Lexical Semantics:
Why, What, and How?
Alan Ritter, Mausam, and Oren Etzioni. 2010. A latent
dirichlet alocation method for selectional preferences.
In Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics.
Jagan Sankaranarayanan, Hanan Samet, Benjamin E.
Teitler, Michael D. Lieberman, and Jon Sperling.
2009. Twitterstand: news in tweets. In Proceedings of
the 17th ACM SIGSPATIAL International Conference
on Advances in Geographic Information Systems.
Nathan Srebro and Tommi Jaakkola. 2003. Weighted
low-rank approximations. In Proceedings of the Twen-
tieth International Conference on Machine Learning.
Harald Steck. 2010. Training and testing of recom-
mender systems on data missing not at random. In
Proceedings of the 16th ACM SIGKDD International
Conference on Knowledge Discovery and Data Min-
ing.
George Tsatsaronis, Iraklis Varlamis, and Michalis Vazir-
giannis. 2010. Text relatedness based on a word the-
saurus. Journal of Articial Intelligence Research, 37.
Wentau Yih, Geoffrey Zweig, and John C. Platt. 2012.
Polarity inducing latent semantic analysis. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning.
744
Liang Zhou, Chin-Yew Lin, Dragos Stefan Munteanu,
and Eduard Hovy. 2006. Paraeval: Using paraphrases
to evaluate summaries automatically. In Proceedings
of Human Language Technology Conference of the
North American Chapter of the ACL,.
745
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1542?1551,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Combining Orthogonal Monolingual and Multilingual Sources of
Evidence for All Words WSD
Weiwei Guo
Computer Science Department
Columbia University
New York, NY, 10115
weiwei@cs.columbia.edu
Mona Diab
Center for Computational Learning Systems
Columbia University
New York, NY, 10115
mdiab@ccls.columbia.edu
Abstract
Word Sense Disambiguation remains one
of the most complex problems facing com-
putational linguists to date. In this pa-
per we present a system that combines
evidence from a monolingual WSD sys-
tem together with that from a multilingual
WSD system to yield state of the art per-
formance on standard All-Words data sets.
The monolingual system is based on a
modification of the graph based state of the
art algorithm In-Degree. The multilingual
system is an improvement over an All-
Words unsupervised approach, SALAAM.
SALAAM exploits multilingual evidence
as a means of disambiguation. In this
paper, we present modifications to both
of the original approaches and then their
combination. We finally report the highest
results obtained to date on the SENSEVAL
2 standard data set using an unsupervised
method, we achieve an overall F measure
of 64.58 using a voting scheme.
1 Introduction
Despite advances in natural language processing
(NLP), Word Sense Disambiguation (WSD) is still
considered one of the most challenging problems
in the field. Ever since the field?s inception, WSD
has been perceived as one of the central problems
in NLP. WSD is viewed as an enabling technology
that could potentially have far reaching impact on
NLP applications in general. We are starting to see
the beginnings of a positive effect of WSD in NLP
applications such as Machine Translation (Carpuat
and Wu, 2007; Chan et al, 2007).
Advances in WSD research in the current mil-
lennium can be attributed to several key factors:
the availability of large scale computational lexi-
cal resources such as WordNets (Fellbaum, 1998;
Miller, 1990), the availability of large scale cor-
pora, the existence and dissemination of standard-
ized data sets over the past 10 years through differ-
ent testbeds such as SENSEVAL and SEMEVAL
competitions,1 devising more robust computing
algorithms to handle large scale data sets, and sim-
ply advancement in hardware machinery.
In this paper, we address the problem of WSD
of all content words in a sentence, All-Words data.
In this framework, the task is to associate all to-
kens with their contextually relevant meaning defi-
nitions from some computational lexical resource.
Our work hinges upon combining two high qual-
ity WSD systems that rely on essentially differ-
ent sources of evidence. The two WSD systems
are a monolingual system RelCont and a multi-
lingual system TransCont. RelCont is an en-
hancement on an existing graph based algorithm,
In-Degree, first described in (Navigli and Lapata,
2007). TransCont is an enhancement over an
existing approach that leverages multilingual evi-
dence through projection, SALAAM, described in
detail in (Diab and Resnik, 2002). Similar to the
leveraged systems, the current combined approach
is unsupervised, namely it does not rely on training
data from the onset. We show that by combining
both sources of evidence, our approach yields the
highest performance for an unsupervised system
to date on standard All-Words data sets.
This paper is organized as follows: Section 2
delves into the problem of WSD in more detail;
Section 3 explores some of the relevant related
work; in Section 4, we describe the two WSD
systems in some detail emphasizing the improve-
ments to the basic systems in addition to a de-
scription of our combination approach; we present
our experimental set up and results in Section 5;
we discuss the results and our overall observations
with error analysis in Section 6; Finally, we con-
1http://www.semeval.org
1542
clude in Section 7.
2 Word Sense Disambiguation
The definition of WSD has taken on several differ-
ent practical meanings in recent years. In the latest
SEMEVAL 2010 workshop, there are 18 tasks de-
fined, several of which are on different languages,
however we recognize the widening of the defi-
nition of the task of WSD. In addition to the tra-
ditional All-Words and Lexical Sample tasks, we
note new tasks on word sense discrimination (no
sense inventory needed, the different senses are
merely distinguished), lexical substitution using
synonyms of words as substitutes both monolin-
gually and multilingually, as well as meaning def-
initions obtained from different languages namely
using words in translation.
Our paper is about the classical All-Words
(AW) task of WSD. In this task, all content bear-
ing words in running text are disambiguated from
a static lexical resource. For example a sen-
tence such as ?I walked by the bank and saw
many beautiful plants there.? will have the verbs
?walked, saw?, the nouns ?bank, plants?, the ad-
jectives ?many, beautiful?, and the adverb ?there?,
be disambiguated from a standard lexical resource.
Hence, using WordNet,2 ?walked? will be assigned
the corresponding meaning definitions of: to use
one?s feet to advance; to advance by steps, ?saw?
will be assigned the meaning definition of: to per-
ceive by sight or have the power to perceive by
sight, the noun ?bank? will be assigned the mean-
ing definition of: sloping land especially the slope
beside a body of water, and so on.
3 Related Works
Many systems over the years have been proposed
for the task. A thorough review of the state of
the art through the late 1990s (Ide and Veronis,
1998) and more recently in (Navigli, 2009). Sev-
eral techniques have been used to tackle the prob-
lem ranging from rule based/knowledge based
approaches to unsupervised and supervised ma-
chine learning techniques. To date, the best ap-
proaches that solve the AW WSD task are super-
vised as illustrated in the different SenseEval and
SEMEVAL AW task (Palmer et al, 2001; Snyder
and Palmer, 2004; Pradhan et al, 2007).
In this paper, we present an unsupervised com-
bination approach to the AW WSD problem that
2http://wordnet.princeton.edu
relies on WN similarity measures in conjunction
with evidence obtained through exploiting multi-
lingual evidence. We will review the closely rele-
vant related work on which this current investiga-
tion is based.3
4 Our Approach
Our current investigation exploits two basic unsu-
pervised approaches that perform at state-of-the-
art for the AW WSD task in an unsupervised set-
ting. Crucially the two systems rely on differ-
ent sources of evidence allowing them to comple-
ment each other to a large extent leading to better
performance than for each system independently.
Given a target content word and co-occurring con-
textual clues, the monolingual system RelCont
attempts to assign the approporiate meaning def-
inition to the target word. Such words by defini-
tion are semantically related words. TransCont,
on the other hand, is the multilingual system.
TransCont defines the notion of context in the
translational space using a foreign word as a fil-
ter for defining the contextual content words for
a given target word. In this multilingual setting,
all the words that are mapped to (aligned with)
the same orthographic form in a foreign language
constitute the context. In the next subsections
we describe the two approaches RelCont and
TransCont in some detail, then we proceed to
describe two combination methods for the two ap-
proaches: MERGE and VOTE.
4.1 Monolingual System RelCont
RelCont is based on an extension of a state-
of-the-art WSD approach by (Sinha and Mihal-
cea, 2007), henceforth (SM07). In the basic
SM07 work, the authors combine different seman-
tic similarity measures with different graph based
algorithms as an extension to work in (Mihal-
cea, 2005). Given a sequence of words W =
{w1, w2...wn}, each word wi with several senses
{si1, si2...sim}. A graph G = (V,E) is defined such
that there exists a vertex v for each sense. Two
senses of two different words may be connected by
an edge e, depending on their distance. That two
senses are connected suggests they should have
influence on each other, accordingly a maximum
3We acknowledge the existence of many research papers
that tackled the AW WSD problem using unsupervised ap-
proaches, yet for lack of space we will not be able to review
most of them.
1543
allowable distance is set. They explore 4 differ-
ent graph based algorithms. The highest yield-
ing algorithm in their work is the In-Degree al-
gorithm combining different WN similarity mea-
sures depending on POS. They used the Jiang
and Conrath (JCN) (Jiang and Conrath., 1997)
similarity measure within nouns, the Leacock &
Chodorow (LCH) (Leacock and Chodorow, 1998)
similarity measure within verbs, and the Lesk
(Lesk, 1986) similarity measure within adjectives,
within adverbs, and among different POS tag pair-
ings. They evaluate their work against the SEN-
SEVAL 2 AW test data (SV2AW). They tune the
parameters of their algorithm ? namely, the nor-
malization ratio for some of these measures ? on
the SENSEVAL 3 data set. They report a state-of-
the-art unsupervised system that yields an overall
performance across all AW POS sets of 57.2%.
In our current work, we extend the SM07 work
in some interesting ways. A detailed narrative
of our approach is described in (Guo and Diab,
2009). Briefly, we focus on the In-Degree
graph based algorithm since it is the best per-
former in the SM07 work. The In-Degree al-
gorithm presents the problem as a weighted graph
with senses as nodes and the similarity between
senses as weights on edges. The In-Degree
of a vertex refers to the number of edges inci-
dent on that vertex. In the weighted graph, the
In-Degree for each vertex is calculated by sum-
ming the weights on the edges that are incident on
it. After all the In-Degree values for each sense
are computed, the sense with maximum value is
chosen as the final sense for that word.
In this paper, we use the In-Degree algo-
rithm while applying some modifications to the
basic similarity measures exploited and the WN
lexical resource tapped into. Similar to the orig-
inal In-Degree algorithm, we produce a prob-
abilistic ranked list of senses. Our modifications
are described as follows:
JCN for Verb-Verb Similarity In our imple-
mentation of the In-Degree algorithm, we use
the JCN similarity measure for both Noun-Noun
similarity calculation similar to SM07. However,
different from SM07, instead of using LCH for
Verb-Verb similarity, we use the JCN metric as it
yields better performance in our experimentations.
Expand Lesk Following the intuition in (Ped-
ersen et al, 2005), henceforth (PEA05), we ex-
pand the basic Lesk similarity measure to take into
account the glosses for all the relations for the
synsets on the contextual words and compare them
with the glosses of the target word senses, there-
fore going beyond the is-a relation. We exploit the
observation that WN senses are too fine-grained,
accordingly the neighbors would be slightly varied
while sharing significant semantic meaning con-
tent. To find similar senses, we use the relations:
hypernym, hyponym, similar attributes, similar
verb group, pertinym, holonym, and meronyms.4
The algorithm assumes that the words in the input
are POS tagged. In PEA05, the authors retrieve all
the relevant neighbors to form a bag of words for
both the target sense and the surrounding senses of
the context words, they specifically focus on the
Lesk similarity measure. In our current work, we
employ the neighbors in a disambiguation strategy
using different similarity measures one pair at a
time. Our algorithm takes as input a target sense
and a sense pertaining to a word in the surrounding
context, and returns a sense similarity score. We
do not apply the WN relations expansion to the
target sense. It is only applied to the contextual
word.5
For the monolingual system, we employ the
same normalization values used in SM07 for the
different similarity measures. Namely for the Lesk
and Expand-Lesk, we use the same cut-off value of
240, accordingly, if the Lesk or Expand-Lesk sim-
ilarity value returns 0 <= 240 it is converted to
a real number in the interval [0,1], any similarity
over 240 is by default mapped to 1. We will refer
to the Expand-Lesk with this threshold as Lesk2.
We also experimented with different thresholds for
the Lesk and Expand-Lesk similarity measure us-
ing the SENSEVAL 3 data as a tuning set. We
found that a cut-off threshold of 40 was also use-
ful. We will refer to this variant of Expand-Lesk
with a cut off threshold of 40 as Lesk3. For JCN,
similar to SM07, the values are from 0.04 to 0.2,
we mapped them to the interval [0,1]. We did not
run any calibration studies beyond the what was
reported in SM07.
4In our experiments, we varied the number of relations to
employ and they all yielded relatively similar results. Hence
in this paper, we report results using all the relations listed
above.
5We experimented with expanding both the contextual
sense and the target sense and we found that the unreliabil-
ity of some of the relations is detrimental to the algorithm?s
performance. Hence we decided empirically to expand only
the contextual word.
1544
SemCor Expansion of WN A part of the
RelCont approach relies on using the Lesk al-
gorithm. Accordingly, the availability of glosses
associated with the WN entries is extremely bene-
ficial. Therefore, we expand the number of glosses
available in WN by using the SemCor data set,
thereby adding more examples to compare. The
SemCor corpus is a corpus that is manually sense
tagged (Miller, 1990).6 In this expansion, depend-
ing on the version of WN, we use the sense-index
file in the WN Database to convert the SemCor
data to the appropriate version sense annotations.
We augment the sense entries for the different POS
WN databases with example usages from SemCor.
The augmentation is done as a look up table exter-
nal to WN proper since we did not want to dabble
with the WN offsets. We set a cap of 30 additional
examples per synset. We used the first 30 exam-
ples with no filtering criteria. Many of the synsets
had no additional examples. WN1.7.1 comprises a
total of 26875 synsets, of which 25940 synsets are
augmented with SemCor examples.7
4.2 Multilingual System TransCont
TransCont is based on the WSD system
SALAAM (Diab and Resnik, 2002), henceforth
(DR02). The SALAAM system leverages word
alignments from parallel corpora to perform WSD.
The SALAAM algorithm exploits the word corre-
spondence cross linguistically to tag word senses
on words in running text. It relies on several un-
derlying assumptions. The first assumption is that
senses of polysemous words in one language could
be lexicalized differently in other languages. For
example, ?bank? in English would be translated as
banque or rive de fleuve in French, depending on
context. The other assumption is that if Language
1 (L1) words are translated to the same ortho-
graphic form in Language 2 (L2), then they share
the some element of meaning, they are semanti-
cally similar.8
The SALAAM algorithm can be described as
follows. Given a parallel corpus of L1-L2 that
6Using SemCor in this setting to augment WN does hint
of using supervised data in the WSD process, however, since
our approach does not rely on training data and SemCor is not
used in our algorithm directly to tag data, but to augment a
rich knowledge resource, we contend that this does not affect
our system?s designation as an unsupervised system.
7Some example sentences are repeated across different
synsets and POS since the SemCor data is annotated as an
All-Words tagged data set.
8We implicitly make the underlying simplifying assump-
tion that the L2 words are less ambiguous than the L1 words.
is sentence and word aligned, group all the word
types in L1 that map to same word in L2 creat-
ing clusters referred to as typesets. Then perform
disambiguation on the typeset clusters using WN.
Once senses are identified for each word in the
cluster, the senses are propagated back to the origi-
nal word instances in the corpus. In the SALAAM
algorithm, the disambiguation step is carried out
as follows: within each of these target sets con-
sider all possible sense tags for each word and
choose sense tags informed by semantic similarity
with all the other words in the whole group. The
algorithm is a greedy algorithm that aims at maxi-
mizing the similarity of the chosen sense across all
the words in the set. The SALAAM disambigua-
tion algorithm used the noun groupings (Noun-
Groupings) algorithm described in DR02. The al-
gorithm applies disambiguation within POS tag.
The authors report only results on the nouns only
since NounGroupings heavily exploits the hierar-
chy structure of the WN noun taxonomy, which
does not exist for adjectives and adverbs, and is
very shallow for verbs.
Essentially SALAAM relies on variability in
translation as it is important to have multiple
words in a typeset to allow for disambiguation.
In the original SALAAM system, the authors au-
tomatically translated several balanced corpora in
order to render more variable data for the approach
to show it?s impact. The corpora that were trans-
lated are: the WSJ, the Brown corpus and all the
SENSEVAL data. The data were translated to dif-
ferent languages (Arabic, French and Spanish) us-
ing state of art MT systems. They employed the
automatic alignment system GIZA++ (Och and
Ney, 2003) to obtain word alignments in a single
direction from L1 to L2.
For TransCont we use the basic SALAAM
approach with some crucial modifications that
lead to better performance. We still rely on par-
allel corpora, we extract typesets based on the in-
tersection of word alignments in both alignment
directions using more advanced GIZA++ machin-
ery. In contrast to DR02, we experiment with
all four POS: Verbs (V), Nouns (N), Adjectives
(A) and Adverbs (R). Moreover, we modified the
underlying disambiguation method on the type-
sets. We still employ WN similarity, however, we
do not use the NounGroupings algorithm. Our
disambiguation method relies on calculating the
sense pair similarity exhaustively across all the
1545
word types in a typeset and choosing the combi-
nation that yields the highest similarity. We exper-
imented with all the WN similarity measures in
the WN similarity package.9 We also experiment
with Lesk2 and Lesk3 as well as other measures,
however we do not use SemCor examples with
TransCont. We found that the best results are
yielded using the Lesk2/Lesk3 similarity measure
for N, A and R POS tagsets, while the Lin and JCN
measures yield the best performance for the verbs.
In contrast to the DR02 approach, we modify the
internal WSD process to use the In-Degree al-
gorithm on the typeset, so each sense obtains a
confidence, and the sense(s) with the highest con-
fidences are returned.
4.3 Combining RelCont and TransCont
Our objective is to combine the different sources
of evidence for the purposes of producing an effec-
tive overall global WSD system that is able to dis-
ambiguate all content words in running text. We
combine the two systems in two different ways.
4.3.1 MERGE
In this combination scheme, the words in the type-
set that result from the TransCont approach are
added to the context of the target word in the
RelCont approach. However the typeset words
are not treated the same as the words that come
from the surrounding context in the In-Degree
algorithm as we recognize that words that are
yielded in the typesets are semantically similar in
terms of content rather than being co-occurring
words as is the case for contextual words in Rel-
Cont. Heeding this difference, we proceed to
calculate similarity for words in the typesets us-
ing different similarity measures. In the case of
noun-noun similarity, in the original RelCont
experiments we use JCN, however with the words
present in the TransCont typesets we use one
of the Lesk variants, Lesk2 or Lesk3. Our obser-
vation is that the JCN measure is relatively coarser
grained, compared to Lesk measures, therefore it
is sufficient in case of lexical relatedness therefore
works well in case of the context words. Yet for
the words yielded in the TransCont typesets a
method that exploits the underlying rich relations
in the noun hierarchy captures the semantic sim-
ilarity more aptly. In the case of verbs we still
maintain the JCN similarity as it most effective
9http://wn-similarity.sourceforge.net/
given the shallowness of the verb hierarchy and
the inherent nature of the verbal synsets which are
differentiated along syntactic rather than semantic
dimensions. We employ the Lesk algorithm still
with A-A and R-R similarity and when comparing
across different POS tag pairings.
4.3.2 VOTE
In this combination scheme, the output of the
global disambiguation system is simply an inter-
section of the two outputs from the two underly-
ing systems RelCont and TransCont. Specif-
ically, we sum up the confidence ranging from
0 to 1 of the two system In-Degree algo-
rithm outputs to obtain a final confidence for each
sense, choosing the sense(s) that yields the high-
est confidences. The fact that TransCont uses
In-Degree internally allows for a seamless in-
tegration.
5 Experiments and Results
5.1 Data
The parallel data we experiment with are the
same standard data sets as in (Diab and Resnik,
2002), namely, Senseval 2 English AW data sets
(SV2AW) (Palmer et al, 2001), and Seneval 3 En-
glish AW (SV3AW) data set. We use the true POS
tag sets in the test data as rendered in the Penn
Tree Bank.10 We present our results on WordNet
1.7.1 for ease of comparison with previous results.
5.2 Evaluation Metrics
We use the scorer2 software to report fine-
grained (P)recision and (R)ecall and (F)-measure.
5.3 Baselines
We consider here several baselines. 1. A random
baseline (RAND) is the most appropriate base-
line for an unsupervised approach.2. We include
the most frequent sense baseline (MFBL), though
we note that we consider the most frequent sense
or first sense baseline to be a supervised baseline
since it depends crucially on SemCor in ranking
the senses within WN.11 3. The SM07 results as a
10We exclude the data points that have a tag of ?U? in the
gold standard for both baselines and our system.
11From an application standpoint, we do not find the first
sense baseline to be of interest since it introduces a strong
level of uniformity ? removing semantic variability ? which
is not desirable. Even if the first sense achieves higher results
in data sets, it is an artifact of the size of the data and the very
limited number of documents under investigation.
1546
monolingual baseline. 4. The DR02 results as the
multilingual baseline.
5.4 Experimental Results
5.4.1 RelCont
We present the results for 4 different experi-
mental conditions for RelCont: JCN-V which
uses JCN instead of LCH for verb-verb similar-
ity comparison, we consider this our base con-
dition; +ExpandL is adding the Lesk Expansion
to the base condition, namely Lesk2;12 +SemCor
adds the SemCor expansion to the base condi-
tion; and finally +ExpandL SemCor, adds the lat-
ter both conditions simultaneously. Table 1 illus-
trates the obtained results for the SV2AW using
WordNet 1.7.1 since it is the most studied data set
and for ease of comparison with previous studies.
We break the results down by POS tag (N)oun,
(V)erb, (A)djective, and Adve(R)b. The coverage
for SV2AW is 98.17% losing some of the verb and
adverb target words.
Our overall results on all the data sets clearly
outperform the baseline as well as state-of-the-
art performance using an unsupervised system
(SM07) in overall f-measure across all the data
sets. We are unable to beat the most frequent
baseline (MFBL) which is obtained using the first
sense. However MFBL is a supervised baseline
and our approach is unsupervised. Our implemen-
tation of SM07 is slightly higher than those re-
ported in (Sinha and Mihalcea, 2007) (57.12% )
is probably due to the fact that we do not consider
the items tagged as ?U? and also we resolve some
of the POS tag mismatches between the gold set
and the test data. We note that for the SV2AW data
set our coverage is not 100% due to some POS tag
mismatches that could not have been resolved au-
tomatically. These POS tag problems have to do
mainly with multiword expressions. In observing
the performance of the overall RelCont, we note
that using JCN for verbs clearly outperforms us-
ing the LCH similarity measure. Using SemCor to
augment WN examples seems to have the biggest
impact. Combining SemCor with ExpandL yields
the best results.
Observing the results yielded per POS in Ta-
ble 1, ExpandL seems to have the biggest impact
on the Nouns only. This is understandable since
the noun hierarchy has the most dense relations
and the most consistent ones. SemCor augmen-
12Using Lesk3 yields almost the same results
tation of WN seemed to benefit all POS signifi-
cantly except for nouns. In fact the performance
on the nouns deteriorated from the base condition
JCN-V from 68.7 to 68.3%. This maybe due to in-
consistencies in the annotations of nouns in Sem-
Cor or the very fine granularity of the nouns in
WN. We know that 72% of the nouns, 74% of
the verbs, 68.9% of the adjectives, and 81.9% of
the adverbs directly exploited the use of SemCor
augmented examples. Combining SemCor and
ExpandL seems to have a positive impact on the
verbs and adverbs, but not on the nouns and adjec-
tives. These trends are not held consistently across
data sets. For example, we see that SemCor aug-
mentation helps all POS tag sets over using Ex-
pandL alone or even when combined with Sem-
Cor. We note the similar trends in performance for
the SV3AW data.
Compared to state of the art systems, RelCont
with an overall F-measure performance of 62.13%
outperforms the best unsupervised system of
57.5% UNED-AW-U2 for SV2 (Navigli, 2009). It
is worth noting that it is higher than several of the
supervised systems. Moreover, RelCont yields
better overall results on SV3 at 59.87 compared to
the best unsupervised system IRST-DDD-U which
yielded an F-measure of 58.3% (Navigli, 2009).
5.4.2 TransCont
For the TransCont results we illustrate the orig-
inal SALAAM results as our baseline. Simi-
lar to the DR02 work, we actually use the same
SALAAM parallel corpora comprising more than
5.5M English tokens translated using a single ma-
chine translation system GlobalLink. Therefore
our parallel corpus is the French English transla-
tion condition mentioned in DR02 work as FrGl.
We have 4 experimental conditions: FRGL using
Lesk2 for all POS tags in the typeset disambigua-
tion (Lesk2); FRGL using Lesk3 for all POS tags
(Lesk3); using Lesk3 for N, A and R but LIN simi-
larity measure for verbs (Lesk3 Lin); using Lesk3
for N, A and R but JCN for verbs (Lesk3 JCN).
In Table 3 we note the the Lesk3 JCN followed
immediately by Lesk3 Lin yield the best perfor-
mance. The trend holds for both SV2AW and
SV3AW. Essentially our new implementation of
the multilingual system significantly outperforms
the original DR02 implementation for all experi-
mental conditions.
1547
Condition N V A R Global F Measure
RAND 43.7 21 41.2 57.4 39.9
MFBL 71.8 41.45 67.7 81.8 65.35
SM07 68.7 33.01 65.2 63.1 59.2
JCN-V 68.7 35.46 65.2 63.1 59.72
+ExpandL 70.2 35.86 65.4 62.45 60.48
+SemCor 68.5 38.66 69.2 67.75 61.79
+ExpandL SemCor 69.0 38.66 68.8 69.45 62.13
Table 1: RelCont F-measure results per POS tag per condition for SV2AW using WN 1.7.1.
Condition N V A R Global F Measure
RAND 39.67 19.34 41.85 92.31 32.97
MFBL 70.4 54.15 66.7 92.88 63.96
SM07 60.9 43.4 57 92.88 53.98
JCN-V 60.9 48.5 57 92.88 55.87
+ExpandL 59.9 48.55 57.95 92.88 55.62
+SemCor 66 48.95 65.55 92.88 59.87
+ExpandL SemCor 65 49.2 65.55 92.88 59.52
Table 2: RelCont F-measure results per POS tag per condition for SV3AW using WN 1.7.1.
5.4.3 Global Combined WSD
In this section we present the results of the global
combined WSD system. All the combined ex-
perimental conditions have the same percentage
coverage.13 We present the results combining us-
ing MERGE and using VOTE. We have chosen
4 baseline systems: (1) SM07; (2) the our base-
line monolingual system using JCN for verb-verb
comparisons (RelCont-BL), so as to distinguish
the level of improvement that could be attributed
to the multilingual system in the combination re-
sults; as well as (3) and (4) our best individual sys-
tem results from RelCont (ExpandL SemCor)
referred to in the tables below as (RelCont-Final)
and TransCont using the best experimental con-
dition (Lesk3 JCN). Table 5 and 6 illustrates the
overall performance of our combined approach.
In Table 5 we note that the combined conditions
outperform the two base systems independently,
using TransCont is always helpful for any of the
3 monolingual systems, no matter we use VOTE or
MERGE. In general the trend is that VOTE outper-
forms MERGE, however they exhibit different be-
haviors with respect to what works for each POS.
In Table 6 the combined result is not always
better than the corresponding monolingual sys-
tem. When applying to our baseline monolin-
13We do not back off in any of our systems to a default
sense, hence the coverage is not at a 100%.
gual system, the combined result is still bet-
ter. However, we observed worse results for Ex-
pandL Semcor, RelCont-Final. There may be 2
main reasons for the loss: (1) SV3 is the tuning
set in SM07, and we inherit the thresholds for
similarity metrics from that study. Accordingly,
an overfitting of the thresholds is probably hap-
pening in this case; (2) TransCont results are
not good enough on the SV3AW data. Compar-
ing the RelCont and TransCont system re-
sults, we find a drop in f-measure of ?1.37%
in SV2AW, in contrast to a much larger drop in
performance for the SV3AW data set where the
drop in performance is ?6.38% when comparing
RelCont-BL to TransCont and nearly ?10%
comparing against RelCont-Final.
6 Discussion
We looked closely at the data in the combined con-
ditions attempting to get a feel for the data and
understand what was captured and what was not.
Some of the good examples that are captured in the
combined system that are not tagged in RelCont
is the case of ringer in Like most of the other 6,000
churches in Britain with sets of bells , St. Michael
once had its own ? band ? of ringers , who would
herald every Sunday morning and evening service
.. The RelCont answer is ringer sense number 4:
(horseshoes) the successful throw of a horseshoe
1548
Condition N V A R Global F Measure
RAND 43.7 21 41.2 57.4 39.9
DR02-FRGL 54.5
SALAAM 65.48 31.77 56.87 67.4 57.23
Lesk2 67.05 30 59.69 68.01 57.27
Lesk3 67.15 30 60.2 68.01 57.41
Lesk3 Lin 67.15 29.27 60.2 68.01 57.61
Lesk3 JCN 67.15 33.88 60.2 68.01 58.35
Table 3: TransCont F-measure results per POS tag per condition for SV2AW using WN 1.7.1.
Condition N V A R Global F Measure
RAND 39.67 19.34 41.85 92.31 32.93
SALAAM 52.42 29.27 54.14 88.89 45.63
Lesk2 53.57 33.58 53.63 88.89 47
Lesk3 53.77 33.30 56.48 88.89 47.5
Lesk3 Lin 53.77 29.24 56.48 88.89 46.37
Lesk3 JCN 53.77 38.43 56.48 88.89 49.29
Table 4: TransCont F-measure results per POS tag per condition for SV3AW using WN 1.7.1.
or quoit so as to encircle a stake or peg. When
the merged system is employed we see the cor-
rect sense being chosen as sense number 1 in the
MERGE condition: defined in WN as a person
who rings church bells (as for summoning the con-
gregation) resulting from a corresponding transla-
tion into French as sonneur.
We did some basic data analysis on the items
we are incapable of capturing. Several of them
are cases of metonymy in examples such as ?the
English are known...?, the sense of English here
is clearly in reference to the people of England,
however, our WSD system preferred the language
sense of the word. These cases are not gotten by
any of our systems. If it had access to syntac-
tic/semantic roles we assume it could capture that
this sense of the word entails volition for example.
Other types of errors resulted from the lack of a
way to explicitly identify multiwords.
Looking at the performance of TransCont we
note that much of the loss is a result of the lack of
variability in the translations which is a key factor
in the performance of the algorithm. For example
for the 157 adjective target test words in SV2AW,
there was a single word alignment for 51 of the
cases, losing any tagging for these words.
7 Conclusions and Future Directions
In this paper we present a framework that com-
bines orthogonal sources of evidence to create a
state-of-the-art system for the task of WSD disam-
biguation for AW. Our approach yields an over-
all global F measure of 64.58 for the standard
SV2AW data set combining monolingual and mul-
tilingual evidence. The approach can be fur-
ther refined by adding other types of orthogo-
nal features such as syntactic features and seman-
tic role label features. Adding SemCor exam-
ples to TransCont should have a positive im-
pact on performance. Also adding more languages
as illustrated by the DR02 work should also yield
much better performance.
References
Marine Carpuat and Dekai Wu. 2007. Improving sta-
tistical machine translation using word sense disam-
biguation. In Proceedings of the 2007 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP-CoNLL), pages 61?72, Prague,
Czech Republic, June. Association for Computa-
tional Linguistics.
Yee Seng Chan, Hwee Tou Ng, and David Chiang.
2007. Word sense disambiguation improves statisti-
cal machine translation. In Proceedings of the 45th
Annual Meeting of the Association of Computational
Linguistics, pages 33?40, Prague, Czech Republic,
June. Association for Computational Linguistics.
Mona Diab and Philip Resnik. 2002. An unsuper-
vised method for word sense tagging using parallel
corpora. In Proceedings of 40th Annual Meeting
of the Association for Computational Linguistics,
1549
Condition N V A R Global F Measure
SM07 68.7 33.01 65.2 63.1 59.2
RelCont-BL 68.7 35.46 65.2 63.1 59.72
RelCont-Final 69.0 38.66 68.8 69.45 62.13
TransCont 67.15 33.88 60.2 68.01 58.35
MERGE: RelCont-BL+TransCont 69.3 36.91 66.7 64.45 60.82
VOTE: RelCont-BL+TransCont 71 37.71 66.5 66.1 61.92
MERGE: RelCont-Final+TransCont 70.7 38.66 69.5 70.45 63.14
VOTE: RelCont-Final+TransCont 74.2 38.26 68.6 71.45 64.58
Table 5: F-measure % for all Combined experimental conditions on SV2AW
Condition N V A R Global F Measure
SM07 60.9 43.4 57 92.88 53.98
RelCont-BL 60.9 48.5 57 92.88 55.87
RelCont-Final 65 49.2 65.55 92.88 59.52
TransCont 53.77 38.43 56.48 88.89 49.29
MERGE: RelCont-BL+TransCont 60.6 49.5 58.85 92.88 56.47
VOTE: RelCont-BL+TransCont 59.3 49.5 59.1 92.88 55.92
MERGE: RelCont-Final+TransCont 63.2 50.3 65.25 92.88 59.07
VOTE: RelCont-Final+TransCont 62.4 49.65 65.25 92.88 58.47
Table 6: F-measure % for all Combined experimental conditions on SV3AW
pages 255?262, Philadelphia, Pennsylvania, USA,
July. Association for Computational Linguistics.
Christiane Fellbaum. 1998. ?wordnet: An electronic
lexical database?. MIT Press.
Weiwei Guo and Mona Diab. 2009. Improvements to
monolingual english word sense disambiguation. In
Proceedings of the Workshop on Semantic Evalua-
tions: Recent Achievements and Future Directions
(SEW-2009), pages 64?69, Boulder, Colorado, June.
Association for Computational Linguistics.
N. Ide and J. Veronis. 1998. Word sense disambigua-
tion: The state of the art. In Computational Linguis-
tics, pages 1?40, 24:1.
J. Jiang and D. Conrath. 1997. Semantic similarity
based on corpus statistics and lexical taxonomy. In
Proceedings of the International Conference on Re-
search in Computational Linguistics, Taiwan.
C. Leacock and M. Chodorow. 1998. Combining lo-
cal context and wordnet sense similarity for word
sense identification. In WordNet, An Electronic Lex-
ical Database. The MIT Press.
M. Lesk. 1986. Automatic sense disambiguation using
machine readable dictionaries: How to tell a pine
cone from an ice cream cone. In In Proceedings of
the SIGDOC Conference, Toronto, June.
Rada Mihalcea. 2005. Unsupervised large-vocabulary
word sense disambiguation with graph-based algo-
rithms for sequence data labeling. In Proceed-
ings of Human Language Technology Conference
and Conference on Empirical Methods in Natural
Language Processing, pages 411?418, Vancouver,
British Columbia, Canada, October. Association for
Computational Linguistics.
George A. Miller. 1990. Wordnet: a lexical database
for english. In Communications of the ACM, pages
39?41.
Roberto Navigli and Mirella Lapata. 2007. Graph
connectivity measures for unsupervised word sense
disambiguation. In Proceedings of the 20th Inter-
national Joint Conference on Artificial Intelligence
(IJCAI), pages 1683?1688, Hyderabad, India.
Roberto Navigli. 2009. Word sense disambiguation:
a survey. In ACM Computing Surveys, pages 1?69.
ACM Press.
Franz Joseph Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
M. Palmer, C. Fellbaum, S. Cotton, L. Delfs, , and
H. Dang. 2001. English tasks: all-words and verb
lexical sample. In In Proceedings of ACL/SIGLEX
Senseval-2, Toulouse, France, June.
Ted Pedersen, Satanjeev Banerjee, and Siddharth Pat-
wardhan. 2005. Maximizing semantic relatedness
to perform word sense disambiguation. In Univer-
sity of Minnesota Supercomputing Institute Research
Report UMSI 2005/25, Minnesotta, March.
1550
Sameer Pradhan, Edward Loper, Dmitriy Dligach, and
Martha Palmer. 2007. Semeval-2007 task-17: En-
glish lexical sample, srl and all words. In Proceed-
ings of the Fourth International Workshop on Se-
mantic Evaluations (SemEval-2007), pages 87?92,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Ravi Sinha and Rada Mihalcea. 2007. Unsupervised
graph-based word sense disambiguation using mea-
sures of word semantic similarity. In Proceedings
of the IEEE International Conference on Semantic
Computing (ICSC 2007), Irvine, CA.
Benjamin Snyder and Martha Palmer. 2004. The en-
glish all-words task. In Rada Mihalcea and Phil
Edmonds, editors, Senseval-3: Third International
Workshop on the Evaluation of Systems for the Se-
mantic Analysis of Text, pages 41?43, Barcelona,
Spain, July. Association for Computational Linguis-
tics.
1551
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 864?872,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Modeling Sentences in the Latent Space
Weiwei Guo
Department of Computer Science,
Columbia University,
weiwei@cs.columbia.edu
Mona Diab
Center for Computational Learning Systems,
Columbia University,
mdiab@ccls.columbia.edu
Abstract
Sentence Similarity is the process of comput-
ing a similarity score between two sentences.
Previous sentence similarity work finds that
latent semantics approaches to the problem do
not perform well due to insufficient informa-
tion in single sentences. In this paper, we
show that by carefully handling words that
are not in the sentences (missing words), we
can train a reliable latent variable model on
sentences. In the process, we propose a new
evaluation framework for sentence similarity:
Concept Definition Retrieval. The new frame-
work allows for large scale tuning and test-
ing of Sentence Similarity models. Experi-
ments on the new task and previous data sets
show significant improvement of our model
over baselines and other traditional latent vari-
able models. Our results indicate comparable
and even better performance than current state
of the art systems addressing the problem of
sentence similarity.
1 Introduction
Identifying the degree of semantic similarity [SS]
between two sentences is at the core of many NLP
applications that focus on sentence level semantics
such as Machine Translation (Kauchak and Barzi-
lay, 2006), Summarization (Zhou et al, 2006), Text
Coherence Detection (Lapata and Barzilay, 2005),
etc.To date, almost all Sentence Similarity [SS] ap-
proaches work in the high-dimensional word space
and rely mainly on word similarity. There are two
main (not unrelated) disadvantages to word similar-
ity based approaches: 1. lexical ambiguity as the
pairwise word similarity ignores the semantic inter-
action between the word and its sentential context;
2. word co-occurrence information is not sufficiently
exploited.
Latent variable models, such as Latent Semantic
Analysis [LSA] (Landauer et al, 1998), Probabilis-
tic Latent Semantic Analysis [PLSA] (Hofmann,
1999), Latent Dirichlet Allocation [LDA] (Blei et
al., 2003) can solve the two issues naturally by mod-
eling the semantics of words and sentences simulta-
neously in the low-dimensional latent space. How-
ever, attempts at addressing SS using LSA perform
significantly below high dimensional word similar-
ity based models (Mihalcea et al, 2006; O?Shea et
al., 2008).
We believe that the latent semantics approaches
applied to date to the SS problem have not yielded
positive results due to the deficient modeling of the
sparsity in the semantic space. SS operates in a very
limited contextual setting where the sentences are
typically very short to derive robust latent semantics.
Apart from the SS setting, robust modeling of the
latent semantics of short sentences/texts is becom-
ing a pressing need due to the pervasive presence of
more bursty data sets such as Twitter feeds and SMS
where short contexts are an inherent characteristic of
the data.
In this paper, we propose to model the missing
words (words that are not in the sentence), a fea-
ture that is typically overlooked in the text model-
ing literature, to address the sparseness issue for the
SS task. We define the missing words of a sentence
as the whole vocabulary in a corpus minus the ob-
served words in the sentence. Our intuition is since
observed words in a sentence are too few to tell us
what the sentence is about, missing words can be
used to tell us what the sentence is not about. We
assume that the semantic space of both the observed
864
and missing words make up the complete semantics
profile of a sentence.
After analyzing the way traditional latent variable
models (LSA, PLSA/LDA) handle missing words,
we decide to model sentences using a weighted ma-
trix factorization approach (Srebro and Jaakkola,
2003), which allows us to treat observed words and
missing words differently. We handle missing words
using a weighting scheme that distinguishes missing
words from observed words yielding robust latent
vectors for sentences.
Since we use a feature that is already implied by
the text itself, our approach is very general (similar
to LSA/LDA) in that it can be applied to any format
of short texts. In contrast, existing work on model-
ing short texts focuses on exploiting additional data,
e.g., Ramage et al (2010) model tweets using their
metadata (author, hashtag, etc.).
Moreover in this paper, we introduce a new eval-
uation framework for SS: Concept Definition Re-
trieval (CDR). Compared to existing data sets, the
CDR data set alows for large scale tuning and test-
ing of SS modules without further human annota-
tion.
2 Limitations of Topic Models and LSA
for Modeling Sentences
Usually latent variable models aim to find a latent
semantic profile for a sentence that is most relevant
to the observed words. By explicitly modeling miss-
ing words, we set another criterion to the latent se-
mantics profile: it should not be related to the miss-
ing words in the sentence. However, missing words
are not as informative as observed words, hence the
need for a model that does a good job of modeling
missing words at the right level of emphasis/impact
is central to completing the semantic picture for a
sentence.
LSA and PLSA/LDA work on a word-sentence
co-occurrence matrix. Given a corpus, the row en-
tries of the matrix are the unique M words in the
corpus, and theN columns are the sentence ids. The
yielded M ?N co-occurrence matrix X comprises
the TF-IDF values in each Xij cell, namely that TF-
IDF value of word wi in sentence sj . For ease of
exposition, we will illustrate the problem using a
special case of the SS framework where the sen-
tences are concept definitions in a dictionary such
as WordNet (Fellbaum, 1998) (WN). Therefore, the
sentence corresponding to the concept definition of
bank#n#1 is a sparse vector in X containing the
following observed words where Xij 6= 0:
the 0.1, financial 5.5, institution 4, that 0.2,
accept 2.1, deposit 3, and 0.1, channel 6, the 0.1,
money 5, into 0.3, lend 3.5, activity 3
All the other words (girl, car,..., check, loan, busi-
ness,...) in matrix X that do not occur in the concept
definition are considered missing words for the con-
cept entry bank#n#1, thereby their Xij = 0 .
Topic models (PLSA/LDA) do not explicitly
model missing words. PLSA assumes each docu-
ment has a distribution over K topics P (zk|dj), and
each topic has a distribution over all vocabularies
P (wi|zk). Therefore, PLSA finds a topic distribu-
tion for each concept definition that maximizes the
log likelihood of the corpus X (LDA has a similar
form):
?
i
?
j
Xij log
?
k
P (zk|dj)P (wi|zk) (1)
In this formulation, missing words do not contribute
to the estimation of sentence semantics, i.e., exclud-
ing missing words (Xij = 0) in equation 1 does not
make a difference.
However, empirical results show that given a
small number of observed words, usually topic mod-
els can only find one topic (most evident topic)
for a sentence, e.g., the concept definitions of
bank#n#1 and stock#n#1 are assigned the fi-
nancial topic only without any further discernabil-
ity. This results in many sentences are assigned ex-
actly the same semantics profile as long as they are
pertaining/mentioned within the same domain/topic.
The reason is topic models try to learn a 100-
dimension latent vector (assume dimension K =
100) from very few data points (10 observed words
on average). It would be desirable if topic models
can exploit missing words (a lot more data than ob-
served words) to render more nuanced latent seman-
tics, so that pairs of sentences in the same domain
can be differentiable.
On the other hand, LSA explicitly models missing
words but not at the right level of emphasis. LSA
finds another matrix X? (latent vectors) with rank K
to approximate X using Singular Vector Decompo-
sition (X ? X? = UK?KV >K ), such that the Frobe-
865
financial sport institution Ro Rm Ro ?Rm Ro ? 0.01Rm
v1 1 0 0 20 600 -580 14
v2 0.6 0 0.1 18 300 -282 15
v3 0.2 0.3 0.2 5 100 -95 4
Table 1: Three possible latent vectors hypotheses for the definition of bank#n#1
nius norm of difference between the two matrices is
minimized: ?
?
?
?
?
i
?
j
(
X?ij ?Xij
)2
(2)
In effect, LSA allows missing and observed words
to equally impact the objective function. Given the
inherent short length of the sentences, LSA (equa-
tion 2) allows for much more potential influence
from missing words rather than observed words
(99.9% cells are 0 in X). Hence the contribution
of the observed words is significantly diminished.
Moreover, the true semantics of the concept defini-
tions is actually related to some missing words, but
such true semantics will not be favored by the objec-
tive function, since equation 2 allows for too strong
an impact by X?ij = 0 for any missing word. There-
fore the LSA model, in the context of short texts,
is allowing missing words to have a significant ?un-
controlled? impact on the model.
2.1 An Example
The three latent semantics profiles in table 1 il-
lustrate our analysis for topic models and LSA. As-
sume there are three dimensions: financial, sports,
institution. We use Rvo to denote the sum of related-
ness between latent vector v and all observed words;
similarly, Rvm is the sum of relatedness between the
vector v and all missing words. The first latent vec-
tor (generated by topic models) is chosen by maxi-
mizing Robs = 600. It suggests bank#n#1 is only
related to the financial dimension. The second la-
tent vector (found by LSA) has the maximum value
of Robs ?Rmiss = 95, but obviously the latent vec-
tor is not related to bank#n#1 at all. This is be-
cause LSA treats observed words and missing words
equally the same, and due to the large number of
missing words, the information of observed words
is lost: Robs?Rmiss ? ?Rmiss. The third vector is
the ideal semantics profile, since it is also related to
the institution dimension. It has a slightly smaller
Robs in comparison to the first vector, yet it has a
substantially smaller Rmiss.
In order to favor the ideal vector over other vec-
tors, we simply need to adjust the objective func-
tion by assigning a smaller weight to Rmiss such as:
Robs?0.01?Rmiss. Accordingly, we use weighted
matrix factorization (Srebro and Jaakkola, 2003) to
model missing words.
3 The Proposed Approach
3.1 Weighted Matrix Factorization
The weighted matrix factorization [WMF] ap-
proach is very similar to SVD, except that it allows
for direct control on each matrix cellXij . The model
factorizes the original matrix X into two matrices
such that X ? P>Q, where P is a K ?M matrix,
and Q is a K ?N matrix (figure 1).
The model parameters (vectors in P and Q) are
optimized by minimizing the objective function:
?
i
?
j
Wij (P?,i ?Q?,j ?Xij)
2 + ?||P ||22 + ?||Q||
2
2 (3)
where ? is a free regularization factor, and the
weight matrix W defines a weight for each cell in
X .
Accordingly, P?,i is a K-dimension latent seman-
tics vector profile for word wi; similarly, Q?,j is the
K-dimension vector profile that represents the sen-
tence sj . Operations on these K-dimensional vec-
tors have very intuitive semantic meanings:
(1) the inner product of P?,i and Q?,j is used to ap-
proximate semantic relatedness of word wi and sen-
tence sj : P?,i ? Q?,j ? Xij , as the shaded parts in
Figure 1;
(2) equation 3 explicitly requires a sentence should
not be related to its missing words by forcing P?,i ?
Q?,j = 0 for missing words Xij = 0.
(3) we can compute the similarity of two sentences
sj and sj? using the cosine similarity between Q?,j ,
Q?,j? .
The latent vectors in P and Q are first randomly
initialized, then can be computed iteratively by the
following equations (derivation is omitted due to
limited space, which can be found in (Srebro and
Jaakkola, 2003)):
P?,i =
(
QW? (i)Q> + ?I
)?1
QW? (i)X>i,?
Q?,j =
(
PW? (j)P> + ?I
)?1
PW? (i)X?,j
(4)
866
Figure 1: Matrix Factorization
where W? (i) = diag(W?,i) is an M ? M diagonal
matrix containing ith row of weight matrixW . Sim-
ilarly, W? (j) = diag(W?,j) is an N ? N diagonal
matrix containing jth column of W .
3.2 Modeling Missing Words
It is straightforward to implement the idea in Sec-
tion 2.1 (choosing a latent vector that maximizes
Robs ? 0.01 ? Rmiss) in the WMF framework, by
assigning a small weight for all the missing words
and minimizing equation 3:
Wi,j =
{
1, if Xij 6= 0
wm, if Xij = 0
(5)
We refer to our model as Weighted Textual Matrix
Factorization [WTMF]. 1
This solution is quite elegant: 1. it explicitly tells
the model that in general all missing words should
not be related to the sentence; 2. meanwhile latent
semantics are mainly generalized based on observed
words, and the model is not penalized too much
(wm is very small) when it is very confident that
the sentence is highly related to a small subset of
missing words based on their latent semantics pro-
files (bank#n#1 definition sentence is related to its
missing words check loan).
We adopt the same approach (assigning a small
weight for some cells in WMF) proposed for rec-
ommender systems [RS] (Steck, 2010). In RS, an
incomplete rating matrix R is proposed, where rows
are users and columns are items. Typically, a user
rates only some of the items, hence, the RS system
needs to predict the missing ratings. Steck (2010)
guesses a value for all the missing cells, and sets a
small weight for those cells.
Compared to (Steck, 2010), we are facing a differ-
ent problem and targeting a different goal. We have
a full matrix X where missing words have a 0 value,
while the missing ratings in RS are unavailable ? the
values are unknown, henceR is not complete. In the
RS setting, they are interested in predicting individ-
ual ratings, while we are interested in the sentence
1An efficient way to compute equation 4 is proposed in
(Steck, 2010).
semantics. More importantly, they do not have the
sparsity issue (each user has rated over 100 items in
the movie lens data2) and robust predictions can be
made based on the observed ratings alone.
4 Evaluation for SS
We need to show the impact of our proposed model
WTMF on the SS task. However we are faced with
a problem, the lack of a suitable large evaluation set
from which we can derive robust observations. The
two data sets we know of for SS are: 1. human-rated
sentence pair similarity data set (Li et al, 2006)
[LI06]; 2. the Microsoft Research Paraphrase Cor-
pus (Dolan et al, 2004) [MSR04]. The LI06 data
set consists of 65 pairs of noun definitions selected
from the Collin Cobuild Dictionary. A subset of 30
pairs is further selected by LI06 to render the sim-
ilarity scores evenly distributed. While this is the
ideal data set for SS, the small size makes it impos-
sible for tuning SS algorithms or deriving significant
performance conclusions.
On the other hand, the MSR04 data set comprises
a much larger set of sentence pairs: 4,076 training
and 1,725 test pairs. The ratings on the pairs are
binary labels: similar/not similar. This is not a prob-
lem per se, however the issue is that it is very strict
in its assignment of a positive label, for example
the following sentence pair as cited in (Islam and
Inkpen, 2008) is rated not semantically similar:
Ballmer has been vocal in the past warning that
Linux is a threat to Microsoft.
In the memo, Ballmer reiterated the open-source
threat to Microsoft.
We believe that the ratings on a data set for SS
should accommodate variable degrees of similarity
with various ratings, however such a large scale set
does not exist yet. Therefore for purposes of evaluat-
ing our proposed approach we devise a new frame-
work inspired by the LI06 data set in that it com-
prises concept definitions but on a large scale.
4.1 Concept Definition Retrieval
We define a new framework for evaluating SS and
project it as a Concept Definition Retrieval (CDR)
task where the data points are dictionary definitions.
The intuition is that two definitions in different dic-
2http://www.grouplens.org/node/73, with 1M data set being
the most widely used.
867
tionaries referring to the same concept should be as-
signed large similarity. In this setting, we design the
CDR task in a search engine style. The SS algorithm
has access to all the definitions in WordNet (WN).
Given an OntoNotes (ON) definition (Hovy et al,
2006), the SS algorithm should rank the equivalent
WN definition as high as possible based on sentence
similarity.
The manual mapping already exists for ON to
WN. One ON definition can be mapped to sev-
eral WN definitions. After preprocessing we obtain
13669 ON definitions mapped to 19655 WN defini-
tions. The data set has the advantage of being very
large and it doesn?t require further human scrutiny.
After the SS model learns the co-occurrence of
words from WN definitions, in the testing phase,
given an ON definition d, the SS algorithm needs to
identify the equivalent WN definitions by comput-
ing the similarity values between all WN definitions
and the ON definition d, then sorting the values in
decreasing order. Clearly, it is very difficult to rank
the one correct definition as highest out of all WN
definitions (110,000 in total), hence we use ATOPd,
area under the TOPKd(k) recall curve for an ON
definition d, to measure the performance. Basically,
it is the ranking of the correct WN definition among
all WN definitions. The higher a model is able to
rank the correct WN definition, the better its perfor-
mance.
Let Nd be the number of aligned WN definitions
for the ON definition d, and Nkd be the number of
aligned WN definitions in the top-k list. Then with
a normalized k ? [0,1], TOPKd(k) and ATOPd is
defined as:
TOPKd(k) = N
k
d /Nd
ATOPd =
? 1
0
TOPKd(k)dk
(6)
ATOPd computes the normalized rank (in the range
of [0, 1]) of aligned WN definitions among all WN
definitions, with value 0.5 being the random case,
and 1 being ranked as most similar.
5 Experiments and Results
We evaluate WTMF on three data sets: 1. CDR
data set using ATOP metric; 2. Human-rated Sen-
tence Similarity data set [LI06] using Pearson and
Spearman Correlation; 3. MSR Paraphrase corpus
[MSR04] using accuracy.
The performance of WTMF on CDR is com-
pared with (a) an Information Retrieval model (IR)
that is based on surface word matching, (b) an n-
gram model (N-gram) that captures phrase overlaps
by returning the number of overlapping ngrams as
the similarity score of two sentences, (c) LSA that
uses svds() function in Matlab, and (d) LDA that
uses Gibbs Sampling for inference (Griffiths and
Steyvers, 2004). WTMF is also compared with all
existing reported SS results on LI06 and MSR04
data sets, as well as LDA that is trained on the
same data as WTMF. The similarity of two sentences
is computed by cosine similarity (except N-gram).
More details on each task will be explained in the
subsections.
To eliminate randomness in statistical models
(WTMF and LDA), all the reported results are aver-
aged over 10 runs. We run 20 iterations for WTMF.
And we run 5000 iterations for LDA; each LDA
model is averaged over the last 10 Gibbs Sampling
iterations to get more robust predictions.
The latent vector of a sentence is computed by:
(1) using equation 4 in WTMF, or (2) summing
up the latent vectors of all the constituent words
weighted by Xij in LSA and LDA, similar to the
work reported in (Mihalcea et al, 2006). For LDA
the latent vector of a word is computed by P (z|w).
It is worth noting that we could directly use the es-
timated topic distribution ?j to represent a sentence,
however, as discussed the topic distribution has only
non-zero values on one or two topics, leading to a
low ATOP value around 0.8.
5.1 Corpus
The corpus we use comprises three dictionaries
WN, ON, Wiktionary [Wik],3 Brown corpus. For
all dictionaries, we only keep the definitions without
examples, and discard the mapping between sense
ids and definitions. All definitions are simply treated
as individual documents. We crawl Wik and remove
the entries that are not tagged as noun, verb, adjec-
tive, or adverb, resulting in 220, 000 entries. For the
Brown corpus, each sentence is treated as a docu-
ment in order to create more coherent co-occurrence
values. All data is tokenized, pos-tagged4, and lem-
3http://en.wiktionary.org/wiki/Wiktionary:Main Page
4http://nlp.stanford.edu/software/tagger.shtml
868
Models Parameters Dev Test
1. IR - 0.8578 0.8515
2. N-gram - 0.8238 0.8171
3. LSA - 0.8218 0.8143
4a. LDA ? = 0.1, ? = 0.01 0.9466? 0.0020 0.9427? 0.0006
4b. LDA ? = 0.05, ? = 0.05 0.9506? 0.0017 0.9470? 0.0005
5. WTMF wm = 1, ? = 0 0.8273? 0.0028 0.8273? 0.0014
6. WTMF wm = 0, ? = 20 0.8745? 0.0058 0.8645? 0.0031
7a. WTMF wm = 0.01, ? = 20 0.9555? 0.0015 0.9511? 0.0003
7b. WTMF wm = 0.0005, ? = 20 0.9610? 0.0011 0.9558? 0.0004
Table 2: ATOP Values of Models (K = 100 for LSA/LDA/WTMF)
matized5. The importance of words in a sentence is
estimated by the TF-IDF schema.
All the latent variable models (LSA, LDA,
WTMF) are built on the same set of cor-
pus: WN+Wik+Brown (393, 666 sentences and
4, 262, 026 words). Words that appear only once are
removed. The test data is never used during training
phrase.
5.2 Concept Definition Retrieval
Among the 13669 ON definitions, 1000 defini-
tions are randomly selected as a development set
(dev) for picking best parameters in the models, and
the rest is used as a test set (test). The performance
of each model is evaluated by the average ATOPd
value over the 12669 definitions (test). We use the
subscript set in ATOPset to denote the average of
ATOPd of a set of ON definitions, where d ? {set}.
If all the words in an ON definition are not covered
in the training data (WN+Wik+Br), then ATOPd for
this instance is set to 0.5.
To compute ATOPd for an ON definition effi-
ciently, we use the rank of the aligned WN definition
among a random sample (size=1000) of WN defini-
tions, to approximate its rank among all WN defini-
tions. In practice, the difference between using 1000
samples and all data is tiny for ATOPtest (?0.0001),
due to the large number of data points in CDR.
We mainly compare the performance of IR, N-
gram, LSA, LDA, and WTMF models. Generally
results are reported based on the last iteration. How-
ever, we observe that for model 6 in table 2, the best
performance occurs at the first few iterations. Hence
for that model we use the ATOPdev to indicate when
to stop.
5http://wn-similarity.sourceforge.net, WordNet::QueryData
5.2.1 Results
Table 2 summarizes the ATOP values on the dev
and test sets. All parameters are tuned based on the
dev set. In LDA, we choose an optimal combination
of ? and ? from {0.01, 0.05, 0.1, 0.5}.In WTMF, we
choose the best parameters of weight wm for miss-
ing words and ? for regularization. We fix the di-
mension K = 100. Later in section 5.2.2, we will
see that a larger value of K can further improve the
performance.
WTMF that models missing words using a small
weight (model 7b with wm = 0.0005) outperforms
the second best model LDA by a large margin. This
is because LDA only uses 10 observed words to infer
a 100 dimension vector for a sentence, while WTMF
takes advantage of much more missing words to
learn more robust latent semantics vectors.
The IR model that works in word space achieves
better ATOP scores than N-gram, although the idea
of N-gram is commonly used in detecting para-
phrases as well as machine translation. Applying
TF-IDF for N-gram is better, but still the ATOPtest is
not higher: 0.8467. The reason is words are enough
to capture semantics for SS, while n-grams/phrases
are used for a more fine-grained level of semantics.
We also present model 5 and 6 (both are WTMF),
to show the impact of: 1. modeling missing words
with equal weights as observed words (wm = 1)
(LSA manner), and 2. not modeling missing words
at all (wm = 0) (LDA manner) in the context of
WTMF model. As expected, both model 5 and
model 6 generate much worse results.
Both LDA and model 6 ignore missing words,
with better ATOPtest scores achieved by LDA. This
may be due to the different inference algorithms.
Model 5 and LSA are comparable, where missing
words are used with a large weight. Both of them
yield low results. This confirms our assumption
869
0.0001 0.0005 0.001 0.005 0.01 0.050.94
0.945
0.95
0.955
wm
ATO
P
 
 WTMF
Figure 2: missing words weight wm in WTMF
50 100 1500.94
0.945
0.95
0.955
K
ATO
P
 
 
WTMFLDA
Figure 3: dimension K in WTMF and LDA
that allowing for equal impact of both observed and
missing words is not the correct characterization of
the semantic space.
5.2.2 Analysis
In these latent variable models, there are several
essential parameters: weight of missing words wm,
and dimensionK. Figure 2 and 3 analyze the impact
of these parameters on ATOPtest.
Figure 2 shows the influence of wm on ATOPtest
values. The peak ATOPtest is around wm = 0.0005,
while other values of wm (except wm = 0.05) also
yield high ATOP values (better than LDA).
We also measure the influence of the dimension
K = {50, 75, 100, 125, 150} on LDA and WTMF
in Figure 3, where parameters for WTMF are wm =
0.0005, ? = 20, and for LDA are ? = 0.05, ? =
0.05. We can see WTMF consistently outperforms
LDA by an ATOP value of 0.01 in each dimension.
Although a larger K yields a better result, we still
use a 100 due to computational complexity.
5.3 LI06: Human-rated Sentence Similarity
We also assess WTMF and LDA model on LI06
data set. We still use K = 100. As we can see
in Figure 2, choosing the appropriate parameter wm
could boost the performance significantly. Since we
do not have any tuning data for this task, we present
Pearson?s correlation r for different values of wm in
Table 3. In addition, to demonstrate that wm does
not overfit the 30 data points, we also evaluate on
30 pairs 35 pairs
wm r ? r ?
0.0005 0.8247 0.8440 0.4200 0.6006
0.001 0.8470 0.8636 0.4308 0.5985
0.005 0.8876 0.8966 0.4638 0.5809
0.01 0.8984 0.9091 0.4564 0.5450
0.05 0.8804 0.8812 0.4087 0.4766
Table 3: Different wm of WTMF on LI06 (K = 100)
the other 35 pairs in LI06. Same as in (Tsatsaronis
et al, 2010), we also include Spearman?s rank order
correlation ?, which is correlation of ranks of simi-
larity values . Note that r and ? are much lower for
35 pairs set, since most of the sentence pairs have
a very low similarity (the average similarity value
is 0.065 in 35 pairs set and 0.367 in 30 pairs set)
and SS models need to identify the tiny difference
among them, thereby rendering this set much harder
to predict.
Using wm = 0.01 gives the best results on 30
pairs while on 35 pairs the peak values of r and ?
happens when wm = 0.005. In general, the cor-
relations in 30 pairs and in 35 pairs are consistent,
which indicates wm = 0.01 or wm = 0.005 does
not overfit the 30 pairs set.
Compared to CDR, LI06 data set has a strong
preference for a larger wm. This could be caused by
different goals of the two tasks: CDR is evaluated
by the rank of the most similar ones among all can-
didates, while the LI06 data set treats similar pairs
and dissimilar pairs as equally important. Using a
smaller wm means the similarity score is computed
mainly from semantics of the observed words. This
benefits CDR, since it gives more accurate similarity
scores for those similar pairs, but not so accurate for
dissimilar pairs. In fact, from Figure 2 and Table 2
we see that wm = 0.01 also produces a very high
ATOPtest value in CDR.
Table 4 shows the results of all current SS models
with respect to the LI06 data set (30 pairs set). We
cite their best performance for all reported results.
Once the correct wm = 0.01 is chosen, WTMF
results in the best Pearson?s r and best Spearman?s
? (wm = 0.005 yields the second best r and ?).
Same as in CDR task, WTMF outperforms LDA by
a large margin in both r and ?. It indicates that the
latent vectors induced by WTMF are able to not only
identify same/similar sentences, but also identify the
?correct? degree of dissimilar sentences.
870
Model r ?
STASIS (Li et al, 2006) 0.8162 0.8126
(Liu et al, 2007) 0.841 0.8538
(Feng et al, 2008) 0.756 0.608
STS (Islam and Inkpen, 2008) 0.853 0.838
LSA (O?Shea et al, 2008) 0.8384 0.8714
Omiotis (Tsatsaronis et al, 2010) 0.856 0.8905
WSD-STS (Ho et al, 2010) 0.864 0.8341
SPD-STS (Ho et al, 2010) 0.895 0.9034
LDA (? = 0.05, ? = 0.05) 0.8422 0.8663
WTMF (wm = 0.005, ? = 20) 0.8876 0.8966
WTMF (wm = 0.01, ? = 20) 0.8984 0.9091
Table 4: Pearson?s correlation r and Spearman?s corre-
lation ? on LI06 30 pairs
Model Accuracy
Random 51.3
LSA (Mihalcea et al, 2006) 68.4
full model (Mihalcea et al, 2006) 70.3
STS (Islam and Inkpen, 2008) 72.6
Omiotis (Tsatsaronis et al, 2010) 69.97
LDA (? = 0.05, ? = 0.05) 68.6
WTMF (wm = 0.01, ? = 20) 71.51
Table 5: Performance on MSR04 test set
5.4 MSR04: MSR Paraphrase Corpus
Finally, we briefly discuss results of applying
WTMF on MSR04 data. We use the same pa-
rameter setting used for the LI06 evaluation set-
ting since both sets are human-rated sentence pairs
(? = 20, wm = 0.01,K = 100). We use the train-
ing set of MSR04 data to select a threshold of sen-
tence similarity for the binary label. Table 5 sum-
marizes the accuracy of other SS models noted in
the literature and evaluated on MSR04 test set.
Compared to previous SS work and LDA, WTMF
has the second best accuracy. It suggests that WTMF
is quite competitive in the paraphrase recognition
task.
It is worth noting that the best system on MSR04,
STS (Islam and Inkpen, 2008), has much lower cor-
relations on LI06 data set. The second best system
among previous work on LI06 uses Spearman cor-
relation, Omiotis (Tsatsaronis et al, 2010), and it
yields a much worse accuracy on MSR04. The other
works do not evaluate on both data sets.
6 Related Work
Almost all current SS methods work in the high-
dimensional word space, and rely heavily on
word/sense similarity measures, which is knowledge
based (Li et al, 2006; Feng et al, 2008; Ho et al,
2010; Tsatsaronis et al, 2010), corpus-based (Islam
and Inkpen, 2008) or hybrid (Mihalcea et al, 2006).
Almost all of them are evaluated on LI06 data set. It
is interesting to see that most works find word sim-
ilarity measures, especially knowledge based ones,
to be the most effective component, while other fea-
tures do not work well (such as word order or syn-
tactic information). Mihalcea et al (2006) use LSA
as a baseline, and O?Shea et al (2008) train LSA
on regular length documents. Both results are con-
siderably lower than word similarity based methods.
Hence, our work is the first to successfully approach
SS in the latent space.
Although there has been work modeling latent se-
mantics for short texts (tweets) in LDA, the focus
has been on exploiting additional features in Twit-
ter, hence restricted to Twitter data. Ramage et al
(2010) use tweet metadata (author, hashtag) as some
supervised information to model tweets. Jin et al
(2011) use long similar documents (the article that
is referred by a url in tweets) to help understand the
tweet. In contrast, our approach relies solely on the
information in the texts by modeling local missing
words, and does not need any additional data, which
renders our approach much more widely applicable.
7 Conclusions
We explicitly model missing words to alleviate the
sparsity problem in modeling short texts. We also
propose a new evaluation framework for sentence
similarity that allows large scale tuning and test-
ing. Experiment results on three data sets show that
our model WTMF significantly outperforms existing
methods. For future work, we would like to compare
the text modeling performance of WTMF with LSA
and LDA on regular length documents.
Acknowledgments
We would like to thank the anonymous reviewers for
their valuable comments and suggestions to improve
the quality of the paper.
This research was funded by the Office of the Di-
rector of National Intelligence (ODNI), Intelligence
Advanced Research Projects Activity (IARPA),
through the U.S. Army Research Lab. All state-
ments of fact, opinion or conclusions contained
herein are those of the authors and should not be
construed as representing the official views or poli-
cies of IARPA, the ODNI or the U.S. Government.
871
References
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. Journal of Machine
Learning Research, 3.
William Dolan, Chris Quirk, and Chris Brockett. 2004.
Unsupervised construction of large paraphrase cor-
pora: Exploiting massively parallel news sources. In
Proceedings of the 20th International Conference on
Computational Linguistics.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
Jin Feng, Yi-Ming Zhou, and Trevor Martin. 2008. Sen-
tence similarity based on relevance. In Proceedings of
IPMU.
Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
Academy of Sciences, 101.
Chukfong Ho, Masrah Azrifah Azmi Murad, Rabiah Ab-
dul Kadir, and Shyamala C. Doraisamy. 2010. Word
sense disambiguation-based sentence similarity. In
Proceedings of the 23rd International Conference on
Computational Linguistics.
Thomas Hofmann. 1999. Probabilistic latent semantic
indexing. In Proceedings of the 22nd annual interna-
tional ACM SIGIR conference on Research and devel-
opment in information retrieval.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. Ontonotes:
The 90% solution. In Proceedings of the Human Lan-
guage Technology Conference of the North American
Chapter of the ACL.
Aminul Islam and Diana Inkpen. 2008. Semantic
text similarity using corpus-based word similarity and
string similarity. ACM Transactions on Knowledge
Discovery from Data, 2.
Ou Jin, Nathan N. Liu, Kai Zhao, Yong Yu, and Qiang
Yang. 2011. Transferring topical knowledge from
auxiliary long texts for short text clustering. In Pro-
ceedings of the 20th ACM international conference on
Information and knowledge management.
David Kauchak and Regina Barzilay. 2006. Paraphras-
ing for automatic evaluation. In Proceedings of the
Human Language Technology Conference of the North
American Chapter of the ACL.
Thomas K Landauer, Peter W. Foltz, and Darrell Laham.
1998. An introduction to latent semantic analysis.
Discourse Processes, 25.
Mirella Lapata and Regina Barzilay. 2005. Automatic
evaluation of text coherence: Models and representa-
tions. In Proceedings of the 19th International Joint
Conference on Artificial Intelligence.
Yuhua Li, Davi d McLean, Zuhair A. Bandar, James D. O
Shea, and Keeley Crockett. 2006. Sentence similar-
ity based on semantic nets and corpus statistics. IEEE
Transaction on Knowledge and Data Engineering, 18.
Xiao-Ying Liu, Yi-Ming Zhou, and Ruo-Shi Zheng.
2007. Sentence similarity based on dynamic time
warping. In The International Conference on Seman-
tic Computing.
Rada Mihalcea, Courtney Corley, and Carlo Strapparava.
2006. Corpus-based and knowledge-based measures
of text semantic similarity. In Proceedings of the 21st
National Conference on Articial Intelligence.
James O?Shea, Zuhair Bandar, Keeley Crockett, and
David McLean. 2008. A comparative study of two
short text semantic similarity measures. In Proceed-
ings of the Agent and Multi-Agent Systems: Technolo-
gies and Applications, Second KES International Sym-
posium (KES-AMSTA).
Daniel Ramage, Susan Dumais, and Dan Liebling. 2010.
Characterizing microblogs with topic models. In Pro-
ceedings of the Fourth International AAAI Conference
on Weblogs and Social Media.
Nathan Srebro and Tommi Jaakkola. 2003. Weighted
low-rank approximations. In Proceedings of the Twen-
tieth International Conference on Machine Learning.
Harald Steck. 2010. Training and testing of recom-
mender systems on data missing not at random. In
Proceedings of the 16th ACM SIGKDD International
Conference on Knowledge Discovery and Data Min-
ing.
George Tsatsaronis, Iraklis Varlamis, and Michalis Vazir-
giannis. 2010. Text relatedness based on a word the-
saurus. Journal of Articial Intelligence Research, 37.
Liang Zhou, Chin-Yew Lin, Dragos Stefan Munteanu,
and Eduard Hovy. 2006. Paraeval: Using paraphrases
to evaluate summaries automatically. In Proceedings
of Human Language Tech-nology Conference of the
North American Chapter of the ACL,.
872
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 65?69,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Genre Independent Subgroup Detection in Online Discussion Threads: A
Pilot Study of Implicit Attitude using Latent Textual Semantics
Pradeep Dasigi
pd2359@columbia.edu
Weiwei Guo
weiwei@cs.columbia.edu
Center for Computational Learning Systems, Columbia University
Mona Diab
mdiab@ccls.columbia.edu
Abstract
We describe an unsupervised approach to
the problem of automatically detecting sub-
groups of people holding similar opinions in
a discussion thread. An intuitive way of iden-
tifying this is to detect the attitudes of discus-
sants towards each other or named entities or
topics mentioned in the discussion. Sentiment
tags play an important role in this detection,
but we also note another dimension to the de-
tection of people?s attitudes in a discussion: if
two persons share the same opinion, they tend
to use similar language content. We consider
the latter to be an implicit attitude. In this pa-
per, we investigate the impact of implicit and
explicit attitude in two genres of social media
discussion data, more formal wikipedia dis-
cussions and a debate discussion forum that
is much more informal. Experimental results
strongly suggest that implicit attitude is an im-
portant complement for explicit attitudes (ex-
pressed via sentiment) and it can improve the
sub-group detection performance independent
of genre.
1 Introduction
There has been a significant increase in discus-
sion forum data in online media recently. Most of
such discussion threads have a clear debate compo-
nent in them with varying levels of formality. Auto-
matically identifying the groups of discussants with
similar attitudes, or subgroup detection, is an inter-
esting problem which allows for a better understand-
ing of the data in this genre in a manner that could
directly benefit Opinion Mining research as well as
Community Mining from Social Networks.
A straight-forward approach to this problem is
to apply Opinion Mining techniques, and extract
each discussant?s attitudes towards other discussants
and entities being discussed. But the challenge is
that Opinion Mining is not mature enough to ex-
tract all the correct opinions of discussants. In ad-
dition, without domain knowledge, using unsuper-
vised techniques to do this is quite challenging.
On observing interactions from these threads, we
believe that there is another dimension of attitude
which is expressed implicitly. We find that people
sharing the same opinion tend to speak about the
same topics even though they do not explicitly ex-
press their sentiment. We refer to this as Implicit
Attitude. One such example may be seen in the two
posts in Table 1. It can be seen that even though dis-
cussants A and B do not express explicit sentiments,
they hold similar views. Hence it can be said that
there is an agreement in their implicit attitudes.
Attempting to find a surface level word similar-
ity between posts of two discussants is not sufficient
as there are typically few overlapping words shared
among the posts. This is quite significant a problem
especially given the relative short context of posts.
Accordingly, in this work, we attempt to model the
implicit latent similarity between posts as a means of
identifying the implicit attitudes among discussants.
We apply variants on Latent Dirichelet Allocation
(LDA) based topic models to the problem (Blei et
al., 2003).
Our goal is identify subgroups with respect to dis-
cussants? attitudes towards each other, the entities
and topics in a discussion forum. To our knowl-
edge, this is the first attempt at using text similar-
ity as an indication of user attitudes. We investigate
the influence of the explicit and implicit attitudes on
two genres of data, one more formal than the other.
We find an interesting trend. Explicit attitude alone
65
as a feature is more useful than implicit attitude in
identifying sub-groups in informal data. But in the
case of formal data, implicit attitude yields better re-
sults. This may be due to the fact that in informal
data, strong subjective opinions about entities/events
or towards other discussants are expressed more ex-
plicitly. This is generally not the case in the formal
genre where ideas do not have as much sentiment as-
sociated with them, and hence the opinions are more
?implicit?. Finally, we observe that combining both
kinds of features improves performance of our sys-
tems for both genres.
2 Related Work
Substantial research exists in the fields of Opin-
ion Identification and Community Mining that is re-
lated to our current work. (Ganapathibhotla and
Liu, 2008) deal with the problem of finding opin-
ions from comparative sentences. Many previous
research efforts related to Opinion Target Identifi-
cation (Hu and Liu, 2004; Kobayashi et al, 2007;
Jakob and Gurevych, 2010), focus on the domain of
product reviews where they exploit the genre in mul-
tiple ways. Somasundaran and Wiebe (2009) used
unsupervised methods to identify stances in online
debates. They mine the web to find associations
indicative of opinions and combine them with dis-
course information. Their problem essentially deals
with the debate genre and finding the stance of an in-
dividual given two options. Ours is a more general
problem since we deal with discussion data in gen-
eral and not debates on specific topics. Hence our
aim is to identify multiple groups, not just two.
In terms of Sentiment Analysis, the work done by
Hassan et al(2010) in using part-of-speech and de-
pendency structures to identify polarities of attitudes
is similar to our work. But they predict binary po-
larities in attitudes, and our goal of identification of
sub-groups is a more general problem in that we aim
at identifying multiple subgroups.
3 Approach
We tackle the problem using Vector Space Mod-
eling techniques to represent the discussion threads.
Each vector represents a discussant in the thread cre-
ating an Attitude Profile (AP). We use a clustering
algorithm to partition the vector space of APs into
multiple sub-groups. The idea is that resulting clus-
ters would comprise sub-groups of discussants with
similar attitudes.
3.1 Basic Features
We use two basic features, namely Negative and
Positive sentiment towards specific discussants and
entities like in the work done by (Abu-Jbara et al,
2012). We start off by determining sentences that
express attitude in the thread, attitude sentences
(AS). We use OpinionFinder (Wilson et al, 2005)
which employs negative and positive polarity cues.
For determining discussant sentiment, we need to
first identify who the target of their sentiment is: an-
other discussant, or an entity, where an entity could
be a topic or a person not participating in the dis-
cussion. Sentiment toward another discussant:
This is quite challenging since explicit sentiment ex-
pressed in a post is not necessarily directed towards
another discussant to whom it is a reply. It is pos-
sible that a discussant may be replying to another
poster but expressing an attitude towards a third en-
tity or discussant. However as a simplifying assump-
tion, similar to the work of (Hassan et al, 2010),
we adopt the view that replies in the sentences that
are determined to be attitudinal and contain second-
person pronouns (you, your, yourself) are assumed
to be directed towards the recipients of the replies.
Sentiment toward an entity: We again adopt a sim-
plifying view by modeling all the named entities in
a sentence without heeding the roles these entities
play, i.e. whether they are targets or not. Accord-
ingly, we extract all the named entities in a sentence
using Stanford?s Name Entity Recognizer (Finkel et
al., 2005). We only focus on Person and Organiza-
tion named entities.
3.2 Extracting Implicit Attitudes
We define implicit attitudes as the semantic sim-
ilarity between texts comprising discussant utter-
ances or posts in a thread. We cannot find enough
overlapping words between posts, since some posts
are very short. Hence we apply LDA (Blei et al,
2003) on texts to extract latent semantics of texts.
We split text into sentences, i.e., each sentence is
treated as a single document. Accordingly, each sen-
tence is represented as a K-dimension vector. By
computing the similarity on these vectors, we obtain
a more accurate semantic similarity.
66
A: There are a few other directors in the history of cinema who have achieved such a singular and consistent worldview as Kubrick.
His films are very philosophically deep, they say something about everything, war, crime, relationships, humanity, etc.
B: All of his films show the true human nature of man and their inner fights and all of them are very
philosophical. Alfred was good in suspense and all, but his work is not as deep as Kubrick?s
Table 1: Example of Agreement based on Implicit Attitude
WIKI CD
Median No. of Discussants (n) 6 29
Predicted No. of Clusters (d
?
n
2 e) 2 4
Median No. of Actual Classes 3 3
Table 2: Number of Clusters
3.3 Clustering Attitude Space
A tree-based (hierarchical) clustering algorithm,
SLINK (Sibson, 1973) is used to cluster the vec-
tor space. Cosine Similarity between the vectors is
used as the inter-data point similarity measure for
clustering.1 We choose the number of clusters to be
d
?n
2 e, described as the rule of thumb by (Mardia et
al., 1979), where n is the number of discussants in
the group. This rule seems to be validated by the fact
that in the data sets with which we experiment, we
note that the predicted number of clusters according
to this rule and the classes identified in the gold data
are very close as illustrated in Table 2. On average
we note that the gold data has the number of classes
per thread to be roughly 2-5.
4 Data
We use data from two online forums - Cre-
ate Debate [CD]2 and discussions from Wikipedia
[WIKI]3. There is a significant difference in the kind
of discussions in these two sources. Our WIKI data
comprises 117 threads crawled from Wikipedia. It is
relatively formal with short threads. It does not have
much negative polarity and discussants essentially
discuss the Wikipedia page in question. Hence it is
closer to an academic discussion forum. The threads
are manually annotated with sub-group information.
Given a thread, the annotator is asked to identify if
there are any sub-groups among the discussants with
similar opinions, and if yes, the membership of those
1We also experimented with K-means (MacQueen, 1967)
and found that it yields worse results compared to SLINK.
There is a fundamental difference between the two algorithms.
Where as K-Means does a random initialization of clusters,
SLINK is a deterministic algorithm. The difference in the per-
formance may be attributed to the fact that the number of initial
data points is too small for random initialization. Hence, tree
based clustering algorithms are more well suited for the current
task.
2http://www.createdebate.com
3en.wikipedia.org
Property WIKI CD
Threads 117 34
Posts per Thread 15.5 112
Sentences per Post 4.5 7.7
Tokens per Post 78.9 118.3
Word Types per Post 11.1 10.6
Discussants per Thread 6.5 34.15
Entities Discovered per Thread 6.15 32.7
Table 3: Data Statistics
subgroups.
On the other hand, CD is a forum where people
debate a specific topic. The CD data we use com-
prises 34 threads. It is more informal (with per-
vasive negative language and personal insults) than
WIKI and has longer threads. It is closer to the de-
bate genre. It has a poll associated with every de-
bate. The votes cast by the discussants in the poll
are used as the class labels for our experiments. De-
tailed statistics related to both the data sets and a
comparison can be found in Table 3.
5 Experimental Conditions
The following three features represent discussant
attitudes:
? Sentiment towards other discussants (SD) - This
corresponds to 2 ? n dimensions in the Attitude Pro-
file (AP) vector, n being the number of discussants
in the thread. This is because there are two polari-
ties and n possible targets. The value representing
this feature is the number of sentences with the re-
spective polarity ? negative or positive ? towards the
particular discussant.
? Sentiment towards entities in discussion (SE) -
Number of dimensions corresponding to this feature
is 2?e, where e is the number of entities discovered.
Similar to SD, the value taken by this feature is the
number of sentences in which that specific polarity
is shown by the discussant towards the entity.
? Implicit Attitude (IA) - n ? t dimensions are ex-
pressed using this feature, where t is the number of
topics that the topic model contains. This means that
the AP of every discussant contains the topic model
distribution of his/her interactions with every other
member in the thread. Hence, the topics in the inter-
ation between the given discussant and other mem-
bers in the thread are being modeled here. Accord-
67
ingly, high vector similarity due to IA between two
members in a thread means that they discussed sim-
ilar topics with the same people in the thread. In
our experiments, we set t = 50. We use the Gibbs
sampling based LDA (Griffiths and Steyvers, 2004).
The LDA model is built on definitions of two online
dictionaries WordNet, and Wiktionary, in addition
to the Brown corpus (BC). To create more context,
each sentence from BC is treated as a document.
The whole corpus contains 393,667 documents and
5,080,369 words.
The degree of agreement among discussants in
terms of these three features is used to identify sub-
groups among them. Our experiments are aimed at
investigating the effect of explicit attitude features
(SD and SE) in comparison with implicit feature
(IA) and how they perform when combined. So
the experimental conditions are: the three features
in isolation, each of the explicit features SD and SE
together with IA, and then all three features together.
SWD-BASE: As a baseline, we employ a simple
word frequency based model to capture topic dis-
tribution, Surface Word Distribution (SWD). SWD
is still topic modeling in the vector space, but the di-
mensions of the vectors are the frequencies of all the
unique words used by the discussant in question.
RAND-BASE: We also apply a very simple base-
line using random assignment of discussants to
groups, however the number of clusters is deter-
mined by the rule of thumb described in Section 3.3.
6 Results and Analysis
Three metrics are used for evaluation, as de-
scribed in (Manning et al, 2008): Purity, Entropy
and F-measure. Table 4 shows the results of the
9 experimental conditions. The following observa-
tions can be made: All the individual conditions SD,
SE and IA clearly outperform SWD-BASE. All the
experimental conditions outperform RAND-BASE
which indicates that using clustering is contributing
positively to the problem. SE performs worse than
SD across both datasets CD and WIKI. This may
be due to two reasons: Firstly, since the problem
is of clustering the discussant space, SD should be
a better indicator than SE. Secondly, as seen from
the comparison in Table 5, there are more polarized
sentences indicating SD than SE. IA clearly outper-
forms SD, SE and SD+SE in the case of WIKI. In
Property WIKI CD
Positive Sentences towards Discussants 5.15 17.94
Negative Sentences towards Discussants 6.75 40.38
Positive Sentences towards Entities 1.65 8.85
Negative Sentences towards Entities 1.59 8.53
Table 5: Statistics of the Attitudinal Sentences per
each Thread in the two data sets
the case of CD, it is exactly the opposite. This is an
interesting result and we believe it is mainly due to
the genre of the data. Explicit expression of senti-
ment usually increases with the increase in the in-
formal nature of discussions. Hence IA is more use-
ful in WIKI which is more formal compared to CD,
where there is less overt sentiment expression. We
note the same trend with the SWD-BASE where per-
formance on WIKI is much better than its perfor-
mance on CD. This also suggests that WIKI might
be an easier data set. A qualitative comparison of the
inter-discussant relations can be gleaned from Ta-
ble 5. There is significantly more negative language
than positive language in CD when compared with
the ratios of negative to positive language in WIKI,
which are almost the same. The best results over-
all are yielded from the combination of IA with SD
and SE, the implicit and explicit features together for
both data sets, which suggests that Implicit and ex-
plicit attitude features complement each other cap-
turing more information than each of them individ-
ually.
7 Conclusions
We proposed the use of LDA based topic mod-
eling as an implicit agreement feature for the task
of identifying similar attitudes in online discussions.
We specifically applied latent modeling to the prob-
lem of sub-group detection. We compared this with
explicit sentiment features in different genres both
in isolation and in combination. We highlighted the
difference in genre in the datasets and the necessity
for capturing different forms of information from
them for the task at hand. The best yielding con-
dition in both the dat sets combines implicit and ex-
plicit features suggesting that there is a complemen-
tarity between the two tpes of feaures.
Acknowledgement
This research was funded by the Office of the Di-
rector of National Intelligence (ODNI), Intelligence
Advanced Research Projects Activity (IARPA),
through the U.S. Army Research Lab.
68
Condition
WIKI CD
Purity Entropy F-measure Purity Entropy F-measure
RAND-BASE 0.6745 0.5629 0.6523 0.3986 0.9664 0.407
SWD-BASE 0.7716 0.4746 0.6455 0.4514 0.9319 0.4322
SD 0.8342 0.3602 0.667 0.8243 0.3942 0.5964
SE 0.8265 0.3829 0.6554 0.7933 0.4216 0.5818
SD+SE 0.8346 0.3614 0.6649 0.82 0.3851 0.6039
IA 0.8527 0.3209 0.6993 0.787 0.3993 0.5891
SD+IA 0.8532 0.3199 0.6977 0.8487 0.3328 0.6152
SE+IA 0.8525 0.3216 0.7015 0.7884 0.3986 0.591
SD+SE+IA 0.8572 0.3104 0.7032 0.8608 0.3149 0.6251
Table 4: Experimental Results
References
Amjad Abu-Jbara, Pradeep Dasigi, Mona Diab, and
Dragomir Radev. 2012. Subgroup detection in ideo-
logical discussions. In Proceedings of the 5oth Annual
Meeting of ACL.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. Journal of Machine
Learning Research, 3.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local information
into information extraction systems by gibbs sampling.
In Proceedings of the 43nd Annual Meeting of the As-
sociation for Computational Linguistics.
Murthy Ganapathibhotla and Bing Liu. 2008. Mining
opinions in comparative sentences. In Proceedings of
the 22nd International Conference on Computational
Linguistics (Coling 2008).
Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
Academy of Sciences, 101.
Ahmed Hassan, Vahed Qazvinian, and Dragomir Radev.
2010. What?s with the attitude? identifying sentences
with attitude in online discussions. In Proceedings of
the 2010 Conference on Empirical Methods in Natural
Language Processing,.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the tenth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining.
Niklas Jakob and Iryna Gurevych. 2010. Using anaphora
resolution to improve opinion target identification in
movie reviews. In Proceedings of the ACL 2010 Con-
ference Short Papers.
Nozomi Kobayashi, Kentaro Inui, and Yuji Matsumoto.
2007. Extracting aspect-evaluation and aspect-of re-
lations in opinion mining. In Proceedings of the
2007 Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural
Language Learning.
J. MacQueen. 1967. Some methods for classification and
analysis of multivariate observations. In Proceedings
of Fifth Berkeley Symposium on Mathematical Statis-
tics and Probability.
Christopher D. Manning, Prabhakar Raghavan, , and Hin-
rich Schtze. 2008. . 2008. Introduction to Information
Retrieval. Cambridge University Press, New York,
NY,USA.
K. V. Mardia, J. T. Kent, and J. M. Bibby. 1979. Multi-
variate Analysis. Publisher.
R. Sibson. 1973. Slink: An optimally efficient algorithm
for the single-link cluster method. In The Computer
Journal (1973) 16 (1): 30-34.
Swapna Somasundaran and Janyce Wiebe. 2009. Rec-
ognizing stances in online debates. In Proceedings of
the Joint Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP.
Theresa Wilson, Paul Hoffmann, Swapna Somasun-
daran, Jason Kessler, JanyceWiebe, Yejin Choi, Claire
Cardie, Ellen Riloff, and Siddharth Patwardhan. 2005.
Opinionfinder: A system for subjectivity analysis. In
Proceedings of HLT/EMNLP 2005 Demonstration.
69
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 140?144,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Learning the Latent Semantics of a Concept from its Definition
Weiwei Guo
Department of Computer Science,
Columbia University,
New York, NY, USA
weiwei@cs.columbia.edu
Mona Diab
Center for Computational Learning Systems,
Columbia University,
New York, NY, USA
mdiab@ccls.columbia.edu
Abstract
In this paper we study unsupervised word
sense disambiguation (WSD) based on sense
definition. We learn low-dimensional latent
semantic vectors of concept definitions to con-
struct a more robust sense similarity measure
wmfvec. Experiments on four all-words WSD
data sets show significant improvement over
the baseline WSD systems and LDA based
similarity measures, achieving results compa-
rable to state of the art WSD systems.
1 Introduction
To date, many unsupervised WSD systems rely on
a sense similarity module that returns a similar-
ity score given two senses. Many similarity mea-
sures use the taxonomy structure of WordNet [WN]
(Fellbaum, 1998), which allows only noun-noun and
verb-verb pair similarity computation since the other
parts of speech (adjectives and adverbs) do not have
a taxonomic representation structure. For example,
the jcn similarity measure (Jiang and Conrath, 1997)
computes the sense pair similarity score based on the
information content of three senses: the two senses
and their least common subsumer in the noun/verb
hierarchy.
The most popular sense similarity measure is the
Extended Lesk [elesk] measure (Banerjee and Peder-
sen, 2003). In elesk, the similarity score is computed
based on the length of overlapping words/phrases
between two extended dictionary definitions. The
definitions are extended by definitions of neighbor
senses to discover more overlapping words. How-
ever, exact word matching is lossy. Below are two
definitions from WN:
bank#n#1: a financial institution that accepts deposits
and channels the money into lending activities
stock#n#1: the capital raised by a corporation through
the issue of shares entitling holders to an ownership in-
terest (equity)
Despite the high semantic relatedness of the two
senses, the overlapping words in the two definitions
are only a, the, leading to a very low similarity score.
Accordingly we are interested in extracting latent
semantics from sense definitions to improve elesk.
However, the challenge lies in that sense defini-
tions are typically too short/sparse for latent vari-
able models to learn accurate semantics, since these
models are designed for long documents. For exam-
ple, topic models such as LDA (Blei et al, 2003),
can only find the dominant topic based on the ob-
served words in a definition (financial topic in
bank#n#1 and stock#n#1) without further dis-
cernibility. In this case, many senses will share the
same latent semantics profile, as long as they are in
the same topic/domain.
To solve the sparsity issue we use missing words
as negative evidence of latent semantics, as in (Guo
and Diab, 2012). We define missing words of a sense
definition as the whole vocabulary in a corpus minus
the observed words in the sense definition. Since
observed words in definitions are too few to reveal
the semantics of senses, missing words can be used
to tell the model what the definition is not about.
Therefore, we want to find a latent semantics pro-
file that is related to observed words in a definition,
but also not related to missing words, so that the in-
duced latent semantics is unique for the sense.
Finally we also show how to use WN neighbor
sense definitions to construct a nuanced sense simi-
larity wmfvec, based on the inferred latent semantic
vectors of senses. We show that wmfvec outperforms
elesk and LDA based approaches in four All-words
WSD data sets. To our best knowledge, wmfvec is
the first sense similarity measure based on latent se-
mantics of sense definitions.
140
financial sport institution Ro Rm
v1 1 0 0 20 600
v2 0.6 0 0.1 18 300
v3 0.2 0.3 0.2 5 100
Table 1: Three possible hypotheses of latent vectors for
the definition of bank#n#1
2 Learning Latent Semantics of Definitions
2.1 Intuition
Given only a few observed words in a definition,
there are many hypotheses of latent vectors that are
highly related to the observed words. Therefore,
missing words can be used to prune the hypotheses
that are also highly related to the missing words.
Consider the hypotheses of latent vectors in ta-
ble 1 for bank#n#1. Assume there are 3 dimen-
sions in our latent model: financial, sport, institu-
tion. We use Rvo to denote the sum of relatedness
between latent vector v and all observed words; sim-
ilarly, Rvm is the sum of relatedness between the
vector v and all missing words. Hypothesis v1 is
given by topic models, where only the financial
dimension is found, and it has the maximum relat-
edness to observed words in bank#n#1 definition
Rv1o = 20. v2 is the ideal latent vector, since it also
detects that bank#n#1 is related to institution. It
has a slightly smaller Rv2o = 18, but more impor-
tantly, its relatedness to missing words, Rv2m = 300,
is substantially smaller than Rv1m = 600.
However, we cannot simply choose a hypothesis
with the maximum Ro ?Rm value, since v3, which
is clearly not related to bank#n#1 but with a min-
imum Rm = 100, will therefore be (erroneously)
returned as the answer. The solution is straightfor-
ward: give a smaller weight to missing words, e.g.,
so that the algorithm tries to select a hypothesis with
maximum value of Ro ? 0.01 ? Rm. We choose
weighted matrix factorization [WMF] (Srebro and
Jaakkola, 2003) to implement this idea.
2.2 Modeling Missing Words by Weighted
Matrix Factorization
We represent the corpus of WN definitions as an
M ?N matrix X , where row entries are M unique
words existing in WN definitions, and columns rep-
resent N WN sense ids. The cell Xij records the
TF-IDF value of word wi appearing in definition of
sense sj .
In WMF, the original matrix X is factorized into
two matrices such that X ? P>Q, where P is a
K ? M matrix, and Q is a K ? N matrix. In
this scenario, the latent semantics of each word wi
or sense sj is represented as a K-dimension vector
P?,i or Q?,j respectively. Note that the inner product
of P?,i and Q?,j is used to approximate the seman-
tic relatedness of word wi and definition of sense sj :
Xij ? P?,i ?Q?,j .
In WMF each cell is associated with a weight, so
missing words cells (Xij=0) can have a much less
contribution than observed words. Assume wm is
the weight for missing words cells. The latent vec-
tors of words P and senses Q are estimated by min-
imizing the objective function:1?
i
?
j
Wij (P?,i ?Q?,j ?Xij)
2 + ?||P ||22 + ?||Q||
2
2
where Wi,j =
{
1, if Xij 6= 0
wm, if Xij = 0
(1)
Equation 1 explicitly requires the latent vector of
sense Q?,j to be not related to missing words (P?,i ?
Q?,j should be close to 0 for missing words Xij =
0). Also weight wm for missing words is very small
to make sure latent vectors such as v3 in table 1 will
not be chosen. In experiments we set wm = 0.01.
After we run WMF on the definitions corpus, the
similarity of two senses sj and sk can be computed
by the inner product of Q?,j and Q?,k.
2.3 A Nuanced Sense Similarity: wmfvec
We can further use the features in WordNet to con-
struct a better sense similarity measure. The most
important feature of WN is senses are connected by
relations such as hypernymy, meronymy, similar at-
tributes, etc. We observe that neighbor senses are
usually similar, hence they could be a good indica-
tor for the latent semantics of the target sense.
We use WN neighbors in a way similar to elesk.
Note that in elesk each definition is extended by in-
cluding definitions of its neighbor senses. Also, they
do not normalize the length. In our case, we also
adopt these two ideas: (1) a sense is represented by
the sum of its original latent vector and its neigh-
bors? latent vectors. Let N(j) be the set of neigh-
bor senses of sense j. then new latent vector is:
Qnew?,j = Q?,j +
?k?N(j)
k Q?,k (2) Inner product (in-
stead of cosine similarity) of the two resulting sense
vectors is treated as the sense pair similarity. We
refer to our sense similarity measure as wmfvec.
1Due to limited space inference and update rules for P and
Q are omitted, but can be found in (Srebro and Jaakkola, 2003)
141
3 Experiment Setting
Task: We choose the fine-grained All-Words Sense
Disambiguation task, where systems are required to
disambiguate all the content words (noun, adjective,
adverb and verb) in documents. The data sets we use
are all-words tasks in SENSEVAL2 [SE2], SENSE-
VAL3 [SE3], SEMEVAL-2007 [SE07], and Semcor.
We tune the parameters in wmfvec and other base-
lines based on SE2, and then directly apply the tuned
models on other three data sets.
Data: The sense inventory is WN3.0 for the four
WSD data sets. WMF and LDA are built on the cor-
pus of sense definitions of two dictionaries: WN and
Wiktionary [Wik].2 We do not link the senses across
dictionaries, hence Wik is only used as augmented
data for WMF to better learn the semantics of words.
All data is tokenized, POS tagged (Toutanova et al,
2003) and lemmatized, resulting in 341,557 sense
definitions and 3,563,649 words.
WSD Algorithm: To perform WSD we need two
components: (1) a sense similarity measure that re-
turns a similarity score given two senses; (2) a dis-
ambiguation algorithm that determines which senses
to choose as final answers based on the sense pair
similarity scores. We choose the Indegree algorithm
used in (Sinha and Mihalcea, 2007; Guo and Diab,
2010) as our disambiguation algorithm. It is a graph-
based algorithm, where nodes are senses, and edge
weight equals to the sense pair similarity. The final
answer is chosen as the sense with maximum inde-
gree. Using the Indegree algorithm allows us to eas-
ily replace the sense similarity with wmfvec. In In-
degree, two senses are connected if their words are
within a local window. We use the optimal window
size of 6 tested in (Sinha and Mihalcea, 2007; Guo
and Diab, 2010).
Baselines: We compare with (1) elesk, the most
widely used sense similarity. We use the implemen-
tation in (Pedersen et al, 2004).
We believe WMF is a better approach to model
latent semantics than LDA, hence the second base-
line (2) LDA using Gibbs sampling (Griffiths and
Steyvers, 2004). However, we cannot directly use
estimated topic distribution P (z|d) to represent the
definition since it only has non-zero values on one
or two topics. Instead, we calculate the latent vec-
2http://en.wiktionary.org/
Data Model Total Noun Adj Adv Verb
SE2 random 40.7 43.9 43.6 58.2 21.6
elesk 56.0 63.5 63.9 62.1 30.8
ldavec 58.6 68.6 60.2 66.1 33.2
wmfvec 60.5 69.7 64.5 67.1 34.9
jcn+elesk 60.1 69.3 63.9 62.8 37.1
jcn+wmfvec 62.1 70.8 64.5 67.1 39.9
SE3 random 33.5 39.9 44.1 - 33.5
elesk 52.3 58.5 57.7 - 41.4
ldavec 53.5 58.1 60.8 - 43.7
wmfvec 55.8 61.5 64.4 - 43.9
jcn+elesk 55.4 60.5 57.7 - 47.4
jcn+wmfvec 57.4 61.2 64.4 - 48.8
SE07 random 25.6 27.4 - - 24.6
elesk 42.2 47.2 - - 39.5
ldavec 43.7 49.7 - - 40.5
wmfvec 45.1 52.2 - - 41.2
jcn+elesk 44.5 52.8 - - 40.0
jcn+wmfvec 45.5 53.5 - - 41.2
Semcor random 35.26 40.13 50.02 58.90 20.08
elesk 55.43 61.04 69.30 62.85 43.36
ldavec 58.17 63.15 70.08 67.97 46.91
wmfvec 59.10 64.64 71.44 67.05 47.52
jcn+elesk 61.61 69.61 69.30 62.85 50.72
jcn+wmfvec 63.05 70.64 71.45 67.05 51.72
Table 2: WSD results per POS (K = 100)
tor of a definition by summing up the P (z|w) of
all constituent words weighted by Xij , which gives
much better WSD results.3 We produce LDA vec-
tors [ldavec] in the same setting as wmfvec, which
means it is trained on the same corpus, uses WN
neighbors, and is tuned on SE2.
At last, we compare wmfvec with a mature WSD
system based on sense similarities, (3) (Sinha and
Mihalcea, 2007) [jcn+elesk], where they evaluate six
sense similarities, select the best of them and com-
bine them into one system. Specifically, in their im-
plementation they use jcn for noun-noun and verb-
verb pairs, and elesk for other pairs. (Sinha and Mi-
halcea, 2007) used to be the state-of-the-art system
on SE2 and SE3.
4 Experiment Results
The disambiguation results (K = 100) are summa-
rized in Table 2. We also present in Table 3 results
using other values of dimensions K for wmfvec and
ldavec. There are very few words that are not cov-
ered due to failure of lemmatization or POS tag mis-
matches, thereby F-measure is reported.
Based on SE2, wmfvec?s parameters are tuned as
? = 20, wm = 0.01; ldavec?s parameters are tuned
as ? = 0.05, ? = 0.05. We run WMF on WN+Wik
for 30 iterations, and LDA for 2000 iterations. For
3It should be noted that this renders LDA a very challenging
baseline to outperform.
142
LDA, more robust P (w|z) is generated by averag-
ing over the last 10 sampling iterations. We also set
a threshold to elesk similarity values, which yields
better performance. Same as (Sinha and Mihalcea,
2007), values of elesk larger than 240 are set to 1,
and the rest are mapped to [0,1].
elesk vs wmfvec: wmfvec outperforms elesk consis-
tently in all POS cases (noun, adjective, adverb and
verb) on four datasets by a large margin (2.9% ?
4.5% in total case). Observing the results yielded
per POS, we find a large improvement comes from
nouns. Same trend has been reported in other distri-
butional methods based on word co-occurrence (Cai
et al, 2007; Li et al, 2010; Guo and Diab, 2011).
More interestingly, wmfvec also improves verbs ac-
curacy significantly.
ldavec vs wmfvec: ldavec also performs very well,
again proving the superiority of latent semantics
over surface words matching. However, wmfvec also
outperforms ldavec in every POS case except Sem-
cor adverbs (at least +1% in total case). We observe
the trend is consistent in Table 3 where different di-
mensions are used for ldavec and wmfvec. These
results show that given the same text data, WMF
outperforms LDA on modeling latent semantics of
senses by exploiting missing words.
jcn+elesk vs jcn+wmfvec: jcn+elesk is a very ma-
ture WSD system that takes advantage of the great
performance of jcn on noun-noun and verb-verb
pairs. Although wmfvec does much better than elesk,
using wmfvec solely is sometimes outperformed by
jcn+elesk on nouns and verbs. Therefore to beat
jcn+elesk, we replace the elesk in jcn+elesk with
wmfvec (hence jcn+wmfvec). Similar to (Sinha and
Mihalcea, 2007), we normalize wmfvec similarity
such that values greater than 400 are set to 1, and
the rest values are mapped to [0,1]. We choose the
value 400 based on the WSD performance on tun-
ing set SE2. As expected, the resulting jcn+wmfvec
can further improve jcn+elesk for all cases. More-
over, jcn+wmfvec produces similar results to state-
of-the-art unsupervised systems on SE02, 61.92%
F-mearure in (Guo and Diab, 2010) using WN1.7.1,
and SE03, 57.4% in (Agirre and Soroa, 2009) us-
ing WN1.7. It shows wmfvec is robust that it not
only performs very well individually, but also can
be easily incorporated with existing evidence as rep-
resented using jcn.
dim SE2 SE3 SE07 Semcor
50 57.4 - 60.5 52.9 - 54.9 43.1 - 44.2 57.90 - 58.99
75 57.8 - 60.3 53.5 - 55.2 43.3 - 44.6 58.12 - 59.07
100 58.6 - 60.5 53.5 - 55.8 43.7 - 45.1 58.17 - 59.10
125 58.2 - 60.2 53.9 - 55.5 43.7 - 45.1 58.26 - 59.19
150 58.2 - 59.8 53.6 - 54.6 44.4 - 45.9 58.13 - 59.15
Table 3: ldavec and wmfvec (latter) results per # of dimensions
4.1 Discussion
We look closely into WSD results to obtain an in-
tuitive feel for what is captured by wmfvec. For ex-
ample, the target word mouse in the context: ... in
experiments with mice that a gene called p53 could
transform normal cells into cancerous ones... elesk
returns the wrong sense computer device, due to the
sparsity of overlapping words between definitions
of animal mouse and the context words. wmfvec
chooses the correct sense animal mouse, by recog-
nizing the biology element of animal mouse and re-
lated context words gene, cell, cancerous.
5 Related Work
Sense similarity measures have been the core com-
ponent in many unsupervised WSD systems and
lexical semantics research/applications. To date,
elesk is the most popular such measure (McCarthy
et al, 2004; Mihalcea, 2005; Brody et al, 2006).
Sometimes people use jcn to obtain similarity of
noun-noun and verb-verb pairs (Sinha and Mihalcea,
2007; Guo and Diab, 2010). Our similarity measure
wmfvec exploits the same information (sense defini-
tions) elesk and ldavec use, and outperforms them
significantly on four standardized data sets. To our
best knowledge, we are the first to construct a sense
similarity by latent semantics of sense definitions.
6 Conclusions
We construct a sense similarity wmfvec from the la-
tent semantics of sense definitions. Experiment re-
sults show wmfvec significantly outperforms previ-
ous definition-based similarity measures and LDA
vectors on four all-words WSD data sets.
Acknowledgments
This research was funded by the Office of the Di-
rector of National Intelligence (ODNI), Intelligence
Advanced Research Projects Activity (IARPA),
through the U.S. Army Research Lab. All state-
ments of fact, opinion or conclusions contained
herein are those of the authors and should not be
construed as representing the official views or poli-
cies of IARPA, the ODNI or the U.S. Government.
143
References
Eneko Agirre and Aitor Soroa. 2009. Proceedings of per-
sonalizing pagerank for word sense disambiguation.
In the 12th Conference of the European Chapter of the
ACL.
Satanjeev Banerjee and Ted Pedersen. 2003. Extended
gloss overlaps as a measure of semantic relatedness.
In Proceedings of the 18th International Joint Confer-
ence on Artificial Intelligence, pages 805?810.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. Journal of Machine
Learning Research, 3.
Samuel Brody, Roberto Navigli, and Mirella Lapata.
2006. Ensemble methods for unsupervised wsd. In
Proceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meeting
of the ACL.
Jun Fu Cai, Wee Sun Lee, and Yee Whye Teh. 2007.
Improving word sense disambiguation using topic fea-
tures. In Proceedings of the 2007 Joint Conference on
Empirical Methods in Natural Language Processing
and Computational Natural Language Learning.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
Academy of Sciences, 101.
Weiwei Guo and Mona Diab. 2010. Combining orthogo-
nal monolingual and multilingual sources of evidence
for all words wsd. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics.
Weiwei Guo and Mona Diab. 2011. Semantic topic mod-
els: Combining word distributional statistics and dic-
tionary definitions. In Proceedings of the 2011 Con-
ference on Empirical Methods in Natural Language
Processing.
Weiwei Guo and Mona Diab. 2012. Modeling sentences
in the latent space. In Proceedings of the 50th Annual
Meeting of the Association for Computational Linguis-
tics.
Jay J. Jiang and David W. Conrath. 1997. Finding pre-
dominant word senses in untagged text. In Proceed-
ings of International Conference Research on Compu-
tational Linguistics.
Linlin Li, Benjamin Roth, and Caroline Sporleder. 2010.
Topic models for word sense disambiguation and
token-based idiom detection. In Proceedings of the
48th Annual Meeting of the Association for Computa-
tional Linguistics.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2004. Finding predominant word senses in
untagged text. In Proceedings of the 42nd Meeting of
the Association for Computational Linguistics.
Rada Mihalcea. 2005. Unsupervised large-vocabulary
word sense disambiguation with graph-based algo-
rithms for sequence data labeling. In Proceedings of
the Joint Conference on Human Language Technology
and Empirical Methods in Natural Language Process-
ing, pages 411?418.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. Wordnet::similarity - measuring the re-
latedness of concepts. In Proceedings of Fifth Annual
Meeting of the North American Chapter of the Associ-
ation for Computational Linguistics.
Ravi Sinha and Rada Mihalcea. 2007. Unsupervised
graph-based word sense disambiguation using mea-
sures of word semantic similarity. In Proceedings of
the IEEE International Conference on Semantic Com-
puting, pages 363?369.
Nathan Srebro and Tommi Jaakkola. 2003. Weighted
low-rank approximations. In Proceedings of the Twen-
tieth International Conference on Machine Learning.
Kristina Toutanova, Dan Klein, Christopher Manning, ,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Pro-
ceedings of the 2003 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics on Human Language Technology.
144
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 239?249,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Linking Tweets to News: A Framework to Enrich Short Text Data in
Social Media
Weiwei Guo? and Hao Li? and Heng Ji? and Mona Diab?
?Department of Computer Science, Columbia University
?Computer Science Department and Linguistic Department,
Queens College and Graduate Center, City University of New York
?Department of Computer Science, George Washington University
weiwei@cs.columbia.edu, {haoli.qc,hengjicuny}@gmail.com, mtdiab@gwu.edu
Abstract
Many current Natural Language Process-
ing [NLP] techniques work well assum-
ing a large context of text as input data.
However they become ineffective when
applied to short texts such as Twitter feeds.
To overcome the issue, we want to find
a related newswire document to a given
tweet to provide contextual support for
NLP tasks. This requires robust model-
ing and understanding of the semantics of
short texts.
The contribution of the paper is two-fold:
1. we introduce the Linking-Tweets-to-
News task as well as a dataset of linked
tweet-news pairs, which can benefit many
NLP applications; 2. in contrast to previ-
ous research which focuses on lexical fea-
tures within the short texts (text-to-word
information), we propose a graph based
latent variable model that models the in-
ter short text correlations (text-to-text in-
formation). This is motivated by the ob-
servation that a tweet usually only cov-
ers one aspect of an event. We show that
using tweet specific feature (hashtag) and
news specific feature (named entities) as
well as temporal constraints, we are able to
extract text-to-text correlations, and thus
completes the semantic picture of a short
text. Our experiments show significant im-
provement of our new model over base-
lines with three evaluation metrics in the
new task.
1 Introduction
Recently there has been an increasing interest in
language understanding of Twitter messages. Re-
searchers (Speriosui et al, 2011; Brody and Di-
akopoulos, 2011; Jiang et al, 2011) were in-
terested in sentiment analysis on Twitter feeds,
and opinion mining towards political issues or
politicians (Tumasjan et al, 2010; Conover et al,
2011). Others (Ramage et al, 2010; Jin et al,
2011) summarized tweets using topic models. Al-
though these NLP techniques are mature, their
performance on tweets inevitably degrades, due to
the inherent sparsity in short texts. In the case
of sentiment analysis, while people are able to
achieve 87.5% accuracy (Maas et al, 2011) on a
movie review dataset (Pang and Lee, 2004), the
performance drops to 75% (Li et al, 2012) on
a sentence level movie review dataset (Pang and
Lee, 2005). The problem worsens when some
existing NLP systems cannot produce any results
given the short texts. Considering the following
tweet:
Pray for Mali...
A typical event extraction/discovery system (Ji
and Grishman, 2008) fails to discover the war
event due to the lack of context information (Ben-
son et al, 2011), and thus fails to shed light on the
users focus/interests.
To enable the NLP tools to better understand
Twitter feeds, we propose the task of linking a
tweet to a news article that is relevant to the tweet,
thereby augmenting the context of the tweet. For
example, we want to supplement the implicit con-
text of the above tweet with a news article such as
the following entitled:
State of emergency declared in Mali
where abundant evidence can be fed into an off-
the-shelf event extraction/discovery system. To
create a gold standard dataset, we download tweets
spanning over 18 days, each with a url linking to a
news article of CNN or NYTIMES, as well as all
the news of CNN and NYTIMES published during
the period. The goal is to predict the url referred
news article based on the text in each tweet.1 We
1The data and code is publicly available at www.cs.
239
believe many NLP tasks will benefit from this task.
In fact, in the topic modeling research, previous
work (Jin et al, 2011) already showed that by in-
corporating webpages whose urls are contained
in tweets, the tweet clustering purity score was
boosted from 0.280 to 0.392.
Given the few number of words in a tweet (14
words on average in our dataset), the traditional
high dimensional surface word matching is lossy
and fails to pinpoint the news article. This con-
stitutes a classic short text semantics impediment
(Agirre et al, 2012). Latent variable models are
powerful by going beyond the surface word level
and mapping short texts into a low dimensional
dense vector (Socher et al, 2011; Guo and Diab,
2012b). Accordingly, we apply a latent variable
model, namely, the Weighted Textual Matrix Fac-
torization [WTMF] (Guo and Diab, 2012b; Guo
and Diab, 2012c) to both the tweets and the news
articles. WTMF is a state-of-the-art unsupervised
model that was tested on two short text similar-
ity datasets: (Li et al, 2006) and (Agirre et al,
2012), which outperforms Latent Semantic Anal-
ysis [LSA] (Landauer et al, 1998) and Latent
Dirichelet Allocation [LDA] (Blei et al, 2003) by
a large margin. We employ it as a strong baseline
in this task as it exploits and effectively models the
missing words in a tweet, in practice adding thou-
sands of more features for the tweet, by contrast
LDA, for example, only leverages observed words
(14 features) to infer the latent vector for a tweet.
Apart from the data sparseness, our dataset pro-
poses another challenge: a tweet usually covers
only one aspect of an event. In our previous ex-
ample, the tweet only contains the location Mali
while the event is about French army participated
in Mali war. In this scenario, we would like to find
the missing elements of the tweet such as French,
war from other short texts, to complete the seman-
tic picture of Pray in Mali tweet. One drawback
of WTMF for our purposes is that it simply mod-
els the text-to-word information without leverag-
ing the correlation between short texts. While
this is acceptable on standard short text similarity
datasets (data points are independently generated),
it ignores some valuable information characteristi-
cally present in our dataset: (1) The tweet specific
features such as hashtags. Hashtags prove to be
a direct indication of the semantics of tweets (Ra-
mage et al, 2010); (2) The news specific features
columbia.edu/?weiwei
such as named entities in a document. Named en-
tities acquired from a news document, typically
with high accuracy using Named Entity Recog-
nition [NER] tools, may be particularly informa-
tive. If two texts mention the same entities then
they might describe the same event; (3) The tem-
poral information in both genres (tweets and news
articles). We note that there is a higher chance
of event description overlap between two texts if
their time of publication is similar.
In this paper, we study the problem of min-
ing and exploiting correlations between texts us-
ing these features. Two texts may be considered
related or complementary if they share a hash-
tag/NE or satisfies the temporal constraints. Our
proposed latent variable model not only models
text-to-word information, but also is aware of the
text-to-text information (illustrated in Figure 1):
two linked texts should have similar latent vec-
tors, accordingly the semantic picture of a tweet is
completed by receiving semantics from its related
tweets. We incorporate this additional information
in the WTMF model. We also show the differ-
ent impact of the text-to-text relations in the tweet
genre and news genre. We are able to achieve sig-
nificantly better results than with a text-to-words
WTMF model. This work can be regarded as a
short text modeling approach that extends previ-
ous work however with a focus on combining the
mining of information within short texts coupled
with utilizing extra shared information across the
short texts.
2 Task and Data
The task is given the text in a tweet, a system aims
to find the most relevant news article. For gold
standard data, we harvest all the tweets that have a
single url link to a CNN or NYTIMES news arti-
cle, dated from the 11th of Jan to the 27th of Jan,
2013. In evaluation, we consider this url-referred
news article as the gold standard ? the most rele-
vant document for the tweet, and remove the url
from the text of the tweet. We also collect all the
news articles from both CNN and NYTIMES from
RSS feeds during the same timeframe. Each tweet
entry has the published time, author, text; each
news entry contains published time, title, news
summary, url. The tweet/news pairs are extracted
by matching urls. We manually filtered ?trivial?
tweets where the tweet content is simply the news
title or news summary. The final dataset results in
240
(a)	 ?
t1	 ? t2	 ?
n2	 ?n1	 ?
t3	 ?
w1	 ? w2	 ? w3	 ? w4	 ? w5	 ? w6	 ? w7	 ? w8	 ?
t1	 ? t2	 ?
n2	 ?n1	 ?
t3	 ?
w1	 ? w2	 ? w3	 ? w4	 ? w5	 ? w6	 ? w7	 ? w8	 ?
#healthcare	 ? Obama	 ?
temporal	 ? (b)	 ?
Figure 1: (a) WTMF. (b) WTMF-G: the tweet nodes t and news nodes n are connected by hashtags, named entities or
temporal edges (for simplicity, the missing tokens are not shown in the figure)
34,888 tweets and 12,704 news articles.
It is worth noting that the news corpus is not
restricted to current events. It covers various gen-
res and topics, such as travel guides. e.g. World?s
most beautiful lakes, and health issues, e.g. The
importance of a ?stop day?, etc.
2.1 Evaluation metric
For our task evaluation, ideally, we would like
the system to be able to identify the news arti-
cle specifically referred to by the url within each
tweet in the gold standard. However, this is very
difficult given the large number of potential can-
didates, especially those with slight variations.
Therefore, following the Concept Definition Re-
trieval task in (Guo and Diab, 2012b) and (Steck,
2010) we use a metric for evaluating the ranking
of the correct news article to evaluate the systems,
namely, ATOPt, area under the TOPKt(k) recall
curve for a tweet t. Basically, it is the normal-
ized ranking ? [0, 1] of the correct news article
among all candidate news articles: ATOPt = 1
means the url-referred news article has the highest
similarity value with the tweet (a correct NARU);
ATOPt = 0.95 means the similarity value with
correct news article is larger than 95% of the can-
didates, i.e. within the top 5% of the candidates.
ATOPt is calculated as follows:
ATOPt =
? 1
0
TOPKt(k)dk (1)
where TOPKt(k) = 1 if the url referred news arti-
cle is in the ?top k? list, otherwise TOPKt(k) = 0.
Here k ? [0, 1] is the relative position (when
k = 1, it means all the candidates).
We also include other metrics to examine if the
system is able to rank the url referred news arti-
cle in the first few returned results: TOP10 recall
hit rate to evaluate whether the correct news is in
the top 10 results, and RR, Reciprocal Rank= 1/r
(i.e., RR= 1/3 when the correct news article is
ranked at the 3rd highest place).
3 Weighted Textual Matrix Factorization
The WTMF model (Guo and Diab, 2012a) has
been successfully applied to the short text simi-
larity task, achieving state-of-the-art unsupervised
performance. This can be attributed to the fact that
it models the missing tokens as features, thereby
adding many more features for a short text. The
missing words of a sentence are defined as all the
vocabulary of the training corpus minus the ob-
served words in a sentence. Missing words serve
as negative examples for the semantics of a short
text: the short text should not be related to the
missing words.
As per (Guo and Diab, 2012b), the corpus is
represented in a matrix X , where each cell stores
the TF-IDF values of words. The rows of X are
words and columns are short texts. As in Figure
2, matrix X is approximated by the product of a
K?M matrix P and a K?N matrix Q. Accord-
ingly, each sentence sj is represented by a K di-
mensional latent vector Q?,j . Similarly a word wi
is generalized by P?,i. Therefore, the inner product
of a word vector P?,i and a short text vector Q?,j is
to approximate the cell Xij (shaded part in Figure
2). In this way, the missing words are modeled by
requiring the inner product of a word vector and
short text vector to be close to 0 (the word and the
short text should be irrelevant).
Since 99% cells in X are missing tokens (0
value), the impact of observed words is signifi-
cantly diminished. Therefore a small weight wm
is assigned for each 0 cell (missing tokens) in the
matrix X in order to preserve the influence of ob-
served words. P andQ are optimized by minimize
the objective function:
241
Figure 2: Weighted Textual Matrix Factorization
?
i
?
j
Wij (P?,i ?Q?,j ?Xij)2 + ?||P ||22 + ?||Q||22
Wi,j =
{
1, if Xij 6= 0
wm, if Xij = 0
(2)
where ? is a regularization term.
4 Creating Text-to-text Relations via
Twitter/News Features
WTMF exploits the text-to-word information in a
very nuanced way, while the dependency between
texts is ignored. In this Section, we introduce how
to create text-to-text relations.
4.1 Hashtags and Named Entities
Hashtags highlight the topics in tweets, e.g., The
#flu season has started. We believe two tweets
sharing the same hashtag should be related, hence
we place a link between them to explicitly inform
the model that these two tweets should be similar.
We find only 8,701 tweets out of 34,888 include
hashtags. In fact, we observe many hashtag words
are mentioned in tweets without explicitly being
tagged with #. To overcome the hashtag sparse-
ness issue, one can resort to keywords recommen-
dation algorithms to mine hashtags for the tweets
(Yang et al, 2012). In this paper, we adopt a sim-
ple but effective approach: we collect all the hash-
tags in the dataset, and automatically hashtag any
word in a tweet if that word appears hashtagged in
any other tweets. This process resulted in 33,242
tweets automatically labeled with hashtags. For
each tweet, and for each hashtag it contains, we
extract k tweets that contain this hashtag, assum-
ing they are complementary to the target tweet,
and link the k tweets to the target tweet. If there
are more than k tweets found, we choose the top
k ones that are most chronologically close to the
target tweet. The statistics of links can be found in
table 2.
Named entities are some of the most salient fea-
tures in a news article. Directly applying Named
Entity Recognition (NER) tools on news titles or
tweets results in many errors (Liu et al, 2011) due
to the noise in the data, such as slang and capital-
ization. Accordingly, we first apply the NER tool
on news summaries, then label named entities in
the tweets in the same way as labeling the hash-
tags: if there is a string in the tweet that matches
a named entity from the summaries, then it is la-
beled as a named entity in the tweet. 25,132 tweets
are assigned at least one named entity.2 To create
the similar tweet set, we find k tweets that also
contain the named entity.
4.2 Temporal Relations
Tweets published in the same time interval have
a larger chance of being similar than those are
not chronologically close (Wang and McCallum,
2006). However, we cannot simply assume any
two tweets are similar only based on the times-
tamp. Therefore, for a tweet we link it to the
k most similar tweets whose published time is
within 24 hours of the target tweet?s timestamp.
We use the similarity score returned by WTMF
model to measure the similarity of two tweets.
We experimented with other features such as au-
thorship. We note that it was not a helpful feature.
While authorship information helps in the task of
news/tweets recommendation for a user (Corso et
al., 2005; Yan et al, 2012), the authorship infor-
mation is too general for this task where we target
on ?recommending? a news article for a tweet.
4.3 Creating Relations on News
We extract the 3 subgraphs (based on hash-
tags, named entities and temporal) on news ar-
ticles. However, automatically tagging hashtags
or named entities leads to much worse perfor-
mance (around 93% ATOP values, a 3% decrease
from baseline WTMF). There are several reasons
for this: 1. When a hashtag-matched word ap-
pears in a tweet, it is often related to the central
meaning of the tweet, however news articles are
generally much longer than tweets, resulting in
many more hashtags/named entities matches even
though these named entities may not be closely re-
lated. 2. The noise introduced during automatic
NER accumulates much faster given the large
number of named entities in news data. There-
fore we only extract temporal relations for news
articles.
2Note that there are some false positive named entities
detected such as apple. We plan to address removing noisy
named entities and hashtags in future work
242
5 WTMF on Graphs
We propose a novel model to incorporate the links
generated as described in the previous section.
If two texts are connected by a link, it means
they should be semantically similar. In the WTMF
model, we would like the latent vectors of two
text nodes Q?,j1 , Q?,j2 to be as similar as possible,
namely that their cosine similarity to be close to 1.
To implement this, we add a regularization term in
the objective function of WTMF (equation 2) for
each linked pairs Q?,j1 , Q?,j2 :
? ? ( Q?,j1 ?Q?,j2|Q?,j1 ||Q?,j2 |
? 1)2 (3)
where |Q?,j | denotes the length of vectorQ?,j . The
coefficient ? denotes the importance of the text-to-
text links. A larger ? means we put more weights
on the text-to-text links and less on the text-to-
word links. We refer to this model as WTMF-G
(WTMF on graphs).
5.1 Inference
Alternating Least Square [ALS] is used for in-
ference in weighted matrix factorization (Srebro
and Jaakkola, 2003). However, ALS is no longer
applicable here with the new regularization term
(equation 3) involving the length of text vectors
|Q?,j |, which is not in quadratic form. Therefore
we approximate the objective function by treating
the vector length |Q?,j | as fixed values during the
ALS iterations:
P?,i =
(
QW? (i)Q> + ?I
)?1
QW? (i)X?,i
Q?,j =
(
PW? (j)P> + ?I + ?L2(j)Q?,s(j)diag(L2(s(j)))Q>?,s(j)
)?1
(
PW? (j)X>j,? + ?L(j)Q?,s(j)Ln(j)
)
(4)
We define n(j) as the linked neighbors of short
text j, and Q?,n(j) as the set of latent vectors of
j?s neighbors. The reciprocal of length of these
vectors in the current iteration are stored in Ls(j).
Similarly, the reciprocal of the length of the short
text vector Q?,j is Lj . W? (i) = diag(W?,i) is an
M ?M diagonal matrix containing the ith row of
weight matrixW . Due to limited space, the details
of the optimization are not shown in this paper;
they can be found in (Steck, 2010).
6 Experiments
6.1 Experiment Setting
Corpora: We use the same corpora as in (Guo
and Diab, 2012b): Brown corpus (each sentence is
treated as a document), sense definitions of Wik-
tionary and Wordnet (Fellbaum, 1998). The tweets
and news articles are also included in the cor-
pus, generating 441,258 short texts and 5,149,122
words. The data is tokenized, POS-tagged by
Stanford POS tagger (Toutanova et al, 2003),
and lemmatized by WordNet::QueryData.pm. The
value of each word in matrixX is its TF-IDF value
in the short text.
Baselines: We present 4 baselines: 1. Informa-
tion Retrieval model [IR], which simply treats a
tweet as a document, and performs traditional sur-
face word matching. 2. LDA-? with Gibbs Sam-
pling as inference method. We use the inferred
topic distribution ? as a latent vector to represent
the tweet/news. 3. LDA-wvec. The problem with
LDA-? is the inferred topic distribution latent vec-
tor is very sparse with only a few non-zero val-
ues, resulting in many tweet/news pairs receiving
a high similarity value as long as they are in the
same topic domain. Hence following (Guo and
Diab, 2012b), we first compute the latent vector
of a word by P (z|w) (topic distribution per word),
then average the word latent vectors weighted by
TF-IDF values to represent the short text, which
yields much better results. 4. WTMF. In these
baselines, hashtags and named entities are simply
treated as words.
To curtail variation in results due to random-
ness, each reported number is the average of 10
runs. For WTMF and WTMF-G, we assign the
same initial random values and run 20 iterations.
In both systems we fix the missing words weight
as wm = 0.01 and regularization coefficient at
? = 20, which is the best condition of WTMF
found in (Guo and Diab, 2012b; Guo and Diab,
2012c). For LDA-? and LDA-wvec, we run Gibbs
Sampling based LDA for 2000 iterations and aver-
age the model over the last 10 iterations.
Evaluation: The similarity between a tweet and
a news article is measured by cosine similarity. A
news article is represented as the concatenation of
its title and its summary, which yields better per-
formance.3
As in (Guo and Diab, 2012b), for each tweet,
we collect the 1,000 news articles published prior
to the tweet whose dates of publication are clos-
est to that of the tweet. 4 The cosine similarity
3While these are separated, WTMF receive ATOP
95.558% for representing news article as titles and 94.385%
for representing news article as summaries
4Ideally we want to include all the news articles published
243
Models Parameters ATOP TOP10 RRdev test dev test dev test
IR - 90.795% 90.743% 73.478% 74.103% 46.024% 46.281%
LDA-? ? = 0.05, ? = 0.05 81.368% 81.251% 32.328% 31.207% 13.134% 12.469%
LDA-wvec ? = 0.05, ? = 0.05 94.148% 94.196% 53.500% 53.952% 28.743% 27.904%
WTMF - 95.964% 96.092% 75.327% 76.411% 45.310% 46.270%
WTMF-G k = 3, ? = 3 96.450% 96.543% 76.485% 77.479% 47.516% 48.665%
WTMF-G k = 5, ? = 3 96.613% 96.701% 76.029% 77.176% 47.197% 48.189%
WTMF-G k = 4, ? = 3 96.510% 96.610% 77.782% 77.782% 47.917% 48.997%
Table 1: ATOP Performance (latent dimension D = 100 for LDA/WTMF/WTMF-G)
0 1 2 3 4
96
96.2
96.4
96.6
96.8
ATO
P
?
 
 
dev
test
(a) ATOP
0 1 2 3 475
75.5
76
76.5
77
77.5
78
TOP
10
?
 
 
dev
test
(b) TOP10
0 1 2 3 445
46
47
48
49
RR
?
 
 
dev
test
(c) RR
Figure 3: Impact of ? (D = 100, k = 4)
score between the url referred news article and the
tweet is compared against the scores of these 1,000
news articles to calculate the metric scores. 1/10 of
the tweet/news pairs are used as development set,
based on which all the parameters are tuned. The
metrics ATOP, TOP10 and RR are used to evaluate
the performance of systems.
6.2 Results
Table 1 summarizes the performance of the base-
lines and WTMF-G at latent dimension D = 100.
All the parameters are chosen based on the de-
velopment set. For WTMF-G, we try different
values of k (the number of neighbors linked to a
tweet/news for a hashtag/NE/time constraint) and
? (the weight of link information). We choose to
model the links in four subgraphs: (a) hashtags
in tweet; (b) named entities in tweet; (c) time in
tweet; (d) time in news article. For LDA we tune
the hyperparameter ? (Dirichlet prior for topic dis-
tribution of a document) and ? (Dirichlet prior for
word distribution given a topic). It is worth noting
that ATOP measures the overall ranking in 1000
samples while TOP10/RR focus on whether the
aligned news article is in the first few returned re-
sults.
Same as reported in (Guo and Diab, 2012b),
LDA-? has the worst results due to directly using
prior to the tweet, however, that will give a bias to some
tweets, since the latter tweets have a larger candidate set than
the earlier ones
the inferred topic distribution of a text ?. The in-
ferred topic vector has only a few non-zero values,
hence a lot of information is missing. LDA-wvec
preserves more information by creating a dense la-
tent vector from the topic distribution of a word
P (z|w), and thus does much better in ATOP.
It is interesting to see that IR model has a
very low ATOP (90.795%) and an acceptable RR
(46.281%) score, in contrast to LDA-wvec with
a high ATOP (94.148%) and a low RR(27.904%)
score. This is caused by the nature of the two mod-
els. LDA-wvec is able to identify global coarse
grained topic information (such as politics vs. eco-
nomics), hence receiving a high ATOP by exclud-
ing the most irrelevant news articles, however it
does not distinguish fine grained difference such
as Hillary vs. Obama. IR model exerts the oppo-
site influence via word matching. It ranks a cor-
rect news article very high if overlapping words
exist (leading to a high RR), or the news article is
ranked very low if no overlapping words (hence a
low ATOP).
We can conclude WTMF is a very strong base-
line given that it achieves high scores with three
metrics. As a latent variable model, it is able to
capture global topics (+1.89% ATOP over LDA-
wvec); moreover, by explicitly modeling missing
words, the existence of a word is also encoded in
the latent vector (+2.31% TOP10 and ?0.011%
RR over IR model).
244
50 75 100 125 15095
95.5
96
96.5
97
ATO
P
D
 
 
WTMFWTMF?G
(a) ATOP
50 75 100 125 15070
72
74
76
78
80
TOP
10
D
 
 
WTMFWTMF?G
(b) TOP10
50 75 100 125 15040
42
44
46
48
50
RR
D
 
 
WTMFWTMF?G
(c) RR
Figure 4: Impact of latent dimension D (k = 4)
Conditions Links ATOP TOP10 RRdev test dev test dev test
hashtags tweets 375,371 +0.397% +0.379% +1.015% +1.021% +0.504% +0.641%
NE tweets 164,412 +0.141% +0.130% +0.598% +0.479% +0.278% +0.294%
time tweet 139,488 +0.126% +0.136% +0.512% +0.503% +0.241% +0.327%
time news 50,008 +0.036% +0.026% +0.156% +0.256% +1.890% +1.924%
full model (all 4 subgraphs) 573,999 +0.546% +0.518% +1.556% +1.371% +2.607% +2.727%
full model minus hashtags tweets 336,963 +0.288% +0.276% +1.129% +1.037% +2.488% +2.541%
full model minus NE tweets 536,333 +0.528% +0.503% +1.518% +1.393% +2.580% +2.680%
full model minus time tweet 466,207 +0.457% +0.426% +1.281% +1.145% +2.449% +2.554%
full model minus time news 523,991 +0.508% +0.490% +1.300% +1.190% +0.632% +0.785%
author tweet 21,318 +0.043% +0.042% +0.028% +0.057% ?0.003% ?0.017%
full model plus author tweet 593,483 +0.575% +0.545% +1.465% +1.336% +2.415% +2.547%
Table 2: Contribution of subgraphs when D = 100, k = 4, ? = 3 (gain over baseline WTMF)
With WTMF being a very challenging baseline,
WTMF-G can still significantly improve all 3 met-
rics. In the case k = 4, ? = 3 compared to WTMF,
WTMF-G receives +1.371% TOP10, +2.727%
RR, and +0.518% ATOP value (this is a signifi-
cant improvement of ATOP value considering that
it is averaged on 30,000 data points, at an already
high level of 96% reducing error rate by 13%). All
the improvement of WTMF-G over WTMF is sta-
tistically signicant at the 99% condence level with
a two-tailed paired t-test.
We also present results using different number
of links k in WTMF-G in table 1. We experi-
ment with k = {3, 4, 5}. k = 4 is found to
be the optimal value (although k = 5 has a bet-
ter ATOP). Figure 3 demonstrates the impact of
? = {0, 1, 2, 3, 4} on each metric when k = 4.
Note when ? = 0 no link is used, which is the
baseline WTMF. We can see using links is always
helpful. When ? = 4, we receive a higher ATOP
value but lower TOP10 and RR.
Figure 4 illustrates the impact of dimension
D = {50, 75, 100, 125, 150} on WTMF and
WTMF-G (k = 4) on the test set. The trends
hold in different D values with a consistent im-
provement. Generally a larger D leads to a better
performance. In all conditions WTMF-G outper-
forms WTMF.
6.3 Contribution of Subgraphs
We are interested in the contribution of each fea-
ture subgraph. Therefore we list the impact of
individual components in table 2. The impact of
each subgraph is evaluated in two conditions: (a)
the subgraph-only; (b) the full-model-minus the
subgraph. The full model is the combination of the
4 subgraphs (which is also the best model k = 4
in table 1). In the last two rows of table 2 we also
present the results of using authorship only and the
full model plus authorship. The 2nd column lists
the number of links in the subgraph. To highlight
the difference, we report the gain of each model
over the baseline model WTMF.
We have several interesting observations from
table 2. It is clear that the hashtag sub-
graph on tweets is the most useful subgraph:
with hashtag tweet it has the best ATOP and
TOP10 values among subgraph-only condition
(ATOP: +0.379% vs. 2nd best +0.136%, TOP10:
+1.021% vs. 2nd best +0.503%), while in the
full-model-minus condition, minus hashtag has
the lowest ATOP and TOP10. Observing that it
also contains the most links, we believe the cover-
age is another important reason for the great per-
formance.
It seems the named entity subgraph helps the
least. Looking into the extracted named entities
and hashtags, we find many popular named enti-
245
ties are covered by hashtags. That said, adding
named entity subgraph into final model has a pos-
itive contribution.
It is worth noting that the time news subgraph
has the most positive influence on RR. This is be-
cause temporal information is very salient in news
domain: usually there are several reports to de-
scribe an event within a short period, therefore the
news latent vector is strengthened by receiving se-
mantics from its neighbors.
At last, we analyze the influence of author-
ship of tweets. Adding authorship into the full
model greatly hurts the scores of TOP10 and RR,
whereas it is helpful to ATOP. This is understand-
able since by introducing author links between
tweets, to some degree we are averaging the la-
tent vectors of tweets written by the same per-
son. Therefore, for a tweet whose topic is vague
and hard to detect, it will get some prior knowl-
edge of topics through the author links (hence in-
crease ATOP), whereas this prior knowledge be-
comes noise for the tweets that are already handled
very well by the model (hence decrease TOP10
and RR).
6.4 Error Analysis
We look closely into ATOP results to obtain an in-
tuitive feel for what is captured and what is not.
For example, the ATOP score of WTMF for the
tweet-news pair below is 89.9%:
Tweet: ...stoked growing speculation that Pak-
istan?s powerful military was quietly supporting
moves... @declanwalsh
News: Pakistan Supreme Court Orders Arrest of
Prime Minister
By identifying ?Pakistan? and ?Supreme Court?
as hashtags/named entity, WTMF-G is able to
propagate the semantics from the following two
informative tweets to the original tweet, hence
achieving a higher ATOP score of 91.9%.
#Pakistan Supreme Court orders the arrest of the
PM on corruption charges.
A discouraging sign from a tumultuous political
system: Pakistan?s Supreme Court ordered the ar-
rest of PM Ashraf today.
Below is an example that shows the deficiency of
both WTMF and WTMF-G:
Tweet: Another reason to contemplate moving: an
early death
News: America flunks its health exam
In this case WTMF and WTMF-G achieve a
low ATOP of 69.8% and 75.1%, respectively. The
only evidence the latent variable models rely on
is lexical items (WTMF-G extract additional text-
to-text correlation by word matching). To pin-
point the url referred news articles, other advanced
NLP features should be exploited. In this case, we
believe sentiment information could be helpful ?
both tweet and the news article contain a negative
polarity.
7 Related Work
Short Text Semantics: The field of short text se-
mantics has progressed immensely in recent years.
Early work focus on word pair similarity in the
high dimensional space. The word pair similarity
is either knowledge based (Mihalcea et al, 2006;
Tsatsaronis et al, 2010) or corpus based (Li et
al., 2006; Islam and Inkpen, 2008), where co-
occurrence information cannot be efficiently ex-
ploited. Guo and Diab (2012b; 2012a; 2013) show
the superiority of the latent space approach in the
WTMF model achieving state-of-the-art perfor-
mance on two datasets. However, all of them only
reply on text-to-word information. In this paper,
we focus on modeling inter-text relations induced
by Twitter/news features. We extend the WTMF
model and adapt it into tweets modeling, achiev-
ing significantly better results.
Modeling Tweets in a Latent Space: Ramage
et al (2010) also use hashtags to improve the la-
tent representation of tweets in a LDA framework,
Labeled-LDA (Ramage et al, 2009), treating each
hashtag as a label. Similar to the experiments pre-
sented in this paper, the result of using Labeled-
LDA alone is worse than the IR model, due to the
sparseness in the induced LDA latent vector. Jin et
al. (2011) apply an LDA based model on cluster-
ing by incorporating url referred documents. The
semantics of long documents are transferred to the
topic distribution of tweets.
News recommendation: A news recommen-
dation system aims to recommend news articles
to a user based on the features (e.g., key words,
tags, category) in the documents that the user likes
(hence these documents form a training set) (Clay-
pool et al, 1999; Corso et al, 2005; Lee and Park,
2007). Our paper resembles it in searching for a
related news article. However, we target on rec-
ommending news article only based on a tweet,
which is a much smaller context than the set of
favorite documents chosen by a user .
246
Research on Tweets: In (Duan et al, 2010), url
availability is an important feature for tweets rank-
ing. However, the number of tweets with an ex-
plicit url is very limited. Huang et al (2012) pro-
pose a graph-based framework to propagate tweet
ranking scores, where relevant web documents is
found to be helpful to discover informative tweets.
Both work can take advantage of our work to ei-
ther extract potential url features or retrieve topi-
cally similar web documents.
(Sankaranarayanan et al, 2009) aims at captur-
ing tweets that correspond to late breaking news.
However, they cluster tweets and simply choose
a url referred news in those tweets as the related
news for the whole cluster (the urls are visible
to the systems). (Abel et al, 2011) is most re-
lated work to our paper, however their focus is the
user profiling task, therefore they do not provide
a paired tweet/news data set and have to conduct
manual evaluation.
8 Conclusion
We propose a Linking-Tweets-to-News task,
which potentially benefits many NLP applications
where off-the-shelf NLP tools can be applied to
the most relevant news. We also collect a gold
standard dataset by crawling tweets each with a url
referring to a news article. We formalize the link-
ing task as a short text modeling problem, and ex-
tract Twitter/news specific features to extract text-
to-text relations, which are incorporated into a la-
tent variable model. We achieve significant im-
provement over baselines.
Acknowledgements
This work was supported by the U.S. Army Re-
search Laboratory under Cooperative Agreement
No. W911NF- 09-2-0053 (NS-CTA), the U.S.
NSF CAREER Award under Grant IIS-0953149,
the U.S. NSF EAGER Award under Grant No. IIS-
1144111, the U.S. DARPA FA8750-13-2-0041 -
Deep Exploration and Filtering of Text (DEFT)
Program and CUNY Junior Faculty Award. The
views and conclusions contained in this document
are those of the authors and should not be inter-
preted as representing the official policies, either
expressed or implied, of the U.S. Government.
The U.S. Government is authorized to reproduce
and distribute reprints for Government purposes
notwithstanding any copyright notation here on.
References
Fabian Abel, Qi Gao, Geert-Jan Houben, and Ke Tao.
2011. Semantic enrichment of twitter posts for user
profile construction on the social web. In Proceed-
ings of the 49th Annual Meeting of the Association
for Computational Linguistics.
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. Semeval-2012 task 6: A
pilot on semantic textual similarity. In First Joint
Conference on Lexical and Computational Seman-
tics (*SEM).
Edward Benson, Aria Haghighi, and Regina Barzilay.
2011. Event discovery in social media feeds. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. Journal of Ma-
chine Learning Research, 3.
Samuel Brody and Nicholas Diakopoulos. 2011.
Cooooooooooooooollllllllllllll!!!!!!!!!!!!!! using
word lengthening to detect sentiment in microblogs.
In Proceedings of the 2011 Conference on Empirical
Methods in Natural Language Processing.
Mark Claypool, Anuja Gokhale, Tim Miranda, Pavel
Murnikov, Dmitry Netes, and Matthew Sartin. 1999.
Combining content-based and collaborative filters in
an online newspaper. In In Proceedings of the ACM
SIGIR Workshop on Recommender Systems.
Michael Conover, Jacob Ratkiewicz, Matthew Fran-
cisco, Bruno Gonc?alves, Filippo Menczer, and
Alessandro Flammini. 2011. Political polarization
on twitter. In ICWSM.
Gianna M. Del Corso, Antonio Gulli, and Francesco
Romani. 2005. Ranking a stream of news. In
WWW, pages 97?106.
Yajuan Duan, Long Jiang, Tao Qin, Ming Zhou, and
Heung-Yeung Shum. 2010. An empirical study on
learning to rank of tweets. In COLING.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
Weiwei Guo and Mona Diab. 2012a. Learning the la-
tent semantics of a concept by its definition. In Pro-
ceedings of the 50th Annual Meeting of the Associa-
tion for Computational Linguistics.
Weiwei Guo and Mona Diab. 2012b. Modeling sen-
tences in the latent space. In Proceedings of the
50th Annual Meeting of the Association for Compu-
tational Linguistics.
Weiwei Guo and Mona Diab. 2012c. Weiwei: A sim-
ple unsupervised latent semantics based approach
for sentence similarity. In First Joint Conference on
Lexical and Computational Semantics (*SEM).
247
Weiwei Guo and Mona Diab. 2013. Improving lexical
semantics for sentential semantics: Modeling selec-
tional preference and similar words in a latent vari-
able model. In The 2013 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies.
Hongzhao Huang, Arkaitz Zubiaga, Heng Ji, Hongbo
Deng, Dong Wang, Hieu Le, Tarek Abdelzather, Ji-
awei Han, Alice Leung, John Hancock, and Clare
Voss. 2012. Tweet ranking based on heterogeneous
networks. In Proceedings of the 24th International
Conference on Computational Linguistics.
Aminul Islam and Diana Inkpen. 2008. Semantic text
similarity using corpus-based word similarity and
string similarity. ACM Transactions on Knowledge
Discovery from Data, 2.
Heng Ji and Ralph Grishman. 2008. Refining event ex-
traction through cross-document inference. In Pro-
ceedings of ACL-08: HLT.
Long Jiang, Mo Yu, Ming Zhou, Xiaohua Liu, and
Tiejun Zhao. 2011. Target-dependent twitter sen-
timent classification. In Proceedings of the 49th An-
nual Meeting of Association for Computational Lin-
guistics.
Ou Jin, Nathan N. Liu, Kai Zhao, Yong Yu, and Qiang
Yang. 2011. Transferring topical knowledge from
auxiliary long texts for short text clustering. In Pro-
ceedings of the 20th ACM international conference
on Information and knowledge management.
Thomas K Landauer, Peter W. Foltz, and Darrell La-
ham. 1998. An introduction to latent semantic anal-
ysis. Discourse Processes, 25.
H. J. Lee and Sung Joo Park. 2007. Moners: A
news recommender for the mobile web. Expert Syst.
Appl., 32(1):143?150.
Yuhua Li, David McLean, Zuhair A. Bandar, James D.
O?Shea, and Keeley Crockett. 2006. Sentence simi-
larity based on semantic nets and corpus statistics.
IEEE Transaction on Knowledge and Data Engi-
neering, 18.
Hao Li, Yu Chen, Heng Ji, Smaranda Muresan, and
Dequan Zheng. 2012. Combining social cognitive
theories with linguistic features for multi-genre sen-
timent analysis. In In Proceedings of the 26th Pa-
cific Asia Conference on Language, Information and
Computation.
Xiaohua Liu, Shaodian Zhang, Furu Wei, and Ming
Zhou. 2011. Recognizing named entities in tweets.
In The Semanic Web: Research and Applications.
Andrew L. Maas, Raymond E. Daly, Peter T. Pham,
Dan Huang, Andrew Y. Ng, and Christopher Potts.
2011. Learning word vectors for sentiment analysis.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies.
Rada Mihalcea, Courtney Corley, and Carlo Strappa-
rava. 2006. Corpus-based and knowledge-based
measures of text semantic similarity. In Proceed-
ings of the 21st National Conference on Articial In-
telligence.
Bo Pang and Lillian Lee. 2004. A sentimental educa-
tion: Sentiment analysis using subjectivity summa-
rization based on minimum cuts. In Proceedings of
the 42nd Meeting of the Association for Computa-
tional Linguistics.
Bo Pang and Lillian Lee. 2005. Seeing stars: Exploit-
ing class relationships for sentiment categorization
with respect to rating scales. In Proceedings of the
43rd Annual Meeting of the Association for Compu-
tational Linguistics.
Daniel Ramage, David Hall, Ramesh Nallapati, and
Christopher D. Manning. 2009. Labeled lda: A su-
pervised topic model for credit attribution in multi-
labeled corpora. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing.
Daniel Ramage, Susan Dumais, and Dan Liebling.
2010. Characterizing microblogs with topic mod-
els. In Proceedings of the Fourth International AAAI
Conference on Weblogs and Social Media.
Jagan Sankaranarayanan, Hanan Samet, Benjamin E.
Teitler, Michael D. Lieberman, and Jon Sperling.
2009. Twitterstand: news in tweets. In Proceedings
of the 17th ACM SIGSPATIAL International Confer-
ence on Advances in Geographic Information Sys-
tems.
Richard Socher, Eric H. Huang, Jeffrey Pennington,
Andrew Y. Ng, and Christopher D. Manning. 2011.
Dynamic pooling and unfolding recursive autoen-
coders for paraphrase detection. In Proceedings
of Advances in Neural Information Processing Sys-
tems.
Michael Speriosui, Nikita Sudan, Sid Upadhyay, and
Jason Baldridge. 2011. Twitter polarity classifica-
tion with label propagation over lexical links and the
follower graph. In Proceedings of the 2011 Con-
ference on Empirical Methods in Natural Language
Processing.
Nathan Srebro and Tommi Jaakkola. 2003. Weighted
low-rank approximations. In Proceedings of the
Twentieth International Conference on Machine
Learning.
Harald Steck. 2010. Training and testing of rec-
ommender systems on data missing not at random.
In Proceedings of the 16th ACM SIGKDD Inter-
national Conference on Knowledge Discovery and
Data Mining.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In HLT-NAACL.
248
George Tsatsaronis, Iraklis Varlamis, and Michalis
Vazirgiannis. 2010. Text relatedness based on a
word thesaurus. Journal of Articial Intelligence Re-
search, 37.
Andranik Tumasjan, Timm Oliver Sprenger, Philipp G.
Sandner, and Isabell M. Welpe. 2010. Predicting
elections with twitter: What 140 characters reveal
about political sentiment. In ICWSM.
Xuerui Wang and Andrew McCallum. 2006. Top-
ics over time: a non-markov continuous-time model
of topical trends. In In Proceedings of the 12th
ACM SIGKDD international conference on Knowl-
edge discovery and data mining.
Rui Yan, Mirella Lapata, and Xiaoming Li. 2012.
Tweet recommendation with graph co-ranking. In
Proceedings of the 24th International Conference on
Computational Linguistics.
Lei Yang, Tao Sun, Ming Zhang, and Qiaozhu Mei.
2012. We know what @you #tag: does the dual role
affect hashtag adoption? In Proceedings of the 21st
international conference on World Wide Web.
249
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 143?147,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Automated Pyramid Scoring of Summaries using Distributional Semantics
Rebecca J. Passonneau? and Emily Chen? and Weiwei Guo? and Dolores Perin?
?Center for Computational Learning Systems, Columbia University
?Department of Computer Science, Columbia University
?Teachers College, Columbia University
(becky@ccls.|ec2805@|weiwei@cs.)columbia.edu, perin@tc.edu
Abstract
The pyramid method for content evaluation of auto-
mated summarizers produces scores that are shown
to correlate well with manual scores used in edu-
cational assessment of students? summaries. This
motivates the development of a more accurate auto-
mated method to compute pyramid scores. Of three
methods tested here, the one that performs best re-
lies on latent semantics.
1 Introduction
The pyramid method is an annotation and scor-
ing procedure to assess semantic content of sum-
maries in which the content units emerge from
the annotation. Each content unit is weighted
by its frequency in human reference summaries.
It has been shown to produce reliable rank-
ings of automated summarization systems, based
on performance across multiple summarization
tasks (Nenkova and Passonneau, 2004; Passon-
neau, 2010). It has also been applied to assessment
of oral narrative skills of children (Passonneau et
al., 2007). Here we show its potential for assess-
ment of the reading comprehension of community
college students. We then present a method to au-
tomate pyramid scores based on latent semantics.
The pyramid method depends on two phases of
manual annotation, one to identify weighted con-
tent units in model summaries written by profi-
cient humans, and one to score target summaries
against the models. The first annotation phase
yields Summary Content Units (SCUs), sets of
text fragments that express the same basic content.
Each SCU is weighted by the number of model
summaries it occurs in.
Figure 1 illustrates a Summary Content Unit
taken from pyramid annotation of five model sum-
maries of an elementary physics text. The ele-
ments of an SCU are its index; a label, created by
the annotator; contributors (Ctr.), or text fragments
from the model summaries; and the weight (Wt.),
corresponding to the number of contributors from
distinct model summaries. Four of the five model
Index 105
Label Matter is what makes up all objects or substances
Ctr. 1 Matter is what makes up all objects or substances
Ctr. 2 matter as the stuff that all objects and substances
in the universe are made of
Ctr. 3 Matter is identified as being present everywhere
and in all substances
Ctr. 4 Matter is all the objects and substances around us
Wt. 4
Figure 1: A Summary Content Unit (SCU)
summaries contribute to SCU 105 shown here.
The four contributors have lexical items in com-
mon (matter, objects, substances), and many dif-
ferences (makes up, being present). SCU weights,
which range from 1 to the number of model sum-
maries M , induce a partition on the set of SCUs
in all summaries into subsets Tw, w ? 1, . . . ,M .
The resulting partition is referred to as a pyramid
because, starting with the subset for SCUs with
weight 1, each next subset has fewer SCUs.
To score new target summaries, they are first
annotated to identify which SCUs they express.
Application of the pyramid method to assessment
of student reading comprehension is impractical
without an automated method to annotate target
summaries. Previous work on automated pyramid
scores of automated summarizers performs well
at ranking systems on many document sets, but
is not precise enough to score human summaries
of a single text. We test three automated pyramid
scoring procedures, and find that one based on dis-
tributional semantics correlates best with manual
pyramid scores, and has higher precision and re-
call for content units in students? summaries than
methods that depend on string matching.
2 Related Work
The most prominent NLP technique applied to
reading comprehension is LSA (Landauer and Du-
mais, 1997), an early approach to latent semantic
analysis claimed to correlate with reading compre-
hension (Foltz et al, 2000). More recently, LSA
143
has been incorporated with a suite of NLP metrics
to assess students? strategies for reading compre-
hension using think-aloud protocols (Boonthum-
Denecke et al, 2011). The resulting tool, and sim-
ilar assesment tools such as Coh-Metrix, assess
aspects of readability of texts, such as coherence,
but do not assess students? comprehension through
their writing (Graesser et al, 2004; Graesser et al,
2011). E-rater is an automated essay scorer for
standardized tests such as GMAT that also relies
on a suite of NLP techniques (Burstein et al, 1998;
Burstein, 2003). The pyramid method (Nenkova
and Passonneau, 2004), was inspired in part by
work in reading comprehension that scores con-
tent using human annotation (Beck et al, 1991).
An alternate line of research attempts to repli-
cate human reading comprehension. An auto-
mated tool to read and answer questions relies on
abductive reasoning over logical forms extracted
from text (Wellner et al, 2006). One of the perfor-
mance issues is resolving meanings of words: re-
moval of WordNet features degraded performance.
The most widely used automated content evalu-
ation is ROUGE (Lin, 2004; Lin and Hovy, 2003).
It relies on model summaries, and depends on
ngram overlap measures of different types. Be-
cause of its dependence on strings, it performs bet-
ter with larger sets of model summaries. In con-
trast to ROUGE, pyramid scoring is robust with as
few as four or five model summaries (Nenkova and
Passonneau, 2004). A fully automated approach
to evaluation for ranking systems that requires no
model summaries incorporates latent semantic dis-
tributional similarities across words (Louis and
Nenkova, 2009). The authors note, however, it
does not perform well on individual summaries.
3 Criteria for Automated Scoring
Pyramid scores of students? summaries correlate
well with a manual main ideas score developed
for an intervention study with community college
freshmen who attended remedial classes (Perin et
al., In press). Twenty student summaries by stu-
dents who attended the same college and took the
same remedial course were selected from a larger
set of 322 that summarized an elementary physics
text. All were native speakers of English, and
scored within 5 points of the mean reading score
for the larger sample. For the intervention study,
student summaries had been assigned a score to
represent how many main ideas from the source
text were covered (Perin et al, In press). Inter-
rater reliability of the main ideas score, as given
by the Pearson correlation coefficient, was 0.92.
One of the co-authors created a model pyra-
mid from summaries written by proficient Masters
of Education students, annotated 20 target sum-
maries against this pyramid, and scored the re-
sult. The raw score of a target summary is the
sum of its SCU weights. Pyramid scores have
been normalized by the number of SCUs in the
summary (analogous to precision), or the average
number of SCUs in model summaries (analogous
to recall). We normalized raw scores as the aver-
age of the two previous normalizations (analogous
to F-measure). The resulting scores have a high
Pearson?s correlation of 0.85 with the main idea
score (Perin et al, In press) that was manually as-
signed to the students? summaries.
To be pedagogically useful, an automated
method to assign pyramid scores to students? sum-
maries should meet the following criteria: 1) reli-
ably rank students? summaries of a source text, 2)
assign correct pyramid scores, and 3) identify the
correct SCUs. A method could do well on crite-
rion 1 but not 2, through scores that have uniform
differences from corresponding manual pyramid
scores. Also, since each weight partition will have
more than one SCU, it is possible to produce the
correct numeric score by matching incorrect SCUs
that have the correct weights. Our method meets
the first two criteria, and has superior performance
on the third to other methods.
4 Approach: Dynamic Programming
Previous work observed that assignment of SCUs
to a target summary can be cast as a dynamic
programming problem (Harnly et al, 2005). The
method presented there relied on unigram overlap
to score the closeness of the match of each eli-
gible substring in a summary against each SCU
in the pyramid. It returned the set of matches
that yielded the highest score for the summary.
It produced good rankings across summarization
tasks, but assigned scores much lower than those
assigned by humans. Here we extend the DP ap-
proach in two ways. We test two new semantic
text similarities, a string comparison method and a
distributional semantic method, and we present a
general mechanism to set a threshold value for an
arbitrary computation of text similarity.
Unigram overlap ignores word order, and can-
not consider the latent semantic content of a
string, only the observed unigram tokens. To
144
take order into account, we use Ratcliff/Obershelp
(R/O), which measures overlap of common sub-
sequences (Ratcliff and Metzener, 1988). To take
the underlying semantics into account, we use co-
sine similarity of 100-dimensional latent vectors
of the candidate substrings and of the textual com-
ponents of the SCU (label and contributors). Be-
cause the algorithm optimizes for the total sum of
all SCUs, when there is no threshold similarity to
count as a match, it favors matching shorter sub-
strings to SCUs with higher weights. Therefore,
we add a threshold to the algorithm, below which
matches are not considered. Because each sim-
ilarity metric has different properties and distri-
butions, a single absolute value threshhold is not
comparable across metrics. We present a method
to set comparable thresholds across metrics.
4.1 Latent Vector Representations
To represent the semantics of SCUs and candidate
substrings of target summaries, we applied the la-
tent vector model of Guo and Diab (2012).1 Guo
and Diab find that it is very hard to learn a 100-
dimension latent vector based only on the lim-
ited observed words in a short text. Hence they
include unobserved words that provide thousands
more features for a short text. This produces more
accurate results for short texts, which makes the
method suitable for our problem. Weighted ma-
trix factorization (WMF) assigns a small weight
for missing words so that latent semantics depends
largely on observed words.
A 100-dimension latent vector representation
was learned for every span of contiguous words
within sentence bounds in a target summary, for
the 20 summaries. The training data was selected
to be domain independent, so that our model could
be used for summaries across domains. Thus we
prepared a corpus that is balanced across topics
and genres. It is drawn from from WordNet sense
definitions, Wiktionary sense definitions, and the
Brown corpus. It yields a co-occurrence matrix
M of unique words by sentences of size 46,619
? 393,666. Mij holds the TF-IDF value of word
wi in sentence sj . Similarly, the contributors
to and the label for an SCU were given a 100-
dimensional latent vector representation. These
representations were then used to compare candi-
dates from a summary to SCUs in the pyramid.
1http://www.cs.columbia.edu/?weiwei/
code.html#wtmf.
4.2 Three Comparison Methods
An SCU consists of at least two text strings: the
SCU label and one contributor. As in Harnly et
al. (2005), we use three similarity comparisons
scusim(X,SCU), where X is the target summary
string. When the comparison parameter is set to
min (max, or mean), the similarity of X to
each SCU contributor and the label is computed
in turn, and the minimum (max, or mean) is re-
turned.
4.3 Similarity Thresholds
We define a threshold parameter for a target SCU
to match a pyramid SCU based on the distributions
of scores each similarity method gives to the target
SCUs identified by the human annotator. Annota-
tion of the target summaries yielded 204 SCUs.
The similarity score being a continuous random
variable, the empirical sample of 204 scores is
very sparse. Hence, we use a Gaussian kernel den-
sity estimator to provide a non-parametric estima-
tion of the probability densities of scores assigned
by each of the similarity methods to the manually
identified SCUs. We then select five threshold val-
ues corresponding to those for which the inverse
cumulative density function (icdf) is equal to 0.05,
0.10, 0.15, 0.20 and 0.25. Each threshold rep-
resents the probability that a manually identified
SCU will be missed.
5 Experiment
The three similarity computations, three methods
to compare against SCUs, and five icdf thresh-
olds yield 45 variants, as shown in Figure 2. Each
variant was evaluated by comparing the unnormal-
ized automated variant, e.g., Lvc, max, 0.64 (its
0.15 icdf) to the human gold scores, using each of
the evaluation metrics described in the next sub-
section. To compute confidence intervals for the
evaluation metrics for each variant, we use boot-
strapping with 1000 samples (Efron and Tibshi-
rani, 1986).
To assess the 45 variants, we compared their
scores to the manual scores. We also compared
the sets of SCUs retrieved. By our criterion 1), an
automated score that correlates well with manual
scores for summaries of a given text could be used
(3 Similarities) ? (3 Comparisons) ? (5 Thresholds) = 45
(Uni, R/O, Lvc) ? (min, mean, max) ? (0.05, . . . , 0.25)
Figure 2: Notation used for the 45 variants
145
Variant (with icdf) P (95% conf.), rank S (95% conf.), rank K (95% conf.), rank ? Diff. T test
LVc, max, 0.64 (0.15) 0.93 (0.94, 0.92), 1 0.94 (0.93, 0.97), 1 0.88 (0.85, 0.91), 1 49.9 15.65 0.0011
R/O, mean, 0.23 (0.15) 0.92 (0.91, 0.93), 3 0.93 (0.91,0.95), 2 0.83 (0.80, 0.86), 3 49.8 15.60 0.0012
R/O, mean, 0.26 (0.20) 0.92 (0.90, 0.93), 4 0.92 (0.90, 0.94) 4 0.80 (0.78, 0.83), 5 47.7 13.45 0.0046
LVc, max, 0.59 (0.10) 0.91 (0.89, 0.92), 8 0.93 (0.91, 0.95) 3 0.83 (0.80, 0.87), 2 52.7 18.50 0.0002
LVc, min, 0.40 (0.20) 0.92 (0.90,0.93), 2 0.87 (0.84, 0.91) 11 0.74 (0.69, 0.79), 11 37.5 3.30 0.4572
Table 1: Five variants from the top twelve of all correlations, with confidence interval and rank (P=Pearson?s, S=Spearman,
K=Kendall?s tau), mean summed SCU weight, difference of mean from mean gold score, T test p-value.
to indicate how well students rank against other
students. We report several types of correlation
tests. Pearsons tests the strength of a linear cor-
relation between the two sets of scores; it will be
high if the same order is produced, with the same
distance between pairs of scores. The Spearman
rank correlation is said to be preferable for ordi-
nal comparisons, meaning where the unit interval
is less relevant. Kendall?s tau, an alternative rank
correlation, is less sensitive to outliers and more
intuitive. It is the proportion of concordant pairs
(pairs in the same order) less the proportion of dis-
cordant pairs. Since correlations can be high when
differences are uniform, we use Student?s T to test
whether differences score means statistically sig-
nificant. Criterion 2) is met if the correlations are
high and the means are not significantly different.
6 Results
The correlation tests indicate that several variants
achieve sufficiently high correlations to rank stu-
dents? summaries (criterion 2). On all correla-
tion tests, the highest ranking automated method
is LVc, max, 0.64; this similarity threshold corre-
sponds to the 0.15 icdf. As shown in Table 1, the
Pearson correlation is 0.93. Note, however, that it
is not significantly higher than many of its com-
petitors. LVc, min, 0.40 did not rank as highly for
Speaman and Kendall?s tau correlations, but the
Student?s T result in column 3 of Table 1 shows
that this is the only variant in the table that yields
absolute scores that are not significantly different
from the human annotated scores. Thus this vari-
ant best balances criteria 1 and 2.
The differences in the unnormalized score com-
puted by the automated systems from the score as-
signed by human annotation are consistently posi-
tive. Inspection of the SCUs retrieved by each au-
tomated variant reveals that the automated systems
lean toward the tendency to identify false posi-
tives. This may result from the DP implementation
decision to maximize the score. To get a measure
of the degree of overlap between the SCUs that
were selected automatically versus manually (cri-
terion 4), we computed recall and precision for the
various methods. Table 2 shows the mean recall
and precision (with standard deviations) across all
five thresholds for each combination of similarity
method and method of comparison to the SCU.
The low standard deviations show that the recall
and precision are relatively similar across thresh-
olds for each variant. The LVc methods outper-
form R/O and unigram overlap methods, particu-
larly for the precision of SCUs retrieved, indicat-
ing the use of distributional semantics is a supe-
rior approach for pyramid summary scoring than
methods based on string matching.
The unigram overlap and R/O methods show the
least variation across comparison methods (min,
mean, max). LVc methods outperform them, on
precision (Table 2). Meeting all three criteria is
difficult, and the LVc method is clearly superior.
7 Conclusion
We extended a dynamic programming frame-
work (Harnly et al, 2005) to automate pyramid
scores more accurately. Improvements resulted
from principled thresholds for similarity, and from
a vector representation (LVc) to capture the latent
semantics of short spans of text (Guo and Diab,
2012). The LVc methods perform best at all three
criteria for a pedagogically useful automatic met-
ric. Future work will address how to improve pre-
cision and recall of the gold SCUs.
Acknowledgements
We thank the reviewers for very valuable insights.
Variant ? Recall (std) ? Precision (std) F score
Uni, min 0.69 (0.08) 0.35 (0.02) 0.52
Uni, max 0.70 (0.03) 0.35 (0.04) 0.53
Uni, mean 0.69 (0.02) 0.39 (0.04) 0.54
R/O, min 0.69 (0.08) 0.34 (0.01) 0.51
R/O, max 0.72 (0.03) 0.33 (0.04) 0.52
R/O, mean 0.71 (0.06) 0.38 (0.02) 0.54
LVc, min 0.61 (0.03) 0.38 (0.04) 0.49
LVc, max 0.74 (0.06) 0.48 (0.01) 0.61
LVc, mean 0.75 (0.06) 0.50 (0.02) 0.62
Table 2: Recall and precision for SCU selection
146
References
Isabel L. Beck, Margaret G. McKeown, Gale M. Sina-
tra, and Jane A. Loxterman. 1991. Revising social
studies text from a text-processing perspective: Ev-
idence of improved comprehensibility. Reading Re-
search Quarterly, pages 251?276.
Chutima Boonthum-Denecke, Philip M. McCarthy,
Travis A. Lamkin, G. Tanner Jackson, Joseph P.
Maglianoc, and Danielle S. McNamara. 2011. Au-
tomatic natural language processing and the de-
tection of reading skills and reading comprehen-
sion. In Proceedings of the Twenty-Fourth Interna-
tional Florida Artificial Intelligence Research Soci-
ety Conference, pages 234?239.
Jill Burstein, Karen Kukich, Susanne Wolff, Chi
Lu, Martin Chodorow, Lisa Braden-Harder, and
Mary Dee Harris. 1998. Automated scoring us-
ing a hybrid feature identification technique. In Pro-
ceedings of the 36th Annual Meeting of the Associ-
ation for Computational Linguistics and 17th Inter-
national Conference on Computational Linguistics,
pages 206?210, Montreal, Quebec, Canada, August.
Association for Computational Linguistics.
Jill Burstein. 2003. The e-rater R?scoring engine: Au-
tomated essay scoring with natural language pro-
cessing. In M. D. Shermis and J. Burstein, editors,
Automated Essay Scoring: A Cross-disciplinary
Perspective. Lawrence Erlbaum Associates, Inc.,
Hillsdale, NJ.
Bradley Efron and Robert Tibshirani. 1986. Boot-
strap methods for standard errors, confidence inter-
vals, and other measures of statistical accuracy. Sta-
tistical Science, 1:54?77.
Peter W. Foltz, Sara Gilliam, and Scott Kendall. 2000.
Supporting content-based feedback in on-line writ-
ing evaluation with LSA. Interactive Learning En-
vironments, 8:111?127.
Arthur C. Graesser, Danielle S. McNamara, Max M.
Louwerse, and Zhiqiang Cai. 2004. Coh-Metrix:
Analysis of text on cohesion and language. Behav-
ior Research Methods, Instruments, and Computers,
36:193202.
Arthur C. Graesser, Danielle S. McNamara, and
Jonna M. Kulikowich. 2011. Coh-Metrix: Provid-
ing multilevel analyses of text characteristics. Edu-
cational Researcher, 40:223?234.
Weiwei Guo and Mona Diab. 2012. Modeling sen-
tences in the latent space. In Proceedings of the
50th Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 864?872.
Aaron Harnly, Ani Nenkova, Rebecca J. Passonneau,
and Owen Rambow. 2005. Automation of summary
evaluation by the Pyramid Method. In Recent Ad-
vances in Natural Language Processing (RANLP),
pages 226?232.
Thomas K Landauer and Susan T. Dumais. 1997. A
solution to Plato?s problem: The latent semantic
analysis theory of acquisition, induction, and rep-
resentation of knowledge. Psychological review,
pages 211?240.
Chin-Yew Lin and Eduard Hovy. 2003. Auto-
matic evaluation of summaries using n-gram co-
occurrence statistics. In Proceedings of the Hu-
man Language Technology Conference of the North
American Chapter of the Association for Computa-
tional Linguistics (HLT-NAACL), pages 71?78.
Chin-Yew Lin. 2004. ROUGE: A package for au-
tomatic evaluation of summaries. In Proceedings
of the Human Language Technology Conference
of the North American Chapter of the Association
for Computational Linguistics (HLT-NAACL), pages
463?470.
Annie Louis and Ani Nenkova. 2009. Evaluating con-
tent selection in summarization without human mod-
els. In Proceedings of the 2009 Conference on Em-
pirical Methods in Natural Language Processing,
pages 306?314, Singapore, August. Association for
Computational Linguistics.
Ani Nenkova and Rebecca J. Passonneau. 2004.
Evaluating content selection in summarization: The
Pyramid Method. In Proceedings of the Hu-
man Language Technology Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 145?152.
Rebecca J. Passonneau, Adam Goodkind, and Elena
Levy. 2007. Annotation of children?s oral narra-
tions: Modeling emergent narrative skills for com-
putational applications. In Proceedings of the Twen-
tieth Annual Meeting of the Florida Artificial Intel-
ligence Research Society (FLAIRS-20), pages 253?
258. AAAI Press.
Rebecca Passonneau. 2010. Formal and functional as-
sessment of the Pyramid Method for summary con-
tent evaluation. Natural Language Engineering, 16.
D. Perin, R. H. Bork, S. T. Peverly, and L. H. Mason.
In press. A contextualized curricular supplement for
developmental reading and writing. Journal of Col-
lege Reading and Learning.
J. W. Ratcliff and D. Metzener. 1988. Pattern match-
ing: the Gestalt approach.
Ben Wellner, Lisa Ferro, Warren R. Greiff, and Lynette
Hirschman. 2006. Reading comprehension tests for
computer-based understanding evaluation. Natural
Language Engineering, 12(4):305?334.
147
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 129?133,
Uppsala, Sweden, 15-16 July 2010.
c?2010 Association for Computational Linguistics
COLEUR and COLSLM: A WSD approach to Multilingual Lexical
Substitution, Tasks 2 and 3 SemEval 2010
Weiwei Guo and Mona Diab
Center for Computational Learning Systems
Columbia University
{weiwei,mdiab}@ccls.columbia.edu
Abstract
In this paper, we present a word sense
disambiguation (WSD) based system for
multilingual lexical substitution. Our
method depends on having a WSD system
for English and an automatic word align-
ment method. Crucially the approach re-
lies on having parallel corpora. For Task
2 (Sinha et al, 2009) we apply a super-
vised WSD system to derive the English
word senses. For Task 3 (Lefever & Hoste,
2009), we apply an unsupervised approach
to the training and test data. Both of our
systems that participated in Task 2 achieve
a decent ranking among the participating
systems. For Task 3 we achieve the highest
ranking on several of the language pairs:
French, German and Italian.
1 Introduction
In this paper, we present our system that was ap-
plied to the cross lingual substitution for two tasks
in SEMEVAL 2010, Tasks 2 and 3. We adopt
the same approach for both tasks with some dif-
ferences in the basic set-up. Our basic approach
relies on applying a word sense disambiguation
(WSD) system to the English data that comes from
a parallel corpus for English and a language of
relevance to the task, language 2 (l2). Then we
automatically induce the English word sense cor-
respondences to l2. Accordingly, for a given test
target word, we return its equivalent l2 words as-
suming that we are able to disambiguate the target
word in context.
2 Our Detailed Approach
We approach the problem of multilingual lexical
substitution from a WSD perspective. We adopt
the hypothesis that the different word senses of
ambiguous words in one language probably trans-
late to different lexical items in another language.
Hence, our approach relies on two crucial compo-
nents: a WSD module for the source language (our
target test words, in our case these are the English
target test words) and an automatic word align-
ment module to discover the target word sense cor-
respondences with the foreign words in a second
language. Our approach to both tasks is unsuper-
vised since we don?t have real training data anno-
tated with the target words and their corresponding
translations into l2 at the onset of the problem.
Accordingly, at training time, we rely on auto-
matically tagging large amounts of English data
(target word instances) with their relevant senses
and finding their l2 correspondences based on au-
tomatically induced word alignments. Each of
these English sense and l2 correspondence pairs
has an associated translation probability value de-
pending on frequency of co-occurrence. This in-
formation is aggregated in a look-up table over
the entire training set. An entry in the table
would have a target word sense type paired with all
the observed translation correspondences l2 word
types. Each of the l2 word types has a probabil-
ity of translation that is calculated as a normal-
ized weighted average of all the instances of this
l2 word type with the English sense aggregated
across the whole parallel corpus. This process re-
sults in an English word sense translation table
(WSTT). The word senses are derived from Word-
Net (Fellbaum, 1998). We expand the English
word sense entry correspondences by adding the
translations of the members of target word sense
synonym set as listed in WordNet.
For alignment, we specifically use the GIZA++
software for inducing word alignments across the
parallel corpora (Och & Ney, 2003). We apply
GIZA++ to the parallel corpus in both directions
English to l2 and l2 to English then take only the
intersection of the two alignment sets, hence fo-
129
cusing more on precision of alignment rather than
recall.
For each language in Task 3 and Task 2, we
use TreeTagger
1
to do the preprocessing for all
languages. The preprocessing includes segmenta-
tion, POS tagging and lemmatization. Since Tree-
Tagger is independent of languages, our system
does not rely on anything that is language spe-
cific; our system can be easily applied to other
languages. We run GIZA++ on the parallel cor-
pus, and obtain the intersection of the alignments
in both directions. Meanwhile, every time a target
English word appears in a sentence, we apply our
WSD system on it, using the sentence as context.
From this information, we build a WSST from
the English sense(s) to their corresponding foreign
words. Moreover, we use WordNet as a means of
augmenting the translation correspondences. We
expand the word sense to its synset from WordNet
adding the l2 words that corresponded to all the
member senses in the synset yielding more trans-
lation variability.
At test time, given a test data target word, we
apply the same WSD system that is applied to the
training corpus to create the WSTT. Once the tar-
get word instance is disambiguated in context, we
look up the corresponding entry in the WSTT and
return the ranked list of l2 correspondences. We
present results for best and for oot which vary only
in the cut off threshold. In the BEST condition we
return the highest ranked candidate, in the oot con-
dition we return the top 10 (where available).
2
Given the above mentioned pipeline, Tasks 2
and 3 are very similar. Their main difference lies
in the underlying WSD system applied.
3 Task 2
3.1 System Details
We use a relatively simple monolingual supervised
WSD system to create the sense tags on the En-
glish data. We use the SemCor word sense anno-
tated corpus. SemCor is a subset of the Brown
Corpus. For each of our target English words
found disambiguated in the SemCor corpus, we
create a sense profile for each of its senses. A
sense profile is a vector of all the content words
that occur in the context of this sense in the Sem-
Cor corpus. The dimensions of the vector are word
1
http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/
2
Some of the target word senses had less than 10 l2 word
correspondences.
Corpus best oot
P R P R
T2-COLSLM 27.59 25.99 46.61 43.91
T2-COLEUR 19.47 18.15 44.77 41.72
Table 1: Precision and Recall results per corpus on
Task 2 test set
types, as in a bag of words model, and the vec-
tor entries are the co-occurrence frequency of the
word sense and the word type. At test time, given
a a target English word, we create a bag of word
types contextual vector for each instance of the
word using the surrounding context. We compare
the created test vector to the SemCor vectors and
choose the highest most similar sense and use that
for sense disambiguation. In case of ties, we return
more than one sense tag.
3.2 Data
We use both naturally occurring parallel data and
machine translation data. The data for our first
Task 2 submission, T2-COLEUR, comprises nat-
urally occurring parallel data, namely, the Span-
ish English portion of the EuroParl data provided
by Task 3 organizers. For the machine transla-
tion data, we use translations of the source En-
glish data pertaining to the following corpora:
the Brown corpus, WSJ, SensEval1, SensEval2
datasets as translated by two machine translation
systems: Global Link (GL), Systran (SYS) (Guo
& Diab, 2010). We refer to the translated corpus
as the SALAAM corpus. The intuition for creating
SALAAM (an artificial parallel corpus) is to create
a balanced translation corpus that is less domain
and genre skewed than the EuroParl data. This lat-
ter corpus results in our 2nd system for this task
T2-COLSLM.
3.3 Results
Table 1 presents our overall results as evaluated by
the organizers.
It is clear that the T2-COLSLM outperforms
T2-COLEUR.
4 Task 3
4.1 System Details
Contrary to Task 2, we apply a context based un-
supervised WSD module to the English side of the
parallel data. Our unsupervised WSD method, as
described in (Guo & Diab, 2009), is a graph based
130
unsupervised WSD method. Given a sequence of
words W = {w
1
, w
2
...w
n
}, each word w
i
with
several senses {s
i1
, s
i2
...s
im
}. A graph G = (V,E)
is defined such that there exists a vertex v for each
sense. Two senses of two different words may be
connected by an edge e, depending on their dis-
tance. That two senses are connected suggests
they should have influence on each other, accord-
ingly a maximum allowable distance is set. They
explore 4 different graph based algorithms.We fo-
cus on the In-Degree graph based algorithm.
The In-Degree algorithm presents the problem
as a weighted graph with senses as nodes and sim-
ilarity between senses as weights on edges. The
In-Degree of a vertex refers to the number of
edges incident on that vertex. In the weighted
graph, the In-Degree for each vertex is calcu-
lated by summing the weights on the edges that are
incident on it. After all the In-Degree values
for each sense are computed, the sense with max-
imum value is chosen as the final sense for that
word. In our implementation of the In-Degree
algorithm, we use the JCN similarity measure for
both Noun-Noun and Verb-Verb similarity calcu-
lation.
4.2 Data
We use the training data from EuroParl provided
by the task organizers for the 5 different language
pairs. We participate in all the language competi-
tions. We refer to our system as T3-COLEUR.
4.3 Results
Table 2 shows our system results on Task 3, spec-
ified by languages.
4.4 Error Analysis and Discussion
As shown in Table 2, our system T3-COLEUR
ranks the highest for the French, German and Ital-
ian language tasks on both best and oot. However
the overall F-measures are very low. Our system
ranks last for Dutch among 3 systems and it is
middle of the pack for the Spanish language task.
In general we note that the results for oot are nat-
urally higher than for BEST since by design it is a
more relaxed measure.
5 Related works
Our work mainly investigates the influence of
WSD on providing machine translation candi-
dates. Carpuat & Wu (2007) and Chan et al(2007)
show WSD improves MT. However, in (Carpuat
& Wu, 2007) classical WSD is missing by ignor-
ing predefined senses. They treat translation can-
didates as sense labels, then find linguistic fea-
tures in the English side, and cast the disambigua-
tion process as a classification problem. Of rele-
vance also to our work is that related to the task
of English monolingual lexical substitution. For
example some of the approaches that participated
in the SemEval 2007 excercise include the follow-
ing. Yuret (2007) used a statistical language model
based on a large corpus to assign likelihoods to
each candidate substitutes for a target word in a
sentence. Martinez et al (2007) uses WordNet to
find candidate substitutes, produce word sequence
including substitutes. They rank the substitutes by
ranking the word sequence including that substi-
tutes using web queries. In (Giuliano C. et al,
2007), they extract synonyms from dictionaries.
They have 2 ways of ranking of the synonyms:
by similarity metric based on LSA and by occur-
rence in a large 5-gram web corpus. Dahl et al
(2007) also extract synonyms from dictionaries.
They present two systems. The first one scores
substitutes based on how frequently the local con-
text match the target word. The second one in-
corporates cosine similarity. Finally, Hassan et al
(2007) extract candidates from several linguistic
resources, and combine many techniques and ev-
idences to compute the scores such as machine
translation, most common sense, language model
and so on to pick the most suitable lexical substi-
tution candidates.
6 Conclusions and Future Directions
In this paper we presented a word sense disam-
biguation based system for multilingual lexical
substitution. The approach relies on having a
WSD system for English and an automatic word
alignment method. Crucially the approach relies
on having parallel corpora. For Task 2 we apply
a supervised WSD system to derive the English
word senses. For Task 3, we apply an unsuper-
vised approach to the training and test data. Both
of our systems that participated in Task 2 achieve
a decent ranking among the participating systems.
For Task 3 we achieve the highest ranking on sev-
eral of the language pairs: French, German and
Italian.
In the future, we would like to investigate the
usage of the Spanish and Italian WordNets for the
131
Language best oot
P R rank P R rank
Dutch 10.71 10.56 3/3 21.47 21.27 3/3
Spanish 19.78 19.59 3/7 35.84 35.46 5/7
French 21.96 21.73 1/7 49.44 48.96 1/5
German 13.79 13.63 1/3 33.21 32.82 1/3
Italian 15.55 15.4 1/3 40.7 40.34 1/3
Table 2: Results of T3-COLEUR per language on Task 3 Test set
task. We would like to also expand our exami-
nation to other sources of bilingual data such as
comparable corpora. Finally, we would like to in-
vestigate using unsupervised clustering of senses
(Word Sense Induction) methods in lieu of the
WSD approaches that rely on WordNet.
References
CARPUAT M. & WU D. (2007). Improving statis-
tical machine translation using word sense disam-
biguation. In Proceedings of the 2007 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP-CoNLL), p. 61?72, Prague,
Czech Republic: Association for Computational
Linguistics.
CHAN Y. S., NG H. T. & CHIANG D. (2007). Word
sense disambiguation improves statistical machine
translation. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
p. 33?40, Prague, Czech Republic: Association for
Computational Linguistics.
DAHL G., FRASSICA A. & WICENTOWSKI R. (2007).
SW-AG: Local Context Matching for English Lexi-
cal Substitution. In Proceedings of the 4th workshop
on Semantic Evaluations (SemEval-2007), Prague,
Czech Republic.
FELLBAUM C. (1998). ?wordnet: An electronic lexical
database?. MIT Press.
GIULIANO C., GLIOZZO A. & STRAPPARAVA C
(2007). FBK-irst: Lexical Substitution Task Ex-
ploiting Domain and Syntagmatic Coherence. In
Proceedings of the 4th workshop on Semantic Eval-
uations (SemEval-2007), Prague, Czech Republic.
GUO W. & DIAB M. (2009). ?Improvements to mono-
lingual English word sense disambiguation?. In
ACL Workshop on Semantics Evaluations.
GUO W. & DIAB M. (2010). ?Combining orthogonal
monolingual and multilingual sources of evidence
for All Words WSD?. In ACL 2010.
HASSAN S., CSOMAI A., BANEA C., SINHA R. &
MIHALCEA R. (2007). UNT: SubFinder: Combin-
ing Knowledge Sources for Automatic Lexical Sub-
stitution. In Proceedings of the 4th workshop on Se-
mantic Evaluations (SemEval-2007), Prague, Czech
Republic.
IDE N. & V RONIS J. (1998). Word sense disambigua-
tion: The state of the art. In Computational Linguis-
tics, p. 1?40.
JIANG J. & CONRATH. D. (1997). Semantic similar-
ity based on corpus statistics and lexical taxonomy.
In Proceedings of the International Conference on
Research in Computational Linguistics, Taiwan.
LEACOCK C. & CHODOROW M. (1998). Combining
local context and wordnet sense similarity for word
sense identification. In WordNet, An Electronic Lex-
ical Database: The MIT Press.
LEFEVER C. & HOSTE V. (2009). SemEval-2010
Task 3: Cross-lingual Word Sense Disambiguation.
In Proceedings of the NAACL HLT Workshop on Se-
mantic Evaluations: Recent Achievements and Fu-
ture Directions, Boulder, Colorado.
LESK M. (1986). Automatic sense disambiguation us-
ing machine readable dictionaries: How to tell a pine
cone from an ice cream cone. In In Proceedings of
the SIGDOC Conference, Toronto.
MARTINEZ D., KIM S. & BALDWIN T. (2007).
MELB-MKB: Lexical Substitution system based
on Relatives in Context In Proceedings of the
4th workshop on Semantic Evaluations (SemEval-
2007), Prague, Czech Republic.
M. PALMER, C. FELLBAUM S. C. L. D. & DANG
H. (2001). English tasks: all-words and verb lex-
ical sample. In In Proceedings of ACL/SIGLEX
Senseval-2, Toulouse, France.
MIHALCEA R. (2005). Unsupervised large-vocabulary
word sense disambiguation with graph-based algo-
rithms for sequence data labeling. In Proceedings
of Human Language Technology Conference and
Conference on Empirical Methods in Natural Lan-
guage Processing, p. 411?418, Vancouver, British
Columbia, Canada: Association for Computational
Linguistics.
MILLER G. A. (1990). Wordnet: a lexical database for
english. In Communications of the ACM, p. 39?41.
132
NAVIGLI R. (2009). Word sense disambiguation: a
survey. In ACM Computing Surveys, p. 1?69: ACM
Press.
OCH F. J. & NEY H. (2003). A systematic compari-
son of various statistical alignment models. Compu-
tational Linguistics, 29(1), 19?51.
PEDERSEN B. & PATWARDHAN (2005). Maximizing
semantic relatedness to perform word sense disam-
biguation. In University of Minnesota Supercomput-
ing Institute Research Report UMSI 2005/25, Min-
nesotta.
PRADHAN S., LOPER E., DLIGACH D. & PALMER
M. (2007). Semeval-2007 task-17: English lexi-
cal sample, srl and all words. In Proceedings of the
Fourth International Workshop on Semantic Evalua-
tions (SemEval-2007), p. 87?92, Prague, Czech Re-
public: Association for Computational Linguistics.
SINHA R. & MIHALCEA R. (2007). Unsupervised
graph-based word sense disambiguation using mea-
sures of word semantic similarity. In Proceedings
of the IEEE International Conference on Semantic
Computing (ICSC 2007), Irvine, CA.
SINHA R., MCCARTHY D. & MIHALCEA R. (2009).
SemEval-2010 Task 2: Cross-Lingual Lexical Sub-
stitution. In Proceedings of the NAACL HLT Work-
shop on Semantic Evaluations: Recent Achieve-
ments and Future Directions, Irvine, CA.
SNYDER B. & PALMER M. (2004). The english all-
words task. In R. MIHALCEA & P. EDMONDS,
Eds., Senseval-3: Third International Workshop on
the Evaluation of Systems for the Semantic Analysis
of Text, p. 41?43, Barcelona, Spain: Association for
Computational Linguistics.
YURET D. (2007). KU: Word sense disambiguation
by substitution. In Proceedings of the 4th workshop
on Semantic Evaluations (SemEval-2007), Prague,
Czech Republic.
133
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 586?590,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Weiwei: A Simple Unsupervised Latent Semantics based Approach for
Sentence Similarity
Weiwei Guo
Department of Computer Science,
Columbia University,
weiwei@cs.columbia.edu
Mona Diab
Center for Computational Learning Systems,
Columbia University,
mdiab@ccls.columbia.edu
Abstract
The Semantic Textual Similarity (STS) shared
task (Agirre et al, 2012) computes the degree
of semantic equivalence between two sen-
tences.1 We show that a simple unsupervised
latent semantics based approach, Weighted
Textual Matrix Factorization that only exploits
bag-of-words features, can outperform most
systems for this task. The key to the approach
is to carefully handle missing words that are
not in the sentence, and thus rendering it su-
perior to Latent Semantic Analysis (LSA) and
Latent Dirichlet Allocation (LDA). Our sys-
tem ranks 20 out of 89 systems according to
the official evaluation metric for the task, Pear-
son correlation, and it ranks 10/89 and 19/89
in the other two evaluation metrics employed
by the organizers.
1 Introduction
Identifying the degree of semantic similarity [SS]
between two sentences is helpful for many NLP top-
ics. In Machine Translation (Kauchak and Barzi-
lay, 2006) and Text Summarization (Zhou et al,
2006), results are automatically evaluated based on
sentence comparison. In Text Coherence Detection
(Lapata and Barzilay, 2005), sentences are linked to-
gether by similar or related words. For Word Sense
Disambiguation, researchers (Banerjee and Peder-
sen, 2003; Guo and Diab, 2012a) construct a sense
similarity measure from the sentence similarity of
the sense definitions.
Almost all SS approaches decompose the task into
word pairwise similarity problems. For example, Is-
1Mona Diab, co-author of this paper, is one of the task orga-
nizers
lam and Inkpen (2008) create a matrix for each sen-
tence pair, where columns are the words in the first
sentence and rows are the words in the second sen-
tence, and each cell stores the distributional similar-
ity of the two words. Then they create an alignment
between words in two sentences, and sentence simi-
larity is calculated based on the sum of the similarity
of aligned word pairs. There are two disadvantages
with word similarity based approaches: 1. lexical
ambiguity as the word pairwise similarity ignores
the semantic interaction between the word and sen-
tence/context. 2. word co-occurrence information
is not as sufficiently exploited as they are in latent
variable models such as Latent Semantic Analysis
(LSA) (Landauer et al, 1998) and Latent Dirichilet
Allocation (LDA) (Blei et al, 2003). On the other
hand, latent variable models can solve the two issues
naturally by modeling the semantics of words and
sentences simultaneously in the low-dimensional la-
tent space.
However, attempts at addressing SS using LSA
perform significantly below word similarity based
models (Mihalcea et al, 2006; O?Shea et al, 2008).
We believe the reason is that the observed words
in a sentence are too few for latent variable mod-
els to learn robust semantics. For example, given
the two sentences of WordNet sense definitions for
bank#n#1 and stock#n#1:
bank#n#1: a financial institution that accepts de-
posits and channels the money into lending activities
stock#n#1: the capital raised by a corporation
through the issue of shares entitling holders to an
ownership interest (equity)
LDA can only find the dominant topic (the
financial topic) based on the observed words with-
out further discernibility. In this case, many sen-
586
tences will share the same latent semantics profile,
as long as they are in the same topic/domain.
In our work (Guo and Diab, 2012b), we propose
to model the missing words (words that are not in
the sentence) to address the sparseness issue for the
SS task. Our intuition is since observed words in a
sentence are too few to tell us what the sentence is
about, missing words can be used to tell us what the
sentence is not about. We assume that the semantic
space of both the observed and missing words make
up the complete semantic profile of a sentence. We
implement our idea using a weighted matrix factor-
ization approach (Srebro and Jaakkola, 2003), which
allows us to treat observed words and missing words
differently.
It should be noted that our approach is very gen-
eral (similar to LSA/LDA) in that it can be applied to
any genre of short texts, in a manner different from
existing work that models short texts by using addi-
tional data, e.g., Ramage et al (2010) model tweets
using their metadata (author, hashtag, etc). Also we
do not extract additional features such as multiwords
expression or syntax from sentences ? all we use is
bag-of-words feature.
2 Related Work
Almost all current SS methods work in the high-
dimensional word space, and rely heavily on
word/sense similarity measures. The word/sense
similarity measure is either knowledge based (Li et
al., 2006; Feng et al, 2008; Ho et al, 2010; Tsatsa-
ronis et al, 2010), corpus-based (Islam and Inkpen,
2008) or hybrid (Mihalcea et al, 2006). Almost all
of them are evaluated on a data set introduced in (Li
et al, 2006). The LI06 data set consists of 65 pairs
of noun definitions selected from the Collin Cobuild
Dictionary. A subset of 30 pairs is further selected
by LI06 to render the similarity scores evenly dis-
tributed. Our approach has outperformed most of the
previous methods on LI06 achieving the second best
Pearson?s correlation and the best Spearman corre-
lation (Guo and Diab, 2012b).
3 Learning Latent Semantics of Sentences
3.1 Intuition
Given only a few observed words in a sentence, there
are many hypotheses of latent vectors that are highly
related to the observed words. Therefore, missing
Figure 1: Matrix Factorization
words can be used to prune the hypotheses that are
also highly related to the missing words.
Consider the hypotheses of latent vectors in Ta-
ble 1 for the sentence of the WordNet definition
of bank#n#1. Assume there are 3 dimensions in
our latent model: financial, sport, institution. We
use Rvo to denote the sum of relatedness between
latent vector v and all observed words; similarly,
Rvm is the sum of relatedness between the vector
v and all missing words. Hypothesis v1 is given
by topic models, where only the financial sen-
tence is found, and it has the maximum relatedness
to observed words in bank#n#1 sentence Rv1o =20.
v2 is the ideal latent vector, since it also detects
that bank#n#1 is related to institution. It has a
slightly smaller Rv2o =18, but more importantly, re-
latedness to missing words Rv2m=300 is substantially
smaller than Rv1m=600.
However, we cannot simply choose a hypothesis
with the maximum Ro ?Rm value, since v3, which
is clearly not related to bank#n#1 but with a min-
imum Rm=100, will be our final answer. The so-
lution is straightforward: give a smaller weight to
missing words, e.g., so that the algorithm tries to
select a hypothesis with maximum value of Ro ?
0.01 ? Rm. To implement this idea, we model the
missing words in the weighted matrix factorization
framework [WMF] (Srebro and Jaakkola, 2003).
3.2 Modeling Missing Words by Weighted
Matrix Factorization
Given a corpus we represent the corpus as an
M ? N matrix X . The row entries of the matrix
are the unique N words in the corpus, and the M
columns are the sentence ids of all the sentences.
The yielded N ?M co-occurrence matrix X com-
prises the TF-IDF values in each Xij cell, namely
that TF-IDF value of word wi in sentence sj .
In WMF, the original matrix X is factorized into
two matrices such thatX ? P>Q, where P is aK?
M matrix, and Q is a K ? N matrix (Figure 1). In
this scenario, the latent semantics of each wordwi or
sentence sj is represented as a K-dimension vector
587
financial sport institution Ro Rm Ro ?Rm Ro ? 0.01Rm
v1 1 0 0 20 600 -580 14
v2 0.6 0 0.1 18 300 -282 15
v3 0.2 0.3 0.2 5 100 -95 4
Table 1: Three possible hypotheses of latent vectors for definition of bank#n#1
P?,i or Q?,j . Note that the inner product of P?,i and
Q?,j is used to approximate the semantic relatedness
of word wi and sentence sj : Xij ? P?,i ?Q?,j , as the
shaded parts in Figure 1.
In WMF each cell is associated with a weight, so
missing words cells (Xij=0) can have a much less
contribution than observed words. Assume wm is
the weight for missing words cells. The latent vec-
tors of words P and sentences Q are estimated by
minimizing the objective function:
?
i
?
j
Wij (P?,i ?Q?,j ?Xij)
2 + ?||P ||22 + ?||Q||
2
2
where Wi,j =
{
1, if Xij 6= 0
wm, if Xij = 0
(1)
Equation 1 explicitly requires the latent vector of
sentence Q?,j to be not related to missing words
(P?,i ? Q?,j should be close to 0 for missing words
Xij = 0). Also weight wm for missing words is
very small to make sure latent vectors such as v3 in
Table 1 will not be chosen. In experiments we set
wm = 0.01. We refer to our approach as Weighted
Textual Matrix Factorization (WTMF).
After we run WTMF on the sentence corpus, the
similarity of the two sentences sj and sk can be com-
puted by the inner product of Q?,j and Q?,k.
3.3 Inference
The latent vectors in P and Q are first randomly
initialized, then can be computed iteratively by the
following equations (derivation is omitted due to
limited space, but can be found in (Srebro and
Jaakkola, 2003)):
P?,i =
(
QW? (i)Q> + ?I
)?1
QW? (i)X>i,?
Q?,j =
(
PW? (j)P> + ?I
)?1
PW? (i)X?,j
(2)
where W? (i) = diag(W?,i) is an M ? M diagonal
matrix containing ith row of weight matrixW . Sim-
ilarly, W? (j) = diag(W?,j) is an N ? N diagonal
matrix containing jth column of W .
Since most of the cells have the same value of 0,
the inference can be further optimized to save com-
putation, which has been described in (Steck, 2010).
4 Data Preprocessing
The data sets for WTMF comprises two dictionar-
ies WordNet (Fellbaum, 1998), Wiktionary,2 and
the Brown corpus. We did not link the senses be-
tween WordNet and Wiktionary, therefore the defini-
tion sentences are simply treated as individual docu-
ments. We crawl Wiktionary and remove the entries
that are not tagged as noun, verb, adjective, or ad-
verb, resulting in 220,000 entries. For both WordNet
and Wiktionary, target words are added to the defini-
tion (e.g. the word bank is added into the definition
sentence of bank#n#1). Also usage examples are
appended to definition sentences (hence sentences
become short texts). For the Brown corpus, each
sentence is treated as a document in order to create
more co-occurrence. The importance of words in a
sentence is estimated by the TF-IDF schema.
All data is tokenized, pos-tagged3, and lemma-
tized4. To reduce word sparsity issue, we take
an additional preprocessing step: for each lemma-
tized word, we find all its possible lemmas, and
choose the most frequent lemma according to Word-
Net::QueryData. For example, the word thinkings is
first lemmatized as thinking, then we discover think-
ing has possible lemmas thinking and think, finally
we choose think as targeted lemma. The STS data is
also preprocessed using the same pipeline.
5 Experiments
5.1 Setting
STS data: The sentence pair data in the STS
task is collected from five sources: 1. MSR Para-
phrase corpus (Dolan et al, 2004), 2. MSR video
data (Chen and Dolan, 2011), 3. SMT europarl data,
2http://en.wiktionary.org/wiki/Wiktionary:Main Page
3http://nlp.stanford.edu/software/tagger.shtml
4http://wn-similarity.sourceforge.net, WordNet::QueryData
588
models MSRpar MSRvid SMT-eur ON-WN SMT-news
LDA 0.274 0.7682 0.452 0.619 0.366
WTMF 0.411(67/89) 0.835(11/89) 0.513(10/89) 0.727(1/89) 0.438(28/89)
Table 2: Performance of LDA and WTMF on each individual test set of Task 6 STS data
ALL ALLnrm Mean
0.695(20/89) 0.830(10/89) 0.608(19/89)
Table 3: Performance of WTMF on all test sets
4. OntoNotes-WordNet data (Hovy et al, 2006), 5.
SMT news data.
Evaluation Metrics: Since the systems are re-
quired to assigned a similarity score to each sentence
pair, Pearson?s correlation is used to measure the
performance of systems on each of the 5 data sets.
However, measuring the overall performance on the
concatenation of 5 data sets is rarely discussed in
previous work. Accordingly the organizers of STS
task provide three evaluation metrics: 1. ALL: Pear-
son correlation with the gold standard for the com-
bined 5 data sets. 2. ALLnrm: Pearson correlation
after the system outputs for each data set are fitted
to the gold standard using least squares. 3. Mean:
Weighted mean across the 5 data sets, where the
weight depends on the number of pairs in the dataset.
WTMF Model: Our model is built on Word-
Net+Wiktionary+Brown+training data of STS. Each
sentence of STS test data is transformed into a latent
vector using Equation 2. Then sentence pair similar-
ity is computed by the cosine similarity of the two
latent vectors. We employ the parameters used in
(Guo and Diab, 2012b) (? = 20, wm = 0.01).
5.2 Results
Table 3 summarizes the overall performance of
WTMF on the concatenation of 5 data sets followed
by the corresponding rank among all participating
systems.5 There are 88 submitted results in total and
1 baseline which is simply the cosine similarity of
surface word vectors.
Table 2 compares the individual performance of
LDA (trained on the same corpus) and WTMF on
each data set. WTMF outperforms LDA by a large
margin. This is because LDA only uses 10 observed
words to infer a 100 dimension vector, while WTMF
takes advantage of much more missing words to
5http://www.cs.york.ac.uk/semeval-2012/
task6/index.php?id=results-update
learn more robust latent semantic vectors.
WTMF model achieves great overall perfor-
mance, with ranks 20, 10, 19 out of 89 reported re-
sults in three evaluation metrics respectively. It is
worth noting that WTMF is unsupervised in that it
does not use the training data similarity values, also
the only feature WTMF uses is bag-of-words fea-
tures without other information such as syntax, sen-
timent, etc. indicating that these additional features
could lead to even more improvement.
Observing the individual performance on each of
the 5 data set, we find WTMF ranks relatively high
in the four data sets: MSRvid (11/89), SMT-eur
(11/89), ON-WN (1/89), SMT-news (28/89). How-
ever, WTMF is outperformed by most of the systems
on MSRpar data set (67/89). We analyze the data set
and find that different from the other four data sets,
MSRpar is related to a lot of other NLP topics such
as textual entailment or sentiment coherence. There-
fore, our feature set (bag of words) is too shallow for
this data set indicating that using syntax and more
semantically oriented features could be helpful.
6 Conclusions
We introduce a new latent variable model WTMF
that is competitive with high dimensional ap-
proaches to the STS task. In WTMF model, we ex-
plicitly model missing words to alleviate the sparsity
problem in modeling short texts. For future work,
we would like to combine our methods with existing
word similarity based approaches and add more nu-
anced features incorporating syntax and semantics
in the latent model.
Acknowledgments
This research was funded by the Office of the Di-
rector of National Intelligence (ODNI), Intelligence
Advanced Research Projects Activity (IARPA),
through the U.S. Army Research Lab. All state-
ments of fact, opinion or conclusions contained
herein are those of the authors and should not be
construed as representing the official views or poli-
cies of IARPA, the ODNI or the U.S. Government.
589
References
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. Semeval-2012 task 6: A pilot
on semantic textual similarity. In Proceedings of the
6th International Workshop on Semantic Evaluation
(SemEval 2012), in conjunction with the First Joint
Conference on Lexical and Computational Semantics
(*SEM 2012).
Satanjeev Banerjee and Ted Pedersen. 2003. Extended
gloss overlaps as a measure of semantic relatedness.
In Proceedings of the 18th International Joint Confer-
ence on Artificial Intelligence, pages 805?810.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. Journal of Machine
Learning Research, 3.
David L. Chen and William B. Dolan. 2011. Collecting
highly parallel data for paraphrase evaluation. In Pro-
ceedings of the 49th Annual Meeting of the Association
for Computational Linguistics.
William Dolan, Chris Quirk, and Chris Brockett. 2004.
Unsupervised construction of large paraphrase cor-
pora: Exploiting massively parallel news sources. In
Proceedings of the 20th International Conference on
Computational Linguistics.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
Jin Feng, Yi-Ming Zhou, and Trevor Martin. 2008. Sen-
tence similarity based on relevance. In Proceedings of
IPMU.
Weiwei Guo and Mona Diab. 2012a. Learning the latent
semantics of a concept from its definition. In Proceed-
ings of the 50th Annual Meeting of the Association for
Computational Linguistics.
Weiwei Guo and Mona Diab. 2012b. Modeling sen-
tences in the latent space. In Proceedings of the 50th
Annual Meeting of the Association for Computational
Linguistics.
Chukfong Ho, Masrah Azrifah Azmi Murad, Rabiah Ab-
dul Kadir, and Shyamala C. Doraisamy. 2010. Word
sense disambiguation-based sentence similarity. In
Proceedings of the 23rd International Conference on
Computational Linguistics.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. Ontonotes:
The 90% solution. In Proceedings of the Human Lan-
guage Technology Conference of the North American
Chapter of the ACL.
Aminul Islam and Diana Inkpen. 2008. Semantic
text similarity using corpus-based word similarity and
string similarity. ACM Transactions on Knowledge
Discovery from Data, 2.
David Kauchak and Regina Barzilay. 2006. Paraphras-
ing for automatic evaluation. In Proceedings of the
Human Language Technology Conference of the North
American Chapter of the ACL.
Thomas K Landauer, Peter W. Foltz, and Darrell Laham.
1998. An introduction to latent semantic analysis.
Discourse Processes, 25.
Mirella Lapata and Regina Barzilay. 2005. Automatic
evaluation of text coherence: Models and representa-
tions. In Proceedings of the 19th International Joint
Conference on Artificial Intelligence.
Yuhua Li, Davi d McLean, Zuhair A. Bandar, James D. O
Shea, and Keeley Crockett. 2006. Sentence similar-
ity based on semantic nets and corpus statistics. IEEE
Transaction on Knowledge and Data Engineering, 18.
Rada Mihalcea, Courtney Corley, and Carlo Strapparava.
2006. Corpus-based and knowledge-based measures
of text semantic similarity. In Proceedings of the 21st
National Conference on Articial Intelligence.
James O?Shea, Zuhair Bandar, Keeley Crockett, and
David McLean. 2008. A comparative study of two
short text semantic similarity measures. In Proceed-
ings of the Agent and Multi-Agent Systems: Technolo-
gies and Applications, Second KES International Sym-
posium (KES-AMSTA).
Daniel Ramage, Susan Dumais, and Dan Liebling. 2010.
Characterizing microblogs with topic models. In Pro-
ceedings of the Fourth International AAAI Conference
on Weblogs and Social Media.
Nathan Srebro and Tommi Jaakkola. 2003. Weighted
low-rank approximations. In Proceedings of the Twen-
tieth International Conference on Machine Learning.
Harald Steck. 2010. Training and testing of recom-
mender systems on data missing not at random. In
Proceedings of the 16th ACM SIGKDD International
Conference on Knowledge Discovery and Data Min-
ing.
George Tsatsaronis, Iraklis Varlamis, and Michalis Vazir-
giannis. 2010. Text relatedness based on a word the-
saurus. Journal of Articial Intelligence Research, 37.
Liang Zhou, Chin-Yew Lin, Dragos Stefan Munteanu,
and Eduard Hovy. 2006. Paraeval: Using paraphrases
to evaluate summaries automatically. In Proceedings
of Human Language Tech-nology Conference of the
North American Chapter of the ACL,.
590
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 32?43, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics
*SEM 2013 shared task: Semantic Textual Similarity
Eneko Agirre
University of the Basque Country
e.agirre@ehu.es
Daniel Cer
Stanford University
danielcer@stanford.edu
Mona Diab
George Washington University
mtdiab@gwu.edu
Aitor Gonzalez-Agirre
University of the Basque Country
agonzalez278@ikasle.ehu.es
Weiwei Guo
Columbia University
weiwei@cs.columbia.edu
Abstract
In Semantic Textual Similarity (STS), sys-
tems rate the degree of semantic equivalence,
on a graded scale from 0 to 5, with 5 be-
ing the most similar. This year we set up
two tasks: (i) a core task (CORE), and (ii)
a typed-similarity task (TYPED). CORE is
similar in set up to SemEval STS 2012 task
with pairs of sentences from sources related
to those of 2012, yet different in genre from
the 2012 set, namely, this year we included
newswire headlines, machine translation eval-
uation datasets and multiple lexical resource
glossed sets. TYPED, on the other hand, is
novel and tries to characterize why two items
are deemed similar, using cultural heritage
items which are described with metadata such
as title, author or description. Several types of
similarity have been defined, including simi-
lar author, similar time period or similar lo-
cation. The annotation for both tasks lever-
ages crowdsourcing, with relative high inter-
annotator correlation, ranging from 62% to
87%. The CORE task attracted 34 participants
with 89 runs, and the TYPED task attracted 6
teams with 14 runs.
1 Introduction
Given two snippets of text, Semantic Textual Simi-
larity (STS) captures the notion that some texts are
more similar than others, measuring the degree of
semantic equivalence. Textual similarity can range
from exact semantic equivalence to complete un-
relatedness, corresponding to quantified values be-
tween 5 and 0. The graded similarity intuitively cap-
tures the notion of intermediate shades of similarity
such as pairs of text differ only in some minor nu-
anced aspects of meaning only, to relatively impor-
tant differences in meaning, to sharing only some
details, or to simply being related to the same topic,
as shown in Figure 1.
One of the goals of the STS task is to create a
unified framework for combining several semantic
components that otherwise have historically tended
to be evaluated independently and without character-
ization of impact on NLP applications. By providing
such a framework, STS will allow for an extrinsic
evaluation for these modules. Moreover, this STS
framework itself could in turn be evaluated intrin-
sically and extrinsically as a grey/black box within
various NLP applications such as Machine Trans-
lation (MT), Summarization, Generation, Question
Answering (QA), etc.
STS is related to both Textual Entailment (TE)
and Paraphrasing, but differs in a number of ways
and it is more directly applicable to a number of NLP
tasks. STS is different from TE inasmuch as it as-
sumes bidirectional graded equivalence between the
pair of textual snippets. In the case of TE the equiv-
alence is directional, e.g. a car is a vehicle, but a ve-
hicle is not necessarily a car. STS also differs from
both TE and Paraphrasing (in as far as both tasks
have been defined to date in the literature) in that,
rather than being a binary yes/no decision (e.g. a ve-
hicle is not a car), we define STS to be a graded sim-
ilarity notion (e.g. a vehicle and a car are more sim-
ilar than a wave and a car). A quantifiable graded
bidirectional notion of textual similarity is useful for
a myriad of NLP tasks such as MT evaluation, infor-
mation extraction, question answering, summariza-
tion, etc.
32
? (5) The two sentences are completely equivalent, as they mean the same thing.
The bird is bathing in the sink.
Birdie is washing itself in the water basin.
? (4) The two sentences are mostly equivalent, but some unimportant details differ.
In May 2010, the troops attempted to invade Kabul.
The US army invaded Kabul on May 7th last year, 2010.
? (3) The two sentences are roughly equivalent, but some important information differs/missing.
John said he is considered a witness but not a suspect.
?He is not a suspect anymore.? John said.
? (2) The two sentences are not equivalent, but share some details.
They flew out of the nest in groups.
They flew into the nest together.
? (1) The two sentences are not equivalent, but are on the same topic.
The woman is playing the violin.
The young lady enjoys listening to the guitar.
? (0) The two sentences are on different topics.
John went horse back riding at dawn with a whole group of friends.
Sunrise at dawn is a magnificent view to take in if you wake up early enough for it.
Figure 1: Annotation values with explanations and examples for the core STS task.
In 2012 we held the first pilot task at SemEval
2012, as part of the *SEM 2012 conference, with
great success: 35 teams participated with 88 sys-
tem runs (Agirre et al, 2012). In addition, we held
a DARPA sponsored workshop at Columbia Uni-
versity1. In 2013, STS was selected as the official
Shared Task of the *SEM 2013 conference. Ac-
cordingly, in STS 2013, we set up two tasks: The
core task CORE, which is similar to the 2012 task;
and a pilot task on typed-similarity TYPED between
semi-structured records.
For CORE, we provided all the STS 2012 data
as training data, and the test data was drawn from
related but different datasets. This is in contrast
to the STS 2012 task where the train/test data
were drawn from the same datasets. The 2012
datasets comprised the following: pairs of sentences
from paraphrase datasets from news and video elic-
itation (MSRpar and MSRvid), machine transla-
tion evaluation data (SMTeuroparl, SMTnews) and
pairs of glosses (OnWN). The current STS 2013
dataset comprises the following: pairs of news head-
lines, SMT evaluation sentences (SMT) and pairs of
glosses (OnWN and FNWN).
The typed-similarity pilot task TYPED attempts
1http://www.cs.columbia.edu/?weiwei/
workshop/
to characterize, for the first time, the reason and/or
type of similarity. STS reduces the problem of judg-
ing similarity to a single number, but, in some appli-
cations, it is important to characterize why and how
two items are deemed similar, hence the added nu-
ance. The dataset comprises pairs of Cultural Her-
itage items from Europeana,2 a single access point
to millions of books, paintings, films, museum ob-
jects and archival records that have been digitized
throughout Europe. It is an authoritative source of
information coming from European cultural and sci-
entific institutions. Typically, the items comprise
meta-data describing a cultural heritage item and,
sometimes, a thumbnail of the item itself.
Participating systems in the TYPED task need to
compute the similarity between items, using the tex-
tual meta-data. In addition to general similarity, par-
ticipants need to score specific kinds of similarity,
like similar author, similar time period, etc. (cf. Fig-
ure 3).
The paper is structured as follows. Section 2 re-
ports the sources of the texts used in the two tasks.
Section 3 details the annotation procedure. Section
4 presents the evaluation of the systems, followed
by the results of CORE and TYPED tasks. Section 6
draws on some conclusions and forward projections.
2http://www.europeana.eu/
33
Figure 2: Annotation instructions for CORE task
year dataset pairs source
2012 MSRpar 1500 news
2012 MSRvid 1500 videos
2012 OnWN 750 glosses
2012 SMTnews 750 MT eval.
2012 SMTeuroparl 750 MT eval.
2013 HDL 750 news
2013 FNWN 189 glosses
2013 OnWN 561 glosses
2013 SMT 750 MT eval.
2013 TYPED 1500 Cultural Heritage items
Table 1: Summary of STS 2012 and 2013 datasets.
2 Source Datasets
Table 1 summarizes the 2012 and 2013 datasets.
2.1 CORE task
The CORE dataset comprises pairs of news head-
lines (HDL), MT evaluation sentences (SMT) and
pairs of glosses (OnWN and FNWN).
For HDL, we used naturally occurring news head-
lines gathered by the Europe Media Monitor (EMM)
engine (Best et al, 2005) from several different news
sources. EMM clusters together related news. Our
goal was to generate a balanced data set across the
different similarity ranges, hence we built two sets
of headline pairs: (i) a set where the pairs come
from the same EMM cluster, (ii) and another set
where the headlines come from a different EMM
cluster, then we computed the string similarity be-
tween those pairs. Accordingly, we sampled 375
headline pairs of headlines that occur in the same
EMM cluster, aiming for pairs equally distributed
between minimal and maximal similarity using sim-
ple string similarity. We sample another 375 pairs
from the different EMM cluster in the same manner.
The SMT dataset comprises pairs of sentences
used in machine translation evaluation. We have two
different sets based on the evaluation metric used:
an HTER set, and a HYTER set. Both metrics use
the TER metric (Snover et al, 2006) to measure the
similarity of pairs. HTER typically relies on several
(1-4) reference translations. HYTER, on the other
hand, leverages millions of translations. The HTER
set comprises 150 pairs, where one sentence is ma-
chine translation output and the corresponding sen-
tence is a human post-edited translation. We sam-
ple the data from the dataset used in the DARPA
GALE project with an HTER score ranging from 0
to 120. The HYTER set has 600 pairs from 3 sub-
sets (each subset contains 200 pairs): a. reference
34
Figure 3: Annotation instructions for TYPED task
vs. machine translation. b. reference vs. Finite State
Transducer (FST) generated translation (Dreyer and
Marcu, 2012). c. machine translation vs. FST gen-
erated translation. The HYTER data set is used in
(Dreyer and Marcu, 2012).
The OnWN/FnWN dataset contains gloss pairs
from two sources: OntoNotes-WordNet (OnWN)
and FrameNet-WordNet (FnWN). These pairs are
sampled based on the string similarity ranging from
0.4 to 0.9. String similarity is used to measure the
similarity between a pair of glosses. The OnWN
subset comprises 561 gloss pairs from OntoNotes
4.0 (Hovy et al, 2006) and WordNet 3.0 (Fellbaum,
1998). 370 out of the 561 pairs are sampled from the
110K sense-mapped pairs as made available from
the authors. The rest, 291 pairs, are sampled from
unmapped sense pairs with a string similarity rang-
ing from 0.5 to 0.9. The FnWN subset has 189
manually mapped pairs of senses from FrameNet 1.5
(Baker et al, 1998) to WordNet 3.1. They are ran-
domly selected from 426 mapped pairs. In combi-
nation, both datasets comprise 750 pairs of glosses.
2.2 Typed-similarity TYPED task
This task is devised in the context of the PATHS
project,3 which aims to assist users in accessing
digital libraries looking for items. The project
tests methods that offer suggestions about items that
might be useful to recommend, to assist in the inter-
pretation of the items, and to support the user in the
discovery and exploration of the collections. Hence
the task is about comparing pairs of items. The pairs
are generated in the Europeana project.
A study in the PATHS project suggested that users
would be interested in knowing why the system is
suggesting related items. The study suggested seven
similarity types: similar author or creator, similar
people involved, similar time period, similar loca-
3http://www.paths-project.eu
35
Figure 4: TYPED pair on our survey. Only general and author similarity types are shown.
tion, similar event or action, similar subject and sim-
ilar description. In addition, we also include general
similarity. Figure 3 shows the definition of each sim-
ilarity type as provided to the annotators.
The dataset is generated in semi-automatically.
First, members of the project manually select 25
pairs of items for each of the 7 similarity types (ex-
cluding general similarity), totalling 175 manually
selected pairs. After removing duplicates and clean-
ing the dataset, we got 163 pairs. Second, we use
these manually selected pairs as seeds to automat-
ically select new pairs as follows: Starting from
those seeds, we use the Europeana API to get similar
items, and we repeat this process 5 times in order to
diverge from the original items (we stored the vis-
ited items to avoid looping). Once removed from
the seed set, we select the new pairs following two
approaches:
? Distance 1: Current item and similar item.
? Distance 2: Current item and an item that is
similar to a similar item (twice removed dis-
tance wise)
This yields 892 pairs for Distance 1 and 445 of
Distance 2. We then divide the data into train and
test, preserving the ratios. The train data contains
82 manually selected pairs, 446 pairs with similarity
distance 1 and 222 pairs with similarity distance 2.
The test data follows a similar distribution.
Europeana items cannot be redistributed, so we
provide their urls and a script which uses the official
36
Europeana API to access and extract the correspond-
ing metadata in JSON format and a thumbnail. In
addition, the textual fields which are relevant for the
task are made accessible in text files, as follows:
? dcTitle: title of the item
? dcSubject: list of subject terms (from some vo-
cabulary)
? dcDescription: textual description of the item
? dcCreator: creator(s) of the item
? dcDate: date(s) of the item
? dcSource: source of the item
3 Annotation
3.1 CORE task
Figure 1 shows the explanations and values for
each score between 5 and 0. We use the Crowd-
Flower crowd-sourcing service to annotate the
CORE dataset. Annotators are presented with the
detailed instructions given in Figure 2 and are asked
to label each STS sentence pair on our 6 point scale
using a dropdown box. Five sentence pairs at a time
are presented to annotators. Annotators are paid
0.20 cents per set of 5 annotations and we collect
5 separate annotations per sentence pair. Annota-
tors are restricted to people from the following coun-
tries: Australia, Canada, India, New Zealand, UK,
and US.
To obtain high quality annotations, we create a
representative gold dataset of 105 pairs that are man-
ually annotated by the task organizers. During an-
notation, one gold pair is included in each set of 5
sentence pairs. Crowd annotators are required to
rate 4 of the gold pairs correct to qualify to work
on the task. Gold pairs are not distinguished in any
way from the non-gold pairs. If the gold pairs are
annotated incorrectly, annotators are told what the
correct annotation is and they are given an explana-
tion of why. CrowdFlower automatically stops low
performing annotators ? those with too many incor-
rectly labeled gold pairs ? from working on the task.
The distribution of scores in the headlines HDL
dataset is uniform, as in FNWN and OnWN, al-
though the scores are slightly lower in FNWN and
slightly higher in OnWN. The scores for SMT are
not uniform, with most of the scores uniformly dis-
tributed between 3.5 and 5, a few pairs between 2
and 3.5, and nearly no pairs with values below 2.
3.2 TYPED task
The dataset is annotated using crowdsourcing. The
survey contains the 1500 pairs of the dataset (750 for
train and 750 for test), plus 20 gold pairs for quality
control. Each participant is shown 4 training gold
questions at the beginning, and then one gold every
2 or 4 questions depending on the accuracy. If accu-
racy dropped to less than 66.7% percent the survey
is stopped and the answers from that particular an-
notator are discarded. Each annotator is allowed to
rate a maximum of 20 pairs to avoid getting answers
from people that are either tired or bored. To ensure
a good comprehension of the items, the task is re-
stricted to only accept annotators from some English
speaking countries: UK, USA, Australia, Canada
and New Zealand.
Participants are asked to rate the similarity be-
tween pairs of cultural heritage items from rang-
ing from 5 to 0, following the instructions shown
in Figure 3. We also add a ?Not Applicable? choice
for cases in which annotators are not sure or didn?t
know. For those cases, we calculate the similarity
score using the values of the rest of the annotators (if
none, we convert it to 0). The instructions given to
the annotators are the ones shown in Figure 3. Fig-
ure 4 shows a pair from the dataset, as presented to
annotators.
The similarity scores for the pairs follow a similar
distribution in all types. Most of the pairs have a
score between 4 and 5, which can amount to as much
as 50% of all pairs in some types.
3.3 Quality of annotation
In order to assess the annotation quality, we measure
the correlation of each annotator with the average of
the rest of the annotators. We then averaged all the
correlations. This method to estimate the quality is
identical to the method used for evaluation (see Sec-
tion 4.1) and it can be thus used as the upper bound
for the systems. The inter-tagger correlation in the
CORE dataset for each of dataset is as follows:
? HDL: 85.0%
? FNWN: 69.9%
? OnWN: 87.2%
? SMT: 65.8%
For the TYPED dataset, the inter-tagger correla-
tion values for each type of similarity is as follows:
? General: 77.0%
37
? Author: 73.1%
? People Involved: 62.5%
? Time period: 72.0%
? Location: 74.3%
? Event or Action: 63.9%
? Subject: 74.5%
? Description: 74.9%
In both datasets, the correlation figures are high,
confirming that the task is well designed. The weak-
est correlations in the CORE task are SMT and
FNWN. The first might reflect the fact that some
automatically produced translations are confusing
or difficult to understand, and the second could be
caused by the special style used to gloss FrameNet
concepts. In the TYPED task the weakest correla-
tions are for the People Involved and Event or Action
types, as they might be the most difficult to spot.
4 Systems Evaluation
4.1 Evaluation metrics
Evaluation of STS is still an open issue. STS ex-
periments have traditionally used Pearson product-
moment correlation, or, alternatively, Spearman
rank order correlation. In addition, we also need a
method to aggregate the results from each dataset
into an overall score. The analysis performed in
(Agirre and Amigo?, In prep) shows that Pearson and
averaging across datasets are the best suited com-
bination in general. In particular, Pearson is more
informative than Spearman, in that Spearman only
takes the rank differences into account, while Pear-
son does account for value differences as well. The
study also showed that other alternatives need to be
considered, depending on the requirements of the
target application.
We leave application-dependent evaluations for
future work, and focus on average weighted Pear-
son correlation. When averaging, we weight each
individual correlation by the size of the dataset.
In addition, participants in the CORE task are al-
lowed to provide a confidence score between 1 and
100 for each of their scores. The evaluation script
down-weights the pairs with low confidence, follow-
ing weighted Pearson.4 In order to compute sta-
tistical significance among system results, we use
4http://en.wikipedia.org/wiki/Pearson_
product-moment_correlation_coefficient#
Calculating_a_weighted_correlation
a one-tailed parametric test based on Fisher?s z-
transformation (Press et al, 2002, equation 14.5.10).
4.2 The Baseline Systems
For the CORE dataset, we produce scores using a
simple word overlap baseline system. We tokenize
the input sentences splitting at white spaces, and
then represent each sentence as a vector in the mul-
tidimensional token space. Each dimension has 1
if the token is present in the sentence, 0 otherwise.
Vector similarity is computed using the cosine sim-
ilarity metric. We also run two freely available sys-
tems, DKPro (Bar et al, 2012) and TakeLab (S?aric? et
al., 2012) from STS 2012,5 and evaluate them on the
CORE dataset. They serve as two strong contenders
since they ranked 1st (DKPro) and 2nd (TakeLab) in
last year?s STS task.
For the TYPED dataset, we first produce XML
files for each of the items, using the fields as pro-
vided to participants. Then we run named entity
recognition and classification (NERC) and date de-
tection using Stanford CoreNLP. This is followed by
calculating the similarity score for each of the types
as follows.
? General: cosine similarity of TF-IDF vectors of
tokens from all fields.
? Author: cosine similarity of TF-IDF vectors for
dc:Creator field.
? People involved, time period and location:
cosine similarity of TF-IDF vectors of loca-
tion/date/people recognized by NERC in all
fields.
? Events: cosine similarity of TF-IDF vectors of
verbs in all fields.
? Subject and description: cosine similarity of
TF-IDF vectors of respective fields.
IDF values are calculated from a subset of the
Europeana collection (Culture Grid collection). We
also run a random baseline several times, yielding
close to 0 correlations in all datasets, as expected.
4.3 Participation
Participants could send a maximum of three system
runs. After downloading the test datasets, they had
a maximum of 120 hours to upload the results. 34
teams participated in the CORE task, submitting 89
5Code is available at http://www-nlp.stanford.
edu/wiki/STS
38
Team and run Head. OnWN FNWN SMT Mean # Team and run Head. OnWN FNWN SMT Mean #
baseline-tokencos .5399 .2828 .2146 .2861 .3639 73 KnCe2013-all .3475 .3505 .1073 .1551 .2639 86
DKPro .7347 .7345 .3405 .3256 .5652 - KnCe2013-diff .4028 .3537 .1284 .1804 .2934 84
TakeLab-best .6559 .6334 .4052 .3389 .5221 - KnCe2013-set .0462 -.1526 .0376 -.0605 -.0397 90
TakeLab-sts12 .4858 .6334 .2693 .2787 .4340 - LCL Sapienza-ADW1 .6943 .4661 .3571 .3311 .4880 43
aolney-w3c3 .5248 .4701 .1777 .2744 .3986 67 LCL Sapienza-ADW2 .6520 .5280 .3598 .3681 .5019 32
BGU-1 .5075 .3252 .0768 .1843 .3181 81 LCL Sapienza-ADW3 .6205 .5108 .4462 .3838 .4996 34
BGU-2 .3608 .3777 -.0173 .0698 .2363 88 LIPN-tAll .7063 .6937 .4037 .3005 .5425 16
BGU-3 .3591 .3360 .0072 .2122 .2748 85 LIPN-tSp .5791 .7199 .3522 .3721 .5261 24
BUAP-RUN1 .5005 .2579 .1766 .2322 .3234 78 MayoClinicNLP-r1wtCDT .6584 .7775 .3735 .3605 .5649 6
BUAP-RUN2 .4860 .2872 .2082 .2117 .3216 79 MayoClinicNLP-r2CDT .6827 .6612 .3960 .3946 .5572 8
BUAP-RUN3 .4817 .2711 .2511 .1990 .3156 82 MayoClinicNLP-r3wtCD .6440 .8295 .3202 .3561 .5671 5
CFILT-1 .5336 .2381 .2261 .2906 .3531 75 NTNU-RUN1 .7279 .5952 .3215 .4015 .5519 9
CLaC-RUN1 .6774 .7667 .3793 .3068 .5511 10 NTNU-RUN2 .5909 .1634 .3650 .3786 .3946 68
CLaC-RUN2 .6921 .7366 .3793 .3375 .5587 7 NTNU-RUN3 .7274 .5882 .3115 .4035 .5498 12
CLaC-RUN3 .5276 .6495 .4158 .3082 .4755 47 PolyUCOMP-RUN1 .5176 .1517 .2496 .2914 .3284 77
CNGL-LPSSVR .6510 .6971 .1180 .2861 .4961 36 SOFTCARDINALITY-run1 .6410 .7360 .3442 .3035 .5273 23
CNGL-LPSSVRTL .6385 .6756 .1823 .3098 .4998 33 SOFTCARDINALITY-run2 .6713 .7412 .3838 .2981 .5402 18
CNGL-LSSVR .6552 .6943 .2016 .3005 .5086 30 SOFTCARDINALITY-run3 .6603 .7401 .3347 .2900 .5294 22
CPN-combined.RandSubSpace .6771 .5135 .3314 .3369 .4939 39 sriubc-System1? .6083 .2915 .2790 .3065 .4011 66
CPN-combined.SVM .6685 .5096 .3621 .3408 .4939 38 sriubc-System2? .6359 .3664 .2713 .3476 .4420 57
CPN-individual.RandSubSpace .6771 .5484 .3314 .2769 .4826 45 sriubc-System3? .5443 .2843 .2705 .3275 .3842 70
DeepPurple-length .6542 .5105 .2507 .2803 .4598 56 SXUCFN-run1 .6806 .5355 .3181 .3980 .5198 27
DeepPurple-linear .6878 .5105 .2693 .2787 .4721 50 SXUCFN-run2 .4881 .6146 .4237 .3844 .4797 46
DeepPurple-lineara .6227 .5105 .3265 .2952 .4607 55 SXUCFN-run3 .6761 .6481 .3025 .4003 .5458 14
deft-baseline .6532 .8431 .5083 .3265 .5795 3 SXULLL-1 .4840 .7146 .0415 .1543 .3944 69
deft-baseline2 .5706 .8111 .5503 .3325 .5495 13 UCam-A .5510 .3099 .2385 .1171 .3200 80
DLS@CU-char .3867 .2386 .3726 .3337 .3309 76 UCam-B .6399 .4440 .3995 .3400 .4709 53
DLS@CU-charSemantic .4669 .4165 .3859 .3411 .4056 64 UCam-C .4962 .5639 .1724 .3006 .4207 62
DLS@CU-charWordSemantic .4921 .3769 .4647 .3492 .4135 63 UCSP-NC? .1736 .0853 .1151 .1658 .1441 89
ECNUCS-Run1 .5656 .2083 .1725 .2949 .3533 74 UMBC EBIQUITY-galactus .7428 .7053 .5444 .3705 .5927 2
ECNUCS-Run2 .7120 .5388 .2013 .2504 .4720 51 UMBC EBIQUITY-ParingWords .7642 .7529 .5818 .3804 .6181 1
ECNUCS-Run3 .6799 .5284 .2203 .3595 .4967 35 UMBC EBIQUITY-saiyan .7838 .5593 .5815 .3563 .5683 4
HENRY-run1 .7601 .4631 .3516 .2801 .4917 41 UMCC DLSI-1 .5841 .4847 .2917 .2855 .4352 58
HENRY-run2 .7645 .4631 .3905 .3593 .5229 26 UMCC DLSI-2 .6168 .5557 .3045 .3407 .4833 44
HENRY-run3 .7103 .3934 .3364 .3308 .4734 48 UMCC DLSI-3 .3846 .1342 -.0065 .2736 .2523 87
IBM EG-run2 .7217 .6110 .3364 .3460 .5365 19 UNIBA-2STEPSML .4255 .4801 .1832 .2710 .3673 71
IBM EG-run5 .7410 .5987 .4133 .3426 .5452 15 UNIBA-DSM PERM .6319 .4910 .2717 .3155 .4610 54
IBM EG-run6 .7447 .6257 .4381 .3275 .5502 11 UNIBA-STACKING .6275 .4658 .2111 .2588 .4293 61
ikernels-sys1 .7352 .5432 .3842 .3180 .5188 28 Unimelb NLP-bahar .7119 .3490 .3813 .3507 .4733 49
ikernels-sys2 .7465 .5572 .3875 .3409 .5339 21 Unimelb NLP-concat .7085 .6790 .3374 .3230 .5415 17
ikernels-sys3 .7395 .4228 .3596 .3294 .4919 40 Unimelb NLP-stacking .7064 .6140 .1865 .3144 .5091 29
INAOE-UPV-run1 .6392 .3249 .2711 .3491 .4332 59 Unitor-SVRegressor run1 .6353 .5744 .3521 .3285 .4941 37
INAOE-UPV-run2 .6390 .3260 .2662 .3457 .4319 60 Unitor-SVRegressor run2 .6511 .5610 .3580 .3096 .4902 42
INAOE-UPV-run3 .6468 .6295 .4090 .3047 .5085 31 Unitor-SVRegressor run3 .6027 .5489 .3269 .3192 .4716 52
KLUE-approach 1 .6521 .6507 .3996 .3367 .5254 25 UPC-AE .6092 .5679 -.1268 .2090 .4037 65
KLUE-approach 2 .6510 .6869 .4189 .3360 .5355 20 UPC-AED .4136 .4770 -.0852 .1662 .3050 83
UPC-AED T .5119 .6386 -.0464 .1235 .3671 72
Table 2: Results on the CORE task. The first rows on the left correspond to the baseline and to two publicly available
systems, see text for details. Note: ? signals team involving one of the organizers, ? for systems submitting past the
120 hour window.
system runs. For the TYPED task, 6 teams partici-
pated, submitting 14 system runs.6
Some submissions had minor issues: one team
had a confidence score of 0 for all items (we re-
placed them by 100), and another team had a few
Not-a-Number scores for the SMT dataset, which
we replaced by 5. One team submitted the results
past the 120 hours. This team, and the teams that in-
6Due to lack of space we can?t detail the full names of au-
thors and institutions that participated.The interested reader can
use the name of the runs in Tables 2 and 3 to find the relevant
paper in these proceedings.
cluded one of the organizers, are explicitly marked.
We want to stress that in these teams the organizers
did not allow the developers of the system to access
any data or information which was not available for
the rest of participants. After the submission dead-
line expired, the organizers published the gold stan-
dard in the task website, in order to ensure a trans-
parent evaluation process.
4.4 CORE Task Results
Table 2 shows the results of the CORE task, with
runs listed in alphabetical order. The correlation in
39
Team and run General Author People involved Time Location Event Subject Description Mean #
baseline .6691 .4278 .4460 .5002 .4835 .3062 .5015 .5810 .4894 8
BUAP-RUN1 .6798 .6166 .0670 .2761 .0163 .1612 .5167 .5283 .3577 14
BUAP-RUN2 .6745 .6093 .1285 .3721 .0163 .1660 .5094 .5546 .3788 13
BUAP-RUN3 .6992 .6345 .1055 .1461 .0000 -.0668 .3729 .5120 .3004 15
BUT-1 .3686 .7468 .3920 .5725 .3604 .2906 .2270 .5882 .4433 9
ECNUCS-Run1 .6040 .7362 .3663 .4685 .3844 .4057 .5229 .6027 .5113 5
ECNUCS-Run2 .6064 .5684 .3663 .4685 .3844 .4057 .5563 .6027 .4948 7
PolyUCOMP-RUN1 .4888 .6940 .3223 .3820 .3621 .1625 .3962 .4816 .4112 12
PolyUCOMP-RUN2 .4893 .6940 .3253 .3777 .3628 .1968 .3962 .4816 .4155 11
PolyUCOMP-RUN3 .4915 .6940 .3254 .3737 .3667 .2207 .3962 .4816 .4187 10
UBC UOS-RUN1? .7256 .4568 .4467 .5762 .4858 .3090 .5015 .5810 .5103 6
UBC UOS-RUN2? .7457 .6618 .6518 .7466 .7244 .6533 .7404 .7751 .7124 4
UBC UOS-RUN3? .7461 .6656 .6544 .7411 .7257 .6545 .7417 .7763 .7132 3
Unitor-SVRegressor lin .7564 .8076 .6758 .7090 .7351 .6623 .7520 .7745 .7341 2
Unitor-SVRegressor rbf .7981 .8158 .6922 .7471 .7723 .6835 .7875 .7996 .7620 1
Table 3: Results on TYPED task. The first row corresponds to the baseline. Note: ? signals team involving one of the
organizers.
each dataset is given, followed by the mean cor-
relation (the official measure), and the rank of the
run. The baseline ranks 73. The highest correla-
tions are for OnWN (84%, by deft) and HDL (78%,
by UMBC), followed by FNWN (58%, by UMBC)
and SMT (40%, by NTNU). This fits nicely with the
inter-tagger correlations (respectively 87, 85, 70 and
65, cf. Section 3). It also shows that the systems get
close to the human correlations in the OnWN and
HDL dataset, with bigger differences for FNWN and
SMT.
The result of the best run (by UMBC) is signif-
icantly different (p-value < 0.05) than all runs ex-
cept the second best. The second best run is only
significantly different to the runs ranking 7th and
below, and the third best to the 14th run and be-
low. The difference between consecutive runs was
not significant. This indicates that many system runs
performed very close to each other.
Only 13 runs included non-uniform confidence
scores. In 10 cases the confidence value allowed
to improve performance, sometimes as much as .11
absolute points. For instance, SXUCFN-run3 im-
proves from .4773 to .5458. The most notable ex-
ception is MayoClinicNLP-r2CDT, which achieves
a mean correlation of .5879 instead of .5572 if they
provide uniform confidence values.
The Table also shows the results of TakeLab
and DKPro. We train the DKPro and TakeLab-
sts12 models on all the training and test STS 2012
data. We additionally train another variant sys-
tem of TakeLab, TakeLab-best, where we use tar-
geted training where the model yields the best per-
formance for each test subset as follows: (1) HDL
is trained on MSRpar 2012 data; (2) OnWN is
trained on all 2012 data; (3) FnWN is trained on
2012 OnWN data; (4) SMT is trained on 2012 SM-
Teuroparl data. Note that Takelab-best is an upper
bound, as the best combination is selected on the
test dataset. TakeLab-sts12, TakeLab-best, DKPro
rank as 58th, 27th and 6th in this year?s system sub-
missions, respectively. The different results yielded
from TakeLab depending on the training data sug-
gests that some STS systems are quite sensitive to
the source of the sentence pairs, indicating that do-
main adaptation techniques could have a role in this
task. On the other hand, DKPro performed ex-
tremely well when trained on all available training,
with no special tweaking for each dataset.
4.5 TYPED Task Results
Table 3 shows the results of TYPED task. The
columns show the correlation for each type of sim-
ilarity, followed by the mean correlation (the offi-
cial measure), and the rank of the run. The best sys-
tem (from Unitor) is best in all types. The baseline
ranked 8th, but the performance difference with the
best system is quite significant. The best result is
significantly different (p-value < 0.02) to all runs.
The second and third best runs are only significantly
different from the run ranking 5th and below. Note
that in this dataset the correlations of the best system
are higher than the inter-tagger correlations. This
might indicate that the task has been solved, in the
sense that the features used by the top systems are
enough to characterize the problem and reach hu-
man performance, although the correlations of some
40
A
cr
on
ym
s
D
is
tr
ib
ut
io
na
lm
em
or
y
D
is
tr
ib
ut
io
na
lt
he
sa
ur
us
M
on
ol
in
gu
al
co
rp
or
a
M
ul
ti
li
ng
ua
lc
or
po
ra
O
pi
ni
on
an
d
S
en
ti
m
en
t
Ta
bl
es
of
pa
ra
ph
ra
se
s
W
ik
ip
ed
ia
W
ik
ti
on
ar
y
W
or
d
em
be
dd
in
gs
W
or
dN
et
C
or
re
fe
re
nc
e
D
ep
en
de
nc
y
pa
rs
e
D
is
tr
ib
ut
io
na
ls
im
il
ar
it
y
K
B
S
im
il
ar
it
y
L
D
A
L
em
m
at
iz
er
L
ex
ic
al
S
ub
st
it
ut
io
n
L
og
ic
al
in
fe
re
nc
e
M
et
ap
ho
r
or
M
et
on
ym
y
M
ul
ti
w
or
d
re
co
gn
it
io
n
N
am
ed
E
nt
it
y
re
co
gn
it
io
n
P
O
S
ta
gg
er
R
O
U
G
E
pa
ck
ag
e
S
co
pi
ng
S
ea
rc
h
en
gi
ne
S
em
an
ti
c
R
ol
e
L
ab
el
in
g
S
tr
in
g
si
m
il
ar
it
y
S
yn
ta
x
Te
xt
ua
le
nt
ai
lm
en
t
T
im
e
an
d
da
te
re
so
lu
ti
on
T
re
e
ke
rn
el
s
W
or
d
S
en
se
D
is
am
bi
gu
at
io
n
aolney-w3c3 x x x
BGU-1 x x x x x x x
BGU-2 x x x x x x x
BGU-3 x x x x x x x
CFILT-APPROACH x x x x x
CLaC-Run1 x x x x x x x x
CLaC-Run2 x x x x x x x x
CLaC-Run3 x x x x x x x x
CNGL-LPSSVR x x x x x
CNGL-LPSSVRTL x x x x x
CNGL-LSSVR x x x x x
CPN-combined.RandSubSpace x x x x x x x x
CPN-combined.SVM x x x x x x x x
CPN-individual.RandSubSpace x x x x x x x x
DeepPurple-length x x x x x x x
DeepPurple-linear x x x x x x x
DeepPurple-lineara x x x x x x x
deft-baseline x x x x
deft-baseline x x x x x x
DLS@CU-charSemantic x x x x
DLS@CU-charWordSemantic x x x x x x
DLS@CU-charWordSemantic x x x
ECNUCS-Run1 x x x x x x x
ECNUCS-Run2 x x x x x x x
ECNUCS-Run3 x x x x x x x
HENRY-run1 x x x x x x x x x
HENRY-run2 x x x x x x x x
IBM EG-run2 x x x x x x
IBM EG-run5 x x x x x x
IBM EG-run6 x x x x x
ikernels-sys1 x x x x x x x x x x x
ikernels-sys2 x x x x x x x x x x x
ikernels-sys3 x x x x x x x x x x x
INAOE-UPV-run1 x x x x x x x
INAOE-UPV-run2 x x x x x x x
INAOE-UPV-run3 x x x x x x x
KLUE-approach 1 x x x x x x x
KLUE-approach 2 x x x x x x
KnCe2013-all x x x x x x x x
KnCe2013-div x x x x x x x x
KnCe2013-div x x x x x x x x
LCL Sapienza-ADW1 x x x
LCL Sapienza-ADW2 x x x
LCL Sapienza-ADW3 x x x
LIPN-tAll x x x x x x x x x x
LIPN-tSp x x x x x x x x x x
MayoClinicNLP-r1wtCDT x x x x x x x x x x x x
MayoClinicNLP-r2CDT x x x x x x x x x x x x
MayoClinicNLP-r3wtCD x x x x x x x x x x x x
NTNU-RUN1 x x x x x x x x x x x x x x x x x x x x x x x
NTNU-RUN2 x x x x x x x x x x x x x x x x x x x x x x x
NTNU-RUN3 x x x x x x x x x x x x x x x x x x x x x x x
PolyUCOMP-RUN1 x x x x
SOFTCARDINALITY-run1 x
SOFTCARDINALITY-run2 x x x
SOFTCARDINALITY-run3 x x x
SXUCFN-run1 x x x
SXUCFN-run2 x x x
SXUCFN-run3 x x x
SXULLL-1 x x
UCam-A x x x x
UCam-B x x x x
UCam-C x x x x
UCSP-NC x x x x x
UMBC EBIQUITY-galactus x x x x x x x
UMBC EBIQUITY-ParingWords x x x x x x
UMBC EBIQUITY-saiyan x x x x x x x
UMCC DLSI-1 x x x x x x x x x x
UMCC DLSI-2 x x x x x x x x x x
UMCC DLSI-3 x x x x x x x x x
UNIBA-2STEPSML x x x x x x x x x x x
UNIBA-DSM PERM x x x x x x
UNIBA-STACKING x x x x x x x x x x x
Unimelb NLP-bahar x x
Unimelb NLP-concat x x x x x x x x x x
Unimelb NLP-stacking x x x x x x x x x x
Unitor-SVRegressor run1 x x x x x x
Unitor-SVRegressor run2 x x x x x x
Unitor-SVRegressor run3 x x x x x x
Total 11 2 12 54 12 5 11 36 7 3 54 3 3 48 40 2 67 14 3 3 10 24 55 3 3 4 9 6 34 9 13 6 6
Table 4: CORE task: Resources and tools used by the systems that submitted a description file. Leftmost columns
correspond to the resources, and rightmost to tools, in alphabetic order.
41
types could be too low for practical use.
5 Tools and resources used
The organizers asked participants to submit a de-
scription file, making special emphasis on the tools
and resources that were used. Tables 4 and 5 show
schematically the tools and resources as reported by
some of the participants for the CORE and TYPED
tasks (respectively). In the last row, the totals show
that WordNet and monolingual corpora were the
most used resources for both tasks, followed by
Wikipedia and the use of acronyms (for CORE and
TYPED tasks respectively). Dictionaries, multilin-
gual corpora, opinion and sentiment analysis, and
lists and tables of paraphrases are also used.
For CORE, generic NLP tools such as lemmati-
zation and PoS tagging are widely used, and to a
lesser extent, distributional similarity, knowledge-
based similarity, syntactic analysis, named entity
recognition, lexical substitution and time and date
resolution (in this order). Other popular tools are
Semantic Role Labeling, Textual Entailment, String
Similarity, Tree Kernels and Word Sense Disam-
biguation. Machine learning is widely used to com-
bine and tune components (and so, it is not men-
tioned in the tables). Several less used tools are
also listed but are used by three or less systems.
The top scoring systems use most of the resources
and tools listed (UMBC EBIQUITY-ParingWords,
MayoClinicNLP-r3wtCD). Other well ranked sys-
tems like deft-baseline are only based on distribu-
tional similarity. Although not mentioned in the
descriptions files, some systems used the publicly
available DKPro and Takelab systems.
For the TYPED task, the most used tools are lem-
matizers, Named Entity Recognizers, and PoS tag-
gers. Distributional and Knowledge-base similarity
is also used, and at least four systems used syntactic
analysis and time and date resolution.7
6 Conclusions and Future Work
We presented the 2013 *SEM shared task on Seman-
tic Textual Similarity.8 Two tasks were defined: a
7For a more detailed analysis, the reader is directed to the
papers in this volume.
8All annotations, evaluation scripts and system outputs are
available in the website for the task9. In addition, a collabora-
tively maintained site10, open to the STS community, contains
A
cr
on
ym
s
M
on
ol
in
gu
al
co
rp
or
a
W
ik
ip
ed
ia
W
or
dN
et
D
is
tr
ib
ut
io
na
ls
im
il
ar
it
y
K
B
S
im
il
ar
it
y
L
em
m
at
iz
er
M
ul
ti
w
or
d
re
co
gn
it
io
n
N
am
ed
E
nt
it
y
re
co
gn
it
io
n
P
O
S
ta
gg
er
S
yn
ta
x
T
im
e
an
d
da
te
re
so
lu
ti
on
T
re
e
ke
rn
el
s
BUT-1 x x x x x x x
PolyUCOMP-RUN2 x x x x
ECNUCS-Run1 x x x
ECNUCS-Run2 x x x x x x x
PolyUCOMP-RUN1 x x x x
PolyUCOMP-RUN3 x x x x
UBC UOS-RUN1 x x x x x x x x x x x
UBC UOS-RUN2 x x x x x x x x x x x x
UBC UOS-RUN3 x x x x x x x x x x x x
Unitor-SVRegressor lin x x x x x x x
Unitor-SVRegressor rbf x x x x x x x
Total 4 7 3 7 7 4 11 3 11 11 4 4 2
Table 5: TYPED task: Resources and tools used by
the systems that submitted a description file. Leftmost
columns correspond to the resources, and rightmost to
tools, in alphabetic order.
core task CORE similar to the STS 2012 task, and
a new pilot on typed-similarity TYPED. We had 34
teams participate in both tasks submitting 89 system
runs for CORE and 14 system runs for TYPED, in
total amounting to a 103 system evaluations. CORE
uses datasets which are related to but different from
those used in 2012: news headlines, MT evalua-
tion data, gloss pairs. The best systems attained
correlations close to the human inter tagger corre-
lations. The TYPED task characterizes, for the first
time, the reasons why two items are deemed simi-
lar. The results on TYPED show that the training
data provided allowed systems to yield high corre-
lation scores, demonstrating the practical viability
of this new task. In the future, we are planning on
adding more nuanced evaluation data sets that in-
clude modality (belief, negation, permission, etc.)
and sentiment. Also given the success rate of the
TYPED task, however, the data in this pilot is rel-
atively structured, hence in the future we are inter-
ested in investigating identifying reasons why two
pairs of unstructured texts as those present in CORE
are deemed similar.
Acknowledgements
We are grateful to the OntoNotes team for sharing OntoNotes
to WordNet mappings (Hovy et al 2006). We thank Lan-
guage Weaver, INC, DARPA and LDC for providing the SMT
data. This work is also partially funded by the Spanish Ministry
of Education, Culture and Sport (grant FPU12/06243). This
a comprehensive list of evaluation tasks, datasets, software and
papers related to STS.
42
work was partially funded by the DARPA BOLT and DEFT pro-
grams.
We want to thank Nikolaos Aletras, German Rigau and
Mark Stevenson for their help designing, annotating and col-
lecting the typed-similarity data. The development of the
typed-similarity dataset was supported by the PATHS project
(http://paths-project.eu) funded by the European Community?s
Seventh Framework Program (FP7/2007-2013) under grant
agreement no. 270082. The tasks were partially financed by
the READERS project under the CHIST-ERA framework (FP7
ERA-Net). We thank Europeana and all contributors to Euro-
peana for sharing their content through the API.
References
Eneko Agirre and Enrique Amigo?. In prep. Exploring
evaluation measures for semantic textual similarity. In
Unpublished manuscript.
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. Semeval-2012 task 6: A
pilot on semantic textual similarity. In *SEM 2012:
The First Joint Conference on Lexical and Computa-
tional Semantics ? Volume 1: Proceedings of the main
conference and the shared task, and Volume 2: Pro-
ceedings of the Sixth International Workshop on Se-
mantic Evaluation (SemEval 2012), pages 385?393,
Montre?al, Canada, 7-8 June. Association for Compu-
tational Linguistics.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The berkeley framenet project. In COLING ?98
Proceedings of the 17th international conference on
Computational linguistics - Volume 1.
Daniel Bar, Chris Biemann, Iryna Gurevych, and Torsten
Zesch. 2012. Ukp: Computing semantic textual sim-
ilarity by combining multiple content similarity mea-
sures. In Proceedings of the 6th International Work-
shop on Semantic Evaluation, in conjunction with the
1st Joint Conference on Lexical and Computational
Semantics.
Clive Best, Erik van der Goot, Ken Blackler, Tefilo Gar-
cia, and David Horby. 2005. Europe media monitor -
system description. In EUR Report 22173-En, Ispra,
Italy.
Markus Dreyer and Daniel Marcu. 2012. Hyter:
Meaning-equivalent semantics for translation evalua-
tion. In Human Language Technologies: Conference
of the North American Chapter of the Association of
Computational Linguistics.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. Ontonotes:
The 90% solution. In Proceedings of the Human Lan-
guage Technology Conference of the North American
Chapter of the ACL.
W.H. Press, S.A. Teukolsky, W.T. Vetterling, and B.P.
Flannery. 2002. Numerical Recipes: The Art of Sci-
entific Computing V 2.10 With Linux Or Single-Screen
License. Cambridge University Press.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of Association for Machine Translation
in the Americas.
Frane S?aric?, Goran Glavas?, Mladen Karan, Jan S?najder,
and Bojana Dalbelo Bas?ic?. 2012. Takelab: Sys-
tems for measuring semantic text similarity. In Pro-
ceedings of the Sixth International Workshop on Se-
mantic Evaluation (SemEval 2012), pages 441?448,
Montre?al, Canada, 7-8 June. Association for Compu-
tational Linguistics.
43
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 81?91,
Dublin, Ireland, August 23-24, 2014.
SemEval-2014 Task 10: Multilingual Semantic Textual Similarity
Eneko Agirre
a
, Carmen Banea
b?
, Claire Cardie
c
, Daniel Cer
d
, Mona Diab
e?
Aitor Gonzalez-Agirre
a
, Weiwei Guo
f
, Rada Mihalcea
b
, German Rigau
a
, Janyce Wiebe
g
a
University of the Basque Country
Basque Country, Spain
b
University of Michigan
Ann Arbor, MI
c
Cornell University
Ithaca, NY
d
Google Inc.
Mountain View, CA
e
George Washington University
Washington, DC
f
Columbia University
New York, NY
g
University of Pittsburgh
Pittsburgh, PA
Abstract
In Semantic Textual Similarity, systems
rate the degree of semantic equivalence
between two text snippets. This year,
the participants were challenged with new
data sets for English, as well as the in-
troduction of Spanish, as a new language
in which to assess semantic similarity.
For the English subtask, we exposed the
systems to a diversity of testing scenar-
ios, by preparing additional OntoNotes-
WordNet sense mappings and news head-
lines, as well as introducing new gen-
res, including image descriptions, DEFT
discussion forums, DEFT newswire, and
tweet-newswire headline mappings. For
Spanish, since, to our knowledge, this is
the first time that official evaluations are
conducted, we used well-formed text, by
featuring sentences extracted from ency-
clopedic content and newswire. The an-
notations for both tasks leveraged crowd-
sourcing. The Spanish subtask engaged 9
teams participating with 22 system runs,
and the English subtask attracted 15 teams
with 38 system runs.
1 Introduction and motivation
Given two snippets of text, Semantic Textual Sim-
ilarity (STS) captures the notion that some texts
are more similar than others, measuring their de-
gree of semantic equivalence. Textual similar-
ity can range from complete unrelatedness to ex-
act semantic equivalence, and a graded similar-
ity intuitively captures the notion of intermediate
?
carmennb@umich.edu, mtdiab@gwu.edu
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
shades of similarity, as pairs of text may differ
from some minor nuanced aspects of meaning, to
relatively important semantic differences, to shar-
ing only some details, or to simply being related
to the same topic (cf. Section 2).
One of the goals of the STS task is to create a
unified framework for combining several seman-
tic components that otherwise have historically
tended to be evaluated independently and with-
out characterization of impact on NLP applica-
tions. By providing such a framework, STS al-
lows for an extrinsic evaluation of these modules.
Moreover, such an STS framework itself could in
turn be evaluated intrinsically and extrinsically as
a grey/black box within various NLP applications
such as Machine Translation (MT), Summariza-
tion, Generation, Question Answering (QA), etc.
STS is related to both Textual Entailment (TE)
and Paraphrasing, but differs in a number of ways
and it is more directly applicable to a number of
NLP tasks. STS is different from TE inasmuch
as it assumes bidirectional graded equivalence be-
tween the pair of textual snippets. In the case of
TE the equivalence is directional, e.g. a car is a
vehicle, but a vehicle is not necessarily a car. STS
also differs from both TE and Paraphrasing (in as
far as both tasks have been defined to date in the
literature) in that, rather than being a binary yes/no
decision (e.g. a vehicle is not a car), we define
STS to be a graded similarity notion (e.g. a ve-
hicle and a car are more similar than a wave and
a car). A quantifiable graded bidirectional notion
of textual similarity is useful for a myriad of NLP
tasks such as MT evaluation, information extrac-
tion, question answering, summarization, etc.
In 2012 we held the first pilot task at SemEval
2012, as part of the *SEM 2012 conference, with
great success: 35 teams participated with 88 sys-
tem runs (Agirre et al., 2012). In addition, we held
81
year dataset pairs source
2012 MSRpar 1500 newswire
2012 MSRvid 1500 videos
2012 OnWN 750 glosses
2012 SMTnews 750 MT eval.
2012 SMTeuroparl 750 MT eval.
2013 HDL 750 newswire
2013 FNWN 189 glosses
2013 OnWN 561 glosses
2013 SMT 750 MT eval.
2014 HDL 750 newswire headlines
2014 OnWN 750 glosses
2014 Deft-forum 450 forum posts
2014 Deft-news 300 news summary
2014 Images 750 image descriptions
2014 Tweet-news 750 tweet-news pairs
Table 2: English subtask: Summary of train (2012
and 2013) and test (2014) datasets.
a DARPA sponsored workshop at Columbia Uni-
versity.
1
In 2013, STS was selected as the offi-
cial Shared Task of the *SEM 2013 conference,
with two subtasks: The Core task, which is sim-
ilar to the 2012 task; and a Pilot task on Typed-
similarity between semi-structured records. The
Core task attracted 34 participants with 89 runs,
and the Typed-similarity task attracted 6 teams
with 14 runs.
For STS 2014 we defined two subtasks: En-
glish and Spanish. For the English subtask we pro-
vided five test datasets: two datasets that extend
already released genres (the OntoNotes-WordNet
sense mappings and news headlines) and three
new genres: image descriptions, DEFT discus-
sion forum data and newswire, as well as tweet-
newswire headline mappings. Participants could
use all datasets released in 2012 and 2013 as train-
ing data. The Spanish subtask introduced two di-
verse datasets on different genres, namely ency-
clopedic descriptions extracted from the Spanish
Wikipedia and contemporary Spanish newswire.
For the Spanish subtask, the participants had ac-
cess to a limited amount of labeled data, consist-
ing of 65 sentence pairs, which they could use for
training.
2 Task Description
2.1 English Subtask
The English dataset comprises pairs of news head-
lines (HDL), pairs of glosses (OnWN), image de-
scriptions (Images), DEFT-related discussion fo-
rums (Deft-forum) and news (Deft-news), and
1
http://www.cs.columbia.edu/
?
weiwei/
workshop/
tweet comments and newswire headline mappings
(Tweets).
For HDL, we used naturally occurring news
headlines gathered by the Europe Media Moni-
tor (EMM) engine (Best et al., 2005) from sev-
eral different news sources. EMM clusters to-
gether related news. Our goal was to generate
a balanced data set across the different similar-
ity ranges, hence we built two sets of headline
pairs: (i) a set where the pairs come from the same
EMM cluster, (ii) and another set where the head-
lines come from a different EMM cluster, then
we computed the string similarity between those
pairs. Accordingly, we sampled 375 headline pairs
of headlines that occur in the same EMM cluster,
aiming for pairs equally distributed between min-
imal and maximal similarity using simple string
similarity. We sampled other 375 pairs from the
different EMM cluster in the same manner.
For OnWN, we used the sense definition pairs
of OntoNotes (Hovy et al., 2006) and WordNet
(Fellbaum, 1998). Different from previous tasks,
the two definition sentences in a pair belong to dif-
ferent senses. We sampled 750 pairs based on a
string similarity ranging from 0.5 to 1.
The Images data set is a subset of the PAS-
CAL VOC-2008 data set (Rashtchian et al., 2010),
which consists of 1,000 images and has been used
by a number of image description systems. It was
also sampled from string similarity values between
0.6 and 1.
Deft-forum and Deft-news are from DEFT
data.
2
Deft-forum contains the forum post sen-
tences, and Deft-news are news summaries. We
selected 450 pairs for Deft-forum and 300 pairs for
Deft-news. They are sampled evenly from string
similarities falling in the interval 0.6 to 1.
The Tweets data set contains tweet-news pairs
selected from the corpus released in (Guo et al.,
2013), where each pair contains a sentence that
pertains to the news title, while the other one rep-
resents a Twitter comment on that particular news.
They are evenly sampled from string similarity
values between 0.5 and 1.
Table 1 shows the explanations and values as-
sociated with each score between 5 and 0. As
in prior years, we used Amazon Mechanical Turk
(AMT)
3
to crowdsource the annotation of the En-
glish pairs.
4
Annotators are presented with the
2
LDC2013E19, LDC2012E54
3
www.mturk.com
4
For STS 2013, we used CrowdFlower as a front-end to
82
Score English Spanish
5/4 The two sentences are completely equivalent, as they mean the same thing.
The bird is bathing in the sink.
Birdie is washing itself in the water basin.
El p?ajaro se esta ba?nando en el lavabo.
El p?ajaro se est?a lavando en el aguamanil.
4 The two sentences are mostly equivalent, but some unimportant details differ.
In May 2010, the troops attempted to invade
Kabul.
The US army invaded Kabul on May 7th last
year, 2010.
3 The two sentences are roughly equivalent, but some important information differs/missing.
John said he is considered a witness but not a
suspect.
?He is not a suspect anymore.? John said.
John dijo que ?el es considerado como testigo, y
no como sospechoso.
?
?
El ya no es un sospechoso,? John dijo.
2 The two sentences are not equivalent, but share some details.
They flew out of the nest in groups.
They flew into the nest together.
Ellos volaron del nido en grupos.
Volaron hacia el nido juntos.
1 The two sentences are not equivalent, but are on the same topic.
The woman is playing the violin.
The young lady enjoys listening to the guitar.
La mujer est?a tocando el viol??n.
La joven disfruta escuchar la guitarra.
0 The two sentences are completely dissimilar.
John went horse back riding at dawn with a
whole group of friends.
Sunrise at dawn is a magnificent view to take
in if you wake up early enough for it.
Al amanecer, Juan se fue a montar a caballo
con un grupo de amigos.
La salida del sol al amanecer es una magn??fica
vista que puede presenciar si usted se despierta
lo suficientemente temprano para verla.
Table 1: Similarity scores with explanations and examples for the English and Spanish subtasks, where
the sentences in Spanish are translations of the English ones.
A similarity score of 5 in English is mirrored by a maximum score of 4 in Spanish; the definitions pertaining to scores 3 and 4
in English were collapsed under a score of 3 in Spanish, with the definition ?The two sentences are mostly equivalent, but some
details differ.?
detailed instructions provided in Figure 1, and
are asked to label each STS sentence pair on our
six point scale, selecting from a dropdown box.
Five sentence pairs are presented to each annota-
tor at once, per human intelligence task (HIT), at
a payrate of $0.20; we collect five separate anno-
tations per sentence pair. Annotators were only el-
igible to work on the task if they had the Mechan-
ical Turk Master Qualification. This is a special
Amazon Mechanical Turk, since it provides numerous useful
tools to assist in running a successful annotation project using
crowdsourcing, such as support for hidden ?golden? questions
that can be used both to train annotators and to automatically
stop people who repeatedly make mistakes from contribut-
ing to the task. However, in 2013, CrowdFlower dropped
Amazon Mechanical Turk as an annotation source. When we
tried running pairs for STS 2014 on CrowdFlower using the
same templates that were successfully used for the 2013 task,
we found that we obtained significantly degraded annotation
quality, with an average Pearson (AMT provider vs. rest of
AMT providers) of only 22.8%. In contrast, when we ran the
task for 2014 on AMT, we obtained a one-vs-rest annotation
of 73.6%.
qualification conferred by AMT (using a priority
statistical model) to annotators who consistently
maintain a very high level of quality across a vari-
ety of tasks from numerous requesters). Access to
these skilled workers entails a 20% surcharge.
To monitor the quality of the annotations, we
use the gold dataset of 105 pairs that were manu-
ally annotated by the task organizers during STS
2013. We include one of these gold pairs in each
set of five sentence pairs, where the gold pairs are
indistinguishable from the rest. Unlike when we
ran on CrowdFlower for STS 2013, the gold pairs
are not used for training purposes, nor are workers
automatically banned from the task if they make
too many mistakes on annotating them. Rather, the
gold pairs are only used to help in identifying and
removing the data associated with poorly perform-
ing annotators. With few exceptions, 90% of the
answers from each individual annotator fall within
+/-1 of the answers selected by the organizers for
83
Figure 1: Annotation instructions for English subtask.
the gold dataset.
The distribution of scores obtained from the
AMT providers in the Deft-forum, Deft-news,
OnWN and tweet-news datasets is roughly uni-
form across the different grades of similarity, al-
though the scores are slightly higher for tweet-
news. Compared to the other data sets, the scores
for OnWN, were more bimodal, ranging between
4.6 to 5 and 0 to 0.4, when compared to middle
values (2.6-3.4).
In order to assess the annotation quality, we
measure the correlation of each annotator with the
average of the rest of the annotators, and then aver-
age the results. This approach to estimate the qual-
ity is identical to the method used for evaluations
(see Section 3), and it can thus be considered as
the upper bound of the systems. The inter-tagger
correlation for each English dataset is as follows:
? HDL: 79.4%
? OnWN: 67.2%
? Deft-forum: 58.6%
? Deft-news: 70.7%
? Images: 83.6%
? Tweets-news: 74.4%
The correlation figures are generally high (over
70%), with the exception of the OnWN and Deft
datasets, which score 67.2% and 58.6%, respec-
tively. The reason for the low inter-tagger correla-
tion on OnWN compared to the higher correlations
in previous years is that we only used unmapped
sense definitions, i.e., the two sentences in a pair
belong to two different senses. For the Deft-forum
dataset, we found that similarity values tend to be
lower than in the other datasets, and more annota-
tion disagreements happen in these low similarity
values.
2.2 Spanish Subtask
The Spanish subtask follows a setup similar to the
English subtask, except that the similarity scores
were adapted to fit a range from 0 to 4 (see Table
1). We thought that the distinction between a score
of 3 and 4 for the English task will pose more dif-
ficulty for us in conveying into Spanish, as the sole
difference between the two lies in how the annota-
tors perceive the importance of additional details
or missing information with respect to the core se-
mantic interpretation of the pair. As this aspect en-
tails a subjective judgement, and since it is the first
time that a Spanish STS evaluation is organized,
we casted the annotation guidelines into straight-
forward and unambiguous instructions, and thus
opted to use a similarity range from 0 to 4.
Prior to the evaluation window, we released 65
Spanish sentence pairs for trial / training. In or-
der to evaluate system performance under differ-
84
ent scenarios, we developed two test datasets, one
extracted from the Spanish Wikipedia
5
(December
2013 dump) and one from contemporary news ar-
ticles collected from media in Spanish (February
2014).
2.2.1 Spanish Wikipedia
The Wikipedia dump was processed using the
Parse::MediaWikiDump Perl library. We removed
all titles, html tags, wiki tags and hyperlinks
(keeping only the surface forms). Each article was
split into paragraphs, where the first paragraph
was considered to be the article?s abstract, while
the remaining ones were deemed to be its content.
Each of these were split into sentences using the
Perl library Uplug::PreProcess::SentDetect, and
only the sentences longer than eight words were
used. We iteratively computed the lexical simi-
larity
6
between every sentence in the abstract and
every sentence in the content, and retained those
pairs whose sentence length ratio was higher than
0.5, and their similarity scored over 0.35.
The final set of sentence pairs was split into five
bins, and their scores normalized to range from
0 to 1. The more interesting and difficult pairs
were found, perhaps not surprisingly, in bins 0 and
1, where synonyms/short paraphrases where more
frequent. An example extracted from those bins,
where the text in italics highlights the differences
between the two sentences:
? ?America? es el segundo continente m?as
grande del planeta, despu?es de Asia.
?America? is the second largest continent in the world,
following Asia.
? America corresponde a la segunda masa de
tierra m?as grande del planeta, luego de Asia.
America is the second largest land mass on the planet,
after Asia.
The Spanish verb ?Es? maps to (En:
7
is), ?cor-
responde a? (En: corresponds to), the phrase ?el
segundo continente? (En: the second continent) is
equivalent to ?la segunda masa de tierra? (the sec-
ond land mass), and ?despues? (En: following) to
?luego? (En: after). Despite the difference in vo-
cabulary choice, the two sentences are paraphrases
of each other.
From the candidate pairs, we manually selected
324 sentence pairs, in order to ensure a diverse
5
es.wikipedia.org
6
Algorithm based on the Linux diff command (Algo-
rithm::Diff Perl module).
7
?En? stands for English.
and challenging set. This set was annotated in two
ways, first by two graduate students in Computer
Science who are native speakers of Spanish, and
second by using AMT.
The AMT framework was set up to contain
seven sentence pairs per HIT, where six of them
were part of the test dataset, while one was used
for control. AMT providers were eligible to com-
plete a task if they had more than 500 accepted
HITs, with 90%+ acceptance rate.
8
We paid $0.30
per HIT, and each HIT was annotated by five AMT
providers. We sought to ensure that only Spanish
speaking annotators would complete the HITs by
providing all the information related to the task (its
title, abstract, description, guidelines and exam-
ples), as well as the control pair in Spanish only.
The participants were instructed to label the pairs
on a scale from 0 to 4 (see Table 1). Each sentence
pair was followed by a comment text box, which
the AMT providers used to provide the topic of the
sentences, corrections, etc.
The two students achieved a Pearson correla-
tion of 0.6974 on the Wikipedia dataset. To see
how their judgement compares to the crowd wis-
dom, we averaged the AMT scores for each pair,
and computed their correlation with our annota-
tors, obtaining 0.824 and 0.742, respectively. Sur-
prisingly enough, both these correlation values are
higher than the correlation among the annotators
themselves. When averaging the annotator scores
and comparing them with the AMT providers?
average score per pair, the correlation becomes
0.8546, indicating that the task is well defined,
and that the annotations contributed by the AMT
providers are of satisfactory quality. Given these
scores, the gold standard was annotated using the
average AMT provider judgement per pair.
2.2.2 Spanish News
The second Spanish dataset was extracted from
news articles published in Spanish language me-
dia from around the world in February 2014. The
hyperlinks to the articles were obtained by pars-
ing the ?International? page of Spanish Google
News,
9
which aggregates or clusters in real time
articles describing a particular event from a di-
verse pool of news sites, where each grouping
8
Initially, Amazon had automatically upgraded our anno-
tation task to require Master level providers (as those partici-
pating in the English annotations), yet after approximately 4
days, no HIT had been completed.
9
news.google.es
85
is labeled with the title of one of the predomi-
nant articles. By leveraging these clusters of links
pointing to the sites where the articles were orig-
inally published, we are able to gather raw text
that has a high probability of containing seman-
tically similar sentences. We encountered several
difficulties while mining the articles, ranging from
each article having its own formatting depend-
ing on the source site, to advertisements, cookie
requirements, to encoding for Spanish diacritics.
We used the lynx text-based browser,
10
which was
able to standardize the raw articles to a degree.
The output of the browser was processed using a
rule based approach taking into account continu-
ous text span length, ratio of symbols and num-
bers to the text, etc., in order to determine when
a paragraph is part of the article content. After
that, a second pass over the predictions corrected
mislabeled paragraphs if they were preceded and
followed by paragraphs identified as content. All
the content pertaining to articles on the same event
was joined, sentence split, and diff pairwise simi-
larities were computed. The set of candidate sen-
tences followed the same requirements as for the
Wikipedia dataset, namely length ratio higher than
0.5 and similarity score over 0.35. From these, we
manually extracted 480 sentence pairs which were
deemed to pose a challenge to an automated sys-
tem.
Due to the high correlations obtained between
the AMT providers? scores and the annotators?
scores on Wikipedia, the news dataset was only
annotated using AMT, following exactly the same
task setup as for Wikipedia.
3 Evaluation
Evaluation of STS is still an open issue.
STS experiments have traditionally used Pearson
product-moment correlation between the system
scores and the GS scores, or, alternatively, Spear-
man rank order correlation. In addition, we also
need a method to aggregate the results from each
dataset into an overall score. The analysis per-
formed in (Agirre and Amig?o, In prep) shows that
Pearson and averaging across datasets are the best
suited combination in general. In particular, Pear-
son is more informative than Spearman, in that
Spearman only takes the rank differences into ac-
count, while Pearson does account for value dif-
ferences as well. The study also showed that other
10
lynx.browser.org
alternatives need to be considered, depending on
the requirements of the target application.
We leave application-dependent evaluations for
future work, and focus on average Pearson correla-
tion. When averaging, we weight each individual
correlation by the size of the dataset. In order to
compute statistical significance among system re-
sults, we use a one-tailed parametric test based on
Fisher?s z-transformation (Press et al., 2002, equa-
tion 14.5.10). In addition, English subtask partic-
ipants could provide an optional confidence mea-
sure between 0 and 100 for each of their predic-
tions. Team RTM-DCU is the only one who has
provided these, and the evaluation of their runs us-
ing weighted Pearson (Pozzi et al., 2012) is listed
at the end of Table 3.
Participants
11
could take part in the shared task
with a maximum of 3 system runs per subtask.
3.1 English Subtask
In order to provide a simple word overlap baseline
(Baseline-tokencos), we tokenize the input sen-
tences splitting on white spaces, and then repre-
sent each sentence as a vector in the multidimen-
sional token space. Each dimension has 1 if the to-
ken is present in the sentence, 0 otherwise. Vector
similarity is computed using the cosine similarity
metric.
We also run the freely available system, Take-
Lab (
?
Sari?c et al., 2012), which yielded state of the
art performance in STS 2012 and strong results
out-of-the-box in 2013.
12
15 teams participated in the English subtask,
submitting 38 system runs. One team submitted
the results past the deadline, as explicitly marked
in Table 3. After the submission deadline expired,
the organizers published the gold standard and par-
ticipant submissions on the task website, in order
to ensure a transparent evaluation process.
Table 3 shows the results of the English sub-
task, with runs listed in alphabetical order. The
correlation in each dataset is given, followed
11
Participating teams: Bielefeld SC (McCrae et al.,
2013), BUAP (Vilari?no et al., 2014), DLS@CU (Sultan et
al., 2014b), FBK-TR (Vo et al., 2014), IBM EG (no in-
formation), LIPN (Buscaldi et al., 2014), Meerkat Mafia
(Kashyap et al., 2014), NTNU (Lynum et al., 2014), RTM-
DCU (Bic?ici and Way, 2014), SemantiKLUE (Proisi et al.,
2014), StanfordNLP (Socher et al., 2014), TeamZ (Gupta,
2014), UMCC DLSI SemSim (Chavez et al., 2014), UNAL-
NLP (Jimenez et al., 2014), UNED (Martinez-Romo et al.,
2011), UoW (Rios, 2014).
12
Code is available at http://ixa2.si.ehu.es/
stswiki
86
Run Name deft deft Headl images OnWN tweet Weighted mean Rank
forum news news
Baseline-tokencos 0.353 0.596 0.510 0.513 0.406 0.654 0.507 -
TakeLab 0.333 0.716 0.720 0.742 0.793 0.650 0.678 -
Bielefeld SC-run1 0.211 0.432 0.321 0.368 0.367 0.415 0.354 32
Bielefeld SC-run2 0.211 0.431 0.311 0.356 0.361 0.409 0.347 33
BUAP-EN-run1 0.456 0.686 0.689 0.697 0.654 0.771 0.671 19
DLS@CU-run1 0.483 0.766 0.765 0.821 0.723 0.764 0.734 7
DLS@CU-run2 0.483 0.766 0.765 0.821 0.859 0.764 0.761 1
FBK-TR-run1 0.322 0.523 0.547 0.601 0.661 0.462 0.535 25
FBK-TR-run2 0.167 0.421 0.485 0.521 0.572 0.359 0.441 28
FBK-TR-run3 0.305 0.405 0.471 0.489 0.551 0.438 0.459 27
IBM EG-run1 0.474 0.743 0.737 0.801 0.760 0.730 0.722 8
IBM EG-run2 0.464 0.641 0.710 0.747 0.732 0.696 0.684 15
LIPN-run1 0.454 0.640 0.653 0.809 - 0.551 0.508 26
LIPN-run2 0.084 - - - - - 0.010 35
Meerkat Mafia-Hulk 0.449 0.785 0.757 0.790 0.787 0.757 0.735 6
Meerkat Mafia-pairingWords 0.471 0.763 0.760 0.801 0.875 0.779 0.761 2
Meerkat Mafia-SuperSaiyan 0.492 0.771 0.767 0.768 0.802 0.765 0.741 5
NTNU-run1 0.437 0.714 0.722 0.800 0.835 0.411 0.663 20
NTNU-run2 0.508 0.766 0.753 0.813 0.777 0.792 0.749 4
NTNU-run3 0.531 0.781 0.784 0.834 0.850 0.675 0.755 3
SemantiKLUE-run1 0.337 0.608 0.728 0.783 0.848 0.632 0.687 14
SemantiKLUE-run2 0.349 0.643 0.733 0.773 0.855 0.640 0.694 13
StanfordNLP-run1 0.319 0.635 0.636 0.758 0.627 0.669 0.627 22
StanfordNLP-run2 0.304 0.679 0.621 0.715 0.625 0.636 0.610 24
StanfordNLP-run3 0.342 0.650 0.602 0.754 0.609 0.638 0.614 23
UMCC DLSI SemSim-run1 0.475 0.662 0.632 0.742 0.813 0.675 0.682 16
UMCC DLSI SemSim-run2 0.469 0.662 0.625 0.739 0.814 0.654 0.676 18
UMCC DLSI SemSim-run3 0.283 0.385 0.267 0.436 0.603 0.278 0.381 30
UNAL-NLP-run1 0.504 0.721 0.762 0.807 0.782 0.614 0.711 12
UNAL-NLP-run2 0.383 0.730 0.765 0.771 0.827 0.403 0.657 21
UNAL-NLP-run3 0.461 0.722 0.761 0.778 0.843 0.658 0.721 9
UNED-run22 p np 0.104 0.315 0.037 0.324 0.509 0.490 0.310 34
UNED-runS5K 10 np 0.118 0.506 0.057 0.498 0.488 0.579 0.379 31
UNED-runS5K 3 np 0.094 0.564 0.018 0.607 0.577 0.670 0.431 29
UoW-run1 0.342 0.751 0.754 0.776 0.799 0.737 0.714 11
UoW-run2 0.342 0.587 0.754 0.788 0.799 0.628 0.682 17
UoW-run3 0.342 0.763 0.754 0.788 0.799 0.753 0.721 10
?RTM-DCU-run1 0.434 0.697 0.620 0.699 0.806 0.688 0.671
?RTM-DCU-run2 0.397 0.681 0.613 0.666 0.799 0.669 0.651
?RTM-DCU-run3 0.308 0.556 0.630 0.647 0.800 0.553 0.608
?RTM-DCU-run1 0.418 0.685 0.622 0.698 0.833 0.687 0.673
?RTM-DCU-run2 0.383 0.674 0.609 0.663 0.826 0.669 0.653
?RTM-DCU-run3 0.273 0.553 0.633 0.644 0.825 0.568 0.611
Table 3: English evaluation results. Results at the top correspond to out-of-the-box systems. Results at
the bottom correspond to results using the confidence score.
Notes: ?-? for not submitted, ??? for post-deadline submission.
87
by the mean correlation (the official measure),
and the rank of the run. The highest correla-
tions are for OnWN (87.5%, by Meerkat Mafia)
and images (83.4%, by NTNU), followed by
Tweets (79.2%, by NTNU), HEADL (78.4%, by
NTNU) and deft news and forums (78.1% and
53.1%, respectively, by NTNU). Compared to the
inter-annotator agreement correlation, the ranking
among datasets is very similar, with the exception
of OnWN, as it gets the best score but has very low
agreement. One possible reason is that the partic-
ipants used previously available data. The results
of the best 4 top system runs are significantly dif-
ferent (p-value < 0.05) from the 5th top scoring
system run and below. The top 4 systems did not
show statistical significant variation among them.
Only three runs (cf. lower rows in Table 3) in-
cluded non-uniform confidence scores, barely af-
fecting their ranking.
Interestingly, the two top performing systems
on the English STS sub-task are both unsuper-
vised. DLS@CU (Sultan et al., 2014b) presents
an unsupervised algorithm which predicts the STS
score based on the proportion of word alignments
in the two sentences. Two related words are
aligned depending on how similar the two words
are, and also on how similar the contexts of the
words are in the respective sentences (Sultan et al.,
2014a). Meerkat Mafia pairingWords (Kashyap
et al., 2014) also follows a fully unsupervised ap-
proach. The authors train LSA on an English cor-
pus of three billion words using a sliding window
approach, resulting in a vocabulary size of 29,000
words associated with 300 dimensions. They ac-
count for named entities and out-of-vocabulary
words by leveraging external resources such as
DBpedia
13
and Wordnik.
14
In Spanish, the sys-
tem equivalent to this run ranked second following
a cross-lingual approach, by applying the English
system to the translated version of the dataset (see
3.2).
The Table also shows the results of TakeLab,
which was trained with all datasets from previ-
ous years. TakeLab would rank 18th, ten absolute
points below the best system, a smaller difference
than in 2013.
13
dbpedia.org
14
wordnick.com
3.2 Spanish Subtask
The Spanish subtask attracted 9 teams with 22
participating systems, out of which 16 were su-
pervised and 6 unsupervised. The participants
were from both Spanish (Colombia, Cuba, Mex-
ico, Spain), and non-Spanish speaking countries
(two teams from France, Germany, Ireland, UK,
US). The evaluation results appear in Table 4.
The top ranking system is the 2nd run of
UMCC DLSI SemSim (Chavez et al., 2014),
which achieves a weighted correlation of 0.807. It
entails a cross-lingual approach, as it leverages a
SVM-based English framework, by mapping the
Spanish words to their English equivalent using
the most common sense in WordNet 3.0. The clas-
sifier uses a combination of features, such as those
derived from traditional knowledge-based ((Lea-
cock and Chodorow, 1998; Wu and Palmer, 1994;
Lin, 1998), and others) and corpus-based metrics
(LSA (Landauer et al., 1997)), paired with lexi-
cal features (such as Dice-Similarity, Euclidean-
distance, etc.). It is trained on a cumulative En-
glish STS dataset comprising train and test data
released as part of tasks in SemEval2012 (Agirre
et al., 2012) and *Sem 2013 (Agirre et al., 2013),
as well as training data available from tasks 1 and
10 in SemEval 2014. Interestingly enough, run 2
of the system performs better than run 1, despite
the fact that it uses half the features, and focuses
on string based similarity measures only. This dif-
ference between runs is noticed on the Wikipedia
dataset only, and it amounts to 4% Pearson corre-
lation. While the system had a robust performance
on the Spanish subtask, for English, its overall
rank was 16, 18, and 33, respectively.
Coming in close at only 0.3% difference, is
Meerkat-Mafia PairingAvg (run 2) (Kashyap et
al., 2014), which also follows a cross-lingual ap-
proach, by applying the system the team devel-
oped for the English subtask to the translated ver-
sion of the datasets (see 3.1). The interesting as-
pect of their work is that in their first submission
(run 1), they only consider the similarity result-
ing from the sentence pair translation through the
Google Translate service.
15
In the second run,
they expand each sentence to 20 possible combi-
nations by accounting for the multiple translation
meanings of a given word, and considering the av-
erage similarity of all resulting pairs. While the
first run achieves a weighted correlation of 73.8%,
15
translate.google.com
88
Run Name System type Wikipedia News Weighted mean Rank
Bielefeld-SC-run1 unsupervised* 0.263 0.554 0.437 22
Bielefeld-SC-run2 unsupervised* 0.265 0.555 0.438 21
BUAP-run1 supervised 0.550 0.679 0.627 17
BUAP-run2 unsupervised 0.640 0.764 0.714 14
RTM-DCU-run1 supervised 0.422 0.700 0.588 18
RTM-DCU-run2 supervised 0.369 0.625 0.522 20
RTM-DCU-run3 supervised 0.424 0.641 0.554 19
LIPN-run1 supervised 0.652 0.826 0.756 11
LIPN-run2 supervised 0.716 0.832 0.785 6
LIPN-run3 supervised 0.716 0.809 0.771 10
Meerkat-Mafia-run1 unsupervised 0.668 0.785 0.738 13
Meerkat-Mafia-run2 unsupervised 0.743 0.845 0.804 2
Meerkat-Mafia-run3 supervised 0.738 0.822 0.788 5
TeamZ-run1 supervised 0.610 0.717 0.674 15
TeamZ-run2 supervised 0.604 0.710 0.667 16
UMCC-DLSI-run1 supervised 0.741 0.825 0.791 4
UMCC-DLSI-run2 supervised 0.7802 0.825 0.807 1
UNAL-NLP-run1 weakly supervised 0.7803 0.815 0.801 3
UNAL-NLP-run2 supervised 0.757 0.783 0.772 9
UNAL-NLP-run3 supervised 0.689 0.796 0.753 12
UoW-run1 supervised 0.748 0.800 0.779 7
UoW-run2 supervised 0.748 0.800 0.779 8
Table 4: Spanish evaluation results in terms of Pearson correlation.
the second one performs significantly better at
80.4%, indicating that the additional context may
also include multiple instances of accurate trans-
lations, hence significantly impacting the overall
similarity score. In English, the system equiva-
lent to run 2 in Spanish, namely Meerkat Mafia-
pairingWords, achieves a competitive ranked per-
formance across all six datasets, ranking second,
at an order of 10
?4
distance from the top sys-
tem. This supports the claim that, despite its unsu-
pervised nature, the system is quite versatile and
highly competitive with the top performing super-
vised frameworks, and that it may achieve an even
higher performance in Spanish if accurate sen-
tence translations were provided.
Overall, most systems were cross-lingual, rely-
ing on different translation approaches, such as 1)
translating the test data into English (as the two
systems above), and then exporting the score ob-
tained for the English sentences back to Spanish,
or 2) performing automatic translation of the En-
glish training data, and learning a classifier di-
rectly in Spanish. (Buscaldi et al., 2014) supple-
mented their training dataset with human annota-
tions conducted in Spanish, using definition pairs
extracted from a Spanish dictionary. A different
angle was explored by (Rios, 2014), who proposed
a multilingual framework using transfer learning
across English and Spanish by training on tradi-
tional lexical, knowledge-based and corpus-based
features. The semantic similarity task was ap-
proached from a monolingual perspective as well
(Gupta, 2014), by focusing on Spanish resources,
such as the trial data we released as part of the
subtask, and the Spanish WordNet;
16
these were
leveraged using meta-learning over variations of
overlap-based metrics. Following the same line,
(Bic?ici and Way, 2014) pursued language inde-
pendent methods, who avoided relying on task or
domain specific information through the usage of
referential translation machines. This approach
models textual semantic similarity as a decision in
terms of translation quality between two datasets
(in our case Spanish STS trial and test data) given
relevant examples from an in-language reference
corpus.
In comparison to the correlations obtained in the
English subtask, where the highest weighted mean
was 76.1%, for Spanish, we obtained 80.7%, prob-
ably due to the more formal nature of the datasets,
since Wikipedia and news articles employ mostly
well formed and grammatically correct sentences,
and we selected all snippets to be longer than 8
words. The overall correlation scores obtained for
English were hurt by the deft-forum data, which
scored significantly lower (at a maximum corre-
lation of 50.8%), when compared to all the other
datasets whose correlation was higher than 70%.
The OnWN data was most similar to our test sets,
and it attained a maximum of 85.9%.
16
grial.uab.es/descarregues.php
89
4 Conclusion
This year?s STS task comprised a multilingual
flair, by introducing Spanish datasets alongside the
English ones. In English, the datasets sought to ex-
pose the participating teams to more diverse sce-
narios compared to the previous years, by intro-
ducing image descriptions, forum and newswire
genre, and tweet-newswire headline mappings.
For Spanish, two datasets were developed consist-
ing of encyclopedic and newswire text acquired
from Spanish sources. Overall, the English sub-
task attracted 15 teams (with 38 system varia-
tions), while the Spanish subtask had 9 teams
(with 22 system runs). Most teams from the Span-
ish subtask have also submitted runs for the En-
glish evaluations.
Acknowledgments
The authors are grateful to Ver?onica P?erez-Rosas
and Vanessa Loza for their help with the anno-
tations for the Spanish subtask. This material is
based in part upon work supported by National
Science Foundation CAREER award #1361274
and IIS award #1018613, by DARPA-BAA-12-
47 DEFT grant #12475008, and by MINECO
CHIST-ERA READERS and SKATER projects
(PCIN-2013-002-C02-01, TIN2012-38584-C06-
02). Aitor Gonzalez Agirre is supported by a doc-
toral grant from MINECO. Any opinions, find-
ings, and conclusions or recommendations ex-
pressed in this material are those of the authors and
do not necessarily reflect the views of the National
Science Foundation or the Defense Advanced Re-
search Projects Agency.
References
Eneko Agirre and Enrique Amig?o. In prep. Exploring
evaluation measures for semantic textual similarity.
In Unpublished manuscript.
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. Semeval-2012 task 6: A
pilot on semantic textual similarity. In *SEM 2012:
The First Joint Conference on Lexical and Compu-
tational Semantics ? Volume 1: Proceedings of the
main conference and the shared task, and Volume 2:
Proceedings of the Sixth International Workshop on
Semantic Evaluation (SemEval 2012), pages 385?
393, Montr?eal, Canada, 7-8 June.
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. 2013. *SEM 2013 Shared
Task: Semantic textual similarity, including a pi-
lot on typed-similarity. In The Second Joint Con-
ference on Lexical and Computational Semantics
(*SEM 2013), pages 32?43.
Clive Best, Erik van der Goot, Ken Blackler, Tefilo
Garcia, and David Horby. 2005. Europe me-
dia monitor - system description. In EUR Report
22173-En, Ispra, Italy.
Ergun Bic?ici and Andy Way. 2014. RTM-DCU: Ref-
erential translation machines for semantic similarity.
In Proceedings of the 8th International Workshop on
Semantic Evaluation (SemEval-2014), Dublin, Ire-
land.
Davide Buscaldi, Jorge J. Garcia Flores, Joseph Le
Roux, Nadi Tomeh, and Belem Priego Sanchez.
2014. LIPN: Introducing a new geographical con-
text similarity measure and a statistical similarity
measure based on the Bhattacharyya coefficient. In
Proceedings of the 8th International Workshop on
Semantic Evaluation (SemEval-2014), Dublin, Ire-
land.
Alexander Chavez, Hector Davila, Yoan Gutierrez,
Antonio Fernandez-Orquin, Andr?es Montoyo, and
Rafael Munoz. 2014. UMCC DLSI SemSim: Mul-
tilingual system for measuring semantic textual sim-
ilarity. In Proceedings of the 8th International
Workshop on Semantic Evaluation (SemEval-2014),
Dublin, Ireland.
Christiane Fellbaum. 1998. WordNet - An electronic
lexical database. MIT Press.
Weiwei Guo, Hao Li, Heng Ji, and Mona Diab. 2013.
Linking tweets to news: A framework to enrich on-
line short text data in social media. In Proceedings
of the 51th Annual Meeting of the Association for
Computational Linguistics, pages 239?249.
Anubhav Gupta. 2014. TeamZ: Measuring semantic
textual similarity for Spanish using an overlap-based
approach. In Proceedings of the 8th International
Workshop on Semantic Evaluation (SemEval-2014),
Dublin, Ireland.
Eduard Hovy, Mitchell Marcus, Martha Palmer,
Lance Ramshaw, and Ralph Weischedel. 2006.
OntoNotes: The 90% solution. In Proceedings of
the Human Language Technology Conference of the
North American Chapter of the ACL, pages 57?60.
Sergio Jimenez, George Due?nas, Julia Baquero, and
Alexander Gelbukh. 2014. UNAL-NLP: Combin-
ing soft cardinality features for semantic textual sim-
ilarity, relatedness and entailment. In Proceedings
of the 8th International Workshop on Semantic Eval-
uation (SemEval-2014), Dublin, Ireland.
Abhay Kashyap, Lushan Han, Roberto Yus, Jennifer
Sleeman, Taneeya Satyapanich, Sunil Gandhi, and
Tim Finin. 2014. Meerkat Mafia: Multilingual and
cross-level semantic textual similarity systems. In
Proceedings of the 8th International Workshop on
90
Semantic Evaluation (SemEval-2014), Dublin, Ire-
land.
Thomas K. Landauer, Darrell Laham, Bob Rehder, and
M. E. Schreiner. 1997. How well can passage mean-
ing be derived without using word order? A compar-
ison of latent semantic analysis and humans. Cogni-
tive Science.
Claudia Leacock and Martin Chodorow. 1998. Com-
bining local context and WordNet similarity for
word sense identification. In WordNet: An Elec-
tronic Lexical Database, pages 305?332.
Dekang Lin. 1998. An information-theoretic defini-
tion of similarity. In Proceedings of the Fifteenth In-
ternational Conference on Machine Learning, pages
296?304, Madison, Wisconsin.
Andr?e Lynum, Partha Pakray, Bj?orn Gamb?ack, and
Sergio Jimenez. 2014. NTNU: Measuring se-
mantic similarity with sublexical feature represen-
tations and soft cardinality. In Proceedings of the
8th International Workshop on Semantic Evaluation
(SemEval-2014), Dublin, Ireland.
Juan Martinez-Romo, Lourdes Araujo, Javier Borge-
Holthoefer, Alex Arenas, Jos?e A. Capit?an, and
Jos?e A. Cuesta. 2011. Disentangling categori-
cal relationships through a graph of co-occurrences.
Phys. Rev. E, 84:046108, Oct.
John P. McCrae, Philipp Cimiano, and Roman Klinger.
2013. Orthonormal explicit topic analysis for cross-
lingual document matching. In Proceedings of the
2013 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1732?1740, Seattle,
Washington, USA.
Francesco Pozzi, Tiziana Di Matteo, and Tomaso Aste.
2012. Exponential smoothing weighted correla-
tions. The European Physical Journal B, 85(6).
William H. Press, Saul A. Teukolsky, William T. Vet-
terling, and Brian P. Flannery. 2002. Numerical
recipes: The art of scientific computing V 2.10 with
Linux or single-screen license. Cambridge Univer-
sity Press.
Thomas Proisi, Stefan Evert, Paul Greiner, and Besim
Kabashi. 2014. SemantiKLUE: Robust semantic
similarity at multiple levels using maximum weight
matching. In Proceedings of the 8th International
Workshop on Semantic Evaluation (SemEval-2014),
Dublin, Ireland.
Cyrus Rashtchian, Peter Young, Micah Hodosh, and
Julia Hockenmaier. 2010. Collecting image annota-
tions using Amazon?s Mechanical Turk. In Proceed-
ings of the NAACL HLT 2010 Workshop on Creating
Speech and Language Data with Amazon?s Mechan-
ical Turk, CSLDAMT ?10, pages 139?147, Strouds-
burg, PA, USA.
Miguel Rios. 2014. UoW: Multi-task learning Gaus-
sian process for semantic textual similarity. In Pro-
ceedings of the 8th International Workshop on Se-
mantic Evaluation (SemEval-2014), Dublin, Ireland.
Richard Socher, Andrej Karpathy, Quoc V. Le, Christo-
pher D. Manning, and Andrew Y. Ng. 2014.
Grounded compositional semantics for finding and
describing images with sentences. Transactions
of the Association for Computational Linguistics,
pages 207?218.
Md Arafat Sultan, Steven Bethard, and Tamara Sum-
ner. 2014a. Back to basics for monolingual align-
ment: Exploiting word similarity and contextual ev-
idence. Transactions of the Association for Compu-
tational Linguistics, 2:219?230.
Md Arafat Sultan, Steven Bethard, and Tamara Sum-
ner. 2014b. DLS@CU: Sentence similarity from
word aligment. In Proceedings of the 8th Interna-
tional Workshop on Semantic Evaluation (SemEval-
2014), Dublin, Ireland.
Darnes Vilari?no, David Pinto, Sa?ul Le?on, Mireya To-
var, and Beatriz Beltr?an. 2014. BUAP: Evaluating
features for multilingual and cross-level semantic
textual similarity. In Proceedings of the 8th Interna-
tional Workshop on Semantic Evaluation (SemEval-
2014), Dublin, Ireland.
Ngoc Phuoc An Vo, Tommaso Caselli, and Octavian
Popescu. 2014. FBK-TR: Applying SVM with
multiple linguistic features for cross-level semantic
similarity. In Proceedings of the 8th International
Workshop on Semantic Evaluation (SemEval-2014),
Dublin, Ireland.
Frane
?
Sari?c, Goran Glava?s, Mladen Karan, Jan
?
Snajder,
and Bojana Dalbelo Ba?si?c. 2012. Takelab: Sys-
tems for measuring semantic text similarity. In Pro-
ceedings of the Sixth International Workshop on Se-
mantic Evaluation (SemEval 2012), pages 441?448,
Montr?eal, Canada, 7-8 June.
Zhibiao Wu and Martha Palmer. 1994. Verbs seman-
tics and lexical selection. In Proceedings of the 32nd
annual meeting on Association for Computational
Linguistics, pages 133?138, Las Cruces, New Mex-
ico.
91
