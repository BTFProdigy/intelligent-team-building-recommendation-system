Proceedings of the ACL-08: HLT Workshop on Mobile Language Processing, pages 10?12,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
A Wearable Headset Speech-to-Speech Translation System 
 
 
Kriste Krstovski, Michael Decerbo, Rohit Prasad, David Stallard, Shirin Saleem, 
Premkumar Natarajan 
Speech and Language Processing Department 
BBN Technologies 
10 Moulton Street, Cambridge, MA, 02138 
{krstovski, mdecerbo, rprasad, stallard, ssaleem, prem}@bbn.com 
 
 
 
 
Abstract 
In this paper we present a wearable, headset 
integrated eyes- and hands-free speech-to-
speech (S2S) translation system. The S2S sys-
tem described here is configured for translin-
gual communication between English and 
colloquial Iraqi Arabic. It employs an n-gram 
speech recognition engine, a rudimentary 
phrase-based translator for translating recog-
nized Iraqi text, and a rudimentary text-to-
speech (TTS) synthesis engine for playing 
back the English translation. This paper de-
scribes the system architecture, the functional-
ity of its components, and the configurations 
of the speech recognition and machine transla-
tion engines.  
1 Background 
Humanitarian personnel, military personnel, and 
visitors in foreign countries often need to commu-
nicate with residents of a host country. Human in-
terpreters are inevitably in short supply, and 
training personnel to speak a new language is diffi-
cult. Under the DARPA TRANSTAC and Babylon 
programs, various teams have developed systems 
that enable two-way communication over a lan-
guage barrier (Waibel et al, 2003; Zhou et al, 
2004; Stallard et al, 2006). The two-way speech-
to-speech (S2S) translation systems seek, in prin-
ciple, to translate any utterance, by using general 
statistical models trained on large amounts of 
speech and text data.  
The performance and usability of such two-way 
speech-to-speech (S2S) translation systems is 
heavily dependent on the computational resources, 
such as processing power and memory, of the plat-
form they are running on. To enable open-ended 
conversation these S2S systems employ powerful 
but highly memory- and computation-intensive 
statistical speech recognition and machine transla-
tion models. Thus, at the very minimum they re-
quire the processing and memory configuration of 
common-of-the-shelf (COTS) laptops.  
Unfortunately, most laptops do not have a form 
factor that is suitable for mobile users. The size, 
weight, and shape of laptops render them unsuit-
able for handheld use. Moreover, simply carrying 
the laptop can be infeasible for users, such as mili-
tary personnel, who are already overburdened with 
other equipment. Embedded platforms, on the 
other hand, offer a more suitable form factor in 
terms of size and weight, but lack the computa-
tional resources required to run more open-ended 
2-way S2S systems. 
In previous work, Prasad et al (2007) reported 
on the development of a S2S system for Windows 
Mobile based handheld computers. To overcome 
the challenges posed by the limited resources of 
that platform, the PDA version of the S2S system 
was designed to be more constrained in terms of 
the ASR and MT vocabulary. As described in de-
tail in (Prasad et al, 2007), the PDA based S2S 
system configured for English/Iraqi S2S translation 
delivers fairly accurate translation at faster than 
real-time.  
In this paper, we present ongoing development 
work on an S2S system that runs on an even more 
constrained hardware platform; namely, a proces-
sor embedded in a wearable headset with just 32 
MB of memory. Compared to the PDA based sys-
10
tem described in (Prasad et al, 2007), the wearable 
system is designed for both eyes- and hands-free 
operation. The headset-integrated translation de-
vice described in this paper is configured for two-
way conversation in English/Iraqi. The target do-
main is the force protection, which includes sce-
narios of checkpoints, house searches, civil affairs, 
medical, etc.   
In what follows, we discuss the hardware and 
software details of the headset-integrated transla-
tion device. 
2 Hardware Platform  
The wearable S2S system described in this paper 
runs on a headset-integrated computational plat-
form developed by Integrated Wave Technologies, 
Inc. (IWT). The headset-integrated platform em-
ploys a 200 MHz StrongARM integer processor 
with a total of just 32MB RAM available for both 
the operating system and the translation software. 
The operating system currently running on the 
platform is Embedded Linux. 
There are two audio cards on the headset plat-
form for two-way communication through separate 
audio input and output channels. The default sound 
card uses the headset integrated close-talking mi-
crophone as an audio input and the second audio 
card can be used with an ambient microphone 
mounted on the device or an external microphone. 
In addition, each headset earpiece contains inner 
and outer set of speakers. The inner earpiece 
speakers are for the English speaking user who 
wears the headset, whereas the outer speakers are 
for the foreign language speaker who is not re-
quired to wear the headset. 
3 Software Architecture 
Depicted in Figure 1 is the software system archi-
tecture for the headset-integrated wearable S2S 
system. We are currently using a fixed-phrase Eng-
lish-to-Iraqi speech translation module from IWT 
for translating from English to Iraqi. In the Iraqi-
to-English (I2E) direction, we use an n-gram ASR 
engine to recognize Iraqi speech, a custom, phrase-
based ?micro translator? for translating Iraqi text to 
English text, and finally a TTS module for convert-
ing the English text into speech.  The rest of this 
paper focuses on the components of the Iraqi-to-
English translation module. 
 
Fixed point ASR Engine: The ASR engine uses 
phonetic hidden Markov models (HMM) with one 
or more forms of the following parameter tying: 
Phonetic-Tied Mixture (PTM), State-Tied Mixture 
(STM), and State-Clustered-Tied Mixture (SCTM) 
models. 
For the headset-integrated platform, we use a 
fixed-point ASR engine described in (Prasad et al, 
2007). As in (Prasad et al, 2007) for real-time per-
formance we use the compact PTM models in both 
recognition passes of our two-pass ASR decoder. 
Phrase-based Micro Translator: Phrase-based 
statistical machine translation (SMT) has been 
widely adopted as the translation engine in S2S 
systems. Such SMT engines require only a large 
corpus of bilingual sentence pairs to deliver robust 
performance on the domain of that corpus. How-
ever, phrase-based SMT engines require significant 
amount of memory, even when configured for me-
dium vocabulary tasks. Given the limited memory 
on the headset platform, we chose to develop in-
stead a phrase-based ?micro translator? module, 
which acts like a bottom-up parser. The micro-
translator uses translation rules derived from our 
phrase-based SMT engine. Rules are created auto-
matically by running the SMT engine on a small 
training corpus and recording the phrase pairs it 
used in decoding it. These phrase pairs then be-
come rules which are treated just as though they 
had been written by hand. The micro translator 
currently makes no use of probabilities.  Instead, as 
shown in Figure 2, for any given Arabic utterance, 
the translator greedily chooses the longest match-
ing source phrase that does not overlap a source 
phrase already chosen. The target phrases for these 
source phrases are then output as the translation. 
These target phrases come out in source-language 
 
Figure 1. Software architecture of the S2S system. 
 
 
11
order, as no language model is currently used for 
reordering.  
The micro translator currently consists of 1300 
rules and 2000 words. Its memory footprint is just 
32KB. This small memory footprint is achieved by 
representing the rules in binary format rather than 
text format.  
 
 
English Playback using TTS: To play the Eng-
lish translation to the headset user we developed a 
rudimentary TTS module. The TTS module parses 
the output of the I2E translator to extract each 
translated word. It then uses the list of extracted 
words to read the appropriate pre-recorded (or syn-
thesized) audio. Once the word pronunciations au-
dio files are read we splice the beginning and the 
end of the audio files to reduce the amount of si-
lence and concatenate them into a single file which 
is then played to the user on the inner earphone 
speakers. 
The total memory footprint of our current Iraqi 
to English translation module running on the head-
set-integrated platform is just 9MB. The current 
configuration of the translation module?s Iraqi 
ASR engine yields word error rate (WER) of 20% 
on test-set utterances without out-of-vocabulary 
(OOV) words.  
4 Conclusions and Future Work 
In this paper we have presented the initial setup of 
a speech-to-speech translation system configured 
for the headset platform. Our current work is fo-
cused on expanding the vocabulary of the Iraqi-to-
English translation module by exploiting the rich 
morphology of Iraqi Arabic. In particular, we are 
investigating the use of morphemes (prefix, stems, 
and suffixes) for expanding the effective vocabu-
lary of the headset translator. We are also develop-
ing use cases for performing a formal evaluation of 
both the usability and performance of the headset 
translator. 
References  
Alex Waibel, Ahmed Badran, Alan W Black, Robert 
Frederking, Donna Gates ,Alon Lavie, Lori Levin, 
Kevin Lenzo, Laura Mayfield Tomokiyo, J?urgen 
Reichert, Tanja Schultz, Dorcas Wallace, Monika 
Woszczyna and Jing Zhang. 2003. ?Speechalator: 
Two-way Speech-to-Speech Translation on a Con-
sumer PDA,? Proc. 8th European Conference on 
Speech Communication and Technology 
(EUROSPEECH 2003), Geneva, Switzerland. 
Bowen Zhou, Daniel D?echelotte and Yuqing Gao. 
2004. ?Two-way Speech-to-Speech Translation on 
Handheld Devices,? Proc. 8th International Confer-
ence on Spoken Language Processing, Jeju Island, 
Korea. 
David Stallard, Frederick Choi, Kriste Krstovski, Prem 
Natarajan and Shirin Saleem. 2006. ?A Hybrid 
Phrase-based/Statistical Speech Translation System,? 
Proc. The 9th International Conference on Spoken 
Language Processing (Interspeech 2006 - ICSLP), 
Pittsburg, PA. 
David Stallard, John Makhoul, Frederick Choi, Ehry 
Macrostie, Premkumar Natarajan, Richard Schwartz 
and Bushra Zawaydeh. 2003. ?Design and Evaluation 
of a Limited two-way  Speech Translator,? Proc. 8th 
European Conference on Speech Communication and 
Technology (EUROSPEECH 2003), Geneva, Swit-
zerland. 
Rohit Prasad, Kriste Krstovski, Frederick Choi, Shirin 
Saleem, Prem Natarajan, Michael Decerbo and David 
Stallard. 2007. ?Real-Time Speech-to-Speech Trans-
lation for PDAs,? Proc. IEEE International Confer-
ence on Portable Information Devices (IEEE Portable 
2007), Orlando, FL. 
 
 
 
 
Figure 2.  Decoding in micro translator. 
 
12
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 626?635,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Discriminative Sample Selection for Statistical Machine Translation?
Sankaranarayanan Ananthakrishnan, Rohit Prasad, David Stallard, and Prem Natarajan
Raytheon BBN Technologies
10 Moulton Street
Cambridge, MA, U.S.A.
{sanantha,rprasad,stallard,prem}@bbn.com
Abstract
Production of parallel training corpora for the
development of statistical machine translation
(SMT) systems for resource-poor languages
usually requires extensive manual effort. Ac-
tive sample selection aims to reduce the la-
bor, time, and expense incurred in produc-
ing such resources, attaining a given perfor-
mance benchmark with the smallest possible
training corpus by choosing informative, non-
redundant source sentences from an available
candidate pool for manual translation. We
present a novel, discriminative sample selec-
tion strategy that preferentially selects batches
of candidate sentences with constructs that
lead to erroneous translations on a held-out de-
velopment set. The proposed strategy supports
a built-in diversity mechanism that reduces
redundancy in the selected batches. Simu-
lation experiments on English-to-Pashto and
Spanish-to-English translation tasks demon-
strate the superiority of the proposed approach
to a number of competing techniques, such
as random selection, dissimilarity-based se-
lection, as well as a recently proposed semi-
supervised active learning strategy.
1 Introduction
Resource-poor language pairs present a significant
challenge to the development of statistical machine
translation (SMT) systems due to the latter?s depen-
dence on large parallel texts for training. Bilingual
human experts capable of producing the requisite
?Distribution Statement ?A? (Approved for Public Release,
Distribution Unlimited)
data resources are often in short supply, and the task
of preparing high-quality parallel corpora is labori-
ous and expensive. In light of these constraints, an
attractive strategy is to construct the smallest pos-
sible parallel training corpus with which a desired
performance benchmark may be achieved.
Such a corpus may be constructed by selecting the
most informative instances from a large collection
of source sentences for translation by a human ex-
pert, a technique often referred to as active learn-
ing. A SMT system trained with sentence pairs thus
generated is expected to perform significantly better
than if the source sentences were chosen using, say,
a na??ve random sampling strategy.
Previously, Eck et al (2005) described a selec-
tion strategy that attempts to maximize coverage by
choosing sentences with the highest proportion of
previously unseen n-grams. Depending on the com-
position of the candidate pool with respect to the
domain, this strategy may select irrelevant outliers.
They also described a technique based on TF-IDF to
de-emphasize sentences similar to those that have al-
ready been selected, thereby encouraging diversity.
However, this strategy is bootstrapped by random
initial choices that do not necessarily favor sentences
that are difficult to translate. Finally, they worked
exclusively with the source language and did not use
any SMT-derived features to guide selection.
Haffari et al (2009) proposed a number of fea-
tures, such as similarity to the seed corpus, transla-
tion probability, n-gram and phrase coverage, etc.,
that drive data selection. They also proposed a
model in which these features combine linearly to
predict a rank for each candidate sentence. The
626
top-ranked sentences are chosen for manual transla-
tion. However, this approach requires that the pool
have the same distributional characteristics as the
development sets used to train the ranking model.
Additionally, batches are chosen atomically. Since
similar or identical sentences in the pool will typi-
cally meet the selection criteria simultaneously, this
can have the undesired effect of choosing redundant
batches with low diversity.
The semi-supervised active learning strategy pro-
posed by Ananthakrishnan et al (2010) uses multi-
layer perceptrons (MLPs) to rank candidate sen-
tences based on various features, including domain
representativeness, translation difficulty, and batch
diversity. A greedy, incremental batch construction
technique encourages diversity. While this strat-
egy was shown to be superior to random as well
as n-gram based dissimilarity selection, its coarse
granularity (reducing a candidate sentence to a low-
dimensional feature vector for ranking) makes it un-
suitable for many situations. In particular, it is seen
to have little or no benefit over random selection
when there is no logical separation of the candidate
pool into ?in-domain? and ?out-of-domain? subsets.
This paper introduces a novel, active sample se-
lection technique that identifies translation errors on
a held-out development set, and preferentially se-
lects candidate sentences with constructs that are
incorrectly translated in the former. A discrimina-
tive pairwise comparator function, trained on the
ranked development set, is used to order candidate
sentences and pick sentences that provide maximum
potential reduction in translation error. The feature
functions that power the comparator are updated af-
ter each selection to encourage batch diversity. In
the following sections, we provide details of the pro-
posed sample selection approach, and describe sim-
ulation experiments that demonstrate its superiority
over a number of competing strategies.
2 Error-Driven Active Learning
Traditionally, unsupervised selection strategies have
dominated the active learning literature for natural
language processing (Hwa, 2004; Tang et al, 2002;
Shen et al, 2004). Sample selection for SMT has
followed a similar trend. The work of Eck et al
(2005) and most of the techniques proposed by Haf-
fari et al (2009) fall in this category. Notable ex-
ceptions include the linear ranking model of Haf-
fari et al (2009) and the semi-supervised selection
technique of Ananthakrishnan et al (2010), both of
which use one or more held-out development sets to
train and tune the sample selector. However, while
the former uses the posterior translation probability
and the latter, a sentence-level confidence score as
part of the overall selection strategy, current active
learning techniques for SMT do not explicitly target
the sources of error.
Error-driven active learning attempts to choose
candidate instances that potentially maximize error
reduction on a reference set (Cohn et al, 1996;
Meng and Lee, 2008). In the context of SMT, this
involves decoding a held-out development set with
an existing baseline (seed) SMT system. The selec-
tion algorithm is then trained to choose, from the
candidate pool, sentences containing constructs that
give rise to translation errors on this set. Assum-
ing perfect reference translations and word align-
ment in subsequent SMT training, these sentences
provide maximum potential reduction in translation
error with respect to the seed SMT system. It is a su-
pervised approach to sample selection. We assume
the following are available.
? A seed parallel corpus S for training the initial
SMT system.
? A candidate pool of monolingual source sen-
tences P from which samples must be selected.
? A held-out development set D for training the
selection algorithm and for tuning the SMT.
? A test set T for evaluating SMT performance.
We further make the following reasonable as-
sumptions: (a) the development set D and the test
set T are drawn from the same distribution and (b)
the candidate pool P consists of both in- and out-
of-domain source sentences, as well as an allowable
level of redundancy (similar or identical sentences).
Using translation errors on the development set to
drive sample selection has the following advantages
over previously proposed active learning strategies
for SMT.
? The seed training corpus S need not be derived
from the same distribution as D and T. The seed
SMT system can be trained with any available
627
parallel corpus for the specified language pair.
This is very useful if, as is often the case, lit-
tle or no in-domain training data is available to
bootstrap the SMT system. This removes a criti-
cal restriction present in the semi-supervised ap-
proach of Ananthakrishnan et al (2010).
? Sentences chosen are guaranteed to be relevant
to the domain, because selection is based on n-
grams derived from the development set. This
alleviates potential problems with approaches
suggested by Eck et al (2005) and several tech-
niques used by Haffari et al (2009), where ir-
relevant outliers may be chosen simply because
they contain previously unseen n-grams, or are
deemed difficult to translate.
? The proposed technique seeks to minimize
held-out translation error rather than maximize
training-set coverage. This is the more intuitive,
direct approach to sample selection for SMT.
? Diversity can be encouraged by preventing n-
grams that appear in previously selected sen-
tences from playing a role in choosing subse-
quent sentences. This provides an efficient alter-
native to the cumbersome ?batch diversity? fea-
ture proposed by Ananthakrishnan et al (2010).
The proposed implementation of error-driven ac-
tive learning for SMT, discriminative sample selec-
tion, is described in the following section.
3 Discriminative Sample Selection
The goal of active sample selection is to induce an
ordering of the candidate instances that satisfies an
objective criterion. Eck et al (2005) ordered can-
didate sentences based on the frequency of unseen
n-grams. Haffari et al (2009) induced a ranking
based on unseen n-grams, translation difficulty, etc.,
as well as one that attempted to incrementally max-
imize BLEU using two held-out development sets.
Ananthakrishnan et al (2010) attempted to order the
candidate pool to incrementally maximize source n-
gram coverage on a held-out development set, sub-
ject to difficulty and diversity constraints.
In the case of error-driven active learning, we at-
tempt to learn an ordering model based on errors
observed on the held-out development set D. We
achieve this in an innovative fashion by casting the
ranking problem as a pairwise sentence compari-
son problem. This approach, inspired by Ailon and
Mohri (2008), involves the construction of a binary
classifier functioning as a relational operator that can
be used to order the candidate sentences. The pair-
wise comparator is trained on an ordering of D that
ranks constituent sentences in decreasing order of
the number of translation errors. The comparator is
then used to rank the candidate pool in decreasing
order of potential translation error reduction.
3.1 Maximum-Entropy Pairwise Comparator
Given a pair of source sentences (u, v), we define,
adopting the notation of Ailon and Mohri (2008), the
pairwise comparator h(u, v) as follows:
h(u, v) =
{
1, u < v
0, u >= v
(1)
In Equation 1, the binary comparator h(u, v)
plays the role of the ?less than? (?<?) relational op-
erator, returning 1 if u is preferred to v in an or-
dered list, and 0 otherwise. As detailed in Ailon and
Mohri (2008), the comparator must satisfy the con-
straint that h(u, v) and h(v, u) be complementary,
i.e. h(u, v) + h(v, u) = 1 to avoid ambiguity. How-
ever, it need not satisfy the triangle inequality.
We implement h(u, v) as a combination of dis-
criminative maximum entropy classifiers triggered
by feature functions drawn from n-grams of u and v.
We define p(u, v) as the conditional posterior prob-
ability of the Bernoulli event u < v given (u, v) as
shown in Equation 2.
p(u, v) = Pr(u < v | u, v) (2)
In our implementation, p(u, v) is the output of
a binary maximum-entropy classifier trained on the
development set. However, this implementation
poses two problems.
First, if we use constituent n-grams of u and v
as feature functions to trigger the classifier, there is
no way to distinguish between (u, v) and (v, u) as
they will trigger the same feature functions. This
will result in identical values for p(u, v) and p(v, u),
a contradiction. We resolve this issue by intro-
ducing a set of ?complementary? feature functions,
which are formed by simply appending a recogniz-
able identifier to the existing n-gram feature func-
628
u: how are you
v: i am going
f(u) = {how:1, are:1, you:1, how*are:2, are*you:2, how*are*you:3}
f(v) = {i:1, am:1, going:1, i*am:2, am*going:2, i*am*going:3}
f ?(u) = {!how:1, !are:1, !you:1, !how*are:2, !are*you:2, !how*are*you:3}
f ?(v) = {!i:1, !am:1, !going:1, !i*am:2, !am*going:2, !i*am*going:3}
Table 1: Standard and complementary trigram feature functions for a source pair (u, v).
tions. Then, to evaluate p(u, v), for instance, we
invoke the classifier with standard feature functions
for u and complementary feature functions for v.
Similarly, p(v, u) is evaluated by triggering comple-
mentary feature functions for u and standard feature
functions for v. Table 1 illustrates this with a simple
example.
Note that each feature function is associated with
a real value, whose magnitude is an indicator of its
importance. In our implementation, an n-gram fea-
ture function (standard or complementary) receives
a value equal to its length. This is based on our intu-
ition that longer n-grams play a more important role
in dictating SMT performance.
Second, the introduction of complementary trig-
gers implies that evaluation of p(u, v) and p(v, u)
now involves disjoint sets of feature functions. Thus,
p(u, v) is not guaranteed to satisfy the complemen-
tarity condition imposed on h(u, v), and therefore
cannot directly be used as the binary pairwise com-
parator. We resolve this by normalizing across the
two possible permutations, as follows:
h?(u, v) = p(u, v)p(u, v) + p(v, u) (3)
h?(v, u) = p(v, u)p(u, v) + p(v, u) (4)
Since h?(u, v) + h?(v, u) = 1, the complemen-
tarity constraint is now satisfied, and h(u, v) is just
a binarized (thresholded) version of h?(u, v). Thus,
the binary pairwise comparator can be constructed
from the permuted classifier outputs.
3.2 Training the Pairwise Comparator
Training the maximum-entropy classifier for the
pairwise comparator requires a set of target labels
and input feature functions, both of which are de-
rived from the held-out development set D. We be-
gin by decoding the source sentences in D with the
seed SMT system, followed by error analysis using
the Translation Edit Rate (TER) measure (Snover
et al, 2006). TER measures translation quality by
computing the number of edits (insertions, substitu-
tions, and deletions) and shifts required to transform
a translation hypothesis to its corresponding refer-
ence. We then rank D in decreasing order of the
number of post-shift edits, i.e. the number of in-
sertions, substitutions, and deletions after the shift
operation is completed. Since shifts are often due to
word re-ordering issues within the SMT decoder (es-
pecially for phrase-based systems), we do not con-
sider them as errors for the purpose of ranking D.
Sentences at the top of the ordered list D? contain
the maximum number of translation errors.
For each pair of sentences (u, v) : u < v in D?,
we generate two training entries. The first, signify-
ing that u appears before v in D?, assigns the label
true to a trigger list consisting of standard feature
functions derived from u, and complementary fea-
ture functions derived from v. The second, reinforc-
ing this observation, assigns the label false to a trig-
ger list consisting of complementary feature func-
tions from u, and standard feature functions from v.
The labeled training set (feature:label pairs) for the
comparator can be expressed as follows:
?(u, v) ? D? : u < v,
{f(u) f ?(v)} : true
{f ?(u) f(v)} : false
Thus, if there are d sentences in D?, we obtain a
total of d(d? 1) labeled examples to train the com-
parator. We use the standard L-BFGS optimization
629
algorithm (Liu and Nocedal, 1989) to estimate the
parameters of the maximum entropy model.
3.3 Greedy Discriminative Selection
The discriminatively-trained pairwise comparator
can be used as a relational operator to sort the candi-
date pool P in decreasing order of potential transla-
tion error reduction. A batch of pre-determined size
K can then be selected from the top of this list to
augment the existing SMT training corpus. Assum-
ing the pool contains N candidate sentences, and
given a fast sorting algorithm such as Quicksort, the
complexity of this strategy is O(N logN). Batches
can be selected iteratively until a specified perfor-
mance threshold is achieved.
A potential downside of this approach reveals it-
self when there is redundancy in the candidate pool.
Since the batch is selected in a single atomic opera-
tion from the sorted candidates, and because similar
or identical sentences will typically occupy the same
range in the ordered list, it is likely that this approach
will result in batches with low diversity. Whereas
we desire diverse batches for better coverage and ef-
ficient use of manual translation resources. This is-
sue was previously addressed in Shen et al (2004) in
the context of named-entity recognition, where they
used a two-step procedure to first select the most in-
formative and representative samples, followed by a
diversity filter. Ananthakrishnan et al (2010) used a
greedy, incremental batch construction strategy with
an integrated, explicit batch diversity feature as part
of the ranking model. Based on these ideas, we de-
sign a greedy selection strategy using the discrimi-
native relational operator.
Rather than perform a full sort on P, we sim-
ply invoke the minh(u,v)(? ? ? ) function to find the
sentence that potentially minimizes translation er-
ror. The subscript indicates that our implementation
of this function utilizes the discriminative relational
operator trained on the development set D. The best
choice sentence s is then added to our batch at the
current position (we begin with an empty batch). We
then remove the standard and complementary fea-
ture functions f(s) and f ?(s) triggered by s from the
global pool of feature functions obtained from D,
so that they do not play a role in the selection of
subsequent sentences for the batch. Subsequently,
a candidate sentence that is similar or identical to
Algorithm 1 Greedy Discriminative Selection
B? ()
for k = 1 to K do
s? minh(u,v)(P)
B(k)? s
P? P? {s}
f(D)? f(D)? f(s)
f ?(D)? f ?(D)? f ?(s)
end for
return B
s will not be preferred, because the feature func-
tions that previously caused it to rank highly will
no longer trigger. Algorithm 1 summarizes our se-
lection strategy in pseudocode. Since each call to
minh(u,v)(? ? ? ) is O(N), the overall complexity of
greedy discriminative selection is O(K ?N).
4 Experiments and Results
We conduct a variety of simulation experiments
with multiple language pairs (English-Pashto and
Spanish-English) and different data configurations
in order to demonstrate the utility of discrimina-
tive sample selection in the context of resource-poor
SMT. We also compare the performance of the pro-
posed strategy to numerous competing active and
passive selection methods as follows:
? Random: Source sentences are uniformly sam-
pled from the candidate pool P.
? Similarity: Choose sentences from P with the
highest fraction of n-gram overlap with the seed
corpus S.
? Dissimilarity: Select sentences from P with the
highest proportion of n-grams not seen in the
seed corpus S (Eck et al, 2005; Haffari et al,
2009).
? Longest: Pick the longest sentences from the
candidate pool P.
? Semi-supervised: Semi-supervised active learn-
ing with greedy incremental selection (Anan-
thakrishnan et al, 2010).
? Discriminative: Choose sentences that po-
tentially minimize translation error using a
maximum-entropy pairwise comparator (pro-
posed method).
630
Identical low-resource initial conditions are ap-
plied to each selection strategy so that they may be
objectively compared. A very small seed corpus S is
sampled from the available parallel training data; the
remainder serves as the candidate pool. Following
the literature on active learning for SMT, our simula-
tion experiments are iterative. A fixed-size batch of
source sentences is constructed from the candidate
pool using one of the above selection strategies. We
then look up the corresponding translations from the
candidate targets (simulating an expert human trans-
lator), augment the seed corpus with the selected
data, and update the SMT system with the expanded
training corpus. The selected data are removed from
the candidate pool. This select-update cycle is then
repeated for either a fixed number of iterations or
until a specified performance benchmark is attained.
At each iteration, we decode the unseen test set T
with the most current SMT configuration and eval-
uate translation performance in terms of BLEU as
well as coverage (defined as the fraction of untrans-
latable source words in the target hypotheses).
We use a phrase-based SMT framework similar to
Koehn et al (2003) for all experiments.
4.1 English-Pashto Simulation
Our English-Pashto (E2P) data originates from a
two-way collection of spoken dialogues, and con-
sists of two parallel sub-corpora: a directional E2P
corpus and a directional Pashto-English (P2E) cor-
pus. Each sub-corpus has its own independent train-
ing, development, and test partitions. The direc-
tional E2P training, development, and test sets con-
sist of 33.9k, 2.4k, and 1.1k sentence pairs, respec-
tively. The directional P2E training set consists of
76.5k sentence pairs. The corpus was used as-is, i.e.
no length-based filtering or redundancy-reduction
(i.e. removal of duplicates, if any) was performed.
The test-set BLEU score with the baseline E2P SMT
system trained from all of the above data was 9.5%.
We obtained a seed training corpus by randomly
sampling 1,000 sentence pairs from the directional
E2P training partition. The remainder of this set, and
the entire reversed P2E training partition were com-
bined to create the pool (109.4k sentence pairs). In
the past, we have observed that the reversed direc-
tional P2E data gives very little performance gain
in the E2P direction even though its vocabulary is
similar, and can be considered ?out-of-domain? as
far as the E2P translation task is concerned. Thus,
our pool consists of 30% in-domain and 70% out-
of-domain sentence pairs, making for a challeng-
ing active learning problem. A pool training set of
10k source sentences is sampled from this collection
for the semi-supervised selection strategy, leaving us
with 99.4k candidate sentences, which we use for all
competing techniques. The data configuration used
in this simulation is identical to Ananthakrishnan et
al. (2010), allowing us to compare various strategies
under the same conditions. We simulated a total of
20 iterations with batches of 200 sentences each; the
original 1,000 sample seed corpus grows to 5,000
sentence pairs and the end of our simulation.
Figure 1(a) illustrates the variation in BLEU
scores across iterations for each selection strategy.
The proposed discriminative sample selection tech-
nique performs significantly better at every iteration
than random, similarity, dissimilarity, longest, and
semi-supervised active selection. At the end of 20
iterations, the BLEU score gained 3.21 points, a rel-
ative improvement of 59.3%. This was followed by
semi-supervised active learning, which improved by
2.66 BLEU points, a 49.2% relative improvement.
Table 2 summarizes the total number of words se-
lected by each strategy, as well as the total area
under the BLEU curve with respect to the base-
line. The latter, labeled BLEUarea and expressed in
percent-iterations, is a better measure of the over-
all performance of each strategy across all iterations
than comparing BLEU scores at the final iteration.
Figure 1(b) shows the variation in coverage (per-
centage of untranslatable source words in target
hypotheses) for each selection technique. Here,
discriminative sample selection was better than all
other approaches except longest-sentence selection.
4.2 Spanish-English Simulation
The Spanish-English (S2E) training corpus was
drawn from the Europarl collection (Koehn, 2005).
To prevent length bias in selection, the corpus was
filtered to only retain sentence pairs whose source
ranged between 7 and 15 words (excluding punc-
tuation). Additionally, redundancy was reduced by
removing all duplicate sentence pairs. After these
steps, we obtained approximately 253k sentence
pairs for training. The WMT10 held-out develop-
631
(a) Variation in BLEU (E2P)
(b) Variation in coverage (E2P)
Figure 1: Simulation results for E2P data selection.
632
(a) Variation in BLEU (S2E)
(b) Variation in coverage (S2E)
Figure 2: Simulation results for S2E data selection.
633
Method E2P size E2P BLEUarea S2E size S2E BLEUarea
Random 58.1k 26.4 26.5k 45.0
Similarity 30.7k 21.9 24.7k 13.2
Dissimilarity 39.2k 12.4 24.2k 54.9
Longest 173.0k 27.5 39.6k 48.3
Semi-supervised 80.0k 34.1 27.6k 45.6
Discriminative 109.1k 49.6 31.0k 64.5
Table 2: Source corpus size (in words) and BLEUarea after 20 sample selection iterations.
ment and test sets (2k and 2.5k sentence pairs, re-
spectively) were used to tune our system and eval-
uate performance. Note that this data configuration
is different from that of the E2P simulation in that
there is no logical separation of the training data into
?in-domain? and ?out-of-domain? sets. The baseline
S2E SMT system trained with all available data gave
a test-set BLEU score of 17.2%.
We randomly sampled 500 sentence pairs from
the S2E training partition to obtain a seed train-
ing corpus. The remainder, after setting aside an-
other 10k source sentences for training the semi-
supervised strategy, serves as the candidate pool. We
again simulated a total of 20 iterations, except in
this case, we used batches of 100 sentences in an at-
tempt to obtain smoother performance trajectories.
The training corpus grows from 500 sentence pairs
to 2,500 as the simulation progresses.
Variation in BLEU scores and coverage for the
S2E simulation are illustrated in Figures 2(a) and
2(b), respectively. Discriminative sample selection
outperformed all other selection techniques across
all iterations of the simulation. After 20 iterations,
we obtained a 4.51 point gain in BLEU, a rela-
tive improvement of 142.3%. The closest com-
petitor was dissimilarity-based selection, which im-
proved by 4.38 BLEU points, a 138.1% relative
improvement. The proposed method also outper-
formed other selection strategies in improving cov-
erage, with significantly better results especially in
the early iterations. Table 2 summarizes the number
of words chosen, and BLEUarea, for each strategy.
5 Conclusion and Future Directions
Building SMT systems for resource-poor language
pairs requires significant investment of labor, time,
and money for the development of parallel training
corpora. We proposed a novel, discriminative sam-
ple selection strategy that can help lower these costs
by choosing batches of source sentences from a large
candidate pool. The chosen sentences, in conjunc-
tion with their manual translations, provide signifi-
cantly better SMT performance than numerous com-
peting active and passive selection techniques.
Our approach hinges on a maximum-entropy pair-
wise comparator that serves as a relational operator
for comparing two source sentences. This allows us
to rank the candidate pool in decreasing order of po-
tential reduction in translation error with respect to
an existing seed SMT system. The discriminative
comparator is coupled with a greedy, incremental se-
lection technique that discourages redundancy in the
chosen batches. The proposed technique diverges
from existing work on active sample selection for
SMT in that it uses machine learning techniques in
an attempt to explicitly reduce translation error by
choosing sentences whose constituents were incor-
rectly translated in a held-out development set.
While the performance of competing strategies
varied across language pairs and data configurations,
discriminative sample selection proved consistently
superior under all test conditions. It provides a pow-
erful, flexible, data selection front-end for rapid de-
velopment of SMT systems. Unlike some selection
techniques, it is also platform-independent, and can
be used as-is with a phrase-based, hierarchical, syn-
tactic, or other SMT framework.
We have so far restricted our experiments to simu-
lations, obtaining expert human translations directly
from the sequestered parallel corpus. We are now
actively exploring the possibility of linking the sam-
ple selection front-end to a crowd-sourcing back-
end, in order to obtain ?non-expert? translations us-
ing a platform such as the Amazon Mechanical Turk.
634
References
Nir Ailon and Mehryar Mohri. 2008. An efficient reduc-
tion of ranking to classification. In COLT ?08: Pro-
ceedings of the 21st Annual Conference on Learning
Theory, pages 87?98.
Sankaranarayanan Ananthakrishnan, Rohit Prasad, David
Stallard, and Prem Natarajan. 2010. A semi-
supervised batch-mode active learning strategy for
improved statistical machine translation. In CoNLL
?10: Proceedings of the 14th International Conference
on Computational Natural Language Learning, pages
126?134, July.
David A. Cohn, Zoubin Ghahramani, and Michael I. Jor-
dan. 1996. Active learning with statistical models.
Journal of Artificial Intelligence Research, 4(1):129?
145.
Matthias Eck, Stephan Vogel, and Alex Waibel. 2005.
Low cost portability for statistical machine translation
based in N-gram frequency and TF-IDF. In Proceed-
ings of IWSLT, Pittsburgh, PA, October.
Gholamreza Haffari, Maxim Roy, and Anoop Sarkar.
2009. Active learning for statistical phrase-based ma-
chine translation. In NAACL ?09: Proceedings of Hu-
man Language Technologies: The 2009 Annual Con-
ference of the North American Chapter of the Associ-
ation for Computational Linguistics, pages 415?423,
Morristown, NJ, USA. Association for Computational
Linguistics.
Rebecca Hwa. 2004. Sample selection for statistical
parsing. Computational Linguistics, 30:253?276.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In NAACL
?03: Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology,
pages 48?54, Morristown, NJ, USA. Association for
Computational Linguistics.
Philipp Koehn. 2005. Europarl: A parallel corpus
for statistical machine translation. In MT Summit X:
Proceedings of the 10th Machine Translation Summit,
pages 79?86.
D. C. Liu and J. Nocedal. 1989. On the limited mem-
ory bfgs method for large scale optimization. Math.
Program., 45(3):503?528.
Qinggang Meng and Mark Lee. 2008. Error-driven
active learning in growing radial basis function net-
works for early robot learning. Neurocomputing, 71(7-
9):1449?1461.
Dan Shen, Jie Zhang, Jian Su, Guodong Zhou, and Chew-
Lim Tan. 2004. Multi-criteria-based active learning
for named entity recognition. In ACL ?04: Proceed-
ings of the 42nd Annual Meeting on Association for
Computational Linguistics, pages 589?596, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings AMTA, pages 223?231, August.
Min Tang, Xiaoqiang Luo, and Salim Roukos. 2002. Ac-
tive learning for statistical natural language parsing.
In ACL ?02: Proceedings of the 40th Annual Meeting
on Association for Computational Linguistics, pages
120?127, Morristown, NJ, USA. Association for Com-
putational Linguistics.
635
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 445?449,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
On-line Language Model Biasing for Statistical Machine Translation
Sankaranarayanan Ananthakrishnan, Rohit Prasad and Prem Natarajan
Raytheon BBN Technologies
Cambridge, MA 02138, U.S.A.
{sanantha,rprasad,pnataraj}@bbn.com
Abstract
The language model (LM) is a critical com-
ponent in most statistical machine translation
(SMT) systems, serving to establish a proba-
bility distribution over the hypothesis space.
Most SMT systems use a static LM, inde-
pendent of the source language input. While
previous work has shown that adapting LMs
based on the input improves SMT perfor-
mance, none of the techniques has thus far
been shown to be feasible for on-line sys-
tems. In this paper, we develop a novel mea-
sure of cross-lingual similarity for biasing the
LM based on the test input. We also illustrate
an efficient on-line implementation that sup-
ports integration with on-line SMT systems by
transferring much of the computational load
off-line. Our approach yields significant re-
ductions in target perplexity compared to the
static LM, as well as consistent improvements
in SMT performance across language pairs
(English-Dari and English-Pashto).
1 Introduction
While much of the focus in developing a statistical
machine translation (SMT) system revolves around
the translation model (TM), most systems do not
emphasize the role of the language model (LM). The
latter generally follows a n-gram structure and is es-
timated from a large, monolingual corpus of target
sentences. In most systems, the LM is independent
of the test input, i.e. fixed n-gram probabilities de-
termine the likelihood of all translation hypotheses,
regardless of the source input.
The views expressed are those of the author and do not reflect the official policy or position of
the Department of Defense or the U.S. Government.
Some previous work exists in LM adaptation for
SMT. Snover et al (2008) used a cross-lingual infor-
mation retrieval (CLIR) system to select a subset of
target documents ?comparable? to the source docu-
ment; bias LMs estimated from these subsets were
interpolated with a static background LM. Zhao
et al (2004) converted initial SMT hypotheses to
queries and retrieved similar sentences from a large
monolingual collection. The latter were used to
build source-specific LMs that were then interpo-
lated with a background model. A similar approach
was proposed by Kim (2005). While feasible in off-
line evaluations where the test set is relatively static,
the above techniques are computationally expensive
and therefore not suitable for low-latency, interac-
tive applications of SMT. Examples include speech-
to-speech and web-based interactive translation sys-
tems, where test inputs are user-generated and pre-
clude off-line LM adaptation.
In this paper, we present a novel technique for
weighting a LM corpus at the sentence level based
on the source language input. The weighting scheme
relies on a measure of cross-lingual similarity evalu-
ated by projecting sparse vector representations of
the target sentences into the space of source sen-
tences using a transformation matrix computed from
the bilingual parallel data. The LM estimated from
this weighted corpus boosts the probability of rele-
vant target n-grams, while attenuating unrelated tar-
get segments. Our formulation, based on simple
ideas in linear algebra, alleviates run-time complex-
ity by pre-computing the majority of intermediate
products off-line.
Distribution Statement ?A? (Approved for Public Release, Distribution Unlimited)
445
2 Cross-Lingual Similarity
We propose a novel measure of cross-lingual simi-
larity that evaluates the likeness between an arbitrary
pair of source and target language sentences. The
proposed approach represents the source and target
sentences in sparse vector spaces defined by their
corresponding vocabularies, and relies on a bilingual
projection matrix to transform vectors in the target
language space to the source language space.
Let S = {s1, . . . , sM} and T = {t1, . . . , tN} rep-
resent the source and target language vocabularies.
Let u represent the candidate source sentence in a
M -dimensional vector space, whose mth dimension
um represents the count of vocabulary item sm in the
sentence. Similarly, v represents the candidate tar-
get sentence in a N -dimensional vector space. Thus,
u and v are sparse term-frequency vectors. Tra-
ditionally, the cosine similarity measure is used to
evaluate the likeness of two term-frequency repre-
sentations. However, u and v lie in different vector
spaces. Thus, it is necessary to find a projection of
v in the source vocabulary vector space before sim-
ilarity can be evaluated.
Assuming we are able to compute a M ? N -
dimensional bilingual word co-occurrence matrix ?
from the SMT parallel corpus, the matrix-vector
product u? = ?v is a projection of the target sen-
tence in the source vector space. Those source terms
of the M -dimensional vector u? will be emphasized
that most frequently co-occur with the target terms
in v. In other words, u? can be interpreted as a ?bag-
of-words? translation of v.
The cross-lingual similarity between the candi-
date source and target sentences then reduces to the
cosine similarity between the source term-frequency
vector u and the projected target term-frequency
vector u?, as shown in Equation 2.1:
S(u,v) = 1?u??u??u
T u?
= 1?u???v?u
T?v (2.1)
In the above equation, we ensure that both u and
u? are normalized to unit L2-norm. This prevents
over- or under-estimation of cross-lingual similarity
due to sentence length mismatch.
We estimate the bilingual word co-occurrence
matrix ? from an unsupervised, automatic word
alignment induced over the parallel training corpus
P. We use the GIZA++ toolkit (Al-Onaizan et al,
1999) to estimate the parameters of IBM Model
4 (Brown et al, 1993), and combine the forward
and backward Viterbi alignments to obtain many-to-
many word alignments as described in Koehn et al
(2003). The (m,n)th entry ?m,n of this matrix is
the number of times source word sm aligns to target
word tn in P.
3 Language Model Biasing
In traditional LM training, n-gram counts are evalu-
ated assuming unit weight for each sentence. Our
approach to LM biasing involves re-distributing
these weights to favor target sentences that are ?sim-
ilar? to the candidate source sentence according to
the measure of cross-lingual similarity developed in
Section 2. Thus, n-grams that appear in the trans-
lation hypothesis for the candidate input will be as-
signed high probability by the biased LM, and vice-
versa.
Let u be the term-frequency representation of the
candidate source sentence for which the LM must be
biased. The set of vectors {v1, . . . ,vK} similarly
represent the K target LM training sentences. We
compute the similarity of the source sentence u to
each target sentence vj according to Equation 3.1:
?j = S(u,vj)
= 1?u???vj?
uT?vj (3.1)
The biased LM is estimated by weighting n-gram
counts collected from the jth target sentence with
the corresponding cross-lingual similarity ?j . How-
ever, this is computationally intensive because: (a)
LM corpora usually consist of hundreds of thou-
sands or millions of sentences; ?j must be eval-
uated at run-time for each of them, and (b) the
entire LM must be re-estimated at run-time from
n-gram counts weighted by sentence-level cross-
lingual similarity.
In order to alleviate the run-time complexity of
on-line LM biasing, we present an efficient method
for obtaining biased counts of an arbitrary target
446
n-gram t. We define ct =
[
c1t , . . . , cKt
]T to be
the indicator-count vector where cjt is the unbi-
ased count of t in target sentence j. Let ? =
[?1, . . . , ?K ]T be the vector representing cross-
lingual similarity between the candidate source sen-
tence and each of the K target sentences. Then, the
biased count of this n-gram, denoted by C?(t), is
given by Equation 3.2:
C?(t) = cTt ?
=
K
?
j=1
1
?u???vj?
cjtuT?vj
= 1?u?u
T
K
?
j=1
1
??vj?
cjt?vj
= 1?u?u
Tbt (3.2)
The vector bt can be interpreted as the projection
of target n-gram t in the source space. Note that bt is
independent of the source input u, and can therefore
be pre-computed off-line. At run-time, the biased
count of any n-gram can be obtained via a simple
dot product. This adds very little on-line time com-
plexity because u is a sparse vector. Since bt is tech-
nically a dense vector, the space complexity of this
approach may seem very high. In practice, the mass
of bt is concentrated around a very small number of
source words that frequently co-occur with target n-
gram t; thus, it can be ?sparsified? with little or no
loss of information by simply establishing a cutoff
threshold on its elements. Biased counts and proba-
bilities can be computed on demand for specific n-
grams without re-estimating the entire LM.
4 Experimental Results
We measure the utility of the proposed LM bias-
ing technique in two ways: (a) given a parallel test
corpus, by comparing source-conditional target per-
plexity with biased LMs to target perplexity with the
static LM, and (b) by comparing SMT performance
with static and biased LMs. We conduct experi-
ments on two resource-poor language pairs commis-
sioned under the DARPA Transtac speech-to-speech
translation initiative, viz. English-Dari (E2D) and
English-Pashto (E2P), on test sets with single as well
as multiple references.
Data set E2D E2P
TM Training 138k pairs 168k pairs
LM Training 179k sentences 302k sentences
Development 3,280 pairs 2,385 pairs
Test (1-ref) 2,819 pairs 1,113 pairs
Test (4-ref) - 564 samples
Table 1: Data configuration for perplexity/SMT experi-
ments. Multi-reference test set is not available for E2D.
LM training data in words: 2.4M (Dari), 3.4M (Pashto)
4.1 Data Configuration
Parallel data were made available under the Transtac
program for both language pairs evaluated in this pa-
per. We divided these into training, held-out devel-
opment, and test sets for building, tuning, and evalu-
ating the SMT system, respectively. These develop-
ment and test sets provide only one reference trans-
lation for each source sentence. For E2P, DARPA
has made available to all program participants an
additional evaluation set with multiple (four) refer-
ences for each test input. The Dari and Pashto mono-
lingual corpora for LM training are a superset of tar-
get sentences from the parallel training corpus, con-
sisting of additional untranslated sentences, as well
as data derived from other sources, such as the web.
Table 1 lists the corpora used in our experiments.
4.2 Perplexity Analysis
For both Dari and Pashto, we estimated a static
trigram LM with unit sentence level weights that
served as a baseline. We tuned this LM by varying
the bigram and trigram frequency cutoff thresholds
to minimize perplexity on the held-out target sen-
tences. Finally, we evaluated test target perplexity
with the optimized baseline LM.
We then applied the proposed technique to es-
timate trigram LMs biased to source sentences in
the held-out and test sets. We evaluated source-
conditional target perplexity by computing the to-
tal log-probability of all target sentences in a par-
allel test corpus against the LM biased by the cor-
responding source sentences. Again, bigram and
trigram cutoff thresholds were tuned to minimize
source-conditional target perplexity on the held-out
set. The tuned biased LMs were used to compute
source-conditional target perplexity on the test set.
447
Eval set Static Biased Reduction
E2D-1ref-dev 159.3 137.7 13.5%
E2D-1ref-tst 178.3 156.3 12.3%
E2P-1ref-dev 147.3 130.6 11.3%
E2P-1ref-tst 122.7 108.8 11.3%
Table 2: Reduction in perplexity using biased LMs.
Witten-Bell discounting was used for smoothing
all LMs. Table 2 summarizes the reduction in target
perplexity using biased LMs; on the E2D and E2P
single-reference test sets, we obtained perplexity re-
ductions of 12.3% and 11.3%, respectively. This in-
dicates that the biased models are significantly better
predictors of the corresponding target sentences than
the static baseline LM.
4.3 Translation Experiments
Having determined that target sentences of a parallel
test corpus better fit biased LMs estimated from the
corresponding source-weighted training corpus, we
proceeded to conduct SMT experiments on both lan-
guage pairs to demonstrate the utility of biased LMs
in improving translation performance.
We used an internally developed phrase-based
SMT system, similar to Moses (Koehn et al, 2007),
as a test-bed for our translation experiments. We
used GIZA++ to induce automatic word alignments
from the parallel training corpus. Phrase translation
rules (up to a maximum source span of 5 words)
were extracted from a combination of forward and
backward word alignments (Koehn et al, 2003).
The SMT decoder uses a log-linear model that com-
bines numerous features, including but not limited to
phrase translation probability, LM probability, and
distortion penalty, to estimate the posterior proba-
bility of target hypotheses. We used minimum error
rate training (MERT) (Och, 2003) to tune the feature
weights for maximum BLEU (Papineni et al, 2001)
on the development set. Finally, we evaluated SMT
performance on the test set in terms of BLEU and
TER (Snover et al, 2006).
The baseline SMT system used the static trigram
LM with cutoff frequencies optimized for minimum
perplexity on the development set. Biased LMs
(with n-gram cutoffs tuned as above) were estimated
for all source sentences in the development and test
Test set BLEU 100-TER
Static Biased Static Biased
E2D-1ref-tst 14.4 14.8 29.6 30.5
E2P-1ref-tst 13.0 13.3 28.3 29.4
E2P-4ref-tst 25.6 26.1 35.0 35.8
Table 3: SMT performance with static and biased LMs.
sets, and were used to decode the corresponding in-
puts. Table 3 summarizes the consistent improve-
ment in BLEU/TER across multiple test sets and
language pairs.
5 Discussion and Future Work
Existing methods for target LM biasing for SMT
rely on information retrieval to select a comparable
subset from the training corpus. A foreground LM
estimated from this subset is interpolated with the
static background LM. However, given the large size
of a typical LM corpus, these methods are unsuitable
for on-line, interactive SMT applications.
In this paper, we proposed a novel LM biasing
technique based on linear transformations of target
sentences in a sparse vector space. We adopted a
fine-grained approach, weighting individual target
sentences based on the proposed measure of cross-
lingual similarity, and by using the entire, weighted
corpus to estimate a biased LM. We then sketched an
implementation that improves the time and space ef-
ficiency of our method by pre-computing and ?spar-
sifying? n-gram projections off-line during the train-
ing phase. Thus, our approach can be integrated
within on-line, low-latency SMT systems. Finally,
we showed that biased LMs yield significant reduc-
tions in target perplexity, and consistent improve-
ments in SMT performance.
While we used phrase-based SMT as a test-bed
for evaluating translation performance, it should be
noted that the proposed LM biasing approach is in-
dependent of SMT architecture. We plan to test its
effectiveness in hierarchical and syntax-based SMT
systems. We also plan to investigate the relative
usefulness of LM biasing as we move from low-
resource languages to those for which significantly
larger parallel corpora and LM training data are
available.
448
References
Yaser Al-Onaizan, Jan Curin, Michael Jahr, Kevin
Knight, John Lafferty, Dan Melamed, Franz Josef Och,
David Purdy, Noah A. Smith, and David Yarowsky.
1999. Statistical machine translation: Final report.
Technical report, JHU Summer Workshop.
Peter E. Brown, Vincent J. Della Pietra, Stephen A.
Della Pietra, and Robert L. Mercer. 1993. The math-
ematics of statistical machine translation: parameter
estimation. Computational Linguistics, 19:263?311.
Woosung Kim. 2005. Language Model Adaptation for
Automatic Speech Recognition and Statistical Machine
Translation. Ph.D. thesis, The Johns Hopkins Univer-
sity, Baltimore, MD.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In NAACL
?03: Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology,
pages 48?54, Morristown, NJ, USA. Association for
Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondr?ej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the ACL on Inter-
active Poster and Demonstration Sessions, ACL ?07,
pages 177?180, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In ACL ?03: Pro-
ceedings of the 41st Annual Meeting on Association
for Computational Linguistics, pages 160?167, Mor-
ristown, NJ, USA. Association for Computational Lin-
guistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. BLEU: A method for automatic
evaluation of machine translation. In ACL ?02: Pro-
ceedings of the 40th Annual Meeting on Association
for Computational Linguistics, pages 311?318, Mor-
ristown, NJ, USA. Association for Computational Lin-
guistics.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings AMTA, pages 223?231, August.
Matthew Snover, Bonnie Dorr, and Richard Schwartz.
2008. Language and translation model adaptation us-
ing comparable corpora. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, EMNLP ?08, pages 857?866, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Bing Zhao, Matthias Eck, and Stephan Vogel. 2004.
Language model adaptation for statistical machine
translation with structured query models. In Proceed-
ings of the 20th international conference on Compu-
tational Linguistics, COLING ?04, Stroudsburg, PA,
USA. Association for Computational Linguistics.
449
Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 126?134,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
A Semi-Supervised Batch-Mode Active Learning Strategy for Improved
Statistical Machine Translation
Sankaranarayanan Ananthakrishnan, Rohit Prasad, David Stallard and Prem Natarajan
BBN Technologies
10 Moulton Street
Cambridge, MA, U.S.A.
{sanantha,rprasad,stallard,prem}@bbn.com
Abstract
The availability of substantial, in-domain
parallel corpora is critical for the develop-
ment of high-performance statistical ma-
chine translation (SMT) systems. Such
corpora, however, are expensive to pro-
duce due to the labor intensive nature of
manual translation. We propose to al-
leviate this problem with a novel, semi-
supervised, batch-mode active learning
strategy that attempts to maximize in-
domain coverage by selecting sentences,
which represent a balance between domain
match, translation difficulty, and batch di-
versity. Simulation experiments on an
English-to-Pashto translation task show
that the proposed strategy not only outper-
forms the random selection baseline, but
also traditional active learning techniques
based on dissimilarity to existing training
data. Our approach achieves a relative im-
provement of 45.9% in BLEU over the
seed baseline, while the closest competitor
gained only 24.8% with the same number
of selected sentences.
1 Introduction
Rapid development of statistical machine transla-
tion (SMT) systems for resource-poor language
pairs is a problem of significant interest to the
research community in academia, industry, and
government. Tight turn-around schedules, bud-
get restrictions, and scarcity of human translators
preclude the production of large parallel corpora,
which form the backbone of SMT systems.
Given these constraints, the focus is on making
the best possible use of available resources. This
usually involves some form of prioritized data col-
lection. In other words, one would like to con-
struct the smallest possible parallel training corpus
that achieves a desired level of performance on un-
seen test data.
Within an active learning framework, this can
be cast as a data selection problem. The goal is
to choose, for manual translation, the most infor-
mative instances from a large pool of source lan-
guage sentences. The resulting sentence pairs, in
combination with any existing in-domain seed par-
allel corpus, are expected to provide a significantly
higher performance gain than a na??ve random se-
lection strategy. This process is repeated until a
certain level of performance is attained.
Previous work on active learning for SMT has
focused on unsupervised dissimilarity measures
for sentence selection. Eck et al (2005) describe a
selection strategy that attempts to maximize cov-
erage by choosing sentences with the highest pro-
portion of previously unseen n-grams. However,
if the pool is not completely in-domain, this strat-
egy may select irrelevant sentences, whose trans-
lations are unlikely to improve performance on an
in-domain test set. They also propose a technique,
based on TF-IDF, to de-emphasize sentences sim-
ilar to those that have already been selected. How-
ever, this strategy is bootstrapped by random ini-
tial choices that do not necessarily favor sentences
that are difficult to translate. Finally, they work
exclusively with the source language and do not
use any SMT-derived features to guide selection.
Haffari et al (2009) propose a number of fea-
tures, such as similarity to the seed corpus, trans-
lation probability, relative frequencies of n-grams
and ?phrases? in the seed vs. pool data, etc., for
active learning. While many of their experiments
use the above features independently to compare
their relative efficacy, one of their experiments
attempts to predict a rank, as a linear combina-
tion of these features, for each candidate sentence.
The top-ranked sentences are chosen for manual
translation. The latter strategy is particularly rel-
evant to this paper, because the goal of our active
126
learning strategy is not to compare features, but to
learn the trade-off between various characteristics
of the candidate sentences that potentially maxi-
mizes translation improvement.
The parameters of the linear ranking model pro-
posed by Haffari et al (2009) are trained using
two held-out development sets D1 and D2 - the
model attempts to learn the ordering of D1 that
incrementally maximizes translation performance
on D2. Besides the need for multiple parallel
corpora and the computationally intensive nature
of incrementally retraining an SMT system, their
approach suffers from another major deficiency.
It requires that the pool have the same distribu-
tional characteristics as the development sets used
to train the ranking model. Additionally, they se-
lect all sentences that constitute a batch in a single
operation following the ranking procedure. Since
similar or identical sentences in the pool will typ-
ically meet the selection criteria simultaneously,
this can have the undesired effect of choosing re-
dundant batches with low diversity. This results in
under-utilization of human translation resources.
In this paper, we propose a novel batch-mode
active learning strategy that ameliorates the above
issues. Our semi-supervised learning approach
combines a parallel ranking strategy with sev-
eral features, including domain representativeness,
translation confidence, and batch diversity. The
proposed approach includes a greedy, incremental
batch selection strategy, which encourages diver-
sity and reduces redundancy. The following sec-
tions detail our active learning approach, includ-
ing the experimental setup and simulation results
that clearly demonstrate its effectiveness.
2 Active Learning Paradigm
Active learning has been studied extensively in the
context of multi-class labeling problems, and the-
oretically optimal selection strategies have been
identified for simple classification tasks with met-
ric features (Freund et al, 1997). However, nat-
ural language applications such as SMT present a
significantly higher level of complexity. For in-
stance, SMT model parameters (translation rules,
language model n-grams, etc.) are not fixed in
number or type, and vary depending on the train-
ing instances. This gives rise to the concept of
domain. Even large quantities of out-of-domain
training data usually do not improve translation
performance. As we will see, this causes simple
active selection techniques based on dissimilarity
or translation difficulty to be ineffective, because
they tend to favor out-of-domain sentences.
Our proposed active learning strategy is moti-
vated by the idea that the chosen sentences should
maximize coverage, and by extension, translation
performance on an unseen test set. It should
pick sentences that represent the target domain,
while simultaneously enriching the training data
with hitherto unseen, difficult-to-translate con-
structs that are likely to improve performance on a
test set. We refer to the former as representative-
ness and to the latter as difficulty.
Since it is computationally prohibitive to re-
train an SMT system for individual translation
pairs, a batch of sentences is usually selected at
each iteration. We desire that each batch be suffi-
ciently diverse; this increases the number of con-
cepts (phrase pairs, translation rules, etc.) that can
be learned from manual translations of a selected
batch. Thus, our active learning strategy attempts,
at each iteration, to select a batch of mutually di-
verse source sentences, which, while introducing
new concepts, shares at least some commonality
with the target domain. This is done in a com-
pletely statistical, data-driven fashion.
In designing this active learning paradigm, we
make the following assumptions.
? A small seed parallel corpus S is available
for training an initial SMT system. This may
range from a few hundred to a few thousand
sentence pairs.
? Sentences must be selected from a large pool
P. This may be an arbitrary collection of in-
and out-of-domain source language sentences.
Some measure of redundancy is permitted and
expected, i.e. some sentences may be identical
or very similar to others.
? A development set D is available to tune the
SMT system and train the selection algorithm.
An unseen test set T is used to evaluate it.
? The seed, development, and test sets are de-
rived from the target domain distribution.
To re-iterate, we do not assume or require the
pool to have the same domain distribution as the
seed, development, and test sets. This reflects a
real-world scenario, where the pool may be drawn
from multiple sources (e.g. targeted collections,
newswire text, web, etc.). This is a key departure
from existing work on active learning for SMT.
127
S e e d  c o r p u s
S M T  s y s t e m
M o n o l i n g u a l  p o o l
P o o l  t r a i n i n g D e v e l .  s e t
D o m a i n  m a t c h
T r a n s .  d i f f i c u l t y
D i v e r s i t y
P r e f e r r e d  o r d e r
C 1
C 2
Input features MLP Classifiers Classifier targets
Figure 1: Flow-diagram of the active learner.
3 Active Learning Architecture
Figure 1 illustrates the proposed active learning
architecture in the form of a high-level flow-
diagram. We begin by randomly sampling a small
fraction of the large monolingual pool P to cre-
ate a pool training set PT, which is used to train
the learner. The remainder, which we call the pool
evaluation set PE, is set aside for active selection.
We also train an initial phrase-based SMT system
(Koehn et al, 2003) with the available seed cor-
pus. The pool training set PT, in conjunction with
the seed corpus S, initial SMT system, and held-
out development set D, is used to derive a number
of input features as well as target labels for train-
ing two parallel classifiers.
3.1 Preferred Ordering
The learner must be able to map input features
to an ordering of the pool sentences that attempts
to maximize coverage on an unseen test set. We
teach it to do this by providing it with an ordering
of PT that incrementally maximizes source cov-
erage on D. This preferred ordering algorithm in-
crementally maps sentences in PT to a ordered set
OT by picking, at each iteration, the sentence with
the highest coverage criterion with respect to D,
and inserting it at the current position within OT.
The coverage criterion is based on content-word
n-gram overlap with D, discounted by constructs
already observed in S and higher-ranked sentences
in OT, as illustrated in Algorithm 1. Our hypoth-
esis is that sentences, which maximally improve
coverage, likely lead to bigger gains in translation
performance as well.
The O(|PT|2) complexity of this algorithm is
one reason we restrict PT to a few thousand sen-
tences. Another reason not to order the entire pool
and simply select the top-ranked sentences, is that
batches thus constructed would overfit the devel-
opment set on which the ordering is based, and
not generalize well to an unseen test set.
3.2 Ranker Features
Each candidate sentence in the pool is represented
by a vector of features, which fall under one of
the three categories, viz. representativeness, dif-
ficulty, and diversity. We refer to the first two
as context-independent, because they can be com-
puted independently for each sentence. Diversity
is a context-dependent feature and must be evalu-
ated in the context of an ordering of sentences.
128
Algorithm 1 Preferred ordering
OT ? ()
Sg ? count(g) ?g ? ngr(S)
Dg ? count(g) ?g ? ngr(D)
for k = 1 to |PT| do
PU ? PT ?OT
y? ? argmax
y?PU
?
g?ngr(y)
yg ?Dg ? n
Sg + 1
OT (k)? y?
Sg ? Sg + y?g ?g ? ngr(y?)
end for
return OT
3.2.1 Domain Representativeness
Domain representativeness features gauge the de-
gree of similarity between a candidate pool sen-
tence and the seed training data. We quantify this
using an n-gram overlap measure between candi-
date sentence x and the seed corpus S defined by
Equation 1.
sim(x,S) =
?
g?ngr(x)
xg ?
min(Sng , Cn)
Cn
?
g?ngr(x)
xg
(1)
xg is the number of times n-gram g occurs in x,
Sg the number of times it occurs in the seed cor-
pus, n its length in words, and Cn the count of
n-grams of length n in S. Longer n-grams that
occur frequently in the seed receive high similar-
ity scores, and vice-versa. In evaluating this fea-
ture, we only consider n-grams up to length five
that contain least one content word.
Another simple domain similarity feature we
use is sentence length. Sentences in conversational
domains are typically short, while those in web
and newswire domains run longer.
3.2.2 Translation Difficulty
All else being equal, the selection strategy should
favor sentences that the existing SMT system finds
difficult to translate. To this end, we estimate a
confidence score for each SMT hypothesis, using
a discriminative classification framework reminis-
cent of Blatz et al (2004). Confidence estima-
tion is treated as a binary classification problem,
where each hypothesized word is labeled ?cor-
rect? or ?incorrect?. Word-level reference labels
for training the classifier are obtained from Trans-
lation Edit Rate (TER) analysis, which produces
the lowest-cost alignment between the hypothe-
ses and the gold-standard references (Snover et
al., 2006). A hypothesized word is ?correct? if
it aligns to itself in this alignment, and ?incorrect?
otherwise.
We derive features for confidence estimation
from the phrase derivations used by the decoder in
generating the hypotheses. For each target word,
we look up the corresponding source phrase that
produced it, and use this information to compute
a number of features from the translation phrase
table and target language model (LM). These in-
clude the in-context LM probability of the target
word, the forward and reverse phrase translation
probabilities, the maximum forward and reverse
word-level lexical translation probabilities, num-
ber of competing target phrases in which the tar-
get word occurs, etc. In all, we use 11 word-level
features (independent of the active learning fea-
tures) to train the classifier in conjunction with the
abovementioned binary reference labels.
A logistic regression model is used to directly
estimate the posterior probability of the binary
word label. Thus, our confidence score is es-
sentially the probability of the word being ?in-
correct?. Sentence-level confidence is computed
as the geometric average of word-level posteriors.
Confidence estimation models are trained on the
held-out development set.
We employ two additional measures of transla-
tion difficulty for active learning: (a) the number
of ?unknown? words in target hypotheses caused
by untranslatable source words, and (b) the aver-
age length of source phrases in the 1-best SMT
decoder derivations.
3.2.3 Batch Diversity
Batch diversity is evaluated in the context of an
explicit ordering of the candidate sentences. In
general, sentences that are substantially similar to
those above them in a ranked list have low diver-
sity, and vice-versa. We use content-word n-gram
overlap to measure similarity with previous sen-
tences, per Equation 2.
d(b | B) = 1.0?
?
g?ngr(b)
n?Bg
?
g?ngr(b)
n?max(Bg, 1.0)
(2)
B represents the set of sentences ranked higher
than the candidate b, for which we wish to evalu-
ate diversity. Bg is the number of times n-gram g
129
occurs in B. Longer, previously unseen n-grams
serve to boost diversity. The first sentence in a
given ordering is always assigned unit diversity.
The coverage criterion used by the preferred or-
dering algorithm in Section 3.1 ensures good cor-
respondence between the rank of a sentence and its
diversity, i.e. higher-ranked in-domain sentences
have higher diversity, and vice-versa.
3.3 Training the Learner
The active learner is trained on the pool training
set PT. The seed training corpus S serves as the
basis for extracting domain similarity features for
each sentence in this set. Translation difficulty fea-
tures are evaluated by decoding sentences in PT
with the seed SMT system. Finally, we compute
diversity for each sentence in PT based on its pre-
ferred order OT according to Equation 2. Learn-
ing is semi-supervised as it does not require trans-
lation references for either PT or D.
Traditional ranking algorithms such as PRank
(Crammer and Singer, 2001) work best when the
number of ranks is much smaller than the sample
size; more than one sample can be assigned the
same rank. In the active learning problem, how-
ever, each sample is associated with a unique rank.
Moreover, the dynamic range of ranks in OT is
significantly smaller than that in PE, to which the
ranking model is applied, resulting in a mismatch
between training and evaluation conditions.
We overcome these issues by re-casting the
ranking problem as a binary classification task.
The top 10% sentences in OT are assigned a ?se-
lect? label, while the remaining are assigned a
contrary ?do-not-select? label. The input features
are mapped to class posterior probabilities using
multi-layer perceptron (MLP) classifiers. The use
of posteriors allows us to assign a unique rank to
each candidate sentence. The best candidate sen-
tence is the one to which the classifier assigns the
highest posterior probability for the ?select? la-
bel. We use one hidden layer with eight sigmoid-
activated nodes in this implementation.
Note that we actually train two MLP classi-
fiers with different sets of input features as shown
in Figure 1. Classifier C1 is trained using only
the context-independent features, whereas C2 is
trained with the full set of features including batch
diversity. These classifiers are used to implement
an incremental, greedy selection algorithm with
parallel ranking, as explained below.
Algorithm 2 Incremental greedy selection
B? ()
for k = 1 to N do
Pci ? {x ? PE | d(x | B) = 1.0}
Pcd ? {x ? PE | d(x | B) < 1.0}
C? C1(fci(Pci)) ? C2(fcd(Pcd,B))
bk ? argmax
x?PE
C(x)
PE ? PE ? {bk}
end for
return B
4 Incremental Greedy Selection
Traditional rank-and-select batch construction ap-
proaches choose constituent sentences indepen-
dently, and therefore cannot ensure that the cho-
sen sentences are sufficiently diverse. Our strat-
egy implements a greedy selection algorithm that
constructs each batch iteratively; the decision bk
(the sentence to fill the kth position in a batch)
depends on all previous decisions b1, ? ? ? , bk?1.
This allows de-emphasizing sentences similar to
those that have already been placed in the batch,
while favoring samples containing previously un-
seen constructs.
4.1 Parallel Ranking
We begin with an empty batch B, to which sen-
tences from the pool evaluation set PE must be
added. We then partition the sentences in PE in
two mutually-exclusive groups Pcd and Pci. The
former contains candidates that share at least one
content-word n-gram with any existing sentences
in B, while the latter consists of sentences that
do not share any overlap with them. Note that
B is empty to start with; thus, Pcd is empty and
Pci = PE at the beginning of the first iteration
of selection. The diversity feature is computed for
each sentence in Pcd based on existing selections
in B, while the context-independent features are
evaluated for sentences in both partitions.
Next, we apply C1 to Pci and C2 to Pcd and in-
dependently obtain posterior probabilities for the
?select? label for both partitions. We take the
union of class posteriors from both partitions and
select the sentence with the highest probability of
the ?select? label to fill the next slot bk, corre-
sponding to iteration k, in the batch. The selected
sentence is subsequently removed from PE.
The above parallel ranking technique (Algo-
rithm 2) is applied iteratively until the batch
130
reaches a pre-determined size N . At itera-
tion k, the remaining sentences in PE are par-
titioned based on overlap with previous selec-
tions b1, ? ? ? , bk?1 and ranked based on the union
of posterior probabilities generated by the corre-
sponding classifiers. This ensures that sentences
substantially similar to those that have already
been selected receive a low diversity score, and are
suitably de-emphasized. Depending on the char-
acteristics of the pool, batches constructed by this
algorithm are likely more diverse than a simple
rank-and-select approach.
5 Experimental Setup and Results
We demonstrate the effectiveness of the proposed
sentence selection algorithm by performing a set
of simulation experiments in the context of an
English-to-Pashto (E2P) translation task. We sim-
ulate a low-resource condition by using a very
small number of training sentence pairs, sampled
from the collection, to bootstrap a phrase-based
SMT system. The remainder of this parallel cor-
pus is set aside as the pool.
At each iteration, the selection algorithm picks a
fixed-size batch of source sentences from the pool.
The seed training data are augmented with the
chosen source sentences and their translations. A
new set of translation models is then estimated and
used to decode the test set. We track SMT perfor-
mance across several iterations and compare the
proposed algorithm to a random selection baseline
as well as other common selection strategies.
5.1 Data Configuration
Our English-Pashto data originates from a two-
way collection of spoken dialogues, and thus con-
sists of two parallel sub-corpora: a directional E2P
corpus and a directional Pashto-to-English (P2E)
corpus. Each sub-corpus has its own independent
training, development, and test partitions. The di-
rectional E2P training, development, and test sets
consist of 33.9k, 2.4k, and 1.1k sentence pairs, re-
spectively. The directional P2E training set con-
sists of 76.5k sentence pairs.
We obtain a seed training corpus for the simula-
tion experiments by randomly sampling 1,000 sen-
tence pairs from the directional E2P training par-
tition. The remainder of this set, and the entire re-
versed directional P2E training partition are com-
bined to create the pool (109.4k sentence pairs). In
the past, we have observed that the reversed direc-
tional P2E data gives very little performance gain
in the E2P direction even though its vocabulary is
similar, and can be considered ?out-of-domain? as
far as the E2P translation task is concerned. Thus,
our pool consists of 30% in-domain and 70% out-
of-domain sentence pairs, making for a challeng-
ing active learning problem. A pool training set of
10k source sentences is sampled from this collec-
tion, leaving us with 99.4k candidate sentences.
5.2 Selection Strategies
We implement the following strategies for sen-
tence selection. In all cases, we use a fixed-size
batch of 200 sentences per iteration.
? Random selection, in which source sentences
are uniformly sampled from PE.
? Similarity selection, where we choose sen-
tences that exhibit the highest content-word n-
gram overlap with S.
? Dissimilarity selection, which selects sen-
tences having the lowest degree of content-
word n-gram overlap with S.
? Active learning with greedy incremental selec-
tion, using a learner to maximize coverage by
combining various input features.
We simulate a total of 30 iterations, with the
original 1,000 sample seed corpus growing to
7,000 sentence pairs.
5.3 Simulation Results
We track SMT performance at each iteration in
two ways. The first and most effective method is
to simply use an objective measure of translation
quality, such as BLEU (Papineni et al, 2001). Fig-
ure 2(a) illustrates the variation in BLEU scores
across iterations for each selection strategy. We
note that the proposed active learning strategy per-
forms significantly better at every iteration than
random, similarity, and dissimilarity-based selec-
tion. At the end of 30 iterations, the BLEU
score gained 2.46 points, a relative improvement
of 45.9%. By contrast, the nearest competitor was
the random selection baseline, whose performance
gained only 1.33 points in BLEU, a 24.8% im-
provement. Note that we tune the phrase-based
SMT feature weights using MERT (Och, 2003)
once in the beginning, and use the same weights
across all iterations. This allowed us to compare
selection methods without variations introduced
by fluctuation of the weights.
131
(a) Trajectory of BLEU
(b) Trajectory of untranslated word ratio
(c) Directionality match (d) Diversity/Uniqueness
Figure 2: Simulation results for data selection. Batch size at each iteration is 200 sentences.
132
The second method measures test set coverage
in terms of the proportion of untranslated words
in the SMT hypotheses, which arise due to the
absence of appropriate in-context phrase pairs in
the training data. Figure 2(b) shows the varia-
tion in this measure for the four selection tech-
niques. Again, the proposed active learning algo-
rithm outperforms its competitors across nearly all
iterations, with very large improvements in the ini-
tial stages. Overall, the proportion of untranslated
words dropped from 8.74% to 2.28% after 30 iter-
ations, while the closest competitor (dissimilarity
selection) dropped to 2.59%.
It is also instructive to compare the distribu-
tion of the 6,000 sentences selected by each strat-
egy at the end of the simulation to determine
whether they came from the ?in-domain? E2P
set or the ?out-of-domain? P2E collection. Fig-
ure 2(c) demonstrates that only 1.3% of sentences
were selected from the reversed P2E set by the
proposed active learning strategy. On the other
hand, 70.9% of the sentences selected by the
dissimilarity-based technique came from the P2E
collection, explaining its low BLEU scores on the
E2P test set. Surprisingly, similarity selection also
chose a large fraction of sentences from the P2E
collection; this was traced to a uniform distribu-
tion of very common sentences (e.g. ?thank you?,
?okay?, etc.) across the E2P and P2E sets.
Figure 2(d) compares the uniqueness and over-
all n-gram diversity of the 6,000 sentences chosen
by each strategy. The similarity selector received
the lowest score on this scale, explaining the lack
of improvement in coverage as measured by the
proportion of untranslated words in the SMT hy-
potheses. Again, the proposed approach exhibits
the highest degree of uniqueness, underscoring its
value in lowering batch redundancy.
It is interesting to note that dissimilarity selec-
tion is closest to the proposed active learning strat-
egy in terms of coverage, and yet exhibits the
worst BLEU scores. This confirms that, while
there is overlap in their vocabularies, the E2P and
P2E sets differ significantly in terms of longer-
span constructs that influence SMT performance.
These results clearly demonstrate the power
of the proposed strategy in choosing diverse, in-
domain sentences that not only provide superior
performance in terms of BLEU, but also improve
coverage, leading to fewer untranslated concepts
in the SMT hypotheses.
6 Conclusion and Future Directions
Rapid development of SMT systems for resource-
poor language pairs requires judicious use of hu-
man translation capital. We described a novel ac-
tive learning strategy that automatically learns to
pick, from a large monolingual pool, sentences
that maximize in-domain coverage. In conjunc-
tion with their translations, they are expected to
improve SMT performance at a significantly faster
rate than existing selection techniques.
We introduced two key ideas that distinguish
our approach from previous work. First, we uti-
lize a sample of the candidate pool, rather than an
additional in-domain development set, to learn the
mapping between the features and the sentences
that maximize coverage. This removes the restric-
tion that the pool be derived from the target do-
main distribution; it can be an arbitrary collection
of in- and out-of-domain sentences.
Second, we construct batches using an incre-
mental, greedy selection strategy with parallel
ranking, instead of a traditional batch rank-and-
select approach. This reduces redundancy, allow-
ing more concepts to be covered in a given batch,
and making better use of available resources.
We showed through simulation experiments that
the proposed strategy selects diverse batches of
high-impact, in-domain sentences that result in a
much more rapid improvement in translation per-
formance than random and dissimilarity-based se-
lection. This is reflected in objective indicators of
translation quality (BLEU), and in terms of cover-
age as measured by the proportion of untranslated
words in SMT hypotheses. We plan to evaluate
the scalability of our approach by running simu-
lations on a number of additional language pairs,
domains, and corpus sizes.
An issue with iterative active learning in gen-
eral is the cost of re-training the SMT system for
each batch. Small batches provide for smooth per-
formance trajectories and better error recovery at
an increased computational cost. We are currently
investigating incremental approaches that allow
SMT models to be updated online with minimal
performance loss compared to full re-training.
Finally, there is no inherent limitation in the
proposed framework that ties it to a phrase-based
SMT system. With suitable modifications to the
input feature set, it can be adapted to work with
various SMT architectures, including hierarchical
and syntax-based systems.
133
References
John Blatz, Erin Fitzgerald, George Foster, Simona
Gandrabur, Cyril Goutte, Alex Kulesza, Alberto
Sanchis, and Nicola Ueffing. 2004. Confidence es-
timation for machine translation. In COLING ?04:
Proceedings of the 20th international conference on
Computational Linguistics, page 315, Morristown,
NJ, USA. Association for Computational Linguis-
tics.
Koby Crammer and Yoram Singer. 2001. Pranking
with ranking. In Advances in Neural Information
Processing Systems 14, pages 641?647. MIT Press.
Matthias Eck, Stephan Vogel, and Alex Waibel. 2005.
Low cost portability for statistical machine transla-
tion based in N-gram frequency and TF-IDF. In
Proceedings of IWSLT, Pittsburgh, PA, October.
Yoav Freund, H. Sebastian Seung, Eli Shamir, and Naf-
tali Tishby. 1997. Selective sampling using the
query by committee algorithm. Machine Learning,
28(2-3):133?168.
Gholamreza Haffari, Maxim Roy, and Anoop Sarkar.
2009. Active learning for statistical phrase-based
machine translation. In NAACL ?09: Proceedings
of Human Language Technologies: The 2009 An-
nual Conference of the North American Chapter
of the Association for Computational Linguistics,
pages 415?423, Morristown, NJ, USA. Association
for Computational Linguistics.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
NAACL ?03: Proceedings of the 2003 Conference
of the North American Chapter of the Association
for Computational Linguistics on Human Language
Technology, pages 48?54, Morristown, NJ, USA.
Association for Computational Linguistics.
Franz Josef Och. 2003. Minimum error rate train-
ing in statistical machine translation. In ACL ?03:
Proceedings of the 41st Annual Meeting on Asso-
ciation for Computational Linguistics, pages 160?
167, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. BLEU: A method for automatic
evaluation of machine translation. In ACL ?02: Pro-
ceedings of the 40th Annual Meeting on Associa-
tion for Computational Linguistics, pages 311?318,
Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings AMTA, pages 223?231, August.
134
