The Effects of Word  Order  and Segmentat ion  on Trans la t ion  
Retr ieva l  Per fo rmance  
T imothy  Ba ldwin  and Hozumi  Tanaka  
27okyo \]nstil;ul;e ()I "~ I e thno logy  
2-1.2-1 Ooka,yama, Meguro -ku ,  qlbkyo 1.52-8552 , JAPAN 
{t im,  tanaka}@cl ,  ca .  t i tech ,  ac .  jp  
Abst ract  
This research looks at tim cIt'ccts of word order 
mL(t scgm(mtation on l;ra.nslation retri(~val t)(~rfor- 
III~\[.11C( ~. lot" ~.111 eXl)erim(:nta.1 Jal>an(>s(>English (;rm>- 
lation memory system. We iml)lem('.nt a num- 
ber of both bag-of-words and word order-s(msitiv(~ 
s;imilarity metrics, and test each over charact(u- 
l/ased m~d word-based indexing. Tim translation 
r(%rieval )elt'ormmm(~ of ca(:h sysi;em (:ontiguration 
is (~valuat(~(1 (mq)iri(:ally through tlm n()ti(>n of word 
edit distan(:(~ \])(}(;W(}(}IL translation (:ml(li(lal;(~ ()ul;lml;s 
mid tim mo(hd translation. Ore resull;s in(li('.at(~ 
(;hat(; (:hm'act(!r-l)as(!d indexing is (:(msislxmtly sup(> 
riot (;() wor(l-bas(:d in(l(:xing, sugg(:sl;ing (;hal; s(:glncn- 
l;al;ion is ;m mm('.cessary luxury in th(', giv(m domain. 
\?or(1 ord(:r-s(:nsi(;iv(: al)i)roach('s at(: do.monsl;rat(:d 
to generally OUtlt(~rform bag-of-words methods, with 
som'(:c bmguagc segment-lev(d e it distan(:o, proving 
th(: most; (:fl'(:(;l;iv(~ similarity m(,,l;ric. 
1 I n t roduct ion  
Transla(.ioll m(unorio,q (TM's) m'c a w(~ll-(!slal)lished 
I,(:(:\]uloliigy wil,llilL (,h(! hlunalL and n|a(:hilm ld'an,qla 
(;ion t'rat('.rnii;i(:s, duo. to the high (raiLslat;ion lit(! - 
(;isioIL (;lmy a flbrd. Esstml;ially, TM's me a list 
of t rans la t ion  records (source la.nguage strings 
paired with a unique target language translation), 
which the TM system accesses in suggcsl;ing a list 
of target languag(', t rans lat ion  cand idates  which 
may l)(,. hell)tiff to (;h(: translator in translating a 
given source language inputJ 
Naturally, TM systems h~w('~ no way of accessing 
the (;a.rgcl; la.nguagc quiv;fl(m(; of tit(: soltr(:(: lan- 
guage input, and hence (;lm list of tautc.l, lanquagc 
tnmslation cmMi(lat(:s is det(:rntined base(l on source 
language similarity between tim (:urr(mt input and 
trmlslation examples within the TM, with transla- 
tion equivalent(s) of maximally similar source lan- 
guage string(s) given as the translation candidate(s). 
This is based on the assumption that structural att(t 
semantic similarities 1)etwe(m targ(:t language trans- 
lations will be reflected in the original source lan- 
guage cquivalenl;s. 
One reason tbr the popularity of TM's is the low 
operational burden they t)(LS(~ to tim user, in that 
translation pairs are largely acquired automatically 
1See \])lanas (1998) for a thorough review of commercial 
TM systems. 
from observai;ion of l;lm incremental (;rmlsl&Lion pro- 
(:(:ss, and translation cml(lidates cml \]m l)roduced on 
(hunand almost insf;ani;ancously. To support his low 
()vt}rlma(1, TM systems must allow first access into 
the l)Oixmtially la.l'g(,.-s(:ah} TM, lint at the stone time 
I)e al)lc to 1)rc(lict .ranslation similarity with high ac- 
curacy. Ilere, th(n'(~ is clearly a trade-off between ac- 
( :ess / re t r i cva l  speed anti predict ive accuracy  of 
(,he retriewfl m(,.ctmnism. 2haditiomflly, resemch on 
TM r(~trieval nmthods has focused on Slme(l, with lit- 
(;1(~ (:ross-(~vahml;ion f (;he accuracy of differ(mr mclh- 
otis. \Vc t>r(~t'(u to focus on ac(:tlracy, and t)r(~s(~ll(; 
(~mlfiLical data (~vid(!ncing tim relative l)r(~di(:l;ivc l>O- 
((u~iial of difl'<u'(mt similarity metrics over different 
l)aram(:t(,.risations. 
In tiffs l)almr, we focus on comparison of differ(mr 
retrieval algorithms for non-segmenting la.nguag(~s, 
1)ascd around a TI~,I sysi;cm from .\]almnese to En- 
glish. Non-s(!gm(ml;ing languages are those which (Io 
not involve d(:limii;ers (e.g. spaces) tmtwe(m words, 
and in(:lude .lapmms(:, (Jhines(: and Thai. W(: are 
tmrticularly int(~'r(~st(:(l in the part tim orlhog(mal 1m- 
rmnet(~rs of s(.,gmentnl;ion and word order play in the 
st)(!cd/a(:(:uracy trad(!-oti'. That is, 1)3" doing away 
with segnl(:ntai;ion in relying soMy on ch\[/t'}lc\[(}l- 
h~v(~l comparis(m (character-1)ased indexing),  do 
w(: signiti(:mitly degrade match tmrt'ormance, ascom- 
pared to word-level comparison (word-based in- 
dexing)? Similm'ly, by ignoring word order and 
treating each sour(:e language string as a "bag of 
words", do \re genuinely lose out over word order- 
s(msitive apl)roacho.s? The. In;fin objective of this 
research is thus (;o (teJ;ermine whether the COmlmi,a- 
tioiml overlmad associated with more stringent ap- 
proaches (i.e. word-based indexing and word order- 
sensitive alH)roaches) is commensura.te with the per- 
formancc gains they ott'er. 
To l)rccmpt what tollows, the major contrilmtions 
of this research are: (a) empirical evaluation of dif- 
thrcnt comparison methods over actual Japanese- 
English TM data, focusing on four orthogonal re- 
triewfl paradigms; (b) the finding that, over tile tar- 
get; data, character-based indexing is consistently 
superior to word-based indexing in identii\[ying the 
translation candidate most sinfilar to tile optimal 
translation for a given inlmt; and (c) empirical ver- 
ification of tim supremacy of word order-sensitive 
exhaustiv(: string comparison methods over boolean 
inal;ch methods. 
In the %llowing sections we discuss the effects 
35 
of segmentation and word order (~ 2) and preseut 
a number of both bag-el;words and word order- 
sensitive sinfilarity metrics (? 3), before going on to 
evaluate the difl'crent lnethods with character-based 
and word-based indexing (? 4). We then conclude 
the paper in Section 5. 
2 Segmentation and word order 
Using segmentat ion  to divide strings into compo- 
nent words or nlori)helnes has tile obvious advml- 
tage of clustering characters into senlantic units, 
which in the case of ideogrmn-based languages uch 
as Japanese (in the fern1 of kanji characters) and 
Chinese, generally disatnbiguates character tnean- 
ing. The kanji character ' J  \[', for example, can be 
used to mean any of "to discern/discriminate", "to 
speak/argue" and "a valve", but word context easily 
resolves uch mnbiguity, hi this sense, our intuition 
is that segmented strings should produce better re- 
sults than non-segmented strings. 
Looking to past research on similarity metrics for 
TM systelns, ahnost all systems involving aal)anese 
as the source language rely on segnlentation (e.g. 
(Nakanmra, 1989; Sulnita and Tsutsumi, 1991; Ki- 
talnura and Yamamoto, 1996; Tmtaka, 19971), with 
Sate (1992) and Sate and Kawase (1994) providing 
rare instances of character-based systelnS. 
By avoiding tile need to segment text;, we: (a) al- 
leviate computational overhead; (b) avoid the need 
to commit ourselves to a particular analysis type in 
the case of ambiguity; (c) avoi(1 the issue of' how 
to deal with unknown words; (d) avoid the need 
for stemming/lenlmatisation; a d (e) to a large ex- 
tent get around problems related to the nornmlisa- 
tion of lexical alternation (see Baldwin and Tanaka 
(1999) for a discussion of problems related to lexical 
alternation in Jal)anese). Additionally, we can use 
the conmlonly anlbiguous na.ture of individual kanji 
characters to our advantage, in modelling seinan- 
tic similarity between related words with character 
overlap. With word-based indexing, this would only 
be possible with tile aid of a thesaurus. 
Similarly for word order,  we would expect hat 
translation records that preserve the word (seg- 
ment) order observed in the inImt string would pro- 
vide closer-matching translations than translation 
records containing those stone segnlents in a differ- 
ent order. Natur~dly, enforcing preservation of word 
order is going to place a significant burden on the 
matching mechanism, in that a number of different 
substring match schenlata re inevitably going to 
be produced between rely two strings, each of which 
nmst be considered on its own merits. 
To the authors' knowledge, there is no TM sys- 
tem operating from Japanese that does not rely 
on word/segment/character order to some degree. 
Tanaka (1997) uses pivotal content words identified, 
by the user to search through the TM and locate 
translation records which contain those same con- 
tent words in the stone order and preferably the stone 
segment distance apart. Nakamura (1989) similarly 
gives preference to translation records in which the 
content words contained in the original input occur 
in the same linear order, although there is tile scope 
to back off to translation records which do not I)re- 
serve the original word order. Sumita and Tsutsmni 
(19911 take the opposite tack in iteratively filter- 
ing out NPs and adverbs to leave only functional 
words and nlatrix-level predicates, and find trmlsla- 
tion records which contain those same key words in 
the same ordering, preferably with the same segment 
types between them in the same numbers. Niren- 
burg et al (1993) propose a word order-sensitive 
metric based on "string composition discrepancy", 
and increlnentally relax the restriction on the qual- 
ity of match required to inehlde word lenmlata, word 
synonynls and then word hyt)ernylns , increasing the 
match penalty as they go. Sate and Kawase (1994) 
employ a more local model of character order in 
modelling similarity according to N-grams fashioned 
from the original string. 
The greatest advantage in ignoring word/segnlent 
order is computational, in that we significantly re- 
duce the search space and require only a single over- 
all comparison per string pair. Below, we analyse 
whether this gain in speed outweighs any losses in 
retrieval perfbrmance. 
3 S imi la r i ty  metr i cs  
Due to o111" interest in the efli~cts of both word order 
and seglnentation, we must have a selection of sim- 
ilarity lnetrics compatible with the various permu- 
tations of these two 1)arameter types. We choose to 
look at a nunlber of bag-of-words and word order- 
sensitive methods which are compatible with both 
character-based and word-based indexing, and vary 
the intmt to model tile etl~ects of the two indexing 
paradigms. The particular bag-of-word approactles 
we target are tlm vector space model (Manning and 
Schiitze, 1.999, p300) and "token intersection", a
silnple ratio-based similarity nletric. For word order- 
sensitive approaches, we test edit distance (Wagner 
and Fisher, 1974; Planas and Furuse, 1999), "se- 
quential correspondence" and "weigllted sequential 
correspondence". 
Each of tile similarity metrics eillpirically de- 
scribes the sintilarity between two inlmt strings tmi 
mid i~., 2 where we define tmi as a source language 
string taken fl'om the TM and i~. as the input string 
which we are seeking to 1hatch within the TM. 
One featnre of all similarity metrics given here is 
that they have fine-grained iscriminatory potential 
and are able to narrow down the final set of trans- 
lation candidates to a handfld of, and in nlost cases 
one, outlmt. This was a deliberate design decision, 
and aimed at example-based machine translation ap- 
plications, where human judgement cannot be relied 
upon to single out the most appropriate translation 
from multiple system outputs. In this, we set our- 
selves apart from the research of Sunlita and Tsut- 
sumi (1.991), for example, who judge the system to 
have been successful if there are a total of 100 or less 
outputs, aud a useful translation is contained within 
them. Note that it would be a relatively simple pro- 
2Note that the ordering here is arbitrary, and that all the 
similarity metrics described herein are commutative for the 
given implementations. 
36 
cedure  to fall ()lit the 11111111)e1" of Olltt)lltS to it ill o l l r  
case, tly taking tim top n ranking outputs. 
For all silnitarity metrics, we weight different 
.\]ai)mmse gment tyl)es according to their exl)ected 
impact on translation, in the form of the sweigh, t 
f l lnct iol l :  
Segment ype s,wcight 
punctuation 0 
other segments 1 
W(' exl)erinlentally trialled intermediate swcight set- 
tings tbr ditt'erent character tyl)es (in the case of 
character-based indexing) or segment yl)eS (in the 
case of word-based indexing), none of which was 
fomtd to apl)reciat)ly iml)rove performance. :~ 
a.1 Simi lar i ty metr ics used in this research 
Vector  space model  
Within our imt)lenmntation of the reactor space 
Inodol (VSM), the segment content of each string 
is (lescril)('.(l as a vector, ma(le u l) of 3 s ingle  dimen- 
sion for each segment tok(,n occurring within tmi or 
in. The. value of each vector eo lnt )onent  is given as 
the weighted frequen(-y of that token accor(ling to 
its sweiqht vahle,  such that any nulnber of 3 given 
i)un(:tuation mark will produce a fl'e(luen(:y of 0. The 
string sinfilarity of t?H, i and in is then detined sis tim 
cosine of the angle l/etween vectors t\[\[~.i and iT\[t, re- 
Sl)ectivety, calculated as: 
tT~,i, i~5, 
cos(t,fi,,,i;4 - It, ll l 0) 
where  dot  l ) roduct  and vect()r length  (:oin(:i(le wil;h 
l;he standard detlnitions. 
The strings tmi of maximal similarity are th()se 
whi(:h i ) roduce the  nmxinuun v3hw, for th(! v(~ctor 
cosine. 
Not(; that  VSM c(msi(lers (inly s(' .gment f re(tueney 
and is insensitive to word order. 
Token  intersect ion 
The token intersection of tmi 3nd in is defined as 
the cumulative intersecting fl'equency of tokens ap- 
pearing in each of the strings, normalised according 
to the combined segment lengths of tm, i and in. For- 
really, this equates to: 
tint(tm~, in) : e ? ~_~, l ' l i l l  (f,'{?(htnl (\[),frcqilz(,)) " m~(l,,,~)+>.,,(i,,) (2) 
where each t is a token (iccurring in e.ither tmi or 
in, freq,(t) is detined as the swei.qht-l)ased fi'equency 
of token t occurring in string s, and Ion(s) is tlm 
aIf anything, weighting down hi,agana characters, fin" ex- 
ample, due to their common occurrence as intlectional suffices 
or particles (as per Fujii and Croft (1993)) led to a significant 
drop in 1)eribrmanee. Simihwly, weighting down stop word- 
like flmetional parts-of-sf)eech in ,lat)anese had little eltiect, 
unlike weighting down stop words in the case of English (see 
below). 
segment length of string s, that is the swcight-1)ased 
COllllt Of segl l lel l ts (:(nltained ill .s'. 
As tbr VSM, the string(s) tmi most similar t;(i in 
arc thos(; which general;e the nlaximum value tbr 
tint(tmi, in). 
Note that word order does not take any part in 
calculation. 
Edit  d istance 
The first of the word order-sensitive methods is edit 
dist3nce (Wagner and Fisher, 1974; l?hmas and Fu- 
ruse, 1999). Essentially, the segment-lmsed it dis- 
tance 1)etwecn strings t'ln, i and in is the minimunl 
numl/er of prilnitive edit operations on single seg- 
ments required to transtbrm tmi into in (and vice 
versa), 1)ased Ul)On the ol)erations of segment equal- 
ity (segments tmi,m and in ,  are identical), segment 
deletion (delete segment a fl'OlIl a given 1)osition in 
string .s') and scgmc'nt insertion (insert segmen~ (t
into a given position in string .s). The cost asso- 
ciated with each ol)eration on segment a is defined 
~/S: 4 
Operat ion Cost 
segment equality () 
segment deletion swcigh, t(a ) 
s(;gment insertion swcigh, t(a) 
Unlike other similarity metrics, smaller v31ues in- 
dicate greater similarity for edit distance, and iden- 
tical strings have edit distmme 0. 
The woM order sensitivity of edit distance is per- 
\]ml)S t)est exeml)litie(l tly way of the following exam- 
1)le, where segment delimiters are given as :.'. 
(1) E - SN-  14-':winter r3in" 
(2a) 2F- $51. l+"summer  rain" 
(21)) 1+" SN- 2F "a rainy summer" 
Itere, the edit distance from (1) to (2a) is 1 -t- 1 = 2, 
as one deletion ol/eration is required to remove E 
\[\]:uyu\] "winter" and one insertion ol)eration required 
to 3dd 2F \[natu\] "summer". The edit distance from 
(1) to (21/), on the other hand, is 1 + 1 + 1 + 1 = 4 
despite (2b) being identical in segment content to 
(2a). In terms of edit distance, therefore, (23) is 
adjudged more similm" to (1) than (21)). 
Sequent ia l  cor respondence 
Sequential corresI)ondence is 3 measure of the m3x- 
innun subsl;ring sinlilarity lmtween tmi and in, nor- 
malised acc(irding to the comt)ined segment lengths 
h'.n(tmi) and len(in). Essentially, this method re- 
quires th3t all substring matches ubmatch (tmi, in) 
between tmi and in be calculated, and the maximum 
scqcorr ratio returned, where scqcorr is delined as: 
, . , 2?max\[su?,mateh(tml,in)\[ 
~ ~ " m~It.,,.)+t~.(~,) (3) 
1Note that dm costs  for deletion and insertioil must be 
equal to maintain commutativity. 
37 
IIere, tile cardinality operator applied to 
submatch(tmi,in) returns tile combined seg- 
ment length of matching substrings, weighted 
according to swcight. That is: 
I~,~ ..... t~(~., ,~.~,~)I=~,j  ~ .... igl~t(s,~j,,~) (4) 
for each segment ssj,t~ of each matching substring 
ssj G submatch(tmi, in). 
Returning to our exmnple from above, the simi- 
larity for (1) and (2a) is 2x2 2 whereas that for 
? 3+3 - -  g 
(1) and (2b)is ')x~ , 
3+3 ~ :~" 
Weighted sequential correspondence 
Weighted sequential correspondence--the lastof the 
word order-sensitive methods--~is an extension of se- 
quential correspondence. It attempts to sut)plement 
the deficiency of sequential correspondence that the 
contiguity of substring matches is not taken into 
consideration. Given input string a~ a2a.~a/,, for 
example, sequential correspondence would suggest 
equal similarity (of ~)  with strings a~ ba~ca:~da/, 
and aj ap. a3 a 4 cfg, despite the second of these being 
more likely to produce a translation at; least partially 
resembling tlmt of the intmt string. 
We get around this by associating all incremen- 
tal weight with each matelfing segment assessing 
the contiguity of left-neighl)ouring segments, in the 
manner (Inscribed by Sato (1992) for chaxactcr- 
based matclfing. Namely, the kth segment of a 
matched substring is given the multiplicative weight 
rain(k, Max), where Max was set to 4 in evaluation 
after Sato. I submatch,(tmi,iu,)l fi'om equation (3) 
thus t)ecomes: 
~ssj ~t,  rain ( k ? swcight(.ssj,~.),Ma, z) (5) 
tbr each sul)string ssj ~ submatch(tmi, i77,). \?e siln- 
ilarly modify tile definition of the lea flmction for a 
string s to: 
lea(s) =- E jmin  (j x sweight(.,'j),Max ) (6) 
for each segment .sj of s. 
3.2 Retrieval speed optirnisation 
While this paper is mainly concerned with accuracy, 
we take a moment out here to discuss the potential 
to accelerate the proposed methods, to get a feel for 
their relative speeds in actual retrieval. 
One immediate and effective way in which we can 
limit the search space for all methods is to use the 
current op-ranking score in establishing upper and 
lower t)ounds on the length of strings which have 
the potential to better that score. For token inter- 
section, for example, fi'om the fixed length lea(in) 
of input string in and current top score a, we can 
calculate the following bounds based on the greatest 
possible degree of lnatch between in and tmi: 
Upper bout, d: le,~(t.~d </(~-~)~n(~'~)J (7) L CZ 
_ F alen('in) 7 Lower bound: len(tmi) >,  2 - ( , ,  (8) 
In a similar fashion, we can stipulate a corridor of al- 
lowable segment lengths for tin i, for sequential corre- 
spondence and weighted sequential correspondence. 
For edit distance, we make the observation that tbr 
a current minimum edit distance of a, the following 
inequality over Icn(tmi) inust be satisfied for tmi to 
have a chance of bettering ct: 
len(in) - ~ < len(tmi) < len(in) + a (9) 
We can also limit the numl)er of string compar- 
isons required to reach the optimal match with in, 
by indexing each tmi by its component segments and 
working through the component segments of in in as- 
cending order of global fi'equency. At each iteration, 
we consider each previously unmatched translation 
record containing the current segment token, adjust- 
ing the upper and lower bounds as we go, given that 
translation records for a given iteration caiulot hmre 
contained segment okens already processed. The 
maxinmm possible segment correspondence b tween 
the strings is therefore decreasing on each iteration. 
We are also able to completely discomlt strings wit}l 
no segment component conunon with iTt in this way. 
Through these two methods, we were able to 
greatly reduce the number of string comparisons in 
word-based indexing evaluation for VSM, token in- 
tersection, sequential correspondence and weighted 
sequential correspondence methods in particular, 
and edit distance to a lesser degree. The degree of 
reduction for character-based indexing was not as 
marked, due to the massive increase in numbers of 
l;ranslation records sharing some character content 
with in. 
There is also considerable scope to accelerate 
the matching mechanisms used by the word order- 
sensitive approaches. Currently, all approaches are 
implemented in Perl 5, and the word order-sensitive 
approaches use a naive, highly recursive method to 
exhaustively generate all substring matches and de- 
ternfine the sinfilarity for each. One obvious way in 
which we could enhance this implelnentation would 
be to use an N-gram index as proposed by Nagao 
and Mori (1.994). Dynamic Programming (DP) tech- 
niques would undoubtedly lead to greater efficiency, 
as suggested by Crmfias et al (1995, 1997) and also 
Planas and Furuse (this volume). 
4 Eva luat ion  
4.1 Evaluation specifications 
Evaluation was partitioned off into character-based 
and word-based indexing for the vm'ious similarity 
methods. For word-based indexing, seginentation 
was carried out with ChaSen v2.0b (Matsmnoto et 
al., 1999). No attempt was made to post-edit he 
segmented outtmt, in interests of maintaining con- 
sistency in the data. Segmented and non-segmented 
strings were tested using a single program, with 
segment length set to a single character for non- 
segmented strings. 
As test data, we used 2336 unique translation 
records deriving fi'om technical field reports on con- 
struction machinery translated from Japanese into 
English. Translation records varied in size from 
38 
CIIAI{ACTEI{- 
BASEl) 
1NI)EXING 
\~)~() 1/J )- 
IL,\SEI) 
INI)I'iXING 
Similarity metric 
Vector space model (0.5) 
Token intersection (0.4) 
Edit distance (/cn(in))- 
Sequential corr. (0.4) 
Weighted seq. (:orr. (0.2) 
Vector sllace model (0.5) 
Token intersection (0.4) 
Edit distmme (h,n(in~- 
Sequential corr, (0.4) 
Weighted seq. corr. (0.2) 
Accuracy 
44.0 
44.3 
Edit 
diserep. 
4.86 
3.25 
1.82 
2.92 
2.89 
Ave,  
outputs 
1.04 (0.97) 
1.01 (0.99) 
1.39 (0.80) 
1.02 (0.98) 
1.04 (0.97) 
50.2 
46.6 
45.6 
43.7 (-0.8%) 
43.0 (-2.9%) 
47.3 (-5.9%) 
43.1 (-7.4%) 
40.7 (-10.7%) 
5.21 
3.12 
2.03 
3.06 
3.30 
1.17 (0.91) 
1.01 (0.99) 
1.90 (0.69) 
1.01 (0.99) 
1.14 (0.92) 
Ave.  
time 
2.14 
2.24 
4.75 
3.20 
4.10 
0.76 
0.88 
1.00 
1.10 
1.24 
Table 1: Results for the different similarity metri(:s under character-1)ased and word-based indexing 
single-word technical terms taken f1'Ol12 SI~ technical 
glossary, to multiple-sentence strings, at an average 
se.glnent length of 13.4 and average character length 
of 26.1. All .lapane, se strings of length 6 chara(:ters 
or more (a l;ol;al of 1802 strings) were extracted fl'om 
the Ix;st da.ta, leaving a resi(hle gh)ssary of te(:hni(:al 
1;erltls (533 strings) as we w(nfld not CXl)e('t to find 
use, hll nlat(:hes in the TM. The retrie, val a(:curacy 
()\,or the 1802 hmger strings was then vcritied t)y \] 0- 
fokt (:ross wflidation, including the glossary in the 
test TM on each iteration. 
Not(; that the test data was llre-1)artitioned into 
single technical terms, single sentences or sen- 
tence clusters, each constitut;i21g a single translation 
record. Partitions were taken as given in evaluation, 
whereas for reM-worhl TM systems, tim automal;i(m 
of th is  i)2"()cess (;Oltll)l'ises ;tll il211)ortalll; COlill)()ll(1Ilt 
of the (/verall sysI;(mL 1)re(',eding translation rel,ri(;val. 
While ackn()wh;(lging the i lnl)ort;an(:(; ()f this step and 
its int(;ra(:l;ion with r(?ri(;val 1)or\[ormall(:(;, we (:boost, 
to sideste l) it for the lmri)os(~s of this pal)c.r , and 
leave it for hltm(; resc.m(:h. 
In an effort to make evaluation as ol)jeci;ive and 
empirical as l)ossibh;, apl)r()i)riatencss of transla- 
tion candidate(s) l rOl)OSed by the different metri(:s 
was evahmted according to the mil2inlunl edit dis- 
tahoe between the translation candidate(s) and the 
unique model translation. In this, we transferred 1,t2(; 
edit distance, method described M)ove directly across 
to the ta.rg(% langustge, (English), with segments its 
words and the fl)lh)wing s'weight schema: 
Segment ype 
tmnctuation 
stop \VOl'dS 
other words 
swcight 
0 
0.2 
1 
Stol) words are defined as those containcd within the 
SMART (Salton, 197\].) stop word l ist) The system 
output was judged to be correct if it contained a 
translation optimally close to the model trmMation; 
the average ol)timal edit distance h'onl the model 
translation was 4.73. 
'5 \[tp://  fl, p.corne, ll.cs.ed U/l)U b/smar t/english,stop 
We set; the additional criterion that the difl'erent 
metrics hould be able to determine whether the top- 
ranking translation (:mMida.te is likeJy to be useflfl to 
the translator, and that no outlmt shouhl lm given if' 
the chlsest nmt('hing translation record was outside 
a certain l '~/Ilg( ~. Of "transla.ti(m uscflflness'. In p2"ac- 
tice, this was set to the, edit distance between the 
model translation and the empty string (i.e. the e.dit 
(:()st; of creating th(; model translation fl'(nn s(:ratch). 
This cut;off' 1)oint vlts realised for the different sim- 
ilarity metrics by thrcshohling over the similarit.y 
scores. The ditferent hresholds ettled Ull(m experi- 
mentally for all similarity metrics are given ill t)ra(:k- 
cts in the second column of Table 1, with the thresh- 
ohl for (;(lit, distance dynamicMly set t(/the edit dis- 
lane(; l~etween the input and tim eml)ty string. 
\Ve set (mrs(;\]ves al)art \]'IX)211 COIlV(;21I;i()IIsll 2'(~S(;D.l'('h 
()n TM r(;hieval lmrl'o2unan(:(; in a(lol)ting this ()l/- 
.i(;(:li\'(; mmmrical (~vahmti()n method. Traditionally, 
r(:i.ri(~val l)erformalm(~ has 1)(!e,n gauged 1)y tlm sub- 
j(~(:t;iv(; useflfln(;ss of the closest matching e.lenmnt of 
the syst;(~lll OUtlmt (as judged 1)y a. hunm,d, mid de- 
scribed by way of a dis(:rete set; of transla.tion (lualit;y 
des('ril)tors ((;.g. (Nakm2mra, 1989; Smnita and Tsut- 
smni, 1991; Sato, 1992)). Perhaps the closest evalua- 
tion a.tte2nt)ts o what we prol)ose are those of' Planas 
and Nn'use (1.999) in s(!tting a mechanical cutoff for 
"translation usability" as the al/ility to generate the 
model translation from a given translation candidate 
1)y editing less than half the component words, and 
Nirenburg et al (1993) ill calculating the weighted 
mmtber of key strokes r(;quirexl to convert he system 
outllut into ;m apl)ropriate translation for the orig- 
inal inllut. Tile method of Nirenburg et al (1993) 
is certainly more indicative of t:rue target language 
useflllness, but is dependent 022 the coml)etence of 
the translator editing the TM system output, and 
not automated to the degree our method is. 
4.2 Resu l ts  
The results for the different similarity metrics with 
character-based and word-based indexing are given 
in Tal)le 1, with the two bag-of-words al)t)roaches 
partitioned off from the three word order-s(msitive 
al)I)roaches tor ea(:h indexing paradigm. "Accuracy" 
is an indication of the prol)ortion of intmts fbr whi(:h 
39 
an optimal translation was produced; character- 
based indexing accuracies in bold indicate a signifi- 
cant ~ advantage over the corresponding wprd-based 
indexing accuracy, and figures in brackets for word- 
based indexing indicate the relative pert'ormaime 
gain over the corresponding character-based index- 
ing configuration. "Edit discrep." refers to the mean 
minimum edit distance discrepancy between trans- 
lation candidate(s) and optimal translation(s) in the 
case of the translation candidate set containiug uo 
optimal translations. "Ave. outputs" describes the 
average number of translation candidates output by 
the system, with the figure in brackets being the 
proportion of int)uts for which a unique translation 
candidate was produced. "Ave. time" describes the 
average time taken to deterlnine the translation era> 
didate(s) for a single output, relative to the time 
taken tbr word-based edit distance retrieval. 
Perhaps the most striking result is ttmt character- 
based indexing produces a superior match accuracy 
to word-based indexing tbr all similarity metrics, at; 
a significant margin tbr all three word order-based 
methods. This is the complete opposite of what we 
had expected, although it does fit in with the find- 
ings of Fujii and Croft (1993) that character-based 
indexing performs comparably with word-based in- 
dexing in Japanese information retrieval. 
Looking to word order, we see that edit distance 
outperforms all other methods for t)oth character- 
and word-based indexing, peaking at just over 50% 
for character-based indexing. Tile relative perfor- 
mance of the remaining methods is variable, with 
the two bag-of-words methods being superior to or 
roughly equivalent to sequential correspondence and 
weighted sequential correspondence tbr word-based 
indexing, but tile word order-based methods having 
a cleat' advantage over the bag-of-words methods for 
character-based indexing. It is thus difticult to draw 
any hard and fast conclusion as to the relative merits 
of word order-based versus bag-of words methods, 
other than to say that edist distance would appear 
to have a clear advantage over other methods. 
The figures for edit discrepancy in the case of non- 
optimal translation candidate(s) are equally inter- 
esting, and suggest hat on the whole, the various 
methods err more conservatively for character-based 
than word-based indexing. The most robust method 
is (source language) edit distance, at all edit dis- 
crepancy of 1.82 and 2.O3 for character-based and 
word-based indexing, respectively. 
All methods were able to produce just over one 
translation candidate on average, with all other than 
edit distance returning a unique translation candi- 
date over 90% of the time. The greater number of 
outtmts for the edit distance method can certainly 
be viewed as one reason for its inflated performance, 
although the lower level of mnbiguity for character- 
based indexing but higher accuracy, would tend to 
suggest otherwise. 
Lastly, word-based indexing was found to be faster 
than character-based indexing across the board, for 
the simple reason that the immber of character seg- 
~As determined by the paired t test (p < 0.05). 
ments is always going to be greater than or equal 
to the number of word segments. The average seg- 
ment lengths quoted above (26.1 characters vs. 13.4 
words) indicate that we generally have twice as many 
characters as words in a given striug. Additionally, 
tile acceleration technique described in ? 3.2 of se- 
quentially working through the segment component 
of the input string in increasing order of global fre- 
quency, has a greater ett>ct for word-tmsed index- 
ing than character-based indexing, accentuating any 
speed disparity. 
4.3 Ref lec t ions  on  the  resul ts  
An immediate xlflanation tbr character-based in- 
dexing's empirical edge over word-based iudexing is 
the semantic smoothing effects of individual kanji 
characters, alluded to above (? 2). To take an exam- 
ple, the single-segment ouns A': n \[s6sa\] and : ng0 
\[sadS\] both mean "operation", but would not match 
under word-based indexing. Character-based index- 
ing, on the other hand, would recogifise the overlap 
in character content, and in the process pick up on 
the semantic orresi)ondenee b tween the two words. 
To take tile opposite tack, one reason wily word- 
based indexing may have been disadvantaged is the 
we did not stem or lemmatise words in word-based 
indexing. Having said this, the. output fl'om ChaSen 
is such that stems of inflecting words are given as 
a single segment, with inflectional morphemes each 
presented as sel)arate segments. In this sense, stem- 
ruing would only act to delete the inflectional mor- 
phemes, and not add allything new. 
Another way in which the outlmt of ChaSen 
could conceivably have atlbcted retrieval perfor- 
iilance is that technical terms tended to be over- 
segmented. Experilnentally combining recognised 
technical terms into a single segment (particularly 
in the case of contiguous katakana segments in the 
manner of Nljii and Croft (1993)), however, de- 
graded rather than lint)roved retrieval performance 
for both character-based and word-based indexing. 
As such, this side-etfect of ChaSen would not appear 
to have impinged on retriewfl accuracy. 
One other plausible reason for tile unexpected re- 
sults is that the test data could have been ill some 
way inherently better suited to character-based in-
dexing than word-based indexing, although the fact 
that the results were cross-wtlidatcd would tend to 
rule out this possibility. 
A surprising result was the lacklustre performance 
of the weighted sequential correspondence method as 
compared to simple sequential correspondence. We 
have no explanation for the drop in accuracy, other 
than to speculate that either the proposed formu- 
lation is in some way flawed or contiguity of match 
does not impinge on translation similarity to the de- 
gree we had expected. 
To return to the original question posed above of 
retrieval speed vs. accuracy, the word order-sensitive 
edit distance approach would seem to hold a gen- 
uine edge over the other methods, to an order that 
would suggest he extra computational overhead is 
warranted, ill both accuracy and translation discrep- 
ancy. It must be said that the TM used in evalua- 
40 
tion was too small to get a gemfine f(;el for the com- 
t)ul;ational overhead that would 1)e cxp(,,ri(;ncc, d in 
~ real-world TM system context of t)ot;entially mil- 
lions rath(;r than thousands of translation records. 
A C the saint', (tim(;, however, coding Ul) the c(lit dis- 
tan(:(; l)roc(',dure in a language fasto, r than Perl using 
chara(;l;(?r ~d;h(~,r \[;\]lall SI;t'illg COIlq)arisol~ 1)roc(?(hlrcs 
mid ai)l)lying (lynami(" 1)rogl'amming t(whni(lu(,,s or 
similar, may well oIl~set h('. large \]nero.as(; in number 
of comparisons dcmand(',d of the system. 
5 Concluding remarks 
This research is concerned with l;}m r(;lativ(~ iml)orl; 
ot7 word order and segm(mta.1;ion n translation re- 
l;rieval i)erformmlc(~ tbr a TM system. Wc mo(Ml('xl 
the elthcts of word order s(msitivity vs. 1)ag-of-wol'dS 
word order ins(msit;ivity 1)y iml)l(mmnl,ing a total of 
live similarity mcla'ics: two bag-of-words al)proach(',s 
(lhe v(',(:tor spa(:(; model and "tol?('.n int(us(!(:tion") 
and tin'('.(', w()r(l ord(',r-s(;nsitive al)l)roach(',s ((',(lit; dis- 
tan(:('., "s(;quential corr(',Sl)ond(',nce" and "wcight(',d 
sequential corr(~st)ondenc(?'). Ea(:h of th(;s(; nw, tri(',s 
was then l;(~sl;e(t Hll(ler (:har;~cl;(;r-1)as(~(\[ al~(t word- 
based in(h'~xing, to deto, rmin(,~ what (;tt'c(:t s(~gm(',nta- 
l;ion wouhl have, on r('.trieval 1)(~rl'orman(:(h Eml)iri- 
c~d evaluation })asc, d }l l 'O l l l ld  \[,h( ~, l;alg(!l, languag(', (;(tit 
distance of t)rot)osed traiMa.tion can(lidal(',s r(~vcaicd 
that (:hara(:tcr-1)ascd indexing consist(mtly produ(:ed 
gr('~atcr accuracy than wordq)ased in(lexiltg; and thai; 
the word or(l(~r-s('atsitivo~ (;(lit distain:(', m(;tri(: clearly 
outl)(',rforme(1 all other methods un(h',r 1)oth in(l(',xing 
paradigms. 
The main area in wlfi(',h we, fc!d this r(~s(!ar(:h c(mht 
1)c, (mhan(:(~d is to validate th(~ findings of this 1)a- 
per in (~Xlmn(ling evahlati()n 1o olh(w domains mid 
l;esl; Set,q, whi(:h wc h'av(', as ;lll il:(?lll 1'()1 t'ulm(~ re- 
s(mr(:h. We also skirl;ed m'(mnd lira issu(~ ()f lrmls- 
lation record partitioning, and wish 11)inv(!stigale 
how difl'(;r(mt 1)mtitioning m(~'tho(ls lmrfl)rm againsl; 
c,;mh other. One important area in which w(; hop(~ 
to eXl)and our resem'ch is to look at tim etl'(~(:ts of 
character type on chm'act(',r-bas(~d in exing, t(anji 
would a,ppear to be helping the case of character- 
based indexing at t)rc, s(mt, ;rod it woul(\[ 1)e highly 
r(;vcaling to look at wh(',th(',r COml)ara,1)l(', ro, sults to 
t\]losc 1)r(:s(;nt('d h(;r(~ would 1)(', t)ro(ht(:ed \[or full 
kaim-basc'd (alphal)c, ti(:) ,lal)an(',sc input, or otlmr 
all)hal)ct-1)ased n(m-s(~gm(ulting languages such as 
Thai. 
Acknowledgements  
Vital input into this research was rcc(~ivcd t?om 
Francis Bond (NTT), Emmanu(;1 Planas (NTT), and 
three anonylllOUS reviewers. 
References 
% Baldwin mul Ill. Tanaka. 1999. The applications of 
unsul)crvised learning to ,I~tl)mmsc gral)\]mmc,-1)honcin(~ 
aligmnent,. In l'roc, of th.e. AUL I.Vortc.d~op oa Uu- 
supervised Learning in Natu'ral Language l~roccs.sin9, 
pages 9 16. 
L. Cranias, H. Ibqmgr.orgiou, and S. Pilmridis. 1995. 
A Matching Technique in Example-Based Machine 
Translation. cmp-lg/9508005. 
L. Cranias, H. Papageorgiou, and S. Piper\]dis. 1997. E?- 
amt)h~ retrieval from a trmlslation memory. Natwral 
Language \]'Jngine.ering, 3(4):255 77. 
1t. Fuji\] and W.B. Croft. 1.993. A comparison of index- 
ing tc(:lmiqu(~s fl)r .lal)ancsc t;c.x|; r('.trieval. In Proc. 
of 161h International ACM-SIGH~, Cot@fence on Re- 
search and Dc'vclopmcnt in Information Ib:tricval (SI- 
GIR'93), pages 237 46. 
It;. Kitamura and II. "~Smmmoto. 1996. Translation 
retrieval systo.m using alignment data flom parallc.l 
texts, in P~wc. of the 5&'d Annual Mccting of tit(" 
II'S,I, volmne 2, pages 385 6. (Ill Ja.t)ancsc ). 
C. Manning and II. S(:hiil;ze. 1999. Foundations of Sta- 
tistical Natural La'ngurtgc P~vccssing. MIT Press. 
Y. Matsmnoto, A. I(i/,auchi, T. Yamashita, and Y. IIi- 
rano. 1999. ,\]apancsc Moudtolo.qical Analysis S?/s- 
l, cm UttaScn Version 2.0 Manual. ~lt'~chnical l/.eporl; 
NAISqUIS-Tl199009, NAIST. 
M. Nagao and S. Mort. 1994. A new method of N-grant 
statist;its tbr large mmflmr of N and ;mtonmtic ex- 
\[;ra(;1;ion of words and l)hrases front large text; data 
of .lapanese. In Proc. of the 15th, lntc~'~u~tioual Con- 
Jcrcncc on Computational Linguistics (COLING '9/~), 
pages 611- 5. 
N. Nakamma. 1989. ~l?~mslat, ion supl)orf by retrieving 
bilingual texts. In l'~wc, of the 38th Annual Mcctin 9 
of the IPSJ, volume 1, pagt;s 357 8. (In Jai)ancs(; ).
S. Nirelflmrg, C. l)omashnc.v, and \]).J. Gramms. 1993. 
Two apt)roa(:hes to mat;thing in eXaml)h>bas(~d rim- 
chin(', translation. In Proc. of the 5th International 
CoT@:rc'ncc on 771corctical and Mcthodologic(d lasucs 
i'tl. Math, inc. 7!ransl,,tio'a 151'M1-93), pages d7 57. 
E. Planas and (). l:uruse. 1999. F(wmalizing translation 
m(m,n'ies. In l)Twc, o.f Math\]n(: Translation ,%m'mit 
VII, pages 331 9. 
1'2. Planas. 1998. A Case, Study on Memory Based Ma- 
chine ~}'anslation 7bols. Phi) Felkm~ \Vorking 1)al)c.r, 
Unil;ed Nations University. 
G. Salton. 1971. The SMAR, T It, err\]oval Sy.stevt: E:rpcr- 
ime.nt.s in Automatic Document Processing. Prentice- 
Hall. 
S. Sato and 3'. Kawase. 1994. A ltigh-Spc.ed B(:st Match 
i{e.tricval Method fin" ,\]apancsc ~}:'a;t. Tct:lulical Rctmrt; 
1S-11R-94-9I, JAIST. 
S. Sato. 1992. CTM: An examlfl('A)ased translation aid 
system. In l"~vc, of the 141h International Confcrc.ncc 
on Computational Linguistics (COLING '92), pages 
1259 63. 
E. Smnit;}~ mtd Y. Tsutsumi. 1991. A 1)ract,ical method 
of retrieving similar examples 1or trmMation aid. 
7Yansaction,s of the IEICE, J74-D-II(10):1437 47. (In 
Japanese). 
It. Tanaka. 1.997. An efficient way of gauging siinilar- 
ity lmtwcen hmg .lalmnc, so, expressions. In Informa- 
tion l~roccssin9 ,%ciety of Japan SIG Notes, vohun(! 
,1t7, no. 85, 1)ages 69 74. (In .l~q)aneso,). 
A. Wagner and M. Fisher. 1974. The' string-to-string 
correction 1)roblcm. Journal of the A CM, 21(1):168 
73. 
41 
	 
 
  	 
 	
 	 	   	
	 
 
	 


  
 



  Bringing the Dictionary to the User: the FOKS system
Slaven Bilac?, Timothy Baldwin? and Hozumi Tanaka?
? Tokyo Institute of Technology
2-12-1 Ookayama, Meguro-ku, Tokyo 152-8552 JAPAN
{sbilac,tanaka}@cl.cs.titech.ac.jp
? CSLI, Ventura Hall, Stanford University
Stanford, CA 94305-4115 USA
tbaldwin@csli.stanford.edu
Abstract
The dictionary look-up of unknown words is partic-
ularly difficult in Japanese due to the complicated
writing system. We propose a system which allows
learners of Japanese to look up words according to
their expected, but not necessarily correct, reading.
This is an improvement over previous systems which
provide no handling of incorrect readings. In prepro-
cessing, we calculate the possible readings each kanji
character can take and different types of phonolog-
ical and conjugational changes that can occur, and
associate a probability with each. Using these prob-
abilities and corpus-based frequencies we calculate a
plausibility measure for each generated reading given
a dictionary entry, based on the naive Bayes model.
In response to a reading input, we calculate the plau-
sibility of each dictionary entry corresponding to the
reading and display a list of candidates for the user
to choose from. We have implemented our system
in a web-based environment and are currently eval-
uating its usefulness to learners of Japanese.
1 Introduction
Unknown words are a major bottleneck for learners
of any language, due to the high overhead involved in
looking them up in a dictionary. This is particularly
true in non-alphabetic languages such as Japanese,
as there is no easy way of looking up the component
characters of new words. This research attempts to
alleviate the dictionary look-up bottleneck by way
of a comprehensive dictionary interface which allows
Japanese learners to look up Japanese words in an ef-
ficient, robust manner. While the proposed method
is directly transferable to other language pairs, for
the purposes of this paper, we will focus exclusively
on a Japanese?English dictionary interface.
The Japanese writing system consists of the
three orthographies of hiragana, katakana and kanji,
which appear intermingled in modern-day texts
(NLI, 1986). The hiragana and katakana syllabaries,
collectively referred to as kana, are relatively small
(46 characters each), and each character takes a
unique and mutually exclusive reading which can
easily be memorized. Thus they do not present a
major difficulty for the learner. Kanji characters
(ideograms), on the other hand, present a much big-
ger obstacle. The high number of these characters
(1,945 prescribed by the government for daily use,
and up to 3,000 appearing in newspapers and formal
publications) in itself presents a challenge, but the
matter is further complicated by the fact that each
character can and often does take on several different
and frequently unrelated readings. The kanji
 
, for
example, has readings including hatsu and ta(tsu),
whereas  has readings including omote, hyou and
arawa(reru). Based on simple combinatorics, there-
fore, the kanji compound
 
 happyou ?announce-
ment? can take at least 6 basic readings, and when
one considers phonological and conjugational varia-
tion, this number becomes much greater. Learners
presented with the string
 
 for the first time will,
therefore, have a possibly large number of potential
readings (conditioned on the number of component
character readings they know) to choose from. The
problem is further complicated by the occurrence of
character combinations which do not take on com-
positional readings. For example   kaze ?com-
mon cold? is formed non-compositionally from 
kaze/fuu ?wind? and  yokoshima/ja ?evil?.
With paper dictionaries, look-up typically occurs
in two forms: (a) directly based on the reading of the
entire word, or (b) indirectly via component kanji
characters and an index of words involving those
kanji. Clearly in the first case, the correct reading
of the word must be known in order to look it up,
which is often not the case. In the second case, the
complicated radical and stroke count systems make
the kanji look-up process cumbersome and time con-
suming.
With electronic dictionaries?both commercial
and publicly available (e.g. EDICT (2000))?the
options are expanded somewhat. In addition to
reading- and kanji-based look-up, for electronic
texts, simply copying and pasting the desired string
into the dictionary look-up window gives us direct
access to the word.1. Several reading-aid systems
1Although even here, life is complicated by Japanese being
a non-segmenting language, putting the onus on the user to
(e.g. Reading Tutor (Kitamura and Kawamura,
2000) and Rikai2) provide greater assistance by seg-
menting longer texts and outputing individual trans-
lations for each segment (word). If the target text
is available only in hard copy, it is possible to use
kana-kanji conversion to manually input component
kanji, assuming that at least one reading or lexical
instantiation of those kanji is known by the user. Es-
sentially, this amounts to individually inputting the
readings of words the desired kanji appear in, and
searching through the candidates returned by the
kana-kanji conversion system. Again, this is com-
plicated and time inefficient so the need for a more
user-friendly dictionary look-up remains.
In this paper we describe the FOKS (Forgiving
Online Kanji Search) system, that allows a learner
to use his/her knowledge of kanji to the fullest extent
in looking up unknown words according to their ex-
pected, but not necessarily correct, reading. Learn-
ers are exposed to certain kanji readings before oth-
ers, and quickly develop a sense of the pervasiveness
of different readings. We attempt to tap into this
intuition, in predicting how Japanese learners will
read an arbitrary kanji string based on the relative
frequency of readings of the component kanji, and
also the relative rates of application of phonological
processes. An overall probability is attained for each
candidate reading using the naive Bayes model over
these component probabilities. Below, we describe
how this is intended to mimic the cognitive ability
of a learner, how the system interacts with a user
and how it benefits a user.
The remainder of this paper is structured as fol-
lows. Section 2 describes the preprocessing steps of
reading generation and ranking. Section 3 describes
the actual system as is currently visible on the in-
ternet. Finally, Section 4 provides an analysis and
evaluation of the system.
2 Data Preprocessing
2.1 Problem domain
Our system is intended to handle strings both in the
form they appear in texts (as a combination of the
three Japanese orthographies) and as they are read
(with the reading expressed in hiragana). Given a
reading input, the system needs to establish a rela-
tionship between the reading and one or more dictio-
nary entries, and rate the plausibility of each entry
being realized with the entered reading.
In a sense this problem is analogous to kana?kanji
conversion (see, e.g., Ichimura et al (2000) and
Takahashi et al (1996)), in that we seek to deter-
mine a ranked listing of kanji strings that could cor-
respond to the input kana string. There is one major
difference, however. Kana?kanji conversion systems
correctly identify word boundaries.
2http://www.rikai.com
are designed for native speakers of Japanese and as
such expect accurate input. In cases when the cor-
rect or standardized reading is not available, kanji
characters have to be converted one by one. This can
be a painstaking process due to the large number of
characters taking on identical readings, resulting in
large lists of characters for the user to choose from.
Our system, on the other hand, does not assume
100% accurate knowledge of readings, but instead
expects readings to be predictably derived from the
source kanji. What we do assume is that the user
is able to determine word boundaries, which is in
reality a non-trivial task due to Japanese being non-
segmenting (see Kurohashi et al (1994) and Na-
gata (1994), among others, for details of automatic
segmentation methods). In a sense, the problem of
word segmentation is distinct from the dictionary
look-up task, so we do not tackle it in this paper.
To be able to infer how kanji characters can be
read, we first determine all possible readings a kanji
character can take based on automatically-derived
alignment data. Then, we machine learn phonologi-
cal rules governing the formation of compound kanji
strings. Given this information we are able to gen-
erate a set of readings for each dictionary entry that
might be perceived as correct by a learner possessing
some, potentially partial, knowledge of the charac-
ter readings. Our generative method is analogous
to that successfully applied by Knight and Graehl
(1998) to the related problem of Japanese (back)
transliteration.
2.2 Generating and grading readings
In order to generate a set of plausible readings we
first extract all dictionary entries containing kanji,
and for each entry perform the following steps:
1. Segment the kanji string into minimal morpho-
phonemic units3 and align each resulting unit
with the corresponding reading. For this pur-
pose, we modified the TF-IDF based method
proposed by Baldwin and Tanaka (2000) to ac-
cept bootstrap data.
2. Perform conjugational, phonological and mor-
phological analysis of each segment?reading
pair and standardize the reading to canonical
form (see Baldwin et al (2002) for full de-
tails). In particular, we consider gemination
(onbin) and sequential voicing (rendaku) as the
most commonly-occurring phonological alterna-
tions in kanji compound formation (Tsujimura,
1996)4. The canonical reading for a given seg-
3A unit is not limited to one character. For example, verbs
and adjectives commonly have conjugating suffices that are
treated as part of the same segment.
4In the previous example of    happyou ?announcement?
the underlying reading of individual characters are hatsu and
hyou respectively. When the compound is formed, hatsu seg-
ment is the basic reading to which conjugational
and phonological processes apply.
3. Calculate the probability of a given segment be-
ing realized with each reading (P (r|k)), and
of phonological (P
phon
(r)) or conjugational
(P
conj
(r)) alternation occurring. The set of
reading probabilities is specific to each (kanji)
segment, whereas the phonological and conju-
gational probabilities are calculated based on
the reading only. After obtaining the compos-
ite probabilities of all readings for a segment,
we normalize them to sum to 1.
4. Create an exhaustive listing of reading candi-
dates for each dictionary entry s and calculate
the probability P (r|s) for each reading, based
on evidence from step 3 and the naive Bayes
model (assuming independence between all pa-
rameters).
P (r|s) = P (r
1..n
|k
1..n
) (1)
P (r
1..n
|k
1..n
) =
n
?
i=1
P (r
i
|k
i
)?
?P
phon
(r
i
) ? P
conj
(r
i
) (2)
5. Calculate the corpus-based frequency F (s) of
each dictionary entry s in the corpus and then
the string probability P (s), according to equa-
tion (3). Notice that the term
?
i
F (s
i
) de-
pends on the given corpus and is constant for
all strings s.
P (s) =
F (s)
?
i
F (s
i
)
(3)
6. Use Bayes rule to calculate the probability
P (s|r) of each resulting reading according to
equation (4).
P (s|r)
P (s)
=
P (r|s)
P (r)
(4)
Here, as we are only interested in the relative
score for each s given an input r, we can ig-
nore P (r) and the constant
?
i
F (s
i
). The final
plausibility grade is thus estimated as in equa-
tion (5).
Grade(s|r) = P (r|s) ? F (s) (5)
The resulting readings and their scores are stored
in the system database to be queried as necessary.
Note that the above processing is fully automated,
a valuable quality when dealing with a volatile dic-
tionary such as EDICT.
ment undergoes gemination and hyou segment undergoes se-
quential voicing resulting in happyou surface form reading.
3 System Description
The section above described the preprocessing steps
necessary for our system. In this section we describe
the actual implementation.
3.1 System overview
The base dictionary for our system is the publicly-
available EDICT Japanese?English electronic dictio-
nary.5 We extracted all entries containing at least
one kanji character and executed the steps described
above for each. Corpus frequencies were calculated
over the EDR Japanese corpus (EDR, 1995).
During the generation step we ran into problems
with extremely large numbers of generated readings,
particularly for strings containing large numbers of
kanji. Therefore, to reduce the size of generated
data, we only generated readings for entries with
less than 5 kanji segments, and discarded any read-
ings not satisfying P (r|s) ? 5 ? 10?5. Finally, to
complete the set, we inserted correct readings for
all dictionary entries s
kana
that did not contain any
kanji characters (for which no readings were gener-
ated above), with plausibility grade calculated by
equation (6).6
Grade(s
kana
|r) = F (s
kana
) (6)
This resulted in the following data:
Total entries: 97,399
Entries containing kanji: 82,961
Average number of segments: 2.30
Total readings: 2,646,137
Unique readings: 2,194,159
Average entries per reading: 1.21
Average readings per entry: 27.24
Maximum entries per reading: 112
Maximum readings per entry: 471
The above set is stored in a MySQL relational
database and queried through a CGI script. Since
the readings and scores are precalculated, there is no
time overhead in response to a user query. Figure 1
depicts the system output for the query atamajou.7
The system is easily accessible through any
Japanese language-enabled web browser. Currently
we include only a Japanese?English dictionary but
it would be a trivial task to add links to translations
in alternative languages.
3.2 Search facility
The system supports two major search modes: sim-
ple and intelligent. Simple search emulates a
conventional electronic dictionary search (see, e.g.,
5http://www.csse.monash.edu.au/~jwb/edict.html
6Here, P (r|s
kana
) is assumed to be 1, as there is only one
possible reading (i.e. r).
7This is a screen shot of the system as it is visible at
http://tanaka-www.titech.ac.jp/foks/.
Figure 1: Example of system display
Breen (2000)) over the original dictionary, taking
both kanji and kana as query strings and displaying
the resulting entries with their reading and transla-
tion. It also supports wild character and specified
character length searches. These functions enable
lookup of novel kanji combinations as long as at least
one kanji is known and can be input into the dictio-
nary interface.
Intelligent search is over the set of generated
readings. It accepts only kana query strings8 and
proceeds in two steps. Initially, the user is provided
with a list of candidates corresponding to the query,
displayed in descending order of the score calculated
from equation (5). The user must then click on the
appropriate entry to get the full translation. This
search mode is what separates our system from ex-
isting electronic dictionaries.
3.3 Example search
Let us explain the benefit of the system to the
Japanese learner through an example. Suppose the
user is interested in looking up    zujou ?over-
head? in the dictionary but does not know the cor-
rect reading. Both   ?head? and  ?over/above?
are quite common characters but frequently realized
with different readings, namely atama, tou, etc. and
ue, jou, etc., respectively. As a result, the user could
interpret the string    as being read as atamajou
or toujou and query the system accordingly. Tables
1 and 2 show the results of these two queries.9 Note
that the displayed readings are always the correct
readings for the corresponding Japanese dictionary
entry, and not the reading in the original query. For
8In order to retain the functionality offered by the simple
interface, we automatically default all queries containing kanji
characters and/or wild characters into simple search.
9Readings here are given in romanized form, whereas they
appear only in kana in the actual system interface. See Figure
1 for an example of real-life system output.
Entry Reading Grade Translation
   zujou 0.40844 overhead
   tousei 0.00271168 head voice
Table 1: Results of search for atamajou
Entry Reading Grade Translation
 
toujou 73.2344 appearance
   zujou 1.51498 overhead
 	
toujou 1.05935 embarkation

 
toujou 0.563065 cylindrical
 
doujou 0.201829 dojo

 toujou 0.126941 going to Tokyo
 
shimoyake 0.0296326 frostbite
 
toushou 0.0296326 frostbite
 
toushou 0.0144911 swordsmith
   tousei 0.0100581 head voice


toushou 0.00858729 sword wound
 
tousou 0.00341006 smallpox
 
tousou 0.0012154 frostbite
 
toushin 0.000638839 Eastern China
Table 2: Results of search for toujou
all those entries where the actual reading coincides
with the user input, the reading is displayed in bold-
face.
From Table 1 we see that only two results are
returned for atamajou, and that the highest rank-
ing candidate corresponds to the desired string  
 . Note that atamajou is not a valid word in
Japanese, and that a conventional dictionary search
would yield no results.
Things get somewhat more complicated for the
reading toujou, as can be seen from Table 2. A total
of 14 entries is returned, for four of which toujou is
the correct reading (as indicated in bold). The string
   is second in rank, scored higher than three en-
tries for which toujou is the correct reading, due to
the scoring procedure not considering whether the
generated readings are correct or not.
For both of these inputs, a conventional system
would not provide access to the desired translation
without additional user effort, while the proposed
system returns the desired entry as a first-pass can-
didate in both cases.
4 Analysis and Evaluation
To evaluate the proposed system, we first provide
a short analysis of the reading set distribution and
then describe results of a preliminary experiment on
real-life data.
4.1 Reading set analysis
Since we create a large number of plausible read-
ings, one potential problem is that a large number
01
2
3
4
5
6
7
8
9
10
11
12
0 10 20 30 40 50 60 70 80 90 100 110 120
N
um
be
r o
f R
ea
di
ng
s(l
og
)
Results per Reading
Number of results returned per reading
All
Existing
Baseline
Figure 2: Distribution of results returned per read-
ing
0
3
6
9
12
15
18
21
24
27
0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
Av
er
ag
e 
nu
m
be
r o
f r
es
ul
ts
 re
tu
rn
ed
Length of reading (in characters)
Average number of results returned depending on length of reading
All
Existing
Baseline
Figure 3: Distribution of results for different query
string lengths
of candidates would be returned for each reading, ob-
scuring dictionary entries for which the input is the
correct reading. This could result in a high penalty
for competent users who mostly search the dictio-
nary with correct readings, potentially making the
system unusable.
To verify this, we tried to establish how many
candidates are returned for user queries over read-
ings the system has knowledge of, and also tested
whether the number of results depends on the length
of the query.
The distribution of results returned for different
queries is given in Figure 2, and the average num-
ber of results returned for different-length queries
is given in Figure 3. In both figures, Baseline is
calculated over only those readings in the original
dictionary (i.e. correct readings); Existing is the
subset of readings in the generated set that existed
in the original dictionary; and All is all readings in
the generated set. The distribution of the latter two
sets is calculated over the generated set of readings.
In Figure 2 the x-axis represents the number of
results returned for the given reading and the y-axis
represents the natural log of the number of readings
returning that number of results. It can be seen that
only a few readings return a high number of entries.
308 out of 2,194,159 or 0.014% readings return over
30 results. As it happens, most of the readings re-
turning a high number of results are readings that
existed in the original dictionary, as can be seen from
the fact that Existing and All are almost identical
for x values over 30. Note that the average number
of dictionary entries returned per reading is 1.21 for
the complete set of generated readings.
Moreover, as seen from Figure 3 the number of
results depends heavily on the length of the reading.
In this figure, the x-axis gives the length of the read-
ing in characters and the y-axis the average number
of entries returned. It can be seen that queries con-
taining 4 characters or more are likely to return 3
results or less on average. Here again, the Exist-
ing readings have the highest average of 2.88 results
returned for 4 character queries. The 308 readings
mentioned above were on average 2.80 characters in
length.
From these results, it would appear that the re-
turned number of entries is ordinarily not over-
whelming, and provided that the desired entries are
included in the list of candidates, the system should
prove itself useful to a learner. Furthermore, if a
user is able to postulate several readings for a target
string, s/he is more likely to obtain the translation
with less effort by querying with the longer of the
two postulates.
4.2 Comparison with a conventional system
As the second part of evaluation, we tested to see
whether the set of candidates returned for a query
over the wrong reading, includes the desired entry.
We ran the following experiment. As a data set we
used a collection of 139 entries taken from a web
site displaying real-world reading errors made by
native speakers of Japanese.10 For each entry, we
queried our system with the erroneous reading to
see whether the intended entry was returned among
the system output. To transform this collection of
items into a form suitable for dictionary querying, we
converted all readings into hiragana, sometimes re-
moving context words in the process. Table 3 gives
a comparison of results returned in simple (con-
ventional) and intelligent (proposed system) search
modes. 62 entries, mostly proper names11 and 4-
10http://www.sutv.zaq.ne.jp/shirokuma/godoku.html
11We have also implemented the proposed system with the
ENAMDICT, a name dictionary in the EDICT distribution,
Conventional Our System
In dictionary 77 77
Ave. # Results 1.53 5.42
Successful 10 34
Mean Rank 1.4 4.71
Table 3: Comparison between a conventional dictio-
nary look-up and our system
character proverbs, were not contained in the dictio-
nary and have been excluded from evaluation. The
erroneous readings of the 77 entries that were con-
tained in the dictionary averaged 4.39 characters in
length.
From Table 3 we can see that our system is able
to handle more than 3 times more erroneous read-
ings then the conventional system, representing an
error rate reduction of 35.8%. However, the average
number of results returned (5.42) and mean rank of
the desired entry (4.71 ? calculated only for suc-
cessful queries) are still sufficiently small to make
the system practically useful.
That the conventional system covers any erro-
neous readings at all is due to the fact that those
readings are appropriate in alternative contexts, and
as such both readings appear in the dictionary.
Whereas our system is generally able to return all
reading-variants for a given kanji string and there-
fore provide the full set of translations for the kanji
string, conventional systems return only the transla-
tion for the given reading. That is, with our system,
the learner will be able to determine which of the
readings is appropriate for the given context based
on the translation, whereas with conventional sys-
tems, they will be forced into attempting to contex-
tualize a (potentially) wrong translation.
Out of 42 entries that our system did not handle,
the majority of misreadings were due to the usage of
incorrect character readings in compounds (17) and
graphical similarity-induced error (16). Another 5
errors were a result of substituting the reading of
a semantically-similar word, and the remaining 5 a
result of interpreting words as personal names.
Finally, for the same data set we compared the
relative rank of the correct and erroneous readings
to see which was scored higher by our grading pro-
cedure. Given that the data set is intended to ex-
emplify cases where the expected reading is different
from the actual reading, we would expect the erro-
neous readings to rank higher than the actual read-
ings. An average of 76.7 readings was created for
allowing for name searches on the same basic methodology.
We feel that this part of the system should prove itself useful
even to the native speakers of Japanese who often experience
problems reading uncommon personal or place names. How-
ever, as of yet, we have not evaluated this part of the system
and will not discuss it in detail.
the 34 entries. The average relative rank was 12.8
for erroneous readings and 19.6 for correct readings.
Thus, on average, erroneous readings were ranked
higher than the correct readings, in line with our
prediction above.
Admittedly, this evaluation was over a data set
of limited size, largely because of the difficulty in
gaining access to naturally-occurring kanji?reading
confusion data. The results are, however, promising.
4.3 Discussion
In order to emulate the limited cognitive abilities of
a language learner, we have opted for a simplistic
view of how individual kanji characters combine in
compounds. In step 4 of preprocessing, we use the
naive Bayes model to generate an overall probability
for each reading, and in doing so assume that com-
ponent readings are independent of each other, and
that phonological and conjugational alternation in
readings does not depend on lexical context. Clearly
this is not the case. For example, kanji readings de-
riving from Chinese and native Japanese sources (on
and kun readings, respectively) tend not to co-occur
in compounds. Furthermore, phonological and con-
jugational alternations interact in subtle ways and
are subject to a number of constraints (Vance, 1987).
However, depending on the proficiency level of
the learner, s/he may not be aware of these rules,
and thus may try to derive compound readings in
a more straightforward fashion, which is adequately
modeled through our simplistic independence-based
model. As can be seen from preliminary experi-
mentation, our model is effective in handling a large
number of reading errors but can be improved fur-
ther. We intend to modify it to incorporate further
constraints in the generation process after observ-
ing the correlation between the search inputs and
selected dictionary entries.
Furthermore, the current cognitive model does not
include any notion of possible errors due to graphic
or semantic similarity. But as seen from our pre-
liminary experiment these error types are also com-
mon. For example,    bochi ?graveyard? and 
 kichi ?base? are graphically very similar but read
differently, and  mono ?thing? and  koto ?thing?
are semantically similar but take different readings.
This leads to the potential for cross-borrowing of er-
rant readings between these kanji pairs.
Finally, we are working under the assumption that
the target string is contained in the original dictio-
nary and thus base all reading generation on the
existing entries, assuming that the user will only at-
tempt to look up words we have knowledge of. We
also provide no immediate solution for random read-
ing errors or for cases where user has no intuition as
to how to read the characters in the target string.
4.4 Future work
So far we have conducted only limited tests of cor-
relation between the results returned and the target
words. In order to truly evaluate the effectiveness of
our system we need to perform experiments with a
larger data set, ideally from actual user inputs (cou-
pled with the desired dictionary entry). The reading
generation and scoring procedure can be adjusted by
adding and modifying various weight parameters to
modify calculated probabilities and thus affect the
results displayed.
Also, to get a full coverage of predictable errors,
we would like to expand our model further to in-
corporate consideration of errors due to graphic or
semantic similarity of kanji.
5 Conclusion
In this paper we have proposed a system design
which accommodates user reading errors and supple-
ments partial knowledge of the readings of Japanese
words. Our method takes dictionary entries con-
taining kanji characters and generates a number of
readings for each. Readings are scored depending on
their likeliness and stored in a system database ac-
cessed through a web interface. In response to a user
query, the system displays dictionary entries likely
to correspond to the reading entered. Initial evalua-
tion indicates that the proposed system significantly
increases error-resilience in dictionary searches.
Acknowledgements
This research was supported in part by the Re-
search Collaboration between NTT Communication
Science Laboratories, Nippon Telegraph and Tele-
phone Corporation and CSLI, Stanford University.
We would particularly like to thank Ryo Okumura
for help in the development of the FOKS system,
Prof. Nishina Kikuko of the International Student
Center (TITech) for initially hosting the FOKS sys-
tem, and Francis Bond, Christoph Neumann and
two anonymous referees for providing valuable feed-
back during the writing of this paper.
References
Timothy Baldwin and Hozumi Tanaka. 2000. A
comparative study of unsupervised grapheme-
phoneme alignment methods. In Proc. of the 22nd
Annual Meeting of the Cognitive Science Society
(CogSci 2000), pages 597?602, Philadelphia.
Timothy Baldwin, Slaven Bilac, Ryo Okumura,
Takenobu Tokunaga, and Hozumi Tanaka. 2002.
Enhanced Japanese electronic dictionary look-up.
In Proc. of LREC. (to appear).
Jim Breen. 2000. A WWW Japanese Dictionary.
Japanese Studies, 20:313?317.
EDICT. 2000. EDICT Japanese-English Dictio-
nary File. ftp://ftp.cc.monash.edu.au/pub/
nihongo/.
EDR. 1995. EDR Electronic Dictionary Technical
Guide. Japan Electronic Dictionary Research In-
stitute, Ltd. In Japanese.
Yumi Ichimura, Yoshimi Saito, Kazuhiro Kimura,
and Hideki Hirakawa. 2000. Kana-kanji conver-
sion system with input support based on pre-
diction. In Proc. of the 18th International Con-
ference on Computational Linguistics (COLING
2000), pages 341?347.
Tatuya Kitamura and Yoshiko Kawamura. 2000.
Improving the dictionary display in a reading
support system. International Symposium of
Japanese Language Education. (In Japanese).
Kevin Knight and Jonathan Graehl. 1998. Ma-
chine transliteration. Computational Linguistics,
24:599?612.
Sadao Kurohashi, Toshihisa Nakamura, Yuji Mat-
sumoto, and Makoto Nagao. 1994. Improvements
of Japanese morphological analyzer JUMAN. In
SNLR, pages 22?28.
Masaaki Nagata. 1994. A stochastic Japanese mor-
phological analyzer using a forward-DP backward-
A? N-best search algorithm. In Proc. of the 15th
International Conference on Computational Lin-
guistics )(COLING 1994, pages 201?207.
NLI. 1986. Character and Writing system Edu-
cation, volume 14 of Japanese Language Educa-
tion Reference. National Language Institute. (in
Japanese).
Masahito Takahashi, Tsuyoshi Shichu, Kenji
Yoshimura, and Kosho Shudo. 1996. Process-
ing homonyms in the kana-to-kanji conversion.
In Proc. of the 16th International Conference
on Computational Linguistics (COLING 1996),
pages 1135?1138.
Natsuko Tsujimura. 1996. An Introduction to
Japanese Linguistics. Blackwell, Cambridge,
Massachusetts, first edition.
Timothy J. Vance. 1987. Introduction to Japanese
Phonology. SUNY Press, New York.
  
	
eBonsai: An integrated environment for annotating treebanks
Ichikawa Hiroshi, Noguchi Masaki, Hashimoto Taiichi, Tokunaga Takenobu, Tanaka Hozumi
Department of Computer Science, Tokyo Institute of Technology
Tokyo Meguro ?Ookayama 2-12-1, Japan
ichikawa@cl.cs.titech.ac.jp
Abstract
Syntactically annotated corpora (tree-
banks) play an important role in re-
cent statistical natural language pro-
cessing. However, building a large tree-
bank is labor intensive and time con-
suming work. To remedy this prob-
lem, there have been many attempts to
develop software tools for annotating
treebanks.
This paper presents an integrated en-
vironment for annotating a treebank,
called eBonsai. eBonsai helps annota-
tors to choose a correct syntactic struc-
ture of a sentence from outputs of a
parser, allowing the annotators to re-
trieve similar sentences in the treebank
for referring to their structures.
1 Introduction
Statistical approach has been a main stream of
natural language processing research for the last
decade. Particularly, syntactically annotated cor-
pora (treebanks), such as Penn Treebank (Marcus
et al, 1993), Negra Corpus (Skut et al, 1997)
and EDR Corpus (Jap, 1994), contribute to im-
prove the performance of morpho-syntactic anal-
ysis systems. It is notorious, however, that build-
ing a large treebank is labor intensive and time
consuming work. In addition, it is quite difficult
to keep quality and consistency of a large tree-
bank. To remedy this problem, there have been
many attempts to develop software tools for anno-
tating treebanks (Plaehn and Brants, 2000; Bird et
al., 2002).
This paper presents an integrated environment
for annotating treebanks, called eBonsai. Fig-
ure 1 shows a snapshot of eBonsai. eBonsai
first performs syntactic analysis of a sentence us-
ing a parser based on GLR algorithm (MSLR
parser) (Tanaka et al, 1993), and provides can-
didates of its syntactic structure. An annotator
chooses a correct structure from these candidates.
When choosing a correct structure, the annotator
can consult the system to retrieve already anno-
tated similar sentences to make the current deci-
sion. Integration of annotation and retrieval is a
significant feature of eBonsai.
To realize the tight coupling of annotation and
retrieval, eBonsai has been implemented as the
following two plug-in modules of an universal
tool platform: Eclipse (The Eclipse Foundation,
2001).
? Annotation plug-in module: This module
helps to choose a correct syntactic structure
from candidate structures.
? Retrieval plug-in module: This module re-
trieves similar sentences to a sentence in
question from already annotated sentences in
the treebank.
These two plug-in modules work cooperatively
in the Eclipse framework. For example, infor-
mation can be transferred easily between these
two modules in a copy-and-past manner. Further-
more, since they are implemented as Eclipse plug-
in modules, these functionalities can also inter-
act with other plug-in modules and Eclipse native
features such as CVS.
108
Figure 1: A snapshot of eBonsai
	

	

	
 
   


   	       
 
  
   
                    
 Feature Selection in Categorizing Procedural Expressions
Mineki Takechi   , Takenobu Tokunaga

, Yuji Matsumoto  , Hozumi Tanaka 
 
Fujitsu Limited
17-25 Shinkamata 1-chome, Ota-ku, Tokyo 144-8588, Japan

Department of Computer Science, Tokyo Institute of Technology
2-12-2 Ookayama, Meguro-ku, Tokyo 152-8552, Japan
 Graduate School of Information Science, Nara Institute of Science and Technology
8916-5 Takayama-cho, Ikoma city, Nara 630-0101, Japan

mineki-t,matsu  @is.aist-nara.ac.jp,  take,tanaka  @cl.cs.titech.ac.jp
Abstract
Text categorization, as an essential com-
ponent of applications for user navigation
on the World Wide Web using Question-
Answering in Japanese, requires more ef-
fective features for the categorization of
documents and the efficient acquisition of
knowledge. In the questions addressed by
such navigation, we focus on those ques-
tions for procedures and intend to clarify
specification of the answers.
1 Introduction
Recent methodologies of text categorization as ap-
plied to Question-Answering(QA) and user naviga-
tion on the Web address new types of problems, such
as the categorization of texts based on the question
type in addition to one based on domain and genre.
For good performance in a shallow approach, which
exploits the shallow specification of texts to cate-
gorize them, requires a great deal of knowledge of
the expressions in the answers corresponding to the
questions. In most past QA research, the types of
question have been primarily restricted to fact-based
questions. However, in user navigation on the Web,
other types of questions should be supported. In this
paper, we focus on questions requiring a procedure
asking for such navigation and intend to study the
features necessary for its extraction by illustrating
the specification of its answer. In the above type of
QA, very few studies have aimed at answering ques-
tions by extracting procedural expressions from web
pages. Accordingly, a) representations in a web text
to indicate a procedure, b) the method of extracting
those representations, and c) the way to combine re-
lated texts as an answer, are issues that have not been
sufficiently clarified. Consequently, past studies do
not provide a general approach for solving this task.
In contrast, it has been reported that the texts re-
lated to QA in web pages contain many lists in the
descriptions. We decided to focus on lists including
procedural expressions and employed an approach
of extracting lists from web pages as answers. This
results in difficulty in extracting the answers written
in a different style. However, compared to seeking
answer candidates from a document set including
various web pages, it is expected that they will be
found relatively more often from the gathered lists.
In this study, our motivation is to provide users with
the means to navigate accurately and credibly to in-
formation on the Web, but not to give a complete
relevant document set with respect to user queries.
In addition, a list is a summarization made by hu-
mans, and thus it is edited to make it easy to under-
stand. Therefore, the restriction to itemized answers
doesn?t lose its effectiveness in our study. In the ini-
tial step of our work for this type of QA, we discuss a
text categorization task that divides a set of lists into
two groups: procedural and non-procedural. First,
we gathered web pages from a search engine and
extracted lists including the procedural expressions
tagged with any HTML(Hyper Text Markup Lan-
guage) list tags found, and observed their character-
istics. Then we examined Support Vector Machines
(SVMs) and sequential pattern mining relative to the
set of lists, and observed the obtained model to find
useful features for extraction of answers to explain
a relevant procedure. In the following section, we
introduce some related work. Section 3 presents the
list features including procedural expressions in the
web pages. Subsequently, we will apply our ma-
chine learning and sequential pattern mining tech-
niques to learn these features, which are briefly il-
lustrated in Section 4. Section 5 shows the results
of our categorization experiments. Finally, Section
6 presents our conclusions and Section 7 gives our
plans for future study.
2 Related Works
The questions related in all procedures were ad-
dressed by an expert system(Barr et al, 1989). How-
ever, in QA and information retrieval for open do-
main documents from the Web, the system requires a
more flexible and more machine-operable approach
because of the diversity and changeable nature of
the information resources. Many competitions, e.g.
TREC and NTCIR, are being held each year and
various studies have been presented (Eguchi et al,
2003; Voorhees, 2001). Recently, the most suc-
cessful approach has been to combine many shal-
low clues in the texts and occasionally in other lin-
guistic resources. In this approach, the performance
of passage retrieval and categorization is vital for
the performance of the entire system. In particular,
the productiveness of the knowledge of expressions
corresponding to each question type, which is prin-
cipally exploited in retrieval and categorization, is
important. In this perspective, that means that the
requirements for categorization in such applications
are different from those in previous categorizations.
Many studies have been made that are related to QA.
Fujii et al(2001) studied QA and knowledge acqui-
sition for definition type questions. Approaches by
seeking any answer text in the pages of FAQs or
newsgroups appeared in some studies(Hamada et al,
2002; Lai et al, 2002). Automatic QA systems in a
support center of organizations was addressed in a
study by Kurohashi et al(2000).
However, most of the previous studies targeting
QA address fact type or definition type questions,
such as ?When was Mozart born?? or ?What is plat-
inum??. Previous research addressing the type of
QA relevant to procedures in Japanese is inconclu-
Table 1: Result from a Search Engine.
Keyword Gathered Retrieved Vaild Pages
tejun 3,713 748 629
houhou 5,998 916 929
Table 2: Domain and Type of List.
Domain Procedures Non-Procedures All
Computer 558 ( 295 ) 1666 ( 724 ) 2224
Others 163 ( 64 ) 1733 ( 476 ) 1896
All 721 3399 4120
sive. In text categorization research, the feature se-
lection has been discussed(Taira and Haruno, 2000;
Yang and Pedersen, 1997). However, most of the
research addressed categorization into taxonomy re-
lated to domain and genre. The features that are
used are primarily content words, such as nouns,
verbs, and adjectives. Function words and frequent
formative elements were usually eliminated. How-
ever, some particular areas of text categorization,
for example, authorship identification, suggested a
feasibility of text categorization with functional ex-
pressions on a different axis of document topics.
From the perspective of seeking methods of domain-
independent categorization for QA, this paper inves-
tigates the feasibility of functional expressions as a
feature for the extraction of lists including procedu-
ral expressions.
3 Extraction of Procedural Expressions
3.1 Answering Procedures with Lists
We can easily imagine a situation in which people
ask procedural questions, for instance a user who
wants to know the procedure for installing the Red-
Hat Linux OS. When using a web search engine,
the user could employ a keyword related to the do-
main, such as ?RedHat,? ?install,? or the synonyms
of ?procedure,? such as ?method? or ?process.? In
conclusion, the search engine will often return a re-
sult that does not include the actual procedures, for
instance, only including the lists of hyperlinks to
some URLs or simple alternatives that have no in-
tentional order as is given.
This paper addresses the issue in the context of
the solution being to return to the actual procedure.
In the initial step of this study, we focused on the
case that the continuous answer candidate passage
is in the original text and furthermore restricted the
form of documentation in the list. The list could
be expected to contain important information, be-
cause it is a summarization done by a human. It
has certain benefits pertaining to computer process-
ing. These are: a) a large number of lists in FAQs or
homepages on web pages, b) some clues before and
after the lists such as title and leads, c) extraction
which is relatively easy by using HTML list tags,
e.g. <OL>,<UL>.
In this study, a binary categorization was con-
ducted, which divided a set of lists into two classes
of procedures and non-procedures. The purpose is
to reveal an effective set of features to extract a list
explaining the procedure by examining the results of
the categorization.
3.2 Collection of Lists from Web Pages
To study the features of lists contained in web pages,
the sets of lists were made according to the follow-
ing steps (see Table 1) :
Step 1 Enter tejun (procedure) and houhou
(method) to Google(Brin and Page, 1998) as
keywords, and obtain a list of URLs that are
to serve as the seeds of collection for the next
step (Gathered).
Step 2 Recursively search from the top page to the
next lower page in the hyperlink structure and
gather the HTML pages (Retrieved).
Step 3 Extract the passages from the pages in Step
2 that are tagged with <OL> or <UL>. If a list
has multiple layers with nested tags, each layer
is decomposed as an independent list (Valid
Pages).
Step 4 Collect lists including no less than two
items. The document is created in such a way
that an article is equal to a list.
Subsequently, the document set was categorized
into procedure type and non-procedure type subsets
by human judgment. For this categorization, the def-
inition of the list to explain the procedure was as
follows: a) The percentage of items including ac-
tions or operations in a list is more than or equal
to 50%. b) The contexts before and after the lists
are ignored in the judgment. An item means an ar-
ticle or an item that is prefixed by a number or a
mark such as a bullet. That generally involves mul-
tiple sentences. In this categorization, two people
categorized the same lists and a kappa test(Siegel
and Castellan, 1988) is applied to the result. We
obtained a kappa value of 0.87, i.e., a near-perfect
match, in the computer domain and 0.66, i.e., a sub-
stantial match, in the other domains. Next, the doc-
uments were categorized according to their domain
by referring to the page including a list. Table 2 lists
the results. The values in parentheses indicate the
number of lists before decomposition of nested tags.
The documents of the Computer domain were dom-
inant; those of the other domains consisted of only a
few documents and were lumped together into a doc-
ument set named ?Others.? This domain consists of
documents regarding education, medical treatment,
weddings, etc. The instructions of software usage or
operation on the home pages of web services were
also assigned to the computer domain.
3.3 Procedural Expressions in the Lists
From the observations of the categorized lists made
by humans, the following results were obtained: a)
The first sentence in an item often describes an ac-
tion or an operation. b) There are two types of items
that terminate the first sentence: nominalized and
nonnominalized. c) In the case of the nominalized
type, verbal nouns are very often used at the end
of sentence. d) Arguments marked by ga (a par-
ticle marking nominative) or ha (a particle mark-
ing topic) and negatives are rarely used, while ar-
guments marked by wo (a particle marking object)
appear frequently. e) At the end of sentences and
immediately before punctuation marks, the same ex-
pressions appear repeatedly. Verbal nouns are inher-
ent expressions verbified by being followed by the
light verb suru in Japanese. If the features above are
domain-independent characteristics, the lists in a mi-
nor domain can be categorized by using the features
that were learned from the lists in the other major
domain. The function words or flections appearing
at the ends of sentences and before punctuation are
known as markers, and specify the style of descrip-
Table 3: Types of Tags.
tag type object types
Document dv list
p item
su sentence
Part of Speech np noun[1]
prefix
snp verbal noun
vp verb
adp particle[2]
adverb
adnominal
conjunction
ajp adjuctive
aup sentece-final-particle
auxiliary verb
suffix
ij interjection
seg others (punctuation, etc.)
unknown unknown word
tion in Japanese. Thus, to explain a procedure, the
list can be expected to have inherent styles of de-
scription.
These features are very similar to those in an au-
thorship identification task(Mingzhe, 2002; Tsuboi
and Matsumoto, 2002). That task uses word n-gram,
distribution of part of speech, etc. In recent research
for web documents, frequent word sequences have
also been examined. Our approach is based on these
features.
4 Features
4.1 Baseline
In addition to the features based on the presence of
specific words, we examined sequences of words for
our task. Tsuboi et al(2002) used a method of se-
quential pattern mining, PrefixSpan, and an algo-
rithm of machine learning, Support Vector Machine
in addition to morphological N-grams. They pro-
posed making use of the frequent sequential patterns
of words in sentences. This approach is expected
to contribute to explicitly use the relationships of
1Except verbal nouns
2Except sentence-final particles
distant words in the categorization. The list con-
tains differences in the omissions of certain particles
and the frequency of a particle?s usage to determine
whether the list is procedural. Such sequential pat-
terns are anticipated to improve the accuracy of cat-
egorization. The words in a sentence are transferred
to PrefixSpan after preprocessing, as follows:
Step 1 By using ChaSen(Matsumoto et al, 1999), a
Japanese POS(Part Of Speech) tagger, we put
the document tags and the POS tags into the
list. Table 3 lists the tag set that was used.
These tags are only used for distinguishing ob-
jects. The string of tags was ignored in sequen-
tial pattern mining.
Step 2 After the first n sentences are extracted from
each list item, a sequence is made for each sen-
tence. Sequential pattern mining is performed
for an item (literal) in a sequence as a mor-
pheme.
By using these features, we conducted categoriza-
tion with SVM. It is one of the large margin classi-
fiers, which shows high generalization performance
even in high dimensional spaces(Vapnik, 1995).
SVM is beneficial for our task, because it is un-
known which features are effective, and we must use
many features in categorization to investigate their
effectiveness. The dimension of the feature space is
relatively high.
4.2 Sequential Pattern Mining
Sequential pattern mining consists of finding all fre-
quent subsequences, that are called sequential pat-
terns, in the database of sequences of literals. Apri-
ori(Agrawal and Srikant, 1994) and PrefixSpan(Pei
et al, 2001) are examples of sequential pattern min-
ing methods. The Apriori algorithm is one of the
most widely used methods, however there is a great
deal of room for improvement in terms of calcula-
tion cost. The PrefixSpan algorithm succeed in re-
ducing the cost of calculation by performing an op-
eration, called projection, which confines the range
of the search to sets of frequent subsequences. De-
tails of the PrefixSpan algorithm are provided in an-
other paper(Pei et al, 2001).
Table 4: Statistics of Data Sets.
Proc. Non-Proc. Comp. Others
Lists 721 3399 2224 1896
Items 4.6 / 2.8 4.9 / 5.7 4.8 / 6.1 4.9 / 4.4
Sen. 1.8 / 1.7 1.3 / 0.9 1.5 / 1.1 1.3 / 1.1
Char. 40.3 / 48.6 32.6 / 42.4 35.6 / 40.1 32.6 / 48.2
5 Experiments and Results
5.1 Experimental Settings
In the first experiment, to determine the categoriza-
tion capability of a domain, we employed a set of
lists in the Computer domain and conducted a cross-
validation procedure. The document set was divided
into five subsets of nearly equal size, and five dif-
ferent SVMs, the training sets of four of the sub-
sets, and the remaining one classified for testing. In
the second experiment, to determine the categoriza-
tion capability of an open domain, we employed a
set of lists from the Others domain with the docu-
ment set in the first experiment. Then, the set of the
lists from the Others domain was used in the test
and the one from the Computer domain was used
in the training, and their training and testing roles
were also switched. In both experiments, recall, pre-
cision, and, occasionally, F-measure value were cal-
culated to evaluate categorization performance. F-
measure is calculated with precision (P) and recall
(R) in formula 1.
  
	

(1)
The lists in the experiment were gathered from those
marked by the list tags in the pages. To focus on
the feasibility of the features in the lists for the cat-
egorization task, the contexts before and after each
list are not targeted. Table 4 lists four groups di-
vided by procedure and domain into columns, and
the numbers of lists, items, sentences, and charac-
ters in each group are in the respective rows. The
two values in each cell in Table 4 are the mean on
the left and the deviation on the right. We employed
Tiny-SVM1 and a implementation of PrefixSpan2 by
T. Kudo. To observe the direct effect of the fea-
tures, the feature vectors were binary, constructed
1http://cl.aist-nara.ac.jp/?taku-ku/software/TinySVM/
2http://cl.aist-nara.ac.jp/?taku-ku/software/prefixspan/
Table 5: POS Groups.
Combination of POS Computer Others
F1 all of words 9885 13031
F2 snp+np+vp+ajp 4570 7818
F3 snp+np+vp+ajp+unknown 9277 12169
F4 aup+adp+seg 608 862
F5 aup+adp+seg+unknown 5315 5213
F6 snp+aup+adp+seg 1493 2360
with word N-gram and patterns; polynomial kernel
degree d for the SVM was equal to one. Support
values for PrefixSpan were determined in an ad hoc
manner to produce a sufficient number of patterns in
our experimental conditions.
To investigate the effective features for list cate-
gorization, feature sets of the lists were divided into
five groups (see Table 5) with consideration given to
the difference of content word and function words
according to our observations (described in Section
3.3). The values in Table 5 indicate the numbers of
differences between words in each domain data set.
The notation of tags above, such as ?snp?, follows
the categories in Table 3. F2 and F3 consist of con-
tent words and F4 and F5 consist of function words.
F6 was a feature group, which added verbal nouns
based on our observations (described in Section 3.3).
To observe the performances of SVM, we com-
pared the results of categorizations in the conditions
of F3 and F5 with a decision tree. For decision tree
learning, j48.j48, which is an implementation of the
C4.5 algorithm by Weka3, was chosen.
In these experiments, only the first sentence in
each list item was used because in our preliminary
experiments, we obtained the best results when only
the first sentence was used in categorization. As
many as a thousand patterns from the top in the rank-
ing of frequencies were selected and used in condi-
tions from F1 to F6. For pattern selection, we ex-
amined the method based on frequency. In addition,
mutual information filtering was conducted in some
conditions for comparison with performances based
only on pattern frequency. By ranking these with the
mutual information filtering, we selected 100, 300,
3http://www.cs.waikato.ac.nz/?ml/weka/
Table 6: Result of Close-Domain.
Computer domain
1 1+2 1+2+3 pattern
F1 0.88/0.88 0.92/0.90 0.93/0.90 0.93/0.92
F2 0.85/0.86 0.90/0.87 0.91/0.85 0.89/0.88
F3 0.87/0.86 0.93/0.87 0.93/0.86 0.91/0.88
F4 0.81/0.81 0.85/0.85 0.86/0.86 0.86/0.86
F5 0.81/0.84 0.86/0.85 0.90/0.86 0.89/0.88
F6 0.85/0.87 0.90/0.89 0.91/0.89 0.89/0.89
Table 7: Results when Learning from Computer Do-
main.
Computer Domain - Others Domain
1 1+2 1+2+3 pattern
F1 0.60/0.46 0.69/0.45 0.72/0.45 0.66/0.48
F2 0.52/0.42 0.69/0.39 0.72/0.37 0.64/0.41
F3 0.56/0.46 0.68/0.44 0.70/0.42 0.63/0.45
F4 0.46/0.51 0.59/0.58 0.58/0.52 0.53/0.60
F5 0.43/0.50 0.52/0.48 0.61/0.48 0.53/0.53
F6 0.53/0.49 0.67/0.53 0.71/0.50 0.61/0.55
and 500 patterns from 1000 patterns. Furthermore,
the features of N-grams were varied to N=1, 1+2,
and 1+2+3 by incrementing N and adding new N-
grams to the features in the experiments.
5.2 Experimental Results
Table 6 lists the results of a 5-fold cross-validation
evaluation of the Computer domain lists. Gradu-
ally, N-grams and patterns were added to input fea-
ture vectors, thus N=1, 2, 3, and patterns. The fea-
ture group primarily constructed of content words
slightly overtook the function group, with the excep-
tion of recall, while trigram and patterns were added.
In the comparison of F2 and F4, differences in per-
formance are not as salient as differences in num-
bers of features. Incorporating verbal nouns into the
categorization slightly improved the results. How-
ever, the patterns didn?t work in this task. The same
experiment-switching the roles of the two list sets,
the Computer and the Others domain, was then per-
formed (see Tables 7 and 8).
Along with adding N-grams, the recall became
worse for the group of content words. In contrast,
the group of function words showed better perfor-
Table 8: Results when Learning from Others Do-
main.
Others Domain - Computer Domain
1 1+2 1+2+3 pattern
F1 0.90/0.52 0.95/0.60 0.97/0.56 0.95/0.64
F2 0.88/0.51 0.92/0.44 0.94/0.37 0.94/0.47
F3 0.90/0.46 0.95/0.48 0.97/0.41 0.96/0.49
F4 0.80/0.33 0.79/0.58 0.79/0.55 0.79/0.59
F5 0.83/0.51 0.85/0.54 0.88/0.51 0.87/0.53
F6 0.81/0.51 0.90/0.56 0.94/0.51 0.89/0.56
mance in the recall, and the overall balance of pre-
cision and recall were well-performed. Calculating
the F-measure with formula 1, in most evaluations of
open domain, the functional group overtook the con-
tent group. This deviation is more salient in the Oth-
ers domain. In the results of both the Computer do-
main and the Others domain, the model trained with
functions performed better than the model trained
with content. The function words in Japanese char-
acterize the descriptive style of the text, meaning
that this result shows a possibility of the acquisi-
tion of various procedural expressions. From an-
other perspective, when trigram was added as a fea-
ture, performance took decreased in recall. Adding
the patterns, however, improved performance. It is
assumed that there are dependencies between words
at a distance greater than three words, which is ben-
eficial in their categorization. Table 9 compares the
results of SVM and j48.j48 decision tree. Table 10
lists the effectiveness of mutual information filter-
ing. In both tables, values show the F-measure cal-
culated with formula 1. According to Table 9, SVM
overtook j48.j48 overall. j48.j48 scarcely changes
with an increase in the number of features, however,
SVM gradually improves performance. For mutual
information filtering, SVM marked the best results
with no-filter in the Computer domain. However,
in the case of learning from the Others domain, the
mutual information filtering appears effective.
5.3 Discussion
The comparison of SVM and decision tree shows the
high degree of generalization of SVM in a high di-
mensional feature space. From the results of mutual
information filtering, we can recognize that the sim-
Table 9: Comparison of SVM and Decision Tree.
1 1+2 1+2+3
SVM j48 SVM j48 SVM j48 #feature
F3 0.84 0.79 0.84 0.83 0.84 0.83 300
0.85 0.76 0.85 0.81 0.84 0.82 500
0.84 0.76 0.86 0.82 0.86 0.83 1000
0.87 0.76 0.87 0.82 0.87 0.83 5000
F5 0.84 0.79 0.84 0.82 0.82 0.81 300
0.85 0.80 0.85 0.81 0.83 0.82 500
0.86 0.80 0.86 0.81 0.84 0.81 1000
0.84 0.80 0.86 0.82 0.87 0.82 5000
Table 10: Results of Pattern Selection with Mutual
Information Filtering.
100 300 500 no-filter
Computer F3 0.53 0.53 0.53 0.52
- Others F5 0.53 0.52 0.50 0.53
Others F3 0.74 0.74 0.75 0.65
- Computer F5 0.75 0.76 0.77 0.66
ple methods of other pre-cleaning are not notably
effective when learning from documents of the same
domain. However, the simple methods work well in
our task when learning from documents consisting
of a variety of domains.
Patterns performed well with mutual information
filtering in a data set including different domains and
genres. It appears that N-grams and credible pat-
terns are effective in acquiring the common char-
acteristics of procedural expressions across differ-
ent domains. There is a possibility that the patterns
are effective for moderate narrowing of the range of
answer candidates in the early process of QA and
Web information retrieval. In the Computer domain,
categorization performed well overall in every POS
group. That is why it includes many instruction
documents, for instance software installation, com-
puter settings, online shopping, etc., and those usu-
ally use similar and restricted vocabularies. Con-
versely, the uniformity of procedural expressions in
the Computer domain causes poorer performance
when learning from the documents of the Computer
domain than when learning from the Others domain.
We also often found in their expressions that for a
Sentence :  ?   [ menyu ]    w o    s ent a k u   s h i ,
                   ?    Sel ect    [ m enu ]    a nd
                      [ h o z o n ]     w o     k ur i k k u    s ur u .   ?
                        cl i ck      th e    s w i tch    o f      [ s a v e]  .   ?
P a tter n 1  :  ? [ ?     ? ] ?     ? w o ?     ? , ?
P a tter n 2  :  ? [ ?     ? ] ?     ? w o ?     ? . ?
Figure 1: Example of Effective Patterns.
particular class of content word, special characters
were adjusted (see Figure 1). This type of pattern
occasionally contributed the correct classification in
our experiment. The movement of the performance
of content and function word along with the addition
of N-grams is notable. It is likely that making use
of the difference of their movement more directly is
useful in the categorization of procedural text.
By error analysis, the following patterns were ob-
tained: those that reflected common expressions,
including the multiple appearance of verbs with a
case-marking particle wo. This worked well for the
case in which the procedural statement partially oc-
cupied the items of the list. Where there were fewer
characters in a list and failing POS tagging, pattern
mismatch was observed.
6 Conclusion
The present work has demonstrated effective fea-
tures that can be used to categorize lists in web pages
by whether they explain a procedure. We show that
categorization to extract texts including procedural
expressions is different from traditional text catego-
rization tasks with respect to the features and behav-
iors related to co-occurrences of words. We also
show the possibility of filtering to extract lists in-
cluding procedural expressions in different domains
by exploiting those features that primarily consist of
function words and patterns with mutual informa-
tion filtering. Lists with procedural expressions in
the Computer domain can be extracted with higher
accuracy.
7 Future works
The augmentation of the volume of data sets within
the Others domain is a considerable task. In this re-
search, the number of lists in each specific domain
of the data set within the Others domain is too few to
reveal its precise nature. In more technical domains,
the categorization of lists by humans is difficult for
people who have no knowledge of the field. An-
other unresolved problem is the nested structure of
lists. In our current method, no list is nested because
it has already been decomposed during preprocess-
ing. In some cases, this treatment incorrectly cate-
gorizes lists that can be regarded as procedural types
into another group based on the condition of accept-
ing a combination of two or more different layers of
nested lists. Another difficult point is related to the
nominal list type. According to the observations of
the differences in categorization in the Others do-
main by humans, some failures are of the nominal
type. It is difficult to distinguish such cases by fea-
tures only in lists, and more clues to recognize the
type of list are required such as, for example, the
contexts before and after the list.
Acknowledgements
My deepest gratitude is to Taku Kudo who provided
Tiny-SVM and an implementation of PrefixSpan.
References
Rakesh Agrawal and Ramakrishnan Srikant. 1994. Fast
Algorithms for Mining Association Rulesr. In Pro-
ceedings of 20th International. Conference. Very Large
Data Bases (VLDB), pages 487?499.
A. Barr, P. R. Cohen, and E. A. Feigenbaum. 1989. The
Handbook of Artificial Intelligence. Kyoritsu Shup-
pan, Tokyo. Japanese Edition Translated by K. Tanaka
and K. Fuchi.
S. Brin and L. Page. 1998. The Anatomy of a Large-
Scale Hypertexual Web Search Engine. In Proceed-
ings of 7th International World Wide Web Conference.
Koji Eguchi, Keizo Oyama, Emi Ishida, Noriko Kando,
and Kazuko Kuriyama. 2003. Overview of the Web
Retrieval Task at the Third NTCIR Workshop. Tech-
nical Report NII-2003-002E, National Institute of In-
formatics.
Atsushi Fujii and Tetsuya Ishikawa. 2001. Organizing
Encyclopedic Knowledge based on the Web and its
Application to Question Answering. In Proceedings of
the 39th Annual Meeting of the Association for Com-
putational Linguistics (ACL-EACL 2001), pages 196?
203, July.
Reiko Hamada, Ichiro Ide, Shuichi Sakai, and Hidehiko
Tanaka. 2002. Structural Analysis of Cooking Prepa-
ration Steps. The Transactions of The Institute of
Electronics, D-II Vol.J85-D-II(1):79?89, January. (in
Japanese).
Sadao Kurohashi and Wataru Higasa. 2000. Dialogue
Helpsystem based on Flexible Matching of User Query
with Natural Language Knowledge Base. In Proceed-
ings of 1st ACL SIGdial Workshop on Discourse and
Dialogue, pages 141?149.
Y. Lai, K. Fung, and C. Wu. 2002. FAQ Mining via List
Detection. In Proceedings of Workshop on Multilin-
gual Summarization and Question Answering (COL-
ING).
Yuji Matsumoto, Akira Kitauchi, Tatsuo Yamashita,
Yoshitaka Hirano, Hiroshi Matsuda, and Tomoaki
Imamura. 1999. Japanese Morphological analy-
sis System ChaSen Manual. Naist Technical Report
NAIST-IS-TR99009, Nara Institute of Science and
Technology. (in Japanese).
Jin Mingzhe. 2002. Authorship Attribution Based on
N-gram Models in Postpositional Particle of Japanese.
Mathematical Linguistic, 23(5):225?240, June.
Jian Pei, Jiawei Han, et al 2001. Prefixspan: Mining
Sequential Patterns by Prefix-Projected Growth. In
Proceedings of International Conference of Data En-
gineering, pages 215?224.
S. Siegel and NJ. Castellan, Jr. 1988. Nonparamet-
ric Statistics for the Behavioral Sciences 2nd Edition.
McGraw-Hill, New York.
Hirotoshi Taira and Masahiko Haruno. 2000. Feature
Selection in SVM Text Categorization. IPSJ Journal,
41(4):1113?1123, April. (in Japanese).
Yuta Tsuboi and Yuji Matsumoto. 2002. Authorship
Identification for Heterogeneous Documents. In IPSJ
SIG Notes, NL-148-3, pages 17?24.
Vladimir N. Vapnik. 1995. The Nature of Statistical
Learning Theory. Springer, New York.
Ellen M. Voorhees. 2001. Overview of the TREC
2001Question Answering Track. In Proceedings of the
2001 Text Retrieval Conference (TREC 2001).
Yiming Yang and Jan O. Pedersen. 1997. A Compara-
tive Study on Feature Selection in Text Categorization.
In Proceedings of ICML-97 14th International Confer-
ence on Machine Learning, pages 412?420.
Paraphrasing Japanese noun phrases using character-based indexing
Tokunaga Takenobu Tanaka Hozumi
Department of Computer Science, Tokyo Institute of Technology
Tokyo Meguro ?Ookayama 2-12-1, 152-8552 Japan
take@cl.cs.titech.ac.jp
Kimura Kenji
Abstract
This paper proposes a novel method to extract
paraphrases of Japanese noun phrases from a
set of documents. The proposed method con-
sists of three steps: (1) retrieving passages us-
ing character-based index terms given a noun
phrase as an input query, (2) filtering the re-
trieved passages with syntactic and seman-
tic constraints, and (3) ranking the passages
and reformatting them into grammatical forms.
Experiments were conducted to evaluate the
method by using 53 noun phrases and three
years worth of newspaper articles. The ac-
curacy of the method needs to be further im-
proved for fully automatic paraphrasing but the
proposed method can extract novel paraphrases
which past approaches could not.
1 Introduction
We can use various linguistic expressions to denote a con-
cept by virtue of richness of natural language. However
this richness becomes a crucial obstacle when processing
natural language by computer. For example, mismatches
of index terms cause failure of retrieving relevant docu-
ments in information retrieval systems, in which docu-
ments are retrieved on the basis of surface string match-
ing. To remedy this problem, the current information re-
trieval system adopts query expansion techniques which
replace a query term with a set of its synonyms (Baeza-
Yates and Riberto-Neto, 1999). The query expansion
works well for single-word index terms, but more sophis-
ticated techniques are necessary for larger index units,
such as phrases. The effectiveness of phrasal indexing
has recently drawn researchers? attention (Lewis, 1992;
Mitra et al, 1997; Tokunaga et al, 2002). However,
query expansion of phrasal index terms has not been fully
investigated yet (Jacquemin et al, 1997).
To deal with variations of linguistic expressions, para-
phrasing has recently been studied for various applica-
tions of natural language processing, such as machine
translation (Mitamura, 2001; Shimohata and Sumita,
2002), dialog systems (Ebert et al, 2001), QA sys-
tems (Katz, 1997) and information extraction (Shinyama
et al, 2002). Paraphrasing is defined as a process of
transforming an expression into another while keeping its
meaning intact. However, it is difficult to define what
?keeping its meaning intact? means, although it is the
core of the definition. On what basis could we consider
different linguistic expressions denoting the same mean-
ing? This becomes a crucial question when finding para-
phrases automatically.
In past research, various types of clues have been used
to find paraphrases. For example, Shinyama et al tried
to find paraphrases assuming that two sentences sharing
many Named Entities and a similar structure are likely
to be paraphrases of each other (Shinyama et al, 2002).
Barzilay and McKeown assume that two translations
from the same original text contain paraphrases (Barzi-
lay and McKeown, 2001). Torisawa used subcategoriza-
tion information of verbs to paraphrase Japanese noun
phrase construction ?NP1 no NP2? into a noun phrase
with a relative clause (Torisawa, 2001). Most of previ-
ous work on paraphrasing took corpus-based approach
with notable exceptions of Jacquemin (Jacquemin et al,
1997; Jacquemin, 1999) and Katz (Katz, 1997). In par-
ticular, text alignment technique is generally used to find
sentence level paraphrases (Shimohata and Sumita, 2002;
Barzilay and Lee, 2002).
In this paper, we follow the corpus-based approach
and propose a method to find paraphrases of a Japanese
noun phrase in a large corpus using information retrieval
techniques. The significant feature of our method is
use of character-based indexing. Japanese uses four
types of writing; Kanzi (Chinese characters), Hiragana,
Katakana, and Roman alphabet. Among these, Hiragana
and Katakana are phonographic, and Kanzi is an ideo-
graphic writing. Each Kanzi character itself has a certain
meaning and provides a basis for rich word formation
ability for Japanese. We use Kanzi characters as index
terms to retrieve paraphrase candidates, assuming that
noun phrases sharing the same Kanzi characters could be
paraphrases of each other. For example, character-based
indexing enables us to retrieve a paraphrase ??????
? (a commuting child)? for ???????? (a child
going to school)?. Note that their head is the same, ??
? (child)?, and their modifiers are different but sharing
common characters ?? (commute)? and ?? (study)?. As
shown in this example, the paraphrases generated based
on Japanese word formation rule cannot be classified in
terms of the past paraphrase classification (Jacquemin et
al., 1997).
The proposed method is summarized as follows. Given
a Japanese noun phrase as input, the method finds its
paraphrases in a set of documents. In this paper, we used
a collection of newspaper articles as a set of documents,
from which paraphrases are retrieved. The process is de-
composed into following three steps:
1. retrieving paraphrase candidates,
2. filtering the retrieved candidates based on syntactic
and semantic constraints, and
3. ranking the resulting candidates.
Newspaper articles are segmented into passages at punc-
tuation symbols, then the passages are indexed based on
Kanzi characters and stored in a database. The database
is searched with a query, an input noun phrase, to obtain a
set of passages, which are paraphrase candidates. In gen-
eral, using smaller index units, such as characters, results
in gains in recall at the cost of precision. To remedy this,
we introduce a filtering step after retrieving paraphrase
candidates. Filtering is performed based on syntactic and
semantic constraints. The resulting candidates are ranked
and provided as paraphrases.
The following three sections 2, 3 and 4 describe each
of three steps in detail. Section 5 describes experiments
to evaluate the proposed method. Finally, section 6 con-
cludes the paper and looks at the future work.
2 Retrieving paraphrase candidates
2.1 Indexing and term expansion
In conventional information retrieval, a query is given to
the system to retrieve a list of documents which are ar-
ranged in descending order of relevance. Our aim is to
obtain paraphrases given a noun phrase as a query, where
retrieved objects should be smaller than documents. We
divide a document into a set of passages at punctuation
symbols. These passages are retrieved by a given query,
a noun phrase.
The input noun phrase and the passages are segmented
into words and they are assigned part of speech tags by
a morphological analyzer. Among these tagged words,
content words (nouns, verbs, adjectives, adverbs) and un-
known words are selected. Kanzi characters contained
in these words are extracted as index terms. In addi-
tion to Kanzi characters, words written in Katakana (most
of them are imported words) and numbers are also used
as index terms. Precisely speaking, different numbers
should be considered to denote different meaning, but to
avoid data sparseness problem, we abstract numbers into
a special symbol ?num?.
As mentioned in section 1, the query expansion tech-
nique is often used in information retrieval to solve the
surface notational difference between queries and docu-
ments. We also introduce query expansion for retrieving
passage. Since we use Kanzi characters as index terms,
we need linguistic knowledge defining groups of simi-
lar characters for query expansion. However this kind of
knowledge is not available at hand. We obtain similar-
ity of Kanzi characters from an ordinary thesaurus which
defines similarity of words.
If a word t is not a Katakana word, we expand it to
a set of Kanzi characters E(t) which is defined by (1),
where Ct is a semantic class including the word t, KC is
a set of Kanzi characters used in words of semantic class
C, fr(k,C) is a frequency of a Kanzi character k used
in words of semantic class C, and Kt is a set of Kanzi
characters in word t.
E(t) = {k|k ? KCt , k? = argmaxl?Kt fr(l, Ct),
fr(k,Ct) > fr(k?, Ct)} ?Kt?
{s|s ? Ct, s is a Katakana word}
(1)
E(t) consists of Kanzi characters which is used in words
of semantic class Ct more frequently, than the most fre-
quent Kanzi character in the word t. If the word t is a
Katakana word, it is not expanded.
Let us see an expansion example of word ??? (hot
spring)?. Here we have t = ???? to expand, and we
have two characters that make the word, i.e. Kt = {
?, ? }. Suppose ???? belongs to a semantic class
Ct in which we find a set of words {??? (hot sprint
place), ???? (lukewarm water), ?? (warm water),
?? (spa), ???? (oasis), . . . }. From this word set,
we extract characters and count their occurence to obtain
KCt = { ? (35), ? (22), ? (20), ? (8),. . . }, where
a number in parentheses denotes the frequency of char-
acters in the semantic class Ct. Since the most frequent
character of Kt in KCt is ??? in this case, more fre-
quently used character ??? is added to E(t). In addi-
tion, Katakana words ???? and ?????? are added
to E(t) as well.
2.2 Term weighting
An index term is usually assigned a certain weight ac-
cording to its importance in user?s query and documents.
There are many proposals of term weighting most of
which are based on term frequency (Baeza-Yates and
Riberto-Neto, 1999) in a query and documents. Term
frequency-based weighting resides on Luhn?s assump-
tion (Luhn, 1957) that a repeatedly mentioned expression
denotes an important concepts. However it is obvious that
this assumption does not hold when retrieving paraphrase
candidates from a set of documents. For term weighting,
we use character frequency in a semantic class rather than
that in a query and documents, assuming that a character
frequently used in words of a semantic class represents
the concept of that semantic class very well.
A weight of a term k in a word t is calculated by (2).
w(k) =
?
???????
???????
100
if k is Katakana word or ?num?
100? log fr(k,Ct)?
k?inE(t)
log fr(k?, Ct)
if k is a Kanzi
(2)
Katakana words and numbers are assigned a constant
value, 100, and a Kanzi character is assigned a weight ac-
cording to its frequency in the semantic class Ct, where
k is used in the word t.
In the previous example of ????, we have obtained
an expanded term set { ?, ?, ?, ??, ???? }.
Among this set, ???? and ?????? are assigned
weight 100 because these are Katakana words, and the
rest three characters are assigned weight according to its
frequency in the class. For example, ??? is assigned
weight 100? log 35log 35+log 22+log 8 = 40.7.
2.3 Similarity
Similarity between an input noun phrase (I) and a pas-
sage (D) is calculated by summing up the weights of
terms which are shared by I and D, as defined in (3). In
the equation, k takes values over the index terms shared
by I and D, w(k) is its weight calculated as described in
the previous section.
sim(I,D) =
?
k?I?k?D
w(k) (3)
Note that since we do not use term frequency in passages,
we do not introduce normalization of passage length.
3 Syntactic and semantic filtering
The proposed method utilizes Kanzi characters as index
terms. In general, making index terms smaller units in-
creases exhaustivity to gain recall, but, at the same time, it
decreases specificity to degrade precision (Sparck Jones,
1972). We aim to gain recall by using smaller units as in-
dex terms at the cost of precision. Even though Kanzi are
ideograms and have more specificity than phonograms,
they are still less specific than words. Therefore there
would be many irrelevant passages retrieved due to coin-
cidentally shared characters. In this section, we describe
a process to filter out irrelevant passages based on the fol-
lowing two viewpoints.
Semantic constraints : Retrieved passages should con-
tain all concepts mentioned in the input noun phrase.
Syntactic constraints : Retrieved passages should have
a syntactically proper structure corresponding to the
input noun phrase.
3.1 Semantic constraints
In the indexing phase, we have decomposed an input
noun phrase and passages into a set of Kanzi characters
for retrieval. In the filtering phase, from these charac-
ters, we reconstruct words denoting a concept and verify
if concepts mentioned in the input noun phrase are also
included in the retrieved passages.
To achieve this, a retrieved passage is syntactically an-
alyzed and dependencies between bunsetu (word phrase)
are identified. Then, the correspondence between words
of the input noun phrase and bunsetu of the passage is
verified. This matching is done on the basis of sharing
the same Kanzi characters or the same Katakana words.
Passages missing any of the concepts mentioned in the
input noun phrase are discarded in this phase.
3.2 Syntactic constraints
Since passages are generated on the basis of punctuation
symbols, each passage is not guaranteed to have a syntac-
tically proper structure. In addition, a part of the passage
tends to be a paraphrase of the input noun phrase rather
than the whole passage. In such cases, it is necessary to
extract a corresponding part from the retrieved passage
and transform it into a proper syntactic structure.
By applying semantic constraints above, we have iden-
tified a set of bunsetu covering the concepts mentioned
in the input noun phrase. We extract a minimum depen-
dency structure which covers all the identified bunsetu.
Finally the extracted structure is transformed into a
proper phrase or clause by changing the ending of the
head (the right most element) and deleting unnecessary
elements such as punctuation symbols, particles and so
on.
Figure 1 illustrates the matching and transforming pro-
cess described in this section. The input noun phrase
is ??? w1 ?? w2 ? w3 ???? w4 (reduction of
telephone rate)? which consists of four words w1 . . . w4.
Suppose a passage ?????????????????
(the company?s telephone rate reduction caused. . . ? is re-
trieved. This passage is syntactically analyzed to give the
dependency structure of four bunsetu b1 . . . b4 as shown
in Figure 1.
Input NP ?? ?? ? ????
 (telephone) (charge) (of) (reduction)w1 w2 w3 w4
Retrieved ??? ????? ????? ???
passage (the company's) (telephone charge) (reduction) (caused)b1 b2 b3 b4
??????????
??????????
Extract proper structure
Transform ending
Figure 1: An example of matching and transformation
Correspondence between word w1 and bunsetu b2 is
made bacause they share a common character ???. Word
w2 corresponds to bunsetu b2 as well due to characters ?
?? and ???. And word w4 corresponds to bunsetu b3.
Although there is no counterpart of word w3, this pas-
sage is not discarded because word w3 is a function word
(postposition). After making correspondences, a mini-
mum dependency structure, the shaded part in Figure 1,
is extracted. Then the ending auxiliary verb is deleted
and the verb is restored to the base form.
4 Ranking
Retrieved passages are ranked according to the similarity
with an input noun phrase as described in section 2. How-
ever this ranking is not always suitable from the view-
point of paraphrasing. Some of the retrieved passages are
discarded and others are transformed through processes
described in the previous section. In this section, we de-
scribe a process to rerank remaining passages according
to their appropriateness as paraphrases of the input noun
phrase. We take into account the following three factors
for reranking.
? Similarity score of passage retrieval
? Distance between words
? Contextual information
The following subsections describe each of these factors.
4.1 Similarity score of retrieval
The similarity score used in passage retrieval is not suffi-
cient for evaluating the quality of the paraphrases. How-
ever, it reflects relatedness between the input noun phrase
and retrieved passages. Therefore, the similarity score
calculated by (3) is taken into account when ranking para-
phrase candidates.
4.2 Distance between words
In general, distance between words which have a de-
pendency relation reflects the strength of their semantic
closeness. We take into account the distance between two
bunsetu which have a dependency relation and contain
adjacent two words in the input noun phrase respectively.
This factor is formalized as in (4), where ti is the ith word
in the input noun phrase, and dist(s, t) is the distance be-
tween two bunsetu each of which contains s and t. A
distance between two bunsetu is defined as the number of
bunsetu between them. When two words are contained in
the same bunsetu, the distance between them is defined
as 0.
Mdistance = 11 +
?
i
dist(ti, ti+1)
(4)
4.3 Contextual information
We assume that phrases sharing the same Kanzi char-
acters likely represent the same meaning. Therefore
they could be paraphrases of each other. However, even
though a Kanzi denotes a certain meaning, its meaning is
often ambiguous. This problem is similar to word sense
ambiguities, which have been studied for many years. To
solve this problem, we adopt an idea one sense per collo-
cation which was introduced in word sense disambigua-
tion research (Yarowsky, 1995). Considering a newspa-
per article in which the retrieved passage and the input
noun phrase is included as the context, the context sim-
ilarity is taken into account for ranking paraphrase can-
didates. More concretely, context similarity is calculated
by following procedure.
1. For each paraphrase candidate, a context vector is
constructed from the newspaper article containing
the passage from which the candidate is derived.
The article is morphologically analyzed and content
words are extracted to make the context vector. The
tf ? idf metric is used for term weighting.
2. Since the input is given in terms of a noun phrase,
there is no corresponding newspaper article for the
input. However there is a case where the retrieved
passages include the input noun phrase. Such pas-
sages are not useful for finding paraphrases, but use-
ful for constructing a context vector of the input
noun phrase. The context vector of the input noun
phrase is constructed in the same manner as that of
paraphrase candidates, except that all newspaper ar-
ticles including the noun phrase are used.
3. Context similarity Mcontext is calculated by cosine
measure of two context vectors as in (5), where
wi(k) and wd(k) are the weight of the k-th term of
the input context vector and the candidate context
vector, respectively.
Mcontext =
?
k wi(k)wd(k)??
k w2i (k)
??
k w2d(k)
(5)
4.4 Ranking paraphrase candidates
Paraphrase candidates are ranked in descending order of
the product of three measures, sim(I,D) (equation (3)),
Mdistance (equation (4)) and Mcontext (equation (5)).
5 Experiments
5.1 Data and preprocessing
As input noun phrases, we used 53 queries excerpted
from Japanese IR test collection BMIR-J21 (Kitani et al,
1998) based on the following criteria.
? A query has two or more index terms.
It is less likely to retrieve proper paraphrases with
only one index term, since we adopt character-based
indexing.
? A query does not contain proper names.
It is generally difficult to paraphrase proper names.
We do not deal with proper name paraphrasing.
? A query contains at most one Katakana word or
number.
The proposed method utilize characteristics of Kanzi
characters, ideograms. It is obvious that the method
does not work well for Kanzi -poor expressions.
We searched paraphrases in three years worth of news-
paper articles (Mainichi Shimbun) from 1991 to 1993. As
described in section 2, each article is segmented into pas-
sages at punctuation marks and symbols. These passages
are assigned a unique identifier and indexed, then stored
in the GETA retrieval engine (IPA, 2003). We used the
JUMAN morphological analyzer (Kurohashi and Nagao,
1998) for indexing the passages. As a result of prepro-
cessing described above, we obtained 6,589,537 passages
to retrieve. The average number of indexes of a passage
was 12.
5.2 Qualitative evaluation
Out of 53 input noun phrases, no paraphrase was obtained
for 7 cases. Output paraphrases could be classified into
the following categories.
1BMIR-2 contains 60 queries.
(1) The paraphrase has the same meaning as that of the
input noun phrase.
e.g. ????? (damage by cool summer) ???
(cool summer damage)2
Note that this example is hardly obtained by the ex-
isting approaches such as syntactic transformation
and word substitution with thesaurus.
(2) The paraphrase does not have exactly the same
meaning but has related meaning. This category is
further divided into three subcategories.
(2-a) The meaning of the paraphrase is more specific
than that of the input noun phrase.
e.g. ?? (agricultural chemicals)?????
?? (insecticide and herbicide)
(2-b) The meaning of the paraphrase is more general
than that of the input noun phrase.
e.g. ???? (stock movement)??????
????? (movement of stock and exchange
rate)
(2-c) The paraphrase has related meaning to the in-
put but is not categorized into above two.
e.g. ??? (drinks) ???????? (inter-
national drink exhibition)
(3) There is no relation between the paraphrase and the
input noun phrase.
Among these categories, (1) and (2-a) are useful from
a viewpoint of information retrieval. By adding the para-
phrase of these classes to a query, we can expect the ef-
fective phrase expansion in queries.
Since the paraphrase of (2-b) generalizes the concept
denoted by the input, using these paraphrases for query
expansion might degrade precision of the retrieval. How-
ever, they might be useful for the recall-oriented retrieval.
The paraphrases of (2-c) have the similar property, since
relatedness includes various viewpoints.
The main reason of retrieval failure and irrelevant re-
trieval (3) are summarized as follows:
? The system cannot generate a paraphrase, when
there is no proper paraphrase for the input. In partic-
ular, this tends to be the case for single-word inputs,
such as ??? (liquid crystal)? and ??? (movie)?.
But this does not imply the proposed method does
not work well for single-words inputs. We had sev-
eral interesting paraphrases for single-word inputs,
such as ??????? (chemicals for agriculture
and gardening)? for ??? (agricultural chemicals)?.
? We used only three years worth of newspaper ar-
ticles due to the limitation of computational re-
soruces. Sometimes, the system could not generate
2The left-hand side of the arrow is the input and the right-
hand side is its paraphrase.
the paraphrase of the input because of the limited
size of the corpus.
5.3 Quantitative evaluation
Since there is no test collection available to evaluate para-
phrasing, we asked three judges to evaluate the output of
the system subjectively. The judges classified the outputs
into the categories introduced in 5.2. The evaluation was
done on the 46 inputs which gave at least one output.
Table 1 shows the results of judgments. Column ?Q?
denotes the query identifier, ?Len.? denotes its length in
morphemes, ?#Para.? denotes the number of outputs and
the columns (1) through (3) denote the number of outputs
which are classified into each category by three judges.
Therefore, the sum of these columns makes a triple of the
number of outputs. The decimal numbers in the paren-
theses denote the generalized raw agreement indices of
each category, which are calculated as given in (6) (Ue-
bersax, 2001), where K is the number of judged cases, C
is the number of categories, njk is the number of times
category j is applied to case k, and nk is calculated by
summing up over categories on case k; nk =
?C
j=1 njk.
ps(j) =
?K
k=1 njk(njk ? 1)?K
k=1 nk ? 1
(6)
In our case, K is the number of outputs (column
?#Para.?), nk is the number of judges, 3, and j moves
over (1) through (3).
As discussed in 5.2, from the viewpoint of information
retrieval, paraphrases of category (1) and (2-a) are use-
ful for query expansion of phrasal index terms. Column
?Acc.? denotes the ratio of paraphrases of category (1)
and (2-a) to the total outputs. Column ?Prec.? denotes
non-interpolated average precision. Since the precision
differs depending on the judge, the column is showing
the average of the precisions given by three judges.
We could obtain 45 paraphrases on average for each
input. But the average accuracy is quite low, 10%, which
means only one tenth of output is useful. Even though
considering that all paraphrases not being in category (3)
are useful, the accuracy only doubled. This means filter-
ing conditions should be more rigid. However, looking
at the agreement indices, we see that category (3) ranks
very high. Therefore, we expect finding the paraphrases
in category (3) is easy for a human. From all this, we
conclude that the proposed method need to be improved
in accuracy to be used for automatic query expansion in
information retrieval, but it is usable to help users to mod-
ify their queries by suggesting possible paraphrases.
Seeing the column ?Len.?, we find that the proposed
method does not work for complex noun phrases. The
average length of input noun phrase is 4.5 morphemes.
The longer input often results in less useful paraphrases.
The number of outputs also decreases for longer inputs.
We require all concepts mentioned in the input to have
their counterparts in its paraphrases as described in 3.1.
This condition seems to be strict for longer inputs. In
addition, we need to take into account syntactic variations
of longer inputs. Integrating syntactic transformation into
the proposed method is one of the possible extensions to
explore when dealing with longer inputs (Yoshikane et
al., 2002).
6 Conclusions and future work
This paper proposed a novel approach to extract para-
phrases of a Japanese noun phrase from a corpus. The
proposed method adopts both information retrieval tech-
niques and natural language processing techniques. Un-
like past research, the proposed method uses Kanzi
(ideograms) characters as index terms and retrieves para-
phrase candidates in a set of passages. The retrieved can-
didates are then filtered out based on syntactic and se-
mantic constraints.
The method was evaluated by a test set of 53 noun
phrases, and paraphrases were extracted for 46 cases.
These paraphrases were evaluated subjectively by three
independent judges. The quantitative evaluation suggests
that the performance needs to be further improved for
fully automatic query expansion in information retrieval,
but is usable to help users modify their queries by sug-
gesting possible paraphrases.
From a qualitative point of view, the proposed method
could extract paraphrases which cannot be obtained by
previous approaches such as syntactic transformation
and word substitution. Considering characteristics of
Japanese word formation by using character-based index-
ing enables us to obtain novel paraphrases.
The performance of the current system needs to be im-
proved for fully automatic paraphrasing. One direction
is introducing more precise filtering criteria. The cur-
rent system adopts only dependency analysis of bunsetu.
We need case analysis as well, to capture relations among
the bunsetu. Integrating syntactic transformation into the
proposed method is another research direction to explore.
In this paper, we evaluated output paraphrases subjec-
tively. Task oriented evaluation should be also conducted.
For example, effectiveness of phrase expansion in infor-
mation retrieval systems should be investigated.
Q Len. #Para. (1) (2-a) (2-b) (2-c) (3) Acc. Prec.
3 1 17 0 (0.00) 7 (0.86) 0 (0.00) 15 (0.60) 29 (0.83) 0.14 0.33
4 1 60 1 (0.00) 61 (0.74) 2 (0.50) 38 (0.47) 78 (0.69) 0.34 0.33
5 1 68 4 (0.75) 8 (0.62) 16 (0.00) 56 (0.14) 120 (0.62) 0.06 0.13
6 1 81 0 (0.00) 0 (0.00) 3 (0.33) 2 (0.00) 238 (0.99) 0.00 0.00
7 2 61 5 (0.60) 20 (0.70) 44 (0.45) 58 (0.66) 56 (0.73) 0.14 0.24
8 1 93 3 (0.00) 22 (0.68) 11 (0.64) 24 (0.42) 218 (0.91) 0.09 0.21
9 2 64 4 (0.75) 6 (0.67) 2 (0.50) 3 (0.33) 177 (0.99) 0.05 0.07
10 3 68 24 (0.42) 37 (0.22) 14 (0.50) 83 (0.41) 45 (0.29) 0.30 0.29
11 2 68 0 (0.00) 12 (0.08) 9 (0.44) 20 (0.25) 163 (0.83) 0.06 0.08
12 2 53 7 (0.14) 54 (0.76) 1 (0.00) 60 (0.37) 37 (0.19) 0.38 0.38
13 2 89 22 (0.32) 23 (0.30) 3 (1.00) 9 (0.11) 210 (0.98) 0.17 0.24
14 3 62 13 (0.85) 0 (0.00) 16 (0.44) 8 (0.12) 149 (0.92) 0.07 0.06
15 3 77 41 (0.49) 18 (0.44) 7 (0.57) 32 (0.38) 133 (0.89) 0.26 0.29
18 2 76 13 (0.08) 18 (0.28) 9 (0.56) 55 (0.42) 133 (0.80) 0.14 0.21
20 3 51 11 (0.82) 19 (0.95) 14 (0.71) 29 (0.62) 80 (0.82) 0.20 0.20
21 2 50 0 (0.00) 4 (0.75) 3 (0.33) 0 (0.00) 143 (0.98) 0.03 0.04
22 3 70 18 (0.72) 7 (0.00) 2 (0.50) 14 (0.36) 169 (0.94) 0.12 0.16
24 3 64 8 (0.88) 1 (0.00) 3 (1.00) 1 (0.00) 179 (0.99) 0.05 0.04
26 4 58 2 (0.50) 22 (0.18) 1 (0.00) 22 (0.27) 127 (0.78) 0.14 0.13
27 6 13 1 (0.00) 7 (0.00) 0 (0.00) 0 (0.00) 31 (0.77) 0.21 0.30
28 4 56 20 (0.25) 8 (0.38) 3 (0.33) 53 (0.30) 83 (0.54) 0.17 0.22
29 6 34 0 (0.00) 3 (1.00) 0 (0.00) 1 (0.00) 97 (0.98) 0.03 0.25
30 4 16 0 (0.00) 12 (0.33) 1 (0.00) 7 (0.14) 28 (0.64) 0.25 0.27
31 6 4 0 (0.00) 0 (0.00) 0 (0.00) 0 (0.00) 12 (1.00) 0.00 0.00
32 4 60 15 (0.80) 19 (0.58) 4 (0.00) 31 (0.39) 111 (0.84) 0.19 0.24
33 4 67 15 (0.60) 58 (0.83) 2 (0.50) 20 (0.65) 105 (0.94) 0.36 0.51
34 4 54 1 (0.00) 12 (0.67) 0 (0.00) 7 (0.57) 142 (0.99) 0.08 0.19
36 7 13 0 (0.00) 1 (0.00) 0 (0.00) 1 (0.00) 37 (0.97) 0.03 0.06
37 5 7 1 (0.00) 1 (0.00) 0 (0.00) 1 (0.00) 18 (0.89) 0.10 0.22
38 5 64 2 (0.50) 1 (0.00) 6 (1.00) 8 (0.38) 175 (0.97) 0.02 1.00
39 4 59 2 (0.50) 4 (0.00) 0 (0.00) 9 (0.56) 162 (0.97) 0.03 0.04
40 4 54 0 (0.00) 11 (0.55) 30 (0.10) 2 (0.50) 119 (0.76) 0.07 0.09
41 5 51 0 (0.00) 4 (0.50) 4 (0.00) 2 (0.00) 143 (0.97) 0.03 0.07
43 5 65 1 (0.00) 1 (0.00) 4 (0.00) 5 (0.20) 184 (0.95) 0.01 0.01
44 7 54 3 (1.00) 0 (0.00) 34 (0.35) 3 (0.00) 122 (0.81) 0.02 0.03
45 6 7 0 (0.00) 0 (0.00) 0 (0.00) 0 (0.00) 21 (1.00) 0.00 0.00
46 7 1 0 (0.00) 0 (0.00) 0 (0.00) 0 (0.00) 3 (1.00) 0.00 0.00
47 9 5 0 (0.00) 0 (0.00) 0 (0.00) 1 (0.00) 14 (0.93) 0.00 0.00
48 7 10 1 (0.00) 1 (0.00) 3 (0.00) 3 (0.00) 22 (0.86) 0.07 0.21
49 8 1 0 (0.00) 0 (0.00) 0 (0.00) 0 (0.00) 3 (1.00) 0.00 0.00
50 8 58 1 (0.00) 1 (0.00) 2 (0.00) 3 (0.00) 167 (0.97) 0.01 0.06
51 6 18 1 (0.00) 13 (0.92) 1 (0.00) 9 (0.78) 30 (1.00) 0.26 0.33
52 7 21 4 (0.00) 1 (0.00) 1 (0.00) 1 (0.00) 56 (0.95) 0.08 0.13
55 7 26 2 (0.00) 1 (0.00) 0 (0.00) 4 (0.00) 71 (0.96) 0.04 0.03
59 10 21 0 (0.00) 0 (0.00) 0 (0.00) 4 (0.50) 59 (0.97) 0.00 0.00
60 12 2 0 (0.00) 0 (0.00) 0 (0.00) 0 (0.00) 6 (1.00) 0.00 0.00
Ave. 4.5 45 5.35 (0.24) 10.8 (0.30) 5.54 (0.23) 15.3 (0.24) 97.9 (0.87) 0.10 0.17
Table 1: Summary of judgment
References
R. Baeza-Yates and B. Riberto-Neto. 1999. Modern In-
formation Retrieval. Addison Wesley.
R. Barzilay and L. Lee. 2002. Bootstrapping lexical
choice via multiple-sequence alignment. In Proceed-
ings of 2002 Conference on Empirical Methods in Nat-
ural Language Processing, pages 164?171.
R. Barzilay and K. R. McKeown. 2001. Extracting para-
phrases from a parallel corpus. In Proceedings of 39th
Annual Meeting of the Association for Computational
Linguistics, pages 50?57.
C. Ebert, L. Shalom, G. Howard, and N. Nicolas. 2001.
Generating full paraphrases of fragments in a dialogue
interpretation. In Proceedings of the 2nd SIGdial
Workshop on Discourse and Dialouge.
IPA. 2003. GETA: Generic Engine for Transposable As-
sociation. http://geta.ex.nii.ac.jp.
C. Jacquemin, J. L. Klavans, and E. Tzoukermann. 1997.
Expansion of multi-word terms for indexing and re-
trieval using morphology and syntax. In Proceedings
of 35th Annual Meeting of the Assosiation for Compu-
tational Linguistics.
C. Jacquemin. 1999. Syntagmatic and paradigmatic rep-
resentation of term variation. In Proceedings of 37th
Annual Meeting of the Assosiation for Computational
Linguistics, pages 341?348.
B. Katz. 1997. Annotating the world wide web using nat-
ural language. In Proceedings of ?Computer-assisted
information searching on Internet? (RIAO ?97), pages
136?155.
T. Kitani, Y. Ogawa, T. Ishikawa, H. Kimoto, I. Keshi,
J. Toyoura, T. Fukushima, K. Matsui, Y. Ueda,
T. Sakai, T. Tokunaga, H. Tsuruoka, H. Nakawatase,
and T. Agata. 1998. Lessons from BMIR-J2: A test
collection for Japanese IR systems. In Proceedings of
the Annual International ACM SIGIR Conference on
Research and Development in Information Retrieval,
pages 345?346.
S. Kurohashi and M. Nagao. 1998. Building a Japanese
parsed corpus while improving the parsing system. In
Proceedings of the 1st International Conference on
Language Resources and Evaluation, pages 719?724.
D. D. Lewis. 1992. An evaluation of phrasal and clus-
tered representations of a text categorization task. In
Proceedings of the Annual International ACM SIGIR
Conference on Research and Development in Informa-
tion Retrieval, pages 37?50.
H. P. Luhn. 1957. A statistical approach to mechanized
encoding and searching of literary information. IBM
Journal of Research and Development, 1(4):390?317.
T. Mitamura. 2001. Automatic rewriting for con-
trolled language translation. In The Sixth Nat-
ural Language Processing Pacific Rim Symposium
(NLPRS2001) Post-Conference Workshop, Automatic
Paraphrasing: Theories and Applications, pages ???
M. Mitra, C. Buckley, A. Singhal, and C. Cardie. 1997.
An analysis of statistical and syntactic phrases. In Pro-
ceedings of RIAO ?97, pages 200?214.
M. Shimohata and E. Sumita. 2002. Automatic para-
phrasing based on parallel corpus for normalization.
In Third International Conference on Language Re-
sources and Evaluation, pages 453?457.
Y. Shinyama, S. Sekine, K. Sudo, and R. Grishman.
2002. Automatic paraphrase acquisition from news ar-
ticles. In Proceedings of Human Language Technology
Conference (HLT2002), pages 40?46.
K. Sparck Jones. 1972. A statistical interpretation of
term specificity and its application in retrieval. Journal
of Documentation, 28(1):11?21.
T. Tokunaga, K. Kenji, H. Ogibayashi, and H. Tanaka.
2002. Selecting effective index terms using a decision
tree. Natural Language Engineering, 8(2-3):193?207.
K. Torisawa. 2001. A nearly unsupervised learning
method for automatic paraphrasing of japanese noun
phrase. In The Sixth Natural Language Processing Pa-
cific Rim Symposium (NLPRS2001) Post-Conference
Workshop, Automatic Paraphrasing: Theories and Ap-
plications, pages 63?72.
J. Uebersax. 2001. Statistical methods for rater agree-
ment. http://ourworld.compuserve.com/
homepages/jsuebersax/agree.htm.
D. Yarowsky. 1995. Unsupervised word sense disam-
biguation rivaling supervised methods. In Proceedings
of 33rd Annual Meeting of the Assosiation for Compu-
tational Linguistics, pages 189?196.
F. Yoshikane, K. Tsuji, K. Kageura, , and C. Jacquemin.
2002. Detecting Japanese term variation in textual
corpus. In Proceedings of 4th International Work-
shop on Information Retrieval with Asian Languages
(IRAL?99), pages 164?171.
Evaluation of a Japanese CFG Derived from a Syntactically Annotated
Corpus with Respect to Dependency Measures
Tomoya Noro  Chimato Koike Taiichi Hashimoto 
Takenobu Tokunaga  Hozumi Tanaka 
  Graduate School of Information Science and Engineering
Tokyo Institute of Technology, Tokyo
 noro@tt,taiichi@cl,take@cl.cs.titech.ac.jp
 Graduate School of Science and Engineering, Tokyo Institute of Technology, Tokyo
chimato@it.ss.titech.ac.jp
  School of Computer and Cognitive Sciences, Chukyo University, Nagoya
htanaka@sccs.chukyo-u.ac.jp
Abstract
Parsing is one of the important
processes for natural language process-
ing and, in general, a large-scale CFG
is used to parse a wide variety of
sentences. For many languages, a
CFG is derived from a large-scale
syntactically annotated corpus, and
many parsing algorithms using CFGs
have been proposed. However, we
could not apply them to Japanese since
a Japanese syntactically annotated
corpus has not been available as of yet.
In order to solve the problem, we have
been building a large-scale Japanese
syntactically annotated corpus. In this
paper, we show the evaluation results
of a CFG derived from our corpus
and compare it with results of some
Japanese dependency analyzers.
1 Introduction
Parsing is one of the important processes for nat-
ural language processing and, in general, a large-
scale CFG is used to parse a wide variety of sen-
tences. Although it is difficult to build a large-
scale CFG manually, a CFG can be derived from
a large-scale syntactically annotated corpus. For
many languages, large-scale syntactically anno-
tated corpora have been built (e.g. the Penn Tree-
bank (Marcus et al, 1993)), and many parsing al-
gorithms using CFGs have been proposed.
However, such a syntactically annotated corpus
has not been built for Japanese as of yet. De-
pendency analysis is preferred in order to analyze
Japanese sentences (dependency relation between
Japanese phrasal unit, called bunsetsu) (Kuro-
hashi and Nagao, 1998; Uchimoto et al, 2000;
Kudo and Matsumoto, 2002), and only a few stud-
ies about Japanese CFG have been conducted.
Since many efficient parsing algorithms for CFG
have been proposed, a Japanese CFG is necessary
to apply the algorithms to Japanese.
We have been building a large-scale Japanese
syntactically annotated corpus to derive a
Japanese CFG for syntactic parsing (Noro et al,
2004a; Noro et al, 2004b). According to the re-
sult, a CFG derived from the corpus can parse
sentences with high accuracy and coverage. How-
ever, as mentioned previously, dependency analy-
sis is usually adopted in Japanese NLP, and it
is difficult to compare our result with results of
other dependency analysis since we evaluated our
CFG with respect to phrase structure based mea-
sure. Although we evaluated with respect to de-
pendency measure as a preliminary experiment in
order to compare, the scale was quite small (eval-
uated on only 100 sentences) and the comparison
was unfair since we did not use the same evalua-
tion data.
In this paper, we show an evaluation result of a
CFG derived from our corpus and compare it with
results of other Japanese dependency analyzers.
We used the Kyoto corpus (Kurohashi and Nagao,
1997) for evaluation data, and chose KNP (Kuro-
hashi and Nagao, 1998) and CaboCha (Kudo and
Matsumoto, 2002) for comparison.
9
Syntactically
Annotated Corpus
CFG Annotation
Policy
Deriving a CFG
Analyzing Causes of Ambiguity,
Deciding on an Annotation Policy
Modifying the Corpus
Figure 1: Procedure of building a syntactically annotated corpus
2 Annotation Policy
In this section, we start by introducing our policy
for annotating a Japanese syntactically annotated
corpus briefly. The details are given in (Noro et
al., 2004a; Noro et al, 2004b)
Although a large-scale CFG can be easily de-
rived from a syntactically annotated corpus, such
a CFG has a problem that it creates a large-
number of parse results during syntactic parsing
(i.e. high ambiguity). A syntactically annotated
corpus should be built so that the derived CFG
would create less ambiguity.
We have been building such Japanese corpus
by using the following method (Figure 1):
1. Derive a CFG from an existing corpus.
2. Analyze major causes of ambiguity.
3. Determine a policy for modifying the cor-
pus.
4. Modify the corpus according to the policy
and derive a CFG from it again.
5. Repeat steps (2) - (4) until most problems are
solved.
We focused on two major causes of ambiguity:
Lack of Syntactic Information: Some syntac-
tic information which is important for syn-
tactic parsing might be lost during the CFG
derivation since CFG rules generally repre-
sent only structures of subtree with the depth
of 1 (relation between a parent node and
some child nodes).
Need for Semantic Information: Not only
syntactic information but also semantic
information is necessary for disambiguation
in some cases.
To avoid the first cause, we considered which syn-
tactic information is necessary for syntactic pars-
ing and added the information to each interme-
diate node in the structure. On the other hand,
we considered ambiguity due to the second cause
better be left to the subsequent semantic process-
ing since it is difficult to reduce such ambiguity
without recourse to semantic information during
syntactic parsing. This can be achieved by rep-
resenting the ambiguous cases as the same struc-
ture. We assume that syntactic analysis based on a
large-scale CFG is followed by semantic analysis,
and the second cause of ambiguity is supposed
to be disambiguated in the subsequent semantic
processing.
The main aspects of our policy are as follows:
Verb Conjugation: Information about verb
conjugation is added to each intermediate
node related to the verb (cf. ?SPLIT-VP?
in (Klein and Manning, 2003) and ?Verb
Form? in (Schiehlen, 2004)).
Compound Noun Structure: Structure ambi-
guity of compound noun is represented as
the same structure regardless of the meaning
or word-formation as Shirai et al described
in (Shirai et al, 1995).
Adnominal and Adverbial Phrase Attachment:
Structure ambiguity of adnominal phrase
attachment is represented as the same
structure regardless of the meaning while
structure ambiguity of adverbial phrase
attachment is distinguished by meaning.
In case of a phrase like ?watashi no chichi
no hon (my father?s book)?, the structure
is same whether the adnominal phrase
?watashi no (my)? attaches to the noun
?chichi (father)? or the noun ?hon (book)?.
On the other hand, in case of a sentence
10
Input
Producing
Possible Parse Trees
Using a CFG
Top-n Possible Parse Trees
Using PCFG, PGLR model, etc.
Final Interpretation
Disambiguation of 
Adnominal Phrase Attachment
One Parse Tree
Disambiguation of
Adverbial Phrase Attachment
Figure 2: Procedure in the subsequent processing
Segmentation Accuracy   # sentences segmented into bunsetsu correctly
# all sentences
Dependency Accuracy   # correct dependency relations
# all dependency relations
Sentence Accuracy   # sentences determined all relations correctly
# sentences segmented in bunsetsu correctly
Figure 3: Dependency measures
like ?kare ga umi wo egaita e wo katta?,
we distinguish the structure according to
whether the adverbial phrase ?kare ga (he)?
attaches to the verb ?egaita (paint)? (it
means ?I bought a picture of a sea painted
by him?) or the verb ?katta (buy)? (it means
?he bought a picture of a sea?).
Conjunctive Structure: Conjunctive structure
is not specified during syntactic parsing, in-
stead their analysis is left for the subsequent
processing (contrary to (Kurohashi and Na-
gao, 1994)).
We have decided to deal with adnominal phrase
attachment and adverbial phrase attachment sep-
arately in our policy since we believe that a dif-
ferent algorithm should be used to disambiguate
them. In the subsequent processing, we assume
that adverbial phrase attachment would be disam-
biguated by choosing one parse tree among the
results at first, and adnominal phrase attachment
would be disambiguated by choosing one inter-
pretation among all of interpretations which the
parse tree represents (Figure 2).
We used the EDR corpus (EDR, 1994) for
developing our annotation policy, and annotated
8,911 sentences in the corpus and 20,190 sen-
tences in the RWC corpus (Hashida et al, 1998).
In the following evaluation, we used the latter
one.
3 Experimental Setup
As mentioned previously, in general, analyzing
dependency relations between bunsetsu is pre-
ferred in Japanese, which makes it difficult to
compare the result by the CFG with the result
by dependency analysis. In order to compare
with other dependency analysis, we evaluated our
derived CFG with respect to dependency mea-
sures shown in Figure 3. Note that sentences
which are not segmented into bunsetsu correctly
are dropped from the evaluation data when we
evaluate dependency accuracy and sentence accu-
racy.
A CFG is derived from all sentences in our cor-
pus, with which we parsed 6,931 sentences (POS
sequences) in the Kyoto corpus 1 by MSLR parser
(Shirai et al, 2000). The Kyoto corpus has an-
1On average, 8.89 bunsetsu in a sentence.
11
Syntactically
Annotated
Corpus
CFG
CFG
Derivation
Top-n Parse Results
(Phrase Structure)
Top-n Parse Results
(Dependency Relations)Extract
Dependency Relations
POS
Sequence
Kyoto Corpus
(Dependency Relations)
Parsing & Ranking
POS Conversion
Evaluation
Figure 4: Evaluation with respect to dependency measure
notation in terms of dependency relations among
bunsetsu, and it is usually used for evaluation of
dependency analysis. The parser is trained ac-
cording to probabilistic generalized LR (PGLR)
model (Inui et al, 2000) (all sentences are used
for training), and parse results are ranked by the
model.
The experiment was carried out as follows
(Figure 4):
1. Convert POS tags automatically to the RWC
tag set.
2. Parse the POS sequence using a CFG derived
from our corpus.
3. Rank the parse results by PGLR model and
pick up the top-  parse results.
4. Extract dependency relations among bun-
setsu for each result.
5. Choose the result which is closest to the
gold-standard and evaluate it.
Since the tag set of the Kyoto corpus is different
from that of the RWC corpus, a POS conversion
in step (1) is necessary. It is a rule-based con-
version, and the accuracy is about 80%. It seems
that the low conversion accuracy would damage
the evaluation result. We will discuss this issue in
the next section.
In the 4th step of the experimental procedure,
we determine boundaries of bunsetsu and depen-
dency relations among the bunsetsu in a sentence
with the CFG rules included in the phrase struc-
ture of the sentence. Some CFG rules in our CFG
indicate positions of bunsetsu boundaries. For ex-
ample, a CFG rule ?NP   AdnP NP? (?NP? and
?AdnP? stand for a noun phrase and an adnom-
inal phrase respectively) indicates that there is a
boundary of bunsetsu between the two phrases in
the right-hand side of the CFG rule (i.e. between
the noun phrase and the adnominal phrase), and
that a bunsetsu including the head word of the ad-
nominal phrase depends on a bunsetsu including
the head word of the noun phrase. An example of
?Nihon teien no nagame ga subarashii (The view
of the Japanese garden is wonderful)? is shown in
Figure 5.
Structure ambiguity of adnominal phrase at-
tachment needs to be disambiguated in extracting
dependency relations in step (4) since it is repre-
sented as the same structure according to our pol-
icy 2. We disambiguate adnominal phrase attach-
ment based on one of the following assumptions:
NEAREST: Every ambiguous adnominal
phrase attaches to the nearest noun among
the nouns which the phrase could attach to.
BEST: Choose the best noun among the nouns
which could be attached to (assume that
disambiguation of adnominal phrase attach-
ment was done correctly) 3.
2Since dependency relations are not categorized in the
Kyoto corpus, it is difficult to know how many relations rep-
resenting adnominal phrase attachment are included in the
evaluation data. On the other hand, among the top parse re-
sults ranked by PGLR model (i.e in case of      in section
4), about 34.1% of all dependency relations represent ad-
nominal phrase attachment, and about 23.4% of them (i.e.
about 8.0% of all relations) remain ambiguous.
3We choose the best noun automatically by referring to
12
Nihon
(Japanese)
teien
(garden)
no nagame
(view)
ga subarashii
(wonderful)
n n p n p adj
<comp.n>
<NP>
<AdnP>
<NP>
<NP>
<PP>
<AdjP>
<AdjP>
<S>
bunsetsu #1 bunsetsu #2 bunsetsu #3
Bunsetsu No. Word Sequence Bunsetsu Which is Depended on
1 nihon teien no 2
2 nagame ga 3
3 subarashii ?
Figure 5: Extracting Dependency Relations from a Pharse Structure
?NEAREST? is a quite simple way for disam-
biguation, and it would be the baseline model.
On the other hand, since we assume that struc-
ture ambiguity of adnominal phrase attachment is
supposed to be disambiguated in the subsequent
semantic processing, ?BEST? would be the upper
bound and we could not overcome the accuracy
even if the disambiguation was done perfectly in
the subsequent processing.
To take two noun phrases ?watashi no chichi
no hon (my father?s book)? and ?watashi no ka-
gaku no hon (my book on science)? as examples
(the correct answer is that the adnominal phrase
?watashi no (my)? attaches to the noun ?chichi
(father)? in the former case, and attaches to the
noun ?hon (book)? in the latter case), ?NEAR-
EST? attaches to the adnominal phrase ?watashi
no? to the nouns ?chichi? and ?kagaku (science)?
regardless of their meanings. ?BEST? attaches
the adnominal phrase to the noun ?chichi? in the
former case, and attaches to the noun ?hon? in the
latter case.
Although structure ambiguity of compound
noun is also represented as the same structure re-
the Kyoto corpus. If the noun which is attached to in the
Kyoto corpus is not in the candidates, we choose the nearest
noun (i.e. ?NEAREST?).
gardless of the meaning or word-formation, we
have nothing to do with the structure ambiguity
since a bunsetsu is a larger unit than a compound
noun. Furthermore, since dependency relations
are not categorized, we do not have to care about
whether two bunsetsu have conjunctive relation
with each other or not.
In order to compare our result with that of
other dependency analyzers, we used two well-
known Japanese dependency analyzers, KNP and
CaboCha, and analyzed dependency structure of
the sentences in the same evaluation data set. In
both cases, POS tagged sentences are used as the
input. Since CaboCha uses the same tagset as the
RWC corpus, we converted POS tags in the same
way as step (1) in our experimental procedure. On
the other hand, since KNP uses the tagset adopted
by the Kyoto corpus, POS tags do not have to be
converted in case of analyzing by KNP.
4 Results
Table 1 shows the results when     , which
means the top parse result of each sentence is used
for evaluation. In this case, ?NEAREST? means
only PGLR model was used for disambiguation
without any other information (e.g. lexical infor-
13
Table 1: Segmentation, dependency, and sentence accuracy (    )
Segmentation Dependency Sentence
NEAREST 65.68% 87.88% 50.47%
BEST 65.68% 90.27% 57.73%
KNP 96.90% 91.32% 60.07%
CaboCha 84.88% 92.88% 64.48%
 45
 50
 55
 60
 65
 70
 75
 80
 85
 90
 95
 100
 0  10  20  30  40  50  60  70  80  90  100
Se
gm
en
ta
tio
n 
/ D
ep
en
de
nc
y 
/ S
en
te
nc
e 
Ac
cu
ra
cy
 (%
)
Rank by PGLR model (top-n parse results)
76.53
93.10
66.85
95.24
75.72
Segmentation Accuracy
Dependency Accuracy (NEAREST)
Dependency Accuracy (BEST)
Sentence Accuracy (NEAREST)
Sentence Accuracy (BEST)
Figure 6: Segmentation, dependency, and sentence accuracy (        )
mation, semantic information, etc.) On the other
hand, ?BEST? means only disambiguation of ad-
nominal phrase attachment was done in the subse-
quent processing. Results by KNP and CaboCha
are shown in the same table for comparison.
As seen from Table 1, accuracy is still lower
than KNP and CaboCha even if disambiguation
of adnominal phrase attachment was done cor-
rectly in the subsequent processing. However,
in this case, we do not use any information but
PGLR model for disambiguation of any relations
except adnominal phrase attachment (i.e. adver-
bial phrase attachment).
Next, assuming that disambiguation of other
relations, we carried out another evaluation
changing   from 1 to 100. The result is shown
in Figure 6. Dependency accuracy could achieve
about 95.24% for ?BEST?, which exceeds the
dependency accuracy by KNP and CaboCha, if
choosing the best result among top-100 parse re-
sults ranked by PGLR model would be done cor-
rectly in the subsequent processing 4. From the
results, we can conclude the accuracy will in-
crease as soon as lexical and semantic informa-
tion is incorporated in the subsequent processing
5
.
However, segmentation accuracy is still signif-
icantly lower. The main reasons are as follows:
POS Conversion Error: As mentioned previ-
ously, we converted POS tags automatically
since the POS system of the Kyoto corpus is
4Even if only top-10 parse results are considered, our
CFG have a possibility to outperform KNP and CaboCha
5In some studies, it is said that lexical information has
little impact on accuracy (Bikel, 2004). However, we think
some lexical information is useful for disambiguation, and
it is necessary to consider what kind of lexical information
could improve the accuracy.
14
different from that of the RWC corpus. How-
ever, accuracy of the conversion is not high
(about 80%). Since we used only POS in-
formation and did not use any word infor-
mation for parsing, the result can be easily
affected by the conversion error. Segmen-
tation accuracy by CaboCha is also a little
lower than accuracy by KNP. Since POS tags
were converted in the same way, we think
the reason is same. However, the difference
between the accuracy by KNP and CaboCha
is smaller since CaboCha uses not only POS
information but also word information.
Difference in Segmentation Policy: There is
difference in bunsetsu segmentation policy
between the Kyoto corpus and our corpus.
For example:
1. 3 gatsu 31 nichi gogo 9 ji 43 fun goro,
jishin ga atta
(An earthquake occurred at around
9:43 p.m., March 1st.)
2. gezan suru no wo miokutta
(We gave up going down the moun-
tain.)
In the former case, the underlined part is
segmented into 5 bunsetsu (?3 gatsu?, ?31
nichi?, ?gogo?, ?9 ji?, and ?43 fun goro,?) in
the Kyoto corpus, while it is not segmented
in our corpus. On the other hand, in the latter
case, the underlined part is segmented into 2
bunsetsu (?gezan suru? and ?no wo?) in our
corpus, while it is not segmented in the Ky-
oto corpus. By correction of these two types
of error, segmentation accuracy improved by
4.35% (76.53%  80.88%) and dependency
accuracy improved by 0.61% (95.24%  
95.85%).
5 Conclusion
We have been building a large-scale Japanese syn-
tactically annotated corpus. In this paper, we eval-
uated a CFG derived from the corpus with re-
spect to dependency measure. We assume that
parse results created by our CFG is supposed to
be re-analyzed in the subsequent processing using
semantic information, and the result shows that
parsing accuracy will increase when semantic in-
formation is incorporated.
We also compared our result with other depen-
dency analyzers, KNP and CaboCha. Although
dependency accuracy of our CFG cannot reach
those of KNP and CaboCha if only PGLR model
is used for disambiguation, it would exceed if
disambiguation in the subsequent processing was
done correctly.
As future work, since we assume that the
parse results created by our CFG are re-analyzed
in the subsequent processing, we need to inte-
grate the subsequent processing into the current
framework. Collins proposed a method for re-
ranking the output from an initial statistical parser
(Collins, 2000). However, it is not enough for us
since we represent some ambiguous cases as the
same structure (we need to consider the ambigu-
ity included in each parse result). Our policy has
been considered with several types of ambiguity:
structure of compound noun, adnominal phrase
attachment, adverbial phrase attachment and con-
junctive structure. We are planning to provide
each method individually and integrate them into
a single process.
Although we attempt to re-analyze after pars-
ing, it seems that some problem should be solved
before parsing. For example, ellipsis often occurs
in Japanese. It is difficult to deal with ellipsis (es-
pecially, postpositions and verbs) in a CFG frame-
work, resulting in higher ambiguity. It would
be helpful if the positions where some words are
omitted in a sentence were detected and marked
in advance.
References
Daniel M. Bikel. 2004. A distributional analysis of a
lexicalized statistical parsing model. In 2004 Con-
ference on Empirical Methods in Natural Language
Processing, pages 182?189.
Michael Collins. 2000. Disriminative reranking for
natural language parsing. In 17th International
Conference on Machine Learning, pages 175?182.
EDR, 1994. EDR Electronic Dictionary User?s Man-
ual, 2.1 edition. In Japanese.
Koichi Hashida, Hitoshi Isahara, Takenobu Tokunaga,
Minako Hashimoto, Shiho Ogino, and Wakako
Kashino. 1998. The RWC text databases. In
15
The First International Conference on Language
Resource and Evaluation, pages 457?461.
Kentaro Inui, Virach Sornlertamvanich, Hozumi
Tanaka, and Takenobu Tokunaga. 2000. Proba-
bilistic GLR parsing. In Harry Bunt and Anton Ni-
jholt, editors, Advances in Probabilistic and Other
Parsing Technologies, pages 85?104. Kluwer Aca-
demic Publishers.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In the 41st Annual
Meeting of the Association for Computational Lin-
guistics, pages 423?430.
Taku Kudo and Yuji Matsumoto. 2002. Japanese
dependency analysis using cascaded chunking. In
CONLL 2002.
Sadao Kurohashi and Makoto Nagao. 1994. A syn-
tactic analysis method of long Japanese sentences
based on the detection of conjunctive structures.
Computational Linguistic, 20(4):507?534.
Sadao Kurohashi and Makoto Nagao. 1997. Kyoto
university text corpus project. In the 3rd Confer-
ence for Natural Language Processing, pages 115?
118. In Japanese.
Sadao Kurohashi and Makoto Nagao. 1998. Build-
ing a Japanese parsed corpus while improving the
parsing system. In the first International Confer-
ence on Language Resources and Evaluation, pages
719?724.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2):313?330.
Tomoya Noro, Taiichi Hashimoto, Takenobu Toku-
naga, and Hozumi Tanaka. 2004a. Building a
large-scale japanese CFG for syntactic parsing. In
The 4th Workshop on Asian Language Processing,
pages 71?78.
Tomoya Noro, Taiichi Hashimoto, Takenobu Toku-
naga, and Hozumi Tanaka. 2004b. A large-scale
japanese CFG derived from a syntactically anno-
tated corpus and its evaluation. In The 3rd Work-
shop on Treebanks and Linguistic Theories, pages
115?126.
Michcael Schiehlen. 2004. Annotation strategies for
probabilistic parsing in German. In the 20th Inter-
national Conference on Computational Linguistics,
pages 390?396.
Kiyoaki Shirai, Takenobu Tokunaga, and Hozumi
Tanaka. 1995. Automatic extraction of Japanese
grammar from a bracketed corpus. In Natural Lan-
guage Processing Pacific Rim Symposium, pages
211?216.
Kiyoaki Shirai, Masahiro Ueki, Taiichi Hashimoto,
Takenobu Tokunaga, and Hozumi Tanaka. 2000.
MSLR parser ? tools for natural language analysis.
Journal of Natural Language Processing, 7(5):93?
112. In Japanese.
Kiyotaka Uchimoto, Masaki Murata, Satoshi Sekine,
and Hitoshi Isahara. 2000. Dependency model us-
ing posterior context. In 6th International Work-
shop on Parsing Technologies.
16
