First Joint Conference on Lexical and Computational Semantics (*SEM), pages 335?339,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
UWashington: Negation Resolution using Machine Learning Methods
James Paul White
University of Washington
Department of Linguistics, Box 354340
Seattle, WA 98195, USA
jimwhite@uw.edu
Abstract
This  paper  reports  on  a  simple  system  for 
resolving the scope of negation in the closed 
track of  the *SEM 2012 Shared Task.   Cue 
detection  is  performed  using  regular 
expression  rules  extracted  from  the  training 
data.  Both  scope  tokens  and  negated  event 
tokens  are  resolved  using  a  Conditional 
Random  Field  (CRF)  sequence  tagger  ? 
namely  the  SimpleTagger  library  in  the 
MALLET machine learning toolkit.  The full 
negation  F1 score  obtained  for  the  task 
evaluation is 48.09% (P=74.02%, R=35.61%) 
which ranks this system fourth among the six 
submitted for the closed track.
1 Introduction
Resolving the scope of negation is an interesting 
area of research for Natural Language Processing 
(NLP) systems because many such systems have 
used methods that are insensitive to polarity.  As a 
result  it  is  fairly  common to have  a system that 
treats ?X does Y? and ?X does not Y? as having 
the same, or very nearly the same, meaning1.   A 
few  application  areas  that  have  been  addressing 
this issue recently are in sentiment analysis,  bio?
medical  NLP,  and  recognition  of  textual  entail?
ment.   Sentiment analysis systems are frequently 
used in corporate and product marketing, call cen?
ter quality control, and within ?recommender? sys?
tems which are all contexts where it is important to 
recognize that ?X does like Y? is contrary to ?X 
does not like Y?.  Similarly in biomedical text such 
1A one token difference between the strings surely indicating 
at least an inexact match.
as research papers and abstracts, diagnostic proce?
dure reports, and medical records it is important to 
differentiate between statements about what is the 
case and what is not the case.
The *SEM 2012 Shared Task is actually two re?
lated tasks run in parallel.  The one this system was 
developed for is the identification of three features 
of  negation:  the  cue,  the  scope,  and  the  factual 
negated event (if any).  The other task is concerned 
with the focus of negation.  Detailed description of 
both subtasks, including definition of the relevant 
concepts  and  terminology  (negation,  cue,  scope, 
event, and focus) appears in this volume (Morante 
and Blanco, 2012).  Roser Morante  and Eduardo 
Blanco  describe  the  corpora  provided  to  partici?
pants with numbers and examples,  methods used 
used to process the data, and briefly describes each 
participant and analyzes the overall results.
Annotation of the corpus was undertaken at the 
University of Antwerp and was performed on sev?
eral Sherlock Holmes works of fiction written by 
Sir Arthur Conan Doyle.  The corpus includes all 
sentences from the original text, not just those em?
ploying  negation.   Roser  Morante  and  Walter 
Daelemans  provide  a  thorough  explanation  of 
those gold annotations of negation cue, scope, and 
negated  event  (if  any)  (Morante  and Daelemans, 
2012).  Their paper explains the motivations for the 
particular annotation decisions and describes in de?
tail the guidelines, including many examples.
2 Related Work
Recognition of phrases containing negation, partic?
ularly in the medical domain, using regular expres?
sions has been described using several different ap?
proaches. Systems such as Negfinder  (Mutalik et 
335
al,  2001) and NegEx  (Chapman et  al,  2001) use 
manually constructed rules to extract phrases from 
text and classify them as to whether they contain 
an expression of negation.  Rokach et alevaluate 
several  methods  and  show their  highest  level  of 
performance (an F1 of 95.9 ? 1.9%) by using cas?
caded decision trees of regular expressions learned 
from labelled narrative medical reports (Rokach et 
al, 2008).  
Those systems perform a different function than 
that  required for this task though.  They classify 
phrases  extracted  from  plain  text  as  to  whether 
they contain negation or not, while the requirement 
of this shared task for negation cue detection is to 
identify the particular token(s) or part of a token 
that signals the presence of negation.  Furthermore, 
those systems only identify the scope of negation 
at the level of phrasal constituents, which is differ?
ent than what is required for this task in which the 
scopes are not necessarily contiguous.
Conditional Random Field (CRF) sequence tag?
gers have been successfully applied to many scope 
resolution problems,  including those of negation. 
The  NegScope  system  (Agarwal  and  Yu,  2010) 
trains a CRF sequence tagger on labelled data to 
identify both the cue and scope of negation.  How?
ever, that system only recognizes a whole word as 
a cue and does not recognize nor generalize nega?
tion cues which are affixes.  There are also systems 
that  use  CRF  sequence  taggers  for  detection  of 
hedge scopes (Tang et al 2010, Zhao et al 2010). 
Morante and Daelemans describe a method for im?
proving  resolution  of  the  scope  of  negation  by 
combining IGTREE, CRF, and Support Vector Ma?
chines (SVM) (Morante and Daelemans, 2009).  
3 System Description
This system is implemented as a three stage cas?
cade  with  the  output  from each  of  the  first  two 
stages  included as  input  to the subsequent  stage. 
The stages are ordered as cue detection, scope de?
tection,  and  finally  negated  event  detection.  The 
format of the inputs and outputs for each stage use 
the shared task?s  CoNLL?style file  format.   That 
simplifies  the  use  of  the  supplied  gold?standard 
data for training of each stage separately.
 Because  this  system  was  designed  for  the 
closed track of the shared task, it makes minimal 
language?specific assumptions and learns (nearly) 
all  language?specific  rules from the gold?labelled 
training data (which includes the development set 
for the final system).
The CRF sequence tagger used by the system is 
that implemented in the SimpleTagger class of the 
MALLET toolkit, which is a Java library distrib?
uted under the Common Public License2.
The system is implemented in the Groovy pro?
gramming  language,  an  agile  and  dynamic  lan?
guage for the Java Virtual Machine3.  The source 
code is available under the GNU Public License on 
GitHub4.
3.1 Cue Detection
Cues are recognized by four different regular ex?
pression rule patterns: affixes (partial token), single 
(whole)  token,  contiguous  multiple  token,  and 
gappy  (discontiguous)  multiple  token.  The  rules 
are learned by a two pass process.  In the first pass, 
for each positive example of a negation cue in the 
training data, a rule that matches that example is 
added to the prospective rule set.  Then, in the sec?
ond pass, the rules are applied to the training data 
and the counts of correct and incorrect matches are 
accumulated. Rules that are wrong more often than 
they are right are removed from the set used by the 
system.
A further  filtering  of  the  prospective  rules  is 
done  in  which  gappy  multiple  token  rules  that 
match the same word type more than once are re?
moved.   Those  prospective  rules  are  created  to 
match cases in the supplied training data where the 
a repetition has occurred and then encoded by the 
annotators as a single cue (and thus scope) of nega?
tion5.  
The single token and multiple token rules match 
both the word string feature (ignoring case) and the 
part?of?speech (POS) feature of each token.  And 
because a single token rule might also match a cue 
that belongs to a multiple token rule, multiple to?
ken rules are checked first.
  Affix rules are of two types: prefix cues and 
non?prefix cues.  The distinction is that while pre?
fix cues must match starting at the beginning of the 
word string, the non?prefix cues may have a suffix 
following them in the word string that is not part of 
the cue.  Affix rules only match against the word 
2http://mallet.cs.umass.edu/  
3http://groovy.codehaus.org/  
4https://github.com/jimwhite/SEMST2012  
5Such as baskervilles12 174: ?Not a whisper, not a rustle, 
rose...? which has a cue annotation of ?Not? gap ?not?.
336
string feature of the tokens and are insensitive to 
the POS feature.
In order to generalize the affix rules, sets are ac?
cumulated of both base word strings (the substring 
following  a  prefix  cue  or  substring  preceding  a 
non?prefix cue) and suffixes (the substring follow?
ing non?prefix cues, if any).  In addition, all other 
word strings and lemma strings in the training cor?
pus that are at least four characters long are added 
to the set of possible base word strings6.  A set of 
negative word  strings is  also  accumulated  in the 
second pass of the rule training to condition against 
false positive matches for each affix rule.
A prefix  cue  rule  will  match  a  token  with  a 
word string that  starts  with the cue string and is 
followed by any of the strings in the base word set. 
Similarly  a  suffix  cue  rule  will  match  a  token 
whose word string contains the cue string preceded 
by a string in the base word set and is either at the 
end  of  the  string  or  is  followed  by  one  of  the 
strings in the suffix string set.  Affix rules, unlike 
the other cue?matching rules, also output the string 
for matched base word as the value of the scope for 
the matched token.  In any case, if the token?s word 
string is in the negative word string set for the rule 
then it will not be matched.
Following submission of the system outputs for 
the shared tasked I discovered that a hand written 
regular expression rule that filters out the (poten?
tial)  cues  detected  for  ?(be|have)  no  doubt?  and 
?none the (worse|less)? was inadvertently included 
in  the  system.   Although  those  rules  could  be 
learned automatically from the training data (and 
such  was  my  intention),  the  system  as  reported 
here does not currently do so.
3.2 Negation Scope Resolution
For  each  cue  detected,  scope  resolution  is  per?
formed as a ternary classification of each token in 
the sentence as to whether it is part of a cue, part of 
a scope, or neither.  The classifier is the CRF se?
quence  tagger  implemented  in  the  SimpleTagger 
class of the MALLET toolkit  (McCallum, 2002). 
Training is performed using the gold?standard data 
including the gold cues.  The output of the tagger is 
not used to determine the scope value of a token in 
6This ?longer than four character? rule was manually created 
to correct for over?generalization observed in the training data.  
If the affix rule learner selected this value using the correct/in?
correct counts as it does with the other rule parameters then 
this bit of language?specific tweaking would be unnecessary.
those cases where an affix rule in the cue detector 
has matched a token and therefore has supplied the 
matched base word string as the value of the scope 
for the token.
For features that are computed in terms of the 
cue  token,  the  first  (lowest  numbered)  token 
marked as a cue is used when there is more than 
one cue token for the scope.  
Features used by the scope CRF sequence tag?
ger are:
? Of the per?token data: word string in low?
ercase, lemma string in lowercase, part?of?
speech  (POS)  tag,  binary  flag  indicating 
whether the token is a cue, a binary flag in?
dicating whether the token is at the edge of 
its parent non?terminal node or an internal 
sibling,  a  binary  flag  indicating  whether 
the token is a cue token, and relative posi?
tion to the cue token in number of tokens.
? Of the cue token data:  word string in low?
ercase,  lemma  string  in  lowercase,   and 
POS tag.
? Of the path through the syntax tree from 
the cue token: an ordered list of the non?
terminal labels of each node up the tree to 
the lowest common parent, an ordered list 
of  the  non?terminal  labels  of  each  node 
down the tree  from that  lowest  common 
parent, a path relation value consisting of 
the  label  of  the  lowest  common  parent 
node  concatenated  with  an  indication  of 
the relative position of the paths to the cue 
and token in terms of sibling order.
3.3 Negated Event Resolution
Detection of the negated event or property is per?
formed using the same CRF sequence tagger and 
features used for scope detection.  The only differ?
ence is that the token classification is in terms of 
whether each token in the sentence is part of a fac?
tual negated event for each negation cue.
3.4 Feature Set Selection
A comparison  of  the  end?to?end  performance  of 
this system using several different sets of per token 
feature  choices  for  the  scope  and  negated  event 
classifiers is shown in Table 1.  In each case the 
training data is the entire training data and the dev 
data is the entire dev data supplied by the organiz?
ers for this shared task.  The scores are computed 
337
by the evaluation program also supplied by the or?
ganizers.  The baseline features are those provided 
in the data, with the exception of the syntactic tree 
fragment: word string in lowercase, lemma in low?
ercase, and POS tag.  The ?set 1? features are the 
remainder of the features described in section 3.2, 
with the exception of those of the path through the 
syntax tree from the cue token.  The ?set 2? fea?
tures are the three baseline features plus the three 
features of the path through the syntax tree from 
the cue token: list of non?terminal labels from cue 
up to the lowest common parent, lowest common 
parent label concatenated with the relative distance 
in nodes between the siblings, list of non?terminals 
from the lowest common parent down to the token. 
The ?system? feature set is the union of set 1 and 
set 2, and is the one used by the submitted system.
The baseline score is an F1 of 31.5% (P=79.1%, 
R=19.7%) on the dev data.  Using either feature set 
1 or 2 results in substantially better performance. 
They achieve nearly the same score on the dev set 
with an F1 of 50?0.5% (P=87?0.2%, R=35?0.3%) 
in which the difference is that between one case of 
true positive  vs.  false  negative out  of  173.   The 
combination  of  those  feature  sets  is  better  still 
though with an F1 of 54.4% (P=88.3%, R=39.3%).
4 Results
Table 2 presents the scores computed for the sys?
tem output on the held?out evaluation data.  The F1 
for  full  negation  is  48.1%  (P=74%,  R=35.6%), 
which  is  noticeably  lower  than  that  seen  for  the 
dev data (54.4%).  That reduction is to be expected 
because the dev data was used for system tuning. 
There was also evidence of significant over?fitting 
to  the  training  data  because  the  F1 for  that  was 
76.5% (P=92%,  R=65.5%).   The  largest  compo?
nent of the fall off in performance is in the recall. 
The worst performing component of the system 
is the negated event detection which has an F1 of 
54.3% (P=58%,  R=51%) on  the evaluation  data. 
One contributor to low precision for the negated 
event detector is that the root word of an affix cue 
is always output as a negated event, bypassing the 
negated  event  CRF  sequence  classifier.   In  the 
combined training and dev data there is a total of 
1157 gold cues (and scopes) of which 738 (63.8%) 
are annotated as having a negated event.  Of the 
1198  cues  the  system outputs  for  that  data,  188 
(15.7%) are affix cues, each of which will also be 
output as a negated event.  Therefore it would be 
reasonable  to  expect  that  approximately  16 
(27.7%) of the false positives for the negated event 
in the evaluation (60) are due to that behavior.
Table 1: Comparison of full negation scores for various feature sets.
 Gold  System  TP FP FN Precision (%) Recall (%) F1 (%)
Baseline  (train) 984 1034 382 56 602 87.21 38.82 53.73
                (dev)  173 164 34 9 139 79.07 19.65 31.48
Set 1        (train) 984 1034 524 56 460 90.34 53.25 67.00
                (dev) 173 164 60 9 113 86.96 34.68 49.59
Set 2        (train) 984 1034 666 56 318 92.24 67.68 78.07
                (dev) 173 164 61 9 112 87.14 35.26 50.21
System    (train) 984 1034 644 56 340 92.00 65.45 76.49
                (dev) 173 164 68 9 105 88.31 39.31 54.40
Table 2: System evaluation on held?out data.
                             Gold  System  TP FP FN Precision (%) Recall (%) F1 (%)
Cues 264 285 243 33 21 88.04 92.05 90.00
Scopes (no cue match) 249 270 158 33 89 82.90 64.26 72.40
Scope tokens (no cue match) 1805 1816 1512 304 293 83.26 83.77 83.51
Negated (no cue match) 173 154 83 60 80 58.04 50.92 54.25
Full negation 264 285 94 33 170 74.02 35.61 48.09
Cues B 264 285 243 33 21 85.26 92.05 88.52
Scopes B (no cue match) 249 270 158 33 89 59.26 64.26 61.66
Negated B (no cue match) 173 154 83 60 80 53.9 50.92 52.37
Full negation B 264 285 94 33 170 32.98 35.61 34.24
338
5 Conclusion
This paper describes the system I implemented for 
the closed track of the *SEM 2012 Shared Task for 
negation cue, scope, and event resolution.  The sys?
tem?s performance on the held?out evaluation data, 
an F1 of  48.09% (P=74.02%, R=35.61%) for the 
full  negation,  relative to the other entries for the 
task  is  fourth  among  the  six  teams  that  partici?
pated.  
The strongest part of this system is the scope re?
solver which performs at a level near that of the 
best?performing  systems  in  this  shared  task.   I 
think it is likely that the performance on scope res?
olution would be equivalent to them with a better 
negation cue detector.  That is supported by the ?no 
cue match? version of the scope resolution evalua?
tion  for  which  this  system  has  the  highest  F1 
(72.4%).
Clearly the weakest  link is  the  negated  event 
detector.  Since one obvious source of error is that 
the root word extracted when an affix cue is de?
tected  is  always  output  as  a  negated  event,  a 
promising approach for improvement would be to 
instead  utilize  that  as  a  feature  for  the  negated 
event?s CRF sequence tagger so that they have a 
chance to be filtered out in non?factual contexts.
Acknowledgements
I  want  to  thank  Roser  Morante  and  Eduardo 
Blanco for organizing this task, the reviewers for 
their  thorough and very  helpful  suggestions,  and 
Emily Bender for her guidance.
References
Shashank Agarwal and Hong Yu.  2010.  Biomedical 
negation scope detection with conditional random 
fields. Journal of the American Medical Informatics As?
sociation, 17(6), 696?701. 
doi:10.1136/jamia.2010.003228
Chapman, W. W., Bridewell, W., Hanbury, P., Cooper, 
G. F., & Buchanan, B. G..  2001.  A simple algorithm 
for identifying negated findings and diseases in dis?
charge summaries. Journal of Biomedical Informatics, 
34(5), 301?310. doi:10.1006/jbin.2001.1029
Andrew McCallum.  2002. MALLET: A Machine 
Learning for Language Toolkit.  Retrieved from 
http://mallet.cs.umass.edu
Roser Morante and Eduardo Blanco.  2012.  *SEM 
2012 Shared Task: Resolving the Scope and Focus of 
Negation. Proceedings of the First Joint Conference on 
Lexical and Computational Semantics. Presented at the 
*SEM 2012, Montreal, Canada.
Roser Morante and Walter Daelemans.  2009.  A Met?
alearning Approach to Processing the Scope of Nega?
tion. Proceedings of the Thirteenth Conference on Com?
putational Natural Language Learning (CoNLL?2009) 
(pp. 21?29). Boulder, Colorado: Association for Com?
putational Linguistics. 
Roser Morante and Walter Daelemans.  2012.  Conan?
Doyle?neg: Annotation of negation in Conan Doyle sto?
ries. Proceedings of the Eighth International Confer?
ence on Language Resources and Evaluation (LREC).
Pradeep G. Mutalik, Aniruddha Deshpande, and Prakash 
M. Nadkarni.  2001.  Use of general?purpose negation 
detection to augment concept indexing of medical docu?
ments: a quantitative study using the UMLS. Journal of 
the American Medical Informatics Association: JAMIA, 
8(6), 598?609.
Lior Rokach, Roni Romano, and Oded Maimon.  2008. 
Negation recognition in medical narrative reports. 
Information Retrieval, 11(6), 499?538. 
doi:10.1007/s10791?008?9061?0
Buzhou Tang, Xiaolong Wang, Xuan Wang, Bo Yuan, 
and Shixi Fan.  2010.  A Cascade Method for Detecting 
Hedges and their Scope in Natural Language Text. Pro?
ceedings of the Fourteenth Conference on Computa?
tional Natural Language Learning (pp. 13?17). Upp?
sala, Sweden: Association for Computational Linguis?
tics. 
Qi Zhao, Chengjie Sun, Bingquan Liu, and Yong Cheng.  
2010. Learning to Detect Hedges and their Scope Using 
CRF. Proceedings of the Fourteenth Conference on 
Computational Natural Language Learning (pp. 100?
105). Uppsala, Sweden: Association for Computational 
Linguistics. 
339
Proceedings of the NAACL HLT 2010 Second Workshop on Computational Approaches to Linguistic Creativity, pages 14?22,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Comparing Semantic Role Labeling with Typed Dependency Parsing in
Computational Metaphor Identification
Eric P. S. Baumer
Department of Informatics
Univ of California, Irvine
5029 Donald Bren Hall
Irvine, CA 92627-3440 USA
ebaumer@ics.uci.edu
James P. White
School of Information and
Computer Sciences
Univ of California, Irvine
Irvine, CA 92627
jpwhite@uci.edu
Bill Tomlinson
Department of Informatics
Univ of California, Irvine
5068 Donald Bren Hall
Irvine, CA 92627-3440 USA
wmt@uci.edu
Abstract
Most computational approaches to metaphor
have focused on discerning between
metaphorical and literal text. Recent work
on computational metaphor identification
(CMI) instead seeks to identify overarching
conceptual metaphors by mapping selectional
preferences between source and target cor-
pora. This paper explores using semantic role
labeling (SRL) in CMI. Its goals are two-fold:
first, to demonstrate that semantic roles can
effectively be used to identify conceptual
metaphors, and second, to compare SRL to
the current use of typed dependency parsing
in CMI. The results show that SRL can be
used to identify potential metaphors and
that it overcomes some of the limitations
of using typed dependencies, but also that
SRL introduces its own set of complications.
The paper concludes by suggesting future
directions, both for evaluating the use of SRL
in CMI, and for fostering critical and creative
thinking about metaphors.
1 Introduction
Metaphor, the partial framing of one concept in
terms of another, pervades human language and
thought (Lakoff and Johnson, 1980; Lakoff, 1993).
A variety of computational approaches to metaphor-
ical language have been developed, e.g., (Martin,
1990; Fass, 1991; Gedigian et al, 2006; Krishnaku-
maran and Zhu, 2007). However, most such meth-
ods see metaphor as an obstacle to be overcome in
the task of discerning the actual, literal meaning of a
phrase or sentence.
In contrast, the work presented here approaches
conceptual metaphor not as an obstacle but as a
resource. Metaphor is an integral part in human
understanding of myriad abstract or complex con-
cepts (Lakoff and Johnson, 1980), and metaphori-
cal thinking can be a powerful component in crit-
ical and creative thinking, cf. (Gordon, 1974;
Oxman-Michelli, 1991). However, ?because they
can be used so automatically and effortlessly, we
find it hard to question [metaphors], if we can
even notice them? (Lakoff and Turner, 1989, p.
65). Computational metaphor identification (CMI)
(Baumer, 2009; Baumer et al, under review) ad-
dresses this difficulty by identifying potential con-
ceptual metaphors in written text. Rather than at-
tempting to discern whether individual phrases are
metaphorical or literal, this technique instead iden-
tifies larger, overarching linguistic patterns. The
goal of CMI is not to state definitively the metaphor
present in a text, but rather to draw potential
metaphors to readers? attention, thereby encourag-
ing both critical examination of current metaphors
and creative generation of alternative metaphors.
CMI identifies potential metaphors by mapping
selectional preferences (Resnik, 1993) from a source
corpus to a target corpus. Previous work on CMI
utilized typed dependency parses (de Marneffe et
al., 2006) to calculate these selectional preferences.
This paper explores the use of semantic role labeling
(SRL) (Gildea and Jurafsky, 2002; Johansson and
Nugues, 2008) to calculate selectional preferences.
Typed dependencies focus on syntactic structure and
grammatical relations, while semantic roles empha-
size conceptual and semantic structure, so SRL may
14
be more effective for identifying potential concep-
tual metaphors. This paper describes how SRL was
incorporated into CMI and compares both the rela-
tional data and the metaphors identified with typed
dependency parsing and semantic role labeling. The
results show that semantic roles enabled effective
identification of potential metaphors. However, nei-
ther typed dependencies nor semantic roles were
necessarily superior. Rather, each provides certain
advantages, both in terms of identifying potential
metaphors, and in terms of promoting critical think-
ing and creativity.
2 Related Work
2.1 Computational Approaches to Metaphor
Many computational approaches have been taken
toward identifying metaphor in written text. MI-
DAS (Martin, 1990) attempts to detect when users of
the Unix Consultant command line help system use
metaphors, for example, ?How do I enter Emacs?? is
interpreted as ?How do I invoke Emacs?? Another
system, met* (Fass, 1991), is designed to distinguish
both metaphor and metonymy from literal text, pro-
viding special techniques for processing these in-
stances of figurative language. More recently, Gedi-
gian et al (2006) used hand-annotated corpora to
train an automatic metaphor classifier. Krishnaku-
maran and Zhu (2007) used violations of WordNet-
based (Fellbaum, 1998) verb-noun expectations to
identify the presence of a metaphor, e.g., ?he is a
brave lion,? would be considered metaphorical, be-
cause ?he,? taken to mean a ?person,? which is not a
WordNet hyponym of ?lion.?
These and similar approaches ascribe to some
degree to the literal meaning hypothesis (Reddy,
1969), which states that every sentence has a lit-
eral meaning, as derived from the meanings of its
constituent words, while some also have a figurative
meaning that goes beyond the meanings of the words
themselves. In this view, a figurative interpretation
is only sought only after a literal interpretation has
been formed and found inconsistent, nonsensical, or
otherwise faulty. However, experimental evidence
has made this account suspect (Gibbs, 1984; Gen-
tner et al, 2001). Even distinguishing whether a
given expression is literal or figurative can be dif-
ficult at best. For example, ?the rock is becoming
brittle with age? (Reddy, 1969, p. 242), has ?a lit-
eral interpretation when uttered about a stone and a
metaphorical one when said about a decrepit profes-
sor emeritus? (Fass, 1991, p. 54).
One previous metaphor system avoids making
such literal/metaphorical distinctions. CorMet (Ma-
son, 2004) is designed to extract known con-
ventional metaphors from domain-specific textual
corpora, which are derived from Google queries.
CorMet calculates selectional preferences and asso-
ciations (Resnik, 1993) for each corpus?s character-
istic verbs, i.e., those verbs at least twice as frequent
in the corpus as in general English. Based on these
selectional associations, CorMet clusters the nouns
for which the characteristic verbs select. To iden-
tify metaphors, mappings are sought from clusters
in the source corpus to clusters in the target corpus,
based on the degree to which the same verbs select
for members of both clusters. For example, CorMet
was used to extract the metaphor MONEY IS A LIQ-
UID1 by mapping from a cluster for the concept liq-
uid in a corpus for the domain LABORATORY to
a cluster for the concept money in a corpus for the
domain FINANCE, based on the selectional associ-
ations of verbs such as ?pour,? ?flow,? ?freeze,? and
?evaporate.? The CMI system described in this pa-
per is informed largely by CorMet (Mason, 2004).
2.2 Semantic Role Labeling
While interpretations vary somewhat, semantic role
labeling (SRL) generally aims to represent some-
thing about the meaning of a phrase at a deeper
level than surface syntactic structure. One of the
most common approaches to performing SRL au-
tomatically is to use a statistical classifier trained
on labeled corpora (Gildea and Jurafsky, 2002),
with FrameNet (Baker et al, 1998) and PropBank
(Palmer et al, 2005) being the primary sources. An
important result of the Gildea and Jurafsky work
was identifying the significant utility of using pre-
segmented constituents as input to their labeler, and
accordingly most SRL systems perform a syntactic
analysis as an initial step.
The principal alternative to using a statistical clas-
sifier is to use a rule-based labeler for operating on
1SMALL CAPS are metaphors, italics are concepts, CAPS
are domains, and ?quotes? are example phrases.
15
the syntactic parse tree. For example, Shi and Mi-
halcea (2004) extract explicit SRL rules by analyz-
ing FrameNet cases. Another system, RelEx (Fun-
del et al, 2006) also uses rules and is structured like
the implementation used here (see below for details),
but despite having the same name, is a different sys-
tem. Statistical and rule-based methods may also be
used within the same system, such as in LTH (Jo-
hansson and Nugues, 2008).
One reason for preferring a rule-based SRL sys-
tem is that rule-based approaches may be less sus-
ceptible to the loss of accuracy that statistically
trained classifiers suffer when applied to domains
that are different than the corpora they are trained
on (Johansson and Nugues, 2008). That problem is
compounded by the limited domain coverage pro-
vided by the labeled corpora currently available for
SRL classifier training (Gildea and Jurafsky, 2002).
3 Computational Metaphor Identification
While space precludes a fully detailed description
of the algorithms involved, this section provides a
high-level summary of the techniques employed in
CMI (Baumer, 2009; Baumer et al, under review).
Metaphors are conceptual mappings wherein a
source concept partially structures the understand-
ing of a target concept. In ELECTION IS WAR, the
target concept election is partially framed in terms
of the source concept war. CMI begins by gather-
ing corpora for the source and target domains. In
this paper, the target corpus consists of posts from
political blogs, described in more detail in the meth-
ods section below. Source corpora are composed of
Wikipedia articles, as they provide a readily avail-
able, categorically organized, large source of con-
tent on a wide variety of topics. A source corpus
for a given domain consists of all the Wikipedia ar-
ticles in the category for that domain, as well as all
articles in its subcategories. All documents in the
source and target corpora are parsed to extract sen-
tence structure and typed dependencies (Klein and
Manning, 2003; de Marneffe et al, 2006).
The crux of CMI is selectional preference learn-
ing (Resnik, 1993), which quantifies the tendency of
particular words to appear with certain other classes
of words in specific grammatical relationships. For
example, words for the concept of food are often
the direct object of the verb ?eat.? Using the parsed
documents, CMI calculates selectional preferences
of the characteristic nouns in a corpus, where char-
acteristic means that the noun is highly frequent in
the corpus relative to its frequency in general En-
glish, as derived from (Kilgarriff, 1996). Selectional
preference is quantified as the relative entropy of the
posterior distribution conditioned on a specific noun
and grammatical relation with respect to the prior
distribution of verbs in general English:
S(c) =
?
v
P (v|c) log
P (v|c)
P (v)
(1)
where c is a class of nouns (i.e., a concept like
food) and a grammatical relation (such as direct ob-
ject), and v ranges over all the verbs for which c ap-
pears in the given relation. These selectional prefer-
ence strengths are then divided among the verbs that
appear in each grammatical relation to determine the
noun class?s selectional association for each verb in
each relation (Resnik, 1993).
Selectional associations are calculated for classes
of words, but the corpora consist of words that may
represent many possible classes of nouns. Thus, in-
dividual nouns count as partial observations of each
word class that they might represent using WordNet
(Fellbaum, 1998). For example, ?vote,? ?primary,?
and ?runoff? can all represent the concept of elec-
tion. Here we use a customized version of WordNet
that includes major political figures from the 2008
US Election. These word classes are then clustered
using two-nearest-neighbor clustering based on the
verbs for which they select. Each cluster represents
a coherent concept in the corpus, and each is auto-
matically labeled based on the synsets it contains.
This approach of using clustered hypernyms res-
onates with Lakoff?s argument that metaphorical
mappings occur not at the level of situational
specifics, but at the superordinate level. For exam-
ple, in the metaphor LOVE IS A JOURNEY, the re-
lationship is a vehicle. Although specific instantia-
tions of the metaphor may frame that vehicle var-
iously as a train (?off the track?), a car (?long,
bumpy road?), or a plane (?just taking off?), ?the
categories mapped will tend to be at the superordi-
nate level rather than the basic level? (Lakoff, 1993,
p. 212). This method of counting each word ob-
served as a partial observation of each of the synsets
16
it might represent causes observations at the basic
level to accumulate in the superordinate levels they
collectively represent. This is not to say that hier-
archical conceptual relations capture every possible
metaphor, but rather that these are the relations on
which we focus here.
To identify metaphors, CMI looks for correspon-
dences between conceptual clusters in the source
and target corpora. For example, in the Military cor-
pus, the cluster for war would frequently select to be
the direct object of ?win,? the object of the preposi-
tion ?during? with the verb ?fight,? the object of the
preposition ?in? with the verb ?defeated,? and so on.
In some blog corpora, the cluster for election also se-
lects for those same verbs in the same grammatical
relationships. Based on the similarity of these selec-
tional associations, each mapping is given a confi-
dence score to indicate how likely the linguistic pat-
terns are to evidence a conceptual metaphor. One of
the strengths of CMI is that it works in the aggre-
gate. While individual instances of phrases such as
?fought during the election? and ?defeated in the pri-
mary? may not at first glance appear metaphorical,
it is the systematicity of these patterns that becomes
compelling evidence for the existence of a metaphor.
An important aspect of CMI is that it identifies
only linguistic patterns potentially indicative of con-
ceptual metaphors, not the metaphors themselves.
As mentioned above, Lakoff (1993) emphasizes that
metaphor is primarily a cognitive phenomenon, and
that metaphorical language serves as evidence for
the cognitive phenomenon. CMI leverages computa-
tional power to search through large bodies of text to
identify patterns of potential interest, then presents
those patterns to a human user along with the po-
tential metaphors they might imply to foster critical
thinking about metaphor. To reiterate, this places the
job of finding patterns in the hands of the computer,
and the job of interpreting those patterns in the hands
of the human user.
4 CMI with Semantic Role Labeling
The work presented in this paper attempts to en-
hance CMI by using SRL to expand the types of
relations between nouns and verbs that can be seen
as instantiating a metaphor. The prior CMI imple-
mentation treats each grammatical dependency type
as a distinct relation. For example, in the sentence,
?The city contained a sacred grove for performing
religious rites,? ?rites? is the direct object of ?per-
form,? as denoted by the dobj dependency. How-
ever, the sentence, ?The religious rites were once
again performed openly,? uses a passive construc-
tion, meaning that ?rites? is the passive subject, or
nsubjpass, of ?perform.? With SRL, the relations
between ?perform? and ?rite? are the same for both
sentences; specifically, Intentionally act:Act (?rite?
is the intentional act being performed) and Transi-
tive action:Patient (?rite? is the recipient of a tran-
sitive action). Because the relations in FrameNet
are organized into an inheritance structure, both the
more general frame Transitive action and the more
specialized frame Intentionally act apply here.
This section describes how SRL was incorporated
into CMI, compares the component data derived
from SRL with the data derived from a typed de-
pendency parse, and compares resulting identified
metaphors.
4.1 Implementation Methods
The CMI system used here takes the prior im-
plementation (described in section 3) and replaces
the Stanford typed dependency parser (de Marn-
effe et al, 2006) with the RelEx SRL system
(http://opencog.org/wiki/RelEx). RelEx performs a
full syntactic parse, then applies a set of syntactic
pattern rules to annotate the parse tree with role la-
bels based (not exactly or completely) on FrameNet.
This implementation uses a rule-based labeler be-
cause CMI hinges on differences in selectional pref-
erences in corpora from different domains, and sta-
tistically trained classifiers are biased by the distri-
butions of the corpora on which they are trained.
For syntactic parsing, RelEx uses the Link
Grammar Parser (LGP) which is based on the
Link Grammar model (Sleator and Temper-
ley, 1993). LGP produces output very sim-
ilar to typed dependencies. The version of
RelEx we use integrates the Another Nearly-
New Information Extraction (ANNIE) system
(http://gate.ac.uk/sale/tao/splitch6.html#chap:annie)
to tag named entities. Sentences are
split using the OpenNLP sentence splitter
(http://opennlp.sourceforge.net/).
Because CMI?s corpora are acquired from public
17
Blogs Religion (Wikipedia)
Docs 546 (604) 3289 (3294)
Sents 5732 (6708) 128,543 (145,193)
Words 148,619 3,300,455
Table 1: Sizes of the target and source corpora; parenthe-
ses show totals including documents without valid sen-
tences and sentences with no relations.
Internet sources, the text must be cleaned to make
it suitable for parsing. Text from Wikipedia arti-
cles undergoes many small filtering steps in order
to remove wiki markup, omit article sections that
do not consist primarily of prose (e.g., ?See Also?
and ?References?), and decompose Unicode letters
and punctuation into compatibility form. Wikipedia
articles also tend to use bulleted lists in the middle
of sentences rather than comma-separated clauses.
We attempt to convert those constructions back into
sentences, which only sometimes results in a rea-
sonable sentence. However, it helps to ensure that
the following sentence is properly recognized by the
sentence splitter. For blog posts, HTML tags were
removed, which at times required multiple decoding
passes due to improperly configured blog feeds, and
characters decomposed into compatible form.
4.2 Data
Table 1 shows statistics on the sizes of the source
and target corpora. Numbers in parentheses are to-
tals, including blank documents and sentences with
no valid relations. There are some sentences for
which RelEx does not produce any parse, e.g., long
sentences that LGP deems ungrammatical. The
Stanford parser produced some result for every sen-
tence, because it will produce a result tree for any
kind of text, even if it does not recognize any gram-
matically valid tokens.
Table 2 lists the number of verb-noun relations
for each corpus, with parentheses showing aver-
age relations per word. Since RelEx often la-
bels the same verb-noun relation with multiple
hierarchically-related frames (as described above),
Table 2 also lists the number of unique verb-noun
pairs labeled. For the blogs corpus, the Stan-
ford parser generated 111 distinct dependency types,
while RelEx labeled 1446 distinct roles. The ten
Stanford Blogs Religion
Reln(v, n) 19,303 (2.88) 425,367 (2.93)
Unique(v, n) 19,303 (2.88) 425,367 (2.93)
RelEx Blogs Religion
Reln(v, n) 57,639 (8.59) 1,219,345 (8.40)
Unique(v, n) 20,962 (3.12) 482,997 (3.33)
Table 2: Relations for the target and source corpora;
parentheses show average relations per word.
Stanford RelEx
Relation Freq Relation Freq
dobj 3815 Transitive action:Patient 4268
nsubj 3739 Transitive action:Agent 3597
prep in 1072 Inheritance:Item 2 1489
prep to 695 Categorization:Category 1489
prep on 563 Attributes:Attribute 1488
nsubjpass 528 Existence:Entity 1277
prep for 491 Categorization:Item 1270
prep with 435 Inheritance:Item 1 1269
prep at 285 Attributes:Entity 1268
dep 279 Purpose:Means 569
Table 3: Most common dependencies and frequencies.
most common of each are listed with their frequen-
cies in Table 3.
These data show that RelEx provides more infor-
mation, both in terms of successfully parsing more
sentences, and in terms of relations-per-word. The
next section explores the impact of these differences
on identified metaphors.
4.3 Results
This section describes metaphors identified when
mapping from the RELIGION source corpus to the
political blogs target corpus. CMI results are usu-
ally culled to include only the upper one percentile
in terms of confidence, but space constraints prohibit
a full analysis of even this upper one percentile. In-
stead, this section compares mappings with the high-
est confidence score from the typed dependency data
and from the semantic role data. RELIGION was
chosen as the source domain because the highest
confidence metaphors from both typed dependencies
and semantic roles had similar target and source con-
cepts, facilitating a better comparison. This analysis
18
Target (label and cluster) Source (label and cluster) Conf
medicine - {medicine, medical specialty}, {medicine,
medication, medicament, medicinal drug}, {music,
medicine}, {medicine, practice of medicine}, {learned
profession}, {drug}, {social control}, {profession},
{punishment, penalty, penalization, penalisation}, {life
science, bioscience}
sacrament - {sacrament},
{baptism}, {religious ceremony,
religious ritual}
1.968
ritual - {ceremony}, {practice,
pattern}, {custom, usage, usance},
{ritual, rite}, {survival}
1.465
Table 4: Metaphors for medicine from RELIGION using typed dependencies.
is not intended to demonstrate that either technique
is superior (for more on possible evaluation meth-
ods, see Discussion section below). Rather, it pro-
vides a detailed depiction of both to ascertain poten-
tial benefits and drawbacks of each.
Table 4 presents the strongest two mappings
from RELIGION: MEDICINE IS A SACRAMENT and
MEDICINE IS A RITUAL; these were the only map-
pings for medicine in the upper one percentile. Each
mapping lists both the automatically identified la-
bels and the full cluster contents for source and tar-
get, along with the confidence score. The table can
be read left-to-right, e.g., ?medicine is like a sacra-
ment.? Confidence scores typically fall in the range
(0, 5) with a few high-confidence mappings and
many low-confidence mappings; see (Baumer, 2009;
Baumer et al, under review) for details of confi-
dence score calculation. Table 5 shows details for
each mapping, including the verb-relation pairs that
mediate the mapping, along with an example frag-
ment from the target and source corpora for each
verb-relation. These examples show why and how
medicine might be like, variously, a sacrament or
a ritual; both are ?practiced,? ?administered,? ?per-
formed,? etc. Note that the passive subject and di-
rect object relations are treated as distinct, e.g., ?Eu-
charist is variously administered? involves a differ-
ent grammatical relation than ?administer the sacra-
ment,? even though the word for sacrament plays a
similar semantic role in both fragments.
Tables 6 and 7 show mappings resulting from se-
mantic roles labeled by RelEx, with formats simi-
lar to those of tables 4 and 5, except that the verb-
relations in table 7 are semantic roles rather than
grammatical relations. The mapping in table 6 was
the strongest mapping from RELIGION and the only
mapping for medication.
Table 7 shows how RelEx can treat different
grammatical relations as the same semantic role. For
example, ?medicine is practiced? and ?practice the
rites? use passive subjective and direct object, re-
spectively, but are both treated as the patient of a
transitive action. Such examples confirm that SRL
is, at least to some extent, performing the job for
which it was intended.
However, these results also expose some prob-
lems with SRL, or at least with RelEx?s implemen-
tation thereof. For example, the phrase ?dispose of
prescription drugs? is labeled with four separate se-
mantic roles, which is an instance of a single verb-
noun relation being labeled with both a superordi-
nate relation, Physical entity:Entity, and a subordi-
nate relation, Physical entity:Constituents (the con-
stituents of a physical entity are themselves an en-
tity). While various approaches might avoid multi-
ple labels, e.g., using only the most general or most
specific frame, those are beyond the scope here.
5 Discussion
As mentioned above, these results do not provide
conclusive evidence that either typed dependencies
or semantic roles are more effective for identify-
ing potential metaphors. However, they do provide
an understanding of both techniques? strengths and
weaknesses for this purpose, and they also suggest
ways in which each may be more or less effective at
fostering critical and creative thinking.
For metaphor identification, the previous sec-
tion described how typed dependency parsing treats
passive subjects and direct object as distinct rela-
tions, whereas SRL will at times conflate them into
identical patient roles. This means that the typed
dependency-based metaphors appear to be mediated
by a greater number of relations. However, it also
19
Target Source Verb-Reln Target Ex Frag Source Ex Frag
medicine
sacrament
practice -
nsubjpass
?medicine is practiced? ?rites were practiced?
administer -
nsubjpass
?antibiotics are regularly
administered?
?Eucharist is variously ad-
ministered?
administer - dobj ?administered medicines? ?administer the sacra-
ment?
perform - dobj ?perform defensive
medicine?
?performed the last rites?
receive - dobj ?received conventional
medicines?
?received the rites?
ritual
perform - dobj ?perform defensive
medicine?
?performed the last rites?
practice -
nsubjpass
?medicine is practiced? ?ceremonies are also prac-
ticed?
administer - dobj ?administered medicines? ?administering the rites?
administer -
nsubjpass
?antibiotics are regularly
administered?
?sacrament is ordinarily
administered?
Table 5: Details of RELIGION metaphors from typed dependencies, including mediators and example phrases.
Target (label and cluster) Source (label and cluster) Conf
medication - {medicine, medication, medica-
ment, medicinal drug}, {drug}, {agent}
ceremony - {ceremony}, {sacrament}, {rite, reli-
gious rite}, {religious ceremony, religious ritual}
2.570
Table 6: Metaphor for medication from RELIGION using semantic roles.
Target Source Verb-Reln Target Ex Frag Source Ex Frag
medication ceremony
practice -
Transitive action:Patient
?medicine is prac-
ticed?
?practice the rites?
perform -
Transitive action:Patient
?perform defensive
medicine?
?perform most reli-
gious rites?
include -
Transitive action:Agent
?medicine including? ?liturgies included?
dispose -
Physical entity:Constituents
?dispose of prescrip-
tion drugs?
?disposed of without
ceremony?
dispose -
Inheritance:Instance
?dispose of prescrip-
tion drugs?
?disposed of without
ceremony?
dispose -
Inheritance:Group
?dispose of prescrip-
tion drugs?
?disposed of without
ceremony?
dispose -
Physical entity:Entity
?dispose of prescrip-
tion drugs?
?disposed of without
ceremony?
Table 7: Details of RELIGION metaphors from semantic roles, including mediators and example phrases.
20
means that less data are available to the selection
preference calculation, in that there are fewer obser-
vations for each relation. On the other hand, SRL
is a much finer-grained classification than typed de-
pendencies. The implementation used here included
111 grammatical relations, whereas RelEx labeled
1446 distinct roles. Thus, overall, RelEx may be
providing fewer observations for each relation, but
those relations may have more semantic import.
For fostering critical thinking and creativity, a
key concern is making identified metaphors read-
ily comprehensible. Ortony (Ortony, 1980) and oth-
ers have suggested that selectional restriction vi-
olations are an important component of metaphor
comprehension. Therefore, tools that employ CMI
often present parallel source and target fragments
side-by-side to make clear the selectional restric-
tion violation, e.g., metaViz, a system for present-
ing computationally identified metaphors in politi-
cal blogs (Baumer et al, 2010). One might assume
that typed dependencies are more readily compre-
hensible, since they are expressed as relatively sim-
ple grammatical relations. However, when present-
ing example fragments to users, there is no need to
explicate the nature of the relationship being demon-
strated, but rather the parallel examples can simply
be placed side-by-side. It is an empirical question
whether users would see phrases such as ?medicine
is practiced? and ?practice the rites? as parallel ex-
amples of the same psycholinguistic relationship.
Thus, the question of whether typed dependencies
or semantic roles better facilitate metaphor compre-
hension may not be as important as the question of
whether example phrases are perceived as parallel.
6 Future Work
This paper is only an initial exploration, demonstrat-
ing that semantic role labeling is viable for use in
CMI. For the sake of comparison, the analysis here
focuses on examples where metaphors identified us-
ing the two techniques were relatively similar. How-
ever, such similarity does not always occur. For
example, using MILITARY as the source domain,
typed dependencies led to results such as A NOM-
INEE IS A FORCE and A NOMINEE IS AN ARMY,
whereas semantic roles gave mappings including AN
INDIVIDUAL IS A WEAPON (here, the label ?indi-
vidual? is a superordinate category including mostly
politicians), and THE US IS A SOLDIER. Future
work should analyze these differences in more de-
tail to provide a broad and deep comparison across
multiple source domains and target corpora.
But how should such an analysis be conducted?
That is, how does one determine which identified
metaphors are ?better,? and by what standard? In
suggesting a number of potential evaluation methods
for CMI, Baumer et al (under review) argue that the
most sensible approach is asking human subjects to
assess metaphors, potentially along a variety of cri-
teria. For example: Does the metaphor make sense?
Is it unexpected? Is it confusing? Such assess-
ments could help evaluate semantic roles vs. typed
dependencies in two ways. First, does either pars-
ing technique lead to metaphors that are consistently
assessed by subjects as better? Second, does ei-
ther parsing technique lead to better alignment (i.e.,
stronger correlations) between human assessments
and CMI confidence scores? Such subjective as-
sessments could provide evidence for an argument
that either typed dependencies or semantic roles are
more effective at identifying conceptual metaphors.
7 Conclusion
This paper explores using semantic role labeling
(SRL) as a technique for improving computational
metaphor identification (CMI). The results show that
SRL can be successfully incorporated into CMI.
Furthermore, they suggest that SRL may be more
effective at identifying relationships with semantic
import than typed dependency parsing, but that SRL
may also make distinctions that are too fine-grained
to serve as effective input for the selectional pref-
erence learning involved in CMI. The results also
demonstrate that, even though the notion of seman-
tic roles may seem more complex than typed de-
pendencies from a user?s perspective, it is possi-
ble to present either in a way that may be readily
comprehensible. Thus, while more work is neces-
sary to compare these two parsing techniques more
fully, semantic role labeling may present an effective
means of improving CMI, both in terms of the tech-
nical process of identifying conceptual metaphors,
and in terms of the broader goal of fostering critical
thinking and creativity.
21
Acknowledgments
This material is based on work supported by the
National Science Foundation under Grant No. IIS-
0757646, by the Donald Bren School of Informa-
tion and Computer Sciences, by the California Insti-
tute for Telecommunications and Information Tech-
nology (Calit2), and by the Undergraduate Research
Opportunities Program (UROP) at UCI.
References
Colin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet project. In Proc 17th
Int?l Conf on Comp Ling, pages 86?90, Montral, Que-
bec, Canada.
Eric P. S. Baumer, Jordan Sinclair, and Bill Tomlinson.
2010. ?America is like Metamucil:? Critical and cre-
ative thinking about metaphor in political blogs. In
ACM SIGCHI Conf, Atlanta, GA. ACM Press.
Eric P. S. Baumer, David Hubin, and Bill Tomlinson. un-
der review. Computational metaphor identification.
Eric Baumer. 2009. Computational Metaphor Identifica-
tion to Foster Critical Thinking and Creativity. Dis-
sertation, University of California, Irvine, Department
of Informatics.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed de-
pendency parses from phrase structure parses. In Lang
Res and Eval (LREC), Genoa, Italy.
Dan Fass. 1991. Met*: A method for discriminating
metonymy and metaphor by computer. Comp Ling,
17(1):49?90.
Christine Fellbaum. 1998. WordNet: An Electronic Lex-
ical Database. MIT Press, Cambridge, MA.
Katrin Fundel, Robert Kuffner, and Ralf Zimmer. 2006.
RelEx-Relation extraction using dependency parse
trees. Bioinformatics, 23(3):365?371.
Matt Gedigian, John Bryant, Srini Narayanan, and Bran-
imir Ciric. 2006. Catching metaphors. In 3rd Work-
shop on Scalable Natural Language Understanding,
New York City. Assoc Comp Ling.
Dedre Gentner, Brian F. Bowdle, Phillip Wolff, and
C. Boronat. 2001. Metaphor is like analogy. In Dedre
Gentner, Keith J. Holyoak, and Boicho Kokinov, edi-
tors, The Analogical Mind, pages 199?253. MIT Press,
Cambridge, MA.
Raymond W. Gibbs. 1984. Literal meaning and psycho-
logical theory. Cognitive Science, 8:275?304.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic la-
beling of semantic roles. Comp Ling, 28(3):245?288.
W.J.J. Gordon. 1974. Some source material in discovery-
by-analogy. Journal of Creative Behavior, 8:239?257.
Richard Johansson and Pierre Nugues. 2008.
Dependency-based semantic role labeling of Prop-
Bank. In Proc Conf on Empirical Meth in Nat Lang
Proc, pages 69?78, Honolulu, HI. Assoc Comp Ling.
Adam Kilgarriff. 1996. BNC word frequency list.
http://www.kilgarriff.co.uk/bnc-readme.html.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Mtg of the Assoc for Comp
Ling, Sapporo, Japan.
Saisuresh Krishnakumaran and Xiaojin Zhu. 2007.
Hunting elusive metaphors using lexical resources.
In Xiaofei Lu and Anna Feldman, editors, Computa-
tional Approaches to Figurative Language, Workshop
at HLT/NAACL 2007, Rochester, NY.
George Lakoff and Mark Johnson. 1980. Metaphors We
Live By. University of Chicago Press, Chicago, IL,
2003 edition.
George Lakoff and Mark Turner. 1989. More Than Cool
Reason: A Field Guide to Poetic Metaphor. University
of Chicago Press, Chicago and London.
George Lakoff. 1993. The contemporary theory of
metaphor. In A. Ortony, editor, Metaphor and thought,
2nd. ed., pages 202?251. Cambridge Univ Press, New
York.
James H. Martin. 1990. A Computational Model of
Metaphor Interpretation. Acad Press, San Diego, CA.
Zachary J. Mason. 2004. CorMet: a computational,
corpus-based conventional metaphor extraction sys-
tem. Comp Ling, 30(1):23?44, March.
Andrew Ortony. 1980. Some psycholinguistic aspects of
metaphor. In R.P. Honeck and H.R. Robert, editors,
Cog and Fig Lang, pages 69?83. Erlbaum Associates,
Hillsdale, NJ.
Wendy Oxman-Michelli. 1991. Critical thinking as cre-
ativity. Technical Report SO 023 597, Montclair State,
Institute for Critical Thinking, Montclair, NJ.
Martha Palmer, Paul Kingsbury, and Daniel Gildea.
2005. The Proposition Bank: An annotated corpus of
semantic roles. Comp Ling, 31(1):71?106, March.
Michael J. Reddy. 1969. A semantic approach to
metaphor. In Chicago Linguistic Society Collected Pa-
pers, pages 240?251. Chicago Univ Press, Chicago.
Philip Resnik. 1993. Selection and Information: A
Class-Based Approach to Lexical Relationships. Dis-
sertation, University of Pennsylvania, Department of
Computer and Information Science.
Lei Shi and Rada Mihalcea. 2004. Open text seman-
tic parsing using FrameNet and WordNet. In Demon-
stration Papers at HLT-NAACL 2004, pages 19?22,
Boston. Assoc for Computational Linguistics.
Daniel Sleator and Davy Temperley. 1993. Parsing En-
glish with a Link Grammar. In Proc Third Interna-
tional Workshop on Parsing Technologies, pages 277?
292.
22
Proceedings of the ACL 2014 Workshop on Semantic Parsing, pages 76?81,
Baltimore, Maryland USA, June 26 2014.
c
?2014 Association for Computational Linguistics
Towards README-EVAL : Interpreting README File Instructions
James Paul White
Department of Linguistics
University of Washington
Seattle WA 98195-4340
jimwhite@uw.edu
Abstract
This abstract describes README-EVAL,
a novel measure for semantic parsing eval-
uation of interpreters for instructions in
computer program README files. That
is enabled by leveraging the tens of thou-
sands of Open Source Software programs
that have been annotated by package main-
tainers of GNU/Linux operating systems.
We plan to make available a public shared
implementation of this evaluation.
1 Introduction
That natural language is learned by humans in
rich grounded perceptual contexts has been rec-
ognized by many researchers for quite some time
(Regier, 1996) (Silberer and Lapata, 2012). But
most efforts at machine learning of natural lan-
guage continue to address tasks which are en-
tirely divorced from any grounding and/or have
perceptual requirements for which machines are
ill-suited. Computers are machines and their nat-
ural perceptual context is that of the computing
machine world. Therefore, to apply the model of
grounded language learning most effectively, we
should choose tasks in which the relevant percepts
are of those in the computing world (e.g., bits,
bytes, characters, files, memory, operations, pro-
grams, events, processes, services, devices, pro-
cessors, drivers, operating systems, and networks).
This abstract describes proposed work aimed
at the goal of deep semantic parsing of the web,
which for us includes the ability to interpret doc-
uments that give instructions for acting on com-
puter systems in human natural language. To facil-
itate research in that direction, we plan to evaluate
systems that build software packages by follow-
ing the README
1
file instructions contained in
1
We use the term README file in a broad sense mean-
ing a document that contains instructions to be read by a hu-
GNU/Linux distributions like Centos and Debian.
Key to this plan is the novel README-EVAL
score which we propose as an extrinsic (i.e. goal-
oriented) performance measure for parsing, map-
ping/planning, and related linguistics tasks. The
planned baseline system is a pipeline using a doc-
ument classifier and instruction sequence extractor
trained on hand-labeled data followed by a rein-
forcement learner for mapping the instructions to
a build script (plan of actions) for that software
package (context).
2 Background
A significant challenge for semantic parsing re-
search is finding a method to measure a system?s
performance that will indicate its effectiveness in
the domain of interest. Traditionally the approach
has been to gather and have human annotators
make judgements that are of the same kind the
system is intended to perform. That process is rel-
atively costly and may result in a corpus which is
actually too small considering the amount of varia-
tion that occurs when humans perform an activity.
Relevant prior work in the computing domain pro-
duced the Linux and Monroe plan corpora (Blay-
lock and Allen, 2005). The Linux Plan Corpus
consists of 457 interactive shell script sessions,
with an average of 6.1 actions each, captured from
human experimental subjects attempting to satisfy
one of 19 different goals stated as an English sen-
tence. Although it has been used successfully by
those and other researchers, the natural variation
in human behavior means that a corpus of such
relatively small size appears to be very noisy. As
a result they have had to rely on artificially gener-
ated data such as the Monroe Plan Corpus in order
to get results that are more easily compared across
system evaluations.
man that concern performing actions on a computer (whether
at the keyboard or some other input device). For this task
we confine ourselves to instructions given for the purpose of
building a software package.
76
More promising therefore is the way some re-
searchers have discovered ways to repurpose data
and/or judgements created for other purposes and
turn them into training data and/or evaluations of
NLP systems. We employ that paradigm here by
repurposing the efforts of Open Source Software
(OSS) package maintainers who have created an-
notations (aka metadata) including dependency re-
lations and scripts that build computer programs.
3 GNU/Linux Software Package Data
The advent of the Internet resulted in explosive
growth for OSS, the premier example of which
is the GNU/Linux operating system family. Cur-
rent distributions contain packages built from over
15,000 program source bundles.
2
The production
of OSS packages for such systems typically in-
volves two different types of programmers work-
ing independently. The authors of the source
computer program usually do not produce pack-
aging metadata for their work and instead tend
to write README files and related documenta-
tion explaining how to build and use the software.
The package maintainers then work out the spe-
cific requirements and scripts necessary to build
the program as some package(s) using the partic-
ular package manager and format of the OS dis-
tribution (aka ?distro?) that they are supporting.
Software package metadata contained in bundles
such as Debian .deb and Fedora RPM .spec
files are rich in annotations.
3,4
See Figure 1 for excerpts of text describing the
Bean Scripting Framework (BSF) from its Source
RPM Package Manager (SRPM) package in the
Fedora Core 17 distribution.
5
The two kinds of
data shown are file contents (1a, 1c, 1e), which
usually originate with the ?upstream? program au-
thor(s), and sections from the RPM Spec file (1b,
1d, 1f), which are annotations (aka metadata) cu-
rated by the package maintainers. There are other
2
Debian Wheezy has over 37,000 packages from about
17,500 source packages https://www.debian.org/
News/2013/20130504 and Fedora 20 has more than
15,000 packages https://admin.fedoraproject.
org/pkgdb/collections/.
3
https://www.debian.org/doc/manuals/
maint-guide/dreq.en.html
4
http://www.rpm.org/max-rpm/
ch-rpm-inside.html
5
For more examples, we refer the interested reader the
author?s web page which includes access to a web linked
data explorer for the entire corpus.
http://students.washington.edu/jimwhite/
sp14.html
sections and fields used in RPM Spec files, but
those tend to more distro-specific and these suffice
for this discussion.
Figure 1a shows some BSF package description
text from the source README.txt file and Figure
1b shows the version appearing the RPM Spec.
That close textual similarity is a common occur-
rence in the data and can be used to identify some
likely README files. Those are only a starting
point though, because the natural language pro-
gram build instructions are often in other files, as
in this case. For many packages those instruc-
tions are in a file named INSTALL. There is an
INSTALL.txt file with some instructions for BSF
here (Figure 1e), but they are for a binary instal-
lation. The instructions for building from source
that we will primarily concerned with here are in
the file BUILDING.txt (Figure 1c).
A potential use for this data that we haven?t ex-
plored yet is its use in summarization tasks. In ad-
dition to the text which is usually in the README
file and RPM Spec DESCRIPTION section, there
is the ?Summary? field of the PACKAGE section.
Although in Figure 1d the value for the summary
field appears as just the package?s full name, this
is typically a full sentence that is a good one-line
summary of the multiple line description section.
It is worthwhile to notice that thousands of
programs have been packaged multiple times for
different systems (e.g. Debian, Fedora, Cygwin,
NixOS, Homebrew, and others) and many pack-
ages have also been internationalized.
6
Both of
those aspects point to opportunities for learning
from parallel data.
For the present discussion we focus on two par-
ticular elements of package metadata: dependen-
cies and build scripts.
7
The packages in a distribu-
tion have dependency relationships which desig-
nate which packages must be built and installed for
other packages to be built, installed, and/or exe-
cuted. These relationships form a directed acyclic
graph (DAG) in which the nodes are packages and
the edges are dependency relationships.
6
Debian for example currently lists more than 800k sen-
tences in the localization database and about 75 human lan-
guages have translations for at least 100k of them with the
top ten languages having over 500k each https://www.
debian.org/international/l10n/po/rank.
7
Packaging systems usually support at least three types of
scripts: build, install, and remove. The build script usually
has more in common with the README instructions than
the install and remove scripts which are more distro specific.
Some packages also have a check script to validate the state
of a build prior to performing the install operation.
77
(a) README.txt file (d) RPM Spec PACKAGE section (metadata)
Bean Scripting Framework (BSF) is a set of Java classes
which provides an easy to use scripting language support
within Java applications. It also provides access to Java
objects and methods from supported scripting languages.
? ? ?
(b) RPM Spec DESCRIPTION section
Bean Scripting Framework (BSF) is a set of Java classes
which provides scripting language support within Java
applications, and access to Java objects and methods from
scripting languages.
? ? ?
(c) BUILDING.txt file
From the ant "build.xml" file:
Master Build file for BSF
Notes:
This is the build file for use with
the Jakarta Ant build tool.
Optional additions:
BeanShell -> http://www.beanshell.org/
Jython -> http://www.jython.org/
JRuby -> http://www.jruby.org/ (3rd ...)
Xalan -> http://xml.apache.org/xalan-j
. . .
Build Instructions:
To build, run
java org.apache.tools.ant.Main <target>
on the directory where this file is
located with the target you want.
Most useful targets:
- all -> creates the binary and src
distributions, and builds the site
- compile -> creates the "bsf.jar"
package in "./build/lib" (default target)
- samples -> creates/compiles the samples
? ? ?
Name: bsf
Version: 2.4.0
Release: 12.fc17
Summary: Bean Scripting Framework
License: ASL 2.0
URL: http://commons.apache.org/bsf/
Group: Development/Libraries
BuildRequires: jpackage-utils >= 1.6
BuildRequires: ant, xalan-j2, jython
BuildRequires: rhino
BuildRequires: apache-commons-logging
Requires: xalan-j2
Requires: apache-commons-logging
Requires: jpackage-utils
BuildArch: noarch
? ? ?
(e) INSTALL.txt file
Installing BSF consists of copying
bsf.jar and .jars for any languages
intended to be supported to a directory
in the execution CLASSPATH of your
application, or simply adding them
to your CLASSPATH.
BSF can be used either as a standalone
system, as a class library, or as part
of an application server. In order to be
used as a class library or as a standalone
system, one must simply download the
bsf.jar file from the BSF web site
(http://jakarta.apache.org/bsf/index.html)
and include it in their CLASSPATH, along
with any required classes or jar files
implementing the desired languages.
? ? ?
(f) RPM Spec BUILD section (shell script)
[ -z "$JAVA_HOME" ] && export JAVA_HOME=/usr/lib/jvm/java
export CLASSPATH=$(build-classpath apache-commons-logging jython xalan-j2 rhino)
ant jar
/usr/bin/rm -rf bsf/src/org/apache/bsf/engines/java
ant javadocs
Figure 1: Bean Scripting Framework (BSF) excerpts from Fedora Core 17 RPMS.
4 From Dependencies to Validation
The idea that turns the package dependency DAG
into training, test, and evaluation data is to choose
dependency targets for test (i.e. the system build
script outputs will be used for them in test) and
dependency sources (the dependent packages) for
validation (their package maintainer written build
scripts are used as is to observe whether the depen-
dencies are likely to be good). Validation subsets
can be arranged for both internal validation (tun-
ing) and external validation (evaluation).
Two kinds of dependency relationships are
of special interest here: Requires and
BuildRequires. The former typically means
the target package (its name appears to the right of
a Requires or BuildRequires in Figure 1d)
is required at both build time and execution time
by the source package (identified by the Name
field of Figure 1d) while the latter means it is only
required at build time. That distinction can be
used to guide the selection of which packages to
choose for the validation and test subsets. Pack-
ages that are the target of a BuildRequires
relationship are more likely to cause their depen-
dents? build scripts to fail when they (the targets)
are built incorrectly than targets of a Requires
relationship.
Analysis of the 2,121 packages in Release 17 of
the Fedora Core SRPM distribution shows 1,673
package nodes that have a build script and some
declared dependency relationship. Those build
scripts average 6.9 non-blank lines each. Of
those nodes, 1,009 are leaves and the 664 inter-
78
nal nodes are the target of an average of 7 de-
pendencies each. There are 218 internal nodes
that are the direct target of at least one leaf node
via a BuildRequires relationship and they av-
erage 12.4 such dependent leaves each. We ex-
pect to have a larger corpus prepared from a full
GNU/Linux distribution (at least 15,000 source
packages) at the time of the workshop.
5 Task Description
The top-level README-EVAL task would be to
generate complete packaging metadata given the
source files for a program thus automating the
work of a package maintainer. Since that task
is somewhat complicated, it is useful to break
it down into multiple subtasks which can be ad-
dressed and evaluated separately before proceed-
ing to combine them. For the discussion here we
will consider a partial solution using a four stage
pipeline: README document classification, in-
struction extraction, dependency relation extrac-
tion, and build script generation.
The corpus? package metadata can be used to
directly evaluate the results of the last two stages
of that pipeline. The first two stages, README
document classification and instruction extraction,
are well understood tasks for which a moderate
amount of manually labelled data can suffice to
train and test effective classifiers.
The dependency relation extraction subtask can
be treated as a conventional information extraction
task concerned with named entity recognition for
packages and relation extraction for dependencies.
We may regard the dependencies in the corpus as
effectively canonical because the package main-
tainers strive to keep those annotations to a rea-
sonable minimum. Therefore computing precision
and recall scores of the dependency DAG edges
and labels of this stage?s output versus the corpus?
metadata will be a meaningful metric.
Work on instruction and direction following is
applicable to the build script generation subtask.
Such systems tend to be somewhat more complex
than shallow extraction systems and may incor-
porate further subcomponents including goal de-
tectors and/or planners that interact with a seman-
tic parser (Branavan et al., 2012). It is possible
to evaluate the final stage output by comparing it
to the build script in the package?s metadata, but
that would suffer from the same sort of evalua-
tion problems that other language generation tasks
have when we are concerned with semantics rather
than syntax. This is where the superiority of an
NLP task where the target language is understood
by computers comes in, because we can also eval-
uate it using execution. Which isn?t to say we can
solve the program equivalence problem in general,
but README-EVAL does a pragmatic determina-
tion of how good a substitute it is based on its us-
age by the package?s dependency sources.
6 README-EVAL Scoring
The README-EVAL score is a measure of how
effective the system under test (SUT) is at gener-
ating software package metadata. For the compo-
nents of the SUT this score can serve as an extrin-
sic indication of their effectiveness.
Let N be a set of tuples (x, y) representing the
corpus in which x is the package data and relevant
metadata subset minus the labels to be generated
and y is a known good label for x. To prepare the
corpus for the task, two disjoint subsets C and T
are selected from the set of all package nodes N .
C is for the common packages which are available
to the SUT for training, and T is for the test pack-
ages that the SUT?s interpretation function will be
tested on. A third set V which is disjoint from T
is selected from N for the validation packages.
Many partitioning schemes are possible. A sim-
ple method is to choose the leaf nodes (packages
that are sources but not targets of dependency re-
lationships) for V . The members of T can then
be chosen as the set of packages which are the di-
rect targets of the dependency relationships from
V . The members of V are expected to be likely
to fail to build correctly if there are errors in the
system outputs for T . Note that for the SUT to
do tuning it will need some leaf node packages in
C. Therefore if V is made disjoint from C then it
should not actually select all of those leaves.
The README-EVAL score R is computed us-
ing a suitable loss function L for the SUT?s la-
bel predictor function
?
Y .
?
Y is presumed to have
been trained on C and it yields a set of (x, y?) tu-
ples given a set of x values. The loss function
L((x, y), D) yields a real number in the range 0
to 1 inclusive that indicates what fraction of the
components in package (x, y) are incorrect given
the context D ? N . It is required for all v ? V
that L(v, (C ? T ? V ) \ {v}) = 0.
For this exposition, assume y is a build script
and L yields 0 if it succeeds and 1 if it fails. Linux
processes typically indicate success by returning a
zero exit code. Therefore a simple realization of
79
L is to return 0 if the process executing the build
script y of (x, y) given D returns zero and 1 oth-
erwise.
The computation iterates over each member
D ? partition(T ) and obtains measures of cor-
rectness by evaluating B(
?
Y (X(D))?C ? T \D)
where X is a function that yields the set of x val-
ues for a given set of (x, y) tuples. To keep the task
as easy as possible, the members of partition(T )
may be singletons.
B(D) = |V | ?
?
v?V
L(v, (D ? V ) \ {v})
Those values are normalized by a scale factor
for each D determined by the value of B given D
minus B given Z(D). Z(D) is the set of tuples
(x, ?) for a given set D where ? is the null label.
A null label for a build script is one which has no
actions and executes successfully.
R(D) =
B(
?
Y (X(D))?C?T\D)
B(C?T )?B(Z(D)?C?T\D)
The final README-EVAL measureR is the av-
erage score over those partitions:
R =
?
D?partition(T )
R(D)
|partition(T )|
6.1 Loss Function Variations
There are other useful implementation variations
for the loss function L. In a system where the
number of components can be determined inde-
pendently from whether they are correct or not, a
possibly superior alternative is to return the num-
ber of incorrect components divided by the total
number of components. To determine loss for a
build script for example, the value may be deter-
mined by counting the number of actions that exe-
cute successfully and dividing by the total number
of steps.
A further consideration in semantic evaluation
is parsimony, which is the general expectation that
the shortest adequate solution is to be preferred
(Gagne et al., 2006). To incorporate parsimony
in the evaluation we can add a measure(s) of the
solution?s cost(s), such as the size of the label y
and/or execution resources consumed, to L.
7 Conclusion
A common objection to tackling this task is that
it seems too hard given the state of our knowl-
edge about human language, computer program-
ming (as performed by humans), and especially
the capabilities of current NLP systems. We con-
sider that to be a feature rather than a bug. It
may be some time before a state-of-the-art im-
plementation of a README interpreter is suffi-
ciently capable to be considered comparable to
an expert human GNU/Linux package maintainer
performance, but that is perfectly fine because we
would like to have an evaluation that is robust,
long-lived, and applicable to many NLP subtasks.
We also have the more pragmatic response given
here which shows that that difficult task can be
decomposed into smaller subtasks like others that
have been addressed in the NLP and computa-
tional linguistics communities.
To conclude, this proposal recommends
README-EVAL as an extrinsic (goal-oriented)
evaluation system for semantic parsing that could
provide a meaningful indication of performance
for a variety of NLP components.
Because the evaluation platform may be some-
what complicated to set up and run, we would
like to make a publicly available shared evalua-
tion platform on which it would be a simple matter
to submit new systems or components for evalua-
tion. The MLcomp.org system developed by Percy
Liang and Jacob Abernethy, a free website for ob-
jectively comparing machine learning programs,
is an especially relevant precedent (Gollub et al.,
2012). But we notice that the NLP tasks on ML-
comp receive little activity (the last new run was
more than a year ago at this writing) which is in
stark contrast to the other ML tasks which are very
active (as they are on sites like Kaggle). With the
README-EVAL task available in such an easy-
to-use manner could draw significant participation
because of its interesting and challenging domain,
especially from ML and other CS students and re-
searchers.
Finally we look forward to discussing this pro-
posal with the workshop attendees, particularly in
working out the details for manual annotation of
the README files for the instruction extractor
(including whether it is needed), and discussing
ideas for a baseline implementation.
8 Acknowledgements
Thank you to my University of Washington col-
leagues who reviewed earlier drafts of this abstract
and the workshop?s blind reviewers for their help-
ful comments.
References
Nate Blaylock and James Allen. 2005. Recognizing
Instantiated Goals using Statistical Methods. In IJ-
CAI Workshop on Modeling Others from Observa-
tions (MOO-2005), page 79.
80
S. R. K. Branavan, Nate Kushman, Tao Lei, and Regina
Barzilay. 2012. Learning High-Level Planning from
Text. In Proceedings of the 50th Annual Meeting
of the Association for Computational Linguistics:
Long Papers-Volume 1, page 126. Association for
Computational Linguistics.
Christian Gagne, Marc Schoenauer, Marc Parizeau,
and Marco Tomassini. 2006. Genetic Programming,
Validation Sets, and Parsimony Pressure. In Genetic
Programming, page 109. Springer.
Tim Gollub, Benno Stein, and Steven Burrows. 2012.
Ousting Ivory Tower Research: Towards a Web
Framework for Providing Experiments as a Service.
In Proceedings of the 35th international ACM SIGIR
conference on Research and development in infor-
mation retrieval, page 1125. ACM.
Terry Regier. 1996. The Human Semantic Potential:
Spatial Language and Constrained Connectionism.
MIT Press.
Carina Silberer and Mirella Lapata. 2012. Grounded
Models of Semantic Representation. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, page 1423. Associa-
tion for Computational Linguistics.
81
