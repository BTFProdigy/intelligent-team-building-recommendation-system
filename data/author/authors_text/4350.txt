c? 2003 Association for Computational Linguistics
Graph-Based Generation of Referring
Expressions
Emiel Krahmer? Sebastiaan van Erk?
Tilburg University Eindhoven University of Technology
Andre? Verleg?
Eindhoven University of Technology
This article describes a new approach to the generation of referring expressions. We propose to
formalize a scene (consisting of a set of objects with various properties and relations) as a labeled
directed graph and describe content selection (which properties to include in a referring expression)
as a subgraph construction problem. Cost functions are used to guide the search process and to
give preference to some solutions over others. The current approach has four main advantages:
(1) Graph structures have been studied extensively, and by moving to a graph perspective we
get direct access to the many theories and algorithms for dealing with graphs; (2) many existing
generation algorithms can be reformulated in terms of graphs, and this enhances comparison and
integration of the various approaches; (3) the graph perspective allows us to solve a number of
problems that have plagued earlier algorithms for the generation of referring expressions; and
(4) the combined use of graphs and cost functions paves the way for an integration of rule-based
generation techniques with more recent stochastic approaches.
1. Introduction
The generation of referring expressions is one of the most common tasks in natural
language generation and has been addressed by many researchers in the past two
decades, including Appelt (1985), Reiter (1990), Dale and Haddock (1991), Dale (1992),
Dale and Reiter (1995), Horacek (1997), Stone and Webber (1998), Krahmer and Theune
(1998, 2002), Bateman (1999), and van Deemter (2000, 2002). In this article, we present
a general, graph-theoretic approach to the generation of referring expressions. We pro-
pose to formalize a scene (i.e., a domain of objects and their properties and relations)
as a labeled directed graph and describe the content selection problem (which proper-
ties and relations to include in a description for an object?) as a subgraph construction
problem. The graph perspective has four main advantages. (1) There are many at-
tractive and well-understood algorithms for dealing with graph structures (see, e.g.,
Gibbons [1985], Cormen, Leiserson, and Rivest [1990], or Chartrand and Oellermann
[1993]). In this article, we describe a straightforward branch and bound algorithm for
finding the relevant subgraphs in which cost functions are used to guide the search
process. (2) By defining different cost functions for the graph perspective, we can simu-
late (and improve) some of the well-known algorithms for the generation of referring
? Communication and Cognition/Computational Linguistics, Faculty of Arts, Tilburg University, Tilburg,
The Netherlands. E-mail: E.J.Krahmer@uvt.nl.
? Tijgerstraat 2, NL-5645 CK, Eindhoven, The Netherlands. E-mail: sebster@sebster.com.
? Ranonkelstraat 67, NL-5644 LB, Eindhoven, The Netherlands. E-mail: andre@astygian.nl.
54
Computational Linguistics Volume 29, Number 1
expressions mentioned above. This facilitates the formal comparison of these algo-
rithms and makes it easier to transfer results from one algorithm to another. (3) The
graph perspective provides a clean solution for some problems that have plagued ear-
lier algorithms. For instance, the generation of relational expressions (i.e., referring
expressions that include references to other objects) is enhanced by the fact that both
properties and relations are formalized in the same way, namely, as edges in a graph.
(4) The combined use of graphs and cost functions paves the way for a natural inte-
gration of traditional rule-based approaches to generating referring expressions and
more recent statistical approaches, such as Langkilde and Knight (1998) and Malouf
(2000), in a single algorithm.
The outline of this article is as follows. In Section 2 the content selection problem
for generating referring expressions is explained, and some well-known solutions to
the problem are discussed. In Section 3, we describe how scenes can be modeled
as labeled directed graphs and show how content selection can be formalized as a
subgraph construction problem. Section 4 contains a sketch of the basic generation
algorithm, which is illustrated with a worked example. In Section 5 various ways to
formalize cost functions are discussed and compared. We end with some concluding
remarks and a discussion of future research directions in Section 6.
2. Generating Referring Expressions
There are many different algorithms for the generation of referring expressions, each
with its own objectives: Some aim at producing the shortest possible description (e.g.,
Dale?s [1992] full brevity algorithm), others focus on psychological realism (e.g., Dale
and Reiter?s [1995] incremental algorithm) or realistic output (e.g., Horacek 1997). The
degree of detail in which the various algorithms are described differs considerably.
Some algorithms are fully formalized and come with explicit characterizations of their
complexity (e.g., Dale and Reiter 1995; van Deemter 2000); others are more conceptual
and concentrate on exploring new directions (e.g., Stone and Webber 1998). Despite
such differences, most algorithms deal with the same problem definition. They take
as input a single object v (the target object) for which a referring expression is to be
generated and a set of objects (the distractors) from which the target object needs to
be distinguished (we use the terminology from Dale and Reiter [1995]). The task of
the algorithm is to determine which set of properties is needed to single out the target
object v from the distractors. This is known as the content determination problem for
referring expressions. On the basis of this set of properties a distinguishing descrip-
tion for v can be generated. Most algorithms do not address the surface realization
problem (how the selected properties should be realized in natural language) in much
detail; it is usually assumed that once the content for a referring expression has been
determined, a standard realizer such as KPML (Bateman 1997) or SURGE (Elhaded and
Robin 1997) can convert the meaning representation to natural language.
Consider the example scene in Figure 1. In this scene, as in any other scene, we
see a finite domain of entities D with properties P. In this particular scene, D =
{d1, d2, d3, d4} is the set of entities and P = { dog, cat, brown, black+white, large, small }
is the set of properties. A scene is usually represented as a database (or knowledge
base) listing the properties of each element in D. Thus:
d1: dog (d1) small (d1) brown (d1)
d2: dog (d2) large (d2) brown (d2)
d3: dog (d3) large (d3) black+white (d3)
d4: cat (d4) small (d4) brown (d4)
55
Krahmer, van Erk, and Verleg Graph-Based Generation
d1 d2 d3 d4
Figure 1
A simple example scene consisting of some domestic animals.
In what is probably the key reference on the topic, Dale and Reiter (1995) describe
and discuss a number of algorithms for the generation of referring expressions. One
of these is the full brevity algorithm (originally due to Dale 1992). This algorithm first
tries to generate a distinguishing description for the target object v using one single
property. If this fails, it considers all possible combinations of two properties to see if
any of these suffices for the generation of a distinguishing description, and so on. It
is readily seen that this algorithm will output the shortest possible description, if one
exists. Suppose the full brevity algorithm is used to generate a description for d1 in
Figure 1. There is no single property that distinguishes the target object d1 from the
distractors {d2, d3, d4}. But when considering all pairs of properties the algorithm will
find that one such pair rules out all distractors, namely, small and dog; ?the small dog?
is a successful and minimal distinguishing description for d1.
Dale and Reiter point out that the full brevity algorithm is both computationally
infeasible (NP hard) and psychologically unrealistic. They offer the incremental algo-
rithm as an alternative. The incremental algorithm considers properties for selection
in a predetermined order, based on the idea that human speakers and listeners prefer
certain kinds of properties (or attributes) when describing objects from a given do-
main. For instance, when discussing domestic animals, it seems likely that a human
speaker would first describe an animal by its type (is it a dog? is it a cat?). If that
does not suffice, first absolute attributes like color are tried, followed by relative ones
such as size. In sum: The list of preferred attributes for our example domain would
be ? type, color, size ?. Essentially, the incremental algorithm iterates through this list,
and for each property it encounters, it determines whether adding this property to
the properties selected so far would rule out any of the remaining distractors. If so,
it is included in the list of selected properties. There is one exception to this general
strategy: Type information is always included, even if it rules out no distractors. The
algorithm stops when all distractors are ruled out (success) or when the end of the
list of preferred attributes is reached (failure).
Suppose we apply the incremental algorithm to d1 from Figure 1 with ? type, color,
size ? as preferred attributes. The type of d1 listed in the database is dog. This property
is selected (since type information is always selected). It rules out d4 (which is a cat).
Next we consider the color of d1; the animal is brown. This property rules out d3 (which
is a black and white dog) and is selected. Finally, we consider the size of our target ob-
ject, which is small. This properly rules out the remaining distractor d2 (which is a large
brown dog) and hence is included as well. At this point, all distractors are ruled out
(success!), and the set of selected properties is {dog, brown, small}, which a linguistic
realizer might express as ?the small brown dog.? This is a successful distinguishing
56
Computational Linguistics Volume 29, Number 1

d
1
d
3
Z
Z
Z
Squibs and Discussions
Real versus Template-Based Natural Language
Generation: A False Opposition?
Kees van Deemter
University of Aberdeen
Emiel Krahmer.
Tilburg University
Marie?t Theune-
University of Twente
This article challenges the received wisdom that template-based approaches to the generation of
language are necessarily inferior to other approaches as regards their maintainability, linguistic
well-foundedness, and quality of output. Some recent NLG systems that call themselves
??template-based?? will illustrate our claims.
1. Introduction
Natural language generation (NLG) systems are sometimes partitioned into application-
dependent systems which lack a proper theoretical foundation, on the one hand, and
theoretically well-founded systems which embody generic linguistic insights, on the
other. Template-based systems are often regarded as automatically falling into the first
category. We argue against this view. First, we describe the received view of both
template-based and ??standard?? NLG systems (section 2). Then we describe a class of
recent template-based systems (section 3) that will serve as a basis for a comparison
between template-based and other NLG systems with respect to their potential for
performing NLG tasks (section 4). We ask what the real difference between template-
based and other systems is and argue that the distinction between the two is becoming
increasingly blurred (section 5). Finally, we discuss the implications of engineering
shortcuts (Mellish 2000) and corpus-based methods (section 6).
2. Templates versus Real NLG: The Received View
Before we can argue against the distinction between template-based and ??real?? NLG
systems, we should first sketch how these two classes are commonly understood. It is
surprisingly difficult to give a precise characterization of the difference between them
(and we will later argue against the usefulness of such a characterization), but the idea
is the following. Template-based systems are natural-language-generating systems
that map their nonlinguistic input directly (i.e., without intermediate representations)
to the linguistic surface structure (cf. Reiter and Dale 1997, pages 83?84). Crucially, this
linguistic structure may contain gaps; well-formed output results when the gaps are
* 2005 Association for Computational Linguistics
 Computing Science Department, King?s College, University of Aberdeen, United Kingdom.
E-mail: KvDeemter@csd.abdn.ac.uk.
. Communication and Cognition/Computational Linguistics, Faculty of Arts, Tilburg University,
Tilburg, The Netherlands. E-mail: E.J.Krahmer@uvt.nl.
- Human Media Interaction Group, Computer Science, University of Twente, The Netherlands.
E-mail: M.Theune@ewi.utwente.nl.
filled or, more precisely, when all the gaps have been replaced by linguistic structures
that do not contain gaps. (Canned text is the borderline case of a template without
gaps.) Adapting an example from Reiter and Dale (1997), a simple template-based
system might start out from a semantic representation saying that the 306 train leaves
Aberdeen at 10:00 AM:
Departure?train306; locationabdn; time1000?
and associate it directly with a template such as
?train is leaving ?townnow
where the gaps represented by [train] and [town] are filled by looking up the relevant
information in a table. Note that this template will be used only when the time referred
to is close to the intended time of speaking; other templates must be used for
generating departure announcements relating to the past or future. ??Real?? or, as we
shall say, standard NLG systems, by contrast, use a less direct mapping between input
and surface form (Reiter 1995; Reiter and Dale 1997). Such systems could start from
the same input semantic representation, subjecting it to a number of consecutive
transformations until a surface structure results. Various NLG submodules would
operate on it (determining, for instance, that 10:00 AM is essentially the intended time
of speaking), jointly transforming the representation into an intermediate representa-
tion like
Leavepresent ?traindemonstrative; Aberdeen; now?
where lexical items and style of reference have been determined while linguistic
morphology is still absent. This intermediate representation may in turn be transformed
into a proper sentence, for example: This train is leaving Aberdeen now. Details vary; in
particular, many systems will contain more intermediate representations.
Template-based and standard NLG systems are said to be ??Turing equivalent??
(Reiter and Dale 1997); that is, each of them can generate all recursively enumerable
languages. However, template-based systems have been claimed to be inferior with
respect to maintainability, output quality and variation, and well-foundedness. Reiter
and Dale (1997) state that template-based systems are more difficult to maintain and
update (page 61) and that they produce poorer and less varied output (pages 60, 84)
than standard NLG systems. Busemann and Horacek (1998) go even further by
suggesting that template-based systems do not embody generic linguistic insights
(page 238). Consistent with this view, template-based systems are sometimes over-
looked. In fact, the only current textbook on NLG (Reiter and Dale 2000) does not
pay any attention to template-based generation, except for a passing mention of the
ECRAN system (Geldof and van de Velde 1997). Another example is a recent overview
of NLG systems in the RAGS project (Cahill et al 1999). The selection criteria employed
by the authors were that the systems had to be fully implemented, complete (i.e.,
generating text from nontextual input), and accepting non-hand-crafted input; al-
though these criteria appear to favor template based systems, none of the 19 systems
investigated were template-based. In what follows, we claim that the two types of
systems have more in common than is generally thought and that it is counter-
productive to treat them as distant cousins instead of close siblings. In fact, we argue
that there is no crisp distinction between the two.
16
Computational Linguistics Volume 31, Number 1
17
3. Template-Based NLG Systems in Practice
In recent years, a number of new template-based systems have seen the light,
including TG/2 (Busemann and Horacek 1998), D2S (van Deemter and Odijk 1997;
Theune et al 2001), EXEMPLARS (White and Caldwell 1998), YAG (McRoy, Channarukul,
and Ali 2003), and XTRAGEN (Stenzhorn 2002). Each of these systems represents a
substantial research effort, achieving generative capabilities beyond what is usually
expected from template-based systems, yet they call themselves template-based,
and they clearly fall within the characterization of template-based systems offered
above.
In this article we draw on our own experiences with a data-to-speech method
called D2S. D2S has been used as the foundation of a number of language-generating
systems, including GOALGETTER, a system that generates soccer reports in Dutch.1 D2S
consists of two modules: (1) a language generation module (LGM) and (2) a speech
generation module (SGM) which turns the generated text into a speech signal. Here
we focus on the LGM and in particular on its use of syntactically structured templates
to convert a typed data structure into a natural language text (annotated with prosodic
information). Data structures in GOALGETTER are simple representations describing
lists of facts, such as

goal-event
TEAM Ajax
PLAYER Kluivert
MINUTE 38
GOAL-TYPE penalty

Besides goal events, there are several other types of events, such as players receiving
yellow or red cards. Figure 1 shows a simple template, which the LGM might use to
express the above fact as, for instance, Kluivert scored a penalty in the 38th minute.
1 See http://www.cs.utwente.nl/?theune/GG/GG_index.html for some example reports.
Figure 1
Sample syntactic template from the GOALGETTER system.
van Deemter, Krahmer, and Theune Real versus Template-Based NLG
Formally, a syntactic template s = bS, E, C, T?, where S is a syntax tree (typically for
a sentence) with open slots in it, E is a set of links to additional syntactic structures
(typically NPs and PPs) which may be substituted in the gaps of S, C is a condition on
the applicability of s, and T is a set of topics. We discuss the four components of a
template in more detail, starting with the syntax tree, S. All S?s interior nodes are
labeled by nonterminal symbols, while the nodes on the frontier are labeled by
terminal or nonterminal symbols: the nonterminal nodes (??gaps??) are open for
substitution and they are marked by a ,. The second element of a syntactic template is
E: the slot fillers. Each open slot in the tree S is associated with a call of some Express
function (ExpressTime, ExpressObject, etc.), which generates a set of expressions that
can be used to fill the slot. The right-hand side of Figure 2 shows an example Express
function, namely, ExpressObject, which generates a set of NP trees and is used to
generate fillers for the player and goal slots in the template of Figure 1. The first, for
example, leads to the generation of NPs such as Kluivert (proper name), the forward
Kluivert, Ajax player Kluivert, Ajax? Kluivert, the striker, and he, depending on the context
in which the NP is generated.
The left-hand side of Figure 2 shows the function ApplyTemplate, which handles
the choice among all possible combinations of slot fillers. ApplyTemplate first calls
FillSlots to obtain the set of all possible trees (all_trees) that can be generated from the
template, using all possible combinations of slot fillers generated by the Express
functions associated with the slots. For each tree in this set, it is checked (1) whether it
does not violate a version of the Chomskyan binding theory and (2) whether it is
compatible with the context model, which is a record containing all the objects
introduced so far and the anaphoric relations among them. From the resulting set of
allowed_trees, one is selected randomly (using the function PickAny) and returned to
the main generation algorithm. The random-choice option was chosen to maximize the
variety of sentences produced by the system.
The mechanisms described so far take care of sentence planning and language
realization. Text planning is performed by components C and T. C is a Boolean
condition. A template s is applicable only if its associated condition is true. An
example is the condition from Figure 1 saying that the template can be used only if
the result of the current match has been conveyed to the user (i.e., is known) and
the current goal is the first one which has not been conveyed (i.e., is not known). To
cater to aspects of text planning that allow a less knowledge-intensive approach,
GOALGETTER associates every template with a set of topics T, which the LGM algo-
rithm uses to group sentences together into coherent chunks of text. For example, any
18
Figure 2
Functions ApplyTemplate (left) and ExpressObject (right).
Computational Linguistics Volume 31, Number 1
19
template associated with the topic of goal scoring can ??fire?? throughout the opening
paragraph of the report.
4. Template-Based NLG: Deep or Shallow?
How do template-based systems measure up against the criteria mentioned in
section 2? When dealing with this question, we are interested as much in what could
be done in principle as in what has been achieved in practice. After some preliminary
remarks, we focus on the criterion of linguistic well-foundedness.
It is far from obvious that template-based systems should always score low on
maintainability. Several template-based systems such as TG/2, EXEMPLARS, and
XTRAGEN have been reused for generation in different languages or in different
domains (cf. Kittredge et al 1994). In the case of D2S, the basic generation algorithm
and such functions as ApplyTemplate and ExpressObject have been used for different
application domains (music, soccer games, route descriptions, and public transport)
and different languages (English, Dutch, and German); D2S has been used for the
generation of both monologues and dialogue contributions (van Deemter and Odijk
1997; Theune et al 2001). When a template-based system is applied to a new domain or
language, many of the templates will have to be written anew (much as new grammar
fragments need to be developed for standard NLG systems), but the underlying
generation mechanisms generally require little or no modification.
As for the output quality and variability of the output, if template-based systems
have the same generative power as standard NLG systems (Reiter and Dale 1997),
there cannot be a difference between the types of output that they are able to generate
in principle. The fact that templates can be specified by hand gives template-based
systems an advantage in cases in which good linguistic rules are not (yet) available or
for constructions which have unpredictable meanings or highly specific conditions of
use. Some template-based systems have variability as one of their central design
specifications: Current D2S-based systems rely mainly on random choice to achieve
variation, but more context-sensitive variations (e.g., varying the output depending on
user characteristics) can also be achieved through the use of parametrized templates
(XTRAGEN) or template specialization hierarchies (EXEMPLARS).
The most crucial question, in our view, is whether a template-based NLG system
can be linguistically well-founded (or ??deep?? in terms of Busemann and Horacek
[1998]), in the sense that the choices inherent in its mapping from input to output are
based on sound linguistic principles. To judge the well-foundedness of template-based
systems, let us look at the different types of decisions that an NLG system needs to
make, as distinguished by Cahill et al (1999) and Reiter and Dale (2000).
4.1 Content Determination
During content determination, it is decided what information is to be conveyed. Since
content determination precedes language generation proper, it is clear that in principle,
template-based systems can treat it in the exact same ways as standard NLG systems.
In practice, template-based systems tend to take their departure from ??flat data?? (e.g.,
database records), whereas standard systems often use richer input, in which some
decisions concerning the linguistic structure of the output (e.g., decisions about
quantificational or rhetorical structure) have already been made. To the extent that this
is the case, the ??generation gap?? to be bridged by template-based systems is actually
wider than the one to be bridged by standard NLG systems.
van Deemter, Krahmer, and Theune Real versus Template-Based NLG
4.2 Referring Expressions
As for the generation of referring expressions, template-based systems vary widely:
The simplest of them (e.g., MSWord-based systems for mail merge) can fill their gaps
with only a limited number of phrases, but more sophisticated systems (called
??hybrid?? systems in Reiter [1995]) have long existed; these effectively use standard
NLG to fill their gaps. Recent systems have moved further in this direction. D2S, for
example, uses well-established rules for constraining the use of anaphors (see, e.g., the
Chomskyan ViolateBindingTheory and Wellformed in ApplyTemplate) and a new
variant of Dale and Reiter?s (1995) algorithm for the generation of referring expressions
that takes contextual salience into account (MakeReferringExp in ExpressObject)
(Krahmer and Theune 2002). A similar range of approaches can be found among NLG
systems that are not template-based; in fact, several systems from the RAGS inventory
do not really address referring expression generation at all (Cahill et al 1999).
4.3 Aggregation
Aggregation is an NLG task in which differences between the two types of systems
may be expected. After all, every template contains a ??fixed?? part, and surely this part
cannot be recombined with other parts? The reality is slightly more complex. The
GOALGETTER system, for instance, uses the following approach: In order to generate a
subject-aggregated sentence of the form A and B got a red card, a separate template is
called of the form X got a red card [syntactic structure omitted], subject to conditions
requiring that the gap X be filled with an appropriate conjoined noun phrase, referring
to the set {A, B}. Other approaches are possible. For example, the system could first
generate A got a red card and B got a red card, then aggregate these two structures (whose
syntactic and semantic structure is known) into the desired conjunctive structure (van
Deemter and Odijk 1997). Whether a system is able to perform operations of this kind
does not depend on whether the system is template based, but on whether it possesses
the required syntactic and semantic information.
4.4 Lexicalization
The same point is relevant for lexicalization. Let us suppose (perhaps rather charitably;
Cahill et al 1999) that a variety of near-synonymous verbs are present in the lexicon of
the NLG system (e.g., give, offer, donate, entrust, present to). How would a standard NLG
system choose among them? Typically, the system does not have a clue, because our
understanding of the differences among these verbs is too imperfect. (The input to the
system might prejudge such decisions by pairing each of these verbs with different
input relations, but that would be cheating.) As with the previous tasks, it is not clear
that standard NLG systems are in a better position to perform them than template-
based ones: The latter could use templates that vary in the choice of words and
stipulate that they are applicable under slightly different conditions (cf. the use of
specialization hierarchies in EXEMPLARS). The condition C for X kicked the ball in the net,
for example (as opposed to X scored or X nudged the ball in) might require that the ball
did not touch the ground after departing the previous player.
4.5 Linguistic Realization
It is in linguistic realization that the most obvious differences between standard and
template-based approaches appear to exist. Many template-based approaches lack a
general mechanism for gender, number, and person agreement, for example. Systems
in the D2S tradition avoid errors by letting functions like ExpressObject use handmade
rules, but this approach becomes cumbersome when coverage increases; general-
20
Computational Linguistics Volume 31, Number 1
21
izations are likely to be missed and portability is reduced, for example, if different
templates are used for John walks and John and Mary walk. One should not, however, let
one?s judgment depend on accidental properties of one or two systems: Nothing keeps
the designer of a template-based system from adding morphological rules; witness
systems like YAG (McRoy, Channarukul, and Ali 2003) and XTRAGEN (Stenzhorn 2002).
The YAG system, for example, allows the subject and verb of a template to be
underspecified for number and person, while using attribute grammar rules to
complete the specification: Returning to the example above, the number attribute of
John and Mary is inferred to be plural (unlike, e.g., John and I); a subject-verb
agreement rule makes the further inference that the verb must be realized as walk,
rather than walks.
5. Templates: An Updated View
A new generation of systems that call themselves template-based have blurred the line
between template-based and standard NLG. This is not only because some systems
combine standard NLG with templates and canned text (Piwek 2003), but also because
modern template-based systems tend to use syntactically structured templates and
allow the gaps in them to be filled recursively (i.e., by filling a gap, a new gap may
result). Some ??template-based?? systems, finally, use grammars to aid linguistic
realization. These developments call into question the very definition of ??template
based?? (section 2), since the systems that call themselves template-based have come to
express their nonlinguistic input with varying degrees of directness.
??Template-based?? systems vary in terms of linguistic coverage, the amount of
syntactic knowledge used, and the number of steps involved in filling the templates,
among other things. Here, we highlight one particular dimension, namely, the size of
(the fixed part of) the templates. A comparison with tree-adjoining grammar (TAG)?
based-approaches to NLG may be useful (Joshi 1987; see also Becker 2002). Joshi (1987,
page 234) points out that ??The initial . . . trees are not constrained in any other manner
than. . . . The idea, however, is that [they] will be minimal in some sense.?? Minimality is
usually interpreted as saying that a tree should not contain more than the lexical head
plus its arguments. Initial trees may be likened to templates. Nonminimal templates/
elementary trees are essential for the treatment of idioms and special collocations.
Generally speaking, however, the larger the templates/elementary trees, the less sys-
tematic the treatment, the less insight it gives into the compositional structure of lan-
guage, and the larger the number of templates/elementary trees needed. Again, the
history of D2S is instructive: The earliest D2S-based NLG system (DYD; van Deemter and
Odijk 1997) used long templates, but the majority of the templates in GOALGETTER are
minimal in the sense explicated above (Theune et al 2001).
6. Discussion: Shortcuts and Statistics in NLG
Let us compare our views with those of Mellish (2000). Mellish points out that NLG
systems often use shortcuts, whereby one or more modules are trivialized, either by
bypassing them (and the representations that they create) or by letting their operations
be dictated by what the other modules expect (e.g., lexical choice may be trivialized
by using a one-to-one mapping between semantic relations/predicates and lexical
items). Mellish argues that shortcuts have a legitimate role in practical NLG when
linguistic rules are missing, provided the existence of the shortcuts is acknowledged:
Even though they lead to diminished generality and maintainability, the unavailability
van Deemter, Krahmer, and Theune Real versus Template-Based NLG
of ??deep?? rules means that there is no alternative (yet). For instance, there is little
added value in using abstract representations from which either a passive or an active
sentence can be generated if we are unable to state a general rule that governs the
choice, in which case one can be forgiven for explicitly specifying which sentences
should be active and which ones passive, avoiding a pretense of linguistic sophis-
tication. It is shortcuts of this kind that a template-based system is well placed to make,
of course. But crucially, template-based systems do not have to use shortcuts any more
than standard NLG systems: Where linguistic rules are available, both types of sys-
tems can use them, as we have seen.
Another response to the absence of linguistic rules is the use of statistical
information derived from corpora, as is increasingly more common in realization, but
also for instance in aggregation (e.g., Walker, Rambow, and Rogati 2002). The point we
want to make here, however, is that ??template-based?? systems may profit from such
corpus-based approaches just as much as ??standard?? NLG systems. The approach of
Langkilde and Knight (1998), for example, in which corpus-derived n-grams are used
for selecting the best ones from among a set of candidates produced by overgenera-
tion, can also be applied to template-based systems (witness the mixed template/
stochastic system of Galley, Fosler-Lussier, and Potamianos [2001]).
We have argued that systems that call themselves template based can, in principle,
perform all NLG tasks in a linguistically well-founded way and that more and more
actually implemented systems of this kind deviate dramatically from the stereotypical
systems that are often associated with the term template. Conversely, most standard
NLG systems perform many NLG tasks in a less than well-founded fashion (e.g.,
relying heavily on shortcuts, and nontransparent ones at that). We doubt that there is
still any important difference between the two classes of systems, since the variation
within each of them is as great as that between them.
22
Acknowledgments
This is a remote descendant of a paper
presented at the workshop ??May I Speak
Freely??? (Becker and Busemann 1999). We
thank three reviewers for comments.
References
Becker, Tilman. 2002. Practical,
template-based natural language
generation with TAG. In Proceedings
of TAG+6, Venice.
Becker, Tilman and Stephan Busemann,
editors. 1999. ??May I Speak Freely??? Between
Templates and Free Choice in Natural
Language Generation: KI-99 Workshop.
DFKI, Saarbru?cken, Germany.
Busemann, Stephan and Helmut
Horacek. 1998. A flexible shallow
approach to text generation. In Proceedings
of the Ninth International Workshop on
Natural Language Generation,
pages 238?247: Niagara-on-the-Lake,
Ontario, Canada.
Cahill, Lynn, Christy Doran, Roger Evans,
Chris Mellish, Daniel Paiva, Mike Reape,
and Donia Scott. 1999. In search of a
reference architecture for NLG systems.
In Proceedings of the Seventh European
Workshop on Natural Language Generation,
pages 77?85: Toulouse, France.
Dale, Robert and Ehud Reiter. 1995.
Computational interpretations of the
Gricean maxims in the generation of
referring expressions. Cognitive Science,
18:233?263.
Galley, Michel, Eric Fosler-Lussier, and
Alexandros Potamianos. 2001. Hybrid
natural language generation for spoken
dialogue systems. In Proceedings of
the Seventh European Conference on
Speech Communication and Technology.
Aalborg, Denmark.
Geldof, Sabine and Walter van de Velde.
1997. An architecture for template based
(hyper)text generation. In Proceedings of
the Sixth European Workshop on Natural
Language Generation, pages 28?37,
Duisburg, Germany.
Joshi, Aravind. 1987. The relevance of
tree adjoining grammar to generation.
In Gerard Kempen, editor. Natural Language
Computational Linguistics Volume 31, Number 1
23
Generation, Martinus Nijhoff, Leiden,
The Netherlands, pages 233?252.
Kittredge, Richard, Eli Goldberg, Myunghee
Kim, and Alain Polgue`re. 1994.
Sublanguage engineering in the FOG
system. In Fourth Conference on Applied
Natural Language Processing, pages 215?216,
Stuttgart, Germany.
Krahmer, Emiel and Marie?t Theune. 2002.
Efficient context-sensitive generation of
descriptions in context. In Kees van
Deemter and Rodger Kibble, editors,
Information Sharing. CSLI Publications,
Stanford, CA, pages 223?264.
Langkilde, Irene and Kevin Knight. 1998.
Generation that exploits corpus-based
statistical knowledge. In Proceedings of
the ACL, pages 704?710, Montreal,
Quebec, Canada.
McRoy, Susan W., Songsak Channarukul,
and Syed S. Ali. 2003. An augmented
template-based approach to text
realization. Natural Language Engineering,
9(4):381?420.
Mellish, Chris. 2000. Understanding shortcuts
in NLG systems. In Proceedings of Impacts in
Natural Language Generation: NLG between
Technology and Applications, pages 43?50,
Dagstuhl, Germany.
Piwek, Paul. 2003. A flexible
pragmatics-driven language generator for
animated agents. In Proceedings of EACL03
(Research Notes), pages 151?154,
Budapest, Hungary.
Reiter, Ehud. 1995. NLG vs. templates. In
Proceedings of the Fifth European Workshop on
Natural Language Generation, pages 95?105,
Leiden, The Netherlands.
Reiter, Ehud and Robert Dale. 1997. Building
applied natural language generation
systems. Natural Language Engineering,
3(1):57?87.
Reiter, Ehud and Robert Dale. 2000.
Building Natural Language Generation
Systems. Cambridge University
Press, Cambridge.
Stenzhorn, Holger. 2002. A natural language
generation system using XML- and
Java-technologies. In Proceedings of the
Second Workshop on NLP and XML,
Taipei, Taiwan.
Theune, Marie?t, Esther Klabbers, Jan-Roelof
de Pijper, Emiel Krahmer, and Jan Odijk.
2001. From data to speech: A general
approach. Natural Language Engineering,
7(1):47?86.
van Deemter, Kees and Jan Odijk. 1997.
Context modelling and the generation of
spoken discourse. Speech Communication,
21(1/2):101?121.
Walker, Marilyn, Owen Rambow, and
Monica Rogati. 2002. Training a sentence
planner for spoken dialogue using
boosting. Computer Speech and Language,
16:409?433.
White, Michael and Ted Caldwell. 1998.
EXEMPLARS: A practical, extensible
framework for dynamic text generation.
In Proceedings of the Ninth International
Workshop on Natural Language Generation,
pages 266?275, Niagara-on-the-Lake,
Ontario, Canada.
van Deemter, Krahmer, and Theune Real versus Template-Based NLG

Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 193?196,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Query-based sentence fusion is better defined and leads to
more preferred results than generic sentence fusion?
Emiel Krahmer
Tilburg University
Tilburg, The Netherlands
E.J.Krahmer@uvt.nl
Erwin Marsi
Tilburg University
Tilburg, The Netherlands
E.C.Marsi@uvt.nl
Paul van Pelt
Tilburg University
Tilburg, The Netherlands
paul.vanpelt@gmail.com
Abstract
We show that question-based sentence fu-
sion is a better defined task than generic sen-
tence fusion (Q-based fusions are shorter, dis-
play less variety in length, yield more identi-
cal results and have higher normalized Rouge
scores). Moreover, we show that in a QA set-
ting, participants strongly prefer Q-based fu-
sions over generic ones, and have a preference
for union over intersection fusions.
1 Introduction
Sentence fusion is a text-to-text generation applica-
tion, which given two related sentences, outputs a
single sentence expressing the information shared
by the two input sentences (Barzilay and McKeown
2005). Consider, for example, the following pair of
sentences:1
(1) Posttraumatic stress disorder (PTSD) is a
psychological disorder which is classified as
an anxiety disorder in the DSM-IV.
(2) Posttraumatic stress disorder (abbrev.
PTSD) is a psychological disorder caused by
a mental trauma (also called psychotrauma)
that can develop after exposure to a terrifying
event.
?Thanks are due to Ed Hovy for discussions on the Rouge
metrics and to Carel van Wijk for statistical advice. The data-
set described in this paper (2200 fusions of pairs of sentences)
is available upon request. This research was carried out within
the Deaso project (http://daeso.uvt.nl/).
1All examples are English translations of Dutch originals.
Fusing these two sentences with the strategy de-
scribed by Barzilay and McKeown (based on align-
ing and fusing the respective dependency trees)
would result in a sentence like (3).
(3) Posttraumatic stress disorder (PTSD) is a
psychological disorder.
Barzilay and McKeown (2005) argue convincingly
that employing such a fusion strategy in a multi-
document summarization system can result in more
informative and more coherent summaries.
It should be noted, however, that there are multi-
ple ways to fuse two sentences. Besides fusing the
shared information present in both sentences, we can
conceivably also fuse them such that all information
present in either of the sentences is kept, without any
redundancies. Marsi and Krahmer (2005) refer to
this latter strategy as union fusion (as opposed to
intersection fusion, as in (3)). A possible union fu-
sion of (1) and (2) would be:
(4) Posttraumatic stress disorder (PTSD) is a
psychological disorder, which is classified
as an anxiety disorder in the DSM-IV,
caused by a mental trauma (also called psy-
chotrauma) that can develop after exposure
to a terrifying event.
Marsi and Krahmer (2005) propose an algorithm
which is capable of producing both fusion types.
Which type is more useful is likely to depend on
the kind of application and information needs of the
user, but this is essentially still an open question.
193
However, there is a complication. Daume? III &
Marcu (2004) argue that generic sentence fusion is
an ill-defined task. They describe experimental data
showing that when participants are given two con-
secutive sentences from a single document and are
asked to fuse them (in the intersection sense), differ-
ent participants produce very different fusions. Nat-
urally, if human participants cannot reliably perform
fusions, evaluating automatic fusion strategies is al-
ways going to be a shaky business. The question
is why different participants come to different fu-
sions. One possibility, which we explore in this pa-
per, is that it is the generic nature of the fusion which
causes problems. In particular, we hypothesize that
fusing two sentences in the context of a preceding
question (the natural setting in QA applications) re-
sults in more agreement among humans. A related
question is of course what the results would be for
union fusion. Will people agree more on the unions
than on the intersections? And is the effect of a pre-
ceding question the same for both kinds of fusion?
In Experiment I, below, we address these questions,
by collecting and comparing four different fusions
for various pairs of related sentences, both generic
and question-based ones, and both intersection and
union ones.
While it seems a reasonable hypothesis that
question-based fusions will lead to more agreement
among humans, the really interesting question is
which fusion strategy (if any) is most appreciated
by users in a task-based evaluation. Given that Ex-
periment I gives us four different fusions per pair of
sentence, an interesting follow-up question is which
leads to the best answers in a QA setting. Do par-
ticipants prefer concise (intersection) or complete
(union) answers? And does it matter whether the
fusion was question-based or not? In Experiment
II, we address these questions via an evaluation
experiment using a (simulated) medical question-
answering system, in which participants have to rank
four answers (resulting from generic and question-
based intersection and union fusions) for different
medical questions.
2 Experiment I: Data-collection
Method To collect pairs of related sentences to be
fused under different conditions, we proceeded as
Fusion type Length M (SD) # Id.
Generic Intersection 15.6 (2.9) 73
Q-Based Intersection 8.1 (2.5) 189
Generic Union 31.2 (7.8) 109
Q-Based Union 19.2 (4.7) 134
Table 1: Mean sentence length (plus Standard Deviation)
and number of identical fusion results as a function of
fusion type (n = 550 for each type).
follows. As our starting point we used a set of
100 medical questions compiled as a benchmark for
evaluating medical QA systems, where all correct
answers were manually retrieved from the available
text material. Based on this set, we randomly se-
lected 25 questions for which more than one answer
could be found (otherwise there would be nothing
to fuse), and where the first two answer sentences
shared at least some information (otherwise inter-
section fusion would be impossible).
Participants were 44 native speakers of Dutch (20
women) with an average age of 30.1 years, none
with a background in sentence fusion research. Ex-
periment I has a mixed between-within subjects de-
sign. Participants were randomly assigned to either
the intersection or the union condition, and within
each condition they first had to produce 25 generic
and then 25 question-based fusions. In the latter
case, participants were given the original question
used to retrieve the sentences to be fused.
The experiment was run using a web-based
script. Participants were told that the purpose of the
experiment was merely to gather data, they were not
informed about our interest in generic vs question
based fusion. Before participants could start with
their task, the concept of sentence fusion (either
fusion or intersection, depending on the condition)
was explained, using a number of worked examples.
After this, the actual experiment started.
Results First consider the descriptive statistics in Ta-
ble 1. Naturally, intersection fusion leads to shorter
sentences on average than union fusion. More in-
terestingly, question (Q)-based fusions lead to sig-
nificantly shorter sentences than their generic coun-
terparts (intersection t = 9.1, p < .001, union:
t = 6.1, p < .001, two-tailed). Also note that
194
Generic Q-Based Generic Q-Based
Intersection Intersection Union Union
Rouge-1 .036 .068 .035 .041
Rouge-SU4 .014 .038 .018 .020
Rouge-SU9 .014 .040 .016 .020
Table 2: Average Rouge-1, Rouge-SU4 and Rouge-SU9 (normalized for sentence length) as a function of fusion type.
the variation among participants decreases in the Q-
based conditions (lower standard deviations). This
suggests that participants in the Q-based conditions
indeed show less variety in their fusions than partic-
ipants in the generic conditions. This is confirmed
by the number of identical (i.e., duplicated) fusions,
which is indeed higher in the Q-based conditions,
although the difference is only significant for inter-
sections (?2(1) = 51.3, p < .001).
We also computed average Rouge-1, Rouge-SU4
and Rouge-SU9 scores for each set of fusions, to
be able to quantify the overlap between participants
in the various conditions. One complication is that
these metrics are sensitive to sentence-length (longer
sentences are more likely to contain overlapping
words than shorter ones), hence in Table 2 we report
on Rouge scores that are normalized with respect
sentence length. The resulting picture is surprisingly
consistent: Q-based fusion on all three metrics re-
sults in higher normalized Rouge scores, where the
difference is generally small in the case of union,
and rather substantial for intersection.
3 Experiment II: Evaluation
The previous experiment indicates that Q-based
fusion is indeed a better-defined summarization task
than generic fusion, in this experiment we address
the question which kind of fusion participants prefer
in a QA application.
Method We selected 20 from the 25 questions
used in Experiment I, for which we made sure
that the fusions in the four categories resulted
in sentences with a sufficiently different content.
For each question, one representative sentence
was selected from the 22 fusions produced by
participants in Experiment I, for each of the four
categories (Q-based intersection, Q-based union,
Generic intersection and Generic union). This
Fusion type Mean Rank
Q-Based Union 1.888
Q-Based Intersection 2.471
Generic Intersection 2.709
Generic Union 2.932
Table 4: Mean rank from 1 (= ?best?) to 4 (=?worst?) as
a function of fusion type.
representative sentence was the most frequent result
for that particular category. When no such sentence
was present for a particular task, a random selection
was made.
Participants were 38 native speakers of Dutch (17
men), with an average age of 39.4 years. None
had participated in Experiment I and none had a
background in sentence fusion research. Participants
were confronted with the selected 20 questions, one
at a time. For each question, participants saw four
alternative answers (one from each category). Fig-
ure 3 shows one question, with four different fusions
derived by participants from example sentences (1)
and (2). Naturally, the labels for the 4 fusion strate-
gies were not part of the experiment. Participants
were asked to rank the 4 answers from ?best? (rank
1) to ?worst? (rank 4), via a forced choice paradigm
(i.e., they also had to make a choice if they felt that
two answers were roughly as good). Experiment II
had a within-subjects design, which means that all
38 participants ranked the answers for all 20 ques-
tions.
Results Table 4 gives the mean rank for the four
fusion types. To test for significance, we per-
formed a repeated measures Analysis of Variance
(ANOVA) with fusion type and question as the in-
dependent variables and average rank as the depen-
dent variable. A main effect was found of fusion
type (F (3, 111) = 20.938, p < .001, ?2 = .361).
195
What is PTSD?
Generic Intersection Posttraumatic stress disorder (PTSD) is a psychological disorder.
Q-based Intersection PTSD stands for posttraumatic stress disorder and is a psychological disorder.
Generic Union Posttraumatic stress disorder (PTSD) is a psychological disorder, which is classified as an
anxiety disorder in the DSM-IV, caused by a mental trauma (also called psychotrauma) that
can develop after exposure to a terrifying event.
Q-based Union PTSD (posttraumatic stress disorder) is a psychological disorder caused by a mental trauma
(also called psychotrauma) that can develop after exposure to a terrifying event.
Table 3: Example question from Experiment II, with four possible answers, based on different fusions strategies
(obtained in Experiment I).
Pairwise comparisons using the Bonferroni method
show that all comparisons are statistically significant
(at p < .001) except for the one between Generic In-
tersection and Generic Union. Thus, in particular:
Q-based union is ranked significantly higher than
Q-based intersection, which in turn is ranked sig-
nificantly higher than both Generic union and inter-
section (whose respective ranks are not significantly
different).
The ANOVA analysis also revealed a significant
interaction between question and type of fusion
(F (57, 2109) = 7.459, p < .001, ?2 = .168).2
What this means is that relative ranking varies for
different questions. To better understand this inter-
action, we performed a series of Friedman tests for
each question (the Friedman test is a standard non-
parametric test for ranked data). The Friedman anal-
yses revealed that the overall pattern (Q-based union
> Q-based intersection > Generic Union / Intersec-
tion) was found to be significant for 13 out of the
20 questions. For four of the remaining seven ques-
tions, Q-based union ranked first as well, while for
two questions Q-based intersection was ranked as
the best answer. For the remaining question, there
was no significant difference between the four fu-
sion types.
4 Conclusion and discussion
In this paper we have addressed two questions. First:
is Q-based fusion a better defined task than generic
fusion? Here, the answer seems to be ?yes?: Q-
based fusions are shorter, display less variety in
length, result in more identically fused sentences
2Naturally, there can be no main effect of question, since
there is no variance; the ranks 1-4 are fixed for each question.
and have higher normalized Rouge scores, where the
differences are larger for intersection than for union.
Inspection of the fused sentences reveals that there
is simply more potential variation on the word level
(do I select this word from one input sentence or
from the other?) for union fusion than for inter-
section fusion. Second: which kind of fusion (if
any) do users of a medical QA system prefer? Here
a consistent preference order was found, with rank
1 = Q-based union, rank 2 = Q-based Intersection,
rank 3/4 = Generic intersection / union. Thus: par-
ticipants clearly prefer Q-based fusions, and prefer
more complete answers over shorter ones.
In future research, we intend to collect new data
with different questions per sentence pair, to find out
to what extent the question and its phrasing drive
the fusion process. In addition, we will also let sen-
tences from different domains be fused, based on the
hypothesis that fusion strategies may differ across
domains.
References
Regina Barzilay and Kathleen McKeown. 2005. Sen-
tence Fusion for Multidocument News Summariza-
tion. Computational Linguistics, 31(3), 297-328.
Hal Daume? III and Daniel Marcu. 2004. Generic Sen-
tence Fusion is an Ill-Defined Summarization Task.
Proceedings of the ACL Text Summarization Branches
Out Workshop, Barcelona, Spain.
Chin-Yew Lin and Eduard Hovy. 2003. Automatic evalu-
ation of summaries using N-gram co-occurrence statis-
tics. Proceedings of NAACL ?03, Edmonton, Canada.
Erwin Marsi and Emiel Krahmer. 2005. Explorations
in Sentence Fusion. Proceedings of the 10th Euro-
pean Workshop on Natural Language Generation, Ab-
erdeen, UK.
196
Proceedings of the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment, pages 1?6,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Classification of semantic relations by humans and machines ?
Erwin Marsi and Emiel Krahmer
Communication and Cognition
Tilburg University, The Netherlands
{e.c.marsi, e.j.krahmer}@uvt.nl
Abstract
This paper addresses the classification of
semantic relations between pairs of sen-
tences extracted from a Dutch parallel cor-
pus at the word, phrase and sentence level.
We first investigate the performance of hu-
man annotators on the task of manually
aligning dependency analyses of the re-
spective sentences and of assigning one
of five semantic relations to the aligned
phrases (equals, generalizes, specifies, re-
states and intersects). Results indicate that
humans can perform this task well, with
an F-score of .98 on alignment and an F-
score of .95 on semantic relations (after
correction). We then describe and evalu-
ate a combined alignment and classifica-
tion algorithm, which achieves an F-score
on alignment of .85 (using EuroWordNet)
and an F-score of .80 on semantic relation
classification.
1 Introduction
An automatic method that can determine how two
sentences relate to each other in terms of seman-
tic overlap or textual entailment (e.g., (Dagan and
Glickman, 2004)) would be a very useful thing to
have for robust natural language applications. A
summarizer, for instance, could use it to extract
the most informative sentences, while a question-
answering system ? to give a second example ?
could use it to select potential answer string (Pun-
yakanok et al, 2004), perhaps preferring more spe-
cific answers over more general ones. In general, it
?This work was carried out within the IMIX-IMOGEN (In-
teractive Multimodal Output Generation) project, sponsored by
the Netherlands Organization of Scientific Research (NWO).
is very useful to know whether some sentence S is
more specific (entails) or more general than (is en-
tailed by) an alternative sentence S?, or whether the
two sentences express essentially the same informa-
tion albeit in a different way (paraphrasing).
Research on automatic methods for recognizing
semantic relations between sentences is still rela-
tively new, and many basic issues need to be re-
solved. In this paper we address two such related is-
sues: (1) to what extent can human annotators label
semantic overlap relations between words, phrases
and sentences, and (2) what is the added value of
linguistically informed analyses.
It is generally assumed that pure string overlap
is not sufficient for recognizing semantic relations;
and that using some form of syntactic analysis may
be beneficial (e.g., (Herrera et al, 2005), (Vander-
wende et al, 2005)). Our working hypothesis is that
semantic overlap at the word and phrase levels may
provide a good basis for deciding the semantic re-
lation between sentences. Recognising semantic re-
lations between sentences then becomes a two-step
procedure: first, the words and phrases in the re-
spective sentences need to be aligned, after which
the relations between the pairs of aligned words and
phrases should be labeled in terms of semantic rela-
tions.
Various alignment algorithms have been devel-
oped for data-driven approaches to machine trans-
lation (e.g. (Och and Ney, 2000)). Initially work
focused on word-based alignment, but more and
more work is also addressing alignment at the higher
levels (substrings, syntactic phrases or trees), e.g.,
(Meyers et al, 1996), (Gildea, 2003). For our pur-
poses, an additional advantage of aligning syntac-
tic structures is that it keeps the alignment feasible
(as the number of arbitrary substrings that may be
aligned grows exponentially to the number of words
1
in the sentence). Here, following (Herrera et al,
2005) and (Barzilay, 2003), we will align sentences
at the level of dependency structures. In addition,
we will label the alignments in terms of five basic
semantic relations to be defined below. We will per-
form this task both manually and automatically, so
that we can address both of the issues raised above.
Section 2 describes a monolingual parallel cor-
pus consisting of two Dutch translations, and for-
malizes the alignment-classification task to be per-
formed. In section 3 we report the results on align-
ment, first describing interannotator agreement on
this task and then the results on automatic alignment.
In section 4, then, we address the semantic relation
classification; again, first describing interannotator
results, followed by results obtained using memory-
based machine learning techniques. We end with a
general discussion.
2 Corpus and Task definition
2.1 Corpus
We have developed a parallel monolingual corpus
consisting of two different Dutch translations of the
French book ?Le petit prince? (the little prince) by
Antoine de Saint-Exupe?ry (published 1943), one by
Laetitia de Beaufort-van Hamel (1966) and one by
Ernst Altena (2000). For our purposes, this proved
to be a good way to quickly find a large enough
set of related sentence pairs, which differ semanti-
cally in interesting and subtle ways. In this work,
we used the first five chapters, with 290 sentences
and 3600 words in the first translation, and 277 sen-
tences and 3358 words in the second translation.
The texts were automatically tokenized and split into
sentences, after which errors were manually cor-
rected. Corresponding sentences from both trans-
lations were manually aligned; in most cases this
was a one-to-one mapping, but occasionally a sin-
gle sentence in one translation mapped onto two or
more sentences in the other: this occurred 23 times
in all five chapters. Next, the Alpino parser for
Dutch (e.g., (Bouma et al, 2001)) was used for part-
of-speech tagging and lemmatizing all words, and
for assigning a dependency analysis to all sentences.
The POS labels indicate the major word class (e.g.
verb, noun, adj, and adv). The dependency rela-
tions hold between tokens and are identical to those
used in the Spoken Dutch Corpus. These include de-
pendencies such as head/subject, head/modifier and
coordination/conjunction. If a full parse could not
be obtained, Alpino produced partial analyses col-
lected under a single root node. Errors in lemmati-
zation, POS tagging, and syntactic dependency pars-
ing were not subject to manual correction.
2.2 Task definition
The task to be performed can be described infor-
mally as follows: given two dependency analyses,
align those nodes that are semantically related. More
precisely: For each node v in the dependency struc-
ture for a sentence S, we define STR(v) as the sub-
string of all tokens under v (i.e., the composition of
the tokens of all nodes reachable from v). An align-
ment between sentences S and S? pairs nodes from
the dependency graphs for both sentences. Aligning
node v from the dependency graph D of sentence
S with node v? from the graph D? of S? indicates
that there is a semantic relation between STR(v) and
STR(v?), that is, between the respective substrings
associated with v and v?. We distinguish five po-
tential, mutually exclusive, relations between nodes
(with illustrative examples):
1. v equals v? iff STR(v) and STR(v?) are literally
identical (abstracting from case). Example: ?a
small and a large boa-constrictor? equals ?a
large and a small boa-constrictor?;
2. v restates v? iff STR(v) is a paraphrase of
STR(v?) (same information content but differ-
ent wording). Example: ?a drawing of a boa-
constrictor snake? restates ?a drawing of a boa-
constrictor?;
3. v specifies v? iff STR(v) is more specific than
STR(v?). Example: ?the planet B 612? specifies
?the planet?;
4. v generalizes v? iff STR(v?) is more specific
than STR(v). Example: ?the planet? general-
izes ?the planet B 612?;
5. v intersects v? iff STR(v) and STR(v?) share
some informational content, but also each ex-
press some piece of information not expressed
in the other. Example: ?Jupiter and Mars? in-
tersects ?Mars and Venus?
Figure 1 shows an example alignment with seman-
tic relations between the dependency structures of
2
hebben
komen
hebben
ik
ik op in in aanraking met
zo contact met in de loop van
veel
heel
persoon
serieus veel
massa
gewichtig
heel
leven
mijn
leven
het
manier
die mens
Figure 1: Dependency structures and alignment for the sentences Zo heb ik in de loop van mijn leven heel
veel contacten gehad met heel veel serieuze personen. (lit. ?Thus have I in the course of my life very
many contacts had with very many serious persons?) and Op die manier kwam ik in het leven met massa?s
gewichtige mensen in aanraking.. (lit. ?In that way came I in the life with mass-of weighty/important people
in touch?). The alignment relations are equals (dotted gray), restates (solid gray), specifies (dotted black),
and intersects (dashed gray). For the sake of transparency, dependency relations have been omitted.
two sentences. Note that there is an intuitive rela-
tion with entailment here: both equals and restates
can be understood as mutual entailment (i.e., if the
root nodes of the analyses corresponding S and S?
stand in an equal or restate relation, S entails S? and
S? entails S), if S specifies S? then S also entails S?
and if S generalizes S? then S is entailed by S?.
In remainder of this paper, we will distinguish two
aspects of this task: alignment is the subtask of pair-
ing related nodes ? or more precise, pairing the to-
ken strings corresponding to these nodes; classifica-
tion of semantic relations is the subtask of labeling
these alignments in terms of the five types of seman-
tic relations.
2.3 Annotation procedure
For creating manual alignments, we developed a
special-purpose annotation tool which shows, side
by side, two sentences, as well as their respective
dependency graphs. When the user clicks on a node
v in the graph, the corresponding string (STR(v)) is
shown at the bottom. The tool enables the user to
manually construct an alignment graph on the basis
of the respective dependency graphs. This is done by
focusing on a node in the structure for one sentence,
and then selecting a corresponding node (if possible)
in the other structure, after which the user can select
the relevant alignment relation. The tool offers addi-
tional support for folding parts of the graphs, high-
lighting unaligned nodes and hiding dependency re-
lation labels.
All text material was aligned by the two authors.
They started with annotating the first ten sentences
of chapter one together in order to get a feel for
the task. They continued with the remaining sen-
tences from chapter one individually (35 sentences
and 521 in the first translation, and 35 sentences and
481 words in the second translation). Next, both
annotators discussed annotation differences, which
triggered some revisions in their respective annota-
tion. They also agreed on a single consensus annota-
tion. Interannotator agreement will be discussed in
the next two sections. Finally, each author annotated
two additional chapters, bringing the total to five.
3 Alignment
3.1 Interannotator agreement
Interannotator agreement was calculated in terms of
precision, recall and F-score (with ? = 1) on aligned
3
(A1, A2) (A1? , A2?) (Ac, A1?) (Ac, A2?)
#real: 322 323 322 322
#pred: 312 321 323 321
#correct: 293 315 317 318
precision: .94 .98 .98 .99
recall: .91 .98 .98 .99
F-score: .92 .98 .98 .99
Table 1: Interannotator agreement with respect
to alignment between annotators 1 and 2 before
(A1, A2) and after (A1? , A2?) revision , and between
the consensus and annotator 1 (Ac, A1?) and annota-
tor 2 (Ac, A2?) respectively.
node pairs as follows:
precision = | Areal ? Apred | / | Apred | (1)
recall = | Areal ? Apred | / | Areal | (2)
F -score = (2 ? prec ? rec) / (prec + rec) (3)
where Areal is the set of all real alignments (the ref-
erence or golden standard), Apred is the set of all
predicted alignments, and Apred?Areal is the set al
correctly predicted alignments. For the purpose of
calculating interannotator agreement, one of the an-
notations (A1) was considered the ?real? alignment,
the other (A2) the ?predicted?. The results are sum-
marized in Table 1 in column (A1, A2).1
As explained in section 2.3, both annotators re-
vised their initial annotations. This improved their
agreement, as shown in column (A1? , A2?). In ad-
dition, they agreed on a single consensus annotation
(Ac). The last two columns of Table 1 show the re-
sults of evaluating each of the revised annotations
against this consensus annotation. The F-score of
.98 can therefore be regarded as the upper bound on
the alignment task.
3.2 Automatic alignment
Our tree alignment algorithm is based on the dy-
namic programming algorithm in (Meyers et al,
1996), and similar to that used in (Barzilay, 2003).
It calculates the match between each node in de-
pendency tree D against each node in dependency
tree D?. The score for each pair of nodes only de-
pends on the similarity of the words associated with
the nodes and, recursively, on the scores of the best
1Note that since there are no classes, we can not calculate
change agreement rethe Kappa statistic.
matching pairs of their descendants. The node simi-
larity function relies either on identity of the lemmas
or on synonym, hyperonym, and hyponym relations
between them, as retrieved from EuroWordNet.
Automatic alignment was evaluated with the con-
sensus alignment of the first chapter as the gold
standard. A baseline was constructed by aligning
those nodes which stand in an equals relation to each
other, i.e., a node v in D is aligned to a node v?
in D? iff STR(v) =STR(v?). This baseline already
achieves a relatively high score (an F-score of .56),
which may be attributed to the nature of our mate-
rial: the translated sentence pairs are relatively close
to each other and may show a sizeable amount of lit-
eral string overlap. In order to test the contribution
of synonym and hyperonym information for node
matching, performance is measured with and with-
out the use of EuroWordNet. The results for auto-
matic alignment are shown in Table 2. In compari-
son with the baseline, the alignment algorithm with-
out use of EuroWordnet loses a few points on preci-
sion, but improves a lot on recall (a 200% increase),
which in turn leads to a substantial improvement on
the overall F-score. The use of EurWordNet leads to
a small increase (two points) on both precision and
recall, and thus to small increase in F-score. How-
ever, in comparison with the gold standard human
score for this task (.95), there is clearly room for
further improvement.
4 Classification of semantic relations
4.1 Interannotator agreement
In addition to alignment, the annotation procedure
for the first chapter of The little prince by two anno-
tators (cf. section 2.3) also involved labeling of the
semantic relation between aligned nodes. Interanno-
tator agreement on this task is shown Table 3, before
and after revision. The measures are weighted preci-
sion, recall and F-score. For instance, the precision
is the weighted sum of the separate precision scores
for each of the five relations. The table also shows
the ?-score. The F-score of .97 can be regarded as
the upper bound on the relation labeling task. We
think these numbers indicate that the classification
of semantic relations is a well defined task which
can be accomplished with a high level of interanno-
tator agreement.
4
Alignment : Prec : Rec : F-score:
baseline .87 .41 .56
algorithm without wordnet .84 .82 .83
algorithm with wordnet .86 .84 .85
Table 2: Precision, recall and F-score on automatic
alignment
(A1, A2) (A1? , A2?) (Ac, A1?) (Ac, A2?)
precision: .86 .96 .98 .97
recall: .86 .95 .97 .97
F-score: .85 .95 .97 .97
?: .77 .92 .96 .96
Table 3: Interannotator agreement with respect to se-
mantic relation labeling between annotators 1 and 2
before (A1, A2) and after (A1? , A2?) revision , and
between the consensus and annotator 1 (Ac, A1?)
and annotator 2 (Ac, A2?) respectively.
4.2 Automatic classification
For the purpose of automatic semantic relation la-
beling, we approach the task as a classification prob-
lem to be solved by machine learning. Alignments
between node pairs are classified on the basis of the
lexical-semantic relation between the nodes, their
corresponding strings, and ? recursively ? on previ-
ous decisions about the semantic relations of daugh-
ter nodes. The input features used are:
? a boolean feature representing string identity
between the strings corresponding to the nodes
? a boolean feature for each of the five semantic
relations indicating whether the relation holds
for at least one of the daughter nodes;
? a boolean feature indicating whether at least
one of the daughter nodes is not aligned;
? a categorical feature representing the lexical se-
mantic relation between the nodes (i.e. the
lemmas and their part-of-speech) as found in
EuroWordNet, which can be synonym, hyper-
onym, or hyponym.2
To allow for the use of previous decisions, the
nodes of the dependency analyses are traversed in
a bottom-up fashion. Whenever a node is aligned,
the classifier assigns a semantic label to the align-
ment. Taking previous decisions into account may
2These three form the bulk of all relations in Dutch Eu-
roWordnet. Since no word sense disambiguation was involved,
we simply used all word senses.
Prec : Rec : F-score:
equals .93? .06 .95? .04 .94? .02
restates .56? .08 .78? .04 .65? .05
specifies n.a. 0 n.a.
generalizes .19? .06 .37? .09 .24? .05
intersects n.a. 0 n.a.
Combined: .62? .01 .70? .02 .64? .02
Table 4: Average precision, recall and F-score (and
SD) over all 5 folds on automatic classification of
semantic relations
cause a proliferation of errors: wrong classification
of daughter nodes may in turn cause wrong classifi-
cation of the mother node. To investigate this risk,
classification experiments were run both with and
without (i.e. using the annotation) previous deci-
sions.
Since our amount of data is limited, we used
a memory-based classifier, which ? in contrast to
most other machine learning algorithms ? performs
no abstraction, allowing it to deal with productive
but low-frequency exceptions typically occurring in
NLP tasks(Daelemans et al, 1999). All memory-
based learning was performed with TiMBL, version
5.1 (Daelemans et al, 2004), with its default set-
tings (overlap distance function, gain-ratio feature
weighting, k = 1).
The five first chapters of The little prince were
used to run a 5-fold cross-validated classification ex-
periment. The first chapter is the consensus align-
ment and relation labeling, while the other four were
done by one out of two annotators. The alignments
to be classified are those from to the human align-
ment. The baseline of always guessing equals ? the
majority class ? gives a precision of 0.26, a recall of
0.51, and an F-score of 0.36. Table 4 presents the re-
sults broken down to relation type. The combined F-
score of 0.64 is almost twice the baseline score. As
expected, the highest score goes to equals, followed
by a reasonable score on restates. Performance on
the other relation types is rather poor, with even no
predictions of specifies and intersects at all.
Faking perfect previous decisions by using the
annotation gives a considerable improvement, as
shown in Table 5, especially on specifies, general-
izes and intersects. This reveals that the prolifera-
tion of classification errors is indeed a problem that
should be addressed.
5
Prec : Rec : F-score:
equals .99? .02 .97? .02 .98? .01
restates .65? .04 .82? .04 .73? .03
specifies .60? .12 .48? .10 .53? .09
generalizes .50? .11 .52? .10 .50? .09
intersects .69? .27 .35? .12 .46? .16
Combined: .82? .02 .81? .02 .80? .02
Table 5: Average precision, recall and F-score (and
SD) over all 5 folds on automatic classification of
semantic relations without using previous decisions.
In sum, these results show that automatic classifi-
cation of semantic relations is feasible and promis-
ing ? especially when the proliferation of classifica-
tion errors can be prevented ? but still not nearly as
good as human performance.
5 Discussion and Future work
This paper presented an approach to detecting se-
mantic relations at the word, phrase and sentence
level on the basis of dependency analyses. We inves-
tigated the performance of human annotators on the
tasks of manually aligning dependency analyses and
of labeling the semantic relations between aligned
nodes. Results indicate that humans can perform this
task well, with an F-score of .98 on alignment and an
F-score of .92 on semantic relations (after revision).
We also described and evaluated automatic methods
addressing these tasks: a dynamic programming tree
alignment algorithm which achieved an F-score on
alignment of .85 (using lexical semantic information
from EuroWordNet), and a memory-based seman-
tic relation classifier which achieved F-scores of .64
and .80 with and without using real previous deci-
sions respectively.
One of the issues that remains to be addressed
in future work is the effect of parsing errors. Such
errors were not corrected, but during manual align-
ment, we sometimes found that substrings could not
be properly aligned because the parser had failed to
identify them as syntactic constituents. As far as
classification of semantic relations is concerned, the
proliferation of classification errors is an issue that
needs to be solved. Classification performance may
be further improved with additional features (e.g.
phrase length information), optimization, and more
data. Also, we have not yet tried to combine au-
tomatic alignment and classification. Yet another
point concerns the type of text material. The sen-
tence pairs from our current corpus are relatively
close, in the sense that both translations more or less
convey the same information. Although this seems a
good starting point to study alignment, we intend to
continue with other types of text material in future
work. For instance, in extending our work to the ac-
tual output of a QA system, we expect to encounter
sentences with far less overlap.
References
R. Barzilay. 2003. Information Fusion for Multidocu-
ment Summarization. Ph.D. Thesis, Columbia Univer-
sity.
G. Bouma, G. van Noord, and R. Malouf. 2001. Alpino:
Wide-coverage computational analysis of Dutch. In
Computational Linguistics in The Netherlands 2000,
pages 45?59.
W. Daelemans, A. Van den Bosch, and J. Zavrel. 1999.
Forgetting exceptions is harmful in language learning.
Machine Learning, Special issue on Natural Language
Learning, 34:11?41.
W. Daelemans, J. Zavrel, K. Van der Sloot, and
A. van den Bosch. 2004. TiMBL: Tilburg memory
based learner, version 5.1, reference guide. ILK Tech-
nical Report 04-02, Tilburg University.
I. Dagan and O. Glickman. 2004. Probabilistic textual
entailment: Generic applied modelling of language
variability. In Learning Methods for Text Understand-
ing and Mining, Grenoble.
D. Gildea. 2003. Loosely tree-based alignment for ma-
chine translation. In Proceedings of the 41st Annual
Meeting of the ACL, Sapporo, Japan.
J. Herrera, A. Pe nas, and F. Verdejo. 2005. Textual
entailment recognition based on dependency analy-
sis and wordnet. In Proceedings of the 1st. PASCAL
Recognision Textual Entailment Challenge Workshop.
Pattern Analysis, Statistical Modelling and Computa-
tional Learning, PASCAL.
A. Meyers, R. Yangarber, and R. Grisham. 1996. Align-
ment of shared forests for bilingual corpora. In Pro-
ceedings of 16th International Conference on Com-
putational Linguistics (COLING-96), pages 460?465,
Copenhagen, Denmark.
F.J. Och and H. Ney. 2000. Statistical machine trans-
lation. In EAMT Workshop, pages 39?46, Ljubljana,
Slovenia.
V. Punyakanok, D. Roth, and W. Yih. 2004. Natural lan-
guage inference via dependency tree mapping: An ap-
plication to question answering. Computational Lin-
guistics, 6(9).
L. Vanderwende, D. Coughlin, and W. Dolan. 2005.
What syntax can contribute in entailment task. In Pro-
ceedings of the 1st. PASCAL Recognision Textual En-
tailment Challenge Workshop, Southampton, U.K.
6
Explorations in Sentence Fusion?
Erwin Marsi and Emiel Krahmer
Communication and Cognition
Faculty of Arts, Tilburg University
P.O.Box 90153, NL-5000 LE Tilburg, The Netherlands
{e.c.marsi, e.j.krahmer}@uvt.nl
Abstract
Sentence fusion is a text-to-text (revision-like) gen-
eration task which takes related sentences as input
and merges these into a single output sentence. In
this paper we describe our ongoing work on de-
veloping a sentence fusion module for Dutch. We
propose a generalized version of alignment which
not only indicates which words and phrases should
be aligned but also labels these in terms of a small
set of primitive semantic relations, indicating how
words and phrases from the two input sentences re-
late to each other. It is shown that human label-
ers can perform this task with a high agreement (F-
score of .95). We then describe and evaluate our
adaptation of an existing automatic alignment al-
gorithm, and use the resulting alignments, plus the
semantic labels, in a generalized fusion and gen-
eration algorithm. A small-scale evaluation study
reveals that most of the resulting sentences are ad-
equate to good.
1 Introduction
Traditionally, Natural Language Generation (NLG) is defined
as the automatic production of ?meaningful texts in (...) hu-
man language from some underlying non-linguistic represen-
tation of information? [Reiter and Dale, 2000, xvii]. Re-
cently, there is an increased interest in NLG applications
that produce meaningful text from meaningful text rather than
from abstract meaning representations. Such applications
are sometimes referred to as text-to-text generation applica-
tions (e.g., [Chandrasekar and Bangalore, 1997], [Knight and
Marcu, 2002], [Lapata, 2003]), and may be likened to ear-
lier revision-based generation strategies, e.g. [Robin, 1994]
[Callaway and Lester, 1997]. Text-to-text generation is often
motivated from practical applications such as summarization,
sentence simplification, and sentence compression. One rea-
son for the interest in such generation systems is the possi-
bility to automatically learn text-to-text generation strategies
from corpora of parallel text.
?This work was carried out within the IMIX-IMOGEN (Inter-
active Multimodal Output Generation) project, sponsored by the
Netherlands Organization of Scientific Research (NWO).
In this paper, we take a closer look at sentence fusion
[Barzilay, 2003][Barzilay et al, 1999], one of the interesting
variants in text-to-text generation. A sentence fusion module
takes related sentences as input, and generates a single sen-
tence summarizing the input sentences. The general strategy
described in [Barzilay, 2003] is to first align the dependency
structures of the two input sentences to find the common in-
formation in both sentences. On the basis of this alignment,
the common information is framed into an fusion tree (i.e.,
capturing the shared information), which is subsequently re-
alized in natural language by generating all traversals of the
fusion tree and scoring their probability using an n-gram lan-
guage model. Of the sentences thus generated the one with
the lowest (length normalized) entropy is selected.
Barzilay and co-workers apply sentence fusion in the con-
text of multi-document summarization, where the input sen-
tences typically come from multiple documents describing
the same event, but sentence fusion seems to be useful for
other applications as well. In question-answering, for in-
stance, sentence fusion could be used to generate more com-
plete answers. Many current QA systems use various parallel
answer-finding strategies, each of which may produce an N-
best list of answers (e.g., [Maybury, 2004]) In response to a
question like ?What causes RSI?? one potential answer sen-
tence could be:
RSI can be caused by repeating the same sequence
of movements many times an hour or day.
And another might be:
RSI is generally caused by a mixture of poor er-
gonomics, stress and poor posture.
These two incomplete answers might be fused into a more
complete answer such as:
RSI can be caused by a mixture of poor er-
gonomics, stress, poor posture and by repeating the
same sequence of movements many times an hour
or day.
The same process of sentence fusion can of course be applied
to the whole list of N-best answers in order to derive a more
specific, or even the most specific, answer, akin to taking the
union of a number of sets. Likewise, we can rely on sentence
fusion to derive a more general answer, or even the most gen-
eral one (cf. intersection), in the hope that this will filter out
irrelevant parts of the answer.
Arguably, such applications call for a generalized version
of sentence fusion, which may have consequences for the var-
ious components (alignment, fusion and generation) of the
sentence fusion pipeline. At the alignment level, we would
like to have a better understanding of how words and phrases
in the input sentences relate to each other. Rather than a bi-
nary choice (align or not), one might want to distinguish more
fine-grained relations such as overlap (if two phrases share
some but not all of their content), paraphrases (if two phrases
express the same information in different ways), entailments
(if one phrase entails the other, but not vice versa), etc. Such
an alignment strategy would be especially useful for applica-
tions such as question answering and information extraction,
where it is often important to know whether two sentences
are paraphrases or stand in an entailment relation [Dagan and
Glickman, 2004]. In the fusion module, we are interested in
the possibilities to generate various kinds of fusions depend-
ing on the relations between the respective sentences, e.g., se-
lecting the more specific or the more general phrase depend-
ing on whether the fusion tree is an intersection or a union
one. Finally, the generation may be more complicated in the
generalized version, and it is an interesting question whether
the use of language models is equally suitable for different
kinds of fusion.
In this paper, we will explore some of these issues re-
lated to a generalized version of sentence fusion. We start
with the basic question whether it is possible at all to reli-
ably align sentences, including different potential relations
between words and phrases (section 2). We then present our
ongoing work on sentence fusion, describing the current sta-
tus and performance of the alignment algorithm (section 3),
as well as the fusion and generation components (section 4).
We end with discussion and description of future plans in sec-
tion 5.
2 Data collection and Annotation
2.1 General approach
Alignment has become standard practice in data-driven ap-
proaches to machine translation (e.g. [Och and Ney, 2000]).
Initially work focused on word-based alignment, but more re-
cent research also addresses alignment at the higher levels
(substrings, syntactic phrases or trees), e.g.,[Gildea, 2003].
The latter approach seems most suitable for current purposes,
where we want to express that a sequence of words in one
sentence is related to a non-identical sequence of words in
another sentence (a paraphrase, for instance). However, if
we allow alignment of arbitrary substrings of two sentences,
then the number of possible alignments grows exponentially
to the number of tokens in the sentences, and the process of
alignment ? either manually or automatically ? may become
infeasible. An alternative, which seems to occupy the middle
ground between word alignment on the one hand and align-
ment of arbitrary substrings on the other, is to align syntac-
tic analyses. Here, following [Barzilay, 2003], we will align
sentences at the level of dependency structures. Unlike to
[Barzilay, 2003], we are interested in a number of different
alignment relations between sentences, and pay special atten-
tion to the feasibility of this alignment task.
verb:
hebben
verb:
hebben
hd/vc
pron:
ik
hd/su
adv:
zo
hd/mod hd/su
noun:
contact
hd/obj1
prep:
met
hd/pc
prep:
in de loop van
hd/mod
det:
veel
hd/det
adv:
heel
hd/mod
noun:
persoon
hd/obj1
adj:
serieus
hd/mod
det:
veel
hd/det
adv:
heel
hd/mod
noun:
leven
hd/obj1
det:
mijn
hd/det
Figure 1: Example dependency structure for the sentence Zo
heb ik in the loop van mijn leven heel veel contacten gehad
met heel veel serieuze personen. (lit. ?Thus have I in the
course of my life very many contacts had with very many
serious persons?).
2.2 Corpus
For evaluation and parameter estimation we have developed
a parallel monolingual corpus consisting of two different
Dutch translations of the French book ?Le petit prince? (the
little prince) by Antoine de Saint-Exupe?ry (published 1943),
one by Laetitia de Beaufort-van Hamel (1966) and one by
Ernst Altena (2000). The texts were automatically tokenized
and split into sentences, after which errors were manually
corrected. Corresponding sentences from both translations
were manually aligned; in most cases this was a one-to-one
mapping but occasionally a single sentence in one version
mapped onto two sentences in the other: Next, the Alpino
parser for Dutch (e.g., [Bouma et al, 2001]) was used for
part-of-speech tagging and lemmatizing all words, and for
assigning a dependency analysis to all sentences. The POS
labels indicate the major word class (e.g. verb, noun, pron,
and adv). The dependency relations hold between tokens
and are the same as used in the Spoken Dutch Corpus (see
e.g., [van der Wouden et al, 2002]). These include depen-
dencies such as head/subject, head/modifier and coordina-
tion/conjunction. See Figure 1 for an example. If a full parse
could not be obtained, Alpino produced partial analyses col-
lected under a single root node. Errors in lemmatization, POS
tagging, and syntactic dependency parsing were not subject to
manual correction.
2.3 Task definition
A dependency analysis of a sentence S yields a labeled di-
rected graph D = ?V,E?, where V (vertices) are the nodes,
and E (edges) are the dependency relations. For each node
v in the dependency structure for a sentence S, we define
STR(v) as the substring of all tokens under v (i.e., the com-
position of the tokens of all nodes reachable from v). For
example, the string associated with node persoon in Figure 1
is heel veel serieuze personen (?very many serious persons?).
An alignment between sentences S and S? pairs nodes from
the dependency graphs for both sentences. Aligning node v
from the dependency graph D of sentence S with node v?
from the graph D? of S? indicates that there is a relation be-
tween STR(v) and STR(v?), i.e., between the respective sub-
strings associated with v and v?. We distinguish five potential,
mutually exclusive, relations between nodes (with illustrative
examples):
1. v equals v? iff STR(v) and STR(v?) are literally identical
(abstracting from case and word order)
Example: ?a small and a large boa-constrictor? equals
?a large and a small boa-constrictor?;
2. v restates v? iff STR(v) is a paraphrase of STR(v?) (same
information content but different wording),
Example: ?a drawing of a boa-constrictor snake? re-
states ?a drawing of a boa-constrictor?;
3. v specifies v? iff STR(v) is more specific than STR(v?),
Example: ?the planet B 612? specifies ?the planet?;
4. v generalizes v? iff STR(v?) is more specific than
STR(v),
Example: ?the planet? generalizes ?the planet B 612?;
5. v intersects v? iff STR(v) and STR(v?) share some in-
formational content, but also each express some piece of
information not expressed in the other,
Example: ?Jupiter and Mars? intersects ?Mars and
Venus?
Note that there is an intuitive relation with entailment here:
both equals and restates can be understood as mutual entail-
ment (i.e., if the root nodes of the analyses corresponding S
and S? stand in an equal or restate relation, S entails S? and
S? entails S), if S specifies S? then S also entails S? and if S
generalizes S? then S is entailed by S?.
An alignment between S and S? can now formally be
defined on the basis of the respective dependency graphs
D = ?V,E? and D? = ?V ?, E?? as a graph A = ?VA, EA?,
such that
EA = {?v, l, v?? | v ? V & v? ? V ? & l(STR(v), STR(v?))},
where l is one of the five relations defined above. The nodes
of A are those nodes from D en D? which are aligned, for-
mally defined as
VA = {v | ?v??l?v, l, v?? ? EA}?{v? | ?v?l?v, l, v?? ? EA}
A complete example alignment can be found in the Appendix,
Figure 3.
(A1, A2) (A1? , A2?) (Ac, A1?) (Ac, A2?)
#real: 322 323 322 322
#pred: 312 321 323 321
#correct: 293 315 317 318
precision: .94 .98 .98 .99
recall: .91 .98 .98 .99
F-score: .92 .98 .98 .99
Table 1: Interannotator agreement with respect to align-
ment between annotators 1 and 2 before (A1, A2) and after
(A1? , A2?) revision , and between the consensus and annota-
tor 1 (Ac, A1?) and annotator 2 (Ac, A2?) respectively.
2.4 Alignment tool
For creating manual alignments, we developed a special-
purpose annotation tool called Gadget (?Graphical Aligner of
Dependency Graphs and Equivalent Tokens?). It shows, side
by side, two sentences, as well as their respective dependency
graphs. When the user clicks on a node v in the graph, the cor-
responding string (STR(v)) is shown at the bottom. The tool
enables the user to manually construct an alignment graph on
the basis of the respective dependency graphs. This is done
by focusing on a node in the structure for one sentence, and
then selecting a corresponding node (if possible) in the other
structure, after which the user can select the relevant align-
ment relation. The tool offers additional support for folding
parts of the graphs, highlighting unaligned nodes and hiding
dependency relation labels. See Figure 4 in the Appendix for
a screen shot of Gadget.
2.5 Results
All text material was aligned by the two authors. They started
doing the first ten sentences of chapter one together in order
to get a feel for the task. They continued with the remaining
sentences from chapter one individually. The total number
of nodes in the two translations of the chapter was 445 and
399 respectively. Inter-annotator agreement was calculated
for two aspects: alignment and relation labeling. With respect
to alignment, we calculated the precision, recall and F-score
(with ? = 1) on aligned node pairs as follows:
precision(Areal, Apred) = | Areal ?Apred || Apred | (1)
recall(Areal, Apred) = | Areal ?Apred || Areal | (2)
F -score = 2? precision? recallprecision+ recall (3)
where Areal is the set of all real alignments (the reference or
golden standard), Apred is the set of all predicted alignments,
and Apred?Areal is the set al correctly predicted alignments.
For the purpose of calculating inter-annotator agreement, one
of the annotations (A1) was considered the ?real? alignment,
the other (A2) the ?predicted?. The results are summarized in
Table 1 in column (A1, A2).
Next, both annotators discussed the differences in align-
ment, and corrected mistaken or forgotten alignments. This
improved their agreement as shown in column (A1? , A2?). In
(A1, A2) (A1? , A2?) (Ac, A1?) (Ac, A2?)
precision: .86 .96 .98 .97
recall: .86 .95 .97 .97
F-score: .85 .95 .97 .97
?: .77 .92 .96 .96
Table 2: Inter-annotator agreement with respect to alignment
relation labeling between annotators 1 and 2 before (A1, A2)
and after (A1? , A2?) revision , and between the consensus and
annotator 1 (Ac, A1?) and annotator 2 (Ac, A2?) respectively.
addition, they agreed on a single consensus annotation (Ac).
The last two columns of Table 1 show the results of evalu-
ating each of the revised annotations against this consensus
annotation. The F-score of .96 can therefore be regarded as
the upper bound on the alignment task.
In a similar way, the agreement was calculated for the task
of labeling the alignment relations. Results are shown in Ta-
ble 2, where the measures are weighted precision, recall and
F-score. For instance, the precision is the weighted sum of
the separate precision scores for each of the five relations.
The table also shows the ?-score, which is another commonly
used measure for inter-annotator agreement [Carletta, 1996].
Again, the F-score of .97 can be regarded as the upper bound
on the relation labeling task.
We think these numbers indicate that the labeled alignment
task is well defined and can be accomplished with a high level
of inter-annotator agreement.
3 Automatic alignment
In this section, we describe the alignment algorithm that we
use (section 3.1), and evaluate its performance (section 3.2).
3.1 Tree alignment algorithm
The tree alignment algorithm is based on [Meyers et al,
1996], and similar to that used in [Barzilay, 2003]. It cal-
culates the match between each node in dependency tree D
against each node in dependency tree D?. The score for each
pair of nodes only depends on the similarity of the words
associated with the nodes and, recursively, on the scores of
the best matching pairs of their descendants. For an efficient
implementation, dynamic programming is used to build up a
score matrix, which guarantees that each score will be calcu-
lated only once.
Given two dependency trees D and D?, the algorithm
builds up a score function S(v, v?) for matching each node
v in D against each node v? in D?, which is stored in a ma-
trix M . The value S(v, v?) is the score for the best match
between the two subtrees rooted at v in D and at v? in D?.
When a value for S(v, v?) is required, and is not yet in the
matrix, it is recursively computed by the following formula:
S(v, v?) = max
?
?
?
TREEMATCH(v, v?)
maxi=1,...,n S(vi, v?)
maxj=1,...,m S(v, v?j)
(4)
where v1, . . . , vn denote the children of v and v?1, . . . , v?m de-
note the children of v?. The three terms correspond to the
three ways that nodes can be aligned: (1) v can be directly
aligned to v?; (2) any of the children of v can be aligned to v?;
(3) v can be aligned to any of the children of v?. Notice that
the last two options imply skipping one or more edges, and
leaving one or more nodes unaligned.1
The function TREEMATCH(v, v?) is a measure of how well
the subtrees rooted at v and v? match:
TREEMATCH(v, v?) = NODEMATCH(v, v?) +
max
p ? P(v,v?)
?
? ?
(i,j) ? p
(
RELMATCH(??v i,??v ?j) + S(vi, v?j)
)
?
?
Here ??v i denotes the dependency relation from v to vi.
P(v, v?) is the set of all possible pairings of the n children
of v against the m children of v?, which is the power set of
{1, . . . , n} ? {1, . . . ,m}. The summation in (5) ranges over
all pairs, denoted by (i, j), which appear in a given pairing
p ? P(v, v?). Maximizing this summation thus amounts to
finding the optimal alignment of children of v to children of
v?.
NODEMATCH(v, v?) ? 0 is a measure of how well the
label of node v matches the label of v?.
RELMATCH(??v i,??v ?j) ? 0 is a measure for how well the
dependency relation between node v and its child vi matches
that of the dependency relation between node v? and its child
vj .
Since the dependency graphs delivered by the Alpino
parser were usually not trees, they required some modifica-
tion in order to be suitable input for the tree alignment al-
gorithm. We first determined a root node, which is defined
as a node from which all other nodes in the graph can be
reached. In the rare case of multiple root nodes, an arbi-
trary one was chosen. Starting from this root node, any cyclic
edges were temporarily removed during a depth-first traver-
sal of the graph. The resulting directed acyclic graphs may
still have some amount of structure sharing, but this poses no
problem for the algorithm.
3.2 Evaluation of automatic alignment
We evaluated the automatic alignment of nodes, abstracting
from relation labels, as we have no algorithm for automatic
labeling of these relations yet. The baseline is achieved by
aligning those nodes with stand in an equals relation to each
other, i.e., a node v in D is aligned to a node v? in D? iff
STR(v) =STR(v?). This alignment can be constructed rela-
tively easy.
The alignment algorithm is tested with the following
NODEMATCH function:
NODEMATCH(v, v?) =
?
??????
??????
10 if STR(v) = STR(v?)
5 if LABEL(v) = LABEL(v?)
2 if LABEL(v) is a synonym
hyperonym or hyponym
of LABEL(v?)
0 otherwise
1In the original formulation of the algorithm by [Meyers et al,
1996], there is a penalty for skipping edges.
Alignment : Prec : Rec : F-score:
baseline .87 .41 .56
algorithm without wordnet .84 .82 .83
algorithm with wordnet .86 .84 .85
Table 3: Precision, recall and F-score on automatic alignment
It reserves the highest value for a literal string match, a some-
what lower value for matching lemmas, and an even lower
value in case of a synonym, hyperonym or hyponym relation.
The latter relations are retrieved from the Dutch part of Eu-
roWordnet [Vossen, 1998]. For the RELMATCH function, we
simply used a value of 1 for identical dependency relations,
and 0 otherwise. These values were found to be adequate in a
number of test runs on two other, manually aligned chapters
(these chapters were not used for the actual evaluation). In the
future we intend to experiment with automatic optimizations.
We measured the alignment accuracy defined as the per-
centage of correctly aligned node pairs, where the consen-
sus alignment of the first chapter served as the golden stan-
dard. The results are summarized in Table 3. In order to test
the contribution of synonym and hyperonym information for
node matching, performance is measured with and without
the use of Eurowordnet. The results show that the algorithm
improves substantially on the baseline. The baseline already
achieves a relatively high score (an F-score of .56), which
may be attributed to the nature of our material: the translated
sentence pairs are relatively close to each other and may show
a sizeable amount of literal string overlap. The alignment al-
gorithm (without use of EuroWordnet) loses a few points on
precision, but improves a lot on recall (a 200% increase with
respect to the baseline), which in turn leads to a substantial
improvement on the overall F-score. The use of Euroword-
net leads to a small increase (two points) on both precision
and recall (and thus to small increase on F-score). Yet, in
comparison with the gold standard human score for this task
(.95), there is clearly room for further improvement.
4 Merging and generation
The remaining two steps in the sentence fusion process are
merging and generation. In general, merging amounts to de-
ciding which information from either sentence should be pre-
served, whereas generation involves producing a grammat-
ically correct surface representation. In order to get an idea
about the baseline performance, we explored a simple, some-
what naive string-based approach. Below, the pseudocode
is shown for merging two dependency trees in order to get
restatements. Given a labeled alignment A between depen-
dency graphs D and D?, if there is a restates relation between
node v from D and node v? from D?, we add the string real-
ization of v? as an alternative to those of v.
RESTATE(A)
1 for each edge ?v, l, v?? ? EA
2 do if l = restates
3 then STR(v) ? STR(v) ? STR(v?)
The same procedure is followed in order to get specifications:
SPECIFY(A)
1 for each edge ?v, l, v?? ? EA
2 do if l = generalizes
3 then STR(v) ? STR(v) ? STR(v?)
The generalization procedure adds the option to omit the re-
alization of a modifier that is not aligned:
GENERALIZE(D,A)
1 for each edge ?v, l, v?? ? EA
2 do if l = specifies
3 then STR(v) ? STR(v) ? STR(v?)
4 for each edge ?v, l, v?? ? ED
5 do if l ? MOD-DEP-RELS and v /? EA
6 then STR(v) ? STR(v) ? NIL
where MOD-DEP-REL is the set of dependency relations be-
tween a node and a modifier (e.g. head/mod and head/predm).
Each procedure is repeated twice, once adding substrings
from D into D? and once the other way around. Next, we
traverse the dependency trees and generate all string realiza-
tions, extending the list of variants for each node that has mul-
tiple realizations. Finally, we filter out multiple copies of the
same string, as well as strings that are identical to the input
sentences.
This procedure for merging and generation was applied to
the 35 sentence pairs from the consensus alignment of chapter
one of ?Le Petit Prince?. Overall this gave rise to 194 restate-
ment, 62 specifications and 177 generalizations, with some
sentence pairs leading to many variants and others to none at
all. Some output showed only minor variations, for instance,
substitution of a synonym. However, others revealed surpris-
ingly adequate generalizations or specifications. Examples of
good and bad output are given in Figure 2.
As expected, many of the resulting variants are ungram-
matical, because constraints on word order, agreement or sub-
categorisation are violated. Following work on statistical sur-
face generation [Langkilde and Knight, 1998] and other work
on sentence fusion [Barzilay, 2003], we tried to filter un-
grammatical variants with an n-gram language model. The
Cambridge-CMU Statistical Modeling Toolkit v2 was used to
train a 3-gram model on over 250M words from the Twente
Newscorpus , using back-off and Good-Turing smoothing.
Variants were ranked in order of increasing entropy. We
found, however, that the ranking was often inadequate, show-
ing ungrammatical variants at the top and grammatical vari-
ants in the lower regions.
To gain some insight into the general performance of the
merging and generation strategy, we performed a small eval-
uation test in which the two authors independently judged all
generated variants in terms of three categories:
1. Perfect: no problems in either semantics or syntax;
2. Acceptable: understandable, but with some minor flaws
in semantics or grammar;
3. Nonsense: serious problems in semantics or grammar
Table 4 shows the number of sentences in each of the three
categories per judge, broken down in restatements, general-
ization and specifications. The ?-score on this classification
Input1: Zo
Thus
heb
have
ik
I
in
in
de
the
loop
course
van
of
mijn
my
leven
life
heel
very
veel
many
contacten
contacts
gehad
had
met
with
heel
very
veel
many
serieuze
serious
personen
persons
Input2: Op
In
die
that
manier
way
kwam
came
ik
I
in
in
het
the
leven
life
met
with
massa?s
masses-of
gewichtige
weighty/important
mensen
people
in
in
aanraking
touch
Restate: op
in
die
that
manier
way
heb
have
ik
I
in
in
de
the
loop
course
van
of
mijn
my
leven
life
heel
very
veel
many
contacten
contacts
gehad
had
met
with
heel
very
veel
many
serieuze
serious
personen
persons
Specific: op
in
die
that
manier
way
kwam
have
ik
I
in
in
de
the
loop
course
van
of
mijn
my
leven
life
met
with
massa?s
masses-of
gewichtige
weighty/important
mensen
people
in
in
aanraking
touch
General: zo
thus
heb
have
ik
I
in
in
het
the
leven
life
veel
many
contacten
contacts
gehad
had
met
with
veel
many
serieuze
serious
personen
persons
Input1: En
And
zo
so
heb
have
ik
I
op
at
mijn
my
zesde
sixth
jaar
year
een
a
prachtige
wonderful
loopbaan
career
als
as
kunstschilder
art-painter
laten
let
varen
sail
.
Input2: Zo
Thus
kwam
came
het
it
,
,
dat
that
ik
I
op
at
zesjarige
six-year
leeftijd
age
een
a
schitterende
bright
schildersloopbaan
painter-career
liet
let
varen
sail
.
Specific: en
and
zo
so
heb
have
ik
I
op
at
mijn
my
zesde
sixth
jaar
year
als
as
kunstschilder
art-painter
laten
let
een
a
schitterende
bright
schildersloopbaan
painter-career
varen
sail
General: zo
so
kwam
came
het
it
dat
that
ik
I
op
at
leeftijd
age
een
a
prachtige
wonderful
loopbaan
career
liet
let
varen
sail
Figure 2: Examples of good (top) and bad (bottom) sentence fusion output
Restate: Specific: General:
J1 J2 J1 J2 J1 J2
Perfect: 109 104 28 22 89 86
Acceptable: 44 58 15 16 34 24
Nonsense: 41 32 19 24 54 67
Total: 194 62 177
Table 4: Results of the evaluation of the sentence fusion out-
put as the number of sentences in each of the three categories
perfect, acceptable and nonsense per judge (J1 and J2), bro-
ken down in restatements, generalizations and specifications.
task is .75, indicating a moderate to good agreement between
the judges. Roughly half of the generated restatements and
generalization are perfect, while this is not the case for spec-
ifications. We have no plausible explanation for this yet.
We think we can conclude from this evaluation that sen-
tence fusion is a viable and interesting approach for produc-
ing restatements, generalization and specifications. However,
there is certainly further work to do; the procedure for merg-
ing dependency graphs should be extended, and the realiza-
tion model clearly requires more linguistic sophistication in
particular to deal with word order, agreement and subcate-
gorisation constraints.
5 Discussion and Future work
In this paper we have described our ongoing work on sen-
tence fusion for Dutch. Starting point was the sentence fusion
model proposed by [Barzilay et al, 1999; Barzilay, 2003]
in which dependency analyses of pairs of sentences are first
aligned, after which the aligned parts (representing the com-
mon information) are fused. The resulting fused dependency
tree is subsequently transfered into natural language. Our
new contributions are primarily in two areas. First, we carried
out an explicit evaluation of the alignment ? both human and
automatic alignment ? whereas [Barzilay, 2003] only evalu-
ates the output of the complete sentence fusion process. We
found that annotators can reliably align phrases and assign
relation labels to them, and that good results can be achieved
with automatic alignment, certainly above an informed base-
line, albeit still below human performance. Second, Barzi-
lay and co-workers developed their sentence fusion model in
the context of multi-document summarization, but arguably
the approach could also be applicable for applications such
as question answering or information extraction. This seems
to call for a more refined version of sentence fusion, which
has consequences for alignment, merging and realization. We
have therefore introduced five different types of semantic re-
lations between strings, namely equals, restates, specifies,
generalizes and intersects. This increases the expressiveness
of the representation, and supports generating restatements,
generalizations and specifications. Finally, we described and
evaluated our first results on sentence realization based on
these refined alignments, with promising results.
Similar work is described in [Pang et al, 2003], who de-
scribe a syntax-based algorithm that builds word lattices from
parallel translations which can be used to generate new para-
phrases. Their alignment algorithm is less refined, and there
is only type of alignment and hence output (only restate-
ments), but their mapping of aligned trees to a word lattice
(or FSA) seems worthwhile to explore in combination with
the approach we have proposed here.
One of the issues that remains to be addressed in future
work is the effect of parsing errors. Such errors were not
manually corrected, but during manual alignment, however,
we sometimes found that substrings could not be properly
aligned because the parser failed to identify them as syntac-
tic constituents. The repercussions of this for the generation
should be investigated by comparing the results obtained here
with alignments on perfect parses. Furthermore, our work on
automatic alignment so far only concerned the alignment of
nodes, not the determination of the relation type. We intend
to address this task with machine learning, initially relying
on shallow features such as the length of the respective token
strings and the amount of overlap. It is also clear that more
work is needed on merging and surface realization. One pos-
sible direction here is to exploit the relatively rich linguistic
representation of the input sentences (POS tags, lemmas and
dependency structures), for instance, along the lines of [Ban-
galore and Rambow, 2000]. Yet another issue concerns the
type of text material. The sentence pairs from our current cor-
pus are relatively close, in the sense that there is usually a 1-
to-1 mapping between sentences, and both translations more
or less convey the same information. Although this seems a
good starting point to study alignment, we intend to continue
with other types of text material in future work. For instance,
in extending our work to the actual output of a QA system,
we expect to encounter sentences with far less overlap. Of
particular interest to us is also whether sentence fusion can
be shown to improve the quality of QA system output.
References
[Bangalore and Rambow, 2000] Srinivas Bangalore and
Owen Rambow. Exploiting a probabilistic hierar-
chical model for generation. In Proceedings of the
17th conference on Computational linguistics, pages
42?48, Morristown, NJ, USA, 2000. Association for
Computational Linguistics.
[Barzilay et al, 1999] R. Barzilay, K. McKeown, and M. El-
haded. Information fusion in the context of multi-
document summarization. In Proceedings of the 37th An-
nual Meeting of the Association for Computational Lin-
guistics (ACL-99), Maryland, 1999.
[Barzilay, 2003] R. Barzilay. Information Fusion for Multi-
document Summarization. Ph.D. Thesis, Columbia Uni-
versity, 2003.
[Bouma et al, 2001] Gosse Bouma, Gertjan van Noord, and
Robert Malouf. Alpino: Wide-coverage computational
analysis of dutch. In Computational Linguistics in The
Netherlands 2000. 2001.
[Callaway and Lester, 1997] C. Callaway and J. Lester. Dy-
namically improving explanations: A revision-based ap-
proach to explanation generation. In Proceedings of the
15th International Joint Conference on Artificial Intelli-
gence (IJCAI 1997), pages 952?958, Nagoya, Japan, 1997.
[Carletta, 1996] Jean Carletta. Assessing agreement on clas-
sification tasks: the kappa statistic. Comput. Linguist.,
22(2):249?254, 1996.
[Chandrasekar and Bangalore, 1997] R. Chandrasekar and
S. Bangalore. Automatic induction of rules for text simpli-
fication. Knowledge-based Systems, 10(3):183?190, 1997.
[Dagan and Glickman, 2004] I. Dagan and O. Glickman.
Probabilistic textual entailment: Generic applied mod-
elling of language variability. In Learning Methods for
Text Understanding and Mining, Grenoble, 2004.
[Gildea, 2003] D. Gildea. Loosely tree-based alignment for
machine translation. In Proceedings of the 41st Annual
Meeting of the ACL, Sapporo, Japan, 2003.
[Imamura, 2001] K. Imamura. Hierarchical phrase align-
ment harmonized with parsing. In Proceedings of the
6th Natural Language Processing Pacific Rim Symposium
(NLPRS 2001), pages 377?384, Tokyo, Japan, 2001.
[Knight and Marcu, 2002] K. Knight and D. Marcu. Sum-
marization beyond sentence extraction: A probabilistic ap-
proach to sentence compression. Artificial Intelligence,
139(1):91?107, 2002.
[Langkilde and Knight, 1998] Irene Langkilde and Kevin
Knight. Generation that exploits corpus-based statistical
knowledge. In Proceedings of the 36th conference on As-
sociation for Computational Linguistics, pages 704?710,
Morristown, NJ, USA, 1998. Association for Computa-
tional Linguistics.
[Lapata, 2003] M. Lapata. Probabilistic text structuring: Ex-
periments with sentence ordering. In Proceedings of the
41st Annual Meeting of the Association for Computational
Linguistics, pages 545?552, Sapporo, 2003.
[Maybury, 2004] M. Maybury. New Directions in Question
Answering. AAAI Press, 2004.
[Meyers et al, 1996] A. Meyers, R. Yangarber, and R. Gr-
isham. Alignment of shared forests for bilingual cor-
pora. In Proceedings of 16th International Conference on
Computational Linguistics (COLING-96), pages 460?465,
Copenhagen, Denmark, 1996.
[Och and Ney, 2000] Franz Josef Och and Hermann Ney.
Statistical machine translation. In EAMT Workshop, pages
39?46, Ljubljana, Slovenia, 2000.
[Pang et al, 2003] Bo Pang, Kevin Knight, and Daniel
Marcu. Syntax-based alignment of multiple translations:
Extracting paraphrases and generating new sentences. In
HLT-NAACL, 2003.
[Reiter and Dale, 2000] E. Reiter and R. Dale. Building Nat-
ural Language Generation Systems. Cambridge University
Press, Cambridge, 2000.
[Robin, 1994] J. Robin. Revision-based generation of Nat-
ural Language Summaries Providing Historical Back-
ground. Ph.D. Thesis, Columbia University, 1994.
[van der Wouden et al, 2002] T. van der Wouden, H. Hoek-
stra, M. Moortgat, B. Renmans, and I. Schuurman. Syntac-
tic analysis in the spoken dutch corpus. In Proceedings of
the third International Conference on Language Resources
and Evaluation, pages 768?773, Las Palmas, Canary Is-
lands, Spain, 2002.
[Vossen, 1998] Piek Vossen, editor. EuroWordNet: a multi-
lingual database with lexical semantic networks. Kluwer
Academic Publishers, Norwell, MA, USA, 1998.
6 Appendix
he
bb
en
ko
m
en
he
bb
en
ik
ik
o
p
in
in
 a
an
ra
ki
ng
m
e
t
zo
co
n
ta
ct
m
e
t
in
 d
e 
lo
op
 v
an
ve
e
l
he
el
pe
rs
oo
n
se
rie
us
ve
e
l
m
a
ss
a
ge
wi
ch
tig
he
el
le
ve
n m
ijn
le
ve
n
he
t
m
a
n
ie
r
di
e
m
e
n
s
Fi
gu
re
3:
D
ep
en
de
nc
y
st
ru
ct
ur
es
an
d
al
ig
nm
en
tf
or
th
e
se
n
te
nc
es
Zo
he
b
ik
in
de
lo
op
va
n
m
ijn
le
ve
n
he
el
ve
el
co
n
ta
ct
en
ge
ha
d
m
et
he
el
ve
el
se
ri
eu
ze
pe
rs
o
n
en
.
(li
t.?
Th
us
ha
v
e
Ii
n
th
e
co
u
rs
e
o
fm
y
lif
e
v
er
y
m
an
y
co
n
ta
ct
s
ha
d
w
ith
v
er
y
m
an
y
se
rio
us
pe
rs
on
s?
)a
n
d
O
p
di
em
a
n
ie
rk
wa
m
ik
in
he
tl
ev
en
m
et
m
a
ss
a
?s
ge
w
ic
ht
ig
e
m
en
se
n
in
a
a
n
ra
ki
ng
.
.
(li
t.
?
In
th
at
w
ay
ca
m
e
Ii
n
th
e
lif
e
w
ith
m
as
s-
o
fw
ei
gh
ty
/im
po
rta
nt
pe
op
le
in
to
uc
h?
).
Th
e
al
ig
nm
en
tr
el
at
io
ns
ar
e
eq
ua
ls
(do
tte
dg
ra
y),
re
st
at
es
(so
lid
gr
ay
),s
pe
ci
fie
s(
do
tte
db
la
ck
),a
n
d
in
te
rs
ec
ts
(da
sh
ed
gr
ay
).
Fo
r
th
e
sa
ke
o
ft
ra
ns
pa
re
nc
y,
de
pe
nd
en
cy
re
la
tio
ns
ha
v
e
be
en
o
m
itt
ed
.
Fi
gu
re
4:
Sc
re
en
sh
ot
o
fG
ad
ge
t,
th
e
to
ol
u
se
d
fo
ra
lig
ni
ng
de
pe
nd
en
cy
st
ru
ct
ur
es
o
fs
en
te
nc
es
.
Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 83?88,
Prague, June 2007. c?2007 Association for Computational Linguistics
Dependency-based paraphrasing for recognizing textual entailment
Erwin Marsi, Emiel Krahmer
Communication & Cognition
Tilburg University
The Netherlands
e.c.marsi@uvt.nl
e.j.krahmer@uvt.nl
Wauter Bosma
Human Media Interaction
University of Twente
The Netherlands
w.e.bosma@ewi.utwente.nl
Abstract
This paper addresses syntax-based para-
phrasing methods for Recognizing Textual
Entailment (RTE). In particular, we de-
scribe a dependency-based paraphrasing al-
gorithm, using the DIRT data set, and its
application in the context of a straightfor-
ward RTE system based on aligning depen-
dency trees. We find a small positive effect
of dependency-based paraphrasing on both
the RTE3 development and test sets, but the
added value of this type of paraphrasing de-
serves further analysis.
1 Introduction
Coping with paraphrases appears to be an essential
subtask in Recognizing Textual Entailment (RTE).
Most RTE systems incorporate some form of lex-
ical paraphrasing, usually relying on WordNet to
identify synonym, hypernym and hyponym rela-
tions among word pairs from text and hypothesis
(Bar-Haim et al, 2006, Table 2). Many systems
also address paraphrasing above the lexical level.
This can take the form of identifying or substitut-
ing equivalent multi-word strings, e.g., (Bosma and
Callison-Burch, 2006). A drawback of this approach
is that it is hard to cope with discontinuous para-
phrases containing one or more gaps. Other ap-
proaches exploit syntactic knowledge in the form
of parse trees. Hand-crafted transformation rules
can account for systematic syntactic alternation like
active-passive form, e.g., (Marsi et al, 2006). Al-
ternatively, such paraphrase rules may be automati-
cally derived from huge text corpora (Lin and Pan-
tel, 2001). There are at least two key advantages of
syntax-based over string-based paraphrasing which
are relevant for RTE: (1) it can cope with discontin-
uous paraphrases; (2) syntactic information such as
dominance relations, phrasal syntactic labels and de-
pendency relations, can be used to refine the coarse
matching on words only.
Here we investigate paraphrasing on the basis of
of syntactic dependency analyses. Our sole resource
is the DIRT data set (Lin and Pantel, 2001), an exten-
sive collection of automatically derived paraphrases.
These have been used for RTE before (de Salvo Braz
et al, 2005; Raina et al, 2005), and similar ap-
proaches to paraphrase mining have been applied
as well (Nielsen et al, 2006; Hickl et al, 2006).
However, in these approaches paraphrasing is al-
ways one factor in a complex system, and as a result
little is known of the contribution of paraphrasing
for the RTE task. In this paper, we focus entirely
on dependency-based paraphrasing in order to get a
better understanding of its usefulness for RTE. In the
next Section, we describe the DIRT data and present
an algorithm for dependency-based paraphrasing in
order to bring a pair?s text closer to its hypothesis.
We present statistics on coverage as well as qual-
itative discussion of the results. Section 3 then de-
scribes our RTE system and results with and without
dependency-based paraphrasing.
2 Dependency-based paraphrasing
2.1 Preprocessing RTE data
Starting from the text-hypothesis pairs in the RTE
XML format, we first preprocess the data. As the
text part may consist of more than one sentence,
we first perform sentence splitting using Mxtermi-
nator (Reynar and Ratnaparkhi, 1997), a maximum
83
entropy-based end of sentence classifier trained on
the Penn Treebank data. Next, each sentence is to-
kenized and syntactically parsed using the Minipar
parser (Lin, 1998). From the parser?s tabular output
we extract the word forms, lemmas, part-of-speech
tags and dependency relations. This information is
then stored in an ad-hoc XML format which repre-
sents the trees as an hierarchy of node elements in
order to facilitate tree matching.
2.2 DIRT data
The DIRT (Discovering Inference Rules from Text)
method is based on extending Harris Distributional
Hypothesis, which states that words that occurred in
the same contexts tend to be similar, to dependency
paths in parse trees (Lin and Pantel, 2001). Each
dependency path consists of at least three nodes: a
root node, and two non-root terminal nodes, which
are nouns. The DIRT data set we used consists of
over 182k paraphrase clusters derived from 1GB of
newspaper text. Each cluster consists of a unique
dependency path, which we will call the paraphrase
source, and a list of equivalent dependency paths,
which we will refer to as the paraphrase transla-
tions, ordered in decreasing value of point-wise mu-
tual information. A small sample in the original for-
mat is
(N:by:V<buy>V:obj:N (sims
N:to:V<sell>V:obj:N 0.211704
N:subj:V<buy>V:obj:N 0.198728
...
))
The first two lines represent the inference rule: X
bought by Y entails X sold to Y.
We preprocess the DIRT data by restoring prepo-
sitions, which were originally folded into a depen-
dency relation, to individual nodes, as this eases
alignment with the parsed RTE data. For the same
reason, paths are converted to the same ad-hoc XML
format as the parsed RTE data.
2.3 Paraphrase substitution
Conceptually, our paraphrase substitution algorithm
takes a straightforward approach. For the purpose of
explanation only, Figure 1 presents pseudo-code for
a naive implementation. The main function takes
two arguments (cf. line 1). The first is a prepro-
cessed RTE data set in which all sentences from text
and hypothesis are dependency parsed. The second
is a collection of DIRT paraphrases, each one map-
ping a source path to one or more translation paths.
For each text/hypothesis pair (cf. line 2), we look
at all the subtrees of the text parses (cf. line 3-4)
and attempt to find a suitable paraphrase of this sub-
tree (cf. line 5). We search the DIRT paraphrases
(cf. line 8) for a source path that matches the text
subtree at hand (cf. line 9). If found, we check
if any of the corresponding paraphrase translation
paths (cf. line 10) matches a subtree of the hypoth-
esis parse (cf. line 11-12). If so, we modify the
text tree by substituting this translation path (cf. line
13). The intuition behind this is that we only accept
paraphrases that bring the text closer to the hypothe-
sis. The DIRT paraphrases are ordered in decreasing
likelihood, so after a successful paraphrase substitu-
tion, we discard the remaining possibilities and con-
tinue with the next text subtree (cf. line 14).
The Match function, which is used for matching
the source path to a text subtree and the translation
path to an hypothesis subtree, requires the path to
occur in the subtree. That is, all lemmas, part-of-
speech tags and dependency relations from the path
must have identical counterparts in the subtree; skip-
ping nodes is not allowed. As the path?s terminals
specify no lemma, the only requirement is that their
counterparts are nouns.
The Substitute function replaces the matched path
in the text tree by the paraphrase?s translation path.
Intuitively, the path ?overlays? a part of the sub-
tree, changing lemmas and dependency relations,
but leaving most of the daughter nodes unaffected.
Note that the new path may be longer or shorter than
the original one, thus introducing or removing nodes
from the text tree.
As an example, we will trace our algorithm as ap-
plied to the first pair of the RTE3 dev set (id=1).
Text: The sale was made to pay Yukos? US$ 27.5 billion tax
bill, Yuganskneftegaz was originally sold for US$ 9.4 bil-
lion to a little known company Baikalfinansgroup which
was later bought by the Russian state-owned oil company
Rosneft.
Hypothesis: Baikalfinansgroup was sold to Rosneft.
Entailment: Yes
While traversing the parse tree of the text, our
algorithm encounters a node with POS tag V and
lemma buy. The relevant part of the parse tree is
shown at the right top of Figure 2. The logical argu-
ments inferred by Minipar are shown between curly
84
(1) def Paraphrase(parsed-rte-data, dirt-paraphrases):
(2) for pair in parsed-rte-data:
(3) for text-tree in pair.text-parses:
(4) for text-subtree in text-tree:
(5) Paraphrase-subtree(text-subtree, dirt-paraphrases, pair.hyp-parse)
(6)
(7) def Paraphrase-subtree(text-subtree, dirt-paraphrases, hyp-tree):
(8) for (source-path, translations) in dirt-paraphrases:
(9) if Match(source-path, text-subtree):
(10) for trans-path in translations:
(11) for hyp-subtree in hyp-tree:
(12) if Match(trans-path, hyp-subtree):
(13) text-subtree = Substitute(trans-path, text-subtree)
(14) return
Figure 1: Pseudo-code for a naive implementation of the dependency-based paraphrase substitution algo-
rithm
brackets, e.g., US$ 9.4 billion. For this combination
of verb and lemma, the DIRT data contains 340 para-
phrase sets, with a total of 26950 paraphrases. The
algorithm starts searching for a paraphrase source
which matches the text. It finds the path shown
at the left top of Figure 2: buy with a PP modi-
fier headed by preposition by, and a nominal object.
This paraphrase source has 108 alternative transla-
tions. It searches for paraphrase translations which
match the hypothesis. The first, and therefore most
likely (probability is 0.22) path it finds is rooted in
sell, with a PP-modifier headed by to and a nominal
object. This translation path, as well as its alignment
to the hypothesis parse tree, is shown in the mid-
dle part of Figure 2. Finally, the source path in the
text tree is substituted by the translation path. The
bottom part of Figure 2 shows the updated text tree
as well as its improved alignment to the hypothesis
tree. The paraphrasing procedure can in effect be
viewed as making the inference that Baikalfinans-
group was bought by Rosneft, therefore Baikalfi-
nansgroup was sold to Rosneft.
The naive implementation of the algorithm is of
course not very efficient. Our actual implementa-
tion uses a number of shortcuts to reduce process-
ing time. For instance, the DIRT paraphrases are
indexed on the lemma of their root in order to speed
up retrieval. As another example, text nodes with
less than two child nodes (i.e. terminal and unary-
branching nodes) are immediately skipped, as they
will never match a paraphrase path.
2.4 Paraphrasing results
We applied our paraphrasing algorithm to the RTE3
development set. Table 1 gives an impression of how
many paraphrases were substituted. The first row
lists the total number of nodes in the dependency
trees of the text parts. The second row shows that
for roughly 15% of these nodes, the DIRT data con-
tains a paraphrase with the same lemma. The next
two rows show in how many cases the source path
matches the text and the translation path matches the
hypothesis (i.e. giving rise to a paraphrase substitu-
tion). Clearly, the number of actual paraphrase sub-
stitutions is relatively small: on average about 0.5%
of all text subtrees are subject to paraphrasing. Still,
about one in six sentences is subject to paraphras-
ing, and close to half of all pairs is paraphrased at
least once. Sentences triggering more than one para-
phrase do occur. Also note that paraphrasing occurs
more frequently in true entailment pairs than in false
entailment pairs. This is to be expected, given that
text and hypothesis are more similar when an entail-
ment relation holds.
2.5 Discussion on paraphrasing
Type of paraphrases A substantial number of the
paraphrases applied are single word synonyms or
verb plus particle combinations which might as well
be obtained from string-based substitution on the ba-
sis of a lexical resource like WordNet. Some ran-
domly chosen examples include X announces Y en-
tails X supports Y, X makes Y entails X sells Y, and
locates X at Y, discovers X at Y. Nevertheless, more
interesting paraphrases do occur. In the pair below
(id=452), we find the paraphrase X wins Y entails X
85
Table 1: Frequency of (partial) paraphrase matches on the RTE3 dev set
IE: IR: QA: SUM: Total:
Text nodes: 8899 10610 10502 8196 38207
Matching paraphrase lemma: 1439 1724 1581 1429 6173
Matching paraphrase source: 566 584 543 518 2211
Matching paraphrase translation: 71 55 23 79 228
Text sentences: 272 350 306 229 1157
Paraphrased text sentences: 63 51 20 66 200
Paraphrased true-entailment pairs: 32 25 12 39 108
Paraphrased false-entailment pairs: 26 21 5 23 75
(is) Y champion.
Text: Boris Becker is a true legend in the sport of tennis. Aged
just seventeen, he won Wimbledon for the first time and
went on to become the most prolific tennis player.
Hypothesis: Boris Becker is a Wimbledon champion.
Entailment: True
Another intriguing paraphrase, which appears to be
false on first sight, is X flies from Y entails X makes
(a) flight to Y. However, in the context of the next
pair (id=777), it turns out to be correct.
Text: The Hercules transporter plane which flew straight here
from the first round of the trip in Pakistan, touched down
and it was just a brisk 100m stroll to the handshakes.
Hypothesis: The Hercules transporter plane made a flight to
Pakistan.
Entailment: True
Coverage Although the DIRT data constitutes a
relatively large collection of paraphrases, it is clear
that many paraphrases required for the RTE3 data
are missing. We tried to improve coverage to some
extent by relaxing the Match function: instead of
an exact match, we allowed for small mismatches
in POS tag and dependency relation, reversing the
order of a path?s left and right side, and even for
skipping nodes. However, subjective evaluation sug-
gested that the results deteriorated. Alternatively,
the coverage might be increased by deducing para-
phrases on the fly using the web as a corpus, e.g.,
(Hickl et al, 2006).
Somewhat surprisingly, the vast majority of para-
phrases concerns verbs. Even though the DIRT data
contains paraphrases for nouns, adjectives and com-
plementizers, the coverage of these word classes is
apparently not nearly as extensive as that of verbs.
Another observation is that fewer paraphrases oc-
cur in pairs from the QA task. We have no explana-
tion for this.
False paraphrases Since the DIRT data was au-
tomatically derived and was not manually checked,
it contains noise in the form of questionable or even
false paraphrases. While some of these surface in
paraphrased RTE3 data (e.g. X leaves for Y entails
X departs Y, and X feeds Y entails Y feeds X), their
number appears to be limited. We conjecture this is
because of the double constraint that a paraphrase
must match both text and hypothesis.
Relevance Not all paraphrase substitutions are rel-
evant for the purpose of recognizing textual entail-
ment. Evidently, paraphrases in false entailment
pairs are counterproductive. However, even in true
entailment pairs paraphrases might occur in parts
of the text that are irrelevant to the task at hand.
Consider the following pair from the RTE3 dev set
(id=417).
Text: When comparing Michele Granger and Brian Goodell,
Brian has to be the clear winner. In 1976, while still a
student at Mission Viejo High, Brian won two Olympic
gold medals at Montreal, breaking his own world records
in both the 400 - and 1,500 - meter freestyle events. He
went on to win three gold medals in he 1979 Pan Ameri-
can Games.
Hypothesis: Brian Goodell won three gold medals in the 1979
Pan American Games.
Entailment: True
The second text sentence and hypothesis match
the paraphrases: (1) X medal at Y entails X medal in
Y, and (2) X record in Y entails X medal in Y. Even
so, virtually all of the important information is in the
third text sentence.
3 Results on RTE3 data
Since our contribution focuses on syntactic para-
phrasing, our RTE3 system is a simplified version
86
Table 2: Percent accuracy on RTE3 set without
paraphrasing (?) and with paraphrasing (+)
Task Dev? Dev+ Test? Test+
IE 59.5 61.0 53.0 53.5
IR 67.0 68.0 58.5 61.5
QA 76.0 76.5 69.0 68.0
SUM 66.0 67.5 53.0 53.5
Overall 66.9 68.2 58.6 59.1
of our RTE2 system as described in (ref supressed
for blind reviewing) The core of the system is still
the tree alignment algorithm from (Meyers et al,
1996), but without normalization of node weights
and applied to Minipar instead of Maltparser out-
put. To keep things simple, we do not apply syntac-
tic normalization, nor do we use WordNet or other
resources to improve node matching. Instead, we
simply align each text tree to the corresponding hy-
pothesis tree and calculate the coverage, which is
defined as the proportion of aligned content words
in the hypothesis. If the coverage is above a task-
specific threshold, we say entailment is true, other-
wise it is false.
The results are summarized in Table 2. Overall
results on the test set are considerably worse than
on the development set, which is most likely due to
overfitting task-specific parameters for node match-
ing and coverage. Our main interest is to what extent
dependency-based paraphrasing improves our base-
line prediction. The improvement on the develop-
ment set is more than 1%. This is reduced to 0.5%
in the case of the test set.
Our preliminary results indicate a small positive
effect of dependency-based paraphrasing on the re-
sults of our RTE system. Unlike most earlier work,
we did not add resources other than Minipar depen-
dency trees and DIRT paraphrase trees, in order to
isolate the contribution of syntactic paraphrases to
RTE. Nevertheless, our RTE3 system may be im-
proved by using WordNet or other lexical resources
to improve node matching, both in the paraphrasing
step and in the tree-alignment step. In future work,
we hope to improve both the paraphrasing method
(along the lines discussed in Section 2.5) and the
RTE system itself.
Acknowledgments We would like to thank Dekang Lin and
Patrick Pantel for allowing us to use the DIRT data. This work
was jointly conducted within the DAESO project funded by the
Stevin program (De Nederlandse Taalunie) and the IMOGEN
project funded by the Netherlands Organization for Scientific
Research (NWO).
References
R. Bar-Haim, I. Dagan, B. Dolan, L. Ferro, D. Giampic-
colo, B. Magnini, and I. Szpektor. 2006. The second
pascal recognising textual entailment challenge. In
Proceedings of the Second PASCAL Challenges Work-
shop on Recognising Textual Entailment, pages 1?9,
Venice, Italy.
W. Bosma and C. Callison-Burch. 2006. Paraphrase sub-
stitution for recognizing textual entailment. In Pro-
ceedings of CLEF.
R. de Salvo Braz, R. Girju, V. Punyakanok, D. Roth, and
M. Sammons. 2005. An inference model for seman-
tic entailemnt in natural language. In Proceedings of
the First Pascal Challenge Workshop on Recognizing
Textual Entailment, pages 29?32.
A. Hickl, J. Williams, J. Bensley, K. Roberts, B. Rink,
and Y. Shi. 2006. Recognizing textual entailment
with lccs groundhog system. In Proceedings of the
Second PASCAL Challenges Workshop on Recognis-
ing Textual Entailment, pages 80?85, Venice, Italy.
Dekang Lin and Patrick Pantel. 2001. Discovery of infer-
ence rules for question answering. Natural Language
Engineering, 7(4):343?360.
Dekang Lin. 1998. Dependency-based evaluation of
minipar. In Proceedings of the Workshop on Evalua-
tion of Parsing Systems at LREC 1998, pages 317?330,
Granada, Spain.
E. Marsi, E. Krahmer, W. Bosma, and M. Theune. 2006.
Normalized alignment of dependency trees for detect-
ing textual entailment. In Proceedings of the Second
PASCAL Challenges Workshop on Recognising Tex-
tual Entailment, pages 56?61, venice, Italy.
Adam Meyers, Roman Yangarber, and Ralph Grisham.
1996. Alignment of shared forests for bilingual cor-
pora. In Proceedings of 16th International Conference
on Computational Linguistics (COLING-96), pages
460?465, Copenhagen, Denmark.
R. Nielsen, W. Ward, and J.H. Martin. 2006. Toward
dependency path based entailment. In Proceedings of
the Second PASCAL Challenges Workshop on Recog-
nising Textual Entailment, pages 44?49, Venice, Italy.
R. Raina, A. Haghighi, C. Cox, J. Finkel, J. Michels,
K. Toutanova, B. MacCartney, M.C. deMarneffe, C.D.
Manning, and A.Y. Ng. 2005. Robust textual infer-
ence using diverse knowledge sources. In Proceedings
of PASCAL Recognising Textual Entailment Workshop.
J. C. Reynar and A. Ratnaparkhi. 1997. A maximum en-
tropy approach to identifying sentence boundaries. In
Proceedings of the Fifth Conference on Applied Natu-
ral Language Processing, Washington, D.C.
87
buy
by
mod
...
obj
buy
...
pcomp-n
by
Rosneft
{US$ 9.4 billion}Baikalfinansgroup
s mod obj
{Baikalfinansgroup}
subj
known
mod
company
nn
{fin}
rel
which
whn
be
i
later
pred
{which}
subj
pcomp-n
the
det
Russian
mod
state-owned
mod
oil company
nn
state
lex-mod
-
lex-mod
oil
lex-mod
sell
to
mod
...
obj
sell
...
pcomp-n
to
Rosneft
{Baikalfinansgroup}Baikalfinansgroup
s
be
be mod obj
pcomp-n
sell
Baikalfinansgroup
s
to
mod
{US$ 9.4 billion}
obj
{Baikalfinansgroup}
subj
known
mod
company
nn
{fin}
rel
which
whn
be
i
later
pred
{which}
subj
Rosneft
pcomp-n
the
det
Russian
mod
state-owned
mod
oil company
nn
state
lex-mod
-
lex-mod
oil
lex-mod
sell
Baikalfinansgroup
s
be
be
to
mod
{Baikalfinansgroup}
obj
Rosneft
pcomp-n
Figure 2: Alignment of paraphrase source to text (top), alignment of paraphrase translation to hypothesis
(mid), and alignment of hypothesis to paraphrased text (bottom) for pair 1 from RTE3 dev set
88
Proceedings of the 12th European Workshop on Natural Language Generation, pages 25?32,
Athens, Greece, 30 ? 31 March 2009. c?2009 Association for Computational Linguistics
Is sentence compression an NLG task?
Erwin Marsi, Emiel Krahmer
Tilburg University
Tilburg, The Netherlands
e.j.krahmer@uvt.nl
e.c.marsi@uvt.nl
Iris Hendrickx, Walter Daelemans
Antwerp University
Antwerpen, Belgium
iris.hendrickx@ua.ac.be
walter.daelemans@ua.ac.be
Abstract
Data-driven approaches to sentence com-
pression define the task as dropping any
subset of words from the input sentence
while retaining important information and
grammaticality. We show that only 16%
of the observed compressed sentences in
the domain of subtitling can be accounted
for in this way. We argue that part of this
is due to evaluation issues and estimate
that a deletion model is in fact compat-
ible with approximately 55% of the ob-
served data. We analyse the remaining
problems and conclude that in those cases
word order changes and paraphrasing are
crucial, and argue for more elaborate sen-
tence compression models which build on
NLG work.
1 Introduction
The task of sentence compression (or sentence re-
duction) can be defined as summarizing a single
sentence by removing information from it (Jing
and McKeown, 2000). The compressed sentence
should retain the most important information and
remain grammatical. One of the applications
is in automatic summarization in order to com-
press sentences extracted for the summary (Lin,
2003; Jing and McKeown, 2000). Other appli-
cations include automatic subtitling (Vandeghin-
ste and Tsjong Kim Sang, 2004; Vandeghinste and
Pan, 2004; Daelemans et al, 2004) and displaying
text on devices with very small screens (Corston-
Oliver, 2001).
A more restricted version defines sentence
compression as dropping any subset of words
from the input sentence while retaining impor-
tant information and grammaticality (Knight and
Marcu, 2002). This formulation of the task pro-
vided the basis for the noisy-channel en decision-
tree based algorithms presented in (Knight and
Marcu, 2002), and for virtually all follow-up work
on data-driven sentence compression (Le and
Horiguchi, 2003; Vandeghinste and Pan, 2004;
Turner and Charniak, 2005; Clarke and Lapata,
2006; Zajic et al, 2007; Clarke and Lapata, 2008)
It makes two important assumptions: (1) only
word deletions are allowed ? no substitutions or
insertions ? and therefore no paraphrases; (2) the
word order is fixed. In other words, the com-
pressed sentence must be a subsequence of the
source sentence. We will call this the subsequence
constraint, and refer to the corresponding com-
pression models as word deletion models. Another
implicit assumption in most work is that the scope
of sentence compression is limited to isolated sen-
tences and that the textual context is irrelevant.
Under this definition, sentence compression is
reduced to a word deletion task. Although one
may argue that even this counts as a form of
text-to-text generation, and consequently an NLG
task, the generation component is virtually non-
existent. One can thus seriously doubt whether it
really is an NLG task.
Things would become more interesting from an
NLG perspective if we could show that sentence
compression necessarily involves transformations
beyond mere deletion of words, and that this re-
quires linguistic knowledge and resources typical
to NLG. The aim of this paper is therefore to chal-
lenge the deletion model and the underlying subse-
quence constraint. To use an analogy, our aim is to
show that sentence compression is less like carv-
ing something out of wood - where material can
only be removed - and more like molding some-
thing out of clay - where the material can be thor-
25
oughly reshaped. In support of this claim we pro-
vide evidence that the coverage of deletion models
is in fact rather limited and that word reordering
and paraphrasing play an important role.
The remainder of this paper is structured as fol-
lows. In Section 2, we introduce our text material
which comes from the domain of subtitling. We
explain why not all material is equally well suited
for studying sentence compression and motivate
why we disregard certain parts of the data. We
also describe the manual alignment procedure and
the derivation of edit operations from it. In Sec-
tion 3, an analysis of the number of deletions, in-
sertions, substitutions, and reorderings in our data
is presented. We determine how many of the com-
pressed sentences actually satisfy the subsequence
constraint, and how many of them could in prin-
ciple be accounted for. That is, we consider al-
ternatives with the same compression ratio which
do not violate the subsequence constraint. Next
is an analysis of the remaining problematic cases
in which violation of the subsequence constraint
is crucial to accomplish the observed compression
ratio. We single out (1) reordering after deletion
and (2) paraphrasing as important factors. Given
the importance of paraphrases, Section 3.4 dis-
cusses the perspectives for automatic extraction of
paraphrase pairs from large text corpora, and tries
to estimate how much text is required to obtain a
reasonable coverage. We finish with a summary
and discussion in Section 4.
2 Material
We study sentence compression in the context of
subtitling. The basic problem of subtitling is that
on average reading takes more time than listen-
ing, so subtitles can not be a verbatim transcrip-
tion of the speech without increasingly lagging be-
hind. Subtitles can be presented at a rate of 690
to 780 characters per minute, while the average
speech rate is considerably higher (Vandeghinste
and Tsjong Kim Sang, 2004). Subtitles are there-
fore often a compressed representation of the orig-
inal spoken text.
Our text material stems from the NOS Journaal,
the daily news broadcast of the Dutch public tele-
vision. It is parallel text with on one side the au-
tocue sentences (aut), i.e. the text the news reader
is reading, and on the other side the corresponding
subtitle sentences (sub). It was originally collected
and processed in two earlier research projects ?
Atranos and Musa ? on automatic subtitling (Van-
deghinste and Tsjong Kim Sang, 2004; Vandegh-
inste and Pan, 2004; Daelemans et al, 2004). All
text was automatically tokenized and aligned at
the sentence level, after which alignments were
manually checked.
The same material was further annotated in an
ongoing project called DAESO1, in which the gen-
eral goal is automatic detection of semantic over-
lap. All aligned sentences were first syntactically
parsed after which their parse trees were manually
aligned in more detail. Pairs of similar syntactic
nodes ? either words or phrases ? were aligned and
labeled according to a set of five semantic similar-
ity relations (Marsi and Krahmer, 2007). For cur-
rent purposes, only the alignment at the word level
is used, ignoring phrasal alignments and relation
labels.
Not all material in this corpus is equally well
suited for studying sentence compression as de-
fined in the introduction. As we will discuss in
more detail below, this prompted us to disregard
certain parts of the data.
Sentence deletion, splitting and merging For a
start, autocue and subtitle sentences are often not
in a one-to-one alignment relation. Table 1 speci-
fies the alignment degree (i.e. the number of other
sentences that a sentence is aligned to) for autocue
and subtitle sentences. The first thing to notice
is that there is a large number of unaligned sub-
titles. These correspond to non-anchor text from,
e.g., interviews or reporters abroad. More inter-
esting is that about one in five autocue sentences
is completely dropped. A small number of about
4 to 8 percent of the sentence pairs are not one-
to-one aligned. A long autocue sentence may be
split into several simpler subtitle sentences, each
containing only a part of the semantic content of
the autocue sentence. Conversely, one or more -
usually short - autocue sentences may be merged
into a single subtitle sentence.
These decisions of sentence deletion, splitting
and merging are worthy research topics in the con-
text of automatic subtitling, but they should not
be confused with sentence compression, the scope
of which is by definition limited to single sen-
tence. Accordingly we disregarded all sentence
pairs where autocue and subtitle are not in a one-
to-one relation with each other. This reduced the
data set from 15289 to 11034 sentence pairs.
1http://daeso.uvt.nl
26
Degree: Autocue: (%) Subtitle: (%)
0 3607 (20.74) 12542 (46.75)
1 12382 (71.19) 13340 (49.72)
2 1313 (7.55) 901 (3.36)
3 83 (0.48) 41 (0.15)
4 8 (0.05) 6 (0.02)
Table 1: Degree of sentence alignment
Word compression A significant part of the re-
duction in subtitle characters is actually not ob-
tained by deleting words but by lexical substitution
of a shorter token. Examples of this include sub-
stitution by digits (?7? for ?seven?), abbreviations
or acronyms (?US? for ?United States?), symbols
(euro symbol for ?Euro?), or reductions of com-
pound words (?elections? for ?state-elections?).
We will call this word compression. Although an
important part of subtitling, we prefer to abstract
from word compression and focus here on sen-
tence compression proper. Removing all sentence
pairs containing a word compression has the dis-
advantage of further reducing the data set. Instead
we choose to measure compression ratio (CR) in
terms of tokens2 rather than characters.
CR =
#toksub
#tokaut
(1)
This means that the majority of the word com-
pressions do not affect the sentence CR.
Variability in compression ratio The CR of
subtitles is not constant, but varies depending
(mainly) on the amount of provided autocue ma-
terial in a given time frame. The histogram in
Figure 1 shows the distribution of the CR (mea-
sured in words) for one-to-one aligned sentences.
In fact, autocue sentences are most likely not to
be compressed at all (thus belonging to the largest
bin, from 1.00 to 1.09 in the histogram).3 In order
to obtain a proper set of compression examples,
we retained only those sentence pairs where the
compression ratio is less than one.
Parsing failures As mentioned earlier detailed
alignment of autocue and subtitle sentences was
carried out on their syntactic trees. However,
for various reasons a small number of sentences
(0.2%) failed to pass the parser and received no
parse tree. As a consequence, their trees could not
2Throughout this study we ignore punctuation and letter
case.
3Some instances even show a CR larger than one, because
occasionally there is sufficient time/space to provide a clari-
fication, disambiguation, update, or stylistic enhancement.
Figure 1: Histogram of compression ratio
Min: Max: Sum: Mean: SD:
aut-tokens 2 43 80651 15.41 5.48
sub-tokens 1 29 53691 10.26 3.72
CR 0.07 0.96 nan 0.69 0.17
Table 2: Properties of the final data set of
5233 pairs of autocue-subtitle sentences: mini-
mum value, maximal value, total sum, mean and
standard deviation for number of tokens per au-
tocue/subtitle sentence and Compression Ratio
be aligned and there is no alignment at the word
level available either. Variability in CR and pars-
ing failures are together responsible for a further
reduction down to 5233 sentence pairs, the final
size of our data set, with an overall CR of 0.69.
Other properties of this data set are summarized in
Table 2.4
Word deletions, insertions and substitutions
Having a manual alignment of similar words in
both sentences allows us to simply deduce word
deletions, substitutions and insertions, as well as
word order changes, in the following way:
? if an autocue word is not aligned to a subtitle
word, then it is was deleted
? if a subtitle word is not aligned to an autocue
word, then it was inserted
? if different autocue and subtitle words are
aligned, then the former was substituted by
the latter
? if alignments cross each other, then the word
order was changed
The remaining option is where the aligned
words are identical (ignoring differences in case).
4We use the acronym nan (?not a number?) for unde-
fined/meaningless values.
27
Without the word alignment, we would have
to resort to automatically calculating the edit dis-
tance, i.e. the sum of the minimal number of
insertions, deletions and substitutions required to
transform one sentence in to the other. However,
this would result in different and often counter-
intuitive sequences of edit operations. Our ap-
proach clearly distinguishes word order changes
from the edit operations; the conventional edit dis-
tance, by contrast, can only account for changes
in word order by sequences of the edit operations.
Another difference is that substitution can also be
accomplished as deletion followed by insertion,
which means edit operations need to have an as-
sociated weight. Global tuning of these weights
turns out to be hard.
3 Analysis
3.1 Edit operations
The observed deletions, insertions, substitutions,
edit distances, and word order changes are shown
in Table 3. As expected, deletion is the most fre-
quent operation, with on average seven deletions
per sentence. Insertion and substitutions are far
less frequent. Note also that ? even though the task
is compression ? insertions are somewhat more
frequent than substitutions. Word order changes
occur in 1688 cases (32.26%). Here, reordering is
a binary variable ? i.e. the word order is changed
or not ? hence Min, Max and SD are undefined.
Another point of view is to look at the number
of sentence pairs containing a certain edit oper-
ation. Here we find 5233 pairs (100.00%) with
deletion, 2738 (52.32%) with substitution, 3263
(62.35%) with insertion, and 1688 (32.26%) with
reordering.
The average CR for subsequences is 0.68
(SD = 0.20) versus 0.69 (SD = 0.17)
for non-subsequences. A detailed inspection of
the relation between the subsequence/non ?
subsequence ratio and CR revealed no clear cor-
relation, so we did not find indications that non-
subsequences occur more frequently at higher
compression ratios.
3.2 Percentage of subsequences
The subtitle is a subsequence of the autocue if
there are no insertions, no substitutions, and no
word order changes. In contrast, if any of these
do occur, the subtitle is not a subsequence. It turns
Min: Max: Sum: Mean: SD:
del 1 34 34728 6.64 4.57
sub 0 6 4116 0.79 0.94
ins 0 17 7768 1.48 1.78
dist 1 46 46612 8.91 5.78
reorder nan nan 1688 0.32 nan
Table 3: Observed word deletions, insertions, sub-
stitutions, and edit distances
out that only 843 (16.11%) subtitles are a subse-
quence, which is rather low.
At first sight, this appears to be bad news for
any deletion model, as it seems to imply that the
model cannot account for close to 84% the ob-
served data. However, the important thing to keep
in mind is that compression of a given sentence
is a problem for which there are usually multiple
solutions (Belz and Reiter, 2006). This is exactly
what makes it so hard to perform automatic evalu-
ation of NLG systems. There may very well exist
semantically equivalent alternatives with the same
CR which do satisfy the subsequence constraint.
For this reason, a substantial part of the observed
non-subsequences may have subsequence counter-
parts which can be accounted for by a deletion
model. The question is: how many?
In order to address this question, we took a
random sample of 200 non-subsequence sentence
pairs. In each case we tried to come up with
an alternative subsequence subtitle with the same
meaning and the same CR (or when opportune,
even a lower CR). Table 4 shows the distribu-
tion of the difference in tokens between the orig-
inal non-subsequence subtitle and the manually-
constructed equivalent subsequence subtitle. Ap-
parently 95 out of 200 (47%) subsequence sub-
titles have the same (or even fewer) tokens, and
thus the same (or an even lower) compression ra-
tio. This suggests that the subsequence constraint
is not as problematic as it seemed and that the cov-
erage of a deletion model is in fact far better than
it appeared to be. Recall that 16% of the original
subtitles were already subsequences, so our anal-
ysis suggests that a deletion model is compatible
with 55% (16% plus 47% of 84%).
3.3 Problematic non-subsequences
Another result of this exercise in rewriting sub-
titles is that it allows us to identify those cases
where the attempt to create a proper subse-
quence fails. In (1), we show one representa-
tive example of a problematic subtitle, for which
28
(1) Aut de
the
bron
source
was
was
een
a
geriatrische
geriatric
patient
patient
die
who
zonder
without
het
it
zelf
self
te
to
merken
notice
uitzonderlijk
exceptionally
veel
many
larven
larvae
bij
with
zich
him
bleek
appeared
te
to
dragen
carry
en
and
een
a
grote
large
verspreiding
spreading
veroorzaakte
caused
?the source was a geriatric patient who unknowingly carried exceptionally many larvae and caused a wide spreading?
Sub een
a
geriatrische
geriatric
patient
patient
met
with
larven
larvae
heeft
has
de
the
verspreiding
spreading
veroorzaakt
caused
Seq de bron was een geriatrische patient die veel larven bij zich bleek te dragen en een verspreiding veroorzaakte
(2) Aut in
in
verband
relation
met
to
de
the
lawineramp
avalanche-disaster
in
in
galu?r
Galtu?r
hebben
have
de
the
politieke
political
partijen
parties
in
in
tirol
Tirol
gezamenlijk
together
besloten
decided
de
the
verkiezingscampagne
election-campaign
voor
for
het
the
regionale
regional
parlement
parliament
op
up
te
to
schorten
postpone
Sub de
the
politieke
political
partijen
parties
in
in
tirol
Tirol
hebben
have
besloten
decided
de
the
verkiezingen
elections
op
up
te
to
schorten
postpone
?Political parties in Tirol have decided to postpone the elections?
(3) Aut velen
many
van
of
hen
them
worden
are
door
by
de
the
servie?rs
Serbs
in
in
volgeladen
crammed
treinen
trains
gedeporteerd
deported
Sub vluchtelingen
refugees
worden
are
per
by
trein
train
gedeporteerd
deported
token-diff: count: (%:)
-2 4 2.00
-1 18 9.00
0 73 36.50
1 42 21.00
2 32 16.00
3 11 5.50
4 9 4.50
5 5 2.50
7 2 1.00
8 2 1.00
9 1 0.50
11 1 0.50
Table 4: Distribution of difference in tokens
between original non-subsequence subtitle and
equivalent subsequence subtitle
the best equivalent subsequence we could ob-
tain still has nine more tokens than the origi-
nal non-subsequence. These problematic non-
subsequences reveal where insertion, substitution
and/or word reordering are essential to obtain a
subtitle with a sufficient CR (i.e. the CR observed
in the real subtitles). At least three different types
of phenomena were observed.
Word order In some cases deletion of a con-
stituent necessitates a change in word order to ob-
tain a grammatical sentence. In example (2), the
autocue sentence has the PP modifier in verband
met de lawineramp in galu?r in its topic position
(first sentence position). Deleting this modifier, as
is done in the subtitle, results in a sentence that
starts with the verb hebben, which is interpreted as
a yes-no question. For a declarative interpretation,
we have to move the subject de politieke partijen
to the first position, as in the subtitle. Incidentally,
this indicates that it is instructive to apply sentence
compression models to multiple languages, as a
word order problem like this never arises in En-
glish.
Similar problems arise whenever an embedded
clause is promoted to a main clause, which re-
quires a change in the position of the finite verb
in Dutch. In total, a word order problem occurred
in 24 out 200 sentences.
Referring expressions Referring expressions
are on many occasions replaced by a shorter
one ? usually a little less precise. For
example, de belgische overheid ?the Belgian
authorities? is replaced by belgie ?Belgium?.
Extreme cases of this occur where a long
NP like deze tweede impeachment-procedure
in de amerikaanse geschiedenis ?this second
impeachment-procedure in the American history?
is replaced by an anaphor like het ?it?.
Since a referring expression or anaphor must be
appropriate in the given context, substitutions like
these transcend the domain of a single sentence
and require taking the preceding textual context
into account. This is especially clear in exam-
ples like (3) in which ?many of them? is replaced
the ?refugees?. It is questionable whether these
types of substitutions belong to the task of sen-
tence compression. We prefer to regard it as one of
the additional tasks in automatic subtitling, apart
from compression. Incidentally, it is interesting
that the challenge of generating referring expres-
sions is also relevant for automatic subtitling.
29
Paraphrasing Apart from the reduced referring
expressions, there are nominal paraphrases reduc-
ing a noun phrases like medewerkers van banken
?employees of banks? to a compound word like
bankmedewerkers ?bank-employees?. Likewise,
there are adverbial paraphrases such as sinds een
paar jaar ?since a few years? to tegenwoordig
?nowadays?, and van de afgelopen tijd ?of the past
time? to recent ?recent?. However, the majority of
the paraphrasing concerns verbs as in the two ex-
amples below.
(4) Aut X
X
neemt
takes
het
the
initiatief
initiative
tot
to
oprichting
raising
van
of
Y
Y
Sub X
X
zet
sets
Y
Y
op
up
(5) Aut X
X
om
for
zijn
his
uitlevering
extradition
vroeg
asked
maar
but
Y
Y
die
that
weigerde
refused
Sub Y
Y
hem
him
niet
not
wilde
wanted
uitleveren
extradite
aan
to
X
Y
?Y refused to extradite him to Y?
Even though not all paraphrases are actually
shorter, it seems that at least some of them boost
compression beyond what can be accomplished
with only word deletion. In the next Section, we
look at the possibilities of automatic extraction of
such paraphrases.
3.4 Perspectives for automatic paraphrase
extraction
There is a growing amount of work on automatic
extraction of paraphrases from text corpora (Lin
and Pantel, 2001; Barzilay and Lee, 2003; Ibrahim
et al, 2003; Dolan et al, 2004). One general pre-
requisite for learning a particular paraphrase pat-
tern is that it must occur in the text corpus with a
sufficiently high frequency, otherwise the chances
of learning the pattern are proportionally small. In
this section, we investigate to what extent the para-
phrases encountered in our random sample of 200
pairs can be retrieved from a reasonably large text
corpus.
In a first step, we manually extracted 106
paraphrase patterns. We filtered these pat-
terns and excluded anaphoric expressions, general
verb alternation patterns like active/passive and
continuous/non-continuous, as well as verbal pat-
terns involving more than two slots. After this fil-
tering step, 59 pairs of paraphrases remained, in-
cluding the examples shown in the preceding Sec-
tion.
The aim was to estimate how big our corpus
has to be to cover the majority of these para-
phrase pairs. We started with counting for each
of the paraphrase pairs in our sample how often
they occur in a corpus of Dutch news texts, the
Twente News Corpus5, which contains approxi-
mately 325M tokens and 20M sentences. We em-
ployed regular expressions to count the number of
paraphrase pattern matches. The corpus turned out
to contain 70% percent of all paraphrase pairs (i.e.
both patterns in the pair occur at least once). We
also counted how many pairs have a frequencies of
at least 10 and 100. To study the effect of corpus
size on the percentage of covered paraphrases, we
performed these counts on 1, 2, 5, 10, 25, 50 and
100% of the corpus. Figure 2 shows the percent-
age of covered paraphrases dependent on the cor-
pus size. The most strict threshold that only counts
pairs that occur at least 100 times in our corpus,
does not retrieve any counts on 1% of the corpus
(3M words). At 10% of the corpus size only 4%
of the paraphases is found, and on the full data set
25% of the pairs is found.
For 51% percent of the patterns (with a fre-
quency of at least 10) we find substantial evidence
in our corpus of 325M tokens. We fitted a curve
through our data points, and found a logarithmic
line fit with adjusted R2 value of .943. This sug-
gests that in order to get 75% of the patterns, we
would need a corpus that is 18 times bigger than
our current one, which amounts to roughly 6 bil-
lion words. Although this seems like a lot of text,
using the WWW as our corpus would easily give
us these numbers. Today?s estimate of the Index
Dutch World Wide Web is 688 million pages6. If
we assume that each page contains at least 100 to-
kens on average, this implies a corpus size of 68
billion tokens.
The patterns used here are word-based and in
many cases they express a particular verb tense or
verb form (e.g. 3rd person singular), and word
order. This implies that our estimations are the
minimum number of matches one can find. For
more abstract matching, we would need syntacti-
cally parsed data (Lin and Pantel, 2001). We ex-
pect that this would also positively affect the cov-
erage.
5http://www.vf.utwente.nl/?druid/TwNC/
TwNC-main.html
6http://www.worldwidewebsize.com/
index.php?lang=NL, as measured in December
2008
30
Figure 2: Percentage of covered paraphrases as a
function of the corpus size
4 Discussion
We found that only 16.11% of 5233 subtitle sen-
tences were proper subsequences of the corre-
sponding autocue sentence, and therefore 84% can
not be accounted for by a deletion model. One
consequence appears to be that the subsequence
constraint greatly reduces the amount of avail-
able training material for any word deletion model.
However, an attempt to rewrite non-subsequences
to semantically equivalent sequences with the
same CR suggests that a deletion model could in
principle be adequate for 55% of the data. More-
over, in those cases where an application can toler-
ate a little slack in the CR, a deletion model might
be sufficient. For instance, if we are willing to tol-
erate up to two more tokens, we can account for as
much as 169 (84%) of the 200 non-subsequences
in our sample, which amounts to 87% (16% plus
84% of 84%) of the total data.
It should be noted that we have been very strict
regarding what counts as a semantically equiva-
lent subtitle: every piece of information occurring
in the non-subsequence subtitle must reoccur in
the sequence subtitle. However, looking at our
original data, it is clear that considerable liberty
is taken as far as conserving semantic content is
concerned: subtitles often drop substantial pieces
of information. If we relax the notion of seman-
tic equivalence a little, an even larger part of the
non-subsequences can be rewritten as proper se-
quences.
The remaining problematic non-subsequences
are those where insertion, substitution and/or word
reordering are essential to obtain a sufficient CR.
One of the issues we identified is that deletion
of certain constituents must be accompanied by a
change in word order to prevent an ungrammati-
cal sentence. Since changes in word order appear
to require grammatical modeling or knowledge,
this brings sentence compression closer to being
an NLG task.
Nguyen and Horiguchi (2003) describe an ex-
tension of the decision tree-based compression
model (Knight and Marcu, 2002) which allows for
word order changes. The key to their approach
is that dropped constituents are temporarily stored
on a deletion stack, from which they can later be
re-inserted in the tree where required. Although
this provides an unlimited freedom for rearranging
constituents, it also complicates the task of learn-
ing the parsing steps, which might explain why
their evaluation results show marginal improve-
ments at best.
In our data, most of the word order changes ap-
pear to be minor though, often only moving the
verb to second position after deleting a constituent
in the topic position. We believe that unrestricted
word order changes are perhaps not necessary and
that the vast majority of the word order problems
can be solved by a fairly restricted way of reorder-
ing. In particular, we plan to implement a parser-
based model with an additional swap operation
that swaps the two topmost items on the stack. We
expect that this is more feasible as a learning task
than a model with a deletion stack.
Apart from reordering, other problems for word
deletion models are the insertions and substitu-
tions as a result of paraphrasing. Within a deci-
sion tree-based model, paraphrasing of words or
continuous phrases may be modeled by a combi-
nation of a paraphrase lexicon and an extra opera-
tion which replaces the n topmost elements on the
stack by the corresponding paraphrase. However,
paraphrases involving variable arguments, as typ-
ical for verbal paraphrases, cannot be accounted
for in this way. More powerful compression mod-
els may draw on existing NLG methods for text
revision (Inui et al, 1992) to accommodate full
paraphrasing.
We also looked at the perspectives for auto-
matic paraphrase extraction from large text cor-
pora. About a quarter of the required paraphrase
patterns was found at least a hundred times in our
corpus of 325M tokens. Extrapolation suggests
that using the web at its current size would give us
a coverage of approximately ten counts for three
31
quarters of the paraphrases.
Incidentally, we identified two other tasks in
automatic subtitling which are closely related to
NLG. First, splitting and merging of sentences
(Jing and McKeown, 2000), which seems related
to content planning and aggregation. Second, gen-
eration of a shorter referring expression or an
anaphoric expression, which is currently one of
the main themes in data-driven NLG.
In conclusion, we have presented evidence that
deletion models for sentence compression are not
sufficient, and that more elaborate models in-
volving reordering and paraphrasing are required,
which puts sentence compression in the field of
NLG.
Acknowledgments
We would like to thank Nienke Eckhardt, Paul van Pelt, Han-
neke Schoormans and Jurry de Vos for the corpus annota-
tion work, and Erik Tsjong Kim Sang and colleagues for the
autocue-subtitle material from the ATRANOS project, and
Martijn Goudbeek for help with curve fitting. This work was
conducted within the DAESO project funded by the Stevin
program (De Nederlandse Taalunie).
References
Regina Barzilay and Lillian Lee. 2003. Learning to
paraphrase: an unsupervised approach using multiple-
sequence alignment. In Proceedings of the 2003 Confer-
ence of the North American Chapter of the Association for
Computational Linguistics on Human Language Technol-
ogy, pages 16?23, Morristown, NJ, USA.
Anja Belz and Ehud Reiter. 2006. Comparing automatic and
human evaluation of NLG systems. In Proceedings of the
11th Conference of the European Chapter of the Associa-
tion for Computational Linguistics, pages 313?320.
James Clarke and Mirella Lapata. 2006. Models for sentence
compression: a comparison across domains, training re-
quirements and evaluation measures. In Proceedings of
the 21st International Conference on Computational Lin-
guistics and the 44th annual meeting of the Association for
Computational Linguistics, pages 377?384, Morristown,
NJ, USA.
James Clarke and Mirella Lapata. 2008. Global inference
for sentence compression an integer linear programming
approach. Journal of Artificial Intelligence Research,
31:399?429.
Simon Corston-Oliver. 2001. Text compaction for display
on very small screens. In Proceedings of the Workshop
on Automatic Summarization (WAS 2001), pages 89?98,
Pittsburgh, PA, USA.
Walter Daelemans, Anita Ho?thker, and Erik Tjong Kim Sang.
2004. Automatic sentence simplification for subtitling in
Dutch and English. In Proceedings of the 4th Interna-
tional Conference on Language Resources and Evalua-
tion, pages 1045?1048.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004. Unsuper-
vised construction of large paraphrase corpora: Exploiting
massively parallel news sources. In Proceedings of the
20th International Conference on Computational Linguis-
tics, pages 350?356, Morristown, NJ, USA.
Ali Ibrahim, Boris Katz, and Jimmy Lin. 2003. Extract-
ing structural paraphrases from aligned monolingual cor-
pora. In Proceedings of the 2nd International Work-
shop on Paraphrasing, volume 16, pages 57?64, Sapporo,
Japan.
Kentaro Inui, Takenobu Tokunaga, and Hozumi Tanaka.
1992. Text Revision: A Model and Its Implementation.
In Proceedings of the 6th International Workshop on Nat-
ural Language Generation: Aspects of Automated Natural
Language Generation, pages 215?230. Springer-Verlag
London, UK.
Hongyan Jing and Kathleen McKeown. 2000. Cut and paste
based text summarization. In Proceedings of the 1st Con-
ference of the North American Chapter of the Association
for Computational Linguistics, pages 178?185, San Fran-
cisco, CA, USA.
Kevin Knight and Daniel Marcu. 2002. Summarization be-
yond sentence extraction: A probabilistic approach to sen-
tence compression. Artificial Intelligence, 139(1):91?107.
Nguyen Minh Le and Susumu Horiguchi. 2003. A New Sen-
tence Reduction based on Decision Tree Model. In Pro-
ceedings of the 17th Pacific Asia Conference on Language,
Information and Computation, pages 290?297.
Dekang Lin and Patrick Pantel. 2001. Discovery of infer-
ence rules for question answering. Natural Language En-
gineering, 7(4):343?360.
Chin-Yew Lin. 2003. Improving summarization perfor-
mance by sentence compression - A pilot study. In Pro-
ceedings of the Sixth International Workshop on Informa-
tion Retrieval with Asian Languages, volume 2003, pages
1?9.
Erwin Marsi and Emiel Krahmer. 2007. Annotating a par-
allel monolingual treebank with semantic similarity re-
lations. In Proceedings of the 6th International Work-
shop on Treebanks and Linguistic Theories, pages 85?96,
Bergen, Norway.
Jenine Turner and Eugene Charniak. 2005. Supervised and
unsupervised learning for sentence compression. In Pro-
ceedings of the 43rd Annual Meeting of the Association
for Computational Linguistics, pages 290?297, Ann Ar-
bor, Michigan, June.
Vincent Vandeghinste and Yi Pan. 2004. Sentence com-
pression for automated subtitling: A hybrid approach. In
Proceedings of the ACL Workshop on Text Summarization,
pages 89?95.
Vincent Vandeghinste and Erik Tsjong Kim Sang. 2004.
Using a Parallel Transcript/Subtitle Corpus for Sentence
Compression. In Proceedings of LREC 2004.
David Zajic, Bonnie J. Dorr, Jimmy Lin, and Richard
Schwartz. 2007. Multi-candidate reduction: Sentence
compression as a tool for document summarization tasks.
Information Processing Management, 43(6):1549?1570.
32
Proceedings of the 12th European Workshop on Natural Language Generation, pages 122?125,
Athens, Greece, 30 ? 31 March 2009. c?2009 Association for Computational Linguistics
Clustering and Matching Headlines for Automatic Paraphrase Acquisition
Sander Wubben, Antal van den Bosch, Emiel Krahmer, Erwin Marsi
Tilburg centre for Creative Computing
Tilburg University
The Netherlands
{s.wubben,antal.vdnbosch,e.j.krahmer,e.c.marsi}@uvt.nl
Abstract
For developing a data-driven text rewriting
algorithm for paraphrasing, it is essential
to have a monolingual corpus of aligned
paraphrased sentences. News article head-
lines are a rich source of paraphrases; they
tend to describe the same event in vari-
ous different ways, and can easily be ob-
tained from the web. We compare two
methods of aligning headlines to construct
such an aligned corpus of paraphrases, one
based on clustering, and the other on pair-
wise similarity-based matching. We show
that the latter performs best on the task of
aligning paraphrastic headlines.
1 Introduction
In recent years, text-to-text generation has re-
ceived increasing attention in the field of Nat-
ural Language Generation (NLG). In contrast
to traditional concept-to-text systems, text-to-text
generation systems convert source text to target
text, where typically the source and target text
share the same meaning to some extent. Ap-
plications of text-to-text generation include sum-
marization (Knight and Marcu, 2002), question-
answering (Lin and Pantel, 2001), and machine
translation.
For text-to-text generation it is important to
know which words and phrases are semantically
close or exchangable in which contexts. While
there are various resources available that capture
such knowledge at the word level (e.g., synset
knowledge in WordNet), this kind of information
is much harder to get by at the phrase level. There-
fore, paraphrase acquisition can be considered an
important technology for producing resources for
text-to-text generation. Paraphrase generation has
already proven to be valuable for Question An-
swering (Lin and Pantel, 2001; Riezler et al,
2007), Machine Translation (Callison-Burch et al,
2006) and the evaluation thereof (Russo-Lassner
et al, 2006; Kauchak and Barzilay, 2006; Zhou et
al., 2006), but also for text simplification and ex-
planation.
In the study described in this paper, we make
an effort to collect Dutch paraphrases from news
article headlines in an unsupervised way to be
used in future paraphrase generation. News ar-
ticle headlines are abundant on the web, and
are already grouped by news aggregators such as
Google News. These services collect multiple arti-
cles covering the same event. Crawling such news
aggregators is an effective way of collecting re-
lated articles which can straightforwardly be used
for the acquisition of paraphrases (Dolan et al,
2004; Nelken and Shieber, 2006). We use this
method to collect a large amount of aligned para-
phrases in an automatic fashion.
2 Method
We aim to build a high-quality paraphrase corpus.
Considering the fact that this corpus will be the ba-
sic resource of a paraphrase generation system, we
need it to be as free of errors as possible, because
errors will propagate throughout the system. This
implies that we focus on obtaining a high precision
in the paraphrases collection process. Where pre-
vious work has focused on aligning news-items at
the paragraph and sentence level (Barzilay and El-
hadad, 2003), we choose to focus on aligning the
headlines of news articles. We think this approach
will enable us to harvest reliable training material
for paraphrase generation quickly and efficiently,
without having to worry too much about the prob-
lems that arise when trying to align complete news
articles.
For the development of our system we use
data which was obtained in the DAESO-project.
This project is an ongoing effort to build a Par-
allel Monolingual Treebank for Dutch (Marsi
122
Placenta sandwich? No, urban legend!
Tom wants to make movie with Katie
Kate?s dad not happy with Tom Cruise
Cruise and Holmes sign for eighteen million
Eighteen million for Tom and Katie
Newest mission Tom Cruise not very convincing
Latest mission Tom Cruise succeeds less well
Tom Cruise barely succeeds with MI:3
Tom Cruise: How weird is he?
How weird is Tom Cruise really?
Tom Cruise leaves family
Tom Cruise escapes changing diapers
Table 1: Part of a sample headline cluster, with
sub-clusters
and Krahmer, 2007) and will be made available
through the Dutch HLT Agency. Part of the data
in the DAESO-corpus consists of headline clusters
crawled from Google News Netherlands in the pe-
riod April?August 2006. For each news article,
the headline and the first 150 characters of the ar-
ticle were stored. Roughly 13,000 clusters were
retrieved. Table 1 shows part of a (translated) clus-
ter. It is clear that although clusters deal roughly
with one subject, the headlines can represent quite
a different perspective on the content of the arti-
cle. To obtain only paraphrase pairs, the clusters
need to be more coherent. To that end 865 clus-
ters were manually subdivided into sub-clusters of
headlines that show clear semantic overlap. Sub-
clustering is no trivial task, however. Some sen-
tences are very clearly paraphrases, but consider
for instance the last two sentences in the example.
They do paraphrase each other to some extent, but
their relation can only be understood properly with
world knowledge. Also, there are numerous head-
lines that can not be sub-clustered, such as the first
three headlines shown in the example.
We use these annotated clusters as development
and test data in developing a method to automat-
ically obtain paraphrase pairs from headline clus-
ters. We divide the annotated headline clusters in a
development set of 40 clusters, while the remain-
der is used as test data. The headlines are stemmed
using the porter stemmer for Dutch (Kraaij and
Pohlmann, 1994).
Instead of a word overlap measure as used by
Barzilay and Elhadad (2003), we use a modified
TF ?IDF word score as was suggested by Nelken
and Shieber (2006). Each sentence is viewed as a
document, and each original cluster as a collection
of documents. For each stemmed word i in sen-
tence j, TFi,j is a binary variable indicating if the
word occurs in the sentence or not. The TF ?IDF
score is then:
TF.IDFi = TFi,j ? log
|D|
|{dj : ti ? dj}|
|D| is the total number of sentences in the clus-
ter and |{dj : ti ? dj}| is the number of sen-
tences that contain the term ti. These scores are
used in a vector space representation. The similar-
ity between headlines can be calculated by using
a similarity function on the headline vectors, such
as cosine similarity.
2.1 Clustering
Our first approach is to use a clustering algorithm
to cluster similar headlines. The original Google
News headline clusters are reclustered into finer
grained sub-clusters. We use the k-means imple-
mentation in the CLUTO1 software package. The
k-means algorithm is an algorithm that assigns
k centers to represent the clustering of n points
(k < n) in a vector space. The total intra-cluster
variances is minimized by the function
V =
k?
i=1
?
xj?Si
(xj ? ?i)
2
where ?i is the centroid of all the points xj ? Si.
The PK1 cluster-stopping algorithm as pro-
posed by Pedersen and Kulkarni (2006) is used to
find the optimal k for each sub-cluster:
PK1(k) =
Cr(k)?mean(Cr[1...?K])
std(Cr[1...?K])
Here, Cr is a criterion function, which mea-
sures the ratio of withincluster similarity to be-
tweencluster similarity. As soon as PK1(k) ex-
ceeds a threshold, k?1 is selected as the optimum
number of clusters.
To find the optimal threshold value for cluster-
stopping, optimization is performed on the devel-
opment data. Our optimization function is an F -
score:
F? =
(1 + ?2) ? (precision ? recall)
(?2 ? precision + recall)
1http://glaros.dtc.umn.edu/gkhome/views/cluto/
123
We evaluate the number of aligments between pos-
sible paraphrases. For instance, in a cluster of four
sentences,
(4
2
)
= 6 alignments can be made. In
our case, precision is the number of alignments
retrieved from the clusters which are relevant, di-
vided by the total number of retrieved alignments.
Recall is the number of relevant retrieved alig-
ments divided by the total number of relevant
alignments.
We use an F?-score with a ? of 0.25 as we
favour precision over recall. We do not want to op-
timize on precision alone, because we still want to
retrieve a fair amount of paraphrases and not only
the ones that are very similar. Through optimiza-
tion on our development set, we find an optimal
threshold for the PK1 algorithm thpk1 = 1. For
each original cluster, k-means clustering is then
performed using the k found by the cluster stop-
ping function. In each newly obtained cluster all
headlines can be aligned to each other.
2.2 Pairwise similarity
Our second approach is to calculate the similarity
between pairs of headlines directly. If the similar-
ity exceeds a certain threshold, the pair is accepted
as a paraphrase pair. If it is below the thresh-
old, it is rejected. However, as Barzilay and El-
hadad (2003) have pointed out, sentence mapping
in this way is only effective to a certain extent.
Beyond that point, context is needed. With this
in mind, we adopt two thresholds and the Cosine
similarity function to calculate the similarity be-
tween two sentences:
cos(?) =
V 1 ? V 2
?V 1??V 2?
where V 1 and V 2 are the vectors of the two sen-
tences being compared. If the similarity is higher
than the upper threshold, it is accepted. If it is
lower than the lower theshold, it is rejected. In
the remaining case of a similarity between the two
thresholds, similarity is calculated over the con-
texts of the two headlines, namely the text snippet
that was retrieved with the headline. If this simi-
larity exceeds the upper threshold, it is accepted.
Threshold values as found by optimizing on the
development data using again an F0.25-score, are
Thlower = 0.2 and Thupper = 0.5. An optional
final step is to add alignments that are implied by
previous alignments. For instance, if headlineA is
paired with headline B, and headline B is aligned
to headline C, headline A can be aligned to C as
Type Precision Recall
k-means clustering 0.91 0.43
clusters only
k-means clustering 0.66 0.44
all headlines
pairwise similarity 0.93 0.39
clusters only
pairwise similarity 0.76 0.41
all headlines
Table 2: Precision and Recall for both methods
Playstation 3 more expensive than
competitor
Playstation 3 will become more
expensive than Xbox 360
Sony postpones Blu-Ray movies
Sony postpones coming of blu-ray dvds
Prices Playstation 3 known: from 499 euros
E3 2006: Playstation 3 from 499 euros
Sony PS3 with Blu-Ray for sale from
November 11th
PS3 available in Europe from
November 17th
Table 3: Examples of correct (above) and incorrect
(below) alignments
well. We do not add these alignments, because in
particular in large clusters when one wrong align-
ment is made, this process chains together a large
amount of incorrect alignments.
3 Results
The 825 clusters in the test set contain 1,751 sub-
clusters in total. In these sub-clusters, there are
6,685 clustered headlines. Another 3,123 head-
lines remain unclustered. Table 2 displays the
paraphrase detection precision and recall of our
two approaches. It is clear that k-means cluster-
ing performs well when all unclustered headlines
are artificially ignored. In the more realistic case
when there are also items that cannot be clustered,
the pairwise calculation of similarity with a back
off strategy of using context performs better when
we aim for higher precision. Some examples of
correct and incorrect alignments are given in Ta-
ble 3.
124
4 Discussion
Using headlines of news articles clustered by
Google News, and finding good paraphrases
within these clusters is an effective route for ob-
taining pairs of paraphrased sentences with rea-
sonable precision. We have shown that a cosine
similarity function comparing headlines and us-
ing a back off strategy to compare context can be
used to extract paraphrase pairs at a precision of
0.76. Although we could aim for a higher preci-
sion by assigning higher values to the thresholds,
we still want some recall and variation in our para-
phrases. Of course the coverage of our method is
still somewhat limited: only paraphrases that have
some words in common will be extracted. This
is not a bad thing: we are particularly interested
in extracting paraphrase patterns at the constituent
level. These alignments can be made with existing
alignment tools such as the GIZA++ toolkit.
We measure the performance of our approaches
by comparing to human annotation of sub-
clusterings. The human task in itself is hard. For
instance, is we look at the incorrect examples in
Table 3, the difficulty of distinguishing between
paraphrases and non-paraphrases is apparent. In
future research we would like to investigate the
task of judging paraphrases. The next step we
would like to take towards automatic paraphrase
generation, is to identify the differences between
paraphrases at the constituent level. This task has
in fact been performed by human annotators in the
DAESO-project. A logical next step would be to
learn to align the different constituents on our ex-
tracted paraphrases in an unsupervised way.
Acknowledgements
Thanks are due to the Netherlands Organization
for Scientific Research (NWO) and to the Dutch
HLT Stevin programme. Thanks also to Wauter
Bosma for originally mining the headlines from
Google News. For more information on DAESO,
please visit daeso.uvt.nl.
References
Regina Barzilay and Noemie Elhadad. 2003. Sentence
alignment for monolingual comparable corpora. In
Proceedings of the 2003 conference on Empirical
methods in natural language processing, pages 25?
32.
Chris Callison-Burch, Philipp Koehn, and Miles Os-
borne. 2006. Improved statistical machine transla-
tion using paraphrases. In Proceedings of the main
conference on Human Language Technology Con-
ference of the North American Chapter of the Asso-
ciation of Computational Linguistics, pages 17?24.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004.
Unsupervised construction of large paraphrase cor-
pora: exploiting massively parallel news sources. In
COLING ?04: Proceedings of the 20th international
conference on Computational Linguistics, page 350.
David Kauchak and Regina Barzilay. 2006. Para-
phrasing for automatic evaluation. In Proceedings
of the Human Language Technology Conference of
the NAACL, Main Conference, pages 455?462, June.
Kevin Knight and Daniel Marcu. 2002. Summa-
rization beyond sentence extraction: a probabilis-
tic approach to sentence compression. Artif. Intell.,
139(1):91?107.
Wessel Kraaij and Rene Pohlmann. 1994. Porters
stemming algorithm for dutch. In Informatieweten-
schap 1994: Wetenschappelijke bijdragen aan de
derde STINFON Conferentie, pages 167?180.
Dekang Lin and Patrick Pantel. 2001. Dirt: Discov-
ery of inference rules from text. In KDD ?01: Pro-
ceedings of the seventh ACM SIGKDD international
conference on Knowledge discovery and data min-
ing, pages 323?328.
Erwin Marsi and Emiel Krahmer. 2007. Annotating
a parallel monolingual treebank with semantic sim-
ilarity relations. In he Sixth International Workshop
on Treebanks and Linguistic Theories (TLT?07).
Rani Nelken and Stuart M. Shieber. 2006. Towards ro-
bust context-sensitive sentence alignment for mono-
lingual corpora. In Proceedings of the 11th Confer-
ence of the European Chapter of the Association for
Computational Linguistics (EACL-06), 3?7 April.
Ted Pedersen and Anagha Kulkarni. 2006. Automatic
cluster stopping with criterion functions and the gap
statistic. In Proceedings of the 2006 Conference
of the North American Chapter of the Association
for Computational Linguistics on Human Language
Technology, pages 276?279.
Stefan Riezler, Alexander Vasserman, Ioannis
Tsochantaridis, Vibhu O. Mittal, and Yi Liu. 2007.
Statistical machine translation for query expansion
in answer retrieval. In ACL.
Grazia Russo-Lassner, Jimmy Lin, and Philip Resnik.
2006. A paraphrase-based approach to machine
translation evaluation. Technical report, University
of Maryland, College Park.
Liang Zhou, Chin-Yew Lin, and Eduard Hovy. 2006.
Re-evaluating machine translation results with para-
phrase support. In Proceedings of the 2006 Con-
ference on Empirical Methods in Natural Language
Processing, pages 77?84, July.
125
Proceedings of the 12th European Workshop on Natural Language Generation, pages 183?184,
Athens, Greece, 30 ? 31 March 2009. c?2009 Association for Computational Linguistics
Realizing the Costs: Template-Based Surface Realisation in the GRAPH
Approach to Referring Expression Generation
Ivo Brugman
University of Twente
The Netherlands
i.h.g.brugman@student.utwente.nl
Marie?t Theune
University of Twente
The Netherlands
m.theune@utwente.nl
Emiel Krahmer
Tilburg University
The Netherlands
e.j.krahmer@uvt.nl
Jette Viethen
Macquarie University
Australia
jviethen@ics.mq.edu.au
Abstract
We describe a new realiser developed for
the TUNA 2009 Challenge, and present its
evaluation scores on the development set,
showing a clear increase in performance
compared to last year?s simple realiser.
1 Introduction
The TUNA Challenge 2009 is the last in a series
of challenges using the TUNA corpus of refer-
ring expressions (Gatt et al 2007) for compara-
tive evaluation of referring expression generation.
The 2009 Challenge is aimed at end-to-end re-
ferring expression generation, which encompasses
two subtasks: (1) attribute selection, choosing a
number of attributes that uniquely characterize a
target object, distinguishing it from other objects
in a visual scene, and (2) realisation, converting
the selected set of attributes into a word string.
Our contributions to the previous Challenges fo-
cused on subtask (1), but this year we focus on
subtask (2). Below, we briefly sketch how attribute
selection is performed in our system, describe our
newly developed realiser, and present our evalua-
tion results on the TUNA 2009 development set.
2 Attribute selection
We use the Graph-based algorithm of Krahmer
et al (2003) for attribute selection. In this ap-
proach, objects and their attributes are represented
in a graph as nodes and edges respectively, and
attribute selection is seen as a graph search prob-
lem that outputs the cheapest distinguishing graph,
given a particular cost function that assigns costs
to attributes. By assigning zero costs to some at-
tributes, e.g., the type of an object, the human
tendency to mention redundant properties can be
mimicked. For the TUNA Challenge 2009 we
use the same settings as last year (Krahmer et al
2008). The used cost function assigns a zero cost
to attributes that are highly frequent in the TUNA
corpus, while the other attributes have a cost of
either 1 (somewhat infrequent) or 2 (very infre-
quent). The order in which attributes are added
is also controlled: to ensure that the cheapest at-
tributes are added first, they are tried in the order
of their frequency in the TUNA (2008) training
corpus. Using these settings, last year the GRAPH
attribute selection algorithm made the top 3 on all
evaluation measures (Gatt et al 2008, Table 11).
3 Realisation
The main resource for realisation is a set of tem-
plates, derived from the human-produced object
descriptions in the TUNA 2009 training data. To
construct the templates, we first grouped the de-
scriptions by the combination of attributes they
expressed. For instance, in the domain of furni-
ture references, all descriptions expressing the at-
tributes colour, type and orientation were grouped
together. This was done for all combinations of
attributes. Next, for each description, parts of the
word string were related to the attributes in the set.
For instance, for the string ?red couch facing left?,
we linked ?red? to colour, ?couch? to type, and
?facing left? to orientation.1 This provided us with
information on how the attributes were expressed
(e.g., by adjectives or prepositional phrases) and
in which order they appeared in the word string.
For each combination of attributes, the surface or-
der that occurred most frequently was selected as
the basis for a template. If multiple orderings
were equally frequent, we chose the most natural-
seeming one. This resulted in templates such as
?the [colour] [type] facing [orientation]? for the at-
tribute set {type, colour, orientation}.
During realisation, the templates are used as fol-
1This corresponds to the ANNOTATED-WORD-STRING
nodes already present in the TUNA corpus. Unfortunately,
various problems prevented us from automatically deriving
our templates from those existing annotations.
183
lows. When a set of attributes is input to the re-
aliser, it checks if there is a template matching this
particular attribute combination. If so, the tem-
plate is selected, and the gaps in the template are
filled with lexical expressions for the attribute val-
ues. The words used to express the values are
those that occurred most frequently in the train-
ing data for this particular template. If no match-
ing template is found, a description is generated
in a simple rule-based fashion, based on the re-
aliser we used last year, but with improved lexical
choices. For example, the old realiser always used
the word ?person? to express the type attribute in
descriptions of people, whereas in the TUNA cor-
pus ?man? is used most frequently. We changed
the realiser to reflect such human preferences.
Template construction for the furniture domain
was fairly straightforward, resulting in 25 tem-
plates. In practice, only 13 of these are used. Since
the GRAPH attribute selection algorithm adds the
type and colour attributes to a description for free,
these attributes are always selected, making any
templates lacking them irrelevant given the current
settings of the algorithm.
For the more realistic people domain, template
construction was more complicated. For exam-
ple, when the hairColour attribute is mentioned in
human descriptions it can refer either to the hair
on a person?s head (?white-haired?) or his beard
(?with a white beard?). The attribute selection al-
gorithm does not make this distinction, leaving it
unclear which of the two realisations should be
used when hairColour and hasBeard attributes are
both to be included in a description. We solved
this by simply using the expression that occurred
most frequently in the training data for each at-
tribute combination, even allowing hairColour to
be mentioned twice if this happened in most hu-
man descriptions. Another problem is that many
attribute combinations occurred only once in the
training data, leading to a very large number (50+)
of potential templates. We reduced this number in
an ad hoc manner, by ignoring combinations in-
volving attributes (such as hasHair) that are very
unlikely to be selected given the current settings
of the attribute selection algorithm. This approach
left us with 40 templates in the people domain.
4 Evaluation
System performance is measured by comparing
the generated word strings to the human descrip-
MED MNED BLEU 3
Furniture 4.94 (5.48) 0.48 (0.50) 0.27 (0.22)
People 5.15 (7.53) 0.46 (0.67) 0.33 (0.07)
Overall 5.03 (6.42) 0.47 (0.58) 0.30 (0.15)
Table 1: Results on the 2009 development set (be-
tween brackets are those using last year?s realiser).
tions in the TUNA development set, comprising
80 furniture and 68 people descriptions. The eval-
uation measures reported here are mean edit dis-
tance (MED), the mean of the token-based Lev-
enshtein edit distance between the reference word
strings and the system word strings, mean nor-
malised edit distance (MNED), where the edit dis-
tance is normalised by the number of tokens, and
cumulative BLEU 3 score. Table 1 summarizes
our evaluation results. For comparison, we also
provide the results obtained when using last year?s
simple realiser, which we reimplemented in Java.
We see a clear improvement when we compare
the performance of the new and the old realiser, in
particular in the people domain. However, further
evaluation experiments are required to determine
whether the improvements are mostly due to our
use of templates derived from human descriptions,
or to the simple improvements in lexical choice
incorporated in the rules used as fall-back in case
no matching templates are found.
To further improve the realiser, we need to add
templates for all remaining attribute combinations
found in the corpus. This should not be difficult,
as the set-up of the realiser allows easy creation of
templates. It should also be easily portable to other
languages; in fact we intend to explore its use for
the realisation of referring expressions in Dutch.
References
Gatt, A., I. van der Sluis and K. van Deemter 2007.
Evaluating algorithms for the generation of referring
expressions using a balanced corpus. Proceedings of
ENLG 2007 49-56.
Gatt, A., A. Belz and E. Kow 2008. The TUNA chal-
lenge 2008: Overview and evaluation results Pro-
ceedings of INLG 2008 198-206.
Krahmer, E., S. van Erk and A. Verleg 2003. Graph-
based generation of referring expressions. Compu-
tational Linguistics, 29(1), 53-72.
Krahmer, E., M. Theune, J. Viethen, and I. Hendrickx
2008. GRAPH: The costs of redundancy in referring
expressions. Proceedings of INLG 2008 227-229.
184
Proceedings of the 2009 Workshop on Language Generation and Summarisation, ACL-IJCNLP 2009, pages 63?66,
Suntec, Singapore, 6 August 2009.
c?2009 ACL and AFNLP
Reducing redundancy in multi-document summarization
using lexical semantic similarity
Iris Hendrickx, Walter Daelemans
University of Antwerp
Antwerpen, Belgium
iris.hendrickx@ua.ac.be
walter.daelemans@ua.ac.be
Erwin Marsi, Emiel Krahmer
Tilburg University
Tilburg, The Netherlands
e.j.krahmer@uvt.nl
e.c.marsi@uvt.nl
Abstract
We present an automatic multi-document
summarization system for Dutch based on
the MEAD system. We focus on redun-
dancy detection, an essential ingredient of
multi-document summarization. We in-
troduce a semantic overlap detection tool,
which goes beyond simple string match-
ing. Our results so far do not confirm
our expectation that this tool would out-
perform the other tested methods.
1 Introduction
One of the main issues in automatic multi-
document summarization is avoiding redundancy.
As the source documents are all related to the
same topic, at least some of their content is likely
to overlap. In fact, this is in part what makes
multi-document summarization feasible. For ex-
ample, news articles that report on a particular
event, or that are based on the same source, often
contain similar information expressed in differ-
ent ways. A multi-document summarizer should
include this overlapping information not more
than once. The backbone of most current ap-
proaches to automatic summarization is a vector
space model in which a sentence is regarded as
a bag of words and a weighted cosine similarity
measure is used to quantify the amount of shared
information between a pair of sentences. Cosine
similarity (in this context) essentially amounts to
calculating word overlap, albeit with weighting of
the terms and normalization for differences in sen-
tence length. It is clear that this approach to detect-
ing redundancy is far from satisfactory, because it
only covers redundancy in its most trivial form,
i.e., identical words. In contrast, the redundancy
that we ultimately want to avoid in summarization
is that at the semantic level. As an extreme case
in point, two sentences with no words in common
can still carry virtually the same meaning.
The remainder of this paper is structured in
the following way. In Section 2 we introduce a
tool for detecting semantic overlap. In section 3
we present a Dutch multi-document summariza-
tion system, based on the MEAD summarization
toolkit (Radev et al, 2004). Next, in section 4 we
describe the experimental setup and the data set
that we used. Section 5 reports on the results, and
we conclude in section 6.
2 Detecting semantic overlap
In this section, we detail the semantic overlap de-
tection tool and the resources we build on.
Parallel/comparable text corpus The basis for
our semantic overlap detection tool is a mono-
lingual parallel/comparable tree-bank of 1 million
words of Dutch text (Marsi and Krahmer, 2007).
Half of the text material has so far been manually
aligned at the sentence level. Subsequently, the
sentences have been parsed and the resulting parse
trees have been aligned at the level of syntactic
nodes. Moreover, aligned nodes have been labeled
according to a set of semantic similarity labels that
express the type of similarity relation between the
nodes. The following five labels are used: gen-
eralize, specify, intersect, restate, and equal. The
corpus serves as the basis for developing tools for
automatic alignment and relation labeling.
Word aligner The word alignment tool takes as
input a pair of source and target sentences and
produces a matching between the words, that is,
a (possibly partial) one-to-one mapping of source
to target words. This aligner is a part of the full
fledged tree aligner currently under development.
The alignment task comprises several subtasks.
First, the input sentences are tokenized and parsed
with the Alpino syntactic parser for Dutch (Bouma
et al, 2001). Apart from the syntactic analysis,
which we disregard in the current work, the parser
63
performs lemmatization, part-of-speech tagging
and compound analysis, all of which are used here.
In addition, the aligner uses lexical-semantic
knowledge from Cornetto, a lexical database for
Dutch (40K entries) similar to the well-known En-
glish WordNet (Vossen et al, 2008). The rela-
tions we use are synonym, hyperonym, and xpos-
near-synonym (align near synonyms with differ-
ent POS labels). In addition we check whether
a pair of content words has a least common sub-
sumer (LCS) in the hyperonym hierarchy. As path
length has been shown to be a poor predictor in
this respect, we calculate the Lin similarity, which
combines the Information Content of the words in-
volved (Lin, 1998). A current limitation is that
we lack word sense disambiguation, hence we take
the maximal score over all the senses of the words.
The components described above can be con-
sidered as experts which predict word alignments
with a certain probability. Since alignments can
support, complement or contradict each other, we
are faced with the problem of how to combine
the evidence. Our approach is to view the align-
ment as a weighted bipartite multigraph. That is,
a graph where source and target nodes are in dis-
joint sets, multiple edges are allowed between the
same pair of nodes, and edges have an associated
weight. Our goal is on the one hand to maximize
the sum of the edge weights, and on the other hand
to reduce this graph to a model in which every
node can have at most one associated edge. This
is a combinatorial optimization problem known as
the assignment problem for which efficient algo-
rithms exist. We use a variant of the The Hungar-
ian Algorithm
1
(Kuhn, 1955), for the computation
of the matches.
Sentence similarity score Given a word align-
ment between a pair of sentences, a similarity
score is required to measure the amount of se-
mantic overlap or redundancy. Evidently the sim-
ilarity score should be proportional to the relative
number of aligned words. However, some align-
ments are more important than others. For exam-
ple, the alignment between two determiners (e.g.
the) is less significant than that between two com-
mon nouns. This is modeled in our similarity score
by weighting alignments according to the idf (in-
verse document frequency) (Sp?arck Jones, 1972)
of the words involved.
1
Also known as the Munkres algorithm
sim(s
1
, s
2
) =
?
w
i
?A
idf(w
i
)
?
w
j
?S
idf(w
j
)
(1)
Here s
1
and s
2
are sentences, S is the longest of
the two sentences, w
j
are the words in S, A is the
subsequence of aligned words in S, and w
i
are the
words in A.
3 Multi-document summarization
The Dutch Multi-Document Summarizer pre-
sented here is based on the MEAD summariza-
tion toolkit (Radev et al, 2004), which offers a
wide range of summarization algorithms and has a
flexible structure. The system creates a summary
by extracting a subset of sentences from the orig-
inal documents. The summarizer reads in a clus-
ter of documents, i.e. a set of documents relevant
for the same topic, and for each sentence it ex-
tracts a set of features. These features are com-
bined to determine an importance score for each
sentence. Next the sentences are sorted accord-
ing to their importance score. The system starts a
summary by adding the sentence with the highest
weight. Then it examines the second most impor-
tant sentence and measures the similarity with the
sentence that is already added. If the overlap is
limited, the sentence is added to the summary, oth-
erwise it is disregarded. This process is repeated
until the intended summary size is reached. The
module that performs this last step of determining
which sentences end up in the final summary is
called the reranker.
We use two baseline systems: the random base-
line system randomly selects a set of sentences
and the lead-based system which selects a sub-
set of initial sentences as summary. We investi-
gated the following features. A simple and effec-
tive features is the position: each sentence gets a
score of 1/position where ?position? is the place
in the document. The length feature is a filter that
removes sentences shorter than the given thresh-
old. The simwf feature presents the overlap of a
sentence with the title of the document computed
with cosine similarity. One of MEAD?s main fea-
tures is centroid-based summarization. Centroids
of clusters are used to determine which words
are important for the cluster and sentences con-
taining these words are considered to be central
sentences. The words are weighted with tf*idf.
64
The aim of query-based summarization is to cre-
ate summaries that are relevant with respect to a
particular query. This can easily be done with fea-
tures that express the overlap between the query
and a source sentence. We examined three differ-
ent query-based features that measure simple word
overlap between the query and the sentence, co-
sine similarity with tf*idf weighting of words and
cosine similarity without tf*idf weighting.
The MEAD toolkit implements multiple
reranker modules, we investigated the following
three: the cosine-reranker, the mmr-reranker and
novelty-reranker. We compare these rerankers
against the semantic overlap detection (sod)
tool detailed in section 2. The cosine-reranker
represents two sentences as tf*idf weighted word
vectors and computes a cosine similarity score
between them. Sentences with a cosine similarity
above the threshold are disregarded. The mmr-
reranker module is based on the maximal margin
relevance criterion (Carbonell and Goldstein,
1998). MMR models the trade-off between a
focused summary and a summary with a wide
scope. The novelty-reranker is an extension of the
cosine-reranker and boosts sentences occurring
after an important sentence by multiplying with
1.2. The reranker tries to mimic human behavior
as people tend to pick clusters of sentences when
summarizing.
4 Experimental setup
To perform proper evaluation of the summariza-
tion system we constructed a new data set for eval-
uating Dutch multi-document summarization. It
consists of 30 query-based document clusters. The
document clusters were created manually follow-
ing the guidelines of DUC 2006 (Dang, 2006).
Each cluster contains a query description and 5 to
25 newspaper articles relevant for that particular
question. For each cluster five annotators wrote
an abstract of approximately 250 words. These
summaries serve as a gold standard for compari-
son with automatically generated extracts.
We split our data set in a test set of 20 clus-
ters and a development set of 10 clusters. We use
the development set for parameter tuning and fea-
ture selection for the summarizer. We try out each
of the characteristics discussed in section 3. The
best combination found on the development set is
the feature combination position, centroid, length
with cut-off 13, and queryCosine. We tested the
different rerankers and vary the similarity thresh-
olds to determine their optimal threshold value. As
the novelty-reranker scored lower than the other
rerankers on the development set, we did not in-
clude it in our experiments on the test set.
For the experiments on the development set, we
compare each of the automatically produced ex-
tracts with five manually written summaries and
report macro-average Rouge-2 and Rouge-SU4
scores (Lin and Hovy, 2003). For the experiments
on the test set, we also perform a manual evalu-
ation. We follow the DUC 2006 guidelines for
manual evaluation of responsiveness and the lin-
guistic quality of the produced summaries. The re-
sponsiveness scores express the information con-
tent of the summary with respect to the query. The
linguistic quality is evaluated on five different ob-
jectives: grammaticality, non-redundancy, coher-
ence, referential clarity and focus. The annotators
can choose a value on a five point scale where
1 means ?very poor? and 5 means ?very good?.
We use two independent annotators to evaluate the
summaries and we report the average scores.
5 Results
The evaluation of the results on the test set are
shown in table 1. The Rouge scores of the different
rerankers are all above both baselines, and they are
very close to each other. The scores for the content
measure and responsiveness show that the values
for the automatic summaries are between 2 (poor)
and 3 (barely acceptable). The optimized summa-
rizers score higher than the two baselines on this
point.
We are most interested in the aspect of ?non-
redundancy?. The random baseline system
achieves a good result here, and the optimized
summarizers all score lower. The chance of over-
lap between randomly selected sentences seems
to be lower than when an automatic summarizer
tries to select only the most important sentences.
When we compare the three optimized systems
with different rerankers on this aspect we see that
the scores are very close. Our semantic overlap de-
tection (sod) reranker does not do any better than
the other two. The optimized summarizers do per-
form better than the baseline systems with respect
to focus and structure.
65
setting Rouge-2 Rouge-SU4 gram redun ref focus struct respons
rand baseline 0.101 0.153 4.08 3.9 2.58 2.6 2 2.25
lead baseline 0.139 0.179 3.05 3.6 3.25 2.88 2.38 2.4
optim-cosine 0.152 0.193 3.9 3.18 2.65 3.15 2.43 2.75
optim-mmr 0.149 0.191 3.98 3.13 2.55 3.13 2.38 2.7
optim-sod 0.150 0.193 4.05 3.13 2.85 3.23 2.5 2.7
Table 1: Macro-average Rouge scores and manual evaluation on the test set on these aspects:
grammaticality, non-redundancy, referential clarity, focus, structure and responsiveness.
6 Discussion and conclusion
We presented an automatic multi-document sum-
marization system for Dutch based on the MEAD
system, supporting the claim that MEAD is largely
language-independent. We experimented with dif-
ferent features and parameter settings of the sum-
marizer, and optimized it for summarization of
Dutch newspaper text. We presented a semantic
overlap detection tool, developed on the basis of a
monolingual corpus of parallel/comparable Dutch
text, which goes beyond simple string matching.
We expected this tool to improve the sentence
reranking step, thereby reducing redundancy in the
summaries. However, we were unable to show a
significant effect. We have several possible expla-
nations for this. First, many of the sentence pairs
that share the same semantic content, also share a
number of identical words. To detect these cases,
therefore, computing cosine similarity may be just
as effective. Second, the accuracy of the align-
ment tool may not be good enough, partly because
of errors in the linguistic analysis or lack of cover-
age, and partly because certain types of knowledge
(word sense, syntactic structure) are not yet ex-
ploited. Third, reranking of sentences is unlikely
to improve the summary in cases where the pre-
ceding step of sentence ranking within documents
performs poorly. We are currently still investigat-
ing this matter and hope to obtain significant re-
sults with an improved version of our tool for de-
tecting semantic overlap.
We plan to work on a more refined version that
not only uses word alignment but also considers
alignments at the parse tree level. This idea is
in line with the work of Barzilay and McKeown
(2005) who use this type of technique to fuse sim-
ilar sentences for multi-document summarization.
Acknowledgements This work was conducted within the
DAESO http://daeso.uvt.nl project funded by the
Stevin program (De Nederlandse Taalunie). The construction
of the evaluation corpus described in this paper was financed
by KP BOF 2008, University of Antwerp. We would like to
thank NIST for kindly sharing their DUC 2006 guidelines.
References
Regina Barzilay and Kathleen R. McKeown. 2005. Sentence
fusion for multidocument news summarization. Compu-
tational Linguistics, 31(3):297?328.
Gosse Bouma, Gertjan van Noord, and Robert Malouf. 2001.
Alpino: Wide-coverage computational analysis of Dutch.
In Computational Linguistics in the Netherlands 2000.,
pages 45?59. Rodopi, Amsterdam, New York.
Jaime Carbonell and Jade Goldstein. 1998. The use of
mmr, diversity-based reranking for reordering documents
and producing summaries. In Proceedings of SIGIR 1998,
pages 335?336, New York, NY, USA. ACM.
H.T. Dang. 2006. Overview of DUC 2006. In Proceedings
of the Document Understanding Workshop, pages 1?10,
Brooklyn, USA.
Harold W. Kuhn. 1955. The Hungarian Method for the as-
signment problem. Naval Research Logistics Quarterly,
2:83?97.
C.-Y. Lin and E.H. Hovy. 2003. Automatic evaluation
of summaries using n-gram co-occurrence statistics. In
Proceedings of HLT-NAACL, pages 71 ? 78, Edmonton,
Canada.
D. Lin. 1998. An information-theoretic definition of similar-
ity. In Proceedings of the ICML, pages 296?304.
Erwin Marsi and Emiel Krahmer. 2007. Annotating a par-
allel monolingual treebank with semantic similarity re-
lations. In Proceedings of the 6th International Work-
shop on Treebanks and Linguistic Theories, pages 85?96,
Bergen, Norway.
Dragomir Radev et al 2004. Mead - a platform for multidoc-
ument multilingual text summarization. In Proceedings of
LREC 2004, Lisabon, Portugal.
Karen Sp?arck Jones. 1972. A statistical interpretation of
term specificity and its application in retrieval. Journal
of Documentation, 28(1):11?21.
P. Vossen, I. Maks, R. Segers, and H. van der Vliet. 2008.
Integrating lexical units, synsets and ontology in the Cor-
netto Database. In Proceedings of LREC 2008, Mar-
rakech, Morocco.
66
A Meta-Algorithm for the Generation of Referring Expressions
Emiel Krahmer, Sebastiaan van Erk and Andre? Verleg
TU/e, Eindhoven University of Technology,
The Netherlands
email: E.J.Krahmer@tue.nl
Abstract
This paper describes a new approach
to the generation of referring expres-
sions. We propose to formalize a scene
as a labeled directed graph and describe
content selection as a subgraph con-
struction problem. Cost functions are
used to guide the search process and
to give preference to some solutions
over others. The resulting graph al-
gorithm can be seen as a meta-algorithm
in the sense that defining cost functions
in different ways allows us to mimic ?
and even improve? a number of well-
known algorithms.
1 Introduction
The generation of referring expressions is one
of the most common tasks in natural language
generation, and has been addressed by many re-
searchers in the past two decades (including Ap-
pelt 1985, Dale 1992, Reiter 1990, Dale & Had-
dock 1991, Dale & Reiter 1995, Horacek 1997,
Stone & Webber 1998, Krahmer & Theune 1999
and van Deemter 2000). As a result, there are
many different algorithms for the generation of
referring expressions, each with its own object-
ives: some aim at producing the shortest possible
description, others focus on efficiency or realistic
output. The degree of detail in which the various
algorithms are described differs considerably, and
as a result it is often difficult to compare the vari-
ous proposals. In addition, most of the algorithms
are primarily concerned with the generation of de-
scriptions only using properties of the target ob-
ject. Consequently, the problem of generating re-
lational descriptions (i.e., descriptions which in-
corporate references to other objects to single out
the target object) has not received the attention it
deserves.
In this paper, we describe a general, graph-
theoretic approach to the generation of referring
expressions. We propose to formalize a scene
(i.e., a domain of objects and their properties and
relations) as a labeled directed graph and describe
the content selection problem ?which proper-
ties and relations to include in a description for
an object?? as a subgraph construction problem.
The graph perspective has three main advantages.
The first one is that there are many attractive al-
gorithms for dealing with graph structures. In
this paper, we describe a branch and bound al-
gorithm for finding the relevant subgraphs, where
we use cost functions to guide the search pro-
cess. Arguably, the proposed algorithm is a meta-
algorithm, in the sense that by defining the cost
function in different ways, we can mimic various
well-known algorithms for the generation of re-
ferring expressions. A second advantage of the
graph-theoretical framework is that it does not run
into problems with relational descriptions, due to
the fact that properties and relations are formal-
ized in the same way, namely as edges in a graph.
The third advantage is that the combined usage
of graphs and cost-functions paves the way for
a natural integration of traditional rule-based ap-
proaches to generation with more recent statist-
ical approaches (e.g., Langkilde & Knight 1998,
Malouf 2000) in a single algorithm.
The outline of this paper is as follows. In sec-
tion 2, we describe how scenes can be described as
labeled directed graphs and show how content se-
lection can be formalized as a subgraph construc-
tion problem. Section 3 contains a sketch of the
branch and bound algorithm, which is illustrated
with a worked example. In section 4 it is argued
that by defining cost functions in different ways,
we can mimic various well-known algorithms for
the generation of referring expressions. We end
with some concluding remarks in section 5.
2 Graphs
Consider the following scene:
 










 

	










Figure 1: An example scene
In this scene, as in any other scene, we see a
finite set of entities  with properties  and
relations  . In this particular scene, the set
ffDetecting problematic turns in human-machine interactions:
Rule-induction versus memory-based learning approaches
Antal van den Bosch  
 ILK / Comp. Ling.
KUB, Tilburg
The Netherlands
antalb@kub.nl
Emiel Krahmer

 

IPO
TU/e, Eindhoven
The Netherlands
E.J.Krahmer@tue.nl
Marc Swerts
 

CNTS
UIA, Antwerp
Belgium
M.G.J.Swerts@tue.nl
Abstract
We address the issue of on-line detec-
tion of communication problems in
spoken dialogue systems. The useful-
ness is investigated of the sequence of
system question types and the word
graphs corresponding to the respective
user utterances. By applying both rule-
induction and memory-based learning
techniques to data obtained with a
Dutch train time-table information
system, the current paper demonstrates
that the aforementioned features indeed
lead to a method for problem detec-
tion that performs significantly above
baseline. The results are interesting
from a dialogue perspective since they
employ features that are present in the
majority of spoken dialogue systems
and can be obtained with little or no
computational overhead. The results
are interesting from a machine learning
perspective, since they show that the
rule-based method performs signific-
antly better than the memory-based
method, because the former is better
capable of representing interactions
between features.
1 Introduction
Given the state of the art of current language and
speech technology, communication problems are
unavoidable in present-day spoken dialogue sys-
tems. The main source of these problems lies
in the imperfections of automatic speech recogni-
tion, but also incorrect interpretations by the nat-
ural language understanding module or wrong de-
fault assumptions by the dialogue manager are
likely to lead to confusion. If a spoken dialogue
system had the ability to detect communication
problems on-line and with high accuracy, it might
be able to correct certain errors or it could in-
teract with the user to solve them. For instance,
in the case of communication problems, it would
be beneficial to change from a relatively natural
dialogue strategy to a more constrained one in
order to resolve the problems (see e.g., Litman
and Pan 2000). Similarly, it has been shown that
users switch to a ?marked?, hyperarticulate speak-
ing style after problems (e.g., Soltau and Waibel
1998), which itself is an important source of re-
cognition errors. This might be solved by using
two recognizers in parallel, one trained on nor-
mal speech and one on hyperarticulate speech. If
there are communication problems, then the sys-
tem could decide to focus on the recognition res-
ults delivered by the engine trained on hyperartic-
ulate speech.
For such approaches to work, however, it is
essential that the spoken dialogue system is able
to automatically detect communication problems
with a high accuracy. In this paper, we investigate
the usefulness for problem detection of the word
graph and the history of system question types.
These features are present in many spoken dia-
logue systems and do not require additional com-
putation, which makes this a very cheap method
to detect problems. We shall see that on the basis
of the previous and the current word graph and the
six most recent system question types, communic-
ation problems can be detected with an accuracy
of 91%, which is a significant improvement over
the relevant baseline. This shows that spoken dia-
logue systems may use these features to better pre-
dict whether the ongoing dialogue is problematic.
In addition, the current work is interesting from
a machine learning perspective. We apply two
machine learning techniques: the memory-based
IB1-IG algorithm (Aha et al 1991, Daelemans et
al. 1997) and the RIPPER rule induction algorithm
(Cohen 1996). As we shall see, some interesting
differences between the two approaches arise.
2 Related work
Recently there has been an increased interest in
developing automatic methods to detect problem-
atic dialogue situations using machine learning
techniques. For instance, Litman et al (1999)
and Walker et al (2000a) use RIPPER (Cohen
1996) to classify problematic and unproblematic
dialogues. Following up on this, Walker et al
(2000b) aim at detecting problems at the utter-
ance level, based on data obtained with AT&Ts
How May I Help You (HMIHY) system (Gorin et
al. 1997). Walker and co-workers apply RIPPER to
43 features which are automatically generated by
three modules of the HMIHY system, namely the
speech recognizer (ASR), the natural language un-
derstanding module (NLU) and the dialogue man-
ager (DM). The best result is obtained using all
features: communication problems are detected
with an accuracy of 86%, a precision of 83% and
a recall of 75%. It should be noted that the NLU
features play first fiddle among the set of all fea-
tures. In fact, using only the NLU features per-
forms comparable to using all features. Walker et
al. (2000b) also briefly compare the performance
of RIPPER with some other machine learning ap-
proaches, and show that it performs comparable
to a memory-based (instance-based) learning al-
gorithm (IB, see Aha et al 1991).
The results which Walker and co-workers de-
scribe show that it is possible to automatically de-
tect communication problems in the HMIHY sys-
tem, using machine learning techniques. Their ap-
proach also raises a number of interesting follow-
up questions, some concerned with problem de-
tection, others with the use of machine learning
techniques. (1) Walker et al train their classi-
fier on a large set of features, and show that the
set of features produced by the NLU module are
the most important ones. However, this leaves an
important general question unanswered, namely
which particular features contribute to what ex-
tent? (2) Moreover, the set of features which the
NLU module produces appear to be rather spe-
cific to the HMIHY system and indicate things like
the percentage of the input covered by the relev-
ant grammar fragment, the presence or absence of
context shifts, and the semantic diversity of sub-
sequent utterances. Many current day spoken dia-
logue systems do not have such a sophisticated
NLU module, and consequently it is unlikely that
they have access to these kinds of features. In
sum, it is uncertain whether other spoken dialogue
systems can benefit from the findings described by
Walker et al (2000b), since it is unclear which fea-
tures are important and to what extent these fea-
tures are available in other spoken dialogue sys-
tems. Finally, (3) we agree with Walker et al (and
the machine learning community at large) that it is
important to compare different machine learning
techniques to find out which techniques perform
well for which kinds of tasks. Walker et al found
that RIPPER does not perform significantly better
or worse than a memory-based learning technique.
Is this incidental or does it reflect a general prop-
erty of the problem detection task?
The current paper uses a similar methodology
for on-line problem detection as Walker et al
(2000b), but (1) we take a bottom-up approach,
focussing on a small number of features and in-
vestigating their usefulness on a per-feature basis
and (2) the features which we study are automat-
ically available in the majority of current spoken
dialogue system: the sequence of system ques-
tion types and the word graphs corresponding to
the respective user utterances. A word graph
is a lattice of word hypotheses, and we conjec-
ture that various features which have been shown
to cue communication problems (prosodic, lin-
guistic and ASR features, see e.g., Hirschberg et
al. 1999, Krahmer et al 1999 and Swerts et al
2000) have correlates in the word graph. The se-
quence of system question types is taken to model
the dialogue history. Finally, (3) to gain further in-
sight into the adequacy of various machine learn-
ing techniques for problem detection we use both
RIPPER and the memory-based IB1-IG algorithm.
3 Approach
3.1 Data and Labeling The corpus we used con-
sisted of 3739 question-answer pairs, taken from
444 complete dialogues. The dialogues consist
of users interacting with a Dutch spoken dialogue
system which provides information about train
time tables. The system prompts the user for un-
known slots, such as departure station, arrival sta-
tion, date, etc., in a series of questions. The sys-
tem uses a combination of implicit and explicit
verification strategies.
The data were annotated with a highly limited
set of labels. In particular, the kind of system
question and whether the reply of the user gave
rise to communication problems or not. The latter
feature is the one to be predicted. The following
labels are used for the system questions.
O open questions (?From where to where do you
want to travel??)
I implicit verification (?When do you want to
travel from Tilburg to Schiphol Airport??)
E explicit verification (?So you want to travel
from Tilburg to Schiphol Airport??)
Y yes/no question (?Do you want me to repeat the
connection??)
M Meta-questions (?Can you please correct
me??)
The difference between an explicit verification
and a yes/no question is that the former but not
the latter is aimed at checking whether what the
system understood or assumed corresponds with
what the user wants. If the current system ques-
tion is a repetition of the previous question it
asked, this is indicated by the suffix R. A ques-
tion only counts as a repetition when it has the
same contents as the previous system question. Of
the user inputs, we only labeled whether they gave
rise to a communication problem or not. A com-
munication problem arises when the value which
the system assigns to a particular slot (departure
station, date, etc.) does not coincide with the
value given for that particular slot by the user in
his or her most recent contribution to the dialogue
or when the system makes an incorrect default as-
sumption (e.g., the dialogue manager assumes that
the date slot should be filled with the current date,
i.e., that the user wants to travel today). Commu-
nication problems are generally easy to label since
the spoken dialogue system under consideration
here always provides direct feedback (via verific-
ation questions) about what it believes the user in-
tends. Consider the following exchange.
U: I want to go to Amsterdam.
S: So you want to go to Rotterdam?
As soon as the user hears the explicit verification
question of the system, it will be clear that his or
her last turn was misunderstood. The problem-
feature was labeled by two of the authors to
avoid labeling errors. Differences between the
two annotators were infrequent and could always
easily be resolved.
3.2 Baselines Of the 3739 user utterances
1564 gave rise to communication problems (an
error rate of 41.8%). The majority class is thus
formed by the unproblematic user utterances,
which form 58.2% of all user utterances. This
suggests that the baseline for predicting com-
munication problems is obtained by always
predicting that there are no communication prob-
lems. This strategy has an accuracy of 58.2%,
and a recall of 0% (all problems are missed). 
The precision is not defined, 	 and consequently
neither is the 


.  This baseline is misleading,
however, when we are interested in predicting
whether the previous user utterance gave rise to
communication problems. There are cases when
the dialogue system is itself clearly aware of
communication problems. This is in particular
the case when the system repeats the question
(labeled with the suffix R) or when it asks a meta-
question (M). In the corpus under investigation
here this happens 1024 times. It would not be

For definitions of accuracy, precision and recall see e.g.,
Manning and Schu?tze (1999:268-269).

Since 0 cases are selected, one would have to divide by
0 to determine precision for this baseline.

Throughout this paper we use the   measure (van
Rijsbergen 1979:174) to combine precision and recall in a
single measure. By setting  equal to 1, precision and recall
are given an equal weight, and the  measure simplifies to
ffProceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 752?760,
Beijing, August 2010
Automatic analysis of semantic similarity in comparable text
through syntactic tree matching
Erwin Marsi
TiCC, Tilburg University
e.c.marsi@uvt.nl
Emiel Krahmer
TiCC, Tilburg University
e.j .krahmer@uvt.nl
Abstract
We propose to analyse semantic similar-
ity in comparable text by matching syn-
tactic trees and labeling the alignments
according to one of five semantic simi-
larity relations. We present a Memory-
based Graph Matcher (MBGM) that per-
forms both tasks simultaneously as a com-
bination of exhaustive pairwise classifica-
tion using a memory-based learner, fol-
lowed by global optimization of the align-
ments using a combinatorial optimization
algorithm. The method is evaluated on a
monolingual treebank consisting of com-
parable Dutch news texts. Results show
that it performs substantially above the
baseline and close to the human reference.
1 Introduction
Natural languages allow us to express essentially
the same underlying meaning as many alterna-
tive surface forms. In other words, there are of-
ten many similar ways to say the same thing.
This characteristic poses a problem for many nat-
ural language processing applications. Automatic
summarizers, for example, typically rank sen-
tences according to their informativity and then
extract the top n sentences, depending on the re-
quired compression rate. Although the sentences
are essentially treated as independent of each
other, they typically are not. Extracted sentences
may have substantial semantic overlap, result-
ing in unintended redundancy in the summaries.
This is particularly problematic in the case of
multi-document summarization, where sentences
extracted from related documents are very likely
to express similar information in different ways
(Radev and McKeown, 1998). Therefore, if se-
mantic similarity between sentences could be de-
tected automatically, this would certainly help to
avoid redundancy in summaries.
Similar arguments can be made for many other
NLP applications. Automatic duplicate and pla-
giarism detection beyond obvious string overlap
requires recognition of semantic similarity. Au-
tomatic question-answering systems may benefit
from clustering semantically similar candidate an-
swers. Intelligent document merging software,
which supports a minimal but lossless merge of
several revisions of the same text, must handle
cases of paraphrasing, restructuring, compression,
etc. Recognizing textual entailments (Dagan et
al., 2005) could arguably be seen as a specific in-
stance of detecting semantic similarity.
In addition to merely detecting semantic simi-
larity, we can ask to what extent two expressions
share meaning. For instance, the meaning of one
sentence can be fully contained in that of another,
the meaning of one sentence can overlap only
partly with that of another, etc. This requires an
analysis of the semantic similarity between a pair
of expressions. Like detection, automatic analy-
sis of semantic similarity can play an important
role in NLP applications. To return to the case
of multi-document summarization, analysing the
semantic similarity between sentences extracted
from different documents provides the basis for
sentence fusion, a process where a new sentence
is generated that conveys all common information
from both sentences without introducing redun-
dancy (Barzilay and McKeown, 2005; Marsi and
Krahmer, 2005b).
752
Analysis of semantic similarity can be ap-
proached from different angles. A basic approach
is to use string similarity measures such as the
Levenshtein distance or the Jaccard similarity co-
efficient. Although cheap and fast, this fails to
account for less obvious cases such as synonyms
or syntactic paraphrasing. At the other extreme,
we can perform a deep semantic analysis of two
expressions and rely on formal reasoning to de-
rive a logical relation between them. This ap-
proach suffers from issues with coverage and ro-
bustness commonly associated with deep linguis-
tic processing. We therefore think that the middle
ground between these two extremes offers the best
option. In this paper we present a new method
for analysing semantic similarity in comparable
text. It relies on a combination of morphologi-
cal and syntactic analysis, lexical resources such
as word nets, and machine learning from exam-
ples. We propose to analyse semantic similarity
between sentences by aligning their syntax trees,
where each node is matched to the most similar
node in the other tree (if any). In addition, we
label these alignments according to the type of
similarity relation that holds between the aligned
phrases. The labeling supports further processing.
For instance, Marsi & Krahmer (2005b; 2008) de-
scribe how to generate different types of sentence
fusions on the basis of this relation labeling.
In the next Section we provide a more formal
definition of the task of matching syntactic trees
and labeling alignments, followed by a discusion
of related work in Section 3. Section 4 describes a
parallel, monolingual treebank used for develop-
ing and testing our approach. In Section 5 we pro-
pose a new algorithm for simultaneous node align-
ment and relation labeling. The results of several
evaluation experiments are presented in Section 6.
We finish with a conclusion.
2 Problem statement
Aligning a pair of similar syntactic trees is the pro-
cess of pairing those nodes that are most similar.
More formally: let v be a node in the syntactic
tree T of sentence S and v? a node in the syntactic
tree T ? of sentence S?. A labeled node alignment
is a tuple < v, v?, r > where r is a label from a set
of relations. A labeled tree alignment is a set of
labeled node alignments. A labeled tree matching
is a tree alignment in which each node is aligned
to at most one other node.
For each node v, its terminal yield STR(v) is de-
fined as the sequence of all terminal nodes reach-
able from v (i.e., a substring of sentence S).
Aligning node v to v? with label r indicates that
relation r holds between their yields STR(v) and
STR(v?). We label alignments according to a small
set of semantic similarity relations. As an exam-
ple, consider the following Dutch sentences:
(1) a. Dagelijks
Daily
koffie
coffee
vermindert
diminishes
risico
risk
op
on
Alzheimer
Alzheimer
en
and
Dementie.
Dementia.
b. Drie
Three
koppen
cups
koffie
coffee
per
a
dag
day
reduceert
reduces
kans
chance
op
on
Parkinson
Parkinson
en
and
Dementie.
Dementia.
The corresponding syntax trees and their (partial)
alignment is shown in Figure 1. We distinguish
the following five mutually exclusive similarity
relations:
1. v equals v? iff lower-cased STR(v) and
lower-cased STR(v?) are identical ? example:
Dementia equals Dementia;
2. v restates v? iff STR(v) is a proper para-
phrase of STR(v?) ? example: diminishes re-
states reduces;
3. v generalizes v? iff STR(v) is more general
than STR(v?) ? example: daily coffee gener-
alizes three cups of coffee a day;
4. v specifies v? iff STR(v) is more specific than
STR(v?) ? example: three cups of coffee a day
specifies dailly coffee;
5. v intersects v? iff STR(v) and STR(v?) share
meaning, but each also contains unique infor-
mation not expressed in the other ? example:
Alzheimer and Dementia intersects Parkin-
son and Dementia.
Our interpretation of these relations is one of
common sense rather than strict logic, akin to
the definition of entailment employed in the RTE
challenge (Dagan et al, 2005). Note also that re-
lations are prioritized: equals takes precedence
753
smain
np vermindert np
Dagelijks koffie
np
smaminpvemr
reduceert
dmrtntmr
risico pp
op conj
Altzheimer en Dementie
conj
Datmirmgtr
Dementie
l jknpr
smain
np
Drie koppen koffie pp
per dag
kans pp
op
Parkinson en
Figure 1: Example of two aligned and labeled syntactic trees. For expository reasons the alignment is
not exhaustive.
over restates, etc. Furthermore, equals, restates
and intersects are symmetrical, whereas general-
izes is the inverse of specifies. Finally, nodes con-
taining unique information, such as Alzheimer and
Parkinson, remain unaligned.
3 Related work
Many syntax-based approaches to machine trans-
lation rely on bilingual treebanks to extract trans-
fer rules or train statistical translation models. In
order to build bilingual treebanks a number of
methods for automatic tree alignment have been
developed, e.g., (Gildea, 2003; Groves et al,
2004; Tinsley et al, 2007; Lavie et al, 2008).
Most related to our approach is the work on dis-
criminative tree alignment by Tiedemann & Kotze?
(2009). However, these algorithms assume that
source and target sentences express the same in-
formation (i.e. parallel text) and cannot cope
with comparable text where parts may remain un-
aligned. See (MacCartney et al, 2008) for further
arguments and empirical evidence that MT align-
ment algorithms are not suitable for aligning par-
allel monolingual text.
MacCartney, Galley, and Manning (2008) de-
scribe a system for monolingual phrase alignment
based on supervised learning which also exploits
external resources for knowledge of semantic re-
latedness. In contrast to our work, they do not
use syntactic trees or similarity relation labels.
Partly similar semantic relations are used in (Mac-
Cartney and Manning, 2008) for modeling seman-
tic containment and exclusion in natural language
inference. Marsi & Krahmer (2005a) is closely
related to our work, but follows a more com-
plicated method: first a dynamic programming-
based tree alignment algorithm is applied, fol-
lowed by a classification of similarity relations us-
ing a supervised-classifier. Other differences are
that their data set is much smaller and consists
of parallel rather than comparable text. A major
drawback of this algorithmic approach it that it
cannot cope with crossing alignments. We are not
aware of other work that combines alignment with
semantic relation labeling, or algorithms which
perform both tasks simultaneously.
4 Data collection
For developing our alignment algorithm we use
the DAESO corpus1. This is a Dutch parallel
monolingual treebank of 1 million words, half
of which were manually annotated. The corpus
consists of pairs of sentences with different lev-
els of semantic overlap, ranging from high (dif-
ferent Dutch translations of books from Darwin,
Montaigne and Saint-Exupe?ry) to low (different
press releases from the two main news agencies
in The Netherlands, ANP and NOVUM). For this
paper, we concentrate on the latter part of the
DAESO corpus, where the proportion of Equals
and Restates is relatively low. This corpus seg-
ment consists of 8,248 pairs of sentences, contain-
ing 162,361 tokens (ignoring punctuation). All
sentences were tokenized and tagged, and subse-
quently parsed by the Alpino dependency parser
for Dutch (Bouma et al, 2001). Two annota-
1http://daeso.uvt.nl
754
Alignment: Labeling:
Eq: Re: Spec: Gen: Int: Macro: Micro:
Words: F: 95.38 95.48 58.50 65.81 65.00 25.85 62.11 88.72
SD: 2.16 2.69 7.63 13.05 11.25 18.74
Full trees: F: 88.31 95.83 71.38 60.21 66.71 62.67 71.36 81.92
SD: 1.15 2.27 3.77 7.63 8.17 6.14
Table 1: Average F-scores (in percentages, with Standard Deviations) for the six human annotators on
alignment and semantic relation labeling, for words and for full syntactic trees.
tors determined which sentences in the compa-
rable news reports contained semantic overlap.
Six other annotators produced manual alignments
of words and phrases in matched sentence pairs,
which resulted in 86,227 aligned pairs of nodes.
A small sample of 10 similar press releases
comprising a total of 48 sentence pairs was inde-
pendently annotated by all six annotators to deter-
mine inter-annotator agreement. We used preci-
sion, recall and F-score on alignment. To calcu-
late these scores for relation labeling, we simply
restrict the set of alignments to those labeled with
a particular relation, ignoring all others. Likewise,
we restrict these sets to terminal node alignments
in order to get scores on word alignment.
Given the six annotations A1, . . . , A6, we re-
peatedly took one as the True annotation against
which the five other annotations were evaluated.
We then computed the average scores over these
6 ? 5 = 30 scores (note that with this proce-
dure, precision, recall and F score end up being
equal). Table 1 summarizes the results, both for
word alignments and for full syntactic tree align-
ment. It can be seen that for alignment of words an
average F-score of over 95 % was obtained, while
alignment for full syntactic trees results in an F-
score of 88%. For relation labeling, the scores dif-
fered per relation, as is to be expected: the average
F-score for Equals was over 95% for both word
and full tree alignment2, and for the other rela-
tions average F-scores between 0.6 and 0.7 were
2At first sight, it may seem that labeling Equals is a trivial
and deterministic task, for which the F-score should always
be close to 100%. However, the same word may occur multi-
ple times in the source or target sentences, which introduces
ambiguity. This frequently occurs with function words such
as determiners and prepositions. Moreover, choosing among
several equivalent Equals alignments may sometimes involve
a somewhat arbitrary decision. This situation arises, for in-
stance, when a proper noun is mentioned just once in the
source sentence but twice in the target sentence.
obtained. The exception to note is Intersects on
word level, which only occurred a few times ac-
cording to a few of the annotators. The macro
and micro (weighted) F-score averages on labeled
alignment are 62.11% and 88.72% for words, and
71.36% and 81.92% for full syntactic trees.
5 Memory-based Graph Matcher
In order to automatically perform the alignment
and labeling tasks described in Section 2, we cast
these tasks simultaneously as a combination of ex-
haustive pairwise classification using a supervised
machine learning algorithm, followed by global
optimization of the alignments using a combina-
torial optimization algorithm. Input to the tree
matching algorithm is a pair of syntactic trees con-
sisting of a source tree Ts and a target tree Tt.
Step 1: Feature extraction For each possible
pairing of a source node ns in tree Ts and a target
node nt in tree Tt, create an instance consisting of
feature values extracted from the input trees. Fea-
tures can represent properties of individual nodes,
e.g. the category of the source node is NP, or rela-
tions between nodes, e.g. source and target node
share the same part-of-speech.
Step 2: Classification A generic supervised
classifier is used to predict a class label for each
instance. The class is either one of the seman-
tic similarity relations or the special class none,
which is interpreted as no alignment. Our im-
plementation employs the memory-based learner
TiMBL (Daelemans et al, 2009), a freely avail-
able, efficient and enhanced implementation of k-
nearest neighbour classification. The classifier is
trained on instances derived according to Step 1
from a parallel treebank of aligned and labeled
syntactic trees.
755
Step 3: Weighting Associate a cost with each
prediction so that high costs indicate low confi-
dence in the predicted class and vice versa. We
use the normalized entropy of the class labels in
the set of nearest neighbours (H) defined as
H = ?
?
c?C p(c) log2 p(c)
log2|C|
(1)
where C is the set of class labels encountered in
the set of nearest neighbours (i.e., a subset of the
five relations plus none), and p(c) is the probabil-
ity of class c, which is simply the proportion of
instances with class label c in the set of nearest
neighbours. Intuitively this means that the cost
is zero if all nearest neighbours are of the same
class, whereas the cost goes to 1 if the nearest
neighbours are equally distributed over all possi-
ble classes.
Step 4: Matching The classification step will
usually give rise to one-to-many alignment of
nodes. In order to reduce this to just one-to-one
alignments, we search for a node matching which
minimizes the sum of costs over all alignments.
This is a well-known problem in combinato-
rial optimization known as the Assignment Prob-
lem. The equivalent in graph-theoretical terms
is a minimum weighted bipartite graph match-
ing. This problem can be solved in polynomial
time (O(n3)) using e.g., the Hungarian algorithm
(Kuhn, 1955). The output of the algorithm is the
labeled tree matching obtained by removing all
node alignments labeled with the special none re-
lation.
6 Experiments
6.1 Experimental setup
Word alignment and full tree alignments are con-
ceptually different tasks, which require partly dif-
ferent features and may have different practical
applications. These are therefore addressed in
separate experiments.
Table 2 summarizes the respective sizes of de-
velopment and the held-out test set in terms of
number of aligned graph pairs, number of aligned
node pairs and number of tokens. The percentage
of aligned nodes over all graphs is calculated rela-
tive to the number of nodes over all graphs. Since
Data Graph Node Tokens Aligned
pairs pairs nodes (%)
word develop 2 664 13 027 45 149 15.71
word test 547 2 858 10 005 14.96
tree develop 2 664 22 741 45 149 47.20
tree test 547 4 894 10 005 47.05
Table 2: Properties of develop and test data sets
Data Eq Re Spec Gen Int
word develop 84.92 6.15 2.10 1.77 5.07
word test 85.62 6.09 2.17 1.99 4.13
tree develop 56.61 6.57 7.52 6.38 22.91
tree test 58.40 7.11 7.40 6.38 20.72
Table 3: Distribution of semantic similarity rela-
tions for word alignment and for full tree align-
ments in both develop and test data sets
alignments involving non-terminal nodes are ig-
nored in the task of word alignment, the number of
aligned node pairs and the percentage of aligned
nodes is lower in the word develop and word test
sets. Table 3 gives the distribution of semantic re-
lations in the development and test set, for word
and tree alignment. It can be observed that the
distribution if fairly skewed with Equals being the
majority class, even more so for word alignments.
Another thing to notice is that Intersects are much
more frequent at the level of non-terminal align-
ments.
Development was carried out using 10-fold
cross validation on the development data and con-
sequently reported scores on the development data
are averages over 10 folds. Only two parameters
were coarsely optimized on the development set.
First, the amount of downsampling of the none
class varied between 0.1 or 0.5. Second, the pa-
rameter k of the memory-based classifier ? the
number of nearest neighbours taken into account
during classification ? ranged from 1 to 15. Opti-
mal settings were finally applied when testing on
the held-out data.
A simple greedy alignment procedure served as
baseline. For word alignment, identical words are
aligned as Equals and identical roots as Restates.
For full tree alignment, this is extended to the level
of phrases so that phrases with identical words are
aligned as Equals and phrases with identical roots
as Restates. The baseline does not predict Spec-
756
ifies, Generalizes or Intersects relations, as that
would require a more involved, knowledge-based
approach.
All features used are described in Table 4.
The word-based features rely on pure string pro-
cessing and require no linguistic preprocessing.
The morphology-based features exploit the lim-
ited amount of morphological analysis provided
by the Alpino parser (Bouma et al, 2001). For
instance, it provides word roots and decomposes
compound words. Likewise the part-of-speech-
based features use the coarse-grained part-of-
speech tags assigned by the Alpino parser. The
lexical-semantic features rely on the Cornetto
database (Vossen et al, 2008), a recent exten-
sion to the Dutch WordNet, to look-up synonym
and hypernym relations among source and tar-
get lemmas. Unfortunately there is no word
sense disambiguation module to identify the cor-
rect senses. In addition, a background corpus
of over 500M words of (mainly) news text pro-
vides the word counts required to calculate the
Lin similarity measure (Lin, 1998). The syntax-
based features use the syntactic structure, which
is a mix of phrase-based and dependency-based
analysis. The phrasal features express similar-
ity between the terminal yields of source and tar-
get nodes. With the exception of same-parent-lc-
phrase, these features are only used for full tree
alignment, not for word alignment.
6.2 Results on word alignment
We evaluate our alignment model in two steps:
first focussing on word alignment and then on full
tree alignment. Table 5 summarizes the results for
MBGM on word alignment (50% downsampling
and k = 3), which we compare statistically to the
baseline performance, and informally with the hu-
man scores reported in Table 1 in Section 4 (note
that the human scores are only for a subset of the
data used for automatic evaluation).
The first thing to observe is that the MBGM
scores on the development and tests sets are
very similar throughout. For predicting word
alignments, the MBGM system performs signif-
icantly better than the baseline system (t(18) =
17.72, p < .0001). On the test set, MBGM ob-
tains an F-score of nearly 89%, which is almost
exactly halfway between the scores of the base-
line system and the human scores. In a similar
vein, the performance of the MBGM system on
relation labeling is considerably better than that
of the baseline system. For all semantic rela-
tions, MBGM performs significantly better than
the baseline (t(18) > 9.4138, p < .0001 for each
relation, trivially so for the Specifies, Generalizes
and Intersects relations, which the baseline system
never predicts).
The macro scores are plain averages over the 5
scores on each relation, whereas the micro scores
are weighted averages. As the Equals is the major-
ity class and at the same time easiest to predict, the
micro scores are higher. The macro scores, how-
ever, better reflect performance on the real chal-
lenge, that is, correctly predicting the relations
other than Equals. The MBGM macro average
is 27.37% higher than the baseline (but still some
10% below the human top line), while the micro
average is 5.83% higher and only 0.75% below
the human top line. Macro scores on the test set
are overall lower than those on the develop set,
presumably because of tuning on the development
data.
6.3 Results on tree alignment
Table 6 contains the results of full tree alignment
(50% downsampling and k = 5); here both termi-
nal and non-terminal nodes are aligned and clas-
sified in one pass. Again scores on the develop-
ment and test set are very similar, the latter being
slightly better. For full tree alignment, MBGM
once again performs significantly better than the
baseline, t(18) = 25.68, p < .0001. With an F-
score on the test set of 86.65, MBGM scores al-
most 20 percent higher than the baseline system.
This F-score is less than 2% lower than the aver-
age F-score obtained by our human annotators on
full tree alignment, albeit not on exactly the same
sample. The picture that emerges for semantic re-
lation labeling is closely related to the one we saw
for word alignments. MBGM significantly out-
performs the baseline, for each semantic relation
(t(18) > 12.6636, p < .0001). MBGM scores a
macro average F-score of 52.24% (an increase of
30.05% over the baseline) and a micro average of
80.03% (12.68% above the base score). It is inter-
757
Feature Type Description
Word
word-subsumption string indicate if source word equals, has as prefix, is a prefix of, has a suffix, is a
suffix of, has as infix or is an infix of target word
shared-pre-/in-/suffix-len int length of shared prefix/infix/suffix in characters
source/target-stop-word bool test if source/target word is in a stop word list of frequent function words
source/target-word-len int length of source/target word in characters
word-len-diff int word length difference in characters
source/target-word-uniq bool test if source/target word is unique in source/target sentence
same-words-lhs/rhs int no. of identical preceding/following words in source and target word contexts
Morphology
root-subsumption string indicate if source root equals, has as prefix, is a prefix of, has a suffix, is a suffix
of, has as infix or is an infix of target root
roots-share-pre-/in-/suffix bool source and target root share a prefix/infix/suffix
Part-of-speech
source/target-pos string source/target part-of-speech
same-pos bool test if source and target have same part-of-speech
source/target-content-word bool test if source/target word is a content word
both-content-word bool test if both source and target word are content words
Lexical-semantic using Cornetto
cornet-restates float 1.0 if source and target words are synonyms and 0.5 if they are near-synonyms,
zero otherwise
cornet-specifies float Lin similarity score if source word is a hyponym of target word, zero otherwise
cornet-generalizes float Lin similarity score if source word is a hypernym of target word, zero otherwise
cornet-intersects float Lin similarity score if source word share a common hypernym, zero otherwise
Syntax
source/target-cat string source/target syntactic category
same-cat bool test if source and target have same syntactic category
source/target-parent-cat string source/target syntactic category of parent node
source/target-deprel string source/target dependency relation
same-deprel bool test if source and target have same dependency relation
same-dephead-root bool test if the dependency heads of the source and target have same root
Phrasal
word-prec/rec float precision/recall on the yields of source and target nodes
same-lc-phrase bool test if lower-cased yields of source and target nodes are identical
same-parent-lc-phrase bool test if lower-cased yields of parents of source and target nodes are identical
source/target-phrase-len int length of source/target phrase in words
phrase-len-diff int phrase length difference in words
Table 4: Features (where slashes indicate multiple versions of the same feature, e.g. source/target-pos
represents the two features source-pos and target-pos)
esting to observe that MBGM obtains higher F-
scores on Equals and on Intersects (the two most
frequent relations) than the human annotators ob-
tained. As a result of this, the micro F-score of
the automatic full tree alignment is less than 2%
lower than the human reference score.
Tree alignment can also be implemented as a
two-step procedure, where in the first step align-
ments and semantic relation classifications at the
word level are produced, while in the second step
these are used to predict alignments and seman-
tic relations for non-terminals. We experimented
with such a two-step procedure as well, in one ver-
sion using the actual word alignments and in the
other the predicted word alignments. The scores
of the two-step prediction are only marginally dif-
ferent from those of one step prediction, both for
alignment and for relation classification, giving
improvements in the order of about 1% for both
subtasks. As is to be expected, the scores with
true word alignments are much better than those
with predicted word alignments. They are inter-
esting though, because they suggest that a fairly
good full tree alignment can be automatically ob-
758
Alignment: Labeling:
Eq: Re: Spec: Gen: Int: Macro: Micro:
Prec: 80.59 81.84 46.26 0.00 0.00 0.00 25.61 80.22
Develop baseline: Rec: 81.58 93.10 34.71 0.00 0.00 0.00 25.56 82.20
F: 81.08 87.11 39.66 0.00 0.00 0.00 25.35 80.70
Prec: 91.72 94.54 61.26 74.60 67.82 45.80 68.80 90.82
Develop MBGM: Rec: 87.82 95.91 46.19 40.87 43.22 27.27 50.61 86.96
F: 89.73 95.02 52.67 52.81 52.80 34.19 57.50 88.85
Prec: 82.45 83.83 43.12 0.00 0.00 0.00 25.39 82.17
Test baseline: Rec: 82.19 93.87 27.01 0.00 0.00 0.00 24.18 82.02
F: 82.32 88.57 33.22 0.00 0.00 0.00 24.36 82.14
Prec: 90.92 94.20 53.33 59.87 54.21 42.47 60.84 89.90
Test MBGM: Rec: 87.09 95.41 40.21 32.75 43.28 20.31 46.39 86.11
F: 88.96 94.80 45.85 42.34 48.17 27.48 51.73 87.97
Table 5: Scores (in percentages) on word alignment and semantic relation labeling
Alignment: Labeling:
Eq: Re: Spec: Gen: Int: Macro: Micro:
Prec: 82.50 83.76 46.72 0.00 0.00 0.00 26.10 82.18
Develop baseline: Rec: 54.54 93.66 20.01 0.00 0.00 0.00 22.74 54.34
F: 65.67 88.43 28.02 0.00 0.00 0.00 23.29 65.42
Prec: 92.23 96.15 55.90 54.40 56.15 70.33 66.59 84.99
Develop MBGM: Rec: 81.04 94.03 26.64 21.71 29.34 70.27 48.40 74.68
F: 86.27 95.08 36.08 31.03 38.54 70.30 54.21 79.50
Prec: 84.23 85.68 42.24 0.00 0.00 0.00 25.58 84.14
Test baseline: Rec: 56.21 94.44 14.08 0.00 0.00 0.00 21.70 56.15
F: 67.43 89.85 21.12 0.00 0.00 0.00 22.19 67.35
Prec: 92.27 96.67 60.25 46.92 56.85 68.64 65.87 85.23
Test MBGM: Rec: 81.67 94.54 27.87 19.55 30.94 71.01 48.87 75.44
F: 86.65 95.60 38.11 27.60 40.07 69.80 54.24 80.03
Table 6: Scores (in percentages) on full tree alignment and semantic relation labeling
tained given a manually checked word alignment.
7 Conclusions
We have proposed to analyse semantic similarity
between comparable sentences by aligning their
syntax trees, matching each node to the most sim-
ilar node in the other tree (if any). In addi-
tion, alignments are labeled with a semantic sim-
ilarity relation. We have presented a Memory-
based Graph Matcher (MBGM) that performs
both tasks simultaneously as a combination of ex-
haustive pairwise classification using a memory-
based learning algorithm, and global optimization
of alignments using a combinatorial optimization
algorithm. It relies on a combination of morpho-
logical/syntactic analysis, lexical resources such
as word nets, and machine learning using a par-
allel monolingual treebank. Results on aligning
comparable news texts from a monolingual paral-
lel treebank for Dutch show that MBGM consis-
tently and significantly outperforms the baseline,
both for alignment and labeling. This holds both
for word alignment and tree alignment.
In future research we will test MBGM on other
data, as the DAESO corpus contains sub-corpora
with various degrees of semantic overlap. In addi-
tion, we intend to explore alternative features from
word space models. Finally, we plan to evaluate
MBGM in the context of NLP applications such
as multi-document summarization. This includes
work on how to define similarity at the sentence
level in terms of the proportion of aligned con-
stituents. Both MBGM and the annotated data set
will be publicly released.2
759
Acknowledgments
This work was conducted within the DAESO
project funded by the Stevin program (De Ned-
erlandse Taalunie).
References
Barzilay, Regina and Kathleen R. McKeown. 2005. Sen-
tence fusion for multidocument news summarization.
Computational Linguistics, 31(3):297?328.
Bouma, Gosse, Gertjan van Noord, and Robert Malouf.
2001. Alpino: Wide-coverage computational analysis of
Dutch. In Daelemans, Walter, Khalil Sima?an, Jorn Veen-
stra, and Jakub Zavre, editors, Computational Linguistics
in the Netherlands 2000., pages 45?59. Rodopi, Amster-
dam, New York.
Daelemans, W., J. Zavrel, K. Van der Sloot, and
A. Van den Bosch. 2009. TiMBL: Tilburg Memory
Based Learner, version 6.2, reference manual. Technical
Report ILK 09-01, Induction of Linguistic Knowledge,
Tilburg University.
Dagan, I., O. Glickman, and B. Magnini. 2005. The PAS-
CAL Recognising Textual Entailment Challenge. In Pro-
ceedings of the PASCAL Challenges Workshop on Recog-
nising Textual Entailment, Southampton, U.K.
Gildea, Daniel. 2003. Loosely tree-based alignment for
machine translation. In Proceedings of the 41st Annual
Meeting on Association for Computational Linguistics,
pages 80?87, Sapporo, Japan.
Groves, D., M. Hearne, and A. Way. 2004. Robust sub-
sentential alignment of phrase-structure trees. In Pro-
ceedings of the 20th International Conference on Com-
putational Linguistics (CoLing ?04), pages 1072?1078.
Krahmer, Emiel, Erwin Marsi, and Paul van Pelt. 2008.
Query-based sentence fusion is better defined and leads
to more preferred results than generic sentence fusion. In
Moore, J., S. Teufel, J. Allan, and S. Furui, editors, Pro-
ceedings of the 46th Annual Meeting of the Association
for Computational Linguistics: Human Language Tech-
nologies, pages 193?196, Columbus, Ohio, USA.
Kuhn, Harold W. 1955. The Hungarian Method for the as-
signment problem. Naval Research Logistics Quarterly,
2:83?97.
Lavie, A., A. Parlikar, and V. Ambati. 2008. Syntax-
driven learning of sub-sentential translation equivalents
and translation rules from parsed parallel corpora. In Pro-
ceedings of the Second Workshop on Syntax and Structure
in Statistical Translation, pages 87?95.
Lin, D. 1998. An information-theoretic definition of similar-
ity. In Proceedings of the 15th International Conference
on Machine Learning, pages 296?304.
MacCartney, B. and C.D. Manning. 2008. Modeling seman-
tic containment and exclusion in natural language infer-
ence. In Proceedings of the 22nd International Confer-
ence on Computational Linguistics-Volume 1, pages 521?
528.
MacCartney, Bill, Michel Galley, and Christopher D. Man-
ning. 2008. A phrase-based alignment model for natural
language inference. In Proceedings of the 2008 Confer-
ence on Empirical Methods in Natural Language Process-
ing, pages 802?811, Honolulu, Hawaii, October.
Marsi, Erwin and Emiel Krahmer. 2005a. Classification of
semantic relations by humans and machines. In Proceed-
ings of the ACL 2005 workshop on Empirical Modeling
of Semantic Equivalence and Entailment, pages 1?6, Ann
Arbor, Michigan.
Marsi, Erwin and Emiel Krahmer. 2005b. Explorations in
sentence fusion. In Proceedings of the 10th European
Workshop on Natural Language Generation, Aberdeen,
GB.
Radev, D.R. and K.R. McKeown. 1998. Generating natural
language summaries from multiple on-line sources. Com-
putational Linguistics, 24(3):469?500.
Tiedemann, J. and G. Kotze?. 2009. Building a Large
Machine-Aligned Parallel Treebank. In Eighth Interna-
tional Workshop on Treebanks and Linguistic Theories,
page 197.
Tinsley, J., V. Zhechev, M. Hearne, and A. Way. 2007. Ro-
bust language-pair independent sub-tree alignment. Ma-
chine Translation Summit XI, pages 467?474.
Vossen, P., I. Maks, R. Segers, and H. van der Vliet. 2008.
Integrating lexical units, synsets and ontology in the Cor-
netto Database. In Proceedings of LREC 2008, Mar-
rakech, Morocco.
760
Last Words
What Computational Linguists Can Learn
from Psychologists (and Vice Versa)
Emiel Krahmer?
Tilburg University
1. Introduction
Sometimes I am amazed by how much the field of computational linguistics has
changed in the past 15 to 20 years. In the mid 1990s, I was working at a research institute
where language and speech technologists worked in relatively close quarters. Speech
technology seemed on the verge of a major breakthrough; this was around the time that
Bill Gates was quoted in Business Week as saying that speech was not just the future of
Windows, but the future of computing itself. At the same time, language technology
was, well, nowhere. Bill Gates certainly wasn?t championing language technology in
those days. And while the possible applications of speech technology seemed endless
(who would use a keyboard in 2010, when speech-driven user interfaces would have re-
placed traditional computers?), the language people were thinking hard about possible
applications for their admittedly somewhat immature technologies.
Predicting the future is a tricky thing. No major breakthrough came for speech
technology?I am still typing this. However, language technology did change almost
beyond recognition. Perhaps one of the main reasons for this has been the explosive
growth of the Internet, which helped language technology in two different ways. On
the one hand it instigated the development and refinement of techniques needed for
searching in document collections of unprecedented size, on the other it resulted in a
large increase of freely available text data. Recently, language technology has been par-
ticularly successful for tasks where huge amounts of textual data is available to which
statistical machine learning techniques can be applied (Halevy, Norvig, and Pereira
2009). As a result of these developments, mainstream computational linguistics is now
a successful, application-oriented discipline which is particularly good at extracting
information from sequences of words.
But there is more to language than that. For speakers, words are the result of a
complex speech production process; for listeners, they are what starts off the similarly
complex comprehension process. However, in many current applications no attention is
given to the processes by which words are produced nor to the processes by which they
can be understood. Language is treated as a product not as a process, in the terminology
of Clark (1996). In addition, we use language not only as a vehicle for factual infor-
mation exchange; speakers may realise all sorts of other intentions with their words:
They may want to convince others to do or buy something, they may want to induce
a particular emotion in the addressee, and so forth. These days, most of computational
linguistics (with a few notable exceptions, more about which subsequently) has little to
? Tilburg Centre for Creative Computing (TiCC), Communication and Cognition research group, Tilburg
University, The Netherlands. E-mail: e.j.krahmer@uvt.nl.
? 2010 Association for Computational Linguistics
Computational Linguistics Volume 36, Number 2
say about how people produce and comprehend language, nor about what the possible
effects of language could be.
It wasn?t always like this; early work in computational linguistics took a different
(and more ambitious) perspective. Winograd (1980), to give one more or less ran-
dom example, explicitly treated language understanding and production as cognitive
processes, which interacted with other cognitive modules such as visual perception;
Hovy (1990), to give another, presented a computational model that generated different
texts from the same underlying facts, depending on pragmatic, interpersonal con-
straints. It is interesting to observe thatWinograd andHovy built on both computational
and psychological research, something which is increasingly rare in the field of compu-
tational linguistics, a point made convincingly by Reiter (2007). By now, it is generally
accepted that the problems that Winograd, Hovy, and others tried to tackle are very
complex, and that the current emphasis on more well-delimited problems is probably
a good thing. However, it is not difficult to come up with computational applications
for which a better understanding would be required of language as a process and the
effects language may have on a user (interactive virtual agents which try to persuade a
user to do something, for example). To learn more about how speakers and addressees
manage to accurately produce and comprehend complex and potentially ambiguous
sentences in real time, and how theymay use these sentences for a whole range of inten-
tions, we have to turn to psycholinguistics and social psychology, respectively. So let us
sample some of the recent findings in these fields, and see if and how they might benefit
computational linguistics. Interestingly, we will find many places where more attention
to what goes on in computational linguistics would benefit psychologists as well.
2. Language Use and Its Social Impact
Social psychologists study persons and the relations they have with others and
with groups. Various social psychologists have concentrated on language (although
perhaps not as many as you would expect given the importance of language for
social interactions). A number of different approaches can be discerned, one of which
concentrates on the psychological functions of function words (Chung and Pennebaker
2007). Function words are understood here to include pronouns, prepositions, articles,
conjunctions, and auxiliary verbs.
2.1 On the Psychological Functions of Pronouns
One reliable finding of this perspective is that first person singular pronouns are associ-
ated with negative affective states. For example, in one study it was found that currently
depressed students used I and me more often than students who were not currently
depressed, and of the latter group those who had known periods of depression used
them more frequently than those who had never had such an episode (Rude, Gortner,
and Pennebaker 2004). Another study found that suicidal poets used first person sin-
gular pronouns in their poems more frequently than non-suicidal poets (Stirman and
Pennebaker 2001). Of course, whether a speaker tends to use I or we more frequently
is also indicative of self- versus other-centeredness. An analysis of blogs following the
events of September 11 revealed that bloggers? use of I and me dropped in the hours
following the attack, while simultaneously their use ofwe and us increased (Cohn,Mehl,
and Pennebaker 2004); this switch is interpreted by the authors as indicating that people
286
Krahmer Last Words
were focusing less on themselves during this period, but instead focusing more on their
friends and families. In a completely different study of adult speakers (both male and
female) who underwent testosterone therapy, it was found that as testosterone levels
dropped, so did their use of I pronouns, while simultaneously the use of non-I pronouns
increased (Pennebaker et al 2004).
Pennebaker and colleagues report comparable effects of age, gender, status, and cul-
ture on personal pronoun use (Chung and Pennebaker 2007). Their corpus (or ?archive?,
as they call it) contains over 400,000 text files, frommany different authors and collected
over many years. It is interesting to observe that Pennebaker was an early adapter of
computers for his analyses, simply because performing them manually was too time-
consuming. The general approach in these analyses is to determine beforehand what
the ?interesting? words are and then simply to count them in the relevant texts, without
taking the linguistic context into account. This obviously creates errors: The relative
frequency of first-person pronouns may be indicative of depression, as we have just
seen, but a sentence such as I love life seems a somewhat implausible cue for a depressed
state of mind. Chung and Pennebaker (2007, page 345) themselves give the example of
mad, which is counted as an anger and negative emotion word, and they point out that
this is wrong for I?m mad about my lover. Clearly, standard methods from computational
linguistics could be used to address this problem, for instance by looking at words in
context and n-grams. Another problem which Chung and Pennebaker mention, and
which will be familiar to many computational linguists, is the problem of deciding
which are the interesting words to count. Here techniques such as feature construction
and selection could be of help. As I will argue in what follows, the observations of
Pennebaker and colleagues are potentially interesting for computational linguistics as
well, but let us first look at another relevant set of psychological findings.
2.2 On the Psychological Functions of Interpersonal Language
A different strand of research on language and social psychology focuses on inter-
personal verbs (a subset of what computational linguists more commonly refer to as
transitive verbs): verbs which express relations between people (Semin 2009). In their
model of interpersonal language (the Linguistic Categorization Model), Semin and
Fiedler (1988) make a distinction between different kinds of verbs and their position
on the concrete?abstract dimension. Descriptive action verbs (Romeo kisses Juliet) are
assumed to be the most concrete, since they refer to a single, observable event. This is
different for state verbs (Romeo loves Juliet), which describe psychological states instead
of single perceptual events, and are therefore more abstract. Most abstract, according
to Semin and Fiedler, are adjectives (Romeo is romantic), because they generalize over
specific events and objects and only refer to characteristics of the subject.
The thing to note is that the same event can, in principle, be referred to in all these
different forms; a speaker has the choice of using a more concrete or a more abstract
way to refer to an event (e.g., John can be described as, from more to less concrete,
hitting a person, hating a person, or being aggressive). Interestingly, the abstractness
level a speaker opts for tells us something about that speaker. This has been found,
for instance, in the communication of ingroup (think of people with the same cul-
tural identity or supporting the same soccer team) and outgroup (different identity,
different team) behavior. There is considerable evidence that speakers describe negative
ingroup and positive outgroup behavior in more concrete terms (e.g., using action
verbs), thereby indicating that the behavior is more incidental, whereas positive ingroup
287
Computational Linguistics Volume 36, Number 2
and negative outgroup behaviors are described in relatively abstract ways (e.g., more
frequently using adjectives), suggesting a more enduring characteristic (see, e.g., Maass
et al 1989). Maass and colleagues showed this phenomenon, which they dubbed the
Linguistic Intergroup Bias, for different Contrada (neighborhoods) participating in the
famous Palio di Siena horse races, reporting about their own performance and that of
the other neighborhoods. Moreover, Wigboldus, Semin, and Spears (2000) have shown
that addressees do indeed pick up these implications, and Douglas and Sutton (2006)
reveal that speakers who describe the behavior of others in relatively abstract terms are
perceived to have biased attitudes and motives as opposed to speakers who describe
this behavior in more concrete ways.
It has been argued that concrete versus abstract language is not only relevant for,
for example, the communication of stereotypes, but also has more fundamental effects,
for instance influencing the way people perceive the world (Stapel and Semin 2007).
In a typical experiment, Stapel and Semin first subtly prime participants with either
abstract or concrete language. This can be done using scrambled sentences, where
participants are given four words (romantic is lamp Romeo) with the instruction to form
a grammatical sentence from three of them, or by giving participants a word-search
puzzle where the words to search for are the primes. After this, participants perform a
seemingly unrelated task, where their perceptual focus (either on the global picture or
on the details) is measured. Stapel and Semin show that processing abstract language
(adjectives) results in a more global perception, whereas processing concrete language
(descriptive action verbs) leads to more perceptual attention to details.
At this point, a computational linguist (and probably other linguists as well) might
start to wonder about the comparison between verbs and adjectives, and by the claim
that adjectives are abstract. What about adjectives like blonde, young, and thin? These
seem to be much more concrete than adjectives such as aggressive or honest. And what
about nouns? There a distinction between concrete (office chair) and abstract (hypothesis)
seems to exist as well. This raises the question whether it is the differences in inter-
personal language use or the more general distinction between concrete and abstract
languagewhich causes the observed effects on perception; a recent series of experiments
suggests it is the latter (Krahmer and Stapel 2009).
2.3 What Can Computational Linguists Learn?
The social psychological findings briefly described here could have an impact on com-
putational linguistics, with potential applications for both text understanding and gen-
eration. So far, it seems fair to say that most computational linguists have concentrated
so much on trying to understand text or on generating coherent texts that the subtle
effects that language may have on the reader were virtually ignored. Function words
were originally not the words computational linguists found most interesting. They
were considered too frequent; early Information Retrieval applications listed function
words on ?stop lists??lists of words that should be ignored during processing?and
many IR applications still do. The work of Pennebaker and colleagues indicates that
pronouns (as well as other function words) do carry potentially relevant information,
for instance about the mental state of the author of a document. Interestingly, for com-
putational applications such as opinion mining and sentiment analysis (Pang and Lee
2008) as well as author attribution and stylometry (Holmes 1998), function words have
been argued to be relevant as well, but it seems that research on the social psychology
of language has made little or no impact on this field.
288
Krahmer Last Words
Consider sentiment analysis, for instance, which is the automatic extraction of
?opinion-oriented? information (e.g., whether an author feels positive or negative about
a certain product) from text. This is a prime example of an emerging research area in
computational linguistics which moves beyond factual information exchange (although
the preferred approach to this problem very much fits with the paradigm sketched by
Halevy et al [2009]: take a large set of data and apply machine learning to it). Pang
and Lee (2008) offer an extensive overview of research related to sentiment analysis,
but do not discuss any of the psychological studies mentioned herein (in fact, of the
332 papers they cite, only one or two could conceivably be interpreted as psychological
in the broadest interpretation). What is especially interesting is that their discussion
of why sentiment analysis is difficult echoes the discussion of Chung and Pennebaker
(2007) on the problems of counting words (by sheer coincidence they even discuss
essentially the same example: madden).
These findings may also have ramifications for the generation of documents. If you
develop an application which automatically produces texts from non-textual data, you
might want to avoid excessive use of the first-person pronoun, lest your readers think
your computer is feeling down. If you want your readers to skim over the details of
what is proposed in a generated text, use abstract language. In addition, you may want
to use action verbs when describing your own accomplishments, and adjectives to refer
to those of others (but do it in a subtle way, because people might notice).
3. Language Comprehension and Production
While the link between computational linguistics and social psychology has seldom
been explored, there has been somewhat more interaction with psycholinguistics. Per-
haps most of this interaction has involved natural language understanding. Various
early parsing algorithms were inspired by human sentence processing, which is hardly
surprising: human listeners are remarkably efficient in processing and adequately re-
sponding to potentially highly ambiguous sentences. Later, when large data sets of
parsed sentences became available, the focus in computational linguistics shifted to
developing statistical models of language processing. Interestingly, recent psycholin-
guistic sentence processing models are inspired in turn by statistical techniques from
computational linguistics (Chater and Manning 2006; Crocker in press; Jurafsky 2003;
Pado, Crocker, and Keller 2009).
3.1 On Producing Referring Expressions
The situation is somewhat different for natural language generation, although superfi-
cially the same kind of interaction can be observed here (albeit with a few years delay).
The seminal work by Dale and Reiter (1995) on the generation of referring expressions
was explicitly inspired by psycholinguistic work. Dale and Reiter concentrated on the
generation of distinguishing descriptions, such as the large black dog, which single out
one target object by ruling out the distractors (typically a set of other domestic animals
of different sizes and colors). Given that the number of distractors may be quite large
and given that each target can be referred to in multiple ways, one of the main issues
in this area is how to keep the search manageable. Current algorithms for referring
expression generation, building on the foundations laid by Dale and Reiter, are good
at quickly computing which set of properties uniquely characterizes a target among a
289
Computational Linguistics Volume 36, Number 2
set of distractors. Some of these algorithms are capable of generating distinguishing
descriptions that human judges find more helpful and better formulated than human-
produced distinguishing descriptions for the same targets (Gatt, Belz, and Kow 2009).
To some this might suggest that the problem is solved. This conclusion, however,
would be too hasty. Most of the algorithms use some very unrealistic assumptions
which limit their applicability. Interestingly, these assumptions can be traced back
directly to classic psycholinguistic work on the production of referring expressions
(Olson 1970). Clark and Bangerter (2004) criticize a number of the unstated assumptions
in Olson?s approach: Reference is treated as a one-step process (a speaker plans and
produces a complete description, and nothing else, in one go) and during that process
the speaker does not take the prior interaction with the addressee into account. By
merely substituting computer for speaker these comments are directly applicable to most
current generation algorithms as well.
The problem, unfortunately, is that recent psycholinguistic research suggests that
these assumptions are wrong. Often this research looks at how speakers produce re-
ferring expressions while interacting with an addressee, and one thing that is often
found is that speakers adapt to their conversational partners while producing refer-
ring expressions (Clark and Wilkes-Gibbs 1986; Brennan and Clark 1996; Metzing and
Brennan 2003). This kind of ?entrainment? or ?alignment? (Pickering and Garrod 2004)
may apply at the level of lexical choice; if a speaker refers to a couch using the word
sofa instead of the more common couch, the addressee is more likely to use sofa instead
of couch as well later on in the dialogue (Branigan et al in press). But the speaker and
addressee may also form a general ?conceptual pact? on how to refer to some object,
deciding together, for instance, to refer to a tangram figure as the tall ice skater.
Although adaptation itself is uncontroversial, psycholinguists argue about the
extent to which speakers are capable of taking the perspective of the addressee into
account (Kronmu?ller and Barr 2007; Brennan and Hanna 2009; Brown-Schmidt 2009),
with some researchers presenting evidence that speakers may have considerable
difficulty doing this (Horton and Keysar 1996; Keysar, Lin, and Barr 2003). In Wardlow
Lane et al (2006) people are instructed to refer to simple targets (geometrical figures that
may be small or larger) in the context of three distractor objects, two of which are visible
to both speaker and addressee (shared) whereas the other is visible to the speaker only
(privileged). If speakers would be able to take the addressees? perspective into account
when referring, the privileged distractor should not play a role in determining which
properties to include in the distinguishing description. However, Wardlow Lane and
colleagues found that speakers do regularly take the privileged distractor into account
(for instance adding a modifier small when referring to the target, even though all the
shared objects are small and only the privileged one is large). Interestingly, speakers
do this more often when explicitly told that they should not leak information about
the privileged object, which Wardlow Lane et al interpret as an ironic processing
effect of the kind observed by Dostoevsky (?Try to pose for yourself this task: not to
think of a polar bear, and you will see that the cursed thing will come to mind every
minute?).
Another interesting psycholinguistic finding is that speakers often include more
information in their referring expressions than is strictly needed for identification (Arts
2004; Engelhardt, Bailey, and Ferreira 2006), for instance referring to a dog as the large
black curly haired dog in a situation where there is only one large black dog. Again,
that speakers are not always ?Gricean? (?be as informative as required, but not more
informative?) is generally agreed upon, but there is an ongoing debate about why and
how speakers overspecify, some arguing that it simplifies the search of the speaker
290
Krahmer Last Words
(Engelhardt, Bailey, and Ferreira 2006) whereas others suggest that overspecified refer-
ences are particularly beneficial for the addressee (Paraboni, van Deemter, andMasthoff
2007).
3.2 What Can Computational Linguists Learn?
Why are these psycholinguistic findings about the way human speakers refer relevant
for generation algorithms? First of all, human-likeness is an important evaluation crite-
rion, so algorithms that are good at emulating human referring expressions are likely to
outperform algorithms that are not. Moreover, it is interesting to observe that generating
overspecified expressions is computationally cheaper than producing minimal ones
(Dale and Reiter 1995). In a similar vein, it can be argued that alignment and adaptation
may reduce the search space of the generation algorithm, because they limit the number
of possibilities that have to be considered.
It is worth emphasizing that psycholinguistic theories have little to say about how
speakers quickly decide which properties, from the large set of potential ones, to use in
a referring expression. In addition, whereas notions such as adaptation, alignment, and
overspecification are intuitively appealing, it has turned out to be remarkably difficult
to specify how these processes operate exactly. In fact, a common criticism is that they
would greatly benefit from ?explicit computational modeling? (Brown-Schmidt and
Tanenhaus 2004). Of course, solving choice problems and computational modeling are
precisely what computational linguistics has to offer. So although generation algorithms
may benefit a lot from incorporating insights from psycholinguistics, they in turn have
the potential to further research in psycholinguistics as well.
4. Discussion
In this brief, highly selective, and somewhat biased overview of work on language in
several areas of psychology, we have seen that words may give valuable information
about the person who produces them (but how to select and count them is tricky), that
abstract or concrete language may tell you something about the opinions and attitudes
a speaker has and may even influence how you perceive things (but the linguistic intu-
itions about what is abstract, andwhat concrete, need somework), and that speakers are
remarkably efficient when producing referring expressions, in part because they adapt
to their addressee and do not necessarily try to be as brief as possible (but making these
intuitive notions precise is difficult). Psychological findings such as these are not merely
intriguing, but could be of real use for computational linguistic applications related to
document understanding or generation (and, conversely, techniques and insights from
computational linguistics could be helpful for psychologists as well). Of course, some
computational linguists do extensively rely on psychological findings for building their
applications (you know who you are), just as some psychologists use sophisticated
computational and statistical models rather than human participants for their studies
(this is especially true in psycholinguistics). But these are exceptions, and certainly do
not belong to mainstream computational linguistics or psychology. Which raises one
obvious question: Why isn?t there more interaction between these two communities?
There seem to be at least three reasons for this. First, and most obvious, many
researchers are not aware of what happens outside their own specialized field. The
articles in psychology are fairly accessible (usually no complex statistical models or
overformalized algorithms there), but many computational linguists may feel that it
291
Computational Linguistics Volume 36, Number 2
would be a better investment of their limited time to read some more of the 17,000
(and counting) journal, conference, and workshop papers they have not yet read in the
invaluable ACL Anthology. For psychologists presumably similar considerations apply,
with the additional complication that many of the anthology papers require a substan-
tial amount of technical prior knowledge. In addition, it might be that the different
publication cultures are a limiting factor here as well: for psychologists, journals are the
main publication outlet; for them most non-journal publications have a low status and
hence might be perceived as not worth exploring.
Another perhaps more interesting reason is that psychologists and computational
linguists have subtly different general objectives. Psychologists want to get a better
understanding of people; how their social context determines their language behavior,
how they produce and comprehend sentences, and so on. Their models are evaluated
in terms of whether there is statistical evidence for their predictions in actual human
behavior. Computational linguists evaluate their models (?algorithms?) on large col-
lections of human-produced data; one model is better than another if it accounts for
more of the data. Of course, a model can perform well when evaluated on human data,
but be completely unrealistic from a psychological point of view. If a computational
linguist develops a referring expression generation algorithm (or a machine translation
system or an automatic summarizer) which accounts for the data in a way which is
psychologically unrealistic, the work will generally not be of interest to psychologists.
Conversely, if psychological insights are difficult to formalize or require complex algo-
rithms or data structures, computational linguists are likely not to be enthusiastic about
applying them. Obviously, this hinders cross-pollination of ideas as well.
Third, and somewhat related to the previous point, it sometimes seems that compu-
tational linguists see trees where psychologists see a forest. Psychologists appear to be
most interested in showing a general effect (and are particularly appreciative of clever
and elegant experimental designs which reveal these effects); if merely counting words
already gives you a statistically reliable effect, thenwhy bother with amore complicated
way of counting n-grams and worrying about back-off smoothing to deal with data
sparseness? Doing so would conceivably give you a better estimate of the significance
and size of your effect, but would probably not change your story in any fundamental
way. Computational linguists, by contrast, evaluate their models on (often shared) data-
sets (and tend to be more impressed by technical prowess?e.g., new statistical machine
learning models?or by smart ways of automatically collecting large quantities of data);
each data point that is processed incorrectly by their model offers a potential advantage
for someone else?s model.
In view of observations such as these, it is perhaps not surprising that compu-
tational linguists and psychologists have remained largely unaware of each other?s
work so far. Predicting the future is a tricky thing, but it seems not unlikely that most
computational linguists and psychologists will continue going their own way in the
future. Nevertheless, I hope to have shown here that both communities could benefit
from the occasional foray into the others? territory. For psychologists, the tools and tech-
niques developed by computational linguists could further their research, by helping to
make their models and theories more explicit and hence easier to test and compare.
For computational linguists, insights from both the social psychology of language and
from psycholinguists could contribute to a range of applications, from opinion mining
to text understanding and generation. Obviously, this contribution could be on the
level of ?words?, but a more substantial contribution is conceivable as well. As we
have seen, psychologists are particularly strong in explanatory theories (on affect, on
interaction, etc.) and perhaps taking these as starting points for our applications (e.g.,
292
Krahmer Last Words
on affective and interactive generation) could make them theoretically more interesting
and empirically more adequate.
Acknowledgments
Thanks to Robert Dale for inviting me to
write a Last Words piece on this topic and
for his useful comments on an earlier
version. This piece grew out of discussions I
had over the years with both computational
linguists and psychologists, including
Martijn Balsters, Kees van Deemter, Albert
Gatt, Roger van Gompel, Erwin Marsi,
Diederik Stapel, Marc Swerts, Marie?t
Theune, and Ad Vingerhoets. Needless
to say, I alone am responsible for the
simplifications and opinions in this work.
I received financial support from the
Netherlands Organization for Scientific
Research (NWO), via the Vici project
?Bridging the gap between computational
linguistics and psycholinguistics: The case
of referring expressions? (277-70-007),
which is gratefully acknowledged.
References
Arts, Anja. 2004. Overspecification in
Instructive Texts. Unpublished Ph.D.
thesis, Tilburg University.
Branigan, Holly P., Martin J. Pickering, Jamie
Pearson, and Janet F. McLean. (in press).
Linguistic alignment between humans
and computers. Journal of Pragmatics.
Brennan, Susan and Herbert H. Clark. 1996.
Conceptual pacts and lexical choice in
conversation. Journal of Experimental
Psychology, 22(6):1482?1493.
Brennan, Susan E. and Joy E. Hanna. 2009.
Partner-specific adaptation in dialog.
Topics in Cognitive Science, 1(2):274?291.
Brown-Schmidt, S. and M. Tanenhaus. 2004.
Priming and alignment: Mechanism or
consequence? commentary on Pickering
and Garrod 2004. Behavioral and Brain
Sciences, 27:193?194.
Brown-Schmidt, Sarah. 2009. Partner-specific
interpretation of maintained referential
precedents during interactive dialog.
Journal of Memory and Language,
61:171?190.
Chater, Nick and Christopher D. Manning.
2006. Probabilistic models of language
processing and acquisition. Trends in
Cognitive Science, 10:335?344.
Chung, C. K. and James W. Pennebaker. 2007.
The psychological function of function
words. In K. Fiedler, editor, Social
Communication: Frontiers of Social
Psychology. Psychology Press, New York,
pages 343?359.
Clark, Herbert H. 1996. Using Language.
Cambridge University Press,
Cambridge, UK.
Clark, Herbert H. and Adrian Bangerter.
2004. Changing ideas about reference. In
Ira A. Noveck and Dan Sperber, editors,
Experimental Pragmatics. Palgrave
Macmillan, Basingstoke, pages 25?49.
Clark, Herbert H. and Deanna Wilkes-Gibbs.
1986. Referring as a collaborative process.
Cognition, 22:1?39.
Cohn, M., M. Mehl, and James W.
Pennebaker. 2004. Linguistic markers
of psychological change surrounding
September 11, 2001. Psychological
Science, 15:687?693.
Crocker, Matthew W. (in press).
Computational psycholinguistics. In Alex
Clark, Chris Fox, and Shalom Lappin,
editors, Computational Linguistics and
Natural Language Processing Handbook.
Wiley Blackwell, London, UK.
Dale, Robert and Ehud Reiter. 1995.
Computational interpretations of the
Gricean maxims in the generation of
referring expressions. Cognitive Science,
18:233?263.
Douglas, Karen and Robbie Sutton.
2006. When what you say about others
says something about you: Language
abstraction and inferences about
describers? attitudes and goals. Journal
of Experimental Social Psychology, 42:
500?508.
Engelhardt, Paul E., Karl G. D. Bailey, and
Fernanda Ferreira. 2006. Do speakers and
listeners observe the Gricean Maxim of
Quantity? Journal of Memory and Language,
54:554?573.
Gatt, Albert, Anja Belz, and Eric Kow. 2009.
The tuna-reg challenge 2009: Overview
and evaluation results. In Proceedings
of the 12th European Workshop on
Natural Language Generation (ENLG),
pages 174?182, Athens.
Halevy, Alon, Peter Norvig, and Fernando
Pereira. 2009. The unreasonable
effectiveness of data. IEEE Intelligent
Systems, 24:8?12.
Holmes, David I. 1998. The evolution of
stylometry in humanities scholarship.
Literary and Linguistic Computing,
13:111?117.
293
Computational Linguistics Volume 36, Number 2
Horton, W. S. and B. Keysar. 1996. When
do speakers take into account common
ground? Cognition, 59:91?117.
Hovy, Eduard H. 1990. Pragmatics and
natural language generation. Artificial
Intelligence, 43:153?197.
Jurafsky, Dan. 2003. Probabilistic
modeling in psycholinguistics: Linguistic
comprehension and production. In Rens
Bod, Jennifer Hay, and Stefanie Jannedy,
editors, Probabilistic Linguistics. MIT Press,
Cambridge, MA, pages 39?96.
Keysar, B., S. Lin, and D. J. Barr. 2003.
Limits on theory of mind use in adults.
Cognition, 89:25?41.
Krahmer, Emiel and Diederik Stapel. 2009.
Abstract language, global perception:
How language shapes what we see. In
Proceedings of the Annual Meeting of the
Cognitive Science Society, pages 286?291,
Amsterdam.
Kronmu?ller, E. and Dale Barr. 2007.
Perspective-free pragmatics: Broken
precedents and the recovery-from-
preemption hypothesis. Journal of
Memory and Language, 56:436?455.
Maass, A., D. Salvi, L. Arcuri, and G. Semin.
1989. Language use in intergroup contexts:
The linguistic intergroup bias. Journal
of Personality and Social Psychology,
57:981?993.
Metzing, Charles A. and Susan E. Brennan.
2003. When conceptual pacts are broken:
Partner effects on the comprehension of
referring expressions. Journal of Memory
and Language, 49:201?213.
Olson, David R. 1970. Language and
thought: Aspects of a cognitive theory
of semantics. Psychological Review,
77:257?273.
Pado, Ulrike, Matthew W. Crocker, and
Frank Keller. 2009. A probabilistic model
of semantic plausibility in sentence
processing. Cognitive Science, 33:794?838.
Pang, B. and L. Lee. 2008. Opinion mining
and sentiment analysis. Foundations and
Trends in Information Retrieval, 2:1?135.
Paraboni, Ivandre?, Kees van Deemter,
and Judith Masthoff. 2007. Generating
referring expressions: Making referents
easy to identity. Computational Linguistics,
33:229?254.
Pennebaker, James W., C. Groom, D. Loew,
and J. Dabbs. 2004. Testosterone as a social
inhibitor: Two case studies of the effect of
testosterone treatment on language. Journal
of Abnormal Psychology, 113:172?175.
Pickering, Martin and Simon Garrod. 2004.
Towards a mechanistic psychology of
dialogue. Behavioural and Brain Sciences,
27:169?226.
Reiter, Ehud. 2007. The shrinking horizons of
computational linguistics. Computational
Linguistics, 33:283?287.
Rude, S., E. Gortner, and James W.
Pennebaker. 2004. Language use of
depressed and depression-vulnerable
college students. Cognition and Emotion,
18:1121?1133.
Semin, Gu?n. 2009. Language and social
cognition. In F. Strack and J. Fo?rster,
editors, Social Cognition: The Basis of Human
Interaction. Psychology Press, London,
pages 269?290.
Semin, Gu?n and Klaus Fiedler. 1988. The
cognitive functions of linguistic categories
in describing persons: Social cognition and
language. Journal of Personality and Social
Psychology, 34:558?568.
Stapel, Diederik and Gu?n Semin. 2007.
The magic spell of language. Journal of
Personality and Social Psychology, 93:23?33.
Stirman, Shannon and James W. Pennebaker.
2001. Word use in the poetry of suicidal
and nonsuicidal poets. Psychosomatic
Medicine, 63:517?522.
Wardlow Lane, Liane, Michelle Groisman,
and Victor S. Ferreira. 2006. Don?t talk
about pink elephants! : Speakers? control
over leaking private information during
language production. Psychological Science,
17:273?277.
Wigboldus, Danie?l, Gu?n Semin, and Russell
Spears. 2000. How do we communicate
stereotypes? linguistic bases and
inferential consequences. Journal of
Personality and Social Psychology, 78:5?18.
Winograd, Terry. 1980. What does it mean to
understand language? Cognitive Science,
4:209?241.
294
Computational Generation of Referring
Expressions: A Survey
Emiel Krahmer?
Tilburg University
Kees van Deemter??
University of Aberdeen
This article offers a survey of computational research on referring expression generation (REG).
It introduces the REG problem and describes early work in this area, discussing what basic
assumptions lie behind it, and showing how its remit has widened in recent years. We discuss
computational frameworks underlying REG, and demonstrate a recent trend that seeks to link
REG algorithms with well-established Knowledge Representation techniques. Considerable at-
tention is given to recent efforts at evaluating REG algorithms and the lessons that they allow
us to learn. The article concludes with a discussion of the way forward in REG, focusing on
references in larger and more realistic settings.
1. Introduction
Suppose one wants to point out a person in Figure 1 to an addressee. Most speakers
have no difficulty in accomplishing this task, by producing a referring expression
such as ?the man in a suit,? for example. Now imagine a computer being confronted
with the same task, aiming to point out individual d1. Assuming it has access to a
database containing all the relevant properties of the people in the scene, it needs to
find some combination of properties which applies to d1, and not to the other two.
There is a choice though: There are many ways in which d1 can be set apart from the
rest (?the man on the left,? ?the man with the glasses,? ?the man with the tie?), and
the computer has to decide which of these is optimal in the given context. Moreover,
optimality can mean different things. It might be thought, for instance, that references
are optimal when they are minimal in length, containing just enough information to
single out the target. But, as we shall see, finding minimal references is computationally
expensive, and it is not necessarily what speakers do, nor what is most useful to hearers.
So, what is Referring Expression Generation? Referring expressions play a central role
in communication, and have been studied extensively in many branches of (com-
putational) linguistics, including Natural Language Generation (NLG). NLG is con-
cerned with the process of automatically converting non-linguistic information (e.g.,
? Tilburg Center for Cognition and Communication (TiCC), Tilburg University, The Netherlands.
E-mail: e.j.krahmer@uvt.nl.
?? Computing Science Department, University of Aberdeen, Scotland, UK. E-mail: k.vdeemter@abdn.ac.uk.
Submission received: 16 December 2009; revised submission received: 27 April 2011; accepted for publication:
15 June 2011.
? 2012 Association for Computational Linguistics
Computational Linguistics Volume 38, Number 1
Figure 1
A simple visual scene.
from a database) into natural language text, which is useful for practical applications
ranging from generating weather forecasts to summarizing medical information (Reiter
and Dale 2000). Of all the subtasks of NLG, Referring Expression Generation (REG) is
among those that have received most scholarly attention. A survey of implemented,
practical NLG systems shows that virtually all of them, regardless of their purpose,
contain an REG module of some sort (Mellish et al 2006). This is hardly surprising
in view of the central role that reference plays in communication. A system providing
advice about air travel (White, Clark, and Moore 2010) needs to refer to flights (?the
cheapest flight,? ?the KLM direct flight?), a pollen forecast system (Turner et al 2008)
needs to generate spatial descriptions for areas with low or high pollen levels (?the cen-
tral belt and further North?), and a robot dialogue system that assembles construction
toys together with a human user (Giuliani et al 2010) needs to refer to the components
(?insert the green bolt through the end of this red cube?).
REG ?is concerned with how we produce a description of an entity that enables
the hearer to identify that entity in a given context? (Reiter and Dale 2000, page 55).
Because this can often be done in many different ways, a REG algorithm needs to make
a number of choices. According to Reiter and Dale (2000), the first choice concerns what
form of referring expression is to be used; should the target be referred to, for instance,
using its proper name, a pronoun (?he?), or a description (?the man with the tie?).
Proper names have limited applicability because many domain objects do not have
a name that is in common usage. For pronoun generation, a simple but conservative
rule is discussed by Reiter and Dale (2000), similar to one proposed by Dale (1989,
pages 150?151): Use a pronoun if the target was mentioned in the previous sentence,
and if this sentence contained no reference to any other entity of the same gender.
Reiter and Dale (2000) concentrate mostly on the generation of descriptions. If the NLG
system decides to generate a description, two choices need to be made: Which set of
properties distinguishes the target (content selection), and how the selected properties
are to be turned into natural language (linguistic realization). Content selection is a
complex balancing act: We need to say enough to enable identification of the intended
referent, but not too much. A selection of information needs to be made, and this needs
to be done quickly. Reiter and Dale discuss various strategies that try to manage this
174
Krahmer and van Deemter Computational Generation of Referring Expressions
balancing act, based on Dale and Reiter (1995), an early survey article that summarizes
and compares various influential algorithms for the generation of descriptions.
Why a survey on REG, and how to read it? REG, like NLG in general, has changed
considerably since the overviews presented in Dale and Reiter (1995) and Reiter and
Dale (2000), owing largely to an increased use of empirical data, and a widening of
the class of referring expressions studied. Moreover, a gradual shift has taken place
towards extended application domains, different input and output formats, and more
flexible interactions with the user, and this shift is starting to necessitate the use of new
REG techniques. Examples include recent systems in areas such as weather forecasting
(Turner, Sripada, and Reiter 2009) and medical care (Portet et al 2009), where complex
references to spatial regions and time periods abound. The results of recent REG
research are scattered over proceedings, books, and journals. The current survey offers
a compact overview of the progress in this area and an assessment of the state of the art.
The concept of reference is difficult to pin down exactly (Searle 1969; Abbott 2010).
Searle therefore suggests that the proper approach is ?to examine those cases which con-
stitute the center of variation of the concept of referring and then examine the borderline
cases in light of similarities and differences from the paradigms? (Searle 1969, pages 26?
27). The ?paradigms? of reference in Reiter and Dale (2000) are definite descriptions whose
primary purpose it is to identify their referent. The vast majority of recent REG research
subscribes to this view as well. Accordingly these paradigmatic cases will also be the
main focus of this survey, although we shall often have occasion to discuss other types of
expressions. However, to do full justice to indefinite or attributive descriptions, proper
names, and personal pronouns would, in our view, require a separate, additional survey.
In Section 2 we offer a brief overview of REG research up to 2000, discussing
some classic algorithms. Next, we zoom in on the new directions in which recent
work has taken REG research: extension of the coverage of algorithms, to include, for
example, vague, relational, and plural descriptions (Section 3), exploration of different
computational frameworks, such as Graph Theory and Description Logic (Section 4),
and collection of data and evaluation of REG algorithms (Section 5). Section 6 highlights
open questions and avenues for future work. Section 7 summarizes our findings.
2. A Very Short History of Pre-2000 REG Research
The current survey focuses primarily on the progress in REG research in the 21st
century, but it is important to have a basic insight into pre-2000 REG research and how
it laid the foundation for much of the current work.
2.1 First Beginnings
REG can be traced back to the earliest days of Natural Language Processing; Winograd
(1972) (Section 8.3.3, Naming Objects and Events), for example, sketches a primitive
?incremental? REG algorithm, used in his SHRDLU program. In the 1980s, researchers
such as Appelt and Kronfeld set themselves the ambitious task of modeling the human
capacity for producing and understanding referring expressions in programs such as
KAMP and BERTRAND (Appelt 1985; Appelt and Kronfeld 1987; Kronfeld 1990). They
argued that referring expressions should be studied as part of a larger speech act. KAMP
(Appelt 1985), for example, was conceived as a general utterance planning system,
building on Cohen and Levesque?s (1985) formal speech act theory. It used logical
175
Computational Linguistics Volume 38, Number 1
axioms and a theorem prover to simulate an agent planning instructions such as ?use
the wheelpuller to remove the flywheel,? which contains two referring expressions, as
part of a larger utterance.
Like many of their contemporaries, Appelt and Kronfeld hoped to gain insight into
the complexities of human communication. Doug Appelt (personal communication):
?the research themes that originally motivated our work on generation were the out-
growth of the methodology in both linguistics and computational linguistics at the time
that research progress was best made by investigating hard, anomalous cases that pose
difficulties for conventional accounts.? Their broad focus allowed these researchers to
recognize that although referring expressions may have identification of the referent
as their main goal, a referring expression can also add information about a target. By
pointing to a tool on a table while saying ?the wheelpuller,? the descriptive content of
the referring expression may serve to inform the hearer about the function of the tool
(Appelt and Kronfeld 1987). They also observed that referring expressions need to be
sensitive to the communicative context in which they are used and that they should be
consistent with the Gricean maxims (see subsequent discussion), which militate against
overly elaborate referring expressions (Appelt 1985).
It is remarkably difficult, after 20 years, to find out how these programs actually
worked, because code was lost and much of what was written about them is pitched at
a high level of abstraction. Appelt and Kronfeld were primarily interested in difficult
questions about human communication, but they were sometimes tantalizingly brief
about humbler matters. Here, for instance, is how Appelt (1985, page 21) explains how
KAMP would attempt to identify a referent:
KAMP chooses a set of basic descriptors when planning a describe action to minimise
both the number of descriptors chosen, and the amount of effort required to plan the
description. Choosing a provably minimal description requires an inordinate amount
of effort and contributes nothing to the success of the action. KAMP chooses a set of
descriptors by first choosing a basic category descriptor (see [Rosch 1978]) for the
intended concept, and then adding descriptors from those facts about the object that are
mutually known by the speaker and the hearer, subject to the constraint that they are all
linguistically realizable in the current noun phrase, until the concept has been uniquely
identified. . . . Some psychological evidence suggests the validity of the minimal
description strategy; however, one does not have to examine very many dialogues
to find counter-examples to the hypothesis that people always produce minimal
descriptions.
This quote contains the seeds of much later work in REG, given its skepticism about
the naturalness of minimal descriptions, its use of Rosch (1978)?style basic categories,
and its acknowledgment of the role of computational complexity. Broadly speaking, it
suggests an incremental generation strategy, compatible with the ones described subse-
quently, although it is uncertain what exactly was implemented. In recent years, the
Appelt?Kronfeld line of research has largely given way to a new research tradition
which focused away from the full complexity of human communication, with notable
exceptions such as Heeman and Hirst (1995), Stone and Webber (1998), O?Donnell,
Cheng and Hitzeman (1998), and Koller and Stone (2007).
2.2 Generating Distinguishing Descriptions
In the early 1990s a new approach to REG started gaining currency, when Dale and
Reiter re-focused on the problem of determining what properties a referring expression
176
Krahmer and van Deemter Computational Generation of Referring Expressions
should use if identification of the referent is the central goal (Dale 1989, 1992; Reiter
1990; Reiter and Dale 1992). This line of work culminated in the seminal paper by
Dale and Reiter (1995). Like Appelt (1985), Dale and Reiter are concerned with the link
between the Gricean maxims and the generation of referring expressions. They discuss
the following pair of examples:
(1) Sit by the table.
(2) Sit by the brown wooden table.
In a situation where there is only one table, which happens to be brown and wooden,
both the descriptions in (1) and (2) would successfully refer to their target. However,
if you hear (2) you might make the additional inference that it is significant to know
that the table is brown and wooden; why else would the speaker mention these
properties? If the speaker merely wanted to refer to the table, your inference would be
an (incorrect) ?conversational implicature,? caused by the speaker?s violation of Grice?s
(1975, page 45) Maxim of Quantity (?Do not make your contribution more informative
than is required?). Dale and Reiter (1995) ask how we can efficiently compute which
properties to include in a description, such that it successfully identifies the target while
not triggering false conversational implicatures. For this, they zoom in on a relatively
straightforward problem definition, and compare a number of concise, well-defined
algorithms solving the problem.
Problem Definition. Dale and Reiter (1995) formulate the REG problem as follows.
Assume we have a finite domain D of objects with attributes A. In our example scene
(Figure 1), D = {d1, d2, d3} and A = {type, clothing, position, . . .}. The type attribute has a
special status in Dale and Reiter (1995) because it represents the semantic content of
the head noun. Alternatively, we could have defined an attribute gender, stating that it
should be realized as the head noun of a description. Typically, domains are represented
in a knowledge base such as Table 1, where different values are clustered together
because they are associated with the same attribute. Left, right, and middle, for example,
belong to the attribute position, and are said to be three values that this attribute can
take. The objects of which a given attribute?value combination (or ?property?) is true
are said to form its denotation. Sometimes we will drop the attribute, writing man,
rather than ?type, man?, for instance.
The REG task is now defined by Dale and Reiter (1995) through what may be
called identification of the target: given a target (or referent) object r ? D, find a set
of attribute?value pairs L whose conjunction is true of the target but not of any of
the distractors (i.e., D ? {r}, the domain objects different from the target). L is called
a distinguishing description of the target. In our simple example, suppose that {d1}
is the target (and hence {d2, d3} the set of distractors), then L could, for example, be
Table 1
Tabular representation of some information in our example scene.
Object type clothing position
d1 man wearing suit left
d2 woman wearing t-shirt middle
d3 man wearing t-shirt right
177
Computational Linguistics Volume 38, Number 1
either {?type, man?, ?clothing, wearing suit?} or {?type, man?, ?position, left?}, which could be
realized as ?the man wearing a suit? or ?the man to the left.? If identification were all
that counted, a simple, fast, and fault-proof REG strategy would be to conjoin all the
properties of the referent: This conjunction will identify the referent if it can be identified
at all. In practice, Dale and Reiter, and others in their wake, include an additional
constraint that is often left implicit: that the referring expressions generated should be as
similar to human-produced ones as possible. In the Evaluation and Conclusion sections,
we return to this ?human-likeness? constraint (and to variations on the same theme).
Full Brevity and Greedy Heuristic. Dale and Reiter (1995) discuss various algorithms
which solve the REG task. One of these is the Full Brevity algorithm (Dale 1989) which
deals with the problem of avoiding false conversational implicatures in a radical way,
by always generating the shortest possible distinguishing description. Originally, the
Full Brevity algorithm was meant to generate both initial and subsequent descriptions,
by relying on a previous step that determines the distractor set based on which objects
are currently salient. Given this set, it first checks whether there is a single property of
the target that rules out all distractors. If this fails, it considers all possible combinations
of two properties, and so on:
1. Look for a description L that distinguishes target r using one property.
If success then return L. Else go to 2.
2. Look for a description L that distinguishes target r using two properties.
If success then return L. Else go to 3.
3. Etcetera
Unfortunately, there are two problems with this approach. First, the problem of finding
a shortest distinguishing description has a high complexity?it is NP hard (see, e.g.,
Garey and Johnson 1979)?and hence is computationally very expensive, making it
prohibitively slow for large domains and descriptions. Second, Dale and Reiter note that
human speakers routinely produce descriptions that are not minimal. This is confirmed
by a substantial body of psycholinguistic research (Olson 1970; Sonnenschein 1984;
Pechmann 1989; Engelhardt, Bailey, and Ferreira 2006).
An approximation of Full Brevity is the Greedy Heuristic algorithm (Dale 1989,
1992), which iteratively selects the property which rules out most of the distractors
not previously ruled out, incrementally augmenting the description based on what
property has most discriminatory power at each stage (as a result, it does not always
generate descriptions of minimal size). The Greedy Heuristic algorithm is a more
efficient algorithm than the Full Brevity one, but it was soon eclipsed by another
algorithm (Reiter and Dale 1992; Dale and Reiter 1995), which turned out to be the
most influential algorithm of the pre-2000 era. It is this later algorithm that came to be
known as ?the? Incremental Algorithm (IA).
The Incremental Algorithm. The basic idea underlying the IA is that speakers ?prefer?
certain properties over others when referring to objects, an intuition supported by the
experimental work of, for instance, Pechmann (1989). Suppose you want to refer to
a person 10 meters away from you. You might mention the person?s gender. If this
is insufficient to single out the referent, you might be more likely to make use of the
color of the person?s coat than to the color of her eyes. Less preferred attributes, such
as eye color, are only considered if other attributes do not suffice. It is this intuition
of a preference order between attributes that the IA exploits. By making this order
178
Krahmer and van Deemter Computational Generation of Referring Expressions
a parameter of the algorithm, a distinction can be made between domain/genre
dependent knowledge (the preferences), and a domain-independent search strategy.
As in the Greedy Heuristic algorithm, descriptions are constructed incrementally;
but unlike the Greedy Heuristic, the IA checks attributes in a fixed order. By grouping
properties into attributes, Dale and Reiter predict that all values of a given attribute have
the same preference order. Ordering attributes rather than values may be disadvanta-
geous, however. A simple shape (e.g., a circle), or a size that is unusual for its target (e.g.,
a tiny whale) might be preferred over a subtle color (purplish gray). Also, some values of
a given attribute might be difficult to express, and ?dispreferred? for this reason (kind of
like a ufo shape with a christmas tree sticking out the side).
Figure 2 contains a sketch of the IA in pseudo code. It takes as input a target
object r, a domain D consisting of a collection of domain objects, and a domain-specific
list of preferred attributes Pref (1). Suppose we apply the IA to d1 of our example
scene, assuming that Pref = type > clothing > position. The description L is initialized
as the empty set (2), and the context set C of distractors (from which d1 needs to be
distinguished) is initialized as D ? {d1} (3). The algorithm then iterates through the list
of preferred attributes (4), for each one looking up the target?s value (5), and checking
whether this attribute?value pair rules out any of the distractors not ruled out so far (6).
The function RulesOut (?Ai, V?) returns the set of objects which have a different value
for attribute Ai than the target object has. If one or more distractors are ruled out, the
attribute?value pair ?Ai, V? is added to the description under construction (7) and a new
set of distractors is computed (8). The first attribute to be considered is type, for which
d1 has the value man. This would rule out d2, the only woman in our domain, and hence
the attribute?value pair ?type, man? is added to L. The new set of distractors is C = {d3},
and the next attribute (clothing) is tried. Our target is wearing suit, and the remaining
distractor is not, so the attribute?value pair ?clothing, wearing suit? is included as well. At
this point all distractors are ruled out (10), a set of properties has been found which
uniquely characterize the target {?type, man?, ?clothing, wearing suit?} (the man wearing a
suit), and we are done (11). If we had reached the end of the list without ruling out all
distractors, the algorithm would have failed (13): No distinguishing description for our
target was found.
The sketch in Figure 2 simplifies the original algorithm in a number of respects.
First, Dale and Reiter always include the type attribute, even if it does not rule out any
Figure 2
Sketch of the core Incremental Algorithm.
179
Computational Linguistics Volume 38, Number 1
distractors, because speakers use type information in virtually all their descriptions.
Second, the original algorithm checks, via a function called UserKnows, whether a given
property is in the common ground, to prevent the selection of properties which the
addressee might not understand. Unlike Appelt and Kronfeld, who discuss detailed
examples that hinge on differences in common ground, Dale and Reiter (1995) treat
UserKnows as a function that returns ?true? for each true proposition, assuming that
all relevant information is shared. Third, the IA can take some ontological information
into account via subsumption hierarchies. For instance, in a dog-and-cat domain, a pet
may be of the chihuahua type, but chihuahua is subsumed by dog, and dog in turn is
subsumed by animal. A special value in such a subsumption hierarchy is reserved for the
so-called basic level values (Rosch 1978); dog in this example. If an attribute comes with
a subsumption hierarchy, the IA first computes the best value for that attribute, which
is defined as the value closest to the basic level value, such that there is no more specific
value that rules out more distractors. In other words, the IA favors dog over chihuahua,
unless the latter rules out more distractors.
The IA is conceptually straightforward and easy to implement. In addition, it
is computationally efficient, with polynomial complexity: Its worst-case run time is a
constant function of the total number of attribute?value combinations available. This
computational efficiency is due to the fact that the algorithm does not perform back-
tracking: once a property has been selected, it is included in the final referring expres-
sion, even if later additions render it superfluous. As a result, the final description may
contain redundant properties. Far from seeing this as a weakness, Dale & Reiter (1995,
page 19) point out that this makes the IA less ?psycholinguistically implausible? than its
competitors. It is interesting to observe that whereas Dale and Reiter (1995) discuss the
theoretical complexity of the various algorithms in detail, later research has tended to
attach more importance to empirical evaluation of the generated expressions (Section 5).
2.3 Discussion
Appelt and Kronfeld?s work, founded on the assumption that REG should be seen as
part of a comprehensive model of communication, started to lose some of its appeal in
the early 1990s because it was at odds with the emerging research ethos in computa-
tional linguistics that stressed simple, well-defined problems allowing for measurable
results. The way current REG systems are shaped is largely due to developments
summarized in Dale and Reiter (1995), which focuses on a specific aspect of REG,
namely, determining which properties serve to identify some target referent. Dale and
Reiter?s work aimed for generating human-like descriptions, but was not yet coupled
with systematic investigation of data.
REG as Search. The algorithms discussed by Dale and Reiter (1995) can be seen as
different instantiations of a general search algorithm (Bohnet and Dale 2005; Gatt 2007).
They all basically search through the same space of states, each consisting of three com-
ponents: a description that is true of the target, a set of distractors, and a set of properties
of the target that have not yet been considered. The initial state can be formalized as the
triple ??, C, P? (no description for the target has been constructed, no distractors have
been ruled out, and all properties P of the target are still available), and the goal state as
?L, ?, P??, for certain L and P?: A description L has been found, which is distinguishing?
the set of distractors is empty. All other states in the search space are intermediate ones,
through which an algorithm might move depending on its search strategy. For instance,
180
Krahmer and van Deemter Computational Generation of Referring Expressions
when searching for a distinguishing description for d1 in our example domain, an inter-
mediate state could be s = ?{?type, man?}, {d3}, {?clothing, wearing suit?, ?position, left?}?.
The algorithms discussed earlier differ in terms of their so-called expand-method,
determining how new states are created, and their queue-method, which determines the
order in which these states are visited (i.e., how states are inserted into the queue). Full
Brevity, for example, uses an expand-method that creates a new state for each attribute
of the target not checked before (as long as it rules out at least one distractor). Starting
from the initial state and applied to our example domain, this expand-method would
result in three new states, creating descriptions including type, clothing, and position
information, respectively. These states would be checked using a queue-method which
is breadth-first. The IA, by contrast, uses a different expand-method, each time creating
a single new state in accordance with the pre-determined preference order. Thus, in
the initial state, and assuming (as before) that type is the most preferred attribute, the
expand-method would create a single new state: s above. Because there is always only
one new state, the queue-method is trivial.
Limitations of pre-2000 REG. In the IA and related algorithms, the focus is on efficiently
computing which properties to use in a distinguishing description. These algorithms
rest on a number of implicit simplifications of the REG task, however. (1) The target is
always just one object, not a larger set (hence, plural noun phrases are not generated). (2)
The algorithms all assume a very simple kind of knowledge representation, consisting
of a set of atomic propositions. Negated propositions are only represented indirectly,
via the Closed World Assumption, so an atomic proposition that is not explicitly listed in
the database is false. (3) Properties are always ?crisp,? never vague. Vague properties
such as small and large are treated as Boolean properties, which do not allow borderline
cases and which keep the same denotation, regardless of the context in which they are
used. (4) All objects in the domain are assumed to be equally salient, which implies that
all distractors have to be removed, even those having a very low salience. (5) The full
REG task includes first determining which properties to include in a description, and
then providing a surface realization in natural language of the selected properties. The
second stage is not discussed, nor is the relation with the first. A substantial part of re-
cent REG research is dedicated to lifting one or more of these simplifying assumptions,
although other limitations are still firmly in place (as we shall discuss in Section 6).
3. Extending the Coverage
3.1 Reference to Sets
Until recently, REG algorithms aimed to produce references to a single object. But
references to sets are ubiquitous in most text genres. In simple cases, it takes only a
slight modification to allow classic REG algorithms to refer to sets. The IA, for example,
can be seen as referring to the singleton set {r} that contains the target r and nothing
else. If in line 1 (Figure 2), {r} is replaced by an arbitrary set S, and line 3 is modified
as saying C ? D ? S instead of C ? D ? {r}, then the algorithm produces a description
that applies to all elements of S. Thus, it is easy enough to let these algorithms produce
expressions like ?the men? or ?the t-shirt wearers,? to identify {d1, d3} and {d2, d3},
respectively. Unfortunately, things are not always so simple. What if we need to refer
to the set {d1, d2}? Based on the properties in Table 1 alone this is not possible, because
d1 and d2 have no properties in common. The natural solution is to treat the target set
181
Computational Linguistics Volume 38, Number 1
as the union of two smaller sets, {d1} ? {d2}, and refer to both sets separately (e.g., ?the
man who wears a suit, and the woman?). Once unions are used, it becomes natural to
allow set complementation as well, as in ?the people who are not on the right.? Note
that set complementation may also be useful for single referents. Consider a situation
where all cats except one are owned by Mary, and the owner of the remaining one is
unknown or non-existent. Complementation allows one to refer to ?the cat not owned
by Mary.? We shall call the resulting descriptions Boolean.
As part of a more general logical analysis of the IA, van Deemter (2002) made a
first stab at producing Boolean descriptions, using a two-stage algorithm whose first
stage is a generalization of the IA, and whose second stage involves the optimization of
the possibly lengthy expressions produced by the first phase. The resulting algorithm is
logically complete in the following sense: If a given set can be described at all using the
properties available then this algorithm will find such a description. With intersection
as the only way to combine properties, REG cannot achieve logical completeness.
The first stage of the algorithm starts by conjoining properties (man, left) (omitting
attributes for the sake of readability) in the familiar manner of the IA; if this does not
suffice for singling out the target set then the same incremental process continues with
unions of two properties (e.g., man ? woman, middle ? left; that is, properties expressing
that a referent is a man or a woman, in the middle or on the left), then with unions of
three properties (e.g., man ? wearing suit ? woman), and so on. The algorithm terminates
when the referent (individual or set) is identified (success) or when all combinations
of properties have been considered (failure). Figure 3 depicts this in schematic form,
where n represents the total number of properties in the domain, and P+/? denotes the
set of all literals (atomic properties such as man, and their complements ?man). Step (1)
generalizes the original IA allowing for negated properties and target sets. As before, L
is the description under construction. It will consist of intersections of unions of literals
such as (woman ? man) ? (woman ? ?wearing suit) (in other words, L is in Conjunctive
Normal Form, CNF).
Note that this first stage is not only incremental at each of its n steps, but also
as a whole: Once a property has been added to the description, later steps will not
remove it. This can lead to redundancies, even more than in the original IA. The
second stage removes the most blatant of these, but only where the redundancy exists
as a matter of logic, rather than world knowledge. Suppose, for example, that Step 2
selects the properties P ? S and P ? R, ruling out all distractors. L now takes the
form (P ? S) ? (P ? R) (e.g., ?the things that are both (women or men) and (women
or wearing suits)?). The second phase uses logic optimization techniques, originally
designed for the minimization of digital circuits (McCluskey 1965), to simplify this to
P ? (S ? R) (?the women, and the men wearing suits?).
Figure 3
Outline of the first stage of van Deemter?s (2002) Boolean REG algorithm.
182
Krahmer and van Deemter Computational Generation of Referring Expressions
Variations and Extensions. Gardent (2002) drew attention to situations where this pro-
posal produces unacceptably lengthy descriptions; suppose, for example, the algorithm
accumulates numerous properties during Steps 1 and 2, before finding one complex
property (a union of three properties) during Step 3 which, on its own would have
sufficed to identify the referent. This will make the description generated much length-
ier than necessary, because the properties from Steps 1 and 2 are now superfluous.
Gardent?s take on this problem amounts to a reinstatement of Full Brevity embedded
in a reformulation of REG as a constraint satisfaction problem (see Section 4.2). The
existence of fast implementations for constraint satisfaction alleviates the problems with
computational tractability to a considerable extent. But by re-instating Full Brevity,
algorithms like Gardent?s could run into the empirical problems noted by Dale and
Reiter, given that human speakers frequently produce non-minimal descriptions (see
Gatt [2007] for evidence pertaining to plurals).
Horacek (2004) makes a case for descriptions in Disjunctive Normal Form (DNF;
unions of intersections of literals). Horacek?s algorithm first generates descriptions
in CNF, then convert these into DNF, skipping superfluous disjuncts. Consider our
example domain (Table 1). To refer to {d1, d2}, a CNF-oriented algorithm might gen-
erate (man ? woman) ? (left ? middle) (?the people who are on the left or middle?).
Horacek converts this, first, into DNF: (man ? left) ? (woman ? middle) ? (man ? middle) ?
(woman ? left), after which the last two disjuncts are dropped, because there are no men
in the middle, and no women on the left. The outcome could be worded as ?the man on
the left and the woman in the middle.? Later work has tended to agree with Horacek in
opting for DNF instead of CNF (Gatt 2007; Khan, van Deemter, and Ritchie 2008).
Perspective and Coherence. Recent studies have started to bring data-oriented methods
to the generation of references to sets (Gatt 2007; Gatt and van Deemter 2007; Khan,
van Deemter, and Ritchie 2008). One finding is that referring expressions benefit from a
?coherent? perspective. For example, ?the Italian and the Greek? is normally a better
way to refer to two people than ?the Italian and the cook,? because the former is
generated from one coherent perspective (i.e., nationalities). Two questions need to be
addressed, however. First, how should coherence be defined? Gatt (2007) opted for a
definition that assesses the coherence of a combination of properties using corpus-based
frequencies as defined by Kilgarriff (2003), which in turn is based on Lin (1998). This
choice was supported by a range of experiments (although the success of the approach
is less well attested for descriptions that contain adjectives). Secondly, what if full
coherence can only be achieved at the expense of brevity? Suppose a domain contains
one Italian and two Greeks. One of the Greeks is a cook, whereas the other Greek and
the Italian are both IT consultants. If this is all that is known, the generator faces a choice
between either generating a description that is fully coherent but unnecessarily lengthy
(?the Italian IT consultant and the Greek cook?), or brief but incoherent (?The Italian
and the cook?). Simply saying ?The Italian and the Greek? would not be distinguishing.
In such cases, coherence becomes a tricky, and computationally complex, optimization
problem (Gatt 2007; Gatt and van Deemter 2007).
Collective Plurals. Reference to sets is a rich topic, where many issues on the border-
line between theoretical, computational, and experimental linguistics are waiting to be
explored. Most computational proposals, so far, use properties that apply to individual
objects. To refer to a set, in this view, is to say things that are true of each member of
the set. Such references may be contrasted with collective ones (e.g., ?the lines that run
parallel to each other,? ?the group of four people?) which are more complicated from a
183
Computational Linguistics Volume 38, Number 1
semantic point of view (Scha and Stallard 1988; L?nning 1997, among others). For initial
ideas about the generation of collective plurals, we refer to Stone (2000).
3.2 Relational Descriptions
Another important limitation of most early REG algorithms is that they are restricted to
one-place predicates (e.g., ?being a man?), instead of relations involving two or more
arguments. Even a property like ?wearing a suit? is modeled as if it were simply a
one-place predicate without internal structure (instead of a relation between a person
and a piece of clothing). This means that the algorithms in question are unable to
identify one object via another, as when we say ?the woman next to the man who wears
a suit,? and so on.
One early paper does discuss relational descriptions, making a number of important
observations about them (Dale and Haddock 1991). First, it is possible to identify an
object through its relations to other objects without identifying each of these objects
separately. Consider a situation involving two cups and two tables, where one cup is on
one of the tables. In this situation, neither ?the cup? nor ?the table? is distinguishing,
but ?the cup on the table? succeeds in identifying one of the two cups. Secondly,
descriptions of this kind can have any level of ?depth?: in a complex situation, one might
say ?the white cup on the red table in the kitchen,? and so on. To be avoided, however,
are the kinds of repetitions that can arise from descriptive loops, because these do not
add information. It would, for example, be useless to describe a cup as ?the cup to the
left of the saucer to the right of the cup to the left of the saucer . . . ? We shall return to this
issue in Section 4, where we shall ask how suitable each of a number of representational
frameworks is for the proper treatment of relational descriptions.
Various researchers have attempted to extend the IA by allowing relational descrip-
tions (Horacek 1996; Krahmer and Theune 2002; Kelleher and Kruijff 2006), often based
on the assumption that relational properties (like ?x is on y?) are less preferred than
non-relational ones (like ?x is white?). If a relation is required to distinguish the target
x, the basic algorithm is applied iteratively to y. It seems, however, that these attempts
were only partly successful. One of the basic problems is that relational descriptions?
just like references to sets, but for different reasons?do not seem to fit in well with an
incremental generation strategy. In addition, it is far from clear that relational properties
are always less preferred than non-relational ones (Viethen and Dale 2008). Viethen
and Dale suggest that even in simple scenes, where objects can easily be distinguished
without relations, participants still use relations frequently (in about one third of the
trials). We return to this in Section 5.
On balance, it appears that the place of relations in reference is only partly under-
stood, with much of the iceberg still under water. If two-place relations can play a role
in REG, then surely so can n-place relations for larger n, as when we say ?the city that
lies between the mountains and the sea? (n = 3). No existing proposal has addressed n-
place relations in general, however. Moreover, human speakers can identify a man as the
man who ?kissed all women,? ?only women,? or ?two women.? The proposals discussed
so far do not cover such quantified relations, but see Ren, van Deemter, and Pan (2010).
3.3 Context Dependency, Vagueness, and Gradability
So far we have assumed that properties have a crisply defined meaning that is fixed,
regardless of the context in which they are used. But many properties fail to fit this
mold. Consider the properties young and old, for example. In Figure 1, it is the leftmost
184
Krahmer and van Deemter Computational Generation of Referring Expressions
male who looks the older of the two. But if we add an old-age pensioner to the scene
then suddenly he is the most obvious target of expressions like ?the older man? or ?the
old man.? Whether a man counts as old or not, in other words, depends on what other
people he is compared to: being old is a context-dependent property. The concept of
being ?on the left? is context-dependent too: Suppose we add five people to the right of
the young man in Figure 1; now all three characters originally depicted are suddenly on
the left, including the man in the t-shirt who started out on the right.
Concepts like ?old? and ?left? involve comparisons between objects. Therefore, if
the knowledge base changes, the objects? descriptions may change as well. But even
if the knowledge base is kept constant, the referent may have to be compared against
different objects, depending on the words in the expression. The word ?short? in ?John
is a short basketball player,? for example, compares John?s height with that of the other
basketball players, whereas ?John is a short man? compares its referent with all the
other men, resulting in different standards for what it means to be short.
?Old? and ?short? are not only context dependent but also gradable, meaning that
you can be more or less of it (older, younger, shorter, taller) (Quirk et al 1980). Gradable
words are extremely frequent, and in many NLG systems they are of great importance,
particularly in those that have numerical input, for example, in weather forecasting
(Goldberg, Driedger, and Kittredge 1994) or medical decision support (Portet et al
2009). In addition to being context-dependent, they are also vague, in the sense that
they allow borderline cases. Some people may be clearly young, others clearly not, but
there are borderline cases for whom it is not quite clear whether they were included.
Context can help to diminish the problem, but it won?t go away: In the expression
?short basketball player,? the noun gives additional information about the intended
height range, but borderline cases still exist.
Generating Vague References. REG, as we know it, lets generation start from a Knowl-
edge Base (KB) whose facts do not change as a function of context. This means that
context-dependent properties like a person?s height need to be stored in the KB in a
manner that does not depend on other facts. It is possible to deal with size adjectives in
a principled way, by letting one?s KB contain a height attribute with numerical values.
Our running example can be augmented by giving each of the three people a precise
height, for example: height(d1) = 170 cm, height(d2) = 180 cm and height(d3) = 180 cm
(here the height of the woman d2 has been increased for illustrative purposes). Now
imagine we want to refer to d3. This target can be distinguished by the set of two
properties {man, height = 180 cm}. Descriptions of this kind can be produced by means
of any of the classic REG algorithms.
Given that type and height identify the referent uniquely, this set of properties can be
realized simply as ?the man who is 180 cm tall.? But other possibilities exist. Given that
180 cm is the greatest height of all men in this KB, the set of properties can be converted
into {man, height = maximum}, where the exact height has been pruned away. The new
description can be realized as ?the tallest man? or simply as ?the tall man? (provided
the referent?s height exceeds a certain minimum value). The algorithm becomes more
complicated when sets are referred to (because the elements of the target set may not all
have the same heights), or when two or more gradable properties are combined (as in
?the strong, tall man in the expensive car?) (van Deemter 2006).
Variations and Extensions. Horacek (2005) integrates vagueness with other types of
uncertainty. Horacek could be said to depict an REG algorithm as essentially a gambler
who wants to maximize the chance of the referent being identified on the basis of
185
Computational Linguistics Volume 38, Number 1
the generated expression. Other things being equal, for example, it may be safer to
identify a dog as being ?owned by John,? than as being ?tall,? because the latter involves
borderline cases. A similar approach can be applied to perceptual uncertainty (as when
it is uncertain whether the hearer will be able to observe a certain property), or to the
uncertainty associated with little-known words (e.g., will the hearer know what a basset
hound is?) Quantifying all types of uncertainties could prove problematic in practice,
yet by portraying a generator as a gambler, Horacek has highlighted an important
aspect of reference generation which had so far been ignored. Crucially, his approach
makes the success of a description a matter of degrees.
The idea that referential success is a matter of degrees appears to be confirmed
by recent applications of REG to geo-spatial data. Here there tend to arise situations
in which it is simply not feasible to produce a referring expression that identifies its
target with absolute precision (though good approximations may exist). Once again, the
degree of success of a referring expression becomes gradable. Suppose you were asked
to describe that area of Scotland where the temperature is expected to fall below zero
on a given night, based on some computer forecast of the weather. Even if we assume
that this is a well-defined area with crisp boundaries, it is not feasible to identify the
area precisely, because listing all the thousands of data points that make up the area
separately is hardly an option. Various approximations are possible, including:
(3) Roads above 500 meters will be icy.
(4) Roads in the Highlands will be icy.
Descriptions of this kind are generated by a system for road gritting, where the decision
of which roads to treat with salt depends on the description generated by the system
(Turner, Sripada, and Reiter 2009): Roads where temperatures are predicted to be icy
should be treated with salt; others should not. These two descriptions are arguably
only partially successful in singling out the target area. Generally speaking, one can
distinguish between false positives and false negatives: The former are roads that are
covered by the description but should not be (because the temperature there is not
predicted to fall below zero); the latter are icy roads that will be left un-gritted. Turner
and colleagues decided that it would be unacceptable to have even one false negative.
In other situations, safety (from accidents) and environmental damage (through salt)
might be traded off in different ways, for example, by associating a finite cost with
each false positive and a possibly different cost with each false negative, and choos-
ing the description that is associated with the lowest total cost (van Deemter 2010,
pages 253?254). Again, a crucial and difficult part is to come up with the right cost
figures.
3.4 Degrees of Salience and the Generation of Pronouns
When we speak about the world around us, we do not pay equal attention to all the
objects in it. In a novel, for example, a sentence like ?Smiley saw the man approaching?
does not mean that Smiley saw the only man: It simply means that Smiley saw the man
who is most salient at this stage of the novel. Passonneau (1996) and Jordan (2000) have
shown how algorithms such as the IA may produce reasonable referring expressions
?in context,? by limiting the set of salient objects in some sensible way?for example,
to those objects mentioned in the previous utterance. Salience, in these works, was
treated as a two-valued, ?black-or-white? concept. But perhaps it is more natural to
186
Krahmer and van Deemter Computational Generation of Referring Expressions
think of salience?just like height or age?as coming in degrees. Existing theories of
linguistic salience do not merely separate what is salient from what is not. They assign
referents to different salience bands, based on factors such as recency of mention and
syntactic structure (Gundel, Hedberg, and Zacharski 1993; Hajic?ova? 1993; Grosz, Joshi,
and Weinstein 1995).
Salience and Context-Sensitive REG. Early REG algorithms (Kronfeld 1990; Dale and
Reiter 1995) assumed that salience could be modeled by means of a focus stack
(Grosz and Sidner 1986): A referring expression is taken to refer to the highest element
on the stack that matches its description (see also DeVault, Rich, and Sidner 2004).
Krahmer and Theune (2002) argue that the focus stack approach is not flexible enough
for context-sensitive generation of descriptions. They propose to assign individual
salience weights (sws) to the objects in the domain, and to reinterpret referring expres-
sions like ?the man? as referring to the currently most salient man. Once such a gradable
notion of salience is adopted, we are back in the territory of Section 3.3. One simple way
to generate context-sensitive referring expressions is to keep the algorithm of Figure 2
exactly as it is, but to limit the set of distractors to only those domain elements whose
salience weight is at least as high as that of the target r. Line 3 (Figure 2) becomes:
3?. C ? {x | sw(x) ? sw(r)} ? {r}
To see how this works, consider the knowledge base of Table 1 once again, assuming
that sw(d1) = sw(d2) = 10, and sw(d3) = 0 (d1 and d2 are salient, for example, because
they were just talked about, and d3 was not). Suppose we keep the same domain and
preference order as before. Now if d1 is the target, then, according to the new definition
3?, C = {d1, d2} ? {d1} = {d2} (i.e., d2 is the only distractor which is at least as salient as
the target, d1). The algorithm will select ?type, man?, which rules out the sole distractor
d2, leading to a successful reference (?The man?). If, however, d3 would be the target
then C = {d1, d2, d3} ? {d3} = {d1, d2}, and the algorithm would operate as normal,
producing a description realizable as ?the man in the t-shirt.? Krahmer and Theune
chose to graft a variant of this idea onto the IA, but application to other algorithms is
straightforward.
Krahmer and Theune (2002) compare two theories of computing linguistic
salience?one based on the hierarchical focus constraints of Hajic?ova? (1993), the other
on the centering constraints of Grosz, Joshi, and Weinstein (1995). They argue that the
centering constraints, combined with a gradual decrease in salience of non-mentioned
objects (as in the hierarchical focus approach) yields the most natural results. Interest-
ingly, the need to compute salience scores can affect the architecture of the REG module.
In Centering Theory, for instance, the salience of a referent is co-determined by the
syntactic structure of the sentence in which the reference is realized; it matters whether
the reference is in subject, object, or another position. This suggests an architecture in
which REG and syntactic realization should be interleaved, a point to which we return
subsequently.
Variations and Extensions. Once salience of referring expressions is taken into account
and they are no longer viewed as de-contextualized descriptions of their referent, a
number of questions come up. When, for example, is it appropriate to use a demon-
strative (?this man,? ?that man?), or a pronoun (?he,? ?she?)? As for demonstratives, it
has proven remarkably difficult to decide when these should be used, and even harder
to choose between the different types of demonstratives (Piwek 2008). Concerning
pronouns, Krahmer and Theune suggested that ?he? abbreviates ?the (most salient)
187
Computational Linguistics Volume 38, Number 1
man,? and ?she? ?the (most salient) woman.? In this way, algorithms for generating
distinguishing descriptions might also become algorithms for pronoun generation. Such
an approach to pronoun generation is too simple, however, because additional factors
are known to determine whether a pronoun is suitable or not (McCoy and Strube 1999;
Henschel, Cheng, and Poesio 2000; Callaway and Lester 2002; Kibble and Power 2004).
Based on analyses of naturally occurring texts, McCoy and Strube (1999), for example,
emphasized the role of topics and discourse structure for pronoun generation, and
pointed out that the changes in time scale are a reliable cue for this. In particular,
they found that in certain places a definite description was used where a pronoun
would have been unambiguous. This happened, for example, when the time frame of
the current sentence differed from that of the sentence in which the previous mention
occurred, as can be signaled, for example, by a change in tense or a cue-phrase such as
?several months ago.? Kibble and Power (2004), in an alternative approach, use Cen-
tering Theory as their starting point in a constraint-based text generation framework,
taking into account constraints such as salience, cohesion, and continuity for the choice
of referring expressions.
Many studies on contextual reference take text as their starting point (Poesio and
Vieira 1998; Belz et al 2010, among others), unlike the majority of REG research dis-
cussed so far, which uses standard knowledge representations of the kind exemplified in
Table 1 (or some more sophisticated frameworks, see Section 4). An interesting variant
is presented by Siddharthan and Copestake (2004), who set themselves the task of gen-
erating a referring expression at a specific point in a discourse, without assuming that a
knowledge base (in the normal sense of the word) is available: All their algorithm has
to go by is text. For example, a text might start saying ?The new president applauded
the old president.? From this alone, the algorithm has to figure out whether, in the next
sentence, it can talk about ?the old president? (or some other suitable noun phrase)
without risk of misinterpretation by the reader. The authors argue that standard REG
methods can achieve reasonable results in such a setting, particularly (as we shall see
next) with respect to the handling of lexical ambiguities that arise when a word can
denote more than one property. Lexical issues such as these transcend the selection of
semantic properties. Clearly, it is time for us to consider matters that lie beyond Content
Determination.
Before we do this, however, we would like to mention that differences in salience
can also be caused by nonlinguistic factors: Some domain objects, for example, may be
less salient because they are further removed from the hearer than others. Paraboni,
van Deemter, and Masthoff (2007) demonstrated experimentally that such situations
may require substantial deviations from existing algorithms to avoid causing unreason-
able amounts of work to the reader. To see the idea, consider the way we refer to a
(non-salient) address on a map: we probably don?t say ?Go to house number 3012 in
Aberdeen,? even if only one house in Aberdeen has that number and this description
is thus perfectly distinguishing. It is more likely we say something like ?Go to house
number 3012 So-and-so Road, in the West End of Aberdeen,? adding logically redun-
dant information specifically to aid the hearer?s search.
3.5 Beyond Content Determination
In many early REG proposals, Lexical Choice and Surface Realization follow Content
Determination, in the style of a pipeline, with most of the actual research focusing
predominantly on Content Determination. One might have thought that good results
are easy to achieve by sending the output of the Content Determination module to
188
Krahmer and van Deemter Computational Generation of Referring Expressions
a generic realizer (i.e., a program converting meaning representations into natural
language). With hindsight, any such expectations must probably count as naive.
Some REG studies have taken a different approach, interleaving Content Determi-
nation and Surface Realization (Horacek 1997; Stone and Webber 1998; Krahmer and
Theune 2002; Siddharthan and Copestake 2004), running counter to the pipeline archi-
tecture (Mellish et al 2006). In this type of approach, syntactic structures are built up
in tandem with semantic descriptions: when ?type, man? has been added to the semantic
description, a partial syntactic tree is constructed for a noun phrase, whose head noun is
man. As more properties are added to the semantic description, appropriate modifiers
are slotted into the syntax tree; finally, the noun phrase is completed by choosing an
appropriate determiner.
Even in these interleaved architectures, it is often assumed that there is a one-to-one
correspondence between properties and words; but often a property can be expressed
by different words, one of which may be more suitable than the other?for example,
because it is unambiguous whereas the other is not (Siddharthan and Copestake 2004).
One president may be ?old? in the sense of former, whereas another is ?old? in the
sense of aged, in which case ?the old president? can become ambiguous between the
two people. To deal with the choice between ?old? and ?former,? Siddharthan and
Copestake propose to look at discriminatory power, the idea being that in this case
?former? rules out more distractors than ?old? (both presidents are old). One wonders,
however, to what extent readers interpret ambiguous words ?charitably?: Suppose two
presidents are aged, while only one is the former president. In this situation, ?the old
president? seems clear enough, because only one of its two interpretations justifies the
definite article (namely the one where ?old? is to be understood as ?former?). Clearly,
people?s processing of ambiguous expressions is an area where there is still much to
explore.
If we turn away from Siddharthan and Copestake?s set-up, and return to the sit-
uation where generation starts from a non-textual knowledge base, similar problems
with ambiguities may arise. In fact, the problem is not confined to Lexical Choice:
Ambiguities can arise during Surface Realization as well. To see this, suppose Content
Determination has selected the properties man and with telescope to refer to a person,
and the result after Surface Realization and Lexical Choice is ?John saw the man with the
telescope?; then, once again, the clarity of the semantic description can be compromised
by putting the description in a larger context, causing an attachment ambiguity, which
may sometimes leave it unclear what man is the intended referent of the description.
The generator can save the day by choosing a different realization, generating ?John saw
the man who holds the telescope? instead. Similar ambiguities occur in conjoined references
to plurals, as in ?the old men and women,? where ?old? may or may not pertain to the
women. These issues have been studied in some detail as part of a systematic study of
the ambiguities that arise in coordinated phrases of the form ?the Adjective Noun and
Noun,? asking when such phrases give rise to actual comprehension problems, and
when they should be avoided by a generator (Chantree et al 2005; Khan, van Deemter,
and Ritchie 2008).
When the generated referring expressions are realized in a medium richer than plain
text, for instance, in the context of a virtual character (Gratch et al 2002), another set of
issues comes into play. It needs to be decided, then, which words should be emphasized
in speech, possibly in combination with visual cues such as eyebrow movements and
other gestures. Doing full justice to the expanding literature on multimodal reference
is beyond the scope of this survey, but a few pointers may be useful. Various early
studies looked at multimodal reference (Lester et al 1999). One account, where pointing
189
Computational Linguistics Volume 38, Number 1
gestures directly enter the Content Determination module of REG, is presented by van
der Sluis and Krahmer (2007), who focus on the trade-off between gestures and words.
Kopp, Bergmann, and Wachsmuth (2008) are more ambitious, modeling different kinds
of pointing gestures and integrating their approach with the generation strategy of
Stone et al (2003).
3.6 Discussion
Early REG research made a number of simplifying assumptions, and as a result the early
REG algorithms could only generate a limited variety of referring expressions. When
researchers started lifting some of these assumptions, this resulted in REG algorithms
with an expanded repertoire, being able to generate, for instance, plural and relational
descriptions. This move created a number of new challenges, however. For instance, the
number of ways in which one can refer to a set of target objects increases, so choosing a
good referring expression is more difficult as well. Should we prefer, for example, ?the
men not wearing an overcoat,? ?the young man and the old man,? or ?the men left of
the woman?? In addition, from a search perspective, the various proposals result in a
larger search space, making computational issues more pressing. For some of the exten-
sions (e.g., where Boolean combinations of properties are concerned), the complexity
of the resulting algorithm is substantially higher than that of the base IA. Moreover,
researchers have often zoomed in on one extension of the IA, developing a new version
which lifts one particular limitation. Combining all the different extensions into one
algorithm which is capable of, say, generating references to salient sets of objects, using
negations and relations and possibly vague properties, is a non-trivial enterprise. To
give just one example, consider what happens when we combine salience with (other)
gradable properties (cf. Sections 3.4 and 3.3). Should ?the old man? be interpreted as
?the oldest of the men that are sufficiently salient? or ?the most salient of the men that
are sufficiently old?? Expressions that combine gradable properties can easily become
unclear, and determining when such combinations are nevertheless acceptable is an
interesting challenge.
Some simplifying assumptions have only just begun to be lifted, through extensions
that are only in their infancy, particularly in terms of their empirical validation. Other
simplifying assumptions are still in place. For instance, there is a dearth of work that ad-
dresses functions of referring expressions other than mere identification. Similarly, even
recent proposals tend to assume that it is unproblematic to determine what information
is shared between speaker and hearer. We return to these issues in Section 6.
4. REG Frameworks
Most early REG algorithms represent knowledge in a very basic way, specifically
designed for REG. This may have been justified at the time, but years of research
in Knowledge Representation (KR) suggest that such a carefree attitude towards the
modeling of knowledge may not be wise in the long run. For example, when well-
established KR frameworks are used, it may become possible to re-use existing algo-
rithms for these frameworks, which have often been optimized for speed, and whose
computational properties are well understood. Depending on the choice of framework,
many other advantages can ensue. Because research that couples REG with KR is rela-
tively new, and technical properties of the frameworks themselves can be easily found
elsewhere, we shall be comparatively brief. For each framework, we focus on three
190
Krahmer and van Deemter Computational Generation of Referring Expressions
questions: (a) How is domain information represented? (b) How is the semantic content
of a referring expression represented? and (c) How can distinguishing descriptions be
found?
4.1 REG Using Graph Search
One of the first attempts to link REG with a more generic mathematical formalism
was the proposal by Krahmer, van Erk, and Verleg (2003), who used labeled directed
graphs for this purpose. In this approach, objects are represented as the nodes (vertices)
in a graph, and the properties of and relations between these objects are represented
as edges connecting the nodes. Figure 4 shows a graph representation of our example
domain. One-place relations (i.e., properties) such as man are modeled as loops (edges
beginning and ending in the same node), whereas two-place relations such as left of are
modeled as edges between different nodes.
Two kinds of graphs play a role: a scene graph representing the knowledge base,
and referring graphs representing the content of referring expressions. The problem
of finding a distinguishing referring expression can now be defined as a comparison
between graphs. More specifically, it is a graph search problem: Given a target object
(i.e., a node in the scene graph), look for a distinguishing referring graph that is a
subgraph of the scene graph and uniquely characterizes the target. Intuitively, such a
distinguishing graph can be ?placed over? the target node with its associated edges,
and not over any other node in the scene graph. The informal notion of one graph being
?placed over? another corresponds with a subgraph isomorphism (Read and Corneil
1977). Figure 5 shows a number of referring graphs which can be placed over our target
object d1. The leftmost, which could be realized as ?the man,? fails to distinguish our
target, because it can be ?placed over? the scene graph in two different ways (over
nodes 1 and 3).
Krahmer, van Erk, and Verleg (2003) use cost functions to guide the search process
and to give preference to some solutions over others. They assume that these cost
functions are monotonic, so extending a graph can never make it cheaper. Graphs are
compatible with many different search algorithms, but Krahmer et al employ a simple
branch and bound algorithm for finding the cheapest distinguishing graph for a given
target object. The algorithm starts from the graph containing only the node representing
the target object and recursively tries to extend this graph by adding adjacent edges:
edges starting from the target, or in any of the other vertices added later on to the
Figure 4
Representation of our example scene in Figure 1 as a labeled directed graph.
191
Computational Linguistics Volume 38, Number 1
Figure 5
Some referring graphs for target d1.
referring graph under construction. For each referring graph, the algorithm checks
which objects in the scene graph it may refer to, other than the target; these are the
distractors. As soon as this set is empty, a distinguishing referring graph has been found.
At this point, only alternatives that are cheaper than this best solution found so far need
to be inspected. In the end, the algorithm returns the cheapest distinguishing graph
which refers to the target, if one exists; otherwise it returns the empty graph.
One way to define the cost function would be to assign each edge a cost of one
point. Then the algorithm will output the smallest graph that distinguishes a target
(if one exists), just as the Full Brevity algorithm would. Alternatively, one could assign
costs in accordance with the list of preferred attributes in the IA, making more preferred
properties cheaper than less preferred ones. A third possibility is to compute the costs
of an edge e in terms of the probability P(e) that e occurs in a distinguishing descrip-
tion (which can be estimated by counting occurrences in a corpus), making frequent
properties cheap and rare ones expensive:
cost(e) = ?log2(P(e))
Experiments with stochastic cost functions have shown that these enable the graph-
based algorithm to capture a lot of the flexibility of human references (Krahmer et al
2008; Viethen et al 2008).
In the graph-based perspective, relations are treated in the same way as individual
properties, and there is no risk of running into infinite loops (?the cup to the left of
the saucer to the right of the cup . . . ?). Unlike Dale and Haddock (1991) and Kelleher
and Kruijff (2006), no special measures are required, because a relational edge is either
included in a referring graph or not: including it twice is not possible. Van Deemter and
Krahmer (2007) show that many of the proposals discussed in Section 3 can be recast in
terms of graphs. They argue, however, that the graph-based approach is ill-suited for
representing disjunctive information. Here, the fact that directed graphs are not a fully
fledged KR formalism makes itself felt. Whenever an REG algorithm needs to reason
with complex information, heavier machinery is required.
4.2 REG Using Constraint Satisfaction
Constraint satisfaction is a computational paradigm that allows efficient solving of NP?
hard combinatoric problems such as scheduling (van Hentenryck 1989). It is among the
earliest frameworks proposed for REG (Dale and Haddock 1991), but in later years, this
approach has seldom been emphasized?with a few notable exceptions, such as Stone
and Webber (1998)?until Gardent (2002) showed how constraint programming can be
192
Krahmer and van Deemter Computational Generation of Referring Expressions
used to generate expressions that refer to sets. She proposed to represent a description
L for a target set S as a pair of set variables:
LS = ?P+S , P
?
S ?,
where one variable (P+S ) ranges over sets of properties that are true of the elements in
S and the other (P?S ) over properties that are false of the elements in S. The challenge?
taken care of by existing constraint solving programs?is to find suitable values (i.e.,
sets of properties) for these variables. To be ?suitable,? values need to fulfil a number of
REG-style constraints:
1. All the properties in P+S are true of all elements in S.
2. All the properties in P?S are false of all elements in S.
3. For each distractor d there is a property in P+S which is false of d, or there is
a property in P?S which is true of d.
The third clause says that every distractor is ruled out by either a positive property (i.e.,
a property in P+S ) or a negative property (i.e., a property in P
?
S ), or both. An example of a
distinguishing description for the singleton target set {d1} in our example scene would
be ?{man}, {right}?, because d1 is the only object in the domain who is both a man and
not on the right. The approach can be adapted to accommodate disjunctive properties
to enable reference to sets (Gardent 2002).
Constraint satisfaction is compatible with a variety of search strategies (Kumar
1992). Gardent opts for a ?propagate-and-distribute? strategy, which means that so-
lutions are searched for in increasing size, first looking for single properties, next for
combinations of two properties, and so forth. This amounts to the Full Brevity search
strategy, of course. Accordingly, Gardent?s algorithm yields a minimal distinguishing
description for a target, provided one exists. Given the empirical questions associated
with Full Brevity, it may well be worthwhile to explore alternative search strategies.
The constraint approach allows an elegant separation between the specification
of the REG problem and its implementation. Moreover, the handling of relations
is straightforwardly applicable to relations with arbitrary numbers of arguments.
Gardent?s approach does not run into the aforementioned problems with infinite loops,
because a set of properties (being a set) cannot contain duplicates. Yet, like the labeled
graphs, the approach proposed by Gardent has significant limitations, which stem from
the fact that it does not rest on a fully developed KR system. General axioms cannot be
expressed, and hence cannot play a role in logical deduction. We are forced to re-visit
the question of what is the best way for REG to represent and reason with knowledge.
4.3 REG Using Modern Knowledge Representation
To find out what is missing, let us see what happens when domains scale up. Consider
a furniture domain, and suppose every chair is in a room, that every room is in an
apartment, and every apartment in a house. Listing all relevant relations between
individual objects separately (?chair a is in room b,? ?room b is in apartment c,? ?chair
a is in apartment c,? ?apartment c is in house d?) is onerous, error prone, space-
consuming, and messy. Modern KR systems solve this problem by employing axioms
(e.g., expressing transitivity of the ?in? relation; if x is in y, and y is in z, then x is in z).
193
Computational Linguistics Volume 38, Number 1
Logical inference allows the KR system to derive implicit information. For example,
from ?chair a is in room b,? ?room b is in apartment c,? and ?apartment c is in house
d,? the transitivity of ?in? allows us to infer that ?chair a is in house d?. This combina-
tion of basic facts and general axioms allows a succinct and insightful representation
of facts.
Modern KR comes in different flavors. Recently, two different KR frameworks have
been linked with REG, one based on Conceptual Graphs (Croitoru and van Deemter
2007), the other on Description Logics (Gardent and Striegnitz 2007; Areces, Koller, and
Striegnitz 2008). The first have their origin in Sowa (1984) and were greatly enhanced
by Baget and Mugnier (2002). The latter grew out of work on KL-ONE (Brachman and
Schmolze 1985) and became even more prominent in the wider world of computing
when they came to be linked with the ontology language OWL, which underpins
current work on the semantic Web (Baader et al 2003). Both formalisms represent at-
tempts to carve out computationally tractable fragments of First-Order Predicate Logic
for defining and reasoning about concepts, and are closely related (Kerdiles 2001). For
reasons of space, we focus on Description Logic.
The basic idea is that a referring expression can be modeled as a formula of De-
scription Logic, and that REG can be viewed as the problem of finding a particular kind
of formula, namely, one that denotes (i.e., refers to) the target set of individuals. Let us
revisit our example domain, casting it as a logical model M, as follows: M = ?D, ?.??,
where D (the domain) is a finite set {d1, d2, d3} and ?.? is an interpretation function
which gives the denotation of the relevant predicates (thus: ?man? = {d1, d3}, ?left-of? =
{?d1, d2?, ?d2, d3?} etc.). Now the REG task can be formalized as: Given a model M
and a target set S ? D, look for a Description Logic formula ? such that ??? = S.
The following three expressions are the Description Logic counterparts of the referring
graphs in Figure 5:
(a) man
(b) man  wears suit
(c) man  ? left-of.(woman  wears t-shirt)
The first, (a), would not be distinguishing for d1 (because its denotation includes d3),
but (b) and (c) would. Note that  represents the conjunction of properties, and ?
represents existential restriction. Negations can be added straightforwardly, as in man
 ? wears suit, which denotes d3.
Areces, Koller, and Striegnitz (2008) search for referring expressions in a somewhat
non-standard way. In particular, their algorithm does not start with one particular target
referent: It simply attempts to find the different sets that can be referred to. They start
from the observation that REG can be reduced to computing the similarity set of each
domain object. The similarity set of an individual x is the set of those individuals that
have all the properties that x has. Areces et al present an algorithm, based on a proposal
by Hopcroft (1971), which computes the similarity sets, along with a Description Logic
formula associated with each set. The algorithm starts by partitioning the domain using
atomic concepts such as man and woman, which splits the domain in two subsets ({d1, d3}
and {d2} respectively). At the next stage, finer partitions are made by making use of con-
cepts of the form ?R.AtomicConcept (e.g., men left of a woman), and so on, always using
concepts established during one phase to construct more complex concepts during the
next. All objects are considered in parallel, so there is no risk of infinite loops. Control
194
Krahmer and van Deemter Computational Generation of Referring Expressions
over the output formulae is achieved by specifying an incremental preference order over
possible expressions, but alternative control strategies could have been chosen.
4.4 Discussion
Even though the role of KR frameworks for REG has received a fair bit of attention in
recent years, one can argue that this constitutes just the first steps of a longer journey.
The question of which KR framework suits REG best, for example, is still open; which
framework has the best coverage, which allows all useful descriptions to be expressed?
Moreover, can referring expressions be found quickly in a given framework, and is it
feasible to convert these representations into adequate linguistic realizations? Given
the wealth of possibilities offered by these frameworks, it is remarkable that much of
their potential is often left unused. In Areces, Koller, and Striegnitz?s (2008) proposal,
for example, generic axioms do not play a role, nor does logical inference. Ren, van
Deemter, and Pan (2010) sketch how REG can benefit if the full power of KR is brought
to bear, using Description Logic as an example. They show how generic axioms can
be exploited, as in the example of the furniture domain, where a simple transitivity
axiom allows a more succinct and insightful representation of knowledge. Similarly,
incomplete information can be used, as when we know that someone is either Dutch or
Belgian, without knowing which of the two. Finally, by making use of more expressive
fragments of Description Logic, it becomes possible to identify objects that previous
REG algorithms were unable to identify, as when we say ?the man who owns three
dogs,? or ?the man who only kisses women,? referring expressions that were typically
not considered by previous REG algorithms.
Extensions of this kind raise new empirical questions, as well. It is an open question,
for instance, when human speakers would be inclined to use such complex descriptions.
These problems existed even in the days of the classic REG algorithms (when it was
already possible to generate lengthy descriptions) but they have become more acute
now that it is possible to generate structurally complex expressions as well. There is
a clear need for empirical work here, which might teach us how the power of these
formalisms ought to be constrained.
5. Evaluating REG
Pre-2000 REG research gave little or no attention to the empirical evaluation of algo-
rithms. More recently, however, REG evaluation studies have started to be carried out
more and more often. It appears that most of these were predicated on the assumption
(debated in Section 7) that REG algorithms should try to generate expressions that are
optimally similar to those produced by human speakers or writers, even though?
importantly?this assumption was seldom made explicit. The dominant method at
the moment is, accordingly, to measure the similarity between generated expressions
and those in a suitable corpus of referring expressions. REG came late to corpus-based
evaluation (compared to other parts of computational linguistics) because suitable data
sets are hard to come by. In this section, we discuss what criteria a data set should meet
to make it suitable for REG evaluation, and we survey which collections are currently
available. In addition, we discuss how one is to determine the performance of an REG
algorithm on a given data set. We shall see that although much work has been done in
recent years, there are still significant open questions, particularly regarding the relation
between automatic metrics and human judgments.
195
Computational Linguistics Volume 38, Number 1
5.1 Corpora for REG Evaluation
Text corpora are full of referring expressions. For evaluating the realization of referring
expressions, such corpora are very suitable, and various researchers have used them,
for instance, to evaluate algorithms for modifier orderings (Shaw and Hatzivassiloglou
1999; Malouf 2000; Mitchell 2009). Text corpora are also important for the study of
anaphoric links between referring expressions. The texts that make up the GNOME
corpus (Poesio et al 2004), for instance, contain descriptions of museum objects and
medical patient information leaflets, with each of the two subcorpora containing some
6,000 NPs. Much information is marked up, including anaphoric links. Yet, text corpora
of this kind are of limited value for evaluating the content selection part of REG
algorithms. For that, one needs a corpus that is fully ?semantically transparent? (van
Deemter, van der Sluis, and Gatt 2006): A corpus that contains the actual properties of
all domain objects as well as the properties that were selected for inclusion in a given
reference to the target. Text corpora such as GNOME do not meet this requirement, and
it is often difficult or impossible to add all necessary information, because of the size
and complexity of the relevant domains. For this reason, data sets for content selection
evaluation are typically collected via experiments with human participants in simple
and controlled settings. Broadly speaking, two kinds of experimental corpora can be
distinguished: corpora specifically collected with reference in mind, and corpora col-
lected wholly or partly for other purposes but which have nevertheless been analyzed
for the referring expressions in them. We will briefly sketch some corpora of the latter
kind, after which we shall discuss the former in more detail.
General-Purpose Corpora. One way to elicit ?natural? references is to let participants per-
form a task for which they need to refer to objects. An example is the corpus of so-called
pear stories of Chafe (1980), in which people were asked to describe a movie about a man
harvesting pears, in a fluent narrative. The resulting narratives featured such sequences
as ?And he fills his thing with pears, and comes down and there?s a basket he puts them
in. . . . And then a boy comes by, on a bicycle, the man is in the tree, and the boy gets off
his bicycle . . . ,? where a limited set of individuals come up several times. The referring
expressions in a subset of these stories were analyzed by Passonneau (1996), who asked
how the form of the re-descriptions (such as ?he,? ?them,? and ?the man?) in these
narratives might best be predicted, comparing ?informational? considerations (which
form the core of most algorithms in the tradition started by Dale and Reiter, as we have
seen) with considerations based on Centering Theory (Grosz, Joshi, and Weinstein 1995).
Passonneau, who tested her rules on 319 noun phrases, found support for an integrated
model, where centering constraints take precedence over informational considerations.
The well-known Map Task corpus (Anderson et al 1991) is another example of a
corpus in which reference plays an important role. It consists of dialogues between two
participants; both have maps with landmarks indicated, but only one (the instruction
giver) has a route on the map and he or she instructs the other (the follower) about this
particular route. Referring expressions are routinely produced in this task to refer to
the landmarks on the maps (?the cliff?). Participants use these not only for identifica-
tion purposes but also, for instance, to verify whether they understood their dialogue
partner correctly. In the original Map Task corpus, the landmarks were labeled with
proper names (?cliff?), making them less suitable for studying content determination.
To facilitate the study of reference, the iMap corpus was created (Guhe and Bard 2008),
a modified version of the Map Task corpus where landmarks are not labelled, and
systematically differ along a number of dimensions, including type (owl, penguin, etc.),
196
Krahmer and van Deemter Computational Generation of Referring Expressions
number (singular, plural) and color; a target may thus be referred to as ?the two purple
owls.? Because participants may refer to targets more than once, it becomes possible to
study initial and subsequent reference (Viethen et al 2010).
Yet another example is the Coconut corpus (Di Eugenio et al 2000), a set of task-
oriented dialogues in which participants negotiate which furniture items they want to
buy on a fixed, shared budget. Referring expressions in this corpus (?a yellow rug for
150 dollars?) do not only contain information to identify a particular piece of furniture,
but also include properties which directly refer to the task at hand (e.g., how much
money is still available for a particular furniture item and what the state of agreement
between the negotiators is).
An attractive aspect of these corpora is that they represent fairly realistic
communication, related to a more or less natural task. However, in these corpora, the
identification of objects tends to be mixed with other communicative tasks (verification,
negotiating). This does not mean that the corpora in question are unsuitable for the
study of reference, of course. More specifically, they have been used for evaluating REG
algorithms, to compare the performance of traditional algorithms with special-purpose
algorithms that take dialogue context into account (Jordan and Walker 2005; Gupta and
Stent 2005; Passonneau 1996). For example, when the speaker attempts to persuade
the hearer to buy an item, Jordan?s Intentional Influences algorithm selects those
properties of the item that make it a better solution than a previously discussed item.
In yet other situations?for example, when a summarization is offered?all mutually
known properties of the item are selected. Jordan?s algorithm outperforms traditional
algorithms, which is perhaps not surprising given that the latter were not designed to
deal with references in interactive settings (Jordan 2000).
Dedicated Corpora. In recent years, a number of new corpora have been collected,
specifically focusing on the types of referring expressions that we are focusing on in
this survey. A number of such corpora are summarized in Table 2. In some ways, these
corpora are remarkably similar. Reflecting the prevalent aims of research on REG, for
example, they focus on descriptions that aim to identify their referent ?in one shot,?
disregarding the linguistic context of the expression (i.e., in the ?null context,? as it is
sometimes called [Viethen and Dale 2007]). In all these corpora, participants were asked
to refer to targets in a visual scene also containing the distractors. This set-up means
that the properties of target objects and their distractors are known, which makes it
comparatively easy to make these corpora semantically transparent by annotating
the references that were produced. In addition, most corpora are ?pragmatically
Table 2
Overview of dedicated Referring Expression corpora (alphabetical), with for each corpus a
representative reference, an indication of the domain, and the number of participants and
collected distinguishing descriptions.
Corpus Reference Domain Participants Descriptions
Name
Bishop Gorniak & Roy (2004) Colored cones in 3D scene 9 447
Drawer Viethen & Dale (2006) Drawers in filing cabinet 20 140
GRE3D3 Viethen & Dale (2008) Spheres, Cubes in 3D scene 63 630
iMap Guhe & Bard (2008) Various objects on a map 64 9,567
TUNA van Deemter et al (in press) Furniture, People 60 2,280
197
Computational Linguistics Volume 38, Number 1
transparent? as well, meaning that the communicative goals of the participants were
known (typically identification).
An early example is the Bishop corpus (Gorniak and Roy 2004). For this data set,
participants were asked to describe objects in various computer generated scenes. Each
of these scenes contained up to 30 objects (?cones?) randomly positioned on a virtual
surface. All objects had the same shape and size, and hence targets could only be
distinguished using their color (either green or purple) and their location on the surface
(?the green cone at the left bottom?). Each participant was asked to identify targets in
one shot, and for the benefit of an addressee who was physically present but did not
interact with the participant.
The Drawer corpus, collected by Viethen and Dale (2006), has a similar objective,
but here targets are real, being one of 16 colored drawers in a filing cabinet. On different
occasions, participants were given a random number between 1 and 16 and asked to
refer to the corresponding drawer for an onlooker. Naturally, they were asked not to
use the number; instead they could refer to the target drawers using color, row, and
column, or some combination of these. In this corpus, referring expressions (?the pink
drawer in the first row, third column?) once again solely serve an identification purpose.
Viethen and Dale (2008) also collected another corpus (GRE3D3) specifically looking
at when participants use spatial relations. For this data collection, participants were pre-
sented with 3D scenes (made with Google SketchUp) containing three simple geometric
objects (spheres and cubes of different colors and sizes, and in different configurations),
of which one was the target. Viethen and Dale (2008) found that spatial relations were
frequently used (?the ball in front of the cube?), even though they were never required
for identification. Whether this generalizes to other visual scenes (in which spatial
relations are less immediately ?available?) is an interesting question for future research.
The TUNA corpus (Gatt, van der Sluis, and van Deemter 2007; van Deemter et al
in press) was collected via a Web-based experiment, in which singular and plural
descriptions were gathered by showing participants one or two targets, where the plural
targets could either be similar (same type) or dissimilar (different type). Targets were
always displayed with six distractors, and the resulting domain objects were randomly
positioned in a 3 x 5 grid, with targets surrounded by a red border. Example trials are
shown in Figure 6.
The corpus contains two different domains: a furniture domain and a people do-
main. The first domain is based on pictures of furniture and household items, taken
from the Object Databank (see http://www.tarrlab.org/). These were manipulated so
that besides type (chair, desk, fan) also color, orientation, and size could systematically
be varied. The number of possible attributes and values in the people domain is much
Figure 6
Example trials from the TUNA corpus, a singular trial for the furniture domain (?the small blue
fan,? left) and a plural trial for the people domain (?the men with glasses,? right).
198
Krahmer and van Deemter Computational Generation of Referring Expressions
larger (and more difficult to pin down); this domain consists of a set of black and white
photographs of people (all famous mathematicians) used in an earlier study of van
der Sluis and Krahmer (2007). Properties of these photographs include gender, head
orientation, age, beard, hair, glasses, suit, shirt, and tie. It is interesting to note that the
TUNA corpus was designed to have one shortest description for each target, whereas
in other data sets, such as Viethen and Dale?s (2006) drawer corpus, a single shortest
description does not always exist. The TUNA corpus has formed the basis of three
shared REG challenges, to which we turn now.
5.2 Evaluation Metrics
How should we compare human descriptions with those produced by a REG algorithm?
When looking for measures that compute the content overlap, one source of inspiration
may come from biology and information retrieval (van Rijsbergen 1979). One measure
used in these fields is the Dice (1945) coefficient, which was originally proposed to
quantify ecologic association between species, and was first applied to REG by Gatt,
van der Sluis, and van Deemter (2007). The Dice coefficient?which is not dissimilar
to the ?match? function used by Jordan (2000)?is computed by scaling the number of
elements that two sets have in common, by the size of the two sets combined:
Dice(A, B) =
2 ? |A ? B|
|A|+ |B| (1)
The Dice measure ranges from 0 (no agreement; i.e., no elements shared between A and
B) to 1 (complete agreement; A and B share all elements). For REG, A and B can be
understood as attributes (e.g., type) or as attribute?value pairs (properties; ?type, man?).
The former option tends to be used in earlier work, but has the somewhat counter-
intuitive consequence that two descriptions which express different values of the same
attribute (?the man? and ?the woman,? say, or ?the dog? and ?the chihuahua,? in the
earlier discussed cats-and-dogs example) have a Dice score of 1. Hence, in the following
discussion we shall measure overlap in terms of properties.
An alternative to Dice that is sometimes used is the MASI (Measuring Agreement
on Set-valued Items) metric of Passonneau (2006):
MASI(A, B) = ?? |A ? B||A ? B| (2)
This is basically an extension of the well-known Jaccard (1901) metric with a weighting
function ? which biases the score in favor of similarity where one set is a sub- or a
superset of the other:
? =
?
?
?
?
?
?
?
?
?
1, if A = B
2
3 , if A ? B or B ? A
0, if A ? B = ?
1
3 , otherwise
(3)
Dice and MASI are straightforward measures for overlap, but they do have their
disadvantages. For example, they assume that all properties are independent and that
all are equally different from each other. Suppose a human participant referred to d1
in our example domain as ?the man in the suit next to a woman,? and consider the
199
Computational Linguistics Volume 38, Number 1
following two references produced by a REG algorithm: ?the man in the suit? and ?the
man next to a woman.? Both omit one property from the human reference and thus have
the same Dice and MASI scores. But only the former reference is distinguishing; the
latter is not. This problem could be solved, for example, by adopting a binary weighted
version of the metrics which multiply the resulting score with 1 for a distinguishing
description and with 0 for a non-distinguishing one.
A more general issue with these overlap metrics can be illustrated with an example
from Richard Power (personal communication). Consider the two (roughly equivalent)
expressions ?the palomino? and ?the horse with the gold coat and white mane and
tail.? Straightforward counting of attribute?value pairs would result in an overlap score
of zero, which would be misleading, because the two descriptions express essentially
the same content, with the latter description combining, in one property, all properties
expressed in the former. This problem clearly calls for a more principled approach to
representing and counting properties.
During evaluations, Dice or MASI scores are typically averaged over references for
different trials and produced by different human participants, making them fairly rough
measures. It could be that an algorithm?s predictions match the descriptions of some
participants very well, but those of other participants not at all. To partially compensate
for this, sometimes also the proportion of times an algorithm achieves a perfect match
with a human reference is reported. This measure is known, somewhat confusingly, as
Recall (Viethen and Dale 2006), the Perfect Recall Percentage (PRP) (Gatt, van der Sluis,
and van Deemter 2007), and Accuracy (Gatt, Belz, and Kow 2008).
The measures discussed so far do not take the actual linguistic realization of the
referring expressions into account. For these, string distance metrics are obvious candi-
dates, because these have proven their worth in various other areas of computational
linguistics. One well-known string distance metric, which has also been proposed for
REG evaluation, is the Levenshtein (1966) distance: The minimal number of insertions,
deletions, and substitutions needed to convert one string into another, possibly nor-
malized with respect to length (Bangalore, Rambow, and Whittaker 2000). The BLEU
(Papineni et al 2002) and NIST (Doddington 2002) metrics, which have their origin
in machine translation evaluation, have also been proposed for REG evaluation. BLEU
measures n-gram overlap between strings; for machine translation n is often set to 4, but
given that referring expressions tend to be short, n = 3 seems a better option for REG
evaluation (Gatt, Belz, and Kow 2009). NIST is a BLEU variant giving more importance
to less frequent (and hence more informative) n-grams. Finally, Belz and Gatt (2008) also
use the rouge-2 and rouge-su4 measures (Lin and Hovy 2003), originally proposed for
evaluating automatically generated summaries.
An obvious benefit of these string metrics is that they are easy to compute automat-
ically, whereas property-based evaluation measures such as Dice require an extensive
manual annotation of selected properties. However, the added value of string-based
metrics for REG is relatively unclear. It is not obvious, for instance, that a smaller
Levenshtein distance is always to be preferred over a longer one; the expressions ?the
man wearing a t-shirt? and ?the woman wearing a t-shirt? are at a mere Levenshtein
distance of 2 from each other, but only the former would be a good description for
target d3. On the other hand, ?the male person on the right? is at a Levenshtein distance
of 15 from ?the man wearing a t-shirt,? and both are perfect descriptions of d3.
Alternatively, referring expressions could also be evaluated by human judges, al-
though this obviously is more time consuming than an automatic evaluation. Gatt, Belz,
and Kow (2009) collected judgments of Adequacy (?How clear is this description? Try
to imagine someone who could see the same grid with the same pictures, but didn?t
200
Krahmer and van Deemter Computational Generation of Referring Expressions
know which of the pictures was the target. How easily would they be able to find
it, based on the phrase given??) and Fluency (?How fluent is this description?. . . Is it
good, clear English??). One may also be interested in the extent to which references are
useful for addressees. This can be evaluated in a number of different ways. Belz and
Gatt (2008), for example, first showed participants a generated description for a trial.
After participants read this description, a scene appeared and participants were asked
to click on the intended target. This allowed them to compute three extrinsic evaluation
metrics: the reading time, the identification time, and the error rate, which they defined
as the number of incorrectly identified targets.
5.3 Discussion
Three lessons can be learned from the recent work on evaluation. First, the emergence of
transparent corpora has greatly facilitated the empirical evaluation of REG algorithms,
particularly for content selection. Focusing on reference in simple situations, a number
of studies based on transparent corpora found that the IA outperformed the Full Brevity
and Greedy Heuristic algorithms (Viethen and Dale 2006; van Deemter et al in press).
There is an important catch, however: As demonstrated in van Deemter et al (in press),
the performance of the IA crucially depends on the chosen preference order. The best
preference order outperforms the other two algorithms, but many other preference
orders perform far worse. This is a problem, because no procedure for finding a good
preference order is known. (For n attributes, there are n! preference orders to consider,
so trial and error is not an option except in extremely simple cases.) Perhaps most
controversially, the authors argue that the evidence is starting to stack up in favor of
the thesis that the Greedy algorithm?or variants of the Greedy algorithm that choose
properties on the basis of more than just their discriminatory power?might be superior
to algorithms that use the same preference order all the time.
Second, evaluations suggest that human-produced descriptions differ from auto-
matically generated ones in a number of ways. Human references often include re-
dundant information, making the descriptions overspecified in ways that were not
accounted for by standard REG algorithms. An additional problem is that there appears
to be considerable individual variation, both within and between speakers, which is
something that existing REG algorithms do not model (Dale and Viethen 2010).
Third, it is still somewhat unclear what the best REG evaluation metrics are. The
three REG Challenges based on the TUNA set-up offer a wealth of information in
this respect (Gatt and Belz 2010). In each of these challenges, a number of research
teams submitted one or more REG generation systems, allowing detailed statistical
analyses over the various metrics. It was found that Dice, MASI, and PRP are very
highly correlated (all r > .95). Interestingly, these metrics correlate negatively with
the proportion of references that are minimally specified (Gatt, Belz, and Kow 2008);
in other words, systems that produce more overspecified references tend to do better
in terms of Dice and other overlap metrics. Concerning the surface realization metrics, it
was found that?when comparing different realizations of a given set of attributes?the
NIST and BLEU string metrics correlate strongly with each other (r = .9), as one might
expect, but neither correlates well with Levenshtein distance (Gatt, Belz, and Kow 2008).
As for the extrinsic measures, Gatt, Belz, and Kow (2008) only report a significant
correlation between reading time and identification time, which suggests that slow
readers are also slow identifiers, or that referring expressions that are hard to read
also make it harder to identify the intended referent. Gatt, Belz, and Kow (2009) let
participants listen to expressions that were produced either automatically or by human
201
Computational Linguistics Volume 38, Number 1
speakers, and found a strong correlation between identification accuracy and adequacy,
suggesting that more adequate references also have more correct identifications. Also,
they found a negative correlation between fluency and identification time, implying
that more fluent descriptions reduce the identification time.
It is notable that essentially no correlations were found between these extrinsic task
performance measures and the automatic metrics for human-likeness (Belz and Gatt
2008; Gatt and Belz 2010). Different explanations are possible for this lack of correlation.
Gatt and Belz (2010) note that the nature of the TUNA data could be partly responsible.
The TUNA data collection was carried out in a Web-based and relatively unrestricted
manner, and idiosyncratic references do occur in it (?a red chair, if you sit on it, your
feet would show the south east?). It is possible that for another, more controlled corpus,
a correlation between the two kinds of metrics would show up. It could also be that
people are not always very good at designing their utterances in a way that is optimal
for hearers (Horton and Keysar 1996; see also Section 6), so producing descriptions that
resemble human-produced ones is not the same as producing descriptions that are of
optimal use for hearers. This suggests that the two sets of metrics measure different
things, and that they correspond with two different aims that the designer of a REG
algorithm might have: One set of metrics could be used if the aim is to mimic speakers,
another if the aim is to produce optimal benefits for hearers.
So far, experimental evaluation has mostly been limited to the simplest of situations,
focusing on algorithms that produce singular descriptions, expressing conjunctions of
basic properties in small and artificial domains. Most of the extensions discussed in
Section 3 have not been evaluated systematically. Moreover, tasks such as the one on
which the TUNA corpus is based can be argued not to be ?ecologically valid?: Human
participants produce typewritten expressions for an imaginary audience on the basis of
abstract visual scenes. The effects of these limitations on the descriptions produced are
partly unknown, although some reassuring results have recently been obtained. It has
been shown, for example, that, speakers who address an imaginary audience refer in
similar ways to those who address an audience that is physically present (van der Wege
2009). Similarly, Koolen et al (2009) show that speaking rather than typing has no effect
on the kind and number of attributes in the referring expressions that are produced,
although speakers tend to use more words than typists to convey the same amount
of information. It would nevertheless be valuable to evaluate REG algorithms in the
context of a specific application, so the added value of different REG algorithms for a
real-life application can be gauged (Gatt, Belz, and Kow 2009).
Two recent evaluation challenges seem promising for these reasons. GREC (Belz
et al 2010) focuses on the task of deciding which form a referring expression should take
in a textual context, which is important for generating coherent texts such as summaries
(see also Section 6). GIVE (Koller et al 2010) focuses on generating directions in a virtual
3D environment, where reference is only one task among a number of others. This new
challenge has so far not included a separate test of REG algorithms employed in the
systems submitted, but it seems likely that GIVE will cause REG research to focus on
harder tasks, including reference in discourse context, reference to sets, and references
that are spread out over several utterances (Denis 2010).
6. Open Issues
In the previous sections we have discussed three main dimensions in which REG re-
search has moved beyond the state-of-the-art of 2000. Along the way, various loose ends
202
Krahmer and van Deemter Computational Generation of Referring Expressions
have been identified. For example, not all simplifying assumptions of early REG work
have been adequately addressed, and the enterprise of combining extensions is still in
its infancy (Section 3). It is unclear whether complex referring expressions can always
be found quickly, particularly where the generation process relies on theorem-proving,
and it is similarly unclear whether it is always feasible to turn the representations into
fluent natural language expressions (Section 4). Finally, empirical data has only been
collected for the simplest referring expressions, and it is still unclear what the proper
evaluation metrics are (Section 5). In this section, we suggest six further questions for
future research.
6.1 How Do We Match an REG Algorithm to a Particular Domain and Application?
Evaluation of classic REG algorithms has shown that with some preference orders, the
IA outperformed the Full Brevity and Greedy Heuristic algorithms, but with others
it performed much worse than these (van Deemter et al in press). The point is that
the IA, as it stands, is under-determined, because it does not contain a procedure for
finding a preference order. Sometimes psycholinguistic experiments come to our aid, for
instance Pechmann?s (1989) study showing that speakers have a preference for absolute
properties (color) over relative ones (size). Unfortunately, for most other attributes, no
such experiments have been done.
It seems reasonable to assume that frequency tells us something about preference:
A property that is used frequently is also more likely to be high on the list of preferred
properties (Gatt and Belz 2010; van Deemter et al in press). But suitable corpora to de-
termine preferences are rare, as we have seen, and their construction is time-consuming.
This raises the question of how much data would be needed to make reasonable guesses
about preferred properties; this could be studied, for instance, by drawing learning
curves where increasingly large proportions of a transparent corpus are used to estimate
a preference order and the corresponding performance is measured.
The IA is more drastically under-determined than most other algorithms: The Full
Brevity and the Greedy Heuristic algorithm are specified completely up to situations
where there is a tie?a tie between two equally lengthy descriptions in the first case, and
a tie between two properties that have the same discriminatory power in the second. To
resolve such ties frequency data would clearly be helpful. Similar questions apply to
other generation algorithms. For instance, the graph-based algorithm as described by
Krahmer et al (2008) assigns one of three different costs to properties (they can be free,
cheap, or somewhat expensive), and frequency data is used to determine which costs
should be assigned to which properties (properties that are almost always used in a
particular domain can be for free, etc.). A recent experiment (Theune et al 2011) suggests
that training the graph-based algorithm on a corpus with a few dozen items may
already lead to a good performance. In general, knowing how much data is required
for a new domain to reach a good level of performance is an important open problem
for many REG algorithms.
6.2 How Do We Move beyond the ?Paradigms? of Reference?
A substantial amount of REG research focuses on what we referred to in the Intro-
duction as the ?paradigms? of reference: ?first-mention? distinguishing descriptions
consisting of a noun phrase starting with ?the? that serve to identify some target, and
that do so without any further context. But how frequent are these ?paradigmatic? kinds
203
Computational Linguistics Volume 38, Number 1
of referring expressions? Poesio and Vieira (1998), in one of the few systematic attempts
to quantify the frequency of different uses of definite descriptions in segments of the
Wall Street Journal corpus, reported that ?first mention definite descriptions? are indeed
the most frequent in these texts. These descriptions often do not refer to visual objects
in terms of perceptual properties but to more abstract entities. One might think that it
matters little whether a description refers to a perceivable object or not; a description
like ?the third quarter? rules out three quarters much like ?the younger-looking man?
in our example scene rules out the older-looking distractor. It appears, however, that the
representation of the relevant facts in such cases tends to be a more complicated affair,
and it is here particularly that more advanced knowledge representation formalisms
of the kind discussed in Section 4 come into their own (a point to which we return
subsequently).
Even though first-mention definite descriptions are the most frequent in Poesio
and Vieira?s (1998) sample, other uses abound, including anaphoric descriptions and
bridging descriptions, whose generation is studied by Gardent and Striegnitz (2007).
Pronouns come to mind as well. The content determination problem for these other
kinds of referring expressions may not be overly complex, but deciding where in
a text or dialogue each kind of referring expression should be used is hard. Still,
this is an important issue for, for example, automatic summarization. One of the
problems of extractive summaries is that co-reference chains may be broken, resulting
in less coherent texts. Regeneration of referring expressions is a potentially attractive
way of regaining some of the coherence of the source document (Nenkova and
McKeown 2003). This issue is even more pressing in multi-document summarization,
where different source documents may refer to a given person in different ways; see
Siddharthan, Nenkova, and McKeown (2011) for a machine-learning approach to this
problem.
REG research often works from the assumption that referents cannot be identified
through proper names. (If proper names were allowed, why bother inventing a de-
scription?) But in real text, proper names are highly frequent. This does not only raise
the question when it?s best to use a proper name, or which version of a proper name
should be used (is it ?Prince Andrei Nikolayevich Bolkonsky,? ?Andrei Bolkonsky,? or
just ?Andrei??), but also how proper names can occur as part of a larger description,
as when we refer to a person using the description ?the author of Primary Colors,? for
example, where the proper name Primary Colors refers to a well-known book (whose
author was long unknown). Surely, it is time for REG to turn proper names into
first-class citizens.
Generation of referring expressions in a text is studied in the GREC (Generating
Referring Expressions in Context) challenges (Belz et al 2008). A corpus of Wikipedia
texts (for persons, cities, countries, rivers, and mountains) was constructed, and in each
text all elements of the coreference chain for the main subject were removed. For each
of the resulting reference gaps, a list of alternative referring expressions, referring to the
subject, was given (including the ?correct? reference, i.e., the one that was removed from
the text). One well-performing entry (Hendrickx et al 2008) predicted the correct type
of referring expression in 76% of the cases, using a memory-based learner. These results
suggest that it is feasible to learn which type of referring expression is best in which
instance. If so, REG in context could be conceived of as a two-stage procedure where
first the form of a reference is predicted, after which the content and realization are
determined. REG algorithms as described in the present survey would naturally fit into
this second phase. It would be interesting to see if such a method could be developed
for a data collection such as that of Poesio and Vieira (1998).
204
Krahmer and van Deemter Computational Generation of Referring Expressions
6.3 How Do We Handle Functions of Referring Expressions Other
than Identification?
Target identification is an important function of referring expressions, but it is not the
only one. Consider the following example, which Dale and Reiter (1995) discuss to
illustrate the limits of their approach:
(5) Sit by the newly painted table.
Here, ?the newly painted table? allows the addressee to infer that it would be better
not to touch the table. To account for examples such as this one, an REG algorithm
should be able to take into account different speaker goals (to identify, to warn, etc.)
and allow these goals to drive the generation process. These issues were already studied
in the plan-based approach to REG of Appelt and Kronfeld (Section 2.1), and more
recent work addresses similar problems using new methods. Heeman and Hirst (1995),
for example, present a plan-based, computational approach to REG where referring
is modeled as goal-directed behavior. This approach accounts for the combination of
different speaker goals, which may be realized in a single referring expression through
?information overloading? (Pollack 1991). Context is crucial here: A variant such as
?What do you think of the newly painted table?? does not trigger the intended ?don?t
touch? inference. In another extension of the plan-based approach to reference, Stone
and Webber (1998) use overloading to generate references that only become distin-
guishing when the rest of the sentence is taken into account. For example, we can say
?Take the rabbit from the hat? if there are two rabbits, as long as only one of them is in
a hat.
Plan-based approaches to natural language processing are not as popular as they
were in the 1980s and early 1990s, in part because they are difficult to develop and main-
tain. However, Jordan and Walker (2005) show that a natural language generator can
be trained automatically on features inspired by a plan-based model for REG (Jordan
2002). Jordan?s Intentional Influences model incorporates multiple communicative and
task-related problem solving goals, besides the traditional identification goal. Jordan
supports her model with data from the Coconut corpus (discussed previously) and
shows that traditional algorithms such as the IA fail to capture which properties
speakers typically select for their references, not only because these algorithms focus
on identification, but also because they ignore the interactive setting (see subsequent
discussion).
In short, it seems possible to incorporate different goals into a REG algorithm,
even without invoking complex planning machinery. This calls for a close coupling of
REG with the generation of the carrier utterance containing the generated expression,
however. What impact this has on the architecture of an NLG system, what the relevant
goals are, how combinations of different goals influence content selection and linguistic
realization, and how such expressions are best evaluated is still mostly unexplored.
Answers might come from studying REG in the context of more complex applications
where the generator may need to refer to objects for different reasons.
6.4 How Do We Generate Suitable Referring Expressions in Interactive Settings?
Ultimately, referring expressions are generated for some addressee, yet most of the
algorithms we have discussed are essentially ?addressee-blind? (Clark and Bangerter
205
Computational Linguistics Volume 38, Number 1
2004). To be fair, some researchers have paid lip service to the importance of taking
the addressee into account (cf. Dale and Reiter?s UserKnows function), but it is still
largely an open question to what extent the classical approaches to REG can be used in
interactions. In fact, there are good reasons to assume that most current REG algorithms
cannot directly be applied in an interactive setting. Psycholinguistic studies on reference
production, for example, show that human speakers do take the addressee into account
when referring?an instance of ?audience design? (Clark and Murphy 1983). Some
psycholinguists have argued that referring is an interactive and collaborative process,
with speaker and addressee forming a ?conceptual pact? on how to refer to some object
(Heeman and Hirst 1995; Brennan and Clark 1996; Metzing and Brennan 2003). This
also implies that referring is not necessarily a ?one shot? affair; rather a speaker may
quickly produce a first approximation of a reference to some target, which may be
refined following feedback from the addressee.
Others have argued that conversation partners automatically ?align? with each
other during interaction (Pickering and Garrod 2004). For instance, Branigan et al (2010)
report on a study showing that if a computer uses the word ?seat? instead of the more
common ?bench? in a referring expression, the user is subsequently more likely to use
?seat? instead of ?bench? as well. This kind of lexical alignment takes place at the level
of linguistic realization, and there is at least one NLG realizer that can mimic this process
(Buschmeier, Bergmann, and Kopp 2009). Goudbeek and Krahmer (2010) found that
speakers in an interactive setting also align at the level of content selection; they present
experimental data showing that human speakers may opt for a ?dispreferred? attribute
(even when a preferred attribute would be distinguishing) when these were salient in
a preceding interaction. The reader may want to consult Arnold (2008) for an overview
of studies on reference choice in context, Clark and Bangerter (2004) for a discussion of
studies on collaborative references, or Krahmer (2010) for a confrontation of some recent
psycholinguistic findings with REG algorithms.
Psycholinguistic studies suggest that traditional REG algorithms which rely on
some predefined ranking of attributes cannot straightforwardly be applied in an inter-
active setting. This is confirmed by the findings of Jordan and Walker (2005) and Gupta
and Stent (2005), who studied references in dialogue corpora discussed in Section 5.
They found that in these data sets, traditional algorithms are outperformed by simple
strategies that pay attention to the referring expressions produced earlier in the dia-
logue. A more recent machine learning experiment on a larger scale, using data from the
iMap corpus, confirmed the importance of features related to the process of alignment
(Viethen, Dale, and Guhe 2011). Other researchers have started exploring the generation
of referring expressions in interactive settings as well. Stoia et al (2006), for example,
presented a system that generates references in situated dialogues, taking into account
both dialogue history and spatial visual context, defined in terms of which distractors
are in the current field of vision of the speakers and how distant they are from the
target. Janarthanam and Lemon (2009) present a method which automatically adapts to
the expertise level of the intended addressee (using ?the router? when communicating
with an expert user, and ?the black block with the lights? while interacting with a
novice). This line of research fits in well with another, more general, strand of research
concentrating on choice optimization during planning based on user data (Walker et al
2007; White, Clark, and Moore 2010).
Interactive settings seem to call for sophisticated addressee modeling. Detailed
reasoning about the addressee can be computationally expensive, however, and some
psychologists have argued, based on clever experiments in which speakers and
addressees have slightly different information available, that speakers only have
206
Krahmer and van Deemter Computational Generation of Referring Expressions
limited capabilities for considering the addressee?s perspective (Horton and Keysar
1996; Keysar, Lin, and Barr 2003; Lane, Groisman, and Ferreira 2006). Some of the studies
mentioned herein, however, emphasize a level of cooperation that may not require
conscious planning: The balance of work on alignment, for example, suggests that it
is predominantly an automatic process which does not take up much computational
resource. Recently, Gatt, Krahmer, and Goudbeek (2011) proposed a new model for
interactive REG, consisting of a preference-based search process based on the IA, which
selects properties concurrently and in competition with a priming-based process, both
contributing properties to a limited capacity working memory buffer. This model offers
a new way to think about interactive REG, and the role therein for REG algorithms of
the kind discussed in this survey.
6.5 What Is the Impact of Visual Information?
In this survey, we have often discussed references to objects in shared visual scenes,
partly because this offers a useful way to illustrate an algorithm. So far, however,
relatively few REG studies have taken visual information seriously.
Most real-life scenes contain a multitude of potential referents. Just look around
you: Every object in your field of vision could be referred to. It is highly unlikely
that speakers would take all these objects into account when producing a referring
expression. Indeed, there is growing evidence that the visual system and the speech
production system are closely intertwined (Meyer, Sleiderink, and Levelt 1998; Spivey
et al 2001; Hanna and Brennan 2007), and that human speakers employ specific strate-
gies when looking at real-world scenes (Itti and Koch 2000; Wooding et al 2002, among
others). Wooding and colleagues, for instance, found that certain properties of an image,
such as changes in intensity and local contrasts, determine viewing patterns to a large
extent. Top?down strategies also play a role: For instance, areas that are currently under
discussion are looked at more frequently and for longer periods of time. Little is known
about how scene perception influences the human production of referring expressions,
however, and how REG algorithms could mimic this.
When discussing visual scenes, most REG researchers assume that some of the
relevant visual information is stored in a database (compare our visual example scene
in Figure 1 and its database representation in Table 1). Still, the conversion from one to
the other is far from trivial. Clearly, the visual scene is much more informative than
the database; how do we decide which visual information we store in the database
and which we ignore? And how do we map visual information to symbolic labels?
These are difficult questions that have received very little attention so far. A partial
answer to the first question can be found in the work of John Kelleher and colleagues,
who argue that visual and linguistic salience co-determine which aspects of a scene
are relevant for the understanding and generation of referring expressions (Kelleher,
Costello, and van Genabith 2005; Kelleher and Kruijff 2006). A partial answer to the
second question is offered by Deb Roy and colleagues (Roy and Pentland 2002; Roy
2005) who present a computational model for automatically grounding attributes based
on sensor data, and by Gorniak and Roy (2004) who apply such a model to referring
expressions.
One impediment to progress in this area is the lack of relevant human data. Most,
if not all, of the dedicated data sets discussed in Section 5 were collected using artificial
visual scenes, either consisting of grids of unrelated objects not forming a coherent
scene, or of coherent scenes of unrealistic simplicity. Generally speaking, the situation in
207
Computational Linguistics Volume 38, Number 1
psycholinguistics is not much better. Recently, some studies started exploring the effects
of more realistic visual scenes on language production. An example is Coco and Keller
(2009), who Photoshopped a number of (more or less) realistic visual scenes, manipulat-
ing the visual clutter and number of actors in each scene. They found that more clutter
and more actors resulted in longer delays before language production started, and that
these factors influenced the syntactic constructions that were used as well. A similar
paradigm could be used to collect a new corpus of human-produced references, with
targets being an integral part of a visual scene (rather than being randomly positioned
in a grid). When participants are subsequently asked to refer to objects in these scenes,
eye tracking can be used to monitor where they are looking before and during the
production of particular references. Such data would be instrumental for developing
REG algorithms which take visual information seriously.
6.6 What Knowledge Representation Framework Suits REG Best?
Recent years have seen a strengthening of the link between REG and Knowledge Rep-
resentation frameworks (see Section 4). There is a new emphasis on questions involving
(1) the expressive power of the formalism in which domain knowledge is expressed
(e.g., does the formalism allow convenient representation of n-place predicates or quan-
tification?), (2) the expressive power of the formalism in which ontological information
is expressed (e.g., can it express more than just subsumption between concepts?), (3) the
amount of support available for logical inference, and (4) the mechanisms available for
controlling the output of the generator.
To illustrate the importance of expressive power and logical inference, consider the
type of examples discussed in Poesio and Vieira (1998). What would it take to generate
an expression like ?the report on the third quarter of 2009?? It would be cumbersome to
represent the relation between all entities separately, saying that 1950 has a first quarter,
which has a report, and the same for all other years. It would be more elegant and
economical to spell out general rules, such as ?Every year has a unique first quarter,?
?Quarter 4 of a given year precedes Quarter 1 of any later year,? ?The relation ?precede?
is transitive,? and so on. As NLG is starting to be applied in large-scale applications,
the ability to capture generalizations of this kind is bound to become increasingly
important.
It is remarkable that most REG research has distanced itself so drastically from other
areas of artificial intelligence, by limiting itself to atomic facts in the knowledge base. If
REG came to be linked with modern knowledge representation formats?as opposed to
the simple attribute?value structures exemplified in Table 1?then atomic formulas are
no longer the substance of the knowledge base but merely its seeds. In many cases, re-
sources developed for the semantic Web?ontology languages such as OWL, reasoning
tools, and even the ontologies themselves?could be re-used in REG. REG could even
link up with ?real AI,? by tapping into models of common-sense knowledge, such as
Lenat (1995) or Lieberman et al (2004). The new possibilities raise interesting scientific
and strategic questions. For example, how do people generate referring expressions of
the kind highlighted by the work of Poesio and colleagues? Is this process best modeled
using a knowledge-rich approach using general axioms and deduction, or do other
approaches offer a more accurate model? Is it possible that, when REG starts to focus a
bit less on identification of the referent, the result might be a different, and possibly less
logic-oriented problem? What role could knowledge representation play in these cases?
Here, as elsewhere in REG, we see ample space for future research.
208
Krahmer and van Deemter Computational Generation of Referring Expressions
7. General Conclusion and Discussion
After preparatory work in the 1980s by Appelt and Kronfeld, and the contributions sum-
marized in Dale and Reiter (1995), the first decade of the new millennium has seen a new
surge of interest in referring expression generation. Progress has been made in three
related areas which have been discussed extensively in this survey. First, researchers
have lifted a number of simplifications present in the work of Dale and Reiter (1995)
and others, thereby considerably extending coverage of REG algorithms to include, for
instance, relational, plural, and vague references (Section 3). Second, proposals have
been put forward to recast REG in terms of existing and well-understood computational
frameworks, such as labeled directed graphs and Description Logic, with various attrac-
tive consequences (Section 4). Third, there has been a shift towards data collection and
empirical evaluation; this has made it possible to empirically evaluate REG algorithms,
which is starting to give us an improved understanding of the strengths and weaknesses
of existing work (Section 5). As a result of these developments, REG is now one of the
best-developed subfields of NLG.
How should the current state of the art in REG be assessed? The good news is that
current REG algorithms can produce natural descriptions, which may even be more
helpful than descriptions produced by people (Gatt, Belz, and Kow 2009). This is only
true when certain simplifying assumptions are made, however, as in the early REG
research typified by Dale and Reiter (1995). When REG leaves this limited ?comfort
zone,? the picture changes drastically. Although in recent years the research community
has gained a much better understanding of the challenges that face REG in that wider
arena, many of these challenges are still waiting to be met (Section 6).
7.1 New Complexities
Recent REG research has revealed various new complexities. Some of these pertain
to the nature of the target. Sets are difficult to refer to, for example, and algorithms
designed to deal with them achieve a lower human-likeness when referring to sets than
to individual objects (van Deemter et al in press). Recent efforts to let REG algorithms
refer to spatial regions suggest that in large, realistic domains, precise identification of
a target is a goal that can be approximated, but seldom achieved (Turner et al 2008;
Turner, Sripada, and Reiter 2009). Little work has been done so far on reference to
events, or to points and intervals in time (e.g., ?When Harry met Sally,? ?the moment
after the impact?), and references to abstract and other uncountable entities (e.g., water,
democracy) are beyond the horizon. Where domain knowledge derives from sensor
data?with unavoidably uncertain and noisy inputs?this is bound to cause problems
not previously addressed by REG. It is in such domains that salience (especially in the
non-linguistic sense) becomes a critical issue. When reference takes place in real life?as
opposed to a typical psycholinguistics experiment?it is often unclear what its salience
depends on. It might be that salience is partly in the eye of the beholder, and that this is
one of the causes of the considerable individual variation that exists between different
human speakers (Dale and Viethen 2010).
7.2 Human-Likeness and Evaluation
In early REG research, including Dale and Reiter (1995), it was often remarkably unclear
what exactly the proposed algorithms aimed to achieve. It was only when evaluation
studies were starting to be conducted that researchers had to ?show their cards? and
209
Computational Linguistics Volume 38, Number 1
say what they regarded as their criterion for success. In most cases, they used a form of
human-likeness as their success criterion, by comparing the expressions generated by
an algorithm with those in a corpus.
The human-likeness criterion dictates that REG algorithms are to mimic humans
?warts and all?: If speakers produce unclear descriptions, then so should algorithms.
But, of course, human-likeness is not the only yardstick that can be used. In NLG
systems whose main aim is to be practically useful, for example, it may be more
important for referring expressions to be clear than to be human-like in all respects. The
difference is important because psycholinguists have shown that human speakers have
only limited capabilities for taking the addressee into account, frequently producing
expressions that cannot be interpreted correctly by an addressee?for example, when
they are under time pressure (Horton and Keysar 1996). If usefulness or successfulness
(Garoufi and Koller 2011), rather than human-likeness, is the yardstick for success then
a different type of evaluation test needs to be used. Possible tests include, for example,
speed and accuracy of task completion (i.e., how often and how fast do readers find
the referent?). A variety of hearer-oriented tests is starting to be used in recent REG
research (Paraboni, van Deemter, and Masthoff 2007; Khan, van Deemter, and Ritchie
2008), but evaluation of REG algorithms (and of NLG in general) remains difficult (see,
e.g., Oberlander 1998; Belz 2009; and Gatt and Belz 2010). Arguably, a central problem
is that many different evaluations metrics are conceivable, and an REG algorithm (like
an NLG system in general) may well score high on some and poorly on other metrics.
Hearer-oriented experiments may also be useful for evaluating referring expres-
sions that are logically complex (cf. Section 4.4). It is one thing for an REG algorithm to
use logical quantification to generate a fairly simple description, such as ?the woman
who owns four cats,? but quite another to generate a highly complex description (?the
woman who owns four cats that are chased by between three and six dogs each of which
is fed only by men?), which can be generated using the same methods. There are difficult
methodological questions to be answered here about whether the aim of the generator is
to model human competence or human performance. And if it is performance that is to
be modeled, then this raises the question of what types of complexities are exploited by
human speakers, and what types of complexities are understandable to human hearers.
Such questions can only be answered by new empirical studies.
7.3 Widening the Scope of REG Algorithms
Much REG research has concentrated on the main ?paradigms? of reference (Searle
1969). Early work on REG treated reference as emphatically part of communication,
as we have seen (Section 2.1). But after the refocusing that went on in the 1990s, many
REG algorithms operate as if describing objects were a goal unto itself, instead of a
part of communication. Still, when referring expressions occur in their natural habitat?
in text or dialogue?then the reference game becomes subtly different, with factors
such as salience and adaptation playing important (and partly unknown) roles. In these
natural contexts, it is also not always necessary to identify a referent ?in one shot?: In
dialogue, an identification of the referent can be seen as a joint responsibility of both
dialogue partners. Even in monologue, an entire sequence of utterances may guide a
hearer towards the referent. In casual conversation, it is even unclear whether exact
identification of the referent is a requirement at all, in which case all existing algorithms
are wrong-footed. Reference in real life is also characterized by domains that are much
larger and complicated than the ones usually studied (at least until they have been
narrowed down by means of some salience metric): The set of people, for example, that
210
Krahmer and van Deemter Computational Generation of Referring Expressions
we are able to refer to in daily life is almost unlimited, and the properties that we can use
to refer to them seem almost unbounded, including not only their physical appearance
and location, but their ideas, actions, and so on. Evaluation challenges such as TUNA
REG, GREC, and GIVE have helped to bring the research community together, focusing
on small domains and, predominantly, on simple types of referring expressions. We
believe that it is time for evaluation studies to extend their remit and look at the types of
complex references that more recent REG research has drawn attention to. Such studies
would do well, in our view, to pay considerable attention to the question of which
referring expressions have the greatest benefit for readers or hearers.
One day, perhaps, all these issues will have been resolved. If there is anything that a
survey of the state of the art in REG makes clear it is that, for all the undeniable progress
in this growing area of NLG, this holy grail is not yet within reach.
Acknowledgments
The order of authors was determined
by chance; both contributed equally.
Emiel Krahmer thanks The Netherlands
Organisation for Scientific Research
(NWO) for VICI grant ?Bridging the Gap
between Computational Linguistics and
Psycholinguistics: The Case of Referring
Expressions? (277-70-007). Kees van
Deemter thanks the EPSRC?s Platform
Grant ?Affecting People with Natural
Language.? We both thank the anonymous
reviewers for their constructive comments,
and Doug Appelt, Johan van Benthem,
Robert Dale, Martijn Goudbeek, Helmut
Horacek, Ruud Koolen, Roman Kutlak,
Chris Mellish, Margaret Mitchell, Ehud
Reiter, Advaith Siddharthan, Matthew Stone,
Marie?t Theune, Jette Viethen, and, especially,
Albert Gatt for discussions and/or
comments on earlier versions of this text.
Thanks to Jette Viethen for her extensive
REG bibliography, and to Harold Miesen
for producing the image in Figure 1 and
for allowing us to use it.
References
Abbott, Barbara. 2010. Reference. Oxford
University Press, Oxford, UK.
Anderson, Anne A., Miles Bader,
Ellen Gurman Bard, Elizabeth Boyle,
Gwyneth Doherty, Simon Garrod,
Stephen Isard, Jacqueline Kowtko,
Jan McAllister, Jim Miller, Catherine
Sotillo, Henry Thompson, and
Regina Weinert. 1991. The HCRC
map task corpus. Language and Speech,
34:351?366.
Appelt, Douglas. 1985. Planning English
referring expressions. Artificial Intelligence,
26:1?33.
Appelt, Douglas and Amichai Kronfeld.
1987. A computational model of referring.
In Proceedings of the 10th International Joint
Conference on Artificial Intelligence (IJCAI),
pages 640?647, Milan.
Areces, Carlos, Alexander Koller, and
Kristina Striegnitz. 2008. Referring
expressions as formulas of Description
Logic. In Proceedings of the 5th International
Natural Language Generation Conference
(INLG), pages 42?49, Salt Fork, OH.
Arnold, Jennifer E. 2008. Reference
production: Production-internal
and addressee-oriented processes.
Language and Cognitive Processes,
23:495?527.
Baader, Franz, Diego Calvanese, Deborah
McGuinness, Daniele Nardi, and Peter
Patel-Schneider. 2003. The Description
Logic Handbook: Theory, Implementation
and Applications. Cambridge University
Press, Cambridge, UK.
Baget, Jean-Franc?ois and Marie-Laure
Mugnier. 2002. Extensions of simple
conceptual graphs: The complexity
of rules and constraints. Journal
of Artificial Intelligence Research,
16:425?465.
Bangalore, Srinivas, Owen Rambow, and
Steven Whittaker. 2000. Evaluation
metrics for generation. In Proceedings
of the 1st International Conference on
Natural Language Generation (INLG),
pages 1?8, Mitzpe Ramon.
Belz, Anja. 2009. That?s nice . . . what can you
do with it? [Last Words]. Computational
Linguistics, 35:111?118.
Belz, Anja and Albert Gatt. 2008. Intrinsic
vs. extrinsic evaluation measures
for referring expression generation.
In Proceedings of the 46th Annual Meeting
of the Association for Computational
Linguistics (ACL), Columbus, OH.
211
Computational Linguistics Volume 38, Number 1
Belz, Anja, Eric Kow, Jette Viethen, and
Albert Gatt. 2008. The GREC challenge
2008: Overview and evaluation results.
In Proceedings of the 5th International
Natural Language Generation Conference
(INLG), pages 183?191, Salt Fork, OH.
Belz, Anja, Eric Kow, Jette Viethen, and
Albert Gatt. 2010. Generating referring
expressions in context: The GREC task
evaluation challenges. In Emiel Krahmer
and Marie?t Theune, editors, Empirical
Methods in Natural Language Generation.
Springer Verlag, Berlin, pages 294?327.
Bohnet, Bernd and Robert Dale. 2005.
Viewing referring expression generation
as search. In Proceedings of the 19th
International Joint Conference on Artificial
Intelligence (IJCAI), pages 1004?1009,
Edinburgh.
Brachman, Ronald J. and James G. Schmolze.
1985. An overview of the KL?ONE
knowledge representation system.
Cognitive Science, 9(2):171?216.
Branigan, Holly P., Martin J. Pickering,
Jamie Pearson, and Janet F. McLean. 2010.
Linguistic alignment between people
and computers. Journal of Pragmatics,
42:2355?2368.
Brennan, Susan and Herbert H. Clark. 1996.
Conceptual pacts and lexical choice in
conversation. Journal of Experimental
Psychology, 22(6):1482?1493.
Buschmeier, Hendrik, Kirsten Bergmann,
and Stefan Kopp. 2009. An alignment-
capable microplanner for natural language
generation. In Proceedings of the 12th
European Workshop on Natural Language
Generation (ENLG), pages 82?89, Athens.
Callaway, Charles and James Lester. 2002.
Pronominalization in generated discourse
and dialogue. In Proceedings of the 40th
Annual Meeting of the Association for
Computational Linguistics (ACL),
pages 88?95, Philadelphia, PA.
Chafe, Wallace W. 1980. The Pear Stories:
Cognitive, Cultural, and Linguistic Aspects of
Narrative Production. Ablex, Norwood, NJ.
Chantree, Francis, Adam Kilgarriff, Anne
de Roeck, and Alistair Willis. 2005.
Disambiguating coordinations using word
distribution information. In Proceedings
of the International Conference on Recent
Advances in Natural Language Processing
(RANLP), pages 144?151, Borovets.
Clark, Herbert H. and Adrian Bangerter.
2004. Changing ideas about reference.
In Ira A. Noveck and Dan Sperber,
editors, Experimental Pragmatics. Palgrave
Macmillan, Basingstoke, UK, pages 25?49.
Clark, Herbert H. and Gregory Murphy.
1983. Audience design in meaning and
reference. In Jean Francois Le Ny and
Walter Kintsch, editors, Language and
Comprehension. North Holland, The
Netherlands, pages 287?299.
Coco, Moreno I. and Frank Keller. 2009.
The impact of visual information on
reference assignment in sentence
production. In Proceedings of the 31st
Annual Conference of the Cognitive
Science Society (CogSci), pages 274?279,
Amsterdam.
Cohen, Philip R. and Hector J. Levesque.
1985. Speech acts and rationality. In
Proceedings of the 23rd Annual Meeting
of the Association of Computational Linguists
(ACL), pages 49?60, Chicago, IL.
Croitoru, Madalina and Kees van Deemter.
2007. A conceptual graph approach to
the generation of referring expressions.
In Proceedings of the 20th International
Joint Conference on Artificial Intelligence
(IJCAI), pages 2456?2461, Hyderabad.
Dale, Robert. 1989. Cooking up referring
expressions. In Proceedings of the 27th
Annual Meeting of the Association for
Computational Linguistics (ACL),
pages 68?75.
Dale, Robert. 1992. Generating Referring
Expressions: Constructing Descriptions in a
Domain of Objects and Processes. The MIT
Press, Cambridge, MA.
Dale, Robert and Nicolas Haddock. 1991.
Generating referring expressions involving
relations. In Proceedings of the 5th Conference
of the European Chapter of the Association
of Computational Linguists (EACL),
pages 161?166, Berlin.
Dale, Robert and Ehud Reiter. 1995.
Computational interpretations of the
Gricean maxims in the generation of
referring expressions. Cognitive Science,
18:233?263.
Dale, Robert and Jette Viethen. 2010.
Attribute-centric referring expression
generation. In Emiel Krahmer and Marie?t
Theune, editors, Empirical Methods in
Natural Language Generation. Springer
Verlag, Berlin, pages 163?179.
Denis, Alexandre. 2010. Generating referring
expressions with reference domain theory.
In Proceedings of the 6th International
Natural Language Generation Conference
(INLG), pages 27?35, Trim.
DeVault, David, Charles Rich, and
Candace L. Sidner. 2004. Natural
language generation and discourse
context: Computing distractor sets from
212
Krahmer and van Deemter Computational Generation of Referring Expressions
the focus stack. In Proceedings of the 17th
International Meeting of the Florida Artificial
Intelligence Research Society (FLAIRS),
Miami Beach, FL.
Di Eugenio, Barbara, Pamela W. Jordan,
Richmond H. Thomason, and Johanna D.
Moore. 2000. The agreement process: an
empirical investigation of human?human
computer-mediated collaborative dialogs.
International Journal of Human?Computer
Studies, 53:1017?1076.
Dice, Lee R. 1945. Measures of the amount
of ecologic association between species.
Ecology, 26:297?302.
Doddington, George. 2002. Automatic
evaluation of machine translation quality
using n-gram co-occurrence statistics.
In Proceedings of the 2nd International
Conference on Human Language Technology
Research (HLT), pages 138?145,
San Diego, CA.
Engelhardt, Paul E., Karl G.D Bailey, and
Fernanda Ferreira. 2006. Do speakers and
listeners observe the Gricean Maxim of
Quantity? Journal of Memory and Language,
54:554?573.
Gardent, Claire. 2002. Generating minimal
definite descriptions. In Proceedings of the
40th Annual Meeting of the Association for
Computational Linguistics (ACL),
pages 96?103, Philadelphia, PA.
Gardent, Claire and Kristina Striegnitz. 2007.
Generating bridging definite descriptions.
In Harry Bunt and Reinhard Muskens,
editors, Computing Meaning, Volume 3.
Studies in Linguistics and Philosophy.
Springer Publishers, pages 369?396,
Berlin, DB.
Garey, Michael R. and David S. Johnson.
1979. Computers and Intractability:
A Guide to the Theory of NP?Completeness.
W. H. Freeman, New York.
Garoufi, Konstantina and Alexander Koller.
2011. Combining symbolic and
corpus-based approaches for the
generation of successful referring
expressions. In Proceedings of the 13th
European Workshop on Natural Language
Generation (ENLG), Nancy.
Gatt, Albert. 2007. Generating Coherent
References to Multiple Entities. Unpublished
Ph.D. thesis, University of Aberdeen.
Gatt, Albert and Anja Belz. 2010. Introducing
shared task evaluation to NLG: The
TUNA shared task evaluation challenges.
In Emiel Krahmer and Marie?t Theune,
editors, Empirical Methods in Natural
Language Generation. Springer Verlag,
Berlin, pages 264?293.
Gatt, Albert, Anja Belz, and Eric Kow. 2008.
The TUNA challenge 2008: Overview and
evaluation results. In Proceedings of the
5th International Conference on Natural
Language Generation (INLG),
pages 198?206, Salt Fork, OH.
Gatt, Albert, Anja Belz, and Eric Kow.
2009. The TUNA?REG challenge 2009:
Overview and evaluation results.
In Proceedings of the 12th European
Workshop on Natural Language Generation
(ENLG), pages 174?182, Athens.
Gatt, Albert, Emiel Krahmer, and Martijn
Goudbeek. 2011. Attribute preference
and priming in reference production:
Experimental evidence and computational
modeling. In Proceedings of the 33rd Annual
Meeting of the Cognitive Science Society
(CogSci), pages 2627?2632, Boston, MA.
Gatt, Albert and Kees van Deemter. 2007.
Lexical choice and conceptual perspective
in the generation of plural referring
expressions. Journal of Logic, Language
and Information, 16:423?443.
Gatt, Albert, Ielka van der Sluis, and Kees
van Deemter. 2007. Evaluating algorithms
for the generation of referring expressions
using a balanced corpus. In Proceedings of
the 11th European Workshop on Natural
Language Generation (ENLG), pages 49?56,
Schloss Dagstuhl.
Giuliani, Manuel, Mary Ellen Foster, Amy
Isard, Colin Matheson, Jon Oberlander,
and Alois Knoll. 2010. Situated reference in
a hybrid human?robot interaction system.
In Proceedings of the 6th International
Natural Language Generation Conference
(INLG), pages 67?76, Dublin.
Goldberg, Eli, Norbert Driedger, and
Richard Kittredge. 1994. Using natural
language processing to produce weather
forecasts. IEEE Expert, 9(2):45?53.
Gorniak, Peter and Deb Roy. 2004. Grounded
semantic composition for visual scenes.
Journal of Artificial Intelligence Research,
21:429?470.
Goudbeek, Martijn and Emiel Krahmer.
2010. Preferences versus adaptation
during referring expression generation.
In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics
(ACL), pages 55?59, Uppsala.
Gratch, Jonathan, Jeff Rickel, Elisabeth
Andre?, Norman Badler, Justine Cassell,
and Eric Petajan. 2002. Creating interactive
virtual humans: Some assembly required.
IEEE Intelligent Systems, 17:54?63.
Grice, Paul. 1975. Logic and conversation. In
Peter Cole and Jeffrey L. Morgan, editors,
213
Computational Linguistics Volume 38, Number 1
Syntax and Semantics, Vol. 3: Speech Acts.
Academic Press, New York, pages 43?58.
Grosz, Barbara J., Aravind K. Joshi, and Scott
Weinstein. 1995. Centering: A framework
for modeling the local coherence of
discourse. Computational Linguistics,
21:203?225.
Grosz, Barbara J. and Candace L. Sidner.
1986. Attention, intentions, and the
structure of discourse. Computational
Linguistics, 12:175?204.
Guhe, Markus and Ellen Gurman Bard.
2008. Adapting referring expressions
to the task environment. In Proceedings of
the 30th Annual Conference of the Cognitive
Science Society (CogSci), pages 2404?2409,
Austin, TX.
Gundel, Jeanette, Nancy Hedberg, and Ron
Zacharski. 1993. Cognitive status and form
of referring expressions in discourse.
Language, 69:247?307.
Gupta, Surabhi and Amanda Stent. 2005.
Automatic evaluation of referring
expression generation using corpora.
In Proceedings of the 1st Workshop on Using
Corpora in Natural Language Generation
(UCNLG), pages 1?6, Brighton.
Hajic?ova?, Eva. 1993. Issues of Sentence
Structure and Discourse Patterns?Theoretical
and Computational Linguistics, Vol. 2.
Charles University, Prague.
Hanna, Joy E. and Susan E. Brennan. 2007.
Speaker?s eye gaze disambiguates
referring expressions early during
face-to-face conversation. Journal of
Memory and Language, 57:596?615.
Heeman, Peter A. and Graeme Hirst. 1995.
Collaborating on referring expressions.
Computational Linguistics, 21(3):351?382.
Hendrickx, Iris, Walter Daelemans, Kim
Luyckx, Roser Morante, and Vincent
Van Asch. 2008. CNTS: Memory-based
learning of generating repeated references.
In Proceedings of the 5th International
Natural Language Generation Conference
(INLG), pages 194?195, Salt Fork, OH.
Henschel, Renate, Hua Cheng, and Massimo
Poesio. 2000. Pronominalisation revisited.
In Proceedings of the 18th International
Conference on Computational Linguistics
(COLING), pages 306?312, Saarbru?cken.
Hopcroft, John. 1971. An n log(n) algorithm
for minimizing states in a finite
automaton. In Zvi Kohave, editor, Theory of
Machines and Computations, pages 189?196.
Academic Press, New York.
Horacek, Helmut. 1996. A new algorithm
for generating referring expressions.
In Proceedings of the 12th European
Conference on Artificial Intelligence (ECAI),
pages 577?581, Budapest.
Horacek, Helmut. 1997. An algorithm for
generating referential descriptions with
flexible interfaces. In Proceedings of the
35th Annual Meeting of the Association
for Computational Linguistics (ACL),
pages 206?213, Madrid.
Horacek, Helmut. 2004. On referring to sets
of objects naturally. In Proceedings of the 3rd
International Conference on Natural Language
Generation (INLG), pages 70?79,
Brockenhurst.
Horacek, Helmut. 2005. Generating
referential descriptions under conditions
of uncertainty. In Proceedings of the 10th
European Workshop on Natural Language
Generation (ENLG), pages 58?67,
Aberdeen.
Horton, William S. and Boaz Keysar. 1996.
When do speakers take into account
common ground? Cognition, 59:91?117.
Itti, Laurent and Christof Koch. 2000.
A saliency-based search mechanism for
overt and covert shifts in visual attention.
Vision Research, 40:1489?1506.
Jaccard, Paul. 1901. E?tude comparative de la
distribution florale dans une portion des
alpes et des jura. Bulletin de la Socie?te?
Vaudoise des Sciences Naturelles, 37:547?579.
Janarthanam, Srinivasan and Oliver Lemon.
2009. Learning lexical alignment policies
for generating referring expressions for
spoken dialogue systems. In Proceedings
of the 12th European Workshop on Natural
Language Generation (ENLG), pages 74?81,
Athens.
Jordan, Pamela W. 2000. Intentional Influences
on Object Redescriptions in Dialogue:
Evidence from an Empirical Study. Ph.D.
thesis, University of Pittsburgh.
Jordan, Pamela W. 2002. Contextual
influences on attribute selection for
repeated descriptions. In Kees van
Deemter and Rodger Kibble, editors,
Information Sharing: Reference and
Presupposition in Language Generation
and Interpretation, pages 295?328. CSLI
Publications, Stanford, CA.
Jordan, Pamela W. and Marilyn Walker.
2005. Learning content selection rules for
generating object descriptions in dialogue.
Journal of Artificial Intelligence Research,
24:157?194.
Kelleher, John, Fintan Costello, and Josef van
Genabith. 2005. Dynamically structuring,
updating and interrelating representations
of visual and linguistics discourse context.
Artificial Intelligence, 167:62?102.
214
Krahmer and van Deemter Computational Generation of Referring Expressions
Kelleher, John and Geert-Jan Kruijff.
2006. Incremental generation of
spatial referring expressions in
situated dialog. In Proceedings of the
21st International Conference on
Computational Linguistics (COLING)
and 44th Annual Meeting of the Association
for Computational Linguistics (ACL),
pages 1041?1048, Sydney.
Kerdiles, Gwen. 2001. Saying It with Pictures:
a Logical Landscape of Conceptual Graphs.
Unpublished Ph.D. thesis, ILLC,
Amsterdam.
Keysar, Boaz, Shuhong Lin, and Dale J. Barr.
2003. Limits on theory of mind use in
adults. Cognition, 89:25?41.
Khan, Imtiaz Hussain, Kees van Deemter,
and Graeme Ritchie. 2008. Generation of
referring expressions: Managing structural
ambiguities. In Proceedings of the 22th
International Conference on Computational
Linguistics (COLING), pages 433?440,
Manchester.
Kibble, Rodger and Richard Power. 2004.
Optimizing referential coherence in text
generation. Computational Linguistics,
30:401?416.
Kilgarriff, Adam. 2003. Thesauruses for
natural language processing. In Proceedings
of the International Conference on Natural
Language Processing and Knowledge
Engineering (NLP-KE), pages 5?13,
Beijing.
Koller, Alexander and Matthew Stone.
2007. Sentence generation as a planning
problem. In Proceedings of the 45th Annual
Meeting of the Association for Computational
Linguistics Conference Proceedings (ACL),
pages 337?343, Prague.
Koller, Alexander, Kristina Striegnitz,
Donna Byron, Justine Cassell, Robert Dale,
Johanna Moore, and Jon Oberlander.
2010. The first challenge on generating
instructions in virtual environments.
In Emiel Krahmer and Marie?t Theune,
editors, Empirical Methods in Natural
Language Generation. Springer Verlag,
Berlin, pages 328?352.
Koolen, Ruud, Albert Gatt, Martijn
Goudbeek, and Emiel Krahmer. 2009.
Need I say more? On factors causing
referential overspecification. In Proceedings
of the CogSci Workshop on the Production of
Referring Expressions (PRE-CogSci 2009),
Amsterdam.
Kopp, Stefan, Kirsten Bergmann, and
Ipke Wachsmuth. 2008. Multimodal
communication from multimodal
thinking: Towards an integrated model
of speech and gesture production.
Semantic Computing, 2:115?136.
Krahmer, Emiel. 2010. What computational
linguists can learn from psychologists
(and vice versa). Computational Linguistics,
36:285?294.
Krahmer, Emiel and Marie?t Theune. 2002.
Efficient context-sensitive generation of
descriptions in context. In Kees van
Deemter and Rodger Kibble, editors,
Information Sharing: Givenness and Newness
in Language Processing. CSLI Publications,
Stanford, CA, pages 223?264.
Krahmer, Emiel, Marie?t Theune, Jette
Viethen, and Iris Hendrickx. 2008.
Graph: The costs of redundancy in
referring expressions. In Proceedings
of the International Conference on
Natural Language Generation (INLG),
pages 227?229, Salt Fork, OH.
Krahmer, Emiel, Sebastiaan van Erk,
and Andre? Verleg. 2003. Graph-based
generation of referring expressions.
Computational Linguistics, 29(1):53?72.
Kronfeld, Amichai. 1990. Reference and
Computation: An Essay in Applied Philosophy
of Language. Cambridge University Press,
Cambridge.
Kumar, Vipin. 1992. Algorithms for
constraint satisfaction problems: a survey.
Artificial Intelligence Magazine, 1:32?44.
Lane, Liane Wardlow, Michelle Groisman,
and Victor S. Ferreira. 2006. Don?t talk
about pink elephants! Speakers? control
over leaking private information during
language production. Psychological Science,
17 (4):273?277.
Lenat, Douglas. 1995. CYC: A large-scale
investment in knowledge infrastructure.
Communication of the ACM, 38:33?38.
Lester, James, Jennifer Voerman, Stuart
Towns, and Charles Callaway. 1999.
Deictic believability: Coordinating
gesture, locomotion, and speech in
lifelike pedagogical agents. Applied
Artificial Intelligence, 13:383?414.
Levenshtein, Vladimir I. 1966. Binary codes
capable of correcting deletions, insertions,
and reversals. Soviet Physics Doklady,
10:707?710.
Lieberman, Henry, Hugo Liu, Push Singh,
and Barbara Barry. 2004. Beating common
sense into interactive applications.
AI Magazine, 25(4):63?76.
Lin, Chin-Yew and Eduard Hovy. 2003.
Automatic evaluation of summaries
using N-gram co?occurrence statistics.
In Proceedings of the Human Language
Technology Conference of the North American
215
Computational Linguistics Volume 38, Number 1
Chapter of the Association for Computational
Linguistics (HLT?NAACL), pages 71?78,
Edmonton.
Lin, Dekang. 1998. An information-theoretic
definition of similarity. In Proceedings
of the 15th International Conference on
Machine Learning (ICML), pages 296?304,
Madison, WI.
L?nning, Jan Tore. 1997. Plurals and
collectivity. In Johan van Benthem and
Alice ter Meulen, editors, Handbook
of Logic and Language. Elsevier,
Amsterdam, pages 1009?1054.
Malouf, Robert. 2000. The order of
prenominal adjectives in natural
language generation. In Proceedings
of the 38th Annual Meeting of the
Association for Computational
Linguistics (ACL), pages 85?92,
Hong Kong.
McCluskey, Edward J. 1965. Introduction
to the Theory of Switching Circuits.
McGraw?Hill, New York.
McCoy, Kathleen and Michael Strube.
1999. Generating anaphoric expressions:
Pronoun or definite description?
In Proceedings of ACL Workshop on
Discourse and Reference Structure,
pages 63?71, College Park, MD.
Mellish, Chris, Donia Scott, Lynn Cahill,
Daniel Paiva, Roger Evans, and Mike
Reape. 2006. A reference architecture
for natural language generation
systems. Natural Language Engineering,
12:1?34.
Metzing, Charles A. and Susan E. Brennan.
2003. When conceptual pacts are broken:
Partner effects on the comprehension of
referring expressions. Journal of Memory
and Language, 49:201?213.
Meyer, Antje S., Astrid M. Sleiderink, and
Willem J. M. Levelt. 1998. Viewing and
naming objects: Eye movements during
noun phrase production. Cognition,
66:B25?B33.
Mitchell, Margaret. 2009. Class-based
ordering of prenominal modifiers.
In Proceedings of the 12th European Workshop
on Natural Language Generation (ENLG),
pages 50?57, Athens.
Nenkova, Ani and Kathleen R. McKeown.
2003. References to named entities:
A corpus study. In Proceedings of the
Human Language Technology (HLT)
Conference, Companion Volume,
pages 70?73, Edmonton.
Oberlander, Jon. 1998. Do the right
thing . . . but expect the unexpected.
Computational Linguistics, 24:501?507.
O?Donnell, Michael, Hua Cheng, and Janet
Hitzeman. 1998. Integrating referring and
informing in NP planning. In Proceedings
of the ACL Workshop on The Computational
Treatment of Nominals, pages 46?55,
Montreal.
Olson, David R. 1970. Language and
thought: Aspects of a cognitive theory
of semantics. Psychological Review,
77:257?273.
Papineni, Kishore, Salim Roukos, Todd
Ward, and Wei-Jing Zhu. 2002. BLEU:
A method for automatic evaluation of
machine translation. In Proceedings of
the 40th Annual Meeting of the Association
for Computational Linguistics (ACL),
pages 311?318, Philadelphia, PA.
Paraboni, Ivandre?, Kees van Deemter,
and Judith Masthoff. 2007. Generating
referring expressions: Making referents
easy to identity. Computational Linguistics,
33:229?254.
Passonneau, Rebecca. 1996. Using centering
to relax Gricean informational constraints
on discourse anaphoric noun phrases.
Language and Speech, 39:229?264.
Passonneau, Rebecca. 2006. Measuring
agreement on set-valued items (MASI)
for semantic and pragmatic annotation.
In Proceedings of the 5th International
Conference on Language Resources and
Evaluation (LREC), pages 831?836,
Genoa.
Pechmann, Thomas. 1989. Incremental
speech production and referential
overspecification. Linguistics, 27:98?110.
Pickering, Martin and Simon Garrod. 2004.
Towards a mechanistic psychology of
dialogue. Behavioural and Brain Sciences,
27:169?226.
Piwek, Paul. 2008. Proximal and distal in
language and cognition: Evidence from
deictic demonstratives in Dutch. Journal
of Pragmatics, 40:694?718.
Poesio, Massimo, Rosemary Stevenson,
Barbara di Eugenio, and Janet Hitzeman.
2004. Centering: A parametric theory and
its instantiations. Computational Linguistics,
30:309?363.
Poesio, Massimo and Renata Vieira. 1998.
A corpus-based investigation of definite
description use. Computational Linguistics,
24:183?216.
Pollack, Martha. 1991. Overloading
intentions for efficient practical reasoning.
Nou?s, 25:513?536.
Portet, Francois, Ehud Reiter, Albert Gatt,
Jim Hunter, Somayajulu Sripada, Yvonne
Freer, and Cindy Sykes. 2009. Automatic
216
Krahmer and van Deemter Computational Generation of Referring Expressions
generation of textual summaries from
neonatal intensive care data. Artificial
Intelligence, 173:789?816.
Quirk, Randolph, Sidney Greenbaum,
Geoffrey Leech, and Jan Svartvik. 1980.
A Grammar of Contemporary English
(9th ed). Longman, Burnt Mill,
Harlow, UK.
Read, Ronald C. and Derek G. Corneil. 1977.
The graph isomorphism disease. Journal
of Graph Theory, 1(1):339?363.
Reiter, Ehud. 1990. The computational
complexity of avoiding conversational
implicatures. In Proceedings of the 28th
Annual Meeting of the Association for
Computational Linguistics (ACL),
pages 97?104, Pittsburgh, PA.
Reiter, Ehud and Robert Dale. 1992. A fast
algorithm for the generation of referring
expressions. In Proceedings of the 14th
International Conference on Computational
Linguistics (COLING), pages 232?238,
Nantes.
Reiter, Ehud and Robert Dale. 2000.
Building Natural Language Generation
Systems. Cambridge University Press,
Cambridge, UK.
Ren, Yuan, Kees van Deemter, and Jeff Pan.
2010. Charting the potential of Description
Logic for the generation of referring
expressions. In Proceedings of the 6th
International Natural Language Generation
Conference (INLG), pages 115?124, Dublin.
Rosch, Eleanor. 1978. Principles of
categorization. In Eleanor Rosch and
Barbara L. Lloyd, editors, Cognition and
Categorization. Erlbaum, Hillsdale, NJ,
pages 27?48.
Roy, Deb. 2005. Grounding words in
perception and action: Computational
insights. Trends in Cognitive Sciences,
9(8):389?396.
Roy, Deb and Alex Pentland. 2002.
Learning words from sights and sounds:
A computational model. Cognitive Science,
26:113?146.
Scha, Remko and David Stallard. 1988.
Multi-level plurals and distributivity.
In Proceedings of the 26th Annual Meeting of
the Association for Computational Linguistics
(ACL), pages 17?24, Buffalo, NY.
Searle, John. 1969. Speech Acts: An Essay in
the Philosophy of Language. Cambridge
University Press, Cambridge, UK.
Shaw, James and Vasileios Hatzivassiloglou.
1999. Ordering among premodifiers.
In Proceedings of the 37th Annual Meeting of
the Association for Computational Linguistics
(ACL), pages 135?143, College Park, MA.
Siddharthan, Advaith and Ann Copestake.
2004. Generating referring expressions
in open domains. In Proceedings of the
42nd Annual Meeting of the Association
for Computational Linguistics (ACL),
pages 407?414, Barcelona.
Siddharthan, Advaith, Ani Nenkova, and
Kathleen McKeown. 2011. Information
status distinctions and referring
expressions: An empirical study of
references to people in news summaries.
Computational Linguistics, 37(4):811?842.
Sonnenschein, Susan. 1984. The effect of
redundant communication on listeners:
Why different types may have different
effects. Journal of Psycholinguistic Research,
13:147?166.
Sowa, John. 1984. Conceptual Structures:
Information Processing in Mind and Machine.
Addison-Wesley, Boston, MA.
Spivey, Michael, Melinda Tyler, Kathleen
Eberhard, and Michael Tanenhaus. 2001.
Linguistically mediated visual search.
Psychological Science, 12:282?286.
Stoia, Laura, Donna K. Byron,
Darla Magdalene Shockley, and
Eric Fosler-Lussier. 2006. Noun
phrase generation for situated dialogs.
In Proceedings of the 4th International
Natural Language Generation Conference
(INLG), pages 81?88, Sydney.
Stone, Matthew. 2000. On identifying sets.
In Proceedings of the 1st International
Conference on Natural Language Generation
(INLG), pages 116?123, Mitzpe Ramon.
Stone, Matthew, Christine Doran,
Bonnie Webber, Tonia Bleam, and
Martha Palmer. 2003. Microplanning
with communicative intentions:
The SPUD system. Computational
Intelligence, 19:311?381.
Stone, Matthew and Bonnie Webber. 1998.
Textual economy through close coupling
of syntax and semantics. In Proceedings
of the 9th International Workshop on
Natural Language Generation (INLG),
pages 178?187, Niagara-on-the-Lake.
Theune, Marie?t, Ruud Koolen, Emiel
Krahmer, and Sander Wubben. 2011. Does
size matter: How much data is required to
train a REG algorithm? In Proceedings of the
49th Annual Meeting of the Association for
Computational Linguistics: Human Language
Technologies (ACL-HLT), pages 660?664,
Portland, OR.
Turner, Ross, Somayajulu Sripada, and
Ehud Reiter. 2009. Generating
approximate geographic descriptions.
In Proceedings of the 12th European
217
Computational Linguistics Volume 38, Number 1
Workshop on Natural Language Generation
(ENLG), pages 42?49, Athens.
Turner, Ross, Somayajulu Sripada,
Ehud Reiter, and Ian P. Davy. 2008.
Using spatial reference frames to
generate grounded textual summaries of
georeferenced data. In Proceedings of
the 5th International Natural Language
Generation Conference (INLG), pages 16?24,
Salt Fork, OH.
van Deemter, Kees. 2002. Generating
referring expressions: Boolean extensions
of the Incremental Algorithm.
Computational Linguistics, 28(1):37?52.
van Deemter, Kees. 2006. Generating
referring expressions that involve
gradable properties. Computational
Linguistics, 32(2):195?222.
van Deemter, Kees. 2010. Not Exactly: In
Praise of Vagueness. Oxford University
Press, Oxford, UK.
van Deemter, Kees, Albert Gatt, Ielka van der
Sluis, and Richard Power (in press).
Generation of referring expressions:
Assessing the Incremental Algorithm.
Cognitive Science, to appear.
van Deemter, Kees and Emiel Krahmer. 2007.
Graphs and Booleans: On the generation of
referring expressions. In Harry Bunt and
Reinhard Muskens, editors, Computing
Meaning, Volume 3. Studies in Linguistics
and Philosophy. Springer Publishers,
Berlin, pages 397?422.
van Deemter, Kees, Ielka van der Sluis, and
Albert Gatt. 2006. Building a semantically
transparent corpus for the generation of
referring expressions. In Proceedings
of the 4th International Conference on
Natural Language Generation (INLG),
pages 130?132, Sydney.
van der Sluis, Ielka and Emiel Krahmer.
2007. Generating multimodal referring
expressions. Discourse Processes,
44(3):145?174.
van der Wege, Mija. 2009. Lexical
entrainment and lexical differentiation
in reference phrase choice. Journal of
Memory and Language, 60:448?463.
van Hentenryck, Pascal. 1989. Constraint
Satisfaction in Logic Programming.
The MIT Press, Cambridge, MA.
van Rijsbergen, C. J. 1979. Information
Retrieval. Butterworths, London,
2nd edition.
Viethen, Jette and Robert Dale. 2006.
Algorithms for generating referring
expressions: Do they do what people
do? In Proceedings of the 4th International
Conference on Natural Language Generation
(INLG), pages 63?70, Sydney.
Viethen, Jette and Robert Dale. 2007.
Evaluation in natural language
generation: Lessons from referring
expression generation. Traitement
Automatique des Langues, 48:141?160.
Viethen, Jette and Robert Dale. 2008.
The use of spatial relations in referring
expressions. In Proceedings of the 5th
International Natural Language Generation
Conference (INLG), pages 59?67,
Salt Fork, OH.
Viethen, Jette, Robert Dale, and Markus
Guhe. 2011. Generating subsequent
reference in shared visual scenes:
Computation vs re-use. In Proceedings
of the Conference on Empirical Methods in
Natural Language Processing (EMNLP),
pages 1158?1167, Edinburgh.
Viethen, Jette, Robert Dale, Emiel Krahmer,
Marie?t Theune, and Pascal Touset. 2008.
Controlling redundancy in referring
expressions. In Proceedings of the 6th
Language Resources and Evaluation
Conference (LREC), pages 239?246,
Marrakech.
Viethen, Jette, Simon Zwarts, Robert Dale,
and Markus Guhe. 2010. Dialogue
reference in a visual domain. In Proceedings
of the 7th Language Resources and Evaluation
Conference (LREC), Valetta.
Walker, Marilyn, Amanda Stent, Franc?ois
Mairesse, and Rashmi Prasad. 2007.
Individual and domain adaptation
in sentence planning for dialogue.
Journal of Artificial Intelligence Research,
30:413?456.
White, Michael, Robert Clark, and
Johanna Moore. 2010. Generating
tailored, comparative descriptions with
contextually appropriate intonation.
Computational Linguistics, 36:159?201.
Winograd, Terry. 1972. Understanding Natural
Language. Academic Press, New York.
Wooding, David, Mark Muggelstone,
Kevin Purdy, and Alastair Gale. 2002.
Eye movements of large populations.
Behavior Research Methods, Instruments
and Computers, 34:509?517.
218
Proceedings of the ACL 2010 Conference Short Papers, pages 55?59,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Preferences versus Adaptation during Referring Expression Generation
Martijn Goudbeek
University of Tilburg
Tilburg, The Netherlands
m.b.goudbeek@uvt.nl
Emiel Krahmer
University of Tilburg
Tilburg, The Netherlands
e.j.krahmer@uvt.nl
Abstract
Current Referring Expression Generation
algorithms rely on domain dependent pref-
erences for both content selection and lin-
guistic realization. We present two exper-
iments showing that human speakers may
opt for dispreferred properties and dispre-
ferred modifier orderings when these were
salient in a preceding interaction (without
speakers being consciously aware of this).
We discuss the impact of these findings for
current generation algorithms.
1 Introduction
The generation of referring expressions is a core
ingredient of most Natural Language Generation
(NLG) systems (Reiter and Dale, 2000; Mellish et
al., 2006). These systems usually approach Refer-
ring Expression Generation (REG) as a two-step
procedure, where first it is decided which prop-
erties to include (content selection), after which
the selected properties are turned into a natural
language referring expression (linguistic realiza-
tion). The basic problem in both stages is one of
choice; there are many ways in which one could
refer to a target object and there are multiple ways
in which these could be realized in natural lan-
guage. Typically, these choice problems are tack-
led by giving preference to some solutions over
others. For example, the Incremental Algorithm
(Dale and Reiter, 1995), one of the most widely
used REG algorithms, assumes that certain at-
tributes are preferred over others, partly based on
evidence provided by Pechmann (1989); a chair
would first be described in terms of its color, and
only if this does not result in a unique charac-
terization, other, less preferred attributes such as
orientation are tried. The Incremental Algorithm
is arguably unique in assuming a complete pref-
erence order of attributes, but other REG algo-
rithms rely on similar distinctions. The Graph-
based algorithm (Krahmer et al, 2003), for ex-
ample, searches for the cheapest description for
a target, and distinguishes cheap attributes (such
as color) from more expensive ones (orientation).
Realization of referring expressions has received
less attention, yet recent studies on the ordering of
modifiers (Shaw and Hatzivassiloglou, 1999; Mal-
ouf, 2000; Mitchell, 2009) also work from the as-
sumption that some orderings (large red) are pre-
ferred over others (red large).
We argue that such preferences are less stable
when referring expressions are generated in inter-
active settings, as would be required for applica-
tions such as spoken dialogue systems or interac-
tive virtual characters. In these cases, we hypothe-
size that, besides domain preferences, also the re-
ferring expressions that were produced earlier in
the interaction are important. It has been shown
that if one dialogue participant refers to a couch as
a sofa, the next speaker is more likely to use the
word sofa as well (Branigan et al, in press). This
kind of micro-planning or ?lexical entrainment?
(Brennan and Clark, 1996) can be seen as a spe-
cific form of ?alignment? (Pickering and Garrod,
2004) between speaker and addressee. Pickering
and Garrod argue that alignment may take place
on all levels of interaction, and indeed it has been
shown that participants also align their intonation
patterns and syntactic structures. However, as far
as we know, experimental evidence for alignment
on the level of content planning has never been
given, and neither have alignment effects in modi-
fier orderings during realization been shown. With
a few notable exceptions, such as Buschmeier et
al. (2009) who study alignment in micro-planning,
and Janarthanam and Lemon (2009) who study
alignment in expertise levels, alignment has re-
ceived little attention in NLG so far.
This paper is organized as follows. Experi-
ment I studies the trade-off between adaptation
55
and preferences during content selection while Ex-
periment II looks at this trade-off for modifier
orderings during realization. Both studies use a
novel interactive reference production paradigm,
applied to two domains ? the Furniture and People
domains of the TUNA data-set (Gatt et al, 2007;
Koolen et al, 2009) ? to see whether adaptation
may be domain dependent. Finally, we contrast
our findings with the performance of state-of-the-
art REG algorithms, discussing how they could be
adapted so as to account for the new data, effec-
tively adding plasticity to the generation process.
2 Experiment I
Experiment I studies what speakers do when re-
ferring to a target that can be distinguished in a
preferred (the blue fan) or a dispreferred way (the
left-facing fan), when in the prior context either
the first or the second variant was made salient.
Method
Participants 26 students (2 male, mean age = 20
years, 11 months), all native speakers of Dutch
without hearing or speech problems, participated
for course credits.
Materials Target pictures were taken from the
TUNA corpus (Gatt et al, 2007) that has been
extensively used for REG evaluation. This cor-
pus consists of two domains: one containing pic-
tures of people (famous mathematicians), the other
containing furniture items in different colors de-
picted from different orientations. From previous
studies (Gatt et al, 2007; Koolen et al, 2009) it
is known that participants show a preference for
certain attributes: color in the Furniture domain
and glasses in the People domain, and disprefer
other attributes (orientation of a furniture piece
and wearing a tie, respectively).
Procedure Trials consisted of four turns in an in-
teractive reference understanding and production
experiment: a prime, two fillers and the experi-
mental description (see Figure 1). First, partici-
pants listened to a pre-recorded female voice re-
ferring to one of three objects and had to indi-
cate which one was being referenced. In this sub-
task, references either used a preferred or a dis-
preferred attribute; both were distinguishing. Sec-
ond, participants themselves described a filler pic-
ture, after which, third, they had to indicate which
filler picture was being described. The two filler
turns always concerned stimuli from the alterna-
Figure 1: The 4 tasks per trial. A furniture trial is
shown; people trials have an identical structure.
tive domain and were intended to prevent a too
direct connection between the prime and the tar-
get. Fourth, participants described the target ob-
ject, which could always be distinguished from its
distractors in a preferred (The blue fan) or a dis-
preferred (The left facing fan) way. Note that at-
56
Figure 2: Proportions of preferred and dispre-
ferred attributes in the Furniture domain.
tributes are primed, not values; a participant may
have heard front facing in the prime turn, while
the target has a different value for this attribute (cf.
Fig. 1).
For the two domains, there were 20 preferred
and 20 dispreferred trials, giving rise to 2 x (20 +
20) = 80 critical trials. These were presented in
counter-balanced blocks, and within blocks each
participant received a different random order. In
addition, there were 80 filler trials (each following
the same structure as outlined in Figure 1). During
debriefing, none of the participants indicated they
had been aware of the experiment?s purpose.
Results
We use the proportion of attribute alignment as
our dependent measure. Alignment occurs when
a participant uses the same attribute in the target
as occurred in the prime. This includes overspeci-
fied descriptions (Engelhardt et al, 2006; Arnold,
2008), where both the preferred and dispreferred
attributes were mentioned by participants. Over-
specification occurred in 13% of the critical trials
(and these were evenly distributed over the exper-
imental conditions).
The use of the preferred and dispreferred at-
tribute as a function of prime and domain is shown
in Figure 2 and Figure 3. In both domains, the
preferred attribute is used much more frequently
than the dispreferred attribute with the preferred
primes, which serves as a manipulation check. As
a test of our hypothesis that adaptation processes
play an important role in attribute selection for
referring expressions, we need to look at partic-
ipants? expressions with the dispreferred primes
(with the preferred primes, effects of adaptation
and of preferences cannot be teased apart). Cur-
rent REG algorithms such as the Incremental Al-
gorithm and the Graph-based algorithm predict
that participants will always opt for the preferred
Figure 3: Proportions of preferred and dispre-
ferred attributes in the People domain.
attribute, and hence will not use the dispreferred
attribute. This is not what we observe: our par-
ticipants used the dispreferred attribute at a rate
significantly larger than zero when they had been
exposed to it three turns earlier (tfurniture [25] =
6.64, p < 0.01; tpeople [25] = 4.78 p < 0.01). Ad-
ditionally, they used the dispreferred attribute sig-
nificantly more when they had previously heard
the dispreferred attribute rather than the preferred
attribute. This difference is especially marked
and significant in the Furniture domain (tfurniture
[25] = 2.63, p < 0.01, tpeople [25] = 0.98, p <
0.34), where participants opt for the dispreferred
attribute in 54% of the trials, more frequently than
they do for the preferred attribute (Fig. 2).
3 Experiment II
Experiment II uses the same paradigm used for
Experiment I to study whether speaker?s prefer-
ences for modifier orderings can be changed by
exposing them to dispreferred orderings.
Method
Participants 28 Students (ten males, mean age =
23 years and two months) participated for course
credits. All were native speakers of Dutch, without
hearing and speech problems. None participated
in Experiment I.
Materials The materials were identical to those
used in Experiment I, except for their arrangement
in the critical trials. In these trials, the participants
could only identify the target picture using two at-
tributes. In the Furniture domain these were color
and size, in the People domain these were having a
beard and wearing glasses. In the prime turn (Task
I, Fig. 1), these attributes were realized in a pre-
ferred way (?size first?: e.g., the big red sofa, or
?glasses first?: the bespectacled and bearded man)
or in a dispreferred way (?color first?: the red big
sofa or ?beard first? the bespectacled and bearded
57
Figure 4: Proportions of preferred and dispre-
ferred modifier orderings in the Furniture domain.
man). Google counts for the original Dutch mod-
ifier orderings reveal that the ratio of preferred to
dispreferred is in the order of 40:1 in the Furniture
domain and 3:1 in the People domain.
Procedure As above.
Results
We use the proportion of modifier ordering align-
ments as our dependent measure, where alignment
occurs when the participant?s ordering coincides
with the primed ordering. Figure 4 and 5 show the
use of the preferred and dispreferred modifier or-
dering per prime and domain. It can be seen that
in the preferred prime conditions, participants pro-
duce the expected orderings, more or less in accor-
dance with the Google counts.
State-of-the-art realizers would always opt for
the most frequent ordering of a given pair of mod-
ifiers and hence would never predict the dispre-
ferred orderings to occur. Still, the use of the dis-
preferred modifier ordering occurred significantly
more often than one would expect given this pre-
diction, tfurniture [27] = 6.56, p < 0.01 and tpeople
[27] = 9.55, p < 0.01. To test our hypotheses con-
cerning adaptation, we looked at the dispreferred
realizations when speakers were exposed to dis-
preferred primes (compared to preferred primes).
In both domains this resulted in an increase of the
anount of dispreferred realizations, which was sig-
nificant in the People domain (tpeople [27] = 1.99,
p < 0.05, tfurniture [25] = 2.63, p < 0.01).
4 Discussion
Current state-of-the-art REG algorithms often rest
upon the assumption that some attributes and some
realizations are preferred over others. The two ex-
periments described in this paper show that this
assumption is incorrect, when references are pro-
duced in an interactive setting. In both experi-
ments, speakers were more likely to select a dis-
Figure 5: Proportions of preferred and dispre-
ferred modifier orderings in the People domain.
preferred attribute or produce a dispreferred mod-
ifier ordering when they had previously been ex-
posed to these attributes or orderings, without be-
ing aware of this. These findings fit in well with
the adaptation and alignment models proposed by
psycholinguists, but ours, as far as we know, is
the first experimental evidence of alignment in at-
tribute selection and in modifier ordering. Inter-
estingly, we found that effect sizes differ for the
different domains, indicating that the trade-off be-
tween preferences and adaptions is a gradual one,
also influenced by the a priori differences in pref-
erence (it is more difficult to make people say
something truly dispreferred than something more
marginally dispreferred).
To account for these findings, GRE algorithms
that function in an interactive setting should be
made sensitive to the production of dialogue part-
ners. For the Incremental Algorithm (Dale and Re-
iter, 1995), this could be achieved by augmenting
the list of preferred attributes with a list of ?previ-
ously mentioned? attributes. The relative weight-
ing of these two lists will be corpus dependent,
and can be estimated in a data-driven way. Alter-
natively, in the Graph-based algorithm (Krahmer
et al, 2003), costs of properties could be based
on two components: a relatively fixed domain
component (preferred is cheaper) and a flexible
interactive component (recently used is cheaper).
Which approach would work best is an open, em-
pirical question, but either way this would consti-
tute an important step towards interactive REG.
Acknowledgments
The research reported in this paper forms part
of the VICI project ?Bridging the gap between
psycholinguistics and Computational linguistics:
the case of referring expressions?, funded by the
Netherlands Organization for Scientific Research
(NWO grant 277-70-007).
58
References
Jennifer Arnold. 2008. Reference produc-
tion: Production-internal and addressee-oriented
processes. Language and Cognitive Processes,
23(4):495?527.
Holly P. Branigan, Martin J. Pickering, Jamie Pearson,
and Janet F. McLean. in press. Linguistic alignment
between people and computers. Journal of Prag-
matics, 23:1?2.
Susan E. Brennan and Herbert H. Clark. 1996. Con-
ceptual pacts and lexical choice in conversation.
Journal of Experimental Psychology: Learning,
Memory, and Cognition, 22:1482?1493.
Hendrik Buschmeier, Kirsten Bergmann, and Stefan
Kopp. 2009. An alignment-capable microplan-
ner for Natural Language Generation. In Proceed-
ings of the 12th European Workshop on Natural
Language Generation (ENLG 2009), pages 82?89,
Athens, Greece, March. Association for Computa-
tional Linguistics.
Robert Dale and Ehud Reiter. 1995. Computational
interpretations of the gricean maxims in the gener-
ation of referring expressions. Cognitive Science,
19(2):233?263.
Paul E. Engelhardt, Karl G. Bailey, and Fernanda Fer-
reira. 2006. Do speakers and listeners observe the
gricean maxim of quantity? Journal of Memory and
Language, 54(4):554?573.
Albert Gatt, Ielka van der Sluis, and Kees van Deemter.
2007. Evaluating algorithms for the generation of
referring expressions using a balanced corpus. In
Proceedings of the 11th European Workshop on Nat-
ural Language Generation.
Srinivasan Janarthanam and Oliver Lemon. 2009.
Learning lexical alignment policies for generating
referring expressions for spoken dialogue systems.
In Proceedings of the 12th European Workshop on
Natural Language Generation (ENLG 2009), pages
74?81, Athens, Greece, March. Association for
Computational Linguistics.
Ruud Koolen, Albert Gatt, Martijn Goudbeek, and
Emiel Krahmer. 2009. Need I say more? on factors
causing referential overspecification. In Proceed-
ings of the PRE-CogSci 2009 Workshop on the Pro-
duction of Referring Expressions: Bridging the Gap
Between Computational and Empirical Approaches
to Reference.
Emiel Krahmer, Sebastiaan van Erk, and Andre? Verleg.
2003. Graph-based generation of referring expres-
sions. Computational Linguistics, 29(1):53?72.
Robert Malouf. 2000. The order of prenominal adjec-
tives in natural language generation. In Proceedings
of the 38th Annual Meeting of the Association for
Computational Linguistics, pages 85?92.
Chris Mellish, Donia Scott, Lynn Cahill, Daniel Paiva,
Roger Evans, and Mike Reape. 2006. A refer-
ence architecture for natural language generation
systems. Natural Language Engineering, 12:1?34.
Margaret Mitchell. 2009. Class-based ordering of
prenominal modifiers. In ENLG ?09: Proceedings of
the 12th European Workshop on Natural Language
Generation, pages 50?57, Morristown, NJ, USA.
Association for Computational Linguistics.
Thomas Pechmann. 1989. Incremental speech produc-
tion and referential overspecification. Linguistics,
27:89?110.
Martin Pickering and Simon Garrod. 2004. Towards
a mechanistic psychology of dialogue. Behavioural
and Brain Sciences, 27:169?226.
Ehud Reiter and Robert Dale. 2000. Building Natural
Language Generation Systems. Cambridge Univer-
sity Press.
James Shaw and Vasileios Hatzivassiloglou. 1999. Or-
dering among premodifiers. In Proceedings of the
37th annual meeting of the Association for Compu-
tational Linguistics on Computational Linguistics,
pages 135?143.
59
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 660?664,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Does Size Matter ? How Much Data is Required to Train a REG Algorithm?
Marie?t Theune
University of Twente
P.O. Box 217
7500 AE Enschede
The Netherlands
m.theune@utwente.nl
Ruud Koolen
Tilburg University
P.O. Box 90135
5000 LE Tilburg
The Netherlands
r.m.f.koolen@uvt.nl
Emiel Krahmer
Tilburg University
P.O. Box 90135
5000 LE Tilburg
The Netherlands
e.j.krahmer@uvt.nl
Sander Wubben
Tilburg University
P.O. Box 90135
5000 LE Tilburg
The Netherlands
s.wubben@uvt.nl
Abstract
In this paper we investigate how much data
is required to train an algorithm for attribute
selection, a subtask of Referring Expressions
Generation (REG). To enable comparison be-
tween different-sized training sets, a system-
atic training method was developed. The re-
sults show that depending on the complexity
of the domain, training on 10 to 20 items may
already lead to a good performance.
1 Introduction
There are many ways in which we can refer to ob-
jects and people in the real world. A chair, for ex-
ample, can be referred to as red, large, or seen from
the front, while men may be singled out in terms
of their pogonotrophy (facial hairstyle), clothing and
many other attributes. This poses a problem for al-
gorithms that automatically generate referring ex-
pressions: how to determine which attributes to use?
One solution is to assume that some attributes
are preferred over others, and this is indeed what
many Referring Expressions Generation (REG) al-
gorithms do. A classic example is the Incremental
Algorithm (IA), which postulates the existence of
a complete ranking of relevant attributes (Dale and
Reiter, 1995). The IA essentially iterates through
this list of preferred attributes, selecting an attribute
for inclusion in a referring expression if it helps sin-
gling out the target from the other objects in the
scene (the distractors). Crucially, Dale and Reiter do
not specify how the ranking of attributes should be
determined. They refer to psycholinguistic research
suggesting that, in general, absolute attributes (such
as color) are preferred over relative ones (such as
size), but stress that constructing a preference order
is essentially an empirical question, which will dif-
fer from one domain to another.
Many other REG algorithms similarly rely on
preferences. The graph-based based REG algorithm
(Krahmer et al, 2003), for example, models prefer-
ences in terms of costs, with cheaper properties be-
ing more preferred. Various ways to compute costs
are possible; they can be defined, for instance, in
terms of log probabilities, which makes frequently
encountered properties cheap, and infrequent ones
more expensive. Krahmer et al (2008) argue that
a less fine-grained cost function might generalize
better, and propose to use frequency information
to, somewhat ad hoc, define three costs: 0 (free),
1 (cheap) and 2 (expensive). This approach was
shown to work well: the graph-based algorithm was
the best performing system in the most recent REG
Challenge (Gatt et al, 2009).
Many other attribute selection algorithms also
rely on training data to determine preferences in one
form or another (Fabbrizio et al, 2008; Gerva?s et
al., 2008; Kelleher, 2007; Spanger et al, 2008; Vi-
ethen and Dale, 2010). Unfortunately, suitable data
is hard to come by. It has been argued that determin-
ing which properties to include in a referring expres-
sion requires a ?semantically transparent? corpus
(van Deemter et al, 2006): a corpus that contains
the actual properties of all domain objects as well
as the properties that were selected for inclusion in
a given reference to the target. Obviously, text cor-
pora tend not to meet this requirement, which is why
660
semantically transparent corpora are often collected
using human participants who are asked to produce
referring expressions for targets in controlled visual
scenes for a given domain. Since this is a time con-
suming exercise, it will not be surprising that such
corpora are thin on the ground (and are often only
available for English). An important question there-
fore is how many human-produced references are
needed to achieve a certain level of performance. Do
we really need hundreds of instances, or can we al-
ready make informed decisions about preferences on
a few or even one training instance?
In this paper, we address this question by sys-
tematically training the graph-based REG algorithm
on a number of ?semantically transparent? data sets
of various sizes and evaluating on a held-out test
set. The graph-based algorithm seems a good can-
didate for this exercise, in view of its performance
in the REG challenges. For the sake of compari-
son, we also follow the evaluation methodology of
the REG challenges, training and testing on two do-
mains (a furniture and a people domain), and using
two automatic metrics (Dice and accuracy) to mea-
sure human-likeness. One hurdle needs to be taken
beforehand. Krahmer et al (2008) manually as-
signed one of three costs to properties, loosely based
on corpus frequencies. For our current evaluation
experiments, this would hamper comparison across
data sets, because it is difficult to do it in a manner
that is both consistent and meaningful. Therefore we
first experiment with a more systematic way of as-
signing a limited number of frequency-based costs
to properties using k-means clustering.
2 Experiment I: k-means clustering costs
In this section we describe our experiment with k-
means clustering to derive property costs from En-
glish and Dutch corpus data. For this experiment we
looked at both English and Dutch, to make sure the
chosen method does not only work well for English.
2.1 Materials
Our English training and test data were taken from
the TUNA corpus (Gatt et al, 2007). This semanti-
cally transparent corpus contains referring expres-
sions in two domains (furniture and people), col-
lected in one of two conditions: in the -LOC con-
dition, participants were discouraged from mention-
ing the location of the target in the visual scene,
whereas in the +LOC condition they could mention
any properties they wanted. The TUNA corpus was
used for comparative evaluation in the REG Chal-
lenges (2007-2009). For training in our current ex-
periment, we used the -LOC data from the training
set of the REG Challenge 2009 (Gatt et al, 2009):
165 furniture descriptions and 136 people descrip-
tions. For testing, we used the -LOC data from the
TUNA 2009 development set: 38 furniture descrip-
tions and 38 people descriptions.
Dutch data were taken from the D-TUNA corpus
(Koolen and Krahmer, 2010). This corpus uses the
same visual scenes and annotation scheme as the
TUNA corpus, but with Dutch instead of English
descriptions. D-TUNA does not include locations as
object properties at all, hence our restriction to -LOC
data for English (to make the Dutch and English data
more comparable). As Dutch test data, we used 40
furniture items and 40 people items, randomly se-
lected from the textual descriptions in the D-TUNA
corpus. The remaining furniture and people descrip-
tions (160 items each) were used for training.
2.2 Method
We first determined the frequency with which each
property was mentioned in our training data, relative
to the number of target objects with this property.
Then we created different cost functions (mapping
properties to costs) by means of k-means clustering,
using the Weka toolkit. The k-means clustering al-
gorithm assigns n points in a vector space to k clus-
ters (S1 to Sk) by assigning each point to the clus-
ter with the nearest centroid. The total intra-cluster
variance V is minimized by the function
V =
k
?
i=1
?
xj?Si
(xj ? ?i)2
where ?i is the centroid of all the points xj ? Si.
In our case, the points n are properties, the vector
space is one-dimensional (frequency being the only
dimension) and ?i is the average frequency of the
properties in Si. The cluster-based costs are defined
as follows:
?xj ? Si, cost(xj) = i? 1
661
where S1 is the cluster with the most frequent
properties, S2 is the cluster with the next most fre-
quent properties, and so on. Using this approach,
properties from cluster S1 get cost 0 and thus can be
added ?for free? to a description. Free properties are
always included, provided they help distinguish the
target. This may lead to overspecified descriptions,
mimicking the human tendency to mention redun-
dant properties (Dale and Reiter, 1995).
We ran the clustering algorithm on our English
and Dutch training data for up to six clusters (k = 2
to k = 6). Then we evaluated the performance of
the resulting cost functions on the test data from
the same language, using Dice (overlap between at-
tribute sets) and Accuracy (perfect match between
sets) as evaluation metrics. For comparison, we also
evaluated the best scoring cost functions from Theu-
ne et al (2010) on our test data. These ?Free-Na??ve?
(FN) functions were created using the manual ap-
proach sketched in the introduction.
The order in which the graph-based algorithm
tries to add attributes to a description is explicitly
controlled to ensure that ?free? distinguishing prop-
erties are included (Viethen et al, 2008). In our
tests, we used an order of decreasing frequency; i.e.,
always examining more frequent properties first.1
2.3 Results
For the cluster-based cost functions, the best perfor-
mance was achieved with k = 2, for both domains
and both languages. Interestingly, this is the coarsest
possible k-means function: with only two costs (0
and 1) it is even less fine-grained than the FN func-
tions advocated by Krahmer et al (2008). The re-
sults for the k-means costs with k = 2 and the FN
costs of Theune et al (2010) are shown in Table 1.
No significant differences were found, which sug-
gests that k-means clustering, with k = 2, can be
used as a more systematic alternative for the manual
assignment of frequency-based costs. We therefore
applied this method in the next experiment.
3 Experiment II: varying training set size
To find out how much training data is required
to achieve an acceptable attribute selection perfor-
1We used slightly different property orders than Theune et
al. (2010), leading to minor differences in our FN results.
Furniture People
Language Costs Dice Acc. Dice Acc.
English k-means 0.810 0.50 0.733 0.29
FN 0.829 0.55 0.733 0.29
Dutch k-means 0.929 0.68 0.812 0.33
FN 0.929 0.68 0.812 0.33
Table 1: Results for k-means costs with k = 2 and the
FN costs of Theune et al (2010) on Dutch and English.
mance, in the second experiment we derived cost
functions and property orders from different sized
training sets, and evaluated them on our test data.
For this experiment, we only used English data.
3.1 Materials
As training sets, we used randomly selected subsets
of the full English training set from Experiment I,
with set sizes of 1, 5, 10, 20 and 30 items. Be-
cause the accidental composition of a training set
may strongly influence the results, we created 5 dif-
ferent sets of each size. The training sets were built
up in a cumulative fashion: we started with five sets
of size 1, then added 4 items to each of them to cre-
ate five sets of size 5, etc. This resulted in five series
of increasingly sized training sets. As test data, we
used the same English test set as in Experiment I.
3.2 Method
We derived cost functions (using k-means clustering
with k = 2) and orders from each of the training
sets, following the method described in Section 2.2.
In doing so, we had to deal with missing data: not all
properties were present in all data sets.2 For the cost
functions, we simply assigned the highest cost (1)
to the missing properties. For the order, we listed
properties with the same frequency (0 for missing
properties) in alphabetical order. This was done for
the sake of comparability between training sets.
3.3 Results
To determine significance, we calculated the means
of the scores of the five training sets for each set
size, so that we could compare them with the scores
of the entire set. We applied repeated measures of
2This problem mostly affected the smaller training sets. By
set size 10 only a few properties were missing, while by set size
20, all properties were present in all sets.
662
variance (ANOVA) to the Dice and Accuracy scores,
using set size (1, 5, 10, 20, 30, entire set) as a within
variable. The mean results for each training set size
are shown in Table 2.3 The general pattern is that
the scores increase with the size of the training set,
but the increase gets smaller as the set sizes become
larger.
Furniture People
Set size Dice Acc. Dice Acc.
1 0.693 0.25 0.560 0.13
5 0.756 0.34 0.620 0.15
10 0.777 0.40 0.686 0.20
20 0.788 0.41 0.719 0.25
30 0.782 0.41 0.718 0.27
Entire set 0.810 0.50 0.733 0.29
Table 2: Mean results for the different set sizes.
In the furniture domain, we found a main effect
of set size (Dice: F(5,185) = 7.209, p < .001; Ac-
curacy: F(5,185) = 6.140, p < .001). To see which
set sizes performed significantly different as com-
pared to the entire set, we conducted Tukey?s HSD
post hoc comparisons. For Dice, the scores of set
size 10 (p = .141), set size 20 (p = .353), and set
size 30 (p = .197) did not significantly differ from
the scores of the entire set of 165 items. The Accu-
racy scores in the furniture domain show a slightly
different pattern: the scores of the entire training set
were still significantly higher than those of set size
30 (p < .05). This better performance when trained
on the entire set may be caused by the fact that not
all of the five training sets that were used for set sizes
1, 5, 10, 20 and 30 performed equally well.
In the people domain we also found a main effect
of set size (Dice: F(5,185) = 21.359, p < .001; Accu-
racy: F(5,185) = 8.074, p < .001). Post hoc pairwise
comparisons showed that the scores of set size 20
(Dice: p = .416; Accuracy: p = .146) and set size
30 (Dice: p = .238; Accuracy: p = .324) did not
significantly differ from those of the full set of 136
items.
3For comparison: in the REG Challenge 2008, (which in-
volved a different test set, but the same type of data), the best
systems obtained overall Dice and accuracy scores of around
0.80 and 0.55 respectively (Gatt et al, 2008). These scores may
well represent the performance ceiling for speaker and context
independent algorithms on this task.
4 Discussion
Experiment II has shown that when using small data
sets to train an attribute selection algorithm, results
can be achieved that are not significantly different
from those obtained using a much larger training
set. Domain complexity appears to be a factor in
how much training data is needed: using Dice as an
evaluation metric, training sets of 10 sufficed in the
simple furniture domain, while in the more complex
people domain it took a set size of 20 to achieve re-
sults that do not significantly differ from those ob-
tained using the full training set.
The accidental composition of the training sets
may strongly influence the attribute selection per-
formance. In the furniture domain, we found clear
differences between the results of specific training
sets, with ?bad sets? pulling the overall performance
down. This affected Accuracy but not Dice, possibly
because the latter is a less strict metric.
Whether the encouraging results found for the
graph-based algorithm generalize to other REG ap-
proaches is still an open question. We also need
to investigate how the use of small training sets af-
fects effectiveness and efficiency of target identifica-
tion by human subjects; as shown by Belz and Gatt
(2008), task-performance measures do not necessar-
ily correlate with similarity measures such as Dice.
Finally, it will be interesting to repeat Experiment II
with Dutch data. The D-TUNA data are cleaner than
the TUNA data (Theune et al, 2010), so the risk of
?bad? training data will be smaller, which may lead
to more consistent results across training sets.
5 Conclusion
Our experiment has shown that with 20 or less train-
ing instances, acceptable attribute selection results
can be achieved; that is, results that do not signif-
icantly differ from those obtained using the entire
training set. This is good news, because collecting
such small amounts of training data should not take
too much time and effort, making it relatively easy
to do REG for new domains and languages.
Acknowledgments
Krahmer and Koolen received financial support from
The Netherlands Organization for Scientific Re-
search (Vici grant 27770007).
663
References
Anja Belz and Albert Gatt. 2008. Intrinsic vs. extrinsic
evaluation measures for referring expression genera-
tion. In Proceedings of ACL-08: HLT, Short Papers,
pages 197?200.
Robert Dale and Ehud Reiter. 1995. Computational in-
terpretation of the Gricean maxims in the generation of
referring expressions. Cognitive Science, 19(2):233?
263.
Giuseppe Di Fabbrizio, Amanda Stent, and Srinivas
Bangalore. 2008. Trainable speaker-based refer-
ring expression generation. In Twelfth Conference on
Computational Natural Language Learning (CoNLL-
2008), pages 151?158.
Albert Gatt, Ielka van der Sluis, and Kees van Deemter.
2007. Evaluating algorithms for the generation of re-
ferring expressions using a balanced corpus. In Pro-
ceedings of the 11th European Workshop on Natural
Language Generation (ENLG 2007), pages 49?56.
Albert Gatt, Anja Belz, and Eric Kow. 2008. The
TUNA Challenge 2008: Overview and evaluation re-
sults. In Proceedings of the 5th International Natural
Language Generation Conference (INLG 2008), pages
198?206.
Albert Gatt, Anja Belz, and Eric Kow. 2009. The TUNA-
REG Challenge 2009: Overview and evaluation re-
sults. In Proceedings of the 12th European Workshop
on Natural Language Generation (ENLG 2009), pages
174?182.
Pablo Gerva?s, Raquel Herva?s, and Carlos Le?on. 2008.
NIL-UCM: Most-frequent-value-first attribute selec-
tion and best-scoring-choice realization. In Proceed-
ings of the 5th International Natural Language Gener-
ation Conference (INLG 2008), pages 215?218.
John Kelleher. 2007. DIT - frequency based incremen-
tal attribute selection for GRE. In Proceedings of the
MT Summit XI Workshop Using Corpora for Natural
Language Generation: Language Generation and Ma-
chine Translation (UCNLG+MT), pages 90?92.
Ruud Koolen and Emiel Krahmer. 2010. The D-TUNA
corpus: A Dutch dataset for the evaluation of refer-
ring expression generation algorithms. In Proceedings
of the 7th international conference on Language Re-
sources and Evaluation (LREC 2010).
Emiel Krahmer, Sebastiaan van Erk, and Andre? Verleg.
2003. Graph-based generation of referring expres-
sions. Computational Linguistics, 29(1):53?72.
Emiel Krahmer, Marie?t Theune, Jette Viethen, and Iris
Hendrickx. 2008. GRAPH: The costs of redundancy
in referring expressions. In Proceedings of the 5th In-
ternational Natural Language Generation Conference
(INLG 2008), pages 227?229.
Philipp Spanger, Takehiro Kurosawa, and Takenobu
Tokunaga. 2008. On ?redundancy? in selecting at-
tributes for generating referring expressions. In COL-
ING 2008: Companion volume: Posters, pages 115?
118.
Marie?t Theune, Ruud Koolen, and Emiel Krahmer. 2010.
Cross-linguistic attribute selection for REG: Compar-
ing Dutch and English. In Proceedings of the 6th In-
ternational Natural Language Generation Conference
(INLG 2010), pages 174?182.
Kees van Deemter, Ielka van der Sluis, and Albert Gatt.
2006. Building a semantically transparent corpus for
the generation of referring expressions. In Proceed-
ings of the 4th International Natural Language Gener-
ation Conference (INLG 2006), pages 130?132.
Jette Viethen and Robert Dale. 2010. Speaker-dependent
variation in content selection for referring expression
generation. In Proceedings of the 8th Australasian
Language Technology Workshop, pages 81?89.
Jette Viethen, Robert Dale, Emiel Krahmer, Marie?t Theu-
ne, and Pascal Touset. 2008. Controlling redundancy
in referring expressions. In Proceedings of the Sixth
International Conference on Language Resources and
Evaluation (LREC 2008), pages 239?246.
664
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 1015?1024,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Sentence Simplification by Monolingual Machine Translation
Sander Wubben
Tilburg University
P.O. Box 90135
5000 LE Tilburg
The Netherlands
s.wubben@uvt.nl
Antal van den Bosch
Radboud University Nijmegen
P.O. Box 9103
6500 HD Nijmegen
The Netherlands
a.vandenbosch@let.ru.nl
Emiel Krahmer
Tilburg University
P.O. Box 90135
5000 LE Tilburg
The Netherlands
e.j.krahmer@uvt.nl
Abstract
In this paper we describe a method for simpli-
fying sentences using Phrase Based Machine
Translation, augmented with a re-ranking
heuristic based on dissimilarity, and trained on
a monolingual parallel corpus. We compare
our system to a word-substitution baseline and
two state-of-the-art systems, all trained and
tested on paired sentences from the English
part of Wikipedia and Simple Wikipedia. Hu-
man test subjects judge the output of the dif-
ferent systems. Analysing the judgements
shows that by relatively careful phrase-based
paraphrasing our model achieves similar sim-
plification results to state-of-the-art systems,
while generating better formed output. We
also argue that text readability metrics such
as the Flesch-Kincaid grade level should be
used with caution when evaluating the output
of simplification systems.
1 Introduction
Sentence simplification can be defined as the process
of producing a simplified version of a sentence by
changing some of the lexical material and grammat-
ical structure of that sentence, while still preserving
the semantic content of the original sentence, in or-
der to ease its understanding. Particularly language
learners (Siddharthan, 2002), people with reading
disabilities (Inui et al, 2003) such as aphasia (Car-
roll et al, 1999), and low-literacy readers (Watanabe
et al, 2009) can benefit from this application. It can
serve to generate output in a specific limited format,
such as subtitles (Daelemans et al, 2004). Sentence
simplification can also serve to preprocess the input
of other tasks, such as summarization (Knight and
Marcu, 2000), parsing, machine translation (Chan-
drasekar et al, 1996), semantic role labeling (Vick-
rey and Koller, 2008) or sentence fusion (Filippova
and Strube, 2008).
The goal of simplification is to achieve an im-
provement in readability, defined as the ease with
which a text can be understood. Some of the factors
that are known to help increase the readability of text
are the vocabulary used, the length of the sentences,
the syntactic structures present in the text, and the
usage of discourse markers. One effort to create a
simple version of English at the vocabulary level has
been the creation of Basic English by Charles Kay
Ogden. Basic English is a controlled language with
a basic vocabulary consisting of 850 words. Accord-
ing to Ogden, 90 percent of all dictionary entries can
be paraphrased using these 850 words. An exam-
ple of a resource that is written using mainly Basic
English is the English Simple Wikipedia. Articles
on English Simple Wikipedia are similar to articles
found in the traditional English Wikipedia, but writ-
ten using a limited vocabulary (using Basic English
where possible). Generally the structure of the sen-
tences in English Simple Wikipedia is less compli-
cated and the sentences are somewhat shorter than
those found in English Wikipedia; we offer more de-
tailed statistics below.
1.1 Related work
Most earlier work on sentence simplification
adopted rule-based approaches. A frequently ap-
plied type of rule, aimed to reduce overall sentence
length, splits long sentences on the basis of syntactic
1015
information (Chandrasekar and Srinivas, 1997; Car-
roll et al, 1998; Canning et al, 2000; Vickrey and
Koller, 2008). There has also been work on lexi-
cal substitution for simplification, where the aim is
to substitute difficult words with simpler synonyms,
derived from WordNet or dictionaries (Inui et al,
2003).
Zhu et al (2010) examine the use of paired doc-
uments in English Wikipedia and Simple Wikipedia
for a data-driven approach to the sentence simplifi-
cation task. They propose a probabilistic, syntax-
based machine translation approach to the problem
and compare against a baseline of no simplification
and a phrase-based machine translation approach.
In a similar vein, Coster and Kauchak (2011) use
a parallel corpus of paired documents from Sim-
ple Wikipedia and Wikipedia to train a phrase-based
machine translation model coupled with a deletion
model. Another useful resource is the edit his-
tory of Simple Wikipedia, from which simplifica-
tions can be learned (Yatskar et al, 2010). Woods-
end and Lapata (2011) investigate the use of Simple
Wikipedia edit histories and an aligned Wikipedia?
Simple Wikipedia corpus to induce a model based
on quasi-synchronous grammar. They select the
most appropriate simplification by using integer lin-
ear programming.
We follow Zhu et al (2010) and Coster and
Kauchak (2011) in proposing that sentence simpli-
fication can be approached as a monolingual ma-
chine translation task, where the source and target
languages are the same and where the output should
be simpler in form from the input but similar in
meaning. We differ from the approach of Zhu et
al. (2010) in the sense that we do not take syntac-
tic information into account; we rely on PBMT to
do its work and implicitly learn simplifying para-
phrasings of phrases. Our approach differs from
Coster and Kauchak (2011) in the sense that instead
of focusing on deletion in the PBMT decoding stage,
we focus on dissimilarity, as simplification does not
necessarily imply shortening (Woodsend and Lap-
ata, 2011), or as the Simple Wikipedia guidelines
state, ?simpler does not mean short?1. Table 1.1
shows the average sentence length and the average
1http://simple.wikipedia.org/wiki/Main_
Page/Introduction
word length for Wikipedia and Simple Wikipedia
sentences in the PWKP dataset used in this study
(Zhu et al, 2010). These numbers suggest that, al-
though the selection criteria for sentences to be in-
cluded in this dataset are biased (see Section 2.2),
Simple Wikipedia sentences are about 17% shorter,
while the average word length is virtually equal.
Sent. length Token length
Simple Wikipedia 20.87 4.89
Wikipedia 25.01 5.06
Table 1: Sentence and token length statistics for the
PWKP dataset (Zhu et al, 2010).
Statistical machine translation (SMT) has already
been successfully applied to the related task of para-
phrasing (Quirk et al, 2004; Bannard and Callison-
Burch, 2005; Madnani et al, 2007; Callison-Burch,
2008; Zhao et al, 2009; Wubben et al, 2010). SMT
typically makes use of large parallel corpora to train
a model on. These corpora need to be aligned at
the sentence level. Large parallel corpora, such as
the multilingual proceedings of the European Parlia-
ment (Europarl), are readily available for many lan-
guages. Phrase-Based Machine Translation (PBMT)
is a form of SMT where the translation model aims
to translate longer sequences of words (?phrases?)
in one go, solving part of the word ordering problem
along the way that would be left to the target lan-
guage model in a word-based SMT system. PMBT
operates purely on statistics and no linguistic knowl-
edge is involved in the process: the phrases that are
aligned are motivated statistically, rather than lin-
guistically. This makes PBMT adaptable to any lan-
guage pair for which there is a parallel corpus avail-
able. The PBMT model makes use of a translation
model, derived from the parallel corpus, and a lan-
guage model, derived from a monolingual corpus in
the target language. The language model is typically
an n-gram model with smoothing. For any given in-
put sentence, a search is carried out producing an
n-best list of candidate translations, ranked by the
decoder score, a complex scoring function includ-
ing likelihood scores from the translation model,
and the target language model. In principle, all of
this should be transportable to a data-driven machine
translation account of sentence simplification, pro-
1016
vided that a parallel corpus is available that pairs text
to simplified versions of that text.
1.2 This study
In this work we aim to investigate the use of phrase-
based machine translation modified with a dissim-
ilarity component for the task of sentence simplifi-
cation. While Zhu et al (2010) have demonstrated
that their approach outperforms a PBMT approach
in terms of Flesch Reading Ease test scores, we are
not aware of any studies that evaluate PBMT for sen-
tence simplification with human judgements. In this
study we evaluate the output of Zhu et al (2010)
(henceforth referred to as ?Zhu?), Woodsend and La-
pata (2011) (henceforth referred to as ?RevILP?),
our PBMT based system with dissimilarity-based
re-ranking (henceforth referred to as ?PBMT-R?), a
word-substitution baseline, and, as a gold standard,
the original Simple Wikipedia sentences. We will
first discuss the baseline, followed by the Zhu sys-
tem, the RevILP system, and our PBMT-R system
in Section 2. We then describe the experiment with
human judges in Section 3, and its results in Sec-
tion 4. We close this paper by critically discussing
our results in Section 5.
2 Sentence Simplification Models
2.1 Word-Substitution Baseline
The word substitution baseline replaces words in
the source sentence with (near-)synonyms that are
more likely according to a language model. For
each noun, adjective and verb in the sentence this
model takes that word and its part-of-speech tag
and retrieves from WordNet al synonyms from all
synsets the word occurs in. The word is then re-
placed by all of its synset words, and each replace-
ment is scored by a SRILM language model (Stol-
cke, 2002) with probabilities that are obtained from
training on the Simple Wikipedia data. The alter-
native that has the highest probability according to
the language model is kept. If no relevant alterna-
tive is found, the word is left unchanged. We use
the Memory-Based Tagger (Daelemans et al, 1996)
trained on the Brown corpus to compute the part-of-
speech tags. The WordNet::QueryData2 Perl mod-
2http://search.cpan.org/dist/
WordNet-QueryData/QueryData.pm
ule is used to query WordNet (Fellbaum, 1998).
2.2 Zhu et al
Zhu et al (2010) learn a sentence simplification
model which is able to perform four rewrite op-
erations on the parse trees of the input sentences,
namely substitution, reordering, splitting, and dele-
tion. Their model is inspired by syntax-based
SMT (Yamada and Knight, 2001) and consists of
a language model, a translation model and a de-
coder. The four mentioned simplification opera-
tions together form the translation model. Their
model is trained on a corpus containing aligned sen-
tences from English Wikipedia and English Simple
Wikipedia called PWKP. The PWKP dataset con-
sists of 108,016 pairs of aligned lines from 65,133
Wikipedia and Simple Wikipedia articles. These ar-
ticles were paired by following the ?interlanguage
link?3. TF*IDF at the sentence level was used to
align the sentences in the different articles (Nelken
and Shieber, 2006).
Zhu et al (2010) evaluate their system using
BLEU and NIST scores, as well as various read-
ability scores that only take into account the output
sentence, such as the Flesch Reading Ease test and
n-gram language model perplexity. Although their
system outperforms several baselines at the level of
these readability metrics, they do not achieve better
when evaluated with BLEU or NIST.
2.3 RevILP
Woodsend and Lapata?s (2011) model is based
on quasi-synchronous grammar (Smith and Eisner,
2006). Quasi-synchronous grammar generates a
loose alignment between parse trees. It operates on
individual sentences annotated with syntactic infor-
mation in the form of phrase structure trees. Quasi-
synchronous grammar is used to generate all pos-
sible rewrite operations, after which integer linear
programming is employed to select the most ap-
propriate simplification. Their model is trained on
two different datasets: one containing alignments
between Wikipedia and English Simple Wikipedia
(AlignILP), and one containing alignments between
edits in the revision history of Simple Wikipedia
(RevILP). RevILP performs best according to the
3http://en.wikipedia.org/wiki/Help:
Interlanguage_links
1017
human judgements conducted in their study. They
show that it achieves better scores than Zhu et al
(2010)?s system and is not scored significantly dif-
ferently from English Simple Wikipedia. In this
study we compare against their best performing sys-
tem, the RevILP system.
0 2 4 6 8 10 12 14 16 180
1
2
3
4
n-best
Le
ve
ns
ht
ei
n
D
ist
an
ce
0 2 4 6 8 10 12 14 16 180
2
4
6
8
10
12
14
n-best
Fl
es
ch
-K
in
ca
id
Figure 1: Levenshtein distance and Flesch-Kincaid score
of output when varying the n of the n-best output of
Moses.
2.4 PBMT-R
We use the Moses software to train a PBMT
model (Koehn et al, 2007). The data we use is the
PWKP dataset created by Zhu et al (2010). In gen-
eral, a statistical machine translation model finds a
best translation e? of a text in language f to a text
in language e by combining a translation model that
finds the most likely translation p(f |e) with a lan-
guage model that outputs the most likely sentence
p(e):
e? = argmax
e?e?
p(f |e)p(e)
The GIZA++ statistical alignment package is
used to perform the word alignments, which are
later combined into phrase alignments in the Moses
pipeline (Och and Ney, 2003) to build the sentence
simplification model. GIZA++ utilizes IBM Models
1 to 5 and an HMM word alignment model to find
statistically motivated alignments between words.
We first tokenize and lowercase all data and use all
unique sentences from the Simple Wikipedia part
of the PWKP training set to train an n-gram lan-
guage model with the SRILM toolkit to learn the
probabilities of different n-grams. Then we invoke
the GIZA++ aligner using the training simplifica-
tion pairs. We run GIZA++ with standard settings
and we perform no optimization. This results in a
phrase table containing phrase pairs from Wikipedia
and Simple Wikipedia and their conditional proba-
bilities as assigned by Moses. Finally, we use the
Moses decoder to generate simplifications for the
sentences in the test set. For each sentence we let
the system generate the ten best distinct solutions
(or less, if fewer than ten solutions are generated) as
ranked by Moses.
Arguably, dissimilarity is a key factor in simpli-
fication (and in paraphrasing in general). As output
we would like to be able to select fluent sentences
that adequately convey the meaning of the original
input, yet that contain differences that operational-
ize the intended simplification. When training our
PBMT system on the PWKP data we may assume
that the system learns to simplify automatically, yet
there is no aspect of the decoder function in Moses
that is sensitive to the fact that it should try to be
different from the input ? Moses may well trans-
late input to unchanged output, as much of our train-
ing data consists of partially equal input and output
strings.
To expand the functionality of Moses in the in-
tended direction we perform post-hoc re-ranking on
the output based on dissimilarity to the input. We
do this to select output that is as different as possi-
ble from the source sentence, so that it ideally con-
1018
tains multiple simplifications; at the same time, we
base our re-ranking on a top-n of output candidates
according to Moses, with a small n, to ensure that
the quality of the output in terms of fluency and ade-
quacy is also controlled for. Setting n = 10, for each
source sentence we re-rank the ten best sentences
as scored by the decoder according to the Leven-
shtein Distance (or edit distance) measure (Leven-
shtein, 1966) at the word level between the input
and output sentence, counting the minimum num-
ber of edits needed to transform the source string
into the target string, where the allowable edit op-
erations are insertion, deletion, and substitution of a
single word. In case of a tie in Levenshtein Distance,
we select the sequence with the better decoder score.
When Moses is unable to generate ten different sen-
tences, we select from the lower number of outputs.
Figure 1 displays Levenshtein Distance and Flesch-
Kincaid grade level scores for different values of n.
We use the Lingua::EN::Fathom module4 to calcu-
late Flesch-Kincaid grade level scores. The read-
ability score stays more or less the same, indicating
no relation between n and readability. The average
edit distance starts out at just above 2 when selecting
the 1-best output string, and increases roughly until
n = 10.
2.5 Descriptive statistics
Table 2 displays the average edit distance and the
percentage of cases in which no edits were per-
formed for each of the systems and for Simple
Wikipedia. We see that the Levenshtein distance be-
tween Wikipedia and Simple Wikipedia is the most
substantial with an average of 12.3 edits. Given
that the average number of tokens is about 25 for
Wikipedia and 21 for Simple Wikipedia (cf. Ta-
ble 1.1), these numbers indicate that the changes in
Simple Wikipedia go substantially beyond the aver-
age four-word length difference. On average, eight
more words are interchanged for other words. About
half of the original tokens in the source sentence do
not return in the output. Of the three simplifica-
tion systems, the Zhu system (7.95) and the RevILP
(7.18) attain similar edit distances, less substantial
than the edits in Simple Wikipedia, but still consid-
4http://http://search.cpan.org/?kimryan/
Lingua-EN-Fathom-1.15/lib/Lingua/EN/
Fathom.pm
erable compared to the baseline word-substitution
system (4.26) and PBMT-R (3.08). Our system is
clearly conservative in its edits.
System LD Perc. no edits
Simple Wikipedia 12.30 3
Word Sub 4.26 0
Zhu 7.95 2
RevILP 7.18 22
PBMT-R 3.08 5
Table 2: Levenshtein Distance and percentage of unal-
tered output sentences.
On the other hand, we observe some differences
in the percentage of cases in which the systems de-
cide to produce a sentence identical to the input.
In 22 percent of the cases the RevILP system does
not alter the sentence. The other systems make this
decision about as often as the gold standard, Sim-
ple Wikipedia, where only 3% of sentences remain
unchanged. The word-substitution baseline always
manages to make at least one change.
3 Evaluation
3.1 Participants
Participants were 46 students of Tilburg University,
who participated for partial course credits. All were
native speakers of Dutch, and all were proficient in
English, having taken a course on Academic English
at University level.
3.2 Materials
We use the test set used by Zhu et al (2010) and
Woodsend and Lapata (2011). This test set consists
of 100 sentences from articles on English Wikipedia,
paired with sentences from corresponding articles in
English Simple Wikipedia. We selected only those
sentences where every system would perform min-
imally one edit, because we only want to compare
the different systems when they actually generate al-
tered, assumedly simplified output. From this sub-
set we randomly pick 20 source sentences, result-
ing in 20 clusters of one source sentence and 5 sim-
plified sentences, as generated by humans (Simple
Wikipedia) and the four systems.
1019
3.3 Procedure
The participants were told that they participated in
the evaluation of a system that could simplify sen-
tences, and that they would see one source sentence
and five automatically simplified versions of that
sentence. They were not informed of the fact that we
evaluated in fact four different systems and the orig-
inal Simple Wikipedia sentence. Following earlier
evaluation studies (Doddington, 2002; Woodsend
and Lapata, 2011), we asked participants to evalu-
ate Simplicity, Fluency and Adequacy of the target
headlines on a five point Likert scale. Fluency was
defined in the instructions as the extent to which a
sentence is proper, grammatical English. Adequacy
was defined as the extent to which the sentence has
the same meaning as the source sentence. Simplic-
ity was defined as the extent to which the sentence
was simpler than the original and thus easier to un-
derstand. The order in which the clusters had to be
judged was randomized and the order of the output
of the various systems was randomized as well.
4 Results
4.1 Automatic measures
The results of the automatic measures are displayed
in Table 3. In terms of the Flesch-Kincaid grade
level score, where lower scores are better, the Zhu
system scores best, with 7.86 even lower than Sim-
ple Wikipedia (8.57). Increasingly worse Flesch-
Kincaid scores are produced by RevILP (8.61) and
PBMT-R (13.38), while the word substitution base-
line scores worst (14.64). With regard to the BLEU
score, where Simple Wikipedia is the reference, the
PBMT-R system scores highest with 0.43, followed
by the RevILP system (0.42) and the Zhu system
(0.38). The word substitution baseline scores low-
est with a BLEU score of 0.34.
System Flesch-Kincaid BLEU
Simple Wikipedia 8.57 1
Word Sub 14.64 0.34
Zhu 7.86 0.38
RevILP 8.61 0.42
PBMT-R 13.38 0.43
Table 3: Flesch-Kincaid grade level and BLEU scores
4.2 Human judgements
To test for significance we ran repeated mea-
sures analyses of variance with system (Sim-
ple Wikipedia, PBMT-R, Zhu, RevILP, word-
substitution baseline) as the independent variable,
and the three individual metrics as well as their com-
bined mean as the dependent variables. Mauchlys
test for sphericity was used to test for homogeneity
of variance, and when this test was significant we
applied a Greenhouse-Geisser correction on the de-
grees of freedom (for the purpose of readability we
report the normal degrees of freedom in these cases).
Planned pairwise comparisons were made with the
Bonferroni method. Table 4 displays these results.
First, we consider the 3 metrics in isolation, be-
ginning with Fluency. We find that participants
rated the Fluency of the simplified sentences from
the four systems and Simple Wikipedia differently,
F (4, 180) = 178.436, p < .001, ?2p = .799. The
word-substitution baseline, Simple Wikipedia and
PBMT-R receive the highest scores (3.86, 3.84 and
3.83 respectively) and don?t achieve significantly
different scores on this dimension. All other pair-
wise comparisons are significant at p < .001. Rev-
ILP attains a score of 3.18, while the Zhu system
achieves the lowest mean judgement score of 2.59.
Participants also rated the systems significantly
differently on the Adequacy scale, F (4, 180) =
116.509, p < .001, ?2p = .721. PBMT-R scores
highest (3.71), followed by the word-substitution
baseline (3.58), RevILP (3.28), and then by Simple
Wikipedia (2.91) and the Zhu system (2.82). Sim-
ple Wikipedia and the Zhu system do not differ sig-
nificantly, and all other pairwise comparisons are
significant at p < .001. The low score of Simple
Wikipedia indicates indirectly that the human edi-
tors of Simple Wikipedia texts often choose to devi-
ate quite markedly from the meaning of the original
text.
Key to the task of simplification are the hu-
man judgements of Simplicity. Participants rated
the Simplicity of the output from the four sys-
tems and Simple Wikipedia differently, F (4, 180) =
74.959, p < .001, ?2p = .625. Simple Wikipedia
scores highest (3.68) and the word substitution base-
line scores lowest (2.42). Between them are the
RevILP (2.96), Zhu (2.93) and PBMT-R (2.88) sys-
1020
System Overall Fluency Adequacy Simplicity
Simple Wikipedia 3.46 (0.39) 3.84 (0.46) 2.91 (0.32) 3.68 (0.39)
Word Sub 3.39 (0.43) 3.86 (0.49) 3.58 (0.35) 2.42 (0.48)
Zhu 2.78 (0.45) 2.59 (0.48) 2.82 (0.37) 2.93 (0.50)
RevILP 3.13 (0.36) 3.18 (0.45) 3.28 (0.32) 2.96 (0.39)
PBMT-R 3.47 (0.46) 3.83 (0.49) 3.71 (0.44) 2.88 (0.46)
Table 4: Mean scores assigned by human subjects, with the standard deviation between brackets
Adequacy Simplicity Flesch-Kincaid BLEU
Fluency 0.45** 0.24* 0.42** 0.26**
Adequacy -0.19 0.40** -0.14
Simplicity -0.45** 0.42**
Flesch-Kincaid -0.11
Table 5: Pearson correlation between the different dimensions as assigned by humans and the automatic metrics.
Scores marked * are significant at p < .05 and scores marked ** are significant at p < .01
tems, which do not score significantly differently
from each other. All other pairwise comparisons are
significant at p < .001.
Finally we report on a combined score created by
averaging over the Fluency, Adequacy and Simplic-
ity scores. Inspection of this score, displayed in the
leftmost column of Table 4, reveals that the PBMT-
R system and Simple Wikipedia score best (3.47
and 3.46 respectively), followed by the word substi-
tution baseline (3.39), which in turn scores higher
than RevILP (3.13) and the Zhu system (2.78).
We find that participants rated the systems signifi-
cantly differently overall, F (4, 180) = 98.880, p <
.001, ?2p = .687. All pairwise comparisons were sta-
tistically significant (p < .01), except the one be-
tween the PBMT-R system and Simple Wikipedia.
4.3 Correlations
Table 5 displays the correlations between the scores
assigned by humans (Fluency, Adequacy and Sim-
plicity) and the automatic metrics (Flesch-Kincaid
and BLEU). We see a significant correlation be-
tween Fluency and Adequacy (0.45), as well as be-
tween Fluency and Simplicity (0.24). There is a neg-
ative significant correlation between Flesch-Kincaid
scores and Simplicity (-0.45) while there is a posi-
tive significant correlation between Flesch-Kincaid
and Adequacy and Fluency. The significant correla-
tions between BLEU and Simplicity (0.42) and Flu-
ency (0.26) are both in the positive direction. There
is no significant correlation between BLEU and Ad-
equacy, indicating BLEU?s relative weakness in as-
sessing the semantic overlap between input and out-
put. BLEU and Flesch-Kincaid do not show a sig-
nificant correlation.
5 Discussion
We conclude that a phrase-based machine trans-
lation system with added dissimilarity-based re-
ranking of the best ten output sentences can suc-
cessfully be used to perform sentence simplifica-
tion. Even though the system merely performs
phrase-based machine translation and is not specif-
ically geared towards simplification were it not for
the dissimilarity-based re-ranking of the output, it
performs not significantly differently from state-of-
the-art sentence simplification systems in terms of
human-judged Simplification. In terms of Fluency
and Adequacy our system is judged to perform sig-
nificantly better. From the relatively low average
numbers of edits made by our system we can con-
clude that our system performs relatively small num-
bers of changes to the input, that still constitute as
sensible simplifications. It does not split sentences
(which the Zhu and RevILP systems regularly do);
it only rephrases phrases. Yet, it does this better
than a word-substitution baseline, which can also be
considered a conservative approach; this is reflected
in the baseline?s high Fluency score (roughly equal
to PBMT-R and Simple Wikipedia) and Adequacy
score (only slightly worse than PBMT-R).
1021
Wikipedia the judge ordered that chapman should receive psychiatric treatment in prison and sentenced
him to twenty years to life , slightly less than the maximum possible of twenty-five years to
life .
Simple
Wikipedia
he was sentenced to twenty-five years to life in prison in 1981 .
Word-
substitution
baseline
the judge ordered that chapman should have psychiatric treatment in prison and sentenced
him to twenty years to life , slightly less than the maximum possible of twenty-five years to
life .
Zhu the judge ordered that chapman should get psychiatric treatment . in prison and sentenced
him to twenty years to life , less maximum possible of twenty-five years to life .
RevILP the judge ordered that chapman should will get psychiatric treatment in prison . he sentenced
him to twenty years to life to life .
PBMT-R the judge ordered that chapman should get psychiatric treatment in prison and sentenced him
to twenty years to life , a little bit less than the highest possible to twenty-five years to life .
Table 6: Example output
The output of all systems, the original and the
simplified version of an example sentence from the
PWKP dataset is displayed in Table 6. The Simple
Wikipedia sentences illustrate that significant por-
tions of the original sentences may be dropped, and
parts of the semantics of the original sentence dis-
carded. We also see the Zhu and RevILP systems
resorting to splitting the original sentence in two,
leading to better Flesch-Kincaid scores. The word-
substitution baseline changes ?receive? in ?have?,
while the PBMT-R system changes the same ?re-
ceive? in ?get?, ?slightly? to ?a little bit?, and ?maxi-
mum? to ?highest?.
In terms of automatic measures we see that the
Zhu system scores particularly well on the Flesch-
Kincaid metric, while the RevILP system and our
PBMT-R system achieve the highest BLEU scores.
We believe that for the evaluation of sentence sim-
plification, BLEU is a more appropriate metric than
Flesch-Kincaid or a similar readability metric, al-
though it should be noted that BLEU was found only
to correlate significantly with Fluency, not with Ad-
equacy. While BLEU and NIST may be used with
this in mind, readability metrics should be avoided
altogether in our view. Where machine translation
evaluation metrics such as BLEU take into account
gold references, readability metrics only take into
account characteristics of the sentence such as word
length and sentence length, and ignore grammatical-
ity or the semantic adequacy of the content of the
output sentence, which BLEU is aimed to implic-
itly approximate by measuring overlap in n-grams.
Arguably, readability metrics are best suited to be
applied to texts that can be considered grammati-
cal and meaningful, which is not necessarily true for
the output of simplification algorithms. A disrup-
tive example that would illustrate this point would
be a system that would randomly split original sen-
tences in two or more sequences, achieving consid-
erably lower Flesch-Kincaid scores, yet damaging
the grammaticality and semantic coherence of the
original text, as is evidenced by the negative cor-
relation for Simplicity and positive correlations for
Fluency and Adequacy in Table 5.
In the future we would like to investigate how we
can boost the number of edits the system performs,
while still producing grammatical and meaning-
preserving output. Although the comparison against
the Zhu system, which uses syntax-driven machine
translation, shows no clear benefit for syntax-based
machine translation, it may still be the case that ap-
proaches such as Hiero (Chiang et al, 2005) and
Joshua (Li et al, 2009), enhanced by dissimilarity-
based re-ranking, would improve over our current
system. Furthermore, typical simplification oper-
ations such as sentence splitting and more radical
syntax alterations or even document-level operations
such as manipulations of the co-reference structure
would be interesting to implement and test
Acknowledgements
We are grateful to Zhemin Zhu and Kristian Woods-
end for sharing their data. We would also like to
thank the anonymous reviewers for their comments.
1022
References
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In ACL ?05:
Proceedings of the 43rd Annual Meeting on Associ-
ation for Computational Linguistics, pages 597?604,
Morristown, NJ, USA. Association for Computational
Linguistics.
Chris Callison-Burch. 2008. Syntactic constraints
on paraphrases extracted from parallel corpora. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP ?08,
pages 196?205, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Yvonne Canning, John Tait, Jackie Archibald, and Ros
Crawley. 2000. Cohesive regeneration of syntacti-
cally simplified newspaper text. In Proceedings of RO-
MAND 2000, Lausanne.
John Carroll, Guido Minnen, Yvonne Canning, Siobhan
Devlin, and John Tait. 1998. Practical simplification
of English newspaper text to assist aphasic readers.
In AAAI-98 Workshop on Integrating Artificial Intelli-
gence and Assistive Technology, Madison, Wisconsin.
John Carroll, Guido Minnen, Darren Pearce, Yvonne
Canning, Siobhan Devlin, and John Tait. 1999. Sim-
plifying text for language-impaired readers. In Pro-
ceedings of EACL?99, Bergen. ACL.
R. Chandrasekar and B. Srinivas. 1997. Automatic
rules for text simplification. Knowledge-Based Sys-
tems, 10:183?190.
Raman Chandrasekar, Christine Doran, and Bangalore
Srinivas. 1996. Motivations and methods for text
simplification. In Proceedings of the Sixteenth In-
ternational Conference on Computational Linguistics
(COLING?96), pages 1041?1044.
David Chiang, Adam Lopez, Nitin Madnani, Christof
Monz, Philip Resnik, and Michael Subotin. 2005. The
hiero machine translation system: extensions, evalua-
tion, and analysis. In Proceedings of the conference on
Human Language Technology and Empirical Methods
in Natural Language Processing, HLT ?05, pages 779?
786, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Will Coster and David Kauchak. 2011. Learning to
simplify sentences using wikipedia. In Proceedings
of the Workshop on Monolingual Text-To-Text Gener-
ation, pages 1?9, Portland, Oregon, June. Association
for Computational Linguistics.
Walter Daelemans, Jakub Zavrel, Peter Berck, and Steven
Gillis. 1996. MBT: A Memory-Based Part of Speech
Tagger-Generator. In Proc. of Fourth Workshop on
Very Large Corpora, pages 14?27. ACL SIGDAT.
Walter Daelemans, Anja Hothker, and Erik Tjong
Kim Sang. 2004. Automatic sentence simplification
for subtitling in dutch and english. In Proceedings
of the 4th International Conference on Language Re-
sources and Evaluation, pages 1045?1048.
George Doddington. 2002. Automatic evaluation of ma-
chine translation quality using n-gram co-occurrence
statistics. In Proceedings of the second interna-
tional conference on Human Language Technology
Research, HLT ?02, pages 138?145, San Francisco,
CA, USA. Morgan Kaufmann Publishers Inc.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. The MIT Press, May.
Katja Filippova and Michael Strube. 2008. Sentence fu-
sion via dependency graph compression. In Proceed-
ings of the 2008 Conference on Empirical Methods in
Natural Language Processing, pages 177?185, Hon-
olulu, Hawaii, October. Association for Computational
Linguistics.
Kentaro Inui, Atsushi Fujita, Tetsuro Takahashi, Ryu
Iida, and Tomoya Iwakura. 2003. Text simplification
for reading assistance: A project note. In Proceedings
of the Second International Workshop on Paraphras-
ing, pages 9?16, Sapporo, Japan, July. Association for
Computational Linguistics.
Kevin Knight and Daniel Marcu. 2000. Statistics-based
summarization ? step one: Sentence compression. In
Proceedings of the 17th National Conference on Ar-
tificial Intelligence (AAAI), pages 703 ? 710, Austin,
Texas, USA, July 30 ? August 3.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris C.
Burch, Marcello Federico, Nicola Bertoldi, Brooke
Cowan, Wade Shen, Christine Moran, Richard Zens,
Chris Dyer, Ondrej Bojar, Alexandra Constantin, and
Evan Herbst. 2007. Moses: Open source toolkit for
statistical machine translation. In ACL. The Associa-
tion for Computer Linguistics.
V. Levenshtein. 1966. Binary codes capable of correct-
ing deletions, insertions, and reversals. Soviet Physics
Doklady, 10(8):707?710.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-
itkevitch, Sanjeev Khudanpur, Lane Schwartz, Wren
N. G. Thornton, Jonathan Weese, and Omar F. Zaidan.
2009. Joshua: an open source toolkit for parsing-
based machine translation. In Proceedings of the
Fourth Workshop on Statistical Machine Translation,
pages 135?139, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Nitin Madnani, Necip Fazil Ayan, Philip Resnik, and
Bonnie J. Dorr. 2007. Using paraphrases for pa-
rameter tuning in statistical machine translation. In
Proceedings of the Second Workshop on Statistical
Machine Translation, StatMT ?07, pages 120?127,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
1023
Rani Nelken and Stuart M. Shieber. 2006. Towards ro-
bust context-sensitive sentence alignment for monolin-
gual corpora. In Proceedings of the 11th Conference
of the European Chapter of the Association for Com-
putational Linguistics (EACL-06), Trento, Italy, 3?7
April.
Franz J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Comput. Linguist., 29(1):19?51, March.
Chris Quirk, Chris Brockett, and William Dolan. 2004.
Monolingual machine translation for paraphrase gen-
eration. In Dekang Lin and Dekai Wu, editors, Pro-
ceedings of EMNLP 2004, pages 142?149, Barcelona,
Spain, July. Association for Computational Linguis-
tics.
Advaith Siddharthan. 2002. An architecture for a text
simplification system. In Language Engineering Con-
ference, page 64. IEEE Computer Society.
David A. Smith and Jason Eisner. 2006. Quasi-
synchronous grammars: Alignment by soft projection
of syntactic dependencies. In Proceedings of the HLT-
NAACL Workshop on Statistical Machine Translation,
pages 23?30, New York, June.
Andreas Stolcke. 2002. SRILM - An Extensible Lan-
guage Modeling Toolkit. In In Proc. Int. Conf. on
Spoken Language Processing, pages 901?904, Denver,
Colorado.
D. Vickrey and D. Koller. 2008. Sentence simplification
for semantic role labeling. In Proceedings of the 46th
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies.
Willian Massami Watanabe, Arnaldo Candido Junior,
Vincius Rodriguez de Uz?da, Renata Pontin de Mat-
tos Fortes, Thiago Alexandre Salgueiro Pardo, and
Sandra M. Alusio. 2009. Facilita: reading assistance
for low-literacy readers. In Brad Mehlenbacher, Aris-
tidis Protopsaltis, Ashley Williams, and Shaun Slat-
tery, editors, SIGDOC, pages 29?36. ACM.
Kristian Woodsend and Mirella Lapata. 2011. Learning
to simplify sentences with quasi-synchronous gram-
mar and integer programming. In Proceedings of
the 2011 Conference on Empirical Methods in Natu-
ral Language Processing, pages 409?420, Edinburgh,
Scotland, UK., July. Association for Computational
Linguistics.
Sander Wubben, Antal van den Bosch, and Emiel Krah-
mer. 2010. Paraphrase generation as monolingual
translation: data and evaluation. In Proceedings of the
6th International Natural Language Generation Con-
ference, INLG ?10, pages 203?207, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Kenji Yamada and Kevin Knight. 2001. A syntax-
based statistical translation model. In Proceedings of
the 39th Annual Meeting on Association for Computa-
tional Linguistics, ACL ?01, pages 523?530, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Mark Yatskar, Bo Pang, Cristian Danescu-Niculescu-
Mizil, and Lillian Lee. 2010. For the sake of simplic-
ity: Unsupervised extraction of lexical simplifications
from Wikipedia. In Proceedings of the NAACL, pages
365?368.
Shiqi Zhao, Xiang Lan, Ting Liu, and Sheng Li. 2009.
Application-driven statistical paraphrase generation.
In Proceedings of the Joint Conference of the 47th
Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Process-
ing of the AFNLP: Volume 2 - Volume 2, ACL ?09,
pages 834?842, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Zhemin Zhu, Delphine Bernhard, and Iryna Gurevych.
2010. A monolingual tree-based translation model for
sentence simplification. In Proceedings of the 23rd In-
ternational Conference on Computational Linguistics
(Coling 2010), pages 1353?1361, Beijing, China, Au-
gust. Coling 2010 Organizing Committee.
1024
GRAPH: The Costs of Redundancy in Referring Expressions
Emiel Krahmer
Tilburg University
The Netherlands
e.j.krahmer@uvt.nl
Marie?t Theune
University of Twente
The Netherlands
m.theune@utwente.nl
Jette Viethen
Macquarie University
Australia
jviethen@ics.mq.edu.au
Iris Hendrickx
University of Antwerp
Belgium
iris.hendrickx@ua.ac.be
Abstract
We describe a graph-based generation sys-
tem that participated in the TUNA attribute se-
lection and realisation task of the REG 2008
Challenge. Using a stochastic cost function
(with certain properties for free), and trying
attributes from cheapest to more expensive,
the system achieves overall .76 DICE and .54
MASI scores for attribute selection on the de-
velopment set. For realisation, it turns out
that in some cases higher attribute selection
accuracy leads to larger differences between
system-generated and human descriptions.
1 Introduction
Referring Expression Generation (REG) is a key-
task in NLG, and the topic of the REG 2008 Chal-
lenge.1 In this context, referring expressions are
understood as distinguishing descriptions: descrip-
tions that uniquely characterize a target object in a
visual scene (e.g., ?the red sofa?), and do not ap-
ply to any of the other objects in the scene (the dis-
tractors). Generating such descriptions is usually as-
sumed to be a two-step procedure: first, it has to be
decided which attributes of the target suffice to char-
acterize it uniquely, and then the selected set of at-
tributes should be converted into natural language.
For the first step, attribute selection, we use a ver-
sion of the Graph-based REG algorithm of Krahmer
et al (2003). In this approach, a visual scene is rep-
resented as a directed labelled graph, where vertices
represent the objects in the scene and edges their at-
tributes. A key ingredient of the approach is that
1See http://www.itri.brighton.ac.uk/research/reg08/.
costs can be assigned to attributes; the generation
of referring expressions can then be defined as a
graph search problem, which outputs the cheapest
distinguishing graph (if one exists) given a particu-
lar cost function. For the second step, realisation, we
use a simple template-based realiser written by Irene
Langkilde-Geary from Brighton University that was
made available to all REG 2008 participants.
A version of the Graph-based algorithm was sub-
mitted for the ASGRE 2007 Challenge (Theune et
al. 2007). For us, one of the most striking, gen-
eral outcomes was the observed ?trend for the mean
DICE score obtained by a system to decrease as the
proportion of minimal descriptions increases? (Belz
and Gatt 2007).2 Thus, while REG systems have
a tendency to produce minimal descriptions, hu-
man speakers tend to include redundant properties in
their descriptions, which is in line with recent find-
ings in psycholinguistics on the production of refer-
ring expressions (e.g., Engelhardt et al 2006).
In principle, the graph-based approach has the po-
tential to deal with redundancy by allowing some at-
tributes to have zero costs. Viethen et al (2008),
however, show that merely assigning zero costs to
an attribute is not a sufficient condition for inclu-
sion; if the search terminates before the free prop-
erties are tried, they will not be included. In other
words: the order in which attributes are tried should
be explicitly controlled as well. In the experiment
we describe here, we consider both these factors and
their interplay.
2DICE (like MASI) is a measure for similarity between a pre-
dicted attribute set and a (human produced) reference set.
227
2 Method
We experimentally combine four cost functions and
two search orders (Table 1). (1) Simple simply as-
signs each edge a 1-point cost. (2) Stochastic asso-
ciates each edge with a frequency-based cost, based
on both the 2008 training and development sets (as-
suming that a larger data set alows for more ac-
curate frequency estimates). (3) Free-Stochastic is
like the previous cost function, except that highly
frequent attributes are assigned 0 costs. For the Fur-
niture domain, this applies to ?colour?; for People
to ?hasBeard = 1? and ?hasGlasses = 1.? (4) Free-
Naive, finally, reduces the relatively fine-grained
costs of Free-Stochastic to three values (0 = free,
1 = cheap, 2 = expensive). In addition, we com-
pare results for two property orderings: (A) Proper-
ties are tried in a Random order. (B) Cost-based,
where properties are tried (in stochastic order) from
cheapest to most expensive. Finally, since human
speakers nearly always include the ?type? property,
we decided to simply always include it. Tables 2 to
4 summarize the evaluation results for all combina-
tions of cost functions and search orders.
3 Attribute Selection Results
The measures used to evaluate attribute selection are
DICE, MASI, attribute accuracy (A-A, the proportion
of times the generated attribute set was identical to
the reference set), and minimality (MIN).
Notice first that the order in which attributes are
tried in the search process matters; the B-systems
nearly always outperform their A-counterparts. Sec-
ond, assigning varying costs also helps; both 1-
variants (Simple costs) perform worse than the sys-
tems building on Stochastic cost functions (2, 3
and 4). Third, adding free properties is also ben-
eficial; the 3 and 4 variants clearly outperform the
1 and 2 variants. It is interesting to observe that
the Free-naive cost function (4) performs equally
well as the more principled Free-stochastic (3), but
only in combination with the Cost-based order (B).
To the extent that it is possible to compare the re-
sults, the submitted GRAPH 4+B outperforms our
best 2007 variant (GRAPH FP in Table 2). This sug-
gests that the interplay between property ordering
and cost function is a flexible and efficient approach
to attribute selection.
Table 1: Overview of cost functions and search orders.
The GRAPH 4+B settings were submitted to the REG
2008 Challenge.
Costs Orders
1 Simple A Random
2 Stochastic B Cost-based
3 Free-stochastic
4 Free-naive
Table 2: Furniture development set results (80 trials).
GRAPH DICE MASI A-A MIN EDIT S-A
1+A .61 .32 .12 .29 5.90 .04
1+B .61 .31 .12 .29 5.89 .04
2+A .71 .47 .31 .11 5.06 .05
2+B .69 .44 .28 .16 5.19 .05
3+A .80 .58 .45 .00 4.90 .05
3+B .80 .58 .45 .00 4.90 .05
4+A .80 .59 .48 .00 4.61 .05
4+B .80 .59 .48 .00 4.61 .05
FP 2007 .71 ? ? ? ? ?
Table 3: People development set results (68 trials).
GRAPH DICE MASI A-A MIN EDIT S-A
1+A .59 .36 .24 .00 6.54 .00
1+B .66 .42 .24 .00 6.78 .00
2+A .66 .42 .24 .00 6.78 .00
2+B .66 .42 .24 .00 6.78 .00
3+A .68 .41 .19 .00 6.79 .00
3+B .72 .48 .28 .00 6.96 .00
4+A .59 .34 .18 .00 6.56 .00
4+B .72 .48 .28 .00 6.96 .00
FP 2007 .67 ? ? ? ? ?
Table 4: Combined Furniture and People development set
results.
GRAPH DICE MASI A-A MIN EDIT S-A
1+A .60 .34 .18 .16 6.20 .02
1+B .63 .36 .18 .16 6.30 .02
2+A .69 .45 .28 .06 5.85 .03
2+B .68 .43 .26 .09 5.92 .03
3+A .74 .51 .33 .00 5.77 .03
3+B .76 .54 .37 .00 5.84 .03
4+A .70 .48 .34 .00 5.51 .03
4+B .76 .54 .39 .00 5.69 .03
FP 2007 .69 ? ? ? ? ?
228
4 Realization Results
To evaluate realisation, the following two word-
string comparison measures were used: string-edit
distance (EDIT), which is the Levenshtein distance
between generated word string and human reference
output, and string accuracy (S-A), which is the pro-
portion of times the word string was identical to the
reference string.
For all settings of the algorithm, we see that S-A
is much lower than A-A. This is as expected, since
any set of attributes can be expressed in many differ-
ent ways, and the chance that the realizer produces
exactly the same string as the human reference is
quite small. For the furniture domain, we see that
S-A has a fairly constant low score, while EDIT fol-
lows the same pattern as A-A: including redundant
(free) properties leads to better results. For the peo-
ple domain, S-A is always 0, and surprisingly EDIT
gets worse as A-A gets better.
To explain these results, we inspect those descrip-
tions where A-A = 1 but S-A = 0, i.e., the attribute
set is identical to the human reference but the word
string is not. In setting 4+B (submitted to REG 2008)
this is the case for 34 furniture and 19 people de-
scriptions. For furniture, we see that the low S-A
score can be largely explained by the fact that in 23
of the 34 descriptions the human reference either in-
cluded no determiner or an indefinite one, whereas
the system always included a definite determiner.
This also explains why S-A hardly improves with
higher A-A scores, since determiner choice is inde-
pendent from attribute selection.
In the people domain, the zero scores for S-A can
be explained by the fact that the realizer always uses
?person? to express the type attribute, where the hu-
man references have either ?man? or ?guy? (in line
with the human preference for basic level values; cf.
Krahmer et al 2003). We also encounter the de-
terminer problem again, aggravated by the fact that
many person descriptions include embedded noun
phrases (e.g., ?man with beard?).
To find out why EDIT gets worse as A-A increases
for different system settings in the people domain,
we look at the six descriptions that have A-A = 1
for setting 4+B but not for 4+A. It turns out that
five of these descriptions are realized as ?the light-
haired person with a beard?, while the human refer-
ence strings are variations of ?the man with a white
beard?, resulting in a relatively high EDIT value. The
problem here is that the link between beard and hair
colour has been lost in the data annotation process.
In general, we can conclude that simply combin-
ing more or less human-like attribute selection with
an off-the-shelf surface realiser is not sufficient to
produce human-like referring expressions.
Acknowledgements We thank the REG 2008 orga-
nizers for making the realiser available, and Hendri
Hondorp for his help with installing and using it.
References
Belz, A. and A. Gatt 2007. The attribute selection for
GRE challenge: Overview and evaluation results Pro-
ceedings of UCNLG+MT 75-83
Engelhardt, P., K. Bailey and F. Ferreira 2006. Do speak-
ers and listeners observe the Gricean Maxim of Quan-
tity? Journal of Memory and Language, 54, 554-573.
Krahmer, E., S. van Erk and A. Verleg 2003. Graph-
based generation of referring expressions. Computa-
tional Linguistics, 29(1), 5372.
Theune, M., P. Touset, J. Viethen, and E. Krahmer. 2007.
Cost-based attribute selection for generating referring
expressions (GRAPH-FP and GRAPH-SC). Proceedings
of the ASGRE Challenge 2007, Copenhagen, Denmark
Viethen, J., R. Dale, E. Krahmer, M. Theune and P. Tou-
set. 2008. Controlling redundancy in referring expres-
sions. Proceedings LREC 08, Marrakech, Morroco.
229
Cross-Linguistic Attribute Selection for REG:
Comparing Dutch and English
Marie?t Theune
University of Twente
The Netherlands
M.Theune@utwente.nl
Ruud Koolen
Tilburg University
The Netherlands
R.M.F.Koolen@uvt.nl
Emiel Krahmer
Tilburg University
The Netherlands
E.J.Krahmer@uvt.nl
Abstract
In this paper we describe a cross-linguistic
experiment in attribute selection for refer-
ring expression generation. We used a
graph-based attribute selection algorithm
that was trained and cross-evaluated on
English and Dutch data. The results indi-
cate that attribute selection can be done in
a largely language independent way.
1 Introduction
A key task in natural language generation is refer-
ring expression generation (REG). Most work on
REG is aimed at producing distinguishing descrip-
tions: descriptions that uniquely characterize a tar-
get object in a visual scene (e.g., ?the red sofa?),
and do not apply to any of the other objects in the
scene (the distractors). The first step in generating
such descriptions is attribute selection: choosing a
number of attributes that uniquely characterize the
target object. In the next step, realization, the se-
lected attributes are expressed in natural language.
Here we focus on the attribute selection step. We
investigate to which extent attribute selection can
be done in a language independent way; that is,
we aim to find out if attribute selection algorithms
trained on data from one language can be success-
fully applied to another language. The languages
we investigate are English and Dutch.
Many REG algorithms require training data, be-
fore they can successfully be applied to generate
references in a particular domain. The Incremen-
tal Algorithm (Dale and Reiter, 1995), for exam-
ple, assumes that certain attributes are more pre-
ferred than others, and it is assumed that determin-
ing the preference order of attributes is an empir-
ical matter that needs to be settled for each new
domain. The graph-based algorithm (Krahmer et
al., 2003), to give a second example, similarly
assumes that certain attributes are preferred (are
?cheaper?) than others, and that data are required
to compute the attribute-cost functions.
Traditional text corpora have been argued to be
of restricted value for REG, since these typically
are not ?semantically transparent? (van Deemter
et al, 2006). Rather what seems to be needed is
data collected from human participants, who pro-
duce referring expressions for specific targets in
settings where all properties of the target and its
distractors are known. Needless to say, collecting
and annotating such data takes a lot of time and ef-
fort. So what to do if one wants to develop a REG
algorithm for a new language? Would this require
a new data collection, or could existing data col-
lected for a different language be used? Clearly,
linguistic realization is language dependent, but to
what extent is attribute selection language depen-
dent? This is the question addressed in this paper.
Below we describe the English and Dutch cor-
pora used in our experiments (Section 2), the
graph-based algorithm we used for attribute se-
lection (Section 3), and the corpus-based attribute
costs and orders used by the algorithm (Section 4).
We present the results of our cross-linguistic at-
tribute selection experiments (Section 5) and end
with a discussion and conclusions (Section 6).
2 Corpora
2.1 English: the TUNA Corpus
For English data, we used the TUNA corpus of
object descriptions (Gatt et al, 2007). This cor-
pus was created by presenting the participants in
an on-line experiment with a visual scene consist-
ing of seven objects and asking them to describe
one of the objects, the target, in such a way that it
could be uniquely identified. There were two ex-
perimental conditions: in the +LOC condition, the
participants were free to describe the target object
using any of its properties, including its location
on the screen, whereas in the -LOC condition they
were discouraged (but not prevented) from men-
tioning object locations. The resulting object de-
scriptions were annotated using XML and com-
bined with an XML representation of the visual
scene, listing all objects and their properties in
terms of attribute-value pairs. The TUNA corpus
is split into two domains: one with descriptions of
furniture and one with descriptions of people.
The TUNA corpus was used for the comparative
evaluation of REG systems in the TUNA Chal-
lenges (2007-2009). For our current experiments,
we used the TUNA 2008 Challenge training and
development sets (Gatt et al, 2008) to train and
evaluate the graph-based algorithm on.
2.2 Dutch: the D-TUNA Corpus
For Dutch, we used the D(utch)-TUNA corpus of
object descriptions (Koolen and Krahmer, 2010).
The collection of this corpus was inspired by the
TUNA experiment described above, and was done
using the same visual scenes. There were three
conditions: text, speech and face-to-face. The
text condition was a replication (in Dutch) of the
TUNA experiment: participants typed identify-
ing descriptions of target referents, distinguishing
them from distractor objects in the scene. In the
other two conditions participants produced spo-
ken descriptions for an addressee, who was either
visible to the speaker (face-to-face condition) or
not (speech condition). The resulting descriptions
were annotated semantically using the XML anno-
tation scheme of the English TUNA corpus.
The procedure in the D-TUNA experiment dif-
fered from that used in the original TUNA exper-
iment in two ways. First, the D-TUNA experi-
ment used a laboratory-based set-up, whereas the
TUNA study was conducted on-line in a relatively
uncontrolled setting. Second, participants in the
D-TUNA experiment were completely prevented
from mentioning object locations.
3 Graph-Based Attribute Selection
For attribute selection, we use the graph-based al-
gorithm of Krahmer et al (2003), one of the
highest scoring attribute selection methods in the
TUNA 2008 Challenge (Gatt et al (2008), table
11). In this approach, a visual scene with tar-
get and distractor objects is represented as a la-
belled directed graph, in which the objects are
modelled as nodes and their properties as looping
edges on the corresponding nodes. To select the
attributes for a distinguishing description, the al-
gorithm searches for a subgraph of the scene graph
that uniquely refers to the target referent. Starting
from the node representing the target, it performs a
depth-first search over the edges connected to the
subgraph found so far. The algorithm?s output is
the cheapest distinguishing subgraph, given a par-
ticular cost function that assigns costs to attributes.
By assigning zero costs to some attributes, e.g.,
the type of an object, the human tendency to men-
tion redundant attributes can be mimicked. How-
ever, as shown by Viethen et al (2008), merely
assigning zero costs to an attribute is not a suffi-
cient condition for inclusion; if the graph search
terminates before the free attributes are tried, they
will not be included. Therefore, the order in which
attributes are tried must be explicitly controlled.
Thus, when using the graph-based algorithm for
attribute selection, two things must be specified:
(1) the cost function, and (2) the order in which the
attributes should be searched. Both can be based
on corpus data, as described in the next section.
4 Costs and Orders
For our experiments, we used the graph-based at-
tribute selection algorithm with two types of cost
functions: Stochastic costs and Free-Na??ve costs.
Both reflect (to a different extent) the relative at-
tribute frequencies found in a training corpus: the
more frequently an attribute occurs in the training
data, the cheaper it is in the cost functions.
Stochastic costs are directly based on the at-
tribute frequencies in the training corpus. They
are derived by rounding ?log2(P (v)) to the first
decimal and multiplying by 10, where P (v) is the
probability that attribute v occurs in a description,
given that the target object actually has this prop-
erty. The probability P (v) is estimated by deter-
mining the frequency of each attribute in the train-
ing corpus, relative to the number of target ob-
jects that possess this attribute. Free-Na??ve costs
more coarsely reflect the corpus frequencies: very
frequent attributes are ?free? (cost 0), somewhat
frequent attributes have cost 1 and infrequent at-
tributes have cost 2. Both types of cost functions
are used in combination with a stochastic ordering,
where attributes are tried in the order of increasing
stochastic costs.
In total, four cost functions were derived from
the English corpus data and four cost functions de-
rived from the Dutch corpus data. For each lan-
guage, we had two Stochastic cost functions (one
for the furniture domain and one for the people do-
main), and two Free-Na??ve cost functions (idem),
giving eight different cost functions in total. For
each language we determined two attribute orders
to be used with the cost functions: one for the fur-
niture domain and one for the people domain.
4.1 English Costs and Order
For English, we used the Stochastic and Free-
Na??ve cost functions and the stochastic order from
Krahmer et al (2008). The Stochastic costs
and order were derived from the attribute frequen-
cies in the combined training and development
sets of the TUNA 2008 Challenge (Gatt et al,
2008), containing 399 items in the furniture do-
main and 342 items in the people domain. The
Free-Na??ve costs are simplified versions of the
stochastic costs. ?Free? attributes are TYPE in
both domains, COLOUR for the furniture domain
and HASBEARD and HASGLASSES for the people
domain. Expensive attributes (cost 2) are X- and
Y-DIMENSION in the furniture domain and HAS-
SUIT, HASSHIRT and HASTIE in the people do-
main. All other attributes have cost 1.
4.2 Dutch Costs and Order
The Dutch Stochastic costs and order were de-
rived from the attribute frequencies in a set of 160
items (for both furniture and people) randomly se-
lected from the text condition in the D-TUNA cor-
pus. Interestingly, our Stochastic cost computa-
tion method led to an assignment of 0 costs to
the COLOUR attribute in the furniture domain, thus
enabling the Dutch Stochastic cost function to in-
clude colour as a redundant property in the gener-
ated descriptions. In the English stochastic costs,
none of the attributes are free. Another difference
is that in the furniture domain, the Dutch stochas-
tic costs for ORIENTATION attributes are much
lower than the English costs (except with value
FRONT); in the people domain, the same holds for
attributes such as HASSUIT and HASTIE. These
cost differences, which are largely reflected in the
Dutch Free-Na??ve costs, do not seem to be caused
by differences in expressibility, i.e., the ease with
which the attributes can be expressed in the two
languages (Koolen et al, 2010); rather, they may
be due to the fact that the human descriptions in D-
TUNA do not include any DIMENSION attributes.
Language Furniture People
Training Test Dice Acc. Dice Acc.
Dutch Dutch 0.92 0.63 0.78 0.28
English 0.83 0.55 0.73 0.29
English Dutch 0.87 0.58 0.75 0.25
English 0.67 0.29 0.67 0.24
Table 1: Evaluation results for stochastic costs.
Language Furniture People
Training Test Dice Acc. Dice Acc.
Dutch Dutch 0.94 0.70 0.78 0.28
English 0.83 0.55 0.73 0.29
English Dutch 0.94 0.70 0.78 0.28
English 0.83 0.55 0.73 0.29
Table 2: Evaluation results for Free-Na??ve costs.
5 Results
All cost functions were applied to both Dutch and
English test data. As Dutch test data, we used a set
of 40 furniture items and a set of 40 people items,
randomly selected from the text condition in the
D-TUNA corpus. These items had not been used
for training the Dutch cost functions. As English
test data, we used a subset of the TUNA 2008 de-
velopment set (Gatt et al, 2008). To make the En-
glish test data comparable to the Dutch ones, we
only included items from the -LOC condition (see
Section 2.1). This resulted in 38 test items for the
furniture domain, and 38 for the people domain.
Tables 1 and 2 show the results of applying the
Dutch and English cost functions (with Dutch and
English attribute orders respectively) to the Dutch
and English test data. The evaluation metrics used,
Dice and Accuracy (Acc.), both evaluate human-
likeness by comparing the automatically selected
attribute sets to those in the human test data. Dice
is a set-comparison metric ranging between 0 and
1, where 1 indicates a perfect match between sets.
Accuracy is the proportion of system outputs that
exactly match the corresponding human data. The
results were computed using the ?teval? evaluation
tool provided to participants in the TUNA 2008
Challenge (Gatt et al, 2008).
To determine significance, we applied repeated
measures analyses of variance (ANOVA) to the
evaluation results, with three within factors: train-
ing language (Dutch or English), cost function
(Stochastic or Free-Na??ve), and domain (furniture
or people), and one between factor representing
test language (Dutch or English).
An overall effect of cost function shows that the
Free-Na??ve cost functions generally perform better
than the Stochastic cost functions (Dice: F(1,76) =
34.853, p < .001; Accuracy: F(1,76) = 13.052, p =
.001). Therefore, in the remainder of this section
we mainly focus on the results for the Free-Na??ve
cost functions (Table 2).
As can be clearly seen in Table 2, Dutch and
English Free-Na??ve cost functions give almost the
same scores in both the furniture and the people
domain, when applied to the same test language.
The English Free-Na??ve cost function performs
slightly better than the Dutch one on the Dutch
people data, but this difference is not significant.
An overall effect of test language shows that the
cost functions (both Stochastic and Free-Na??ve)
generally give better Dice results on the Dutch
data than for the English data (Dice: F(1,76) =
7.797, p = .007). In line with this, a two-way in-
teraction between test language and training lan-
guage (Dice: F(1,76) = 6.870, p = .011) shows that
both the Dutch and the English cost functions per-
form better on the Dutch data than on the English
data. However, the overall effect of test language
did not reach significance for Accuracy, presum-
ably due to the fact that the Accuracy scores on the
English people data are slightly higher than those
on the Dutch people data.
Finally, the cost functions generally perform
better in the furniture domain than in the people
domain (Dice: F(1,76) = 10.877, p = .001; Accu-
racy: F(1,76) = 16.629, p < .001).
6 Discussion
The results of our cross-linguistic attribute selec-
tion experiments show that Free-Na??ve cost func-
tions, which only roughly reflect the attribute fre-
quencies in the training corpus, have an overall
better performance than Stochastic cost functions,
which are directly based on the attribute frequen-
cies. This holds across the two languages we in-
vestigated, and corresponds with the findings of
Krahmer et al (2008), who compared Stochas-
tic and Free-Na??ve functions that were trained and
evaluated on English data only. The difference in
performance is probably due to the fact that Free-
Na??ve costs are less sensitive to the specifics of
the training data (and are therefore more generally
applicable) and do a better job of mimicking the
human tendency towards redundancy.
Moreover, we found that Free-Na??ve cost func-
tions trained on different languages (English or
Dutch) performed equally well when tested on the
same data (English or Dutch), in both the furniture
and people domain. This suggests that attribute
selection can in fact be done in a language inde-
pendent way, using cost functions that have been
derived from corpus data in one language to per-
form attribute selection for another language.
Our results did show an effect of test language
on performance: both English and Dutch cost
functions performed better when tested on the
Dutch D-TUNA data than on the English TUNA
data. However, this difference does not seem to
be caused by language-specific factors but rather
by the quality of the respective test sets. Although
the English test data were restricted to the -LOC
condition, in which using DIMENSION attributes
was discouraged, still more than 25% of the En-
glish test data (both furniture and people) included
one or more DIMENSION attributes, which were
never selected for inclusion by either the English
or the Dutch Free-Na??ve cost functions. The Dutch
test data, on the other hand, did not include any
DIMENSION attributes. In addition, the English
test data contained more non-unique descriptions
of target objects than the Dutch data, in particu-
lar in the furniture domain. These differences may
be due to the fact that data collection was done
in a more controlled setting for D-TUNA than for
TUNA. In other words, the seeming effect of test
language does not contradict our main conclusion
that attribute selection is largely language inde-
pendent, at least for English and Dutch.
The success of our cross-linguistic experiments
may have to do with the fact that English and
Dutch hardly differ in the expressibility of object
attributes (Koolen et al, 2010). To determine the
full extent to which attribute selection can be done
in a language-dependent way, additional experi-
ments with less similar languages are necessary.
Acknowledgements
We thank the TUNA Challenge organizers for the
English data and the evaluation tool used in our
experiments; Martijn Goudbeek for helping with
the statistical analysis; and Pascal Touset, Ivo
Brugman, Jette Viethen, and Iris Hendrickx for
their contributions to the graph-based algorithm.
This research is part of the VICI project ?Bridg-
ing the gap between psycholinguistics and com-
putational linguistics: the case of referring expres-
sions?, funded by the Netherlands Organization for
Scientific Research (NWO Grant 277-70-007).
References
R. Dale and E. Reiter. 1995. Computational interpre-
tation of the Gricean maxims in the generation of re-
ferring expressions. Cognitive Science, 19(2):233?
263.
A. Gatt, I. van der Sluis, and K. van Deemter. 2007.
Evaluating algorithms for the generation of refer-
ring expressions using a balanced corpus. In Pro-
ceedings of the 11th European Workshop on Natural
Language Generation (ENLG 2007), pages 49?56.
A. Gatt, A. Belz, and E. Kow. 2008. The TUNA Chal-
lenge 2008: Overview and evaluation results. In
Proceedings of the 5th International Natural Lan-
guage Generation Conference (INLG 2008), pages
198?206.
R. Koolen and E. Krahmer. 2010. The D-TUNA cor-
pus: A Dutch dataset for the evaluation of referring
expression generation algorithms. In Proceedings
of the 7th international conference on Language Re-
sources and Evaluation (LREC 2010).
R. Koolen, A. Gatt, M. Goudbeek, and E. Krahmer.
2010. Overspecification in referring expressions:
Causal factors and language differences. Submitted.
E. Krahmer, S. van Erk, and A. Verleg. 2003. Graph-
based generation of referring expressions. Compu-
tational Linguistics, 29(1):53?72.
E. Krahmer, M. Theune, J. Viethen, and I. Hendrickx.
2008. Graph: The costs of redundancy in refer-
ring expressions. In Proceedings of the 5th Inter-
national Natural Language Generation Conference
(INLG 2008), pages 227?229.
K. van Deemter, I. I. van der Sluis, and A. Gatt. 2006.
Building a semantically transparent corpus for the
generation of referring expressions. In Proceedings
of the 4th International Natural Language Genera-
tion Conference (INLG 2006), pages 130?132.
J. Viethen, R. Dale, E. Krahmer, M. Theune, and
P. Touset. 2008. Controlling redundancy in refer-
ring expressions. In Proceedings of the Sixth In-
ternational Conference on Language Resources and
Evaluation (LREC 2008), pages 239?246.
Paraphrase Generation as Monolingual Translation: Data and Evaluation
Sander Wubben, Antal van den Bosch, Emiel Krahmer
Tilburg centre for Cognition and Communication
Tilburg University
Tilburg, The Netherlands
{s.wubben,antal.vdnbosch,e.j.krahmer}@uvt.nl
Abstract
In this paper we investigate the auto-
matic generation and evaluation of senten-
tial paraphrases. We describe a method
for generating sentential paraphrases by
using a large aligned monolingual cor-
pus of news headlines acquired automat-
ically from Google News and a stan-
dard Phrase-Based Machine Translation
(PBMT) framework. The output of this
system is compared to a word substitu-
tion baseline. Human judges prefer the
PBMT paraphrasing system over the word
substitution system. We demonstrate that
BLEU correlates well with human judge-
ments provided that the generated para-
phrased sentence is sufficiently different
from the source sentence.
1 Introduction
Text-to-text generation is an increasingly studied
subfield in natural language processing. In con-
trast with the typical natural language generation
paradigm of converting concepts to text, in text-
to-text generation a source text is converted into a
target text that approximates the meaning of the
source text. Text-to-text generation extends to
such varied tasks as summarization (Knight and
Marcu, 2002), question-answering (Lin and Pan-
tel, 2001), machine translation, and paraphrase
generation.
Sentential paraphrase generation (SPG) is the
process of transforming a source sentence into a
target sentence in the same language which dif-
fers in form from the source sentence, but approx-
imates its meaning. Paraphrasing is often used as
a subtask in more complex NLP applications to
allow for more variation in text strings presented
as input, for example to generate paraphrases of
questions that in their original form cannot be an-
swered (Lin and Pantel, 2001; Riezler et al, 2007),
or to generate paraphrases of sentences that failed
to translate (Callison-Burch et al, 2006). Para-
phrasing has also been used in the evaluation of
machine translation system output (Russo-Lassner
et al, 2006; Kauchak and Barzilay, 2006; Zhou
et al, 2006). Adding certain constraints to para-
phrasing allows for additional useful applications.
When a constraint is specified that a paraphrase
should be shorter than the input text, paraphras-
ing can be used for sentence compression (Knight
and Marcu, 2002; Barzilay and Lee, 2003) as well
as for text simplification for question answering or
subtitle generation (Daelemans et al, 2004).
We regard SPG as a monolingual machine trans-
lation task, where the source and target languages
are the same (Quirk et al, 2004). However, there
are two problems that have to be dealt with to
make this approach work, namely obtaining a suf-
ficient amount of examples, and a proper eval-
uation methodology. As Callison-Burch et al
(2008) argue, automatic evaluation of paraphras-
ing is problematic. The essence of SPG is to gen-
erate a sentence that is structurally different from
the source. Automatic evaluation metrics in re-
lated fields such as machine translation operate on
a notion of similarity, while paraphrasing centers
around achieving dissimilarity. Besides the eval-
uation issue, another problem is that for an data-
driven MT account of paraphrasing to work, a
large collection of data is required. In this case,
this would have to be pairs of sentences that are
paraphrases of each other. So far, paraphrasing
data sets of sufficient size have been mostly lack-
ing. We argue that the headlines aggregated by
Google News offer an attractive avenue.
2 Data Collection
Currently not many resources are available for
paraphrasing; one example is the Microsoft Para-
phrase Corpus (MSR) (Dolan et al, 2004; Nelken
and Shieber, 2006), which with its 139,000 aligned
Police investigate Doherty drug pics
Doherty under police investigation 
Police to probe Pete pics 
Pete Doherty arrested in drug-photo probe 
Rocker photographed injecting unconscious fan 
Doherty ?injected unconscious fan with drug? 
Photos may show Pete Doherty injecting passed-out fan
Doherty ?injected female fan? 
Figure 1: Part of a sample headline cluster, with
aligned paraphrases
paraphrases can be considered relatively small. In
this study we explore the use of a large, automat-
ically acquired aligned paraphrase corpus. Our
method consists of crawling the headlines aggre-
gated and clustered by Google News and then
aligning paraphrases within each of these clusters.
An example of such a cluster is given in Figure 1.
For each pair of headlines in a cluster, we calcu-
late the Cosine similarity over the word vectors of
the two headlines. If the similarity exceeds a de-
fined upper threshold it is accepted; if it is below
a defined lower threshold it is rejected. In the case
that it lies between the thresholds, the process is
repeated but then with word vectors taken from a
snippet from the corresponding news article. This
method, described in earlier work Wubben et al
(2009), was reported to yield a precision of 0.76
and a recall of 0.41 on clustering actual Dutch
paraphrases in a headline corpus. We adapted this
method to English. Our data consists of English
headlines that appeared in Google News over the
period of April to September 2006. Using this
method we end up with a corpus of 7,400,144 pair-
wise alignments of 1,025,605 unique headlines1.
3 Paraphrasing methods
In our approach we use the collection of au-
tomatically obtained aligned headlines to train
a paraphrase generation model using a Phrase-
Based MT framework. We compare this ap-
proach to a word substitution baseline. The gen-
erated paraphrases along with their source head-
1This list of aligned pairs is available at
http://ilk.uvt.nl/?swubben/resources.html
lines are presented to human judges, whose rat-
ings are compared to the BLEU (Papineni et al,
2002), METEOR (Banerjee and Lavie, 2005) and
ROUGE (Lin, 2004) automatic evaluation metrics.
3.1 Phrase-Based MT
We use the MOSES package to train a
Phrase-Based Machine Translation model
(PBMT) (Koehn et al, 2007). Such a model
normally finds a best translation e? of a text in
language f to a text in language e by combining
a translation model p(f |e) with a language model
p(e):
e? = argmax
e?e?
p(f |e)p(e)
GIZA++ is used to perform the word align-
ments (Och and Ney, 2003) which are then used in
the Moses pipeline to generate phrase alignments
in order to build the paraphrase model. We first to-
kenize our data before training a recaser. We then
lowercase all data and use all unique headlines in
the training data to train a language model with the
SRILM toolkit (Stolcke, 2002). Then we invoke
the GIZA++ aligner using the 7M training para-
phrase pairs. We run GIZA++ with standard set-
tings and we perform no optimization. Finally, we
use the MOSES decoder to generate paraphrases
for our test data.
Instead of assigning equal weights to language
and translation model, we assign a larger weight
of 0.7 to the language model to generate better
formed (but more conservative) paraphrases. Be-
cause dissimilarity is a factor that is very impor-
tant for paraphrasing but not implemented in a
PBMT model, we perform post-hoc reranking of
the different candidate outputs based on dissimi-
larity. For each headline in the testset we generate
the ten best paraphrases as scored by the decoder
and then rerank them according to dissimilarity to
the source using the Levenshtein distance measure
at the word level. The resulting headlines are re-
cased using the previously trained recaser.
3.2 Word Substitution
We compare the PBMT results with a simple word
substitution baseline. For each noun, adjective and
verb in the sentence this model takes that word and
its Part of Speech tag and retrieves from Word-
Net its most frequent synonym from the most fre-
quent synset containing the input word. We use the
Memory Based Tagger (Daelemans et al, 1996)
System Headline
Source Florida executes notorious serial killer
PBMT Serial killer executed in Florida
Word Sub. Florida executes ill-famed series slayer
Source Dublin evacuates airport due to bomb scare
PBMT Dublin airport evacuated after bomb threat
Word Sub. Dublin evacuates airdrome due to bomb panic
Source N. Korea blasts nuclear sanctions
PBMT N. Korea nuclear blast of sanctions
Word Sub. N. Korea blasts atomic sanctions
Table 1: Examples of generated paraphrased head-
lines
trained on the Brown corpus to generate the POS-
tags. The WordNet::QueryData2 Perl module is
used to query WordNet (Fellbaum, 1998). Gener-
ated headlines and their source for both systems
are given in Table 1.
4 Evaluation
For the evaluation of the generated paraphrases
we set up a human judgement study, and compare
the human judges? ratings to automatic evaluation
measures in order to gain more insight in the auto-
matic evaluation of paraphrasing.
4.1 Method
We randomly select 160 headlines that meet the
following criteria: the headline has to be compre-
hensible without reading the corresponding news
article, both systems have to be able to produce a
paraphrase for each headline, and there have to be
a minimum of eight paraphrases for each headline.
We need these paraphrases as multiple references
for our automatic evaluation measures to account
for the diversity in real-world paraphrases, as the
aligned paraphrased headlines in Figure 1 witness.
The judges are presented with the 160 head-
lines, along with the paraphrases generated by
both systems. The order of the headlines is ran-
domized, and the order of the two paraphrases for
each headline is also randomized to prevent a bias
towards one of the paraphrases. The judges are
asked to rate the paraphrases on a 1 to 7 scale,
where 1 means that the paraphrase is very bad and
7 means that the paraphrase is very good. The
judges were instructed to base their overall quality
judgement on whether the meaning was retained,
the paraphrase was grammatical and fluent, and
whether the paraphrase was in fact different from
2http://search.cpan.org/dist/WordNet-
QueryData/QueryData.pm
system mean stdev.
PBMT 4.60 0.44
Word Substitution 3.59 0.64
Table 2: Results of human judgements (N = 10)
the source sentence. Ten judges rated two para-
phrases per headline, resulting in a total of 3,200
scores. All judges were blind to the purpose of the
evaluation and had no background in paraphrasing
research.
4.2 Results
The average scores assigned by the human judges
to the output of the two systems are displayed in
Table 2. These results show that the judges rated
the quality of the PBMT paraphrases significantly
higher than those generated by the word substitu-
tion system (t(18) = 4.11, p < .001).
Results from the automatic measures as well
as the Levenshtein distance are listed in Table 3.
We use a Levenshtein distance over tokens. First,
we observe that both systems perform roughly the
same amount of edit operations on a sentence, re-
sulting in a Levenshtein distance over words of
2.76 for the PBMT system and 2.67 for the Word
Substitution system. BLEU, METEOR and three
typical ROUGE metrics3 all rate the PBMT sys-
tem higher than the Word Substitution system.
Notice also that the all metrics assign the high-
est scores to the original sentences, as is to be ex-
pected: because every operation we perform is in
the same language, the source sentence is also a
paraphrase of the reference sentences that we use
for scoring our generated headline. If we pick a
random sentence from the reference set and score
it against the rest of the set, we obtain similar
scores. This means that this score can be regarded
as an upper bound score for paraphrasing: we can
not expect our paraphrases to be better than those
produced by humans. However, this also shows
that these measures cannot be used directly as an
automatic evaluation method of paraphrasing, as
they assign the highest score to the ?paraphrase? in
which nothing has changed. The scores observed
in Table 3 do indicate that the paraphrases gener-
3ROUGE-1, ROUGE-2 and ROUGE-SU4 are also
adopted for the DUC 2007 evaluation campaign,
http://www-nlpir.nist.gov/projects/duc/
duc2007/tasks.html
System BLEU ROUGE-1 ROUGE-2 ROUGE-SU4 METEOR Lev.dist. Lev. stdev.
PBMT 50.88 0.76 0.36 0.42 0.71 2.76 1.35
Wordsub. 24.80 0.59 0.22 0.26 0.54 2.67 1.50
Source 60.58 0.80 0.45 0.47 0.77 0 0
Table 3: Automatic evaluation and sentence Levenshtein scores
0 1 2 3 4 5 6Levenshtein distance
0
0.2
0.4
0.6
0.8
corr
elat
ion
BLEU
ROUGE-1
ROUGE-2
ROUGE-SU4
METEOR
Figure 2: Correlations between human judge-
ments and automatic evaluation metrics for vari-
ous edit distances
ated by PBMT are less well formed than the orig-
inal source sentence.
There is an overall medium correlation between
the BLEU measure and human judgements (r =
0.41, p < 0.001). We see a lower correlation
between the various ROUGE scores and human
judgements, with ROUGE-1 showing the highest
correlation (r = 0.29, p < 0.001). Between the
two lies the METEOR correlation (r = 0.35, p <
0.001). However, if we split the data according to
Levenshtein distance, we observe that we gener-
ally get a higher correlation for all the tested met-
rics when the Levenshtein distance is higher, as
visualized in Figure 2. At Levenshtein distance 5,
the BLEU score achieves a correlation of 0.78 with
human judgements, while ROUGE-1 manages to
achieve a 0.74 correlation. Beyond edit distance
5, data sparsity occurs.
5 Discussion
In this paper we have shown that with an automat-
ically obtained parallel monolingual corpus with
several millions of paired examples, it is possi-
ble to develop an SPG system based on a PBMT
framework. Human judges preferred the output
of our PBMT system over the output of a word
substitution system. We have also addressed the
problem of automatic paraphrase evaluation. We
measured BLEU, METEOR and ROUGE scores,
and observed that these automatic scores corre-
late with human judgements to some degree, but
that the correlation is highly dependent on edit
distance. At low edit distances automatic metrics
fail to properly assess the quality of paraphrases,
whereas at edit distance 5 the correlation of BLEU
with human judgements is 0.78, indicating that at
higher edit distances these automatic measures can
be utilized to rate the quality of the generated para-
phrases. From edit distance 2, BLEU correlates
best with human judgements, indicating that MT
evaluation metrics might be best for SPG evalua-
tion.
The data we used for paraphrasing consists of
headlines. Paraphrase patterns we learn are those
used in headlines and therefore different from
standard language. The advantage of our approach
is that it paraphrases those parts of sentences that
it can paraphrase, and leaves the unknown parts
intact. It is straightforward to train a language
model on in-domain text and use the translation
model acquired from the headlines to generate
paraphrases for other domains. We are also inter-
ested in capturing paraphrase patterns from other
domains, but acquiring parallel corpora for these
domains is not trivial.
Instead of post-hoc dissimilarity reranking of
the candidate paraphrase sentences we intend to
develop a proper paraphrasing model that takes
dissimilarity into account in the decoding pro-
cess. In addition, we plan to investigate if our
paraphrase generation approach is applicable to
sentence compression and simplification. On the
topic of automatic evaluation, we aim to define
an automatic paraphrase generation assessment
score. A paraphrase evaluation measure should be
able to recognize that a good paraphrase is a well-
formed sentence in the source language, yet it is
clearly dissimilar to the source.
References
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An automatic metric for MT evaluation with im-
proved correlation with human judgments. In Pro-
ceedings of the ACL Workshop on Intrinsic and Ex-
trinsic Evaluation Measures for Machine Transla-
tion and/or Summarization, pages 65?72, June.
Regina Barzilay and Lillian Lee. 2003. Learn-
ing to paraphrase: an unsupervised approach using
multiple-sequence alignment. In NAACL ?03: Pro-
ceedings of the 2003 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics on Human Language Technology, pages
16?23.
Chris Callison-Burch, Philipp Koehn, and Miles Os-
borne. 2006. Improved statistical machine transla-
tion using paraphrases. In Proceedings of the main
conference on Human Language Technology Con-
ference of the North American Chapter of the Asso-
ciation of Computational Linguistics, pages 17?24.
Chris Callison-Burch, Trevor Cohn, and Mirella Lap-
ata. 2008. Parametric: an automatic evaluation met-
ric for paraphrasing. In COLING ?08: Proceedings
of the 22nd International Conference on Computa-
tional Linguistics, pages 97?104.
Walter Daelemans, Jakub Zavrel, Peter Berck, and
Steven Gillis. 1996. Mbt: A memory-based part of
speech tagger-generator. In Proc. of Fourth Work-
shop on Very Large Corpora, pages 14?27.
Walter Daelemans, Anja Hothker, and Erik Tjong
Kim Sang. 2004. Automatic sentence simplification
for subtitling in dutch and english. In Proceedings of
the 4th International Conference on Language Re-
sources and Evaluation, pages 1045?1048.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004.
Unsupervised construction of large paraphrase cor-
pora: exploiting massively parallel news sources. In
COLING ?04: Proceedings of the 20th international
conference on Computational Linguistics, page 350.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database, May.
David Kauchak and Regina Barzilay. 2006. Para-
phrasing for automatic evaluation. In Proceedings
of the Human Language Technology Conference of
the NAACL, Main Conference, pages 455?462, June.
Kevin Knight and Daniel Marcu. 2002. Summa-
rization beyond sentence extraction: a probabilis-
tic approach to sentence compression. Artif. Intell.,
139(1):91?107.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris C.
Burch, Marcello Federico, Nicola Bertoldi, Brooke
Cowan, Wade Shen, Christine Moran, Richard Zens,
Chris Dyer, Ondrej Bojar, Alexandra Constantin,
and Evan Herbst. 2007. Moses: Open source toolkit
for statistical machine translation. In ACL.
Dekang Lin and Patrick Pantel. 2001. Dirt: Discov-
ery of inference rules from text. In KDD ?01: Pro-
ceedings of the seventh ACM SIGKDD international
conference on Knowledge discovery and data min-
ing, pages 323?328.
Chin-Yew Lin. 2004. Rouge: A package for automatic
evaluation of summaries. In Proc. ACL workshop on
Text Summarization Branches Out, page 10.
Rani Nelken and Stuart M. Shieber. 2006. Towards ro-
bust context-sensitive sentence alignment for mono-
lingual corpora. In Proceedings of the 11th Confer-
ence of the European Chapter of the Association for
Computational Linguistics (EACL-06), 3?7 April.
Franz J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Comput. Linguist., 29(1):19?51, March.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In ACL ?02: Proceed-
ings of the 40th Annual Meeting on Association for
Computational Linguistics, pages 311?318.
Chris Quirk, Chris Brockett, and William Dolan.
2004. Monolingual machine translation for para-
phrase generation. In Dekang Lin and Dekai Wu,
editors, Proceedings of EMNLP 2004, pages 142?
149, July.
Stefan Riezler, Alexander Vasserman, Ioannis
Tsochantaridis, Vibhu O. Mittal, and Yi Liu. 2007.
Statistical machine translation for query expansion
in answer retrieval. In ACL.
Grazia Russo-Lassner, Jimmy Lin, and Philip Resnik.
2006. A paraphrase-based approach to machine
translation evaluation. Technical report, University
of Maryland, College Park.
Andreas Stolcke. 2002. Srilm - an extensible language
modeling toolkit. In In Proc. Int. Conf. on Spoken
Language Processing, pages 901?904.
Sander Wubben, Antal van den Bosch, Emiel Krahmer,
and Erwin Marsi. 2009. Clustering and matching
headlines for automatic paraphrase acquisition. In
ENLG ?09: Proceedings of the 12th European Work-
shop on Natural Language Generation, pages 122?
125.
Liang Zhou, Chin-Yew Lin, and Eduard Hovy. 2006.
Re-evaluating machine translation results with para-
phrase support. In Proceedings of the 2006 Con-
ference on Empirical Methods in Natural Language
Processing, pages 77?84, July.
Workshop on Monolingual Text-To-Text Generation, pages 27?33,
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 27?33,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
Comparing Phrase-based and Syntax-based Paraphrase Generation
Sander Wubben
Tilburg University
P.O. Box 90135
5000 LE Tilburg
The Netherlands
s.wubben@uvt.nl
Erwin Marsi
NTNU
Sem Saelandsvei 7-9
NO-7491 Trondheim
Norway
emarsi@idi.ntnu.no
Antal van den Bosch
Tilburg University
P.O. Box 90135
5000 LE Tilburg
The Netherlands
antal.vdnbosch@uvt.nl
Emiel Krahmer
Tilburg University
P.O. Box 90135
5000 LE Tilburg
The Netherlands
e.j.krahmer@uvt.nl
Abstract
Paraphrase generation can be regarded as ma-
chine translation where source and target lan-
guage are the same. We use the Moses statisti-
cal machine translation toolkit for paraphras-
ing, comparing phrase-based to syntax-based
approaches. Data is derived from a recently
released, large scale (2.1M tokens) paraphrase
corpus for Dutch. Preliminary results indicate
that the phrase-based approach performs bet-
ter in terms of NIST scores and produces para-
phrases at a greater distance from the source.
1 Introduction
One of the challenging properties of natural lan-
guage is that the same semantic content can typically
be expressed by many different surface forms. As
the ability to deal with paraphrases holds great po-
tential for improving the coverage of NLP systems,
a substantial body of research addressing recogni-
tion, extraction and generation of paraphrases has
emerged (Androutsopoulos and Malakasiotis, 2010;
Madnani and Dorr, 2010). Paraphrase Generation
can be regarded as a translation task in which source
and target language are the same. Both Paraphrase
Generation and Machine Translation (MT) are in-
stances of Text-To-Text Generation, which involves
transforming one text into another, obeying certain
restrictions. Here these restrictions are that the gen-
erated text must be grammatically well-formed and
semantically/translationally equivalent to the source
text. Addionally Paraphrase Generation requires
that the output should differ from the input to a cer-
tain degree.
The similarity between Paraphrase Generation
and MT suggests that methods and tools originally
developed for MT could be exploited for Paraphrase
Generation. One popular approach ? arguably the
most successful so far ? is Statistical Phrase-based
Machine Translation (PBMT), which learns phrase
translation rules from aligned bilingual text corpora
(Och et al, 1999; Vogel et al, 2000; Zens et al,
2002; Koehn et al, 2003). Prior work has explored
the use of PBMT for paraphrase generation (Quirk et
al., 2004; Bannard and Callison-Burch, 2005; Mad-
nani et al, 2007; Callison-Burch, 2008; Zhao et al,
2009; Wubben et al, 2010)
However, since many researchers believe that
PBMT has reached a performance ceiling, ongo-
ing research looks into more structural approaches
to statistical MT (Marcu and Wong, 2002; Och and
Ney, 2004; Khalilov and Fonollosa, 2009). Syntax-
based MT attempts to extract translation rules in
terms of syntactic constituents or subtrees rather
than arbitrary phrases, presupposing syntactic struc-
tures for source, target or both languages. Syntactic
information might lead to better results in the area
of grammatical well-formedness, and unlike phrase-
based MT that uses contiguous n-grams, syntax en-
ables the modeling of long-distance translation pat-
terns.
While the verdict on whether or not this approach
leads to any significant performance gain is still
out, a similar line of reasoning would suggest that
syntax-based paraphrasing may offer similar advan-
tages over phrase-based paraphrasing. Considering
the fact that the success of PBMT can partly be at-
tributed to the abundance of large parallel corpora,
27
and that sufficiently large parallel corpora are still
lacking for paraphrase generation, using more lin-
guistically motivated methods might prove benefi-
cial for paraphrase generation. At the same time,
automatic syntactic analysis introduces errors in the
parse trees, as no syntactic parser is perfect. Like-
wise, automatic alignment of syntactic phrases may
be prone to errors.
The main contribution of this paper is a systematic
comparison between phrase-based and syntax-based
paraphrase generation using an off-the-shelf statis-
tical machine translation (SMT) decoder, namely
Moses (Koehn et al, 2007) and the word-alignment
tool GIZA++ (Och and Ney, 2003). Training data
derives from a new, large scale (2.1M tokens) para-
phrase corpus for Dutch, which has been recently
released.
The paper is organized as follows. Section 2 re-
views the paraphrase corpus from which provides
training and test data. Next, Section 3 describes the
paraphrase generation methods and the experimen-
tal setup. Results are presented in Section 4. In
Section 5 we discuss our findings and formulate our
conclusions.
2 Corpus
The main bottleneck in building SMT systems is
the need for a substantial amount of parallel aligned
text. Likewise, exploiting SMT for paraphrasing re-
quires large amounts of monolingual parallel text.
However, paraphrase corpora are scarce; the situa-
tion is more dire than in MT, and this has caused
some studies to focus on the automatic harvesting
of paraphrase corpora. The use of monolingual par-
allel text corpora was first suggested by Barzilay
and McKeown (2001), who built their corpus us-
ing various alternative human-produced translations
of literary texts and then applied machine learn-
ing or multi-sequence alignment for extracting para-
phrases. In a similar vein, Pang et al (2003) used a
corpus of alternative English translations of Chinese
news stories in combination with a syntax-based al-
gorithm that automatically builds word lattices, in
which paraphrases can be identified.
So-called comparable monolingual corpora, for
instance independently written news reports describ-
ing the same event, in which some pairs of sentences
exhibit partial semantic overlap have also been in-
vestigated (Shinyama et al, 2002; Barzilay and Lee,
2003; Shen et al, 2006; Wubben et al, 2009)
The first manually collected paraphrase corpus is
the Microsoft Research Paraphrase (MSRP) Corpus
(Dolan et al, 2004), consisting of 5,801 sentence
pairs, sampled from a larger corpus of news arti-
cles. However, it is rather small and contains no sub-
sentential allignments. Cohn et al (2008) developed
a parallel monolingual corpus of 900 sentence pairs
annotated at the word and phrase level. However, all
of these corpora are small from an SMT perspective.
Recently a new large-scale paraphrase corpus for
Dutch, the DAESO corpus, was released. The cor-
pus contains both samples of parallel and compa-
rable text in which similar sentences, phrases and
words are aligned. One part of the corpus is manu-
ally aligned, whereas another part is automatically
aligned using a data-driven aligner trained on the
first part. The DAESO corpus is extensively de-
scribed in (Marsi and Krahmer, 2011); the summary
here is limited to aspects relevant to the work at
hand.
The corpus contains the following types of text:
(1) alternative translations in Dutch of three liter-
ary works of fiction; (2) autocue text from televion
broadcast news as read by the news reader, and the
corresponding subtitles; (3) headlines from similar
news articles obtained from Google News Dutch;
(4) press releases about the same news topic from
two different press agencies; (5) similar answers re-
trieved from a document collection in the medical
domain, originally created for evaluating question-
answering systems.
In a first step, similar sentences were automati-
cally aligned, after which alignments were manu-
ally corrected. In the case of the parallel book texts,
aligned sentences are (approximate) paraphrases. To
a lesser degree, this is also true for the news head-
lines. The autocue-subtitle pairs are mostly exam-
ples of sentence compression, as the subtitle tends
to be a compressed version of the read autocue text.
In contrast, the press releases and the QA answers,
are characterized by a great deal of one-to-many
sentence alignments, as well as sentences left un-
aligned, as is to be expected in comparable text.
Most sentences in these types of text tend to have
only partial overlap in meaning.
28
Table 1: Properties of the manually aligned corpus
Autosub Books Headlines News QA Overall
aligned trees 18 338 6 362 32 627 11 052 118 68 497
tokens 217 959 115 893 179 629 162 361 2 230 678 072
tokens/sent 11.89 18.22 5.51 14.69 18.90 9.90
nodes 365 157 191 636 318 399 271 192 3734 1 150 118
nodes/tree 19.91 30.12 9.76 24.54 31.64 16.79
uniquely aligned trees (%) 92.93 92.49 84.57 63.61 50.00 84.10
aligned nodes (%) 73.53 66.83 73.58 53.62 38.62 67.62
Next, aligned sentences were tokenized and
parsed with the Alpino parser for Dutch (Bouma et
al., 2001). The parser provides a relatively theory-
neutral syntactic analysis which is a blend of phrase
structure analysis and dependency analysis, with a
backbone of phrasal constituents and arcs labeled
with syntactic function/dependency labels.
The alignments not only concern paraphrases in
the strict sense, i.e., expressions that are semanti-
cally equivalent, but extend to expressions that are
semantically similar in less strict ways, for instance,
where one phrase is either more specific or more
general than the related phrase. For this reason,
alignments are also labeled according to a limited
set of semantic similarity relations. Since these rela-
tions were not used in the current study, we will not
discuss them further here.
The corpus comprises over 2.1 million tokens,
678 thousand of which are manually annotated and
1,511 thousand are automatically processed.
To give a more complete overview of the sizes
of different corpus segments, some properties of the
manually aligned corpus are listed in Table 1. Prop-
erties of the automatically aligned part are similar,
except for the fact that it only contains text of the
news and QA type.
3 Paraphrase generation
Phrase-based MT models consider translation as a
mapping of small text chunks, with possible re-
ordering (Och and Ney, 2004). Operations such as
insertion, deletion and many-to-one, one-to-many
or many-to-many translation are all covered in the
structure of the phrase table. Phrase-based models
have been used most prominently in the past decade,
as they have shown to outperform other approaches
(Callison-Burch et al, 2009).
One issue with the phrase-based approach is that
recursion is not handled explicitly. It is gener-
ally acknowledged that language contains recursive
structures up to certain depths. So-called hierarchi-
cal models have introduced the inclusion of non-
terminals in the mapping rules, to allow for recur-
sion (Chiang et al, 2005). However, using a generic
non-terminal X can introduce many substitutions
in translations that do not make sense. By mak-
ing the non-terminals explicit, using syntactic cat-
egories such as NP s and V P s, this phenomenon
is constrained, resulting in syntax-based translation.
Instead of phrase translations, translation rules in
terms of syntactic constituents or subtrees are ex-
tracted, presupposing the availability of syntactic
structures for source, target, or both languages.
Incorporating syntax can guide the translation
process and unlike phrase-based MT syntax it en-
ables the modeling of long-distance translation pat-
terns. Syntax-based systems may parse the data on
the target side (string-to-tree), source side (tree-to-
string), or both (tree-to-tree).
In our experiments we use tree-to-tree syntax-
based MT. We also experiment with relaxing the
parses by a method proposed under the label of
syntax-augmented machine translation (SAMT), de-
scribed in (Zollmann and Venugopal, 2006). This
method combines any neighboring nodes and labels
previously unlabeled nodes, removing the syntactic
constraint on the grammar1.
We train all systems on the DAESO data (218,102
lines of aligned sentences) and test on a held-out
set consisting of manually aligned headlines that ap-
1This method is implemented in the Moses package in the
program relax-parse as option SAMT 4
29
Table 2: Examples of output of the phrase-based and syntax-based systems
Source jongen ( 7 ) zwaargewond na aanrijding boy (7) severely-injured after crash
Phrase-based 7-jarige gewond na botsing 7-year-old injured after collision
Syntax-based jongen ( 7 ) zwaar gewond na aanrijding boy (7) severely injured after crash
Source jeugdwerkloosheid daalt vooral bij voldoende opleiding youth-unemployment drops especially with adequate training
Phrase-based werkloosheid jongeren daalt , vooral bij voldoende studie unemployment youths drops, especially with sufficient study
Syntax-based * jeugdwerkloosheid daalt vooral in voldoende opleiding youth-unemployment drops especially in adequate training
Source kritiek op boetebeleid ns criticism of fining-policy ns
Phrase-based * kritiek op de omstreden boetebeleid en criticism of the controversial and
Syntax-based kritiek op omstreden boetebeleid nederlandse spoorwegen criticism of controversial fining-policy dutch railways
Source weer bestuurders radboud weg again directors radboud [hospital] leaving
Phrase-based * weer de weg ziekenhuis again the leaving hospital
Syntax-based alweer bestuurders ziekenhuis weg yet-again directors hospital leaving
peared in May 2006.2 We test on 773 headlines that
have three or more aligned paraphrasing reference
headlines. We use an SRILM (Stolcke, 2002) lan-
guage model trained on the Twente news corpus3.
To investigate the effect of the amount of training
data on results, we also train a phrase-based model
on more data by adding more aligned headlines orig-
inating from data crawled in 2010 and aligned using
tf.idf scores over headline clusters and Cosine sim-
ilarity as described in (Wubben et al, 2009), result-
ing in an extra 612,158 aligned headlines.
Evaluation is based on the assumption that a good
paraphrase is well-formed and semantically similar
but structurally different from the source sentence.
We therefore score the generated paraphrases not
only by an MT metric (we use NIST scores), but
also factor in the edit distance between the input
sentence and the output sentence. We take the 10-
best generated paraphrases and select from these the
one most dissimilar from the source sentence in term
of Levenshtein distance on tokens. We then weigh
NIST scores according to their corresponding sen-
tence Levenshtein Distance, to calculate a weighted
2Syntactic trees were converted to the XML format used by
Moses for syntax-based MT. A minor complication is that the
word order in the tree is different from the word order in the
corresponding sentence in about half of the cases. The technical
reason is that Alpino internally produces dependency structures
that can be non-projective. Conversion to a phrase structure tree
therefore necessitates moving some words to a different posi-
tion in the tree. We performed a subsequent reordering of the
trees, moving terminals to make the word order match the sur-
face word order.
3http://www.vf.utwente.nl/?druid/TwNC/
TwNC-main.html
average score. This implies that we penalize sys-
tems that provide output at Levenshtein distance 0,
which are essentially copies of the input, and not
paraphrases. Formally, the score is computed as fol-
lows:
NISTweightedLD = ?
?
i=LD(1..8)
(i ?Ni ?NISTi)
?
i=LD(1..8)
(i ?Ni)
where ? is the percentage of output phrases that have
a sentence Levenshtein Distance higher than 0. In-
stead of NIST scores, other MT evaluation scores
can be plugged into this formula, such as METEOR
(Lavie and Agarwal, 2007) for languages for which
paraphrase data is available.
4 Results
Figure 1 shows NIST scores per Levenshtein Dis-
tance. It can be observed that overall the NIST score
decreases as the distance to the input increases, indi-
cating that more distant paraphrases are of less qual-
ity. The relaxed syntax-based approach (SAMT)
performs mildly better than the standard syntax-
based approach, but performs worse than the phrase-
based approach. The distribution of generated para-
phrases per Levenshtein Distance is shown in Fig-
ure 2. It reveals that the Syntax-based approaches
tend to stay closer to the source than the phrase-
based approaches.
In Table 2 a few examples of output from both
Phrase- and Syntax-based systems are given. The
30
2 4 6 8 10
2
4
6
8
10
LevenshteinDistance
N
IS
T
sc
or
e
Phrase
Phrase extra data
Syntax
Syntax relaxed
Figure 1: NIST scores per Levenshtein distance
top two examples show sentences where the phrase-
based approach scores better, and the bottom two
show examples where the syntax-based approach
scores better. In general, we observe that the
phrase-based approach is often more drastic with its
changes, as shown also in Figure 2. The syntax-
based approach is less risky, and reverts more to
single-word substitution.
The weighted NIST score for the phrase-based
approach is 7.14 versus 6.75 for the syntax-based
approach. Adding extra data does not improve the
phrase-based approach, as it yields a score of 6.47,
but the relaxed method does improve the syntax-
based approach (7.04).
5 Discussion and conclusion
We have compared a phrase-based MT approach
to paraphrasing with a syntax-based MT approach.
The Phrase-based approach performs better in terms
of NIST score weighted by edit distance of the out-
put. In general, the phrase-based MT system per-
forms more edits and these edits seem to be more
reliable than the edits done by the Syntax-based ap-
proach. A relaxed Syntax-based approach performs
better, while adding more data to the Phrase-based
approach does not yield better results. To gain a bet-
ter understanding of the quality of the output gener-
ated by the different approaches, it would be desir-
able to present the output of the different systems to
human judges. In future work, we intend to com-
pare the effects of using manual word alignments
from the DAESO corpus instead of the automatic
alignments produced by GIZA++. We also wish to
0 2 4 6 8 10
0
100
200
300
LevenshteinDistance
N
Phrase
Phrase extra data
Syntax
Syntax relaxed
Figure 2: Distribution of generated paraphrases per Lev-
enshtein distance
further explore the effect of the nature of the data
that we train on: the DAESO corpus consists of var-
ious data sources from different domains. Our aim
is also to incorporate the notion of dissimilarity into
the paraphrase model, by adding dissimilarity scores
to the model.
31
References
Ion Androutsopoulos and Prodromos Malakasiotis.
2010. A survey of paraphrasing and textual entailment
methods. Journal of Artificial Intelligence Research,
38:135?187, May.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In ACL ?05:
Proceedings of the 43rd Annual Meeting on Associ-
ation for Computational Linguistics, pages 597?604,
Morristown, NJ, USA. Association for Computational
Linguistics.
Regina Barzilay and Lillian Lee. 2003. Learning to
paraphrase: an unsupervised approach using multiple-
sequence alignment. In NAACL ?03: Proceedings of
the 2003 Conference of the North American Chapter of
the Association for Computational Linguistics on Hu-
man Language Technology, pages 16?23, Morristown,
NJ, USA. Association for Computational Linguistics.
Regina Barzilay and Kathleen McKeown. 2001. Extract-
ing paraphrases from a parallel corpus. In Proceedings
of Meeting of the Association for Computational Lin-
guistics, pages 50?57, Toulouse, France.
Gosse Bouma, Gertjan van Noord, and Robert Malouf.
2001. Alpino: Wide-coverage computational analy-
sis of Dutch. In Walter Daelemans, Khalil Sima?an,
Jorn Veenstra, and Jakub Zavre, editors, Computa-
tional Linguistics in the Netherlands 2000., pages 45?
59. Rodopi, Amsterdam, New York.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009
Workshop on Statistical Machine Translation. In
Proceedings of the Fourth Workshop on Statistical
Machine Translation, pages 1?28, Athens, Greece,
March. Association for Computational Linguistics.
Chris Callison-Burch. 2008. Syntactic constraints
on paraphrases extracted from parallel corpora. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP ?08,
pages 196?205, Stroudsburg, PA, USA. Association
for Computational Linguistics.
David Chiang, Adam Lopez, Nitin Madnani, Christof
Monz, Philip Resnik, and Michael Subotin. 2005. The
hiero machine translation system: extensions, evalua-
tion, and analysis. In Proceedings of the conference on
Human Language Technology and Empirical Methods
in Natural Language Processing, HLT ?05, pages 779?
786, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Trevor Cohn, Chris Callison-Burch, and Mirella Lapata.
2008. Constructing corpora for the development and
evaluation of paraphrase systems. Computational Lin-
guistics, 34(4):597?614.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004.
Unsupervised construction of large paraphrase cor-
pora: Exploiting massively parallel news sources. In
Proceedings of the 20th International Conference on
Computational Linguistics, pages 350?356, Morris-
town, NJ, USA.
Maxim Khalilov and Jose? A. R. Fonollosa. 2009. N-
gram-based statistical machine translation versus syn-
tax augmented machine translation: comparison and
system combination. In Proceedings of the 12th Con-
ference of the European Chapter of the Association
for Computational Linguistics, EACL ?09, pages 424?
432, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Philip Koehn, Franz Josef Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings of
the 2003 Conference of the North American Chapter
of the Association for Computational Linguistics on
Human Language Technology-Volume 1, pages 48?54.
Association for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris C.
Burch, Marcello Federico, Nicola Bertoldi, Brooke
Cowan, Wade Shen, Christine Moran, Richard Zens,
Chris Dyer, Ondrej Bojar, Alexandra Constantin, and
Evan Herbst. 2007. Moses: Open source toolkit for
statistical machine translation. In ACL. The Associa-
tion for Computer Linguistics.
Alon Lavie and Abhaya Agarwal. 2007. Meteor: an au-
tomatic metric for mt evaluation with high levels of
correlation with human judgments. In Proceedings
of the Second Workshop on Statistical Machine Trans-
lation, StatMT ?07, pages 228?231, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Nitin Madnani and Bonnie J. Dorr. 2010. Gener-
ating phrasal and sentential paraphrases: A survey
of data-driven methods. Computational Linguistics,
36(3):341?387.
Nitin Madnani, Necip Fazil Ayan, Philip Resnik, and
Bonnie J. Dorr. 2007. Using paraphrases for pa-
rameter tuning in statistical machine translation. In
Proceedings of the Second Workshop on Statistical
Machine Translation, StatMT ?07, pages 120?127,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Daniel Marcu and William Wong. 2002. A phrase-based,
joint probability model for statistical machine trans-
lation. In Proceedings of the ACL-02 conference on
Empirical methods in natural language processing -
Volume 10, EMNLP ?02, pages 133?139, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Erwin Marsi and Emiel Krahmer. 2011. Construction of
an aligned monolingual treebank for studying seman-
tic similarity. (submitted for publication).
32
Franz J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Comput. Linguist., 29(1):19?51, March.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Comput. Linguist., 30:417?449, December.
Franz J. Och, Christoph Tillmann, and Hermann Ney.
1999. Improved alignment models for Statistical Ma-
chine Translation. In Proceedings of the Joint Work-
shop on Empirical Methods in NLP and Very Large
Corpora, pages 20?28, Maryland, USA.
Bo Pang, Kevin Knight, and Daniel Marcu. 2003.
Syntax-based alignment of multiple translations: Ex-
tracting paraphrases and generating new sentences. In
HLT-NAACL.
Chris Quirk, Chris Brockett, and William Dolan. 2004.
Monolingual machine translation for paraphrase gen-
eration. In Dekang Lin and Dekai Wu, editors, Pro-
ceedings of EMNLP 2004, pages 142?149, Barcelona,
Spain, July. Association for Computational Linguis-
tics.
Siwei Shen, Dragomir R. Radev, Agam Patel, and Gu?nes?
Erkan. 2006. Adding syntax to dynamic program-
ming for aligning comparable texts for the generation
of paraphrases. In Proceedings of the COLING/ACL
2006 Main Conference Poster Sessions, pages 747?
754, Sydney, Australia, July. Association for Compu-
tational Linguistics.
Yusuke Shinyama, Satoshi Sekine, Kiyoshi Sudo, and
Ralph Grishman. 2002. Automatic paraphrase acqui-
sition from news articles. In Proceedings of Human
Language Technology Conference (HLT 2002), pages
313?318, San Diego, USA.
Andreas Stolcke. 2002. SRILM - An Extensible Lan-
guage Modeling Toolkit. In In Proc. Int. Conf. on
Spoken Language Processing, pages 901?904, Denver,
Colorado.
S. Vogel, Franz Josef Och, and Hermann Ney. 2000. The
statistical translation module in the verbmobil system.
In KONVENS 2000 / Sprachkommunikation, Vortrge
der gemeinsamen Veranstaltung 5. Konferenz zur Ve-
rarbeitung natrlicher Sprache (KONVENS), 6. ITG-
Fachtagung ?Sprachkommunikation?, pages 291?293,
Berlin, Germany, Germany. VDE-Verlag GmbH.
Sander Wubben, Antal van den Bosch, Emiel Krahmer,
and Erwin Marsi. 2009. Clustering and matching
headlines for automatic paraphrase acquisition. In
E. Krahmer and M. Theune, editors, The 12th Eu-
ropean Workshop on Natural Language Generation,
pages 122?125, Athens. Association for Computa-
tional Linguistics.
Sander Wubben, Antal van den Bosch, and Emiel Krah-
mer. 2010. Paraphrase generation as monolingual
translation: Data and evaluation. In B. Mac Namee
J. Kelleher and I. van der Sluis, editors, Proceedings of
the 10th International Workshop on Natural Language
Generation (INLG 2010), pages 203?207, Dublin.
Richard Zens, Franz Josef Och, and Hermann Ney. 2002.
Phrase-based statistical machine translation. In Pro-
ceedings of the 25th Annual German Conference on
AI: Advances in Artificial Intelligence, KI ?02, pages
18?32, London, UK. Springer-Verlag.
Shiqi Zhao, Xiang Lan, Ting Liu, and Sheng Li. 2009.
Application-driven statistical paraphrase generation.
In Proceedings of the Joint Conference of the 47th
Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Process-
ing of the AFNLP: Volume 2 - Volume 2, ACL ?09,
pages 834?842, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proceedings of the Workshop on Statistical Machine
Translation, StatMT ?06, pages 138?141, Stroudsburg,
PA, USA. Association for Computational Linguistics.
33
INLG 2012 Proceedings of the 7th International Natural Language Generation Conference, pages 3?11,
Utica, May 2012. c?2012 Association for Computational Linguistics
Learning Preferences for Referring Expression Generation:
Effects of Domain, Language and Algorithm
Ruud Koolen
Tilburg University
P.O. Box 90135
5000 LE Tilburg
The Netherlands
r.m.f.koolen@uvt.nl
Emiel Krahmer
Tilburg University
P.O. Box 90135
5000 LE Tilburg
The Netherlands
e.j.krahmer@uvt.nl
Marie?t Theune
University of Twente
P.O. Box 217
7500 AE Enschede
The Netherlands
m.theune@utwente.nl
Abstract
One important subtask of Referring Expres-
sion Generation (REG) algorithms is to se-
lect the attributes in a definite description for
a given object. In this paper, we study how
much training data is required for algorithms
to do this properly. We compare two REG al-
gorithms in terms of their performance: the
classic Incremental Algorithm and the more
recent Graph algorithm. Both rely on a notion
of preferred attributes that can be learned from
human descriptions. In our experiments, pref-
erences are learned from training sets that vary
in size, in two domains and languages. The
results show that depending on the algorithm
and the complexity of the domain, training on
a handful of descriptions can already lead to a
performance that is not significantly different
from training on a much larger data set.
1 Introduction
Most practical NLG systems include a dedicated
module for Referring Expression Generation (REG)
in one form or another (Mellish et al, 2006). One
central problem a REG module needs to address is
deciding on the contents of a description. Jordan
and Walker (2005), for example, studied human-
produced descriptions in a furniture scenario, and
found that speakers can refer to a target in many dif-
ferent ways (?the yellow rug?, ?the $150 rug?, etc.).
The question, then, is how speakers decide which at-
tributes to include in a description, and how this de-
cision process can be modeled in a REG algorithm.
When we focus on the generation of distinguish-
ing descriptions (which is often done in REG), it is
usually assumed that some attributes are more pre-
ferred than others: when trying to identify a chair,
for example, its colour is probably more helpful than
its size. It is precisely this intuition of preferred at-
tributes which is incorporated in the Incremental Al-
gorithm (Dale and Reiter, 1995), arguably one of the
most influential REG algorithms to date. The Incre-
mental Algorithm (IA) assumes the existence of a
complete, ordered list of preferred attributes. The
algorithm basically iterates through this list, adding
an attribute (e.g., COLOUR) to the description under
construction if its value (e.g., yellow) helps ruling
out one or more of the remaining distractors.
Even though the IA is exceptional in that it re-
lies on a complete ordering of attributes, most cur-
rent REG algorithms make use of preferences in
some way (Fabbrizio et al, 2008; Gerva?s et al,
2008; Kelleher, 2007; Spanger et al, 2008; Viethen
and Dale, 2010). The graph-based REG algorithm
(Krahmer et al, 2003), for example, models prefer-
ences in terms of costs, where cheaper is more pre-
ferred. Contrary to the IA, the graph-based algo-
rithm assumes that preferences operate at the level
of attribute-value pairs (or properties) rather than at
the level of attributes; in this way it becomes pos-
sible to prefer a straightforward size (large) over a
subtle colour (mauve, taupe). Moreover, the graph-
based algorithm looks for the cheapest overall de-
scription, and may opt for a description with a sin-
gle, relatively dispreferred property (?the man with
the blue eyes?) when the alternative would be to
combine many, relatively preferred properties (?the
large, balding man with the bow tie and the striped
tuxedo?). This flexibility is arguably one of the
3
reasons why the graph-based REG approach works
well: it was the best performing system in the most
recent REG Challenge (Gatt et al, 2009).
But where do the preferences used in the algo-
rithms come from? Dale and Reiter point out that
preferences are domain dependent, and that deter-
mining them for a given domain is essentially an
empirical question. Unfortunately, they do not spec-
ify how this particular empirical question should be
answered. The general preference for colour over
size is experimentally well-established (Pechmann,
1989), but for most other cases experimental data
are not readily available. An alternative would be
to look at human data, preferably in a ?semantically
transparent? corpus (van Deemter et al, 2006), that
is: a corpus that contains the attributes and values of
all domain objects, together with the attribute-value
pairs actually included in a target reference. Such
corpora are typically collected using human partic-
ipants, who are asked to produce referring expres-
sions for targets in controlled visual scenes. One
example is the TUNA corpus, which is a publicly
available data set containing 2280 human-produced
descriptions in total, and which formed the basis of
various REG Challenges. Clearly, building a corpus
such as TUNA is a time consuming and labour in-
tensive exercise, so it will not be surprising that only
a handful of such corpora exists (and often only for
English).
This raises an important question: how many
human-produced references are needed to make a
good estimate of which attributes and properties are
preferred? Do we really need hundreds of instances,
or is it conceivable that a few of them (collected in a
semantically transparent way) will do? This is not an
easy matter, since various factors might play a role:
from which data set are example references sampled,
what are the domains of interest, and, perhaps most
importantly, which REG algorithm is considered? In
this paper, we address these questions by systemati-
cally training two REG algorithms (the Incremental
Algorithm and the graph-based REG algorithm) on
sets of human-produced descriptions of increasing
size and evaluating them on a held-out test set; we
do this for two different domains (people and furni-
ture descriptions) and two data sets in two different
languages (TUNA and D-TUNA, the Dutch version
of TUNA).
That size of the training set may have an impact
on the performance of a REG algorithm was already
suggested by Theune et al (2011), who used the En-
glish TUNA corpus to determine preferences (costs)
for the graph-based algorithm using a similar learn-
ing curve set-up as we use here. However, the cur-
rent paper expands on Theune et al (2011) in three
major ways. Firstly, and most importantly, where
Theune et al reported results for only one algorithm
(the graph-based one), we directly compare the per-
formance of the graph-based algorithm and the In-
cremental Algorithm (something which, somewhat
surprisingly, has not been done before). Secondly,
we test whether these algorithms perform differently
in two different languages (English and Dutch), and
thirdly, we use eight training set sizes, which is more
than the six set sizes that were used by Theune et al
Below we first explain in more detail which algo-
rithms (Section 2) and corpora (Section 3) we used
for our experiments. Then we describe how we de-
rived costs and orders from subsets of these corpora
(Section 4), and report the results of our experiments
focusing on effects of domain, language and size
of the training set (Section 5). We end with a dis-
cussion and conclusion (Section 6), where we also
compare the performance of the IA trained on small
set sizes with that of the classical Full Brevity and
Greedy algorithms (Dale and Reiter, 1995).
2 The Algorithms
In this section we briefly describe the two algo-
rithms, and their settings, used in our experiment.
For details about these algorithms we refer to the
original publications.
The Incremental Algorithm (IA) The basic
assumption underlying the Incremental Algorithm
(Dale and Reiter, 1995) is that speakers ?prefer?
certain attributes over others when referring to
objects. This intuition is formalized in the notion
of a list of attributes, ranked in order of preference.
When generating a description for a target, the al-
gorithm iterates through this list, adding an attribute
to the description under construction if its value
helps rule out any of the distractors not previously
ruled out. There is no backtracking in the IA, which
means that a selected attribute is always realized in
4
the final description, even if the inclusion of later
attributes renders it redundant. In this way, the IA is
capable of generating overspecified descriptions, in
accordance with the human tendency to mention re-
dundant information (Pechmann, 1989; Engelhardt
et al, 2006; Arts et al, 2011). The TYPE attribute
(typically realized as the head noun) has a special
status in the IA. After running the algorithm it is
checked whether TYPE is in the description; if not,
it is added, so that TYPE is always included even if
it does not rule out any distractors.
To derive preference orders from human-
produced descriptions we proceeded as follows:
given a set of n descriptions sampled from a
larger corpus (where n is the set size, a variable
we systematically control in our experiment), we
counted the number of times a certain attribute
occurred in the n descriptions. The most frequently
occurring attribute was placed at the first position of
the preferred attributes list, followed by the second
most frequent attribute, etc. In the case of a tie (i.e.,
when two attributes occurred equally often, which
typically is more likely to happen in small training
sets), the attributes were ordered alphabetically. In
this way, we made sure that all ties were treated in
the same, comparable manner, which resulted in a
complete ranking of attributes, as required by the IA.
The Graph-based Algorithm (Graph) In the
graph-based algorithm (Krahmer et al, 2003),
which we refer to as Graph, information about
domain objects is represented as a labelled directed
graph, and REG is modeled as a graph-search
problem. The output of the algorithm is the
cheapest distinguishing subgraph, given a particular
cost function assigning costs to properties (i.e.,
attribute-value pairs). By assigning zero costs to
some properties Graph is also capable of generating
overspecified descriptions, including redundant
properties. To ensure that the graph search does not
terminate before the free properties are added, the
search order must be explicitly controlled (Viethen
et al, 2008). To ensure a fair comparison with the
IA, we make sure that if the target?s TYPE property
was not originally selected by the algorithm, it is
added afterwards.
In this study, both the costs and orders required
by Graph are derived from corpus data. We base
the property order on the frequency with which each
attribute-value pair is mentioned in a training cor-
pus, relative to the number of target objects with
this property. The properties are then listed in or-
der of decreasing frequency. Costs can be derived
from the same corpus frequencies; here, following
Theune et al (2011), we adopt a systematic way of
deriving costs from frequencies based on k-means
clustering. Theune and colleagues achieved the best
performance with k = 2, meaning that the prop-
erties are divided in two groups based on their fre-
quency. The properties in the group with the high-
est frequency get cost 0. These ?free? properties are
always included in the description if they help dis-
tinguish the target. The properties in the less fre-
quent group get cost 1; of these properties, the al-
gorithm only adds the minimum number necessary
to achieve a distinguishing description. Ties due to
properties occurring with the same frequency need
not be resolved when determining the cost function,
since Graph does not assume the existence of a com-
plete ordering. Properties that did not occur in a
training corpus were automatically assigned cost 1.
Like we did for the IA, we listed attribute-value pairs
with the same frequency in alphabetical order.
3 Corpora
Training and test data for our experiment were
taken from two corpora of referring expressions,
one English (TUNA) and one Dutch (D-TUNA).
TUNA The TUNA corpus (Gatt et al, 2007)
is a semantically transparent corpus consisting of
object descriptions in two domains (furniture and
people). The corpus was collected in an on-line
production experiment, in which participants were
presented with visual scenes containing one target
object and six distractor objects. These objects were
ordered in a 5 ? 3 grid, and the participants were
asked to describe the target in such a way that it
could be uniquely distinguished from its distractors.
Table 1 shows the attributes and values that were
annotated for the descriptions in the two domains.
There were two experimental conditions: in
the +LOC condition, the participants were free
to describe the target using any of its properties,
including its location on the screen (represented
5
Furniture
Attribute Possible values
TYPE chair, desk, sofa, fan
COLOUR green, red, blue, gray
ORIENTATION front, back, left, right
SIZE large, small
X-DIMENSION 1, 2, 3, 4, 5
Y-DIMENSION 1, 2, 3
People
Attribute Possible values
TYPE person
AGE old, young
HAIRCOLOUR light, dark
ORIENTATION front, left, right
HASBEARD true, false
HASGLASSES true, false
HASSHIRT true, false
HASSUIT true, false
HASTIE true, false
X-DIMENSION 1, 2, 3, 4, 5
Y-DIMENSION 1, 2, 3
Table 1: Attributes and values in the furniture and people
domains. X- and Y-DIMENSION refer to an object?s hori-
zontal and vertical position in a scene grid and only occur
in the English TUNA corpus.
in Table 1 as the X- and Y-DIMENSION), whereas
in the -LOC condition they were discouraged (but
not prevented) from mentioning object locations.
However, some descriptions in the -LOC condition
contained location information anyway.
D-TUNA For Dutch, we used the D-TUNA
corpus (Koolen and Krahmer, 2010). This corpus
uses the same visual scenes and annotation scheme
as the TUNA corpus, but consists of Dutch instead
of English target descriptions. Since the D-TUNA
experiment was performed in laboratory conditions,
its data is relatively ?cleaner? than the TUNA data,
which means that it contains fewer descriptions that
are not fully distinguishing and that its descriptions
do not contain X- and Y-DIMENSION attributes. Al-
though the descriptions in D-TUNA were collected
in three different conditions (written, spoken, and
face-to-face), we only use the written descriptions
in this paper, as this condition is most similar to the
data collection in TUNA.
4 Method
To find out how much training data is required
to achieve an acceptable attribute selection perfor-
mance for the IA and Graph, we derived orders and
costs from different sized training sets. We then
evaluated the algorithms, using the derived orders
and costs, on a test set. Training and test sets were
taken from TUNA and D-TUNA.
As Dutch training data, we used 160 furniture and
160 people items, randomly selected from the tex-
tual descriptions in the D-TUNA corpus. The re-
maining furniture and people descriptions (40 items
each) were used for testing. As English training
data, we took all -LOC data from the training set
of the REG Challenge 2009 (Gatt et al, 2009): 165
furniture and 136 people descriptions. As English
test data we used all -LOC data from the REG 2009
development set: 38 furniture and 38 people descrip-
tions. We only used -LOC data to increase compa-
rability to the Dutch data.
From the Dutch and English furniture and people
training data, we selected random subsets of 1, 5,
10, 20, 30, 40 and 50 descriptions. Five different
sets of each size were created, since the accidental
composition of a training set could strongly influ-
ence the results. All training sets were built up in a
cumulative fashion, starting with five randomly se-
lected sets of size 1, then adding 4 items to each of
them to create five sets of size 5, and so on, for each
combination of language and domain. We used these
different training sets to derive preference orders of
attributes for the IA, and costs and property orders
for Graph, as outlined above.
We evaluated the performance of the derived pref-
erence orders and cost functions on the test data for
the corresponding domain and language, using the
standard Dice and Accuracy metrics for evaluation.
Dice measures the overlap between attribute sets,
producing a value between 1 and 0, where 1 stands
for a perfect match and 0 for no overlap at all. Ac-
curacy is the percentage of perfect matches between
the generated attribute sets and the human descrip-
tions in the test set. Both metrics were used in the
REG Generation Challenges.
6
English furniture
IA Graph
Set size Dice Acc.(%) Dice Acc.(%)
1 0.764 36.8 0.693 24.7
5 0.829 55.3 0.756 33.7
10 0.829 55.3 0.777 39.5
20 0.829 55.3 0.788 40.5
30 0.829 55.3 0.782 40.5
40 0.829 55.3 0.793 45.3
50 0.829 55.3 0.797 45.8
All 0.829 55.3 0.810 50.0
Dutch furniture
IA Graph
Set size Dice Acc.(%) Dice Acc.(%)
1 0.925 63.0 0.876 44.5
5 0.935 67.5 0.917 62.0
10 0.929 68.5 0.923 66.0
20 0.930 65.5 0.923 64.0
30 0.931 67.0 0.924 65.5
40 0.931 67.0 0.931 67.5
50 0.929 66.0 0.929 67.0
All 0.926 65.0 0.929 67.5
Table 2: Performance for each set size in the furniture
domain. For sizes 1 to 50, means over five sets are given.
The full sets are 165 English and 160 Dutch descriptions.
Note that the scores of the IA for the English sets of sizes
1 to 30 were also reported in Theune et al (2011).
5 Results
5.1 Overall analysis
To determine the effect of domain and language on
the performance of REG algorithms, we applied re-
peated measures analyses of variance (ANOVA) to
the Dice and Accuracy scores, using set size (1, 5,
10, 20, 30, 40, 50, all) and domain (furniture, peo-
ple) as within variables, and algorithm (IA, Graph)
and language (English, Dutch) as between variables.
The results show main effects of domain (Dice:
F(1,152) = 56.10, p < .001; Acc.: F(1,152) = 76.36,
p < .001) and language (Dice: F(1,152) = 30.30,
p < .001; Acc.: F(1,152) = 3.380, p = .07). Regard-
ing the two domains, these results indicate that both
the IA and the Graph algorithm generally performed
better in the furniture domain (Dice: M = .86, SD =
.01; Acc.: M = .56, SD = .03) than in the people do-
main (Dice: M = .72, SD = .01; Acc.: M = .20, SD =
.02). Regarding the two languages, the results show
that both algorithms generally performed better on
English people
IA Graph
Set size Dice Acc.(%) Dice Acc.(%)
1 0.519 7.4 0.558 12.6
5 0.605 15.8 0.617 14.5
10 0.682 21.1 0.683 20.0
20 0.710 22.1 0.716 24.7
30 0.682 15.3 0.716 26.8
40 0.716 26.3 0.723 26.3
50 0.718 27.9 0.727 26.3
All 0.724 31.6 0.730 28.9
Dutch people
IA Graph
Set size Dice Acc.(%) Dice Acc.(%)
1 0.626 4.5 0.682 17.5
5 0.737 16.0 0.738 21.0
10 0.738 12.5 0.741 19.5
20 0.765 12.5 0.778 25.5
30 0.762 14.5 0.789 25.0
40 0.763 11.5 0.792 25.0
50 0.764 10.5 0.798 26.0
All 0.775 12.5 0.812 32.5
Table 3: Performance for each set size in the people do-
main. For sizes 1 to 50, means over five sets are given.
The full sets are 136 English and 160 Dutch descriptions.
Note that the scores of the IA for the English sets of sizes
1 to 30 were also reported in Theune et al (2011).
the Dutch data (Dice: M = .84, SD = .01; Acc.: M
= .41, SD = .03) than on the English data (Dice: M
= .74, SD = .01; Acc.: M = .34, SD = .03). There
is no main effect of algorithm, meaning that over-
all, the two algorithms had an equal performance.
However, this is different when we look separately
at each domain and language, as we do below.
5.2 Learning curves per domain and language
Given the main effects of domain and language de-
scribed above, we ran separate ANOVAs for the dif-
ferent domains and languages. For these four analy-
ses, we used set size as a within variable, and algo-
rithm as a between variable. To determine the effects
of set size, we calculated the means of the scores
of the five training sets for each set size, so that we
could compare them with the scores of the entire set.
The results are shown in Tables 2 and 3.
We made planned post hoc comparisons to test
which is the smallest set size that does not perform
significantly different from the entire training set in
7
terms of Dice and Accuracy scores (we call this the
?ceiling?). We report results both for the standard
Bonferroni method, which corrects for multiple
comparisons, and for the less strict LSD method
from Fisher, which does not. Note that with the
Bonferroni method we are inherently less likely to
find statistically significant differences between the
set sizes, which implies that we can expect to reach
a ceiling earlier than with the LSD method. Table 4
shows the ceilings we found for the algorithms, per
domain and language.
The furniture domain Table 2 shows the Dice
and Accuracy scores in the furniture domain. We
found significant effects of set size for both the
English data (Dice: F(7,518) = 15.59, p < .001;
Acc.: F(7,518) = 17.42, p < .001) and the Dutch data
(Dice: F(7,546) = 5.322, p < .001; Acc.: F(7,546)
= 5.872, p < .001), indicating that for both lan-
guages, the number of descriptions used for training
influenced the performance of both algorithms in
terms of both Dice and Accuracy. Although we
did not find a main effect of algorithm, suggesting
that the two algorithms performed equally well, we
did find several interactions between set size and
algorithm for both the English data (Dice: F(7,518) =
1.604, ns; Acc.: F(7,518) = 2.282, p < .05) and the
Dutch data (Dice: F(7,546) = 3.970, p < .001; Acc.:
F(7,546) = 3.225, p < .01). For the English furniture
data, this interaction implies that small set sizes
have a bigger impact for the IA than for Graph.
For example, moving from set size 1 to 5 yielded a
Dice improvement of .18 for the IA, while this was
only .09 for Graph. For the Dutch furniture data,
however, a reverse pattern was observed; moving
from set size 1 to 5 yielded an improvement of .01
(Dice) and .05 (Acc.) for the IA, while this was .11
(Dice) and .18 (Acc.) for Graph.
Post hoc tests showed that small set sizes were
generally sufficient to reach ceiling performance:
the general pattern for both algorithms and both
languages was that the scores increased with the size
of the training set, but that the increase got smaller
as the set sizes became larger. For the English
furniture data, Graph reached the ceiling at set size
10 for Dice (5 with the Bonferroni test), and at set
size 40 for Accuracy (again 5 with Bonferroni),
while this was the case for the IA at set size 5 for
English furniture Dutch furniture
Dice Accuracy Dice Accuracy
IA 5 (5) 5 (5) 1 (1) 1 (1)
Graph 10 (5) 40 (5) 5 (1) 5 (1)
English people Dutch people
Dice Accuracy Dice Accuracy
IA 10 (10) 40 (1) 20 (5) 1 (1)
Graph 20 (10) 20 (1) 30 (20) 5 (1)
Table 4: Ceiling set sizes computed using LSD, with
Bonferroni between brackets.
both Dice and Accuracy (also 5 with Bonferroni).
For the Dutch furniture data, Graph reached the
ceiling at set size 5 for both Dice and Accuracy
(and even at 1 with the Bonferroni test), while this
was at set size 1 for the IA (again 1 with Bonferroni).
The people domain Table 3 shows the Dice
and Accuracy scores in the people domain. Again,
we found significant effects of set size for both the
English data (Dice: F(7,518) = 39.46, p < .001;
Acc.: F(7,518) = 11.77, p < .001) and the Dutch data
(Dice: F(7,546) = 33.90, p < .001; Acc.: F(7,546)
= 3.235, p < .01). Again, this implies that for
both languages, the number of descriptions used
for training influenced the performance of both
algorithms in terms of both Dice and Accuracy.
Unlike we did in the furniture domain, we found
no interactions between set size and algorithm, but
we did find a main effect of algorithm for the Dutch
people data (Dice: F(1,78) = .751, ns; Acc.: F(1,78)
= 5.099, p < .05), showing that Graph generated
Dutch descriptions that were more accurate than
those generated by the IA.
As in the furniture domain, post hoc tests showed
that small set sizes were generally sufficient to reach
ceiling performance. For the English data, Graph
reached the ceiling at set size 20 for both Dice and
Accuracy (with Bonferroni: 10 for Dice, 1 for Accu-
racy), while this was the case for the IA at set size 10
for Dice (also 10 with Bonferroni), and at set size 40
for Accuracy (and even at 1 with Bonferroni). For
the Dutch data, Graph reached the ceiling at set size
30 for Dice (20 with Bonferroni), and at set size 5
for Accuracy (1 with Bonferroni). For the IA, ceil-
ing was reached at set size 20 for Dice (Bonferroni:
5), and already at 1 for Accuracy (Bonferroni: 1).
8
6 Discussion and Conclusion
Our main goal was to investigate how many human-
produced references are required by REG algo-
rithms such as the Incremental Algorithm and the
graph-based algorithm to determine preferences (or
costs) for a new domain, and to generate ?human-
like? descriptions for new objects in these domains.
Our results show that small data sets can be used
to train these algorithms, achieving results that are
not significantly different from those derived from
a much larger training set. In the simple furniture
domain even one training item can already be suffi-
cient, at least for the IA. As shown in Table 4, on the
whole the IA needed fewer training data than Graph
(except in the English people domain, where Graph
only needed a set size of 10 to hit the ceiling for
Dice, while the IA needed a set size of 20).
Given that the IA ranks attributes, while the
graph-based REG algorithm ranks attribute-value
pairs, the difference in required training data is
not surprising. In any domain, there will be more
attribute-value pairs than attributes, so determining
an attribute ranking is an easier task than determin-
ing a ranking of attribute-value pairs. Another ad-
vantage of ranking attributes rather than attribute-
value pairs is that it is less vulnerable to the problem
of ?missing data?. More specifically, the chance that
a specific attribute does not occur in a small train-
ing set is much smaller than the chance that a spe-
cific attribute-value pair does not occur. As a conse-
quence, the IA needs fewer data to obtain complete
attribute orderings than Graph needs to obtain costs
for all attribute-value pairs.
Interestingly, we only found interactions between
training set size and algorithm in the furniture do-
main. In the people domain, there was no signifi-
cant difference between the size of the training sets
required by the algorithms. This could be explained
by the fact that the people domain has about twice as
many attributes as the furniture domain, and fewer
values per attribute (see Table 1). This means that
for people the difference between the number of at-
tributes (IA) and the number of attribute-value pairs
(Graph) is not as big as for furniture, so the two al-
gorithms are on more equal grounds.
Both algorithms performed better on furniture
than on people. Arguably, the people pictures in the
TUNA experiment can be described in many more
different ways than the furniture pictures can, so it
stands to reason that ranking potential attributes and
values is more difficult in the people than in the fur-
niture domain. In a similar vein, we might expect
Graph?s flexible generation strategy to be more use-
ful in the people domain, where more can be gained
by the use of costs, than in the furniture domain,
where there are relatively few options anyway, and a
simple linear ordering may be quite sufficient.
This expectation was at least partially confirmed
by the results: although in most cases the differences
are not significant, Graph tends to perform numeri-
cally better than the IA in the people domain. Here
we see the pay-off of Graph?s more fine-grained
preference ranking, which allows it to distinguish
between more and less salient attribute values. In the
furniture domain, most attribute values appear to be
more or less equally salient (e.g., none of the colours
gets notably mentioned more often), but in the peo-
ple domain certain values are clearly more salient
than others. In particular, the attributes HASBEARD
and HASGLASSES are among the most frequent at-
tributes in the people domain when their value is
true (i.e., the target object can be distinguished by
his beard or glasses), but they hardly get mentioned
when their value is false. Graph quickly learns this
distinction, assigning low costs and a high ranking
to <HASBEARD, true> and <HASGLASSES, true>
while assigning high costs and a low ranking to
<HASBEARD, false> and <HASGLASSES, false>.
The IA, on the other hand, does not distinguish be-
tween the values of these attributes.
Moreover, the graph-based algorithm is arguably
more generic than the Incremental Algorithm, as it
can straightforwardly deal with relational properties
and lends itself to various extensions (Krahmer et
al., 2003). In short, the larger training investment
required for Graph in simple domains may be com-
pensated by its versatility and better performance
on more complex domains. To test this assump-
tion, our experiment should be repeated using data
from a more realistic and complex domain, e.g., ge-
ographic descriptions (Turner et al, 2008). Unfortu-
nately, currently no such data sets are available.
Finally, we found that the results of both algo-
rithms were better for the Dutch data than for the
English ones. We think that this is not so much an ef-
9
fect of the language (as English and Dutch are highly
comparable) but rather of the way the TUNA and D-
TUNA corpora were constructed. The D-TUNA cor-
pus was collected in more controlled conditions than
TUNA and as a result, arguably, it contains training
data of a higher quality. Also, because the D-TUNA
corpus does not contain any location properties (X-
and Y-DIMENSION) its furniture and people domains
are slightly less complex than their TUNA counter-
parts, making the attribute selection task a bit easier.
One caveat of our study is that so far we have
only used the standard automatic metrics on REG
evaluation (albeit in accordance with many other
studies in this area). However, it has been found
that these do not always correspond to the results of
human-based evaluations, so it would be interesting
to see whether the same learning curve effects
are obtained for extrinsic, task based evaluations
involving human subjects. Following Belz and
Gatt (2008), this could be done by measuring
reading times, identification times or error rates as a
function of training set size.
Comparing IA with FB and GR We have shown
that small set sizes are sufficient to reach ceiling for
the IA. But which preference orders (PO?s) do we
find with these small set sizes? And how does the
IA?s performance with these orders compare to the
results obtained by alternative algorithms such as
Dale and Reiter?s (1995) classic Full Brevity (FB)
and Greedy Algorithm (GR)? ? a question explicitly
asked by van Deemter et al (2012). In the furniture
domain, all five English training sets of size 5 yield
a PO for which van Deemter et al showed that it
causes the IA to significantly outperform FB and
GR (i.e., either C(olor)O(rientation)S(ize) or CSO;
note that here we abstract over TYPE which van
Deemter and colleagues do not consider). When
we look at the English people domain and consider
set size 10 (ceiling for Dice), we find that four
out of five sets have a preference order where
HAIRCOLOUR, HASBEARD and HASGLASSES are
in the top three (again disregarding TYPE); one of
these is the best performing preference order found
by van Deemter and colleagues (GBH), another
performs slightly less well but still significantly
better than FB and GR (BGH); the other two score
statistically comparable to the classical algorithms.
The fifth people PO includes X- and Y-DIMENSION
in the top three, which van Deemter et al ignore. In
sum: in almost all cases, small set sizes (5 and 10
respectively) yield POs with which the IA performs
at least as well as the FB and GR algorithms, and in
most cases significantly better.
Conclusion We have shown that with few training
instances, acceptable attribute selection results can
be achieved; that is, results that do not significantly
differ from those obtained using a much larger
training set. Given the scarcity of resources in
this field, we feel that this is an important result
for researchers working on REG and Natural
Language Generation in general. We found that less
training data is needed in simple domains with few
attributes, such as the furniture domain, and more in
relatively more complex domains such as the people
domain. The data set being used is also of influence:
better results were achieved with D-TUNA than
with the TUNA corpus, which probably not so much
reflects a language difference, but a difference in
the way the corpora were collected.
We found some interesting differences between
the IA and Graph algorithms, which can be largely
explained by the fact that the former ranks attributes,
and the latter attribute-value pairs. The advantage
of the former (coarser) approach is that overall,
fewer training items are required, while the latter
(more fine-grained) approach is better equipped to
deal with more complex domains. In the furniture
domain both algorithms had a similar performance,
while in the people domain Graph did slightly better
than the IA. It has to be kept in mind that these
results are based on the relatively simple furniture
and people domains, and evaluated in terms of a
limited (though standard) set of evaluation met-
rics. We hope that in the near future semantically
transparent corpora for more complex domains will
become available, so that these kinds of learning
curve experiments can be replicated.
Acknowledgments Krahmer and Koolen re-
ceived financial support from The Netherlands
Organization for Scientific Research (NWO Vici
grant 27770007). We thank Albert Gatt for allowing
us to use his implementation of the IA, and Sander
Wubben for help with k-means clustering.
10
References
Anja Arts, Alfons Maes, Leo Noordman, and Carel
Jansen. 2011. Overspecification facilitates object
identification. Journal of Pragmatics, 43(1):361?374.
Anja Belz and Albert Gatt. 2008. Intrinsic vs. extrinsic
evaluation measures for referring expression genera-
tion. In Proceedings of the 46th Annual Meeting of the
Association for Computational Linguistics (ACL?08),
pages 197?200.
Robert Dale and Ehud Reiter. 1995. Computational in-
terpretation of the Gricean maxims in the generation of
referring expressions. Cognitive Science, 19(2):233?
263.
Paul E. Engelhardt, Karl G.D Bailey, and Fernanda Fer-
reira. 2006. Do speakers and listeners observe the
Gricean Maxim of Quantity? Journal of Memory and
Language, 54:554?573.
Giuseppe Di Fabbrizio, Amanda Stent, and Srinivas
Bangalore. 2008. Trainable speaker-based refer-
ring expression generation. In Twelfth Conference on
Computational Natural Language Learning (CoNLL-
2008), pages 151?158.
Albert Gatt, Ielka van der Sluis, and Kees van Deemter.
2007. Evaluating algorithms for the generation of re-
ferring expressions using a balanced corpus. In Pro-
ceedings of the 11th European Workshop on Natural
Language Generation (ENLG 2007), pages 49?56.
Albert Gatt, Anja Belz, and Eric Kow. 2009. The TUNA-
REG Challenge 2009: Overview and evaluation re-
sults. In Proceedings of the 12th European Workshop
on Natural Language Generation (ENLG 2009), pages
174?182.
Pablo Gerva?s, Raquel Herva?s, and Carlos Le?on. 2008.
NIL-UCM: Most-frequent-value-first attribute selec-
tion and best-scoring-choice realization. In Proceed-
ings of the 5th International Natural Language Gener-
ation Conference (INLG 2008), pages 215?218.
Pamela W. Jordan and Marilyn Walker. 2005. Learning
content selection rules for generating object descrip-
tions in dialogue. Journal of Artificial Intelligence Re-
search, 24:157?194.
John Kelleher. 2007. DIT - frequency based incremen-
tal attribute selection for GRE. In Proceedings of the
MT Summit XI Workshop Using Corpora for Natural
Language Generation: Language Generation and Ma-
chine Translation (UCNLG+MT), pages 90?92.
Ruud Koolen and Emiel Krahmer. 2010. The D-TUNA
corpus: A Dutch dataset for the evaluation of refer-
ring expression generation algorithms. In Proceedings
of the 7th international conference on Language Re-
sources and Evaluation (LREC 2010).
Emiel Krahmer, Sebastiaan van Erk, and Andre? Verleg.
2003. Graph-based generation of referring expres-
sions. Computational Linguistics, 29(1):53?72.
Chris Mellish, Donia Scott, Lynn Cahill, Daniel Paiva,
Roger Evans, and Mike Reape. 2006. A reference
architecture for natural language generation systems.
Natural Language Engineering, 12:1?34.
Thomas Pechmann. 1989. Incremental speech pro-
duction and referential overspecification. Linguistics,
27:98?110.
Ehud Reiter and Anja Belz. 2009. An investigation
into the validity of some metrics for automatically
evaluating NLG systems. Computational Linguistics,
35(4):529?558.
Philipp Spanger, Takehiro Kurosawa, and Takenobu
Tokunaga. 2008. On ?redundancy? in selecting at-
tributes for generating referring expressions. In COL-
ING 2008: Companion volume: Posters, pages 115?
118.
Marie?t Theune, Ruud Koolen, Emiel Krahmer, and
Sander Wubben. 2011. Does size matter ? How much
data is required to train a REG algorithm? In Pro-
ceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies, pages 660?664, Portland, Oregon, USA.
Ross Turner, Somayajulu Sripada, Ehud Reiter, and Ian P.
Davy. 2008. Using spatial reference frames to gener-
ate grounded textual summaries of georeferenced data.
In Proceedings of the 5th International Natural Lan-
guage Generation Conference (INLG), pages 16?24.
Kees van Deemter, Ielka van der Sluis, and Albert Gatt.
2006. Building a semantically transparent corpus for
the generation of referring expressions. In Proceed-
ings of the 4th International Natural Language Gener-
ation Conference (INLG 2006), pages 130?132.
Kees van Deemter, Albert Gatt, Ielka van der Sluis, and
Richard Power. 2012. Generation of referring expres-
sions: Assessing the Incremental Algorithm. Cogni-
tive Science, to appear.
Jette Viethen and Robert Dale. 2010. Speaker-dependent
variation in content selection for referring expression
generation. In Proceedings of the 8th Australasian
Language Technology Workshop, pages 81?89.
Jette Viethen, Robert Dale, Emiel Krahmer, Marie?t Theu-
ne, and Pascal Touset. 2008. Controlling redundancy
in referring expressions. In Proceedings of the Sixth
International Conference on Language Resources and
Evaluation (LREC 2008), pages 239?246.
11
Proceedings of the 14th European Workshop on Natural Language Generation, pages 72?81,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
Graphs and Spatial Relationsin the Generation of Referring ExpressionsJette Viethen
h.a.e.viethen@uvt.nl
TiCC
University of Tilburg
Tilburg, The Netherlands
Margaret Mitchell
m.mitchell@jhu.edu
HLT Centre of Excellence
Johns Hopkins University
Baltimore, USA
Emiel Krahmer
e.j.krahmer@uvt.nl
TiCC
University of Tilburg
Tilburg, The NetherlandsAbstract
When they introduced the Graph-Based
Algorithm (GBA) for referring expression
generation, Krahmer et al (2003) flaunted
the natural way in which it deals with re-
lations between objects; but this feature
has never been tested empirically. We fill
this gap in this paper, exploring referring
expression generation from the perspec-
tive of the GBA and focusing in particu-
lar on generating human-like expressions
in visual scenes with spatial relations. We
compare the original GBA against a variant
that we introduce to better reflect human
reference, and find that although the orig-
inal GBA performs reasonably well, our
new algorithm offers an even better match
to human data (77.91% Dice). Further, it
can be extended to capture speaker vari-
ation, reaching an 82.83% Dice overlap
with human-produced expressions.1 Introduction
Ten years ago, Krahmer et al (2003) published the
Graph-Based Algorithm (GBA) for referring ex-
pression generation (REG). REG has since become
one of the most researched areas within Natural
Language Generation, due in a large part to the
central role it plays in communication: referring
allows humans and language generation systems
alike to invoke the entities that the discourse is
about in the mind of a listener or reader.
Like most REG algorithms, the GBA is focussed
on the task of selecting the semantic content for a
referring expression, uniquely identifying a target
referent among all objects in its visual or linguistic
context. The framework used by the GBA is par-
ticularly attractive because it provides fine-grained
control for finding the ?best? referring expression,
encompassing several previous approaches. This
control is made possible by defining a desired
cost function over object properties to guide the
construction of the output expression and using a
search mechanism that does not stop at the first
solution found.
One characteristic of the GBA particularly em-
phasized by Krahmer et al (2003), advancing
from research on algorithms such as the Incre-
mental Algorithm (Dale and Reiter, 1995) and the
Greedy Algorithm (Dale, 1989), was the treatment
of relations between entities. Relations such as ontop of or to the left of fall out naturally from the
graph-based representation of the domain, a facet
missing in earlier algorithms. We believe that this
makes the GBA particularly well-suited for gener-
ating language in spatial visual domains.
In the years since the inception of the GBA,
the REG community has become increasingly in-
terested in evaluating algorithms against human-
produced data in visual domains, aiming to mimic
human references to objects. This interest has
manifested most prominently in the 2007-2009
REG Challenges (Belz and Gatt, 2007; Gatt et al,
2008; Gatt et al, 2009) based on the TUNA Cor-
pus (van Deemter et al, 2012). The GBA per-
formed among the best algorithms in all three of
these challenges. However, in particular its abil-
ity to analyze relational information could not be
assessed, because the TUNA Corpus does not con-
tain annotated relational descriptions.
We rectify this omission in the current work by
testing the GBA on the GRE3D3 Corpus, which
was designed to study the use of spatial rela-
tions in referring expressions (Viethen and Dale,
2008). We compare against a variant of the GBA
that we introduce to build longer referring expres-
72
sions, following the observation that humans tend
to overspecify (i.e., not be maximally brief) in
their referring expressions (Sonnenschein, 1985;
Pechmann, 1989; Engelhardt et al, 2006; Arts et
al., 2011). For both algorithms, we experiment
with cost functions defined at different granular-
ities to produce the best match to human data. We
find that we can match human data better than
the original GBA with the variant that encourages
overspecification.
With this model, we aim to further ad-
vance towards human-like reference by develop-
ing a method to capture speaker-specific varia-
tion. Speaker variation cannot easily be modeled
by the classic input variables of REG algorithms,
but a number of authors have shown that system
output can be improved by using speaker identity
as an additional feature; this has often been ac-
companied by the observation that commonalities
can be found in the reference behaviour of differ-
ent speakers (Bohnet, 2008; Di Fabbrizio et al,
2008a; Mitchell et al, 2011b), particularly for spa-
tial relations (Viethen and Dale, 2009). In the sec-
ond experiment reported in this paper, we combine
these insights by automatically clustering groups
of speakers with similar behaviour and then defin-
ing separate cost functions for each group to better
guide the algorithms.
Before we assess the ability of the GBA and our
variant to produce human-like referring expres-
sions containing relations (Sections 5 and 6), we
will give an overview of the relevant background
to the treatment of relations in REG, a short history
of the GBA, and the relevance of individual vari-
ation (Section 2). We introduce our new variant
graph-based algorithm, LongestFirst, in Section 3.2 Relations, Graphs and IndividualVariation2.1 Relations in REG
In the knowledge representation underlying most
work in REG, each object in a scene is modeled as
a set of attribute-value pairs describing the object?s
properties, such as hsize, largei. Such a represen-
tation is used in the two of the classic algorithms,
the Greedy Algorithm (Dale, 1989) and the Incre-
mental Algorithm (IA) (Dale and Reiter, 1995).
Neither of these was originally intended to process
relations between objects.
Several attempts have been made to adapt the
traditional REG algorithms to include relations be-
tween objects in their output, but all of them suf-
fer from problems with the knowledge representa-
tion not being suited to relations. Dale and Had-
dock (1991) use a constraint network and a recur-
sive loop to extend the Greedy Algorithm, which
uses the discriminatory power of an attribute as
the main selection criterion. They treat relations
the same as other attributes; but in most cases a
certain spatial relation to a particular other ob-
ject is fully distinguishing, which easily leads to
strange chains of relations in the output omitting
most other attributes (Viethen and Dale, 2006).
Krahmer and Theune (2002) suggest a simi-
lar adjustment for the IA by introducing a re-
cursive loop if a relation to another object is in-
troduced to the referring expression under con-
struction. They treat relations as fundamentally
different from other attributes in order to recog-
nize when to enter the recursive loop, however,
they fail to address the problem of infinite regress,
whereby the objects in a domain might be de-
scribed in a circular manner by the relations hold-
ing between them. Another relational extension to
the IA has been proposed by Kelleher and Kruijff
(2006), treating relations as a completely different
class from other attributes. Both extensions of the
IA make the simplifying assumption that relations
should only be considered if it is not possible to
fully distinguish the target referent from the sur-
rounding objects in any other way, with the idea
that it takes less effort to consider and describe
only one object (Krahmer and Theune, 2002; Vie-
then and Dale, 2008).2.2 A Short History of the GBA
A new approach to REG was proposed by Krah-
mer et al (2003). In this approach, a scene is
represented as a labeled directed graph (see Fig-
ure 1(b)), and content selection is a subgraph con-
struction problem. Assuming a scene graph G =
hVG, EGi, where vertices VG represent objects and
edges EG represent the properties and relations of
these objects with associated costs, their algorithm
returns the cheapest distinguishing subgraph that
uniquely refers to the target object v 2 VG. Re-
lations between objects (i.e., edges between dif-
ferent vertices) are a natural part of this repre-
sentation, without requiring special computational
mechanisms. In addition to cost functions, the
GBA requires a preference ordering (PO) over the
edges to arbitrate between equally cheap descrip-
tions (Viethen et al, 2008).
73
(a) Scene 7 from the GRE3D3 Corpus.
b
e
l
o
w
a
b
o
v
e
left-of
right-of
yellow
small
ball
large
small
ball
red
cube
yellow
right-
hand
l
e
f
t
-
o
f
r
i
g
h
t
-
o
f
right-
hand
right-
hand
(b) A graph representing the scene to the left.
Figure 1: An example scene from the GRE3D3 Corpus and the corresponding domain graph.
As the cost functions and preference orders are
specified over edges (i.e., properties), they allow
much more fine-grained control over which prop-
erties to generate for a target referent than the
attribute-based preference orders employed by the
IA and its descendants. The cost functions can be
used to give preference to a commonly used size
value, such as large, over a rarely used color value,
such as mauve, although in general color is de-
scribed more often than size. This process is aided
by a branch-and-bound search that guarantees to
find the cheapest (i.e., ?best?) referring expression.
Since its inception, the GBA has been shown to
be useful for several referential phenomena. Krah-
mer and van der Sluis (2003) combined verbal
descriptions with pointing gestures by modelling
each such gesture as additional looping edges on
all objects that it might be aimed at. While the au-
thors confirmed the ideas implemented in the al-
gorithm in psycholinguistic studies (van der Sluis,
2005), they never assessed its output in an actual
domain.
van Deemter and Krahmer (2007) demonstrated
how the GBA could be used to generate reference
to sets as well as to negated and gradable prop-
erties by representing implicit information as ex-
plicit edges in domain graphs. They also presented
a simple way to account for discourse salience
based on restricting the distractor set. Its ability
to cover such a breadth of referential phenomena
makes the GBA a reasonably robust algorithm for
further exploring the generation of human-like ref-
erence.
The GBA was systematically tested against
human-produced referring expressions for the first
time in the ASGRE Challenge 2007 (Belz and
Gatt, 2007). This entry is described in detail in
(Viethen et al, 2008) and was very successful as
well in the following 2008 and 2009 REG Chal-
lenges (Gatt et al, 2008; Gatt et al, 2009) with
a free-na??ve cost function. This cost function as-
signs 0 cost to the most common attributes, 2 to
the rarest, and 1 to all others. By making the most
common attributes free, it became possible to in-
clude these attributes redundantly in a referring
expression, even if they were not strictly neces-
sary for identifying the target. The cost functions
used in the challenges were attribute-based, and
did therefore not make use of the refined control
capabilities of the GBA.
Theune et al (2011) used k-means clustering
on the property frequencies in order to provide
a more systematic method to transfer the FREE-
NAI?VE cost function to new domains. They found
that using only two clusters (a high frequency and
a low frequency group with associated costs of 0
and 1) achieves the best results, with no significant
differences to the FREE-NAI?VE cost function on
the TUNACorpus. Subsequently they showed that
on this corpus, a training set of only 20 descrip-
tions suffices to determine a 2-means cost function
that performs as well as one based on 165 descrip-
tions. In (Koolen et al, 2012), the same authors
extended these experiments to a Dutch version of
the TUNA Corpus (Koolen and Krahmer, 2010)
and came to a similar conclusion. Neither of the
corpora used in these experiments included rela-
tions between objects.2.3 Individual Variation in REG
A number of authors have argued that to be able to
produce human-like referring expressions, an al-
gorithm must account for speaker variation: Dif-
ferent speakers will refer to the same object in
different ways, and modeling this variation can
bring us closer to generating the rich variety of ex-
74
pressions that people produce. Several approaches
have been made in this direction.
Although this was not explicitly discussed in
(Jordan and Walker, 2005), the machine-learned
models presented there performed significantly
better at replicating human-produced referring
expressions when a feature set was used that
included information about the identity of the
speaker. In (Viethen and Dale, 2010), the impact
of speaker identity as a machine-learning feature
is more systematically tested. They show that ex-
act knowledge about which speaker produced a
referring expression boosts performance, but also
find many commonalities between different speak-
ers? strategies for content selection. Mitchell et
al. (2011b) used participant identity in a machine
learner to successfully predict the kind of size
modifier to be used in a referring expression. Ad-
ditionally, various submissions to the REG chal-
lenges, particularly by Bohnet and Fabbrizio et al
(Bohnet, 2008; Bohnet, 2009; Di Fabbrizio et al,
2008a; Di Fabbrizio et al, 2008b) used speaker-
specific POs to increase performance in their adap-
tations of the IA.
All of these systems used the exact speaker
identity as input, although many of the authors
noted that groups of speakers behave similarly
(Viethen and Dale, 2010; Mitchell et al, 2011b).
We build off of this idea by clustering similar
speakers together before learning parameters, and
then generate for speaker-specific clusters. This
method results in a significant improvement in per-
formance.3 LongestFirst: a New Search Strategy
The GBA guarantees to return the cheapest pos-
sible subgraph that fully distinguishes the target.
However, many distinguishing subgraphs can have
the same cost, for example, if a target can be iden-
tified either by its color or by its size, and color
and size have the same cost. Viethen et al (2008)
discuss some examples in more detail.
In the case that more than one cheapest sub-
graph exists, the original GBA will generate the
first it encountered. Due to its branch-and-bound
search strategy, this is also the smallest subgraph,
corresponding to the shortest possible description
that can be found at the cheapest cost. Because
its pruning mechanism does not allow further ex-
pansion of a graph once it is distinguishing, the
number of attributes that the algorithm can include
redundantly is limited, in particular if relations
are involved. Attributes of visually salient nearby
landmark objects that are introduced to the refer-
ring subgraph by a relation are only considered af-
ter all other attributes of the target object. This is
the case even if these attributes are free and feature
early in the preference order.
The GBA is therefore not able to replicate many
overspecified descriptions that human speakers
may use: if a subgraph containing a relation is
already distinguishing before the attributes of a
landmark object are considered, the algorithm will
not include any information about the landmark.
Not only is it unlikely that a landmark object
should be included in a description without any
further information about it, it also seems intu-
itive that speakers with a preference for certain
attributes (such as color) would include these at-
tributes not only for the target referent, but for a
landmark object as well.
We solve this problem by amending the search
algorithm in a way that finds the longest of all
the cheapest subgraphs, and call the resulting al-
gorithm LongestFirst. This search strategy results
in a much larger number of subgraphs to check, in
particular, when used with cost functions that in-
volve a lot of free edges. In order to keep our sys-
tems tractable, we therefore limit the number of
attributes the LongestFirst algorithm can include
to four, based on the finding from (Mitchell et al,
2011a) that people rarely include more than four
modifiers in a noun phrase. In Experiment 2 we
additionally test a setting in which the maximum
number of attributes is determined on the basis of
the average description length in the training data.4 Implementation Note
The original implementation of the GBA did not
provide a method to specify the order in which
edges were tried, although the edge order deter-
mines the order in which distinguishing subgraphs
are found by the algorithm (Krahmer et al, 2003).
This was fixed in (Viethen et al, 2008) by adding
a PO as parameter to the GBA to arbitrate between
equally cheap solutions.
A further issue arose in this implementation
when tested on the GRE3D3 domain, because
there was no simple way to specify which object
each property belonged to; for the TUNA domain
where the GBA has traditionally been evaluated, it
is safe to always assume a property belongs to the
75
target referent. We have therefore provided addi-
tional functionality to the GBA that requires that
not only hattribute, valuei pairs are specified, but
hentity1, attribute, value, entity2i tuples, which
can be translated directly into graph edges. For ex-
ample the tuple htg:relation:above:lmi represents
the edge labelled above between the yellow ball
and the red cube in Figure 1. For direct attributes,
such as size or color, entity1 and entity2 in these
tuples are identical, resulting in loop edges. This
Java implementation of the GBA and the Python
implementation of the LongestFirst algorithm are
available at www.m-mitchell.com/code.5 Experiment 1: Relational Descriptions
In our first experiment, we evaluate how well the
GBA produces human-like reference in a corpus
that uses spatial relations. We compare against the
LongestFirst variant that encourages overspecifi-
cation.5.1 Material
To evaluate the different systems, we use the
GRE3D3 Corpus. It consists of 630 distinguish-
ing descriptions for objects in simple 3D scenes.
Each of the 20 scenes contains three objects in
different spatial relations relative to one another
(see Figure 1). The target referent, marked by an
arrow, was always in a direct adjacency relation
(on   top   of or in   front   of) to one of the
other two objects, while the third object was al-
ways placed at a small distance to the left or right.
The objects are either spheres or cubes and differ
in size and color. In addition to these attributes, the
63 human participants who contributed to the cor-
pus used the objects? location as well as the spatial
relation between the target referent and the closest
landmark object. Each participant described one
of two sets of 10 scenes. The scenes in the two sets
are not identical, but equivalent, so the sets can be
conflated for most analyses. Spatial relations were
used in 36.6% (232) of the descriptions, although
they were never necessary to distinguish the target
object. Further details about the corpus may be
found in (Viethen and Dale, 2008).5.2 Approaches to Parameter Settings
As discussed above, the GBA behaves differently
depending on the PO and the cost functions over
its edges. To find the best match with human
data, we explore several different approaches to
setting these two parameters. An important dis-
tinction between the approaches we try hinges
on the difference between attributes and proper-ties. Attributes correspond to, e.g., color, size, orlocation, while properties are attribute-value pairs,
e.g., hcolor, redi, hsize, largei, hlocation,middlei.
Previous evaluations of the GBA typically used
parameter settings based on either attribute fre-
quency (Viethen et al, 2008) or property fre-
quency (Koolen et al, 2012). We compare both
methods for setting the parameters. Because the
scenes on which the corpus is based were not bal-
anced for the different attribute-values, the fre-
quency of a property is calculated as the pro-
portion of descriptions in which it was used for
those scenes where the target actually possessed
this property. For our evaluation, the trainable
costs and the POs are determined using cross-
validation (see Section 5.3). We use the following
approaches:
0-COST-PROP: All edges have 0 cost, and the
PO is based on property frequency. Each property
is included (regardless of how distinguishing it is)
until a distinguishing subgraph is found.
0-COST-ATT: As 0-COST-PROP, but the PO is
based on attribute frequency.
FREE-NAI?VE-PROP: Properties that occur in
more than 75% of descriptions where they could
be used cost 0, properties with a frequency below
20% cost 2, and all others cost 1 (Viethen et al,
2008). The PO is based on property frequency.
FREE-NAI?VE-ATT: As FREE-NAI?VE-PROP:, but
costs and PO are based on attribute frequency.
K-PROP: Costs are assigned using k-means clus-
tering over property frequencies with k=2 (Theune
et al, 2011). The PO is based on property fre-
quency.
K-ATT: As K-PROP, but the k-means clustering
and the PO are based on attribute frequency.5.3 Evaluation Setup
We evaluate the version of the GBA used by Vie-
then et al (2008), with additional handling for
relations between entities (see Section 4). We
compare against our LongestFirst algorithm from
Section 3 on all approaches described in Sec-
tion 5.2. As baselines, we compare against the
Incremental Algorithm (Dale and Reiter, 1995)
and a simple informed approach that includes at-
tributes/properties seen in more than 50% of the
76
training descriptions. We do not use the IA?s re-
lational extensions (Krahmer and Theune, 2002;
Kelleher and Kruijff, 2006), because these would
deliver the same relation-free output as the basic
IA (relations are never necessary for identifying
the target in GRE3D3). These two baselines are
tried with an attribute-based PO and a property-
based one. We do not expect a difference between
the attribute- and the property-based PO on the IA,
as this difference would only come to the fore in a
situation where a choice has to be made between
two values of the same attribute. In the IA?s anal-
ysis of the GRE3D3 domain, this can only happen
with relations, which it will not use in this domain.
We use Accuracy and Dice, the two most com-
mon metrics for human-likeness in REG (Gatt and
Belz, 2008; Gatt et al, 2009), to assess our sys-
tems. Accuracy reports the relative frequency with
which the generated attribute set and the human-
produced attribute set match exactly. Dice mea-
sures the overlap between the two attribute sets.
For details, see, for example, Krahmer and van
Deemter?s (2012) survey paper. We train and test
our systems using 10-fold cross-validation.5.4 Results
The original version of the Graph-Based Algo-
rithm shows identical performance for all ap-
proaches (See Table 1). All use a preference order
starting with type, followed by color and size, and
a cost function that favors the same attributes. As
these attributes always suffice to distinguish the in-
tended referent, the algorithm stops before spatial
relations are considered. For the scene in Figure 1
it includes the minimal content htg:type:balli, but
for a number of scenes it overspecifies the descrip-
tion.
The LongestFirst/0-COST systems and the
LongestFirst/K-PROP system are the only sys-
tems that include relations in their output.
The LongestFirst/0-COST systems both in-
clude a relation in every description; however,
not always the one that was included in the
human-produced reference, resulting in 521
false-positives for the attribute-based version
and 398 for the property-based one. For the
scene in Figure 1 they include htg:color:yellow,tg:size:small, tg:type:ball, tg:right of:obj3i and
htg:color:yellow, tg:size:small, tg:type:ball,tg:on top of:lmi, respectively. The first
one of these two attribute sets (produced by
Original Longest
GBA First
0-COST- Acc 39.21 0.16
PROP Dice 73.40 68.75
0-COST- Acc 39.21 0.00
ATT Dice 73.40 64.34
FREE-NAI?VE Acc 39.21 46.51
-ATT Dice 73.40 77.91
FREE-NAI?VE Acc 39.21 38.10
-PROP Dice 73.40 74.99
K-PROP Acc 39.21 35.08Dice 73.40 74.66
K-ATT Acc 39.21 35.08Dice 73.40 74.56
50%-Base IA
prop- Acc 27.30 37.14
based PO Dice 72.17 72.21
att- Acc 24.92 37.14
based PO Dice 71.16 72.21
Table 1: Experiment 1: System performance in %.
We used  2 on Accuracy and paired t-tests on Dice
to check for statistical significance. The best per-
formance is highlighted in boldface. It is statisti-
cally significantly different from all other systems
(Acc: p < 0.02, Dice: p < 0.0001).
LongestFirst/0-COST-ATT) includes the rela-
tion between the target and the third object
to the right, which was almost never included
in the human-produced references, leading to
many false-positives. The LongestFirst/K-PROP
system results in only 45 true-positives and
81 false-positives. It includes the attribute set
htg:color:yellow, tg:type:balli for Figure 1.
One of its relational descriptions (for Scene 5)
contains the set htg:size:small, tg:color:blue,tg:on top of:lmi.
The 50%-baseline system outperforms the
LongestFirst/0-COST systems, which illustrates
the utility of cost functions in combination with
a PO. It includes the attribute set htg:color:yellow,tg:type:balli for the scene in Figure 1. The best
performing system is the LongestFirst algorithm
with the attribute-based FREE-NAI?VE approach,
although this system produces no spatial relations.6 Experiment 2: Individual Variation
We now extend our methods to take into account
individual variation in the content selection for
referring expressions, and evaluate whether we
have better success at reproducing participants? re-
lational descriptions. Rather than using speaker
identity as an input parameter to the system (Sec-
tion 2.3), we automatically find groups of people
77
who behave similarly to each other, but signifi-
cantly different to speakers in the other groups.6.1 Evaluation Setup
We use k-means clustering to group the speak-
ers in the GRE3D3 Corpus based on the number
of times they used each attribute and the average
length of their descriptions. We tried values be-
tween 2 and 5 for k, but found that any value
above 2 resulted in two very large clusters accom-
panied by a number of extremely small clusters.
As these small clusters would not be suitable for
x-fold cross-validation, we proceed with two clus-
ters, one consisting of speakers preferring rela-
tively long descriptions that often contain spatial
relations (Cluster CL0, 16 speakers, 160 descrip-
tions), and one consisting of speakers preferring
short, non-relational descriptions (Cluster CL1, 47
speakers, 470 descriptions).
We train cost functions and POs separately for
the two clusters in order to capture the different
behaviour patterns they are based on. We use the
FREE-NAI?VE cost functions for this experiment,
which outperformed all others in Experiment 1.
We again use 10-fold cross-validation for the eval-
uation. In this experiment, we vary the maximum
length setting for the LongestFirst algorithm. In
Experiment 1, the maximum length for a referring
expression was set to 4 based on previous empiri-
cal findings. Here we additionally test setting it to
the rounded average length for each training fold.
On Cluster CL0 this average length is 6 in all folds,
on Cluster CL1 it is 3.6.2 Results
As shown in Table 2, the LongestFirst algorithm
performs best at generating human-like spatial re-
lations (Cluster CL0), with property-based param-
eters and a maximum description length deter-
mined by the training set. It produces the attribute
set hlm:type:cube, tg:on top of:lm, tg:type:ball,tgcolouryellow, lm:colour:redi for Figure 1. The
difference to the other systems is statistically sig-
nificant for both Accuracy ( 2>15, p<0.0001)
and Dice (t>13, p<0.0001). The attribute-based
parameters and the original GBA perform very
badly on this cluster. For participants who do
not tend to use spatial relations (Cluster CL1),
the maximum length setting has no influence,
but attribute-based parameters perform better than
property-based ones. The attribute-based Longest-
First systems also outperform the original GBA
CL0 CL1 avg
FN Acc 19.38 48.94 41.43
LongestFirst -PROP Dice 75.61 80.27 79.08
-max-av FN Acc 0.00 60.00 44.76
-ATT Dice 55.74 85.28 77.78
FN Acc 0.63 48.94 36.67
LongestFirst -PROP Dice 72.15 80.21 78.17
-max4 FN Acc 0.00 60.00 44.76
-ATT Dice 59.01 85.28 78.61
FN Acc 5.00 48.30 37.30
Original -PROP Dice 49.36 80.77 72.79
GBA FN Acc 5.00 50.85 39.21
-ATT Dice 49.36 81.58 73.40
Table 2: Experiment 2: Performance in % of the
LongestFirst and OriginalGraph algorithms on the
two speaker clusters and overall using the FREE-
NAI?VE (FN) approaches. We used  2 on Accu-
racy and paired t-tests on Dice to check for statis-
tical significance. The best performance in each
column and those that are statistically not signifi-
cantly different are highlighted in boldface.
on CL1, but interestingly none of the differences
are as large as on CL0. For the scene in Fig-
ure 1 they produce the attribute set htg:type:ball,tg:colour:yellowi.
The average results over both clusters (shown
in the last column Table 2) are not conclusive
as to which setting should be used overall, al-
though it is clear that the LongestFirst version is
preferable when evaluated by Dice. The differ-
ent result patterns on the two clusters suggest that
the different referential behaviour of the partici-
pants in the two clusters are ideally modeled us-
ing different parameters. In particular, it appears
that property-based costs are useful for replicat-
ing descriptions containing relations to other ob-
jects, while attribute-based costs are useful for
replicating shorter descriptions. The best over-
all performance, achieved by combining the best
performing systems on each cluster (LongestFirst-
max-av/FN-PROP on CL0 and LongestFirst/FN-
ATT with either maximum length setting on CL1),
lies at 49.68% Accuracy and 82.83% Dice. The
Dice score in this combined model is significantly
higher than the best achieved by LongestFirst-
max-av/FN-PROP and from the best Dice score
achieved on the unclustered data in Experiment 1
(t=8.2, p<0.0001). The difference in Accuracy is
not significant ( 2=1.2, p> 0.2).
To get an idea of how successful the new
LongestFirst approach is at replicating the use of
relations on the clustered data, we take a closer
look at the output of the best-performing systems
78
on the two clusters. On CL0, the cluster of partic-
ipants who produce longer descriptions contain-
ing more spatial relations, the best match to the
human data comes from LongestFirst-max-av/FN-
PROP. 147 of the 160 descriptions in this cluster
contain a relation, and the system includes the cor-
rect relation for all 147. It falsely also includes a
relation for the remaining 13 descriptions. This
shows that with the appropriate parameter settings
the LongestFirst algorithm is able to replicate hu-
man relational reference behaviour, but personal
speaker preferences are the main driving factor for
the human use of relations.
CL1, the cluster with shorter descriptions,
contains only 85 (18%) relational descriptions.
The best performing system on this cluster
(LongestFirst/FN-ATT) does not produce any rela-
tions. This is not surprising as the cost functions
and POs for this cluster are necessarily dominated
by the non-relational attributes used more regu-
larly. The cases in which relations are used stem
from participants who do not show a clear prefer-
ence for or against relations and would therefore
be hard to model in any system. With more data it
might be possible to group these participants into
a third cluster and find suitable parameter settings
for them. This would only be possible if their use
of relations is influenced by other factors available
to the algorithm, such as the spatial configuration
of the scene. Viethen and Dale?s (2008) analysis of
the GRE3D3 Corpus suggests that this is the case
at least to some extent.7 Conclusions and Future Work
We have evaluated the Graph-Based Algorithm for
REG (Krahmer et al, 2003) as well as a novel
search algorithm, LongestFirst, that functions on
the same graph-based representation, to assess
their ability to generate referring expressions that
contain spatial relations. We coupled the search
algorithms with a number of different approaches
to setting the cost functions and preference orders
that guide the search.
In Experiment 1, we found that ignoring the cost
function (our 0-cost approaches) is not helpful; but
the LongestFirst algorithm, which produces longer
descriptions, leads to more human-like output for
the visuospatial domain we evaluate on than the
original Graph-Based Algorithm or the Incremen-
tal Algorithm (Dale and Reiter, 1995). However,
in order for spatial relations to be included in a
human-like way, it was necessary to take into ac-
count speaker preferences. We modeled these in
Experiment 2 by clustering the participants who
had contributed to the evaluation corpus based on
their referential behaviour. By training separate
cost functions and preference orders for the dif-
ferent clusters, we enabled the LongestFirst al-
gorithm to correctly reproduce 100% of relations
used by people who regularly mentioned relations.
Our findings suggest that the graph-based rep-
resentation proposed by Krahmer et al (2003)
can be used to successfully generate relational de-
scriptions, however their original search algorithm
needs to be amended to allow more overspecifica-
tion. Furthermore, we have shown that variation
in the referential behaviour of individual speak-
ers has to be taken into account in order to suc-
cessfully model the use of relations in referring
expressions. We have proposed a clustering ap-
proach to advance this goal based directly on the
referring behaviour of speakers rather than speaker
identity. We have found that the best models use
fine-grained property-based parameters for speak-
ers who tend to use spatial relations, and coarser
attribute-based parameters for speakers who tend
to use shorter descriptions.
In future work, we hope to expand to more
complex domains, beyond the simple properties
available in the GRE3D3 Corpus. We also aim
to explore further graph-based representations and
search strategies, modeling non-spatial properties
as separate vertices, similar to the approach by
Croitoru and van Deemter (2007).8 Acknowledgements
Viethen and Krahmer received financial support
from The Netherlands Organization for Scientific
Research, (NWO, Vici grant 277-70-007), and
Mitchell received financial support from the Scot-
tish Informatics and Computer Science Alliance
(SICSA), which is gratefully acknowledged.References
Anja Arts, AlfonsMaes, Leonard Noordman, and Carel
Jansen. 2011. Overspecification in written instruc-
tion. Linguistics, 49(3):555?574.
Anja Belz and Albert Gatt. 2007. The Attribute Se-
lection for GRE Challenge: Overview and evalu-
ation results. In Proceedings of the Workshop onUsing Corpora for NLG: Language Generation and
79
Machine Translation (UCNLG+MT), pages 75?83,
Copenhagen, Denmark.
Bernd Bohnet. 2008. The fingerprint of human refer-
ring expressions and their surface realization with
graph transducers. In Proceedings of the 5th Inter-national Conference on Natural Language Genera-tion, pages 207?210, Salt Fork OH, USA.
Bernd Bohnet. 2009. Generation of referring expres-
sion with an individual imprint. In Proceedings ofthe 12th European Workshop on Natural LanguageGeneration, pages 185?186, Athens, Greece.
Madalina Croitoru and Kees van Deemter. 2007. A
conceptual graph approach to the generation of re-
ferring expressions. In Proceedings of the 20thInternational Joint Conference on Artificial Intelli-gence, pages 2456?2461, Hyderabad, India.
Robert Dale and Nicholas Haddock. 1991. Gener-
ating referring expressions involving relations. InProceedings of the 5th Conference of the EuropeanChapter of the Association for Computational Lin-guistics, pages 161?166, Berlin, Germany.
Robert Dale and Ehud Reiter. 1995. Computational
interpretations of the Gricean maxims in the gener-
ation of referring expressions. Cognitive Science,
19(2):233?263.
Robert Dale. 1989. Cooking up referring expressions.
In Proceedings of the 27th Annual Meeting of the As-sociation for Computational Linguistics, pages 68?
75, Vancouver BC, Canada.
Giuseppe Di Fabbrizio, Amanda Stent, and Srinivas
Bangalore. 2008a. Referring expression generation
using speaker-based attribute selection and trainable
realization (ATTR). In Proceedings of the 5th Inter-national Conference on Natural Language Genera-tion, pages 211?214, Salt Fork OH, USA.
Giuseppe Di Fabbrizio, Amanda J. Stent, and Srinivas
Bangalore. 2008b. Referring expression generation
using speaker-based attribute selection and train-
able realization. In Twelfth Conference on Com-putational Natural Language Learning, Manchester,
UK.
Paul E. Engelhardt, Karl D. Bailey, and Fernanda Fer-
reira. 2006. Do speakers and listeners observe the
gricean maxim of quantity? Journal of Memory andLanguage, 54:554?573.
Albert Gatt and Anja Belz. 2008. Attribute selection
for referring expression generation: New algorithms
and evaluation methods. In Proceedings of the5th International Conference on Natural LanguageGeneration, pages 50?58, Salt Fork OH, USA.
Albert Gatt, Anja Belz, and Eric Kow. 2008. The
TUNA Challenge 2008: Overview and evaluation
results. In Proceedings of the 5th InternationalConference on Natural Language Generation, pages
198?206, Salt Fork OH, USA.
Albert Gatt, Anja Belz, and Eric Kow. 2009. The
TUNA-REG Challenge 2009: Overview and eval-
uation results. In Proceedings of the 12th EuropeanWorkshop on Natural Language Generation, pages
174?182, Athens, Greece.
Pamela W. Jordan and Marilyn Walker. 2005. Learn-
ing content selection rules for generating object de-
scriptions in dialogue. Journal of Artificial Intelli-gence Research, 24:157?194.
John Kelleher and Geert-Jan Kruijff. 2006. Incre-
mental generation of spatial referring expressions in
situated dialog. In Proceedings of the 21st Inter-national Conference on Computational Linguisticsand the 44th Annual Meeting of the Association forComputational Linguistics, pages 1041?1048, Syd-
ney, Australia.
Ruud Koolen and Emiel Krahmer. 2010. The D-
TUNA Corpus: A dutch dataset for the evaluation
of referring expression generation algorithms. InProceedings of the 7th International Conference onLanguage Resources and Evaluation, Valetta, Malta.
Ruud Koolen, Emiel Krahmer, and Marie?t Theune.
2012. Learning preferences for referring expression
generation: Effects of domain, language and algo-
rithm. In Proceedings of the 7th International Nat-ural Language Generation Conference, pages 3?11,
Starved Rock, IL, USA.
Emiel Krahmer and Marie?t Theune. 2002. Effi-
cient context-sensitive generation of referring ex-
pressions. In Kees van Deemter and Rodger Kibble,
editors, Information Sharing: Reference and Pre-supposition in Language Generation and Interpre-tation, pages 223?264. CSLI Publications, Stanford
CA, USA.
Emiel Krahmer and Kees van Deemter. 2012. Compu-
tational generation of referring expressions: A sur-
vey. Computational Linguistics, 38(1):173?218.
Emiel Krahmer and Ielka van der Sluis. 2003. A new
model for generating multimodal referring expres-
sions. In Proceedings of the 9th European Workshopon Natural Language Generation, pages 47?57, Bu-
dapest, Hungary.
Emiel Krahmer, Sebastiaan van Erk, and Andre? Verleg.
2003. Graph-based generation of referring expres-
sions. Computational Linguistics, 29(1):53?72.
Margaret Mitchell, Aaron Dunlop, and Brian Roark.
2011a. Semi-supervised modeling for prenominal
modifier ordering. In Proceedings of the 49th An-nual Meeting of the Association for ComputationalLinguistics: Human Language Technologies, pages
236?241, Portland OR, USA.
Margaret Mitchell, Kees van Deemter, and Ehud Re-
iter. 2011b. Applying machine learning to the
choice of size modifiers. In Proceedings of the 2ndWorkshop on the Production of Referring Expres-sions, Boston MA, USA.
80
Thomas Pechmann. 1989. Incremental speech produc-
tion and referential overspecification. Linguistics,
27:89?110.
Susan Sonnenschein. 1985. The development of ref-
erential communication skills: Some situations in
which speakers give redundant messages. Journalof Psycholinguistic Research, 14(5):489?508.
Marie?t Theune, Ruud Koolen, Emiel Krahmer, and
Sander Wubben. 2011. Does size matter - how
much data is required to train a REG algorithm? InProceedings of the 49th Annual Meeting of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies, pages 660?664, Portland OR,
USA.
Kees van Deemter and Emiel Krahmer. 2007. Graphs
and Booleans: On the generation of referring ex-
pressions. In Harry C. Bunt and Reinhard Muskens,
editors, Computing Meaning, volume 3, pages 397?
422. Kluwer, Dordrecht, The Netherlands.
Kees van Deemter, Albert Gatt, Ielka van der Sluis,
and Richard Power. 2012. Generation of referring
expressions: Assessing the incremental algorithm.Cognitive Science, 36(5):799?836.
Ielka van der Sluis. 2005. Multimodal Reference, Stud-ies in Automatic Generation of Multimodal Refer-ring Expressions. Ph.D. thesis, Tilburg University,
The Netherlands.
Jette Viethen and Robert Dale. 2006. Algorithms for
generating referring expressions: Do they do what
people do? In Proceedings of the 4th InternationalConference on Natural Language Generation, pages
63?70, Sydney, Australia.
Jette Viethen and Robert Dale. 2008. The use of spatial
relations in referring expression generation. In Pro-ceedings of the 5th International Conference on Nat-ural Language Generation, pages 59?67, Salt Fork
OH, USA.
Jette Viethen and Robert Dale. 2009. Referring ex-
pression generation: What can we learn from human
data? In Proceedings of the 2009 Workshop on Pro-duction of Referring Expressions: Bridging the GapBetween Computational and Empirical Approachesto Reference, Amsterdam, The Netherlands.
Jette Viethen and Robert Dale. 2010. Speaker-
dependent variation in content selection for refer-
ring expression generation. In Proceedings of the8th Australasian Language Technology Workshop,
pages 81?89, Melbourne, Australia.
Jette Viethen, Robert Dale, Emiel Krahmer, Marie?t
Theune, and Pascal Touset. 2008. Controlling re-
dundancy in referring expressions. In Proceedingsof the 6th International Conference on LanguageResources and Evaluation, Marrakech, Morocco.
81
Proceedings of the 7th Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 11?19,
Sofia, Bulgaria, August 8 2013. c?2013 Association for Computational Linguistics
Using character overlap to improve language transformation
Sander Wubben
Tilburg University
P.O. Box 90135
5000 LE Tilburg
The Netherlands
s.wubben@uvt.nl
Emiel Krahmer
Tilburg University
P.O. Box 90135
5000 LE Tilburg
The Netherlands
e.j.krahmer@uvt.nl
Antal van den Bosch
Radboud University Nijmegen
P.O. Box 9103
6500 HD Nijmegen
The Netherlands
a.vandenbosch@let.ru.nl
Abstract
Language transformation can be defined
as translating between diachronically dis-
tinct language variants. We investigate the
transformation of Middle Dutch into Mod-
ern Dutch by means of machine transla-
tion. We demonstrate that by using char-
acter overlap the performance of the ma-
chine translation process can be improved
for this task.
1 Introduction
In this paper we aim to develop a system to para-
phrase between diachronically distinct language
variants. For research into history, historical lin-
guistics and diachronic language change, histori-
cal texts are of great value. Specifically from ear-
lier periods, texts are often the only forms of in-
formation that have been preserved. One prob-
lem that arises when studying these texts is the
difference between the language the text is writ-
ten in and the modern variant that the researchers
who want to study the texts know and speak them-
selves. It takes a great deal of deciphering and in-
terpretation to be able to grasp these texts. Our aim
is to facilitate scholars such as historians who do
not possess extensive knowledge of Middle Dutch
who are studying medieval texts. We do this by at-
tempting to generate literal translations of the sen-
tences in the text into modern language. In par-
ticular we focus on the task of translating Middle
Dutch to modern Dutch. The transformation be-
tween language variants, either synchronically or
diachronically, can be seen as a paraphrase and a
translation task, as it is often impossible to catego-
rize two languages as either variants or different
languages.
We define Middle Dutch as a collection of
closely related West Germanic dialects that were
spoken and written between 1150 and 1500 in the
area that is now defined as the Netherlands and
parts of Belgium. One of the factors that make
Middle Dutch difficult to read is the fact that at
the time no overarching standard language existed.
Modern Dutch is defined as Dutch as spoken from
1500. The variant we investigate is contempo-
rary Dutch. An important difference with regular
paraphrasing is the amount of parallel data avail-
able. The amount of parallel data for the vari-
ant pair Middle Dutch - Modern Dutch is sev-
eral orders of magnitude smaller than bilingual
parallel corpora typically used in machine trans-
lation (Koehn, 2005) or monolingual parallel cor-
pora used for paraphrase generation by machine
translation (Wubben et al, 2010).
We do expect many etymologically related
words to show a certain amount of character
overlap between the Middle and Modern variants.
An example of the data is given below, from the
work ?Van den vos Reynaerde? (?About Reynard
the Fox?), part of the Comburg manuscript that
was written between 1380-1425. Here, the first
text is the original text, the second text is a
modern translation in Dutch by Walter Verniers
and a translation in English is added below that
for clarity.
?Doe al dat hof versamet was,
Was daer niemen, sonder die das,
Hine hadde te claghene over Reynaerde,
Den fellen metten grijsen baerde.?
?Toen iedereen verzameld was,
was er niemand -behalve de das-
die niet kwam klagen over Reynaert,
die deugniet met zijn grijze baard.?
?When everyone was gathered,
there was noone -except the badger-
who did not complain about Reynaert,
that rascal with his grey beard.?
11
We can observe that although the two Dutch
texts are quite different, there is a large amount
of character overlap in the words that are used.
Our aim is to use this character overlap to compen-
sate for the lower amount of data that is available.
We compare three different approaches to trans-
late Middle Dutch into Modern Dutch: a standard
Phrase-Based machine translation (PBMT) ap-
proach, a PBMT approach with additional prepro-
cessing based on Needleman-Wunsch sequence
alignment, and a character bigram based PBMT
approach. The PBMT approach with preprocess-
ing identifies likely translations based on character
overlap and adds them as a dictionary to improve
the statistical alignment process. The PBMT ap-
proach based on character bigrams rather than
translating words, transliterates character bigrams
and in this way improves the transformation pro-
cess. We demonstrate that these two approaches
outperform standard PBMT in this task, and that
the PBMT transliteration approach based on char-
acter bigrams performs best.
2 Related work
Language transformation by machine translation
within a language is a task that has not been stud-
ied extensively before. Related work is the study
by Xu et al (2012). They evaluate paraphrase sys-
tems that attempt to paraphrase a specific style of
writing into another style. The plays of William
Shakespeare and the modern translations of these
works are used in this study. They show that their
models outperform baselines based on dictionar-
ies and out-of-domain parallel text. Their work
differs from our work in that they target writing
in a specific literary style and we are interested in
translating between diachronic variants of a lan-
guage.
Work that is slightly comparable is the work by
Zeldes (2007), who extrapolates correspondences
in a small parallel corpus taken from the Modern
and Middle Polish Bible. The correspondences are
extracted using machine translation with the aim
of deriving historical grammar and lexical items.
A larger amount of work has been published about
spelling normalization of historical texts. Baron
and Rayson (2008) developed tools for research in
Early Modern English. Their tool, VARD 2, finds
candidate modern form replacements for spelling
variants in historical texts. It makes use of a
dictionary and a list of spelling rules. By plug-
ging in other dictionaries and spelling rules, the
tool can be adapted for other tasks. Kestemont et
al. (2010) describe a machine learning approach
to normalize the spelling in Middle Dutch Text
from the 12th century. They do this by converting
the historical spelling variants to single canonical
(present-day) lemmas. Memory-based learning is
used to learn intra-lemma spelling variation. Al-
though these approaches normalize the text, they
do not provide a translation.
More work has been done in the area of trans-
lating between closely related languages and deal-
ing with data sparsity that occurs within these lan-
guage pairs (Hajic? et al, 2000; Van Huyssteen
and Pilon, 2009). Koehn et al (2003) have shown
that there is a direct negative correlation between
the size of the vocabulary of a language and the
accuracy of the translation. Alignment models
are directly affected by data sparsity. Uncommon
words are more likely to be aligned incorrectly to
other words or, even worse, to large segments of
words (Och and Ney, 2003). Out of vocabulary
(OOV) words also pose a problem in the trans-
lation process, as systems are unable to provide
translations for these words. A standard heuristic
is to project them into the translated sentence un-
translated.
Various solutions to data sparsity have been
studied, among them the use of part-of-speech
tags, suffixes and word stems to normalize
words (Popovic and Ney, 2004; De Gispert and
Marino, 2006), the treatment of compound words
in translation (Koehn and Knight, 2003), translit-
eration of names and named entities, and advanced
models that combine transliteration and transla-
tion (Kondrak et al, 2003; Finch et al, 2012)
or learn unknown words by analogical reason-
ing (Langlais and Patry, 2007).
Vilar et al (2007) investigate a way to han-
dle data sparsity in machine translation between
closely related languages by translating between
characters as opposed to words. The words in the
parallel sentences are segmented into characters.
Spaces between words are marked with a special
character. The sequences of characters are then
used to train a standard machine translation model
and a language model with n-grams up to n = 16.
They apply their system to the translation between
the related languages Spanish and Catalan, and
find that a word based system outperforms their
12
letter-based system. However, a combined sys-
tem performs marginally better in terms of BLEU
scores.
Tiedemann (2009) shows that combining
character-based translation with phrase-based
translation improves machine translation quality
in terms of BLEU and NIST scores when trans-
lating between Swedish and Norwegian if the
OOV-words are translated beforehand with the
character-based model.
Nakov and Tiedemann (2012) investigate the
use of character-level models in the translation be-
tween Macedonian and Bulgarian movie subtitles.
Their aim is to translate between the resource poor
language Macedonian to the related language Bul-
garian, in order to use Bulgarian as a pivot in or-
der to translate to other languages such as English.
Their research shows that using character bigrams
shows improvement over a word-based baseline.
It seems clear that character overlap can be used
to improve translation quality in related languages.
We therefore use character overlap in language
transformation between two diachronic variants of
a language.
3 This study
In this study we investigate the task of translating
from Middle Dutch to Modern Dutch. Similarly
to resource poor languages, one of the problems
that are apparent is the small amount of parallel
Middle Dutch - Modern Dutch data that is avail-
able. To combat the data sparseness we aim to use
the character overlap that exists between the Mid-
dle Dutch words and their Modern Dutch counter-
parts. Examples of overlap in some of the words
given in the example can be viewed in Table1. We
are interested in the question how we can use this
overlap to improve the performance of the transla-
tion model. We consider three approaches: (A)
Perform normal PBMT without any preprocess-
ing, (B) Apply a preprocessing step in which we
pinpoint words and phrases that can be aligned
based on character overlap and (C) perform ma-
chine translation not to words but to character bi-
grams in order to make use of the character over-
lap.
We will first discuss the PBMT baseline, fol-
lowed by the PBMT + overlap system and the
character bigram PBMT transliteration system in
Section 4. We then describe the experiment with
human judges in Section 6, and its results in Sec-
tion 7. We close with a discussion of our results in
Section 8.
Middle Dutch Modern Dutch
versamet verzameld
was was
niemen niemand
die de
das das
claghene klagen
over over
Reynaerde Reynaert
metten met zijn
grijsen grijze
baerde baard
Table 1: Examples of character overlap in words
from a fragment of ?Van den vos Reynaerde?
4 Language transformation Models
4.1 PBMT baseline
For our baseline we use the Moses software to
train a phrase based machine translation (PBMT)
model (Koehn et al, 2007). In general, a statistical
machine translation model normally finds a best
translation E? of a text in language F for a text
in language E by combining a translation model
P (F |E) with a language model P (E):
E? = arg max
E?E?
P (F |E)P (E)
In phrase-based machine translation the sen-
tence F is segmented into a sequence of I phrases
during decoding. Each source phrase is then trans-
lated into a target phrase to form sentence E.
Phrases may be reordered.
The GIZA++ statistical alignment package
(Och and Ney, 2003) is used to perform the word
alignments, which are later combined into phrase
alignments in the Moses pipeline to build the
language transformation model. GIZA++ imple-
ments IBM Models 1 to 5 and an HMM word
alignment model to find statistically motivated
alignments between words. We first tokenize our
data. We then lowercase all data and use all sen-
tences from the Modern Dutch part of the cor-
pus to train an n-gram language model with the
SRILM toolkit (Stolcke, 2002). Then we run the
GIZA++ aligner using the training pairs of sen-
tences in Middle Dutch and Modern Dutch. We
13
execute GIZA++ with standard settings and we
optimize using minimum error rate training with
BLEU scores. The Moses decoder is used to gen-
erate the translations.
4.2 PBMT with overlap-based alignment
Before using the Moses pipeline we perform a
preprocessing alignment step based on character
overlap. Word and phrase pairs that exhibit a large
amount of character overlap are added to the par-
allel corpus that GIZA++ is trained on. Every time
we find a phrase or word pair with large overlap it
is added to the corpus. This helps bias the align-
ment procedure towards aligning similar words
and reduces alignment errors. To perform the pre-
processing step we use the Needleman-Wunsch
algorithm (Needleman and Wunsch, 1970). The
Needleman-Wunsch algorithm is a dynamic pro-
gramming algorithm that performs a global align-
ment on two sequences. Sequence alignment is
a method to find commonalities in two (or more)
sequences of some items or characters. One of-
ten used example is the comparison of sequences
of DNA to find evolutionary differences and sim-
ilarities. Sequence alignment is also used in lin-
guistics, where it is applied to finding the longest
common substring or the differences or similari-
ties between strings.
The Needleman-Wunsch algorithm is a se-
quence alignment algorithm that optimizes a score
function to find an optimal alignment of a pair of
sequences. Each possible alignment is scored ac-
cording to the score function, where the alignment
giving the highest similarity score is the optimal
alignment of a pair of sequences. If more than
one alignment yields the highest score, there are
multiple optimal solutions. The algorithm uses
an iterative matrix to calculate the optimal solu-
tion. All possible pairs of characters containing
one character from each sequence are plotted in
a 2-dimensional matrix. Then, all possible align-
ments between those characters can be represented
by pathways through the matrix. Insertions and
deletions are allowed, but can be penalized by
means of a gap penalty in the alignment.
The first step is to initialize the matrix and fill in
the gap scores in the top row and leftmost column.
In our case we heuristically set the values of the
scores to +1 for matches, -2 for mismatches and
-1 for gaps after evaluating on our development
set. After initialization, we can label the cells
in the matrix C(i, j) where i = 1, 2, ..., N and
j = 1, 2, ...,M , the score of any cell C(i, j) is
then the maximum of:
qdiag = C(i? 1, j ? 1) + s(i, j)
qdown = C(i? 1, j) + g
qright = C(i, j ? 1) + g
Here, s(i, j) is the substitution score for let-
ters i and j, and g is the gap penalty score. If i
and j match, the substitution score is in fact the
matching score. The table is filled this way recur-
sively, filling each cell with the maximum score of
the three possible options (diagonally, down and
right). After this is done, an optimal path can
be found by performing a traceback, starting in
the lower right corner of the table and ending in
the upper left corner, by visiting cells horizontally,
vertically or diagonally, but only those cells with
the highest score. After this process we end up
with an alignment.
We use the Needleman-Wunsch algorithm to
find an optimal alignment of the Middle Dutch
- Modern Dutch sentence pairs. We regard each
line as a sentence. In case of rhyming text, a
frequent phenomenon in Middle Dutch text, lines
are usually parts of sentences. We then consider
each line a string, and we try to align as many
characters and whitespaces to their equivalents in
the parallel line. We split the aligned sentences
in each position where two whitespaces align and
we consider the resulting aligned words or phrases
as alignments. For each aligned word or phrase
pair we calculate the Jaccard coefficient and if that
is equal or higher than a threshold we add the
aligned words or phrases to the training material.
We heuristically set this threshold to 0.5. By using
this method we already can find many-to-one and
one-to-many alignments. In this way we help the
GIZA++ alignment process by biasing it towards
aligning words and phrases that show overlap. Ta-
ble 2 illustrates this process for two lines.
4.3 Character bigram transliteration
Another somewhat novel approach we propose
for Language Transformation is Character-based
transliteration. To circumvent the problem of
14
Middle Dutch: hine hadde+ ++te claghene over Reynaerde ,
Modern Dutch: di+e ++niet kwam klag+en+ over Reynaer+t ,
Jaccard 0.4 0.14 0 0.63 1 0.70 1
Middle Dutch: +den fe++llen met++ten grijsen baerde .
Modern Dutch: die+ deugniet met zijn grijze+ baard+ .
Jaccard 0.50 0.09 0.50 0.71 0.8 1
Table 2: Alignment of lines with Jaccard scores for the aligned phrases. A + indicates a gap introduced
by the Needleman Wunsch alignment.
OOV-words and use the benefits of character over-
lap more directly in the MT system, we build
a translation model based on character bigrams,
similar to (Nakov and Tiedemann, 2012). Where
they use this approach to translate between closely
related languages, we use it to translate between
diachronic variants of a language. The sentences
in the parallel corpus are broken into charac-
ter bigrams, with a special character representing
whitespaces. These bigrams are used to train the
translation model and the language model. An ex-
ample of the segmentation process is displayed in
Table 3. We train an SRILM language model on
the character bigrams and model sequences of up
to 10 bigrams. We then run the standard Moses
pipeline, using GIZA++ with standard settings to
generate the phrase-table and we use the Moses
decoder to decode the bigram sequences. A num-
ber of sample entries are shown in Table 4. As a
final step, we recombine the bigrams into words.
The different sizes of the Phrase-table for the dif-
ferent approaches can be observed in Table 5.
original segmented
Hine #H Hi in ne e#
hadde #h ha ad dd de e#
te #t te e#
claghene #c cl la ag gh he en ne e#
over #o ov ve er r#
Reynaerde #R Re ey yn na ae er rd de e#
, #, ,#
Table 3: Input and output of the character bigram
segmenter
5 Data Set
Our training data consists of various Middle Dutch
literary works with their modern Dutch transla-
tion. A breakdown of the different works is in
#d da at t# en n# #d da aa ar
#d da at t# et t# #s st
#d da at t# et t# #s
#d da at t# ie et t# #s st
#d da at t# ie et t# #s
#d da at t# la an n#
#d da at t# le et t#
#d da at t# n# #d da aa ar ro
#d da at t# n# #d da aa ar
#d da at t# n#
#d da at t# rd da at t#
#d da at ts s# #d da at t#
#d da at ts si i# #h he eb bb be en
#d da at ts #d da at t#
#d da at ts #w wa at t#
#d da at tt tu #w wa at t# #j
Table 4: Example entries from the character bi-
gram Phrase-table, without scores.
system lines
PBMT 20,092
PBMT + overlap 27,885
character bigram transliteration 93,594
Table 5: Phrase-table sizes of the different models
Table 6. All works are from the Digital Library of
Dutch Literature1. ?Middle Dutch? is a very broad
definition. It encompasses all Dutch language spo-
ken and written between 1150 and 1500 in the
Netherlands and parts of Belgium. Works stem-
ming from different centuries, regions and writers
can differ greatly in their orthography and spelling
conventions. No variant of Dutch was considered
standard or the norm; Middle Dutch can be con-
sidered a collection of related lects (regiolects, di-
alects). This only adds to the problem of data
1http://www.dbnl.org
15
sparsity. Our test set consists of a selection of
sentences from the Middle Dutch work Beatrijs,
a Maria legend written around 1374 by an anony-
mous author.
source text lines date of origin
Van den vos Reynaerde 3428 around 1260
Sint Brandaan 2312 12th century
Gruuthuuse gedichten 224 around 1400
?t Prieel van Trojen 104 13th century
Various poems 42 12th-14th cent.
Table 6: Middle Dutch works in the training set
Middle Si seide: ?Ic vergheeft u dan.
Dutch Ghi sijt mijn troest voer alle man
Modern Ze zei: ?ik vergeef het je dan.
Dutch Je bent voor mij de enige man
PBMT Ze zei : ? Ik vergheeft u dan .
Gij ze alles in mijn enige voor al man
PBMT + Ze zei : ? Ik vergheeft u dan .
Overlap dat ze mijn troest voor al man
Char. Bigram Ze zeide : ? Ik vergeeft u dan .
PBMT Gij zijt mijn troost voor alle man
Middle Dat si daer snachts mochte bliven.
Dutch ?Ic mocht u qualijc verdriven,?
Modern omdat ze nu niet verder kon reizen.
Dutch ?Ik kan u echt de deur niet wijzen,?
PBMT dat ze daar snachts kon bliven .
? Ik kon u qualijc verdriven , ?
PBMT + dat ze daar s nachts kon blijven .
Overlap ? Ik kon u qualijc verdriven , ?
Char. Bigram dat zij daar snachts mocht blijven .
PBMT ? Ik mocht u kwalijk verdrijven ,
Table 7: Example output
6 Experiment
In order to evaluate the systems, we ran an exper-
iment to collect human rankings of the output of
the systems. We also performed automatic evalu-
ation.
6.1 Materials
Because of the nature of our data, in which sen-
tences often span multiple lines, it is hard to eval-
uate the output on the level of separate lines. We
therefore choose to evaluate pairs of lines. We ran-
domly choose a line, and check if it is part of a
sensible sentence that can be understood without
more context. If that is the case, we select it to in-
clude in our test set. In this way we select 25 pairs
of lines. We evaluate the translations produced by
the three different systems for these sentences. Ex-
amples of the selected sentences and the generated
corresponding output are displayed in Table 7.
6.2 Participants
The participants in this evaluation study were 22
volunteers. All participants were native speakers
of Dutch, and participated through an online in-
terface. All participants were adults, and 12 were
male and 10 female. In addition to the 22 partici-
pants, one expert in the field of Middle Dutch also
performed the experiment, in order to be able to
compare the judgements of the laymen and the ex-
pert.
6.3 Procedure
The participants were asked to rank three different
automatic literal translations of Middle Dutch text.
For reference, they were also shown a modern (of-
ten not literal) translation of the text by Dutch au-
thor Willem Wilmink. The order of items to judge
was randomized for each participant, as well as the
order of the output of the systems for each sen-
tence. The criterium for ranking was the extent to
which the sentences could be deciphered and un-
derstood. The participants were asked to always
provide a ranking and were not allowed to assign
the same rank to multiple sentences. In this way,
each participant provided 25 rankings where each
pair of sentences received a distinct rank. The pair
with rank 1 is considered best and the pair with 3
is considered worst.
system mean rank 95 % c. i.
PBMT 2.44 (0.03) 2.38 - 2.51
PBMT + Overlap 2.00 (0.03) 1.94 - 2.06
char. bigram PBMT 1.56 (0.03) 1.50 - 1.62
Table 8: Mean scores assigned by human subjects,
with the standard error between brackets and the
lower and upper bound of the 95 % confidence in-
terval
7 Results
7.1 Human judgements
In this section we report on results of the exper-
iment with judges ranking the output of the sys-
tems. To test for significance of the difference
in the ranking of the different systems we ran re-
peated measures analyses of variance with system
(PBMT, PBMT + Overlap, character bigram MT)
as the independent variable, and the ranking of the
output as the dependent variable. Mauchly?s test
for sphericity was used to test for homogeneity of
16
PBMT PBMT +
overlap
char.
bigram
PBMT
X2
2.05 2.59 1.36 16.636**
2.77 1.82 1.41 21.545**
2.50 1.27 2.23 18.273**
1.95 1.45 2.59 14.273**
2.18 2.36 1.45 10.182**
2.45 2.00 1.55 9.091*
2.91 1.77 1.32 29.545**
2.18 2.27 1.55 6.903*
2.14 2.00 1.86 0.818
2.27 1.73 2.00 3.273
2.68 1.68 1.64 15.364**
2.82 1.95 1.23 27.909**
2.68 2.09 1.23 23.545**
1.95 2.55 1.50 12.091**
2.77 1.86 1.36 22.455**
2.32 2.23 1.45 9.909**
2.86 1.91 1.23 29.727**
2.18 1.09 2.73 30.545**
2.05 2.09 1.86 0.636
2.73 2.18 1.09 30.545**
2.41 2.27 1.32 15.545**
2.68 2.18 1.14 27.364**
1.82 2.95 1.23 33.909**
2.73 1.95 1.32 21.909**
2.91 1.77 1.32 29.545**
Table 9: Results of the Friedman test on each of
the 25 sentences. Results marked * are significant
at p < 0.05 and results marked ** are significant
at p < 0.01
variance, but was not significant, so no corrections
had to be applied. Planned pairwise comparisons
were made with the Bonferroni method. The mean
ranking can be found in Table 8 together with the
standard deviation and 95 % confidence interval.
We find that participants ranked the three systems
differently, F (2, 42) = 135, 604, p < .001, ?2p =
.866. All pairwise comparisons are significant at
p < .001. The character bigram model receives
the best mean rank (1.56), then the PBMT + Over-
lap system (2.00) and the standard PBMT system
is ranked lowest (2.44). We used a Friedman test
to detect differences across multiple rankings. We
ran the test on each of the 25 K-related samples,
and found that for 13 sentences the ranking pro-
vided by the test subjects was equal to the mean
ranking: the PBMT system ranked lowest, then the
PBMT + Overlap system and the character bigram
system scored highest for each of these cases at
p < .005. These results are detailed in Table 9.
When comparing the judgements of the partici-
pants with the judgements of an expert, we find
a significant medium Pearson correlation of .65
(p < .001) between the judgements of the expert
and the mean of the judgements of the participants.
This indicates that the judgements of the laymen
are indeed useful.
7.2 Automatic judgements
In order to attempt to measure the quality of the
transformations made by the different systems au-
tomatically, we measured NIST scores by compar-
ing the output of the systems to the reference trans-
lation. We do realize that the reference translation
is in fact a literary interpretation and not a literal
translation, making automatic assessment harder.
Having said that, we still hope to find some effect
by using these automatic measures. We only re-
port NIST scores here, because BLEU turned out
to be very uninformative. In many cases sentences
would receive a BLEU score of 0. Mauchly?s test
for sphericity was used to test for homogeneity of
variance for the NIST scores, and was not signifi-
cant. We ran a repeated measures test with planned
pairwise comparisons made with the Bonferroni
method. We found that the NIST scores for the dif-
ferent systems differed significantly (F (2, 48) =
6.404, p < .005, ?2p = .211). The average NIST
scores with standard error and the lower and up-
per bound of the 95 % confidence interval can be
seen in Table 10. The character bigram translit-
eration model scores highest with 2.43, followed
by the PBMT + Overlap model with a score of
2.30 and finally the MT model scores lowest with
a NIST score of 1.95. We find that the scores
for the PBMT model differ significantly from the
PBMT + Overlap model (p < .01) and the charac-
ter bigram PBMT model (p < .05), but the scores
for the PBMT + Overlap and the character bigram
PBMT model do not differ significantly. When
we compare the automatic scores to the human as-
signed ranks we find no significant Pearson corre-
lation.
8 Conclusion
In this paper we have described two modifications
of the standard PBMT framework to improve the
transformation of Middle Dutch to Modern Dutch
17
system mean NIST 95 % c. i.
PBMT 1.96 (0.18) 1.58 - 2.33
PBMT + overlap 2.30 (0.21) 1.87 - 2.72
char. bigram PBMT 2.43 (0.20) 2.01 - 2.84
Table 10: Mean NIST scores, with the standard
error between brackets and the lower and upper
bound of the 95 % confidence interval
by using character overlap in the two variants. We
described one approach that helps the alignment
process by adding words that exhibit a certain
amount of character overlap to the parallel data.
We also described another approach that translates
sequences of character bigrams instead of words.
Reviewing the results we conclude that the use of
character overlap between diachronic variants of a
language is beneficial in the translation process.
More specifically, the model that uses character
bigrams in translation instead of words is ranked
best. Also ranked significantly better than a stan-
dard Phrase Based machine translation approach
is the approach using the Needleman-Wunsch al-
gorithm to align sentences and identify words or
phrases that exhibit a significant amount of char-
acter overlap to help the GIZA++ statistical align-
ment process towards aligning the correct words
and phrases. We have seen that one issue that
is encountered when considering the task of lan-
guage transformation from Middle Dutch to Mod-
ern Dutch is data sparseness. The number of lines
used to train on amounts to a few thousand, and
not millions as is more common in SMT. It is
therefore crucial to use the inherent character over-
lap in this task to compensate for the lack of data
and to make more informed decisions. The char-
acter bigram approach is able to generate a trans-
lation for out of vocabulary words, which is also
a solution to the data sparseness problem. One
area where the character bigram model often fails,
is translating Middle Dutch words into Modern
Dutch words that are significantly different. One
example can be seen in Table 7, where ?mocht?
is translated by the PBMT and PBMT + Overlap
systems to ?kon? and left the same by the charac-
ter bigram transliteration model. This is probably
due to the fact that ?mocht? still exists in Dutch,
but is not as common as ?kon? (meaning ?could?).
another issue to consider is the fact that the char-
acter bigram model learns character mappings that
are occurring trough out the language. One exam-
ple is the disappearing silent ?h? after a ?g?. This
often leads to transliterated words of which the
spelling is only partially correct. Apparently the
human judges rate these ?half words? higher than
completely wrong words, but automatic measures
such as NIST are insensitive to this.
We have also reported the NIST scores for the
output of the standard PBMT approach and the
two proposed variants. We see that the NIST
scores show a similar patterns as the human judge-
ments: the PBMT + Overlap and character bigram
PBMT systems both achieve significantly higher
NIST scores than the normal PBMT system. How-
ever, the PBMT + Overlap and character bigram
PBMT models do not differ significantly in scores.
It is interesting that we find no significant corre-
lation between human and automatic judgements,
leading us to believe that automatic judgements
are not viable in this particular scenario. This is
perhaps due to the fact that the reference transla-
tions the automatic measures rely on are in this
case not literal translations but rather more loosely
translated literary interpretations of the source text
in modern Dutch. The fact that both versions are
written on rhyme only worsens this problem, as
the author of the Modern Dutch version is often
very creative.
We think techniques such as the ones described
here can be of great benefit to laymen wishing to
investigate works that are not written in contem-
porary language, resulting in improved access to
these older works. Our character bigram translit-
eration model may also play some role as a com-
putational model in the study of the evolution of
orthography in language variants, as it often will
generate words that are strictly speaking not cor-
rect, but do resemble Modern Dutch in some way.
Automatic evaluation is another topic for future
work. It would be interesting to see if an automatic
measure operating on the character level correlates
better with human judgements.
References
Alistair Baron and Paul Rayson. 2008. VARD 2: A
tool for dealing with spelling variation in historical
corpora. In Proceedings of the Postgraduate Confer-
ence in Corpus Linguistics, Birmingham, UK. Aston
University.
A. De Gispert and J. B. Marino. 2006. Linguistic
knowledge in statistical phrase-based word align-
ment. Natural Language Engineering, 12(1):91?
108.
18
Andrew Finch, Paul Dixon, and Eiichiro Sumita. 2012.
Rescoring a phrase-based machine transliteration
system with recurrent neural network language mod-
els. In Proceedings of the 4th Named Entity Work-
shop (NEWS) 2012, pages 47?51, Jeju, Korea, July.
Association for Computational Linguistics.
Jan Hajic?, Jan Hric, and Vladislav Kubon?. 2000. Ma-
chine translation of very close languages. In Pro-
ceedings of the sixth conference on Applied natural
language processing, pages 7?12. Association for
Computational Linguistics.
Mike Kestemont, Walter Daelemans, and Guy
De Pauw. 2010. Weigh your words?memory-
based lemmatization for Middle Dutch. Literary
and Linguistic Computing, 25(3):287?301, Septem-
ber.
Philipp Koehn and Kevin Knight. 2003. Feature-rich
statistical translation of noun phrases. In ACL ?03:
Proceedings of the 41st Annual Meeting on Asso-
ciation for Computational Linguistics, pages 311?
318, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
Philip Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology-
Volume 1, pages 48?54. Association for Computa-
tional Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris C.
Burch, Marcello Federico, Nicola Bertoldi, Brooke
Cowan, Wade Shen, Christine Moran, Richard Zens,
Chris Dyer, Ondrej Bojar, Alexandra Constantin,
and Evan Herbst. 2007. Moses: Open source toolkit
for statistical machine translation. In ACL. The As-
sociation for Computer Linguistics.
P. Koehn. 2005. Europarl: A parallel corpus for statis-
tical machine translation. In MT summit, volume 5.
Grzegorz Kondrak, Daniel Marcu, and Kevin Knight.
2003. Cognates can improve statistical translation
models. In HLT-NAACL.
Philippe Langlais and Alexandre Patry. 2007. Trans-
lating unknown words by analogical learning. In
EMNLP-CoNLL, pages 877?886. ACL.
Preslav Nakov and Jo?rg Tiedemann. 2012. Combin-
ing word-level and character-level models for ma-
chine translation between closely-related languages.
In Proceedings of the 50th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 2:
Short Papers), pages 301?305, Jeju Island, Korea,
July. Association for Computational Linguistics.
S. B. Needleman and C. D. Wunsch. 1970. A general
method applicable to the search for similarities in
the amino acid sequence of two proteins. Journal of
molecular biology, 48(3):443?453, March.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Maja Popovic and Hermann Ney. 2004. Towards the
use of word stems and suffixes for statistical ma-
chine translation. In LREC. European Language Re-
sources Association.
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In In Proc. Int. Conf. on
Spoken Language Processing, pages 901?904, Den-
ver, Colorado.
Jo?rg Tiedemann. 2009. Character-based PSMT for
closely related languages. In Llu??s Marque?s and
Harold Somers, editors, Proceedings of 13th An-
nual Conference of the European Association for
Machine Translation (EAMT?09), pages 12 ? 19,
Barcelona, Spain, May.
Gerhard B Van Huyssteen and Sule?ne Pilon. 2009.
Rule-based conversion of closely-related languages:
a dutch-to-afrikaans convertor. 20th Annual Sympo-
sium of the Pattern Recognition Association of South
Africa, December.
David Vilar, Jan-Thorsten Peter, and Hermann Ney.
2007. Can we translate letters? In Second Work-
shop on Statistical Machine Translation, pages 33?
39, Prague, Czech Republic, jun. Association for
Computational Linguistics.
Sander Wubben, Antal van den Bosch, and Emiel
Krahmer. 2010. Paraphrase generation as mono-
lingual translation: Data and evaluation. In
B. Mac Namee J. Kelleher and I. van der Sluis, ed-
itors, Proceedings of the 10th International Work-
shop on Natural Language Generation (INLG
2010), pages 203?207, Dublin.
Wei Xu, Alan Ritter, Bill Dolan, Ralph Grishman, and
Colin Cherry. 2012. Paraphrasing for style. In
COLING, pages 2899?2914.
Amir Zeldes. 2007. Machine translation between lan-
guage stages: Extracting historical grammar from a
parallel diachronic corpus of Polish. In Matthew
Davies, Paul Rayson, Susan Hunston, and Pernilla
Danielsson, editors, Proceedings of the Corpus Lin-
guistics Conference CL2007. University of Birming-
ham.
19
