Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 873?880
Manchester, August 2008
A Framework for Identifying Textual Redundancy
Kapil Thadani and Kathleen McKeown
Department of Computer Science,
Columbia University,
New York, NY USA
{kapil,kathy}@cs.columbia.edu
Abstract
The task of identifying redundant infor-
mation in documents that are generated
from multiple sources provides a signifi-
cant challenge for summarization and QA
systems. Traditional clustering techniques
detect redundancy at the sentential level
and do not guarantee the preservation of
all information within the document. We
discuss an algorithm that generates a novel
graph-based representation for a document
and then utilizes a set cover approximation
algorithm to remove redundant text from it.
Our experiments show that this approach
offers a significant performance advantage
over clustering when evaluated over an an-
notated dataset.
1 Introduction
This paper approaches the problem of identifying
and reducing redundant information in documents
that are generated from multiple sources. This task
is closely related to many well-studied problems
in the field of natural language processing such as
summarization and paraphrase recognition. Sys-
tems that utilize data from multiple sources, such
as question-answering and extractive summariza-
tion systems that operate on news data, usually in-
clude a component to remove redundant informa-
tion from appearing in their generated output.
However, practical attempts at reducing redun-
dancy in the output of these types of systems usu-
ally involve clustering the sentences of the gener-
ated output, picking a representative sentence from
each cluster and discarding the rest. Although
this strategy would remove some redundant in-
formation, clustering approaches tuned for coarse
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
matches could also remove non-redundant infor-
mation whereas clustering approaches tuned for
near-exact matches could end up removing very
little repeated information. This is simply a con-
sequence of the fact that information can, and usu-
ally does, exist at the sub-sentential level and that
clusters of sentences don?t necessarily correspond
to clusters of information.
In this paper, we discuss a framework for build-
ing a novel graph-based representation to detect re-
dundancy within documents. We identify redun-
dancy at the sub-sentential level through pairwise
alignment between the sentences of a document
and use this to build a bipartite graph which en-
ables us to keep track of redundant information
across all sentences. Common information be-
tween pairs of sentences, detected with the align-
ment algorithm, can be extrapolated to document-
wide units of information using the graph struc-
ture. Individual sentences that are encompassed
by the information in the rest of the document can
then be identified and removed efficiently by us-
ing a well-known greedy algorithm adapted for this
representation.
2 Related Work
The challenge of minimizing redundant informa-
tion is commonly faced by IR engines and extrac-
tive summarization systems when generating their
responses. A well-known diversity-based rerank-
ing technique for these types of systems is MMR
(Carbonell and Goldstein, 1998), which attempts
to reduce redundancy by preferring sentences that
differ from the sentences already selected for the
summary. However, this approach does not at-
tempt to identify sub-sentential redundancy.
Alternative approaches to identifying redun-
dancy use clustering at the sentence level (Lin and
Hovy, 2001) to remove sentences that are largely
repetitive; however, as noted earlier, this is not
well-suited to the redundancy task. The use of sen-
873
tence simplification in conjunction with clustering
(Siddharthan et al, 2004) could help alleviate this
problem by effectively clustering smaller units, but
this issue cannot be avoided unless sentences are
simplified to atomic elements of information.
Other research has introduced the notion of
identifying concepts in the input text (Filatova and
Hatzivassiloglou, 2004), using a set cover algo-
rithm to attempt to include as many concepts as
possible. However, this approach uses tf-idf to
approximate concepts and thus doesn?t explicitly
identify redundant text. Our work draws on this
approach but extends it to identify all detectable
redundancies within a document set.
Another approach does identify small sub-
sentential units of information within text called
?Basic Elements? and uses these for evaluating
summarizations (Hovy et al, 2006). Our approach,
in contrast, does not make assumptions about the
size or structure of redundant information since
this is uncovered through alignments.
We thus require the use of an alignment algo-
rithm to extract the common information between
two pieces of text. This is related to the well-
studied problem of identifying paraphrases (Barzi-
lay and Lee, 2003; Pang et al, 2003) and the more
general variant of recognizing textual entailment,
which explores whether information expressed in
a hypothesis can be inferred from a given premise.
Entailment problems have also been approached
with a wide variety of techniques, one of which
is dependency tree alignment (Marsi et al, 2006),
which we utilize as well to align segments of text
while respecting syntax. However, our definition
of redundancy does not extend to include unidi-
rectional entailment, and the alignment process is
simply required to identify equivalent information.
3 Levels of Information
In describing the redundancy task, we deal with
multiple levels of semantic abstraction from the
basic lexical form. This section describes the ter-
minology used in this paper and the graph-based
representation that is central to our approach.
3.1 Terminology
The following terms are used throughout this paper
to refer to different aspects of a document.
Snippet: This is any span of text in the doc-
ument and is a lexical realization of information.
While a snippet generally refers to a single sen-
tence within a document, it can apply to multiple
sentences or phrases within sentences. Since re-
dundancy will be reduced by removing whole snip-
pets, a snippet can be defined as the smallest unit
of text that can be dropped from a document for
the purpose of reducing redundancy.
To illustrate the levels of information that we
consider, consider the following set of short sen-
tences as snippets. Although this is a synthe-
sized example to simplify presentation, sentences
with this type of overlapping information occur
frequently in the question-answering scenario over
news in which our approach has been used.
x
1
: Whittington is an attorney.
x
2
: Cheney shot Whittington, a lawyer.
x
3
: Whittington, an attorney, was shot in Texas.
x
4
: Whittington was shot by Cheney while hunting quail.
x
5
: This happened during a quail hunt in Texas.
We can see that all the information in x
1
is con-
tained in both x
2
and x
3
. While no other snip-
pet is completely subsumed by any single snippet,
they can be made redundant given combinations of
other snippets; for example, x
4
is redundant given
x
2
, x
3
and x
5
. In order to identify these combina-
tions, we need to identify the elements of informa-
tion within each snippet.
Concept: This refers to a basic unit of informa-
tion within a document. Concepts may be facts,
opinions or details. These are not necessarily se-
memes, which are atomic units of meaning, but
simply units of information that are seen as atomic
within the document. We further restrict the defi-
nition of a concept to a unit of information seen in
more than one snippet, since we are only interested
in concepts which help in identifying redundancy.
Formally, a document can be defined as a set of
S snippets X = {x
1
, . . . ,x
S
}, which is a literal
representation of the document. However, it can
also be defined in terms of its information content.
We use Z = {z
1
, . . . , z
C
} to represent the set of C
concepts that cover all information appearing more
than once in the document. In the example above,
we can identify five non-overlapping concepts:
z
A
: Whittington was shot
z
B
: Whittington is an attorney
z
C
: The shooting occurred in Texas
z
D
: It happened during a hunt for quail
z
E
: Cheney was the shooter
We use subscripts for snippet indices and super-
scripts for concept indices throughout this paper.
874
Nugget: This is a textual representation of a
concept in a snippet and therefore expresses some
information which is also expressed elsewhere in
the document. Different nuggets for a given con-
cept may have unique lexico-syntactic realizations,
as long as they all embody the same semantics.
With regard to the notation used above, nuggets
can be represented by an S ? C matrix Y where
each y
c
s
denotes the fragment of text (if any) from
the snippet x
s
that represents concept z
c
.
Since a concept itself has no unique textual re-
alization, it can be simply represented by the com-
bination of all its nuggets. For instance, in the ex-
ample shown above, concept z
D
is seen in both x
4
and x
5
in the form of two nuggets y
D
4
(?... while
hunting quail?) and y
D
5
(?... during a quail hunt?),
which are paraphrases. The degree to which we
can consider this and other types of lexical or syn-
tactic differences between nuggets that have the
same semantic identity depends on the alignment
algorithm used.
Intersection: This is the common information
between two snippets that can be obtained through
their alignment. For example, the intersection
from the alignment between x
2
and x
4
consists
of two fragments of text that express that Cheney
shot Whittington (an active-voiced fragment from
x
2
and a passive-voiced fragment from x
4
).
In general, aligning x
i
and x
j
produces an in-
tersection v
i,j
which is simply a pair of aligned
text fragments covering the set of concepts that x
i
and x
j
have in common. However, these undi-
vided segments of text may actually contain mul-
tiple nuggets from a document-wide perspective.
We assume that intersections can be decomposed
into smaller intersections through further align-
ments with snippets or other intersections; this pro-
cess is explained in Subsection 4.3.
3.2 Concept graph representation
Figure 1 illustrates the example introduced in Sub-
section 3.1 as a network with intersections repre-
sented as edges between snippets. This is the type
of graph that would be built using pairwise align-
ments between all snippets. Note that although
some intersections such as v
1,2
(between x
1
and
x
2
) and v
3,5
express concepts directly, other inter-
sections such as v
2,3
and v
2,4
are undivided com-
binations of concepts. Since we cannot directly
identify concepts and their nuggets, this graph is
not immediately useful for reducing redundancy.
x
1
B
x
2
ABE
x
3
ABC
x
4
ADE
x
5
CD
B
B
AB
AE
A
C
D
Figure 1: Graph representing pairwise alignments
between the example snippets from Section 3. For
clarity, alphabetic labels like A represent concepts
z
A
. Node labels show concepts within snippets and
edge labels indicate concepts seen in intersections.
x
1
B
x
2
ABE
x
3
ABC
x
4
ADE
x
5
CD
z
A
z
B
z
C
z
D
z
E
y
B
1
y
A
2
y
B
2
y
E
2
y
A
3
y
B
3
y
C
3
y
A
4
y
D
4
y
E
4
y
C
5
y
D
5
Figure 2: Structure of the equivalent concept graph
for the example document illustrated in Figure 1.
Circular nodes x
s
represent snippets, large squares
z
c
represent concepts and small squares y
c
s
depict
nuggets for each concept within a snippet.
Now, since the matrix Y describes the interac-
tion of concepts with snippets, it can be viewed as
an incidence matrix that defines a bipartite graph
between snippets and concepts with nuggets rep-
resenting the edges. In this concept graph repre-
sentation, each snippet can connect to any number
of other snippets via a shared concept. Since con-
cepts serve to connect multiple snippets together,
the concept graph can also be seen as a hypergraph,
which is a generalization of a graph in which each
edge may connect together multiple vertices.
875
Figure 2 illustrates the structure of the equiva-
lent concept graph for the previous example. This
is simply the bipartite graph with the two types of
nodes, namely snippets and concepts, represented
using different symbols. For clarity, nuggets are
also depicted as nodes in the graph, thereby reduc-
ing edges to simple links indicating membership.
This representation identifies the redundancy be-
tween snippets in terms of non-overlapping con-
cepts and is therefore more useful than the graph
from Figure 1 for reducing redundancy.
4 Constructing the Concept Graph
We now describe how a concept graph can be con-
structed from a document by using dependency
tree alignment and leveraging the existing struc-
ture of the graph during construction.
4.1 Alignment of snippets
In order to obtain the concept graph representation,
the common information between each pair of
snippets in the document must first be discovered
by aligning all pairs of snippets with each other.
We make use of dependency parsing and alignment
of dependency parse trees to obtain intersections
between each pair of snippets, where each inter-
section may be a discontiguous span of text corre-
sponding to an aligned subtree within each snip-
pet. In our experiments, dependency parsing is
accomplished with Minipar (Lin, 1998) and align-
ment is done using a bottom-up tree alignment al-
gorithm (Barzilay and McKeown, 2005) modified
to account for the shallow semantic role labels pro-
duced by the parser. The alignment implementa-
tion is not the focus of this work, however, and the
framework described here could by applied using
any alignment technique between segments of text
in potentially any language.
As seen in Figure 1, the intersections that can be
extracted solely by pairwise comparisons are not
unique and may contain multiple concepts. A truly
information-preserving approach requires the ex-
plicit identification of concepts as in the concept
graph from Figure 2, but efficiently converting the
former into the latter poses a non-trivial challenge.
4.2 Extraction of irreducible concepts
Our approach attempts to obtain a set of irre-
ducible concepts such that each concept in this set
cannot wholly or partially contain any other con-
cept in the set (thereby conforming to the defini-
tion of a concept in Subsection 3.1).
We attempt to build the concept graph and main-
tain irreducible concepts alongside each of the
S(S ? 1)/2 pairwise alignment steps. Every in-
tersection found by aligning a pair of snippets is
assumed to represent some concept that these snip-
pets share; it is then compared with existing con-
cepts and is decomposed into smaller intersections
if it overlaps partially with any one of them. This
implies a worst-case of C comparisons at each
pairwise alignment step (2C if both fragments of
an intersection are compared separately). How-
ever, this can be made more efficient by exploiting
the structure of the graph. A new intersection only
has to be compared with concepts which might be
affected by it and only affects the other snippets
containing these concepts. We can show that this
leads to an algorithm that requires fewer than C
comparisons, and additionally, that these compar-
isons can be performed efficiently.
Consider the definition of alignment along the
lines of a mathematical relation. We require snip-
pet algnment to be an equivalence relation and it
therefore must have the following properties.
Symmetry: If an intersection v
i,j
contains a
concept z
?
, then v
j,i
will also contain z
?
. This
property allows only S(S ? 1)/2 alignments to
suffice instead of the full S(S ? 1). Therefore,
without loss of generality, we can specify that all
alignments between x
i
and x
j
should have i < j.
Transitivity: If intersections v
i,j
and v
j,k
both
contain some concept z
?
, then v
i,k
will also con-
tain z
?
. This property leads to an interesting con-
sequence. Assuming we perform alignments in
order (initially aligning x
1
and x
2
and iterating
for j within each i), we observe that x
i
has been
aligned with snippets {x
1
, . . . ,x
j?1
} and, for any
i > 1, snippets {x
1
, . . . ,x
i?1
} were aligned with
all snippets {x
1
, . . . ,x
S
}. Since i < j, this im-
plies that x
i
was directly aligned with snippets
{x
1
, . . . ,x
i?1
} which in turn were each aligned
with all S snippets. Therefore, due to the prop-
erty of transitivity, all concepts contained in a
new intersection v
i,j
that also exist in the partly-
constructed graph would already be directly asso-
ciated with x
i
. Note that this does not hold for
x
j
as well, since x
j
has not been aligned with
{x
i+1
, . . . ,x
j?1
}; therefore, it may not have en-
countered all relevant concepts.
This implies that for any i and j, all concepts
that might be affected by a new intersection v
i,j
876
have already been uncovered in x
i
and thus v
i,j
only needs to be compared to these concepts.
4.3 Comparisons after alignment
For every new intersection v
i,j
produced by an
alignment between x
i
and x
j
, the algorithm com-
pares it (specifically, the fragment from x
i
) with
each existing nugget y
k
i
for each concept z
k
al-
ready seen in x
i
. Checking for the following cases
ensures that the graph structure contains only irre-
ducible concepts for all the alignments seen:
1. If v
i,j
doesn?t overlap with any current
nugget from x
i
, it becomes a new concept that
links to x
i
and x
j
. In our example, the first
intersection v
1,2
contains ?Whittington ... an
attorney? from x
1
and ?... Whittington, a
lawyer? from x
2
; this becomes a new concept
z
B
since x
1
has no other nuggets.
2. If v
i,j
overlaps completely with a nugget y
k
i
,
then x
j
must also be linked to concept z
k
. For
example, x
1
?s fragment in the second inter-
section v
1,3
is also ?Whittington ... an attor-
ney?, so x
3
must also link to z
B
.
3. If v
i,j
subsumes y
k
i
, it is split up and the non-
overlapping portion is rechecked against ex-
isting nuggets recursively. For example, x
2
?s
fragment in the intersection v
2,3
is ?... shot
Whittington, a lawyer?, part of which over-
laps with y
B
2
, so this intersection is divided
up and the part representing ?... shot Whit-
tington ...? becomes a new concept z
A
.
4. If, on the other hand, y
k
i
subsumes v
i,j
, the
concept z
k
is itself split up along with all
nuggets that it links to, utilizing the present
structure of the graph.
When comparing intersections, we can restrict the
decomposition of nuggets to prevent the creation
of overly-granular concepts. For instance, we
can filter out intersections containing only isolated
named-entities or syntactic artifacts like determin-
ers since they contain no information by them-
selves. We can also prevent verbs and their ar-
guments from being split apart using information
from a snippet?s dependency parse, if available.
4.4 Efficiency of the algorithm
Instead of C additional comparisons in the worst
case after each pairwise snippet algnment, we
need no more comparisons in the worst case than
the maximum number of concepts that can exist in
a single snippet. Since this value grows no faster
than C as S increases, this is a significant improve-
ment. Other factors, such as the overhead required
to split up concepts, remain unchanged.
Furthermore, since all the additional compar-
isons are carried out between nuggets of the same
snippet, we don?t need to perform any further
alignment among nuggets or concepts. Alignments
are expensive; each is O(n
1
n
2
) where n
1
and n
2
are the number of words in the two segments of
text being aligned (if dependency tree alignment is
used) along with an overhead for checking word
similarity. However, since we now only need to
compare text from the same snippet, the com-
parison can be performed in linear time by sim-
ply comparing spans of word indices, thereby also
eliminating the overhead for comparing words.
5 Decreasing redundancy
The concept graph can now be applied to the task
of reducing redundancy in the document by drop-
ping snippets which contain no information that is
not already present in the rest of the document.
5.1 Reduction to set cover
Every snippet x
s
in a document can be represented
as a set of concepts {z
c
: y
c
s
? Y}. Since concepts
are defined as information that is seen in more than
one snippet as per the definition in Subsection 3.1,
representing snippets as sets of concepts will over-
look any unique information present in a snippet.
Without loss of generality, we can add any such
unique information in the form of an artificial con-
cept for each snippet to Z so that snippets can be
completely represented as sets of concepts from Z.
Note that the union of snippets
?
S
s=1
x
s
equals Z.
Reducing redundancy in the document while
preserving all information requires us to identify
the most snippets whose entire informational con-
tent is covered by the rest of the snippets in the
document, thereby targeting them for removal.
Since we express informational content in con-
cepts, this problem reduces to the task of finding
the smallest group of snippets that together cover
all the concepts in the document, i.e. we need to
find the smallest subset X
?
? X such that, if X
?
contains R snippets x
?
r
, the union of these snippets
?
R
r=1
x
?
r
also equals Z. Therefore, every concept
in a snippet from X?X
?
also exists in at least one
snippet from X
?
and no concept from Z is lost.
This formulation of the problem is the classic
877
set cover problem, which seeks to find the smallest
possible group of subsets of a universe that cov-
ers all the other subsets. A more general variant
of this problem is weighted set cover in which the
subsets have weights to be maximized or costs to
be minimized. While this problem is known to
be NP-hard, there exists a straightforward local
maximization approach (Hochbaum, 1997) which
runs in polynomial time and is proven to give so-
lutions within a known bound of the optimal solu-
tion. This greedy approximation algorithm can be
adapted to our representation.
5.2 Selecting non-redundant snippets
The algorithm selects a snippet x
r
to the subset X
?
such that information content of X ? x
r
is max-
imized. In general, this implies that the snippet
with the highest degree over uncovered concepts
must be selected at each iteration. Other measures
such as snippet length, fluency, or rank in an or-
dered list can be included in a weight measure in
order to break ties and introduce a preference for
shorter, more fluent, or higher-ranked snippets.
Consider the example from Section 3. The can-
didates for selection are x
2
, x
3
and x
4
since they
contain the most uncovered concepts. If x
2
is se-
lected, its concepts z
A
, z
B
and z
E
are covered.
At this stage, x
5
contains two uncovered concepts
while x
3
and x
4
contain just one each. Thus, x
5
is selected next and its concepts z
C
and z
D
are
covered. Since no uncovered concepts remain, all
snippets which haven?t been selected are redun-
dant. This solution, which is shown in Figure 3,
selects the following text to cover all the snippets:
x
2
: Cheney shot Whittington, a lawyer.
x
5
: This happened during a quail hunt in Texas.
Other solutions are also possible depending on
the factors involved in choosing the snippet to be
selected at each iteration. For example, the algo-
rithm might choose to select x
3
first instead of x
2
,
thereby yielding the following solution:
x
3
: Whittington, an attorney, was shot in Texas.
x
4
: Whittington was shot by Cheney while hunting quail.
6 Experiments
To evaluate the effectiveness of this framework
empirically, we ran experiments over documents
containing annotations corresponding to concepts
within the document. We also defined a metric
x
1
B
x
2
ABE
x
3
ABC
x
4
ADE
x
5
CD
z
A
z
B
z
C
z
D
z
E
y
B
1
y
A
2
y
B
2
y
E
2
y
A
3
y
B
3
y
C
3
y
A
4
y
D
4
y
E
4
y
C
5
y
D
5
Figure 3: Pruned version of the concept graph ex-
ample shown in Figure 2, illustrating the outcome
of removing redundant snippets.
for comparing any concept graph over a document
to a gold-standard concept graph. This was used
to compare the concept graphs created by our ap-
proach to perturbed versions of the gold-standard
graphs and graphs created by clustering.
6.1 Dataset
Due to the scarcity of available annotated datasets
suitable for evaluating redundancy, we utilized the
pyramid dataset from DUC 2005 (Nenkova et al,
2007) which was created from 20 articles for the
purpose of summarization evaluation. Each pyra-
mid document is a hierarchical representation of 7
summaries of the orginal news article. These sum-
maries have been annotated to identify the indi-
vidual semantic content units or SCUs where each
SCU represents a certain fact, observation or piece
of information in the summary. A sentence frag-
ment representing an occurrence of an SCU in a
summary is a contributor to the SCU.
The pyramid construction for a group of sum-
maries of the same article mirrors the concept
graph representation described in Subsection 3.2.
SCUs with more than two contributors are simi-
lar in definition to concepts while their contribu-
tors fill the role of nuggets. Using this analogy,
each dataset consists of a combination of the seven
summaries in a single pyramid document; the 20
pyramid documents therefore yield 20 datasets.
6.2 Evaluation metrics
The evaluation task requires us to compare the con-
cept graph generated by our algorithm to the ideal
878
x1
x
2
x
3
x
4
x
1
x
2
x
3
x
4
Concepts
SCUs
(merge)
(split)
L
alg
L
pyr
Figure 4: The bipartite graph on the left shows
snippets x
s
linked to concepts produced automati-
cally; the one on the right shows the same snippets
linked to SCUs from annotated data. Dashed lines
indicate mappings between concepts and SCUs.
concept graph extracted from the pyramid docu-
ment annotations. Standard metrics do not ap-
ply easily to the problem of comparing bipartite
graphs, so we define a novel metric modeled on
the well-known IR measures of precision, recall
and F-measure. Figure 4 illustrates the elements
involved in the evaluation task.
We define the metrics of precision, recall and F-
measure over the links between snippets and con-
cepts. Assuming we have a mapping between gen-
erated concepts and gold-standard SCUs, we can
judge whether each link is correct. Let each single
link between a snippet and a concept have an asso-
ciated weight of 1 by default and let L indicate a
set of such links. We use L
alg
and L
pyr
to distin-
guish between the sets of links generated by the al-
gorithm and retrieved from the annotations respec-
tively. Precision and recall are defined as follows
while F-measure retains its traditional definition as
their harmonic mean.
Precision =
Sum of weights in L
alg
? L
pyr
Sum of weights in L
alg
Recall =
Sum of weights in L
alg
? L
pyr
Sum of weights in L
pyr
To determine a mapping between concepts and
SCUs, we identify every concept and SCU pair,
say z
c
and z
s
, which has one or more snippets in
common and, for each snippet x
i
that they have
in common, we find the longest common subse-
quence between their nuggets y
c
i
and y
s
i
to obtain
the following score which ranges from 0 to 1.
LCS score =
length(LCS)
min (length(y
c
i
), length(y
s
i
))
Measure Random Clustering Concepts
Precision 0.0510 0.2961 0.4496
Recall 0.0515 0.1162 0.3266
F
1
score 0.0512 0.1669 0.3783
Table 1: Summary of the evaluation metrics aver-
aged over all 20 pyramid documents when m=0.5
This score is compared with a user-defined map-
ping threshold m to determine if the concept and
SCU are sufficiently similar. In order to avoid bi-
asing the metric by permitting multiple mappings
per concept, we adjust for merges or 1 : N map-
pings by cloning the concept and creating N 1 : 1
mappings in its place. We then adjust for splits or
N : 1 mappings by dividing the weight of each of
the links connected to a participating concept by
N . Due to this normalization, the metrics are ob-
served to be stable over variations in m.
6.3 Baselines
We compare the performance of the algorithm
against two baselines. The first approach involves
a random concept assignment scheme to build arti-
ficial concept graphs using the distributional prop-
erties of the gold-standard concept graphs. The
number of concepts C and the number of snippets
that each concept links to is determined by sam-
pling from distributions over these properties de-
rived from the statistics of the actual SCU graph
for that document. For evaluation, these artificial
concepts are randomly mapped to SCUs using m
to control the likelihood of mapping. The best
scores from 100 evaluation runs were considered.
The second baseline used for comparison is a
clustering algorithm, since clustering is the most
common approach to dealing with redundancy. For
this purpose, we use a recursive spectral partition-
ing algorithm, a variant of spectral clustering (Shi
and Malik, 2000) which obtains an average V-
measure (Rosenberg and Hirschberg, 2007) of 0.93
when clustering just pyramid contributors labeled
by their SCUs. The algorithm requires a parame-
ter that controls the homogeneity of each cluster;
we run it over the entire range of settings of this
parameter. We consider the clustering that maxi-
mizes F-measure in order to avoid any uncertainty
regarding optimal parameter selection and to im-
plicitly compare our algorithm against an entire hi-
erarchy of possible clusterings.
879
6.4 Results
Table 1 shows the F
1
scores over evaluation runs
using the random concept assignment, clustering
and concept graph techniques. These results are
obtained at a mapping threshold of m = 0.5,
which implies that we consider a mapping between
a concept and an SCU if their nuggets over com-
mon sentences share more than 50% of their words
on average. The results do not vary significantly at
different settings of m.
We observe that the concepts extracted by our
graph-based approach perform significantly better
than the best-performing clustering configuration.
Despite a fairly limited alignment approach that
doesn?t use synonyms or semantic analysis, the
concept graph outperforms the baselines by nearly
an order of magnitude on each document. This
validates our initial hypothesis that clustering ap-
proaches are not suitable for tackling the redun-
dancy problem at the sub-sentential level.
7 Conclusions and Future Work
We have described a graph-based algorithm for
identifying redundancy at the sub-snippet level and
shown that it outperforms clustering methods that
are traditionally applied to the redundancy task.
Though the algorithm identifies redundancy at
the sub-snippet level, redundancy can be decreased
by dropping only entirely redundant snippets. We
hope to be able to overcome this limitation by
extending this information-preserving approach to
the synthesis of new non-redundant snippets which
minimize redundant content in the document.
In addition, this work currently assumes that re-
dundancy is bidirectional; however, we intend to
also address the case of unidirectional redundancy
by considering entailment recognition approaches.
Acknowledgements
We are grateful to Andrew Rosenberg, David El-
son, Mayank Lahiri and the anonymous review-
ers for their useful feedback. This material is
based upon work supported by the Defense Ad-
vanced Research Projects Agency under Contract
No. HR0011-06-C-0023.
References
Barzilay, Regina and Lillian Lee. 2003. Learn-
ing to paraphrase: an unsupervised approach us-
ing multiple-sequence alignment. In Proceedings of
HLT-NAACL, pages 16?23.
Barzilay, Regina and Kathleen R. McKeown. 2005.
Sentence fusion for multidocument news summa-
rization. Computational Linguistics, 31(3):297?328.
Carbonell, Jaime G. and Jade Goldstein. 1998. The
use of MMR, diversity-based reranking for reorder-
ing documents and producing summaries. In Pro-
ceedings of ACM-SIGIR, pages 335?336.
Filatova, Elena and Vasileios Hatzivassiloglou. 2004.
A formal model for information selection in multi-
sentence text extraction. In Proceedings of COL-
ING, page 397.
Hochbaum, Dorit S. 1997. Approximating covering
and packing problems: set cover, vertex cover, in-
dependent set, and related problems. In Approxi-
mation algorithms for NP-hard problems, pages 94?
143. PWS Publishing Co., Boston, MA, USA.
Hovy, Eduard, Chin-Yew Lin, Liang Zhou, and Junichi
Fukumoto. 2006. Automated summarization evalu-
ation with basic elements. In Proceedings of LREC.
Lin, Chin-Yew and Eduard Hovy. 2001. From single
to multi-document summarization: a prototype sys-
tem and its evaluation. In Proceedings of ACL, pages
457?464.
Lin, Dekang. 1998. Dependency-based evaluation of
MINIPAR. In Proceedings of the Workshop on the
Evaluation of Parsing Systems, LREC.
Marsi, Erwin, Emiel Krahmer, Wauter Bosma, and Ma-
riet Theune. 2006. Normalized alignment of depen-
dency trees for detecting textual entailment. In Sec-
ond PASCAL Recognising Textual Entailment Chal-
lenge, pages 56?61.
Nenkova, Ani, Rebecca Passonneau, and Kathleen
McKeown. 2007. The pyramid method: Incorporat-
ing human content selection variation in summariza-
tion evaluation. ACM Transactions on Speech and
Language Processing, 4(2):4.
Pang, Bo, Kevin Knight, and Daniel Marcu. 2003.
Syntax-based alignment of multiple translations: ex-
tracting paraphrases and generating new sentences.
In Proceedings of HLT-NAACL, pages 102?109.
Rosenberg, Andrew and Julia Hirschberg. 2007. V-
measure: A conditional entropy-based external clus-
ter evaluation measure. In Proceedings of EMNLP,
pages 410?420.
Shi, Jianbo and Jitendra Malik. 2000. Normalized cuts
and image segmentation. IEEE Transactions on Pat-
tern Analysis and Machine Intelligence, 22(8):888?
905.
Siddharthan, Advaith, Ani Nenkova, and Kathleen
McKeown. 2004. Syntactic simplification for im-
proving content selection in multi-document summa-
rization. In Proceedings of COLING, page 896.
880
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 317?320,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Time-Efficient Creation of an Accurate Sentence Fusion Corpus
Kathleen McKeown, Sara Rosenthal, Kapil Thadani and Coleman Moore
Columbia University
New York, NY 10027, USA
{kathy,sara,kapil}@cs.columbia.edu, cjm2140@columbia.edu
Abstract
Sentence fusion enables summarization and
question-answering systems to produce out-
put by combining fully formed phrases from
different sentences. Yet there is little data
that can be used to develop and evaluate fu-
sion techniques. In this paper, we present a
methodology for collecting fusions of simi-
lar sentence pairs using Amazon?s Mechani-
cal Turk, selecting the input pairs in a semi-
automated fashion. We evaluate the results
using a novel technique for automatically se-
lecting a representative sentence from multi-
ple responses. Our approach allows for rapid
construction of a high accuracy fusion corpus.
1 Introduction
Summarization and question-answering systems
must transform input text to produce useful output
text, condensing an input document or document set
in the case of summarization and selecting text that
meets the question constraints in the case of question
answering. While many systems use sentence ex-
traction to facilitate the task, this approach risks in-
cluding additional, irrelevant or non-salient informa-
tion in the output, and the original sentence wording
may be inappropriate for the new context in which
it appears. Instead, recent research has investigated
methods for generating new sentences using a tech-
nique called sentence fusion (Barzilay and McKe-
own, 2005; Marsi and Krahmer, 2005; Filippova and
Strube, 2008) where output sentences are generated
by fusing together portions of related sentences.
While algorithms for automated fusion have been
developed, there is no corpus of human-generated
fused sentences available to train and evaluate such
systems. The creation of such a dataset could pro-
vide insight into the kinds of fusions that people
produce. Furthermore, since research in the related
task of sentence compression has benefited from
the availability of training data (Jing, 2000; Knight
and Marcu, 2002; McDonald, 2006; Cohn and La-
pata, 2008), we expect that the creation of this cor-
pus might encourage the development of supervised
learning techniques for automated sentence fusion.
In this work, we present a methodology for cre-
ating such a corpus using Amazon?s Mechanical
Turk1, a widely used online marketplace for crowd-
sourced task completion. Our goal is the generation
of accurate fusions between pairs of sentences that
have some information in common. To ensure that
the task is performed consistently, we abide by the
distinction proposed by Marsi and Krahmer (2005)
between intersection fusion and union fusion. In-
tersection fusion results in a sentence that contains
only the information that the sentences had in com-
mon and is usually shorter than either of the original
sentences. Union fusion, on the other hand, results
in a sentence that contains all information content
from the original two sentences. An example of in-
tersection and union fusion is shown in Figure 1.
We solicit multiple annotations for both union and
intersection tasks separately and leverage the differ-
ent responses to automatically choose a representa-
tive response. Analysis of the responses shows that
our approach yields 95% accuracy on the task of
union fusion. This is a promising first step and indi-
cates that our methodology can be applied towards
efficiently building a highly accurate corpus for sen-
tence fusion.
1https://www.mturk.com
317
1. Palin actually turned against the bridge project only after it
became a national symbol of wasteful spending.
2. Ms. Palin supported the bridge project while running for
governor, and abandoned it after it became a national scandal.
Intersection: Palin turned against the bridge project after it
became a national scandal.
Union: Ms. Palin supported the bridge project while running
for governor, but turned against it when it became a national
scandal and a symbol of wasteful spending.
Figure 1: Examples of intersection and union
2 Related Work
The combination of fragments of sentences on a
common topic has been studied in the domain of sin-
gle document summarization (Jing, 2000; Daume? III
and Marcu, 2002; Xie et al, 2008). In contrast to
these approaches, sentence fusion was introduced to
combine fragments of sentences with common infor-
mation for multi-document summarization (Barzilay
and McKeown, 2005). Automated fusion of sen-
tence pairs has since received attention as an inde-
pendent task (Marsi and Krahmer, 2005; Filippova
and Strube, 2008). Although generic fusion of sen-
tence pairs based on importance does not yield high
agreement when performed by humans (Daume? III
and Marcu, 2004), fusion in the context of a query
has been shown to produce better agreement (Krah-
mer et al, 2008). We examine similar fusion an-
notation tasks in this paper, but we asked workers
to provide two specific types of fusion, intersection
and union, thus avoiding the less specific definition
based on importance. Furthermore, as our goal is
the generation of corpora, our target for evaluation
is accuracy rather than agreement.
This work studies an approach to the automatic
construction of large fusion corpora using workers
through Amazon?s Mechanical Turk service. Previ-
ous studies using this online task marketplace have
shown that the collective judgments of many work-
ers are comparable to those of trained annotators
on labeling tasks (Snow et al, 2008) although these
judgments can be obtained at a fraction of the cost
and effort. However, our task presents an additional
challenge: building a corpus for sentence fusion re-
quires workers to enter free text rather than simply
choose between predefined options; the results are
prone to variation and this makes comparing and ag-
gregating multiple responses problematic.
A. After a decade on the job, Gordon had become an experi-
enced cop.
B. Gordon has a lot of experience in the police force.
Figure 2: An example of sentences that were judged to be
too similar for inclusion in the dataset
3 Collection Methodology
Data collection involved the identification of the
types of sentence pairs that would make suitable
candidates for fusion, the development of a sys-
tem to automatically identify good pairs and manual
filtering of the sentence pairs to remove erroneous
choices. The selected sentence pairs were then pre-
sented to workers on Mechanical Turk in an inter-
face that required them to manually type in a fused
sentence (intersection or union) for each case.
Not all pairs of related sentences are useful for the
fusion task. When sentences are too similar, the re-
sult of fusion is simply one of the input sentences.
For example (Fig. 2), if sentence A contains all the
information in sentence B but not vice versa, then
B is also their intersection while A is their union
and no sentence generation is required. On the other
hand, if the two sentences are too dissimilar, then
no intersection is possible and the union is just the
conjunction of the sentences.
We experimented with different similarity metrics
aimed at identifying pairs of sentences that were in-
appropriate for fusion. The sentences in this study
were drawn from clusters of news articles on the
same event from the Newsblaster summarization
system (McKeown et al, 2002). While these clus-
ters are likely to contain similar sentences, they will
contain many more dissimilar than similar pairs and
thus a metric that emphasizes precision over recall
is important. We computed pairwise similarity be-
tween sentences within each cluster using three stan-
dard metrics: word overlap, n-gram overlap and co-
sine similarity. Bigram overlap yielded the best pre-
cision in our experiments. We empirically arrived at
a lower threshold of .35 to remove dissimilar sen-
tences and an upper threshold of .65 to avoid near-
identical sentences, yielding a false-positive rate of
44.4%. The remaining inappropriate pairs were then
manually filtered. This semi-automated procedure
enabled fast selection of suitable sentence pairs: one
person was able to select 30 pairs an hour yielding
the 300 pairs for the full experiment in ten hours.
318
Responses Intersection Union
All (1500) 0.49 0.88
Representatives (300) 0.54 0.95
Table 1: Union and intersection accuracy
3.1 Using Amazon?s Mechanical Turk
Based on a pilot study with 20 sentence pairs, we
designed an interface for the full study. For inter-
section tasks, the interface posed the question ?How
would you combine the following two sentences into
a single sentence conveying only the information
they have in common??. For union tasks, the ques-
tion was ?How would you combine the following two
sentences into a single sentence that contains ALL of
the information in each??.
We used all 300 pairs of similar sentences for
both union and intersection and chose to collect five
worker responses per pair, given the diversity of
responses that we found in the pilot study. This
yielded a total of 3000 fused sentences with 1500
intersections and 1500 unions.
3.2 Representative Responses
Using multiple workers provides little benefit unless
we are able to harness the collective judgments of
their responses. To this end, we experiment with
a simple technique to select one representative re-
sponse from all responses for a case, hypothesizing
that such a response would have a lower error rate.
We test the hypothesis by comparing the accuracy of
representative responses with the average accuracy
over all responses.
Our strategy for selecting representatives draws
on the common assumption used in human com-
putation that human agreement in independently-
generated labels implies accuracy (von Ahn and
Dabbish, 2004). We approximate agreement be-
tween responses using a simple and transparent
measure for overlap: cosine similarity over stems
weighted by tf-idf where idf values are learned over
the Gigawords corpus2. After comparing all re-
sponses in a pairwise fashion, we need to choose a
representative response. As using the centroid di-
rectly might not be robust to the presence of er-
roneous responses, we first select the pair of re-
sponses with the greatest overlap as candidates and
2LDC Catalog No. LDC2003T05
Errors Intersection Union
Missing clause 2 7
Union/Intersection 46 6
S1/S2 21 8
Additional clause 10 1
Lexical 3 1
Table 2: Errors seen in 30 random cases (150 responses)
then choose the candidate which has the greatest to-
tal overlap with all other responses.
4 Results and Error Analysis
For evaluating accuracy, fused sentences were man-
ually compared to the original sentence pairs. Due to
the time-consuming nature of the evaluation, 50% of
the 300 cases were randomly selected for analysis.
10% were initially analyzed by two of the authors; if
a disagreement occurred, the authors discussed their
differences and came to a unified decision. The re-
maining 40% were then analyzed by one author. In
addition to this high-level analysis, we further ana-
lyzed 10% of the cases to identify the the types of
errors made in fusion as well as the techniques used
and the effect of task difficulty on performance.
The accuracy for intersection and union tasks is
shown in Table 1. For both tasks, accuracy of the se-
lected representatives significantly exceeded the av-
erage response accuracy. In our error analysis, we
found that workers often answered the intersection
task by providing a union, possibly due to a misin-
terpretation of the question. This caused intersection
accuracy to be significantly worse than union. We
analyzed the impact of this error by computing ac-
curacy on the first 30 cases (10%) without this error
and the accuracy for intersection increased 22%.
Error types were categorized as ?missing clause?,
?using union for intersection and vice versa?,
?choosing an input sentence (S1/S2)?, ?additional
clause? and ?lexical error?. Table 2 shows the num-
ber of occurrences of each in 10% of the cases.
We binned the sentence pairs according to
the difficulty of the fusion task for each pair
(easy/medium/hard) and found that performance
was not dependent on difficulty level; accuracy was
relatively similar across bins. We also observed that
workers typically performed fusion by selecting one
sentence as a base and removing clauses or merging
in additional clauses from the other sentence.
319
Figure 3: Number of cases in which x/5 workers pro-
vided accurate responses for fusion
In order to determine the benefit of using many
workers, we studied the number of workers who an-
swered correctly for each case. Figure 3 reveals that
2/5 or more workers (summing across columns) re-
sponded accurately in 99% of union cases and 82%
of intersection cases. The intersection results are
skewed due to the question misinterpretation issue
which, though it was the most common error, was
made by 3/5 workers only 17% of the time. Thus, in
the majority of the cases, accurate fusions can still
be found using the representative method.
5 Conclusion
We presented a methodology to build a fusion cor-
pus which uses semi-automated techniques to select
similar sentence pairs for annotation on Mechanical
Turk3. Additionally, we showed how multiple re-
sponses for each fusion task can be leveraged by au-
tomatically selecting a representative response. Our
approach yielded 95% accuracy for union tasks, and
while intersection fusion accuracy was much lower,
our analysis showed that workers sometimes pro-
vided unions instead of intersections and we sus-
pect that an improved formulation of the question
could lead to better results. Construction of the fu-
sion dataset was relatively fast; it required only ten
hours of labor on the part of a trained undergraduate
and seven days of active time on Mechanical Turk.
Acknowledgements
This material is based on research supported in part
by the U.S. National Science Foundation (NSF) un-
der IIS-05-34871 Any opinions, findings and con-
clusions or recommendations expressed in this ma-
terial are those of the authors and do not necessarily
reflect the views of the NSF.
3The corpus described in this work is available at
http://www.cs.columbia.edu/?kathy/fusioncorpus
References
Regina Barzilay and Kathleen R. McKeown. 2005. Sen-
tence fusion for multidocument news summarization.
Computational Linguistics, 31(3):297?328.
Trevor Cohn and Mirella Lapata. 2008. Sentence com-
pression beyond word deletion. In Proceedings of
COLING, pages 137?144.
Hal Daume? III and Daniel Marcu. 2002. A noisy-channel
model for document compression. In Proceedings of
ACL, pages 449?456.
Hal Daume? III and Daniel Marcu. 2004. Generic sen-
tence fusion is an ill-defined summarization task. In
Proceedings of the ACL Text Summarization Branches
Out Workshop, pages 96?103.
Katja Filippova and Michael Strube. 2008. Sentence fu-
sion via dependency graph compression. In Proceed-
ings of EMNLP, pages 177?185.
Hongyan Jing. 2000. Sentence reduction for automatic
text summarization. In Proceedings of Applied Natu-
ral Language Processing, pages 310?315.
Kevin Knight and Daniel Marcu. 2002. Summariza-
tion beyond sentence extraction: a probabilistic ap-
proach to sentence compression. Artificial Intelli-
gence, 139(1):91?107.
Emiel Krahmer, Erwin Marsi, and Paul van Pelt. 2008.
Query-based sentence fusion is better defined and
leads to more preferred results than generic sentence
fusion. In Proceedings of ACL, pages 193?196.
Erwin Marsi and Emiel Krahmer. 2005. Explorations in
sentence fusion. In Proceedings of the European Work-
shopon Natural Language Generation, pages 109?117.
Ryan McDonald. 2006. Discriminative sentence com-
pression with soft syntactic evidence. In Proceedings
of EACL, pages 297?304.
Kathleen R. McKeown, Regina Barzilay, David Evans,
Vasileios Hatzivassiloglou, Judith L. Klavans, Ani
Nenkova, Carl Sable, Barry Schiffman, and Sergey
Sigelman. 2002. Tracking and summarizing news on
a daily basis with Columbia?s Newsblaster. In Pro-
ceedings of HLT, pages 280?285.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and
Andrew Y. Ng. 2008. Cheap and fast?but is it
good?: Evaluating non-expert annotations for natu-
ral language tasks. In Proceedings of EMNLP, pages
254?263.
Luis von Ahn and Laura Dabbish. 2004. Labeling im-
ages with a computer game. In Proceedings of the
SIGCHI conference on Human Factors in Computing
Systems, pages 319?326.
Zhuli Xie, Barbara Di Eugenio, and Peter C. Nel-
son. 2008. From extracting to abstracting: Gener-
ating quasi-abstractive summaries. In Proceedings of
LREC, May.
320
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 254?259,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Optimal and Syntactically-Informed Decoding for Monolingual
Phrase-Based Alignment
Kapil Thadani and Kathleen McKeown
Department of Computer Science
Columbia University
New York, NY 10027, USA
{kapil,kathy}@cs.columbia.edu
Abstract
The task of aligning corresponding phrases
across two related sentences is an important
component of approaches for natural language
problems such as textual inference, paraphrase
detection and text-to-text generation. In this
work, we examine a state-of-the-art struc-
tured prediction model for the alignment task
which uses a phrase-based representation and
is forced to decode alignments using an ap-
proximate search approach. We propose in-
stead a straightforward exact decoding tech-
nique based on integer linear programming
that yields order-of-magnitude improvements
in decoding speed. This ILP-based decoding
strategy permits us to consider syntactically-
informed constraints on alignments which sig-
nificantly increase the precision of the model.
1 Introduction
Natural language processing problems frequently in-
volve scenarios in which a pair or group of related
sentences need to be aligned to each other, establish-
ing links between their common words or phrases.
For instance, most approaches for natural language
inference (NLI) rely on alignment techniques to es-
tablish the overlap between the given premise and a
hypothesis before determining if the former entails
the latter. Such monolingual alignment techniques
are also frequently employed in systems for para-
phrase generation, multi-document summarization,
sentence fusion and question answering.
Previous work (MacCartney et al, 2008) has pre-
sented a phrase-based monolingual aligner for NLI
(MANLI) that has been shown to significantly out-
perform a token-based NLI aligner (Chambers et
al., 2007) as well as popular alignment techniques
borrowed from machine translation (Och and Ney,
2003; Liang et al, 2006). However, MANLI?s use
of a phrase-based alignment representation appears
to pose a challenge to the decoding task, i.e. the
task of recovering the highest-scoring alignment un-
der some parameters. Consequently, MacCartney et
al. (2008) employ a stochastic search algorithm to
decode alignments approximately while remaining
consistent with regard to phrase segmentation.
In this paper, we propose an exact decoding tech-
nique for MANLI that retrieves the globally opti-
mal alignment for a sentence pair given some pa-
rameters. Our approach is based on integer lin-
ear programming (ILP) and can leverage optimized
general-purpose LP solvers to recover exact solu-
tions. This strategy boosts decoding speed by an
order of magnitude over stochastic search in our ex-
periments. Additionally, we introduce hard syntac-
tic constraints on alignments produced by the model,
yielding better precision and a large increase in the
number of perfect alignments produced over our
evaluation corpus.
2 Related Work
Alignment is an integral part of statistical MT (Vo-
gel et al, 1996; Och and Ney, 2003; Liang et al,
2006) but the task is often substantively different
from monolingual alignment, which poses unique
challenges depending on the application (MacCart-
ney et al, 2008). Outside of NLI, prior research has
also explored the task of monolingual word align-
254
ment using extensions of statistical MT (Quirk et al,
2004) and multi-sequence alignment (Barzilay and
Lee, 2002).
ILP has been used extensively for applications
ranging from text-to-text generation (Clarke and La-
pata, 2008; Filippova and Strube, 2008; Wood-
send et al, 2010) to dependency parsing (Martins
et al, 2009). It has also been recently employed for
finding phrase-based MT alignments (DeNero and
Klein, 2008) in a manner similar to this work; how-
ever, we further build upon this model through syn-
tactic constraints on the words participating in align-
ments.
3 The MANLI Aligner
Our alignment system is structured identically to
MANLI (MacCartney et al, 2008) and uses the same
phrase-based alignment representation. An align-
ment E between two fragments of text T1 and T2
is represented by a set of edits {e1, e2, . . .}, each be-
longing to one of the following types:
? INS and DEL edits covering unaligned words in
T1 and T2 respectively
? SUB and EQ edits connecting a phrase in T1 to
a phrase in T2. EQ edits are a specific case of
SUB edits that denote a word/lemma match; we
refer to both types as SUB edits in this paper.
Every token in T1 and T2 participates in exactly one
edit. While alignments are one-to-one at the phrase
level, a phrase-based representation effectively per-
mits many-to-many alignments at the token level.
This enables the aligner to properly link paraphrases
such as death penalty and capital punishment by ex-
ploiting lexical resources.
3.1 Dataset
MANLI was trained and evaluated on a corpus of
human-generated alignment annotations produced
by Microsoft Research (Brockett, 2007) for infer-
ence problems from the second Recognizing Tex-
tual Entailment (RTE2) challenge (Bar-Haim et al,
2006). The corpus consists of a development set
and test set that both feature 800 inference prob-
lems, each of which consists of a premise, a hy-
pothesis and three independently-annotated human
alignments. In our experiments, we merge the an-
notations using majority rule in the same manner as
MacCartney et al (2008).
3.2 Features
A MANLI alignment is scored as a sum of weighted
feature values over the edits that it contains. Fea-
tures encode the type of edit, the size of the phrases
involved in SUB edits, whether the phrases are con-
stituents and their similarity (determined by lever-
aging various lexical resources). Additionally, con-
textual features note the similarity of neighboring
words and the relative positions of phrases while
a positional distortion feature accounts for the dif-
ference between the relative positions of SUB edit
phrases in their respective sentences.
Our implementation uses the same set of fea-
tures as MacCartney et al (2008) with some mi-
nor changes: we use a shallow parser (Daume? and
Marcu, 2005) for detecting constituents and employ
only string similarity and WordNet for determining
semantic relatedness, forgoing NomBank and the
distributional similarity resources used in the orig-
inal MANLI implementation.
3.3 Parameter Inference
Feature weights are learned using the averaged
structured perceptron algorithm (Collins, 2002), an
intuitive structured prediction technique. We deviate
from MacCartney et al (2008) and do not introduce
L2 normalization of weights during learning as this
could have an unpredictable effect on the averaged
parameters. For efficiency reasons, we parallelize
the training procedure using iterative parameter mix-
ing (McDonald et al, 2010) in our experiments.
3.4 Decoding
The decoding problem is that of finding the highest-
scoring alignment under some parameter values for
the model. MANLI?s phrase-based representation
makes decoding more complex because the segmen-
tation of T1 and T2 into phrases is not known before-
hand. Every pair of phrases considered for inclusion
in an alignment must adhere to some consistent seg-
mentation so that overlapping edits and uncovered
words are avoided.
Consequently, the decoding problem cannot be
factored into a number of independent decisions
and MANLI searches for a good alignment using
a stochastic simulated annealing strategy. While
seemingly quite effective at avoiding local maxima,
255
System Data P% R% F1% E%
MANLI dev 83.4 85.5 84.4 21.7
(reported 2008) test 85.4 85.3 85.3 21.3
MANLI dev 85.7 84.8 85.0 23.8
(reimplemented) test 87.2 86.3 86.7 24.5
MANLI-Exact dev 85.7 84.7 85.2 24.6
(this work) test 87.8 86.1 86.8 24.8
Table 1: Performance of aligners in terms of precision, re-
call, F-measure and number of perfect alignments (E%).
this iterative search strategy is computationally ex-
pensive and moreover is not guaranteed to return the
highest-scoring alignment under the parameters.
4 Exact Decoding via ILP
Instead of resorting to approximate solutions, we
can simply reformulate the decoding problem as the
optimization of a linear objective function with lin-
ear constraints, which can be solved by well-studied
algorithms using off-the-shelf solvers1. We first de-
fine boolean indicator variables xe for every possible
edit e between T1 and T2 that indicate whether e is
present in the alignment or not. The linear objective
that maximizes the score of edits for a given param-
eter vector w is expressed as follows:
f(w) = max
?
e
xe ? scorew(e)
= max
?
e
xe ?w ? ?(e) (1)
where ?(e) is the feature vector over an edit. This
expresses the score of an alignment as the sum of
scores of edits that are present in it, i.e., edits e that
have xe = 1.
In order to address the phrase segmentation issue
discussed in ?3.4, we merely need to add linear con-
straints ensuring that every token participates in ex-
actly one edit. Introducing the notation e ? t to in-
dicate that edit e covers token t in one of its phrases,
this constraint can be encoded as:
?
e: e?t
xe = 1 ?t ? Ti, i = {1, 2}
On solving this integer program, the values of the
variables xe indicate which edits are present in the
1We use LPsolve: http://lpsolve.sourceforge.net/
Corpus Size Approximate Exact
Search ILP
RTE2 dev 800 2.58 0.11
test 800 1.67 0.08
McKeown et al
(2010)
297 61.96 2.45
Table 2: Approximate running time per decoding task in
seconds for the search-based approximate decoder and
the ILP-based exact decoder on various corpora (see text
for details).
highest-scoring alignment under w. A similar ap-
proach is employed by DeNero and Klein (2008) for
finding optimal phrase-based alignments for MT.
4.1 Alignment experiments
For evaluation purposes, we compare the perfor-
mance of approximate search decoding against ex-
act ILP-based decoding on a reimplementation of
MANLI as described in ?3. All models are trained
on the development section of the Microsoft Re-
search RTE2 alignment corpus (cf. ?3.1) using
the training parameters specified in MacCartney
et al (2008). Aligner performance is determined
by counting aligned token pairs per problem and
macro-averaging over all problems. The results are
shown in Table 1.
We first observe that our reimplemented version
of MANLI improves over the results reported in
MacCartney et al (2008), gaining 2% in precision,
1% in recall and 2-3% in the fraction of alignments
that exactly matched human annotations. We at-
tribute at least some part of this gain to our modified
parameter inference (cf. ?3.3) which avoids normal-
izing the structured perceptron weights and instead
adheres closely to the algorithm of Collins (2002).
Although exact decoding improves alignment per-
formance over the approximate search approach, the
gain is marginal and not significant. This seems to
indicate that the simulated annealing search strategy
is fairly effective at avoiding local maxima and find-
ing the highest-scoring alignments.
4.2 Runtime experiments
Table 2 contains the results from timing alignment
tasks over various corpora on the same machine us-
ing the models trained as per ?4.1. We observe a
256
twenty-fold improvement in performance with ILP-
based decoding. It is important to note that the spe-
cific implementations being compared2 may be re-
sponsible for the relative speed of decoding.
The short hypotheses featured in the RTE2 cor-
pus (averaging 11 words) dampen the effect of the
quadratic growth in number of edits with sentence
length. For this reason, we also run the aligners on
a corpus of 297 related sentence pairs which don?t
have a particular disparity in sentence lengths (McK-
eown et al, 2010). The large difference in decoding
time illustrates the scaling limitations of the search-
based decoder.
5 Syntactically-Informed Constraints
The use of an integer program for decoding pro-
vides us with a convenient mechanism to prevent
common alignment errors by introducting additional
constraints on edits. For example, function words
such as determiners and prepositions are often mis-
aligned just because they occur frequently in many
different contexts. Although MANLI makes use
of contextual features which consider the similar-
ity of neighboring words around phrase pairs, out-
of-context alignments of function words often ap-
pear in the output. We address this issue by adding
constraints to the integer program from ?4 that look
at the syntactic structure of T1 and T2 and prevent
matching function words from appearing in an align-
ment unless they are syntactically linked with other
words that are aligned.
To enforce token-based constraints, we define
boolean indicator variables yt for each token t in
text snippets T1 and T2 that indicate whether t is in-
volved in a SUB edit or not. The following constraint
ensures that yt = 1 if and only if it is covered by a
SUB edit that is present in the alignment.
yt ?
?
e: e?t,
e is SUB
xe = 0 ?t ? Ti, i = {1, 2}
We refer to tokens t with yt = 1 as being active in
the alignment. Constraints can now be applied over
any token with specific part-of-speech (POS) tag in
2Our Python reimplementation closely follows the original
Java implementation of MANLI and was optimized for perfor-
mance. MacCartney et al (2008) report a decoding time of
about 2 seconds per problem.
System Data P% R% F1% E%
MANLI-Exact with dev 86.8 84.5 85.6 25.3
M constraints test 88.8 85.7 87.2 29.9
MANLI-Exact with dev 86.1 84.6 85.3 24.5
L constraints test 88.2 86.4 87.3 27.6
MANLI-Exact with dev 87.1 84.4 85.8 25.4
M + L constraints test 89.5 86.2 87.8 33.0
Table 3: Performance of MANLI-Exact featuring addi-
tional modifier (M) and lineage (L) constraints. Figures
in boldface are statistically significant over the uncon-
strained MANLI reimplementation (p ? 0.05).
order to ensure that it can only be active if a differ-
ent token related to it in a dependency parse of the
sentence is also active. We consider the following
classes of constraints:
Modifier constraints: Tokens t that represent con-
junctions, determiners, modals and cardinals can
only be active if their parent tokens pi(t) are active.
yt ? ypi(t) <= 0
if POS(t) ? {CC, CD, MD, DT, PDT, WDT}
Lineage constraints: Tokens t that represent prepo-
sitions and particles (which are often confused by
parsers) can only be active if one of their ancestors
?(t) or descendants ?(t) is active. These constraints
are less restrictive than the modifier constraints in
order to account for attachment errors.
yt ?
?
a??(t)
ya ?
?
d??(t)
yd <= 0
if POS(t) ? {IN, TO, RP}
5.1 Alignment experiments
A TAG-based probabilistic dependency parser (Ban-
galore et al, 2009) is used to formulate the above
constraints in our experiments. The results are
shown in Table 3 and indicate a notable increase in
alignment precision, which is to be expected as the
constraints specifically seek to exclude poor edits.
Despite the simple and overly general restrictions
being applied, recall is almost unaffected. Most
compellingly, the number of perfect alignments pro-
duced by the system increases significantly when
257
compared to the unconstrained models from Table 1
(a relative increase of 35% on the test corpus).
6 Discussion
The results of our evaluation indicate that exact de-
coding via ILP is a robust and efficient technique for
solving alignment problems. Furthermore, the in-
corporation of simple constraints over a dependency
parse can help to shape more accurate alignments.
An examination of the alignments produced by our
system reveals that many remaining errors can be
tackled by the use of named-entity recognition and
better paraphrase corpora; this was also noted by
MacCartney et al (2008) with regard to the original
MANLI system. In addition, stricter constraints that
enforce the alignment of syntactically-related tokens
(rather than just their inclusion in the solution) may
also yield performance gains.
Although MANLI?s structured prediction ap-
proach to the alignment problem allows us to encode
preferences as features and learn their weights via
the structured perceptron, the decoding constraints
used here can be used to establish dynamic links be-
tween alignment edits which cannot be determined
a priori. The interaction between the selection of
soft features for structured prediction and hard con-
straints for decoding is an interesting avenue for fur-
ther research on this task. Initial experiments with
a feature that considers the similarity of dependency
heads of tokens in an edit (similar to MANLI?s con-
textual features that look at preceding and following
words) yielded some improvement over the base-
line models; however, this did not perform as well
as the simple constraints described above. Specific
features that approximate soft variants of these con-
straints could also be devised but this was not ex-
plored here.
In addition to the NLI applications considered in
this work, we have also employed the MANLI align-
ment technique to tackle alignment problems that
are not inherently asymmetric such as the sentence
fusion problems from McKeown et al (2010). Al-
though the absence of asymmetric alignment fea-
tures affects performance marginally over the RTE2
dataset, all the performance gains exhibited by exact
decoding with constraints appear to be preserved in
symmetric settings.
7 Conclusion
We present a simple exact decoding technique as an
alternative to approximate search-based decoding in
MANLI that exhibits a twenty-fold improvement in
runtime performance in our experiments. In addi-
tion, we propose novel syntactically-informed con-
straints to increase precision. Our final system im-
proves over the results reported in MacCartney et al
(2008) by about 4.5% in precision and 1% in recall,
with a large gain in the number of perfect alignments
over the test corpus. Finally, we analyze the align-
ments produced and suggest that further improve-
ments are possible through careful feature/constraint
design, as well as the use of named-entity recogni-
tion and additional resources.
Acknowledgments
The authors are grateful to Bill MacCartney for pro-
viding a reference MANLI implementation and the
anonymous reviewers for their useful feedback. This
material is based on research supported in part by
the U.S. National Science Foundation (NSF) under
IIS-05-34871. Any opinions, findings and conclu-
sions or recommendations expressed in this material
are those of the authors and do not necessarily reflect
the views of the NSF.
References
Srinivas Bangalore, Pierre Boullier, Alexis Nasr, Owen
Rambow, and Beno??t Sagot. 2009. MICA: a prob-
abilistic dependency parser based on tree insertion
grammars. In Proceedings of HLT-NAACL, pages
185?188.
Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo
Giampiccolo, Bernardo Magnini, and Idan Szpektor.
2006. The second PASCAL Recognising Textual En-
tailment challenge. In Proceedings of the Second
PASCAL Challenges Workshop on Recognising Tex-
tual Entailment.
Regina Barzilay and Lilian Lee. 2002. Bootstrapping
lexical choice via multiple-sequence alignment. In
Proceedings of EMNLP.
Chris Brockett. 2007. Aligning the 2006 RTE cor-
pus. Technical Report MSR-TR-2007-77, Microsoft
Research.
Nathanael Chambers, Daniel Cer, Trond Grenager,
David Hall, Chloe Kiddon, Bill MacCartney, Marie-
Catherine de Marneffe, Daniel Ramage, Eric Yeh, and
258
Christopher D. Manning. 2007. Learning alignments
and leveraging natural logic. In Proceedings of the
ACL-PASCAL Workshop on Textual Entailment and
Paraphrasing, pages 165?170.
James Clarke and Mirella Lapata. 2008. Global infer-
ence for sentence compression: an integer linear pro-
gramming approach. Journal of Artifical Intelligence
Research, 31:399?429, March.
Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models. In Proceedings of
EMNLP, pages 1?8.
Hal Daume?, III and Daniel Marcu. 2005. Learning as
search optimization: approximate large margin meth-
ods for structured prediction. In Proceedings of ICML,
pages 169?176.
John DeNero and Dan Klein. 2008. The complexity of
phrase alignment problems. In Proceedings of ACL-
HLT, pages 25?28.
Katja Filippova and Michael Strube. 2008. Sentence fu-
sion via dependency graph compression. In Proceed-
ings of EMNLP, pages 177?185.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of HLT-NAACL,
pages 104?111.
Bill MacCartney, Michel Galley, and Christopher D.
Manning. 2008. A phrase-based alignment model
for natural language inference. In Proceedings of
EMNLP, pages 802?811.
Andre? F. T. Martins, Noah A. Smith, and Eric P. Xing.
2009. Concise integer linear programming formula-
tions for dependency parsing. In Proceedings of ACL-
IJCNLP, pages 342?350.
Ryan McDonald, Keith Hall, and Gideon Mann. 2010.
Distributed training strategies for the structured per-
ceptron. In Proceedings of HLT-NAACL, pages 456?
464.
Kathleen McKeown, Sara Rosenthal, Kapil Thadani, and
Coleman Moore. 2010. Time-efficient creation of an
accurate sentence fusion corpus. In Proceedings of
HLT-NAACL, pages 317?320.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29:19?51, March.
Chris Quirk, Chris Brockett, and William Dolan. 2004.
Monolingual machine translation for paraphrase gen-
eration. In In Proceedings of EMNLP, pages 142?149,
July.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. Hmm-based word alignment in statistical trans-
lation. In Proceedings of COLING, pages 836?841.
Kristian Woodsend, Yansong Feng, and Mirella Lapata.
2010. Title generation with quasi-synchronous gram-
mar. In Proceedings of EMNLP, pages 513?523.
259
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 670?675,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
A Hierarchical Model of Web Summaries
Yves Petinot and Kathleen McKeown and Kapil Thadani
Department of Computer Science
Columbia University
New York, NY 10027
{ypetinot|kathy|kapil}@cs.columbia.edu
Abstract
We investigate the relevance of hierarchical
topic models to represent the content of Web
gists. We focus our attention on DMOZ,
a popular Web directory, and propose two
algorithms to infer such a model from its
manually-curated hierarchy of categories. Our
first approach, based on information-theoretic
grounds, uses an algorithm similar to recur-
sive feature selection. Our second approach
is fully Bayesian and derived from the more
general model, hierarchical LDA. We evalu-
ate the performance of both models against a
flat 1-gram baseline and show improvements
in terms of perplexity over held-out data.
1 Introduction
The work presented in this paper is aimed at lever-
aging a manually created document ontology to
model the content of an underlying document col-
lection. While the primary usage of ontologies is
as a means of organizing and navigating document
collections, they can also help in inferring a signif-
icant amount of information about the documents
attached to them, including path-level, statistical,
representations of content, and fine-grained views
on the level of specificity of the language used in
those documents. Our study focuses on the ontology
underlying DMOZ1, a popular Web directory. We
propose two methods for crystalizing a hierarchical
topic model against its hierarchy and show that the
resulting models outperform a flat unigram model in
its predictive power over held-out data.
1http://www.dmoz.org
To construct our hierarchical topic models, we
adopt the mixed membership formalism (Hofmann,
1999; Blei et al, 2010), where a document is rep-
resented as a mixture over a set of word multi-
nomials. We consider the document hierarchy H
(e.g. the DMOZ hierarchy) as a tree where internal
nodes (category nodes) and leaf nodes (documents),
as well as the edges connecting them, are known a
priori. Each node Ni in H is mapped to a multi-
nomial word distribution MultNi , and each path cd
to a leaf node D is associated with a mixture over
the multinonials (MultC0 . . .MultCk ,MultD) ap-
pearing along this path. The mixture components
are combined using a mixing proportion vector
(?C0 . . . ?Ck), so that the likelihood of string w be-
ing produced by path cd is:
p(w|cd) =
|w|?
i=0
|cd|?
j=0
?jp(wi|cd,j) (1)
where:
|cd|?
j=0
?j = 1,?d (2)
In the following, we propose two models that fit
in this framework. We describe how they allow the
derivation of both p(wi|cd,j) and ? and present early
experimental results showing that explicit hierarchi-
cal information of content can indeed be used as a
basis for content modeling purposes.
2 Related Work
While several efforts have focused on the DMOZ
corpus, often as a reference for Web summarization
670
tasks (Berger and Mittal, 2000; Delort et al, 2003)
or Web clustering tasks (Ramage et al, 2009b), very
little research has attempted to make use of its hier-
archy as is. The work by Sun et al (2005), where
the DMOZ hierarchy is used as a basis for a hierar-
chical lexicon, is closest to ours although their con-
tribution is not a full-fledged content model, but a
selection of highly salient vocabulary for every cat-
egory of the hierarchy. The problem considered in
this paper is connected to the area of Topic Modeling
(Blei and Lafferty, 2009) where the goal is to reduce
the surface complexity of text documents by mod-
eling them as mixtures over a finite set of topics2.
While the inferred models are usually flat, in that
no explicit relationship exists among topics, more
complex, non-parametric, representations have been
proposed to elicit the hierarchical structure of vari-
ous datasets (Hofmann, 1999; Blei et al, 2010; Li
et al, 2007). Our purpose here is more specialized
and similar to that of Labeled LDA (Ramage et al,
2009a) or Fixed hLDA (Reisinger and Pas?ca, 2009)
where the set of topics associated with a document is
known a priori. In both cases, document labels are
mapped to constraints on the set of topics on which
the - otherwise unaltered - topic inference algorithm
is to be applied. Lastly, while most recent develop-
ments have been based on unsupervised data, it is
also worth mentioning earlier approaches like Topic
Signatures (Lin and Hovy, 2000) where words (or
phrases) characteristic of a topic are identified using
a statistical test of dependence. Our first model ex-
tends this approach to the hierarchical setting, build-
ing actual topic models based on the selected vocab-
ulary.
3 Information-Theoretic Approach
The assumption that topics are known a-priori al-
lows us to extend the concept of Topic Signatures to
a hierarchical setting. Lin and Hovy (2000) describe
a Topic Signature as a list of words highly correlated
with a target concept, and use a ?2 estimator over
labeled data to decide as to the allocation of a word
to a topic. Here, the sub-categories of a node corre-
spond to the topics. However, since the hierarchy is
naturally organized in a generic-to-specific fashion,
2Here we use the term topic to describe a normalized distri-
bution over a fixed vocabulary V .
for each node we select words that have the least dis-
criminative power between the node?s children. The
rationale is that, if a word can discriminate well be-
tween one child and all others, then it belongs in that
child?s node.
3.1 Word Assignment
The algorithm proceeds in two phases. In the first
phase, the hierarchy tree is traversed in a bottom-up
fashion to compile word frequency information un-
der each node. In the second phase, the hierarchy
is traversed top-down and, at each step, words get
assigned to the current node based on whether they
can discriminate between the current node?s chil-
dren. Once a word has been assigned on a given
path, it can no longer be assigned to any other node
on this path. Thus, within a path, a word always
takes on the meaning of the one topic to which it has
been assigned.
The discriminative power of a term with respect
to node N is formalized based on one of the follow-
ing measures:
Entropy of the a posteriori children category dis-
tribution for a given w.
Ent(w) = ?
?
C?Sub(N)
p(C|w) log(p(C|w) (3)
Cross-Entropy between the a priori children cat-
egory distribution and the a posteriori children cate-
gories distribution conditioned on the appearance of
w.
CrossEnt(w) = ?
?
C?Sub(N)
p(C) log(p(C|w)) (4)
?2 score, similar to Lin and Hovy (2000) but ap-
plied to classification tasks that can involve an ar-
bitrary number of (sub-)categories. The number of
degrees of freedom of the ?2 distribution is a func-
tion of the number of children.
?2(w) =
?
i?{w,w}
?
C?Sub(N)
(nC(i)? p(C)p(i))2
p(C)p(i)
(5)
To identify words exhibiting an unusually low dis-
criminative power between the children categories,
we assume a gaussian distribution of the score used
and select those whose score is at least ? = 2 stan-
dard deviations away from the population mean3.
3Although this makes the decision process less arbitrary
671
Algorithm 1 Generative process for hLLDA
? For each topic t ? H
? Draw ?t = (?t,1, . . . , ?t,V )T ? Dir(?|?)
? For each document, d ? {1, 2 . . .K}
? Draw a random path assignment cd ? H
? Draw a distribution over levels along cd, ?d ?
Dir(?|?)
? Draw a document length n ? ?H
? For each word wd,i ? {wd,1, wd,2, . . . wd,n},
? Draw level zd,i ?Mult(?d)
? Draw word wd,i ?Mult(?cd [zd,i])
3.2 Topic Definition & Mixing Proportions
Based on the final word assignments, we estimate
the probability of word wi in topic Tk, as:
P (wi|Tk) =
nCk(wi)
nCk
(6)
with nCk(wi) the total number of occurrence of wi
in documents under Ck, and nCk the total number of
words in documents under Ck.
Given the individual word assignments we eval-
uate the mixing proportions using corpus-level esti-
mates, which are computed by averaging the mixing
proportions of all the training documents.
4 Hierarchical Bayesian Approach
The previous approach, while attractive in its sim-
plicity, makes a strong claim that a word can be
emitted by at most one node on any given path. A
more interesting model might stem from allowing
soft word-topic assignments, where any topic on the
document?s path may emit any word in the vocabu-
lary space.
We consider a modified version of hierarchical
LDA (Blei et al, 2010), where the underlying tree
structure is known a priori and does not have to
be inferred from data. The generative story for this
model, which we designate as hierarchical Labeled-
LDA (hLLDA), is shown in Algorithm 1. Just as
with Fixed Structure LDA4 (Reisinger and Pas?ca,
than with a hand-selected threshold, this raises the issue of iden-
tifying the true distribution for the estimator used.
4Our implementation of hLLDA was partially
based on the UTML toolkit which is available at
https://github.com/joeraii/
2009), the topics used for inference are, for each
document, those found on the path from the hierar-
chy root to the document itself. Once the target path
cd ? H is known, the model reduces to LDA over
the set of topics comprising cd. Given that the joint
distribution p(?, z, w|cd) is intractable (Blei et al,
2003), we use collapsed Gibbs-sampling (Griffiths
and Steyvers, 2004) to obtain individual word-level
assignments. The probability of assigning wi, the
ith word in document d, to the jth topic on path cd,
conditioned on all other word assignments, is given
by:
p(zi = j|z?i,w, cd) ?
nd?i,j + ?
|cd|(?+ 1)
?
nwi?i,j + ?
V (? + 1)
(7)
where nd?i,j is the frequency of words from docu-
ment d assigned to topic j, nwi?i,j is the frequency
of word wi in topic j, ? and ? are Dirichlet con-
centration parameters for the path-topic and topic-
word multinomials respectively, and V is the vocab-
ulary size. Equation 7 can be understood as defin-
ing the unormalized posterior word-level assignment
distribution as the product of the current level mix-
ing proportion ?i and of the current estimate of the
word-topic conditional probability p(wi|zi). By re-
peatedly resampling from this distribution we ob-
tain individual word assignments which in turn al-
low us to estimate the topic multinomials and the
per-document mixing proportions. Specifically, the
topic multinomials are estimated as:
?cd[j],i = p(wi|zcd[j]) =
nwizcd[j] + ??
n?zcd[j] + V ?
(8)
while the per-document mixing proportions ?d can
be estimated as:
?d,j ?
nd?,j + ?
nd + |cd|?
,?j ? 1, . . . , cd (9)
Although we experimented with hyper-parameter
learning (Dirichlet concentration parameter ?), do-
ing so did not significantly impact the final model.
The results we report are therefore based on stan-
dard values for the hyper-parameters (? = 1 and
? = 0.1).
5 Experimental Results
We compared the predictive power of our model to
that of several language models. In every case, we
672
compute the perplexity of the model over the held-
out dataW = {w1 . . .wn} given the modelM and
the observed (training) data, namely:
perplM(W) = exp(?
1
n
n?
i=1
1
|wi|
|wi|?
j=1
log pM(wi,j))
(10)
5.1 Data Preprocessing
Our experiments focused on the English portion of
the DMOZ dataset5 (about 2.1 million entries). The
raw dataset was randomized and divided according
to a 98% training (31M words), 1% development
(320k words), 1% testing (320k words) split. Gists
were tokenized using simple tokenization rules, with
no stemming, and were case-normalized. Akin to
Berger and Mittal (2000) we mapped numerical to-
kens to the NUM placeholder and selected the V =
65535 most frequent words as our vocabulary. Any
token outside of this set was mapped to the OOV to-
ken. We did not perform any stop-word filtering.
5.2 Reference Models
Our reference models consists of several n-gram
(n ? [1, 3]) language models, none of which makes
use of the hierarchical information available from
the corpus. Under these models, the probability of
a given string is given by:
p(w) =
|s|?
i=1
p(wi|wi?1, . . . ,wi?(n?1)) (11)
We used the SRILM toolkit (Stolcke, 2002), en-
abling Kneser-Ney smoothing with default param-
eters.
Note that an interesting model to include here
would have been one that jointly infers a hierarchy
of topics as well as the topics that comprise it, much
like the regular hierarchical LDA algorithm (Blei et
al., 2010). While we did not perform this experiment
as part of this work, this is definitely an avenue for
future work. We are especially interested in seeing
whether an automatically inferred hierarchy of top-
ics would fundamentally differ from the manually-
curated hierarchy used by DMOZ.
5We discarded the Top/World portion of the hierarchy.
5.3 Experimental Results
The perplexities obtained for the hierarchical and n-
gram models are reported in Table 1.
reg all
# documents 1153000 2083949
avg. gist length 15.47 15.36
1-gram 1644.10 1414.98
2-gram 352.10 287.09
3-gram 239.08 179.71
entropy 812.91 1037.70
cross-entropy 1167.07 1869.90
?2 1639.29 1693.76
hLLDA 941.16 983.77
Table 1: Perplexity of the hierarchical models and the
reference n-gram models over the entire DMOZ dataset
(all), and the non-Regional portion of the dataset (reg).
When taken on the entire hierarchy (all), the per-
formance of the Bayesian and entropy-based mod-
els significantly exceeds that of the 1-gram model
(significant under paired t-test, both with p-value <
2.2 ? 10?16) while remaining well below that of ei-
ther the 2 or 3 gram models. This suggests that, al-
though the hierarchy plays a key role in the appear-
ance of content in DMOZ gists, word context is also
a key factor that needs to be taken into account: the
two families of models we propose are based on the
bag-of-word assumption and, by design, assume that
words are drawn i.i.d. from an underlying distribu-
tion. While it is not clear how one could extend the
information-theoretic models to include such con-
text, we are currently investigating enhancements to
the hLLDA model along the lines of the approach
proposed in Wallach (2006).
A second area of analysis is to compare the per-
formance of the various models on the entire hier-
archy versus on the non-Regional portion of the tree
(reg). We can see that the perplexity of the proposed
models decreases while that of the flat n-grams mod-
els increase. Since the non-Regional portion of the
DMOZ hierarchy is organized more consistently in
a semantic fashion6, we believe this reflects the abil-
ity of the hierarchical models to take advantage of
6The specificity of the Regional sub-tree has also been dis-
cussed by previous work (Ramage et al, 2009b), justifying a
special treatment for that part of the DMOZ dataset.
673
Figure 1: Perplexity of the proposed algorithms against the 1-gram baseline for each of the 14 top level DMOZ cate-
gories: Arts, Business, Computer, Games, Health, Home, News, Recreation, Reference, Regional, Science, Shopping,
Society, Sports.
the corpus structure to represent the content of the
summaries. On the other hand, the Regional por-
tion of the dataset seems to contribute a significant
amount of noise to the hierarchy, leading to a loss in
performance for those models.
We can observe that while hLLDA outperforms
all information-theoretical models when applied to
the entire DMOZ corpus, it falls behind the entropy-
based model when restricted to the non-regional
section of the corpus. Also if the reduction in
perplexity remains limited for the entropy, ?2 and
hLLDA models, the cross-entropy based model in-
curs a more significant boost in performance when
applied to the more semantically-organized portion
of the corpus. The reason behind such disparity in
behavior is not clear and we plan on investigating
this issue as part of our future work.
Further analyzing the impact of the respective
DMOZ sub-sections, we show in Figure 1 re-
sults for the hierarchical and 1-gram models when
trained and tested over the 14 main sub-trees of
the hierarchy. Our intuition is that differences
in the organization of those sub-trees might af-
fect the predictive power of the various mod-
els. Looking at sub-trees we can see that the
trend is the same for most of them, with the best
level of perplexity being achieved by the hierar-
chical Bayesian model, closely followed by the
information-theoretical model using entropy as its
selection criterion.
6 Conclusion
In this paper we have demonstrated the creation of a
topic-model of Web summaries using the hierarchy
of a popular Web directory. This hierarchy provides
a backbone around which we crystalize hierarchical
topic models. Individual topics exhibit increasing
specificity as one goes down a path in the tree. While
we focused on Web summaries, this model can be
readily adapted to any Web-related content that can
be seen as a mixture of the component topics appear-
ing along a paths in the hierarchy. Such model can
become a key resource for the fine-grained distinc-
tion between generic and specific elements of lan-
guage in a large, heterogenous corpus.
Acknowledgments
This material is based on research supported in part
by the U.S. National Science Foundation (NSF) un-
der IIS-05-34871. Any opinions, findings and con-
clusions or recommendations expressed in this ma-
terial are those of the authors and do not necessarily
reflect the views of the NSF.
674
References
A. Berger and V. Mittal. 2000. Ocelot: a system for
summarizing web pages. In Proceedings of the 23rd
Annual International ACM SIGIR Conference on Re-
search and Development in Information Retrieval (SI-
GIR?00), pages 144?151.
David M. Blei and J. Lafferty. 2009. Topic models. In A.
Srivastava and M. Sahami, editors, Text Mining: The-
ory and Applications. Taylor and Francis.
David M. Blei, Andrew Ng, and Michael Jordan. 2003.
Latent dirichlet alocation. JMLR, 3:993?1022.
David M. Blei, Thomas L. Griffiths, and Micheal I. Jor-
dan. 2010. The nested chinese restaurant process and
bayesian nonparametric inference of topic hierarchies.
In Journal of ACM, volume 57.
Jean-Yves Delort, Bernadette Bouchon-Meunier, and
Maria Rifqi. 2003. Enhanced web document sum-
marization using hyperlinks. In Hypertext 2003, pages
208?215.
Thomas L. Griffiths and Mark Steyvers. 2004. Finding
scientific topics. PNAS, 101(suppl. 1):5228?5235.
Thomas Hofmann. 1999. The cluster-abstraction model:
Unsupervised learning of topic hierarchies from text
data. In Proceedings of IJCAI?99.
Wei Li, David Blei, and Andrew McCallum. 2007. Non-
parametric bayes pachinko allocation. In Proceedings
of the Proceedings of the Twenty-Third Conference An-
nual Conference on Uncertainty in Artificial Intelli-
gence (UAI-07), pages 243?250, Corvallis, Oregon.
AUAI Press.
C.-Y. Lin and E. Hovy. 2000. The automated acqui-
sition of topic signatures for text summarization. In
Proceedings of the 18th conference on Computational
linguistics, pages 495?501.
Daniel Ramage, David Hall, Ramesh Nallapati, and
Christopher D. Manning. 2009a. Labeled lda: A
supervised topic model for credit attribution in multi-
labeled corpora. In Proceedings of the 2009 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing (EMNLP 2009), Singapore, pages 248?256.
Daniel Ramage, Paul Heymann, Christopher D. Man-
ning, and Hector Garcia-Molina. 2009b. Clustering
the tagged web. In Proceedings of the Second ACM In-
ternational Conference on Web Search and Data Min-
ing, WSDM ?09, pages 54?63, New York, NY, USA.
ACM.
Joseph Reisinger and Marius Pas?ca. 2009. Latent vari-
able models of concept-attribute attachment. In ACL-
IJCNLP ?09: Proceedings of the Joint Conference of
the 47th Annual Meeting of the ACL and the 4th Inter-
national Joint Conference on Natural Language Pro-
cessing of the AFNLP: Volume 2, pages 620?628, Mor-
ristown, NJ, USA. Association for Computational Lin-
guistics.
Andreas Stolcke. 2002. Srilm - an extensible language
modeling toolkit. In Proc. Intl. Conf. on Spoken Lan-
guage Processing, vol. 2, pages 901?904, September.
Jian-Tao Sun, Dou Shen, Hua-Jun Zeng, Qiang Yang,
Yuchang Lu, and Zheng Chen. 2005. Web-page sum-
marization using clickthrough data. In SIGIR 2005,
pages 194?201.
Hanna M. Wallach. 2006. Topic modeling: Beyond bag-
of-words. In Proceedings of the 23rd International
Conference on Machine Learning, Pittsburgh, Penn-
sylvania, U.S., pages 977?984.
675
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1241?1251,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Approximation Strategies for Multi-Structure Sentence Compression
Kapil Thadani
Department of Computer Science
Columbia University
New York, NY 10025, USA
kapil@cs.columbia.edu
Abstract
Sentence compression has been shown to
benefit from joint inference involving both
n-gram and dependency-factored objec-
tives but this typically requires expensive
integer programming. We explore instead
the use of Lagrangian relaxation to decou-
ple the two subproblems and solve them
separately. While dynamic programming
is viable for bigram-based sentence com-
pression, finding optimal compressed trees
within graphs is NP-hard. We recover ap-
proximate solutions to this problem us-
ing LP relaxation and maximum spanning
tree algorithms, yielding techniques that
can be combined with the efficient bigram-
based inference approach using Lagrange
multipliers. Experiments show that these
approximation strategies produce results
comparable to a state-of-the-art integer
linear programming formulation for the
same joint inference task along with a sig-
nificant improvement in runtime.
1 Introduction
Sentence compression is a text-to-text genera-
tion task in which an input sentence must be
transformed into a shorter output sentence which
accurately reflects the meaning in the input
and also remains grammatically well-formed.
The compression task has received increasing
attention in recent years, in part due to the
availability of datasets such as the Ziff-Davis cor-
pus (Knight and Marcu, 2000) and the Edinburgh
compression corpora (Clarke and Lapata, 2006),
from which the following example is drawn.
Original: In 1967 Chapman, who had cultivated a con-
ventional image with his ubiquitous tweed jacket and pipe,
by his own later admission stunned a party attended by his
friends and future Python colleagues by coming out as a
homosexual.
Compressed: In 1967 Chapman, who had cultivated a
conventional image, stunned a party by coming out as a
homosexual.
Following an assumption often used in compres-
sion systems, the compressed output in this corpus
is constructed by dropping tokens from the input
sentence without any paraphrasing or reordering.
1
A number of diverse approaches have been
proposed for deletion-based sentence compres-
sion, including techniques that assemble the out-
put text under an n-gram factorization over the
input text (McDonald, 2006; Clarke and Lapata,
2008) or an arc factorization over input depen-
dency parses (Filippova and Strube, 2008; Galanis
and Androutsopoulos, 2010; Filippova and Altun,
2013). Joint methods have also been proposed that
invoke integer linear programming (ILP) formu-
lations to simultaneously consider multiple struc-
tural inference problems?both over n-grams and
input dependencies (Martins and Smith, 2009) or
n-grams and all possible dependencies (Thadani
and McKeown, 2013). However, it is well-
established that the utility of ILP for optimal infer-
ence in structured problems is often outweighed
by the worst-case performance of ILP solvers
on large problems without unique integral solu-
tions. Furthermore, approximate solutions can
often be adequate for real-world generation sys-
tems, particularly in the presence of linguistically-
motivated constraints such as those described by
Clarke and Lapata (2008), or domain-specific
1
This is referred to as extractive compression by Cohn and
Lapata (2008) & Galanis and Androutsopoulos (2010) fol-
lowing the terminology used in document summarization.
1241
pruning strategies such as the use of sentence tem-
plates to constrain the output.
In this work, we develop approximate inference
strategies to the joint approach of Thadani and
McKeown (2013) which trade the optimality guar-
antees of exact ILP for faster inference by sep-
arately solving the n-gram and dependency sub-
problems and using Lagrange multipliers to en-
force consistency between their solutions. How-
ever, while the former problem can be solved
efficiently using the dynamic programming ap-
proach of McDonald (2006), there are no efficient
algorithms to recover maximum weighted non-
projective subtrees in a general directed graph.
Maximum spanning tree algorithms, commonly
used in non-projective dependency parsing (Mc-
Donald et al, 2005), are not easily adaptable to
this task since the maximum-weight subtree is not
necessarily a part of the maximum spanning tree.
We therefore consider methods to recover ap-
proximate solutions for the subproblem of finding
the maximum weighted subtree in a graph, com-
mon among which is the use of a linear program-
ming relaxation. This linear program (LP) ap-
pears empirically tight for compression problems
and our experiments indicate that simply using the
non-integral solutions of this LP in Lagrangian re-
laxation can empirically lead to reasonable com-
pressions. In addition, we can recover approxi-
mate solutions to this problem by using the Chu-
Liu Edmonds algorithm for recovering maximum
spanning trees (Chu and Liu, 1965; Edmonds,
1967) over the relatively sparse subgraph defined
by a solution to the relaxed LP. Our proposed ap-
proximation strategies are evaluated using auto-
mated metrics in order to address the question: un-
der what conditions should a real-world sentence
compression system implementation consider ex-
act inference with an ILP or approximate infer-
ence? The contributions of this work include:
? An empirically-useful technique for approx-
imating the maximum-weight subtree in a
weighted graph using LP-relaxed inference.
? Multiple approaches to generate good ap-
proximate solutions for joint multi-structure
compression, based on Lagrangian relaxation
to enforce equality between the sequential
and syntactic inference subproblems.
? An analysis of the tradeoffs incurred by joint
approaches with regard to runtime as well as
performance under automated measures.
2 Multi-Structure Sentence Compression
Even though compression is typically formulated
as a token deletion task, it is evident that drop-
ping tokens independently from an input sentence
will likely not result in fluent and meaningful com-
pressive text. Tokens in well-formed sentences
participate in a number of syntactic and seman-
tic relationships with other tokens, so one might
expect that accounting for heterogenous structural
relationships between tokens will improve the co-
herence of the output sentence. Furthermore,
much recent work has focused on the challenge
of joint sentence extraction and compression, also
known as compressive summarization (Martins
and Smith, 2009; Berg-Kirkpatrick et al, 2011;
Almeida and Martins, 2013; Li et al, 2013; Qian
and Liu, 2013), in which questions of efficiency
are paramount due to the larger problems in-
volved; however, these approaches largely restrict
compression to pruning parse trees, thereby im-
posing a dependency on parser performance. We
focus in this work on a sentence-level compression
system to approximate the ILP-based inference of
Thadani and McKeown (2013) which does not re-
strict compressions to follow input parses but per-
mits the generation of novel dependency relations
in output compressions.
The rest of this section is organized as fol-
lows: ?2.1 provies an overview of the joint se-
quential and syntactic objective for compression
from Thadani and McKeown (2013) while ?2.2
discusses the use of Lagrange multipliers to en-
force consistency between the different structures
considered. Following this, ?2.3 discusses a dy-
namic program to find maximum weight bigram
subsequences from the input sentence, while ?2.4
covers LP relaxation-based approaches for ap-
proximating solutions to the problem of finding a
maximum-weight subtree in a graph of potential
output dependencies. Finally, ?2.5 discusses the
features and model training approach used in our
experimental results which are presented in ?3.
2.1 Joint objective
We begin with some notation. For an input sen-
tence S comprised of n tokens including dupli-
cates, we denote the set of tokens in S by T ,
{t
i
: 1 ? i ? n}. Let C represent a compres-
sion of S and let x
i
? {0, 1} denote an indicator
variable whose value corresponds to whether to-
ken t
i
? T is present in the compressed sentence
1242
C. In addition, we define bigram indicator vari-
ables y
ij
? {0, 1} to represent whether a particular
order-preserving bigram
2
?t
i
, t
j
? from S is present
as a contiguous bigram inC as well as dependency
indicator variables z
ij
? {0, 1} corresponding to
whether the dependency arc t
i
? t
j
is present in
the dependency parse of C. The score for a given
compression C can now be defined to factor over
its tokens, n-grams and dependencies as follows.
score(C) =
?
t
i
?T
x
i
? ?
tok
(t
i
)
+
?
t
i
?T?{START},
t
j
?T?{END}
y
ij
? ?
bgr
(?t
i
, t
j
?)
+
?
t
i
?T?{ROOT},
t
j
?T
z
ij
? ?
dep
(t
i
? t
j
) (1)
where ?
tok
, ?
bgr
and ?
dep
are feature-based scoring
functions for tokens, bigrams and dependencies
respectively. Specifically, each ?
v
(?) ? w
>
v
?
v
(?)
where ?
v
(?) is a feature map for a given vari-
able type v ? {tok, bgr, dep} and w
v
is the cor-
responding vector of learned parameters.
The inference task involves recovering the high-
est scoring compression C
?
under a particular set
of model parameters w.
C
?
= argmax
C
score(C)
= argmax
x,y,z
x
>
?
tok
+ y
>
?
bgr
+ z
>
?
dep
(2)
where the incidence vector x , ?x
i
?
t
i
?T
repre-
sents an entire token configuration over T , with y
and z defined analogously to represent configura-
tions of bigrams and dependencies. ?
v
, ??
v
(?)?
denotes a corresponding vector of scores for each
variable type v under the current model parame-
ters. In order to recover meaningful compressions
by optimizing (2), the inference step must ensure:
1. The configurations x, y and z are consistent
with each other, i.e., all configurations cover
the same tokens.
2. The structural configurations y and z are
non-degenerate, i.e, the bigram configuration
y represents an acyclic path while the depen-
dency configuration z forms a tree.
2
Although Thadani and McKeown (2013) is not restricted
to bigrams or order-preserving n-grams, we limit our discus-
sion to this scenario as it also fits the assumptions of McDon-
ald (2006) and the datasets of Clarke and Lapata (2006).
These requirements naturally rule out simple ap-
proximate inference formulations such as search-
based approaches for the joint objective.
3
An
ILP-based inference solution is demonstrated in
Thadani and McKeown (2013) that makes use of
linear constraints over the boolean variables x
i
, y
ij
and z
ij
to guarantee consistency, as well as aux-
iliary real-valued variables and constraints repre-
senting the flow of commodities (Magnanti and
Wolsey, 1994) in order to establish structure in y
and z. In the following section, we propose an al-
ternative formulation that exploits the modularity
of this joint objective.
2.2 Lagrangian relaxation
Dual decomposition (Komodakis et al, 2007) and
Lagrangian relaxation in general are often used
for solving joint inference problems which are
decomposable into individual subproblems linked
by equality constraints (Koo et al, 2010; Rush
et al, 2010; Rush and Collins, 2011; DeNero
and Macherey, 2011; Martins et al, 2011; Das
et al, 2012; Almeida and Martins, 2013). This
approach permits sub-problems to be solved sepa-
rately using problem-specific efficient algorithms,
while consistency over the structures produced is
enforced through Lagrange multipliers via itera-
tive optimization. Exact solutions are guaranteed
when the algorithm converges on a consistent pri-
mal solution, although this convergence itself is
not guaranteed and depends on the tightness of
the underlying LP relaxation. The primary advan-
tage of this technique is the ability to leverage the
underlying structure of the problems in inference
rather than relying on a generic ILP formulation
while still often producing exact solutions.
The multi-structure inference problem de-
scribed in the previous section seems in many
ways to be a natural fit to such an approach since
output scores factor over different types of struc-
ture that comprise the output compression. Even if
ILP-based approaches perform reasonably at the
scale of single-sentence compression problems,
the exponential worst-case complexity of general-
purpose ILPs will inevitably pose challenges when
scaling up to (a) handle larger inputs, (b) use
higher-order structural fragments, or (c) incorpo-
rate additional models.
3
This work follows Thadani and McKeown (2013) in re-
covering non-projective trees for inference. However, recov-
ering projective trees is tractable when a total ordering of out-
put tokens is assumed. This will be addressed in future work.
1243
Consider once more the optimization problem
characterized by (2) The two structural problems
that need to be solved in this formulation are
the extraction of a maximum-weight acyclic sub-
sequence of bigrams y from the lattice of all
order-preserving bigrams from S and the recov-
ery of a maximum-weight directed subtree z. Let
?(y) ? {0, 1}
n
denote the incidence vector of
tokens contained in the n-gram sequence y and
?(z) ? {0, 1}
n
denote the incidence vector of
words contained in the dependency tree z. We can
now rewrite the objective in (2) while enforcing
the constraint that the words contained in the se-
quence y are the same as the words contained in
the tree z, i.e., ?(y) = ?(z), by introducing a
vector of Lagrange multipliers ? ? R
n
. In addi-
tion, the token configuration x can be rewritten in
the form of a weighted combination of ?(y) and
?(z) to ensure its consistency with y and z. This
results in the following Lagrangian:
L(?,y, z) = y
>
?
bgr
+ z
>
?
dep
+ ?
>
tok
(? ??(y) + (1? ?) ? ?(z))
+ ?
>
(?(y)? ?(z)) (3)
Finding the y and z that maximize this Lagrangian
above yields a dual objective, and the dual prob-
lem corresponding to the primal objective speci-
fied in (2) is therefore the minimization of this ob-
jective over the Lagrange multipliers ?.
min
?
max
y,z
L(?,y, z)
=min
?
max
y
y
>
?
bgr
+ (?+ ? ? ?
tok
)
>
?(y)
+ max
z
z
>
?
dep
? (?+ (? ? 1) ? ?
tok
)
>
?(z)
=min
?
max
y
f(y,?, ?,?)
+ max
z
g(z,?, ?,?) (4)
This can now be solved with the iterative subgra-
dient algorithm illustrated in Algorithm 1. In each
iteration i, the algorithm solves for y
(i)
and z
(i)
under ?
(i)
, then generates ?
(i+1)
to penalize in-
consistencies between ?(y
(i)
) and ?(z
(i)
). When
?(y
(i)
) = ?(z
(i)
), the resulting primal solution is
exact, i.e., y
(i)
and z
(i)
represent the optimal struc-
tures under (2). Otherwise, if the algorithm starts
oscillating between a few primal solutions, the un-
derlying LP must have a non-integral solution in
which case approximation heuristics can be em-
Algorithm 1 Subgradient-based joint inference
Input: scores ?, ratio ?, repetition limit l
max
,
iteration limit i
max
, learning rate schedule ?
Output: token configuration x
1: ?
(0)
? ?0?
n
2: M ? ?,M
repeats
? ?
3: for iteration i < i
max
do
4:
?
y? argmax
y
f(y,?, ?,?)
5:
?
z ? argmax
z
g(z,?, ?,?)
6: if ?(
?
y) = ?(
?
z) then return ?(
?
y)
7: if ?(
?
y) ?M then
8: M
repeats
?M
repeats
? {?(
?
y)}
9: if ?(
?
z) ?M then
10: M
repeats
?M
repeats
? {?(
?
z)}
11: if |M
repeats
| ? l
max
then break
12: M ?M ? {?(
?
y),?(
?
z)}
13: ?
(i+1)
? ?
(i)
? ?
i
(?(
?
y)? ?(
?
z))
return argmax
x?M
repeats
score(x)
ployed.
4
The application of this Lagrangian relax-
ation strategy is contingent upon the existence of
algorithms to solve the maximization subproblems
for f(y,?, ?,?) and g(z,?, ?,?). The following
sections discuss our approach to these problems.
2.3 Bigram subsequences
McDonald (2006) provides a Viterbi-like dynamic
programming algorithm to recover the highest-
scoring sequence of order-preserving bigrams
from a lattice, either in unconstrained form or with
a specific length constraint. The latter requires a
dynamic programming table Q[i][r] which repre-
sents the best score for a compression of length r
ending at token i. The table can be populated us-
ing the following recurrence:
Q[i][1] = score(S, START, i)
Q[i][r] = max
j<i
Q[j][r ? 1] + score(S, i, j)
Q[i][R+ 1] = Q[i][R] + score(S, i, END)
where R is the required number of output tokens
and the scoring function is defined as
score(S, i, j) , ?
bgr
(?t
i
, t
j
?) + ?
j
+ ? ? ?
tok
(t
j
)
so as to solve f(y,?, ?,?) from (4). This ap-
proach requires O(n
2
R) time in order to identify
4
Heuristic approaches (Komodakis et al, 2007; Rush et
al., 2010), tightening (Rush and Collins, 2011) or branch and
bound (Das et al, 2012) can still be used to retrieve optimal
solutions, but we did not explore these strategies here.
1244
AB
C D
-20
3
10 2
1
Figure 1: An example of the difficulty of recover-
ing the maximum-weight subtree (B?C, B?D)
from the maximum spanning tree (A?C, C?B,
B?D).
the highest scoring sequence y and corresponding
token configuration ?(y).
2.4 Dependency subtrees
The maximum-weight non-projective subtree
problem over general graphs is not as easily
solved. Although the maximum spanning tree for
a given token configuration can be recovered ef-
ficiently, Figure 1 illustrates that the maximum-
scoring subtree is not necessarily found within
it. The problem of recovering a maximum-weight
subtree in a graph has been shown to be NP-hard
even with uniform edge weights (Lau et al, 2006).
In order to produce a solution to this subprob-
lem, we use an LP relaxation of the relevant
portion of the ILP from Thadani and McKeown
(2013) by omitting integer constraints over the to-
ken and dependency variables in x and z respec-
tively. For simplicity, however, we describe the
ILP version rather than the relaxed LP in order to
motivate the constraints with their intended pur-
pose rather than their effect in the relaxed prob-
lem. The objective for this LP is given by
max
x,z
x
>
?
?
tok
+ z
>
?
dep
(5)
where the vector of token scores is redefined as
?
?
tok
, (1? ?) ? ?
tok
? ? (6)
in order to solve g(z,?, ?,?) from (4).
Linear constraints are introduced to produce de-
pendency structures that are close to the optimal
dependency trees. First, tokens in the solution
must only be active if they have a single active in-
coming dependency edge. In addition, to avoid
producing multiple disconnected subtrees, only
one dependency is permitted to attach to the ROOT
pseudo-token.
x
j
?
?
i
z
ij
= 0, ?t
j
? T (7)
?
j
z
ij
= 1, if t
i
= ROOT (8)
ROOT
Production was closed down at Ford last night .
5
?
3,1
= 1
2
1
?
3,9
= 1
Figure 2: An illustration of commodity values for
a valid solution of the non-relaxed ILP.
In order to avoid cycles in the dependency tree,
we include additional variables to establish single-
commodity flow (Magnanti and Wolsey, 1994) be-
tween all pairs of tokens. These ?
ij
variables carry
non-negative real values which must be consumed
by active tokens that they are incident to.
?
ij
? 0, ?t
i
, t
j
? T (9)
?
i
?
ij
?
?
k
?
jk
= x
j
, ?t
j
? T (10)
These constraints ensure that cyclic structures are
not possible in the non-relaxed ILP. In addition,
they serve to establish connectivity for the de-
pendency structure z since commodity can only
originate in one location?at the pseudo-token
ROOT which has no incoming commodity vari-
ables. However, in order to enforce these prop-
erties on the output dependency structure, this
acyclic, connected commodity structure must con-
strain the activation of the z variables.
?
ij
? C
max
z
ij
? 0, ?t
i
, t
j
? T (11)
where C
max
is an arbitrary upper bound on the
value of ?
ij
variables. Figure 2 illustrates how
these commodity flow variables constrain the out-
put of the ILP to be a tree. However, the effect
of these constraints is diminished when solving an
LP relaxation of the above problem.
In the LP relaxation, x
i
and z
ij
are redefined as
real-valued variables in [0, 1], potentially resulting
in fractional values for dependency and token indi-
cators. As a result, the commodity flow network is
able to establish connectivity but cannot enforce a
tree structure, for instance, directed acyclic struc-
tures are possible and token indicators x
i
may be
partially be assigned to the solution structure. This
poses a challenge in implementing ?(z) which is
needed to recover a token configuration from the
solution of this subproblem.
We propose two alternative solutions to address
this issue in the context of the joint inference strat-
egy. The first is to simply use the relaxed token
configuration identified by the LP in Algorithm 1,
1245
i.e., to set ?(z?) = x? where x? and z? represent the
real-valued counterparts of the incidence vectors x
and z. The viability of this approximation strategy
is due to the following:
? The relaxed LP is empirically fairly tight,
yielding integral solutions 89% of the time on
the compression datasets described in ?3.
? The bigram subproblem is guaranteed to re-
turn a well-formed integral solution which
obeys the imposed compression rate, so we
are assured of a source of valid?if non-
optimal?solutions in line 13 of Algorithm 1.
We also consider another strategy that attempts to
approximate a valid integral solution to the depen-
dency subproblem. In order to do this, we first
include an additional constraint in the relaxed LP
which restrict the number of tokens in the output
to a specific number of tokens R that is given by
an input compression rate.
?
i
x
i
= R (12)
The addition of this constraint to the relaxed LP
reduces the rate of integral solutions drastically?
from 89% to approximately 33%?but it serves to
ensure that the resulting token configuration x? has
at least as many non-zero elements asR, i.e., there
are at least as many tokens activated in the LP so-
lution as are required in a valid solution.
We then construct a subgraph G(z?) consisting
of all dependency edges that were assigned non-
zero values in the solution, assigning to each edge
a score equal to the score of that edge in the LP as
well as the score of its dependent word, i.e., each
z
ij
in G(z?) is assigned a score of ?
dep
(?t
i
, t
j
?) ?
?
j
+ (1? ?) ? ?
tok
(t
j
). Since the commodity flow
constraints in (9)?(11) ensure a connected z?, it is
therefore possible to recover a maximum-weight
spanning tree from G(z?) using the Chu-Liu Ed-
monds algorithm (Chu and Liu, 1965; Edmonds,
1967).
5
Although the runtime of this algorithm
is cubic in the size of the input graph, it is fairly
speedy when applied on relatively sparse graphs
such as the solutions to the LP described above.
The resulting spanning tree is a useful integral
approximation of z? but, as mentioned previously,
may contain more nodes than R due to fractional
values in x?; we therefore repeatedly prune leaves
5
A detailed description of the Chu-Liu Edmonds algo-
rithm for MSTs is available in McDonald et al (2005).
with the lowest incoming edge weight in the cur-
rent tree until exactly R nodes remain. The result-
ing tree is assumed to be a reasonable approxima-
tion of the optimal integral solution to this LP.
The Chu-Liu Edmonds algorithm is also em-
ployed for another purpose: when the underly-
ing LP for the joint inference problem is not
tight?a frequent occurrence in our compression
experiments?Algorithm 1 will not converge on
a single primal solution and will instead oscillate
between solutions that are close to the dual opti-
mum. We identify this phenomenon by counting
repeated solutions and, if they exceed some thresh-
old l
max
with at least one repeated solution from
either subproblem, we terminate the update proce-
dure for Lagrange multipliers and instead attempt
to identify a good solution from the repeating ones
by scoring them under (2). It is straightforward to
recover and score a bigram configuration y from a
token configuration ?(z). However, scoring so-
lutions produced by the dynamic program from
?2.3 also requires the score over a corresponding
parse tree; this can be recovered by constructing
a dependency subgraph containing across only the
tokens that are active in ?(y) and retrieving the
maximum spanning tree for that subgraph using
the Chu-Liu Edmonds algorithm.
2.5 Learning and Features
The features used in this work are largely based on
the features from Thadani and McKeown (2013).
? ?
tok
contains features for part-of-speech
(POS) tag sequences of length up to 3 around
the token, features for the dependency label
of the token conjoined with its POS, lexical
features for verb stems and non-word sym-
bols and morphological features that identify
capitalized sequences, negations and words
in parentheses.
? ?
bgr
contains features for POS patterns in a
bigram, the labels of dependency edges in-
cident to it, its likelihood under a Gigaword
language model (LM) and an indicator for
whether it is present in the input sentence.
? ?
dep
contains features for the probability of
a dependency edge under a smoothed depen-
dency grammar constructed from the Penn
Treebank and various conjunctions of the fol-
lowing features: (a) whether the edge appears
as a dependency or ancestral relation in the
input parse (b) the directionality of the depen-
1246
dency (c) the label of the edge (d) the POS
tags of the tokens incident to the edge and
(e) the labels of their surrounding chunks and
whether the edge remains within the chunk.
For the experiments in the following section, we
trained models using a variant of the structured
perceptron (Collins, 2002) which incorporates
minibatches (Zhao and Huang, 2013) for easy par-
allelization and faster convergence.
6
Overfitting
was avoided by averaging parameters and mon-
itoring performance against a held-out develop-
ment set during training. All models were trained
using variants of the ILP-based inference approach
of Thadani and McKeown (2013). We followed
Martins et al (2009) in using LP-relaxed inference
during learning, assuming algorithmic separabil-
ity (Kulesza and Pereira, 2007) for these problems.
3 Experiments
We ran compression experiments over the
newswire (NW) and broadcast news transcription
(BN) corpora compiled by Clarke and Lapata
(2008) which contain gold compressions pro-
duced by human annotators using only word
deletion. The datasets were filtered to eliminate
instances with less than 2 and more than 110
tokens for parser compatibility and divided into
training/development/test sections following the
splits from Clarke and Lapata (2008), yielding
953/63/603 instances for the NW corpus and
880/78/404 for the BN corpus. Gold dependency
parses were approximated by running the Stanford
dependency parser
7
over reference compressions.
Following evaluations in machine translation
as well as previous work in sentence compres-
sion (Unno et al, 2006; Clarke and Lapata, 2008;
Martins and Smith, 2009; Napoles et al, 2011b;
Thadani and McKeown, 2013), we evaluate sys-
tem performance using F
1
metrics over n-grams
and dependency edges produced by parsing sys-
tem output with RASP (Briscoe et al, 2006) and
the Stanford parser. All ILPs and LPs were solved
using Gurobi,
8
a high-performance commercial-
grade solver. Following a recent analysis of com-
pression evaluations (Napoles et al, 2011b) which
revealed a strong correlation between system com-
pression rate and human judgments of compres-
sion quality, we constrained all systems to produce
6
We used a minibatch size of 4 in all experiments.
7
http://nlp.stanford.edu/software/
8
http://www.gurobi.com
compressed output at a specific rate?determined
by the the gold compressions available for each
instance?to ensure that the reported differences
between the systems under study are meaningful.
3.1 Systems
We report results over the following systems
grouped into three categories of models: tokens +
n-grams, tokens + dependencies, and joint models.
? 3-LM: A reimplementation of the unsuper-
vised ILP of Clarke and Lapata (2008) which
infers order-preserving trigram variables pa-
rameterized with log-likelihood under an LM
and a significance score for token variables
inspired by Hori and Furui (2004), as well as
various linguistically-motivated constraints
to encourage fluency in output compressions.
? DP: The bigram-based dynamic program of
McDonald (2006) described in ?2.3.
9
? LP?MST: An approximate inference ap-
proach based on an LP relaxation of ILP-
Dep. As discussed in ?2.4, a maximum span-
ning tree is recovered from the output of the
LP and greedily pruned in order to generate
a valid integral solution while observing the
imposed compression rate.
? ILP-Dep: A version of the joint ILP of
Thadani and McKeown (2013) without n-
gram variables and corresponding features.
? DP+LP?MST: An approximate joint infer-
ence approach based on Lagrangian relax-
ation that uses DP for the maximum weight
subsequence problem and LP?MST for the
maximum weight subtree problem.
? DP+LP: Another Lagrangian relaxation ap-
proach that pairs DP with the non-integral
solutions from an LP relaxation of the maxi-
mum weight subtree problem (cf. ?2.4).
? ILP-Joint: The full ILP from Thadani and
McKeown (2013), which provides an upper
bound on the performance of the proposed
approximation strategies.
The learning rate schedule for the Lagrangian re-
laxation approaches was set as ?
i
, ?/(? + i),
10
while the hyperparameter ? was tuned using the
9
For consistent comparisons with the other systems, our
reimplementation does not include the k-best inference strat-
egy presented in McDonald (2006) for learning with MIRA.
10
? was set to 100 for aggressive subgradient updates.
1247
Inference n-grams F
1
% Syntactic relations F
1
% Inference
objective technique n = 1 2 3 4 z Stanford RASP time (s)
n-grams
3-LM (CL08) 74.96 60.60 46.83 38.71 - 60.52 57.49 0.72
DP (McD06) 78.80 66.04 52.67 42.39 - 63.28 57.89 0.01
deps
LP?MST 79.61 64.32 50.36 40.97 66.57 66.82 59.70 0.07
ILP-Dep 80.02 65.99 52.42 43.07 72.43 67.63 60.78 0.16
DP + LP?MST 79.50 66.75 53.48 44.33 64.63 67.69 60.94 0.24
joint DP + LP 79.10 68.22 55.05 45.81 65.74 68.24 62.04 0.12
ILP-Joint (TM13) 80.13 68.34 55.56 46.60 72.57 68.89 62.61 0.31
Table 1: Experimental results for the BN corpus, averaged over 3 gold compressions per instance. All
systems were restricted to compress to the size of the median gold compression yielding an average
compression rate of 77.26%.
Inference n-grams F
1
% Syntactic relations F
1
% Inference
objective technique n = 1 2 3 4 z Stanford RASP time (s)
n-grams
3-LM (CL08) 66.66 51.59 39.33 30.55 - 50.76 49.57 1.22
DP (McD06) 73.18 58.31 45.07 34.77 - 56.23 51.14 0.01
deps
LP?MST 73.32 55.12 41.18 31.44 61.01 58.37 52.57 0.12
ILP-Dep 73.76 57.09 43.47 33.44 65.45 60.06 54.31 0.28
DP + LP?MST 73.13 57.03 43.79 34.01 57.91 58.46 53.20 0.33
joint DP + LP 72.06 59.83 47.39 37.72 58.13 58.97 53.78 0.21
ILP-Joint (TM13) 74.00 59.90 47.22 37.01 65.65 61.29 56.24 0.60
Table 2: Experimental results for the NW corpus with all systems compressing to the size of the gold
compression, yielding an average compression rate of 70.24%. In both tables, bold entries show signifi-
cant gains within a column under the paired t-test (p < 0.05) and Wilcoxon?s signed rank test (p < 0.01).
development split of each corpus.
11
3.2 Results
Tables 1 and 2 summarize the results from our
compression experiments on the BN and NW cor-
pora respectively. Starting with the n-gram ap-
proaches, the performance of 3-LM leads us to
observe that the gains of supervised learning far
outweigh the utility of higher-order n-gram factor-
ization, which is also responsible for a significant
increase in wall-clock time. In contrast, DP is an
order of magnitude faster than all other approaches
studied here although it is not competitive under
parse-based measures such as RASP F
1
% which
is known to correlate with human judgments of
grammaticality (Clarke and Lapata, 2006).
We were surprised by the strong performance
of the dependency-based inference techniques,
which yielded results that approached the joint
model in both n-gram and parse-based measures.
11
We were surprised to observe that performance improved
significantly when ? was set closer to 1, thereby emphasiz-
ing token features in the dependency subproblem. The final
values chosen were ?
BN
= 0.9 and ?
NW
= 0.8.
The exact ILP-Dep approach halves the run-
time of ILP-Joint to produce compressions that
have similar (although statistically distinguish-
able) scores. Approximating dependency-based
inference with LP?MST yields similar perfor-
mance for a further halving of runtime; however,
the performance of this approach is notably worse.
Turning to the joint approaches, the strong
performance of ILP-Joint is expected; less so
is the relatively high but yet practically reason-
able runtime that it requires. We note, how-
ever, that these ILPs are solved using a highly-
optimized commercial-grade solver that can uti-
lize all CPU cores
12
while our approximation
approaches are implemented as single-processed
Python code without significant effort toward op-
timization. Comparing the two approximation
strategies shows a clear performance advantage
for DP+LP over DP+LP?MST: the latter ap-
proach entails slower inference due to the over-
head of running the Chu-Liu Edmonds algorithm
at every dual update, and furthermore, the error in-
troduced by approximating an integral solution re-
12
16 cores in our experimental environment.
1248
sults in a significant decrease in dependency recall.
In contrast, DP+LP directly optimizes the dual
problem by using the relaxed dependency solution
to update Lagrange multipliers and achieves the
best performance on parse-based F
1
outside of the
slower ILP approaches. Convergence rates also
vary for these two techniques: DP+LP has a lower
rate of empirical convergence (15% on BN and 4%
on NW) when compared to DP+LP?MST (19%
on BN and 6% on NW).
Figure 3 shows the effect of input sentence
length on inference time and performance for ILP-
Joint and DP+LP over the NW test corpus.
13
The
timing results reveal that the approximation strat-
egy is consistently faster than the ILP solver. The
variation in RASP F
1
% with input size indicates
the viability of a hybrid approach which could bal-
ance accuracy and speed by using ILP-Joint for
smaller problems and DP+LP for larger ones.
4 Related Work
Sentence compression is one of the better-studied
text-to-text generation problems and has been ob-
served to play a significant role in human summa-
rization (Jing, 2000; Jing and McKeown, 2000).
Most approaches to sentence compression are su-
pervised (Knight and Marcu, 2002; Riezler et
al., 2003; Turner and Charniak, 2005; McDon-
ald, 2006; Unno et al, 2006; Galley and McK-
eown, 2007; Nomoto, 2007; Cohn and Lapata,
2009; Galanis and Androutsopoulos, 2010; Gan-
itkevitch et al, 2011; Napoles et al, 2011a; Fil-
ippova and Altun, 2013) following the release of
datasets such as the Ziff-Davis corpus (Knight and
Marcu, 2000) and the Edinburgh compression cor-
pora (Clarke and Lapata, 2006; Clarke and Lap-
ata, 2008), although unsupervised approaches?
largely based on ILPs?have also received con-
sideration (Clarke and Lapata, 2007; Clarke and
Lapata, 2008; Filippova and Strube, 2008). Com-
pression has also been used as a tool for document
summarization (Daum?e and Marcu, 2002; Zajic
et al, 2007; Clarke and Lapata, 2007; Martins
and Smith, 2009; Berg-Kirkpatrick et al, 2011;
Woodsend and Lapata, 2012; Almeida and Mar-
tins, 2013; Molina et al, 2013; Li et al, 2013;
Qian and Liu, 2013), with recent work formulating
the summarization task as joint sentence extrac-
tion and compression and often employing ILP or
Lagrangian relaxation. Monolingual compression
13
Similar results were observed for the BN test corpus.
Figure 3: Effect of input size on (a) inference time,
and (b) the corresponding difference in RASP
F
1
% (ILP-Joint ? DP+LP) on the NW corpus.
also faces many obstacles common to decoding in
machine translation, and a number of approaches
which have been proposed to combine phrasal and
syntactic models (Huang and Chiang, 2007; Rush
and Collins, 2011) inter alia offer directions for
future research into compression problems.
5 Conclusion
We have presented approximate inference strate-
gies to jointly compress sentences under bigram
and dependency-factored objectives by exploiting
the modularity of the task and considering the two
subproblems in isolation. Experiments show that
one of these approximation strategies produces re-
sults comparable to a state-of-the-art integer linear
program for the same joint inference task with a
60% reduction in average inference time.
Acknowledgments
The author is grateful to Alexander Rush for help-
ful discussions and to the anonymous reviewers
for their comments. This work was supported
by the Intelligence Advanced Research Projects
Activity (IARPA) via Department of Interior Na-
tional Business Center (DoI/NBC) contract num-
ber D11PC20153. The U.S. Government is autho-
rized to reproduce and distribute reprints for Gov-
ernmental purposes notwithstanding any copy-
right annotation thereon.
14
14
The views and conclusions contained herein are those of
the authors and should not be interpreted as necessarily repre-
senting the official policies or endorsements, either expressed
or implied, of IARPA, DoI/NBC, or the U.S. Government.
1249
References
Miguel Almeida and Andr?e F. T. Martins. 2013. Fast
and robust compressive summarization with dual de-
composition and multi-task learning. In Proceed-
ings of ACL, pages 196?206, August.
Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein.
2011. Jointly learning to extract and compress. In
Proceedings of ACL-HLT, pages 481?490.
Ted Briscoe, John Carroll, and Rebecca Watson. 2006.
The second release of the RASP system. In Pro-
ceedings of the ACL-COLING Interactive Presenta-
tion Sessions.
Yoeng-jin Chu and Tseng-hong Liu. 1965. On the
shortest arborescence of a directed graph. Science
Sinica, 14:1396?1400.
James Clarke and Mirella Lapata. 2006. Models
for sentence compression: a comparison across do-
mains, training requirements and evaluation mea-
sures. In Proceedings of ACL-COLING, pages 377?
384.
James Clarke and Mirella Lapata. 2007. Modelling
compression with discourse constraints. In Proceed-
ings of EMNLP-CoNLL, pages 1?11.
James Clarke and Mirella Lapata. 2008. Global in-
ference for sentence compression: an integer linear
programming approach. Journal for Artificial Intel-
ligence Research, 31:399?429, March.
Trevor Cohn and Mirella Lapata. 2008. Sentence com-
pression beyond word deletion. In Proceedings of
COLING, pages 137?144.
Trevor Cohn and Mirella Lapata. 2009. Sentence com-
pression as tree transduction. Journal of Artificial
Intelligence Research, 34(1):637?674, April.
Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models. In Proceedings of
EMNLP, pages 1?8.
Dipanjan Das, Andr?e F. T. Martins, and Noah A.
Smith. 2012. An exact dual decomposition algo-
rithm for shallow semantic parsing with constraints.
In Proceedings of the First Joint Conference on Lexi-
cal and Computational Semantics (*SEM), SemEval
?12, pages 209?217.
Hal Daum?e, III and Daniel Marcu. 2002. A noisy-
channel model for document compression. In Pro-
ceedings of ACL, pages 449?456.
John DeNero and Klaus Macherey. 2011. Model-
based aligner combination using dual decomposi-
tion. In Proceedings of ACL-HLT, pages 420?429.
Jack R. Edmonds. 1967. Optimum branchings. Jour-
nal of Research of the National Bureau of Standards,
71B:233?240.
Katja Filippova and Yasemin Altun. 2013. Overcom-
ing the lack of parallel data in sentence compression.
In Proceedings of EMNLP, pages 1481?1491.
Katja Filippova and Michael Strube. 2008. Depen-
dency tree based sentence compression. In Proceed-
ings of INLG, pages 25?32.
Dimitrios Galanis and Ion Androutsopoulos. 2010. An
extractive supervised two-stage method for sentence
compression. In Proceedings of HLT-NAACL, pages
885?893.
Michel Galley and Kathleen McKeown. 2007. Lex-
icalized Markov grammars for sentence compres-
sion. In Proceedings of HLT-NAACL, pages 180?
187, April.
Juri Ganitkevitch, Chris Callison-Burch, Courtney
Napoles, and Benjamin Van Durme. 2011. Learn-
ing sentential paraphrases from bilingual parallel
corpora for text-to-text generation. In Proceedings
of EMNLP, pages 1168?1179.
Chiori Hori and Sadaoki Furui. 2004. Speech summa-
rization: an approach through word extraction and a
method for evaluation. IEICE Transactions on In-
formation and Systems, E87-D(1):15?25.
Liang Huang and David Chiang. 2007. Forest rescor-
ing: Faster decoding with integrated language mod-
els. In Proceedings of ACL, pages 144?151, June.
Hongyan Jing and Kathleen R. McKeown. 2000. Cut
and paste based text summarization. In Proceedings
of NAACL, pages 178?185.
Hongyan Jing. 2000. Sentence reduction for auto-
matic text summarization. In Proceedings of the
Conference on Applied Natural Language Process-
ing, pages 310?315.
Kevin Knight and Daniel Marcu. 2000. Statistics-
based summarization - step one: Sentence compres-
sion. In Proceedings of AAAI, pages 703?710.
Kevin Knight and Daniel Marcu. 2002. Summariza-
tion beyond sentence extraction: a probabilistic ap-
proach to sentence compression. Artificial Intelli-
gence, 139(1):91?107, July.
Nikos Komodakis, Nikos Paragios, and Georgios Tzir-
itas. 2007. MRF optimization via dual decomposi-
tion: Message-passing revisited. In Proceedings of
ICCV, pages 1?8, Oct.
Terry Koo, Alexander M. Rush, Michael Collins,
Tommi Jaakkola, and David Sontag. 2010. Dual
decomposition for parsing with non-projective head
automata. In Proceedings of EMNLP, pages 1288?
1298.
Alex Kulesza and Fernando Pereira. 2007. Structured
learning with approximate inference. In John C.
Platt, Daphne Koller, Yoram Singer, and Sam T.
Roweis, editors, NIPS. Curran Associates, Inc.
1250
Hoong Chuin Lau, Trung Hieu Ngo, and Bao Nguyen
Nguyen. 2006. Finding a length-constrained
maximum-sum or maximum-density subtree and
its application to logistics. Discrete Optimization,
3(4):385 ? 391.
Chen Li, Fei Liu, Fuliang Weng, and Yang Liu. 2013.
Document summarization via guided sentence com-
pression. In Proceedings of EMNLP, pages 490?
500, Seattle, Washington, USA, October.
Thomas L. Magnanti and Laurence A. Wolsey. 1994.
Optimal trees. In Technical Report 290-94,
Massechusetts Institute of Technology, Operations
Research Center.
Andr?e F. T. Martins and Noah A. Smith. 2009. Sum-
marization with a joint model for sentence extraction
and compression. In Proceedings of the Workshop
on Integer Linear Programming for Natural Lan-
gauge Processing, pages 1?9.
Andr?e F. T. Martins, Noah A. Smith, and Eric P. Xing.
2009. Concise integer linear programming formu-
lations for dependency parsing. In Proceedings of
ACL-IJCNLP, pages 342?350.
Andr?e F. T. Martins, Noah A. Smith, Pedro M. Q.
Aguiar, and M?ario A. T. Figueiredo. 2011. Dual
decomposition with many overlapping components.
In Proceedings of EMNLP, pages 238?249.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Haji?c. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings
of EMNLP-HLT, pages 523?530.
Ryan McDonald. 2006. Discriminative sentence com-
pression with soft syntactic evidence. In Proceed-
ings of EACL, pages 297?304.
Alejandro Molina, Juan-Manuel Torres-Moreno, Eric
SanJuan, Iria da Cunha, and Gerardo Eugenio
Sierra Mart??nez. 2013. Discursive sentence com-
pression. In Computational Linguistics and Intelli-
gent Text Processing, volume 7817, pages 394?407.
Springer.
Courtney Napoles, Chris Callison-Burch, Juri Ganitke-
vitch, and Benjamin Van Durme. 2011a. Paraphras-
tic sentence compression with a character-based
metric: tightening without deletion. In Proceedings
of the Workshop on Monolingual Text-To-Text Gen-
eration, pages 84?90.
Courtney Napoles, Benjamin Van Durme, and Chris
Callison-Burch. 2011b. Evaluating sentence com-
pression: pitfalls and suggested remedies. In Pro-
ceedings of the Workshop on Monolingual Text-To-
Text Generation, pages 91?97.
Tadashi Nomoto. 2007. Discriminative sentence com-
pression with conditional random fields. Infor-
mation Processing and Management, 43(6):1571?
1587, November.
Xian Qian and Yang Liu. 2013. Fast joint compression
and summarization via graph cuts. In Proceedings
of EMNLP, pages 1492?1502, Seattle, Washington,
USA, October.
Stefan Riezler, Tracy H. King, Richard Crouch, and
Annie Zaenen. 2003. Statistical sentence condensa-
tion using ambiguity packing and stochastic disam-
biguation methods for lexical-functional grammar.
In Proceedings of HLT-NAACL, pages 118?125.
Alexander M. Rush and Michael Collins. 2011. Ex-
act decoding of syntactic translation models through
Lagrangian relaxation. In Proceedings of ACL-HLT,
pages 72?82.
Alexander M. Rush, David Sontag, Michael Collins,
and Tommi Jaakkola. 2010. On dual decomposition
and linear programming relaxations for natural lan-
guage processing. In Proceedings of EMNLP, pages
1?11.
Kapil Thadani and Kathleen McKeown. 2013. Sen-
tence compression with joint structural inference. In
Proceedings of CoNLL.
Jenine Turner and Eugene Charniak. 2005. Super-
vised and unsupervised learning for sentence com-
pression. In Proceedings of ACL, pages 290?297.
Yuya Unno, Takashi Ninomiya, Yusuke Miyao, and
Jun?ichi Tsujii. 2006. Trimming CFG parse trees
for sentence compression using machine learning
approaches. In Proceedings of ACL-COLING, pages
850?857.
Kristian Woodsend and Mirella Lapata. 2012. Mul-
tiple aspect summarization using integer linear pro-
gramming. In Proceedings of EMNLP, pages 233?
243.
David Zajic, Bonnie J. Dorr, Jimmy Lin, and Richard
Schwartz. 2007. Multi-candidate reduction: Sen-
tence compression as a tool for document summa-
rization tasks. Information Processing and Manage-
ment, 43(6):1549?1570, Nov.
Kai Zhao and Liang Huang. 2013. Minibatch and par-
allelization for online large margin structured learn-
ing. In Proceedings of HLT-NAACL, pages 370?
379, Atlanta, Georgia, June.
1251
Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 13?20,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Corpus Creation for New Genres:
A Crowdsourced Approach to PP Attachment
Mukund Jha, Jacob Andreas, Kapil Thadani, Sara Rosenthal and Kathleen McKeown
Department of Computer Science
Columbia University
New York, NY 10027, USA
{mj2472,jda2129}@columbia.edu, {kapil,sara,kathy}@cs.columbia.edu
Abstract
This paper explores the task of building an ac-
curate prepositional phrase attachment corpus
for new genres while avoiding a large invest-
ment in terms of time and money by crowd-
sourcing judgments. We develop and present
a system to extract prepositional phrases and
their potential attachments from ungrammati-
cal and informal sentences and pose the subse-
quent disambiguation tasks as multiple choice
questions to workers from Amazon?s Mechan-
ical Turk service. Our analysis shows that
this two-step approach is capable of producing
reliable annotations on informal and poten-
tially noisy blog text, and this semi-automated
strategy holds promise for similar annotation
projects in new genres.
1 Introduction
Recent decades have seen rapid development in nat-
ural language processing tools for parsing, semantic
role-labeling, machine translation, etc., and much of
this success can be attributed to the study of statisti-
cal techniques and the availability of large annotated
corpora for training. However, the performance of
these systems is heavily dependent on the domain
and genre of their training data, i.e. systems trained
on data from a particular domain tend to perform
poorly when applied to other domains and adap-
tation techniques are not always able to compen-
sate (Dredze et al, 2007). For this reason, achiev-
ing high performance on new domains and genres
frequently necessitates the collection of annotated
training data from those domains and genres, a time-
consuming and frequently expensive process.
This paper examines the problem of collecting
high-quality annotations for new genres with a focus
on time and cost efficiency. We explore the well-
studied but non-trivial task of prepositional phrase
(PP) attachment and describe a semi-automated sys-
tem for identifying accurate attachments in blog
data, which is frequently noisy and difficult to parse.
PP attachment disambiguation involves finding a
correct attachment for a prepositional phrase in a
sentence. For example, in the sentence ?We went to
John?s house on Saturday?, the phrase ?on Satur-
day? attaches to the verb ?went?. In another exam-
ple, ?We went to John?s house on 12th Street?, the
PP ?on 12th street? attaches to the noun ?John?s
house?. This sort of disambiguation requires se-
mantic knowledge about sentences that is difficult
to glean from their surface form, a problem which
is compounded by the informal nature and irregular
vocabulary of blog text.
In this work, we investigate whether crowd-
sourced human judgments are capable of distin-
guishing appropriate attachments. We present a sys-
tem that simplifies the attachment problem and rep-
resents it in a format that can be intuitively tackled
by humans.
Our approach to this task makes use of a heuristic-
based system built on a shallow parser that identi-
fies the likely words or phrases to which a PP can
attach. To subsequently select the correct attach-
ment, we leverage human judgments from multi-
ple untrained annotators (referred to here as work-
ers) through Amazon?s Mechanical Turk 1, an online
marketplace for work. This two-step approach of-
1http://www.mturk.amazon.com
13
fers distinct advantages: the automated system cuts
down the space of potential attachments effectively
with little error, and the disambiguation task can be
reduced to small multiple choice questions which
can be tackled quickly and aggregated reliably.
The remainder of this paper focuses on the PP at-
tachment task over blog text and our analysis of the
resulting aggregate annotations. We note, however,
that this type of semi-automated approach is poten-
tially applicable to any task which can be reliably
decomposed into independent judgments that un-
trained annotators can tackle (e.g., quantifier scop-
ing, conjunction scope). This work is intended as
an initial step towards the development of efficient
hybrid annotation tools that seamlessly incorporate
aggregate human wisdom alongside effective algo-
rithms.
2 Related Work
Identifying PP attachments is an essential task for
building syntactic parse trees. While this task has
been studied using fully-automated systems, many
of them rely on parse tree output for predicting po-
tential attachments (Ratnaparkhi et al, 1994; Yeh
and Vilain, 1998; Stetina and Nagao, 1997; Zavrel
et al, 1997). However, systems that rely on good
parses are unlikely to perform well on new genres
such as blogs and machine translated texts for which
parse tree training data is not readily available.
Furthermore, the predominant dataset for eval-
uating PP attachment is the RRR dataset (Ratna-
parkhi et al, 1994) which consists of PP attach-
ment cases from the Wall Street Journal portion of
the Penn Treebank. Instead of complete sentences,
this dataset consists of sets of the form {V,N1,P,N2}
where {P,N2} is the PP and {V,N1} are the poten-
tial attachments. This simplification of the PP at-
tachment task to a choice between two alternatives
is unrealistic when considering the potential long-
distance attachments encountered in real-world text.
While blogs and other web text, such as discus-
sion forums and emails, have been studied for a va-
riety of tasks such as information extraction (Hong
and Davison, 2009), social networking (Gruhl et
al., 2004), and sentiment analysis (Leshed and
Kaye, 2006), we are not aware of any previous ef-
forts to gather syntactic data (such as PP attach-
ments) in the genre. Syntactic methods such as
POS tagging, parsing and structural disambiguation
are commonly used when analyzing well-structured
text. Including the use of syntactic information
has yielded improvements in accuracy in speech
recognition (Chelba and Jelenik, 1998; Collins et
al., 2005) and machine translation (DeNeefe and
Knight, 2009; Carreras and Collins, 2009). We an-
ticipate that datasets such as ours could be useful for
such tasks as well.
Amazon?s Mechanical Turk (MTurk) has become
very popular for manual annotation tasks and has
been shown to perform equally well over labeling
tasks such as affect recognition, word similarity, rec-
ognizing textual entailment, event temporal order-
ing and word sense disambiguation, when compared
to annotations from experts (Snow et al, 2008).
While these tasks were small in scale and intended to
demonstrate the viability of annotation via MTurk,
it has also proved effective in large-scale tasks in-
cluding the collection of accurate speech transcrip-
tions (Gruenstein et al, 2009). In this paper we ex-
plore a method for corpus building on a large scale
in order to extend annotation into new domains and
genres.
We previously evaluated crowdsourced PP attach-
ment annotation by using MTurk workers to repro-
duce PP attachments from the Wall Street Journal
corpus (Rosenthal et al, 2010). The results demon-
strated that MTurk workers are capable of identi-
fying PP attachments in newswire text, but the ap-
proach used to generate attachment options is de-
pendent on the existing gold-standard parse trees
and cannot be used on corpora where parse trees are
not available. In this paper, we build on the semi-
automated annotation principle while avoiding the
dependency on parsers, allowing us to apply this
technique to the noisy and informal text found in
blogs.
3 System Description
Our system must both identify PPs and generate a
list of potential attachments for each PP in this sec-
tion. Figure 1 illustrates the structure of the system.
First, the system extracts sentences from scraped
blog data. Text is preprocessed by stripping HTML
tags, advertisements, non-Latin and non-printable
14
PPs
PPs
Question 
 Builder
PP Identifier
Chunker
+
Preprocessor
sentences
Chunked 
Chunked 
sentences
point predictor
Attachment
attachments
Potential
Mechanical
     Turk
Questions
forNew domain
data (Blogs)
Figure 1: Overview of question generation system
characters. Emoticon symbols are removed using a
standard list. 2
The cleaned data is then partitioned into sentences
using the NLTK sentence splitter. 3 In order to
compensate for the common occurrence of informal
punctuation and web-specific symbols in blog text,
we replace all punctuation symbols between quo-
tation marks and parentheses with placeholder tags
(e.g. ?QuestionMark?) during the sentence splitting
process and do the same for website names, time
markers and referring phrases (e.g. @John). Ad-
ditionally, we attempt to re-split sentences at ellipsis
boundaries if they are longer than 80 words and dis-
card them if this fails.
As parsers trained on news corpora tend to per-
form poorly on unstructured texts like blogs, we
rely on a chunker to partition sentences into phrases.
Choosing a good chunker is essential to this ap-
proach: around 35% of the cases in which the cor-
rect attachment is not predicted by the system are
due to chunker error. We experimented with differ-
ent chunkers over a random sample of 50 sentences
before selecting a CRF-based chunker (Phan, 2006)
for its robust performance.
The chunker output is initially processed by fus-
ing together chunks in order to ensure that a single
chunk represents a complete attachment point. Two
consecutive NP chunks are fused if the first contains
an element with a possessive part of speech tag (e.g.
John?s book), while particle chunks (PRT) are fused
with the VP chunks that precede them (e.g. pack
up). These chunked sentences are then processed
to identify PPs and potential attachment points for
them, which can then be used to generate questions
2http://www.astro.umd.edu/?marshall/
smileys.html
3http://www.nltk.org
for MTurk workers.
3.1 PP Extraction
PPs can be classified into two broad categories based
on the number of chunks they contain. A simple
PP consists of only two chunks: a preposition and
one noun phrase, while a compound PP has multi-
ple simple PPs attached to its primary noun phrase.
For example, in the sentence ?I just made some last-
minute changes to the latest issue of our newsletter?,
the PP with preposition ?to? can be considered to be
either the simple PP ?to the latest issue? or the com-
pound PP ?to the latest issue of our newsletter?.
We handle compound PPs by breaking them down
into multiple simple PPs; compound PPs can be re-
covered by identifying the attachments of their con-
stituent simple PPs. Our simple PP extraction al-
gorithm identifies PPs as a sequence of chunks that
consist of one or more prepositions terminating in a
noun phrase or gerund.
3.2 Attachment Point Prediction
A PP usually attaches to the noun or verb phrase pre-
ceding it or, in some cases, can modify a following
clause by attaching to the head verb. We build a set
of rules based on this intuition to pick out the poten-
tial attachments in the sentence; these rules are de-
scribed in Table 1. The rules are applied separately
for each PP in a sentence and in the same sequence
as mentioned in the table (except for rule 4, which
is applied while choosing a chunk using any of the
other rules).
15
Rule Example
1 Choose closest NP and VP preceding the PP. I made modifications to our newsletter.
2 Choose next closest VP preceding the PP if the VP selected in (1)
contains a VBG.
He snatched the disk flying away with one hand.
3 Choose first VP following the PP. On his desk he has a photograph.
4 All chunks inside parentheses are skipped, unless the PP falls within
parentheses.
Please refer to the new book (second edition) for
more notes.
5 Choose anything immediately preceding the PP that is not out of
chunk and has not already been picked.
She is full of excitement.
6 If a selected NP contains the word and, expand it into two options,
one with the full expression and one with only the terms following
and.
He is president and chairman of the board.
7 For PPs in chains of the form P-NP-P-NP (PP-PP), choose all the
NPs in the chain preceding the PP and apply all the above rules
considering the whole chain as a single PP.
They found my pictures of them from the concert.
8 If there are fewer than four options after applying the above rules,
also select the VP preceding the last VP selected, the NP preceding
the last NP selected, and the VP following the last VP picked.
Table 1: List of rules for attachment point predictor. In the examples, PPs are denoted by boldfaced text and potential
attachment options are underlined.
4 Experiments
An experimental study was undertaken to test our
hypothesis that we could obtain reliable annotations
on informal genres using MTurk workers. Here we
describe the dataset and our methods.
4.1 Dataset and Interface
We used a corpus of blog posts made on LiveJour-
nal 4 for system development and evaluation. Only
posts from English-speaking countries (i.e. USA,
Canada, UK, Australia and New Zealand) were con-
sidered for this study.
The interface provided to MTurk workers showed
the sentence on a plain background with the PP high-
lighted and a statement prompting them to pick the
phrase in the sentence that the given PP modified.
The question was followed by a list of options. In
addition, we provided MTurk workers the option to
indicate problems with the given PP or the listed op-
tions. Workers could write in the correct attachment
if they determined that it wasn?t present in the list of
options, or the correct PP if the one they were pre-
sented with was malformed. This allowed them to
correct errors made by the chunker and automated
attachment point predictor. In all cases, workers
were forced to pick the best answer among the op-
tions regardless of errors. We also supplied a num-
4http://www.livejournal.com
ber of examples covering both well-formed and er-
roneous cases to aid them in identifying appropriate
attachments.
4.2 Experimental Setup
For our experiment, we randomly selected 1000
questions from the output produced by the system
and provided each question to five different MTurk
workers, thereby obtaining five different judgments
for each PP attachment case. Workers were paid four
cents per question and the average completion time
per task was 48 seconds. In total $225 was spent
on the full study with $200 spent on the workers and
$25 on MTurk fees.The total time taken for the study
was approximately 16 hours.
A pilot study was carried out with 50 sentences
before the full study to test the annotation interface
and experiment with different ways of presenting the
PP and attachment options to workers. During this
study, we observed that while workers were will-
ing to suggest correct answers or PPs when faced
with erroneous questions, they often opted to not
pick any of the options provided unless the question
was well-formed. This was problematic because, in
many cases, expert annotators were able to identify
the most appropriate attachment option. Therefore,
in the final study we forced them to pick the most
suitable option from the given choices before indi-
cating errors and writing in alternatives.
16
Workers in agreement Number of questions Accuracy Coverage
5 (unanimity) 389 97.43% 41.33%
? 4 (majority) 689 94.63% 73.22%
? 3 (majority) 887 88.61% 94.26%
? 2 (plurality) 906 87.75% 96.28%
Total 941 84.48% 100%
Table 2: Accuracy and coverage over agreement thresholds
5 Evaluation corpus
In order to determine if the MTurk results were re-
liable, worker responses had to be validated by hav-
ing expert annotators perform the same task. For
this purpose, two of the authors annotated the 1000
questions used for the experiment independently and
compared their judgments. Disagreements were ob-
served in 127 cases; these were then resolved by a
pool of non-author annotators. If all three annota-
tors on a case disagreed with each other the question
was discarded; this situation occured 43 times. An
additional 16 questions were discarded because they
did not have a valid PP. For example, ?I am painting
with my blanket on today?. Here ?on today? is in-
correctly extracted as a PP because the particle ?on?
is tagged as a preposition. The rest of the analysis
presented in this section was performed on the re-
maining 941 sentences.
The annotators? judgments were compared to the
answers provided by the MTurk workers and, in
the case of disagreement between the experts and
the majority of workers, the sentences were man-
ually inspected to determine the reason. In five
cases, more than one valid attachment was possi-
ble; for example, in the sentence ?The video below is
of my favourite song on the album - A Real Woman?,
the PP ?of my favourite song? could attach to either
the noun phrase ?the video? or the verb ?is? and con-
veys the same meaning. In such cases, both the ex-
perts and the workers were considered to have cho-
sen the correct answer.
In 149 cases, the workers also augmented their
choices by providing corrections to incomplete an-
swers and badly constructed PPs. For example,
the PP ?of the Rings and Mikey? in the sentence
?Samwise from Lord of the Rings and Mikey from
The Goonies are the same actor ?? was corrected to
?of the Rings?. In 34/39 of the cases where the cor-
rect answer was not present in the options provided,
at least one worker indicated correct attachment for
the PP.
5.1 Attachment Prediction Evaluation
We measure the recall for our attachment point pre-
dictor as the number of questions for which the cor-
rect attachment appeared among the generated op-
tions divided by the total number of questions. The
system achieves a recall of 95.85% (902/941 ques-
tions). We observed that in many cases where the
correct attachment point was not predicted, it was
due to a chunker error. For example, in the following
sentence, ?Stop all the clocks , cut off the telephone
, Prevent the dog from barking with a juicy bone...?,
the PP ?from barking? attaches to the verb ?Pre-
vent?; however, due to an error in chunking ?Pre-
vent? is tagged as a noun phrase and hence is not
picked by our system. The correct attachment was
also occasionally missed when the attachment point
was too far from the PP. For example, in the sentence
?Fitting as many people as possible on one sofa and
under many many covers and getting intimate?, the
correct attachment for the PP ?under many many
covers? is the verb ?Fitting? but it is not picked by
our system.
Even though the correct attachment was not al-
ways given, the workers could still provide their own
correct answer. In the first example above, 3/5 work-
ers indicated that the correct attachment was not in
the list of options and wrote it in.
6 Results
Table 2 summarizes the results of the experiment.
We assess both the coverage and reliability of
worker predictions at various levels of worker agree-
ment. This serves as an indicator of the effective-
ness of the MTurk results: the accuracy can be taken
17
Figure 2: The number of questions in which exactly x
workers provided the correct answer
as a general confidence measure for worker predic-
tions; when five workers agree we can be 97.43%
confident in the correctness of their prediction, when
at least four workers agree we can be 94.63% con-
fident, etc. Unanimity indicates that all workers
agreed on an answer, majority indicates that more
than half of workers agreed on an answer, and plu-
rality indicates that two workers agreed on a single
answer, while the remaining three workers each se-
lected different answers. We observe that at high
levels of worker agreement, we get extremely high
accuracy but limited coverage of the data set; as
we decrease our standard for agreement, coverage
increases rapidly while accuracy remains relatively
high.
Figure 2 shows the number of workers providing
the correct answer on a per-question basis. This
illustrates the distribution of worker agreements
across questions. Note that in the majority of cases
(69.2%), at least four workers provided the correct
answer; in only 3.6% of cases were no workers able
to select the correct attachment.
Figure 3 shows the distribution of worker agree-
ments. Unlike Table 2, these figures are not cumu-
lative and include non-plurality two-worker agree-
ments. Note that the number of agreements dis-
cussed in this figure is greater than the 941 evaluated
because in some cases there were multiple agree-
ments on a single question. As an example, three
workers may choose one answer while the remain-
ing two workers choose another; this question then
produces both a three-worker agreement as well as a
two-worker agreement.
Figure 3: The number of cases in which exactly x work-
ers agreed on an answer
No. of options No. of cases Accuracy
< 4 179 86.59%
4 718 84.26%
> 4 44 79.55%
Table 3: Variation in worker performance with the num-
ber of attachment options presented
All questions on which there is agreement also
produce a majority vote, with one exception: the
2/2/1 agreement. Although the correct answer was
selected by one set of two workers in every case of
2/2/1 agreement, this is not particularly useful for
corpus-building as we have no way to identify a pri-
ori which set is correct. Fortunately, 2/2/1 agree-
ments were also quite rare and occurred in only 3%
of cases.
Figure 3 appears to indicate that instances of
agreement between two workers are unlikely to pro-
duce good attachments; they have a an average ac-
curacy of 37.2%. However, this is due in large part
to cases of 3/2 agreement, in which the two workers
in the minority are usually wrong, as well as cases of
2/2/1 agreement which contain at least one incorrect
instance of two-worker agreement. However, if we
only consider cases in which the two-worker agree-
ment forms a plurality (i.e. all other workers dis-
agree amongst themselves), we observe an average
accuracy of 64.3% which is similar to that of cases
of three-worker agreement (67.7%).
We also attempted to study the variation in worker
performance based on the complexity of the task;
specifically looking at how response accuracy var-
ied depending on the number of options that workers
were presented with. Although our system aimed to
18
Figure 4: Variation in accuracy with sentence length.
generate four attachment options per case, fewer op-
tions were produced for small sentences and opening
PPs while additional options were generated in sen-
tences containing PP-NP chains (see Table 1 for the
complete list of rules). Table 3 shows the variation in
accuracy with the number of options provided to the
workers. We might expect that an increased number
of options may be correlated with decreased accu-
racy and the data does indeed seem to suggest this
trend; however, we do not have enough datapoints
for the cases with fewer or more than four options to
verify whether this effect is significant.
We also analyzed the relationship between the
length of the sentence (in terms of number of words)
and the accuracy. Figure 4 indicates that as the
length of the sentence increases, the average accu-
racy decreases. This is not entirely unexpected as
lengthy sentences tend to be more complicated and
therefore harder for human readers to parse.
7 Conclusions and Future Work
We have shown that by working in conjunction
with automated attachment point prediction sys-
tems, MTurk workers are capable of annotating PP
attachment problems with high accuracy, even when
working with unstructured and informal blog text.
This work provides an immediate framework for the
building of PP attachment corpora for new genres
without a dependency on full parsing.
More broadly, the semi-automated framework
outlined in this paper is not limited to the task of
annotating PP attachments; indeed, it is suitable for
almost any syntactic or semantic annotation task
where untrained human workers can be presented
with a limited number of options for selection. By
dividing the desired annotation task into smaller
sub-tasks that can be tackled independently or in a
pipelined manner, we anticipate that more syntac-
tic information can be extracted from unstructured
text in new domains and genres without the sizable
investment of time and money normally associated
with hiring trained linguists to build new corpora.
To this end, we intend to further leverage the advent
of crowdsourcing resources in order to tackle more
sophisticated annotation tasks.
Acknowledgements
The authors would like to thank Kevin Lerman for
his help in formulating the original ideas for this
work. This material is based on research supported
in part by the U.S. National Science Foundation
(NSF) under IIS-05-34871. Any opinions, findings
and conclusions or recommendations expressed in
this material are those of the authors and do not nec-
essarily reflect the views of the NSF.
References
Xavier Carreras and Michael Collins. 2009. Non-
projective parsing for statistical machine translation.
In Proceedings of EMNLP, pages 200?209.
Ciprian Chelba and Frederick Jelenik. 1998. Structured
language modeling for speech recognition. In Pro-
ceedings of NLDB.
Michael Collins, Brian Roark, and Murat Saraclar.
2005. Discriminative syntactic language modeling for
speech recognition. In Proceedings of ACL, pages
507?514.
Steve DeNeefe and Kevin Knight. 2009. Synchronous
tree adjoining machine translation. In Proceedings of
EMNLP, pages 727?736.
Mark Dredze, John Blitzer, Partha Pratim Talukdar, Kuz-
man Ganchev, Joa?o Graca, and Fernando Pereira.
2007. Frustratingly hard domain adaptation for depen-
dency parsing. In Proceedings of the CoNLL Shared
Task Session of EMNLP-CoNLL, pages 1051?1055,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Alex Gruenstein, Ian McGraw, and Andrew Sutherland.
2009. A self-transcribing speech corpus: collecting
continuous speech with an online educational game.
In Proceedings of the Speech and Language Technol-
ogy in Education (SLaTE) Workshop.
19
Figure 5: HIT Interface for PP attachment task
Daniel Gruhl, R. Guha, David Liben-Nowell, and An-
drew Tomkins. 2004. Information diffusion through
blogspace. In Proceedings of WWW, pages 491?501.
Liangjie Hong and Brian D. Davison. 2009. A
classification-based approach to question answering in
discussion boards. In Proceedings of SIGIR, pages
171?178.
Gilly Leshed and Joseph ?Jofish? Kaye. 2006. Under-
standing how bloggers feel: recognizing affect in blog
posts. In CHI ?06 extended abstracts on Human fac-
tors in computing systems, pages 1019?1024.
Xuan-Hieu Phan. 2006. CRFChunker: CRF
English phrase chunker. http://crfchunker.
sourceforge.net.
Adwait Ratnaparkhi, Jeff Reynar, and Salim Roukos.
1994. A maximum entropy model for prepositional
phrase attachment. In Proceedings of HLT, pages 250?
255.
Sara Rosenthal, William J. Lipovsky, Kathleen McKe-
own, Kapil Thadani, and Jacob Andreas. 2010. Semi-
automated annotation for prepositional phrase attach-
ment. In Proceedings of LREC, Valletta, Malta.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-
drew Y. Ng. 2008. Cheap and fast?but is it good?:
evaluating non-expert annotations for natural language
tasks. In Proceedings of EMNLP, pages 254?263.
Jiri Stetina and Makoto Nagao. 1997. Corpus based PP
attachment ambiguity resolution with a semantic dic-
tionary. In Proceedings of the Workshop on Very Large
Corpora, pages 66?80.
Alexander S. Yeh and Marc B. Vilain. 1998. Some prop-
erties of preposition and subordinate conjunction at-
tachments. In Proceedings of COLING, pages 1436?
1442.
Jakub Zavrel, Walter Daelemans, and Jorn Veenstra.
1997. Resolving PP attachment ambiguities with
memory-based learning. In Proceedings of the Work-
shop on Computational Language Learning (CoNLL),
pages 136?144.
Appendix A: Mechanical Turk Interface
Figure 5 shows a screenshot of the interface pro-
vided to the Mechanical Turk workers for the PP at-
tachment task. By default, examples and additional
options are hidden but can be viewed using the links
provided. The screenshot illustrates a case in which
a worker is confronted with an incorrect PP and uses
the additional options to correct it.
20
Workshop on Monolingual Text-To-Text Generation, pages 43?53,
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 43?53,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
Towards Strict Sentence Intersection: Decoding and Evaluation Strategies
Kapil Thadani and Kathleen McKeown
Department of Computer Science
Columbia University
New York, NY 10027, USA
{kapil,kathy}@cs.columbia.edu
Abstract
We examine the task of strict sentence inter-
section: a variant of sentence fusion in which
the output must only contain the informa-
tion present in all input sentences and nothing
more. Our proposed approach involves align-
ment and generalization over the input sen-
tences to produce a generation lattice; we then
compare a standard search-based approach for
decoding an intersection from this lattice to an
integer linear program that preserves aligned
content while minimizing the disfluency in
interleaving text segments. In addition, we
introduce novel evaluation strategies for in-
tersection problems that employ entailment-
style judgments for determining the validity
of system-generated intersections. Our experi-
ments show that the proposed models produce
valid intersections a majority of the time and
that the segmented decoder yields advantages
over the search-based approach.
1 Introduction
In recent years, there has been growing interest
in text-to-text generation problems which transform
text according to specifications. Tasks such as sen-
tence compression, which strives to retain the most
salient content of an input sentence, and sentence fu-
sion, which attempts to combine the important con-
tent in related sentences, are useful components for
tackling larger natural language problems such as
abstractive summarization of documents. Systems
for these types of text-to-text problems are typically
evaluated on the informativeness of the output text
as judged by human annotators.
A natural aspect of most text generation systems
is that a given input can map to a range of lexi-
cally diverse outputs. However, text-to-text tasks
defined with vague criteria such as the preserva-
tion of the ?important? information in text can also
permit outputs that are semantically distinct. This
can make evaluation difficult; for instance, system-
generated sentences may differ (partially or com-
pletely) in informational content from reference
human-annotated text. This phenomenon has been
noted and discussed in the task of pairwise sentence
fusion (Daume? III and Marcu, 2004) and also in sen-
tence compression (McDonald, 2006). Some exam-
ples are listed in Table 1.
In this work, we examine the task of sentence in-
tersection: a variant of sentence fusion that does not
permit semantic variation in the output. A strict1 in-
tersection system is expected to produce a fused sen-
tence that contains all the information common to its
input sentences and avoid information that is in just
one of the inputs. In other words, a valid intersection
should only contain information that is substantiated
by all input sentences. The set-theoretic notions of
intersection (along with union) have been employed
to describe variants of sentence fusion tasks in previ-
ous work (Marsi and Krahmer, 2005; Krahmer et al,
2008) but, to our knowledge, this work is the first to
explicitly tackle and evaluate the strict intersection
task.
We focus on the case of unsupervised pairwise
sentence intersection and propose a strategy to yield
1We use the term strict to make explicit the distinction from
traditional fusion systems, which generally aim at notions of
intersection but are not formally evaluated with respect to it.
43
(a) Fusion example from
Daume? III and Marcu (2004)
(i) After years of pursuing separate and conflicting paths, AT&T and Digital Equip-
ment Corp. agreed in June to settle their computer-to-PBX differences.
(ii) The two will jointly develop an applications interface that can be shared by
computers and PBXs of any stripe.
Human fusion #1 AT&T and Digital Equipment Corp. agreed in June to settle their computer-to-
PBX differences and develop an applications interface that can be shared by any
computer or PBX.
Human fusion #2 After years of pursuing different paths, AT&T and Digital agreed to jointly develop
an applications interface that can be shared by computers and PBXs of any stripe.
(b) Compression example
from McDonald (2006)
TapeWare , which supports DOS and NetWare 286 , is a value-added process that
lets you directly connect the QA150-EXAT to a file server and issue a command
from any workstation to back up the server
Human compression #1 TapeWare supports DOS and NetWare 286
Human compression #2 TapeWare lets you connect the QA150-EXAT to a file server
(hypothesized)
Table 1: Examples of text-to-text generation problems with multiple valid human-generated outputs that differ signif-
icantly in semantic content. Italicized text is used to indicate fragments that are semantically identical.
valid intersections that follows the basic framework
of previous unsupervised fusion systems (Barzilay
and McKeown, 2005; Filippova and Strube, 2008b).
In our approach, the input sentences are first aligned
using a modified version of a recent phrase-based
alignment approach (MacCartney et al, 2008). We
assume the alignments that are produced define as-
pects of the input that must appear in the output fu-
sion and consider decoding strategies to recover in-
tersections that preserve these alignments. In addi-
tion to a search-based decoding strategy, we propose
a constrained integer linear programming (ILP) for-
mulation that attempts to decode the most fluent sen-
tence covering all these aspects while minimizing
the size and disfluency of interleaving text. This is a
fairly general model which can also be extended to
other alignment-based tasks such as pairwise union
and difference.
As this is a substantially more constrained task
than generic sentence fusion, we also present a
novel evaluation approach that avoids out-of-context
salience judgments. We make use of a recently-
released corpus of fusion candidates (McKeown et
al., 2010) and propose a crowdsourced entailment-
style evaluation to determine the validity of gener-
ated intersections, as well as the grammaticality of
the sentences produced. Additionally, automated
machine translation (MT) metrics are explored to
quantify the amount of information missing from
valid intersections. Our decoding strategies show
promise under these experiments and we discuss po-
tential directions for improving intersection perfor-
mance.
2 Related Work
The distinction between intersection and union of
text was introduced in the context of sentence fu-
sion (Krahmer et al, 2008; Marsi and Krahmer,
2005) in order to distinguish between traditional fu-
sion strategies that attempted to include only com-
mon content and fusions that attempted to include
all non-redundant content from the input. We fo-
cus here on strict sentence intersection, explicitly
incorporating a constraint that requires that a pro-
duced fusion must not contain information that is
not present in all input sentences. This distin-
guishes our approach from traditional sentence fu-
sion approaches (Jing and McKeown, 2000; Barzi-
lay and McKeown, 2005; Filippova and Strube,
2008b) which generally attempt to retain common
information but are typically evaluated in an abstrac-
tive summarization context in which additional in-
formation in the fusion output does not negatively
impact judgments.
This task is also related to the field of sentence
compression which has received much attention in
recent years (Turner and Charniak, 2005; McDon-
ald, 2006; Clarke and Lapata, 2008; Filippova and
Strube, 2008a; Cohn and Lapata, 2009; Marsi et al,
2010). Intersections can be viewed as guided com-
44
pressions in which the redundancy of information
content across input sentences in a multidocument
setting is assumed to directly indicate its salience,
thereby consigning it to the output.
Additionally, in this work, we frequently con-
sider the sentence intersection task from the per-
spective of textual entailment (cf. ?5.1). The textual
entailment task involves automatically determining
whether a given hypothesis can be inferred from a
textual premise (Dagan et al, 2005; Bar-Haim et al,
2006). Automatic construction of positive and neg-
ative entailment examples has been explored in the
past (Bensley and Hickl, 2008) to provide training
data for entailment systems; however the produc-
tion of text that is simultaneously entailed by two
(or more) sentences is a far more constrained and
difficult challenge.
ILP has been used extensively for text-to-text gen-
eration problems in recent years (Clarke and Lapata,
2008; Filippova and Strube, 2008b; Woodsend et al,
2010), including techniques which incorporate syn-
tax directly into the decoding to imporove the flu-
ency of the resulting text. In this paper, we focus on
generating valid intersections and do not incorporate
syntactic and semantic constraints into our ILP mod-
els; these are areas we intend to explore in the future.
3 The Intersection Task
The need for strict variants of fusion is motivated
by considerations of evaluation and utility in text-to-
text generation tasks. Without explicit constraints on
the semantic content of valid output, the operational
definition of fusion can encompass the full spectrum
from sentence intersection to sentence union. This
makes the comparison of different fusion systems
dependent on task-based utility2. In addition, inter-
section comprises an interesting problem in its own
right. It necessitates the use of generalization over
phrases in order to convey only the content of the
input sentences when different wording is used and
therefore involves more than just word deletion.
The analogy to set-theoretic intersection in this
task implies an underlying consideration of each
sentence as a set of informational concepts, sim-
2For instance, systems may trade off conciseness against
grammaticality, or informational content with degree of support
across the input sentences.
ilar to previous work in summarization and re-
dundancy (Filatova and Hatzivassiloglou, 2004;
Thadani and McKeown, 2008). While we don?t
commit to any semantic representation for such el-
ements of information, we can nevertheless attempt
to identify repeated information using well-studied
natural language analysis techniques such as align-
ment and paraphrase recognition, and furthermore
isolate this information through text-to-text genera-
tion techniques.
Consider, for example, the first sentence pair from
the examples in Table 2. A valid intersection for
these sentences must not contain any information
that is not substantiated by both of them, so a fu-
sion that mentions ?Mr Litvinenko?s poisoning?,
?Britain? or ?Sunday? would not satisfy this crite-
rion. In other words, a valid intersection must neces-
sarily be textually entailed by every input sentence.
Following this, we can interpret the sentence inter-
section task as one that requires the generation of
fluent text that is mutually entailed by all input sen-
tences3. We use this perspective in developing an
evaluation technique for strict intersection in ?5.1.
A major distinguishing factor between this work
and previous work on fusion is that simply adding
or deleting words in a sentence is not adequate; in
many cases, intersections require additional words
or phrases to be introduced in order to general-
ize over related but non-interchangeable aligned
terms (such as ?go? and ?expand?). Additionally,
we must attempt to avoid introducing additional
content-bearing text in the output while simultane-
ously striving to maintain the fluency of text.
3.1 Dataset
A corpus of sentence fusion instances was recently
made available by McKeown et al (2010), consist-
ing of 297 sentence pairs taken from newswire clus-
ters and manually judged as being good candidates
for fusion. Each sentence pair is accompanied by
human-produced intersections and unions collected
via Amazon?s Mechanical Turk service4. McKeown
et al (2010) noted that union responses are mostly
valid but intersections are frequently incorrect and
3From this perspective, the complementary task of sentence
union involves the generation of fluent text that entails all the
input sentences.
4http://www.mturk.com
45
1 (i) Home Secretary John Reid said Sunday the inquiry would go wherever ?the police take it.?
(ii) It comes as Home Secretary John Reid said the inquiry into Mr Litvinenko?s poisoning would expand beyond
Britain.
2 (i) Traces of polonium have been found on the planes on which they are believed to have travelled between
London and Moscow.
(ii) Small traces of radioactive substances had been found on the planes.
3 (i) Prosecutors allege that the accuser, who appeared in the program, was molested after the show aired.
(ii) Prosecutors allege that the boy, a cancer survivor, was molested twice after the program aired.
Table 2: Example sentence pairs from the McKeown et al (2010) corpus. Table 3 contains the corresponding system-
generated intersections for these sentence pairs.
hypothesized that the task is more confusing for
untrained annotators. A similar phenomenon was
noted by Krahmer et al (2008): while demonstrat-
ing that query-based human fusions exhibited less
variation than generic fusions, it was also observed
that intersections varied more than unions.
Due to the absence of adequate training data for
intersection, our approach to the task is unsuper-
vised, similar to previous work in fusion (Barzilay
and McKeown, 2005; Filippova and Strube, 2008b)
and sentence compression (Clarke and Lapata, 2008;
Filippova and Strube, 2008a). Additionally, we fo-
cus on the case of pairwise sentence intersection and
assume that the common information between the
input sentence pair can be represented within a sin-
gle output sentence. As a result, although the McK-
eown et al (2010) corpus cannot be used for training
an intersection model, we can make use of the sen-
tence pairs it contains for evaluation.
4 Models for intersection
Our proposed strategies for sentence intersection in-
volve phrase-based alignment, intermediate general-
ization steps that build a generation lattice and tech-
niques for decoding an output sentence, as described
below.
4.1 Phrase-based alignment
The alignment phase is a major component of any
intersection system as it is used to uncover the
common segments in the input that must be pre-
served in the output. We make use of an adapta-
tion of the supervised MANLI phrase-based align-
ment technique originally developed for textual en-
tailment systems (MacCartney et al, 2008); our
implementation replaces approximate search-based
decoding with exact ILP-based alignment decod-
ing and incorporates syntactic constraints to pro-
duce more precise alignments (Thadani and McKe-
own, 2011). The aligner is trained on a corpus of
human-generated alignment annotations produced
by Microsoft Research (Brockett, 2007) for infer-
ence problems from the second Recognizing Tex-
tual Entailment (RTE2) challenge (Bar-Haim et al,
2006).
Entailment problems are inherently asymmetric
because premise text is generally larger than hypoth-
esis text; however, this does not apply to our inter-
section problems and consequently our MANLI im-
plementation drops asymmetric indicator features.
The absence of these features impacts alignment
performance on RTE2 data but our reimplementa-
tion performs comparably to the original model un-
der the alignment evaluation from MacCartney et al
(2008).
4.2 Ontology-based generalization
An aligned phrase pair produced by the previous
step does not necessarily indicate that the phrases
are equivalent but merely that they are similar in
the given sentence context (such as ?accuser? and
?boy? in the third example from Table 2). We need
to generalize over these phrases as they are not inter-
changeable from the perspective of the intersection
task. We consider an alignment as containing three
types of aligned phrases:
1. Identical phrases or paraphrases: Either of
these may appear in the output
2. Entailed phrases: Only the entailed phrase
must appear in a valid intersection
3. Instances of a general concept: The common
concept must be lexicalized in the output
46
Although generalization of words within stan-
dalone sentences is usually hampered by word sense
ambiguity, our approach is less likely to encounter
this problem because we can generalize simultane-
ously over phrases which have already been aligned
using additional information (such as their neighbor-
ing context), thus avoiding generalizations that do
not fit the alignment.
For our experiments, we make use of the Wordnet
ontology (Miller, 1995) to find the hypernyms com-
mon to every aligned pair of non-identical phrases,
and only attempt to detect entailments which are
comprised of specific instances that entail general
concepts. This approach can be augmented by the
use of entailment corpora and distributional cluster-
ing which we intend to explore in future work. We
also use the lexical resource CatVar (Habash and
Dorr, 2003) to try to generate morphological vari-
ants of aligned words that enable them to be inter-
changed without creating disfluencies.
4.3 Pragmatic abstraction
Our strategy assumes that aligned text must be pre-
served in output intersections whereas unaligned
text must be minimized. However, unaligned text
cannot simply be dropped as it may contain vital
portions for generating fluent text. In addition, un-
aligned phrases can be caused by paraphrased or
metaphorical text that the aligner is not capable of
identifying. For example, the phrases ?polonium?
and ?radioactive substances? in the second sentence
pair from Table 2 fail to align with each other.
On the other hand, retaining unaligned text from
one of the input sentences for the sake of fluency
is likely to introduce information that is not sup-
ported by the other input sentence. We therefore
need to abstract away as much content from the un-
aligned portions of the text as possible. For this
purpose, we generate a large number of potential
compressions and abstractions for every unaligned
span that occurs between two consecutive aligned
phrases in each sentence. These compressions and
abstractions, referred to as interleaving paths, be-
tween pairs of aligned phrases essentially construct
a lattice over the input sentences that encodes all po-
tential intersection outputs.
Generation of interleaving paths is accomplished
through the application of rules on the dependency
parse structure over unaligned text spans from a sin-
gle sentence (as well as spans that occur before the
first aligned phrase and after the last aligned phrase
in each sentence). Interleaving paths are generated
by applying rules that:
1. Drop insignificant dependent words and un-
aligned prepositional phrases
2. Replace content-bearing verbs with tense-
adjusted generic variants such as ?did some-
thing? and ?happened?, with an exception for
statement verbs
3. Replace nouns with generic words such as
?someone? or ?something?, using Wordnet to
determine which generic variant fits a noun
4. Suggest connective text fragments such as
?something about? to cover long spans and
clause boundaries
Our abstraction rules are relatively simple but can
often generate reasonable interleaving paths. In gen-
eral, we note that shorter abstractions are less likely
to include glaring grammatical errors because long
unaligned spans are often indicative of problematic
alignments that either incorrectly relate unconnected
terms or fail to recognize paraphrases.
4.4 Decoding strategies
After sentence alignment, generalization over
aligned phrases and the construction of interleav-
ing paths, we are left with a lattice that encodes
potential intersections of the input sentence. Fig-
ure 1 describes the general structure of this lattice.
Every alignment link encompasses a set of aligned
phrases. Phrases may be identical or generaliza-
tions, in which case they can appear in the context
of either sentence, or they may be sentence-specific
(for example, verbs with different tenses or nominal-
izations like ?nominated? and ?nominations?). Ad-
ditionally, the abstraction phase generates interleav-
ing paths from unaligned spans between all pairs of
alignment links. These paths are generated from in-
dividual sentences and can only be used to connect
phrases that appear in the context of those sentences.
Our task now reduces to recovering a well-formed
intersection from this lattice. We make use of a lan-
guage model (LM) to judge fluency and propose two
techniques to decode high-scoring text from the lat-
tice: a simple beam-search technique and an ILP
47
Alignment link k
m
m
m
m
Phrases
from S1
Shared
phrases
Phrases
from S2
Alignment link l
Phrases
from S1
Shared
phrases
Phrases
from S2
Figure 1: The general structure of one segment of the
alignment lattice, illustrating the potential interleaving
paths between aligned phrases. Solid lines indicate paths
derived from sentence 1 and dashed lines indicate paths
derived from sentence 2
strategy that leverages our initial assumption that all
aligned phrases must appear in the output.
4.4.1 Beam search
Search-based decoding is often employed in phrase-
based MT systems (Och and Ney, 2003) and is
implemented in the Moses toolkit5; similar ap-
proaches have also been used in text-to-text gener-
ation tasks (Barzilay and McKeown, 2005; Soricut
and Marcu, 2006). This technique attempts to find
the highest-scoring sentence string under the LM by
unwrapping and searching through a lattice. Since
the dynamic programming search could require an
exponential number of search states, a fixed-width
beam can be used to control the number of search
states being actively considered at each step.
In order to decode an intersection problem, we
first pick a beam size B and initialize the list of can-
didate search states with the first interleaving paths
in each sentence. At every iteration, we consider the
B candidates with the highest normalized scores un-
der the LM and remove them from the candidate list.
Each candidate is then advanced, i.e., all aligned
phrases and interleaving paths following it are ex-
amined, scored and added to the candidate list. We
continue searching in this manner until B candidates
have covered all aligned phrases; the highest scoring
candidate is then retrieved as the target intersection.
4.4.2 Segmented decoding
While beam search is a viable strategy for decoding
intersections, its performance is contingent on the
5http://www.statmt.org/moses/
beam size parameter and it is not guaranteed to re-
turn the highest scoring sentence under the LM. For
instance, if a potential intersection starts with un-
usual text, it is unlikely to be explored by the search-
based approach even if it is the optimal solution to
the decoding problem. To address this, we also pro-
pose an alternative decoding problem that can be
formulated as the optimization of a linear objective
function with linear constraints. This can then be
solved exactly by well-studied algorithms using off-
the-shelf ILP solvers6.
This decoding problem does not look for the
highest scoring sentence under the LM; instead, it
attempts to find the set of interleaving paths and
aligned phrases that are most locally coherent7 un-
der the LM. Good phrase-path combinations that oc-
cur towards the tail end of an intersection can thus
be put on even footing with the combinations that
appear in the beginning. Although the two problems
consider different objective functions, they are both
engaged in the same overall goal: that of recovering
a fluent sentence from the lattice.
We first define boolean indicator variables aki ?
Ak for every aligned phrase in each aligned link Ak
present in the intersection problem I. We also in-
troduce indicator variables pklij for every possible in-
terleaving path between aligned phrases aki and a
l
j .
The linear objective for I that maximizes the local
coherence of all phrases can be expressed as
f = max
?
Ak,Al?I
|Ak|?
i=0
|Al|?
j=0
pklij ? score(p
kl
ij )
where score(pklij ) is the normalized LM score of the
fragment of text representing aki p
kl
ij a
l
j . In other
words, the score for each interleaving path is cal-
culated by appending it and the two phrases it con-
nects into a single fragment of text and determining
the score of that fragment under an LM8.
6We use LPsolve: http://lpsolve.sourceforge.net/
7As noted by Clarke and Lapata (2008), normalizing LM
scores cannot be easily accomplished with linear constraints
and we do not have training data to devise appropriate word-
insertion penalties as used in MT.
8If the fragment of text is smaller than the LM size, we
consider additional sentence context around the aligned phrases
rather than backing off to a smaller LM size to avoid a bias to-
wards short but ungrammatical interleaving paths.
48
We now introduce linear constraints to keep the
problem well-formed. First, we add a restriction
to ensure that only one phrase from each alignment
link is present in the solution.
?
aki ?Ak
aki = 1 ?Ak ? I
We can also ensure that interleaving paths are only in
the solution when the aligned phrases that they con-
nect together are themselves present using the fol-
lowing set of constraints.
aki ?
|Ak|?
i=0
pk?i? = 1 ?a
k
i ? Ak, Ak ? I
alj ?
|Al|?
j=0
p?l?j = 1 ?a
l
j ? Al, Al ? I
pklij ? a
k
i <= 0 ?i, j, k, l
pklij ? a
l
j <= 0 ?i, j, k, l
As we don?t restrict the structure of the lattice in any
way and allow crossing alignment links, the program
as defined thus far is capable of generating cyclic
and fragmented solutions. To combat this, we add
dummy start and end phrase variables and introduce
additional single commodity flow constraints (Mag-
nanti and Wolsey, 1994) adapted from Martins et
al. (2009) over the interleaving paths to guarantee
that the output will only involve a linear sequence of
aligned phrases and paths.
5 Evaluation
We now turn to the design of experiments for the
strict sentence intersection task and discuss the per-
formance of the proposed models using the corpus
provided by McKeown et al (2010). We use a beam
size of 50 for the beam search decoder and a 4-gram
LM for all experiments. Dependency parsing is ac-
complished with MICA, a TAG-based parser (Ban-
galore et al, 2009). Our primary considerations
for studying system-generated fusions are validity
(whether the output contains only the information
common to each sentence), coverage (whether the
output contains all the common information in the
input sentences) and the fluency of the output.
5.1 Evaluating Validity and Fluency
Evaluating the validity of an intersection involves
determining whether it contains only the informa-
tion contained in each sentence and nothing else. In
order to do this, we make use of the interpretation of
valid intersections as being mutually entailed by the
input sentences. It follows that the task of judging
the validity of an intersection can simply be decom-
posed into two tasks that judge whether the intersec-
tion is entailed by each input sentence.
We make use of Amazon?s Mechanical Turk
(AMT) platform to have humans evaluate the in-
tersections produced. Crowdsourcing annotations
and judgments in this manner has been shown to be
cheap and effective for natural language tasks (Snow
et al, 2008) and has recently been employed in sim-
ilar entailment-detection tasks (Negri and Mehdad,
2010; Buzek et al, 2010). Since we only seek judg-
ments on produced intersections and avoid present-
ing both input sentences to users, we do not antic-
ipate the noisiness that was noted by McKeown et
al. (2010) when asking AMT users to generate in-
tersections.
Each entailment task is framed as a multiple
choice question. An AMT user is shown just one
input sentence (the premise in entailment terminol-
ogy) along with a potential intersection (the hypoth-
esis) and is required to respond to whether there is
any new or different information in the latter that is
not in the former. They can respond on a 3-point
scale (yes/no/maybe) where maybe is clarified to in-
clude ambiguous rewording in the intersection. For
a given intersection instance, the responses9 using
each input sentence as the premise are averaged sep-
arately and then combined10 to give a measure of
how well the intersection is entailed by both sen-
tences.
A second question allows the user to specify the
grammaticality of the intersection on a 4-point scale.
As this measure doesn?t depend on the input sen-
tence presented to the AMT user, all scores provided
are simply averaged per intersection.
9Each instance is presented to 6 AMT users, 3 per premise.
Responses were automatically filtered for spam and removing
the largest outlier from each per-premise or per-intersection
group did not yield a notable change in relative performance.
10We use the harmonic mean for combination, but the results
are largely similar when using an arithmetic mean.
49
Intersection output Fluency Validity
Aligned words (i) Home Secretary John Reid said the inquiry would go. 0.667 0.800
(ii) Home Secretary John Reid said the inquiry would expand. 0.778
Beam search Home Secretary John Reid said something about the inquiry would move
wherever ?the something take it?.
0.389 0.667
Segmented decoder Home Secretary John Reid said the inquiry would change. 0.944 0.909
Aligned words (i) Traces of have been found on the planes. 0.445 1.000
(ii) traces of had been found on the planes. 0.556
Beam search Small traces of some things have been found on the planes. 0.611 0.909
Segmented decoder Small traces of had been found on the planes. 0.500 0.741
Aligned words (i) Prosecutors allege that the accuser the program was molested after aired. 0.167 0.800
(ii) Prosecutors allege that the boy was molested after the program aired. 1.000
Beam search Prosecutors allege that the being, who did something in the program, was
molested after something about aired.
0.400 0.909
Segmented decoder Prosecutors allege that the organism, who did something, was molested
after the program aired.
0.667 0.857
Table 3: Intersections produced for the examples introduced in Table 2 along with judgments from AMT users.
Validity Fluency Har. Mean
Other sentence 0.188 0.945 0.314
Aligned words 0.863 0.563? 0.682?
Beam search 0.729 0.450 0.557
Segmented decoder 0.812? 0.504 0.622
Oracle combination 0.813? 0.575? 0.674?
Table 4: Results of the AMT evaluation described in ?5.1.
Statistically insignificant differences within columns are
indicated with ?; all other entries are significantly distinct
at p ? 0.05.
5.2 Results of AMT evaluation
Table 4 contains the results from this evaluation
over the McKeown et al (2010) corpus11 and Ta-
ble 3 shows the system-produced intersections cor-
responding to the examples from ?3. We report nor-
malized scores of validity and fluency for ease of
comparison, as well as their unweighted harmonic
mean as a crude measure of combined human judg-
ment. In addition to the beam search and segmented
decoders, we report the performance of two upper-
bound systems that present artificial hypothesis sen-
tences to AMT users. Other sentence is simply the
sentence that is not the current premise from the sen-
tence pair; although this is rarely an appropriate in-
tersection in the data, it is useful as a measure of
how well humans judge grammaticality and infor-
11The first 20 sentence pairs of the corpus were examined
when devising abstraction rules and are therefore excluded from
these results.
mation content. Aligned words is the aligned subset
of the premise sentence; this is quite likely to be con-
sidered a valid entailment by AMT users as no new
words are introduced. Although the latter also scores
surprisingly well on fluency, we must note that this
is not an actual intersection solution: the aligned
words displayed to AMT users for a given intersec-
tion instance are different depending on which input
sentence is displayed as the premise.
Turning to the systems under study, we observe
that the ILP-based segmented decoder produces text
that is judged more fluent on average than the beam
search decoder. In order to judge the degree of over-
lap between the two systems, we also report the
performance of a pseudo-hybrid oracle combination
system which assumes the presence of an oracle that
runs both decoders and always chooses the output
intersection that is more grammatical. The improved
performance illustrates that each decoder has its ad-
vantages and that a real hybrid system might yield
improvements over either approach.
5.3 Evaluating Coverage
While validity experiments test whether the pro-
posed intersections contain extraneous or unsup-
ported information, we also need to check whether
the intersections contain all the information that is
shared between the input sentences. This cannot be
factored into a task that involves only one input sen-
tence and therefore cannot be easily accomplished
50
BLEU NIST
Aligned words 0.682 11.10
Beam search 0.726 10.53
Segmented decoder 0.818 11.56
Table 5: Results of the automated evaluation for coverage
of intersections described in ?5.3.
without annotators who understand the concept of
intersection.
We instead attempt to utilize the high-quality
human-generated union dataset from McKeown et
al. (2010) in evaluating the coverage of our inter-
section systems. Using the simple absorption law
A ? (A ? B) = A, we assume that the coverage
of intersection systems can be judged by how well
they can recover an input sentence from human-
generated unions. The resulting outputs are com-
pared to the original input sentences in an MT-
style evaluation under two commonly-used metrics:
BLEU (Papineni et al, 2002) and NIST (Dodding-
ton, 2002).
The results of this automated evaluation are
shown in Table 5. The aligned words system here
always considers words from the union sentence and
can therefore be seen as a baseline system. We ob-
serve that the segmented decoder produces output
that is judged most similar to the input sentences
under BLEU, which measures n-gram overlap, al-
though results under NIST (which gives additional
weight to rarer n-grams) are less conclusive.
6 Discussion
The experimental results indicate that the two sys-
tems we describe, particularly the segmented de-
coder, do a reasonable job of finding valid intersec-
tions with good coverage; however, producing fluent
output remains a challenge. Analysis of the inter-
sections produced leads us to note that the quality
of interleaving paths is the prime obstacle to im-
proving intersection output (cf. Table 3): produc-
ing syntactically-valid textual abstractions to con-
nect text is a challenge that is not met by our sim-
ple rule-based approach. Furthermore, we notice
that the quality of alignment also factors in to this
problem: systems that miss phrases which should
be aligned or systems that mistakenly align faraway
fragments both cause spans of unaligned text that
must be then abstracted over.
We hypothesize that these issues could be tackled
with the use of joint models: a system that aligns
as it decodes could reduce the need for abstrac-
tion over long unaligned spans, although care would
have to be taken to ensure that coverage is main-
tained. Additionally, richer lexical resources such
as wider-coverage ontologies (Snow et al, 2006)
and entailment/paraphrase dictionaries could aid in
improving coverage. Finally, previous work in fu-
sion (Filippova and Strube, 2008b; Filippova and
Strube, 2009) has noted that models based on syntax
outperform techniques that rely solely on LM scores
to determine fluency, and strict intersection appears
to be well-suited for further exploration in this vein.
7 Conclusion
We have examined the text-to-text generation task of
strict sentence intersection, which restricts semantic
variation in the output and necessarily invokes the
problems of generalization and abstraction in addi-
tion to the usual challenge of producing fluent text.
We tackle the task as lattice decoding and discuss
two decoding strategies for producing valid intersec-
tions. In addition, we assume that strict intersec-
tion tasks are best considered as problems of mu-
tual entailment generation and describe evaluation
strategies for this task that make use of both human
judgments as well as automated metrics run over a
related corpus. Experimental results indicate that
these systems are fairly effective at generating valid
intersections and that our novel segmented decoder
strategy outperforms the traditional beam search ap-
proach. Although fluency remains a challenge, we
hypothesize that the use of joint models, syntac-
tic constraints and lexical resources could bring im-
provements.
Acknowledgments
The authors are grateful to the anonymous reviewers
for their helpful feedback. This material is based on
research supported in part by the U.S. National Sci-
ence Foundation (NSF) under IIS-05-34871. Any
opinions, findings and conclusions or recommenda-
tions expressed in this material are those of the au-
thors and do not necessarily reflect the views of the
NSF.
51
References
Srinivas Bangalore, Pierre Boullier, Alexis Nasr, Owen
Rambow, and Beno??t Sagot. 2009. MICA: a prob-
abilistic dependency parser based on tree insertion
grammars. In Proceedings of HLT-NAACL: Short Pa-
pers, pages 185?188.
Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo
Giampiccolo, Bernardo Magnini, and Idan Szpektor.
2006. The second PASCAL Recognising Textual En-
tailment challenge. In Proceedings of the Second
PASCAL Challenges Workshop on Recognising Tex-
tual Entailment.
Regina Barzilay and Kathleen R. McKeown. 2005. Sen-
tence fusion for multidocument news summarization.
Computational Linguistics, 31(3):297?328.
Jeremy Bensley and Andrew Hickl. 2008. Unsupervised
resource creation for textual inference applications. In
Proceedings of LREC.
Chris Brockett. 2007. Aligning the 2006 RTE cor-
pus. Technical Report MSR-TR-2007-77, Microsoft
Research.
Olivia Buzek, Philip Resnik, and Benjamin B. Bederson.
2010. Error driven paraphrase annotation using me-
chanical turk. In Proceedings of the NAACL HLT 2010
Workshop on Creating Speech and Language Data
with Amazon?s Mechanical Turk, pages 217?221.
James Clarke and Mirella Lapata. 2008. Global infer-
ence for sentence compression: an integer linear pro-
gramming approach. Journal of Artifical Intelligence
Research, 31:399?429, March.
Trevor Cohn and Mirella Lapata. 2009. Sentence com-
pression as tree transduction. Journal of Artificial In-
telligence Research, 34:637?674.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2005. The pascal recognising textual entailment chal-
lenge. In Proceedings of the PASCAL Challenges
Workshop on Recognising Textual Entailment.
Hal Daume? III and Daniel Marcu. 2004. Generic sen-
tence fusion is an ill-defined summarization task. In
Proceedings of the ACL Text Summarization Branches
Out Workshop, pages 96?103.
George Doddington. 2002. Automatic evaluation of ma-
chine translation quality using n-gram co-occurrence
statistics. In Proceedings of HLT, pages 138?145.
Elena Filatova and Vasileios Hatzivassiloglou. 2004.
A formal model for information selection in multi-
sentence text extraction. In Proceedings of COLING,
page 397.
Katja Filippova and Michael Strube. 2008a. Dependency
tree based sentence compression. In Proceedings of
the Fifth International Natural Language Generation
Conference, INLG ?08, pages 25?32.
Katja Filippova and Michael Strube. 2008b. Sentence fu-
sion via dependency graph compression. In Proceed-
ings of EMNLP, pages 177?185.
Katja Filippova and Michael Strube. 2009. Tree
linearization in English: improving language model
based approaches. In Proceedings of NAACL, pages
225?228.
Nizar Habash and Bonnie Dorr. 2003. A categorial vari-
ation database for English. In Proceedings of NAACL,
NAACL ?03, pages 17?23.
Hongyan Jing and Kathleen R. McKeown. 2000. Cut
and paste based text summarization. In Proceedings
of NAACL, pages 178?185.
Emiel Krahmer, Erwin Marsi, and Paul van Pelt. 2008.
Query-based sentence fusion is better defined and
leads to more preferred results than generic sentence
fusion. In Proceedings of ACL, pages 193?196.
Bill MacCartney, Michel Galley, and Christopher D.
Manning. 2008. A phrase-based alignment model
for natural language inference. In Proceedings of
EMNLP, pages 802?811.
Thomas L. Magnanti and Laurence A. Wolsey. 1994.
Optimal trees. In Technical Report 290-94,
Massechusetts Institute of Technology, Operations Re-
search Center.
Erwin Marsi and Emiel Krahmer. 2005. Explorations
in sentence fusion. In Proceedings of the European
Workshop on Natural Language Generation, pages
109?117.
Erwin Marsi, Emiel Krahmer, Iris Hendrickx, and Wal-
ter Daelemans. 2010. On the limits of sentence
compression by deletion. In Emiel Krahmer and
Marie?t Theune, editors, Empirical Methods in Natural
Language Generation, pages 45?66. Springer-Verlag,
Berlin, Heidelberg.
Andre? F. T. Martins, Noah A. Smith, and Eric P. Xing.
2009. Concise integer linear programming formula-
tions for dependency parsing. In Proceedings of ACL-
IJCNLP, pages 342?350.
Ryan McDonald. 2006. Discriminative sentence com-
pression with soft syntactic evidence. In Proceedings
of EACL, pages 297?304.
Kathleen McKeown, Sara Rosenthal, Kapil Thadani, and
Coleman Moore. 2010. Time-efficient creation of an
accurate sentence fusion corpus. In Proceedings of
NAACL-HLT.
George A. Miller. 1995. Wordnet: a lexical database
for english. Communications of the ACM, 38:39?41,
November.
Matteo Negri and Yashar Mehdad. 2010. Creating a
bi-lingual entailment corpus through translations with
mechanical turk: $100 for a 10-day rush. In Proceed-
ings of the NAACL HLT 2010 Workshop on Creating
52
Speech and Language Data with Amazon?s Mechani-
cal Turk, pages 212?216.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29:19?51, March.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalu-
ation of machine translation. In Proceedings of ACL,
ACL ?02, pages 311?318, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2006.
Semantic taxonomy induction from heterogenous evi-
dence. In Proceedings of ACL, pages 801?808.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-
drew Y. Ng. 2008. Cheap and fast?but is it good?:
evaluating non-expert annotations for natural language
tasks. In Proceedings of EMNLP, pages 254?263.
Radu Soricut and Daniel Marcu. 2006. Stochastic lan-
guage generation using widl-expressions and its appli-
cation in machine translation and summarization. In
Proceedings of ACL, pages 1105?1112.
Kapil Thadani and Kathleen McKeown. 2008. A frame-
work for identifying textual redundancy. In Proceed-
ings of COLING, pages 873?880.
Kapil Thadani and Kathleen McKeown. 2011. Optimal
and syntactically-informed decoding for monolingual
phrase-based alignment. In Proceedings of ACL.
Jenine Turner and Eugene Charniak. 2005. Supervised
and unsupervised learning for sentence compression.
In Proceedings of ACL, pages 290?297.
Kristian Woodsend, Yansong Feng, and Mirella Lapata.
2010. Title generation with quasi-synchronous gram-
mar. In Proceedings of EMNLP, EMNLP ?10, pages
513?523.
53
Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 65?74,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
Sentence Compression with Joint Structural Inference
Kapil Thadani and Kathleen McKeown
Department of Computer Science
Columbia University
New York, NY 10025, USA
{kapil,kathy}@cs.columbia.edu
Abstract
Sentence compression techniques often
assemble output sentences using frag-
ments of lexical sequences such as n-
grams or units of syntactic structure such
as edges from a dependency tree repre-
sentation. We present a novel approach
for discriminative sentence compression
that unifies these notions and jointly pro-
duces sequential and syntactic represen-
tations for output text, leveraging a com-
pact integer linear programming formula-
tion to maintain structural integrity. Our
supervised models permit rich features
over heterogeneous linguistic structures
and generalize over previous state-of-the-
art approaches. Experiments on corpora
featuring human-generated compressions
demonstrate a 13-15% relative gain in 4-
gram accuracy over a well-studied lan-
guage model-based compression system.
1 Introduction
Recent years have seen increasing interest in text-
to-text generation tasks such as paraphrasing and
text simplification, due in large part to their direct
utility in high-level natural language tasks such as
abstractive summarization. The task of sentence
compression in particular has benefited from the
availability of a number of useful resources such
as the the Ziff-Davis compression corpus (Knight
and Marcu, 2000) and the Edinburgh compression
corpus (Clarke and Lapata, 2006b) which make
compression problems highly relevant for data-
driven approaches involving language generation.
The sentence compression task addresses the
problem of minimizing the lexical footprint of a
sentence, i.e., the number of words or characters
in it, while preserving its most salient informa-
tion. This is illustrated in the following example
from the compression corpus of Clarke and Lap-
ata (2006b):
Original: In 1967 Chapman, who had cultivated a
conventional image with his ubiquitous tweed jacket
and pipe, by his own later admission stunned a party
attended by his friends and future Python colleagues
by coming out as a homosexual.
Compressed: In 1967 Chapman, who had cultivated
a conventional image, stunned a party by coming out
as a homosexual.
Compression can therefore be viewed as analo-
gous to text summarization1 defined at the sen-
tence level. Unsurprisingly, independent selec-
tion of tokens for an output sentence does not
lead to fluent or meaningful compressions; thus,
compression systems often assemble output text
from units that are larger than single tokens such
as n-grams (McDonald, 2006; Clarke and Lap-
ata, 2008) or edges in a dependency structure (Fil-
ippova and Strube, 2008; Galanis and Androut-
sopoulos, 2010). These systems implicitly rely on
a structural representation of text?as a sequence
of tokens or as a dependency tree respectively?to
to underpin the generation of an output sentence.
In this work, we present structured transduc-
tion: a novel supervised framework for sen-
tence compression which employs a joint infer-
ence strategy to simultaneously recover sentence
compressions under both these structural repre-
sentations of text?a token sequence as well as a
tree of syntactic dependencies. Sentence genera-
tion is treated as a discriminative structured pre-
diction task in which rich linguistically-motivated
1To further the analogy, compression is most often formu-
lated as a word deletion task which parallels the popular view
of summarization as a sentence extraction task.
65
features can be used to predict the informative-
ness of specific tokens within the input text as well
as the fluency of n-grams and dependency rela-
tionships in the output text. We present a novel
constrained integer linear program that optimally
solves the joint inference problem, using the no-
tion of commodity flow (Magnanti and Wolsey,
1994) to ensure the production of valid acyclic se-
quences and trees for an output sentence.
The primary contributions of this work are:
? A supervised sequence-based compression
model which outperforms Clarke & Lapata?s
(2008) state-of-the-art sequence-based com-
pression system without relying on any hard
syntactic constraints.
? A formulation to jointly infer tree structures
alongside sequentially-ordered n-grams,
thereby permitting features that factor over
both phrases and dependency relations.
The structured transduction models offer addi-
tional flexibility when compared to existing mod-
els that compress via n-gram or dependency fac-
torizations. For instance, the use of commodity
flow constraints to ensure well-formed structure
permits arbitrary reorderings of words in the input
and is not restricted to producing text in the same
order as the input like much previous work (Mc-
Donald, 2006; Clarke and Lapata, 2008; Filippova
and Strube, 2008) inter alia.2
We ran compression experiments with the pro-
posed approaches on well-studied corpora from
the domains of written news (Clarke and Lapata,
2006b) and broadcast news (Clarke and Lapata,
2008). Our supervised approaches show signif-
icant gains over the language model-based com-
pression system of Clarke and Lapata (2008) un-
der a variety of performance measures, yielding
13-15% relative F1 improvements for 4-gram re-
trieval over Clarke and Lapata (2008) under iden-
tical compression rate conditions.
2 Joint Structure Transduction
The structured transduction framework is driven
by the fundamental assumption that generating
fluent text involves considerations of diverse struc-
tural relationships between tokens in both input
and output sentences. Models for sentence com-
pression often compose text from units that are
2We do not evaluate token reordering in the current work
as the corpus used for experiments in ?3 features human-
generated compressions that preserve token ordering.
larger than individual tokens, such as n-grams
which describe a token sequence or syntactic re-
lations which comprise a dependency tree. How-
ever, our approach is specifically motivated by the
perspective that both these representations of a
sentence?a sequence of tokens and a tree of de-
pendency relations?are equally meaningful when
considering its underlying fluency and integrity. In
other words, models for compressing a token se-
quence must also account for the compression of
its dependency representation and vice versa.
In this section, we discuss the problem of re-
covering an optimal compression from a sen-
tence as a linear optimization problem over het-
erogenous substructures (cf. ?2.1) that can be
assembled into valid and consistent representa-
tions of a sentence (cf. ?2.2). We then consider
rich linguistically-motivated features over these
substructures (cf. ?2.3) for which corresponding
weights can be learned via supervised structured
prediction (cf. ?2.4).
2.1 Linear Objective
Consider a single compression instance involving
a source sentence S containing m tokens. The no-
tation S? is used to denote a well-formed compres-
sion of S. In this paper, we follow the standard
assumption from compression research in assum-
ing that candidate compressions S? are assembled
from the tokens in S, thereby treating compression
as a word-deletion task. The inference step aims
to retrieve the output sentence S?? that is the most
likely compression of the given input S, i.e., the S?
that maximizes p(S?|S) ? p(S?, S) or, in an equiv-
alent discriminative setting, the S? that maximizes
a feature-based score for compression
S?? , argmax
S?
w>?(S, S?) (1)
where ?(S, S?) denotes some feature map param-
eterized by a weight vector w.
Let T , {ti : 1 ? i ? m} represent the set
of tokens in S and let xi ? {0, 1} represent a
token indicator variable whose value corresponds
to whether token ti is present in the output sen-
tence S?. The incidence vector x , ?x1, . . . , xm?>
therefore represents an entire token configuration
that is equivalent to some subset of T .
If we were to consider a simplistic bag-of-
tokens scenario in which the features factor en-
tirely over the tokens from T , the highest-scoring
66
compression under (1) would simply be the to-
ken configuration that maximizes a linear combi-
nation of per-token scores, i.e., ?ti?T xi ? ?tok(i)where ?tok : N? R denotes a linear scoring func-
tion which measures the relative value of retain-
ing ti in a compression of S based on its features,
i.e., ?tok(i) , w>tok?tok(ti). Although this can
be solved efficiently under compression-rate con-
straints, the strong independence assumption used
is clearly unrealistic: a model that cannot consider
any relationship between tokens in the output does
not provide a token ordering or ensure that the re-
sulting sentence is grammatical.
The natural solution is to include higher-order
factorizations of linguistic structures such as n-
grams in the objective function. For clarity of ex-
position, we assume the use of trigrams without
loss of generality. Let U represent the set of all
possible trigrams that can be constructed from the
tokens of S; in other words U , {?ti, tj , tk? : ti ?
T ? {START}, tj ? T, tk ? T ? {END}, i 6= j 6=
k}. Following the notation for token indicators, let
yijk ? {0, 1} represent a trigram indicator variable
for whether the contiguous sequence of tokens
?ti, tj , tk? is in the output sentence. The incidence
vector y , ?yijk??ti,tj ,tk??U hence representssome subset of the trigrams in U . Similarly, let V
represent the set of all possible dependency edges
that can be established among the tokens of S and
the pseudo-token ROOT, i.e., V , {?i, j? : i ?
T ? {ROOT}, j ? T, tj is a dependent of ti in S}.
As before, zij ? {0, 1} represents a dependency
arc indicator variable indicating whether tj is a di-
rect dependent of ti in the dependency structure of
the output sentence, and z , ?zij??ti,tj??V repre-
sents a subset of the arcs from V .
Using this notation, any output sentence S? can
now be expressed as a combination of some to-
ken, trigram and dependency arc configurations
?x,y, z?. Defining ?ngr and ?dep analogously to
?tok for trigrams and dependency arcs respectively,
we rewrite (1) as
S?? = argmax
x,y,z
?
ti?T
xi ? ?tok(i)
+
?
?ti,tj ,tk??U
yijk ? ?ngr(i, j, k)
+
?
?ti,tj??V
zij ? ?dep(i, j)
= argmax
x,y,z
x>?tok + y>?ngr + z>?dep (2)
where ?tok , ??tok(i)?ti?T denotes the vector of
token scores for all tokens ti ? T and ?ngr and
?dep represent vectors of scores for all trigrams
and dependency arcs in U and V respectively. The
joint objective in (2) is an appealingly straightfor-
ward and yet general formulation for the compres-
sion task. For instance, the use of standard sub-
structures like n-grams permits scoring of the out-
put sequence configuration y under probabilistic
n-gram language models as in Clarke and Lapata
(2008). Similarly, consideration of dependency
arcs allows the compressed dependency tree z to
be scored using a rich set of indicator features over
dependency labels, part-of-speech tags and even
lexical features as in Filippova and Strube (2008).
However, unlike the bag-of-tokens scenario,
these output structures cannot be constructed effi-
ciently due to their interdependence. Specifically,
we need to maintain the following conditions in
order to obtain an interpretable token sequence y:
? Trigram variables yijk must be non-zero if
and only if their corresponding word vari-
ables xi, xj and xk are non-zero.
? The non-zero yijk must form a sentence-like
linear ordering, avoiding disjoint structures,
cycles and branching.
Similarly, a well-formed dependency tree z will
need to satisfy the following conditions:
? Dependency variables zij must be non-zero if
and only if the corresponding word variables
xi and xj are.
? The non-zero zij must form a directed tree
with one parent per node, a single root node
and no cycles.
2.2 Constrained ILP Formulation
We now discuss an approach to recover exact so-
lutions to (2) under the appropriate structural con-
straints, thereby yielding globally optimal com-
pressions S? ? ?x,y, z? given some input sentence
S and model parameters for the scoring functions.
For this purpose, we formulate the inference task
for joint structural transduction as an integer linear
program (ILP)?a type of linear program (LP) in
which some or all of the decision variables are re-
stricted to integer values. A number of highly op-
timized general-purpose solvers exist for solving
ILPs thereby making them tractable for sentence-
level natural language problems in which the num-
ber of variables and constraints is described by a
low-order polynomial over the size of the input.
67
Recent years have seen ILP applied to many
structured NLP applications including depen-
dency parsing (Riedel and Clarke, 2006; Martins
et al, 2009), text alignment (DeNero and Klein,
2008; Chang et al, 2010; Thadani et al, 2012)
and many previous approaches to sentence and
document compression (Clarke and Lapata, 2008;
Filippova and Strube, 2008; Martins and Smith,
2009; Clarke and Lapata, 2010; Berg-Kirkpatrick
et al, 2011; Woodsend and Lapata, 2012).
2.2.1 Basic structural constraints
We start with constraints that define the behavior
of terminal tokens. Let y?jk, yij? and z?j denote
indicator variables for the sentence-starting tri-
gram ?START, tj , tk?, the sentence-ending trigram
?ti, tj , END? and the root dependency ?ROOT, tj?
respectively. A valid output sentence will started
and terminate with exactly one trigram (perhaps
the same); similarly, exactly one word should act
as the root of the output dependency tree.
?
j,k
y?jk = 1 (3)
?
i,j
yij? = 1 (4)
?
j
z?j = 1 (5)
Indicator variables for any substructure, i.e., n-
gram or dependency arc, must be kept consistent
with the token variables that the substructure is de-
fined over. For instance, we require constraints
which specify that tokens can only be active (non-
zero) in the solution when, for 1 ? p ? n, there
is exactly one active n-gram in the solution which
contains this word in position p.3 Tokens and de-
pendency arcs can similarly be kept consistent by
ensuring that a word can only be active when one
incoming arc is active.
xl ?
?
i,j,k:
l?{i,j,k}
yijk = 0, ?tl ? T (6)
xj ?
?
i
zij = 0, ?tj ? T (7)
3Note that this does not always hold for n-grams of or-
der n > 2 due to the way terminal n-grams featuring START
and END are defined. Specifically, in a valid linear ordering
of tokens and ?r ? 1 . . . n? 2, there can be no n-grams that
feature the last n?r?1 tokens in the r?th position or the first
n?r?1 tokens in the (n?r+1)?th position. However, this
is easily tackled computationally by assuming that the termi-
nal n-gram replaces these missing n-grams for near-terminal
tokens in constraint (6).
2.2.2 Flow-based structural constraints
A key challenge for structured transduction mod-
els lies in ensuring that output token sequences and
dependency trees are well formed. This requires
that output structures are fully connected and that
cycles are avoided. In order to accomplish this, we
introduce additional variables to establish single-
commodity flow (Magnanti and Wolsey, 1994) be-
tween all pairs of tokens, inspired by recent work
in dependency parsing (Martins et al, 2009). Lin-
ear token ordering is maintained by defining real-
valued adjacency commodity flow variables ?adjij
which must be non-zero whenever tj directly fol-
lows ti in an output sentence. Similarly, tree-
structured dependencies are enforced using addi-
tional dependency commodity flow variables ?depij
which must be non-zero whenever tj is the de-
pendent of ti in the output sentence. As with the
structural indicators, flow variables ?adj?j , ?adji? , ?dep?j
are also defined for the terminal pseudo-tokens
START, END and ROOT respectively.
Each active token in the solution consumes one
unit of each commodity from the flow variables
connected to it. In conjunction with the consis-
tency constraints from equations (6) and (7), this
ensures that cycles cannot be present in the flow
structure for either commodity.
?
i
?cij ?
?
k
?cjk = xj , ?tj ? T, (8)
?c ? {adj, dep}
By itself, (8) would simply set al token indica-
tors xi simultaneously to 0. However, since START
and ROOT have no incoming flow variables, the
amount of commodity in the respective outgo-
ing flow variables ?adj?j and ?dep?j remains uncon-
strained. These flow variables therefore provide
a point of origin for their respective commodities.
In order for commodity flow to be meaningful,
it should be confined to mirroring active structural
indicators; for this, we first restrict the amount of
commodity in any ?cij to be non-negative.
?cij ? 0, ?ti, tj ? T (9)
?c ? {adj, dep}
The adjacency commodity is then linked to the n-
grams that would actually establish an adjacency
relationship between two tokens, while the depen-
dency commodity is linked to its corresponding
dependency arcs. In conjunction with (8?9), these
68
START END
ROOT
Production was closed down at Ford last night for the Christmas period .
8 ?adj1,3 = 7 6 5 4 3 2 1
7
?dep3,1 = 1 2 1
2
1
1
Figure 1: An illustration of commodity values for a valid solution of the program. The adjacency com-
modity ?adj and dependency commodity ?dep are denoted by solid and dashed lines respectively.
constraints also serve to establish connectivity for
their respective structures.
?adjij ? Cmax
?
k
yijk ? 0, ?ti, tj ? T (10)
?adjjk ? Cmax
?
i
yijk ? 0, ?tj , tk ? T (11)
?depij ? Cmaxzij ? 0, ?ti, tj ? T (12)
where Cmax is the maximum amount of commod-
ity that the ?ij variables may carry and serves as an
upper bound on the number of tokens in the output
sentence. Since we use commodity flow to avoid
cyclical structure and not to specify spanning ar-
borescences (Martins et al, 2009), Cmax can sim-
ply be set to an arbitrary large value.
2.2.3 Compression rate constraints
The constraints specified above are adequate to en-
force structural soundness in an output compres-
sion. In addition, compression tasks often involve
a restriction on the size of the output sentence.
When measured in tokens, this can simply be ex-
pressed via constraints over token indicators.
?
i
xi ? Rmin (13)
?
i
xi ? Rmax (14)
where the compression rate is enforced by restrict-
ing the number of output tokens to [Rmin, Rmax].
2.3 Features
The scoring functions ? that guide inference for a
particular compression instance are defined above
as linear functions over structure-specific features.
We employ the following general classes of fea-
tures for tokens, trigrams and dependency arcs.
1. Informativeness: Good compressions might
require specific words or relationships be-
tween words to be preserved, highlighted, or
perhaps explicitly rejected. This can be ex-
pressed through features on token variables
that indicate a priori salience.4 For this pur-
pose, we rely on indicator features for part-
of-speech (POS) sequences of length up to 3
that surround the token and the POS tag of the
token?s syntactic governor conjoined with the
label. Inspired by McDonald (2006), we also
maintain indicator features for stems of verbs
(at or adjacent to the token) as these can be
useful indications of salience in compression.
Finally, we maintain features for whether to-
kens are negation words, whether they appear
within parentheses and if they are part of a
capitalized sequence of tokens (an approxi-
mation of named entities).
2. Fluency: These features are intended to cap-
ture how the presence of a given substructure
contributes to the overall fluency of a sen-
tence. The n-gram variables are scored with a
feature expressing their log-likelihood under
an LM. For n-gram variables, we include fea-
tures that indicate the POS tags and depen-
dency labels corresponding to the tokens it
covers. Dependency variable features involve
indicators for the governor POS tag con-
joined with the dependency direction. In ad-
dition, we also use lexical features for prepo-
sitions in the governor position of depen-
dency variables in order to indicate whether
certain prepositional phrases are likely to be
preserved in compressions.
3. Fidelity: One might reasonably expect that
many substructures in the input sentence will
appear unchanged in the output sentence.
Therefore, we propose boolean features that
indicate that a substructure was seen in the
input. Fidelity scores are included for all
n-gram variables alongside label-specific fi-
4Many compression systems (Clarke and Lapata, 2008;
Filippova and Strube, 2008) use a measure based on tf*idf
which derives from informativeness score of Hori and Furui
(2004), but we found this to be less relevant here.
69
delity scores for dependency arc variables,
which can indicate whether particular labels
are more or less likely to be dropped.
4. Pseudo-normalization: A drawback of us-
ing linear models for generation problems
is an inability to employ output sentence
length normalization in structure scoring. For
this purpose, we use the common machine
translation (MT) strategy of employing word
penalty features. These are essentially word
counts whose parameters are intended to bal-
ance out the biases in output length which are
induced by other features.
Each scale-dependent feature is recorded both ab-
solutely as well as normalized by the length of the
input sentence. This is done in order to permit the
model to acquire some robustness to variation in
input sentence length when learning parameters.
2.4 Learning
In order to leverage a training corpus to recover
weight parameters w? for the above features that
encourage good compressions for unseen data,
we rely on the structured perceptron of Collins
(2002). A fixed learning rate is used and param-
eters are averaged to limit overfitting.5 In our ex-
periments, we observed fairly stable convergence
for compression quality over held-out develop-
ment corpora, with peak performance usually en-
countered by 10 training epochs.
3 Experiments
In order to evaluate the performance of the
structured transduction framework, we ran com-
pression experiments over the newswire (NW)
and broadcast news transcription (BN) corpora
collected by Clarke and Lapata (2008). Sen-
tences in these datasets are accompanied by gold
compressions?one per sentence for NW and
three for BN?produced by trained human anno-
tators who were restricted to using word deletion,
so paraphrasing and word reordering do not play
a role. For this reason, we chose to evaluate the
systems using n-gram precision and recall (among
other metrics), following Unno et al (2006) and
standard MT evaluations.
We filtered the corpora to eliminate instances
with less than 2 and more than 110 tokens and used
5Given an appropriate loss function, large-margin struc-
tured learners such as k-best MIRA (McDonald et al, 2005)
can also be used as shown in Clarke and Lapata (2008).
the same training/development/test splits from
Clarke and Lapata (2008), yielding 953/63/603
sentences respectively for the NW corpus and
880/78/404 for the BN corpus. Dependency parses
were retrieved using the Stanford parser6 and ILPs
were solved using Gurobi.7 As a state-of-the-art
baseline for these experiments, we used a reim-
plementation of the LM-based system of Clarke
and Lapata (2008), which we henceforth refer to
as CL08. This is equivalent to a variant of our pro-
posed model that excludes variables for syntactic
structure, uses LM log-likelihood as a feature for
trigram variables and a tf*idf -based significance
score for token variables, and incorporates several
targeted syntactic constraints based on grammat-
ical relations derived from RASP (Briscoe et al,
2006) designed to encourage fluent output.
Due to the absence of word reordering in the
gold compressions, trigram variables y that were
considered in the structured transduction approach
were restricted to only those for which tokens
appear in the same order as the input as is the
case with CL08. Furthermore, in order to reduce
computational overhead for potentially-expensive
ILPs, we also excluded dependency arc variables
which inverted an existing governor-dependent re-
lationship from the input sentence parse.
A recent analysis of approaches to evaluating
compression (Napoles et al, 2011b) has shown a
strong correlation between the compression rate
and human judgments of compression quality,
thereby concluding that comparisons of systems
which compress at different rates are unreliable.
Consequently, all comparisons that we carry out
here involve a restriction to a particular compres-
sion rate to ensure that observed differences can
be interpreted meaningfully.
3.1 Results
Table 1 summarizes the results from compression
experiments in which the target compression rate
is set to the average gold compression rate for
each instance. We observe a significant gain for
the joint structured transduction system over the
Clarke and Lapata (2008) approach for n-gram F1.
Since n-gram metrics do not distinguish between
content words and function words, we also in-
clude an evaluation metric that observes the pre-
cision, recall and F-measure of nouns and verbs
6http://nlp.stanford.edu/software/
7http://www.gurobi.com
70
Corpus System n-grams F1% Content words Syntactic relations F1%n = 1 2 3 4 P% R% F1% Stanford RASP
NW CL08 66.65 53.08 40.35 31.02 73.84 66.41 69.38 51.51 50.21Joint ST 71.91 58.67 45.84 35.62 76.82 76.74 76.33 55.02 50.81
BN CL08 75.08 61.31 46.76 37.58 80.21 75.32 76.91 60.70 57.27Joint ST 77.82 66.39 52.81 42.52 80.77 81.93 80.75 61.38 56.47
Table 1: Experimental results under various quality metrics (see text for descriptions). Systems were
restricted to produce compressions that matched their average gold compression rate. Boldfaced entries
indicate significant differences (p < 0.0005) under the paired t-test and Wilcoxon?s signed rank test.
as a proxy for the content in compressed output.
From these, we see that the primary contribution
of the supervised joint approach is in enhancing
the recall of meaning-bearing words.
In addition to the direct measures discussed
above, Napoles et al (2011b) indicate that various
other metrics over syntactic relations such as those
produced by RASP also correlate significantly
with human judgments of compression quality.
Compressed sentences were therefore parsed with
RASP as well as the Stanford dependency parser
and their resulting dependency graphs were com-
pared to those of the gold compressions. These
metrics show statistically insignificant differences
except in the case of F1 over Stanford dependen-
cies for the NW corpus.8
Comparisons with CL08 do not adequately ad-
dress the question of whether the performance
gain observed is driven by the novel joint infer-
ence approach or the general power of discrimina-
tive learning. To investigate this, we also studied
a variant of the proposed model which eliminates
the dependency variables z and associated com-
modity flow machinery, thereby bridging the gap
between the two systems discussed above. This
system, which we refer to as Seq ST, is other-
wise trained under similar conditions as Joint ST.
Table 2 contains an example of incorrect system
output for the three systems under study and il-
lustrates some specific quirks of each, such as the
tendency of CL08 to preserve deeply nested noun
phrases, the limited ability of Seq ST to identify
heads of constituents and the potential for plausi-
ble but unusual output parses from Joint ST.
Figure 2 examines the variation of content word
F1% when the target compression rate is varied
for the BN corpus, which contains three refer-
8Our RASP F1 results for Clarke and Lapata (2008) in
Table 1 outperform their reported results by about 10% (ab-
solute) which may stem from our Gigaword-trained LM or
improvements in recent versions of RASP.
Input When Los Angeles hosted the Olympics in
1932 , Kurtz competed in high platform diving.
Gold When Los Angeles hosted the Olympics , Kurtz
competed in high diving .
CL08 When Los Angeles hosted Olympics in 1932 ,
in high platform diving .
Seq ST When Los Angeles hosted the Olympics , Kurtz
competed in high platform
Joint ST When Los Angeles hosted the Olympics in
1932 , Kurtz competed diving .
Table 2: Examples of erroneous system compres-
sions for a test instance from the NW corpus.
ence compressions per instance. Although the
gold compressions are often unreachable under
low rates, this provides a view into a model?s abil-
ity to select meaningful words under compression
constraints. We observe that the Joint ST model
consistently identifies content words more accu-
rately than the sequence-only models despite shar-
ing all token and trigram features with Seq ST.
Figure 3 studies the variation of RASP gram-
matical relation F1% with compression rate as an
approximate measure of grammatical robustness.
As all three systems track each other fairly closely,
the plot conveys the absolute difference of the ST
systems from the CL08 baseline, which reveals
that Joint ST largely outperforms Seq ST under
different compression conditions. We also note
that a high compression rate, i.e., minimal com-
pression, is generally favorable to CL08 under the
RASP F1 measure and conjecture that this may be
due to the hard syntactic constraints employed by
CL08, some of which are defined over RASP re-
lations. At higher compression rates, these con-
straints largely serve to prevent the loss of mean-
ingful syntactic relationships, e.g., that between a
preposition and its prepositional phrase; however,
a restrictive compression rate would likely result
in all such mutually-constrained components be-
ing dropped rather than simultaneously preserved.
71
Figure 2: Informativeness of compressions in the
BN test corpus indicated by noun and verb F1%
with respect to gold at different compression rates.
4 Related Work
An early notion of compression was proposed
by Dras (1997) as reluctant sentence paraphras-
ing under length constraints. Jing and McKe-
own (2000) analyzed human-generated summaries
and identified a heavy reliance on sentence re-
duction (Jing, 2000). The extraction by Knight
and Marcu (2000) of a dataset of natural com-
pression instances from the Ziff-Davis corpus
spurred interest in supervised approaches to the
task (Knight and Marcu, 2002; Riezler et al, 2003;
Turner and Charniak, 2005; McDonald, 2006;
Unno et al, 2006; Galley and McKeown, 2007;
Nomoto, 2007). In particular, McDonald (2006)
expanded on Knight & Marcu?s (2002) transition-
based model by using dynamic programming to
recover optimal transition sequences, and Clarke
and Lapata (2006a) used ILP to replace pairwise
transitions with trigrams. Other recent work (Fil-
ippova and Strube, 2008; Galanis and Androut-
sopoulos, 2010) has used dependency trees di-
rectly as sentence representations for compres-
sion. Another line of research has attempted to
broaden the notion of compression beyond mere
word deletion (Cohn and Lapata, 2009; Ganitke-
vitch et al, 2011; Napoles et al, 2011a). Finally,
progress on standalone compression tasks has also
enabled document summarization techniques that
jointly address sentence selection and compres-
sion (Daume? and Marcu, 2002; Clarke and Lapata,
2007; Martins and Smith, 2009; Berg-Kirkpatrick
et al, 2011; Woodsend and Lapata, 2012), a num-
ber of which also rely on ILP-based inference.
Monolingual text-to-text generation research
also faces many obstacles common to MT. Re-
Figure 3: Relative grammaticality of BN test cor-
pus compressions indicated by the absolute differ-
ence of RASP relation F1% from that of CL08.
cent work in MT decoding has proposed more ef-
ficient approaches than ILP to produced text op-
timally under syntactic and sequential models of
language (Rush and Collins, 2011). We are cur-
rently exploring similar ideas for compression and
other text-to-text generation problems.
5 Conclusion
We have presented a supervised discriminative
approach to sentence compression that elegantly
accounts for two complementary aspects of sen-
tence structure?token ordering and dependency
syntax. Our inference formulation permits rich,
linguistically-motivated features that factor over
the tokens, n-grams and dependencies of the out-
put. Structural integrity is maintained by linear
constraints based on commodity flow, resulting in
a flexible integer linear program for the inference
task. We demonstrate that this approach leads to
significant performance gains over a state-of-the-
art baseline compression system without resorting
to hand-picked constraints on output content.
Acknowledgments
This work was supported by the Intelligence Ad-
vanced Research Projects Activity (IARPA) via
Department of Interior National Business Cen-
ter (DoI/NBC) contract number D11PC20153.
The U.S. Government is authorized to reproduce
and distribute reprints for Governmental purposes
notwithstanding any copyright annotation thereon.
The views and conclusions contained herein are
those of the authors and should not be interpreted
as necessarily representing the official policies
or endorsements, either expressed or implied, of
IARPA, DoI/NBC, or the U.S. Government.
72
References
Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein.
2011. Jointly learning to extract and compress. In
Proceedings of ACL-HLT, pages 481?490.
Ted Briscoe, John Carroll, and Rebecca Watson. 2006.
The second release of the RASP system. In Pro-
ceedings of the ACL-COLING Interactive Presenta-
tion Sessions.
Ming-Wei Chang, Dan Goldwasser, Dan Roth, and
Vivek Srikumar. 2010. Discriminative learning over
constrained latent representations. In Proceedings
of HLT-NAACL, pages 429?437.
James Clarke and Mirella Lapata. 2006a. Constraint-
based sentence compression: an integer program-
ming approach. In Proceedings of ACL-COLING,
pages 144?151.
James Clarke and Mirella Lapata. 2006b. Models
for sentence compression: a comparison across do-
mains, training requirements and evaluation mea-
sures. In Proceedings of ACL-COLING, pages 377?
384.
James Clarke and Mirella Lapata. 2007. Modelling
compression with discourse constraints. In Proceed-
ings of EMNLP-CoNLL, pages 1?11.
James Clarke and Mirella Lapata. 2008. Global in-
ference for sentence compression: an integer linear
programming approach. Journal for Artificial Intel-
ligence Research, 31:399?429, March.
James Clarke and Mirella Lapata. 2010. Discourse
constraints for document compression. Computa-
tional Linguistics, 36(3):411?441.
Trevor Cohn and Mirella Lapata. 2009. Sentence com-
pression as tree transduction. Journal of Artificial
Intelligence Research, 34(1):637?674, April.
Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models. In Proceedings of
EMNLP, pages 1?8.
Hal Daume?, III and Daniel Marcu. 2002. A noisy-
channel model for document compression. In Pro-
ceedings of ACL, pages 449?456.
John DeNero and Dan Klein. 2008. The complexity of
phrase alignment problems. In Proceedings of ACL-
HLT, pages 25?28.
Mark Dras. 1997. Reluctant paraphrase: Textual re-
structuring under an optimisation model. In Pro-
ceedings of PacLing, pages 98?104.
Katja Filippova and Michael Strube. 2008. Depen-
dency tree based sentence compression. In Proceed-
ings of INLG, pages 25?32.
Dimitrios Galanis and Ion Androutsopoulos. 2010. An
extractive supervised two-stage method for sentence
compression. In Proceedings of HLT-NAACL, pages
885?893.
Michel Galley and Kathleen McKeown. 2007. Lex-
icalized Markov grammars for sentence compres-
sion. In Proceedings of HLT-NAACL, pages 180?
187, April.
Juri Ganitkevitch, Chris Callison-Burch, Courtney
Napoles, and Benjamin Van Durme. 2011. Learn-
ing sentential paraphrases from bilingual parallel
corpora for text-to-text generation. In Proceedings
of EMNLP, pages 1168?1179.
Chiori Hori and Sadaoki Furui. 2004. Speech summa-
rization: an approach through word extraction and a
method for evaluation. IEICE Transactions on In-
formation and Systems, E87-D(1):15?25.
Hongyan Jing and Kathleen R. McKeown. 2000. Cut
and paste based text summarization. In Proceedings
of NAACL, pages 178?185.
Hongyan Jing. 2000. Sentence reduction for auto-
matic text summarization. In Proceedings of the
Conference on Applied Natural Language Process-
ing, pages 310?315.
Kevin Knight and Daniel Marcu. 2000. Statistics-
based summarization - step one: Sentence compres-
sion. In Proceedings of AAAI, pages 703?710.
Kevin Knight and Daniel Marcu. 2002. Summariza-
tion beyond sentence extraction: a probabilistic ap-
proach to sentence compression. Artificial Intelli-
gence, 139(1):91?107, July.
Thomas L. Magnanti and Laurence A. Wolsey. 1994.
Optimal trees. In Technical Report 290-94,
Massechusetts Institute of Technology, Operations
Research Center.
Andre? F. T. Martins and Noah A. Smith. 2009. Sum-
marization with a joint model for sentence extraction
and compression. In Proceedings of the Workshop
on Integer Linear Programming for Natural Lan-
gauge Processing, pages 1?9.
Andre? F. T. Martins, Noah A. Smith, and Eric P. Xing.
2009. Concise integer linear programming formu-
lations for dependency parsing. In Proceedings of
ACL-IJCNLP, pages 342?350.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In Proceedings of ACL, pages 91?
98.
Ryan McDonald. 2006. Discriminative sentence com-
pression with soft syntactic evidence. In Proceed-
ings of EACL, pages 297?304.
Courtney Napoles, Chris Callison-Burch, Juri Ganitke-
vitch, and Benjamin Van Durme. 2011a. Paraphras-
tic sentence compression with a character-based
metric: tightening without deletion. In Proceedings
of the Workshop on Monolingual Text-To-Text Gen-
eration, pages 84?90.
73
Courtney Napoles, Benjamin Van Durme, and Chris
Callison-Burch. 2011b. Evaluating sentence com-
pression: pitfalls and suggested remedies. In Pro-
ceedings of the Workshop on Monolingual Text-To-
Text Generation, pages 91?97.
Tadashi Nomoto. 2007. Discriminative sentence com-
pression with conditional random fields. Infor-
mation Processing and Management, 43(6):1571?
1587, November.
Sebastian Riedel and James Clarke. 2006. Incremental
integer linear programming for non-projective de-
pendency parsing. In Proceedings of EMNLP, pages
129?137.
Stefan Riezler, Tracy H. King, Richard Crouch, and
Annie Zaenen. 2003. Statistical sentence condensa-
tion using ambiguity packing and stochastic disam-
biguation methods for lexical-functional grammar.
In Proceedings of HLT-NAACL, pages 118?125.
Alexander M. Rush and Michael Collins. 2011. Ex-
act decoding of syntactic translation models through
lagrangian relaxation. In Proceedings of ACL-HLT,
pages 72?82.
Kapil Thadani, Scott Martin, and Michael White.
2012. A joint phrasal and dependency model for
paraphrase alignment. In Proceedings of COLING,
pages 1229?1238.
Jenine Turner and Eugene Charniak. 2005. Super-
vised and unsupervised learning for sentence com-
pression. In Proceedings of ACL, pages 290?297.
Yuya Unno, Takashi Ninomiya, Yusuke Miyao, and
Jun?ichi Tsujii. 2006. Trimming CFG parse trees
for sentence compression using machine learning
approaches. In Proceedings of ACL-COLING, pages
850?857.
Kristian Woodsend and Mirella Lapata. 2012. Mul-
tiple aspect summarization using integer linear pro-
gramming. In Proceedings of EMNLP, pages 233?
243.
74
