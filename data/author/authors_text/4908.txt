Period Disambiguation with Maxent Model
Chunyu Kit and Xiaoyue Liu
Department of Chinese, Translation and Linguistics,
City University of Hong Kong, 83 Tat Chee Ave., Kowloon, Hong Kong
{ctckit, xyliu0}@cityu.edu.hk
Abstract. This paper presents our recent work on period disambigua-
tion, the kernel problem in sentence boundary identification, with the
maximum entropy (Maxent) model. A number of experiments are con-
ducted on PTB-II WSJ corpus for the investigation of how context
window, feature space and lexical information such as abbreviated and
sentence-initial words affect the learning performance. Such lexical in-
formation can be automatically acquired from a training corpus by a
learner. Our experimental results show that extending the feature space
to integrate these two kinds of lexical information can eliminate 93.52%
of the remaining errors from the baseline Maxent model, achieving an
F-score of 99.8227%.
1 Introduction
Sentence identification is an important issue in practical natural language pro-
cessing. It looks simple at first glance since there are a very small number of
punctuations, namely, period (?.?), question mark (???), and exclamation (?!?),
to mark sentence ends in written texts. However, not all of them are consistently
used as sentence ends. In particular, the use of the dot ?.? is highly ambiguous
in English texts. It can be a full stop, a decimal point, or a dot in an abbreviated
word, a numbering item, an email address or a ULR. It may be used for other
purposes too. Below are a number of examples from PTB-II WSJ Corpus to
illustrate its ambiguities.
(1) Pierre Vinken, 61 years old, will join the board as a nonexecutive
director Nov. 29.
(2) The spinoff also will compete with International Business Machines
Corp. and Japan?s Big Three -- Hitachi Ltd., NEC Corp. and Fujitsu
Ltd.
(3) The government?s construction spending figures contrast with a report
issued earlier in the week by McGraw-Hill Inc.?s F.W. Dodge Group.
Frequently, an abbreviation dot coincides with a full stop, as exemplified by
?Ltd.? in (2) above. A number followed by a dot can be a numbering item, or
simply a normal number at sentence end.
In contrast to ?.?, ?!? and ??? are rarely ambiguous. They are seldom used
for other purposes than exclamation and question marks. Thus, the focus of
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 223?232, 2005.
c
? Springer-Verlag Berlin Heidelberg 2005
224 C. Kit and X. Liu
sentence identification is on period disambiguation to resolve the ambiguity of
?.?: Whenever a dot shows up in a text token, we need to determine whether or
not it is a true period. It is a yes-no classification problem that is suitable for
various kinds of machine learning technology to tackle.
Several approaches were developed for sentence splitting. These approaches
can be categorized into three classes: (1) rule-based models consisting of man-
ually constructed rules (e.g., in the form of regular expression), supplemented
with abbreviation lists, proper names and other relevant lexical resources, as
illustrated in [1]; (2) machine learning algorithms, e.g., decision tree classifiers
[11], maximum entropy (Maxent) modelling [10] and neural networks [8], among
many others; and (3) syntactic methods that utilize syntactic information, e.g.,
[6] is based on a POS tagger. The machine learning approaches are popular, for
period disambiguation is a typical classification problem for machine learning,
and the training data is easily available.
Our research reported in this paper explores how context length and feature
space affects the performance of the Maxent model for period disambiguation.
The technical details involved in this research are introduced in Section 2, with a
focus on feature selection and training algorithm. Section 3 presents experiments
to show the effectiveness of context length and feature selection on learning
performance. Section 4 concludes the paper with our findings: putting frequent
abbreviated words or sentence-initial words into the feature space significantly
enhances the learning performance, and using a three-word window context gives
better performance than others in terms of the F-score. The best combination of
the two kinds of lexical information achieves an F-score of 99.8227%, eliminating
93.5% remaining errors from the baseline Maxent model.
2 Feature Selection
The problem of period disambiguation can be formulated as a statistical classi-
fication problem. Our research is aimed at exploring the effectiveness of Maxent
model [2,12] tackling this problem when trained with various context length and
feature sets.
Maxent model is intended to achieve the most unbiased probabilistic distri-
bution on the data set for training. It is also a nice framework for integrating
heterogeneous information into a model for classification purpose. It has been
popular in NLP community for various language processing tasks since Berger
et al [2] and Della Pietra et al [3] presenting its theoretical basis and basic
training techniques. Ratnaparkhi [9] applied it to tackle several NL ambiguity
problems, including sentence boundary detection. Wallach [14] and Malouf [4]
compared the effectiveness of several training algorithms for Maxent model.
There are a number of full-fledged implementations of Maxent models avail-
able from the Web. Using the OpenNLP MAXENT package from http://
maxent.sourceforge.net/, acknowledged here with gratitude, we are released
from the technical details of its implementation and can concentrate on exam-
ining the effectiveness of context length and feature space on period disam-
Period Disambiguation with Maxent Model 225
biguation. Basically, our exploration is carried out along the following working
procedure: (1) prepare a set of training data in terms of the feature space we
choose; (2) train the Maxent model, and test its performance with a set of testing
data; (3) examine the errors in the test outcomes and adjust the feature space
for the next round of training and testing towards possible improvement.
2.1 Context and Features
To identify sentence boundaries, a machine learner needs to learn from the train-
ing data the knowledge whether or not a dot is a period in a given context .
Classification decision is based on the available contextual information. A con-
text is the few tokens next to the target. By ?target? we refer to the ?.? to
be determined whether or not it is a period, and by ?target word? (or ?dotted
word?) we refer to the token that carries the dot in question. The dot divides
the target word into prefix and suffix, both of which can be empty. Each dot has
a true or false answer for whether it is a true period in a particular context, as
illustrated by the following general format.
[ preceding-words prefix .suffix following-words ] ? Answer: true/false . (1)
Contextual information comes from all context words surrounding the target
dot, including its prefix and suffix. However, instead of feeding the above con-
textual items to a machine learner as a number of strings for training and
testing, extracting special and specific features from them for the training is
expected to achieve more effective results. To achieve a learning model as unbi-
ased as possible, we try to extract as many features as possible from the con-
text words, and let the training algorithm to determine their significance. The
main cost of using a large feature set is the increase of training time. However,
this may be paid off by giving the learner a better chance to achieve a better
model.
Table 1. Features for a context word
Feature Description Example
IsCap Starting with a capital letter On
IsRpunct Ending with a punctuation Calgary,
IsLpunct Starting with a punctuation ??We
IsRdot Ending with a dot billions.
IsRcomma Ending with a comma Moreover,
IsEword An English word street
IsDigit An numeric item 25%, 36
IsAllCap Consisting of only capital letters (& dots) WASHINGTON
The feature set for a normal context word that we have developed through sev-
eral rounds of experiments along the above working procedure are presented in
Table 1. Basically, we extract from a word all features that we can observe from its
226 C. Kit and X. Liu
Table 2. Features for a target word
Feature Description Example
IsHiphenated Containing a dash non-U.S.
IsAllCap Consisting of only capital letters (& dots) D.C.
IsMultiDot Containing more than one dot N.Y.,
prefixIsNull A null prefix .270
prefixIsRdigit Ending with a digit 45.6
prefixIsRpunct Ending with a punctuation 0.2%.
prefixIsEword An English word slightly.
prefixIsCap Starting with a capital letter Co.
suffixIsNull A null suffix Mr.
suffixIsLdigit Starting with a digit 78.99
suffixIsLpunct Starting with a punctuation Co.?s
suffixIsRword Ending with a word Calif.-based
suffixIsCap Starting with a capital letter B.A.T
text form. For feature extraction, this set is applied equally, in a principled way, to
all context words. The feature set for both parts of a target word is highly similar
to that for a context word, except for a few specific to prefix and/or suffix, as given
in Table 2, of 13 features in total. The data entry for a given dot, for either training
or testing, consists of all such features from its target word and each of its context
words. Given a context window of three tokens, among which one is target word,
there are 2?8+13=29 features, plus an answer, in each data entry for training.
After feature extraction, each data entry originally in the form of (1) is turned
into a more general form for machine learning, as shown in (2) below, consisting
of a feature value vector and an answer.
f : [f1 =v1, f2 =v2, f3 =v3, ? ? ? , fn =vn] ? a: true/false . (2)
Accordingly, the Maxent model used in our experiments has the following
distribution in the exponential form:
p(a|f) = 1
Z(f)
exp(
?
i
?i?(fi, a)) , (3)
where ?i is a parameter to be estimated for each i through training, the fea-
ture function ?i(fi, a) = vi for the feature fi in a data entry f ? a, and the
normalization factor
Z(f) =
?
a
exp(
?
i
?i?(fi, a)) . (4)
2.2 Abbreviation List and Sentence-Initial Words
In addition to the above features, other types of contextual information can
be helpful too. For example, abbreviated words like ?Dr.?, ?Mr.? and ?Prof.?
Period Disambiguation with Maxent Model 227
may give a strong indication that the dot they carry is very unlikely to be a
period. They may play the role of counter-examples. Another kind of useful
lexical resource is sentence-initial words, e.g., ?The?, ?That? and ?But?, which
give a strong indication that a preceding dot is very likely to be a true period.
In order to integrate these two kinds of lexical resource into the Maxent
model, we introduce two multi-valued features, namely, isAbbr and isSentInit,
for the target word and its following word, respectively. They are both multi-
valued feature function. A list of abbreviated words and a list of sentence-initial
words can be easily compiled from a training corpus. Theoretically, the larger
the lists are, the better the learning performance could be. Our experiments, to
be reported in the next section, show, however, that this is not true, although
using the most frequent words in the two lists up to a certain number does lead
to a significant improvement.
3 Experiments and Results
3.1 Corpus
The corpus used for our experiments is the PTB-II WSJ corpus, a refined version
of PTB [5]. It is particularly suitable for our research purpose. In contrast to
BNC and Brown corpus, the WSJ corpus indeed contains many more dots used
in different ways for various purposes. Sentence ends are clearly marked in its
POS tagged version, although a few mistakes need manual correction. Among
53K sentences from the corpus, 49K end with ?.?. This set of data is divided
into two for training and testing by the ratio of 2:1. The baseline performance
by brute-force guess of any dot as a period is 65.02% over the entire set of data.
3.2 Baseline Learning Performance
Our first experiment is to train a Maxent model on the training set with a
three-word context window in terms of the features in Tables 1 and 2 above.
The performance on the open test is presented in Table 3. It is the baseline
performance of the Maxent model.
Table 3. Baseline learning performance of Maxent model
Precision (%) Recall (%) F-score (%)
97.55 96.97 97.26
3.3 Effectiveness of Context Window
To examine how context words affect the learning performance, we carry out a
number of experiments with context windows of various size. The experimental
results are presented in Fig. 1, where x stands for the position of target word and
228 C. Kit and X. Liu
 0.9
 0.91
 0.92
 0.93
 0.94
 0.95
 0.96
 0.97
 0.98
 0.99
 1
 0  2  4  6  8  10
F-
s c
o r
e
Context Type
00x00
01x00 11x00
00x10 00x11
01x10
11x10
01x11
11x11
Fig. 1. Effectiveness of context window
1 for a context word in use. For example, 01x10 represents a context window con-
sisting of a target word, its preceding and following words. Each such window is
itself a context type.
We can observe from the results that (1) the features extracted from the
target word itself already lead the Maxent model to an F-score beyond 92%,
(2) the context words preceding the target word are less effective, in general,
than those following the target, and (3) combining context words on both sides
outperforms those on only one side. The best three context types and the cor-
respondent performance are presented in Table 4. Since they are more effective
than others, the experiments to test the effectiveness of abbreviated words and
sentence-initial words are based on them.
Table 4. Outperforming context types and their performance
Context Type 01x10 11x10 11x11
F-score (%) 97.2623 97.6949 97.6909
3.4 Effectiveness of Abbreviated Words
Information about whether a target word is an abbreviation plays a critical role in
determining whether a dot is truly a period. To examine the significance of such
information, an abbreviation list is acquired from the training data by dotted word
collection, and sorted in terms of the difference of each item?s occurrences in the
middle and at the end of a sentence. It is assumed that the greater this difference is,
the more significant a dotted word would be as a counter-example. In total, 469
such words are acquired, among which many are not really abbreviated words.
A series of experiments are then conducted by adding the next 50 most frequent
dotted words to the abbreviation list for model training each time. To utilize such
Period Disambiguation with Maxent Model 229
 0.994
 0.9945
 0.995
 0.9955
 0.996
 0.9965
 0.997
 0.9975
 0.998
 0  100  200  300  400  500
F-
s c
o r
e
Abbreviated Word Number
01x10
11x10
11x11
Fig. 2. Effectiveness of abbreviation list
Table 5. Effectiveness of abbreviation list
Context Type 01x10 11x10 11x11
F-score (%) 99.6908 99.6908 99.6815
Increase +2.4285 +1.9959 +1.9906
lexical resource, a multi-valued feature isAbbr is introduced to the feature set to
indicate whether a target word is in the abbreviation list and what it is. That is,
all words in the list actually play a role equivalent to individual bi-valued features,
under the umbrella of this new feature.
The outcomes from the experiments are presented in Fig. 2, showing that
performance enhancement reaches rapidly to the top around 150. The perfor-
mance of the three best context types at this point is given in Table 5, indi-
cating that an abbreviation list of 150 words leads to an enhancement of 1.99?
2.43 percentage points, in comparison to Table 4. This enhancement is very
significant at this performance level. Beyond this point, the performance goes
down slightly.
3.5 Effectiveness of Sentence-Initial Words
In a similar way, we carry out a series of experiments to test the effectiveness
of sentence-initial words. In total, 4190 such words (word types) are collected
from the beginning of all sentences in the training corpus. Every time the next
200 most frequent words are added to the sentence-initial word list for training,
with the aid of another multi-valued feature isSentInit for the context word
immediately following the target word.
Experimental outcomes are presented in Fig. 3, showing that the performance
maintains roughly at the same level when the list grows. Until the very end,
230 C. Kit and X. Liu
 0.94
 0.95
 0.96
 0.97
 0.98
 0.99
 1
 0  500  1000  1500  2000  2500  3000  3500  4000
F-
s c
o r
e
Sentence Initial Word Number
01x10
11x10
11x11
Fig. 3. Effectiveness of sentence-initial words
Table 6. Performance enhancement by sentence-initial words
Context Type 01x10 11x10 11x11
List size 1200 1000 1200
F-score (%) 98.4307 98.4868 98.5463
Increase +1.1784 +0.7919 +0.8554
when those most infrequent (or untypical) sentence-initial words are added, the
performance drops rapidly. The numbers of sentence-initial words leading to the
best performance with various context types are presented in Table 6. This list
of words lead to a significant performance enhancement of 0.79?1.18 percentage
points, in comparison to Table 4.
3.6 Combination of Two Lists
Through the experiments reported above we find the optimal size of abbreviation
list and sentence-initial words, both in the order of their frequency ranks, in
each context type of our interests. The straightforward combination of these two
lists in terms of these optimal sizes leads to almost no difference from using
abbreviation list only, as presented in Table 7.
Table 7. Performance from simple combination of the two lists
Context Type 01x10 11x10 11x11
Sentence-initial words 1200 1000 1200
Abbreviation list 150 150 150
F-score (%) 99.7064 99.7156 99.6912
Period Disambiguation with Maxent Model 231
Table 8. Performance from various size combination of the two lists
Sentence-initial Abbreviation F-score
words list 01x10 11x10 11x11
100 200 99.7646% 99.7738% 99.7707%
100 400 99.7125% 99.7033% 99.7002%
100 600 99.7033% 99.6971% 99.6971%
100 800 99.6788% 99.6941% 99.6911%
100 1000 99.6696% 99.6818% 99.6696%
100 1200 99.6635% 99.6574% 99.6544%
150 200 99.8013% 99.7890% 99.7921%
150 400 99.7431% 99.7339% 99.7369%
150 600 99.7431% 99.7370% 99.7370%
150 800 99.7401% 99.7309% 99.7278%
150 1000 99.7156% 99.7156% 99.7064%
150 1200 99.7064% 99.7034% 99.6912%
200 200 99.8227% 99.7890% 99.7921%
200 400 99.7584% 99.7461% 99.7339%
200 600 99.7523% 99.7431% 99.7339%
200 800 99.7462% 99.7370% 99.7340%
200 1000 99.7309% 99.7125% 99.7064%
200 1200 99.7095% 99.6973% 99.6911%
To explore the optimal combination of the two lists, a series of experi-
ments are carried out near each list?s optimal size. The results are presented in
Table 8, showing that the best combination is around 200 words from each list
and any deviation from this point would lead to observable performance declina-
tion. The best performance at this optimal point is 99.8227% F-score, achieved
with the 01x10 context type, which is significantly better than the best perfor-
mance using any single list of the two.
Comparing to the baseline performance of the Maxent model in Table 4,
we can see that this improvement increases only 99.8227 - 97.2623 = 2.5604
percentage points. Notice, however, that it is achieved near the ceiling level. Its
particular significance lies in the fact that 99.8227?97.2623100?97.2623 = 93.52% remaining
errors from the baseline model are further eliminated by this combination of the
two lists, both of which are of a relatively small size.
4 Conclusions
We have presented in the above sections our recent investigation into how con-
text window, feature space and simple lexical resources like abbreviation list and
sentence-initial words affect the performance of the Maxent model on period dis-
ambiguation, the kernel problem in sentence identification. Our experiments on
PTB-II WSJ corpus suggest the following findings: (1) the target word itself pro-
vides most useful information for identifying whether or not the dot it carries is a
232 C. Kit and X. Liu
true period, achieving an F-score beyond 92%; (2) unsurprisingly, the most useful
context words are the two words next to the target word, and the context words to
its right is more informative in general than those to its left; and (3) extending the
feature space to utilize lexical information from the most frequent 200 abbreviated
words and sentence-initial words, all of which can be straightforwardly collected
from the training corpus, can eliminate 93.52% remaining errors from the baseline
model in the open test, achieving an F-score of 99.8227%.
Acknowledgements
The work described in this paper was supported by the Research Grants Council
of HKSAR, China, through the CERG grant 9040861 (CityU 1318/03H). We
wish to thank Alex Fang for his help.
References
1. Aberdeen, J., Burger, J., Day, D., Hirschman, L., Robinson, P., and Vilain, M.:
Mitre: Description of the alembic system used for muc-6. In Proceedings of the
Sixth Message Understanding Conference (MUC-6), Columbia, Maryland. Morgan
Kaufmann (1995)
2. Berger, A., Pietra, S.D., and Pietra, V.D.: A maximum entropy approach to natural
language processing. Computational linguistics. (1996) 22(1):39?71
3. Della Pietra, S., Della Pietra, V., and Lafferty, J.: Inducing features of ran-
dom fields. Transactions Pattern Analysis and Machine Intelligence. (1997) 19(4):
380?393
4. Malouf, R.: A comparison of algorithms for maximum entropy parameter estima-
tion. In Proceedings of CoNLL-2002, Taipei, Taiwan (2002) 49?55
5. Marcus, M.P., Santorini, B., and Marcinkiewicz, M.A.: Building a large annotated
corpus of english: The penn treebank. Computational Linguistics. (1993) 19(2):
313?329
6. Mikheev, A.: Tagging sentence boundaries. In Proceedings of the First Meeting
of the North American Chapter of the Association for Computational Linguistics
(NAACL?2000). (2000)
7. Mitchell, T.: Machine Learning. McGraw Hill, New York (1997)
8. Palmer, D.D. and Hearst, M.A.: Adaptive Multilingual Sentence Boundary Disam-
biguation. Computational Linguistics. (1997) 23(2):241?267
9. Ratnaparkhi, A.: Maximum entropy models for natural language ambiguity resolu-
tion. Ph.D. dissertation, University of Pennsylvania (1998)
10. Reynar, J.C. and Ratnaparkhi, A.: A maximum entropy approach to identifying
sentence boundaries. In Proceedings of the Fifth Conference on Applied Natural
Language Processing, Washington, D.C. (1997)
11. Riley, M.D.: Some applications of tree-based modelling to speech and language
indexing. In Proceedings of the DARPA Speech and Natural Language Workshop.
Morgan Kaufmann (1989) 339?352
12. Rosenfeld, R.: Adaptive statistical language modeling: A Maximum Entropy Ap-
proach. PhD thesis CMU-CS-94. (1994)
13. Van Rijsbergen, C.J.: Information Retrieval. Butterworths, London (1979)
14. Wallach, H.M.: Efficient training of conditional random fields. Master?s thesis, Uni-
versity of Edinburgh (2002)
An Example-Based Chinese Word Segmentation System for CWSB-2
Chunyu Kit Xiaoyue Liu
Department of Chinese, Translation and Linguistics
City University of Hong Kong
Tat Chee Ave., Kowloon, Hong Kong
{ctckit, xyliu0}@cityu.edu.hk
Abstract
This paper reports the example-based
segmentation system for our participa-
tion in the second Chinese Word Seg-
mentation Bakeoff (CWSB-2), present-
ing its basic ideas, technical details and
evaluation. It is a preliminary imple-
mentation. CWSB-2 valuation shows
that it performs very well in identify-
ing known words. Its unknown word
detection module also illustrates great
potential. However, proper facilities for
identifying time expressions, numbers
and other types of unknown words are
needed for improvement.
1 Introduction
Word segmentation is to identify lexical items,
especially individual word forms, in a text. It
involves two fundamental tasks, both aiming at
minimizing segmentation errors: one is to in-
fer out-of-vocabulary (OOV) words, also known
as unknown (or unseen) word detection, and the
other to identify in-vocabulary (IV) words, with
an emphasis on disambiguation. OOV words and
ambiguities are the two major causes to segmen-
tation errors.
Accordingly?word segmentation approaches
can be divided into the categories summarized in
Table 1 in terms of the resources in use to tackle
these two causes. The closed and open tracks in
CWSB correspond, respectively, to the last two
categories, both involving inferring OOV words
Category Resource in use Major Task
Lexicon Tr. Corpus OOV Disamb.
WDa - (-)b +
WS/CLc + - - +
WS/ILd + - + +
WS/TCe (+)f + + +
WS/TC+Lg + + + +
aWord discovery, or unsupervised lexical acquisition
bInput data is used for unsupervised training
cWord segmentation with a complete lexicon
dWord segmentation with an incomplete lexicon
eWord segmentation with a pre-segmented training corpus
fIt can be extracted from the given training corpus.
gWord segmentation with a pre-segmented training corpus
and an extra lexicon
Table 1: Categories of segmentation approach
beyond disambiguating IV words. Word discov-
ery and OOV word detection pursue a similar tar-
get, i.e., inferring new words. The continuum
connecting them is the size of the lexicon in use:
the former assumes few words known and the lat-
ter an existing lexicon to some scale. Inferring
new words is an essential task in word segmen-
tation, for a complete lexicon is rarely a realistic
assumption in practice.
This paper presents our segmentation system
for participation in CWSB-2. It takes an example-
based approach to recognize IV words and fol-
lows description length gain (DLG) to infer OOV
words in terms of their text compression effect.
Sections 2 and 3 below introduce the example-
based and DLG-based segmentation respectively.
Section 4 presents a strategy to combine their
strength and Section 5 reports our system?s per-
formance in CWSB-2. Following error analysis
in Section 6, Section 7 concludes the paper.
146
2 Example-based segmentation
How to utilize as much information as possi-
ble from the training corpus to adapt a segmen-
tation system towards a segmentation standard
has been a critical issue. Kit et al (2002) and
Kit et al (2003) attempt to integrate case-based
learning with statistical models (e.g., n-gram) by
extracting transformation rules from the train-
ing corpus for disambiguation via error correc-
tion; Gao et al (2004) adopt a similar strategy
for adaptive segmentation, with transformation
templates (instead of case-based rules) to modify
word boundaries (instead of individual words).
The basic idea of example-based segmentation
is very simple: existing pre-segmented strings in
training corpus provide reliable examples for seg-
menting similar strings in input texts. In contrast
to dictionary checking for locating possible words
in an input sentence to facilitate later segmenta-
tion operations, pre-segmented examples give ex-
act segmentation to copy.
The example-based segmentation can be im-
plemented in the following steps.
1. Find all exemplar pre-segmented fragments,
with regards to a training corpus, and all
possible words, with regards to a lexicon,
from each character in an input sentence;
2. Identify the optimal sequence, among all
possibilities, of the above items over the sen-
tence following some optimization criterion.
If adopting the minimal number of fragments or
words in a sequence as optimization criterion, we
have a maximal matching approach to word seg-
mentation. However, it differs remarkably from
the previous maximal matching approaches: it
matches pre-segmented fragments, instead of dic-
tionary words, against an input sentence. It can be
carried out by a best-first strategy: repeatedly se-
lect the next longest example or word until the en-
tire sentence is properly covered. Unfortunately,
the best-first approach does not guarantee to give
the best answer. For CWSB-2, we implemented
a program following the Viterbi algorithm to per-
form a complete search in terms of the number of
fragments, and then words, in a sequence.
However, a serious problem with this example-
based approach is the sparse data problem. Long
exemplar fragments are more reliable but small
in number, whereas short ones are large in num-
ber but less reliable. In the case of no exemplar
fragment available for an input sentence, this ap-
proach draws back to the maximal match segmen-
tation with a dictionary. How to incorporate sta-
tistical inference into example-based segmenta-
tion to infer more reliable optimal segmentation
beyond string matching remains a critical issue
for us to tackle.
3 DLG-based segmentation
DLG is formulated in Kit and Wilks (1999) and
Kit (2000) as an empirical measure for the com-
pression effect of extracting a substring from a
given corpus as a lexical item. DLG optimization
is applied to detect OOV words for our participa-
tion in CWSB-2. It works as follows in two steps.
1. Calculate the DLG for all known words
and all new word candidate (i.e., substrings
with frequency ? 2, preferably, in the test
corpus), based on frequency information in
the training and the test corpora;
2. Find the optimal sequence of such items over
an input sentence with the greatest sum of
DLG.
Step 2 above in our system re-implements only
the first round of DLG-based lexical learning in
Kit (2000). It is implemented by the same algo-
rithm as the one for example-based segmentation,
with DLG as optimization criterion. Evaluation
results show that this learning-via-compression
approach discovers many OOV words success-
fully, in particular, person names.
4 Integration
The example-based segmentation is good at iden-
tifying IV words but incapable of recognizing any
new words. In contrast, the DLG-based segmen-
tation performs slightly worse but has potential to
detect new words. It is expected that the strength
of the two could be combined together for perfor-
mance enhancement.
However, because of inadequate time we had
to take a shortcut in order to catch the CWSB-
2 deadline: DLG segmentation is only applied
to recognize new words among the sequences of
mono-character items in the example-based seg-
mentation output.
147
Track P R F OOV ROOV RIV
ASc .944 .902 .923 .043 .234 .976
PKUc .929 .904 .916 .058 .252 .971
MSRc .965 .935 .950 .026 .189 .986
Table 2: System performance in CWSB-2
5 Performance
Our group took part in three closed tracks in
CWSB-2, namely, ASc, PKUc and MSRc, with a
preliminary implementation of the example-based
word segmentation presented above. Our sys-
tem?s performance in terms of CWSB-2?s offi-
cial scores is presented in Table 2. Its ROOV
scores look undesirable, showing that applying
the first round of DLG-based segmentation to se-
quences of mono-character items is inadequate
for the OOV word discovery task. Nevertheless,
its RIV scores are, in general, quite close to the
top systems in CWSB-2, although it does not have
a disambiguation module to polish its maximal
matching output.
However, this is not to say that the DLG-based
segmentation deserves no credit in unknown word
detection. It does recognize many OOV words,
as shown in Table 3. The low ROOV rate has to
do with our system?s incapability in handling time
expressions, numbers, and foreign words.
6 Error analysis
Most errors made by our system are due to
the following causes: (1) no knowledge, overt
or implicit, in use for recognizing time expres-
sions, numbers and foreign words, as restricted by
CWSB-2 rules, (2) a premature module for OOV
word detection, (3) no further disambiguation be-
sides example application, and (4) significant in-
consistency in the training and test data.
The inconsistency exists not only between the
training and test corpora for each track but, more
surprisingly, also within individual training cor-
pora. Some suspected cases are illustrated in Ta-
bles 4, 5 and 6. They are observed to be in a large
number in the CWSB-2 corpora. Scoring with a
golden standard involving so many of them ap-
pears to be problematic, for it penalizes the sys-
tems for handling such cases right and rewards
the others for producing ?correct? answers. What
ASc: ??(106) ???(45) ??(31) 29)??) ??(21) ??(20) ?
?(18) ???(17) ???(16) ???(15) ???(13) ??(12) ?
??(11) ??(11) ????(11) ??10)?) ??(9) ???(8) ?
?(8) ???(7) ??(7) ???(7) ???(6) ????(6) ?
?(5) ???(5) ???(5) ???(5) ???(5) ???(5) ??(5) ?
?5)?) ???(4) ??(4) ??(4) ????(4) ???(3) ?
?(3) ????(3) ???(2) ??(2) ????2)?) ??(2) ? ? ? ? ? ?
PKUc: ??(38) ??(23) ??(21) ???20)???) ?
?(19) 17)???) ?16)?) ??(15) ???(12) ??(11) ??
?(10) ??(10) ??9)?) ??(9) ?8)??) ??(8) ???(8) ?
?(7) ???(7) ??(6) ???(6) ??6)?) ??(6) ?6)?) ??
?(5) ????(5) 5)??) ???(5) ???(5) ??(5) ??(5) ?
?(5) ???(4) ???(4) ???(4) ?4)?) ???(4) ??(4) ?
?(4) ??(4) ?4)?) ???(4) ???(4) ??(4) ??3)?) ?
??(3) ?????(3) ??(3) ???(3) ??(3) ??(3) ?
?(3) ???(3) ???(3) ? ? ? ? ? ?
MSRc: ?26)?) ??(19) ???(19) ???(17) ?15)?) ?
14)?) ??(14) ???(13) ??(13) ????????(12) ?
?(12) ??(11) ???(10) ??(10) ?10)???) ???(10) ?
?(10) ???(9) ??9)?) ???(8) ?8)?) ??(8) ???(7) ?
???(7) ???(7) ??(6) ???(6) ??(6) ?6)?) ??(6) ?
?(5) ???(5) ??(5) ??(5) ???(5) ???(5) ???(4) ?
?(4) ?4)?) ??(4) ???(4) ???(4) ???(3) ???(3) ?
?(3) ???(3) ???(3) ???(3) ??3)?) ??(3) ???(3) ?
??(3) ???(3) ???(3) ??(3) ???(3) ????(2) ? ? ? ? ? ?
Table 3: Illustration of new words successfully
detected, with frequency in parentheses
is even more worth noting is that (1) an inconsis-
tent case involves more than one word, and (2)
the difference between a correct and an erroneous
judgment of a word is 1, in a sense, but the differ-
ence between one system that loses it for doing
right and another that earns it by doing wrong is
surely greater.
7 Conclusions
In the above sections we have reported the
example-based word segmentation system for
our participation in CWSB-2, including its ba-
sic ideas, technical details and evaluation results.
It has illustrated an excellent performance in IV
word identification and nice potential in OOV
word discovery. However, its weakness in han-
dling time expressions, numbers and other types
of unknown words has hindered it from perform-
ing better. We are expecting to implement a full-
fledged version of the system for improvement.
Acknowledgements
The work described in this paper was supported
by the RGC of HKSAR, China, through the
CERG grant 9040861. We wish to thank Alex
Fang and Robert Neather for their help.
148
Training & Answer fT/fA Golden Standard fT/fA
?? ? 4/8 ??? 0/0
? ? 28/7 ?? 0/0
?? ? 5/7 ??? 0/0
? ?? 11/6 ??? 0/0
? ? 186/5 ?? 0/0
?? ? 41/4 ??? 0/0
? ? ? 29/4 ??? 0/0
? ? ? 129/4 ??? 0/0
? ? ? 23/3 ??? 0/0
? ?? 47/3 ??? 0/0
?? ?? 33/2 ???? 0/0
?? ?? 32/2 ???? 0/0
?? ?? 85/2 ???? 0/0
???? 10/2 ?? ?? 0/0
?? ? 62/2 ??? 0/0
? ?? 23/2 ??? 0/0
???? 192/1 ? ?? ? 0/0
??? 149/1 ? ?? 0/0
?? ?? 66/1 ???? 0/0
??? 31/1 ?? ? 0/0
? ?? 80/1 ??? 0/0
?? ? 68/1 ??? 0/0
? ? ?? 13/1 ???? 0/0
?? ?? 13/1 ???? 0/0
? ?? 20/1 ??? 0/0
? ? ? ? 6/1 ???? 0/0
? ? ? 29/1 ??? 0/0
?? ? ? ? 4/1 ????? 0/0
???? 24/7 ?? ?? 25/0
???? 17/3 ?? ?? 53/0
? ? ? 1201/2 ??? 2/0
Table 4: Some inconsistent cases in AS corpus
Training & Answer fT/fA Golden Standard fT/fA
??? 14/26 ? ?? 0/0
??? 6/1 ? ?? 0/0
??? 5/21 ? ?? 0/0
???? 24/19 ?? ? ? 0/0
? ? 23/18 ?? 0/0
???? 66/15 ?? ?? 0/0
????? 10/9 ?? ??? 0/0
????? 10/5 ? ?? ?? 0/0
???? 45/5 ?? ?? 0/0
???? 42/5 ?? ?? 0/0
???? 27/4 ?? ?? 0/0
???? 21/4 ?? ?? 0/0
???? 126/4 ?? ?? 0/0
???? 20/4 ?? ?? 0/0
???? 15/4 ?? ?? 0/0
???? 25/4 ? ??? 0/0
???? 25/3 ?? ?? 0/0
????? 13/3 ?? ??? 0/0
??? 32/3 ?? ? 0/0
???? 30/3 ?? ?? 0/0
??? 11/3 ? ?? 0/0
???? 15/3 ?? ?? 0/0
???? 22/3 ?? ?? 0/0
?? ? 11/2 ? ?? 0/0
?? 25/2 ? ? 0/0
??????? 3/1 ???? ? ? ? 0/0
??? 13/1 ? ? ? 0/0
??? 24/5 ? ?? 1/0
??? 49/4 ?? ? 1/0
?? 112/3 ? ? 14/0
???? 48/1 ?? ?? 1/0
Table 5: Some inconsistent cases in PKU corpus
Training & Answer fT/fA Golden Standard fT/fA
?? ? 12/7 ??? 0/0
??? 16/6 ? ?? 0/0
o ?? 29/5 o?? 0/0
?? ???? 6/3 ???? ?? 0/0
?? ? 3/3 ? ?? 0/0
???? 1/2 ?? ?? 0/0
????? 4/2 ?? ?? 0/0
? ? 10/2 ?? 0/0
?? 3/2 ? ? 0/0
???? ? ? 7/1 ?????? 0/0
??? 2/1 ?? ? 0/0
??? ? 1/1 ? ? ?? 0/0
??a?a?? 4/1 ?? a ? a ? 0/0
????? 1/1 ??? ?? 0/0
????? ? 1/1 ?? ???? 0/0
??? ???? 1/1 ????? ?? 0/0
? ??? 10/1 ?? ? 0/0
??? 16/1 ? ? 0/0
??? 4/1 ? ? 0/0
???? 16/1 ? ? ? 0/0
???? 122/1 ?? ?? 0/0
???? ?? 3/1 ?? ???? 0/0
Table 6: Some inconsistent cases in MSR corpus
References
E. Brill. 1993. A Corpus-Based Approach to Lan-
guage Learning. PhD thesis, University of Pennsyl-
vania, Philadelphia.
J. Gao, A. Wu, M. Li, C. Huang, H. Li, X. Xia and H.
Qin. 2004. Adaptive Chinese word segmentation.
In ACL-04. Barcelona, July 21-26.
C. Kit and Y. Wilks. 1999. Unsupervised learning of
word boundary with description length gain. In M.
Osborne and E. T. K. Sang (eds.), CoNLL-99, pp.1-
6. Bergen, Norway, June 12.
C. Kit 2000. Unsupervised Lexical Learning as
Inductive Inference. PhD thesis, University of
Sheffield.
C. Kit, H. Pan and H. Chen. 2002. Learning case-
based knowledge for disambiguating Chinese word
segmentation: A preliminary study. SIGHAN-1,
pp.33?39. Taipei, Sept. 1, 2002.
C. Kit, Z. Xu and J. J. Webster. 2003. Integrating
n-gram model and case-based learning for Chinese
word segmentation. In Q. Ma and F. Xia (eds.),
SIGHAN-2, pp.160-163. Sapporo, 11 July, 2003.
D. Palmer. A trainable rule-based algorithm for word
segmentation. In ACL-97, pp.321-328. Madrid.
149
Harvesting the Bitexts of the Laws of Hong Kong From the Web
Chunyu Kit Xiaoyue Liu KingKui Sin Jonathan J. Webster
Department of Chinese, Translation and Linguistics
City University of Hong Kong, Tat Chee Ave., Kowloon, Hong Kong
{ctckit, xyliu0, ctsinkk, ctjjw}@cityu.edu.hk
Abstract
In this paper we present our recent work
on harvesting English-Chinese bitexts
of the laws of Hong Kong from the
Web and aligning them to the subpara-
graph level via utilizing the number-
ing system in the legal text hierarchy.
Basic methodology and practical tech-
niques are reported in detail. The re-
sultant bilingual corpus, 10.4M English
words and 18.3M Chinese characters,
is an authoritative and comprehensive
text collection covering the specific and
special domain of HK laws. It is par-
ticularly valuable to empirical MT re-
search. This piece of work has also laid
a foundation for exploring and harvest-
ing English-Chinese bitexts in a larger
volume from the Web.
1 Introduction
Bitexts, also referred to as parallel texts or bilin-
gual corpora, collections of bilingual text pairs
aligned at various levels of granularity, have been
playing a critical role in the current development
of machine translation technology. It is such
large data sets that give rise to the plausibility
of empirical approaches to machine translation,
most of which involve the application of a variety
of machine learning techniques to infer various
types of translation knowledge from bitext data
to facilitate automatic translation and enhance
translation quality. Large volumes of training
data of this kind are indispensable for construct-
ing statistical translation models (Brown et al,
1993; Melamed, 2000), acquiring bilingual lex-
icon (Gale and Church, 1991; Melamed, 1997),
and building example-based machine translation
(EBMT) systems (Nagao, 1984; Carl and Way,
2003; Way and Gough, 2003). They also provide
a basis for inferring lexical connection between
vocabularies in cross-languages information re-
trieval (Davis and Dunning, 1995).
Existing parallel corpora have illustrated their
particular value in empirical NLP research, e.g.,
Canadian Hansard Corpus (Gale and Church,
1991b), HK Hansard (Wu, 1994), INTERSECT
(Salkie, 1995), ENPC (Ebeling, 1998), the Bible
parallel corpus (Resnik et al, 1999) and many
others. The Web is being explored not only as a
super corpus for NLP and linguistic research (Kil-
garriff and Grefenstette, 2003) but also, more im-
portantly to MT research, as a treasure for mining
bitexts of various language pairs (Resnik, 1999;
Chen and Nie, 2000; Nie and Cai, 2001; Nie
and Chen, 2002; Resnik and Smith, 2003; Way
and Gough, 2003). The Web has been the play-
ground for many NLPers. More and more Web
sites are found to have cloned their Web pages in
several languages, aiming at conveying informa-
tion to audience in different languages. This gives
rise to a huge volume of wonderful bilingual or
multi-lingual resources freely available from the
Web for research. What we need to do is to har-
vest the right resources for the right applications.
In this paper we present our recent work on
harvesting English-Chinese parallel texts of the
laws of Hong Kong from the Web and construct-
71
ing a subparagraph-aligned bilingual corpus of
about 20 million words. The bilingual texts of the
laws is introduced in Section 2, with an emphasis
on HK?s legislation text hierarchy and its num-
bering system that can be utilized for text align-
ment to subparagraph level. Section 3 presents
basic methodology and technical details for har-
vesting and aligning bilingual Web page pairs, ex-
tracting content texts from the pages, and align-
ing text structures in terms of the text hierarchy
via utilizing consistent intrinsic features in the
Web pages and content texts. Section 4 presents
XML schema for encoding the alignment results
and illustrates the display mode for browsing the
aligned bilingual corpus. Section 5 concludes
the paper, highlighting the value of the corpus in
term of its volume, translation quality, specificity
and comprehensiveness, and alignment granular-
ity. Our future work to explore the Web for har-
vesting more quantities of parallel bitexts is also
briefly outlined.
2 Bilingual Texts of the Laws of HK
The laws of Hong Kong (HK) before 1987 were
exclusively enacted in English. They were trans-
lated into Chinese in the run-up to the handover
in 1997. Since then all HK laws have been en-
acted in both English and Chinese, both versions
being equally authentic. This gives rise to a valu-
able set of bitexts in large quantity and high qual-
ity that can be utilized to facilitate empirical MT
research.
2.1 BLIS Corpus
The bilingual texts of the laws of Hong Kong
have been made available to the public in re-
cent years by the Justice Department of the HK-
SAR through the bilingual laws information sys-
tem (BLIS). All these texts are freely accessible
from http://www.justice.gov.hk/.
BLIS provides the most comprehensive docu-
mentation of HK legislation. It contains all statute
laws of Hong Kong currently in operation, includ-
ing all ordinances and subsidiary legislation of
HK (and some of their past versions dating back
to 60 June 1997), the Basic Law and the Sino-
British Joint Declaration, the constitution of PRC
and national laws that apply in HK, and other rel-
evant instruments. The entire bilingual corpus of
Figure 1: Illustration of BLIS hierarchy
BLIS legal texts contains approximately 10 mil-
lion English words and 18 million Chinese char-
acters. Lexical resources of this kind are particu-
larly useful in bilingual legal terminology studies
and text alignment work.
2.2 Text Hierarchy
BLIS organizes the legal texts in terms of the
hierarchy of the Loose-Leaf Edition of the Laws
of Hong Kong. At the top level, the ordinances
are arranged by chapters, each of which is identi-
fied by an assigned number and a short title, e.g.,
Chapter 5 OFFICIAL LANGUAGES ORDINANCE /
?5? ??????. The assigned number for a
subsidiary legislation chapter consists of a chap-
ter number and a following uppercase letter, e.g.,
CAP 5C HIGH COURT CIVIL PROCEDURE (USE
OF LANGUAGE) RULES / ?5C? ???????
?(????)??.
The content of an ordinance, exclusive of its
long title, is divided and identified according to a
very rigid numbering system which encodes the
hierarchy of the texts of the laws. Both the Chi-
nese and English versions of an ordinance fol-
low exactly the same hierarchical structures such
as chapters (?), parts (?), sections (?), sub-
sections (?), paragraphs (?) and subparagraphs
(?). This allows us to align the bitexts along
72
Figure 2: BLIS texts in pair
this hierarchical structure, once they are down-
loaded from the BLIS official site. To our knowl-
edge, a well-aligned bilingual corpus of this size
covering a special domain so comprehensively is
seldom readily available for the Chinese-English
language pair.
Excerpts from the BLIS corpus are illustrated
in Figure 1 and 2, one illustrating its hierarchy and
the other a pair of BLIS bitexts. From the excerpts
we can see that not everything has an exact match
between a pair of BLIS Web pages. For example,
the Chinese side has a gazette number ?25 of 1998
s. 2? and a piece of ?remarks? at the beginning of
content text, whereas its English counterpart has
none of them.
3 Harvesting Bitexts from the Web
Basically two phases are involved in construct-
ing the bilingual corpus of the laws of HK. The
first phase is to harvest the monolingual texts of
HK laws from the BLIS site and align them into
pairs. It involves the following steps: (1) down-
loading Web pages one by one with the aid of a
Web crawler, (2) extracting the texts from them
by filtering out the HTML markup, and (3) align-
ing the extracted monolingual texts into bilingual
Figure 3: BLIS web pages connected as two dou-
ble linked lists
pairs. The second phase is to align finer-grained
text structures within each text pair.
3.1 Downloading BLIS Web Pages
A BLIS Web page does not necessarily corre-
spond to any particular text structure such as a
chapter, a part, a section, a subsection, or a para-
graph in the BLIS hierarchy. A chapter, espe-
cially a short one, may be organized into a few
sections in a Web page or in several contiguous
pages. Some sections, e.g., the long ones, are di-
vided into several pages. In general, BLIS does
not maintain any reliable match between its Web
pages and any particular text hierarchical struc-
tures.
Fortunately, in most cases a BLIS page always
has a counterpart in the other language. There is
a ?switch language? button on each page to link
to the counterpart page. Such linkage allows us
to download the Web pages in pairs and, conse-
quently, harvest a list of page-to-page aligned bi-
texts.
In addition to the pair link, each BLIS page also
carries links for the ?next? and the ?previous sec-
tion of enactment?. These two kinds of linkage
turn the pages into two double linked lists, each
in a language, as illustrated in Figure 3, with each
page as a node. Nodes in pairs are also double
linked between the two lists.
However, the pairwise linkage is not reliable
in the BLIS site, because there are missing Web
pages in one of the two languages in question
(see Table 3 below for more details). In order to
download all bitexts of legislation from the site,
we need to go through one linked list and down-
load each page and its counterpart, if there is one,
in the other language. Such scanning gives a list
of text pairs, where some pages may have a null
73
Total time Downloaded files
English 17 hours 50,638 (429MB)
Chinese 18 hours 50,510 (460MB)
Table 1: File downloading
File name
BLIS HTML page title Chinese English
Cap 5A ... 5A c.txt 5A e.txt
Cap 5A s 1 ... 5A-1 c.txt 5A-1 e.txt
Cap 5A s 2 ... 5A-2 c.txt 5A-2 e.txt
Cap 5A s 3 ... 5A-3 c.txt 5A-3 e.txt
Table 2: Naming downloaded files in terms of
BLIS numbering
counterpart. An alternative strategy is to down-
load each list separately, and then match the pages
into pairs sequentially with the aid of numbering
information in the header of each page ? see 3.2
below. These two strategies verify one another,
making sure that all pages are downloaded and
put in the right pairs.
The downloading is carried out by a Web
crawler implemented in Java. In order to accom-
plish the above strategies, it also has to handle a
number of technical issues.
? It sleeps for a while (e.g., 10 seconds) when
it finishes downloading a certain number of
pages (e.g., 50 pages), because the BLIS site
refuses continuous access from one site for a
too long time.
? When an error occurs, it remembers the cur-
rent URL. Then it re-starts from where it
stops.
The data about the file downloading from BLIS
site is given in Table 1. One can conceive that
if the time intervals for sleep and downloading
could be automatically tuned by the crawler to
maximize the downloading efficiency, it would
get the job done significantly more quickly. Our
option for 10 seconds sleep between every 50 files
is based on error records of a number of test runs.
3.2 Aligning Web Pages
Every BLIS Web page is identified by a subti-
tle that carries numbering information about the
page, as illustrated in Figure 1. Such a subtitle
is exactly retained in the page as its HTML title.
Files English Chinese
Aligned 50,506 (62.3MB)a 50,506 (38.5MB)
Missing 132 4
Total 50,638 50,510
Sizeb 10.4M words 18.3M char.s
aThe size of extracted texts.
bExclusive of punctuation marks.
Table 3: The number of aligned and missing files
This feature is utilized to align BLIS pages: all
downloaded files are named in terms of the num-
bering information extracted from their HTML ti-
tles, as illustrated in Table 2. Consequently, all
files are naturally aligned in pairs by their names.
Any file names not in a pair indicate the missing
counterparts in the other language. The statistics
of file alignment are given in Table 3.
3.3 Text Extraction
Basically, this task involves two aspects, namely,
filtering HTML markup and extracting content
text. A straightforward strategy is that we first
clean up HTML tags in each page and then the
non-legal content. The tags are in brackets, and
non-legal content in a consistent pattern through-
out all BLIS pages. However, a more convenient
way to do it is to make use of a reliable feature
in the BLIS pages: legal content is placed in be-
tween two ? the only two ? horizontal bars in each
page. Accordingly, we implement a strategy to
first extract every thing in between the two bars
and then clean up remaining HTML tags. The
output from this procedure includes
? a header as a fixed set of items, including
chapter number, title, heading, etc., and
? a piece of content text as a list of numbered
items each in a line. (See the header and con-
tent text in Figure 2.)
The text in a BLIS page is displayed as a sequence
of hierarchically numbered items, such as subsec-
tions, paragraphs and subparagraphs.
3.4 Text Alignment within Text Pairs
After page (or file) alignment, each page finds its
counterpart in the other language. After text ex-
traction, a page gives a content text consisting of
a list of numbered items, each in a line. A such
74
Remarks:
Adaptation amendments retroactively made - see
26 of 1999 s.3//a
(1) All Ordinances shall be enacted and published
in both official languages.//
(2) Nothing in subsection (1) shall require an
Ordinance to be enacted and published in
both official languages where that Ordinance
amends another Ordinance and-//
(a) that other Ordinance was enacted in the
English language only; and//
(b) no authentic text of that Ordinance has been
published in the Chinese language under
section 4B(1).//
(3) Nothing in subsection (1) shall require an
Ordinance to be enacted and published in both
official languages where the Chief Executive
in Council- (Amended 26 of 1999 s.3)//
aIndicating a text line break.
Table 4: Anchors in a sample text
item can be divided into a numbering item and the
remaining content text in the line, as illustrated in
Table 4. The Chinese counterpart of this text car-
ries similar lines, if no missing line in any page of
the pair.
Unfortunately, missing lines are found in some
BLIS pages, as exemplified in Figure 2. There is
no guarantee that matching text lines one by one
in sequence would carry out the expected align-
ment within a page pair. However, the numbering
items at the beginning of each line can be utilized
as anchors to facilitate the alignment. The strat-
egy along this line is given as follows.
1. Anchor identification: numbering items at
the beginning of each line are recognized
as anchors, with the beginning and the end
of the whole content text as two special an-
chors, resulting in a list of anchors for each
page;
2. Anchor alignment: match the two lists of an-
chors sequentially. If a pair of anchors does
not match, give up the smaller one (in terms
of the BLIS numbering hierarchy) and move
on to the next possible pair, working in ex-
actly the same procedure as matching iden-
tical anchor pairs between two sorted lists of
anchors.
3. Text line alignment: a pair of matched an-
chors give a pair of matched lines; an un-
matched anchor indicates a missing line in
the other language.
4 XML Markup for the Aligned Corpus
XML is applied to encode the text alignment
outcomes output from the above alignment pro-
cedure. It has been a standard for data repre-
sentation and exchange on the Web, and also
accepted by the NLP community as a standard
for linguistic data annotation and representation
(Ide et al, 2000; Mengel and Lezius, 2000;
Kim et al, 2001). There are a series of yearly
NLPXML workshops for it since 2001. It pro-
vides a platform-independent flexible and sophis-
ticated plain text format for data encoding and
manipulation. It is particularly suitable for hier-
archical linguistic data such as the hierarchically-
aligned bilingual corpus that we have produced.
What?s more, converting data to XML format not
only significantly reduces the complexity of data
exchange among different computer systems but
also enhances data transmission reliability and
eases Web browsing.
There have been many corpora that are anno-
tated with XML, e.g., HCRC Map Task Corpus
(Anderson et al, 1991), American National Cor-
pus (Ide and Macleod, 2001), the La Republica
corpus (Baroni et al, 2004). Below we present
the XML schema for our subparagraph-aligned
BLIS bitexts, with sample annotation, and nec-
essary Web browsing.
4.1 XML Schema
The current version of the XML schema for the
bilingual BLIS corpus, as given in Figure 4, fo-
cuses on encoding all text structures in the BLIS
hierarchy, including all elements in each BLIS
Web page. It is to be extended to cover finer-
grained structures such as clauses, phrases and
words, as we proceed to align the BLIS bitexts
at these linguistic levels. For simplicity, we al-
low para to subsume all types of text line, be
they a section, subsection, paragraph or subpara-
graph. The annotation of a sample bitext with this
schema is illustrated in Figure 5. Annotation of
this kind is carried out by a Java program auto-
matically for the entire bitext corpus.
4.2 Corpus Browsing
A number of display modes are designed for
browsing the subparagraph-aligned bitexts, in-
cluding bilingual modes and monolingual modes.
75
Figure 4: XML schema for aligned BLIS bitexts
In a bilingual mode, text line pairs are displayed
in sequence. Switch of language order or from
one mode to another is allowed any time during
browsing. The bilingual display mode is illus-
trated in Figure 6.
5 Conclusion
We have presented in the above sections our re-
cent work on harvesting and aligning the bitexts
of the laws of Hong Kong, including basic tech-
niques for downloading English-Chinese bilin-
gual legal texts from BLIS official site, sound
strategies for aligning the bitexts by utilizing the
numbering system in the legal texts, and neces-
sary XML annotation for the alignment results.
The value of the outcomes, i.e., the subparagraph-
aligned bilingual corpus, can be evaluated in
terms of the following aspects.
Corpus size The entire corpus is of 10.4M En-
glish words and 18.3M Chinese characters,
several times larger than the well-known
Penn Treebank Corpus in size.
Figure 5: Sample bitext in XML encoding
Translation quality All texts of the corpus are
prepared by the Law Drafting Division of
the Department of Justice, Hong Kong Gov-
ernment. Legal texts are known to be more
precise and less ambiguous than most other
types of text.
Specificity and comprehensiveness The corpus
covers specifically the domain of Hong Kong
legislation. It is the most authoritative and
complete text collection of the laws of Hong
Kong.
Alignment granularity The entire corpus is
aligned precisely to the subparagraph level.
Most subparagraphs in the legal texts are
phrases, fragments of a clause, or clauses; as
shown in Table 4.
76
Figure 6: Illustration of browsing modes
A bilingual corpus of this size and quality cov-
ering a specific domain so comprehensively is
particularly useful not only in empirical MT re-
search but also in computational studies of bilin-
gual terminology and legislation. Our future work
will focus on word alignment for inferring bilin-
gual lexical resources and on automatic recogni-
tion of legal terminology.
Also, our experience in constructing this bilin-
gual corpus has laid a foundation for us to con-
tinue to harvest more bilingual text materials from
the Web, e.g., from Hong Kong government?s
Web sites. We find that almost all Hong Kong
government web sites, which are in large num-
bers, maintain their Web pages consistently par-
allel in English and Chinese. We are not sure if
such bitexts in such pages are larger than that in
the BLIS site in volume. We do know they cover
a large number of distinct domains. This is partic-
ularly useful for MT. If we can harvest and align
the bitexts from such Web pages efficiently via
utilizing their intrinsic characteristics of URL cor-
respondence and text structure, it would not be a
dream any more to put an end to the time of hav-
ing too few existing translation materials for em-
pirical MT studies, at least, for the language pair
of Chinese and English.
Acknowledgements
The work described in this paper was supported
by the Research Grants Council of HKSAR,
China, through the CERG grants 9040861 and
9040482. We wish to thank our team members
for their help.
References
Anne H. Anderson, Miles Bader, Ellen G. Bard, Eliz-
abeth Boyle, Gwyneth Doherty, Simon Garrod,
Stephen Isard, Jacqueline Kowtko, Jan McAllis-
ter, Jim Miller, Catherine Sotillo, Henry Thompson,
and Regina Weinert. 1991. The HCRC map task
corpus. Language and Speech, 34(4):351?366.
Marco Baroni, Silvia Bernardini, Federica Comastri,
Lorenzo Piccioni, Alessandra Volpi, Guy Aston,
and Marco Mazzoleni. 2004. Introducing the La
Repubblica corpus: A large, annotated, TEI(XML)-
compliant corpus of newspaper Italian. In LREC
2004, pp. 1771-1774.
Simon P. Botley, Anthony M. McEnery, and Andrew
Wilson (eds.). 2000. Multilingual Corpora in
Teaching and Research. Amsterdam: Rodopi.
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The
mathematics of statistical machine translation: Pa-
rameter estimation. Computational Linguistics,
19(2):263?311.
Michael Carl and Andy Way (eds.). 2003. Recent
Advances in Example-based Machine Translation.
Dordrecht: Kluwer.
Jiang Chen and Jian Y. Nie. 2000. Parallel Web text
mining for cross-language information retrieval. In
RIAO?2000, pp. 62?77. Paris.
Mark Davis and Ted Dunning. 1995. A TREC evalu-
ation of query translation methods for multi-lingual
text retrieval. In TREC-4, pp. 483?498. NIST.
Jarle Ebeling. 1998. Contrastive linguistics, transla-
tion, and parallel corpora. In Meta, 43(4):602?615.
William A. Gale and Kenneth W. Church. 1991. Iden-
tifying word correspondences in parallel texts. In
Fourth DARPA Workshop on Speech and Natural
Language, pp. 152?157. Asilomar, California.
William A. Gale and Kenneth W. Church. 1991b. A
Program for Aligning Sentences in Bilingual Cor-
pora. In ACL?91, pp. 177?184. Berkeley.
Nancy Ide, Patrice Bonhomme, and Laurent Romary.
2000. XCES: an XML-based encoding standard
for linguistic corpora. In LREC2000, pp. 825?830.
Athens, Greece.
77
Nancy Ide and Catherine Macleod. 2001. The Amer-
ican National Corpus: A Standardized Resource of
American English. Proceedings of Corpus Linguis-
tics 2001, Lancaster UK.
Adam Kilgarriff and Gregory Grefenstette. 2003. In-
troduction to the Special Issue on the Web as Cor-
pus. Computational Linguistics, 29(3):333?347.
Jin-Dong Kim, Tomoko Ohta, Yuka Tateisi, Hideki
Mima and Jun?ichi Tsujii. 2001. XML-based lin-
guistic annotation of corpus. In NLPXML-1, pp. 47?
54. Tokyo.
I. Dan Melamed. 1997. Automatic discovery of
non-compositional compounds in parallel data. In
EMNLP?97, pp. 97?108. Brown University, Au-
gust.
I. Dan Melamed. 2000. Models of translational equiv-
alence among words. Computational Linguistics,
26(2):221?249.
Andreas Mengel and Wolfgang Lezius. 2000. An
XML-based representation format for syntactically
annotated corpora. In LREC2000, Volume 1,
pp. 121?126. Athens, Greece.
Makoto Nagao. 1984. A framework of a mechanical
translation between Japanese and English by anal-
ogy principle. Artificial and Human Intelligence,
pp. 173?180. Amsterdam: North-Holland.
Jian Y. Nie and Jian Cai. 2001. Filtering noisy paral-
lel corpora of Web pages. In IEEE Symposium on
Natural Language Processing and Knowledge En-
gineering, pp. 453?458. Tucson, AZ.
Jian Y. Nie and Jiang Chen. 2002. Exploiting the
Web as Parallel Corpora for Cross-Language Infor-
mation Retrieval. Web Intelligence, pp. 218?239.
Philip Resnik, Mari B. Olse, and Mona Diab. 1999.
The Bible as a parallel corpus: Annotating the
?Book of 2000 Tongues?. Computers and the Hu-
manities, 33(1-2):129?153.
Philip Resnik. 1999b. Mining the Web for Bilingual
Text. In ACL?99, pp. 527?534. Maryland.
Philip Resnik and Noah A. Smith. 2003. The Web
as a Parallel Corpus. Computational Linguistics,
29(3):349?380.
Raphael Salkie. 1995. INTERSECT: a parallel cor-
pus project at Brighton University. Computers and
Texts 9 (May 1995), pp. 4?5.
Jean Veronis. 2000. Parallel Text Processing. Dor-
drecht: Kluwer.
Andy Way and Nano Gough. 2003. wEBMT:
Developing and validating an example-based ma-
chine translation system using the World Wide Web.
Computational Linguistics, 29(3):421?457.
Dekai Wu. 1994. Aligning a parallel English-Chinese
corpus statistically with lexical criteria. In ACL?94,
pp. 80?87. Las Cruces, New Mexico, U.S.A.
78
Proceedings of the Workshop on Innovative Hybrid Approaches to the Processing of Textual Data (Hybrid2012), EACL 2012, pages 61?68,
Avignon, France, April 23 2012. c?2012 Association for Computational Linguistics
Collaborative Annotation of Dialogue Acts:  
Application of a New ISO Standard to the Switchboard Corpus 
Alex C. Fang1, Harry Bunt2, Jing Cao3, and Xiaoyue Liu4 
1,3,4The Dialogue Systems Group, Department of Chinese, Translation and Linguistics 
City University of Hong Kong, Hong Kong, SAR  
2Tilburg Centre for Cognition and Communication  
Tilburg University, The Netherlands 
3School of Foreign Languages, Zhongnan University of Economics and Law, China 
E-mail: {1acfang, 3cjing3, 4xyliu0}@cityu.edu.hk, 2harry.bunt@uvt.nl  
Abstract 
This article reports some initial results from the collaborative work on converting SWBD-DAMSL annotation scheme used in the 
Switchboard Dialogue Act Corpus to ISO DA annotation framework, as part of our on-going research on the interoperability of 
standardized linguistic annotations. A qualitative assessment of the conversion between the two annotation schemes was performed to 
verify the applicability of the new ISO standard using authentic transcribed speech. The results show that in addition to a major part of 
the SWBD-DAMSL tag set that can be converted to the ISO DA scheme automatically, some problematic SWBD-DAMSL tags still 
need to be handled manually. We shall report the evaluation of such an application based on the preliminary results from automatic 
mapping via machine learning techniques. The paper will also describe a user-friendly graphical interface that was designed for manual 
manipulation. The paper concludes with discussions and suggestions for future work. 
 
 
 
1. Introduction 
This article describes the collaborative work on applying 
the newly proposed ISO standard for dialogue act 
annotation to the Switchboard Dialogue Act (SWBD-DA) 
Corpus, as part of our on-going effort to promote 
interoperability of standardized linguistic annotations 
with the ultimate goal of developing shared and open 
language resources.  
Dialogue acts (DA) play a key role in the 
interpretation of the communicative behaviour of 
dialogue participants and offer valuable insight into the 
design of human-machine dialogue systems (Bunt et al, 
2010). More recently, the emerging ISO DIS 24617-2 
(2010) standard for dialogue act annotation defines 
dialogue acts as the ?communicative activity of a 
participant in dialogue interpreted as having a certain 
communicative function and semantic content, and 
possibly also having certain functional dependence 
relations, rhetorical relations and feedback dependence 
relations? (p. 3). The semantic content specifies the 
objects, relations, events, etc. that the dialogue act is 
about; the communicative function can be viewed as a 
specification of the way an addressee uses the semantic 
content to update his or her information state when he or 
she understands the corresponding stretch of dialogue. 
Continuing efforts have been made to identify and 
classify the dialogue acts expressed in dialogue utterances 
taking into account the empirically proven 
multifunctionality of utterances, i.e., the fact that 
utterances often express more than one dialogue act (see 
Bunt, 2009 and 2011). In other words, an utterance in 
dialogue typically serves several functions. See Example 
(1) taken from the SWBD-DA Corpus 
(sw_0097_3798.utt). 
 
(1) A: Well, Michael, what do you think about, uh, 
funding for AIDS research? Do you? 
B:   Well, uh, uh, that?s something I?ve thought a lot 
about.  
 
With the first utterance, Speaker A performs two 
dialogue acts: he (a) assigns the next turn to the 
participant Michael, and (b) formulates an open question. 
Speaker B, in his response, (a) accepts the turn, (b) stalls 
for time, and (c) answers the question by making a 
statement.  
Our concern in this paper is to explore the 
applicability of the new ISO Standard to the existing 
Switchboard corpus with joint efforts of automatic and 
manual mapping. In the rest of the paper, we shall first 
describe the Switchboard Dialogue Act (SWBD-DA) 
Corpus and its annotation scheme (i.e. SWBD-DAMSL). 
We shall then describe the new ISO Standard and explain 
our mapping of SWBD-DAMSL to the ISO DIS 24617-2 
DA tag set. In addition, machine learning techniques are 
employed for automatic DA classification on the basis of 
lexical features to evaluate the application of the new ISO 
DA scheme using authentic transcribed speech. We shall 
then introduce the user interface designed for manual 
mapping and explain the annotation guidelines. Finally, 
the paper will conclude with discussions and suggestions 
for future work.  
2. Corpus Resource 
This study uses the Switchboard Dialog Act (SWBD-DA) 
Corpus as the corpus resource, which is available online 
from the Linguistic Data Consortium 1 . The corpus 
                                                          
1 http://www.ldc.upenn.edu/ 
61
contains 1,155 5-minute conversations2, orthographically 
transcribed in about 1.5 million word tokens. It should be 
noted that the minimal unit of utterances for DA 
annotation in the SWBD Corpus is the so called ?slash 
unit? (Meteer and Taylor, 1995), defined as ?maximally a 
sentence but can be smaller unit? (p. 16), and ?slash-units 
below the sentence level correspond to those parts of the 
narrative which are not sentential but which the annotator 
interprets as complete? (p. 16). See Table 1 for the basic 
statistics of the SWBD-DA Corpus. 
 
Table 1: Basic Statistics of the SWBD-DA Corpus 
 
Altogether, the corpus comprises 223,606 slash-units and 
each is annotated for its communicative function 
according to a set of dialogue acts specified in the 
SWBD-DAMSL scheme (Jurafsky et al, 1997) and 
assigned a DA tag. See Example (2) taken from 
sw_0002_4330.utt, where qy is the DA tag for yes/no 
questions.  
 
(2) qy   A.1 utt1: {D Well, } {F uh, } does the company 
you work for test for drugs? /   
 
A total of 303 different DA tags are identified throughout 
the corpus, which is different from the total number of 
220 tags mentioned in Jurafsky et al (1997: 3). To ensure 
enough instances for the different DA tags, we also 
conflated the DA tags together with their secondary 
carat-dimensions, and yet we did not use the seven special 
groupings by Jurafsky et al (1997) as we kept them as 
separate DA types (see Section 4 for further explanations). 
In the end, the 303 tags were clustered into 60 different 
individual communicative functions. See Table 2 for the 
basic statistics of the 60 DA clusters.  
According to Table 2, we observe that the 60 DA 
clusters range from 780,570 word tokens for the 
top-ranking statement-non-opinion to only 4 word 
                                                          
2 Past studies (e.g. Stolcke et al, 2000; Jurafsky et al, 
1997; Jurafsky et al, 1998a; Jurafsky et al, 1998b) have 
been focused on only 1115 conversations in the 
SWBD-DA Corpus as the training set. As there is no clear 
description which 40 conversations have been used as the 
testing set or for future use, we use all the 1155 
conversations.   
tokens for you?re-welcome. In Table 2, the Token % 
column lists the relative importance of DA types 
measured as the proportion of the word tokens in the 
SWBD-DA corpus as whole. It can be observed that, as 
yet another example to illustrate the uneven use of DA 
types, statement-opinion accounts for 21.04% of the 
total number of word tokens in the corpus.  
 
60 DAs Tokens Token % Cum % 
Statement-non-opinion 780,570 51.79 51.79 
Statement-opinion 317,021 21.04 72.83 
Segment-(multi-utterance) 135,632 9.00 81.83 
Acknowledge-(backchannel) 40,696 2.70 84.53 
Abandoned 35,214 2.34 86.87 
Yes-no-question 34,817 2.31 89.18 
Accept 20,670 1.37 90.55 
Statement-expanding-y/n-answer 14,479 0.96 91.51 
Wh-question 14,207 0.94 92.45 
Appreciation 13,957 0.93 93.38 
Declarative-yes-no-question 10,062 0.67 94.05 
Conventional-closing 9,017 0.60 94.65 
Quoted-material 7,591 0.50 95.15 
Summarize/reformulate 6,750 0.45 95.60 
Action-directive 5,860 0.39 95.99 
Rhetorical-questions 5,759 0.38 96.37 
Hedge 5,636 0.37 96.74 
Open-question 4,884 0.32 97.06 
Affirmative-non-yes-answers 4,199 0.28 97.34 
Uninterpretable 4,138 0.27 97.61 
Yes-answers 3,512 0.23 97.84 
Completion 2,906 0.19 98.03 
Hold-before-answer/agreement 2,860 0.19 98.22 
Or-question 2,589 0.17 98.39 
Backchannel-in-question-form 2,384 0.16 98.55 
Acknowledge-answer 2,038 0.14 98.69 
Negative-non-no-answers 1,828 0.12 98.81 
Other-answers 1,727 0.11 98.92 
No-answers 1,632 0.11 99.03 
Or-clause 1,623 0.11 99.14 
Other 1,578 0.10 99.24 
Dispreferred-answers 1,531 0.10 99.34 
Repeat-phrase 1,410 0.09 99.43 
Reject 891 0.06 99.49 
Transcription-errors:-slash-units 873 0.06 99.55 
Declarative-wh-question 855 0.06 99.61 
Signal-non-understanding 770 0.05 99.66 
Self-talk 605 0.04 99.70 
Offer 522 0.03 99.73 
Conventional-opening 521 0.03 99.76 
3rd-party-talk 458 0.03 99.79 
Accept-part 399 0.03 99.82 
Downplayer 341 0.02 99.84 
Apology 316 0.02 99.86 
Exclamation 274 0.02 99.88 
Commit 267 0.02 99.90 
Thanking 213 0.01 99.91 
Double-quote 183 0.01 99.92 
Reject-part 164 0.01 99.93 
Tag-question 143 0.01 99.94 
Maybe 140 0.01 99.95 
Sympathy 80 0.01 99.96 
Explicit-performative 78 0.01 99.97 
Open-option 76 0.01 99.98 
Other-forward-function 42 0.00 99.98 
Correct-misspeaking 37 0.00 99.98 
No-plus-expansion 26 0.00 99.98 
Yes-plus-expansion 22 0.00 99.98 
You?re-welcome 4 0.00 99.98 
Double-labels 2 0.00 100.00 
 Total 1,507,079 100.00 100.00 
Table 2: Basic Statistics of the 60 DAs 
 
If the cumulative proportion (Cum%) is considered, we 
Folder 
# of 
Conversations 
# of 
Slash-units 
# of 
Tokens 
sw00  99 14,277 103,045 
sw01 100 17,430 119,864 
sw02 100 20,032 132,889 
sw03 100 18,514 127,050 
sw04 100 19,592 132,553 
sw05 100 20,056 131,783 
sw06 100 19,696 135,588 
sw07 100 20,345 136,630 
sw08 100 19,970 134,802 
sw09 100 20,159 133,676 
sw10 100 22,230 143,205 
sw11  16   3,213   20,493 
sw12  11   2,773   18,164 
sw13  29   5,319   37,337 
Total      1,155   223,606 1,507,079 
62
see that the top 10 DA types alone account for 93.38% of 
the whole corpus, suggesting again the uneven occurrence 
of DA types in the corpus and hence the disproportional 
use of communication functions in conversational 
discourse.  
It is particularly worth mentioning that 
segment-(multi-utterance) is not really a DA type 
indicating communicative function and yet it is the third 
most frequent DA tag in SWBD-DAMSL.  As a matter of 
fact, the SWBD-DAMSL annotation scheme contains 
quite a number of such non-communicative DA tags, such 
as abandoned, and quoted-material. 
3. ISO DIS 24617-2 (2010) 
A basic premise of the emerging ISO standard for 
dialogue act annotation, i.e., ISO DIS 24617-2 (2010), is 
that utterances in dialogue are often multifunctional; 
hence the standard supports so-called ?multidimensional 
tagging?, i.e., the tagging of utterances with multiple DA 
tags. It does so in two ways: First of all, it defines nine 
dimensions to which a dialogue act can belong: 
? Task 
? Auto-Feedback 
? Allo-Feedback 
? Turn Management 
? Time Management 
? Discourse Structuring 
? Social Obligations Management 
? Own Communication Management 
? Partner Communication Management 
Secondly, it takes a so-called ?functional segment? as 
the unit in dialogue to be tagged with DA information, 
defined as a ?minimal stretch of communicative behavior 
that has one or more communicative functions? (Bunt et 
al., 2010). A functional segment is allowed to be 
discontinuous, and to overlap with or be included in 
another functional segment. A functional segment may be 
tagged with at most one DA tag for each dimension. 
Another important feature is that an ISO DA tag 
consists not only of a communicative function encoding, 
but also of a dimension indication, with optional attributes 
for representing certainty, conditionality, sentiment, and 
links to other dialogue units expressing semantic, 
rhetorical and feedback relations. 
Thus, two broad differences can be observed between 
SWBD-DAMSL and ISO. The first concerns the 
treatment of the basic unit of analysis. While in 
SWBD-DAMSL this is the slash-unit, ISO DIS 24617-2 
(2010) employs the functional segment, which serves well 
to emphasise the multifunctionality of dialogue utterances. 
An important difference here is that the ISO scheme 
identifies multiple DAs per segment and assigns multiple 
tags via the stand-off annotation mechanism. 
The second difference is that each slash-unit (or 
utterance) in the SWBD-DA Corpus is annotated with one 
SWBD-DAMSL label, while each DA tag in the ISO 
scheme is additionally associated with a dimension tag 
and, when appropriate, with function qualifiers and 
relations to other dialogue units. See the following 
example taken from the Schiphol Corpus. 
 
(3) A: I?m most grateful for your help 
 
While the utterance in Example (3) would be annotated 
with only a functional tag in SWBD-DAMSL, it is 
annotated to contain the communicative function ?inform? 
and in addition the dimension of social obligation 
management:  
 
    communicativeFunction = ?inform? 
  dimension = ?socialObligationManagement? 
4. Mapping SWBD-DAMSL to ISO  
4.1 Data Pre-processing 
For the benefit of the current study and potential 
follow-up work, the banners between folders were 
removed and each slash-unit was extracted to create a set 
of files. See Example (4), the tenth slash-unit taken from 
the file sw_0052_4378.utt in the folder sw00.    
 
(4) sd     B.7 utt1: {C And,} {F uh,} <inhaling> we?ve  
                             done <sigh> lots to it. /  
 
The following set of files is created: 
 
sw00-0052-0010-B007-01.txt  the original utterance 
sw00-0052-0010-B007-01-S.da  SWBD-DAMSL tag 
 
In the .txt file, there is the original utterance:  
 
     {C And,} {F uh,} <inhaling> we?ve                             
done <sigh> lots to it. /  
 
While the *-S.da file only contains the DA label: sd^t. 
Still another one or more files (depending on the number 
of dimensions) will be added to this set after converting 
the SWBD-DAMSL to the ISO tag sets.  Take Example (4) 
for instance. Two more files will be created, namely,   
 
sw00-0052-0010-B007-01-ISO-0.da  ISO DA tag 
sw00-0052-0010-B007-01-ISO-1.da  ISO DA tag 
 
The *-ISO-0.da file will contain in this case:  
 
   communicativeFunction = ?inform? 
   dimension = ?task?3 
 
and the *-ISO-1.da file will contain4:  
 
   communicativeFunction = ?stalling? 
   dimension = ?timeManagement? 
                                                          
3 The same function Inform have been observed to occur 
in different dimensions. See ISO DIS 24617-2 (2010) for 
detailed description.  
4 See Section 4.2 for more explanation of the multi-layer 
annotations in ISO standard.  
63
4.2 Assessment of the Conversion 
When mapping SWBD-DAMSL tags to functional ISO 
tags, it is achieved in terms of semantic contents rather 
than the surface labels. To be more exact, four situations 
were identified in the matching process.  
The first is what is named as ?exact matches?. It is 
worth mentioning that since we are not matching the 
labels in the two annotation schemes, even for the exact 
matches, the naming in SWBD-DAMSL is not always the 
same as that in the ISO scheme, but they have the same or 
very similar meaning. Table 3 lists the exact matches. 
 
SWBD-DAMSL ISO 
Open-question Question  
Dispreferred answers Disconfirm 
Offer Offer 
Commit Promise 
Open-option Suggest 
Hold before answer/ agreement Stalling 
Completion Completion 
Correct-misspeaking CorrectMisspeaking 
Apology Apology 
Downplayer AcceptApology 
Thanking Thanking 
You?re-welcome AcceptThanking 
Signal-non-understanding AutoNegative 
Conventional-closing InitialGoodbye 
Table 3: Exact Matches 
It can also be noted that in the previous study on the 42 
DA types in SWBD-DAMSL, open-option (oo), 
offer (co), commit (cc) are treated as one DA type. In 
the current study, they are treated as individual DA types, 
which makes more sense especially when mapping to the 
ISO DA tag sets since each of them corresponds to a 
different ISO tag, suggest, offer, and promise 
respectively.   The same is also true for the 
you?re-welcome (fw) and correct-misspeaking 
(bc), which are combined together in SWBD-DAMSL 
and correspond to different ISO DA label.  
 
SWBD-DAMSL ISO 
Wh-question; Declarative wh-question SetQuestion 
Or-question; Or-clause ChoiceQuestion 
Yes-no-question;  
Backchannel in question form PropositionalQuestion 
Tag-question;  
Declarative Yes-no-question CheckQuestion 
Statement-non-opinion;  
Statement-opinion;  
Rhetorical-question;  
Statement expanding y/n answer; Hedge 
Inform 
Maybe; Yes-answer;  
Affirmative non-yes answers;  
Yes plus expansion; No-answer;  
Negative non-no answers;  
No plus expansion 
Answer 
Acknowledge (backchannel); 
Acknowledge answer; Appreciation; 
Sympathy; Summarize/reformulate;  
Repeat-phrase 
AutoPositive 
Accept-part; Reject-part Correction 
Table 4: Many-to-one Matches 
The second situation is where more than one 
SWBD-DAMSL tags can be matched to the one ISO DA 
type, as defined as many-to-one matches. Table 4 shows 
the many-to-one matches. Such matches occur because 
semantically identical functions are sometimes given 
different names in SWBD-DAMSL in order to distinguish 
differences in lexical or syntactic form. For example, an 
affirmative non-yes answer is defined as an 
affirmative answer that does not contain the word yes or 
one of its variants (like yeah and yep). 
 The most complex issue is with the one-to-many 
matches, where a DA function in SWBD-DAMSL is too 
general and corresponds to a set of different DAs in the 
ISO scheme. Consider the DA type of accept in 
SWBD-DAMSL. It is a broad function applicable to a 
range of different situations. For instance, accept 
annotated as aa in Example (5) taken from 
sw_0005_4646.utt corresponds to Agreement in ISO 
DIS 24617-2 (2010). 
 
(5) sd    A.25 utt1: {C Or } people send you there as a  
                                  last resort. / 
     aa     B.26 utt1: Right,  / 
 
However, accept (aa) in Example (6) taken from 
sw_0098_3830.utt actually corresponds to 
acceptOffer in ISO/DIS 24617-2 (2010).  
 
(6) co    B.26 utt1: I can tell you my last job or --/ 
      aa    A.27 utt1: Okay,  / 
 
As a matter of fact, accept in SWBD-DAMSL may 
correspond to several different DAs in the ISO tag set 
such as: 
? Agreement  
? AcceptRequest (addressRequest) 
? AccpetSuggestion (addressSuggestion) 
? AcceptOffer (addressOffer) 
? etc. 
 
Other cases include reject, action-directive and 
other answers.  
Finally, the remaining tags are unique to 
SWBD-DAMSL, including  
 
? quoted material 
? uninterpretable 
? abandoned 
? self-talk 
? 3rd-party-talk 
? double labels  
? explicit-performative  
? exclamation 
? other-forward-function 
 
It is not difficult to notice that 6 out of the 9 DA types 
mainly concern the marking up of other phenomena than 
dialogue acts. The last three unique DA types only 
account for a marginal portion of the whole set, about 
0.03% all together (See Table 2).  
64
In addition, multi-layer annotations of ISO can be 
added to the original markup of SWBD (Meteer and 
Taylor 1995), especially in cases such as Stalling and 
Self-Correction. See Example (7) taken from 
sw_0052_4378.utt. 
 
(7) sd   A.12  utt2 : [ I, + {F uh, } two months ago I ]  
                               went to Massachusetts -- /  
 
According to Meteer and Taylor (1995), the {F ?} is 
used to mark up ?filler? in utterances, which corresponds 
to Stalling in ISO DIS 24617-2 (2010). In addition, the 
markup of [ ? + ?] indicates the repairs (Meteer and 
Taylor, 1995), which suits well the definition of 
Self-correction in the ISO standard. As a result, the 
utterance in Example (7) is thus annotated in three 
dimensions:  
communicativeFunction = ?inform? 
dimension = ?task? 
 
communicativeFunction = ?stalling? 
dimension = ?timeManagement? 
 
communicativeFunction = ?self-correction? 
dimension = ?ownCommManagement? 
4.3 Mapping Principles 
Given the four setting of the matching, there major 
principles were made:  
1) Cases in both ?exact matches? and ?many-to-one 
matches? can be automatically mapped to ISO tags by 
programming. 
2) Tags that are unique to SWBD-DAMSL would not 
be considered at the current stage due to the absence of 
ISO counterparts and their marginal proportion. 
3) Cases in ?one-to-many matches? are more complex 
and call for manual mapping, which will be further 
discussed in Section 6.  
4) Different DA dimensions will be also automatically 
added accordingly to each utterance in the format of 
stand-off annotation.  
5. Application Verification 
To evaluate the applicability of mapping SWBD-DAMSL 
tag set to the new ISO standard (ISO DIS 24617-2, 2010), 
machine learning techniques are employed, based on the 
preliminary results from the automatic mapping, to see 
how well the SWBD-ISO DA tags can be automatically 
identified and classified based on lexical features. The 
result is also compared with that obtained from the 
Top-15 SWBD-DAMSL tags. It will be particularly 
interesting to find out whether the emerging ISO DA 
annotation standard will produce better automatic 
prediction accuracy. In this paper, we evaluate the 
performance of automatic DA classification in the two DA 
annotation schemes by employing the unigrams as the 
feature set.  
Two classification tasks were then identified 
according to the two DA annotation schemes. Task 1 is to 
automatically classify the DA types in the 
SWBD-DAMSL. Based on the observations mentioned 
above, it was decided to use the top 15 DA types to 
investigate the distribution of word types in order to 
ascertain the lexical characteristics of DAs. Furthermore, 
since segment-(multi-utterance), abandoned, and 
quoted-material do not relate to dialogue acts per se, 
these three were replaced with rhetorical-questions, 
open-question and 
affirmative-non-yes-answers. We thus derive 
Table 6 below, showing that the revised list of top 15 DA 
types account for 85.13% of the SWBD corpus. The DA 
types are arranged according to Token% in descending 
order.  
 
Top-15 SWBD-DAMSL DAs Tokens Token % Cum % 
Statement-non-opinion 780,570 51.79 51.79 
Statement-opinion 317,021 21.04 72.83 
Acknowledge-(backchannel) 40,696 2.70 75.53 
Yes-no-question 34,817 2.31 77.84 
Accept 20,670 1.37 79.21 
Statement-expanding-y/n-answer 14,479 0.96 80.17 
Wh-question 14,207 0.94 81.11 
Appreciation 13,957 0.93 82.04 
Declarative-yes-no-question 10,062 0.67 82.71 
Conventional-closing 9,017 0.60 83.31 
Summarize/reformulate 6,750 0.45 83.76 
Action-directive 5,860 0.39 84.15 
Rhetorical-questions 5,759 0.38 84.53 
Open-question 4,884 0.32 84.85 
Affirmative-non-yes-answers 4,199 0.28 85.13 
Total 1,282,948 85.13  
Table 6: Top-15 SWBD-DAMSL DA types 
Next, accordingly, task 2 is to classify the top 15 ISO 
DAs based on the results from the automatic mapping. It 
should be pointed out that only one layer of annotation in 
the ISO DA tags is considered in order to make the result 
comparable to that from SWBD-DAMSL, and the 
dimension of task is the priority when it comes to 
multi-layer annotations.  
 
Top-15 SWBD-ISO DAs Tokens Token % Cum % 
Inform 1,117,829   74.17 74.17 
AutoPositive 64,851 4.30 78.47 
PropositionalQuestion 37,201 2.47 80.94 
SetQuestion 15,062 1.00 81.94 
Answer 11,171 0.74 82.68 
CheckQuestion 10,062 0.67 83.35 
InitialGoodbye 9,017 0.60 83.95 
Question 4,884 0.32 84.27 
ChoiceQuestion 4,212 0.28 84.55 
Completion 2,906 0.19 84.75 
Stalling 2,860 0.19 84.94 
Disconfirm 1,531 0.10 85.04 
AutoNegative 770 0.05 85.09 
Offer 522 0.03 85.12 
AcceptApology 341 0.02 85.15 
Total 1,283,219   85.15  
Table 7: Top-15 SWBD-ISO DA types 
The Na?ve Bayes Multinomial classifier was 
employed, which is available from Waikato Environment 
for Knowledge Analysis, known as Weka (Hall et al, 
2009). 10-fold cross validation was performed and the 
65
results evaluated in terms of precision, recall and F-score 
(F1). 
Table 8 presents the results for classification task 1. 
The SWBD-DAMSL DAs are arranged according to 
F-score in descending order. 
 
Top 15 SWBD-DAMSL DAs Precision Recall F1 
Acknowledge-(backchannel) 0.821 0.968 0.888 
Statement-non-opinion 0.732 0.862 0.792 
Appreciation 0.859 0.541 0.664 
Statement-opinion 0.538 0.584 0.560 
Conventional-closing 0.980 0.384 0.552 
Accept 0.717 0.246 0.367 
Yes-no-question 0.644 0.204 0.309 
Wh-question 0.760 0.189 0.303 
Open-question 0.932 0.084 0.154 
Action-directive 1.000 0.007 0.013 
Statement-expanding-y/n-answer 0.017 0 0.001 
Declarative-yes-no-question 0 0 0 
Summarize/reformulate 0 0 0 
Rhetorical-questions 0 0 0 
Affirmative-non-yes-answers 0 0 0 
Weighted Average 0.704 0.725 0.692 
Table 8: Results from Task 1 
As can be noted, the weighted average F-score is 69.2%. 
To be more specific, acknowledge-(backchannel) 
achieves the best F-score of 0.888, followed by 
statement-non-opinion with an F-score of 0.792. 
Surprisingly, the action-directive has the highest 
precision of 100%, but has the second lowest recall of 
over 0.7%. It can also be noted that the last four types of 
DAs cannot be classified with the F-score of 0%.  
 
Top 15 SWBD-ISO DAs Precision Recall F1 
Inform 0.879 0.987 0.930 
Answer 0.782 0.767 0.775 
AutoPositive 0.711 0.507 0.592 
InitialGoodbye 0.972 0.351 0.516 
PropositionalQuestion 0.521 0.143 0.224 
SetQuestion 0.668 0.120 0.203 
Question 0.854 0.051 0.097 
AutoNegative 0.889 0.026 0.051 
ChoiceQuestion 0.286 0.008 0.015 
Stalling 0.400 0.003 0.007 
CheckQuestion 0.042 0.001 0.001 
AcceptApology 0 0 0 
Completion 0 0 0 
Disconfirm 0 0 0 
Offer 0 0 0 
Weighted Average 0.832 0.865 0.831 
Table 9: Results from Task 2 
Table 9 presents the results for classification task 2. 
The DAs are arranged according to F-score in descending 
order. As can be noted, the weighted average F-score is 
83.1%, over 10% higher than task 1. To be more specific, 
Inform achieves the best F-score of 0.93, followed by 
Answer with an F-score of 0.775. The DA 
InitialGoodbye has the highest precision, of about 
97%, whereas Inform has the highest recall of over 98%. 
Similar to the results obtained in Task 1, the last four types 
of DAs in Task 2 also cannot be classified with the 
F-score of 0%. 
Meanwhile, as mentioned earlier, when the data size 
for each DA type is taken into consideration, Task 2 may 
be more challenging than Task 1 in that 6 out of the 15 
SWBD-ISO DA types has a total number of word tokens 
fewer than 4,000 whereas all the 15 SWBD-DAMSL DA 
types has a total number of over 4,000. Therefore, the 
much higher average F-score suggests that the application 
of ISO standard DA scheme could lead to better 
classification performance, suggesting that the ISO DA 
standard represents a better option for automatic DA 
classification. 
To sum up, with a comparable version of the 
SWBD-DA Corpus, results from the automatic DA 
classification tasks show that the ISO DA annotation 
scheme produces better automatic prediction accuracy, 
which encourages the completion of the manual mapping. 
6. Manual Mapping 
6.1 Analysis of Problematic DA Types 
As mentioned earlier, there are mainly four problematic 
SWBD-DAMSL tags, namely, accept (aa), reject 
(ar), action-directive (ad) and other answers 
(no). They are problematic in that they carry a broad 
function applicable to a range of different situations 
according to the new ISO standard, as evidenced in the 
case of accept discussed in Section 4.2. Consequently, to 
map the problematic SWBD-DAMSL tags to the ISO tags 
calls for manual manipulation. 
A close look into those four types shows that the 
mapping could be further divided into two setting. Again, 
take accept (aa) for example. In the first setting, a 
sub-division of accept (aa) can also be automatically 
matched according to the previous utterance by the other 
speaker in the adjacent pair. See Example (8) taken from 
sw_0001_4325.utt.  
 
(8) sv     A.49 utt3: take a long time to find the right  
                                 place / 
      x      A.49 utt4: <laughter>. 
      aa     B.50 utt1: Yeah,  / 
 
Here accept (aa) corresponds to Agreement because of 
the DA type in A.49 utt3 but not the immediate previous 
DA as in A.49 utt4. With this principle, the particular 
sub-groups for automatic mapping were identified for 
accept (aa). See Table 10. 
 
SWBD-DAMSL 
ISO 
Previous DA Current DA 
Statement-non-opinion; 
Statement-opinion; Hedge 
Rhetorical-question;  
Statement expanding y/n answer,  
accept 
Agreement 
Offer AcceptOffer 
Open-option AcceptRequest 
Thanking AcceptThanking 
Apology AcceptApology 
Table 10: Sub-groups of accept for Auto Mapping 
The remaining cases, in the second setting, call for 
manual annotation. For instance, when the previous DA 
type is also a problematic one, annotators need to decide 
66
the corresponding ISO DA tag for the previous 
SWBD-DAMSL one before converting the accept (aa).  
See Example (9) taken from sw_0423_3325.utt.  
 
(9) ad    B.128 utt2: {C so } we'll just wait. / 
      aa    A.129 utt1: Okay,  / 
 
Here, action-directive (ad) is first decided as a 
suggestion, and therefore accept (aa) turns out to 
actually correspond to acceptSuggestion 
(addressSuggestion) in ISO/DIS 24617-2 (2010).  
6.2 Design of a User Interface 
Given the analysis of those four DA tags, a user-friendly 
interface was then designed to assist annotators to 
maximize the inter-annotator agreement.  See Figure 1.  
 
Figure 1: User Interface 
 
Figure 1 shows the screenshot when the targeted 
SWBD-DAMSL type is accept (aa). As can be noted 
above, the basic functional bars have been designed, 
including: 
? Input: the path of the input 
? Automatch: to filter out the sub-groups that can be 
automatically matched 
? DA Tag: the targeted problematic DAs, namely, 
? aa (accept) 
? ar (reject) 
? ad (action-directive) and 
? no (other answers) 
? Previous: to go back to the previous instance of the 
targeted DA type 
? Next: to move on to the next instance of the targeted 
DA type 
? Current: the extraction of the adjacent turns 
? Previous5T: the extraction of the previous five turns 
when necessary 
? PreviousAll: the extraction of all the previous turns 
when necessary 
? MatchInfo: Bars for mapping information with five 
options: 
? Four pre-defined ISO DA types 
? Other: a user-defined mapping with a 
two-fold function: for user defined ISO DA 
type and for extra pre-defined ISO DA types 
(since the pre-defined DA types differ for 
the four targeted SWBD-DAMSL types).  
? Output: the path of the output 
? Result: export the results to the chosen path 
 
With this computer-aided interface, three annotators are 
invited to carry out the manual mapping. They are all 
postgraduates with linguistic background. After a month 
of training on the understanding of the two annotation 
schemes (in process), they will work on the 
SWBD-DAMSL DA instances from 115 randomly chosen 
files, and map them into ISO DA tags independently. The 
kappa value will be calculated to measure the 
inter-annotator agreement.  
 
7. Conclusion 
 
In this paper, we reported our efforts in applying the 
ISO-standardized dialogue act annotations to the 
Switchboard Dialogue Act (SWBD-DA) Corpus. In 
particular, the SWBD-DAMSL tags employed in the 
SWBD-DA Corpus were analyzed and mapped onto the 
ISO DA tag set (ISO DIS 24617-2 2010) according to 
their communicative functions and semantic contents. 
Such a conversion is a collaborative process involving 
both automatic mapping and manual manipulation.  With 
the results from the automatic mapping, machine learning 
techniques were employed to evaluate the applicability of 
the new ISO standard for dialogue act annotation in 
practice. With the encouraging results from the evaluation, 
the manual mapping was carried out. A user-friendly 
interface was designed to assist annotators. The 
immediate future work would be finish the manual 
mapping and thus to  produce a comparable version of the 
SWBD-DA Corpus was produced so that the two 
annotation schemes (i.e. SWBD-DAMSL vs. SWBD-ISO) 
can be effectively compared on the basis of empirical data. 
Furthermore, with the newly built resource, i.e., 
SWBD-ISO, we plan to examine the effect of 
grammatical and syntactic cues on the performance of DA 
classification, with a specific view on whether dialogue 
acts exhibit differentiating preferences for grammatical 
and syntactic constructions that have been overlooked 
before.  
 
 
8. Acknowledgements 
Research described in this article was supported in part by 
grants received from City University of Hong Kong 
(Project Nos 7008002, 9610188, 7008062 and 6454005). 
It was also partially supported by the General Research 
Fund of the Research Grants Council of Hong Kong 
(Project No 142711). 
9. References 
 
Bunt, H. (2009). Multifunctionality and multidimensional 
dialogue semantics. In Proceedings of DiaHolmia 
Workshop on the Semantics and Pragmatics of 
67
Dialogue, Stockholm, 2009.  
Bunt, H. (2011). Multifunctionality in dialogue and its 
interpretation. Computer, Speech and Language, 25 (2), 
pp. 225--245.  
Bunt, H., Alexandersson, J., Carletta, J., Choe, J.-W., 
Fang, A.C., Hasida, K., Lee, K., Petukhova, V., 
Popescu-Belis, A., Romary, L., Soria, C. and Traum, D. 
(2010). Towards an ISO standard for dialogue act 
annotation. In Proceedings of the Seventh International 
Conference on Language Resources and Evaluation. 
Valletta, MALTA, 17-23 May 2010. 
Hall, M., Frank, E., Holmes, G., Pfahringer, B., 
Reutemann, P. and Witten, I. H. (2009). The WEKA 
data mining software: an update. SIGKDD 
Explorations, 11 (1), pp. 10--18. 
ISO DIS 24617-2. (2010). Language resource 
management ? Semantic annotation framework 
(SemAF), Part 2: Dialogue acts. ISO, Geneva, January 
2010. 
Jurafsky, D., Shriberg, E. and Biasca, D. (1997). 
Switchboard SWBD-DAMSL 
shallow-discourse-function annotation coders manual, 
Draft 13.  University of Colorado, Boulder Institute of 
Cognitive Science Technical Report 97-02. 
Jurafsky, D., Bates, R., Coccaro, N., Martin, R., Meteer, 
M., Ries, K., Shriberg, E., Stolcke, A., Taylor,  P. and 
Ess-Dykema, C. V. (1998a). Switchbaod Discourse 
Language Modeling Project and Report. Research Note 
30, Center for Language and Speech Processing, Johns 
Hopkins University, Baltimore, MD, January. 
Jurafsky, D., Shriberg, E., Fox B. and Curl, T. (1998b).  
Lexical, prosodic, and syntactic cues for dialog acts. 
ACL/COLING-98 Workshop on Discourse Relations 
and Discourse Markers.  
Meeter, M., Taylor, A. (1995). Dysfluency annotation 
stylebook for the Switchboard Corpus. Available at 
ftp://ftp.cis.upenn.edu/pub/treebank/swbd/doc/DFL-bo
ok.ps. 
Stolcke, A., Ries, K., Coccaro, N., Shriberg, E., Bates, R., 
Jurfsky, D., Taylor, P., Martin, R., Ess-Dykema, C.V. 
and Meteer, M.  (2000). Dialogue Act Modeling for 
Automatic Tagging and Recognition of Conversational 
Speech. Computational Linguistics, 26 (3), pp. 
339--373.  
68
