Proceedings of the NAACL HLT 2012 Student Research Workshop, pages 48?53,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Deep Unsupervised Feature Learning for Natural Language Processing
Stephan Gouws
MIH Media Lab, Stellenbosch University
Stellenbosch, South Africa
stephan@ml.sun.ac.za
Abstract
Statistical natural language processing (NLP) builds
models of language based on statistical features ex-
tracted from the input text. We investigate deep
learning methods for unsupervised feature learning
for NLP tasks. Recent results indicate that features
learned using deep learning methods are not a sil-
ver bullet and do not always lead to improved re-
sults. In this work we hypothesise that this is the
result of a disjoint training protocol which results
in mismatched word representations and classifiers.
We also hypothesise that modelling long-range de-
pendencies in the input and (separately) in the out-
put layers would further improve performance. We
suggest methods for overcoming these limitations,
which will form part of our final thesis work.
1 Introduction
Natural language processing (NLP) can be seen as build-
ing models h : X ? Y for mapping an input encoding
x ? X representing a natural language (NL) fragment, to
an output encoding y ? Y representing some construct or
formalism used in the particular NLP task of interest, e.g.
part-of-speech (POS) tags, begin-, inside-, outside (BIO)
tags for information extraction, semantic role labels, etc.
Since the 90s, the predominant approach has been sta-
tistical NLP, where one models the problem as learning a
predictive function h for mapping from h : X ? Y using
machine learning techniques. Machine learning consists
of a hypothesis function which learns this mapping based
on latent or explicit features extracted from the input data.
In this framework, h is usually trained in a supervised
setting from labelled training pairs (xi, yi) ? X ? Y .
Additionally, the discriminant function h typically oper-
ates on a transformed representation of the data to a com-
mon feature space encoded as a feature vector ?(x), and
then learns a mapping from feature space to the output
space, h : ?(x) ? y. In supervised learning, the idea
~x ? X the cat sits on the mat
?(~x) ?(x1) ?(x2) ?(x3) ?(x4) ?(x5) ?(x6)
~y ? Y B-NP I-NP B-VP O B-NP I-NP
NE Tags [the cat] NP [sits] VP [on] O [the mat] NP
Table 1: Example NLP syntactic chunking task for the sentence
?the cat sits on the mat?. X represents the words in the input
space, Y represents labels in the output space. ?(~x) is a feature
representation for the input text ~x and the bottom row represents
the output named entity tags in a more standard form.
is generally that features represent strong discriminating
characteristics of the problem gained through manual en-
gineering and domain-specific insight.
As a concrete example, consider the task of syntactic
chunking, also called ?shallow parsing?, (Gildea and Ju-
rafsky, 2002): Given an input string, e.g.
?the cat sits on the mat?,
the chunking problem consists of labelling segments of a
sentence with syntactic constituents such as noun or verb
phrases (NPs or VPs). Each word is assigned one unique
tag often encoded using the BIO encoding1. We repre-
sent the input text as a vector of words xi ? ~x, and each
word?s corresponding label is represented by yi ? ~y (see
Table 1). Given a feature generating function ?(xi) and
a set of labelled training pairs (xi, yi) ? X ? Y , the task
then reduces to learning a suitable mapping h : ?(X ) ?
Y .
Most previous works have focused on manually en-
gineered features and simpler, linear models, includ-
ing ?shallow? model architectures, like the percep-
tron (Rosenblatt, 1957), linear SVM (Cortes and Vap-
nik, 1995) and linear-chain conditional random fields
(CRFs) (Lafferty, 2001). However, a shallow learning ar-
chitecture is only as good as its input features. Due to the
complex nature of NL, deeper architectures may be re-
1E.g. B-NP means ?begin NP?, I-NP means ?inside NP?, and O
means other/outside.
48
quired to learn data representations which contain the ap-
propriate level of information for the task at hand. Prior to
2006, it was computationally infeasible to perform infer-
ence in hierarchical (?deep?), non-linear models such as
multi-layer perceptrons with more than one hidden layer.
However, Hinton (2006) proposed an efficient, layer-wise
greedy method for learning the model parameters in these
architectures, which spurred a renewed interest in deep
learning research.
Still, creating annotated training data is labour-
intensive and costly, and manually designing and extract-
ing discriminating features from the data to be used in
the learning process is a costly procedure requiring sig-
nificant levels of domain expertise. Over the last two
decades, the growth of available unlabeled data x ? X
and the ubiquity of scalable computing power has shifted
research focus to unsupervised approaches for automat-
ically learning appropriate feature representations ?(x)
from large collections of unlabeled text.
Several methods have been proposed for unsuper-
vised feature learning, including simple k-means cluster-
ing (Lloyd, 1982), Brown clustering (Brown et al, 1992),
mutual information (Shannon and Weaver, 1962), princi-
pal components analysis (PCA) (Jolliffe, 2002), and in-
dependent component analysis (ICA) (Hyva?rinen et al,
2001).
However, natural language has complex mappings
from text to meaning, arguably involving higher-order
correlations between words which these simpler meth-
ods struggle to model adequately. Advances in the ?deep
learning? community allow us to perform efficient unsu-
pervised feature learning in highly complex and high-
dimensional input feature spaces, making it an attrac-
tive method for learning features in e.g. vision or lan-
guage (Bengio, 2009).
The standard deep learning approach is to learn
lower-dimensional embeddings from the raw high-
dimensional2 input space X to lower dimensional (e.g.
50-dimensional) feature spaces in an unsupervised man-
ner, via repeated, layer-wise, non-linear transformation
of the input features, e.g.
y? = f (k)(? ? ? f (2)(f (1)(~x)) ? ? ?),
where f (i)(x) is some non-linear function (typically
tanh) for which the parameters are learned by back prop-
agating error gradients. This configuration is referred to
as a ?deep? architecture with k layers (see Figure 1 for an
example).
For feature generation, we present a trained network
with a new vector ~x representing the input data on its
2E.g. a ?one-hot? 50,000-dimensional vector of input words, with a
?1? indicating the presence of the word at that index, and a ?0? every-
where else.
Figure 1: Example of a deep model. The input vector x is trans-
formed into the hidden representation, here denoted as h1, using
an affine transformation W and a non-linearity. Each subse-
quent hidden layer hk takes as input the output of its preceding
layer h(k?1) (Bengio, 2009).
input layer. After performing one iteration of forward-
propagation through the network, we can then view the
activation values in the hidden layers as dense, so-called
?distributed representations? (features) of the input data.
These features can in turn be passed to an output clas-
sifier layer to produce some tagging task of interest. Re-
cent work in deep learning show state-of-the-art results in
part-of-speech parsing, chunking and named-entity tag-
ging (Collobert, 2011), however performance in more
complex NLP tasks like entity and event disambiguation
and semantic role labelling are still trailing behind.
In this work we focus specifically on extending current
state of the art deep neural models to improve their per-
formance on these more difficult tasks. In the following
section we briefly review and discuss the merits and lim-
itations of three of the current state of the art deep learn-
ing models for NLP. We then identify our primary re-
search questions and introduce our proposed future work
roadmap.
2 Current State-of-the-Art and
Limitations
Most work builds on the idea of a neural probabilistic
language model (NPLM) where words are represented
by learned real-valued embeddings, and a neural network
combines word embeddings to predict the most likely
next word. The first successful NPLM was introduced
by Bengio et al in 2003 (Bengio et al, 2003).
49
Historically, training and testing these models were
slow, scaling linearly in the vocabulary size. However,
several recent approaches have been proposed which
overcome these limitations (Morin and Bengio, 2005),
including the work by Collobert and Weston (2008) and
Mnih and Hinton (2009) discussed next.
2.1 Collobert & Weston (2008)
Collobert and Weston (2008) present a discriminative,
non-probabilistic, non-linear neural language model that
can be scaled to train over billions of words since each
training iteration only computes a loss gradient over a
small stochastic sample of the training data.
All K-dimensional word embeddings are initially set
to a random state. During each training iteration, an n-
gram is read from the training data and each word is
mapped to its respective embedding. All embeddings are
then concatenated to form a nK-length positive training
vector. A corrupted n-gram is also created by replac-
ing the n?th (last) word by some word uniformly chosen
from the vocabulary. The training criterion is that the net-
work must predict positive training vectors with a score
at least some margin higher than the score predicted for
corrupted n-grams. Model parameters are trained simul-
taneously with word embeddings via gradient descent.
2.2 The Hierarchical Log Bilinear (HLBL) Model
Mnih and Hinton (2007) proposed a simple probabilis-
tic linear neural language model called the log bilin-
ear (LBL) model. For an n-gram context window, the
LBL model concatenates the first (n? 1)K-dimensional
word embeddings and then learns a linear mapping from
R(n?1)K to RK for predicting the embedding of the nth
word. For predicting the next word, the model outputs
a probability distribution over the entire vocabulary by
computing the dot product between the predicted embed-
ding and the embedding for each word in the vocabulary
in an output softmax layer. This softmax computation is
linear in the length of the vocabulary for each prediction,
and is therefore the performance bottleneck.
In follow-up work, Mnih and Hinton (2009) speed up
training and testing time by extending the LBL model to
predict the next word by hierarchically decomposing the
search through the vocabulary by traversing a binary tree
constructed over the vocabulary. This speeds up train-
ing and testing exponentially, but initially reduces model
performance as the construction of the binary partition-
ing has a strong effect on the model?s performance. They
introduce a method for bootstrapping the construction of
the tree by initially using a random binary tree to learn
word embeddings, and then rebuilding the tree based on
a clustering of the learned embeddings. Their final results
are superior to the standard LBL in both model perplexity
and model testing time.
2.3 Recursive Neural Networks (RNNs)
Socher (2010; 2011) introduces a recursive neural net-
work (RNN) framework for parsing natural language.
Previous approaches dealt with variable-length sentences
by either
i using a window approach (shifting a window of n
words over the input, processing each fixed-size win-
dow at a time), or
ii by using a convolutional layer where each word is
convolved with its neighbours within some sentence-
or window-boundary.
RNNs operate by recursively applying the same neural
network to segments of its input, thereby allowing RNNs
to naturally operate on variable-length inputs. Each pair
of neighbouring words is scored by the network to reflect
how likely these two words are considered to form part of
a phrase. Each such operation takes two K-dimensional
word vectors and outputs another K-dimensional vector
and a score. Socher (2010) proposes several strategies
(ranging from local and greedy to global and optimal) for
choosing which pairs of words to collapse into a new K-
dimensional vector representing the phrase comprised of
the two words. By viewing these collapsing operations
as branch merging decisions, one can construct a binary
parse tree over the words in a bottom-up fashion.
2.4 Discussion of Limitations
Neural language models are appealing since they can
more easily deal with missing data (unknown word com-
binations) due to their inherent continuous-space repre-
sentation, whereas n-gram language models (Manning et
al., 1999) need to employ (sometimes ad hoc) methods
for smoothing unseen and hence zero probability word
combinations.
The original NPLM performs well in terms of model
perplexity on held-out data; however, its training and test-
ing time is very slow. Furthermore, it provides no support
for handling multiple word senses, the property that any
word can have more than one meaning, since each word
is assigned an embedding based on its literal string repre-
sentation (i.e. from a lookup table).
The Collobert & Weston model still provides no mech-
anism for handling word senses, but improves on the
NPLM by adding several non-linear layers which in-
crease its modelling capacity, and a convolutional layer
for modelling longer range dependencies between words.
Recursive neural nets (RNNs) directly address the prob-
lem of longer-range dependencies by allowing neighbour
words to be combined into their phrasal equivalents in a
bottom-up process.
The LBL model, despite its very simple linear struc-
ture, provides very good performance in terms of model
50
perplexity, but shares the problem of slow training and
testing times and the inability to handle word senses or
dependencies between words (outside its n-gram con-
text).
In the HLBL model, Mnih and Hinton address the slow
testing performance of the LBL model by using a hierar-
chical search tree over the vocabulary to exponentially
speed up testing time, analogous to the concept of class-
based language models (Brown et al, 1992). The HLBL
model can also handle multiple word senses, but in their
evaluation they show that in practice the model learns
multiple senses (codes) for infrequently observed words
instead of words with more than one meaning (Mnih and
Hinton, 2009). The performance is strongly dependent
on the initialisation of the tree, for which they present an
iterative but non-optimal bootstrap-and-train procedure.
Despite being non-optimal, it is shown to outperform the
standard LBL model in terms of perplexity.
3 Mismatched Word Representations and
Classifiers
The deep learning ideal is to train deep, non-linear mod-
els over large collections of unlabeled data, and then use
these models to automatically extract information-rich,
higher-level features3 to integrate into standard NLP or
image processing systems as added features to improve
performance. However, several recent papers report sur-
prising and seemingly contradicting results for this ideal.
In the most direct comparison for NLP, Turian (2010)
compares features extracted using Brown clustering (a
hierarchical clustering technique for clustering words
based on their observed co-occurrence patterns), the hi-
erarchical log-bilinear (HLBL) embeddings (Mnih and
Hinton, 2007) and Collobert and Weston (C+W) em-
beddings (Collobert and Weston, 2008), by integrating
these as additional features in standard supervised condi-
tional random field (CRF) classification systems for NLP.
Somewhat surprisingly, they find that using the more
complex C+W and HLBL features do not improve signif-
icantly over Brown features. Indeed, under several con-
ditions the Brown features give the best results.
These results are important for several reasons (we
highlight these results in Table 2). The goal was to im-
prove classification performance in structured prediction
tasks in natural language by integrating features learned
in a deep, unsupervised approach within a standard lin-
ear classification framework. Yet these complex, deep
methods are outperformed by simpler unsupervised fea-
ture extraction methods.
3?Higher-level? features simply mean combining simpler features
extracted from a text to produce conceptually more abstract indicators,
e.g. combining word-indicators for ?attack?, ?soldier?, etc. to form an
indicator for WAR, even though ?war? is not mentioned anywhere in the
text.
System Dev Test MUC7
Baseline 90.03 84.39 67.48
HLBL 100-dim 92.00 88.13 75.25
C&W 50-dim 92.27 87.93 75.74
Brown, 1000 clusters 92.32 88.52 78.84
C&W 200-dim 92.46 87.96 75.51
Table 2: Final NER F1 results reported by Turian (2010).
In a sense, these seem to be negative results for the
utility of deep learning in NLP. However, in this work we
argue that these seemingly anomalous results stem from
a mismatch between the feature learning function and the
classifier that was used in the classification (and hence
evaluation) process.
We consider the learning problem h : X ? Y to
decompose into h = h?(?(X )), where ? is the feature
learning function and h? is a standard supervised classi-
fier. ? reads input from X and outputs encodings in fea-
ture space ?(x). h reads input in feature space ?(x) and
outputs encodings in the output label space Y .
Note that this easily extends to deep feature
learning models by simply replacing ?(X ) with
?(k)(? ? ??(2)(?(1)(X )) ? ? ?), for a k-layer architecture,
where the first layer reads input inX and each subsequent
layer reads the output of the previous layer.
Within this view of the deep learning process, we can
see that unsupervised feature learning does not happen in
isolation. Instead, the learned features only make sense
within some learning framework, since the output of the
feature learning function ? (and each deep layer ?(k?1))
maps to a region in feature code space which becomes
in turn the input to the output classifier h? (or subsequent
layer ?(k)) . We therefore argue that in a semi-supervised
or unsupervised classification problem, the feature learn-
ing function ? should be strongly dependent on the clas-
sifier h? that interprets those features, and vice versa.
This notion ties in with the standard deep-learning
training protocol of unsupervised pre-training followed
by joint supervised fine-tuning (Hinton et al, 2006) of the
top classification layer and the deeper feature extraction
layers. We conjecture that jointly training a deep feature
extraction model with a linear output classifier leads to
better linearly separable feature vectors ?(x) than train-
ing both independently. Note that this is in contrast to
how Turian (2010) integrated the unsupervised features
into existing NLP systems via disjoint training.
4 Proposed Work and Research Questions
For simpler sequence tagging tasks such as part-of-
speech tagging and noun phrase chunking, the state-
of-the-art models introduced in Section 2 perform ade-
quately. However, in order to make use of the increased
51
modelling capacity of deep neural models, and to suc-
cessfully model more complex semantic tasks such as
anaphora resolution and semantic role labelling, we hy-
pothesise that the model needs to avoid modelling purely
local lexical semantics and needs to efficiently handle
multiple word senses and long-range dependencies be-
tween input words (or phrases) and output labels. We
propose to overcome the limitations of previous models
with regard to these design goals, by focusing on the fol-
lowing key areas:
Input language representation: Neural models rely
on vector representations of their input (as opposed to
discrete representations as in, for instance, HMMs). In
NLP, sentences are therefore encoded as real-valued em-
bedding vectors. These vectors are learned in either a
task-specific setting (as in the C+W model) or as part of
a language model (as in the LBL model), where the goal
is to predict the next word given the learned representa-
tions of the previous words. In order to maximise the
information available to the model, we need to provide
information-rich representations to the model. Current
approaches represent each word in a sentence using a dis-
tinct word vector based on its literal string representation.
However, as noted earlier, in NL the same words can have
different senses based on the context in which it appears
(polysemy). We propose to extend the hierarchical log-
bilinear (HLBL) language model (see Section 2.2) in two
important ways. We choose the HLBL model for its sim-
plicity and good performance compared to more complex
models.
Firstly, we propose to replace the iterative bootstrap-
and-train process for learning the hierarchical tree struc-
ture over the vocabulary with a modified self-balancing
binary tree. The tree rebalances itself from an initial
random tree to leave most frequently accessed words
near the root (for shorter codes and faster access times),
while moving words between clusters to maximise over-
all model perplexity.
Secondly, we propose to add a word sense disambigua-
tion layer capable of modelling long-range dependencies
between input words. For this layer we will compare a
modified RNN layer to a convolutional layer. The modi-
fied RNN will embed each focus word with its nmost dis-
criminative neighbour words (in a sentence context win-
dow) into a new K-dimensional, sense-disambiguated
embedding vector for the focus word. We will evaluate
and optimise the final model?s learned representations by
evaluating language model perplexity on held out data.
Model architecture and internal representation:
Deep models derive their modelling power from their hi-
erarchical structure. Each layer transforms the output
representation of its previous layer, allowing the model
to learn more general and abstract feature combinations
in the higher layers which are relevant for the current
task. The representations on the hidden layers serve
as transformed feature representations of the input data
for the output classifier. Enforcing sparsity on the hid-
den layers has been shown to produce stronger features
for certain tasks in vision (Coates et al, 2010). Ad-
ditionally, individual nodes might be highly correlated,
which can also reduce the performance of certain clas-
sifiers which make strong independence assumptions (for
instance naive Bayes). We propose to study the effect that
enforcing sparsity in the learned feature representations
has on task performance in NLP. Additionally, we pro-
pose to evaluate the effect that an even stronger training
objective ? one that encourages statistical independence
between hidden nodes by learning factorial code repre-
sentations (Hochreiter and Schmidhuber, 1999) ? has on
model performance.
Modelling structure in the output space: Tasks
in NLP mostly involve predicting labels which exhibit
highly regular structure. For instance, in part-of-speech
tagging, two determiners have a very low likelihood of
following directly on one another, e.g. ?the the?. In or-
der to successfully model this phenomenon, a model must
take into account previous (and potentially future) predic-
tions when making the current prediction, e.g. as in hid-
den Markov models and conditional random fields. We
propose to include sequential dependencies in the output
labels and to compare this with including a convolutional
layer below the output layer, for predicting output labels
in complex NLP tasks such as coreference resolution and
event structure detection.
5 Conclusion
Deep learning methods offer an attractive unsupervised
approach for extracting higher-level features from large
quantities of text data to be used for NLP tasks. However
current attempts at integrating these features into existing
NLP systems do not produce the desired performance im-
provements. We conjecture that this is due to a mismatch
between the learned word representations and the classi-
fiers used as a result of disjoint training schemes, and our
thesis roadmap suggests three key areas for overcoming
these limitations.
References
Y. Bengio, R. Ducharme, P. Vincent, and C. Janvin. 2003. A
neural probabilistic language model. Journal of Machine
Learning Research, 3:1137?1155, March.
Y. Bengio. 2009. Learning deep architectures for ai. Founda-
tions and Trends R? in Machine Learning, 2(1):1?127.
P.F. Brown, P.V. Desouza, R.L. Mercer, V.J.D. Pietra, and J.C.
Lai. 1992. Class-based n-gram models of natural language.
Computational linguistics, 18(4):467?479.
52
A. Coates, H. Lee, and A.Y. Ng. 2010. An analysis of single-
layer networks in unsupervised feature learning. Ann Arbor,
1001:48109.
R. Collobert and J. Weston. 2008. A unified architecture
for natural language processing: Deep neural networks with
multitask learning. In Proceedings of the 25th International
Conference on Machine Learning, pages 160?167. ACM.
R. Collobert. 2011. Deep learning for efficient discrimina-
tive parsing. In International Conference on Artificial In-
telligence and Statistics (AISTATS).
C. Cortes and V. Vapnik. 1995. Support-vector networks. Ma-
chine learning, 20(3):273?297.
D. Gildea and D. Jurafsky. 2002. Automatic labeling of seman-
tic roles. Computational Linguistics, 28(3):245?288.
G.E. Hinton, S. Osindero, and Y.W. Teh. 2006. A fast learn-
ing algorithm for deep belief nets. Neural computation,
18(7):1527?1554.
S. Hochreiter and J. Schmidhuber. 1999. Feature extraction
through lococode. Neural Computation, 11(3):679?714.
A. Hyva?rinen, J. Karhunen, and E. Oja. 2001. Independent
component analysis, volume 26. Wiley Interscience.
I.T. Jolliffe. 2002. Principal component analysis, volume 2.
Wiley Online Library.
J. Lafferty. 2001. Conditional random fields: Probabilistic
models for segmenting and labeling sequence data. In Pro-
ceedings of ICML, 2001.
S. Lloyd. 1982. Least squares quantization in pcm. IEEE
Transactions on Information Theory, 28(2):129?137.
C.D. Manning, H. Schu?tze, and MITCogNet. 1999. Founda-
tions of statistical natural language processing, volume 999.
MIT Press.
A. Mnih and G. Hinton. 2007. Three new graphical models for
statistical language modelling. In Proceedings of the 24th
international conference on Machine learning, pages 641?
648. ACM.
A. Mnih and G.E. Hinton. 2009. A scalable hierarchical dis-
tributed language model. Advances in neural information
processing systems, 21:1081?1088.
F. Morin and Y. Bengio. 2005. Hierarchical probabilistic neural
network language model. In AISTATS05, pages 246?252.
F. Rosenblatt. 1957. The Perceptron, a Perceiving and Recog-
nizing Automaton Project Para. Cornell Aeronautical Labo-
ratory.
C.E. Shannon and W. Weaver. 1962. The mathematical theory
of communication, volume 19. University of Illinois Press
Urbana.
R. Socher, C.D. Manning, and A.Y. Ng. 2010. Learning contin-
uous phrase representations and syntactic parsing with recur-
sive neural networks. In Proceedings of the NIPS-2010 Deep
Learning and Unsupervised Feature Learning Workshop.
R. Socher, C.C. Lin, A.Y. Ng, and C.D. Manning. 2011. Pars-
ing natural scenes and natural language with recursive neural
networks. In Proceedings of the 26th International Confer-
ence on Machine Learning (ICML), volume 2, page 7.
J. Turian, L. Ratinov, and Y. Bengio. 2010. Word represen-
tations: A simple and general method for semi-supervised
learning. In Proceedings of the 48th Annual Meeting of the
Association for Computational Linguistics, pages 384?394.
Association for Computational Linguistics.
53
Proceedings of the 2nd Workshop on ?Collaboratively Constructed Semantic Resources?, Coling 2010, pages 46?54,
Beijing, August 2010
Measuring Conceptual Similarity by Spreading Activation over
Wikipedia?s Hyperlink Structure
Stephan Gouws, G-J van Rooyen, and Herman A. Engelbrecht
Stellenbosch University
{stephan,gvrooyen,hebrecht}@ml.sun.ac.za
Abstract
Keyword-matching systems based on
simple models of semantic relatedness
are inadequate at modelling the ambigu-
ities in natural language text, and cannot
reliably address the increasingly com-
plex information needs of users. In
this paper we propose novel methods
for computing semantic relatedness by
spreading activation energy over the hy-
perlink structure of Wikipedia. We
demonstrate that our techniques can
approach state-of-the-art performance,
while requiring only a fraction of the
background data.
1 Introduction
The volume of information available to users
on the World Wide Web is growing at an
exponential rate (Lyman and Varian, 2003).
Current keyword-matching information retrieval
(IR) systems suffer from several limitations,
most notably an inability to accurately model
the ambiguities in natural language, such as syn-
onymy (different words having the same mean-
ing) and polysemy (one word having multiple
different meanings), which is largely governed
by the context in which a word appears (Metzler
and Croft, 2006).
In recent years, much research attention has
therefore been given to semantic techniques of
information retrieval. Such systems allow for
sophisticated semantic search, however, require
the use of a more difficult-to-understand query-
syntax (Tran et al, 2008). Furthermore, these
methods require specially encoded (and thus
costly) ontologies to describe the particular do-
main knowledge in which the system operates,
and the specific interrelations of concepts within
that domain.
In this paper, we focus on the problem of
computationally estimating similarity or related-
ness between two natural-language documents.
A novel technique is proposed for comput-
ing semantic similarity by spreading activation
over the hyperlink structure of Wikipedia, the
largest free online encyclopaedia. New mea-
sures for computing similarity between individ-
ual concepts (inter-concept similarity, such as
?France? and ?Great Britain?), as well as be-
tween documents (inter-document similarity)
are proposed and tested. It will be demonstrated
that the proposed techniques can achieve compa-
rable inter-concept and inter-document similar-
ity accuracy on similar datasets as compared to
the current state of the art Wikipedia Link-based
Measure (WLM) (Witten and Milne, 2008) and
Explicit Semantic Analysis (ESA) (Gabrilovich
and Markovitch, 2007) methods respectively.
Our methods outperform WLM in computing
inter-concept similarity, and match ESA for
inter-document similarity. Furthermore, we use
the same background data as for WLM, which is
less than 10% of the data required for ESA.
In the following sections we introduce work
related to our work and an overview of our
approach and the problems that have to be
solved. We then discuss our method in detail and
present several experiments to test and compare
it against other state-of-the-art methods.
46
2 Related Work and Overview
Although Spreading Activation (SA) is foremost
a cognitive theory modelling semantic mem-
ory (Collins and Loftus, 1975), it has been ap-
plied computationally to IR with various lev-
els of success (Preece, 1982), with the biggest
hurdle in this regard the cost of creating an as-
sociative network or knowledge base with ad-
equate conceptual coverage (Crestani, 1997).
Recent knowledge-based methods for comput-
ing semantic similarity between texts based on
Wikipedia, such as Wikipedia Link-based Mea-
sure (WLM) (Witten and Milne, 2008) and Ex-
plicit Semantic Analysis (ESA) (Gabrilovich and
Markovitch, 2007), have been found to out-
perform earlier WordNet-based methods (Bu-
danitsky and Hirst, 2001), arguably due to
Wikipedia?s larger conceptual coverage.
WLM treats the anchor text in Wikipedia arti-
cles as links to other articles (all links are treated
equally), and compare concepts based on how
much overlap exists in the out-links of the arti-
cles representing them. ESA discards the link
structure and uses only the text in articles to de-
rive an explicit concept space in which each di-
mension represents one article/concept. Text is
categorised as vectors in this concept space and
similarity is computed as the cosine similarity of
their ESA vectors. The most similar work to ours
is Yeh (2009) in which the authors derive a graph
structure from the inter-article links in Wikipedia
pages, and then perform random walks over the
graph to compute relatedness.
In Wikipedia, users create links between arti-
cles which are seen to be related to some degree.
Since links relate one article to its neighbours,
and by extension to their neighbours, we ex-
tract and process this hyperlink structure (using
SA) as an Associative Network (AN) (Berger
et al, 2004) of concepts and links relating them
to one another. The SA algorithm can briefly
be described as an iterative process of propagat-
ing real-valued energy from one or more source
nodes, via weighted links over an associative net-
work (each such a propagation is called a pulse).
The algorithm consists of two steps: First, one
or more pulses are triggered, and second, ter-
mination checks determine whether the process
should continue or halt. This process of acti-
vating more and more nodes in the network and
checking for termination conditions are repeated
pulse after pulse, until all termination conditions
are met, which results in a final activation state
for the network. These final node activations
are then translated into a score of relatedness be-
tween the initial nodes.
Our work presents a computational imple-
mentation of SA over the Wikipedia graph.
We therefore overcome the cost of produc-
ing a knowledge base of adequate coverage by
utilising the collaboratively-created knowledge
source Wikipedia. However, additional strate-
gies are required for translating the hyperlink
structure of Wikipedia into a suitable associative
network format, and for this new techniques are
proposed and tested.
3 Extracting the Hyperlink Graph
Structure
One article in Wikipedia covers one specific
topic (concept) in detail. Hyperlinks link a page
A to a page B, and are thus directed. We
can model Wikipedia?s hyperlink structure us-
ing standard graph theory as a directed graph G,
consisting of a set of vertices V, and a set of
edges E. Each edge eij ? E connects two ver-
tices vi, vj ? V. For consistency, we use the
term node to refer to a vertex (Wikipedia article)
in the graph, and link to refer to an edge (hyper-
link) between such nodes.
In this model, each Wikipedia article is seen
to represent a single concept, and the hyperlink
structure relates these concepts to one another. In
order to compute relatedness between two con-
cepts vi and vj , we use spreading activation and
rely on the fundamental principle of an associa-
tive network, namely that it connects nodes that
are associated with one another via real-valued
links denoting how strongly the objects are re-
lated. Since Wikipedia was not created as an as-
sociative network, but primarily as an online en-
cyclopaedia, none of these weights exist, and we
will have to deduce these (see Fan-out constraint
in Section 4).
47
Links into pages are used, since this leads to
better results (Witten and Milne, 2008). The
Wikipedia graph structure is represented in an
adjacency list structure, i.e. for each node vi we
store its list of neighbour nodes in a dictionary
using vi?s id as key. This approach is preferred
over an adjacency matrix structure, since most
articles are linked to by only 34 articles on aver-
age, which would lead to a very sparse adjacency
matrix structure.
4 Adapting Spreading Activation for
Wikipedia?s Hyperlink Structure
Each pulse in the Spreading Activation (SA) pro-
cess consists of three stages: 1) pre-adjustment,
2) spreading, and 3) post-adjustment (Crestani,
1997). During pre- and post-adjustment, some
form of activation decay is optionally applied to
the active nodes. This serves both to avoid re-
tention of activation from previous pulses, and,
from a connectionist point of view, models ?loss
of interest? when nodes are not continually acti-
vated.
Let ai,in denote the total energy input (acti-
vation) for node vi, and N(vi) the set of vi?s
neighbour nodes with incoming links to vi. Also,
let aj,out denote the output activation of a node
vj connected to node vi, and let wij denote the
weight of connection between node vi and vj .
For a node vi, we can then describe the pure
model of spreading activation as follows:
ai,in =
?
vj?N(vi)
aj,outwij . (1)
This pure model of SA has several significant
problems, the most notable being that activation
can saturate the entire network unless certain
constraints are imposed, namely limiting how
far activation can spread from the initially acti-
vated nodes (distance constraint), and limiting
the effect of very highly-connected nodes (fan-
out constraint) (Crestani, 1997). In the following
three sections we discuss how these constraints
were implemented in our model for SA.
Distance constraint
For every pulse in the spreading process, a
node?s activation value is multiplied by a global
network decay parameter 0 < d < 1. We
therefore substitute wij in Equation 1 for wijd.
This decays activation exponentially in the path
length. For a path length of one, activation is de-
cayed by d, for a path length of two, activation
is decays by dd = d2, etc. This penalises activa-
tion transfer over longer paths. We also include
a maximum path length parameter Lp,max which
limits how far activation can spread.
Fan-out constraint
As noted above, in an associative network, links
have associated real-valued weights to denote the
strength of association between the two nodes
they connect (i.e. wij in Equation 1). These
weights have to be estimated for the Wikipedia
hyperlink graph, and for this purpose we propose
the use of three weighting schemes:
In pure Energy Distribution (ED), a node
vi?s weight w is made inversely proportional to
its in-degree (number of neighbours N(vi) ? 1
with incoming links to vi1). Thus ED(vi, vj) =
wij = 1|N(vi)| . This reduces the effect of veryconnected nodes on the spreading process (con-
straint 2 above).
For instance, we consider a path connecting
two nodes via a general article such as USA (con-
nected to 322,000 articles) not nearly as indica-
tive of a semantic relationship, as a path con-
necting them via a very specific concept, such
as Hair Pin (only connected to 20 articles).
Inverse Link-Frequency (ILF) is inspired by
the term-frequency inverse document-frequency
(tf-idf) heuristic (Salton and McGill, 1983) in
which a term?s weight is reduced as it is con-
tained in more documents in the corpus. It is
based on the idea that the more a term appears
in documents across the corpus, the less it can
discriminate any one of those documents.
We define a node vi?s link-frequency as the
number of nodes that vi is connected to |N(vi)|
divided by the number of possible nodes it could
be connected to in the entire Wikipedia graph
1All orphan nodes are removed from the AN.
48
|G|, and therefore give the log-smoothed inverse
link-frequency of node vi as:
ILF(vi) , log
( |G|
|N(vi)|
)
? 0 (2)
As noted above for pure energy distribution, we
consider less connected nodes as more specific.
If one node connects to another via a very spe-
cific node with a low in-degree, |G||N(vi)| is verylarge and ILF(vi) > 1, thus boosting that spe-
cific link?s weight. This has the effect of ?boost-
ing? paths (increasing their contribution) which
contain nodes that are less connected, and there-
fore more meaningful in our model.
To evaluate the effect of this boosting ef-
fect described above, we also define a third
normalised weighting scheme called the Nor-
malised Inverse Link-Frequency (NILF), 0 ?
NILF(vi) ? 1:
NILF(vi) , ILF(vi)log |G| . (3)
ILF reaches a maximum of log |G| when
|N(vi)| = 1 (see Equation 2). We therefore di-
vide by log |G| to normalise its range to [0,1].
Threshold constraint
Finally, the above-mentioned constraints are en-
forced through the use of a threshold parameter
0 < T < 1. Activation transfer to a next node
ceases when a node?s activation value drops be-
low a certain threshold T .
5 Strategies for Interpreting
Activations
After spreading has ceased, we are left with a
vector of nodes and their respective values of
activation (an activation vector). We wish to
translate this activation vector into a score re-
sembling strength of association or relatedness
between the two initial nodes.
We approach this problem using two differ-
ent approaches, the Target Activation Approach
(TAA) and the Agglomerative Approach (AA).
These approaches are based on two distinct hy-
potheses, namely: Relatedness between two
nodes can be measured as either 1) the ratio of
initial energy that reaches the target node, or 2)
the amount of overlap between their individual
activation vectors by spreading from both nodes
individually.
Target Activation Approach (TAA)
To measure the relatedness between vi and vj ,
we set ai to some initial valueKinit (usually 1.0),
and all node activations including aj = 0. Af-
ter the SA process has terminated, vj is activated
with some aj,in. Relatedness is computed as the
ratio simTAA(vi, vj) , aj,inKinit .
Agglomerative Approach (AA)
The second approach is called the Agglomera-
tive Approach since we agglomerate all activa-
tions into one score resembling relatedness. Af-
ter spreading has terminated, relatedness is com-
puted as the amount of overlap between the indi-
vidual nodes? activation vectors, using either the
cosine similarity (AA-cos), or an adapted ver-
sion of the information theory based WLM (Wit-
ten and Milne, 2008) measure.
Assume the same set of initial nodes vi and
vj . Let Ak be the N -dimensional vector of real-
valued activation values obtained by spreading
over the N nodes in the graph from node vk
(called an activation vector). We use akx to de-
note the element at position x in Ak. Further-
more, let Vk = {vk1, ..., vkM} denote the set of
M nodes activated by spreading from vk, i.e. the
set of identifiers of nodes with non-zero activa-
tions in Ak after spreading has terminated (and
therefore M ? N ).
We then define the cosine Agglomerative Ap-
proach (henceforth called AA-cos) as
simAA,cos(Ai,Aj)
, Ai ?Aj||Ai||||Aj || (4)
For our adaptation of the Wikipedia Link-based
Measure (WLM) approach to spreading activa-
tion, we define the WLM Agglomerative Ap-
proach (henceforth called AA-wlm2) as
2AA-wlm is our adaptation of WLM (Witten and Milne,
2008) for SA, not to be confused with their method, which
we simply call WLM.
49
simAA,wlm(Vi,Vj)
, log(max(|Vi|,|Vj|))?log(|Vi?Vj|)log(|G|)?log(min(|Vi|,|Vj|)) (5)
with |G| representing the number of nodes in the
entire Wikipedia hyperlink graph. Note that the
AA-wlm method does not take activations into
account, while the AA-cos method does.
6 Spreading Activation Algorithm
Both the TAA and AA approaches described
above rely on a function to spread activation
from one node to all its neighbours, and itera-
tively to all their neighbours, subject to the con-
straints listed. TAA stops at this point and com-
putes relatedness as the ratio of energy received
to energy sent between the target and source
node respectively. However, AA repeats the pro-
cess from the target node and computes related-
ness as some function (cosine or information the-
ory based) of the two activation vectors, as given
by Equation 4 and Equation 5.
We therefore define SPREAD UNIDIR() as
shown in Algorithm 1. Prior to spreading from
some node vi, its activation value ai is set to
some initial activation value Kinit (usually 1.0).
The activation vector A is a dynamic node-
value-pair list, updated in-place. P is a dynamic
list of nodes in the path to vi to avoid cycles.
7 Parameter Optimisation:
Inter-concept Similarity
The model for SA as introduced in this paper re-
lies on several important parameters, namely the
spreading strategy (TAA, AA-cos, or AA-wlm),
weighting scheme (pure ED, ILF, and NILF),
maximum path length Lp,max, network decay d,
and threshold T . These parameters have a large
influence on the accuracy of the proposed tech-
nique, and therefore need to be optimised.
Experimental Method
In order to compare our method with results re-
ported by Gabrilovich and Markovitch (2007)
and Witten and Milne (2008), we followed
the same approach by randomly selecting
Algorithm 1 Pseudo code to spread activation
depth-first from node vi up to level Lp,max, us-
ing global decay d, and threshold T , given an
adjacency list graph structure G and a weighting
scheme W such that 0 < wij ?W < 1.
Require: G,Lp,max, d, T
function SPREAD UNIDIR(vi,A,P)
if (vi, ai) /? A or ai < T then . Threshold
return
end if
Add vi to P . To avoid cycles
for vj ? N(vi) do . Process neighbours
if (vj , aj) /? A then
aj = 0
end if
if vj /? P and |P| ? Lp,max then
a?j = aj + ai ? wij ? d
Replace (vj , aj) ? A with (vj , a?j )
SPREAD UNIDIR(vj ,A,P)
end if
end for
return
end function
50 word-pairs from the WordSimilarity-353
dataset (Gabrilovich, 2002) and correlating
our method?s scores with the human-assigned
scores. To reduce the possibility of overestimat-
ing the performance of our technique on a sam-
ple set that happens to be favourable to our tech-
nique, we furthermore implemented a technique
of repeated holdout (Witten and Frank, 2005):
Given a sample test set of N pairs of words
with human-assigned ratings of relatedness, ran-
domly divide this set into k parts of roughly
equal size3. Hold out one part of the data and
iteratively evaluate the performance of the algo-
rithm on the remaining k?1 parts until all k parts
have been held out once. Finally, average the al-
gorithm?s performance over all k runs into one
score resembling the performance for that set of
parameters.
Since there are five parameters (spreading
strategy, weighting scheme, path length, network
decay, and threshold), a grid search was imple-
mented by holding three of the five parameters
constant, and evaluating combinations of decay
and threshold by stepping over the possible pa-
rameter space using some step size. A coarse-
grained grid search was first conducted with step
3k was chosen as 5.
50
Table 1: Spreading results by spreading
strategy (TAA=Target Activation Approach,
AA=Agglomerative Approach, Lp,max = max-
imum path length used, ED=energy distri-
bution only, ILF=Inverse Link Frequency,
NILF=normalised ILF.) Best results in bold.
Strategy ?max Parameters
TAA 0.56 ED, Lp,max=3, d=0.6, T=0.001
AA-wlm 0.60 NILF, Lp,max=3, d=0.1, T=10?6
AA-cos 0.70 ILF, Lp,max=3, d=0.5, T=0.1
size of 0.1 over d and a logarithmic scale over
T , thus T = {0, 0.1, 0.01, 0.001, ..., 10?9}. The
best values for d and T were then chosen to con-
duct a finer-grained grid search.
Influence of the different Parameters
The spreading strategy determines how activa-
tions resulting from the spreading process are
converted into scores of relatedness or similar-
ity between two nodes. Table 1 summarises the
best results obtained for each of the three strate-
gies, with the specific set of parameters that were
used in each run.
Results are better using the AA (?max =
0.70 for AA-cos) than using the TAA (?max =
0.56). Secondly, the AA-cos spreading strat-
egy significantly outperforms the AA-wlm strat-
egy over this sample set (?max,wlm = 0.60
vs ?max,cos = 0.70). These results compare
favourably to similar inter-concept results re-
ported for WLM (Witten and Milne, 2008) (? =
0.69) and ESA (Gabrilovich and Markovitch,
2007) (? = 0.75).
Maximum path length Lp,max is related to
how far one node can spread its activation in the
network. We extend the first-order link model
used by WLM, by approaching the link structure
as an associative network and by using spreading
activation.
To evaluate if this is a useful approach, tests
were conducted by using maximum path lengths
of one, two, and three. Table 2 summarises
the results for this experiment. Increasing path
length from one to two hops increases per-
formance from ?max = 0.47 to ?max =
Table 2: Spreading results by maximum path
length Lp,max. Best results in bold.
Lp,max ?max Parameters
1 0.47 TAA, ED/ILF/NILF
2 0.66 AA-cos, ILF, d=0.4, T=0.1
3 0.70 AA-cos, ILF, d=0.5, T=0.1
Table 3: Spreading results by weighting scheme
w. Best results in bold.
w ?max Parameters
NILF 0.63 AA-cos, Lp,max = 3, d=0.9, T=0.01
ED 0.64 AA-cos, Lp,max = 3, d=0.9, T=0.01
ILF 0.70 AA-cos, Lp,max = 3, d=0.5, T=0.1
0.66. Moreover, increasing Lp,max from two to
three hops furthermore increases performance to
?max = 0.70.
In an associative network, each link has a
real-valued weight denoting the strength of as-
sociation between the two nodes it connects.
The derived Wikipedia hyperlink graph lacks
these weights. We therefore proposed three new
weighting schemes (pure ED, ILF, and NILF) to
estimate these weights.
Table 3 summarises the best performances us-
ing the different weighting schemes. ILF outper-
forms both ED and NILF. Furthermore, both ED
and NILF perform best using higher decay val-
ues (both 0.9) and lower threshold values (both
0.01), compared to ILF (0.5 and 0.1 respectively
for d and T ). We attribute this observation to
the boosting effect of the ILF weighting scheme
for less connected nodes, and offer the following
explanation:
Recall from the section on ILF that in our
model, strongly connected nodes are viewed as
more general, and nodes with low in-degrees
are seen as very specific concepts. We argued
that a path connecting two concepts via these
more specific concepts are more indicative of
a stronger semantic relationship than through
some very general concept. In the ILF weighting
scheme, paths containing these less connected
nodes are automatically boosted to be more im-
51
portant. Therefore, by not boosting less mean-
ingful paths, a lower decay and higher threshold
effectively limits the amount of non-important
nodes that are activated, since their activations
are more quickly decayed, whilst at the same
time requiring a higher threshold to continue
spreading. Boosting more important nodes can
therefore lead to activation vectors which capture
the semantic context of the source nodes more
accurately, leading to higher performance.
8 Computing document similarity
To compute document similarity, we first extract
key representative Wikipedia concepts from a
document to produce document concept vec-
tors4. This process is known as wikifica-
tion (Csomai and Mihalcea, 2008), and we used
an implementation of Milne and Witten (2008).
This produces document concept vectors of the
form Vi = {(id1, w1), (id2, w2), ...} with idi
some Wikipedia article identifier andwi a weight
denoting how strongly the concept relates to the
current document. We next present two algo-
rithms, MAXSIM and WIKISPREAD, for com-
puting document similarity, and test these over
the Lee (2005) document similarity dataset, a
set of 50 documents between 51 and 126 words
each, with the averaged gold standard similarity
ratings produced by 83 test subjects (see (Lee et
al., 2005)).
The first metric we propose is called
MAXSIM (see Algorithm 2) and is based on
the idea of measuring document similarity by
pairing up each Wikipedia concept in one docu-
ment?s concept vector with its most similar con-
cept in the other document. We average those
similarities to produce an inter-document simi-
larity score, weighted by how strongly each con-
cept is seen to represent a document (0 < pi <
1). The contribution of a concept is further
weighted by its ILF score, so that more specific
concepts contribute more to final relatedness.
The second document similarity metric we
propose is called the WIKISPREAD method and
is a natural extension of the inter-concept spread-
4Vectors of Wikipedia topics (concepts) and how
strongly they are seen to relate to the current document.
Algorithm 2 Pseudo code for the MaxSim al-
gorithm for computing inter-document similar-
ity. vi is a Wikipedia concept and 0 < pi < 1
how strongly it relates to the current document.
Require: ILF lookup function
function MAXSIM(V1,V2)
num=0
den=0
for (vi, pi) ? V1 do
sk = 0 . sk = maxj sim(vi, vj)
for vj ? V2 do . Find most related topic
sj = sim(vi, vj)
if sj > sk then
vk = vj. Topic in V2 most related to vi
sk = sj
end if
end for
num += skpiILF(vk)
den += ILF(vk)
end for
return num / den
end function
Algorithm 3 Pseudo code for the WikiSpread al-
gorithm for computing inter-document similar-
ity. Kinit = 1.0.
function WIKISPREAD(V1,V2)
A1 = ? . Dynamic activation vectors.
A2 = ?
for (vi, pi) ? V1 do . Document 1
ai = Kinit ? pi . Update ai ? pi
Add (vi, ai) to A1
SPREAD UNIDIR(vi,A1, ?)
end for
for (vj , pj) ? V2 do . Document 2
aj = Kinit ? pj
Add (vj , aj) to A2
SPREAD UNIDIR(vj ,A2, ?)
end for
Compute similarity using AA-cos or AA-wlm
end function
ing activation work introduced in the previous
section. We view a document concept vector as
a cluster of concepts, and build a single docu-
ment activation vector (see Algorithm 3) ? i.e. a
vector of article ids and their respective activa-
tions ? for each document, by iteratively spread-
ing from each concept in the document concept
vector. Finally, similarity is computed using ei-
ther the AA-cos or AA-wlm methods given by
Equation 4 and Equation 5 respectively.
Knowledge-based approaches such as the
Wikipedia-based methods can capture more
complex lexical and semantic relationships than
52
Table 4: Summary of final document similarity
correlations over the Lee & Pincombe document
similarity dataset. ESA score from Gabrilovich
and Markovitch (2007).
Pearson ?
Cosine VSM (with tf-idf) only 0.56
MaxSim method 0.68
WikiSpread method 0.62
ESA 0.72
Combined (Cosine + MaxSim) 0.72
keyword-matching approaches, however, noth-
ing can be said about concepts not adequately
represented in the underlying knowledge base
(Wikipedia). We therefore hypothesise that com-
bining the two approaches will lead to more ro-
bust document similarity performance. There-
fore, the final document similarity metric we
evaluate (COMBINED) is a linear combination
of the best-performing Wikipedia-based meth-
ods described above, and the well-known Vector
Space Model (VSM) with cosine similarity and
tf-idf (Salton and McGill, 1983).
Results
The results obtained on the Lee (2005) document
similarity dataset using the three document sim-
ilarity metrics (MAXSIM, WIKISPREAD, and
COMBINED) are summarised in Table 4. Of
the two Wikipedia-only methods, the MaxSim
method achieves the best correlation score of
? = 0.68. By combining the standard co-
sine VSM with tf-idf with the MaxSim metric
in the ratio ? and (1 ? ?) for 0 < ? < 1,
and performing a parameter sweep over ?, we
can weight the contributions made by the indi-
vidual methods and observe the effect this has
on final performance. The results are shown
in Fig 1. Note that both methods contribute
equally (? = 0.5) to the final best correlation
score of ? = 0.72. This suggests that selective
knowledge-based augmentation of simple VSM
methods can lead to more accurate document
similarity performance.
Figure 1: Parameter sweep over ? showing con-
tributions from cosine (?) and Wikipedia-based
MAXSIM method (1 ? ?) to the final perfor-
mance over the Lee (2005) dataset.
9 Conclusion
In this paper, the problem of computing con-
ceptual similarity between concepts and docu-
ments are approached by spreading activation
over Wikipedia?s hyperlink graph. New strate-
gies are required to infer weights of associa-
tion between articles, and for this we introduce
and test three new weighting schemes and find
our Inverse Link-Frequency (ILF) to give best
results. Strategies are also required for trans-
lating resulting activations into scores of relat-
edness, and for this we propose and test three
new strategies, and find that our cosine Agglom-
erative Approach gives best results. For com-
puting document similarity, we propose and test
two new methods using only Wikipedia. Finally,
we show that using our best Wikipedia-based
method to augment the cosine VSM method us-
ing tf-idf, leads to the best results. The final
result of ? = 0.72 is equal to that reported
for ESA (Gabrilovich and Markovitch, 2007),
while requiring less than 10% of the Wikipedia
database required for ESA. Table 4 summarises
the document-similarity results.
Acknowledgements
We thank Michael D. Lee for his document simi-
larity data and MIH Holdings Ltd. for financially
supporting this research.
53
References
Berger, Helmut, Michael Dittenbach, and Dieter
Merkl. 2004. An adaptive information retrieval
system based on associative networks. APCCM
?04: Proceedings of the first Asian-Pacific confer-
ence on Conceptual Modelling, pages 27?36.
Budanitsky, A. and G. Hirst. 2001. Semantic dis-
tance in WordNet: An experimental, application-
oriented evaluation of five measures. In Work-
shop on WordNet and Other Lexical Resources,
volume 2. Citeseer.
Collins, A.M. and E.F. Loftus. 1975. A spreading-
activation theory of semantic processing. Psycho-
logical review, 82(6):407?428.
Crestani, F. 1997. Application of Spreading Activa-
tion Techniques in Information Retrieval. Artifi-
cial Intelligence Review, 11(6):453?482.
Csomai, A. and R. Mihalcea. 2008. Linking docu-
ments to encyclopedic knowledge. IEEE Intelli-
gent Systems, 23(5):34?41.
Gabrilovich, E. and S. Markovitch. 2007. Comput-
ing Semantic Relatedness Using Wikipedia-based
Explicit Semantic Analysis. Proceedings of the
20th International Joint Conference on Artificial
Intelligence, pages 6?12.
Gabrilovich, E. 2002. The WordSimilarity-353 Test
Collection. Using Information Content to Evalu-
ate Semantic Similarity in a Taxonomy.
Lee, M.D., B. Pincombe, and M. Welsh. 2005. A
Comparison of Machine Measures of Text Docu-
ment Similarity with Human Judgments. In 27th
Annual Meeting of the Cognitive Science Society
(CogSci2005), pages 1254?1259.
Lyman, P. and H.R. Varian. 2003. How much
information? http://www2.sims.
berkeley.edu/research/projects/
how-much-info-2003/index.htm. Ac-
cessed: May, 2010.
Metzler, Donald and W. Bruce Croft. 2006. Beyond
bags of words: Modeling implicit user preferences
in information retrieval. AAAI?06: Proceedings of
the 21st National Conference on Artificial Intelli-
gence, pages 1646?1649.
Milne, David and Ian H. Witten. 2008. Learning to
link with wikipedia. CIKM ?08: Proceeding of the
17th ACM Conference on Information and Knowl-
edge Management, pages 509?518.
Preece, SE. 1982. Spreading Activation Network
Model for Information Retrieval. Ph.D. thesis.
Salton, G. and M.J. McGill. 1983. Introduction to
Modern Information Retrieval. McGraw-Hill New
York.
Tran, T., P. Cimiano, S. Rudolph, and R. Studer.
2008. Ontology-based Interpretation of Keywords
for Semantic Search. The Semantic Web, pages
523?536.
Witten, I.H. and E. Frank. 2005. Data Min-
ing: Practical Machine Learning Tools and Tech-
niques. Morgan Kaufmann.
Witten, I.H. and D. Milne. 2008. An Effective, Low-
Cost Measure of Semantic Relatedness Obtained
From Wikipedia Links. In Proceeding of AAAI
Workshop on Wikipedia and Artificial Intelligence:
an Evolving Synergy, AAAI Press, Chicago, USA,
pages 25?30.
Yeh, E., D. Ramage, C.D. Manning, E. Agirre, and
A. Soroa. 2009. WikiWalk: Random walks on
Wikipedia for semantic relatedness. In Proceed-
ings of the 2009 Workshop on Graph-based Meth-
ods for Natural Language Processing, pages 41?
49. Association for Computational Linguistics.
54
Proceedings of the Workshop on Language in Social Media (LSM 2011), pages 20?29,
Portland, Oregon, 23 June 2011. c?2011 Association for Computational Linguistics
Contextual Bearing on Linguistic Variation in Social Media
Stephan Gouws?, Donald Metzler, Congxing Cai and Eduard Hovy
{gouws, metzler, ccai, hovy}@isi.edu
USC Information Sciences Institute
Marina del Rey, CA
90292, USA
Abstract
Microtexts, like SMS messages, Twitter posts,
and Facebook status updates, are a popular
medium for real-time communication. In this
paper, we investigate the writing conventions
that different groups of users use to express
themselves in microtexts. Our empirical study
investigates properties of lexical transforma-
tions as observed within Twitter microtexts.
The study reveals that different populations of
users exhibit different amounts of shortened
English terms and different shortening styles.
The results reveal valuable insights into how
human language technologies can be effec-
tively applied to microtexts.
1 Introduction
Microtexts, like SMS messages, Twitter posts, and
Facebook status updates, are becoming a popular
medium for real-time communication in the modern
digital age. The ubiquitous nature of mobile phones,
tablets, and other Internet-enabled consumer devices
provide users with the ability to express what is
on their mind nearly anywhere and at just about
any time. Since such texts have the potential to
provide unique perspectives on human experiences,
they have recently become the focus of many studies
within the natural language processing and informa-
tion retrieval research communities.
The informal nature of microtexts allows users
to invent ad hoc writing conventions that suit their
?This work was done while the first author was a visiting stu-
dent at ISI from the MIH Media Lab at Stellenbosch University,
South Africa. Correspondence may alternatively be directed to
stephan@ml.sun.ac.za.
particular needs. These needs strongly depend on
various user contexts, such as their age, geographic
location, how they want to be outwardly perceived,
and so on. Hence, social factors influence the way
that users express themselves in microtexts and other
forms of media.
In addition to social influences, there are also us-
ability and interface issues that may affect the way a
user communicates using microtexts. For example,
the Twitter microblog service imposes an explicit
message length limit of 140 characters. Users of
such services also often send messages using mobile
devices. There may be high input costs associated
with using mobile phone keypads, thus directly im-
pacting the nature of how users express themselves.
In this paper, we look specifically at understand-
ing the writing conventions that different groups
of users use to express themselves. This is ac-
complished by carrying out a novel empirical in-
vestigation of the lexical transformation character-
istics observed within Twitter microtexts. Our em-
pirical evaluation includes: (i) an analysis of how
frequently different user populations apply lexical
transformations, and (ii) a study of the types of
transformations commonly employed by different
populations of users. We investigate several ways of
defining user populations (e.g., based on the Twitter
client, time zone, etc.). Our results suggest that not
all microtexts are created equal, and that certain pop-
ulations of users are much more likely to use certain
types of lexical transformations than others.
This paper has two primary contributions. First,
we present a novel methodology for contextualized
analysis of lexical transformations found within mi-
20
crotexts. The methodology leverages recent ad-
vances in automated techniques for cleaning noisy
text. This approach enables us to study the fre-
quency and types of transformations that are com-
mon within different user populations and user con-
texts. Second, we present results from an empirical
evaluation over microtexts collected from the Twit-
ter microblog service. Our empirical analysis re-
veals that within Twitter microtexts, different user
populations and user contexts give rise to different
forms of expression, by way of different styles of
lexical transformations.
The remainder of this paper is laid out as follows.
Section 2 describes related work, while Section 3
motivates our investigation. Our multi-pronged
methodology for analyzing lexical transformations
is described in Section 4. Section 5 describes our
experimental results. Finally, Section 6 concludes
the paper and describes possible directions for fu-
ture work.
2 Related Work
Although our work is primarily focused on analyz-
ing the lexical variation in language found in on-
line social media, our analysis methodology makes
strong use of techniques for normalizing ?noisy text?
such as SMS-messages and Twitter messages into
standard English.
Normalizing text can traditionally be approached
using three well-known NLP metaphors, namely
that of spell-checking, machine translation (MT) and
automatic speech recognition (ASR) (Kobus et al,
2008).
In the spell-checking approach, corrections from
?noisy? words to ?clean? words proceed on a word-
by-word basis. Choudhury (2007) implements
the noisy channel model (Shannon and Weaver,
1948) using a hidden Markov model to handle both
graphemic and phonemic variations, and Cook and
Stevenson (2009) improve on this model by adapt-
ing the channel noise according to several predefined
word formations such as stylistic variation, word
clipping, etc. However, spelling correction is tra-
ditionally conducted in media with relatively high
percentages of well-formed text where one can per-
form word boundary detection and thus tokenization
to a high degree of accuracy. The main drawback is
the strong confidence this approach places on word
boundaries (Beaufort et al, 2010), since detecting
word boundaries in noisy text is not a trivial prob-
lem.
In the machine translation approach (Bangalore
et al, 2002; Aw et al, 2006), normalizing noisy
text is considered as a translation task from a source
language (the noisy text) to a target language (the
cleansed text). Since noisy- and clean text typically
vary wildly, it satisfies the notion of translating be-
tween two languages. However, since these trans-
formations can be highly creative, they usually need
a wide context (more than one word) to be resolved
adequately. Kobus (2008) also points out that de-
spite the fairly good results achieved with this sys-
tem, such a purely phrase-based translation model
cannot adequately handle the wide level of lexical
creativity found in these media.
Finally, the ASR approach is based on the ob-
servation that many noisy word forms in SMSes
or other noisy text are based on phonetic plays of
the clean word. This approach starts by convert-
ing the input message into a phone lattice, which
is converted to a word lattice using a phoneme-
grapheme dictionary. Finally the word lattice is de-
coded by applying a language model to the word lat-
tice and using a best-path algorithm to recover the
most likely original word sequence. This approach
has the advantage of being able to handle badly seg-
mented word boundaries efficiently, however it pre-
vents the next normalization steps from knowing
what graphemes were in the initial sequence (Kobus
et al, 2008).
What fundamentally separates the noisy text
cleansing task from the spell-checking problem is
that most often lexical ill-formedness in these me-
dia is intentional. Han (2011) proposes that this
might be in an attempt to save characters in length-
constrained media (such as Twitter or SMS), for
social identity (conversing in the dialect of a spe-
cific group), or due to convention of the medium.
Emotional context is typically expressed with re-
peat characters such as ?I am sooooooo tired? or
excessive punctuation. At times, however, out-
of-vocabulary tokens (spelling errors) might result
purely as the result of cognitive oversight.
Cook and Stevenson (2009) are one of the first to
explicitly analyze the types of transformations found
21
in short message domains. They identify: 1) stylis-
tic variation (better?betta), 2) subsequence abbre-
viation (doing?dng), 3) clipping of the letter ?g?
(talking?talkin), 4) clipping of ?h? (hello?ello),
and 5) general syllable clipping (anyway?neway),
to be the most frequent transformations. Cook and
Stevenson then incorporate these transformations
into their model. The idea is that such an unsuper-
vised approach based on the linguistic properties of
creative word forms has the potential to be adapted
for normalization in other similar genres without the
cost of developing a large training corpus. Most im-
portantly, they find that many creative texting forms
are the result of a small number of specific word for-
mation processes.
Han (2011) performs a simple analysis on the out-
of-vocabulary words found in Twitter, and find that
the majority of ill-formed words in Twitter can be
attributed to instances where letters are missing or
where there are extraneous letters, but the lexical
correspondence to the target word is trivially acces-
sible. They find that most ill-formed words are based
on morphophonemic variations.
3 Motivation
All of the previous work described in Section 2 ei-
ther
i) only focus on recovering the most likely ?stan-
dard English? form of a message, disregarding
the stylistic structure of the original noisy text,
or
ii) considers the structure of the noisy text found
in a medium as a whole, only as a first step
(the means) to identify common types of noisy
transformations which can subsequently be ac-
counted for (or ?corrected?) to produce normal-
ized messages (the desired end result).
However, based on the fact that language is highly
contextual, we ask the question: What influence
does the context in which a message is produced
have on the resulting observed surface structure and
style of the message?
In general, since some topics are for instance
more formal or informal than others, vocabulary and
linguistic style often changes based on the topic that
is being discussed. Moreover, in social media one
can identify several other types of context. Specif-
ically in Twitter, one might consider a user?s geo-
graphical location, the client from which a user is
broadcasting her message, how long she has been
using the Twitter service, and so forth.
The intuition is that the unconstrained nature of
these media afford users the ability to invent writing
conventions to suit their needs. Since users? needs
depend on their circumstances, and hence their con-
text, we hypothesize that the observed writing sys-
tems might be influenced by some elements of their
context. For instance, phonemic writing systems
might be related to a user?s dialect which is re-
lated to a user?s geographical location. Furthermore,
highly compressed writing conventions (throwing
away vowels, using prefixes of words, etc.) might
result from the relatively high input cost associ-
ated with using unwieldy keypads on some mobile
clients, etc.
The present work is focused on looking at these
stylistic elements of messages found in social media,
by analyzing the types of stylistic variation at the
lexical level, across these contextual dimensions.
4 Method
In the following discussion we make a distinc-
tion between within-tweet context and the general
message-context in which a message is created.
Within-tweet context is the linguistic context (the
other terms) that envelopes a term in a Twitter mes-
sage. The general context of a Twitter message is the
observable elements of the environment in which it
was conceived. For the current study, we record
1. the user?s location, and
2. the client from which the message was sent,
We follow a two-pronged analytic approach:
Firstly, we conduct a na??ve, context-free analysis
(at the linguistic level) of all words not commonly
found in standard, everyday English. This analy-
sis purely looks at the terminology that are found
on Twitter, and does not attempt to normalize these
messages in any way. Therefore, different surface
forms of the same word, such as ?today?, ?2day?,
?2d4y?, are all considered distinct terms. We then
analyse the terminology over different contextual di-
mensions such as client and location.
22
Secondly, we perform a more in-depth and con-
textual analysis (at the word level) by first normaliz-
ing the potentially noisy message to recover the most
likely surface form of the message and recording the
types of changes that were made, and then analyz-
ing these types of changes across different general
contextual dimensions (client and location).
As noted in Section 2, text message normalization
is not a trivial process. As shown by Han (2011),
most transformations from in-vocabulary words to
out-of-vocabulary words can be attributed to a single
letter that is changed, removed, or added. Further-
more, they note that most ill-formed words are re-
lated to some morphophonemic variation. We there-
fore implemented a text cleanser based on the de-
sign of Contractor (2010) using pre-processing tech-
niques discussed in (Kaufmann and Kalita, 2010).
It works as follows: For each input message, we
replace @-usernames with ?*USR*? and urls with
?*URL*?. Hash tags can either be part of the sen-
tence (?just got a #droid today?) or be peripheral to
the sentence (?what a loooong day! #wasted?). Fol-
lowing Kaufmann (2010) we remove hashtags at the
end of messages when they are preceded by typical
end-of-sentence punctuation marks. Hash tags in the
middle of messages are retained, and the hash sign
removed.
Next we tokenize this preprocessed message us-
ing the NLTK tokenizer (Loper and Bird, 2002). As
noted earlier, standard NLP tools do not perform
well on noisy text out-of-the-box. Based on inspec-
tion of incorrectly tokenized output, we therefore in-
clude a post-tokenization phase where we split all
tokens that include a punctuation symbol into the in-
dividual one or two alphanumeric tokens (on either
side of the punctuation symbol), and the punctuation
symbol1. This heuristic catches most cases of run-on
sentences.
Given a set of input tokens, we process these one
by one, by comparing each token to the words in
the lexicon L and constructing a confusion network
CN. Each in-vocabulary term, punctuation token or
other valid-but-not-in-vocabulary term is added to
CN with probability 1.0 as shown in Algorithm 1.
1This is easily accomplished using a regular expression
group-substitution of the form (\w*)([P])(\w*)?[\1,
\2, \3], where \w represents the set of alphanumeric char-
acters, and P is the set of all punctuation marks [.,;?". . .]
Character Transliteration candidates
1 ?1?, ?l?, ?one?
2 ?2?, ?to?, ?too?, ?two?
3 ?3?, ?e?, ?three?
4 ?4?, ?a?, ?for?, ?four?
5 ?5?, ?s?, ?five?
6 ?6?, ?b?, ?six?
7 ?7?, ?t?, ?seven?
8 ?8?, ?ate?, ?eight?
9 ?9?, ?g?, ?nine?
0 ?0?, ?o?, ?zero?
?@? ?@?, ?at?
?&? ?&?, ?and?
Table 1: Transliteration lookup table.
valid tok(wi) checks for ?*USR*?, ?*URL*?, or
any token longer than 1 character with no alphabet-
ical characters. This heuristic retains tokens such as
?9-11?, ?12:44?, etc.
At this stage, all out-of-vocabulary (OOV) terms
represent the terms that we are uncertain about, and
hence candidate terms for cleansing. First, for each
OOV term, we enumerate each possibly ambiguous
character into all its possible interpretations with the
transliteration table shown in Table 1. This expands,
for example, ?t0day?? [?t0day?, ?today?], and also
?2day?? [?2day?, ?twoday?, ?today?], etc.
Each transliterated candidate word in each con-
fusion set produced this way is then scored with
the original word and ranked using the heuristic
function (sim()) described in (Contractor et al,
2010)2. We also evaluated a purely phonetic edit-
distance similarity function, based on the Double
Metaphone algorithm (Philips, 2000), but found the
string-similarity-based function to give more reli-
able results.
Each confusion set produced this way (see Al-
gorithm 2) is joined to its previous set to form a
growing confusion lattice. Finally this lattice is de-
coded by converting it into the probabilistic finite-
state grammar format, and by using the SRI-LM
toolkit?s (Stolcke, 2002) lattice-tool com-
mand to find the best path through the lattice by
2The longest common subsequence between the two words,
normalized by the edit distances between their consonant skele-
tons.
23
Transformation Type Rel %
single char (?see?? ?c?) 29.1%
suffix (?why?? ?y?) 18.8%
drop vowels (?be?? ?b?) 16.4%
prefix (?tomorrow?? ?tom?) 9.0%
you to u (?you?? ?u?) 8.3%
drop last char (?running?? ?runnin?) 7.0%
repeat letter (?so?? ?soooo?) 5.5%
contraction (?you will?? ?you?ll?) 5.0%
th to d (?this?? ?dis?) 1.0%
Table 2: Most frequently observed types of transforma-
tions with an example in parentheses. Rel % shows the
relative percentage of the top-10 transformations which
were identified (excluding unidentified transformations)
to belong to a specific class.
making use of a language model to promote fluid-
ity in the text, and trained as follows:
We generated a corpus containing roughly 10M
tokens of clean English tweets. We used a simple
heuristic for selecting clean tweets: For each tweet
we computed if #(OOV )#(IV )+1 < ?, where ? = 0.5
was found to give good results. On this corpus
we trained a trigram language model, using Good-
Turing smoothing. Next, a subset of the LA Times
containing 30M words was used to train a ?general
English? language model in the same way. These
two models were combined3 in the ratio 0.7 to 0.3.
The result of the decoding process is the hypoth-
esized clean tokens of the original sentence. When-
ever the cleanser makes a substitution, it is recorded
for further analysis. Upon closer inspection, it was
found that most transformation types can be recog-
nized by using a fairly simple post-processing step.
Table 2 lists the most frequent types of transforma-
tions. While these transformations do not have per-
fect coverage, they account for over 90% of the (cor-
rect) transformations produced by the cleanser. The
rules fail to cover relatively infrequent edge cases,
such as ?l8r ? later?, ?cuz ? because?, ?dha ?
the?, and ?yep? yes? 4.
3Using the -mix-lm and -lambda and -mix-lambda2
options to the SRI-LM toolkit?s ngram module.
4To our surprise these ?typical texting forms? disappeared
into the long tail in our data set.
Original Cleansed
Swet baby jeebus, some-
one PLEASE WINE ME!
sweet baby jesus , some-
one please wine me !
2 years with Katie today! two years with katie to-
day!
k,hope nobody was
hurt.gud mornin jare
okay , hope nobody was
hurt . good morning jamie
When u a bum but think u
da best person on da court
you doodooforthebooboo
when you a bum but think
you the best person on the
court you dorothy
NYC premiere 2morrow. nice premiere tomorrow .
Table 3: Examples of original and automatically cleansed
versions of Twitter messages.
Algorithm 1 Main cleanser algorithm pseudo code.
The decode() command converts the confusion
network (CN) into PFSG format and decodes it us-
ing the lattice-tool of the SRI-LM toolkit.
Require: Lexicon L, Punctuation set P
function CLEANSE MAIN(Min)
for wi ?Min do
if wi ? L ? P or valid tok(wi) then
Add (1.0, wi) to CNout . Probability 1.0
else
Add conf set(wi) to CNout
end if
end for
return decode(CNout)
end function
Table 3 illustrates some example corrections
made by the cleanser. As the results show, the
cleanser is able to correct many of the more com-
mon types of transformations, but can fail when it
encounters infrequent or out-of-vocabulary terms.
5 Evaluation
This section describes our empirical evaluation and
analysis of how users in different contexts express
themselves differently using microtexts. We focus
specifically on the types of lexical transformations
that are commonly applied globally, within popula-
tions of users, and in a contextualized manner.
24
Algorithm 2 Algorithm pseudo code for generating
confusion set CS. L[wi] is the lexicon partitioning
function for word wi.
Require: Lexicon L, confusion set CS, implemented as
top-K heap containing (si, wi), indexed on si
function CONF SET(wi)
W? translits(wi)
for wj ?W do
for wk ? L[wj ] do
sk ? sim(wj , wk)
if sk > min(CS) then
Add (sk, wk) to CS
end if
end for
end for
return CS
end function
5.1 Out-of-Vocabulary Analysis
We begin by analyzing the types of terms that are
common in microtexts but not typically used in
proper, everyday English texts (such as newspapers).
We refer to such terms as being out-of-vocabulary,
since they are not part of the common written En-
glish lexicon. The goal of this analysis is to un-
derstand how different contexts affect the number
of out-of-vocabulary terms found in microtexts. We
hypothesize that certain contextual factors may in-
fluence a user?s ability (or interest) to formulate
clean microtexts that only contain common English
terms.
We ran our analysis over a collection of one mil-
lion Twitter messages collected using the Twitter
streaming API during 2010. Tweets gathered from
the Twitter API are tagged with a language identifier
that indicates the language a user has chosen for his
or her account. However, we found that many tweets
purported to be English were in fact not. Hence,
we ran all of the tweets gathered through a simple
English language classifier that was trained using a
small set of manually labeled tweets, uses character
trigrams and average word length as features, and
achieves an accuracy of around 93%. The every-
day written English lexicon, which we treat as the
?gold standard? lexicon, was distilled from the same
collection of LA Times news articles described in
Section 4. This yielded a comprehensive lexicon of
approximately half a million terms.
Timezone % In-Vocabulary
Australia 86%
UK 85%
US (Atlantic) 84%
Hong Kong 83%
US (Pacific) 81%
Hawaii 81%
Overall 81%
Table 4: Percentage of in-vocabulary found in large En-
glish lexicon for different geographic locations.
For each tweet, the tokenized terms were looked
up in the LA Times lexicon to determine if the
term was out-of-vocabulary or not. Not surprisingly,
the most frequent out-of-vocabulary terms identi-
fied are Twitter usernames, URLs, hasthags, and RT
(the terminology for a re-broadcast, or re-tweeted,
message). These tokens alone account for approx-
imately half of all out-of-vocabulary tokens. The
most frequent out-of-vocabulary terms include ?lol?,
?haha?, ?gonna?, ?lmao?, ?wanna?, ?omg?, ?gotta?.
Numerous expletives also appear amongst the most
common out-of-vocabulary terms, since such terms
never appear in the LA Times. Out of vocabulary
terms make up 19% of all terms in our data set.
In the remainder of this section, we examine
the out-of-vocabulary properties of different popu-
lations of users based on their geographic location
and their client (e.g., Web-based or mobile phone-
based).
5.1.1 Geographic Locations
To analyze the out-of-vocabulary properties of
users in different geographic locations, we extracted
the time zone information from each Tweet in our
data set. Although Twitter allows users to specify
their location, many users leave this field blank, use
informal terminology (?lower east side?), or fabri-
cate non-existent locations (e.g., ?wherever i want
to be?). Therefore, we use the user?s time zone as
a proxy for their actual location, in hopes that users
have less incentive to provide incorrect information.
For the Twitter messages associated with a given
time zone, we computed the percentage of tokens
found within our LA Times-based lexicon. The re-
sults from this analysis are provided in Table 4. It is
25
Client % In-Vocabulary
Facebook 88%
Twitter for iPhone 84%
Twitter for Blackberry 83%
Web 82%
UberTwitter 78%
Snaptu 73%
Overall 81%
Table 5: Percentage of in-vocabulary found in large En-
glish lexicon for different Twitter clients.
important to note that these results were computed
over hundreds of thousands of tokens, and hence
the variance of our estimates is very small. This
means that the differences observed here are statis-
tically meaningful, even though the absolute differ-
ences tend to be somewhat small.
These results indicate that microtexts composed
by users in different geographic locations exhibit
different amounts of out-of-vocabulary terms. Users
in Australia, the United Kingdom, Hong Kong, and
the East Coast of the United States (e.g., New York
City) include fewer out-of-vocabulary terms in their
Tweets than average. However, users from the West
Coast of the United States (e.g., Los Angeles, CA)
and Hawaii are on-par with the overall average, but
include 5% more out-of-vocabulary terms than the
Australian users.
As expected, the locations with fewer-than-
average in-vocabulary tokens are associated with
non-English speaking countries, despite the output
from the classifier.
5.1.2 Twitter Clients
In a similar experiment, we also investigated the
frequency of out-of-vocabulary terms conditioned
on the Twitter client (or ?source?) used to compose
the message. Example Twitter clients include the
Web-based client at www.twitter.com, official
Twitter clients for specific mobile platforms (e.g.,
iPhone, Android, etc.), and third-party clients. Each
client has its own characteristics, target user base,
and features.
In Table 5, we show the percentage of in-
vocabulary terms for a sample of the most widely
used Twitter clients. Unlike the geographic location-
based analysis, which showed only minor differ-
ences amongst the user populations, we see much
more dramatic differences here. Some clients, such
as Facebook, which provides a way of cross-posting
status updates between the two services, has the
largest percentage of in-vocabulary terms of the ma-
jor clients in our data.
One interesting, but unexpected, finding is that the
mobile phone (i.e., iPhone and Blackberry) clients
have fewer out-of-vocabulary terms, on average,
than the Web-based client. This suggests that ei-
ther the users of the clients are less likely to misspell
words or use slang terminology or that the clients
may have better or more intuitive spell checking ca-
pabilities. A more thorough analysis is necessary to
better understand the root cause of this phenomenon.
At the other end of the spectrum are the UberTwit-
ter and Snaptu clients, which exhibit a substantially
larger number of out-of-vocabulary terms. These
clients are also typically used on mobile devices. As
with our previous analysis, it is difficult to pinpoint
the exact cause of such behavior, but we hypothe-
size that it is a function of user demographics and
difficulties associated with inputting text on mobile
devices.
5.2 Contextual Analysis
In this section, we test the hypothesis that different
user populations make use of different types of lex-
ical transformations. To achieve this goal, we make
use of our noisy text cleanser. For each Twitter mes-
sage run through the cleanser, we record the origi-
nal and cleaned version of each term. For all of the
terms that the cleanser corrects, we automatically
identify which (if any) of the transformation rules
listed in Table 2 explain the transformation between
the original and clean version of the term. We use
this output to analyze the distribution of transforma-
tions observed across different user populations.
We begin by analyzing the types of transforma-
tions observed across Twitter clients. Figure 1 plots
the (normalized) distribution of lexical transforma-
tions observed for the Web, Twitter for Blackberry,
Twitter for iPhone, and UberTwitter clients, grouped
by the transformations. We also group the trans-
formations by the individual clients in Figure 2 for
more direct comparison.
The results show that Web users tend to use more
26
Figure 1: Proportion of transformations observed across
Twitter clients, grouped by transformation type.
contractions than Blackberry and UberTwitter users.
We relate this result to the differences in typing on
a virtual compared to a multi-touch keypad. It was
surprising to see that iPhone users tended to use con-
siderably more contractions than the other mobile
device clients, which we relate to its word-prediction
functionality. Another interesting result is the fact
that Web users often drop vowels to shorten terms
more than their mobile client counterparts. Instead,
mobile users often use suffix-style transformations
more, which is often more aggressive than the drop-
ping vowels transformation, and possibly a result of
the pervasiveness of mobile phones: Large popu-
lations of people?s first interaction with technology
these days are through a mobile phone, a device
where strict length limits are imposed on texting,
and which hence enforce habits of aggressive lex-
ical compression, which might transfer directly to
their use of PCs. Finally, we observe that mobile de-
vice users replace ?you? with ?u? substantially more
than users of the Web client.
We also performed the same analysis across time
zones/locations. The results are presented in Fig-
ure 3 by transformation-type, and again grouped by
location for direct comparison in Figure 4. We ob-
serve, perhaps not surprisingly, that the East Coast
US, West Coast US, and Hawaii are the most similar
with respect to the types of transformations that they
Figure 2: Proportion of transformations observed across
Twitter clients, grouped by client.
commonly use. However, the most interesting find-
ing here is that British users tend to utilize a notice-
ably different set of transformations than American
users in the Pacific time zones. For example, British
users are much more likely to use contractions and
suffixes, but far less likely to drop the last letter of
a word, drop all of the vowels in a word, use prefix-
style transformations, or to repeat a given letter mul-
tiple times. In a certain sense, this suggests that
British users tend to write more proper, less informal
English and make use of strikingly different styles
for shortening words compared to American users.
This might be related to the differences in dialects
between the two regions manifesting itself during a
process of phonetic transliteration when composing
the messages: Inhabitants of the south-west regions
in the US are known for pronouncing for instance
running as runnin?, which manifests as dropping the
last letter, and so forth.
Therefore, when taken with our out-of-vocabulary
analysis, our experimental evaluation shows clear
evidence that different populations of users express
themselves differently online and use different types
of lexical transformations depending on their con-
text. It is our hope that the outcome of this study
will spark further investigation into these types of
issues and ultimately lead to effective contextually-
aware natural language processing and information
retrieval approaches that can adapt to a wide range
of user contexts.
27
Figure 3: Proportion of transformations observed across
geographic locations, grouped by transformation type.
6 Conclusions and Future Work
This paper investigated the writing conventions that
different groups of users use to express themselves
in microtexts. We analyzed characteristics of terms
that are commonly found in English Twitter mes-
sages but are never seen within a large collection
of LA Times news articles. The results showed
that a very small number of terms account for a
large proportion of the out-of-vocabulary terms. The
same analysis revealed that different populations of
users exhibit different propensities to use out-of-
vocabulary terms. For example, it was found that
British users tend to use fewer out-of-vocabulary
terms compared to users within the United States.
We also carried out a contextualized analysis that
leveraged a state-of-the-art noisy text cleanser. By
analyzing the most common types of lexical trans-
formations, it was observed that the types of trans-
formations used varied across Twitter clients (e.g.,
Web-based clients vs. mobile phone-based clients)
and geographic location. This evidence supported
our hypothesis that the measurable contextual indi-
cators surrounding messages in social media play an
important role in determining how messages in these
media vary at the surface (lexical) level from what
might be considered standard English.
The outcome of our empirical evaluation and
subsequent analysis suggests that human language
Figure 4: Proportion of transformations observed across
geographic locations, grouped by location.
technologies (especially natural language process-
ing techniques that rely on well-formed inputs) are
likely to be highly susceptible to failure as the result
of lexical transformations across nearly all popula-
tions and contexts. However, certain simple rules
can be used to clean up a large number of out-of-
vocabulary tokens. Unfortunately, such rules would
not be able to properly correct the long tail of
the out-of-vocabulary distribution. In such cases,
more sophisticated approaches, such as the noisy
text cleanser used in this work, are necessary to
combat the noise. Interestingly, most of the lexical
transformations observed affect non-content words,
which means that most information retrieval tech-
niques will be unaffected by such transformations.
As part of future work, we are generally interested
in developing population and/or context-aware lan-
guage processing and understanding techniques on
top of microtexts. We are also interested in ana-
lyzing different user contexts, such as those based
on age and gender and to empirically quantify the
effect of noise on actual natural language process-
ing and information retrieval tasks, such as part of
speech tagging, parsing, summarization, etc.
Acknowledgments
We would like to thank the anonymous reviewers for
their insightful comments. Stephan Gouws would
like to thank MIH Holdings Ltd. for financial sup-
port during the course of this work.
28
References
A.T. Aw, M. Zhang, J. Xiao, and J. Su. 2006. A Phrase-
based Statistical Model for SMS Text Normalization.
In Proceedings of the COLING/ACL Main Conference
Poster Sessions, pages 33?40. Association for Compu-
tational Linguistics.
S. Bangalore, V. Murdock, and G. Riccardi. 2002. Boot-
strapping Bilingual Data Using Consensus Transla-
tion for a Multilingual Instant Messaging System. In
Proceedings of the 19th International Conference on
Computational Linguistics-Volume 1, pages 1?7. As-
sociation for Computational Linguistics.
R. Beaufort, S. Roekhaut, L.A. Cougnon, and C. Fa-
iron. 2010. A Hybrid Rule/Model-based Finite-State
Framework for Normalizing SMS Messages. In Pro-
ceedings of the 48th Annual Meeting of the Association
for Computational Linguistics, pages 770?779. Asso-
ciation for Computational Linguistics.
M. Choudhury, R. Saraf, V. Jain, A. Mukherjee, S. Sarkar,
and A. Basu. 2007. Investigation and Modeling of the
Structure of Texting Language. International Journal
on Document Analysis and Recognition, 10(3):157?
174.
D. Contractor, T.A. Faruquie, and L.V. Subramaniam.
2010. Unsupervised Cleansing of Noisy Text. In
Proceedings of the 23rd International Conference on
Computational Linguistics: Posters, pages 189?196.
Association for Computational Linguistics.
P. Cook and S. Stevenson. 2009. An Unsupervised
Model for Text Message Normalization. In Proceed-
ings of the Workshop on Computational Approaches
to Linguistic Creativity, pages 71?78. Association for
Computational Linguistics.
Bo Han and Timothy Baldwin. 2011. Lexical Normal-
isation of Short Text Messages: Makn Sens a #twit-
ter. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies. Association for Compu-
tational Linguistics.
M. Kaufmann and J. Kalita. 2010. Syntactic Normaliza-
tion of Twitter Messages.
C. Kobus, F. Yvon, and G. Damnati. 2008. Normaliz-
ing SMS: Are Two Metaphors Better Than One? In
Proceedings of the 22nd International Conference on
Computational Linguistics-Volume 1, pages 441?448.
Association for Computational Linguistics.
E. Loper and S. Bird. 2002. NLTK: The Natural Lan-
guage Toolkit. In Proceedings of the ACL-02 Work-
shop on Effective tools and Methodologies for Teach-
ing Natural Language Processing and Computational
Linguistics-Volume 1, pages 63?70. Association for
Computational Linguistics.
L. Philips. 2000. The Double Metaphone Search Algo-
rithm. CC Plus Plus Users Journal, 18(6):38?43.
C.E. Shannon and W. Weaver. 1948. The Mathemati-
cal Theory of Communication. Bell System Technical
Journal, 27:623?656.
A. Stolcke. 2002. SRILM - An Extensible Language
Modeling Toolkit. In Proceedings of the Interna-
tional Conference on Spoken Language Processing,
volume 2, pages 901?904.
29
Proceedings of EMNLP 2011, Conference on Empirical Methods in Natural Language Processing, pages 82?90,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Unsupervised Mining of Lexical Variants from Noisy Text
Stephan Gouws?, Dirk Hovy and Donald Metzler
stephan@ml.sun.ac.za, {dirkh, metzler}@isi.edu
USC Information Sciences Institute
Marina del Rey, CA
90292, USA
Abstract
The amount of data produced in user-
generated content continues to grow at a stag-
gering rate. However, the text found in these
media can deviate wildly from the standard
rules of orthography, syntax and even seman-
tics and present significant problems to down-
stream applications which make use of this
noisy data. In this paper we present a novel
unsupervised method for extracting domain-
specific lexical variants given a large volume
of text. We demonstrate the utility of this
method by applying it to normalize text mes-
sages found in the online social media service,
Twitter, into their most likely standard English
versions. Our method yields a 20% reduction
in word error rate over an existing state-of-the-
art approach.
1 Introduction
The amount of data produced in user-generated con-
tent, e.g. in online social media, and from machine-
generated sources such as optical character recog-
nition (OCR) and automatic speech recognition
(ASR), surpasses that found in more traditional me-
dia by orders of magnitude and continues to grow
at a staggering rate. However, the text found in
these media can deviate wildly from the standard
rules of orthography, syntax and even semantics and
present significant problems to downstream applica-
tions which make use of this ?noisy? data. In social
?This work was done while the first author was a visiting
student at ISI from the MIH Media Lab at Stellenbosch Univer-
sity, South Africa.
media this noise might result from the need for so-
cial identity, simple spelling errors due to high in-
put cost associated with the device (e.g. typing on
a mobile phone), space constraints imposed by the
specific medium or even a user?s location (Gouws et
al., 2011). In machine-generated texts, noise might
result from imperfect inputs, imperfect conversion
algorithms, or various degrees of each.
Recently, several works have looked at the pro-
cess of normalizing these ?noisy? types of text into
more standard English, or in other words, to convert
the various forms of idiosyncratic spelling and writ-
ing errors found in these media into what would nor-
mally be considered standard English orthography.
Many of these works rely on supervised methods
which share the common burden of requiring train-
ing data in the form of noisy input and clean output
pairs. The problem with developing large amounts
of annotated training data is that it is costly and re-
quires annotators with sufficient expertise. However,
the volume of data that is available in these media
makes this a suitable domain for applying semi- and
even fully unsupervised methods.
One interesting observation is that these noisy
out-of-vocabulary (OOV) words are typically
formed through some semi-deterministic process
which doesn?t render them completely indiscernible
at a lexical level from the original words they are
meant to represent. We therefore refer to these OOV
tokens as lexical variants of the clean in-vocabulary
(IV) tokens they are derived from. For instance,
in social media ?2morrow? ?2morow? and ?2mrw?
still share at least some lexical resemblance with
?tomorrow?, due to the fact that it is mainly the
82
Figure 1: A plot of the OOV distribution found in Twit-
ter. Also indicated is the potential for using (OOV,most-
likely-IV) training pairs found on this curve for either
exception dictionary entries (the most frequent pairs),
or for learning lexical transformations (the long tail).
The threshold between the two (vertical bar) is domain-
specific.
result of a phonetic transliteration procedure. Also,
?computer? and ?conpu7er? share strong lexical
overlap, and might be the result of noise in the OCR
process.
As with many aspects of NLP, the distribution of
these OOV tokens resemble a power law distribution
(see Figure 1 for the OOV distribution in Twitter).
Thus, some words are commonly converted to some
OOV representation (e.g. domain-specific abbrevia-
tions in social media, or words which are commonly
incorrectly detected in OCR) and these account for
most of the errors, with the rest making up the long
tail. If one could somehow automatically extract a
list of all the domain-specific OOV tokens found in
a collection of texts, along with the most likely clean
word (or words) each represents, then this could play
a key role in for instance normalizing individual
messages. Very frequent (noisy, clean) pairs at the
head of the distribution could be used for extracting
common domain-specific abbreviations, and word-
pairs in the long tail may be used as input to learn-
ing algorithms for automatically learning the types
of transformations found in these media, as shown
in Figure 1.
For example, taking Twitter as our target domain,
examples for learning common exception pairs may
include ?gf ???girlfriend?. For learning types of lex-
ical transformations, one might learn from ?think-
ing???thinkin? and ?walking???walkin? that ?ng?
could go to ?n? (known as ?g-clipping?).
In this paper we present a novel unsupervised
method for extracting an approximation to such a
domain-specific list of (noisy, clean) pairs, given
only a large volume of representative text. We fur-
thermore demonstrate the utility of this method by
applying it to normalize text messages found in the
online social media service, Twitter, into their most
likely standard English versions.
The primary contributions of this paper are:
? We present an unsupervised method that mines
(noisy, clean) pairs and requires only large
amounts of domain-specific noisy data
? We demonstrate the utility of this method by in-
corporating it into a standard method for noisy
text normalization, which results in a signifi-
cant reduction in the word error rate compared
to the original method.
2 Training Pair Mining
Given a large corpus of noisy text, our challenge is to
automatically mine pairs of domain-specific lexical
variants that can be used as training data for a va-
riety of natural language processing tasks. The key
challenge is how to develop an effective approach
that is both domain-specific and robust to noisy cor-
pora. Our proposed approach requires nothing more
than a large ?common English? corpus (e.g., a large
newswire corpus) and a large corpus of domain text
(e.g., a large corpus of Twitter data, a query log,
OCR output, etc.). Using these two sources of ev-
idence, the approach mines domain-specific lexical
variants in a fully unsupervised manner.
Before describing the details of our approach, we
first describe the characteristics that we would like
the mined lexical variants to have. First, the variants
should be semantically related to each other. Pairs
of words that are lexically similar, but semantically
unrelated are not of particular interest since such
pairs can be found using basic edit distance-based
approaches. Second, the variants should be domain-
specific. Variants that capture common English lexi-
cal variations (e.g., ?running? and ?run?) can be cap-
tured using standard normalization procedures, such
83
Figure 2: Flow chart illustrating our procedure for mining
pairs of lexical variants.
as stemming. Instead, we are interested in identify-
ing domain-specific variations (e.g., ?u? and ?you?
in the SMS and Twitter domains) that cannot eas-
ily be handled by existing approaches. Finally, the
variants should be lexically similar, by definition.
Hence, ideal variants will be domain-specific, lex-
ically similar, and semantically related.
To mine such variants we synthesize ideas from
natural language processing and large-scale text
mining to derive a novel mining procedure. Our pro-
cedure can be divided into three atomic steps. First
we identify semantically similar pairs, then we filter
out common English variants, and finally we rescore
the resulting list based on lexical similarity (see Fig-
ure 2). The remainder of this section describes the
complete details of each of these steps.
2.1 Identifying Semantically Similar Pairs
The first step of our mining procedure harvests se-
mantically similar pairs of terms from both the com-
mon English corpus and the domain corpus. There
are many different ways to measure semantic relat-
edness. In this work, we use distributional similar-
ity as our measure of semantic similarity. However,
since we are taking a fully unsupervised approach,
we do not know a priori which pairs of terms may
be related to each other. Hence, we must compute
the semantic similarity between all possible pairs of
terms within the lexicon. To solve this computa-
tionally challenging task, we use a large-scale all-
pairs distributional similarity approach similar to the
one originally proposed by Pasca and Dienes (Pasca
and Dienes, 2005). Our implementation, which
makes use of Hadoop?s MapReduce distributed pro-
gramming paradigm, can efficiently compute all-
pairs distributional similarity over very large corpora
(e.g., the Twitter pairs we use later were mined from
a corpus of half a billion Twitter messages).
Using a similar strategy as Pasca and Dienes, we
define term contexts as the bigrams that appear to
the left and to the right of a given word (Pasca and
Dienes, 2005). Following standard practice, the con-
textual vectors are weighted according to pointwise
mutual information and the similarity between the
vectors is computed using the cosine similarity met-
ric (Lin and Pantel, 2001; Bhagat and Ravichandran,
2008). It is important to note that there are many
other possible ways to compute distributional and
semantic similarity, and that just about any approach
can be used within our framework. The approach
used here was chosen because we had an existing
implementation. Indeed, other approaches may be
more apt for other data sets and tasks.
This approach is applied to both the common En-
glish corpus and the domain corpus. This yields two
sets of semantically (distributionally) similar word
pairs that will ultimately be used to distill unsuper-
vised lexical variants.
2.2 Filtering Common English Variants
Given these two sets of semantically similar word
pairs, the next step in our procedure is designed to
identify the domain-specific pairs by filtering out the
common English variants. The procedure that we
follow is very simple, yet highly effective. Given
the semantically similar word pairs harvested from
the domain corpus, we eliminate all of the pairs that
are also found in the semantically similar common
English pairs.
Any type of ?common English? corpus can be
used for this purpose, depending on the task. How-
ever, we found that a large corpus of newswire ar-
ticles tends to work well. Most of the semanti-
cally similar word pairs harvested from such a cor-
pus are common lexical variants and synonyms. By
eliminating these common variants from the har-
vested domain corpus pairs, we are left with only
the domain-specific semantically similar word pairs.
2.3 Lexical Similarity-Based Re-ordering
The first step of our mining procedure identified
semantically similar term pairs using distributional
similarity, while the second identified those that
were domain-specific by filtering out common En-
glish variants. The third, and final, step of our pro-
cedure re-orders the output of the second step to ac-
count for lexical similarity.
For each word pair (from the second step of our
procedure), we compute two scores: 1) a seman-
84
tic similarity score, and 2) a lexical similarity score.
The final score of the pair is then simply the prod-
uct of the two scores. In this work, we use the
cosine similarity score as our semantic similarity
score, since it is already computed during the first
step of our procedure.
In the social media domain, as in the mobile tex-
ting domain, compressed writing schemes typically
involve deleting characters or replacing one or more
characters with some other characters. For example,
users might delete vowels (?tomorrow???tmrrw?),
or replace ?ph? with its phonetic equivalent ?f ?,
as in ?phone???fone?. We make use of a subse-
quence similarity function (Lodhi et al, 2002) which
can still capture the structural overlap (in the form
of string subsequences) between the remaining un-
changed letters in the noisy word and the original
clean word from which it was derived. In this work
we use a subsequence length of 2, but as with the
other steps in our procedure, this one is purpose-
fully defined in a general way. Any semantic sim-
ilarity score, lexical similarity score, and combina-
tion function can be used in practice.
The output of the entire procedure is a scored list
of word pairs that are semantically related, domain-
specific, and lexically similar, thereby exhibiting the
characteristics that we initially defined as important.
We treat these (scored) pairs as pseudo training data
that has been derived in a fully unsupervised manner.
We anticipate that these pairs will serve as powerful
training data for a variety of tasks, such as noisy text
normalization, which we will return to in Section 3.
2.4 Example and Error Analysis
As an illustrative example of this procedure in prac-
tice, Table 1 shows the actual output of our system
for each step of the mining procedure. To generate
this example, we used a corpus of 2GB of English
news articles as our ?common English? corpus and
a corpus of approximately 500 million Twitter mes-
sages as our domain corpus. In this way, our goal
is to identify Twitter-specific lexical variants, which
we will use in the next section to normalize noisy
Twitter messages.
Column (A) of the table shows that our distribu-
tional similarity approach is capable of identifying
a variety of semantically similar terms in the Twit-
ter corpus. However, the list contains a large num-
Rank Precision
P@50 0.90
P@100 0.88
Table 2: Precision at 50 and 100 of the induced exception
dictionary.
ber of common English variants that are not spe-
cific to Twitter. Column (B) shows the outcome of
eliminating all of the pairs that were found in the
newswire corpus. Many of the common pairs have
been eliminated and the list now contains mostly
Twitter-specific variants. Finally, Column (C) shows
the result of re-ordering the domain-specific pairs to
account for lexical similarity.
In our specific case, the output of step 1 yielded
a list of roughly 3.3M potential word variants. Fil-
tering out common English variants reduced this to
about 314K pairs. In order to estimate the quality of
the list we computed the precision at 50 and at 100
for which the results are shown in Table 2. Further-
more, we find that up to position 500 the pairs are
still of reasonable quality. Thereafter, the number of
errors start to increase noticeably. In particular, we
find that the most common types of errors are
1. Number-related: e.g. ?30? and ?30pm? (due to
incorrect tokenization), or ?5800? and ?5530?;
2. Lemma-related: e.g. ?incorrect? and ?incor-
rectly?; and
3. Negations: e.g. ?could? and ?couldnt?.
Performance can thus be improved by making
use of better tokenization, lemmatizing words, fil-
tering out common negations and filtering out pairs
of numbers.
Still, the resulting pairs satisfy all of our de-
sired qualities rather well, and hence we hypothesize
would serve as useful training data for a number of
different Twitter-related natural language processing
tasks. Indeed, we will now describe one such possi-
ble application and empirically validate the utility of
the automatically mined pairs.
85
(A) (B) (C)
i? you u? you ur? your
my? the seeking? seeks wit? with
u? you 2? to to? too
is? was lost? won goin? going
a? the q? que kno? know
i? we f*ck? hell about? bout
my? your feat? ft wat? what
and? but bday? birthday jus? just
seeking? seeks ff? followfriday talkin? talking
me? you yang? yg gettin? getting
2? to wit? with doin? doing
am? was a? my so? soo
are? were are? r you? your
lost? won amazing? awesome dnt? dont
he? she til? till bday? birthday
q? que fav? favorite nothin? nothing
it? that mostly? partly people? ppl
f*ck? hell northbound? southbound lil? little
can? could hung? toned sayin? saying
im? its love? miss so? sooo
Table 1: Column (A) shows the highest weighted distributionally similar terms harvested from a large Twitter corpus.
Column (B) shows which pairs from (A) remain after filtering out distributionally similar word pairs mined from a
large news corpus. Column (C) shows the effect of reordering the pairs from (B) using a string similarity kernel.
3 Deriving A Common Exception
Dictionary for Text Normalization as a
Use Case for Mining Lexical Variants
As discussed in Section 1, these training pairs may
aid methods which attempt to normalize noisy text
by translating from the ill-formed text into stan-
dard English. Since the OOV distribution in noisy
text mostly resemble a power law distribution (see
Figure 1), one may use the highest scoring train-
ing pairs to induce ?exception dictionaries? (lists of
(noisy word)?(most likely clean word)) of the most
common domain-specific abbreviations found in the
text.
We will demonstrate the utility of our derived
pairs in one specific use case, namely inducing a
domain-specific exception dictionary to augment a
vanilla normalization method. We leave the sec-
ond proposed use-case, namely using pairs in the
long tail for learning transformation rules, for future
work.
We evaluate the first use case in Section 4.
3.1 Baseline Normalization Method
We make use of a competitive heuristic text nor-
malization method over Twitter data as a baseline,
and compare its accuracy to an augmented method
which makes use of an automatically induced excep-
tion dictionary (using the method described in Sec-
tion 2) as a first step, before resorting to the same
baseline method as a ?back-off? for words not found
in the dictionary.
As we point out in Section 5, there are various
metaphors within which the noisy text normalization
problem has been approached. In general, however,
the problem of noisy text normalization may be ap-
proached by using a three step process (Gouws et al,
2011):
1. In the out-of-vocabulary (OOV) detection
step, we detect unknown words which are can-
didates for normalization
2. In the candidate selection step, we find the
weighted lists of most likely candidates (from
a list of in-vocabulary (IV) words) for the OOV
words and group them into a confusion set. The
86
confusion sets are then appended to one another
to create a confusion- network or lattice
3. Finally, in the decoding step, we use a lan-
guage model to rescore the confusion network,
and then find the most likely posterior path
(Viterbi path) through this network.
The words at each node in the resulting posterior
Viterbi path represents the words of the hypothe-
sized original clean sentence.
In this work, we reimplement the method de-
scribed in Contractor (2010) as our baseline method.
We next describe the details of this method in the
context of the framework presented above. See
(Gouws et al, 2011) for more details.
OOV DETECTION is a crucial part of the nor-
malizaton process, since false-positives will result
in undesirable attempts to ?correct? IV words, hence
bringing down the method?s accuracy. We imple-
ment OOV detection as a simple lexicon-lookup pro-
cedure, with heuristics for handling specific out-of-
vocabulary-but-valid tokens such as hash tags and
@usernames.
CANDIDATE SELECTION involves comparing
an unknown OOV word to a list of words which
are deemed in-vocabulary, and producing a top-K
ranked list with candidate words and their estimated
probabilities of relevance as output. This process re-
quires a function with which to compute the simi-
larity or alternatively, distance, between two words.
More traditional string-similarity functions like the
simple Lehvenshtein string edit distance do not fare
too well in this domain.
We implement the IBM-similarity (Contractor et
al., 2010) which employs a slightly more advanced
similarity function. It finds the length of the longest
common subsequence (LCS) between two strings s1
and s2, normalized by the edit distance (ED) be-
tween the consonants in each string (referred to as
the ?consonant skeleton? (CS)), thus
sim(s1, s2) =
LCS(s1, s2)
ED(CS(s1),CS(s2))
Finally, the DECODING step takes an input word
lattice (lattice of concatenated, weighted confusion
sets), and produces a new lattice by incorporating
the probabilities from an n-gram language model
with the prior probabilities in the lattice to produce a
reranked posterior lattice. The most likely (Viterbi)
path through this lattice represents the decoded clean
output. We use SRI-LM (Stolcke, 2002) for this.
3.2 Augmenting the Baseline: Our Method
In order to demonstrate the utility of the mined lex-
ical variant pairs, we first construct a (noisy, clean)
lookup table from the mined pairs. We (arbitrarily)
use the 50 mined pairs with the highest overall com-
bined score (see Section 2.3) for the exception dic-
tionary. For each pair, we map the OOV term (noisy
and typically shorter) to the IV term (clean and usu-
ally longer). The exception lookup list is then used
to augment the baseline method (see Section 3.1) in
the following way: When the method encounters a
new word, it first checks to see if the word is in the
exception dictionary. If it is, we normalize to the
value in the dictionary. If it is not, we pass the ill-
formed word to the baseline method to proceed as
normal.
4 Evaluation
4.1 Dataset
We make use of the Twitter dataset discussed in
Han (2011). It consists of a random sampling of 549
English tweets, annotated by three independent an-
notators. All OOV words were pre-identified and the
annotators were requested to determine the standard
form (gold standard) for each ill-formed word.
4.2 Evaluation Metrics
In this study, we are interested in measuring the
quality of our mined training pairs by evaluating its
utility on an external task: Using the training pairs
to induce a (noisy?clean) exception dictionary to
augment the working of a standard noisy text nor-
malization system. Hence, our focus is entirely on
the accuracy of the candidate selection procedure as
defined in Section 3.1. We compute this accuracy
in terms of the word error rate (WER), defined as
the number of token substitutions, insertions or dele-
tions one has to make to turn the system output into
the gold standard, normalized by the total number of
tokens in the output. In order to remove the possi-
ble bias introduced by our very basic OOV-detection
87
Method WER % Change
Naive baseline 10.7% ?
IBM-baseline 7.8% ?27.1%
Our method 5.6% ?47.7%
Table 3: Word error rate (WER, lower is better) results
of our method against a naive baseline and the much
stronger IBM-baseline (Contractor et al, 2010). We also
show the relative change in WER for our method and the
IBM-baseline compared to the naive baseline.
mechanism, we evaluate the output of all systems
only on the oracle pairs. Oracle pairs are defined as
the (input,system-output,gold) pairs where input and
gold do not match. In other words, we remove the
possible confounding impact of imperfect OOV de-
tection on the accuracy of the normalization process
by assuming a perfect OOV-detection step.
4.3 Discussion of Results
The results of our experiments are displayed in Ta-
ble 3. It is important to note that the focus is not
on achieving the best WER compared to other sys-
tems (although we achieve very competitive scores),
but to evaluate the added utility of integrating an
exception dictionary which is based purely on the
mined (noisy, clean) pairs with an already competi-
tive baseline method.
The ?naive baseline? shows the results if we make
no changes to the input tokens for all oracle pairs.
Therefore it reflects the total level of errors that are
present in the corpus.
The IBM-method is seen to reduce the amount of
errors by a substantial 27.1%. However, the aug-
mented method results in a further 20.6% reduction
in errors, for a total reduction of 47.7% of all er-
rors in the dataset, compared to the IBM-baseline?s
27.1%.
Since we replace matches in the dictionary indis-
criminately, and since the dictionary comprise those
pairs that typically occur most frequently in the cor-
pus from which they were mined, it is important to
note that if these pairs are of poor quality, then their
sheer frequency will drive the overall system accu-
racy down. Therefore, the accuracy of these pairs
are strongly reflected in the WER performance of
the augmented method.
Noisy Clean % Oracle Pairs
u you 8.7
n and 1.4
ppl people 1
da the 1
w with 0.7
cuz because 0.5
y why 0.5
yu you 0.5
lil little 0.5
dat that 0.5
wat what 0.4
tha the 0.4
kno know 0.4
r are 0.4
Table 4: Error analysis for all (noisy, clean) normaliza-
tions missed by the vanilla IBM-baseline method, but in-
cluded in the top-50 pairs used for constructing the ex-
ception dictionary. We also show the percentage of all
oracle pairs that are corrected by including each pair in
an exception dictionary.
Table 4 shows the errors missed by the IBM-
baseline, but contained in the mined exception dic-
tionary. We also show each pair?s frequency of oc-
currence in the oracle pairs (hence its contribution
towards lowering WER).
5 Related work
To the best of our knowledge, we are the first to ad-
dress the problem of mining pairs of lexical variants
from noisy text in an unsupervised and purely sta-
tistical manner that does not require aligned noisy
and clean messages. To obtain aligned clean and
noisy text without annotated data implies the use
of some normalizing method first. Yvon (2010)
presents one such approach, where they generate ex-
ception dictionaries from their finite-state system?s
normalized output. However, their method is still
trained on annotated training pairs, and hence su-
pervised. A related direction is ?transliteration min-
ing? (Jiampojamarn et al, 2010) which aims to au-
tomatically obtain bilingual lists of names written in
different scripts. They also employ string-similarity
measures to find similar string pairs written in differ-
ent scripts. However, their input data is constrained
88
to Wikipedia articles written in different languages,
whereas we impose no constrains on our input data,
and merely require a large collection thereof.
Noisy text normalization, on the other hand, has
recently received a lot of focus. Most works con-
strue the problem in the metaphors of either ma-
chine translation (MT) (Bangalore et al, 2002;
Aw et al, 2006; Kaufmann and Kalita, 2010),
spelling correction (Choudhury et al, 2007; Cook
and Stevenson, 2009), or automated speech recog-
nition (ASR) (Kobus et al, 2008). For our evalua-
tion, we developed an implementation of Contrac-
tor (2010) which works on the same general ap-
proach as Han (2011).
6 Conclusions and Future Work
The ability to automatically extract lexical variants
from large noisy corpora has many practical appli-
cations, including noisy text normalization, query
spelling suggestion, fixing OCR errors, and so on.
This paper developed a novel methodology for au-
tomatically mining such pairs from a large domain-
specific corpus. The approach makes use of distri-
butional similarity for measuring semantic similar-
ity, a novel approach for filtering common English
pairs by comparing against pairs mined from a large
news corpus, and a substring similarity measure for
re-ordering the pairs according to their lexical simi-
larity.
To demonstrate the utility of the method, we used
automatically mined pairs to construct an unsuper-
vised exception dictionary, that was used in con-
junction with a string similarity measure, to form
a highly effective hybrid noisy text normalization
technique. By exploiting the properties of the power
law distribution, the exception dictionary can suc-
cessfully correct a large number of cases, while the
heuristic string similarity-based approach handled
many of the less common test cases from the tail of
the distribution. The hybrid approach showed sub-
stantial reductions in WER (around 20%) versus the
string similarity approach, hence validating our pro-
posed approach.
For future work we are interested in exploiting the
(noisy, clean) pairs contained in the long tail as input
to learning algorithms for acquiring domain-specific
lexical transformations.
Acknowledgments
Stephan Gouws would like to thank MIH Holdings
Ltd. for financial support during the course of this
work.
References
A.T. Aw, M. Zhang, J. Xiao, and J. Su. 2006. A phrase-
based statistical model for SMS text normalization. In
Proceedings of the COLING/ACL on Main conference
poster sessions, pages 33?40. Association for Compu-
tational Linguistics.
S. Bangalore, V. Murdock, and G. Riccardi. 2002.
Bootstrapping bilingual data using consensus transla-
tion for a multilingual instant messaging system. In
Proceedings of the 19th International Conference on
Computational Linguistics Volume 1, pages 1?7. As-
sociation for Computational Linguistics.
Rahul Bhagat and Deepak Ravichandran. 2008. Large
scale acquisition of paraphrases for learning surface
patterns. In Proceedings of ACL-08: HLT, pages 674?
682, Columbus, Ohio, June. Association for Computa-
tional Linguistics.
M. Choudhury, R. Saraf, V. Jain, A. Mukherjee, S. Sarkar,
and A. Basu. 2007. Investigation and modeling of the
structure of texting language. International Journal on
Document Analysis and Recognition, 10(3):157?174.
D. Contractor, T.A. Faruquie, and L.V. Subramaniam.
2010. Unsupervised cleansing of noisy text. In
Proceedings of the 23rd International Conference on
Computational Linguistics: Posters, pages 189?196.
Association for Computational Linguistics.
P. Cook and S. Stevenson. 2009. An unsupervised model
for text message normalization. In Proceedings of the
Workshop on Computational Approaches to Linguis-
tic Creativity, pages 71?78. Association for Computa-
tional Linguistics.
S. Gouws, D. Metzler, C. Cai, and E. Hovy. 2011. Con-
textual Bearing on Linguistic Variation in Social Me-
dia. In Proceedings of the ACL-11 Workshop on Lan-
guage in Social Media. Association for Computational
Linguistics.
Bo Han and Timothy Baldwin. 2011. Lexical Normal-
isation of Short Text Messages: Makn Sens a #twit-
ter. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies. Association for Compu-
tational Linguistics.
S. Jiampojamarn, K. Dwyer, S. Bergsma, A. Bhargava,
Q. Dou, M.Y. Kim, and G. Kondrak. 2010. Translit-
eration generation and mining with limited training
89
resources. In Proceedings of the 2010 Named Enti-
ties Workshop, pages 39?47. Association for Compu-
tational Linguistics.
M. Kaufmann and J. Kalita. 2010. Syntactic Normaliza-
tion of Twitter Messages. In International Conference
on Natural Language Processing, Kharagpur, India.
C. Kobus, F. Yvon, and G. Damnati. 2008. Normal-
izing SMS: are two metaphors better than one? In
Proceedings of the 22nd International Conference on
Computational Linguistics-Volume 1, pages 441?448.
Association for Computational Linguistics.
Dekang Lin and Patrick Pantel. 2001. Discovery of in-
ference rules for question-answering. Nat. Lang. Eng.,
7:343?360, December.
H. Lodhi, C. Saunders, J. Shawe-Taylor, N. Cristianini,
and C. Watkins. 2002. Text classification using string
kernels. The Journal of Machine Learning Research,
2:419?444.
Marius Pasca and Pter Dienes. 2005. Aligning needles
in a haystack: Paraphrase acquisition across the web.
In Robert Dale, Kam-Fai Wong, Jian Su, and Oi Yee
Kwong, editors, Natural Language Processing IJC-
NLP 2005, volume 3651 of Lecture Notes in Computer
Science, pages 119?130. Springer Berlin / Heidelberg.
A. Stolcke. 2002. SRILM-an extensible language mod-
eling toolkit. In Proceedings of the International Con-
ference on Spoken Language Processing, volume 2,
pages 901?904. Citeseer.
F. Yvon. 2010. Rewriting the orthography of sms mes-
sages. Journal of Natural Language Engineering,
16(02):133?159.
90
