Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 217?224
Manchester, August 2008
Efficient Parsing with the Product-Free Lambek Calculus
Timothy A. D. Fowler
Department of Computer Science
University of Toronto
10 King?s College Road, Toronto, ON, M5S 3G4, Canada
tfowler@cs.toronto.edu
Abstract
This paper provides a parsing algorithm
for the Lambek calculus which is polyno-
mial time for a more general fragment of
the Lambek calculus than any previously
known algorithm. The algorithm runs in
worst-case time O(n5) when restricted to
a certain fragment of the Lambek calcu-
lus which is motivated by empirical anal-
ysis. In addition, a set of parameterized
inputs are given, showing why the algo-
rithm has exponential worst-case running
time for the Lambek calculus in general.
1 Introduction
A wide variety of grammar formalisms have been
explored in the past for parsing natural language
sentences. The most prominent of these for-
malisms has been context free grammars (CFGs)
but a collection of formalisms known as categorial
grammar (CG) (Ajdukiewicz, 1935; Dowty et al,
1981; Steedman, 2000) has received interest be-
cause of some significant advantages over CFGs.
First, CG is inherently lexicalized due to the fact
that all of the variation between grammars is cap-
tured by the lexicon. This is a result of the rich
categories which CG uses in its lexicon to specify
the functor-argument relationships between lexical
items. A distinct advantage of this lexicalization
is that the processing of sentences depends upon
only those categories contained in the string and
not some global set of rules. Second, CG has
the advantage that it centrally adopts the princi-
ple of compositionality, as outlined in Montague
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
grammar (Montague, 1974), allowing the semantic
derivation to exactly parallel the syntactic deriva-
tion. This leads to a semantical form which is eas-
ily extractable from the syntactic parse.
A large number of CG formalisms have been
introduced including, among others, the Lambek
calculus (Lambek, 1958) and Combinatory Cat-
egorial Grammar (CCG) (Steedman, 2000). Of
these, CCG has received the most zealous com-
putational attention. Impressive results have been
achieved culminating in the state-of-the-art parser
of Clark and Curran (2004) which has been used as
the parser for the Pascal Rich Textual Entailment
Challenge entry of Bos and Markert (2005). The
appeal of CCG can be attributed to the existence of
efficient parsing algorithms for it and the fact that
it recognizes a mildly context-sensitive language
class (Joshi et al, 1989), a language class more
powerful than the context free languages (CFLs)
that has been argued to be necessary for natural
language syntax. The Lambek calculus provides
an ideal contrast between CCG and CFGs by be-
ing a CG formalism like CCG but by recognizing
the CFLs like CFGs (Pentus, 1997).
The primary goal of this paper is to provide
an algorithm for parsing with the Lambek calcu-
lus and to sketch its correctness. Furthermore, a
time bound of O(n5) will be shown for this algo-
rithm when restricted to product-free categories of
bounded order (see section 2 for a definition). The
restriction to bounded order is not a significant re-
striction, due to the fact that categories in CCG-
bank1 (Hockenmaier, 2003), a CCG corpus, have a
maximum order of 5 and an average order of 0.78
by token. In addition to the presentation of the al-
gorithm, we will provide a parameterized set of in-
1Although CCGbank was built for CCG, we believe that
transforming it into a Lambek calculus bank is feasible.
217
puts (of unbounded order) on which the algorithm
has exponential running time.
The variant of the Lambek calculus considered
here is the product-free Lambek calculus chosen
for three reasons. First, it is the foundation of
all other non-associative variants of the Lambek
calculus including the original Lambek calculus
(Lambek, 1958) and the multi-modal Lambek cal-
culus (Moortgat, 1996). Second, the calculus with
product is NP-complete (Pentus, 2006), while the
sequent derivability in the product-free fragment
is still unknown. Finally, the only connectives in-
cluded are / and \, which are the same connectives
as in CCG, providing a corpus for future work such
as building a probabilistic Lambek calculus parser.
2 Problem specification
Parsing with the Lambek calculus is treated as a
logical derivation problem. First, the words of a
sentence are assigned categories which are built
from basic categories (e.g. NP and S ) and the
connectives \ and /. For example, the category for
transitive verbs is (NP\S )/NP and the category
for adverbs is (S/NP)\(S/NP)2. Intuitively, the
\ and / operators specify the arguments of a word
and the direction in which those arguments need to
be found. Next, the sequent is built by combining
the sequence of the categories for the words with
the ? symbol and the sentence category (e.g. S ).
Strictly speaking, this paper only considers the
parsing of categories without considering multi-
ple lexical entries per word. However, using tech-
niques such as supertagging, the results presented
here yield an efficient method for the broader prob-
lem of parsing sentences. Therefore, we can take
the size of the input n to be the number of basic
categories in the sequent.
A parse tree for the sentence corresponds to a
proof of its sequent and is restricted to rules fol-
lowing the templates in figure 1. In figure 1, lower-
case Greek letters represent categories and upper-
case Greek letters represent sequences of cate-
gories. A proof for the sentence ?Who loves him??
is given in figure 2.
The version of the Lambek calculus presented
above is known as the product-free Lambek calcu-
lus allowing empty premises and will be denoted
by L. In addition, we will consider the fragment
L
k
, obtained by restricting L to categories of order
bounded by k. The order of a category, which can
2We use Ajdukiewicz notation, not Steedman notation.
? ? ?
? ? ? ??? ? ?
???\?? ? ?
?? ? ?
? ? ?\?
? ? ? ??? ? ?
??/??? ? ?
?? ? ?
? ? ?/?
Figure 1: The sequent presentation of L.
NP ? NP
S ? S
NP ? NP S ? S
NP NP\S ? S
NP\S ? NP\S
S/(NP\S) NP\S ? S
S/(NP\S) (NP\S)/NP NP ? S
Who loves him
Figure 2: A derivation for ?Who loves him??.
be viewed as the depth of the nesting of argument
implications, is defined as:
o(?) = 0 for ? a basic category
o(?/?) = o(?\?) = max(o(?), o(?) + 1)
For example, o((NP\S)/NP) = 1 and o((S/
NP)\(S/NP )) = 2.
3 Related work
Two other papers have provided algorithms similar
to the one presented here.
Carpenter and Morrill (2005) provided a graph
representation and a dynamic programming algo-
rithm for parsing in the Lambek calculus with
product. However, due to there use of the Lam-
bek calculus with product and to their choice of
correctness conditions, they did not obtain a poly-
nomial time algorithm for any significant fragment
of the calculus.
Aarts (1994) provided an algorithm for L2
which is not correct for L. Ours is polynomial time
for Lk, for any constant k, and is correct for L, al-
beit in exponential running time.
A number of authors have provided polynomial
time algorithms for parsing with CCG which gives
some insight into how good our bound of O(n5)
is. In particular, Vijay-Shanker and Weir (1994)
provided a chart parsing algorithm for CCG with a
time bound of O(n6).
4 An algorithm for parsing with L
This section presents a chart parsing algorithm
similar to CYK where entries in the chart are arcs
annotated with graphs. The graphs will be referred
218
to as abstract term graphs (ATGs) since they are
graph representations of abstractions over seman-
tic terms. ATGs will be presented in this section
by construction. See section 5 for their connection
to the proof structures of Roorda (1991).
The algorithm consists of two steps. First, the
base case is computed by building the base ATG
B and determining the set of surface variables by
using the proof frames of Roorda (1991). Second,
the chart is filled in iteratively according to the al-
gorithms specified in the appendix. The details for
these two steps can be found in sections 4.1 and
4.2, respectively. Section 4.3 introduces a proce-
dure for culling extraneous ATGs which is nec-
essary for the polynomial time proof and section
4.4 discusses recovery of proofs from the packed
chart. An example of the algorithm is given in fig-
ure 3.
For parsing with L, the input is a sequent and for
parsing with Lk, the input is a sequent with cate-
gories whose order is bounded by k. Upon com-
pletion, the algorithm outputs ?YES? if there is an
arc from 0 to n? 1 and ?NO? otherwise.
4.1 Computing the base case
Computing the base case consists of building the
proof frame and then translating it into a graph,
the base ATG B.
4.1.1 Building the proof frame
Proof frames are the part of the theory of proof
nets which we need to build the base ATG. The
proof frame for a sequent is a structure built on top
of the categories of the sentence. To build the proof
frame, all categories in the sequent are assigned
a polarity and labelled by a fresh variable. Cate-
gories to the left of ? are assigned negative polarity
and the category to the right of ? is assigned pos-
itive polarity. Then, the four decomposition rules
shown in table 1 are used to build a tree-like struc-
ture (see figure 3). The decomposition rules are
read from bottom to top and show how to decom-
pose a category based on its main connective and
polarity. In table 1, d is the label of the category
being decomposed, f , g and h are fresh variables
and order of premises is important.
The bottom of the proof frame consists of the
original sequent?s categories with labels and po-
larities. These are called terminal formulae. The
top of the proof frame consists of basic categories
with labels and polarities. These are called the ax-
iomatic formulae. In addition, we will distinguish
?
+
: f ?
?
: df
?\?
?
: d
?
+
: h ?
?
: g
?\?
+
: d
?
?
: df ?
+
: f
?/?
?
: d
?
?
: g ?
+
: h
?/?
+
: d
Table 1: The proof frame decomposition rules.
the leftmost variable in the label of each axiomatic
formula as its surface variable. See figure 3 for an
example.
4.1.2 Building the Base ATG
The base ATG B is built from the proof frame in
the following way. The vertices of the base ATG
are the surface variables plus a new special ver-
tex ? . The edges of ATGs come in two forms:
Labelled and unlabeled, specified as ?s, d, l? and
?s, d?, respectively, where s is the source, d is the
destination and l, where present, is the label.
To define the edge set of B, we need the follow-
ing:
Definition. For a variable u that labels a positive
category in a proof frame, the axiomatic reflection,
?(u), is the unique surface variable v such that on
the upward path from u and v in the proof frame,
there is no formula of negative polarity. For exam-
ple, in figure 3, ?(b) = c.
The edgeset E of the base ATG is as follows:
1. ?m,?(p
i
)? ? E for 1 ? i ? k where
mp
1
. . . p
k
appears as the label of some nega-
tive axiomatic formula
2. ??, ?(t)? ? E where t is the label of the posi-
tive terminal formula
3. For each rule with a positive conclusion,
negative premise labelled by g and positive
premise labelled by h, ??(h), g, g? ? E
A labeled edge in an ATG specifies that its
source must eventually connect to its destination
to complete a path corresponding to its label. For
example, G
1
contains the edge ?c, e, d? which in-
dicates that to complete the path from c to d, we
must connect c to e. In contrast, an unlabeled edge
in an ATG specifies that its source is already con-
nected to its destination. For example, in figure 3,
G
3
contains the edge ?a, f? which indicates that
there is some path, over previously deleted nodes,
which connects a to f .
219
0 1 2 3 4 5 6 7
a c d g e f h i
S
?
: ab S
+
: c NP
?
: d
NP\S
+
: b
S/(NP\S)
?
: a
NP
+
: g S
?
: efg
NP\S
?
: ef
NP
+
: f
(NP\S)/NP
?
: e
NP
?
: h S
+
: i
Who loves him ?
?
G
6
=
?
i
a
G
5
=
?
i
a
f
h
G
3
=
?
g
a c
d
d
G
4
=
?
i
a c e
f
d
h
G
1
=
?
i
a c
d
d
e
g
G
2
=
B =
?
i
a c
d
d
e
f
g
h
B B B B B B B
Sentence
Proof
Frame
Surface
Variables
Chart
Figure 3: The algorithm?s final state on the sequent S/(NP\S) (NP\S)/NP NP ? S.
Note that all nodes in an ATG have unlabeled
in-degree of either 0 or 1 and that the vertices of
an ATG are the surface variables found outside its
arc.
4.2 Filling in the chart
Once the base ATG and the sequence of surface
variables is determined, we can begin filling in the
chart. The term entry refers to the collection of
arcs beginning and ending at the same nodes of the
chart. An arc?s length is the difference between
its beginning and end points, which is always odd.
Note that each entry in the example in figure 3 con-
tains only one arc. We will iterate across the en-
tries of the chart and at each entry, we will attempt
a Bracketing and a number of Adjoinings. If an at-
tempt results in a violation, no new ATG is inserted
into the chart. Otherwise, a new ATG is computed
and inserted at an appropriate entry.
Bracketing is an operation on a single ATG
where we attempt to extend its arc by connecting
two nodes with the same basic category and op-
posite polarity. For example, G
3
is the result of
bracketing G
1
. Adjoining, on the other hand, is an
operation on two adjacent ATGs where we attempt
to unify their ATGs into one larger ATG. For ex-
ample, G
5
is the result of adjoining G
3
and G
2
.
The chart filling process is described by algo-
rithm 1 in the appendix. The chart in figure 3 is
filled by the graphs G
1
, . . . , G
6
, in that order. A
walk through of the example is given in the re-
mainder of this section. Arcs of length 1 are treated
specially, since they are derived directly from the
base ATG. To show this, the base ATG is shown at
pseudo-nodes, labeled by Bs.
4.2.1 Inserting arcs of length 1
This section corresponds to lines 1-2 of algo-
rithm 1 in the appendix. For each arc from i to
i+1, we will attempt to bracket the base ATG from
axiomatic formula i to axiomatic formula i+ 1.
To follow our example, the first step is to con-
sider inserting an arc from 0 to 1 by bracketing B.
Bracketing causes a positive surface variable to be
connected to a negative surface variable and in this
case, a cycle from a to c and back to a is formed
resulting in the violation on line 12 of algorithm 2.
Therefore, no arc is inserted.
Then, the second step considers inserting an arc
from 1 to 2. However, axiomatic formula 1 has cat-
egory S and axiomatic formula 2 has category NP
which results in the violation on line 3 of algorithm
2 since they are not the same.
Next, we attempt to insert an arc from 2 to 3.
In this case, no violations occur meaning that we
can insert the arc. The intuition is that the ATG
for this arc is obtained by connecting g to d in
the base ATG. Since c must eventually connect
to d (c ?
d
d), and now g connects to d, the in-
degree constraint on ATG nodes requires that the
path connecting c to d pass through g. Further-
220
- -
?
i
a c e
f
d
h
?
i
a c e
f
h
?
i
a
f
h
Figure 4: The intuition for bracketing from c to e.
more, the only way to connect c to g is through e.
So c ?
d
e. Then, we delete d and g.
This procedure continues until we have consid-
ered all possible arcs of length 1.
4.2.2 Inserting arcs of length 3 and greater
Next, we iterate across graphs in the chart and
for each, consider whether its ATG can be brack-
eted with the axiomatic formulae on either side of
it and whether it can be adjoined with any of the
other graphs in the chart. This process closely re-
sembles CYK parsing as described on lines 3-10
of algorithm 1. The choice of shortest to longest
is important because part of the invariant of our
dynamic program is that all derivable ATGs on
shorter arcs have already been added.
Following our example, the first graph to be con-
sidered is G
1
. First, we attempt to bracket it from
axiomatic formulae 1 to 4. As before, this intu-
itively involves connecting c to e in the ATG for
this arc. This is allowed because no cycles are
formed and no labelled edges are prohibited from
eventually being connected. Then, as before, we
delete the vertices c and e and as a result connect
a to f , resulting in G
3
. The bracketing process is
illustrated in figure 4.
Next, we consider all graphs to which G
1
could
adjoin and there are none, since such graphs would
need to annotate arcs which either end at 1 or begin
at 4. After processing G
1
, we process G
2
, which
has a successful bracketing resulting in G
4
and no
successful adjoinings.
Next, we process G
3
. Bracketing it is prohib-
ited, as it would result in a cycle from a to f and
back to a. However, it is possible to adjoin G
3
with
G
2
, since they are adjacent.
The adjoining of two graphs can be viewed as a
kind of intersection of the two ATGs, in the sense
that we are combining the information in both
graphs to yield a single more concise graph. At-
tempting an adjoining involves traversing the two
graphs being adjoined and the base ATG in both a
forward and a backward direction as specified in
algorithms 4 and 5 in the appendix.
The intuition behind these traversals is to gen-
erate a picture of what the combination of the two
@
R
 

-
?
i
a
f
h
?
i
a c
d
d
e
g
?
i
a c e
f
g
h
d
?
i
a
Figure 5: The intuition for adjoining two ATGs.
graphs must look like as illustrated in figure 5. In
general, we can only reconstruct those parts of the
graph which are necessary for determining the re-
sultant ATG and no more. The dotted edges in-
dicate uncertainty about the edges present at this
stage of the algorithm. Adjoining G
2
and G
3
does
not fail and the resultant graph is G
5
.
Note that this example does not contain any
instances of two identical ATGs being inserted
multiple times into the chart which occurs often
in large examples yielding significant savings of
computation.
4.3 Culling of extraneous ATGs
It often happens that an entry in the chart contains
two ATGs such that if one of them is extendable
to a complete proof then the other necessarily is as
well. In this case, the former can be discarded. We
will outline such a method here that is important
for the polynomial time proof.
Definition. ATGs G
1
and G
2
are equivalent if
some surjection of edge labels to edge labels ap-
plied to the those of G
1
yields those of G
2
.
Then, if two ATGs in a chart are equivalent, one
can be discarded.
4.4 Recovering proofs from a packed chart
The algorithm as described above is a method for
answering the decision problem for sequent deriv-
ability in the Lambek calculus. However, we can
annotate the ATGs with the ATGs they are derived
from so that a complete set of Roorda-style proof
nets, and thus the proofs themselves, can be recov-
ered.
5 Correctness
Correctness of the algorithm is obtained by using
structural induction to prove the equivalence of the
constructive definition of ATGs outlined in section
4 and a definition based on semantic terms given
in this section:
221
S?
: ab
S
+
: c NP
?
: d
NP\S
+
: b
S/(NP\S)
?
: a
NP
+
: g S
?
: efg
NP\S
?
: ef NP
+
: f
(NP\S)/NP
?
: e NP
?
: h
S
+
: i
Figure 6: A proof structure for ?Who loves him??.
Definition. A partial proof structure is a proof
frame together with a matching of the axiomatic
formulae. A proof structure is a partial proof struc-
ture whose matching is complete.
An example is given in figure 6. Proof struc-
tures correspond to proofs under certain conditions
and our conditions will be based on the semantic
term of the proof given to us by the Curry-Howard
isomorphism for the Lambek calculus (Roorda,
1991). To do this, we interpret left rules as func-
tional application and right rules as functional ab-
straction of lambda terms. Under this interpreta-
tion, the semantic term obtained from the proof
structure in figure 6 is a?d.ehd.
As in Roorda (1991), proof structures corre-
spond to a proof if the semantic term assigned to
the sentence category is a well formed lambda term
which includes all the terms assigned to the words
of the sentence. Then, ATGs are graph represen-
tations of abstractions of the undetermined portion
of semantic terms of partial proof structures. Unla-
beled edges correspond to functional applications
whose arguments must still be determined and la-
belled edges correspond to functional abstractions
whose body does not yet contain an instance of the
abstracted variable. The violations which occur
during the execution of the algorithm correspond
to the various ways in which a lambda term can be
ill formed.
6 Asymptotic Running Time Complexity
In this section we provide proof sketches for the
runtime of the algorithm. Let f(n) be a bound
on the number of arcs occurring in an entry in the
chart where n is the number of axiomatic formu-
lae. Then, observe that the number of edges within
an ATG is O(n2) and the number of edges adja-
cent to a vertex is O(n), due to basic properties of
ATGs.
Then, it is not hard to prove that the worst case
running time of Bracketing is O(n2), which is
dominated by the for loops of lines 20-23 of al-
gorithm 2.
Next, with some effort, we can see that the worst
case running time of Adjoining is dominated by the
execution of the procedures Fore and Back. But,
since there are at most a linear number of labels l
and for each label l we need to visit each vertex in
G
1
and G
2
at most a constant number of times, the
worst case running time is O(n2).
Then, for each ATG, we attempt at most one
bracketing and adjoinings with at most 2n+1 other
entries for which there can be (2n+1)f(n) ATGs.
Therefore, each entry can be processed in worst
case time O(n3f(n)2).
Finally, there are O(n2) entries in the chart,
which means that the entire algorithm takes time
O(n
5
f(n)
2
) in the worst case. Sections 6.1 and
6.2 discuss the function f(n).
6.1 Runtime for Lk
By structural induction on the proof frame decom-
position rules and the base ATG building algo-
rithm, it can be proven that in Lk the length of the
longest path in the base ATG is bounded by k.
Next, consider a partition of the surface vari-
ables into a pair of sets such that the axiomatic
formulae corresponding to the surface variables
within each set are contiguous. For the example in
figure 3, one such pair of sets is S
1
= {a, c, d, g}
and S
2
= {e, f, h, i}. Then, given such a partition,
it can be proven that there is at least one maximal
path P in the base ATG such that all vertices in
one set that are adjacent to a vertex in the other
set are either in P or adjacent to some vertex in
P . For example, a maximal path for S
1
and S
2
is
P = e ? g.
An entry in the chart induces two such parti-
tions, one at the left edge of the entry and one at
the right edge. Therefore, we obtain two such max-
imal paths and for any ATG G in this entry and any
vertex v not in or adjacent to one of these paths, ei-
ther v is not in G or v has the same neighbourhood
in G as it has in the base ATG. Then, the number
of vertices adjacent to vertices in these paths can
be as many as n. However, if we put these vertices
into sets such that vertices in a set have identical
222
neighbourhoods, the number of sets is dependant
only on k.
In the worst case, the out-neighbourhood of one
of these sets can be any set of these sets. So, we
get a bound for f(n) to be O(k24k). Therefore,
because k is constant in Lk, f(n) is constant and
the running time of the algorithm for Lk is O(n5)
in the worst case.
6.2 Runtime for L
Despite the results of section 6.1, this algorithm
has an exponential running time for L. We demon-
strate this with the following set of parameterized
sequents:
F (1) = ((A/A)\A)\A
F (i) = ((A/(A/F
i?1
))\A)\A for i > 1
U(n) = F
n
F
n
? A\A
Theorem. There are (2n?1)!
n!(n?1)!
? ?(4
n
) distinct
arcs in the entry from n to 3n ? 1 in the chart for
U(n).
Proof. By induction and a mapping from the pos-
sible matchings to the possible permutations of a
sequence of length 2n ? 1 such that two subse-
quences of length n and n? 1 are in order.
7 Conclusions and Future Work
We have presented a novel algorithm for parsing in
the Lambek calculus, sketched its correctness and
shown that it is polynomial time in the bounded-
order case. Furthermore, we presented a set of pa-
rameterized sequents proving that the algorithm is
exponential time in the general case, which aids
future research in finding either a polynomial time
algorithm or an NP-completeness proof for L.
In addition, this algorithm provides another step
toward evaluating the Lambek calculus against
both CFGs (to evaluate the importance of Cate-
gorial Grammar) and CCG (to evaluate the impor-
tance of the mildly context-sensitive languages).
In the future, we plan on determining the run-
ning time of this algorithm on an actual corpus,
such as a modified version of CCGbank, and
then to empirically evaluate the Lambek calculus
for natural language processing. In addition, we
would like to investigate extending this algorithm
to more complex variants of the Lambek calculus
such as the multi-modal calculus using the proof
nets of Moot and Puite (2002).
Acknowledgments
Many thanks to Gerald Penn, for his insightful
comments and for guiding this research.
References
Aarts, Erik. 1994. Proving Theorems of the Second
Order Lambek Calculus in Polynomial Time. Studia
Logica, 53:373?387.
Ajdukiewicz, Kazimierz. 1935. Die syntaktische Kon-
nexitat. Studia Philosophica, 1(1-27).
Bos, Johan and Katja Markert. 2005. Recognising tex-
tual entailment with logical inference. Proceedings
of HLT and EMNLP, pages 628?635.
Carpenter, Bob and Glyn Morrill. 2005. Switch Graphs
for Parsing Type Logical Grammars. Proceedings of
IWPT ?05, Vancouver.
Clark, Steven and James R. Curran. 2004. Parsing the
WSJ using CCG and log-linear models. Proceedings
of ACL ?04, pages 104?111.
Dowty, David R., Robert E. Wall, and Stanley Peters.
1981. Introduction to Montague Semantics. Reidel.
Hockenmaier, Julia. 2003. Data and Models for Sta-
tistical Parsing with Combinatory Categorial Gram-
mar. Ph.D. thesis, University of Edinburgh.
Joshi, Aravind K., K. Vijay-Shanker, and David J. Weir.
1989. The Convergence of Mildly Context-sensitive
Grammar Formalisms. University of Pennsylvania.
Lambek, Joachim. 1958. The mathematics of sen-
tence structure. American Mathematical Monthly,
65:154?170.
Montague, Richard. 1974. Formal philosophy: se-
lected papers of Richard Montague. Yale University
Press New Haven.
Moortgat, Michael. 1996. Multimodal linguistic infer-
ence. Journal of Logic, Language and Information,
5(3):349?385.
Moot, Richard and Quintijn Puite. 2002. Proof Nets for
the Multimodal Lambek Calculus. Studia Logica,
71(3):415?442.
Pentus, Mati. 1997. Product-Free Lambek Calculus
and Context-Free Grammars. The Journal of Sym-
bolic Logic, 62(2):648?660.
Pentus, Mati. 2006. Lambek calculus is NP-complete.
Theoretical Computer Science, 357(1-3):186?201.
Roorda, Dirk. 1991. Resource Logics: Proof-
theoretical Investigations. Ph.D. thesis, Universiteit
van Amsterdam.
Steedman, Mark. 2000. The Syntactic Process. Brad-
ford Books.
223
Vijay-Shanker, K. and David J. Weir. 1994. Parsing
Some Constrained Grammar Formalisms. Computa-
tional Linguistics, 19(4):591?636.
Appendix. Algorithm Pseudocode
The term source set refers to the out-
neighbourhood of ? . The term minus variable
refers to surface variables obtained from negative
axiomatic formulae plus ? . X
i
refers to the ith
axiomatic formula.
Algorithm 1 Chart Iteration
1: for i = 0 to n? 1 do
2: Bracketing(B, X
i
, X
i+1
)
3: for l = 1, 3, 5, . . . to n? 1 do
4: for e = 0 to n? l ? 1 do
5: for each arc from e to e + l with ATG G do
6: Bracketing(G, X
e?1
to X
e+l+1
)
7: Adjoin G to ATGs from e? l ? 1 to e? 1
8: for al = 1, 3, ..., l ? 2 do
9: Adjoin G to ATGs from e? al ? 1 to e? 1
10: Adjoin G to ATGs from e+l+1 to e+l+al+1
Algorithm 2 Bracketing(G, X
i
, X
j
)
1: C
i
p
i
: l
i
= X
i
and C
j
p
j
: l
j
= X
j
2: if C
i
6= C
j
then
3: V iolation : Mismatched Basic Categories
4: if p
i
= p
j
then
5: V iolation : Mismatched Polarities
6: Let m,p ? {i, j} such that p
m
is negative and p
p
is
positive
7: if G is not from 1 to n? 1 and the source set of G is the
singleton l
p
and l
m
has out-degree 0 in G then
8: V iolation : Empty Source Set
9: if the edge ?l
m
, l
p
? ? G then
10: V iolation : Cycle Exists
11: if l
p
is in the source set of G and there exists an in-edge
of m with label l such that no edge from p to m has label
l and no edge from a vertex other than p to a vertex other
than m has label l then
12: V iolation : Path Completion Impossible
13: if m has out-degree 0 and and there exists an out-edge
of p with label l such that no edge from p to m has label
l and no edge from a vertex other than p to a vertex other
than m has label l then
14: V iolation : Path Completion Impossible
15: Copy G to yield H
16: for each edge ?l
p
, l
m
, l? ? G do
17: Delete all edges from H with label l
18: Delete l
m
, l
p
and all their incident edges from H
19: Let in
p
be the in-neighbour of l
p
in G
20: for each q in the out-neighbourhood of l
m
in G do
21: Insert ?in
p
, q? into H
22: for each edge ?p, d, l? in G do
23: Insert ?q, d, l? into H
24: for each edge ?q,m, l? in G do
25: Insert ?q, in
p
, l? into H
26: if H contains a cycle then
27: V iolation : Future Cycle Required
28: return H
Algorithm 3 Adjoining(G
1
, G
2
)
1: Let V
H
be the intersection of the vertices in G
1
and G
2
2: if V
H
6= ? and Fore(?,G
1
, G
2
) ? V
H
= ? then
3: V iolation : Empty Source Set
4: for each l such that l labels an edge in G
1
and G
2
do
5: Let ?p,m, l? be the unique edge labelled l in B
6: if Fore(p,G
1
, G
2
, l)?Back(m,G
1
, G
2
) = ? then
7: if Fore(p) ? V
H
= ? then
8: V iolation : Path Completion Impossible
9: if Back(m) ? V
H
= ? then
10: V iolation : Path Completion Impossible
11: Let H be the graph with vertex set V
H
and no edges
12: for each minus variable m ? V
H
do
13: for each p ? Fore(m,G
1
, G
2
, ?) do
14: Insert ?m,p? into H
15: for each l such that l labels an edge in G
1
and G
2
do
16: Let ?p,m, l? be the unique edge labelled l in B
17: if Fore(p,G
1
, G
2
, l)?Back(m,G
1
, G
2
) = ? then
18: for each q ? Fore(p,G
1
, G
2
, l) ? V
H
do
19: Insert ?q,Back(m,G
1
, G
2
) ? V
H
, l? into H
20: return H
Algorithm 4 Fore(v, G
1
, G
2
, l)
1: if v ? G
1
and v ? G
2
then
2: return {v}
3: else
4: if v is a minus vertex then
5: S = ?
i?{1,2}
Out-neighbourhood
G
i
v
6: else if v is a plus vertex then
7: Let j be such that v ? G
j
8: S = ?
e?Edges labelled bylSource of e
F = S
9: while S is not empty do
10: Remove any element u from S
11: Let m be the in-neighbour of u in B
12: if u does not appear in one of G
1
, G
2
and m does
not appear in the other then
13: Let i be such that m ? G
i
14: Let O be the out-neighbourhood of m in G
i
15: S = S ? O
16: F = F ? O
17: F = F ? {m}
18: return F
Algorithm 5 Back(m, G
1
, G
2
)
1: if m ? G
1
and m ? G
2
then
2: return {m}
3: else
4: Let i, j ? {1, 2} be such that m ? G
i
and m /? G
j
5: Let m? be the destination of the edges labelled by m
in G
j
6: M = {m,m?}
7: while m? /? G
1
and m? /? G
2
do
8: Let i?, j? ? {1, 2} be such that m? ? G
i
? and m? /?
G
j
?
9: Let p ? G
j
? be an out-neighbour of m? in B
10: Let m?? be the in-neighbour of p in G
j
?
11: m? = m??
12: M = M ? {m??}
13: return M
224
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 335?344,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Accurate Context-Free Parsing with Combinatory Categorial Grammar
Timothy A. D. Fowler and Gerald Penn
Department of Computer Science, University of Toronto
Toronto, ON, M5S 3G4, Canada
{tfowler, gpenn}@cs.toronto.edu
Abstract
The definition of combinatory categorial
grammar (CCG) in the literature varies
quite a bit from author to author. How-
ever, the differences between the defini-
tions are important in terms of the lan-
guage classes of each CCG. We prove
that a wide range of CCGs are strongly
context-free, including the CCG of CCG-
bank and of the parser of Clark and Cur-
ran (2007). In light of these new results,
we train the PCFG parser of Petrov and
Klein (2007) on CCGbank and achieve
state of the art results in supertagging ac-
curacy, PARSEVAL measures and depen-
dency accuracy.
1 Introduction
Combinatory categorial grammar (CCG) is a vari-
ant of categorial grammar which has attracted in-
terest for both theoretical and practical reasons.
On the theoretical side, we know that it is mildly
context-sensitive (Vijay-Shanker and Weir, 1994)
and that it can elegantly analyze a wide range of
linguistic phenomena (Steedman, 2000). On the
practical side, we have corpora with CCG deriva-
tions for each sentence (Hockenmaier and Steed-
man, 2007), a wide-coverage parser trained on that
corpus (Clark and Curran, 2007) and a system for
converting CCG derivations into semantic repre-
sentations (Bos et al, 2004).
However, despite being treated as a single uni-
fied grammar formalism, each of these authors use
variations of CCG which differ primarily on which
combinators are included in the grammar and the
restrictions that are put on them. These differences
are important because they affect whether the
mild context-sensitivity proof of Vijay-Shanker
and Weir (1994) applies. We will provide a gen-
eralized framework for CCG within which the full
variation of CCG seen in the literature can be de-
fined. Then, we prove that for a wide range of
CCGs there is a context-free grammar (CFG) that
has exactly the same derivations. Included in this
class of strongly context-free CCGs are a grammar
including all the derivations in CCGbank and the
grammar used in the Clark and Curran parser.
Due to this insight, we investigate the potential
of using tools from the probabilistic CFG com-
munity to improve CCG parsing results. The
Petrov parser (Petrov and Klein, 2007) uses la-
tent variables to refine the grammar extracted from
a corpus to improve accuracy, originally used
to improve parsing results on the Penn treebank
(PTB). We train the Petrov parser on CCGbank
and achieve the best results to date on sentences
from section 23 in terms of supertagging accuracy,
PARSEVAL measures and dependency accuracy.
These results should not be interpreted as proof
that grammars extracted from the Penn treebank
and from CCGbank are equivalent. Bos?s system
for building semantic representations from CCG
derivations is only possible due to the categorial
nature of CCG. Furthermore, the long distance de-
pendencies involved in extraction and coordina-
tion phenomena have a more natural representa-
tion in CCG.
2 The Language Classes of Combinatory
Categorial Grammars
A categorial grammar is a grammatical system
consisting of a finite set of words, a set of cate-
gories, a finite set of sentential categories, a finite
lexicon mapping words to categories and a rule
system dictating how the categories can be com-
bined. The set of categories are constructed from a
finite set of atoms A (e.g. A = {S,NP,N,PP})
and a finite set of binary connectives B (e.g.
B = {/, \}) to build an infinite set of categories
C(A,B) (e.g. C(A,B) = {S, S\NP, (S\NP )/
NP, . . .}). For a category C , its size |C| is the
335
number of atom occurrences it contains. When not
specified, connectives are left associative.
According to the literature, combinatory cate-
gorial grammar has been defined to have a vari-
ety of rule systems. These rule systems vary from
a small rule set, motivated theoretically (Vijay-
Shanker and Weir, 1994), to a larger rule set,
motivated linguistically, (Steedman, 2000) to a
very large rule set, motivated by practical cover-
age (Hockenmaier and Steedman, 2007; Clark and
Curran, 2007). We provide a definition general
enough to incorporate these four main variants of
CCG, as well as others.
A combinatory categorial grammar (CCG) is a
categorial grammar whose rule system consists of
rule schemata where the left side is a sequence of
categories and the right side is a single category
where the categories may include variables over
both categories and connectives. In addition, rule
schemata may specify a sequence of categories
and connectives using the . . . convention1 . When
. . . appears in a rule, it matches any sequence of
categories and connectives according to the con-
nectives adjacent to the . . .. For example, the rule
schema for forward composition is:
X/Y, Y/Z ? X/Z
and the rule schema for generalized forward
crossed composition is:
X/Y, Y |1Z1|2 . . . |nZn ? X|1Z1|2 . . . |nZn
where X, Y and Zi for 1 ? i ? n are variables
over categories and |i for 1 ? i ? n are variables
over connectives. Figure 1 shows a CCG deriva-
tion from CCGbank.
A well-known categorial grammar which is not
a CCG is Lambek categorial grammar (Lambek,
1958) whose introduction rules cannot be charac-
terized as combinatory rules (Zielonka, 1981).
2.1 Classes for defining CCG
We define a number of schema classes general
enough that the important variants of CCG can be
defined by selecting some subset of the classes. In
addition to the schema classes, we also define two
restriction classes which define ways in which the
rule schemata from the schema classes can be re-
stricted. We define the following schema classes:
1The . . . convention (Vijay-Shanker and Weir, 1994) is
essentially identical to the $ convention of Steedman (2000).
(1) Application
? X/Y, Y ? X
? Y,X\Y ? X
(2) Composition
? X/Y, Y/Z ? X/Z
? Y \Z,X\Y ? X\Z
(3) Crossed Composition
? X/Y, Y \Z ? X\Z
? Y/Z,X\Y ? X/Z
(4) Generalized Composition
? X/Y, Y/Z1/ . . . /Zn ? X/Z1/ . . . /Zn
? Y \Z1\ . . . \Zn,X\Y ? X\Z1\ . . . \Zn
(5) Generalized Crossed Composition
? X/Y, Y |1Z1|2 . . . |nZn
? X|1Z1|2 . . . |nZn
? Y |1Z1|2 . . . |nZn,X\Y
? X|1Z1|2 . . . |nZn
(6) Reducing Generalized Crossed Composition
Generalized Composition or Generalized
Crossed Composition where |X| ? |Y |.
(7) Substitution
? (X/Y )|1Z, Y |1Z ? X|1Z
? Y |1Z, (X\Y )|1Z ? X|1Z
(8) D Combinator2
? X/(Y |1Z), Y |2W ? X|2(W |1Z)
? Y |2W,X\(Y |1Z) ? X|2(W |1Z)
(9) Type-Raising
? X ? T/(T\X)
? X ? T\(T/X)
(10) Finitely Restricted Type-Raising
? X ? T/(T\X) where ?X,T ? ? S for fi-
nite S
? X ? T\(T/X) where ?X,T ? ? S for fi-
nite S
(11) Finite Unrestricted Variable-Free Rules
? ~X ? Y where ? ~X, Y ? ? S for finite S
2Hoyt and Baldridge (2008) argue for the inclusion of the
D Combinator in CCG.
336
Mr. Vinken is chairman of Elsevier N.V. , the Dutch publishing group .
N/N N S[dcl]\NP/NP N NP\NP/NP N/N N , NP [nb]/N N/N N/N N .
N
N
NP
NP [conj]
N
NP
NP
NP\NP
NP
NP
S[dcl]\NP
N
NP
S[dcl]
S[dcl]
Figure 1: A CCG derivation from section 00 of CCGbank.
We define the following restriction classes:
(A) Rule Restriction to a Finite Set
The rule schemata in the schema classes of a
CCG are limited to a finite number of instan-
tiations.
(B) Rule Restrictions to Certain Categories 3
The rule schemata in the schema classes of a
CCG are limited to a finite number of instan-
tiations although variables are allowed in the
instantiations.
Vijay-Shanker and Weir (1994) define CCG to
be schema class (4) with restriction class (B).
Steedman (2000) defines CCG to be schema
classes (1-5), (6), (10) with restriction class (B).
2.2 Strongly Context-Free CCGs
Proposition 1. The set of atoms in any derivation
of any CCG consisting of a subset of the schema
classes (1-8) and (10-11) is finite.
Proof. A finite lexicon can introduce only a finite
number of atoms in lexical categories.
Any rule corresponding to a schema in the
schema classes (1-8) has only those atoms on the
right that occur somewhere on the left. Rules in
classes (10-11) can each introduce a finite number
of atoms, but there can be only a finite number of
3Baldridge (2002) introduced a variant of CCG where
modalities are added to the connectives / and \ along with
variants of the combinatory rules based on these modalities.
Our proofs about restriction class (B) are essentially identical
to proofs regarding the multi-modal variant.
such rules, limiting the new atoms to a finite num-
ber.
Definition 1. The subcategories for a category c
are c1 and c2 if c = c1 ? c2 for ? ? B and c if c is
atomic. Its second subcategories are the subcate-
gories of its subcategories.
Proposition 2. Any CCG consisting of a subset
of the rule schemata (1-3), (6-8) and (10-11) has
derivations consisting of only a finite number of
categories.
Proof. We first prove the proposition excluding
schema class (8). We will use structural induction
on the derivations to prove that there is a bound on
the size of the subcategories of any category in the
derivation. The base case is the assignment of a
lexical category to a word and the inductive step is
the use of a rule from schema classes (1-4), (6-7)
and (10-11).
Given that the lexicon is finite, there is a bound
k on the size of the subcategories of lexical cate-
gories. Furthermore, there is a bound l on the size
of the subcategories of categories on the right side
of any rule in (10) and (11). Let m = max(k, l).
For rules from schema class (1), the category
on the right is a subcategory of the first category
on the left, so the subcategories on the right are
bound by m. For rules from schema classes (2-3),
the category on the right has subcategories X and
Z each of which is bound in size by m since they
occur as subcategories of categories on the left.
For rules from schema class (6), since reduc-
ing generalized composition is a special case of re-
337
ducing generalized crossing composition, we need
only consider the latter. The category on the right
has subcategories X|1Z1|2 . . . |n?1|Zn?1 and Zn.
Zn is bound in size by m because it occurs as
a subcategory of the second category on the left.
Then, the size of Y |1Z1|2 . . . |n?1|Zn?1 must be
bound by m and since |X| ? |Y |, the size of
X|1Z1|2 . . . |n?1|Zn?1 must also be bound by m.
For rules from schema class (7), the category on
the right has subcategories X and Z . The size of
Z is bound by m because it is a subcategory of a
category on the left. The size of X is bound by
m because it is a second subcategory of a category
on the left.
Finally, the use of rules in schema classes (10-
11) have categories on the right that are bounded
by l, which is, in turn, bounded by m. Then, by
proposition 1, there must only be a finite number
of categories in any derivation in a CCG consisting
of a subset of rule schemata (1-3), (6-7) and (10-
11).
The proof including schema class (8) is essen-
tially identical except that k must be defined in
terms of the size of the second subcategories.
Definition 2. A grammar is strongly context-free
if there exists a CFG such that the derivations of
the two grammars are identical.
Proposition 3. Any CCG consisting of a subset
of the schema classes (1-3), (6-8) and (10-11) is
strongly context-free.
Proof. Since the CCG generates derivations
whose categories are finite in number let C be that
set of categories. Let S(C,X) be the subset of C
matching category X (which may have variables).
Then, for each rule schema C1, C2 ? C3 in (1-3)
and (6-8), we construct a context-free rule C ?3 ?
C ?1, C ?2 for each C ?i in S(C,Ci) for 1 ? i ? 3.
Similarly, for each rule schema C1 ? C2 in (10),
we construct a context-free rule C ?2 ? C ?1 which
results in a finite number of such rules. Finally, for
each rule schema ~X ? Z in (11) we construct a
context-free rule Z ? ~X. Then, for each entry in
the lexicon w ? C , we construct a context-free
rule C ? w.
The constructed CFG has precisely the same
rules as the CCG restricted to the categories in C
except that the left and right sides have been re-
versed. Thus, by proposition 2, the CFG has ex-
actly the same derivations as the CCG.
Proposition 4. Any CCG consisting of a subset of
the schema classes (1-3), (6-8) and (10-11) along
with restriction class (B) is strongly context-free.
Proof. If a CCG is allowed to restrict the use of
its rules to certain categories as in schema class
(B), then when we construct the context-free rules
by enumerating only those categories in the set C
allowed by the restriction.
Proposition 5. Any CCG that includes restriction
class (A) is strongly context-free.
Proof. We construct a context-free grammar with
exactly those rules in the finite set of instantiations
of the CCG rule schemata along with context-
free rules corresponding to the lexicon. This
CFG generates exactly the same derivations as the
CCG.
We have thus proved that of a wide range of the
rule schemata used to define CCGs are context-
free.
2.3 Combinatory Categorial Grammars in
Practice
CCGbank (Hockenmaier and Steedman, 2007)
is a corpus of CCG derivations that was semi-
automatically converted from the Wall Street Jour-
nal section of the Penn treebank. Figure 2 shows
a categorization of the rules used in CCGbank ac-
cording to the schema classes defined in the pre-
ceding section where a rule is placed into the least
general class to which it belongs. In addition to
having no generalized composition other than the
reducing variant, it should also be noted that in all
generalized composition rules, X = Y implying
that the reducing class of generalized composition
is a very natural schema class for CCGbank.
If we assume that type-raising is restricted to
those instances occurring in CCGbank4, then a
CCG consisting of schema classes (1-3), (6-7) and
(10-11) can generate all the derivations in CCG-
bank. By proposition 3, such a CCG is strongly
context-free. One could also observe that since
CCGbank is finite, its grammar is not only a
context-free grammar but can produce only a finite
number of derivations. However, our statement is
much stronger because this CCG can generate all
of the derivations in CCGbank given only the lex-
icon, the finite set of unrestricted rules and the fi-
nite number of type-raising rules.
4Without such an assumption, parsing is intractable.
338
Schema Class Rules Instances
Application 519 902176
Composition 102 7189
Crossed Composition 64 14114
Reducing Generalized 50 612
Crossed Composition
Generalized Composition 0 0
Generalized Crossed 0 0
Composition
Substitution 3 4
Type-Raising 27 3996
Unrestricted Rules 642 335011
Total 1407 1263102
Figure 2: The rules of CCGbank by schema class.
The Clark and Curran CCG Parser (Clark and
Curran, 2007) is a CCG parser which uses CCG-
bank as a training corpus. Despite the fact that
there is a strongly context-free CCG which gener-
ates all of the derivations in CCGbank, it is still
possible that the grammar learned by the Clark
and Curran parser is not a context-free grammar.
However, in addition to rule schemata (1-6) and
(10-11) they also include restriction class (A) by
restricting rules to only those found in the train-
ing data5. Thus, by proposition 5, the Clark and
Curran parser is a context-free parser.
3 A Latent Variable CCG Parser
The context-freeness of a number of CCGs should
not be considered evidence that there is no ad-
vantage to CCG as a grammar formalism. Unlike
the context-free grammars extracted from the Penn
treebank, these allow for the categorial semantics
that accompanies any categorial parse and for a
more elegant analysis of linguistic structures such
as extraction and coordination. However, because
we now know that the CCG defined by CCGbank
is strongly context-free, we can use tools from the
CFG parsing community to improve CCG parsing.
To illustrate this point, we train the Petrov
parser (Petrov and Klein, 2007) on CCGbank.
The Petrov parser uses latent variables to refine
a coarse-grained grammar extracted from a train-
ing corpus to a grammar which makes much more
fine-grained syntactic distinctions. For example,
5The Clark and Curran parser has an option, which is dis-
abled by default, for not restricting the rules to those that ap-
pear in the training data. However, they find that this restric-
tion is ?detrimental to neither parser accuracy or coverage?
(Clark and Curran, 2007).
in Petrov?s experiments on the Penn treebank, the
syntactic category NP was refined to the more
fine-grained NP 1 and NP 2 roughly correspond-
ing to NP s in subject and object positions. Rather
than requiring such distinctions to be made in the
corpus, the Petrov parser hypothesizes these splits
automatically.
The Petrov parser operates by performing a
fixed number of iterations of splitting, merging
and smoothing. The splitting process is done
by performing Expectation-Maximization to de-
termine a likely potential split for each syntactic
category. Then, during the merging process some
of the splits are undone to reduce grammar size
and avoid overfitting according to the likelihood
of the split against the training data.
The Petrov parser was chosen for our experi-
ments because it refines the grammar in a mathe-
matically principled way without altering the na-
ture of the derivations that are output. This is
important because the input to the semantic back-
end and the system that converts CCG derivations
to dependencies requires CCG derivations as they
appear in CCGbank.
3.1 Experiments
Our experiments use CCGbank as the corpus and
we use sections 02-21 for training (39603 sen-
tences), 00 for development (1913 sentences) and
23 for testing (2407 sentences).
CCGbank, in addition to the basic atoms S, N ,
NP and PP , also differentiates both the S and
NP atoms with features allowing more subtle dis-
tinctions. For example, declarative sentences are
S[dcl], wh-questions are S[wq] and sentence frag-
ments are S[frg] (Hockenmaier and Steedman,
2007). These features allow finer control of the use
of combinatory rules in the resulting grammars.
However, this fine-grained control is exactly what
the Petrov parser does automatically. Therefore,
we trained the Petrov parser twice, once on the
original version of CCGbank (denoted ?Petrov?)
and once on a version of CCGbank without these
features (denoted ?Petrov no feats?). Furthermore,
we will evaluate the parsers obtained after 0, 4, 5
and 6 training iterations (denoted I-0, I-4, I-5 and
I-6). When we evaluate on sets of sentences for
which not all parsers return an analysis, we report
the coverage (denoted ?Cover?).
We use the evalb package for PARSEVAL
evaluation and a modified version of Clark and
339
Parser Accuracy % No feats %
C&C Normal Form 92.92 93.38
C&C Hybrid 93.06 93.52
Petrov I-5 93.18 93.73
Petrov no feats I-6 - 93.74
Figure 3: Supertagging accuracy on the sentences
in section 00 that receive derivations from the four
parsers shown.
Parser Accuracy % No feats %
C&C Hybrid 92.98 93.43
Petrov I-5 93.10 93.59
Petrov no feats I-6 - 93.62
Figure 4: Supertagging accuracy on the sentences
in section 23 that receive derivations from the
three parsers shown.
Curran?s evaluate script for dependency eval-
uation. To determine statistical significance, we
obtain p-values from Bikel?s randomized parsing
evaluation comparator6, modified for use with tag-
ging accuracy, F-score and dependency accuracy.
3.2 Supertag Evaluation
Before evaluating the parse trees as a whole, we
evaluate the categories assigned to words. In the
supertagging literature, POS tagging and supertag-
ging are distinguished ? POS tags are the tradi-
tional Penn treebank tags (e.g. NN, VBZ and DT)
and supertags are CCG categories. However, be-
cause the Petrov parser trained on CCGbank has
no notion of Penn treebank POS tags, we can only
evaluate the accuracy of the supertags.
The results are shown in figures 3 and 4 where
the ?Accuracy? column shows accuracy of the su-
pertags against the CCGbank categories and the
?No feats? column shows accuracy when features
are ignored. Despite the lack of POS tags in the
Petrov parser, we can see that it performs slightly
better than the Clark and Curran parser. The dif-
ference in accuracy is only statistically significant
between Clark and Curran?s Normal Form model
ignoring features and the Petrov parser trained on
CCGbank without features (p-value = 0.013).
3.3 Constituent Evaluation
In this section we evaluate the parsers using the
traditional PARSEVAL measures which measure
recall, precision and F-score on constituents in
6http://www.cis.upenn.edu/ dbikel/software.html
both labeled and unlabeled versions. In addition,
we report a variant of the labeled PARSEVAL
measures where we ignore the features on the cat-
egories. For reasons of brevity, we report the PAR-
SEVAL measures for all sentences in sections 00
and 23, rather than for sentences of length is less
than 40 or less than 100. The results are essentially
identical for those two sets of sentences.
Figure 5 gives the PARSEVAL measures on sec-
tion 00 for Clark and Curran?s two best models
and the Petrov parser trained on the original CCG-
bank and the version without features after various
numbers of training iterations. Figure 7 gives the
accuracies on section 23.
In the case of Clark and Curran?s hybrid model,
the poor accuracy relative to the Petrov parsers can
be attributed to the fact that this model chooses
derivations based on the associated dependencies
at the expense of constituent accuracy (see section
3.4). In the case of Clark and Curran?s normal
form model, the large difference between labeled
and unlabeled accuracy is primarily due to the mis-
labeling of a small number of features (specifi-
cally, NP[nb] and NP[num]). The labeled accu-
racies without features gives the results when fea-
tures are disregarded.
Due to the similarity of the accuracies and the
difference in the coverage between I-5 of the
Petrov parser on CCGbank and I-6 of the Petrov
parser on CCGbank without features, we reevalu-
ate their results on only those sentences for which
they both return derivations in figures 6 and 8.
These results show that the features in CCGbank
actually inhibit accuracy (to a statistically signifi-
cant degree in the case of unlabeled accuracy on
section 00) when used as training data for the
Petrov parser.
Figure 9 gives a comparison between the Petrov
parser trained on the Penn treebank and on CCG-
bank. These numbers should not be directly com-
pared, but the similarity of the unlabeled measures
indicates that the difference between the structure
of the Penn treebank and CCGbank is not large.7
3.4 Dependency Evaluation
The constituent-based PARSEVAL measures are
simple to calculate from the output of the Petrov
parser but the relationship of the PARSEVAL
7Because punctuation in CCG can have grammatical
function, we include it in our accuracy calculations result-
ing in lower scores for the Petrov parser trained on the Penn
treebank than those reported in Petrov and Klein (2007).
340
Labeled % Labeled no feats % Unlabeled %
Parser R P F R P F R P F Cover
C&C Normal Form 71.14 70.76 70.95 80.66 80.24 80.45 86.16 85.71 85.94 98.95
C&C Hybrid 50.08 49.47 49.77 58.13 57.43 57.78 61.27 60.53 60.90 98.95
Petrov I-0 74.19 74.27 74.23 74.66 74.74 74.70 78.65 78.73 78.69 99.95
Petrov I-4 85.86 85.78 85.82 86.36 86.29 86.32 89.96 89.88 89.92 99.90
Petrov I-5 86.30 86.16 86.23 86.84 86.70 86.77 90.28 90.13 90.21 99.90
Petrov I-6 85.95 85.68 85.81 86.51 86.23 86.37 90.22 89.93 90.08 99.22
Petrov no feats I-0 - - - 72.16 72.59 72.37 76.52 76.97 76.74 99.95
Petrov no feats I-5 - - - 86.67 86.57 86.62 90.30 90.20 90.25 99.90
Petrov no feats I-6 - - - 87.45 87.37 87.41 90.99 90.91 90.95 99.84
Figure 5: Constituent accuracy on all sentences from section 00.
Labeled % Labeled no feats % Unlabeled %
Parser R P F R P F R P F
Petrov I-5 86.56 86.46 86.51 87.10 87.01 87.05 90.43 90.33 90.38
Petrov no feats I-6 - - - 87.45 87.37 87.41 90.99 90.91 90.95
p-value - - - 0.089 0.090 0.088 0.006 0.008 0.007
Figure 6: Constituent accuracy on the sentences in section 00 that receive a derivation from both parsers.
Labeled % Labeled no feats % Unlabeled %
Parser R P F R P F R P F Cover
C&C Normal Form 71.15 70.79 70.97 80.73 80.32 80.53 86.31 85.88 86.10 99.58
Petrov I-5 86.94 86.80 86.87 87.47 87.32 87.39 90.75 90.59 90.67 99.83
Petrov no feats I-6 - - - 87.49 87.49 87.49 90.81 90.82 90.81 99.96
Figure 7: Constituent accuracy on all sentences from section 23.
Labeled % Labeled no feats % Unlabeled %
Parser R P F R P F R P F
Petrov I-5 86.94 86.80 86.87 87.47 87.32 87.39 90.75 90.59 90.67
Petrov no feats I-6 - - - 87.48 87.49 87.49 90.81 90.82 90.81
p-value - - - 0.463 0.215 0.327 0.364 0.122 0.222
Figure 8: Constituent accuracy on the sentences in section 23 that receive a derivation from both parsers.
Labeled % Unlabeled %
Parser R P F R P F Cover
Petrov on PTB I-6 89.65 89.97 89.81 90.80 91.13 90.96 100.00
Petrov on CCGbank I-5 86.94 86.80 86.87 90.75 90.59 90.67 99.83
Petrov on CCGbank no feats I-6 87.49 87.49 87.49 90.81 90.82 90.81 99.96
Figure 9: Constituent accuracy for the Petrov parser on the corpora on all sentences from Section 23.
Mr. Vinken is chairman of Elsevier N.V. , the Dutch publishing group .
N/N N S[dcl]\NP/NP N NP\NP/NP N/N N , NP [nb]/N N/N N/N N .
Figure 10: The argument-functor relations for the CCG derivation in figure 1.
341
Mr. Vinken is chairman of Elsevier N.V. , the Dutch publishing group .
N/N N S[dcl]\NP/NP N NP\NP/NP N/N N , NP [nb]/N N/N N/N N .
Figure 11: The set of dependencies obtained by reorienting the argument-functor edges in figure 10.
Labeled % Unlabeled %
Parser R P F R P F Cover
C&C Normal Form 84.39 85.28 84.83 90.93 91.89 91.41 98.95
C&C Hybrid 84.53 86.20 85.36 90.84 92.63 91.73 98.95
Petrov I-0 79.87 78.81 79.34 87.68 86.53 87.10 96.45
Petrov I-4 84.76 85.27 85.02 91.69 92.25 91.97 96.81
Petrov I-5 85.30 85.87 85.58 92.00 92.61 92.31 96.65
Petrov I-6 84.86 85.46 85.16 91.79 92.44 92.11 96.65
Figure 12: Dependency accuracy on CCGbank dependencies on all sentences from section 00.
Labeled % Unlabeled %
Parser R P F R P F
C&C Hybrid 84.71 86.35 85.52 90.96 92.72 91.83
Petrov I-5 85.50 86.08 85.79 92.12 92.75 92.44
p-value 0.005 0.189 0.187 < 0.001 0.437 0.001
Figure 13: Dependency accuracy on the section 00 sentences that receive an analysis from both parsers.
Labeled % Unlabeled %
Parser R P F R P F
C&C Hybrid 85.11 86.46 85.78 91.15 92.60 91.87
Petrov I-5 85.73 86.29 86.01 92.04 92.64 92.34
p-value 0.013 0.278 0.197 < 0.001 0.404 0.005
Figure 14: Dependency accuracy on the section 23 sentences that receive an analysis from both parsers.
Training Time Parsing Time Training RAM
Parser in CPU minutes in CPU minutes in gigabytes
Clark and Curran Normal Form Model 1152 2 28
Clark and Curran Hybrid Model 2672 4 37
Petrov on PTB I-0 1 5 2
Petrov on PTB I-5 180 20 8
Petrov on PTB I-6 660 21 16
Petrov on CCGbank I-0 1 5 2
Petrov on CCGbank I-4 103 70 8
Petrov on CCGbank I-5 410 600 14
Petrov on CCGbank I-6 2760 2880 24
Petrov on CCGbank no feats I-0 1 5 2
Petrov on CCGbank no feats I-5 360 240 7
Petrov on CCGbank no feats I-6 1980 390 13
Figure 15: Time and space usage when training on sections 02-21 and parsing on section 00.
342
scores to the quality of a parse is not entirely clear.
For this reason, the word to word dependencies
of categorial grammar parsers are often evaluated.
This evaluation is aided by the fact that in addition
to the CCG derivation for each sentence, CCG-
bank also includes a set of dependencies. Fur-
thermore, extracting dependencies from a CCG
derivation is well-established (Clark et al, 2002).
A CCG derivation can be converted into de-
pendencies by, first, determining which arguments
go with which functors as specified by the CCG
derivation. This can be represented as in figure
10. Although this is not difficult, some care must
be taken with respect to punctuation and the con-
junction rules. Next, we reorient some of the
edges according to information in the lexical cat-
egories. A language for specifying these instruc-
tions using variables and indices is given in Clark
et al (2002). This process is shown in figures 1,
10 and 11 with the directions of the dependencies
reversed from Clark et al (2002).
We used the CCG derivation to dependency
converter generate included in the C&C tools
package to convert the output of the Petrov parser
to dependencies. Other than a CCG derivation,
their system requires only the lexicon of edge re-
orientation instructions and methods for convert-
ing the unrestricted rules of CCGbank into the
argument-functor relations. Important for the pur-
pose of comparison, this system does not depend
on their parser.
An unlabeled dependency is correct if the or-
dered pair of words is correct. A labeled depen-
dency is correct if the ordered pair of words is cor-
rect, the head word has the correct category and
the position of the category that is the source of
that edge is correct. Figure 12 shows accuracies
from the Petrov parser trained on CCGbank along
with accuracies for the Clark and Curran parser.
We only show accuracies for the Petrov parser
trained on the original version of CCGbank be-
cause the dependency converter cannot currently
generate dependencies for featureless derivations.
The relatively poor coverage of the Petrov
parser is due to the failure of the dependency con-
verter to output dependencies from valid CCG
derivations. However, the coverage of the depen-
dency converter is actually lower when run on the
gold standard derivations indicating that this cov-
erage problem is not indicative of inaccuracies in
the Petrov parser. Due to the difference in cover-
age, we again evaluate the top two parsers on only
those sentences that they both generate dependen-
cies for and report those results in figures 13 and
14. The Petrov parser has better results by a sta-
tistically significant margin for both labeled and
unlabeled recall and unlabeled F-score.
3.5 Time and Space Evaluation
As a final evaluation, we compare the resources
that are required to both train and parse with the
Petrov parser on the Penn Treebank, the Petrov
parser on the original version of CCGbank, the
Petrov parser on CCGbank without features and
the Clark and Curran parser using the two mod-
els. All training and parsing was done on a 64-bit
machine with 8 dual core 2.8 Ghz Opteron 8220
CPUs and 64GB of RAM. Our training times are
much larger than those reported in Clark and Cur-
ran (2007) because we report the cumulative time
spent on all CPUs rather than the maximum time
spent on a CPU. Figure 15 shows the results.
As can be seen, the Clark and Curran parser
has similar training times, although signifi-
cantly greater RAM requirements than the Petrov
parsers. In contrast, the Clark and Curran parser is
significantly faster than the Petrov parsers, which
we hypothesize to be attributed to the degree
to which Clark and Curran have optimized their
code, their use of C++ as opposed to Java and
their use of a supertagger to prune the lexicon.
4 Conclusion
We have provided a number of theoretical results
proving that CCGbank contains no non-context-
free structure and that the Clark and Curran parser
is actually a context-free parser. Based on these
results, we trained the Petrov parser on CCGbank
and achieved state of the art results in terms of
supertagging accuracy, PARSEVAL measures and
dependency accuracy.
This demonstrates the following. First, the abil-
ity to extract semantic representations from CCG
derivations is not dependent on the language class
of a CCG. Second, using a dedicated supertagger,
as opposed to simply using a general purpose tag-
ger, is not necessary to accurately parse with CCG.
Acknowledgments
We would like to thank Stephen Clark, James Cur-
ran, Jackie C. K. Cheung and our three anonymous
reviewers for their insightful comments.
343
References
J. Baldridge. 2002. Lexically Specified Deriva-
tional Control in Combinatory Categorial Gram-
mar. Ph.D. thesis, University of Edinburgh.
J. Bos, S. Clark, M. Steedman, J. R Curran, and
J. Hockenmaier. 2004. Wide-coverage semantic
representations from a CCG parser. In Proceedings
of COLING, volume 4, page 1240?1246.
S. Clark and J. R. Curran. 2007. Wide-Coverage ef-
ficient statistical parsing with CCG and Log-Linear
models. Computational Linguistics, 33(4):493?552.
S. Clark, J. Hockenmaier, and M. Steedman. 2002.
Building deep dependency structures with a wide-
coverage CCG parser. In Proceedings of the 40th
Meeting of the ACL, page 327?334.
J. Hockenmaier and M. Steedman. 2007. CCGbank:
a corpus of CCG derivations and dependency struc-
tures extracted from the penn treebank. Computa-
tional Linguistics, 33(3):355?396.
F. Hoyt and J. Baldridge. 2008. A logical basis for
the d combinator and normal form in CCG. In Pro-
ceedings of ACL-08: HLT, page 326?334, Colum-
bus, Ohio. Association for Computational Linguis-
tics.
J. Lambek. 1958. The mathematics of sen-
tence structure. American Mathematical Monthly,
65(3):154?170.
S. Petrov and D. Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of NAACL
HLT 2007, page 404?411.
M. Steedman. 2000. The syntactic process. MIT
Press.
K. Vijay-Shanker and D. Weir. 1994. The equivalence
of four extensions of context-free grammars. Math-
ematical Systems Theory, 27(6):511?546.
W. Zielonka. 1981. Axiomatizability of Ajdukiewicz-
Lambek calculus by means of cancellation schemes.
Zeitschrift fur Mathematische Logik und Grundla-
gen der Mathematik, 27:215?224.
344
