Unsupervised Relation Extraction from Web Documents
Kathrin Eichler, Holmer Hemsen and Gu?nter Neumann
DFKI GmbH, LT-Lab, Stuhlsatzenhausweg 3 (Building D3 2), D-66123 Saarbru?cken
{FirstName.SecondName}@dfki.de
Abstract
The IDEX system is a prototype of an interactive dynamic Information Extraction (IE) system. A user of the system
expresses an information request in the form of a topic description, which is used for an initial search in order to retrieve
a relevant set of documents. On basis of this set of documents, unsupervised relation extraction and clustering is done by
the system. The results of these operations can then be interactively inspected by the user. In this paper we describe the
relation extraction and clustering components of the IDEX system. Preliminary evaluation results of these components are
presented and an overview is given of possible enhancements to improve the relation extraction and clustering components.
1. Introduction
Information extraction (IE) involves the process of au-
tomatically identifying instances of certain relations of
interest, e.g., produce(<company>, <product>, <lo-
cation>), in some document collection and the con-
struction of a database with information about each
individual instance (e.g., the participants of a meet-
ing, the date and time of the meeting). Currently, IE
systems are usually domain-dependent and adapting
the system to a new domain requires a high amount
of manual labour, such as specifying and implement-
ing relation?specific extraction patterns manually (cf.
Fig. 1) or annotating large amounts of training cor-
pora (cf. Fig. 2). These adaptations have to be made
offline, i.e., before the specific IE system is actually
made. Consequently, current IE technology is highly
statical and inflexible with respect to a timely adapta-
tion to new requirements in the form of new topics.
Figure 1: A hand-coded rule?based IE?system (schemat-
ically): A topic expert implements manually task?specific
extraction rules on the basis of her manual analysis of a
representative corpus.
1.1. Our goal
The goal of our IE research is the conception and im-
plementation of core IE technology to produce a new
Figure 2: A data?oriented IE system (schematically): The
task?specific extraction rules are automatically acquired by
means of Machine Learning algorithms, which are using
a sufficiently large enough corpus of topic?relevant docu-
ments. These documents have to be collected and costly
annotated by a topic?expert.
IE system automatically for a given topic. Here, the
pre?knowledge about the information request is given
by a user online to the IE core system (called IDEX)
in the form of a topic description (cf. Fig. 3). This
initial information source is used to retrieve relevant
documents and extract and cluster relations in an un-
supervised way. In this way, IDEX is able to adapt
much better to the dynamic information space, in par-
ticular because no predefined patterns of relevant re-
lations have to be specified, but relevant patterns are
determined online. Our system consists of a front-end,
which provides the user with a GUI for interactively in-
specting information extracted from topic-related web
documents, and a back-end, which contains the rela-
tion extraction and clustering component. In this pa-
per, we describe the back-end component and present
preliminary evaluation results.
1.2. Application potential
However, before doing so we would like to motivate
the application potential and impact of the IDEX ap-
Figure 3: The dynamic IE system IDEX (schematically):
a user of the IDEX IE system expresses her information
request in the form of a topic description which is used for
an initial search in order to retrieve a relevant set of doc-
uments. From this set of documents, the system extracts
and collects (using the IE core components of IDEX) a set
of tables of instances of possibly relevant relations. These
tables are presented to the user (who is assumed to be the
topic?expert), who will analyse the data further for her in-
formation research. The whole IE process is dynamic, since
no offline data is required, and the IE process is interactive,
since the topic expert is able to specify new topic descrip-
tions, which express her new attention triggered by a novel
relationship she was not aware of beforehand.
proach by an example application. Consider, e.g., the
case of the exploration and the exposure of corruptions
or the risk analysis of mega construction projects. Via
the Internet, a large pool of information resources of
such mega construction projects is available. These
information resources are rich in quantity, but also
in quality, e.g., business reports, company profiles,
blogs, reports by tourists, who visited these construc-
tion projects, but also web documents, which only
mention the project name and nothing else. One of
the challenges for the risk analysis of mega construc-
tion projects is the efficient exploration of the possibly
relevant search space. Developing manually an IE sys-
tem is often not possible because of the timely need
of the information, and, more importantly, is proba-
bly not useful, because the needed (hidden) informa-
tion is actually not known. In contrast, an unsuper-
vised and dynamic IE system like IDEX can be used
to support the expert in the exploration of the search
space through pro?active identification and clustering
of structured entities. Named entities like for example
person names and locations, are often useful indicators
of relevant text passages, in particular, if the names are
in some relationship. Furthermore, because the found
relationships are visualized using an advanced graph-
ical user interface, the user can select specific names
and find associated relationships to other names, the
documents they occur in or she can search for para-
phrases of sentences.
2. System architecture
The back-end component, visualized in Figure 4, con-
sists of three parts, which are described in detail in this
section: preprocessing, relation extraction and relation
clustering.
2.1. Preprocessing
In the first step, for a specific search task, a topic of
interest has to be defined in the form of a query. For
this topic, documents are automatically retrieved from
the web using the Google search engine. HTML and
PDF documents are converted into plain text files. As
the tools used for linguistic processing (NE recogni-
tion, parsing, etc.) are language-specific, we use the
Google language filter option when downloading the
documents. However, this does not prevent some doc-
uments written in a language other than our target
language (English) from entering our corpus. In ad-
dition, some web sites contain text written in several
languages. In order to restrict the processing to sen-
tences written in English, we apply a language guesser
tool, lc4j (Lc4j, 2007) and remove sentences not clas-
sified as written in English. This reduces errors on
the following levels of processing. We also remove sen-
tences that only contain non-alphanumeric characters.
To all remaining sentences, we apply LingPipe (Ling-
Pipe, 2007) for sentence boundary detection, named
entity recognition (NER) and coreference resolution.
As a result of this step database tables are created,
containing references to the original document, sen-
tences and detected named entities (NEs).
2.2. Relation extraction
Relation extraction is done on the basis of parsing po-
tentially relevant sentences. We define a sentence to be
of potential relevance if it at least contains two NEs.
In the first step, so-called skeletons (simplified depen-
dency trees) are extracted. To build the skeletons, the
Stanford parser (Stanford Parser, 2007) is used to gen-
erate dependency trees for the potentially relevant sen-
tences. For each NE pair in a sentence, the common
root element in the corresponding tree is identified and
the elements from each of the NEs to the root are col-
lected. An example of a skeleton is shown in Figure 5.
In the second step, information based on dependency
types is extracted for the potentially relevant sen-
tences. Focusing on verb relations (this can be ex-
tended to other types of relations), we collect for each
verb its subject(s), object(s), preposition(s) with ar-
guments and auxiliary verb(s). We can now extract
verb relations using a simple algorithm: We define a
verb relation to be a verb together with its arguments
(subject(s), object(s) and prepositional phrases) and
consider only those relations to be of interest where at
least the subject or the object is an NE. We filter out
relations with only one argument.
2.3. Relation clustering
Relation clusters are generated by grouping relation
instances based on their similarity.
web documents document
retrieval
topic specific documents plain text documents
sentence/documents+
 NE tables
languagefiltering
syntactic +typed dependencyparsing 
sov?relationsskeletons +
clustering
conversion
Preprocessing
Relation extraction
Relation clustering
sentencesrelevant
filtering of
relationfiltering
table of clustered relations
sentence boundary
resolutioncoreference
detection,NE recognition,
Figure 4: System architecture
Figure 5: Skeleton for the NE pair ?Hohenzollern? and ?Brandenburg? in the sentence ?Subsequent members of
the Hohenzollern family ruled until 1918 in Berlin, first as electors of Brandenburg.?
The comparably large amount of data in the corpus
requires the use of an efficient clustering algorithm.
Standard ML clustering algorithms such as k-means
and EM (as provided by the Weka toolbox (Witten
and Frank, 2005)) have been tested for clustering the
relations at hand but were not able to deal with the
large number of features and instances required for an
adequate representation of our dataset. We thus de-
cided to use a scoring algorithm that compares a re-
lation to other relations based on certain aspects and
calculates a similarity score. If this similarity score ex-
ceeds a predefined threshold, two relations are grouped
together.
Similarity is measured based on the output from the
different preprocessing steps as well as lexical informa-
tion from WordNet (WordNet, 2007):
? WordNet: WordNet information is used to deter-
mine if two verb infinitives match or if they are in
the same synonym set.
? Parsing: The extracted dependency information is
used to measure the token overlap of the two sub-
jects and objects, respectively. We also compare
the subject of the first relation with the object of
the second relation and vice versa. In addition,
we compare the auxiliary verbs, prepositions and
preposition arguments found in the relation.
? NE recognition: The information from this step
is used to count how many of the NEs occurring
in the contexts, i.e., the sentences in which the
two relations are found, match and whether the
NE types of the subjects and objects, respectively,
match.
? Coreference resolution: This type of information
is used to compare the NE subject (or object) of
one relation to strings that appear in the same
coreference set as the subject (or object) of the
second relation.
Manually analyzing a set of extracted relation in-
stances, we defined weights for the different similarity
measures and calculated a similarity score for each re-
lation pair. We then defined a score threshold and clus-
tered relations by putting two relations into the same
cluster if their similarity score exceeded this threshold
value.
3. Experiments and results
For our experiments, we built a test corpus of doc-
uments related to the topic ?Berlin Hauptbahnhof?
by sending queries describing the topic (e.g., ?Berlin
Hauptbahnhof?, ?Berlin central station?) to Google
and downloading the retrieved documents specifying
English as the target language. After preprocessing
these documents as described in 2.1., our corpus con-
sisted of 55,255 sentences from 1,068 web pages, from
which 10773 relations were automatically extracted
and clustered.
3.1. Clustering
From the extracted relations, the system built 306 clus-
ters of two or more instances, which were manually
evaluated by two authors of this paper. 81 of our clus-
ters contain two or more instances of exactly the same
relation, mostly due to the same sentence appearing in
several documents of the corpus. Of the remaining 225
clusters, 121 were marked as consistent, 35 as partly
consistent, 69 as not consistent. We defined consis-
tency based on the potential usefulness of a cluster to
the user and identified three major types of potentially
useful clusters:
? Relation paraphrases, e.g.,
accused (Mr Moore, Disney, In letter)
accused (Michael Moore, Walt Disney
Company)
? Different instances of the same pattern, e.g.,
operates (Delta, flights, from New York)
offers (Lufthansa, flights, from DC)
? Relations about the same topic (NE), e.g.,
rejected (Mr Blair, pressure, from Labour
MPs)
reiterated (Mr Blair, ideas, in speech, on
March)
created (Mr Blair, doctrine)
...
Of our 121 consistent clusters, 76 were classified as be-
ing of the type ?same pattern?, 27 as being of the type
?same topic? and 18 as being of the type ?relation para-
phrases?. As many of our clusters contain two instances
only, we are planning to analyze whether some clusters
should be merged and how this could be achieved.
3.2. Relation extraction
In order to evaluate the performance of the relation ex-
traction component, we manually annotated 550 sen-
tences of the test corpus by tagging all NEs and verbs
and manually extracting potentially interesting verb
relations. We define ?potentially interesting verb rela-
tion? as a verb together with its arguments (i.e., sub-
ject, objects and PP arguments), where at least two
of the arguments are NEs and at least one of them
is the subject or an object. On the basis of this crite-
rion, we found 15 potentially interesting verb relations.
For the same sentences, the IDEX system extracted 27
relations, 11 of them corresponding to the manually
extracted ones. This yields a recall value of 73% and
a precision value of 41%.
There were two types of recall errors: First, errors in
sentence boundary detection, mainly due to noisy in-
put data (e.g., missing periods), which lead to parsing
errors, and second, NER errors, i.e., NEs that were
not recognised as such. Precision errors could mostly
be traced back to the NER component (sequences of
words were wrongly identified as NEs).
In the 550 manually annotated sentences, 1300 NEs
were identified as NEs by the NER component. 402
NEs were recognised correctly by the NER, 588
wrongly and in 310 cases only parts of an NE were
recognised. These 310 cases can be divided into three
groups of errors. First, NEs recognised correctly, but
labeled with the wrong NE type. Second, only parts
of the NE were recognised correctly, e.g., ?Touris-
mus Marketing GmbH? instead of ?Berlin Tourismus
Marketing GmbH?. Third, NEs containing additional
words, such as ?the? in ?the Brandenburg Gate?.
To judge the usefulness of the extracted relations, we
applied the following soft criterion: A relation is con-
sidered useful if it expresses the main information given
by the sentence or clause, in which the relation was
found. According to this criterion, six of the eleven
relations could be considered useful. The remaining
five relations lacked some relevant part of the sen-
tence/clause (e.g., a crucial part of an NE, like the
?ICC? in ?ICC Berlin?).
4. Possible enhancements
With only 15 manually extracted relations out of 550
sentences, we assume that our definition of ?potentially
interesting relation? is too strict, and that more inter-
esting relations could be extracted by loosening the ex-
traction criterion. To investigate on how the criterion
could be loosened, we analysed all those sentences in
the test corpus that contained at least two NEs in order
to find out whether some interesting relations were lost
by the definition and how the definition would have to
be changed in order to detect these relations. The ta-
ble in Figure 6 lists some suggestions of how this could
be achieved, together with example relations and the
number of additional relations that could be extracted
from the 550 test sentences.
In addition, more interesting relations could be
found with an NER component extended by more
types, e.g., DATE and EVENT. Open domain NER
may be useful in order to extract NEs of additional
types. Also, other types of relations could be inter-
esting, such as relations between coordinated NEs,
option example additional relations
extraction of relations,
where the NE is not the
complete subject, object or
PP argument, but only part
of it
Co-operation with <ORG>M.A.X.
2001<\ORG> <V>is<\V> clearly of
benefit to <ORG>BTM<\ORG>.
25
extraction of relations with
a complex VP
<ORG>BTM<\ORG> <V>invited and or
supported<\V> more than 1,000 media rep-
resentatives in <LOC>Berlin<\LOC>.
7
resolution of relative pro-
nouns
The <ORG>Oxford Centre for Maritime
Archaeology<\ORG> [...] which will
<V>conduct<\V> a scientific symposium in
<LOC>Berlin<\LOC>.
2
combination of several of the
options mentioned above
<LOC>Berlin<\LOC> has <V>developed to
become<\V> the entertainment capital of
<LOC>Germany<\LOC>.
7
Figure 6: Table illustrating different options according to which the definition of ?potentially interesting relation?
could be loosened. For each option, an example sentence from the test corpus is given, together with the number
of relations that could be extracted additionally from the test corpus.
e.g., in a sentence like The exhibition [...] shows
<PER>Clemens Brentano<\PER>, <PER>Achim
von Arnim<\PER> and <PER>Heinrich von
Kleist<\PER>, and between NEs occurring in the
same (complex) argument, e.g., <PER>Hanns Peter
Nerger<\PER>, CEO of <ORG>Berlin Tourismus
Marketing GmbH (BTM) <\ORG>, sums it up [...].
5. Related work
Our work is related to previous work on domain-
independent unsupervised relation extraction, in par-
ticular Sekine (2006), Shinyama and Sekine (2006) and
Banko et al (2007).
Sekine (2006) introduces On-demand information ex-
traction, which aims at automatically identifying
salient patterns and extracting relations based on these
patterns. He retrieves relevant documents from a
newspaper corpus based on a query and applies a POS
tagger, a dependency analyzer and an extended NE
tagger. Using the information from the taggers, he ex-
tracts patterns and applies paraphrase recognition to
create sets of semantically similar patterns. Shinyama
and Sekine (2006) apply NER, coreference resolution
and parsing to a corpus of newspaper articles to ex-
tract two-place relations between NEs. The extracted
relations are grouped into pattern tables of NE pairs
expressing the same relation, e.g., hurricanes and their
locations. Clustering is performed in two steps: they
first cluster all documents and use this information to
cluster the relations. However, only relations among
the five most highly-weighted entities in a cluster are
extracted and only the first ten sentences of each arti-
cle are taken into account.
Banko et al (2007) use a much larger corpus, namely
9 million web pages, to extract all relations between
noun phrases. Due to the large amount of data, they
apply POS tagging only. Their output consists of mil-
lions of relations, most of them being abstract asser-
tions such as (executive, hired by, company) rather
than concrete facts.
Our approach can be regarded as a combination of
these approaches: Like Banko et al (2007), we extract
relations from noisy web documents rather than com-
parably homogeneous news articles. However, rather
than extracting relations from millions of pages we re-
duce the size of our corpus beforehand using a query in
order to be able to apply more linguistic preprocessing.
Like Sekine (2006) and Shinyama and Sekine (2006),
we concentrate on relations involving NEs, the assump-
tion being that these relations are the potentially in-
teresting ones. The relation clustering step allows us
to group similar relations, which can, for example, be
useful for the generation of answers in a Question An-
swering system.
6. Future work
Since many errors were due to the noisiness of the ar-
bitrarily downloaded web documents, a more sophisti-
cated filtering step for extracting relevant textual infor-
mation from web sites before applying NE recognition,
parsing, etc. is likely to improve the performance of
the system.
The NER component plays a crucial role for the qual-
ity of the whole system, because the relation extraction
component depends heavily on the NER quality, and
thereby the NER quality influences also the results of
the clustering process. A possible solution to improve
NER in the IDEX System is to integrate a MetaNER
component, combining the results of several NER com-
ponents. Within the framework of the IDEX project
a MetaNER component already has been developed
(Heyl, to appear 2008), but not yet integrated into the
prototype. The MetaNER component developed uses
the results from three different NER systems. The out-
put of each NER component is weighted depending on
the component and if the sum of these values for a pos-
sible NE exceeds a certain threshold it is accepted as
NE otherwise it is rejected.
The clustering step returns many clusters containing
two instances only. A task for future work is to in-
vestigate, whether it is possible to build larger clus-
ters, which are still meaningful. One way of enlarging
cluster size is to extract more relations. This could
be achieved by loosening the extraction criteria as de-
scribed in section 4. Also, it would be interesting to see
whether clusters could be merged. This would require
a manual analysis of the created clusters.
Acknowledgement
The work presented here was partially supported by a
research grant from the?Programm zur Fo?rderung von
Forschung, Innovationen und Technologien (ProFIT)?
(FKZ: 10135984) and the European Regional Develop-
ment Fund (ERDF).
7. References
Michele Banko, Michael J. Cafarella, Stephen Soder-
land, Matthew Broadhead, and Oren Etzioni. 2007.
Open information extraction from the web. In Proc.
of the International Joint Conference on Artificial
Intelligence (IJCAI).
Andrea Heyl. to appear 2008. Unsupervised relation
extraction. Master?s thesis, Saarland University.
Lc4j. 2007. Language categorization library for Java.
http://www.olivo.net/software/lc4j/.
LingPipe. 2007. http://www.alias-i.com/lingpipe/.
Satoshi Sekine. 2006. On-demand information extrac-
tion. In ACL. The Association for Computer Lin-
guistics.
Yusuke Shinyama and Satoshi Sekine. 2006. Preemp-
tive information extraction using unrestricted re-
lation discovery. In Proc. of the main conference
on Human Language Technology Conference of the
North American Chapter of the Association of Com-
putational Linguistics, pages 304?311. Association
for Computational Linguistics.
Stanford Parser. 2007. http://nlp.stanford.edu/
downloads/lex-parser.shtml.
Ian H. Witten and Eibe Frank. 2005. Data Min-
ing: Practical machine learning tools and techniques.
Morgan Kaufmann, San Francisco, 2nd edition.
WordNet. 2007. http://wordnet.princeton.edu/.
Event Detection and Summarization in Weblogs with Temporal Collocations 
Chun-Yuan Teng and Hsin-Hsi Chen 
Department of Computer Science and Information Engineering 
National Taiwan University 
Taipei, Taiwan 
{r93019, hhchen}@csie.ntu.edu.tw 
Abstract 
 
This paper deals with the relationship between weblog content and time. With the proposed temporal mutual information, we analyze 
the collocations in time dimension, and the interesting collocations related to special events. The temporal mutual information is 
employed to observe the strength of term-to-term associations over time. An event detection algorithm identifies the collocations that 
may cause an event in a specific timestamp. An event summarization algorithm retrieves a set of collocations which describe an event. 
We compare our approach with the approach without considering the time interval. The experimental results demonstrate that the 
temporal collocations capture the real world semantics and real world events over time. 
 
1. 
2. 
Introduction 
Compared with traditional media such as online news 
and enterprise websites, weblogs have several unique 
characteristics, e.g., containing abundant life experiences 
and public opinions toward different topics, highly 
sensitive to the events occurring in the real world, and 
associated with the personal information of bloggers. 
Some works have been proposed to leverage these 
characteristics, e.g., the study of the relationship between 
the content and bloggers? profiles (Adamic & Glance, 
2005; Burger & Henderson, 2006; Teng & Chen, 2006), 
and content and real events (Glance, Hurst & Tornkiyo, 
2004; Kim, 2005; Thelwall, 2006; Thompson, 2003). 
In this paper, we will use temporal collocation to 
model the term-to-term association over time.  In the past, 
some useful collocation models (Manning & Sch?tze, 
1999) have been proposed such as mean and variance, 
hypothesis test, mutual information, etc. Some works 
analyze the weblogs from the aspect of time like the 
dynamics of weblogs in time and location (Mei, et al, 
2006), the weblog posting behavior (Doran, Griffith & 
Henderson, 2006; Hurst, 2006), the topic extraction (Oka, 
Abe & Kato, 2006), etc. The impacts of events on social 
media are also discussed, e.g., the change of weblogs after 
London attack (Thelwall, 2006), the relationship between 
the warblog and weblogs (Kim, 2005; Thompson, 2003), 
etc. 
This paper is organized as follows. Section 2 defines 
temporal collocation to model the strength of term-to-term 
associations over time.  Section 3 introduces an event 
detection algorithm to detect the events in weblogs, and 
an event summarization algorithm to extract the 
description of an event in a specific time with temporal 
collocations. Section 4 shows and discusses the 
experimental results.  Section 5 concludes the remarks. 
Temporal Collocations 
We derive the temporal collocations from Shannon?s 
mutual information (Manning & Sch?tze, 1999) which is 
defined as follows (Definition 1). 
Definition 1 (Mutual Information) The mutual 
information of two terms x and y is defined as: 
)()(
),(log),(),(
yPxP
yxPyxPyxI =  
where P(x,y) is the co-occurrence probability of x and y, 
and P(x) and P(y) denote the occurrence probability of x 
and y, respectively. 
Following the definition of mutual information, we 
derive the temporal mutual information modeling the 
term-to-term association over time, and the definition is 
given as follows.  
 Definition 2 (Temporal Mutual Information) Given 
a timestamp t and a pair of terms x and y, the temporal 
mutual information of x and y in t is defined as: 
)|()|(
)|,(log)|,()|,(
tyPtxP
tyxPtyxPtyxI =
where P(x,y|t) is the probability of co-occurrence of terms 
x and y in timestamp t, P(x|t) and P(y|t) denote the 
probability of occurrences of x and y in timestamp t, 
respectively. 
To measure the change of mutual information in time 
dimension, we define the change of temporal mutual 
information as follows. 
Definition 3 (Change of Temporal Mutual 
Information) Given time interval [t1, t2], the change of 
temporal mutual information is defined as: 
12
12
21
)|,()|,(),,,(
tt
tyxItyxIttyxC ?
?=  
where C(x,y,t1,t2) is the change of temporal mutual 
information of terms x and y in time interval [t1, t2], I(x,y| 
t1) and I(x,y| t2) are the temporal mutual information in 
time t1 and t2, respectively. 
3. Event Detection 
Event detection aims to identify the collocations 
resulting in events and then retrieve the description of 
events. Figure 1 sketches an example of event detection. 
The weblog is parsed into a set of collocations. All 
collocations are processed and monitored to identify the 
plausible events.  Here, a regular event ?Mother?s day? 
and an irregular event ?Typhoon Chanchu? are detected.  
The event ?Typhoon Chanchu? is described by the words  
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1: An Example of Event Detection
?Typhoon?, ?Chanchu?, ?2k?, ?Eye?, ?Path? and 
?chinaphillippine?.  
The architecture of an event detection system includes 
a preprocessing phase for parsing the weblogs and 
retrieving the collocations; an event detection phase 
detecting the unusual peak of the change of temporal 
mutual information and identifying the set of collocations 
which may result in an event in a specific time duration; 
and an event summarization phase extracting the 
collocations related to the seed collocations found in a 
specific time duration. 
The most important part in the preprocessing phase is 
collocation extraction. We retrieve the collocations from 
the sentences in blog posts. The candidates are two terms 
within a window size. Due to the size of candidates, we 
have to identify the set of tracking terms for further 
analysis. In this paper, those candidates containing 
stopwords or with low change of temporal mutual 
information are removed. 
In the event detection phase, we detect events by 
using the peak of temporal mutual information in time 
dimension.  However, the regular pattern of temporal 
mutual information may cause problems to our detection. 
Therefore, we remove the regular pattern by seasonal 
index, and then detect the plausible events by measuring 
the unusual peak of temporal mutual information. 
If a topic is suddenly discussed, the relationship 
between the related terms will become higher. Two 
alternatives including change of temporal mutual 
information and relative change of temporal mutual 
information are employed to detect unusual events. Given 
timestamps t1 and t2 with temporal mutual information 
MI1 and MI2, the change of temporal mutual information 
is calculated by (MI2-MI1). The relative change of 
temporal mutual information is calculated by (MI2-
MI1)/MI1. 
For each plausible event, there is a seed collocation, 
e.g., ?Typhoon Chanchu?. In the event description 
retrieval phase, we try to select the collocations with the 
highest mutual information with the word w in a seed 
collocation. They will form a collocation network for the 
event.  Initially, the seed collocation is placed into the 
network.  When a new collocation is added, we compute 
the mutual information of the multiword collocations by 
the following formula, where n is the number of 
collocations in the network up to now. 
?= n iMInInformatioMutualMultiwo  
If the multiword mutual information is lower than a 
threshold, the algorithm stops and returns the words in the 
collocation network as a description of the event.  Figure 
2 sketches an example.  The collocations ?Chanchu?s 
path?, ?Typhoon eye?, and ?Chanchu affects? are added 
into the network in sequence based on their MI. 
We have two alternatives to add the collocations to 
the event description. The first method adds the 
collocations which have the highest mutual information 
as discussed above. In contrast, the second method adds 
the collocations which have the highest product of mutual 
information and change of temporal mutual information. 
 
 
 
 
 
 
Figure 2: An Example of Collocation network 
4. 
4.1. 
Experiments and Discussions 
Temporal Mutual Information versus 
Mutual Information 
In the experiments, we adopt the ICWSM weblog data 
set (Teng & Chen, 2007; ICWSM, 2007). This data set 
collected from May 1, 2006 through May 20, 2006 is 
about 20 GB. Without loss of generality, we use the 
English weblog of 2,734,518 articles for analysis. 
To evaluate the effectiveness of time information, we 
made the experiments based on mutual information 
(Definition 1) and temporal mutual information 
(Definition 2). The former called the incremental 
approach measures the mutual information at each time 
point based on all available temporal information at that 
time. The latter called the interval-based approach 
considers the temporal mutual information in different 
time stamps.  Figures 3 and 4 show the comparisons 
between interval-based approach and incremental 
approach, respectively, in the event of Da Vinci Code.   
We find that ?Tom Hanks? has higher change of 
temporal mutual information compared to ?Da Vinci 
Code?. Compared to the incremental approach in Figure 4, 
the interval-based approach can reflect the exact release 
date of ?Da Vinci Code.? 
 rd
=i 1 4.2. Evaluation of Event Detection 
We consider the events of May 2006 listed in 
wikipedia1 as gold standard. On the one hand, the events 
posted in wikipedia are not always complete, so that we 
adopt recall rate as our evaluation metric.  On the other 
hand, the events specified in wikipedia are not always 
discussed in weblogs.  Thus, we search the contents of 
blog post to verify if the events were touched on in our 
blog corpus. Before evaluation, we remove the events 
listed in wikipedia, but not referenced in the weblogs. 
 
 
 
 
 
 
 
 
 
 
 
Figure 3: Interval-based Approach in Da Vinci Code  
 
 
 
 
 
 
 
 
Figure 4: Incremental Approach in Da Vinci Code 
gure 5 sketches the idea of evaluation.  The left side 
of t s figure shows the collocations detected by our event 
dete tion system, and the right side shows the events 
liste  in wikipedia.  After matching these two lists, we 
can find that the first three listed events were correctly 
identified by our system.  Only the event ?Nepal Civil 
War? was listed, but not found. Thus, the recall rate is 
75% in this case. 
 
 
 
 
 
 
 
Figure 5: Evaluation of Event Detection Phase 
As discussed in Section 3, we adopt change of 
temporal mutual information, and relative change of 
temporal mutual information to detect the peak. In Figure 
6, we compare the two methods to detect the events in 
weblogs. The relative change of temporal mutual 
information achieves better performance than the change 
of temporal mutual information. 
                                                     
1 http://en.wikipedia.org/wiki/May_2006 
Table 1 and Table 2 list the top 20 collocations based 
on these two approaches, respectively. The results of the 
first approach show that some collocations are related to 
the feelings such as ?fell left? and time such as ?Saturday 
night?. In contrast, the results of the second approach 
show more interesting collocations related to the news 
events at that time, such as terrorists ?zacarias 
moussaoui? and ?paramod mahajan.? These two persons 
were killed in May 3. Besides, ?Geena Davis? got the 
golden award in May 3. That explains why the 
collocations detected by relative change of temporal 
mutual information are better than those detected by 
change of temporal mutual information. 
-20
-15
-10
-5
0
5
10
1 3 5 7 9 11 13 15 17 19
Time (day)
M
ut
ua
l i
nf
or
m
at
io
n
Da-Vinci Tom Hanks
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 6: Performance of Event Detection Phase 
-15
-10
-5
0
5
10
1 3 5 7 9 11 13 15 17 19
Time (day)
M
ut
ua
l i
nf
or
m
at
io
n
Da-Vinci Tom Hanks
Collocations CMI Collocations CMI 
May 03 9276.08 Current music 1842.67
Illegal immigrants 5833.17 Hate studying 1722.32
Feel left 5411.57 Stephen Colbert 1709.59
Saturday night 4155.29 Thursday night 1678.78
Past weekend 2405.32 Can?t believe 1533.33
White house 2208.89 Feel asleep 1428.18
Red sox 2208.43 Ice cream 1373.23
Album tool 2120.30 Oh god 1369.52
Sunday morning 2006.78 Illegalimmigration 1368.12
16.56
f 
CMI
32.50
31.63
29.09
28.45
28.34
28.13Sunday night 1992.37 Pretty cool 13
Table 1: Top 20 collocations with highest change o
temporal mutual information 
Collocations CMI Collocations 
casinos online 618.36 Diet sodas 
zacarias moussaoui 154.68 Ving rhames 
Tsunami warning 107.93 Stock picks 
Conspirator zacarias 71.62 Happy hump 
Artist formerly 57.04 Wong kan 
Federal  
Jury 
41.78 Sixapartcom 
movabletype Wed 3 39.20 Aaron echolls 27.48
Pramod mahajan 35.41 Phnom penh 25.78
BBC  
Version 
35.21 Livejournal 
sixapartcom 
23.83  Fi
hi
c
dGeena davis 33.64 George yeo 20.34
Table 2: Top 20 collocations with highest relative change 
of mutual information 
4.3. Evaluation of Event Summarization 
As discussed in Section 3, we have two methods to 
include collocations to the event description. Method 1 
employs the highest mutual information, and Method 2 
utilizes the highest product of mutual information and 
change of temporal mutual information. Figure 7 shows 
the performance of Method 1 and Method 2. We can see 
that the performance of Method 2 is better than that of 
Method 1 in most cases. 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 7: Overall Performance of Event Summarization 
The results of event summarization by Method 2 are 
shown in Figure 8. Typhoon Chanchu appeared in the 
Pacific Ocean on May 10, 2006, passed through 
Philippine and China and resulted in disasters in these 
areas on May 13 and 18, 2006.  The appearance of the 
typhoon Chanchu cannot be found from the events listed 
in wikipedia on May 10.  However, we can identify the 
appearance of typhoon Chanchu from the description of 
the typhoon appearance such as ?typhoon named? and 
?Typhoon eye.  In addition, the typhoon Chanchu?s path 
can also be inferred from the retrieved collocations such 
as ?Philippine China? and ?near China?. The response of 
bloggers such as ?unexpected typhoon? and ?8 typhoons? 
is also extracted.   
 
 
 
 
 
 
 
 
 
 
Figure 8: Event Summarization for Typhoon Chanchu 
5. Concluding Remarks 
This paper introduces temporal mutual information to 
capture term-term association over time in weblogs. The 
extracted collocation with unusual peak which is in terms 
of relative change of temporal mutual information is 
selected to represent an event.  We collect those 
collocations with the highest product of mutual 
information and change of temporal mutual information 
to summarize the specific event.  The experiments on 
ICWSM weblog data set and evaluation with wikipedia 
event lists at the same period as weblogs demonstrate the 
feasibility of the proposed temporal collocation model 
and event detection algorithms. 
Currently, we do not consider user groups and 
locations. This methodology will be extended to model 
the collocations over time and location, and the 
relationship between the user-preferred usage of 
collocations and the profile of users. 
Acknowledgments 
Research of this paper was partially supported by 
National Science Council, Taiwan (NSC96-2628-E-002-
240-MY3) and Excellent Research Projects of National 
Taiwan University (96R0062-AE00-02). 
References 
Adamic, L.A., Glance, N. (2005). The Political 
Blogosphere and the 2004 U.S. Election: Divided 
They Blog. In: Proceedings of the 3rd International 
Workshop on Link Discovery, pp. 36--43. 
Burger, J.D., Henderson J.C. (2006). An Exploration of 
Observable Features Related to Blogger Age. In: 
Proceedings of AAAI 2006 Spring Symposium on 
Computational Approaches to Analysing Weblogs, pp. 
15--20. 
Doran, C., Griffith, J., Henderson, J. (2006). Highlights 
from 12 Months of Blogs. In: Proceedings of AAAI 
2006 Spring Symposium on Computational 
Approaches to Analysing Weblogs, pp. 30--33. 
Glance, N., Hurst, M., Tornkiyo, T. (2004). Blogpulse: 
Automated Trend Discovery for Weblogs. In: 
Proceedings of WWW 2004 Workshop on the 
Weblogging Ecosystem: Aggregation, Analysis, and 
Dynamics. 
Hurst, M. (2006). 24 Hours in the Blogosphere. In: 
Proceedings of AAAI 2006 Spring Symposium on 
Computational Approaches to Analysing Weblogs, pp. 
73--77. 
ICWSM (2007). http://www.icwsm.org/data.html 
Kim, J.H. (2005). Blog as an Oppositional Medium? A 
Semantic Network Analysis on the Iraq War Blogs. In: 
Internet Research 6.0: Internet Generations. 
 
Manning, C.D., Sch?tze, H. (1999). Foundations of 
Statistical Natural Language Processing, The MIT 
Press, London England. 
Mei, Q., Liu, C., Su, H., Zhai, C. (2006). A Probabilistic 
Approach to Spatiotemporal Theme Pattern Mining on 
Weblogs. In: Proceedings of the 15th International 
Conference on World Wide Web, Edinburgh, Scotland, 
pp. 533--542. 
Oka, M., Abe, H., Kato, K. (2006). Extracting Topics 
from Weblogs Through Frequency Segments. In: 
Proceedings of WWW 2006 Annual Workshop on the 
Weblogging Ecosystem: Aggregation, Analysis, and 
Dynamics. 
Teng, C.Y., Chen, H.H. (2006). Detection of Bloggers? 
Interest: Using Textual, Temporal, and Interactive 
Features. In: Proceeding of IEEE/WIC/ACM 
International Conference on Web Intelligence, pp. 
366--369. 
Teng, C.Y., Chen, H.H. (2007). Analyzing Temporal 
Collocations in Weblogs. In: Proceeding of 
International Conference on Weblogs and Social 
Media, 303--304. 
Thelwall, M. (2006). Blogs During the London Attacks: 
Top Information Sources and Topics. In: Proceedings 
of 3rd Annual Workshop on the Weblogging 
Ecosystem: Aggregation, Analysis and Dynamics. 
Thompson, G. (2003). Weblogs, Warblogs, the Public 
Sphere, and Bubbles. Transformations, 7(2). 
