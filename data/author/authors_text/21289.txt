Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 924?932,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Parsing Graphs with Hyperedge Replacement Grammars
David Chiang
Information Sciences Institute
University of Southern California
Jacob Andreas
Columbia University
University of Cambridge
Daniel Bauer
Department of Computer Science
Columbia University
Karl Moritz Hermann
Department of Computer Science
University of Oxford
Bevan Jones
University of Edinburgh
Macquarie University
Kevin Knight
Information Sciences Institute
University of Southern California
Abstract
Hyperedge replacement grammar (HRG)
is a formalism for generating and trans-
forming graphs that has potential appli-
cations in natural language understand-
ing and generation. A recognition al-
gorithm due to Lautemann is known to
be polynomial-time for graphs that are
connected and of bounded degree. We
present a more precise characterization of
the algorithm?s complexity, an optimiza-
tion analogous to binarization of context-
free grammars, and some important im-
plementation details, resulting in an algo-
rithm that is practical for natural-language
applications. The algorithm is part of Boli-
nas, a new software toolkit for HRG pro-
cessing.
1 Introduction
Hyperedge replacement grammar (HRG) is a
context-free rewriting formalism for generating
graphs (Drewes et al, 1997), and its synchronous
counterpart can be used for transforming graphs
to/from other graphs or trees. As such, it has great
potential for applications in natural language un-
derstanding and generation, and semantics-based
machine translation (Jones et al, 2012). Fig-
ure 1 shows some examples of graphs for natural-
language semantics.
A polynomial-time recognition algorithm for
HRGs was described by Lautemann (1990), build-
ing on the work of Rozenberg and Welzl (1986)
on boundary node label controlled grammars, and
others have presented polynomial-time algorithms
as well (Mazanek and Minas, 2008; Moot, 2008).
Although Lautemann?s algorithm is correct and
tractable, its presentation is prefaced with the re-
mark: ?As we are only interested in distinguish-
ing polynomial time from non-polynomial time,
the analysis will be rather crude, and implemen-
tation details will be explicated as little as possi-
ble.? Indeed, the key step of the algorithm, which
matches a rule against the input graph, is described
at a very high level, so that it is not obvious (for a
non-expert in graph algorithms) how to implement
it. More importantly, this step as described leads
to a time complexity that is polynomial, but poten-
tially of very high degree.
In this paper, we describe in detail a more effi-
cient version of this algorithm and its implementa-
tion. We give a more precise complexity analysis
in terms of the grammar and the size and maxi-
mum degree of the input graph, and we show how
to optimize it by a process analogous to binariza-
tion of CFGs, following Gildea (2011). The re-
sulting algorithm is practical and is implemented
as part of the open-source Bolinas toolkit for hy-
peredge replacement grammars.
2 Hyperedge replacement grammars
We give a short example of how HRG works, fol-
lowed by formal definitions.
2.1 Example
Consider a weighted graph language involving just
two types of semantic frames (want and believe),
two types of entities (boy and girl), and two roles
(arg0 and arg1). Figure 1 shows a few graphs from
this language.
Figure 2 shows how to derive one of these
graphs using an HRG. The derivation starts with
a single edge labeled with the nonterminal sym-
bol S . The first rewriting step replaces this edge
with a subgraph, which we might read as ?The
924
boy?girl?
want? arg0
arg1
boy?
believe? arg1
want?
believe? arg1
want? arg1
girl?
arg0
boy?
arg0
arg1arg0
Figure 1: Sample members of a graph language,
representing the meanings of (clockwise from up-
per left): ?The girl wants the boy,? ?The boy is
believed,? and ?The boy wants the girl to believe
that he wants her.?
boy wants something (X) involving himself.? The
second rewriting step replaces the X edge with an-
other subgraph, which we might read as ?The boy
wants the girl to believe something (Y) involving
both of them.? The derivation continues with a
third rewriting step, after which there are no more
nonterminal-labeled edges.
2.2 Definitions
The graphs we use in this paper have edge labels,
but no node labels; while node labels are intu-
itive for many graphs in NLP, using both node and
edge labels complicates the definition of hyper-
edge grammar and algorithms. All of our graphs
are directed (ordered), as the purpose of most
graph structures in NLP is to model dependencies
between entities.
Definition 1. An edge-labeled, ordered hyper-
graph is a tuple H = ?V, E, ??, where
? V is a finite set of nodes
? E ? V+ is a finite set of hyperedges, each of
which connects one or more distinct nodes
? ? : E ? C assigns a label (drawn from the
finite set C) to each edge.
For brevity we use the terms graph and hyper-
graph interchangeably, and similarly for edge and
hyperedge. In the definition of HRGs, we will use
the notion of hypergraph fragments, which are the
elementary structures that the grammar assembles
into hypergraphs.
Definition 2. A hypergraph fragment is a tuple
?V, E, ?, X?, where ?V, E, ?? is a hypergraph and
X ? V+ is a list of distinct nodes called the ex-
ternal nodes.
The function of graph fragments in HRG is
analogous to the right-hand sides of CFG rules
and to elementary trees in tree adjoining gram-
mars (Joshi and Schabes, 1997). The external
nodes indicate how to integrate a graph into an-
other graph during a derivation, and are analogous
to foot nodes. In diagrams, we draw them with a
black circle ( ).
Definition 3. A hyperedge replacement grammar
(HRG) is a tuple G = ?N,T, P, S ? where
? N and T are finite disjoint sets of nonterminal
and terminal symbols
? S ? N is the start symbol
? P is a finite set of productions of the form
A ? R, where A ? N and R is a graph frag-
ment over N ? T .
We now describe the HRG rewriting mecha-
nism.
Definition 4. Given a HRG G, we define the re-
lation H ?G H? (or, H? is derived from H in one
step) as follows. Let e = (v1 ? ? ? vk) be an edge in
H with label A. Let (A? R) be a production ofG,
where R has external nodes XR = (u1 ? ? ? uk). Then
we write H ?G H? if H? is the graph formed by
removing e from H, making an isomorphic copy
of R, and identifying vi with (the copy of) ui for
i = 1, . . . , k.
Let H ??G H? (or, H? is derived from H) be thereflexive, transitive closure of?G. The graph lan-
guage of a grammar G is the (possibly infinite) set
of graphs H that have no edges with nonterminal
labels such that
S ??G H.
When a HRG rule (A ? R) is applied to an
edge e, the mapping of external nodes in R to the
925
1
X ?
believe? arg1
girl?
arg0
1
Y
1 2
Y
?
12
want?
arg0
arg1
S
1
boy?
X
want? arg1
arg0
2 believe? arg1
want? arg1
girl?
arg0
boy?
arg0
Y
3
want?
believe? arg1
want? arg1
girl?
arg0
boy?
arg0
arg1arg0
Figure 2: Derivation of a hyperedge replacement grammar for a graph representing the meaning of ?The
boy wants the girl to believe that he wants her.?
nodes of e is implied by the ordering of nodes
in e and XR. When writing grammar rules, we
make this ordering explicit by writing the left hand
side of a rule as an edge and indexing the external
nodes of R on both sides, as shown in Figure 2.
HRG derivations are context-free in the sense
that the applicability of each production depends
on the nonterminal label of the replaced edge only.
This allows us to represent a derivation as a deriva-
tion tree, and sets of derivations of a graph as a
derivation forest (which can in turn represented as
hypergraphs). Thus we can apply many of the
methods developed for other context free gram-
mars. For example, it is easy to define weighted
and synchronous versions of HRGs.
Definition 5. If K is a semiring, a K-weighted
HRG is a tuple G = ?N,T, P, S , ??, where
?N, T, P, S ? is a HRG and ? : P ? K assigns a
weight in K to each production. The weight of a
derivation ofG is the product of the weights of the
productions used in the derivation.
We defer a definition of synchronous HRGs un-
til Section 4, where they are discussed in detail.
3 Parsing
Lautemann?s recognition algorithm for HRGs is a
generalization of the CKY algorithm for CFGs.
Its key step is the matching of a rule against the
input graph, analogous to the concatenation of
two spans in CKY. The original description leaves
open how this matching is done, and because it
tries to match the whole rule at once, it has asymp-
totic complexity exponential in the number of non-
terminal edges. In this section, we present a re-
finement that makes the rule-matching procedure
explicit, and because it matches rules little by lit-
tle, similarly to binarization of CFG rules, it does
so more efficiently than the original.
Let H be the input graph. Let n be the number of
nodes in H, and d be the maximum degree of any
node. Let G be a HRG. For simplicity, we assume
that the right-hand sides of rules are connected.
This restriction entails that each graph generated
by G is connected; therefore, we assume that H is
connected as well. Finally, let m be an arbitrary
node of H called the marker node, whose usage
will become clear below.1
3.1 Representing subgraphs
Just as CKY deals with substrings (i, j] of the in-
put, the HRG parsing algorithm deals with edge-
induced subgraphs I of the input. An edge-
induced subgraph of H = ?V, E, ?? is, for some
1To handle the more general case where H is not con-
nected, we would need a marker for each component.
926
subset E? ? E, the smallest subgraph containing
all edges in E?. From now on, we will assume that
all subgraphs are edge-induced subgraphs.
In CKY, the two endpoints i and j com-
pletely specify the recognized part of the input,
wi+1 ? ? ?w j. Likewise, we do not need to store all
of I explicitly.
Definition 6. Let I be a subgraph of H. A bound-
ary node of I is a node in I which is either a node
with an edge in H\I or an external node. A bound-
ary edge of I is an edge in I which has a boundary
node as an endpoint. The boundary representation
of I is the tuple ?bn(I), be(I, v),m ? I?, where
? bn(I) is the set of boundary nodes of I
? be(I, v) be the set of boundary edges of v in I
? (m ? I) is a flag indicating whether the
marker node is in I.
The boundary representation of I suffices to
specify I compactly.
Proposition 1. If I and I? are two subgraphs of H
with the same boundary representation, then I =
I?.
Proof. Case 1: bn(I) is empty. If m ? I and m ? I?,
then all edges of H must belong to both I and I?,
that is, I = I? = H. Otherwise, if m < I and m < I?,
then no edges can belong to either I or I?, that is,
I = I? = ?.
Case 2: bn(I) is nonempty. Suppose I , I?;
without loss of generality, suppose that there is an
edge e that is in I \ I?. Let ? be the shortest path
(ignoring edge direction) that begins with e and
ends with a boundary node. All the edges along ?
must be in I \ I?, or else there would be a boundary
node in the middle of ?, and ? would not be the
shortest path from e to a boundary node. Then, in
particular, the last edge of ?must be in I \ I?. Since
it has a boundary node as an endpoint, it must be a
boundary edge of I, but cannot be a boundary edge
of I?, which is a contradiction. 
If two subgraphs are disjoint, we can use their
boundary representations to compute the boundary
representation of their union.
Proposition 2. Let I and J be two subgraphs
whose edges are disjoint. A node v is a boundary
node of I ? J iff one of the following holds:
(i) v is a boundary node of one subgraph but not
the other
(ii) v is a boundary node of both subgraphs, and
has an edge which is not a boundary edge of
either.
An edge is a boundary edge of I ? J iff it has a
boundary node of I ? J as an endpoint and is a
boundary edge of I or J.
Proof. (?) v has an edge in either I or J and an
edge e outside both I and J. Therefore it must be a
boundary node of either I or J. Moreover, e is not
a boundary edge of either, satisfying condition (ii).
(?) Case (i): without loss of generality, assume
v is a boundary node of I. It has an edge e in I, and
therefore in I ? J, and an edge e? outside I, which
must also be outside J. For e < J (because I and
J are disjoint), and if e? ? J, then v would be a
boundary node of J. Therefore, e? < I ? J, so v is
a boundary node of I ? J. Case (ii): v has an edge
in I and therefore I ? J, and an edge not in either
I or J. 
This result leads to Algorithm 1, which runs in
time linear in the number of boundary nodes.
Algorithm 1 Compute the union of two disjoint
subgraphs I and J.
for all v ? bn(I) do
E ? be(I, v) ? be(J, v)
if v < bn(J) or v has an edge not in E then
add v to bn(I ? J)
be(I ? J, v)? E
for all v ? bn(J) do
if v < bn(I) then
add v to bn(I ? J)
be(I ? J, v)? be(I, v) ? be(J, v)
(m ? I ? J)? (m ? I) ? (m ? J)
In practice, for small subgraphs, it may be more
efficient simply to use an explicit set of edges in-
stead of the boundary representation. For the Geo-
Query corpus (Tang and Mooney, 2001), whose
graphs are only 7.4 nodes on average, we gener-
ally find this to be the case.
3.2 Treewidth
Lautemann?s algorithm tries to match a rule
against the input graph all at once. But we can op-
timize the algorithm by matching a rule incremen-
tally. This is analogous to the rank-minimization
problem for linear context-free rewriting systems.
Gildea has shown that this problem is related to
927
the notion of treewidth (Gildea, 2011), which we
review briefly here.
Definition 7. A tree decomposition of a graph
H = ?V, E? is a tree T , each of whose nodes ?
is associated with sets V? ? V and E? ? E, with
the following properties:
1. Vertex cover: For each v ? V , there is a node
? ? T such that v ? V?.
2. Edge cover: For each e = (v1 ? ? ? vk) ? E,
there is exactly one node ? ? T such that e ?
E?. We say that ? introduces e. Moreover,
v1, . . . , vk ? V?.
3. Running intersection: For each v ? V , the set
{? ? T | v ? V?} is connected.
The width of T is max |V?| ? 1. The treewidth of H
is the minimal width of any tree decomposition
of H.
A tree decomposition of a graph fragment
?V, E, X? is a tree decomposition of ?V, E? that has
the additional property that all the external nodes
belong to V? for some ?. (Without loss of general-
ity, we assume that ? is the root.)
For example, Figure 3b shows a graph, and Fig-
ure 3c shows a tree decomposition. This decom-
position has width three, because its largest node
has 4 elements. In general, a tree has width one,
and it can be shown that a graph has treewidth at
most two iff it does not have the following graph
as a minor (Bodlaender, 1997):
K4 =
Finding a tree decomposition with minimal
width is in general NP-hard (Arnborg et al, 1987).
However, we find that for the graphs we are inter-
ested in in NLP applications, even a na??ve algo-
rithm gives tree decompositions of low width in
practice: simply perform a depth-first traversal of
the edges of the graph, forming a tree T . Then,
augment the V? as necessary to satisfy the running
intersection property.
As a test, we extracted rules from the Geo-
Query corpus (Tang and Mooney, 2001) using the
SynSem algorithm (Jones et al, 2012), and com-
puted tree decompositions exactly using a branch-
and-bound method (Gogate and Dechter, 2004)
and this approximate method. Table 1 shows that,
in practice, treewidths are not very high even when
computed only approximately.
method mean max
exact 1.491 2
approximate 1.494 3
Table 1: Mean and maximum treewidths of rules
extracted from the GeoQuery corpus, using exact
and approximate methods.
(a) 0
a
believe? arg1
b
girl?
arg0
1
Y
(b) 0
1
0
b 1
0
a
b 1
arg1
a
b 1
Y
?
0
b
arg0
b
girl?
?
0believe?
?
Figure 3: (a) A rule right-hand side, and (b) a nice
tree decomposition.
Any tree decomposition can be converted into
one which is nice in the following sense (simpli-
fied from Cygan et al (2011)). Each tree node ?
must be one of:
? A leaf node, such that V? = ?.
? A unary node, which introduces exactly one
edge e.
? A binary node, which introduces no edges.
The example decomposition in Figure 3c is nice.
This canonical form simplifies the operation of the
parser described in the following section.
Let G be a HRG. For each production (A ?
R) ? G, find a nice tree decomposition of R and
call it TR. The treewidth of G is the maximum
928
treewidth of any right-hand side in G.
The basic idea of the recognition algorithm is
to recognize the right-hand side of each rule incre-
mentally by working bottom-up on its tree decom-
position. The properties of tree decomposition al-
low us to limit the number of boundary nodes of
the partially-recognized rule.
More formally, let RD? be the subgraph of R in-
duced by the union of E?? for all ?? equal to or
dominated by ?. Then we can show the following.
Proposition 3. Let R be a graph fragment, and as-
sume a tree decomposition of R. All the boundary
nodes of RD? belong to V? ? Vparent(?).
Proof. Let v be a boundary node of RD?. Node v
must have an edge in RD? and therefore in R?? for
some ?? dominated by or equal to ?.
Case 1: v is an external node. Since the root
node contains all the external nodes, by the run-
ning intersection property, both V? and Vparent(?)
must contain v as well.
Case 2: v has an edge not in RD?. Therefore
there must be a tree node not dominated by or
equal to ? that contains this edge, and therefore
v. So by the running intersection property, ? and
its parent must contain v as well. 
This result, in turn, will allow us to bound the
complexity of the parsing algorithm in terms of the
treewidth of G.
3.3 Inference rules
We present the parsing algorithm as a deductive
system (Shieber et al, 1995). The items have
one of two forms. A passive item has the form
[A, I, X], where X ? V+ is an explicit ordering
of the boundary nodes of I. This means that we
have recognized that A ??G I. Thus, the goalitem is [S ,H, ?]. An active item has the form
[A? R, ?, I, ?], where
? (A? R) is a production of G
? ? is a node of TR
? I is a subgraph of H
? ? is a bijection between the boundary nodes
of RD? and those of I.
The parser must ensure that ? is a bijection when
it creates a new item. Below, we use the notation
{e 7? e?} or {e 7? X} for the mapping that sends
each node of e to the corresponding node of e?
or X.
Passive items are generated by the following
rule:
? Root [B? Q, ?, J, ?]
[B, J, X]
where ? is the root of TQ, and X j = ?(XQ, j).
If we assume that the TR are nice, then the in-
ference rules that generate active items follow the
different types of nodes in a nice tree decomposi-
tion:
? Leaf
[A? R, ?, ?, ?]
where ? is a leaf node of TR.
? (Unary) Nonterminal
[A? R, ?1, I, ?] [B, J, X]
[A? R, ?, I ? J, ? ? {e 7? X}]
where ?1 is the only child of ?, and e is intro-
duced by ? and is labeled with nonterminal B.
? (Unary) Terminal
[A? R, ?1, I, ?]
[A? R, ?, I ? {e?}, ? ? {e 7? e?}]
where ?1 is the only child of ?, e is introduced
by ?, and e and e? are both labeled with ter-
minal a.
? Binary
[A? R, ?1, I, ?1] [A? R, ?2, J, ?2]
[A? R, ?, I ? J, ?1 ? ?2]
where ?1 and ?2 are the two children of ?.
In the Nonterminal, Terminal, and Binary rules,
we form unions of subgraphs and unions of map-
pings. When forming the union of two subgraphs,
we require that the subgraphs be disjoint (however,
see Section 3.4 below for a relaxation of this con-
dition). When forming the union of two mappings,
we require that the result be a bijection. If either
of these conditions is not met, the inference rule
cannot apply.
For efficiency, it is important to index the items
for fast access. For the Nonterminal inference
rule, passive items [B, J, X] should be indexed by
key ?B, |bn(J)|?, so that when the next item on the
agenda is an active item [A ? R, ?1, I, ?], we
know that all possible matching passive items are
929
S ?
X
X
X
X ?
a
a a
a
a
(a) (b)
a
a a
a aa
(c)
Figure 4: Illustration of unsoundness in the recog-
nition algorithm without the disjointness check.
Using grammar (a), the recognition algorithm
would incorrectly accept the graph (b) by assem-
bling together the three overlapping fragments (c).
under key ??(e), |e|?. Similarly, active items should
be indexed by key ??(e), |e|? so that they can be
found when the next item on the agenda is a pas-
sive item. For the Binary inference rule, active
items should be indexed by their tree node (?1
or ?2).
This procedure can easily be extended to pro-
duce a packed forest of all possible derivations
of the input graph, representable as a hypergraph
just as for other context-free rewriting formalisms.
The Viterbi algorithm can then be applied to
this representation to find the highest-probability
derivation, or the Inside/Outside algorithm to set
weights by Expectation-Maximization.
3.4 The disjointness check
A successful proof using the inference rules above
builds an HRG derivation (comprising all the
rewrites used by the Nonterminal rule) which de-
rives a graph H?, as well as a graph isomorphism
? : H? ? H (the union of the mappings from all
the items).
During inference, whenever we form the union
of two subgraphs, we require that the subgraphs
be disjoint. This is a rather expensive operation:
it can be done using only their boundary represen-
tations, but the best algorithm we are aware of is
still quadratic in the number of boundary nodes.
Is it possible to drop the disjointness check? If
we did so, it would become possible for the algo-
rithm to recognize the same part of H twice. For
example, Figure 4 shows an example of a grammar
and an input that would be incorrectly recognized.
However, we can replace the disjointness check
with a weaker and faster check such that any
derivation that merges two non-disjoint subgraphs
will ultimately fail, and therefore the derived
graph H? is isomorphic to the input graph H? as
desired. This weaker check is to require, when
merging two subgraphs I and J, that:
1. I and J have no boundary edges in common,
and
2. If m belongs to both I and J, it must be a
boundary node of both.
Condition (1) is enough to guarantee that ? is lo-
cally one-to-one in the sense that for all v ? H?, ?
restricted to v and its neighbors is one-to-one. This
is easy to show by induction: if ?I : I? ? H and
?J : J? ? H are locally one-to-one, then ?I ? ?J
must also be, provided condition (1) is met. Intu-
itively, the consequence of this is that we can de-
tect any place where ? changes (say) from being
one-to-one to two-to-one. So if ? is two-to-one,
then it must be two-to-one everywhere (as in the
example of Figure 4).
But condition (2) guarantees that ? maps only
one node to the marker m. We can show this again
by induction: if ?I and ?J each map only one node
to m, then ?I??J must map only one node to m, by
a combination of condition (2) and the fact that the
inference rules guarantee that ?I , ?J , and ?I ? ?J
are one-to-one on boundary nodes.
Then we can show that, since m is recognized
exactly once, the whole graph is also recognized
exactly once.
Proposition 4. If H and H? are connected graphs,
? : H? ? H is locally one-to-one, and ??1 is de-
fined for some node of H, then ? is a bijection.
Proof. Suppose that ? is not a bijection. Then
there must be two nodes v?1, v?2 ? H? such that
?(v?1) = ?(v?2) = v ? H. We also know that thereis a node, namely, m, such that m? = ??1(m) is de-
fined.2 Choose a path ? (ignoring edge direction)
from v to m. Because ? is a local isomorphism,
we can construct a path from v?1 to m? that mapsto ?. Similarly, we can construct a path from v?2to m? that maps to ?. Let u? be the first node that
these two paths have in common. But u? must have
two edges that map to the same edge, which is a
contradiction. 
2If H were not connected, we would choose the marker in
the same connected component as v.
930
3.5 Complexity
The key to the efficiency of the algorithm is that
the treewidth of G leads to a bound on the number
of boundary nodes we must keep track of at any
time.
Let k be the treewidth of G. The time complex-
ity of the algorithm is the number of ways of in-
stantiating the inference rules. Each inference rule
mentions only boundary nodes of RD? or RD?i , all
of which belong to V? (by Proposition 3), so there
are at most |V?| ? k + 1 of them. In the Nonter-
minal and Binary inference rules, each boundary
edge could belong to I or J or neither. Therefore,
the number of possible instantiations of any infer-
ence rule is in O((3dn)k+1).
The space complexity of the algorithm is the
number of possible items. For each active item
[A? R, ?, I, ?], every boundary node of RD? must
belong to V??Vparent(?) (by Proposition 3). There-
fore the number of boundary nodes is at most k+1
(but typically less), and the number of possible
items is in O((2dn)k+1).
4 Synchronous Parsing
As mentioned in Section 2.2, because HRGs have
context-free derivation trees, it is easy to define
synchronous HRGs, which define mappings be-
tween languages of graphs.
Definition 8. A synchronous hyperedge re-
placement grammar (SHRG) is a tuple G =
?N, T, T ?, P, S ?, where
? N is a finite set of nonterminal symbols
? T and T ? are finite sets of terminal symbols
? S ? N is the start symbol
? P is a finite set of productions of the form
(A? ?R,R?,??), where R is a graph fragment
over N ? T and R? is a graph fragment over
N ? T ?. The relation ? is a bijection linking
nonterminal mentions in R and R?, such that
if e ? e?, then they have the same label. We
call R the source side and R? the target side.
Some NLP applications (for example, word
alignment) require synchronous parsing: given a
pair of graphs, finding the derivation or forest of
derivations that simultaneously generate both the
source and target. The algorithm to do this is a
straightforward generalization of the HRG parsing
algorithm. For each rule (A? ?R,R?,??), we con-
struct a nice tree decomposition of R?R? such that:
? All the external nodes of both R and R? be-
long to V? for some ?. (Without loss of gen-
erality, assume that ? is the root.)
? If e ? e?, then e and e? are introduced by the
same tree node.
In the synchronous parsing algorithm, passive
items have the form [A, I, X, I?, X?] and active
items have the form [A? R : R?, ?, I, ?, I?, ??].
For brevity we omit a re-presentation of all the in-
ference rules, as they are very similar to their non-
synchronous counterparts. The main difference is
that in the Nonterminal rule, two linked edges are
rewritten simultaneously:
[A? R : R?, ?1, I, ?, I?, ??] [B, J, X, J?, X?]
[A? R : R?, ?, I ? J, ? ? {e j 7? X j},
I? ? J?, ?? ? {e?j 7? X?j}]
where ?1 is the only child of ?, e and e? are both
introduced by ? and e ? e?, and both are labeled
with nonterminal B.
The complexity of the parsing algorithm is
again in O((3dn)k+1), where k is now the max-
imum treewidth of the dependency graph as de-
fined in this section. In general, this treewidth will
be greater than the treewidth of either the source or
target side on its own, so that synchronous parsing
is generally slower than standard parsing.
5 Conclusion
Although Lautemann?s polynomial-time extension
of CKY to HRGs has been known for some time,
the desire to use graph grammars for large-scale
NLP applications introduces some practical con-
siderations not accounted for in Lautemann?s orig-
inal presentation. We have provided a detailed de-
scription of our refinement of his algorithm and its
implementation. It runs in O((3dn)k+1) time and
requires O((2dn)k+1) space, where n is the num-
ber of nodes in the input graph, d is its maximum
degree, and k is the maximum treewidth of the
rule right-hand sides in the grammar. We have
also described how to extend this algorithm to
synchronous parsing. The parsing algorithms de-
scribed in this paper are implemented in the Boli-
nas toolkit.3
3The Bolinas toolkit can be downloaded from
?http://www.isi.edu/licensed-sw/bolinas/?.
931
Acknowledgements
We would like to thank the anonymous reviewers
for their helpful comments. This research was sup-
ported in part by ARO grant W911NF-10-1-0533.
References
Stefan Arnborg, Derek G. Corneil, and Andrzej
Proskurowski. 1987. Complexity of finding embed-
dings in a k-tree. SIAM Journal on Algebraic and
Discrete Methods, 8(2).
Hans L. Bodlaender. 1997. Treewidth: Algorithmic
techniques and results. In Proc. 22nd International
Symposium on Mathematical Foundations of Com-
puter Science (MFCS ?97), pages 29?36, Berlin.
Springer-Verlag.
Marek Cygan, Jesper Nederlof, Marcin Pilipczuk,
Micha? Pilipczuk, Johan M. M. van Rooij, and
Jakub Onufry Wojtaszczyk. 2011. Solving connec-
tivity problems parameterized by treewidth in single
exponential time. Computing Research Repository,
abs/1103.0534.
Frank Drewes, Hans-Jo?rg Kreowski, and Annegret Ha-
bel. 1997. Hyperedge replacement graph gram-
mars. In Grzegorz Rozenberg, editor, Handbook of
Graph Grammars and Computing by Graph Trans-
formation, pages 95?162. World Scientific.
Daniel Gildea. 2011. Grammar factorization by
tree decomposition. Computational Linguistics,
37(1):231?248.
Vibhav Gogate and Rina Dechter. 2004. A complete
anytime algorithm for treewidth. In Proceedings of
the Conference on Uncertainty in Artificial Intelli-
gence.
Bevan Jones, Jacob Andreas, Daniel Bauer,
Karl Moritz Hermann, and Kevin Knight. 2012.
Semantics-based machine translation with hyper-
edge replacement grammars. In Proc. COLING.
Aravind K. Joshi and Yves Schabes. 1997. Tree-
adjoining grammars. In Grzegorz Rozenberg and
Arto Salomaa, editors, Handbook of Formal Lan-
guages and Automata, volume 3, pages 69?124.
Springer.
Clemens Lautemann. 1990. The complexity of
graph languages generated by hyperedge replace-
ment. Acta Informatica, 27:399?421.
Steffen Mazanek and Mark Minas. 2008. Parsing of
hyperedge replacement grammars with graph parser
combinators. In Proc. 7th International Work-
shop on Graph Transformation and Visual Modeling
Techniques.
Richard Moot. 2008. Lambek grammars, tree ad-
joining grammars and hyperedge replacement gram-
mars. In Proc. TAG+9, pages 65?72.
Grzegorz Rozenberg and Emo Welzl. 1986. Bound-
ary NLC graph grammars?basic definitions, nor-
mal forms, and complexity. Information and Con-
trol, 69:136?167.
Stuart M. Shieber, Yves Schabes, and Fernando C. N.
Pereira. 1995. Principles and implementation of
deductive parsing. Journal of Logic Programming,
24:3?36.
Lappoon Tang and Raymond Mooney. 2001. Using
multiple clause constructors in inductive logic pro-
gramming for semantic parsing. In Proc. European
Conference on Machine Learning.
932
Proceedings of the ACL 2011 Workshop on Relational Models of Semantics (RELMS 2011), pages 28?36,
Portland, Oregon, USA, June 23, 2011. c?2011 Association for Computational Linguistics
VigNet: Grounding Language in Graphics using Frame Semantics
Bob Coyne and Daniel Bauer and Owen Rambow
Columbia University
New York, NY 10027, USA
{coyne, bauer, rambow}@cs.columbia.edu
Abstract
This paper introduces Vignette Semantics, a
lexical semantic theory based on Frame Se-
mantics that represents conceptual and graph-
ical relations. We also describe a lexical re-
source that implements this theory, VigNet,
and its application in text-to-scene generation.
1 Introduction
Our goal is to build a comprehensive text-to-
graphics system. When considering sentences such
as John is washing an apple and John is washing
the floor, we discover that rather different graphical
knowledge is needed to generate static scenes rep-
resenting the meaning of these two sentences (see
Figure 1): the human actor is assuming different
poses, he is interacting differently with the thing be-
ing washed, and the water, present in both scenes,
is supplied differently. If we consider the types of
knowledge needed for scene generation, we find that
we cannot simply associate a single set of knowl-
edge with the English verb wash. The question
arises: how can we organize this knowledge and
associate it with lexical items, so that the resulting
lexical knowledge base both is usable in a wide-
coverage text-to-graphics system, and can be pop-
ulated with the required knowledge using limited re-
sources?
In this paper, we present a new knowledge base
that we use for text-to-graphics generation. We dis-
tinguish three types of knowledge needed for our
task. The first is conceptual knowledge, which is
knowledge about concepts, often evoked by words.
For example, if I am told John bought an apple, then
I know that that event necessarily also involved the
seller and money. Second, we need world knowl-
Figure 1: Mocked-up scenes using the WASH-SMALL-
FRUIT vignette (?John washes the apple?) and WASH-
FLOOR-W-SPONGE vignette (?John washes the floor?).
edge. For example, apples grow on trees in cer-
tain geographic locations at certain times of the year.
Third, we need grounding knowledge, which tells
us how concepts are related to sensory experiences.
In our application, we model grounding knowledge
with a database of 3-dimensional graphical models.
We will refer to this type of grounding knowledge
as graphical knowledge. An example of grounding
knowledge is knowing that several specific graphical
models represent apple trees.
Conceptual knowledge is already the object of ex-
tensive work in frame semantics; FrameNet (Rup-
penhofer et al, 2010) is an extensive (but not com-
plete) relational semantic encoding of lexical mean-
ing in a frame-semantic conceptual framework. We
use this prior work, both the theory and the resource,
in our work. The encoding of world knowledge has
been the topic of much work in Artificial Intelli-
gence. Our specific contribution in this paper is the
integration of the representation for world knowl-
edge and graphical knowledge into a frame-semantic
approach. In order to integrate these knowledge
types, we extend FrameNet in three manners.
1. Frames describe complex relations between
their frame elements, but these relations, i.e.
28
the internal structure of a frame, is not explic-
itly formulated in frame semantics. FrameNet
frames do not have any intensional meaning
besides the informal English definition of the
frames (and what is expressed by so-called
?frame-to-frame relations?). From the point
of view of graphics generation, internal struc-
ture is necessary. While for many applications
a semantic representation can remain vague, a
scene must contain concrete objects and spatial
relations between them.
2. Some frames are not semantically specific
enough. For example, there is a frame
SELF MOTION, which includes both walk and
swim; these verbs clearly need different graph-
ical realizations, but they are also different
from a general semantic point of view. While
this situation could be remedied by extend-
ing the inventory of frames by adding WALK
and SWIM frames, which would inherit from
SELF MOTION, the situation is more complex.
Consider wash an apple and wash the floor,
discussed above. While the core meaning of
wash is the same in both phrases, the graphi-
cal realization is again very different. However,
we cannot simply create two new frames, since
at some level (though not the graphical level)
the meaning is indeed compositional. We thus
need a new mechanism.
3. FrameNet is a lexical resource that illustrates
how language can be used to refer to frames,
which are abstract definitions of concepts, and
their frame elements. It is not intended to be
a formalism for deep semantic interpretation.
The FrameNet annotations show the frame ele-
ments of frames (e.g. the goal frame element of
the SELF MOTION frame) being filled with text
passages (e.g. into the garden) rather than with
concrete semantic objects (e.g. an ?instance?
of a LOCALE BY USE frame evoked by gar-
den). Because such objects are needed in or-
der to fully represent the meaning of a sentence
and to assert world knowledge, we introduce
semantic nodes which are discourse referents
of lexical items (whereas frames describe their
meanings).
In this paper, we present VigNet, a resource which
extends FrameNet to incorporate world and graph-
ical knowledge. We achieve this goal by address-
ing the three issues above. We first extend frames
by adding more information to them (specifically,
about decomposition relevant to graphical ground-
ing and more precise selectional restrictions). We
call a frame with graphical information a vignette.
We then extend the structure defined by FrameNet
by adding new frames and vignettes, for example
for wash an apple. The result we call VigNet. Fi-
nally, we extend VigNet with a system of nodes
which instantiate frames; these nodes we call se-
mantic nodes. They get their meaning only from the
frames they instantiate. All three extensions are con-
servative extensions of frames and FrameNet. The
semantic theory that VigNet instantiates we call Vi-
gnette Semantics and we believe it to be a conser-
vative extension (and thus in the spirit of) frame se-
mantics.
This paper is structured as follows. In Section 2,
we review frame semantics and FrameNet. Section 3
presents a more detailed description of VigNet, and
we provide examples in Section 4. Since VigNet is
intended to be used in a large-coverage system, the
population of VigNet with knowledge is a crucial is-
sue which we address in Section 5. We discuss re-
lated work in Section 6 and conclude in Section 7.
2 Frame Semantics and FrameNet
Frame Semantics (FS; Fillmore (1982)) is based on
the idea that the meaning of a word can only be fully
understood in context of the entire conceptual struc-
ture surrounding it, called the word?s frame. When
the meaning of a word is evoked in a hearer?s mind
all related concepts are activated simultaneously and
we can rely on this structure to transfer information
in a conversation. Frames can describe states-of-
affairs, events or complex objects. Each frame con-
tains a set of specific frame elements (FEs), which
are labeled semantic argument slots describing par-
ticipants in the frame. For instance, the word buy
evokes the frame for a commercial transaction sce-
nario, which includes a buyer and a seller that ex-
change money for goods. A speaker is aware of what
typical buyers, sellers, and goods are. He may also
have a mental prototype of the visual scenario itself
29
(e.g. standing at a counter in a store). In FS the
role of syntactic theory and the lexicon is to explain
how the syntactic dependents of a word that realizes
a frame (i.e. arguments and adjuncts) are mapped to
frame elements via valence patterns.
FrameNet (FN; Baker et al (1998), Ruppenhofer
et al (2010)) is a lexical resource based on FS.
Frames in FN (around 1000) 1 are defined in terms
of their frame elements, relations to other frames
and semantic types of FEs. Beyond this, the mean-
ing of the frame (how the FEs are related to each
other) is only described in natural language. FN
contains about 11,800 lexical units, which are pair-
ings of words and frames. These come with anno-
tated example sentences (about 150,000) to illustrate
their valence patterns. FN contains a network of
directed frame-to-frame relations. In the INHERI-
TANCE relation a child-frame inherits all semantic
properties from the superframe. The frame rela-
tions SUBFRAME and PRECEDES refer to sub-events
and events following in temporal order respec-
tively. The parent frame?s FEs are mapped to the
child?s FEs. For instance CAUSE TO WAKE inher-
its from TRANSITIVE ACTION and its sleeper FE
maps to agent. Other relations include PERSPEC-
TIVE ON, CAUSATIVE OF, and INCHOATIVE OF.
Frame relations captures important semantic facts
about frames. For instance the hierarchical organi-
zation of INHERITANCE allows to view an event on
varying levels of specificity. Finally, FN contains
a small ontology of semantic types for frame ele-
ments, which can be interpreted as selectional re-
strictions (e.g. an agent frame element must be
filled by a sentient being).
3 Vignette Semantics
In Section 1, we motivated VigNet by the need
for a resource that allows us to relate language to
a grounded semantics, where for us the graphical
representation is a stand-in for grounding. We de-
scribed three reasons for extending FrameNet to Vi-
gNet: we need more meaning in a frame, we need
more frames and more types of frames, and we need
to instantiate frames in a clean manner. We discuss
these refinements in more detail in this section.
1Numbers refer to FrameNet 1.5
? Vignettes are frames that are decomposed into
graphical primitives and can be visualized.
Like other fames they are motivated by frame
semantics; they correspond to a conceptual
structure evoked by the lexical units which are
associated with it.
? VigNet includes individual frames for each
(content) lexical item. This provides finer-
grained semantics than given with FrameNet
frames themselves. These lexically-coupled
frames leverage the existing structure of their
parent frames. For example, the SELF MOTION
frame contains lexical items for run and swim
which have very different meaning even though
they share the same frame and FEs (such as
SOURCE, GOAL, and PATH). We therefore
define frames for RUN and SWIM which in-
herit from SELF MOTION. We assume also that
frames and lexical items that are missing from
FrameNet are defined and linked to the rest of
FrameNet as needed.
? Even more specific frames are created to rep-
resent composed vignettes. These are vi-
gnettes that ground meaning in different ways
than the primitive vignette that they special-
ize. The only motivation for their existence
is the graphical grounding. For example, we
cannot determine how to represent washing an
apple from the knowledge of how to repre-
sent generic washing and an apple. So we de-
fine a new vignette specifically for washing a
small fruit. From the point of view of lexi-
cal semantics, it uses two lexical items (wash
and apple) and their interpretation, but for us,
since we are interested in grounding, it is a
single vignette. Note that it is not necessary
to create specific vignettes for every concrete
verb/argument combination. Because vignettes
are visually inspired relatively few general vi-
gnettes (e.g. manipulate an object on a fixture)
suffices to visualize many possible scenarios.
? A new type of frame-to-frame relation, which
we call SUBFRAME-PARALLEL is used to de-
compose vignettes into a set of more primitive
semantic relations between their arguments.
Unlike FrameNet?s SUBFRAME relation which
30
represents temporally sequential subframes, in
SUBFRAME-PARALLEL, the subframes are all
active at the same time, provide a conceptual
and spatial decomposition of the frame, and can
serve as spatial constraints on the frame ele-
ments. A frame is called a vignette if it can
be decomposed into graphical primitives using
SUBFRAME-PARALLEL relations. For instance
in the vignette WASH-SMALL-OBJ for washing
a small object in a sink, the washer has to be
in front of the sink. We assert a SUBFRAME-
PARALLEL relation between WASH-SMALL-
OBJ and FRONTOF, mapping the washer FE
to the figure FE and sink to ground.
? FrameNet has a very limited number of seman-
tic types that are used to restrict the values
of FEs. Vignette semantics uses selectional
restrictions to differentiate between vignettes
that have the same parent. For example, the
vignette invoked for washing a small object in
a sink would restrict the semantic type of the
theme (the entity being washed) to anything
small, or, more generally, to any object that is
washed in this way (apples, hard-boiled eggs,
etc). The vignette used for washing a vehicle in
a driveway with a hose would restrict its theme
to some set of large objects or vehicle types.
Selectional restrictions are asserted using the
same mechanism as decompositions.
? As mentioned in Section 1, in FrameNet an-
notations frame elements (FEs) are filled with
text spans. Therefore, while frame seman-
tics in general is a deep semantic theory,
FrameNet annotations only represent shallow
semantics and it is not immediately obvious
how FrameNet can be used to build a full se-
mantic representations of a sentence. In Vi-
gnette semantics, when a frame is evoked by
a lexical item, it is instantiated as a semantic
node. Its FEs are then bound not to subphrases,
but to semantic nodes which are the instantia-
tions of the frames evoked by those subphrases.
Section 3.1 investigates semantic nodes in more de-
tail. Section 3.2 illustrates different types of vi-
gnettes (objects, actions, locations) and how they are
defined using the SUBFRAME PARALLEL relation.
In Section 3.3 we discuss selectional restrictions.
3.1 Semantic Nodes and Relational Knowledge
The intuition behind semantic nodes is that they rep-
resent objects, events or situations. They can also
represent plurals or generics. For instance we could
have semantic node city, denoting the class of cities
and a semantic node paris, that denotes the city
Paris. Note that there is also a frame CITY and a
frame PARIS that contain the conceptual structure
associated with the words city and Paris. Frames
represent the linguistic and the conceptual aspect
of knowledge; the intensional meaning of a word.
They provide knowledge to answer questions such
as ?What is an apple?? or ?How do you wash an ap-
ple??. In contrast, semantic nodes are extensional,
i.e. denotations. They represent the knowledge to
answer questions such as ?In what season are apples
harvested?? or ?How did Percy wash that apple just
now??.
As mentioned above semantic nodes allow us to
build full meaning representations of entire sen-
tences in discourse. Therefore, while frame defi-
nitions are fixed, semantic nodes can be added dy-
namically during discourse understanding or gener-
ation to model the instances of frames that language
is evoking. We call such nodes temporary seman-
tic nodes. They they are closely related to the dis-
course referents of Discourse Representation Theory
(Kamp, 1981) and related concepts in other theories.
In contrast, persistent semantic nodes are used to
store world knowledge which is distinct from the
conceptual knowledge encoded within frames and
their relations; for example, the frame for moon will
not encode the fact that the moon?s circumference is
6,790 miles, but we may record that using a knowl-
edge based of external assertions semantic nodes are
given their meaning by corresponding frames (CIR-
CUMFERENCE, MILE, etc.). A temporary semantic
node can become persistent by being retained in the
knowledge base.
3.2 Vignette Types and their Decomposition
A vignette is a frame in the FrameNet sense that is
decomposed to a set of more primitive frames us-
ing the SUBFRAME-PARALLEL frame-to-frame re-
lation. The frame elements (FEs) of a vignette are
31
defined as in FrameNet, except that our grounding
in the graphical representation gives us a new, strong
criterion to choose what the FEs are: they are the ob-
jects necessarily involved in the visual scene associ-
ated with that vignette. The subframes represent the
spatial and other relations between the FEs. The re-
sulting semantic relations specify how the scene el-
ements are spatially arranged. This mechanism cov-
ers several different cases.
For actions, we conceptually freeze the action in
time, much as in a comic book panel, and repre-
sent it in a vignette with a set of objects, spatial
relations between those objects, and poses charac-
teristic for the humans (and other pliable beings) in-
volved in that action. Action vignettes will typically
be specialized to composed vignettes, so that the ap-
plicability of different vignettes with the same par-
ent frame will depend on the values of the FEs of
the parent. In the process of creating composed vi-
gnettes, FEs are often added because additional ob-
jects are required to play auxiliary roles. As a re-
sult, the FEs of an action vignette are the union of
the semantic roles of the important participants and
props involved in that enactment of the action with
the FEs of the parent frame. For instance the follow-
ing vignette describes one concrete way of washing
a small fruit. Note that we have included a new FE
sink which is not motivated in the frame WASH.2
Note also that this vignette also contains a selec-
tional restriction on its theme, which we will dis-
cuss in the next subsection and which is not shown
here.
WASH-SMALL-FRUIT(washer, theme, sink)
FRONTOF(figure:washer, figure:sink)
FACING(figure:washer, figure:sink)
GRASP(grasper:washer, theme:theme)
REACH(reacher:washer, target:sink)
In this notation the head row contains the vignette
name and its FEs in parentheses. For readability we
will often omit FEs that are part of the vignette but
not restricted or used in any mentioned relation. The
lower box contains the vignette decomposition and
implicitly specifies SUBFRAME-PARALLEL frame-
to-frame relations. In the decomposition of a vi-
gnette V we use the notation F(a:b, ? ? ? ) to indicate
that the FE a of frame F is mapped to the FE b of V.
2FrameNet does not currently contain a WASH frame, but if
it did, it would not contain an FE sink.
When V is instantiated the semantic node binding to
a must also be able to bind to b in F.
Locations are represented by vignettes which ex-
press constraints between a set of objects character-
istic for the given location. The FEs of location vi-
gnettes include these constituent objects. For exam-
ple, one type of living room (of many possible ones)
might contain a couch, a coffee table, and a fireplace
in a certain arrangement.
LIVING-ROOM 42(left wall, far wall, couch,
coffee table, fireplace)
TOUCHING(figure:couch, ground:left wall)
FACING(figure:couch, ground:right wall)
FRONTOF(figure:coffee table, ground: sofa)
EMBEDDED(figure:fire-place, ground:far wall)
Even ordinary physical objects will have certain
characteristic parts with size, shape, and spatial re-
lations that can be expressed by vignettes. For ex-
ample, an object type such as a kind of stop sign can
be defined as a two-foot-wide, red, hexagonal metal
sheet displaying the word ?STOP? positioned on the
top of a 6 foot high post.
STOP-SIGN(sign-part, post-part, texture)
MATERIAL(theme:sign-part, material:METAL)
MATERIAL(theme:post-part, material:METAL)
DIAMETER(theme:sign-part, diameter:2 feet)
HEIGHT(theme:post-part, height:6 feet)
ONTOP(figure:sign-part, ground:post-part)
TEXTURE(theme:sign-part, texture:?STOP?)
In addition, many real-world objects do not corre-
spond to lexical items but are elaborations on them
or combinations. These sublexical entities can be
represented by vignettes as well. For example, one
such 3D object in our text-to-scene system is a goat
head mounted on a piece of wood. This object is
represented by a vignette with two FEs (ghead,
gwood) representing the goat?s head and the wood.
The vignette decomposes into ON(ghead, gwood).
While there can be many vignettes for a single
lexical item, representing the many ways a location,
action, or object can be constituted, vignettes need
not be specialized for every particular situation and
can be more or less general. In one exteme creat-
ing vignettes for every verb/argument combination
would clearly lead to a combinatorial explosion and
is not feasible. In the other extreme we can define
rather general vignettes. For example, a vignette
32
USE-TOOL for using a tool on a theme can be repre-
sented by the user GRASPING the tool and REACH-
ING towards the theme. These vignettes can be
used in decompositions of more concrete vignettes
(e.g. HAMMER-NAIL-INTO-WALL). They can also
be used directly if no other more concrete vignette
can be applied (because it does not exist or its selec-
tional restrictions cannot be satisfied). In this way
by defining a small set of such vignettes we can vi-
sualize approximate scenes for a large number of de-
scriptions.
3.3 Selectional Restrictions on Frame Elements
To define a frame we need to specify selectional re-
strictions on the semantic type of its FEs. Instead
of relying on a fixed inventory of semantic types,
we assert conceptual knowledge and external asser-
tions over persistent semantic types. This allows us
to use VigNet?s large set of frames to represent such
knowledge. For example, an apple can be defined as
a small round fruit.
APPLE(self)
SHAPEOF(figure:self, shape:spherical)
SIZEOF(figure:self, size:small)
APPLE is simply a frame that contains a self FE,
which allows us to make assertions about the con-
cept (i.e. about any semantic node bound to the
self FE). Frame elements of this type are not un-
usual in FrameNet, where they are mainly used for
frames containing common nouns (for instance the
Substance FE contains a substance FE). In Vi-
gNet we implicitly use self in all frames, including
frames describing situations and events.
We use the same mechanism to define specialized
compound vignettes such as WASH SMALL FRUIT.
We extend WASH in the following way to restrict
it to small fruits (we abreviate F(self:a) as a=F for
readability).
WASH-SMALL-FRUIT(washer, theme, sink)
% selectional restrictions
sink=SINK, washer=PERSON,
theme=x, x=FRUIT,
SIZEOF(figure:x,size:small)
% decomposition
FRONTOF(figure:washer, figure:sink)
FACING(figure:washer, figure:sink)
GRASP(grasper:washer, theme:theme)
REACH(reacher:washer, target:sink)
4 Examples
In this section we give further examples of visual
action vignettes for the verb wash. The selectional
restrictions and graphical decomposition of these vi-
gnettes vary depending on the type of object be-
ing washed. The first example shows a vignette for
washing a vehicle.
WASH-VEHICLE(washer, theme, instr, location)
washer=PERSON, theme=VEHICLE,
instr=HOSE, location=DRIVEWAY
ONSURFACE(figure:theme, ground:location)
FRONTOF(figure:washer, ground:theme)
FACING(figure:washer, ground:theme)
GRASP(grasper:washer, theme:instrument)
AIM(aimer:washer, theme:instr, target:theme)
The following two vignettes represent a case where
the object being washed alone does not determine
which vignette to apply. If the instrument is unspec-
ified one or the other could be used. We illustrate
one option in figure 1 (right).
WASH-FLOOR-W-SPONGE(washer,theme,instr)
washer=PERSON, theme=FLOOR,
instr=SPONGE
KNEELING(agent:washer),
GRASP(grasper:washer, theme:instr),
REACH(reacher:washer, target:theme)
WASH-FLOOR-W-MOP(washer, theme, instr)
washer=PERSON, theme=FLOOR, instr=MOP
GRASP(grasper:washer, theme:instr),
REACHWITH(reacher:washer, target:theme,
instr:instr)
It is easy to come up with other concrete vi-
gnettes for wash (washing windows, babies, hands,
dishes...). As mentioned in section 3.2 more gen-
eral vignettes can be defined for very broad object
classes. In choosing vignettes, the most specific will
be used (looking at type matching hierarchies), so
general vignettes will only be chosen when more
specific ones are unavailable. The following generic
vignette describes washing any large object.
WASH-LARGE-OBJECT(washer, theme instrument)
washer=PERSON, theme=OBJECT,
instrument=SPONGE,
SIZEOF(figure:theme, size:large)
FACING(figure:washer, ground:theme)
GRASP(grasper:washer, theme:instrument)
REACH(reacher:washer, target:theme)
33
In our final example, a vignette for picking fruit uses
the following assertion of world knowledge about
particular types of fruit and the trees they come
from:
SOURCE-OF(theme:x, source:y), APPLE(self:x),
APPLETREE(self:y)
In matching the vignette to the verb frame and its ar-
guments, the source frame element is bound to the
type of tree for the given theme (fruit).
PICK-FRUIT(picker, theme, source)
picker=PERSON, theme=FRUIT, source=TREE,
SOURCEOF(theme:theme, source:source)
UNDERCANOPY(figure:picker, canopy:source)
GRASP(grasper:picker, theme:theme)
REACH(reacher:picker, target:source.branch)
5 VigNet
We are developing VigNet as a general purpose re-
source, but with the specific goal of using it in text-
to-scene generation. In this section we first describe
various methods to populate VigNet. We then sketch
how we create graphical representations from Vi-
gNet meaning representations.
5.1 Populating VigNet
VigNet is being populated using several approaches:
? Amazon Mechanical Turk is being used to ac-
quire scene elements for location and action vi-
gnettes as well as the spatial relations among
those elements. For locations, Turkers are
shown representative pictures of different lo-
cations as well as variants of similar locations,
thereby providing distinct vignettes for each lo-
cation. We also use Mechanical Turk to acquire
general purpose relational information for ob-
jects and actions such as default locations, ma-
terials, contents, and parts.
? We extract relations such as typical locations
for actions from corpora based on co-occurance
patterns of location and action terms. This is
based on ideas described in (Sproat, 2001). We
also rely on corpora to induce new lexical units
and selectional preferences.
? A large set of semantic nodes and frames for
nouns has been imported from the noun lexicon
of the WordsEye text-to-scene system (Coyne
and Sproat, 2001). This lexicon currently con-
tains 15,000 lexical items and is tied to a li-
brary of 2,200 3D objects and 10,000 images
Semantic relations between these nodes include
parthood, containment, size, style (e.g. antique
or modern), overall shape, material, as well as
spatial tags denoting important spatial regions
on the object. We also import graphically-
oriented vignettes from WordsEye. These are
used to capture the meaning of sub-lexical 3D
objects such as the mounted goat head de-
scribed earlier.
? Finally, we intend to use WordsEye itself to al-
low users to visualize vignettes as they define
them, as a way to improve vignette accuracy
and relevancy to the actual use of the system.
While the population of VigNet is not the fo-
cus of this paper, it is our goal to create a usable
resource that can be populated with a reasonable
amount of effort. We note that opposed to resources
like FrameNet that require skilled lexicographers,
we only need simple visual annotation that can eas-
ily be done by untrained Mechanical Turkers. In
addition, as described in section 3.2, vignettes de-
fined at more abstract levels of the frame hierar-
chy can be used and composed to cover large num-
bers of frames in a plausible manner. This allows
more specific vignettes to be defined where the dif-
ferences are most significant. VigNet is is focused
on visually-oriented language involving tangible ob-
jects. However, abstract, process-oriented language
and relations such as negation can be depicted icon-
ically with general vignettes. Examples of these can
be seen in the figurative and metaphorical depictions
shown in (Coyne and Sproat, 2001).
5.2 Using VigNet in Text-to-Scene Generation
To compose a scene from text input such as the
man is washing the apple it is necessary to parse
the sentence into a semantic representation (evoking
frames for each content word) and to then resolve
the language-level semantics to a set of graphical
entities and relations. To create a low-level graph-
ical representation all frame elements need to be
filled with appropriate semantic nodes. Frames sup-
port the selection of these nodes by specifying con-
straints on them using selectional restrictions. The
34
SUBFRAME-PARALLEL decomposition of vignettes
then ultimately relates these nodes using elementary
spatial vignettes (FRONTOF, ON, ...).
Note that it is possible to describe scenes directly
using these vignettes (such as The man is in front of
the sink. He is holding an apple.), as was used to
create the mock-ups in figure 1.
Vignettes can be directly applied or composed to-
gether. Composing vignettes involves unifying their
frame elements. For example, in washing an ap-
ple, the WASH-SMALL-FRUIT vignette uses a sink.
From world knowledge we know (via instances of
the TYPICAL-LOCATION frame) that washing food
typically takes place in the KITCHEN. To create a
scene we compose the two vignettes together by uni-
fying the sink in the location vignette with the sink
in the action vignette.
6 Related Work
The grounding of natural language to graphical re-
lations has been investigated in very early text-to-
scene systems (Boberg, 1972), (Simmons, 1975),
(Kahn, 1979), (Adorni et al, 1984), and then later
in Put (Clay and Wilhelms, 1996), and WordsEye
(Coyne and Sproat, 2001). Other systems, such as
CarSim (Dupuy et al, 2001), Jack (Badler et al,
1998), and CONFUCIUS (Ma and McKevitt, 2006)
target animation and virtual environments rather
than scene construction. A graphically grounded
lexical-semantic resource such as VigNet would be
of use to these and related domains. The concept of
vignettes as graphical realizations of more general
frames was introduced in (Coyne et al, 2010).
In addition to FrameNet, much work has been
done in developing theories and resources for lexi-
cal semantics and common-sense knowledge. Verb-
Net (Kipper et al, 2000) focuses on verb subcat pat-
terns grouped by Levin verb classes (Levin, 1993),
but also grounds verb semantics into a small num-
ber of causal primitives representing temporal con-
straints tied to causality and state changes. VerbNet
lacks the ability to compose semantic constraints
or use arbitrary semantic relations in those con-
straints. Conceptual Dependency theory (Schank
and Abelson, 1977) specifies a small number of
state-change primitives into which all verbs are re-
duced. Event Logic (Siskind, 1995) decomposes ac-
tions into intervals describing state changes and al-
lows visual grounding by specifying truth conditions
for a small set of spatial primitives (a similar for-
malism is used by Ma and McKevitt (2006)). (Bai-
ley et al, 1998) and related work proposes a rep-
resentation in many ways similar to ours, in which
lexical items are paired with a detailed specifica-
tion of actions in terms of elementary body poses
and movements. In contrast to these temporally-
oriented approaches, VigNet grounds semantics in
spatial constraints active at a single moment in time.
This allows for and emphasizes contextual reason-
ing rather than causal reasoning. In addition, VigNet
emphasizes a holistic frame semantic perspective,
rather than emphasizing decomposition alone. Sev-
eral resources for common-sense knowledge exist or
have been proposed. In OpenMind and ConceptNet
(Havasi et al, 2007) online crowd-sourcing is used
to collect a large set of common-sense assertions.
These assertions are normalized into a set of a cou-
ple dozen relations. The Cyc project is using the web
to augment its large ontology and knowledge base of
common sense knowledge (Matuszek et al, 2005).
PRAXICON (Pastra, 2008) is a grounded concep-
tual resources that integrates motor-sensoric, visual,
pragmatic and lexical knowledge (via WordNet). It
targets the embodied robotics community and does
not directly focus on scene generation. It also fo-
cuses on individual lexical items, while VigNet, like
FrameNet, takes syntactic context into account.
7 Conclusion
We have described a new semantic paradigm that we
call vignette semantics. Vignettes are extensions of
FrameNet frames and represent the specific ways in
which semantic frames can be realized in the world.
Mapping frames to vignettes involves translating be-
tween high-level frame semantics and the lower-
level relations used to compose a scene. Knowledge
about objects, both in terms of their semantic types
and the affordances they provide is used to make that
translation. FrameNet frames, coupled with seman-
tic nodes representing entity classes, provide a pow-
erful relational framework to express such knowl-
edge. We are developing a new resource VigNet
which will implement this framework and be used
in our text-to-scene generation system.
35
References
G. Adorni, M. Di Manzo, and F. Giunchiglia. 1984. Nat-
ural Language Driven Image Generation. In Proceed-
ings of COLING 1984, pages 495?500, Stanford, CA.
N. Badler, R. Bindiganavale, J. Bourne, M. Palmer, J. Shi,
and W. Schule. 1998. A parameterized action rep-
resentation for virtual human agents. In Workshop
on Embodied Conversational Characters, Tahoe City,
CA.
D. Bailey, N. Chang, J. Feldman, and S. Narayanan.
1998. Extending Embodied Lexical Development. In
Proceedings of the Annual Meeting of the Cognitive
Science Society, Madison, WI.
C. Baker, C. Fillmore, and J. Lowe. 1998. The Berkeley
Framenet Project. In Proceedings of COLING 1998,
pages 86?90.
R. Boberg. 1972. Generating line drawings from ab-
stract scene descriptions. Master?s thesis, Dept. of
Elec. Eng, MIT, Cambridge, MA.
S. R. Clay and J. Wilhelms. 1996. Put: Language-based
interactive manipulation of objects. IEEE Computer
Graphics and Applications, 16(2):31?39.
B. Coyne and R. Sproat. 2001. WordsEye: An automatic
text-to-scene conversion system. In Proceedings of
the Annual Conference on Computer Graphics, pages
487?496, Los Angeles, CA.
B. Coyne, O. Rambow, J. Hirschberg, and R. Sproat.
2010. Frame Semantics in Text-to-Scene Generation.
In Proceedings of the KES?10 workshop on 3D Visual-
isation of Natural Language, Cardiff, Wales.
S. Dupuy, A. Egges, V. Legendre, and P. Nugues. 2001.
Generating a 3D Simulation Of a Car Accident from a
Written Description in Natural Language: The CarSim
System. In Proceedings of ACL Workshop on Tem-
poral and Spatial Information Processing, pages 1?8,
Toulouse, France.
C. J. Fillmore. 1982. Frame semantics. In Linguistic
Society of Korea, editor, Linguistics in the Morning
Calm, pages 111?137. Hanshin Publishing Company,
Seoul.
C. Havasi, R. Speer, and J. Alonso. 2007. ConceptNet 3:
a Flexible, Multilingual Semantic Network for Com-
mon Sense Knowledge. In Proceedings of RANLP
2007, Borovets, Bulgaria.
K. Kahn. 1979. Creation of Computer Animation from
Story Descriptions. Ph.D. thesis, MIT, AI Lab, Cam-
bridge, MA.
H. Kamp. 1981. A Theory of Truth and Semantic Rep-
resentation. In Groenendijk, J. and Janssen, T. and
Stokhof, M., editor, Formal Methods in the Study of
Language, pages 277?322. de Gruyter, Amsterdam.
K. Kipper, H. T. Dang, and M. Palmer. 2000. Class-
Based Construction of a Verb Lexicon. In Proceedings
of AAAI 2000, Austin, TX.
B. Levin. 1993. English verb classes and alternations:
a preliminary investigation. University Of Chicago
Press.
M. Ma and P. McKevitt. 2006. Virtual human anima-
tion in natural language visualisation. Artificial Intel-
ligence Review, 25:37?53, April.
C. Matuszek, M. Witbrock, R. C. Kahlert, J. Cabral,
D. Schneider, P. Shah, and D. Lenat. 2005. Search-
ing for Common Sense: Populating Cyc from the Web.
In Proceedings of AAAI 2005, pages 1430?1435, Pitts-
burgh, PA.
K. Pastra. 2008. PRAXICON: The Development of a
Grounding Resource. In Proceedings of the Interna-
tional Workshop on Human-Computer Conversation,
Bellagio, Italy.
J. Ruppenhofer, M. Ellsworth, M. Petruck, C. R. John-
son, and J. Scheffczyk. 2010. Framenet II: Extended
Theory and Practice. ICSI Berkeley.
R. C. Schank and R. Abelson. 1977. Scripts, Plans,
Goals, and Understanding. Earlbaum, Hillsdale, NJ.
R. Simmons. 1975. The CLOWNS Microworld. In Pro-
ceedings of the Workshop on Theoretical Issues in Nat-
ural Language Processing, pages 17?19, Cambridge,
MA.
J. M. Siskind. 1995. Grounding language in perception.
Artificial Intelligence Review, 8:371?391.
R. Sproat. 2001. Inferring the environment in a text-to-
scene conversion system. In International Conference
on Knowledge Capture, Victoria, BC.
36
Proceedings of the 2014 Workshop on the Use of Computational Methods in the Study of Endangered Languages, pages 6?14,
Baltimore, Maryland, USA, 26 June 2014.
c?2014 Association for Computational Linguistics
Documenting Endangered Languages with the WordsEye Linguistics Tool
Morgan Ulinski
?
mulinski@cs.columbia.edu
Anusha Balakrishnan
?
ab3596@columbia.edu
Daniel Bauer
?
bauer@cs.columbia.edu
Bob Coyne
?
coyne@cs.columbia.edu
Julia Hirschberg
?
julia@cs.columbia.edu
Owen Rambow
?
rambow@ccls.columbia.edu
?
Department of Computer Science
?
CCLS
Columbia University
New York, NY, USA
Abstract
In this paper, we describe how field lin-
guists can use the WordsEye Linguistics
Tool (WELT) to study endangered lan-
guages. WELT is a tool under devel-
opment for eliciting endangered language
data and formally documenting a lan-
guage, based on WordsEye (Coyne and
Sproat, 2001), a text-to-scene generation
tool that produces 3D scenes from text in-
put. First, a linguist uses WELT to create
elicitation materials and collect language
data. Next, he or she uses WELT to for-
mally document the language. Finally, the
formal models are used to create a text-
to-scene system that takes input in the en-
dangered language and generates a picture
representing its meaning.
1 Introduction
Although languages have appeared and disap-
peared throughout history, today languages are
facing extinction at an unprecedented pace. Over
40% of the estimated 7,000 languages in the world
are at risk of disappearing. When languages die,
we lose access to an invaluable resource for study-
ing the culture, history, and experience of people
who spoke them (Alliance for Linguistic Diversity,
2013). Efforts to document languages and develop
tools to support these efforts become even more
important with the increasing rate of extinction.
Bird (2009) emphasizes a particular need to make
use of computational linguistics during fieldwork.
To address this issue, we are developing the
WordsEye Linguistics Tool, WELT. In one mode
of operation, we provide field linguists with tools
for building elicitation sessions based on custom
3D scenes. In another, we provide a way to for-
mally document the endangered language. For-
mal hypotheses can be verified using a text-to-
scene system that takes input in the endangered
language, analyzes it based on the formal model,
and generates a picture representing the meaning.
WELT provides important advantages to field
linguists for elicitation over the current practice of
using a set of pre-fabricated static pictures. Using
WELT the linguist can create and modify scenes
in real time, based on informants? responses, cre-
ating follow-up questions and scenes to support
them. Since the pictures WELT supports are 3D
scenes, the viewpoint can easily be changed, al-
lowing exploration of linguistic descriptions based
on different frames of reference, as for elicitations
of spatial descriptions. Finally, since scenes and
objects can easily be added in the field, the lin-
guist can customize the images used for elicitation
to be maximally relevant to the current informants.
Creating a text-to-scene system for an endan-
gered language with WELT also has advantages.
First, WELT allows documentation of the seman-
tics of a language in a formal way. Linguists can
customize the focus of their studies to be as deep
or shallow as they wish; however, we believe that a
major advantage of documenting a language with
WELT is that it enables studies that are much more
precise. The fact that a text-to-scene system is cre-
ated from this documentation will allow linguists
to test the theories they develop with native speak-
ers, making changes to grammars and semantics
in real time. The resulting text-to-scene system
can also be an important tool for language preser-
vation, spreading interest in the language among
younger generations of the community and re-
cruiting new speakers.
In this paper, we discuss the WELT toolkit and
its intended use, with examples from Arrernte and
Nahuatl. In Section 2 we discuss prior work on
field linguistics computational tools. In Section 3
we present an overview of the WELT system. We
describe using WELT for elicitation in Section 4
and describe the tools for language documentation
in Section 5. We conclude in Section 6.
6
2 Related Work
Computational tools for field linguistics fall into
two categories: tools for native speakers to use
directly, without substantial linguist intervention,
and tools for field linguists to use. Tools intended
for native speakers include the PAWS starter kit
(Black and Black, 2009), which uses the answers
to a series of guided questions to produce a draft
of a grammar. Similarly, Bird and Chiang (2012)
describe a simplified workflow and supporting MT
software that lets native speakers produce useable
documentation of their language on their own.
One of the most widely-used toolkits in the lat-
ter category is SIL FieldWorks (SIL FieldWorks,
2014), or specifically, FieldWorks Language Ex-
plorer (FLEx). FLEx includes tools for elicit-
ing and recording lexical information, dictionary
development, interlinearization of texts, analysis
of discourse features, and morphological analy-
sis. An important part of FLEx is its ?linguist-
friendly? morphological parser (Black and Si-
mons, 2006), which uses an underlying model
of morphology familiar to linguists, is fully in-
tegrated into lexicon development and interlin-
ear text analysis, and produces a human-readable
grammar sketch as well as a machine-interpretable
parser. The morphological parser is constructed
?stealthily? in the background, and can help a lin-
guist by predicting glosses for interlinear texts.
Linguist?s Assistant (Beale, 2011) provides a
corpus of semantic representations for linguists to
use as a guide for elicitation. After eliciting the
language data, a linguist writes rules translating
these semantic representations into surface forms.
The result is a description of the language that can
be used to generate text from documents that have
been converted into the semantic representation.
Linguists are encouraged to collect their own elic-
itations and naturally occurring texts and translate
them into the semantic representation.
The LinGO Grammar Matrix (Bender et al.,
2002) facilitates formal modeling of syntax by
generating basic HPSG ?starter grammars? for
languages from the answers to a typological ques-
tionnaire. Extending a grammar beyond the proto-
type, however, does require extensive knowledge
of HPSG, making this tool more feasibly used by
grammar engineers and computational linguists.
For semantics, the most common resource for for-
mal documentation across languages is FrameNet
(Filmore et al., 2003); FrameNets have been de-
veloped for many languages, including Spanish,
Japanese, and Portuguese. However, FrameNet is
also targeted toward computational linguists.
In general, we also lack tools for creating cus-
tom elicitation materials. With WELT, we hope to
fill some of the gaps in the range of available field
linguistics tools. WELT will enable the creation of
custom elicitation material and facilitate the man-
agement sessions with an informant. WELT will
also enable formal documentation of the semantics
of a language without knowledge of specific com-
putational formalisms. This is similar to the way
FLEx allows linguists to create a formal model of
morphology while also documenting the lexicon
of a language and glossing interlinear texts.
3 Overview of WELT Workflow
In this section, we briefly describe the workflow
for using WELT; a visual representation is pro-
vided in Figure 1. Since we are still in the early
stages of our project, this workflow has not been
tested in practice. The tools for scene creation and
elicitation are currently useable, although more
features will be added in the future. The tools for
modeling and documentation are still in develop-
ment; although some functionality has been imple-
mented, we are still testing it with toy grammars.
First, WELT will be used to prepare a set of 3D
scenes to be used to elicit targeted descriptions or
narratives. An important part of this phase will be
the cultural adaptation of the graphical semantics
used in WordsEye, so that scenes will be relevant
to the native speakers a linguist works with. We
will discuss cultural adaptation in more detail in
Section 4.1. Next, the linguist will work with an
informant to generate language data based on pre-
pared 3D scenes. This can be a dynamic process;
as new questions come up, a linguist can easily
modify existing scenes or create new ones. WELT
also automatically syncs recorded audio with open
scenes and provides an interface for the linguist to
write notes, textual descriptions, and glosses. We
will discuss creating scenes and eliciting data with
WELT in Section 4.2. After the elicitation session,
the linguist can use WELT to review the data col-
lected, listen to the audio recorded for each scene,
and revise notes and glosses. The linguist can then
create additional scenes to elicit more data or be-
gin the formal documentation of the language.
Creating a text-to-scene system with WELT re-
quires formal models of the morphology, syntax,
7
Define	 ?Lexicon	 ?
Cultural	 ?
Adapta?on	 ?of	 ?
VigNet	 ?
Create	 ?Scenes	 ? Collect	 ?Data	 ?from	 ?informant	 ?
Clean-??up	 ?notes/
glosses	 ?
Modify	 ?&	 ?add	 ?vigne?es	 ?
Define	 ?syntax	 ?to	 ?seman?cs	 ?rules	 ?
Define	 ?Morphology	 ?
Define	 ?Syntax	 ?
L2	 ?
Lexicon	 ?
L2	 ?Syntax-??
Seman?cs	 ?rules	 ?VigNet	 ?Resources 
Output	 ?&	 ?
Collabora?on	 ? Prepare	 ?L2	 ?scenes	 ?
Verify	 ?with	 ?
informant	 ?
XLE	 ? FieldWorks	 ?Tools WELT	 ?
Figure 1: WELT workflow
and semantics of a language. Since the focus
of WELT is on semantics, the formalisms used
to model morphology and syntax may vary. We
are using FieldWorks to document Nahuatl mor-
phology, XFST (Beesley and Karttunen, 2003) to
model Arrernte morphology, and XLE (Crouch et
al., 2011) to model syntax in the LFG formal-
ism (Kaplan and Bresnan, 1982). We will provide
tools to export WELT descriptions and glosses
into FLEx format and to export the lexicon cre-
ated during documentation into FLEx and XLE.
WELT will provide user interfaces for modeling
the syntax-semantics interface, lexical semantics,
and graphical semantics of a language. We will
discuss these in more detail in Section 5.3.
Once models of morphology, syntax, and se-
mantics are in place (note that these can be work-
ing models, and need not be complete), WELT
puts the components together into a text-to-scene
system that takes input in the endangered language
and uses the formal models to generate pictures.
This system can be used to verify theories with in-
formants and revise grammars. As new questions
arise, WELT can also continue to be used to create
elicitation materials and collect linguistic data.
Finally, we will create a website for WELT so
linguists can share resources such as modified ver-
sions of VigNet, 3D scenes, language data col-
lected, and formal grammars. This will allow
comparison of analyses across languages, as well
as facilitate the documentation of other languages
that are similar linguistically or spoken by cul-
turally similar communities. In addition, sharing
the resulting text-to-scene systems with a wider
audience can generate interest in endangered lan-
guages and, if shared with endangered-language-
speaking communities, encourage younger mem-
bers of the community to use the language.
4 Elicitation with WELT
WELT organizes elicitation sessions around a set
of 3D scenes, which are created by inputting En-
glish text into WordsEye. Scenes can be imported
and exported between sessions, so that useful
scenes can be reused and data compared. WELT
also provides tools for recording audio (which is
automatically synced with open scenes), textual
descriptions, glosses, and notes during a session.
Screenshots are included in Figure 2.
4.1 Cultural Adaptation of VigNet
To interpret input text, WordsEye uses VigNet
(Coyne et al., 2011), a lexical resource based on
FrameNet (Baker et al., 1998). As in FrameNet,
lexical items are grouped in frames according to
shared semantic structure. A frame contains a set
of frame elements (semantic roles). FrameNet de-
fines the mapping between syntax and semantics
for a lexical item with valence patterns that map
syntactic functions to frame elements.
VigNet extends FrameNet in order to capture
?graphical semantics?, a set of graphical con-
straints representing the position, orientation, size,
color, texture, and poses of objects in the scene,
8
Figure 2: Screenshots of WELT elicitation interfaces
which is used to construct and render a 3D
scene. Graphical semantics are added to frames by
adding primitive graphical (typically, spatial) rela-
tions between frame element fillers. VigNet distin-
guishes between meanings of words that are dis-
tinguished graphically. For example, the specific
objects (e.g., implements) and spatial relations in
the graphical semantics for cook depend on the
object being cooked and on the culture in which
it is being cooked (cooking turkey in Baltimore
vs. cooking an egg in Alice Springs), even though
at an abstract level cook an egg in Alice Springs
and cook a turkey in Baltimore are perfectly com-
positional semantically. Frames augmented with
graphical semantics are called vignettes.
Vignette Tailoring: Without digressing into a dis-
cussion on linguistic relativity, we assume that
large parts of VigNet are language- and culture-
independent. The low-level graphical relations
used to express graphical semantics are based on
physics and human anatomy and do not depend on
language. However, the graphical semantics for a
vignette may be culture-specific, and some new vi-
gnettes will need to be added for a culture. In the
U.S., for example, the sentence The woman boiled
the water might invoke a scene with a pot of wa-
ter on a stove in a kitchen. Among the Arrernte
people, it would instead invoke a woman sitting
on the ground in front of a kettle on a campfire.
Figure 3 shows an illustration from the Eastern
and Central Arrernte Picture Dictionary (Broad,
2008) of the sentence Ipmenhe-ipmenhele kwatye
urinpe-ilemele iteme, ?My grandmother is boiling
the water.? The lexical semantics for the English
verb boil and the Arrente verb urinpe-ileme are
the same, the relation APPLY-HEAT.BOIL. How-
ever, the vignettes map to different, culture-typical
graphical semantics. The vignettes for our exam-
ple are shown in Figure 4.
Figure 3: Illustration from Broad (2008).
To handle cultural differences like these, a lin-
guist will use WELT to extend VigNet with new
9
Figure 4: Vignettes for the woman boils the water.
The high-level semantics of APPLY-HEAT.BOIL
are decomposed into sets of objects and primitive
graphical relations that depend on cultural context.
graphical semantics for existing vignettes that
need to be modified, and new vignettes for scenar-
ios not already covered. We will create interfaces
so that VigNet can easily be adapted.
Custom WordsEye Objects: Another way to
adapt WordsEye to a culture or region is to add rel-
evant 3D objects to the database. WordsEye also
supports 2D-cutout images, which is an easy way
to add new material without 3D modeling. We
have created a corpus of 2D and 3D models for
WordsEye that are specifically relevant to aborig-
inal speakers of Arrernte, including native Aus-
tralian plants and animals and culturally relevant
objects and gestures. Many of the pictures we cre-
ated are based on images from IAD Press, used
with permission, which we enhanced and cropped
in PhotoShop. Some scenes that use these images
are included in Figure 5. Currently, each new ob-
ject has to be manually incorporated into Words-
Eye, but we will create tools to allow WELT users
to easily add pictures and objects.
New objects will also need to be incorporated
into the semantic ontology. VigNet?s ontology
consists of semantic concepts that are linked to-
gether with ISA relations. The ontology supports
multiple inheritance, allowing a given concept to
be a sub-type of more than one concept. For exam-
ple, a PRINCESS.N is a subtype of both FEMALE.N
and ARISTOCRAT.N, and a BLACK-WIDOW.N is a
subtype of SPIDER.N and POISONOUS-ENTITY.N.
Concepts are often linked to corresponding lexi-
cal items. If a lexical item has more than one
word sense, the different word senses would be
represented by different concepts. In addition, ev-
ery graphical object in VigNet is represented by
a unique concept. For example, a particular 3D
model of a dog would be a linked to the general
DOG.N concept by the ISA relation. The semantic
concepts in VigNet include the graphical objects
available in WordsEye as well as concepts tied to
related lexical items. While WordsEye might only
have a handful of graphical objects for dogs, Vi-
gNet will have concepts representing all common
types of dogs, even if there is no graphical object
associated with them. We will provide interfaces
both for adding new objects and for modifying the
semantic concepts in VigNet to reflect the differ-
ing lexical semantics of a new language.
4.2 Preparing Scenes and Eliciting Data
The next step in the workflow is the preparation
of scenes and elicitation of descriptions. To test
creating elicitation materials with WELT, we built
a set of scenes based on the Max Planck topolog-
ical relations picture series (Bowerman and Ped-
erson, 1992). In creating these, we used a feature
of WordsEye that allows highlighting specific ob-
jects (or parts of objects) in a scene. We used these
scenes to elicit descriptions from a native Nahuatl
speaker; some examples are included in Figure 6.
(a) in tapamet? t?atsakwa se kali
the fence/wall around the house
(b) in tsopelik katsekotok t?atsint?a in t?apetS
the candy sticking under the table
Figure 6: Nahuatl examples elicited with WELT
One topic we will explore with WELT is the re-
lationship in Arrernte between case and semantic
interpretation of a sentence. It is possible to signif-
icantly alter a sentence?s meaning by changing the
case on an argument. For example, the sentences
in (1) from Wilkins (1989) show that adding dative
10
Figure 5: WordsEye scenes using custom 2D gum tree and dingo from our corpus
case to the direct object of the sentence changes
the meaning from shooting and hitting the kanga-
roo to shooting at the kangaroo and not hitting it.
Wilkins calls this the ?dative of attempt.?
(1) a. re aherre tyerre-ke
he kangaroo shot-pc
He shot the kangaroo.
b. re aherre-ke tyerre-ke
he kangaroo-DAT shot-pc
He shot at the kangaroo (but missed).
In order to see how this example generalizes,
we will create pairs of pictures, one in which the
object of the sentence is acted upon, and one in
which the object fails to be acted upon. Figure 7
shows a pair of scenes contrasting an Australian
football player scoring a goal with a player aiming
at the goal but missing the shot. Sentences (2) and
(3) are two ways of saying ?score a goal? in Ar-
rernte; we want to see if a native Arrernte speaker
would use goal-ke in place of goal in this context.
(2) artwe le goal arrerne-me
man ERG goal put-NP
The man kicks a goal.
(3) artwe le goal kick-eme-ile-ke
man ERG goal kick-VF-TV-PST
The man kicked a goal.
5 Modeling a Language with WELT
WELT includes tools for documenting the seman-
tics of the language. It also uses this documenta-
tion to automatically generate a text-to-scene sys-
tem for the language. Because WELT is centered
around the idea of 3D scenes, the formal docu-
mentation will tend to focus on the parts of the se-
mantics that can be represented graphically. Note
that this can include figurative concepts as well,
although the visual representation of these may be
culture-specific. However, linguists do not need
to be limited by the graphical output; WELT can
be used to document other aspects of semantics as
well, but linguists will not be able to verify these
theories using the text-to-scene system.
To explain the necessary documentation, we
briefly describe the underlying architecture of
WordsEye, and how we are adapting it to sup-
port text-to-scene systems for other languages.
The WordsEye system parses each input sentence
into a labeled syntactic dependency structure, then
converts it into a lexical-semantic structure using
lexical valence patterns and other lexical and se-
mantic information. The resulting set of seman-
tic relations is converted to a ?graphical seman-
tics?, the knowledge needed to generate graphical
scenes from language.
To produce a text-to-scene system for a new lan-
guage, WELT must replace the English linguistic
processing modules with models for the new lan-
guage. The WELT processing pipeline is illus-
trated in Figure 8, with stages of the pipeline on
top and required resources below. In this section,
we will discuss creating the lexicon, morphologi-
cal and syntactic parsers, and syntax-to-semantics
rules. The vignettes and 3D objects will largely
have been done during cultural adaptation of Vi-
gNet; additional modifications needed to handle
the semantics can be defined using the same tools.
5.1 The Lexicon
The lexicon in WELT is a list of word forms
mapped to semantic concepts. The process of
building the lexicon begins during elicitation.
WELT?s elicitation interface includes an option to
display each object in the scene individually be-
fore progressing to the full scene. When an object
is labeled and glossed in this way, the word and
the semantic concept represented by the 3D ob-
ject are immediately added to the lexicon. Word
forms glossed in scene descriptions will also be
added to the lexicon, but will need to be mapped
to semantic concepts later. WELT will provide
11
Figure 7: WordsEye scenes to elicit the ?dative of attempt.?
Morph	 ? Lexical	 ?Seman?cs	 ?
Graphical	 ?
Seman?cs	 ? Scene	 ?
Input	 ?
Text	 ?
Processing	 ?Pipeline	 ?
VigNet	 ?
Vigne?s	 ? 2D/3D	 ?objects	 ?Lexicon	 ?
Syntax	 ?
Morphological	 ?
Analyzer	 ?
Syntac?c	 ?
Parser	 ?
Syntax-??Seman?cs	 ?
Rules	 ?
Figure 8: WELT architecture
tools for completing the lexicon by modifying
the automatically-added items, adding new lexical
items, and mapping each lexical item to a seman-
tic concept in VigNet. Figure 9(a) shows a partial
mapping of the nouns in our Arrernte lexicon.
WELT includes a visual interface for search-
ing VigNet?s ontology for semantic concepts and
browsing through the hierarchy to select a partic-
ular category. Figure 9(b) shows a portion of the
ontology that results from searching for cup. Here,
we have decided to map panikane to CUP.N. Se-
mantic categories are displayed one level at a time,
so initially only the concepts directly above and
below the search term are shown. From there, it is
simple to click on relevant concepts and navigate
the graph to find an appropriate semantic category.
To facilitate the modeling of morphology and syn-
tax, WELT will also export the lexicon into for-
mats compatible with FieldWorks and XLE, so the
list of word forms can be used as a starting point.
5.2 Morphology and Syntax
As mentioned earlier, the focus of our work on
WELT is on modeling the interface between syn-
tax, lexical semantics, and graphical semantics.
Therefore, although WELT requires models of
morphology and syntax to generate a text-to-scene
system, we are relying on third-party tools to build
those models. For morphology, a very good tool
already exists in FLEx, which allows the creation
Lexical VigNet
Item Concept
artwe PERSON.N
panikane CUP.N
angepe CROW.N
akngwelye DOG.N
tipwele TABLE.N
(a) (b)
Figure 9: (a) Arrernte lexical items mapped to Vi-
gNet concepts; (b) part of the VigNet ontology
of a morphological parser without knowledge of
any particular grammatical formalism. For syn-
tax, we are using XLE for our own work while
researching other options that would be more ac-
cessible to non-computational linguists. It is im-
portant to note, though, that the modeling done in
WELT does not require a perfect syntactic parser.
In fact, one can vastly over-generate syntax and
still accurately model semantics. Therefore, the
syntactic grammars provided as models do not
need to be complex. However, the question of syn-
tax is still an open area of research in our project.
5.3 Semantics
To use the WordsEye architecture, the system
needs to be able to map between the formal syntax
of the endangered language and a representation of
semantics compatible with VigNet. To accomplish
12
Figure 10: Creating syntax-semantics rules in WELT
this, WELT includes an interface for the linguist to
specify a set of rules that map from syntax to (lex-
ical) semantics. Since we are modeling Arrernte
syntax with LFG, the rules currently take syntactic
f-structures as input, but the system could easily be
modified to accommodate other formalisms. The
left-hand side of a rule consists of a set of con-
ditions on the f-structure elements and the right-
hand side is the desired semantic structure. Rules
are specified by defining a tree structure for the
left-hand (syntax) side and a DAG for the right-
hand (semantics) side.
As an example, we will construct a rule to
process sentence (2) from Section 4.2, artwe le
goal arrerneme. For this sentence, our Arrernte
grammar produces the f-structure in Figure 11.
We create a rule that selects for predicate ar-
rerne with object goal and any subject. Figure
10 shows the construction of this rule in WELT.
Note that var-1 on the left-hand side becomes
VIGNET(var-1) on the right-hand side; this in-
dicates that the lexical item found in the input is
mapped into a semantic concept using the lexicon.
Figure 11: F-structure for sentence 2, Section 4.2.
The rule shown in Figure 10 is a very sim-
ple example. Nodes on the left-hand side of
the rule can also contain boolean logic, if we
wanted to allow the subject to be [(artwe ?man? OR
arhele ?woman?) AND NOT ampe ?child?]. Rules
need not specify lexical items directly but may
refer to more general semantic categories. For
example, our rule could require a particular se-
mantic category for VIGNET(var-1), such as
ANIMATE-BEING.N. These categories are chosen
through the same ontology browser used to cre-
ate the lexicon. Finally, to ensure that our sen-
tence can be converted into graphics, we need
to make sure that a vignette definition exists for
CAUSE MOTION.KICK so that the lexical seman-
tics on the right-hand side of our rule can be aug-
mented with graphical semantics; the vignette def-
inition is given in Figure 12. The WordsEye sys-
tem will use the graphical constraints in the vi-
gnette to build a scene and render it in 3D.
Figure 12: Vignette definition for
CAUSE MOTION.KICK
6 Summary
We have described a novel tool under develop-
ment for linguists working with endangered lan-
guages. It will provide a new way to elicit data
from informants, an interface for formally docu-
menting the lexical semantics of a language, and
allow the creation of a text-to-scene system for any
language. In this paper, we have focused specifi-
cally on the workflow that a linguist would fol-
low while studying an endangered language with
WELT. WELT will provide useful tools for field
linguistics and language documentation, from cre-
ating elicitation materials, to eliciting data, to for-
mally documenting a language. In addition, the
text-to-scene system that results from document-
ing an endangered language with WELT will be
valuable for language preservation, generating in-
terest in the wider world, as well as encouraging
younger members of endangered language com-
munities to use the language.
Acknowledgments
This material is based upon work supported by
the National Science Foundation under Grant No.
1160700.
13
References
Alliance for Linguistic Diversity. 2013. The En-
dangered Languages Project. http://www.
endangeredlanguages.com.
C. Baker, J. Fillmore, and J. Lowe. 1998. The Berkeley
FrameNet project. In 36th Meeting of the Associa-
tion for Computational Linguistics and 17th Inter-
national Conference on Computational Linguistics
(COLING-ACL?98), pages 86?90, Montr?eal.
Stephen Beale. 2011. Using Linguist?s Assistant for
Language Description and Translation. In IJCNLP
2011 System Demonstrations, pages 5?8.
Kenneth R. Beesley and Lauri Karttunen. 2003. Finite-
State Morphology Homepage. http://www.
fsmbook.com.
E. Bender, D. Flickinger, and S. Oepen. 2002. The
Grammar Matrix. In J. Carroll, N. Oostdijk, and
R. Sutcliffe, editors, Workshop on Grammar En-
gineering and Evaluation at the 19th International
Conference on Computational Linguistics, pages 8?
14, Taipei, Taiwan.
S. Bird and D. Chiang. 2012. Machine translation for
language preservation. In COLING 2012: Posters,
pages 125?134, Mumbai, December.
S. Bird. 2009. Natural language processing and
linguistic fieldwork. Computational Linguistics,
35(3):469?474.
Cheryl A Black and H Andrew Black. 2009. PAWS:
Parser and Writer for Syntax. In SIL Forum for Lan-
guage Fieldwork 2009-002.
H.A. Black and G.F. Simons. 2006. The SIL Field-
Works Language Explorer approach to morphologi-
cal parsing. In Computational Linguistics for Less-
studied Languages: Texas Linguistics Society 10,
Austin, TX, November.
M. Bowerman and E. Pederson. 1992. Topological re-
lations picture series. In S. Levinson, editor, Space
stimuli kit 1.2, page 51, Nijmegen. Max Planck In-
stitute for Psycholinguistics.
N. Broad. 2008. Eastern and Central Arrernte Picture
Dictionary. IAD Press.
B. Coyne and R. Sproat. 2001. WordsEye: An au-
tomatic text-to-scene conversion system. In SIG-
GRAPH.
B. Coyne, D. Bauer, and O. Rambow. 2011. Vignet:
Grounding language in graphics using frame seman-
tics. In ACL Workshop on Relational Models of Se-
mantics (RELMS), Portland, OR.
D. Crouch, M. Dalrymple, R. Kaplan, T. King,
J. Maxwell, and P. Newman. 2011. XLE Doc-
umentation. http://www2.parc.com/isl/
groups/nltt/xle/doc/xle_toc.html.
C. Filmore, C. Johnson, and M. Petruck. 2003. Back-
ground to FrameNet. In International Journal of
Lexicography, pages 235?250.
R.M. Kaplan and J.W. Bresnan. 1982. Lexical-
functional grammar: A formal system for grammat-
ical representation. In J.W. Bresnan, editor, The
Mental Representation of Grammatical Relations.
MIT Press, Cambridge, Mass., December.
SIL FieldWorks. 2014. SIL FieldWorks. http://
fieldworks.sil.org.
D. Wilkins. 1989. Mparntwe Arrernte (Aranda): Stud-
ies in the structure and semantics of grammar. Ph.D.
thesis, Australian National University.
14
Proceedings of Frame Semantics in NLP: A Workshop in Honor of Chuck Fillmore (1929?2014), pages 30?33,
Baltimore, Maryland USA, June 27, 2014.
c?2014 Association for Computational Linguistics
Using Frame Semantics in Natural Language Processing
Apoorv Agarwal
Dept. of Computer Science
Columbia University
New York, NY
apoorv@cs.columbia.edu
Daniel Bauer
Dept. of Computer Science
Columbia University
New York, NY
bauer@cs.columbia.edu
Owen Rambow
CCLS
Columbia University
New York, NY
rambow@ccls.columbia.edu
Abstract
We summarize our experience using
FrameNet in two rather different projects
in natural language processing (NLP).
We conclude that NLP can benefit from
FrameNet in different ways, but we sketch
some problems that need to be overcome.
1 Introduction
We present two projects at Columbia in which we
use FrameNet. In these projects, we do not de-
velop basic NLP tools for FrameNet, and we do
not develop FramNets for new languages: we sim-
ply use FrameNet or a FrameNet parser in an NLP
application. The first application concerns the ex-
traction of social networks from narrative texts.
The second application aims at generating three-
dimensional pictures from textual descriptions.
The applications are very different: they differ
in terms of their goals, and they differ in terms
of how they use FrameNet. However, they have
in common that they can use FrameNet because it
provides a particular level of semantic abstraction
which is suited for both applications. Consider
verbs of saying, such as declare, deny, mention,
remark, tell, or say: they do not have the same
meaning. However, they share enough common
meaning, and in particular they share the same set
of participants, so that for our two applications
they can be considered as interchangeable: they
represent the communication of verbal informa-
tion (the Message) from a Speaker to an Ad-
dressee. This is precisely what the Statement
frame encodes. We will use this example in the
next two sections, in which we discuss our two
projects in more detail.
2 Using an Off-the-Shelf FrameNet
Parser
Our first application is SINNET, a system that ex-
tracts a social network from narrative text. It uses
the notion of a social event (Agarwal et al., 2010),
a particular kind of event which involves (at least)
two people such that at least one of them is aware
of the other person. If only one person is aware
of the event, we call it Observation (OBS): for
example, someone is talking about someone else
in their absence. If both people are aware of the
event, we call it Interaction (INR): for example,
one person is telling the other a story. Our claim
is that links in social networks are in fact made
up of social events: OBS social events give rise
to one-way links, and INR social events to two-
way links. For more information, see (Agarwal
and Rambow, 2010; Agarwal et al., 2013a; Agar-
wal et al., 2013b).
From an NLP point of view, we have a difficult
cluster of phenomena: we have a precise defini-
tion of what we want to find, but it is based on the
cognitive state of the event participants, which is
almost never described explicitly in the text. Fur-
thermore, the definitions cover a large number of
diverse situations such as talking, spying, having
lunch, fist fighting, or kissing. Furthermore, some
semantic differences are not relevant: verbs such
as talk, tell, deny, all have the same meaning with
respect to social events. Finally, in order to de-
code the events in terms of social events, we need
to understand the roles: if I am talking to Sudeep
about Mae, Sudeep and I have an INR social event
with each other, and we both have a OBS social
event with Mae. Thus, this problem sounds like
an excellent application for frame semantics!
We present initial results in (Agarwal et al.,
2014), and summarize them here. We use Semafor
(Chen et al., 2010) as a black box to obtain the se-
mantic parse of a sentence. However, there are
several problems:
? FrameNet does not yet have complete lexical
coverage.
? Semafor does not produce a single semantic
30
representation for a sentence, as we would
want in order to perform subsequent process-
ing. Instead, it annotates separate, discon-
nected frame structures for each frame evok-
ing element it finds.
? The data annotated with FrameNet consists
of the example sentences as well as a compar-
atively small corpus. For this reason, it is not
easy to use standard machine learning tech-
niques for frame semantic parsing. As a re-
sult, the output is fairly errorful (as compared
to, say, a state-of-the-art dependency parser
trained on nearly a million annotated words).
Errors include mislabeled frames, mislabeled
frame elements, and missing frame elements.
To overcome these problems, we constructed
several tree representations out of the partial an-
notations returned by Semafor. We then used tree
kernels on these syntactic and semantic tree rep-
resentations, as well as bags of words. The tree
kernels can automatically identify important sub-
structures in the syntactic and semantic trees with-
out the need for feature engineering on our part.
Our hypothesis is that the kernels can learn which
parts of the semantic structures are reliable and
can be used for prediction.
The tree structures are shown in Figure 1. The
structure on the left (FrameForest) is created by
taking all identified instances of frames, and col-
lecting them under a common root node. The
frame elements are filled in with dependency syn-
tax. The structure on the right (FrameTree) is our
attempt to create a single arborescent structure to
capture the semantics of the whole sentence. Our
third structure, FrameTreeProp (not shown), is de-
rived from FrameTree by multiplying the nodes of
interest up the path from their normal place to the
root. This allows us to overcome problems with
the limited locality of the tree kernels.
We present some results in Table 1. Compar-
ing lines ?Syntax? with ?Synt FrameTreeProp?,
we see a slight but statistically significant increase.
This increase comes from using FrameNet seman-
tics. When we look at only the semantic structures,
we see that they all perform worse than syntax on
its own. ?BOF? is simply a bag of frames; we
see that the arborescent structures outperform it,
so semantic structure is useful in addition to se-
mantic tags. ?RULES? is a comprehensive set of
hand-written rules we attached to frames; if frame
Detection
Model P R F1
Syntax 0.464 0.751 0.574
RULES 0.508 0.097 0.164
BOF 0.296 0.416 0.346
FrameForest 0.331 0.594 0.425
FrameTree 0.295 0.594 0.395
FrameTreeProp 0.308 0.554 0.396
All 0.494 0.641 0.558
Synt FrameTreeProp 0.484 0.740 0.585
Table 1: Results for Social Event Detection.
?Syntax? is an optimized model using various
syntactic representations (Agarwal and Rambow,
2010). The next five models are the novel se-
mantic features and structures. ?All? refers to the
model that uses all the listed structures together.
?Synt FrameTreeProp? is a linear combination of
?Syntax? and FrameTreeProp.
semantic parsing were perfect, these rules should
perform pretty well. They do in fact achieve the
best precision of all our systems, but the recall is
so low that overall they are not useful. We inter-
pret this result as supporting our claim that part of
the problem with using frame-semantic parsers is
the high error rate.
Even though the gain so far from frame seman-
tic parsing is small, we are encouraged by the fact
that an off-the-shelf semantic parser can help at
all. We are currently exploring other semantic
structures we can create from the semantic parse,
including structures which are dags rather than
trees. We would like to point out that the com-
bination of the parser, the creation of our seman-
tic trees, and the training with tree kernels can be
applied to any other problem that is sensitive to
the meaning of text. Based on our experience, we
expect to see an increase in ?black box? uses of
FrameNet parsing for other applications in NLP.
3 Extending the FrameNet Resource
FrameNet can be a useful starting point for a richer
knowledge representation which is needed for a
specific task. In our example, we need a repre-
sentation that we can use in the WordsEye project
(Coyne and Sproat, 2001), in which pictures are
created automatically from text descriptions. This
can be understood as providing a particular type
of decompositional semantics for the input text.
31
ROOT
Commerce buy
Target
4
Buyer
T1-Ind
Seller
from
T2-Grp
Statement
Target
claimed
4
Speaker
T1?-Ind
Message
4
Statement
Speaker
T1-Ind
Coleman
Message
Commerce buy
Buyer
T1?-Ind
he
Seller
T2-Grp
defendants
Figure 1: Semantic trees for the sentence ?Coleman claimed [he]
T1?Ind
bought drugs from the
[defendants]
T2?Grp
.?. The tree on the left is FrameForest and the tree on the right is FrameTree. 4
in FrameForest refers to the subtree (bought (T1-Ind) (from T2-Grp)). Ind refers to individual and Grp
refers to group.
We extend FrameNet in two ways to obtain the re-
source we need, which we call VigNet (Coyne et
al., 2011).
The pictures created by the WordsEye system
are based on spatial arrangements (scenes) of pre-
defined 3D models. At a low level, scenes are de-
scribed by primitive spatial relations between sets
of these models (The man is in front of the woman.
He is looking at her. His mouth is open.). We
would like to use WordsEye to depict scenarios,
events, and actions (John told Mary his life story).
These can be seen as complex relations between
event participants.
We turn to FrameNet frames as representations
for such relations. FrameNet offers a large in-
ventory of frames, together with additional struc-
tured information about them in the form of frame
relations. Most importantly, FrameNet provides
example annotations illustrating the patterns in
which frames are evoked and syntactic arguments
are mapped to frame elements.
However, there are two main problems if we
want to turn frame annotations into pictures. First,
in frame annotations frame elements are only filled
with text spans, not with semantic objects. Anno-
tations are therefore restricted to individual predi-
cate/argument structures and do not represent the
meaning of a full sentence. To address this prob-
lem we essentially use FrameNet frames as an in-
ventory of predicates in a graph-based semantic
representation. We use semantic nodes, which are
identifiers representing events and entities that fill
frame elements. Frame instances then describe re-
lations between these semantic nodes, building a
graph structure that can represent a full text frag-
ment (including coreference). We are planning
to develop parsers that convert text directly into
such graph-based representations, inspired by re-
cent work on semantic parsing (Jones et al., 2012).
Second, FrameNet frames usually describe
functional relationships between frame elements,
not graphical ones. To turn a frame into its graphi-
cal representation we therefore need (a) a set of of
graphical frames and a formal way of decompos-
ing these frames into primitives and (b) a mech-
anism for relating FrameNet frames to graphi-
cal frames. Our solution is VigNet (Coyne et
al., 2011), an extension of FrameNet. VigNet
makes use of existing frame-to-frame relations
to extend FrameNet with a number of graphical
frames called Vignettes. Vignettes are subframes
of FrameNet frames, each representing a specific
way in which a frame can be realized based on the
specific lexical unit or on context. For instance,
a proper visualization of the INGESTION frame
will depend on the INGESTOR (human vs. ani-
mals of different sizes), the INGESTIBLE (differ-
ent types of foods and drinks are ingested accord-
ing to different social conventions, each a differ-
ent Vignette). Note however, that many FrameNet
frames provide useful abstractions that allow us
to use a single Vignette as a good default visu-
alization for the entire frame. For instance, all
lexical units in the STATEMENT frame can be de-
picted as the SPEAKER standing opposite of the
ADDRESSEE with an open mouth.
A new frame-to-frame relation, called subframe
parallel, is used to decompose a Vignette into
32
graphical sub-relations, which are in turn frames
(either graphical primitives or other vignettes).
Like any frame-to-frame relation, it maps frame
elements of the source frame to frame elements
of the target frame. New frame elements can also
be introduced. For instance, one Vignette for IN-
GESTION that can be used if the INGESTIBLE is a
liquid contains a new frame element CONTAINER.
The INGESTOR is holding the container and the
liquid is in the container.
We have populated the VigNet resource us-
ing a number of different approaches (Coyne et
al., 2012), including multiple choice questions on
Amazon Mechanical Turk to define vignettes for
locations (rooms), using the system itself to define
locations, and a number of web-based annotation
tools to define vignettes for actions.
An ongoing project is exploring the use of
WordsEye and VigNet as a tool for field linguists
and for language documentation and preserva-
tion. The WordsEye Linguistics Toolkit (WELT,
(Ulinski et al., 2014)) makes it easy to produce
pictures for field linguistic elicitation. It will
also provide an environment to essentially de-
velop language specific VigNets as models of the
syntax/semantics interface and conceptual cate-
gories. This work may be relevant to other projects
that aim to build non-English and multi-lingual
FrameNets.
4 Conclusion
We have tried to motivate the claim that FrameNet
provides the right layer of semantic abstraction
for many NLP applications by summarizing two
ongoing NLP projects at Columbia. We have
also suggested that part of the problem in using
FrameNet in NLP projects is the lack of a single
structure that is produced, either in manual anno-
tations, or in the output of a FrameNet parser. We
suspect that research into how to construct such
unified semantic representations will continue to
be a major component of the use of FrameNet in
NLP.
Acknowledgments
This paper is based upon work supported in part by
the NSF (grants IIS-0713548 and IIS-0904361),
and by the DARPA DEFT Program. We thank
our collaborators on the two projects used as ex-
amples in this extended abstract. We thank Chuck
Fillmore for FrameNet.
References
Apoorv Agarwal and Owen Rambow. 2010. Automatic de-
tection and classification of social events. In Proceedings
of the 2010 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1024?1034, Cambridge,
MA, October. Association for Computational Linguistics.
Apoorv Agarwal, Owen C. Rambow, and Rebecca J. Passon-
neau. 2010. Annotation scheme for social network ex-
traction from text. In Proceedings of the Fourth Linguistic
Annotation Workshop.
Apoorv Agarwal, Anup Kotalwar, and Owen Rambow.
2013a. Automatic extraction of social networks from lit-
erary text: A case study on alice in wonderland. In the
Proceedings of the 6th International Joint Conference on
Natural Language Processing (IJCNLP 2013).
Apoorv Agarwal, Anup Kotalwar, Jiehan Zheng, and Owen
Rambow. 2013b. Sinnet: Social interaction network ex-
tractor from text. In Sixth International Joint Conference
on Natural Language Processing, page 33.
Apoorv Agarwal, Sriramkumar Balasubramanian, Anup Ko-
talwar, Jiehan Zheng, and Owen Rambow. 2014. Frame
semantic tree kernels for social network extraction from
text. In Proceedings of the 14th Conference of the Euro-
pean Chapter of the Association for Computational Lin-
guistics, Gothenburg, Sweden.
Desai Chen, Nathan Schneider, Dipanjan Das, and Noah A.
Smith. 2010. Semafor: Frame argument resolution with
log-linear models. In Proceedings of the 5th International
Workshop on Semantic Evaluation, pages 264?267, Up-
psala, Sweden, July. Association for Computational Lin-
guistics.
Bob Coyne and Richard Sproat. 2001. Wordseye: an au-
tomatic text-to-scene conversion system. In 28th annual
conference on Computer graphics and interactive tech-
niques.
Bob Coyne, Daniel Bauer, and Owen Rambow. 2011. Vi-
gnet: Grounding language in graphics using frame seman-
tics. In ACL Workshop on Relational Semantics (RELMS),
Portland, Oregon.
Bob Coyne, Alex Klapheke, Masoud Rouhizadeh, Richard
Sproat, and Daniel Bauer. 2012. Annotation tools and
knowledge representation for a text-to-scene system. In
COLING, Mumbai, India.
Bevan Jones, Jacob Andreas*, Daniel Bauer*, Karl Moritz
Hermann*, and Kevin Knight. 2012. Semantics-based
machine translation with hyperedge replacement gram-
mars. In COLING, Mumbai, India. *first authorship
shared.
Morgan Ulinski, Anusha Balakrishnan, Daniel Bauer, Bob
Coyne, Julia Hirschberg, and Owen Rambow. 2014. Doc-
umenting endangered languages with the wordseye lin-
guistics tool. In Proceedings of the ACL ComputEL work-
shop: The use of computational methods in the study of
endangered languages, Baltimore, MD, USA.
33
