Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1059?1070, Dublin, Ireland, August 23-29 2014.
Automatic Classification of Communicative Functions of Definiteness
Archna Bhatia
?,?
Chu-Cheng Lin
?
Nathan Schneider
?
Yulia Tsvetkov
?
Fatima Talib Al-Raisi
?
Laleh Roostapour
?
Jordan Bender
?
Abhimanu Kumar
?
Lori Levin
?
Mandy Simons
?
Chris Dyer
?
?
Carnegie Mellon University
?
University of Pittsburgh
Pittsburgh, PA 15213 Pittsburgh, PA 15260
?archnab@cs.cmu.edu
Abstract
Definiteness expresses a constellation of semantic, pragmatic, and discourse properties?the
communicative functions?of an NP. We present a supervised classifier for English NPs that
uses lexical, morphological, and syntactic features to predict an NP?s communicative function in
terms of a language-universal classification scheme. Our classifiers establish strong baselines for
future work in this neglected area of computational semantic analysis. In addition, analysis of
the features and learned parameters in the model provides insight into the grammaticalization of
definiteness in English, not all of which is obvious a priori.
1 Introduction
Definiteness is a morphosyntactic property of noun phrases (NPs) associated with semantic and pragmatic
characteristics of entities and their discourse status. Lyons (1999), for example, argues that definite
markers prototypically reflect identifiability (whether a referent for the NP can be identified by the
discourse participants or not); other aspects identified in the literature include uniqueness of the entity
in the world and whether the hearer is already familiar with the entity given the context and preceding
discourse (Roberts, 2003; Abbott, 2006). While some morphosyntactic forms of definiteness are employed
by all languages?namely, demonstratives, personal pronouns, and possessives?languages display a vast
range of variation with respect to the form and meaning of definiteness. For example, while languages
like English make use of definite and indefinite articles to distinguish between the discourse status of
various entities (the car vs. a car vs. cars), many other languages?including Czech, Indonesian, and
Russian?do not have articles (although they do have demonstrative determiners). Sometimes definiteness
is marked with affixes or clitics, as in Arabic. Sometimes it is expressed with other constructions, as in
Chinese (a language without articles), where the existential construction can be used to express indefinite
subjects and the ba- construction can be used to express definite direct objects (Chen, 2004).
Aside from this variation in the form of (in)definite NPs within and across languages, there is also vari-
ability in the mapping between semantic, pragmatic, and discourse functions of NPs and the (in)definites
expressing these functions. We refer to these as communicative functions of definiteness, following
Bhatia et al. (2014). Croft (2003, pp. 6?7) shows that even when two languages have access to the
same morphosyntactic forms of definiteness, the conditions under which an NP is marked as definite
or indefinite (or not at all) are language-specific. He illustrates this by contrasting English and French
translations (both languages use definite as well as indefinite articles) such as:
(1) He showed extreme care. (unmarked)
Il montra un soin extr?me. (indef.)
(2) I love artichokes and asparagus. (unmarked)
J?aime les artichauts et les asperges. (def.)
(3) His brother became a soldier. (indef.)
Son fr?re est devenu soldat. (unmarked)
This work is licensed under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings
footer are added by the organisers. License details: http://creativecommons.org/licenses/by/4.0/
1059
? NONANAPHORA [?A,?B] 999
? UNIQUE [+U] 287
*
UNIQUE_HEARER_OLD [+F,?G,+S] 251
? UNIQUE_PHYSICAL_COPRESENCE [+R] 13
? UNIQUE_LARGER_SITUATION [+R] 237
? UNIQUE_PREDICATIVE_IDENTITY [+P] 1
*
UNIQUE_HEARER_NEW [?F] 36
? NONUNIQUE [?U] 581
*
NONUNIQUE_HEARER_OLD [+F] 169
? NONUNIQUE_PHYSICAL_COPRESENCE [?G,+R,+S] 39
? NONUNIQUE_LARGER_SITUATION [?G,+R,+S] 117
? NONUNIQUE_PREDICATIVE_IDENTITY [+P] 13
*
NONUNIQUE_HEARER_NEW_SPEC [?F,?G,+R,+S] 231
*
NONUNIQUE_NONSPEC [?G,?S] 181
? GENERIC [+G,?R] 131
*
GENERIC_KIND_LEVEL 0
*
GENERIC_INDIVIDUAL_LEVEL 131
? ANAPHORA [+A] 1574
? BASIC_ANAPHORA [?B,+F] 795
*
SAME_HEAD 556
*
DIFFERENT_HEAD 329
? EXTENDED_ANAPHORA [+B] 779
*
BRIDGING_NOMINAL [?G,+R,+S] 43
*
BRIDGING_EVENT [+R,+S] 10
*
BRIDGING_RESTRICTIVE_MODIFIER [?G,+S] 614
*
BRIDGING_SUBTYPE_INSTANCE [?G] 0
*
BRIDGING_OTHER_CONTEXT [+F] 112
? MISCELLANEOUS [?R] 732
? PLEONASTIC [?B,?P] 53
? QUANTIFIED 248
? PREDICATIVE_EQUATIVE_ROLE [?B,+P] 58
? PART_OF_NONCOMPOSITIONAL_MWE 100
? MEASURE_NONREFERENTIAL 125
? OTHER_NONREFERENTIAL 148
+ ? 0 + ? 0 + ? 0 + ? 0
Anaphoric 1574 999 732 Generic 131 1476 1698 Predicative 72 53 3180 Specific 1305 181 1819
Bridging 779 1905 621 Familiar 1327 267 1711 Referential 690 863 1752 Unique 287 581 2437
Figure 1: CFD (Communicative Functions of Definiteness) annotation scheme, with frequencies in the
corpus. Internal (non-leaf) labels are in bold; these are not annotated or predicted. +/? values are shown
for ternary attributes Anaphoric, Bridging, Familiar, Generic, Predicative, Referential, Specific, and
Unique; these are inherited from supercategories, but otherwise default to 0. Thus, for example, the
full attribute specification for UNIQUE_PHYSICAL_COPRESENCE is [?A,?B,+F,?G,0P,+R,+S,+U].
Counts for these attributes are shown in the table at bottom.
A cross-linguistic classification of communicative functions should be able to characterize the aspects
of meaning that account for the different patterns of definiteness marking exhibited in (1?3): e.g., that
(2) concerns a generic class of entities while (3) concerns a role filled by an individual. For more on
communicative functions, see ?2.
This paper develops supervised classifiers to predict communicative function labels for English NPs
using lexical, morphological, and syntactic features. The contribution of our work is in both the output of
the classifiers and the models themselves (features and weights). Each classifier predicts communicative
function labels that capture aspects of discourse-newness, uniqueness, specificity, and so forth. Such
functions are useful in a variety of language processing applications. For example, they should usually be
preserved in translation, even when the grammatical mechanisms for expressing them are different. The
communicative function labels also represent the discourse status of entities, making them relevant for
entity tracking, knowledge base construction, and information extraction.
Our log-linear model is a form-meaning mapping that relates syntactic, lexical, and morphological
features to properties of communicative functions. The learned weights of this model can, e.g., gener-
ate plausible hypotheses regarding the form-meaning relationship which can then be tested rigorously
through controlled experiments. This hypothesis generation is linguistically significant as it indicates new
grammatical mechanisms beyond the obvious a and the articles that are used for expressing definiteness
in English.
To build our models, we leverage a cross-lingual definiteness annotation scheme (?2) and annotated
English corpus (?3) developed in prior work (Bhatia et al., 2014). The classifiers, ?4, are supervised
models with features that combine lexical and morphosyntactic information and the prespecified attributes
or groupings of the communicative function labels (such as Anaphoric, Bridging, Specific in fig. 1) to
predict leaf labels (the non-bold faced labels in fig. 1); the evaluation measures (?5) include one that
exploits these label groupings to award partial credit according to relatedness. ?6 presents experiments
comparing several models and discussing their strengths and weaknesses; computational work and
applications related to definiteness are addressed in ?7.
1060
2 Annotation scheme
The literature on definiteness describes functions such as uniqueness, familiarity, identifiability, anaphoric-
ity, specificity, and referentiality (Birner and Ward, 1994; Condoravdi, 1992; Evans, 1977, 1980; Gundel
et al., 1988, 1993; Heim, 1990; Kadmon, 1987, 1990; Lyons, 1999; Prince, 1992; Roberts, 2003; Russell,
1905, inter alia) as being related to definiteness. Reductionist approaches to definiteness try to define
it in terms of one or two of the aforementioned communicative functions. For example, Roberts (2003)
proposes that the combination of uniqueness and a presupposition of familiarity underlie all definite
descriptions. However, possessive definite descriptions (John?s daughter) and the weak definites (the son
of Queen Juliana of the Netherlands) are neither unique nor necessarily familiar to the listener before they
are spoken. In contrast to the reductionist approaches are approaches to grammaticalization (Hopper and
Traugott, 2003) in which grammar develops over time in such a way that each grammatical construction
has some prototypical communicative functions, but may also have many non-prototypical communica-
tive functions. The scheme we are adopting for this work?the annotation scheme for Communicative
Functions of Definiteness (CFD) as described in Bhatia et al. (2014)?assumes that there may be multiple
functions to definiteness. CFD is based on a combination of these functions and is summarized in fig. 1. It
was developed by annotating texts in two languages (English and Hindi) for four different genres?namely
TED talks, a presidential inaugural speech, news articles, and fictional narratives?keeping in mind the
communicative functions that have been associated with definiteness in the linguistic literature.
CFD is hierarchically organized. This hierarchical organization serves to reduce the number of decisions
that an annotator needs to make for speed and consistency. We now highlight some of the major distinctions
in the hierarchy.
At the highest level, the distinction is made between Anaphora, Nonanaphora, and Miscellaneous
functions of an NP (the annotatable unit). Anaphora and Nonanaphora respectively describe whether
an entity is old or new in the discourse; the Miscellaneous function is mainly assigned to various kinds of
nonreferential NPs.
The Anaphora category has two subcategories: Basic_Anaphora and Extended_Anaphora. Ba-
sic_Anaphora applies to NPs referring to entities that have been mentioned before. Extended_Anaphora
applies to any NP whose referent has not been mentioned itself, but is evoked by a previously mentioned
entity. For example, after mentioning a wedding, the bride, the groom, and the cake are considered to be
Extended_Anaphora.
Within the Nonanaphora category, a first distinction is made between Unique, Nonunique, and
Generic. The Unique function applies to NPs whose referent becomes unique in a context for any of
several reasons. For example, Obama can safely be considered unique in contemporary political discourse
in the United States. The function Nonunique applies to NPs that start out with multiple possible referents
and that may or may not become identifiable in a speech situation. For example, a little riding hood of
red velvet in fig. 2 could be annotated with the label Nonunique. Finally, Generic NPs refer to classes
or types of entities rather than specific entities. For example, Dinosaurs in Dinosaurs are extinct. is a
Generic NP.
Another important distinction CFD makes is between Hearer_Old for references to entities that are
familiar to the hearer (e.g., if they are physically present in the speech situation), versus Hearer_New
for nonfamiliar references. This distinction cuts across the two subparts of the hierarchy, Anaphora
and Nonanaphora; thus, labels marking Hearer_Old or Hearer_New also encode other distinctions
(e.g., Unique_Hearer_Old, Unique_Hearer_New, Nonunique_Hearer_Old). For further details on
the annotation scheme, see fig. 1 and Bhatia et al. (2014).
Because the ordering of distinctions determines the tree structure of the hierarchy, the same commu-
nicative functions could have been organized in a superficially different way. In fact, Komen (2013) has
proposed a hierarchy with similar leaf nodes, but different internal structure. Since it is possible that
some natural groupings of labels are not reflected in the hierarchy we used, we also decompose each
label into fundamental communicative functions, which we call attributes. Each label type is associated
with values for attributes Anaphoric, Bridging, Familiar, Generic, Predicative, Referential, Specific, and
Unique. These attributes can have values of +, ?, or 0, as shown in fig. 1. For instance, with the Anaphoric
1061
Once upon a time there was a dear little girl who was loved by everyone who looked at her, but most of all by her grandmother,
and there was nothing that she would not have given to the child.
Once she
SAME_HEAD
gave her
DIFFERENT_HEAD
a little riding hood of red velvet
OTHER_NONREFERENTIAL
NONUNIQUE_HEARER_NEW_SPEC
, which suited her
SAME_HEAD
so well that
she
SAME_HEAD
would never wear anything else
QUANTIFIED
; so she
SAME_HEAD
was always called ?Little Red Riding Hood
UNIQUE_HEARER_NEW
.?
Figure 2: An annotated sentence from ?Little Red Riding Hood.? The previous sentence is shown for
context.
attribute, a value of + applies to labels that can never mark NPs new to the discourse, ? applies to labels
that can only apply if the NP is new in the discourse, and 0 applies to labels such as Pleonastic (where
anaphoricity is not applicable because there is no discourse referent).
3 Data
We use the English definiteness corpus of Bhatia et al. (2014), which consists of texts from multiple genres
annotated with the scheme described in ?2.
1
The 17 documents consist of prepared speeches (TED talks
and a presidential address), published news articles, and fictional narratives. The TED data predominates
(75% of the corpus);
2
the presidential speech represents about 16%, fictional narratives 5%, and news
articles 4%. All told, the corpus contains 13,860 words (868 sentences), with 3,422 NPs (the annotatable
units). Bhatia et al. (2014) report high inter-annotator agreement, estimating Cohen?s ? = 0.89 within the
TED genre as well as for all genres.
Figure 2 is an excerpt from the ?Little Red Riding Hood? annotated with the CFD scheme.
4 Classification framework
To model the relationship between the grammar of definiteness and its communicative functions in a
data-driven fashion, we work within the supervised framework of feature-rich discriminative classification,
treating the functional categories from ?2 as output labels y and various lexical, morphological, and
syntactic characteristics of the language as features of the input x. Specifically, we learn two kinds
of probabilistic models. The first is a log-linear model similar to multiclass logistic regression, but
deviating in that logistic regression treats each output label (response) as atomic, whereas we decompose
each into attributes based on their linguistic definitions, enabling commonalities between related labels
to be recognized. Each weight in the model corresponds to a feature that mediates between percepts
(characteristics of the input NP) and attributes (characteristics of the label). This is aimed at attaining
better predictive accuracy as well as feature weights that better describe the form?function interactions we
are interested in recovering. We also train a random forest model on the hypothesis that it would allow us
to sacrifice interpretability of the learned parameters for predictive accuracy.
Our setup is formalized below, where we discuss the mathematical models and linguistically motivated
features.
4.1 Models
We experiment with two classification methods: a log-linear model and a nonlinear tree-based ensemble
model. Due to their consistency and interpretability, linear models are a valuable tool for quantifying and
analyzing the effects of individual features. Non-linear models, while less interpretable, often outperform
logistic regression (Perlich et al., 2003), and thus could be desirable when the predictions are needed for a
downstream task.
1
The data can be obtained from http://www.cs.cmu.edu/~ytsvetko/definiteness_corpus.
2
The TED talks are from a large parallel corpus obtained from http://www.ted.com/talks/.
1062
4.1.1 Log-linear model
At test time, we model the probability of communicative function label y conditional on an NP x as
follows:
p
?
(y?x) = log
exp?
?
f(x,y)
?
y
??Y exp?
?
f(x,y?)
(1)
where ? ?Rd is a vector of parameters (feature weights), and f ?X ?Y ?Rd is the feature function over
input?label pairs. The feature function is defined as follows:
f(x,y) = ? (x)? ??(y) (2)
where the percept function ? ?X ?Rc produces a vector of real-valued characteristics of the input, and
the attribute function
?
? ?Y ? {0,1}a encodes characteristics of each label. There is a feature for every
percept?attribute pairing: so d = c ?a and f(i?1)a+ j(x,y) = ?i(x) ?? j(y),1 ? i ? c,1 ? j ? a.
3
The contents of
the percept and attribute functions are detailed in ?4.2 and ?4.3 respectively.
For prediction, having learned weights
?
? we use the Bayes-optimal decision rule for minimizing
misclassification error, selecting the y that maximizes this probability:
y?? argmax
y?Y
p
?
?
(y?x) (3)
Training optimizes
?
? so as to maximize a convex L
2
-regularized
4
learning objective over the training data
D:
?
? = argmax
?
?? ??? ??
2
2
+ ?
?x,y??D
log
exp?
?
f(x,y)
?
y
??Y exp(?
?
f(x,y?))
(4)
With
?
?(y) = the identity of the label, this reduces to standard logistic regression.
4.1.2 Non-linear model
We employ a random forest classifier (Breiman, 2001), an ensemble of decision tree classifiers learned
from many independent subsamples of the training data. Given an input, each tree classifier assigns a
probability to each label; those probabilities are averaged to compute the probability distribution across
the ensemble.
An important property of the random forests, in addition to being an effective tool in prediction, is
their immunity to overfitting: as the number of trees increases, they produce a limiting value of the
generalization error.
5
Thus, no hyperparameter tuning is required. Random forests are known to be
robust to sparse data and to label imbalance (Chen et al., 2004), both of which are challenges with the
definiteness dataset.
4.2 Percepts
The characteristics of the input that are incorporated in the model, which we call percepts to distinguish
them from model features linking inputs to outputs, see ?4.1, are intended to capture the aspects of English
morphosyntax that may be relevant to the communicative functions of definiteness.
After preprocessing the text with a dependency parser and coreference resolver, which is described in
?6.1, we extract several kinds of percepts for each NP.
4.2.1 Basic
Words of interest. These are the head within the NP, all of its dependents, and its governor (external to
the NP). We are also interested in the attached verb, which is the first verb one encounters when traversing
the dependency path upward from the head. For each of these words, we have separate percepts capturing:
the token, the part-of-speech (POS) tag, the lemma, the dependency relation, and (for the head only) a
3
Chahuneau et al. (2013) use a similar parametrization for their model of morphological inflection.
4
As is standard practice with these models, bias parameters (which capture the overall frequency of percepts/attributes) are
excluded from regularization.
5
See Theorem 1.2 in Breiman (2001) for details.
1063
binary indicator of plurality (determined from the POS tag). As there may be multiple dependents, we
have additional features specific to the first and the last one. Moreover, to better capture tense, aspect
and modality, we collect the attached verb?s auxiliaries. We also make note of the negative particle (with
dependency label neg) if it is a dependent of the verb.
Structural. The structural percepts are: the path length from the head up to the root, and to the attached
verb. We also have percepts for the number of dependents, and the number of dependency relations that
link non-neighbors. Integer values were binarized with thresholding.
Positional. These percepts are the token length of the NP, the NP?s location in the sentence (first or
second half), and the attached verb?s position relative to the head (left or right). 12 additional percept
templates record the POS and lemma of the left and right neighbors of the head, governor, and attached
verb.
4.2.2 Contextual NPs
When extracting features for a given NP (call it the ?target?), we also consider NPs in the following
relationship with the target NP: its immediate parent, which is the smallest NP whose span fully subsumes
that of the target; the immediate child, which is the largest NP subsumed within the target; the immediate
precedent and immediate successor within the sentence; and the nearest preceding coreferent mention.
For each of these related NPs, we include all of their basic percepts conjoined with the nature of the
relation to the target.
4.3 Attributes
As noted above, though CFD labels are organized into a tree hierarchy, there are actually several dimensions
of commonality that suggest different groupings. These attributes are encoded as ternary characteristics;
for each label (including internal labels), every one of the 8 attributes is assigned a value of +, ?, or 0
(refer to fig. 1). In light of sparse data, we design features to exploit these similarities via the attribute
vector function
?(y) = [y,A(y),B(y),F(y),G(y),P(y),R(y),S(y),U(y)]
?
(5)
where A ?Y ? {+,?,0} returns the value for Anaphoric, B(y) for Bridging, etc. The identity of the label
is also included in the vector so that different labels are always recognized as different by the attribute
function. The categorical components of this vector are then binarized to form
?
?(y); however, instead
of a binary component that fires for the 0 value of each ternary attribute, there is a component that fires
for any value of the attribute?a sort of bias term. The weights assigned to features incorporating + or ?
attribute values, then, are easily interpreted as deviations relative to the bias.
5 Evaluation
The following measures are used to evaluate our predictor against the gold standard for the held-out
evaluation (dev or test) set E :
? Exact Match: This accuracy measure gives credit only where the predicted and gold labels are identical.
? By leaf label: We also compute precision and recall of each leaf label to determine which categories
are reliably predicted.
? Soft Match: This accuracy measure gives partial credit where the predicted and gold labels are
related. It is computed as the proportion of attributes-plus-full-label whose (categorical) values match:
??(y)??(y?)?/9.
6 Experiments
6.1 Experimental Setup
Data splits. The annotated corpus of Bhatia et al. (2014) (?3) contains 17 documents in 3 genres:
13 prepared speeches (mostly TED talks),
6
2 newspaper articles, and 2 fictional narratives. We arbitrarily
choose some documents to hold out from each genre; the resulting test set consists of 2 TED talks
6
We have combined the TED talks and presidential speech genres since both involved prepared speeches.
1064
Condition ?? ? ? Exact Match Acc. Soft Match Acc.
Majority baseline ? ? 12.1 47.8
Log-linear classifier, attributes only 473,064 100 38.7 77.1
Log-linear classifier, labels only 413,931 100 40.8 73.6
Full log-linear classifier (labels + attributes) 926,417 100 43.7 78.2
Random forest classifier 20,363 ? 49.7 77.5
Table 1: Classifiers and baseline, as measured on the test set. The first two columns give the number of
parameters and the tuned regularization hyperparameter, respectively; the third and fourth columns give
accuracies as percentages. The best in each column is bolded.
(?Alisa_News?, ?RobertHammond_park?), 1 newspaper article (?crime1_iPad_E?), and 1 narrative
(?Little Red Riding Hood?). The test set then contains 19,28 tokens (111 sentences), in which there are
511 annotated NPs; while the training set contains 2,911 NPs among 11,932 tokens (757 sentences).
Preprocessing. Automatic dependency parses and coreference information were obtained with the
parser and coreference resolution system in Stanford CoreNLP v. 3.3.0 (Socher et al., 2013; Recasens
et al., 2013) for use in features (?4.2). Syntactic features were extracted from the Basic dependencies
output by the parser. To evaluate the performance of Stanford system on our data, we manually inspected
the dependencies and coreference information for a subset of sentences from our corpus (using texts
from TED talks and fictional narratives genres) and recorded the errors. We found that about 70% of the
sentences had all correct dependencies, and only about 0.04% of the total dependencies were incorrect
for our data. However, only 62.5% of the coreference links were correctly identified by the coreference
resolver. The rest of them were either missing or incorrectly identified. We believe this may have caused a
portion of the classifier errors while predicting the Ananphoic labels.
Throughout our experiments (training as well as testing), we use the gold NP boundaries identified by
the human annotators. The automatic dependency parses are used to extract percepts for each gold NP.
If there is a conflict between the gold NP boundaries and the parsed NP boundaries, to avoid extracting
misleading percepts, we assign a default value.
Learning. The log-linear model variants are trained with an in-house implementation of supervised
learning with L
2
-regularized AdaGrad (Duchi et al., 2011). Hyperparameters are tuned on a development
set formed by holding out every tenth instance from the training set (test set experiments use the full
training set): the power of 10 giving the highest Soft Match accuracy was chosen for ? .
7
The Python
scikit-learn toolkit (Pedregosa et al., 2011) was used for the random forest classifier.8
6.2 Results
Measurements of overall classification performance appear in table 1. While far from perfect, our
classifiers achieve promising accuracy levels given the small size of the training data and the number of
labels in the annotation scheme. The random forest classifier is the most accurate in Exact Match, likely
due to the robustness of that technique under conditions where the data are small and the frequencies
of individual labels are imbalanced. By the Soft Match measure, our attribute-aware log-linear models
perform very well. The most successful of the log-linear models is the richest model, which combines the
fine-grained communicative function labels with higher-level attributes of those labels. But notably the
attribute-only model, which decomposes the semantic labels into attributes without directly considering
the full label, performs almost as well as the random forest classifier in Soft Match. This is encouraging
because it suggests that the model has correctly exploited known linguistic generalizations to account for
the grammaticalization of definiteness in English.
Table 2 reports the precision and recall of each leaf label predicted. Certain leaf labels are found
to be easier for the classifier to predict: e.g., the communicative function label Pleonastic has a high
F
1
score. This is expected as the Ploenastic CFD for English is quite regular and captured by the EX
7
Preliminary experiments with cross-validation on the training data showed that the value of ? was stable across folds.
8
Because it is a randomized algorithm, the results may vary slightly between runs; however, a cross-validation experiment on
the training data found very little variance in accuracy.
1065
Leaf label N P R F
1
Leaf label N P R F
1
Pleonastic 44 100 78 88 Part_of_Noncompositional_MWE 88 20 17 18
Bridging_Restrictive_Modifier 552 58 84 68 Bridging_Nominal 33 33 10 15
Quantified 213 57 57 57 Generic_Individual_Level 113 14 11 13
Unique_Larger_Situation 97 52 58 55 Nonunique_Nonspec 173 9 25 13
Same_Head 452 41 41 41 Bridging_Other_Context 96 33 6 11
Measure_Nonreferential 98 88 26 40 Bridging_Event 9 ? 0 ?
Nonunique_Hearer_New_Spec 190 36 46 40 Nonunique_Physical_Copresence 36 0 0 ?
Other_Nonreferential 134 39 36 37 Nonunique_Predicative_Identity 10 ? 0 ?
Different_Head 271 32 33 32 Predicative_Nonidentity 57 0 0 ?
Nonunique_Larger_Situation 97 29 25 27 Unique_Hearer_New 26 ? 0 ?
Table 2: Number of training set instances and precision, recall, and F
1
percentages for leaf labels.
part-of-speech tag. The classifier finds predictions of certain CFD labels, such as Bridging_Event,
Bridging_Nominal and Nonunique_Nonspecific, to be more difficult due to data sparseness: it appears
that there were not enough training instances for the classifier to learn the generalizations corresponding
to these CFDs. Bridging_Other_Context was hard to predict as this was a category which referred not
to the entities previously mentioned but to the whole speech event from the past. There seem to be no
clear morphosyntactic cues associated with this CFD, so to train a classifier to predict this category label,
we would need to model more complex semantic and discourse information. This also applies to the
classifier confusion between the Same_Head and Different_Head, since both of these labels share all
the semantic attributes used in this study.
An advantage of log-linear models is that inspecting the learned feature weights can provide useful
insights into the model?s behavior. Figure 3 lists 10 features that received the highest positive weights
in the full model for the + and ? values of the Specific attribute. These confirm some known properties
of English definites and indefinites. The definite article, possessives (PRP$), proper nouns (NNP), and the
second person pronoun are all associated with specific NPs, while the indefinite article is associated with
nonspecific NPs. The model also seems to have picked up on the less obvious but well-attested tendency
of objects to be nonspecific (Aissen, 2003).
In addition to confirming known grammaticalization patterns of definiteness, we can mine the highly-
weighted features for new hypotheses: e.g., in figs. 3 and 4, the model thinks that objects of ?from? are
especially likely to be Specific, and that NPs with comparative adjectives (JJR) are especially likely to be
nonspecific (fig. 3). From fig. 3, we also know that Num. of dependents, dependent?s POS: 1,PRP$ has
a higher weight than, say, Num. of dependents, dependent?s POS: 2,PRP$. This observation suggests a
hypothesis that in English the NPs which have possessive pronouns immediately preceding the head are
more likely to be specific than the NPs which have intervening words between the possessive pronoun
and the head. Similarly, looking at another example in fig. 4, the following two percepts get high weights
for the NP the United States of America to be Specific: last dependent?s POS: NNP and first dependent?s
lemma: the. Since frequency and other factors affect the feature weights learned by the classifier, these
differences in weights may or may not reflect an inherent association with Specificity. Whether these
are general trends, or just an artifact of the sentences that happened to be in the training data and our
statistical learning procedure, will require further investigation, ideally with additional datasets and more
rigorous hypothesis testing.
Finally, we can remove features to test their impact on predictive performance. Notably, in experiments
ablating features indicating articles?the most obvious exponents of definiteness in English?we see
a decrease in performance, but not a drastic one. This suggests that the expression of communicative
functions of definiteness is in fact much richer than morphological definiteness.
Errors. Several labels are unattested or virtually unattested in the training data, so the models unsurpris-
ingly fail to predict them correctly at test time. Same_Head and Different_Head, though both common,
are confused quite frequently. Whether the previous coreferent mention has the same or different head is a
simple distinction for humans; low model accuracy is likely due to errors propagated from coreference
resolution. This problem is so frequent that merging these two categories and retraining the random
forest model improves Exact Match accuracy by 8% absolute and Soft Match accuracy by 5% absolute.
1066
Percepts
+Specific ?Specific
First dependent?s POS PRP$ First dependent?s lemma a
Head?s left neighbor?s POS PRP$ Last dependent?s lemma a
Last dependent?s lemma you Num. of dependents, dependent?s lemma 1,a
Num. of dependents, dependent?s lemma 1,you Head?s left neighbor?s POS JJR
Num. of dependents, dependent?s POS 1,PRP$ Last dependent?s POS JJR
Governor?s right neighbor?s POS PRP$ Num. of dependents, dependent?s lemma 2,a
Last dependent?s POS NNP First dependent?s lemma new
Last dependent?s POS PRP$ Last dependent?s lemma new
First dependent?s lemma the Num. of dependents, dependent?s POS 2,JJR
Governor?s lemma from Governor?s left neighbor?s POS VB
Figure 3: Percepts receiving highest positive weights in association with values of the Specific attribute.
Example Relevant percepts from fig. 3 CFD annotation
This is just for the United States of America. Last dependent?s POS: NNP
First dependent?s lemma: the
Unique_Larger_Situation
We were driving from our home in Nashville
to a little farm we have 50 miles east of
Nashville ? driving ourselves.
First dependent?s POS: PRP$
Head?s left neighbor?s POS: PRP$
Governor?s right neighbor?s POS: PRP$
Governor?s lemma: from
Bridging_Restrictive_Modifier
Figure 4: Sentences from our corpus illustrating percepts fired for gold NPs and their CFD annotations.
Another common confusion is between the highly frequent category Unique_Larger_Situation and the
rarer category Unique_Hearer_New; the latter is supposed to occur only for the first occurrence of a
proper name referring to a entity that is not already part of the knowledge of the larger community. In
other words, this distinction requires world knowledge about well-known entities, which could perhaps be
mined from the Web or other sources.
7 Related Work
Because semantic/pragmatic analysis of referring expressions is important for many NLP tasks, a compu-
tational model of the communicative functions of definiteness has the potential to leverage diverse lexical
and grammatical cues to facilitate deeper inferences about the meaning of linguistic input. We have used
a coreference resolution system to extract features for modeling definiteness, but an alternative would be
to predict definiteness functions as input to (or jointly with) the coreference task. Applications such as
information extraction and dialogue processing could be expected to benefit not only from coreference
information, but also from some of the semantic distinctions made in our framework, including specificity
and genericity.
Better computational processing of definiteness in different languages stands to help machine translation
systems. It has been noted that machine translation systems face problems when the source and the target
language use different grammatical strategies to express the same information (Stymne, 2009; Tsvetkov
et al., 2013). Previous work on machine translation has attempted to deal with this in terms of either
(a) preprocessing the source language to make it look more like the target language (Collins et al., 2005;
Habash, 2007; Nie?en and Ney, 2000; Stymne, 2009, inter alia); or (b) post-processing the machine
translation output to match the target language, (e.g., Popovi
?
c et al., 2006). Attempts have also been made
to use syntax on the source and/or the target sides to capture the syntactic differences between languages
(Liu et al., 2006; Yamada and Knight, 2002; Zhang et al., 2007). Automated prediction of (in)definite
articles has been found beneficial in a variety of applications, including postediting of MT output (Knight
and Chander, 1994), text generation (Elhadad, 1993; Minnen et al., 2000), and identification and correction
of ESL errors (Han et al., 2006; Rozovskaya and Roth, 2010). More recently, Tsvetkov et al. (2013)
trained a classifier to predict where English articles might plausibly be added or removed in a phrase, and
used this classifier to improve the quality of statistical machine translation.
While definiteness morpheme prediction has been thoroughly studied in computational linguistics,
1067
studies on additional, more complex aspects of definiteness are limited. Reiter and Frank (2010) exploit
linguistically-motivated features in a supervised approach to distinguish between generic and specific
NPs. Hendrickx et al. (2011) investigated the extent to which a coreference resolution system can resolve
the bridging relations. Also in the context of coreference resolution, Ng and Cardie (2002) and Kong
et al. (2010) have examined anaphoricity detection. To the best of our knowledge, no studies have been
conducted on automatic prediction of semantic and pragmatic communicative functions of definiteness
more broadly.
Our work is related to research in linguistics on the modeling of syntactic constructions such as dative
shift and the expression of possession with ?of? or ??s?. Bresnan and Ford (2010) used logistic regression
with semantic features to predict syntactic constructions. Although we are doing the opposite (using
syntactic features to predict semantic categories), we share the assumption that reductionist approaches (as
mentioned earlier) are not able to capture all the nuances of a linguistic phenomenon. Following Hopper
and Traugott (2003) we observe that grammaticalization is accompanied by function drift, resulting in
multiple communicative functions for each grammatical construction. Other attempts have also been made
to capture, using classifiers, (propositional as well as non propositional) aspects of meaning that have
been grammaticalized: see, for instance, Reichart and Rappoport (2010) for tense sense disambiguation,
Prabhakaran et al. (2012) for modality tagging, and Srikumar and Roth (2013) for semantics expressed by
prepositions.
8 Conclusion
We have presented a data-driven approach to modeling the relationship between universal communicative
functions associated with (in)definiteness and their lexical/grammatical realization in a particular language.
Our feature-rich classifiers can give insights into this relationship as well as predict communicative
functions for the benefit of NLP systems. Exploiting the higher-level semantic attributes, our log-linear
classifier compares favorably to the random forest classifier in Soft Match accuracy. Further improvements
to the classifier may come from additional features or better preprocessing. This work has focused on
English, but in future work we plan to build similar models for other languages?including languages
without articles, under the hypothesis that such languages will rely on other, subtler devices to encode
many of the functions of definiteness.
Acknowledgments
This work was sponsored by the U. S. Army Research Laboratory and the U. S. Army Research Office
under contract/grant number W911NF-10-1-0533. We thank the reviewers for their useful comments.
References
Barbara Abbott. 2006. Definite and indefinite. In Keith Brown, editor, Encyclopedia of Language and Linguistics,
pages 3?392. Elsevier.
Judith Aissen. 2003. Differential object marking: iconicity vs. economy. Natural Language & Linguistic Theory,
21(3):435?483.
Archna Bhatia, Mandy Simons, Lori Levin, Yulia Tsvetkov, Chris Dyer, and Jordan Bender. 2014. A unified anno-
tation scheme for the semantic/pragmatic components of definiteness. In Proc. of LREC. Reykjav?k, Iceland.
Betty Birner and Gregory Ward. 1994. Uniqueness, familiarity and the definite article in English. In Proc. of the
Twentieth Annual Meeting of the Berkeley Linguistics Society, pages 93?102.
Leo Breiman. 2001. Random forests. Machine Learning, 45(1):5?32.
Joan Bresnan and Marilyn Ford. 2010. Predicting syntax: Processing dative constructions in American and Aus-
tralian varieties of English. Language, 86(1):168?213.
Victor Chahuneau, Eva Schlinger, Noah A. Smith, and Chris Dyer. 2013. Translating into morphologically rich
languages with synthetic phrases. In Proc. of EMNLP, pages 1677?1687. Seattle, Washington, USA.
1068
Chao Chen, Andy Liaw, and Leo Breiman. 2004. Using random forest to learn imbalanced data. University of
California, Berkeley.
Ping Chen. 2004. Identifiability and definiteness in Chinese. Linguistics, 42:1129?1184.
Michael Collins, Philipp Koehn, and Ivona Kucerova. 2005. Clause restructuring for statistical machine translation.
In Proc. of ACL, pages 531?540. Ann Arbor, Michigan.
Cleo Condoravdi. 1992. Strong and weak novelty and familiarity. In Proc. of SALT II, pages 17?37.
William Croft. 2003. Typology and Universals. Cambridge University Press.
John Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive subgradient methods for online learning and stochastic
optimization. Journal of Machine Learning Research, 12(Jul):2121?2159.
Michael Elhadad. 1993. Generating argumentative judgment determiners. In Proc. of AAAI, pages 344?349.
Gareth Evans. 1977. Pronouns, quantifiers and relative clauses. Canadian Journal of Philosophy, 7(3):46.
Gareth Evans. 1980. Pronouns. Linguistic Inquiry, 11.
Jeanette K. Gundel, Nancy Hedberg, and Ron Zacharski. 1988. The generation and interpretation of demonstrative
expressions. In Proc. of XIIth International Conference on Computational Linguistics, pages 216?221.
Jeanette K. Gundel, Nancy Hedberg, and Ron Zacharski. 1993. Cognitive status and the form of referring expres-
sions in discourse. Language, 69:274?307.
Nizar Habash. 2007. Syntactic preprocessing for statistical machine translation. In MT Summit XI, pages 215?222.
Copenhagen.
Na-Rae Han, Martin Chodorow, and Claudia Leacock. 2006. Detecting errors in english article usage by non-native
speakers. Natural Language Engineering, 12:115?129.
Irene Heim. 1990. E-type pronouns and donkey anaphora. Linguistics and Philosophy, 13:137?177.
Iris Hendrickx, Orph?e De Clercq, and V?ronique Hoste. 2011. Analysis and reference resolution of bridge
anaphora across different text genres. In Iris Hendrickx, Sobha Lalitha Devi, Antonio Horta Branco, and Ruslan
Mitkov, editors, DAARC, volume 7099 of Lecture Notes in Computer Science, pages 1?11. Springer.
Paul J. Hopper and Elizabeth Closs Traugott. 2003. Grammaticalization. Cambridge University Press.
Nirit Kadmon. 1987. On unique and non-unique reference and asymmetric quantification. Ph.D. thesis, University
of Massachusetts.
Nirit Kadmon. 1990. Uniqueness. Linguistics and Philosophy, 13:273?324.
Kevin Knight and Ishwar Chander. 1994. Automated postediting of documents. In Proc. of the National Conference
on Artificial Intelligence, pages 779?779. Seattle, WA.
Erwin Ronald Komen. 2013. Finding focus: a study of the historical development of focus in English. LOT,
Utrecht.
Fang Kong, Guodong Zhou, Longhua Qian, and Qiaoming Zhu. 2010. Dependency-driven anaphoricity determi-
nation for coreference resolution. In Proc. of COLING, pages 599?607. Beijing, China.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-string alignment template for statistical machine translation.
In Proc. of COLING/ACL, pages 609?616. Sydney, Australia.
Christopher Lyons. 1999. Definiteness. Cambridge University Press.
Guido Minnen, Francis Bond, and Ann Copestake. 2000. Memory-based learning for article generation. In Proc. of
1069
the 2nd Workshop on Learning Language in Logic and the 4th Conference on Computational Natural Language
Learning, pages 43?48.
Vincent Ng and Claire Cardie. 2002. Identifying anaphoric and non-anaphoric noun phrases to improve coreference
resolution. In Proc. of COLING. Taipei, Taiwan.
Sonja Nie?en and Hermann Ney. 2000. Improving SMT quality with morpho-syntactic analysis. In Proc. of
COLING, pages 1081?1085.
Fabian Pedregosa, Gael Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Math-
ieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David Cour-
napeau, Matthieu Brucher, M. Perrot, and Edouard Duchesnay. 2011. Scikit-learn: Machine learning in Python.
Journal of Machine Learning Research, 12:2825?2830.
Claudia Perlich, Foster Provost, and Jeffrey S. Simonoff. 2003. Tree induction vs. logistic regression: a learning-
curve analysis. Journal of Machine Learning Research, 4:211?255.
Maja Popovi?c, Daniel Stein, and Hermann Ney. 2006. Statistical machine translation of German compound words.
In Advances in Natural Language Processing, pages 616?624. Springer.
Vinodkumar Prabhakaran, Michael Bloodgood, Mona Diab, Bonnie Dorr, Lori Levin, Christine D. Piatko, Owen
Rambow, and Benjamin Van Durme. 2012. Statistical modality tagging from rule-based annotations and crowd-
sourcing. In Proc. of the Workshop on Extra-Propositional Aspects of Meaning in Computational Linguistics,
ExProM ?12, pages 57?64.
Ellen F. Prince. 1992. The ZPG letter: Subjects, definiteness and information status. In S. Thompson and W. Mann,
editors, Discourse description: diverse analyses of a fund raising text, pages 295?325. John Benjamins.
Marta Recasens, Marie-Catherine de Marneffe, and Christopher Potts. 2013. The life and death of discourse
entities: identifying singleton mentions. In Proc. of NAACL-HLT, pages 627?633. Atlanta, Georgia, USA.
Roi Reichart and Ari Rappoport. 2010. Tense sense disambiguation: A new syntactic polysemy task. In Proc. of
EMNLP, EMNLP ?10, pages 325?334.
Nils Reiter and Anette Frank. 2010. Identifying generic noun phrases. In Proc. of ACL, pages 40?49. Uppsala,
Sweden.
Craig Roberts. 2003. Uniqueness in definite noun phrases. Linguistics and Philosophy, 26:287?350.
Alla Rozovskaya and Dan Roth. 2010. Training paradigms for correcting errors in grammar and usage. In Proc.
of NAACL-HLT, pages 154?162.
Bertrand Russell. 1905. On denoting. Mind, New Series, 14:479?493.
Richard Socher, John Bauer, Christopher D. Manning, and Andrew Y. Ng. 2013. Parsing with compositional vector
grammars. In Proc. of ACL, pages 455?465. Sofia, Bulgaria.
Vivek Srikumar and Dan Roth. 2013. An inventory of preposition relations. CoRR, abs/1305.5785.
Sara Stymne. 2009. Definite noun phrases in statistical machine translation into Danish. In Proc. of Workshop on
Extracting and Using Constructions in NLP, pages 4?9.
Yulia Tsvetkov, Chris Dyer, Lori Levi, and Archna Bhatia. 2013. Generating English determiners in phrase-based
translation with synthetic translation options. In Proc. of WMT.
Kenji Yamada and Kevin Knight. 2002. A decoder for syntax-based statistical MT. In Proc. of ACL, pages 303?310.
Philadelphia, Pennsylvania, USA.
Yuqi Zhang, Richard Zens, and Hermann Ney. 2007. Improved chunk-level reordering for statistical machine
translation. In IWSLT 2007: International Workshop on Spoken Language Translation, pages 21?28.
1070
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1001?1012,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
A Dependency Parser for Tweets
Lingpeng Kong Nathan Schneider Swabha Swayamdipta
Archna Bhatia Chris Dyer Noah A. Smith
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{lingpenk,nschneid,swabha,archna,cdyer,nasmith}@cs.cmu.edu
Abstract
We describe a new dependency parser for
English tweets, TWEEBOPARSER. The
parser builds on several contributions: new
syntactic annotations for a corpus of tweets
(TWEEBANK), with conventions informed
by the domain; adaptations to a statistical
parsing algorithm; and a new approach to
exploiting out-of-domain Penn Treebank
data. Our experiments show that the parser
achieves over 80% unlabeled attachment
accuracy on our new, high-quality test set
and measure the benefit of our contribu-
tions.
Our dataset and parser can be found at
http://www.ark.cs.cmu.edu/TweetNLP.
1 Introduction
In contrast to the edited, standardized language of
traditional publications such as news reports, social
media text closely represents language as it is used
by people in their everyday lives. These informal
texts, which account for ever larger proportions of
written content, are of considerable interest to re-
searchers, with applications such as sentiment anal-
ysis (Greene and Resnik, 2009; Kouloumpis et al.,
2011). However, their often nonstandard content
makes them challenging for traditional NLP tools.
Among the tools currently available for tweets are
a POS tagger (Gimpel et al., 2011; Owoputi et al.,
2013) and a named entity recognizer (Ritter et al.,
2011)?but not a parser.
Important steps have been taken. The English
Web Treebank (Bies et al., 2012) represents an
annotation effort on web text?which likely lies
somewhere between newspaper text and social me-
dia messages in formality and care of editing?that
was sufficient to support a shared task (Petrov and
McDonald, 2012). Foster et al. (2011b) annotated
a small test set of tweets to evaluate parsers trained
on the Penn Treebank (Marcus et al., 1993), aug-
mented using semi-supervision and in-domain data.
Others, such as Soni et al. (2014), have used exist-
ing Penn Treebank?trained models on tweets.
In this work, we argue that the Penn Treebank
approach to annotation?while well-matched to
edited genres like newswire?is poorly suited to
more informal genres. Our starting point is that
rapid, small-scale annotation efforts performed
by imperfectly-trained annotators should provide
enough evidence to train an effective parser. We
see this starting point as a necessity, given observa-
tions about the rapidly changing nature of tweets
(Eisenstein, 2013), the attested difficulties of do-
main adaptation for parsing (Dredze et al., 2007),
and the expense of creating Penn Treebank?style
annotations (Marcus et al., 1993).
This paper presents TWEEBOPARSER, the first
syntactic dependency parser designed explicitly for
English tweets. We developed this parser follow-
ing current best practices in empirical NLP: we
annotate a corpus (TWEEBANK) and train the pa-
rameters of a statistical parsing algorithm. Our
research contributions include:
? a survey of key challenges posed by syntactic
analysis of tweets (by humans or machines) and
decisions motivated by those challenges and by
our limited annotation-resource scenario (?2);
? our annotation process and quantitative mea-
sures of the quality of the annotations (?3);
? adaptations to a statistical dependency parsing
algorithm to make it fully compatible with the
above, and also to exploit information from out-
of-domain data cheaply and without a strong
commitment (?4); and
? an experimental analysis of the parser?s unla-
beled attachment accuracy?which surpasses
80%?and contributions of various important
components (?5).
The dataset and parser can be found at http://www.
ark.cs.cmu.edu/TweetNLP.
1001
2 Annotation Challenges
Before describing our annotated corpus of tweets
(?3), we illustrate some of the challenges of syn-
tactic analysis they present. These challenges moti-
vate an approach to annotation that diverges signif-
icantly from conventional approaches to treebank-
ing. Figure 1 presents a single example illustrating
four of these: token selection, multiword expres-
sions, multiple roots, and structure within noun
phrases. We discuss each in turn.
2.1 Token Selection
Many elements in tweets have no syntactic function.
These include, in many cases, hashtags, URLs, and
emoticons. For example:
RT @justinbieber : now Hailee get a twitter
The retweet discourse marker, username, and colon
should not, we argue, be included in the syntactic
analysis. By contrast, consider:
Got #college admissions questions ? Ask them
tonight during #CampusChat I?m looking
forward to advice from @collegevisit
http://bit.ly/cchOTk
Here, both the hashtags and the at-mentioned user-
name are syntactically part of the utterances, while
the punctuation and the hyperlink are not. In the
example of Figure 1, the unselected tokens include
several punctuation tokens and the final token #be-
lieber, which marks the topic of the tweet.
Typically, dependency parsing evaluations ig-
nore punctuation token attachment (Buchholz and
Marsi, 2006), and we believe it is a waste of an-
notator (and parser) time to decide how to attach
punctuation and other non-syntactic tokens. Ma
et al. (2014) recently proposed to treat punctua-
tion as context features rather than dependents, and
found that this led to state-of-the-art performance
in a transition-based parser. A small adaptation
to our graph-based parsing approach, described in
?4.2, allows a similar treatment.
Our approach to annotation (?3) forces annota-
tors to explicitly select tokens that have a syntactic
function. 75.6% tokens were selected by the anno-
tators. Against the annotators? gold standard, we
found that a simple rule-based filter for usernames,
hashtags, punctuation, and retweet tokens achieves
95.2% (with gold-standard POS tags) and 95.1%
(with automatic POS tags) average accuracy in the
task of selecting tokens with a syntactic function
in a ten-fold cross-validation experiment. To take
context into account, we developed a first-order
sequence model and found that it achieves 97.4%
average accuracy (again, ten-fold cross-validated)
with either gold-standard or automatic POS tags.
Features include POS; shape features that recog-
nize the retweet marker, hashtags, usernames, and
hyperlinks; capitalization; and a binary feature
for tokens that include punctuation. We trained
the model using the structured perceptron (Collins,
2002).
2.2 Multiword Expressions
We consider multiword expressions (MWEs) of
two kinds. The first, proper names, have been
widely modeled for information extraction pur-
poses, and even incorporated into parsing (Finkel
and Manning, 2009). (An example found in Fig-
ure 1 is LA Times.) The second, lexical idioms,
have been a ?pain in the neck? for many years (Sag
et al., 2002) and have recently received shallow
treatment in NLP (Baldwin and Kim, 2010; Con-
stant and Sigogne, 2011; Schneider et al., 2014).
Constant et al. (2012), Green et al. (2012), Candito
and Constant (2014), and Le Roux et al. (2014)
considered MWEs in parsing. Figure 1 provides
LA Times and All the Rage as examples.
Penn Treebank?style syntactic analysis (and de-
pendency representations derived from it) does
not give first-class treatment to this phenomenon,
though there is precedent for marking multiword
lexical units and certain kinds of idiomatic relation-
ships (Haji
?
c et al., 2012; Abeill? et al., 2003).
1
We argue that internal analysis of MWEs is not
critical for many downstream applications, and
therefore annotators should not expend energy on
developing and respecting conventions (or mak-
ing arbitrary decisions) within syntactically opaque
or idiosyncratic units. We therefore allow annota-
tors to decide to group words as explicit MWEs,
including: proper names (Justin Bieber, World
Series), noncompositional or entrenched nominal
compounds (belly button, grilled cheese), connec-
tives (as well as), prepositions (out of), adverbials
(so far), and idioms (giving up, make sure).
From an annotator?s perspective, a MWE func-
tions as a single node in the dependency parse,
with no internal structure. For idioms whose in-
ternal syntax is easily characterized, the parse can
be used to capture compositional structure, an at-
1
The popular Stanford typed dependencies (de Marneffe
and Manning, 2008) scheme includes a special dependency
type for multiwords, though this is only applied to a small list.
1002
Helvetica font or similar (such as Arial). No italics. The current gray color for unselected tokens is fine.
#1385F0#17AD3F
#C82506
#FA9F1B
ROOT
COORD MWEmultiword expressionscoordination noun phrase internal structure
multiple roots
OMG I <3 the Biebs & want to have his babies ! ?> LATimes : Social Media ? #belieber
NOUN PHRASE INTERNAL STRUCTURE
NOUN PHRASE ?INTERNAL STRUCTUREOMG I ? the Biebs & want to have his babies ! ?> LA Times : Teen Pop Star Heartthrob is All the Rage on Social Media ? #belieberROOT MULTIPLE ROOTSCOORD ROOT MWE MWE
ROOT ROOT
? #belieber
Helvetica font or similar (such as Arial). No italics. The current gray color for unselected tokens is fine.
#1385F0#17AD3F
#C82506
#FA9F1B
ROOT
COORD MWEmultiword expressionscoordination noun phrase internal structure
multiple roots
OMG I <3 the Biebs & want to have his babies ! ?> LATimes : Social Media ? #belieber
NOUN PHRASE INTERNAL STRUCTURE
NOUN PHRASE ?INTERNAL STRUCTUREOMG I ? the Biebs & want to have his babies ! ?> LA Times : Teen Pop Star Heartthrob is All the Rage on Social Media ? #belieberROOT MULTIPLE ROOTSCOORD ROOT MWE MWE
ROOT ROOT
Figure 1: Parse tree for a (constructed) example illustrating annotation challenges discussed in ?2. Colors highlight token
selection (gray; ?2.1), multiword expressions (blue; ?2.2), multiple roots (red; ?2.3), coordination (dotted arcs, green; ?3.2), and
noun phrase internal structure (orange; ?2.4). The internal structure of multiword expressions (dashed arcs below the sentence)
was predicted automatically by a parser, as described in ?2.2.
tractive property from the perspective of semantic
processing.
To allow training a fairly conventional statisti-
cal dependency parser from these annotations, we
find it expedient to apply an automatic conversion
to the MWE annotations, in the spirit of Johnson
(1998). We apply an existing dependency parser,
the first-order TurboParser (Martins et al., 2009)
trained on the Penn Treebank, to parse each MWE
independently, assigning structures like those for
LA Times and All the Rage in Figure 1. Arcs
involving the MWE in the annotation are then re-
connected to the MWE-internal root, so that the re-
sulting tree respects the original tokenization. The
MWE-internal arcs are given a special label so that
the transformation can be reversed and MWEs re-
constructed from parser output.
2.3 Multiple Roots
For news text such as that found in the Penn Tree-
bank, sentence segmentation is generally consid-
ered a very easy task (Reynar and Ratnaparkhi,
1997). Tweets, however, often contain multiple
sentences or fragments, which we call ?utterances,?
each with its own syntactic root disconnected from
the others. The selected tokens in Figure 1 com-
prise four utterances.
Our approach to annotation allows multiple ut-
terances to emerge directly from the connectedness
properties of the graph implied by an annotator?s
decisions. Our parser allows multiple attachments
to the ?wall? symbol, so that multi-rooted analyses
can be predicted.
2.4 Noun Phrase Internal Structure
A potentially important drawback of deriving de-
pendency structures from phrase-structure annota-
tions, as is typically done using the Penn Treebank,
is that flat annotations lead to loss of information.
This is especially notable for noun phrases in the
Penn Treebank (Vadas and Curran, 2007). Consider
Teen Pop Star Heartthrob in Figure 1; Penn Tree-
bank conventions would label this as a single NP
with four NN children and no internal structure. De-
pendency conversion tools would likely attach the
first three words in the NP to Heartthrob. Direct de-
pendency annotation (rather than phrase-structure
annotation followed by automatic conversion) al-
lows a richer treatment of such structures, which is
potentially important for semantic analysis (Vecchi
et al., 2013).
3 A Twitter Dependency Corpus
In this section, we describe the TWEEBANK cor-
pus, highlighting data selection (?3.1), the annota-
tion process (?3.2), important convention choices
(?3.3), and measures of quality (?3.4).
3.1 Data Selection
We added manual dependency parses to 929 tweets
(12,318 tokens) drawn from the POS-tagged Twit-
ter corpus of Owoputi et al. (2013), which are tok-
enized and contain manually annotated POS tags.
Owoputi et al.?s data consists of two parts. The
first, originally annotated by Gimpel et al. (2011),
consists of tweets sampled from a particular day,
October 27, 2010?this is known as OCT27. Due
to concerns about overfitting to phenomena specific
to that day (e.g., tweets about a particular sports
game), Owoputi et al. (2013) created a new set of
547 tweets (DAILY547) consisting of one random
English tweet per day from January 2011 through
June 2012.
Our corpus is drawn roughly equally from
OCT27 and DAILY547.
2
Despite its obvious tem-
poral skew, there is no reason to believe this sample
is otherwise biased; our experiments in ?5 suggest
that this property is important.
3.2 Annotation
Unlike a typical treebanking project, which may
take years and involve thousands of person-hours
of work by linguists, most of TWEEBANK was built
in a day by two dozen annotators, most of whom
had only cursory training in the annotation scheme.
2
This results from a long-term goal to fully annotate both.
1003
(1) RT @FRIENDSHlP : Friendship is love without
kissing ...
Friendship > is < love < without < kissing
(2) bieber is an alien ! :O he went down to earth .
bieber > is** < alien < an
he > [went down]** < to < earth
(3) RT @YourFavWhiteGuy : Helppp meeeee . I?mmm
meltiiinngggg ? http://twitpic.com/316cjg
Helppp** < meeeee
I?mmm** < meltiiinngggg
Figure 2: Examples of GFL annotations from the corpus.
Our annotators used the Graph Fragment Lan-
guage (GFL), a text-based notation that facilitates
keyboard entry of parses (Schneider et al., 2013). A
Python Flask web application allows the annotator
to validate and visualize each parse (Mordowanec
et al., 2014). Some examples are shown in Fig-
ure 2. Note that all of the challenges in ?2 are
handled easily by GFL notation: ?retweet? infor-
mation, punctuation, and a URL are not selected by
virtue of their exclusion from the GFL expression;
in (2) went down is annotated as a MWE using
GFL?s square bracket notation; in (3) the tokens
are grouped into two utterances whose roots are
marked by the ** symbol.
Schneider et al.?s GFL offers some additional fea-
tures, only some of which we made use of in this
project. One important feature allows an annotator
to leave the parse underspecified in some ways. We
allowed our annotators to make use of this feature;
however, we excluded from our training and test-
ing data any parse that was incomplete (i.e., any
parse that contained multiple disconnected frag-
ments with no explicit root, excluding unselected
tokens). Learning to parse from incomplete anno-
tations is a fascinating topic explored in the past
(Hwa, 2001; Pereira and Schabes, 1992) and, in the
case of tweets, left for future work.
An important feature of GFL that we did use is
special notation for coordination structures. For
the coordination structure in Figure 1, for example,
the notation is:
$a :: {? want} :: {&}
where $a creates a new node in the parse tree as it is
visualized for the annotator, and this new node at-
taches to the syntactic parent of the conjoined struc-
ture, avoiding the classic forced choice between
coordinator and conjunct as parent. For learning to
parse, we transform GFL?s coordination structures
into specially-labeled dependency parses collaps-
ing nodes like $a with the coordinator and labeling
the attachments specially for postprocessing, fol-
lowing Schneider et al. (2013). In our evaluation
(?5), these are treated like other attachments.
3.3 Annotation Conventions
A wide range of dependency conventions are in use;
in many cases these are conversion conventions
specifying how dependency trees can be derived
from phrase-structure trees. For English, the most
popular are due to Yamada and Matsumoto (2003)
and de Marneffe and Manning (2008), known as
?Yamada-Matsumoto? (YM) and ?Stanford? depen-
dencies, respectively. The main differences be-
tween them are in whether the auxiliary is the par-
ent of the main verb (or vice versa) and whether the
preposition or its argument heads a prepositional
phrase (Elming et al., 2013).
A full discussion of our annotation conventions
is out of scope. We largely followed the conven-
tions suggested by Schneider et al. (2013), which in
turn are close to those of YM. Auxiliary verbs are
parents of main verbs, and prepositions are parents
of their arguments. The key differences from YM
are in coordination structures (discussed in ?3.2;
YM makes the first conjunct the head) and posses-
sive structures, in which the possessor is the child
of the clitic, which is the child of the semantic head,
e.g., the > king > ?s > horses.
3.4 Intrinsic Quality
Our approach to developing this initial corpus of
syntactically annotated tweets was informed by an
aversion to making the perfect the enemy of the
good; that is, we sought enough data of sufficient
quality to build a usable parser within a relatively
short amount of time. If our research goals had
been to develop a replicable process for annotation,
more training and more quality control would have
been called for. Under our budgeted time and anno-
tator resources, this overhead was simply too costly.
Nonetheless, we performed a few analyses that give
a general picture of the quality of the annotations.
Inter-annotator agreement. 170 of the tweets
were annotated by multiple users. By the softCom-
Prec measure (Schneider et al., 2013),
3
the agree-
ment rate on dependencies is above 90%.
Expert linguistic judgment. A linguist co-
author examined a stratified sample (balanced
3
softComPrec is a generalization of attachment accuracy
that handles unselected tokens and MWEs.
1004
across annotators) of 60 annotations and rated their
quality on a 5-point scale. 30 annotations were
deemed to have ?no obvious errors,? 15 only minor
errors, 3 a major error (i.e., clear violation of an-
notation guidelines),
4
4 a major error and at least
one minor error, and 8 as containing multiple major
errors. Thus, 75% are judged as having no major
errors. We found this encouraging, considering that
this sample is skewed in favor of people who anno-
tated less (including many of the less experienced
and/or lower-proficiency annotators).
Pairwise ranking. For 170 of the doubly anno-
tated tweets, an experienced annotator examined
whether one or the other was markedly better. In
100 cases the two annotations were of comparable
quality (neither was obviously better) and did not
contain any obvious major errors. In only 7 pairs
did both of the annotations contain a serious error.
Qualitatively, we found several unsurprising
sources of error or disagreement, including em-
bedded/subordinate clauses, subject-auxiliary in-
version, predeterminers, and adverbial modifiers
following a modal/auxiliary verb and a main verb.
Clarification of the conventions, or even explicit
rule-based checking in the validation step, might
lead to quality improvements in further annotation
efforts.
4 Parsing Algorithm
For parsing, we start with TurboParser, which is
open-source and has been found to perform well on
a range of parsing problems in different languages
(Martins et al., 2013; Kong and Smith, 2014). The
underlying model allows for flexible incorporation
of new features and changes to specification in the
output space. We briefly review the key ideas in
TurboParser (?4.1), then describe decoder modifi-
cations required for our problem (?4.2). We then
discuss features we added to TurboParser (?4.3).
4.1 TurboParser
Let an input sentence be denoted by x and the set
of possible dependency parses for x be denoted by
Y
x
. A generic linear scoring function based on a
4
What we deemed major errors included, for example,
an incorrect dependency relation between an auxiliary verb
and the main verb (like ima > [have to]). Minor errors
included an incorrect attachment between two modifiers of
the same head, as in the > only > [grocery store]?the
correct annotation would have two attachments to a single
head, i.e. the > [grocery store] < only (or equivalent).
feature vector representation g is used in parsing
algorithms that seek to find:
parse
?(x) = argmax
y?Y
x
w
?
g(x,y) (1)
The score is parameterized by a vector w of
weights, which are learned from data (most com-
monly using MIRA, McDonald et al., 2005a).
The decomposition of the features into local
?parts? is a critical choice affecting the computa-
tional difficulty of solving Eq. 1. The most aggres-
sive decomposition leads to an ?arc-factored? or
?first-order? model, which permits exact, efficient
solution of Eq. 1 using spanning tree algorithms
(McDonald et al., 2005b) or, with a projectivity
constraint, dynamic programming (Eisner, 1996).
Second- and third-order models have also been
introduced, typically relying on approximations,
since less-local features increase the computational
cost, sometimes to the point of NP-hardness (Mc-
Donald and Satta, 2007). TurboParser attacks the
parsing problem using a compact integer linear pro-
gramming (ILP) representation of Eq. 1 (Martins
et al., 2009), then employing alternating directions
dual decomposition (AD
3
; Martins et al., 2011).
This enables inclusion of second-order features
(e.g., on a word with its sibling or grandparent;
Carreras, 2007) and third-order features (e.g., a
word with its parent, grandparent, and a sibling, or
with its parent and two siblings; Koo and Collins,
2010).
For a collection of (possibly overlapping) parts
for input x, S
x
(which includes the union of all
parts of all trees in Y
x
), we will use the following
notation. Let
g(x,y) = ?
s?S
x
f
s
(x,y), (2)
where f
s
only considers part s and is nonzero only
if s is present in y. In the ILP framework, each s
has a corresponding binary variable z
s
indicating
whether part s is included in the output. A col-
lection of constraints relating z
s
define the set of
feasible vectors z that correspond to valid outputs
and enfore agreement between parts that overlap.
Many different versions of these constraints have
been studied (Riedel and Clarke, 2006; Smith and
Eisner, 2008; Martins et al., 2009, 2010).
A key attraction of TurboParser is that many
overlapping parts can be handled, making use of
separate combinatorial algorithms for efficiently
handling subsets of constraints. For example, the
constraints that force z to encode a valid tree can
be exploited within the framework by making calls
1005
to classic arborescence algorithms (Chu and Liu,
1965; Edmonds, 1967). As a result, when describ-
ing modifications to TurboParser, we need only to
explain additional constraints and features imposed
on parts.
4.2 Adapted Parse Parts
The first collection of parts we adapt are simple
arcs, each consisting of an ordered pair of indices
of words in x; arc(p,c) corresponds to the attach-
ment of x
c
as a child of x
p
(iff z
arc(p,c) = 1). Our rep-
resentation explicitly excludes some tokens from
being part of the syntactic analysis (?2.1); to han-
dle this, we constrain z
arc(i, j) = 0 whenever xi or x j
is excluded.
The implication is that excluded tokens are still
?visible? to feature functions that involve other
edges. For example, some conventional first-order
features consider the tokens occurring between a
parent and child. Even if a token plays no syntactic
role of its own, it might still be informative about
the syntactic relationships among other tokens. We
note three alternative methods:
1. We might remove all unselected tokens from
x before running the parser. In ?5.6 we find
this method to fare 1.7?2.3% worse than our
modified decoding algorithm.
2. We might remove unselected tokens but use
them to define new features, so that they still
serve as evidence. This is the approach taken
by Ma et al. (2014) for punctuation. We judge
our simple modification to the decoding algo-
rithm to be more expedient, and leave the trans-
lation of existing context-word features into that
framework for future exploration.
3. We might incorporate the token selection deci-
sions into the parser, performing joint inference
for selection and parsing. The AD
3
algorithm
within TurboParser is well-suited to this kind
of extension: z-variables for each token?s se-
lection could be added, and similar scores to
those of our token selection sequence model
(?2.1) could be integrated into parsing. Given,
however, that the sequence model achieves over
97% accuracy, and that perfect token selection
would gain only 0.1?1% in parsing accuracy (re-
ported in ?5.5), we leave this option for future
work as well.
For first-order models, the above change is all
that is necessary. For second- and third-order
models, TurboParser makes use of head automata,
in particular ?grand-sibling head automata? that
assign scores to word tuples of x
g
, its child x
p
,
and two of x
p
?s adjacent children, x
c
and x
?
c
(Koo
et al., 2010). The second-order models in our
experiments include parts for sibling(p,c,c?) and
grandparent(p,c,g) and use the grand-sibling head
automaton to reason about these together. Au-
tomata for an unselected x
p
or x
g
, and transitions
that consider unselected tokens as children, are
eliminated. In order to allow the scores to depend
on unselected tokens between x
c
and x
?
c
, we added
the binned counts of unselected tokens (mostly
punctuation) joint with the word form and POS
tag of x
p
and the POS tag of x
c
and x
?
c
as features
scored in the sibling(p,c,c?) part. The changes dis-
cussed above comprise the totality of adaptations
we made to the TurboParser algorithm; we refer to
them as ?parsing adaptations? in the experiments.
4.3 Additional Features
Brown clusters. Owoputi et al. (2013) found that
Brown et al. (1992) clusters served as excellent fea-
tures in Twitter POS tagging. Others have found
them useful in parsing (Koo et al., 2008) and other
tasks (Turian et al., 2010). We therefore follow
Koo et al. in incorporating Brown clusters as fea-
tures, making use of the publicly available Twitter
clusters from Owoputi et al.
5
We use 4 and 6 bit
cluster representations to create features wherever
POS tags are used, and full bit strings to create
features wherever words were used.
Penn Treebank features. A potential danger of
our choice to ?start from scratch? in developing
a dependency parser for Twitter is that the result-
ing annotation conventions, data, and desired out-
put are very different from dependency parses de-
rived from the Penn Treebank. Indeed, Foster et al.
(2011a) took a very different approach, applying
Penn Treebank conventions in annotation of a test
dataset for evaluation of a parser trained using Penn
Treebank trees. In ?5.4, we replicate, for depen-
dencies, their finding that a Penn Treebank?trained
parser is hard to beat on their dataset, which was
not designed to be topically representative of En-
glish Twitter. When we turn to a more realistic
dataset like ours, we find the performance of the
Penn Treebank?trained parser to be poor.
Nonetheless, it is hard to ignore such a large
amount of high-quality syntactic data. We there-
5http://www.ark.cs.cmu.edu/TweetNLP/clusters/
50mpaths2
1006
fore opted for a simple, stacking-inspired incor-
poration of Penn Treebank information into our
model.
6
We define a feature on every candidate arc
whose value is the (quantized) score of the same arc
under a first-order model trained on the Penn Tree-
bank converted using head rules that are as close
as possible to our conventions (discussed in more
detail in ?5.1). This lets a Penn Treebank model
literally ?weigh in? on the parse for a tweet, and
lets the learning algorithm determine how much
consideration it deserves.
5 Experiments
Our experiments quantify the contributions of vari-
ous components of our approach.
5.1 Setup
We consider two test sets. The first, TEST-NEW,
consists of 201 tweets from our corpus annotated
by the most experienced of our annotators (one
of whom is a co-author of this work). Given very
limited data, we believe using the highest quality
data for measuring performance, and lower-quality
data for training, is a sensibly realistic choice.
Our second test set, TEST-FOSTER, is the dataset
annotated by Foster et al. (2011b), which consists
of 250 sentences. Recall that their corpus was
annotated with phrase structures according to Penn
Treebank conventions. Conversion to match our
annotation conventions was carried out as follows:
1. We used the PennConverter tool with head rule
options selected to approximate our annotation
conventions as closely as possible.
7
2. An experienced annotator manually modified
the automatically converted trees by:
(a) Performing token selection (?2.1) to remove
the tokens which have no syntactic function.
(b) Grouping MWEs (?2.2). Here, most of the
MWEs are named entities such as Manch-
ester United.
(c) Attaching the roots of the utterance in tweets
to the ?wall? symbol (?2.3).
8
6
Stacking is a machine learning method where the predic-
tions of one model are used to create features for another. The
second model may be from a different family. Stacking has
been found successful for dependency parsing by Nivre and
McDonald (2008) and Martins et al. (2008). Johansson (2013)
describes further advances that use path features.
7http://nlp.cs.lth.se/software/treebank_
converter; run with -rightBranching=false
-coordStructure=prague -prepAsHead=true
-posAsHead=true -subAsHead=true -imAsHead=true
-whAsHead=false.
8
This was infrequent; their annotations split most multi-
TRAIN TEST-NEW TEST-FOSTER
tweets 717 201 < 250?
unique tweets 569 201 < 250?
tokens 9,310 2,839 2,841
selected tokens 7,015 2,158 2,366
types 3,566 1,461 1,230
utterances 1,473 429 337
multi-root tweets 398 123 60
MWEs 387 78 109
Table 1: Statistics of our datasets. (A tweet with k annotations
in the training set is counted k times for the totals of tokens,
utterances, etc.).
?
TEST-FOSTER contains 250 manually split
sentences. The number of tweets should be smaller but is not
recoverable from the data release.
(d) Recovering the internal structure of the noun
phrases.
(e) Fixing a difference in conventions with re-
spect to subject-auxiliary inversion.
9
We consider two training sets. TRAIN-NEW con-
sists of the remaining 717 tweets from our corpus
(?3) annotated by the rest of the annotators. Some
of these tweets have annotations from multiple an-
notators; 11 annotations for tweets that also oc-
curred in TEST-NEW were excluded. TRAIN-PTB
is the conventional training set from the Penn Tree-
bank (?2?21). The PennConverter tool was used
to extract dependencies, with head rule options se-
lected to approximate our annotation conventions
as closely as possible (see footnote 7). The result-
ing annotations lack the same attention to noun
phrase?internal structure (?2.4) and handle subject-
auxiliary inversions differently than our data. Part-
of-speech tags were coarsened to be compatible
with the Twitter POS tags, using the mappings spec-
ified by Gimpel et al. (2011).
Statistics for the in-domain datasets are given in
Table 1. As we can see in the table, more than half
of the tweets in our corpus have multiple utterances.
The out-of-vocabulary rate for our TRAIN/TEST-
NEW split is 33.7% by token and 62.5% by type;
for TRAIN/TEST-FOSTER it is 41.4% and 64.6%
respectively. These are much higher than the 2.5%
and 13.2% in the standard Penn Treebank split.
All evaluations here are on unlabeled attachment
F
1
scores.
10
Our parser provides labels for coordi-
nation structures and MWEs (?2), but we do not
present detailed evaluations of those due to space
constraints.
utterance tweets into separate sentence-instances.
9
For example, in the sentence Is he driving, we attached
he to driving while PennConverter attaches it to Is.
10
Because of token selection, precision and recall may not
be equal.
1007
5.2 Preprocessing
Because some of the tweets in our test set were
also in the training set of Owoputi et al. (2013),
we retrained their POS tagger on all the annotated
data they have minus the 201 tweets in our test
set. Its tagging accuracy was 92.8% and 88.7% on
TEST-NEW and TEST-FOSTER, respectively. The
token selection model (?2.1) achieves 97.4% on
TEST-NEW with gold or automatic POS tagging;
and on TEST-FOSTER, 99.0% and 99.5% with gold
and automatic POS tagging, respectively.
As noted in ?4.3, Penn Treebank features were
developed using a first-order TurboParser trained
on TRAIN-PTB; Brown clusters were included in
computing these Penn Treebank features if they
were available in the parser to which the features
(i.e. Brown clusters) were added.
5.3 Main Parser
The second-order TurboParser described in ?4,
trained on TRAIN-NEW (default hyperparameter
values), achieves 80.9% unlabeled attachment ac-
curacy on TEST-NEW and 76.1% on TEST-FOSTER.
The experiments consider variations on this main
approach, which is the version released as TWEE-
BOPARSER.
The discrepancy between the two test sets is
easily explained: as noted in ?3.1, the dataset
from which our tweets are drawn was designed
to be representative of English on Twitter. Fos-
ter et al. (2011b) selected tweets from Berming-
ham and Smeaton?s (2010) corpus, which uses fifty
predefined topics like politics, business, sports,
and entertainment?in short, topics not unlike
those found in the Penn Treebank. Relative to
the Penn Treebank training set, the by-type out-
of-vocabulary rates are 45.2% for TEST-NEW and
only 21.6% for TEST-FOSTER (cf. 13.2% for the
Penn Treebank test set).
Another mismatch is in the handling of utter-
ances. In our corpus, utterance segmentation
emerges from multi-rooted annotations (?2.3). Fos-
ter et al. (2011b) manually split each tweet into
utterances and treat those as separate instances in
their corpus, so that our model trained on often
multi-rooted tweets from TRAIN is being tested
only on single-rooted utterances.
5.4 Experiment: Which Training Set?
We consider the direct use of TRAIN-PTB instead
of TRAIN-NEW. Table 2 reports the results on both
Unlabeled Attachment F
1
(%)
mod. POS POS as-is
TEST-NEW
Baseline 73.0 73.5
+ Brown 73.7 73.3
+ Brown & PA 72.9 73.1
TEST-FOSTER
Baseline 76.3 75.2
+ Brown 75.5 76.7
+ Brown & PA 76.9 77.0
Table 2: Performance of second-order TurboParser trained on
TRAIN-PTB, with various preprocessing options. The main
parser (?5.3) achieves 80.9% and 76.1% on the two test sets,
respectively; see ?5.4 for discussion.
test sets, with various options. ?Baseline? is off-
the-shelf second-order TurboParser. We consider
augmenting it with Brown cluster features (?4.3;
?+ Brown?) and then also with the parsing adapta-
tions of ?4.2 (?+ Brown & PA?). Another choice
is whether to modify the POS tags at test time; the
modified version (?mod. POS?) maps at-mentions
to pronoun, and hashtags and URLs to noun.
We note that comparing these scores to our main
parser (?5.3) conflates three very important inde-
pendent variables: the amount of training data
(39,832 Penn Treebank sentences vs. 1,473 Twitter
utterances), the annotation method, and the source
of the data. However, we are encouraged that, on
what we believe is the superior test set (TEST-NEW),
our overall approach obtains a 7.8% gain with an
order of magnitude less annotated data.
5.5 Experiment: Effect of Preprocessing
Table 3 (second block, italicized) shows the per-
formance of the main parser on both test sets with
gold-standard and automatic POS tagging and to-
ken selection. On TEST-NEW, with either gold-
standard POS tags or gold-standard token selection,
performance increases by 1.1%; with both, it in-
creases by 2.3%. On TEST-FOSTER, token selec-
tion matters much less, but POS tagging accounts
for a drop of more than 6%. This is consistent with
Foster et al.?s finding: using a fine-grained Penn
Treebank?trained POS tagger (achieving around
84% accuracy on Twitter), they saw 5?8% improve-
ment in unlabeled dependency attachment accuracy
using gold-standard POS tags.
5.6 Experiment: Ablations
We ablated each key element of our main parser?
PTB features, Brown features, second order fea-
tures and decoding, and the parsing adaptations of
1008
0.76
0.78
0.8
0.82
Unla
beled
 Atta
chme
nt F 1
 
 
Baselin
e ?PA?PTB ?P
A
?Brow
n ?PTB?Brow
n ?PA ?PTB ?Brow
n Main
First?OrderSecond?Order
(a) TEST-NEW
0.7
0.72
0.74
0.76
Unla
beled
 Atta
chme
nt F 1
Baselin
e ?PA?PTB ?P
A
?Brow
n ?PTB?Brow
n ?PA ?PTB ?Brow
n Main
 
 First?OrderSecond?Order
(b) TEST-FOSTER
Figure 3: Feature ablations; these charts present the same scores shown in Table 3 and more variants of the first-order model.
Unlabeled Attachment F
1
(%)
TEST-NEW TEST-FOSTER
Main parser 80.9 76.1
Gold POS and TS 83.2 82.8
Gold POS, automatic TS 82.0 82.3
Automatic POS, gold TS 82.0 76.2
Single ablations:
? PTB 80.2 72.6
? Brown 81.2 75.4
? 2nd order 80.1 75.6
? PA 79.2 73.7
Double ablations:
? PTB, ? Brown 79.5 72.8
? PTB, ? 2nd order 78.5 72.2
? PTB, ? PA 77.4 69.6
? Brown, ? 2nd order 80.7 74.5
? Brown, ? PA 78.2 73.7
? 2nd order, ? PA 77.7 73.5
Baselines:
Second order 76.5 70.4
First order 76.1 70.4
Table 3: Effects of gold-standard POS tagging and token
selection (TS; ?5.5) and of feature ablation (?5.6). The ?base-
lines? are TurboParser without the parsing adaptations in ?4.2
and without Penn Treebank or Brown features. The best result
in each column is bolded. See also Figure 3.
?4.2?as well as each pair of these. These condi-
tions use automatic POS tags and token selection.
The ?? PA? condition, which ablates parsing adap-
tations, is accomplished by deleting punctuation
(in training and test data) and parsing using Turbo-
Parser?s existing algorithm.
Results are shown in Table 3. Further results
with first- and second-order TurboParsers are plot-
ted in Figure 3. Notably, a 2?3% gain is obtained by
modifying the parsing algorithm, and our stacking-
inspired use of Penn Treebank data contributes in
both cases, quite a lot on TEST-FOSTER (unsur-
prisingly given that test set?s similarity to the Penn
Treebank). More surprisingly, we find that Brown
cluster features do not consistently improve perfor-
mance, at least not as instantiated here, with our
small training set.
6 Conclusion
We described TWEEBOPARSER, a dependency
parser for English tweets that achieves over 80%
unlabeled attachment score on a new, high-quality
test set. This is on par with state-of-the-art re-
ported results for news text in Turkish (77.6%;
Koo et al., 2010) and Arabic (81.1%; Martins
et al., 2011). Our contributions include impor-
tant steps taken to build the parser: a considera-
tion of the challenges of parsing tweets that in-
formed our annotation process, the resulting new
TWEEBANK corpus, adaptations to a statistical
parsing algorithm, a new approach to exploiting
data in a better-resourced domain (the Penn Tree-
bank), and experimental analysis of the decisions
we made. The dataset and parser can be found at
http://www.ark.cs.cmu.edu/TweetNLP.
Acknowledgments
The authors thank the anonymous reviewers
and Andr? Martins, Yanchuan Sim, Wang Ling,
Michael Mordowanec, and Alexander Rush for
helpful feedback, as well as the annotators Waleed
Ammar, Jason Baldridge, David Bamman, Dallas
Card, Shay Cohen, Jesse Dodge, Jeffrey Flanigan,
Dan Garrette, Lori Levin, Wang Ling, Bill Mc-
Dowell, Michael Mordowanec, Brendan O?Connor,
Rohan Ramanath, Yanchuan Sim, Liang Sun, Sam
Thomson, and Dani Yogatama. This research was
supported in part by the U. S. Army Research Lab-
oratory and the U. S. Army Research Office under
contract/grant number W911NF-10-1-0533 and by
NSF grants IIS-1054319 and IIS-1352440.
1009
References
Anne Abeill?, Lionel Cl?ment, and Fran?ois Toussenel.
2003. Building a treebank for French. In Treebanks,
pages 165?187. Springer.
Timonthy Baldwin and Su Nam Kim. 2010. Multi-
word expressions. In Handbook of Natural Lan-
guage Processing, Second Edition. CRC Press, Tay-
lor and Francis Group.
Adam Bermingham and Alan F. Smeaton. 2010. Clas-
sifying sentiment in microblogs: Is brevity an advan-
tage? In Proc. of CIKM.
Ann Bies, Justin Mott, Colin Warner, and Seth
Kulick. 2012. English Web Treebank. Techni-
cal Report LDC2012T13, Linguistic Data Consor-
tium. URL http://www.ldc.upenn.edu/Catalog/
catalogEntry.jsp?catalogId=LDC2012T13.
Peter F Brown, Peter V Desouza, Robert L Mercer, Vin-
cent J. Della Pietra, and Jenifer C. Lai. 1992. Class-
based n-gram models of natural language. Computa-
tional Linguistics, 18(4):467?479.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proc. of CoNLL.
Marie Candito and Matthieu Constant. 2014. Strate-
gies for contiguous multiword expression analysis
and dependency parsing. In Proc. of ACL.
Xavier Carreras. 2007. Experiments with a higher-
order projective dependency parser. In Proc. of
EMNLP-CoNLL.
Yoeng-Jin Chu and Tseng-Hong Liu. 1965. On shortest
arborescence of a directed graph. Scientia Sinica,
14(10):1396.
Michael Collins. 2002. Discriminative training meth-
ods for Hidden Markov Models: theory and ex-
periments with perceptron algorithms. In Proc. of
EMNLP.
Matthieu Constant and Anthony Sigogne. 2011. MWU-
aware part-of-speech tagging with a CRF model and
lexical resources. In Proc. of the Workshop on Multi-
word Expressions: from Parsing and Generation to
the Real World.
Matthieu Constant, Anthony Sigogne, and Patrick Wa-
trin. 2012. Discriminative strategies to integrate mul-
tiword expression recognition and parsing. In Proc.
of ACL.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The Stanford typed dependencies repre-
sentation. In Proc. of COLING Workshop on Cross-
Framework and Cross-Domain Parser Evaluation.
Mark Dredze, John Blitzer, Partha Pratim Taluk-
dar, Kuzman Ganchev, Joao Graca, and Fernando
Pereira. 2007. Frustratingly hard domain adapta-
tion for dependency parsing. In Proc. of EMNLP-
CoNLL.
Jack Edmonds. 1967. Optimum branchings. Journal
of Research of the National Bureau of Standards B,
71(233-240):160.
Jacob Eisenstein. 2013. What to do about bad language
on the internet. In Proc. of NAACL-HLT.
Jason Eisner. 1996. Three new probabilistic models
for dependency parsing: An exploration. In Proc. of
COLING.
Jakob Elming, Anders Johannsen, Sigrid Klerke,
Emanuele Lapponi, H?ctor Mart?nez Alonso, and
Anders S?gaard. 2013. Down-stream effects of tree-
to-dependency conversions. In Proc. of NAACL-
HLT.
Jenny Rose Finkel and Christopher D. Manning. 2009.
Joint parsing and named entity recognition. In Proc
of ACL-HLT.
Jennifer Foster, ?zlem ?etinoglu, Joachim Wagner,
Joseph Le Roux, Stephen Hogan, Joakim Nivre,
Deirdre Hogan, and Josef van Genabith. 2011a.
#hardtoparse: POS tagging and parsing the Twitter-
verse. In Proc. of AAAI Workshop on Analyzing Mi-
crotext.
Jennifer Foster, ?zlem ?etino?glu, Joachim Wagner,
Joseph Le Roux, Joakim Nivre, Deirdre Hogan, and
Josef van Genabith. 2011b. From news to comment:
resources and benchmarks for parsing the language
of Web 2.0. In Proc. of IJCNLP.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein,
Michael Heilman, Dani Yogatama, Jeffrey Flanigan,
and Noah A. Smith. 2011. Part-of-speech tagging
for Twitter: annotation, features, and experiments.
In Proc. of ACL-HLT.
Spence Green, Marie-Catherine de Marneffe, and
Christopher D. Manning. 2012. Parsing models for
identifying multiword expressions. Computational
Linguistics, 39(1):195?227.
Stephan Greene and Philip Resnik. 2009. Syntac-
tic packaging and implicit sentiment. In Proc. of
NAACL.
1010
Jan Haji?c, Eva Haji?cov?, Jarmila Panevov?, Petr Sgall,
Silvie Cinkov?, Eva Fu?c?kov?, Marie Mikulov?,
Petr Pajas, Jan Popelka, Ji?r? Semeck`y, Jana
?indlerov?, Jan ?t?ep?nek, Josef Toman, Zde?nka
Ure?ov?, and Zden?ek ?abokrtsk?. 2012. Prague
Czech-English Dependency Treebank 2.0. Techni-
cal Report LDC2012T08, Linguistic Data Consor-
tium. URL http://www.ldc.upenn.edu/Catalog/
catalogEntry.jsp?catalogId=LDC2012T08.
Rebecca Hwa. 2001. Learning Probabilistic Lexical-
ized Grammars for Natural Language Processing.
Ph.D. thesis, Harvard University.
Richard Johansson. 2013. Training parsers on incom-
patible treebanks. In Proc. of NAACL-HLT.
Mark Johnson. 1998. PCFG models of linguistic
tree representations. Computational Linguistics,
24(4):613?632.
Lingpeng Kong and Noah A. Smith. 2014. An empiri-
cal comparison of parsing methods for Stanford de-
pendencies. ArXiv:1404.4314.
Terry Koo, Xavier Carreras, and Michael Collins. 2008.
Simple semi-supervised dependency parsing. In
Proc. of ACL.
Terry Koo and Michael Collins. 2010. Efficient third-
order dependency parsers. In Proc. of ACL.
Terry Koo, Alexander M. Rush, Michael Collins,
Tommi Jaakkola, and David Sontag. 2010. Dual de-
composition for parsing with non-projective head au-
tomata. In Proc. of EMNLP.
Efthymios Kouloumpis, Theresa Wilson, and Johanna
Moore. 2011. Twitter sentiment analysis: The good
the bad and the OMG! In Proc. of ICWSM.
Joseph Le Roux, Matthieu Constant, and Antoine
Rozenknop. 2014. Syntactic parsing and compound
recognition via dual decomposition: application to
French. In Proc. of COLING.
Ji Ma, Yue Zhang, and Jingbo Zhu. 2014. Punctua-
tion processing for projective dependency parsing.
In Proc. of ACL.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large annotated
corpus of English: The Penn Treebank. Computa-
tional linguistics, 19(2):313?330.
Andr? F.T. Martins, Miguel Almeida, and Noah A.
Smith. 2013. Turning on the turbo: Fast third-order
non-projective turbo parsers. In Proc. of ACL.
Andr? F.T. Martins, Dipanjan Das, Noah A. Smith, and
Eric P. Xing. 2008. Stacking dependency parsers. In
Proc. of EMNLP.
Andr? F.T. Martins, Noah A. Smith, Pedro M.Q.
Aguiar, and M?rio A.T. Figueiredo. 2011. Dual de-
composition with many overlapping components. In
Proc. of EMNLP.
Andr? F.T. Martins, Noah A. Smith, and Eric P. Xing.
2009. Concise integer linear programming formu-
lations for dependency parsing. In Proc. of ACL-
IJCNLP.
Andr? F.T. Martins, Noah A. Smith, Eric P. Xing, Pe-
dro M.Q. Aguiar, and M?rio A.T. Figueiredo. 2010.
Turbo parsers: Dependency parsing by approximate
variational inference. In Proc. of EMNLP.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005a. Online large-margin training of de-
pendency parsers. In Proc. of ACL.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Haji
?
c. 2005b. Non-projective dependency pars-
ing using spanning tree algorithms. In Proc. of HLT-
EMNLP.
Ryan McDonald and Giorgio Satta. 2007. On the com-
plexity of non-projective data-driven dependency
parsing. In Proc. of IWPT.
Michael T. Mordowanec, Nathan Schneider, Chris.
Dyer, and Noah A. Smith. 2014. Simplified depen-
dency annotations with GFL-Web. In Proc. of ACL,
demonstration track.
Joakim Nivre and Ryan McDonald. 2008. Integrat-
ing graph-based and transition-based dependency
parsers. In Proc. of ACL-HLT.
Olutobi Owoputi, Brendan O?Connor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A.
Smith. 2013. Improved part-of-speech tagging for
online conversational text with word clusters. In
Proc. of NAACL-HLT.
Fernando Pereira and Yves Schabes. 1992. Inside-
outside reestimation from partially bracketed cor-
pora. In Proc. of ACL.
Slav Petrov and Ryan McDonald. 2012. Overview of
the 2012 Shared Task on Parsing the Web. In Notes
of the First Workshop on Syntactic Analysis of Non-
Canonical Language.
Jeffrey C. Reynar and Adwait Ratnaparkhi. 1997. A
maximum entropy approach to identifying sentence
boundaries. In Proc. of ANLP.
1011
Sebastian Riedel and James Clarke. 2006. Incremen-
tal integer linear programming for non-projective de-
pendency parsing. In Proc. of EMNLP.
Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.
2011. Named entity recognition in tweets: an ex-
perimental study. In Proc. of EMNLP.
Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann
Copestake, and Dan Flickinger. 2002. Multiword
expressions: A pain in the neck for NLP. In Proc. of
CICLing.
Nathan Schneider, Emily Danchik, Chris Dyer, and
Noah A. Smith. 2014. Discriminative lexical se-
mantic segmentation with gaps: Running the MWE
gamut. Transactions of the Association for Compu-
tational Linguistics, 2:193?206.
Nathan Schneider, Brendan O?Connor, Naomi Saphra,
David Bamman, Manaal Faruqui, Noah A. Smith,
Chris Dyer, and Jason Baldridge. 2013. A frame-
work for (under)specifying dependency syntax with-
out overloading annotators. In Proc. of the 7th Lin-
guistic Annotation Workshop and Interoperability
with Discourse.
David A. Smith and Jason Eisner. 2008. Dependency
parsing by belief propagation. In Proc. of EMNLP.
Sandeep Soni, Tanushree Mitra, Eric Gilbert, and Jacob
Eisenstein. 2014. Modeling factuality judgments in
social media text. In Proc. of ACL.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Proc. of ACL.
David Vadas and James Curran. 2007. Adding noun
phrase structure to the Penn Treebank. In Proc. of
ACL.
Eva Maria Vecchi, Roberto Zamparelli, and Marco Ba-
roni. 2013. Studying the recursive behaviour of
adjectival modification with compositional distribu-
tional semantics. In Proc. of EMNLP.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statis-
tical dependency analysis with support vector ma-
chines. In Proc. of IWPT.
1012
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 162?173,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Recall-Oriented Learning of Named Entities in Arabic Wikipedia
Behrang Mohit? Nathan Schneider? Rishav Bhowmick? Kemal Oflazer? Noah A. Smith?
School of Computer Science, Carnegie Mellon University
?P.O. Box 24866, Doha, Qatar ?Pittsburgh, PA 15213, USA
{behrang@,nschneid@cs.,rishavb@qatar.,ko@cs.,nasmith@cs.}cmu.edu
Abstract
We consider the problem of NER in Arabic
Wikipedia, a semisupervised domain adap-
tation setting for which we have no labeled
training data in the target domain. To fa-
cilitate evaluation, we obtain annotations
for articles in four topical groups, allow-
ing annotators to identify domain-specific
entity types in addition to standard cate-
gories. Standard supervised learning on
newswire text leads to poor target-domain
recall. We train a sequence model and show
that a simple modification to the online
learner?a loss function encouraging it to
?arrogantly? favor recall over precision?
substantially improves recall and F1. We
then adapt our model with self-training
on unlabeled target-domain data; enforc-
ing the same recall-oriented bias in the self-
training stage yields marginal gains.1
1 Introduction
This paper considers named entity recognition
(NER) in text that is different from most past re-
search on NER. Specifically, we consider Arabic
Wikipedia articles with diverse topics beyond the
commonly-used news domain. These data chal-
lenge past approaches in two ways:
First, Arabic is a morphologically rich lan-
guage (Habash, 2010). Named entities are ref-
erenced using complex syntactic constructions
(cf. English NEs, which are primarily sequences
of proper nouns). The Arabic script suppresses
most vowels, increasing lexical ambiguity, and
lacks capitalization, a key clue for English NER.
Second, much research has focused on the use
of news text for system building and evaluation.
Wikipedia articles are not news, belonging instead
to a wide range of domains that are not clearly
1The annotated dataset and a supplementary document
with additional details of this work can be found at:
http://www.ark.cs.cmu.edu/AQMAR
delineated. One hallmark of this divergence be-
tween Wikipedia and the news domain is a dif-
ference in the distributions of named entities. In-
deed, the classic named entity types (person, or-
ganization, location) may not be the most apt for
articles in other domains (e.g., scientific or social
topics). On the other hand, Wikipedia is a large
dataset, inviting semisupervised approaches.
In this paper, we describe advances on the prob-
lem of NER in Arabic Wikipedia. The techniques
are general and make use of well-understood
building blocks. Our contributions are:
? A small corpus of articles annotated in a new
scheme that provides more freedom for annota-
tors to adapt NE analysis to new domains;
? An ?arrogant? learning approach designed to
boost recall in supervised training as well as
self-training; and
? An empirical evaluation of this technique as ap-
plied to a well-established discriminative NER
model and feature set.
Experiments show consistent gains on the chal-
lenging problem of identifying named entities in
Arabic Wikipedia text.
2 Arabic Wikipedia NE Annotation
Most of the effort in NER has been fo-
cused around a small set of domains and
general-purpose entity classes relevant to those
domains?especially the categories PER(SON),
ORG(ANIZATION), and LOC(ATION) (POL),
which are highly prominent in news text. Ara-
bic is no exception: the publicly available NER
corpora?ACE (Walker et al 2006), ANER (Be-
najiba et al 2008), and OntoNotes (Hovy et al
2006)?all are in the news domain.2 However,
2OntoNotes contains news-related text. ACE includes
some text from blogs. In addition to the POL classes, both
corpora include additional NE classes such as facility, event,
product, vehicle, etc. These entities are infrequent and may
not be comprehensive enough to cover the larger set of pos-
162
History Science Sports Technology
dev: Damascus Atom Rau?l Gonza?les Linux
Imam Hussein Shrine Nuclear power Real Madrid Solaris
test: Crusades Enrico Fermi 2004 Summer Olympics Computer
Islamic Golden Age Light Christiano Ronaldo Computer Software
Islamic History Periodic Table Football Internet
Ibn Tolun Mosque Physics Portugal football team Richard Stallman
Ummaya Mosque Muhammad al-Razi FIFA World Cup X Window System
Claudio Filippone (PER) 	??J. ?J

	
? ?K
X???; Linux (SOFTWARE) ??
	
JJ
?; Spanish
League (CHAMPIONSHIPS) ?


	
GAJ.?B@ ?

P?Y?@; proton (PARTICLE) 	??K?QK. ; nuclear
radiation (GENERIC-MISC) ?


??
	
J? @ ?A? ?B@; Real Zaragoza (ORG)

???

Q??? ?AK
P
Table 1: Translated titles
of Arabic Wikipedia arti-
cles in our development
and test sets, and some
NEs with standard and
article-specific classes.
Additionally, Prussia and
Amman were reserved
for training annotators,
and Gulf War for esti-
mating inter-annotator
agreement.
appropriate entity classes will vary widely by do-
main; occurrence rates for entity classes are quite
different in news text vs. Wikipedia, for instance
(Balasuriya et al 2009). This is abundantly
clear in technical and scientific discourse, where
much of the terminology is domain-specific, but it
holds elsewhere. Non-POL entities in the history
domain, for instance, include important events
(wars, famines) and cultural movements (roman-
ticism). Ignoring such domain-critical entities
likely limits the usefulness of the NE analysis.
Recognizing this limitation, some work on
NER has sought to codify more robust invento-
ries of general-purpose entity types (Sekine et al
2002; Weischedel and Brunstein, 2005; Grouin
et al 2011) or to enumerate domain-specific
types (Settles, 2004; Yao et al 2003). Coarse,
general-purpose categories have also been used
for semantic tagging of nouns and verbs (Cia-
ramita and Johnson, 2003). Yet as the number
of classes or domains grows, rigorously docu-
menting and organizing the classes?even for a
single language?requires intensive effort. Ide-
ally, an NER system would refine the traditional
classes (Hovy et al 2011) or identify new entity
classes when they arise in new domains, adapting
to new data. For this reason, we believe it is valu-
able to consider NER systems that identify (but
do not necessarily label) entity mentions, and also
to consider annotation schemes that allow annota-
tors more freedom in defining entity classes.
Our aim in creating an annotated dataset is to
provide a testbed for evaluation of new NER mod-
els. We will use these data as development and
sible NEs (Sekine et al 2002). Nezda et al(2006) anno-
tated and evaluated an Arabic NE corpus with an extended
set of 18 classes (including temporal and numeric entities);
this corpus has not been released publicly.
testing examples, but not as training data. In ?4
we will discuss our semisupervised approach to
learning, which leverages ACE and ANER data
as an annotated training corpus.
2.1 Annotation Strategy
We conducted a small annotation project on Ara-
bic Wikipedia articles. Two college-educated na-
tive Arabic speakers annotated about 3,000 sen-
tences from 31 articles. We identified four top-
ical areas of interest?history, technology, sci-
ence, and sports?and browsed these topics un-
til we had found 31 articles that we deemed sat-
isfactory on the basis of length (at least 1,000
words), cross-lingual linkages (associated articles
in English, German, and Chinese3), and subjec-
tive judgments of quality. The list of these arti-
cles along with sample NEs are presented in ta-
ble 1. These articles were then preprocessed to
extract main article text (eliminating tables, lists,
info-boxes, captions, etc.) for annotation.
Our approach follows ACE guidelines (LDC,
2005) in identifying NE boundaries and choos-
ing POL tags. In addition to this traditional form
of annotation, annotators were encouraged to ar-
ticulate one to three salient, article-specific en-
tity categories per article. For example, names
of particles (e.g., proton) are highly salient in the
Atom article. Annotators were asked to read the
entire article first, and then to decide which non-
traditional classes of entities would be important
in the context of article. In some cases, annotators
reported using heuristics (such as being proper
3These three languages have the most articles on
Wikipedia. Associated articles here are those that have been
manually hyperlinked from the Arabic page as cross-lingual
correspondences. They are not translations, but if the associ-
ations are accurate, these articles should be topically similar
to the Arabic page that links to them.
163
Token position agreement rate 92.6% Cohen?s ?: 0.86
Token agreement rate 88.3% Cohen?s ?: 0.86
Token F1 between annotators 91.0%
Entity boundary match F1 94.0%
Entity category match F1 87.4%
Table 2: Inter-annotator agreement measurements.
nouns or having an English translation which is
conventionally capitalized) to help guide their de-
termination of non-canonical entities and entity
classes. Annotators produced written descriptions
of their classes, including example instances.
This scheme was chosen for its flexibility: in
contrast to a scenario with a fixed ontology, anno-
tators required minimal training beyond the POL
conventions, and did not have to worry about
delineating custom categories precisely enough
that they would extend straightforwardly to other
topics or domains. Of course, we expect inter-
annotator variability to be greater for these open-
ended classification criteria.
2.2 Annotation Quality Evaluation
During annotation, two articles (Prussia and Am-
man) were reserved for training annotators on
the task. Once they were accustomed to anno-
tation, both independently annotated a third ar-
ticle. We used this 4,750-word article (Gulf War,

?J

	
K A

J? @ i. J
?
	
m?'@ H. Qk) to measure inter-annotator
agreement. Table 2 provides scores for token-
level agreement measures and entity-level F1 be-
tween the two annotated versions of the article.4
These measures indicate strong agreement for
locating and categorizing NEs both at the token
and chunk levels. Closer examination of agree-
ment scores shows that PER and MIS classes have
the lowest rates of agreement. That the mis-
cellaneous class, used for infrequent or article-
specific NEs, receives poor agreement is unsur-
prising. The low agreement on the PER class
seems to be due to the use of titles and descriptive
terms in personal names. Despite explicit guide-
lines to exclude the titles, annotators disagreed on
the inclusion of descriptors that disambiguate the
NE (e.g., the father in H.

B@ ??K. h. Qk. : George
Bush, the father).
4The position and boundary measures ignore the distinc-
tions between the POLM classes. To avoid artificial inflation
of the token and token position agreement rates, we exclude
the 81% of tokens tagged by both annotators as not belong-
ing to an entity.
History: Gulf War, Prussia, Damascus, Crusades
WAR CONFLICT ? ? ?
Science: Atom, Periodic table
THEORY ? CHEMICAL ? ?
NAME ROMAN ? PARTICLE ? ?
Sports: Football, Rau?l Gonza?les
SPORT ? CHAMPIONSHIP ?
AWARD ? NAME ROMAN ?
Technology: Computer, Richard Stallman
COMPUTER VARIETY ? SOFTWARE ?
COMPONENT ?
Table 3: Custom NE categories suggested by one or
both annotators for 10 articles. Article titles are trans-
lated from Arabic. ? indicates that both annotators vol-
unteered a category for an article; ? indicates that only
one annotator suggested the category. Annotators were
not given a predetermined set of possible categories;
rather, category matches between annotators were de-
termined by post hoc analysis. NAME ROMAN indi-
cates an NE rendered in Roman characters.
2.3 Validating Category Intuitions
To investigate the variability between annotators
with respect to custom category intuitions, we
asked our two annotators to independently read
10 of the articles in the data (scattered across our
four focus domains) and suggest up to 3 custom
categories for each. We assigned short names to
these suggestions, seen in table 3. In 13 cases,
both annotators suggested a category for an article
that was essentially the same (?); three such cat-
egories spanned multiple articles. In three cases
a category was suggested by only one annotator
(?).5 Thus, we see that our annotators were gen-
erally, but not entirely, consistent with each other
in their creation of custom categories. Further, al-
most all of our article-specific categories corre-
spond to classes in the extended NE taxonomy of
(Sekine et al 2002), which speaks to the reason-
ableness of both sets of categories?and by exten-
sion, our open-ended annotation process.
Our annotation of named entities outside of the
traditional POL classes creates a useful resource
for entity detection and recognition in new do-
mains. Even the ability to detect non-canonical
types of NEs should help applications such as QA
and MT (Toral et al 2005; Babych and Hart-
ley, 2003). Possible avenues for future work
include annotating and projecting non-canonical
5When it came to tagging NEs, one of the two annota-
tors was assigned to each article. Custom categories only
suggested by the other annotator were ignored.
164
NEs from English articles to their Arabic coun-
terparts (Hassan et al 2007), automatically clus-
tering non-canonical types of entities into article-
specific or cross-article classes (cf. Frietag, 2004),
or using non-canonical classes to improve the
(author-specified) article categories in Wikipedia.
Hereafter, we merge all article-specific cate-
gories with the generic MIS category. The pro-
portion of entity mentions that are tagged as MIS,
while varying to a large extent by document, is
a major indication of the gulf between the news
data (<10%) and the Wikipedia data (53% for the
development set, 37% for the test set).
Below, we aim to develop entity detection mod-
els that generalize beyond the traditional POL en-
tities. We do not address here the challenges of
automatically classifying entities or inferring non-
canonical groupings.
3 Data
Table 4 summarizes the various corpora used in
this work.6 Our NE-annotated Wikipedia sub-
corpus, described above, consists of several Ara-
bic Wikipedia articles from four focus domains.7
We do not use these for supervised training data;
they serve only as development and test data. A
larger set of Arabic Wikipedia articles, selected
on the basis of quality heuristics, serves as unla-
beled data for semisupervised learning.
Our out-of-domain labeled NE data is drawn
from the ANER (Benajiba et al 2007) and
ACE-2005 (Walker et al 2006) newswire cor-
pora. Entity types in this data are POL cate-
gories (PER, ORG, LOC) and MIS. Portions of the
ACE corpus were held out as development and
test data; the remainder is used in training.
4 Models
Our starting point for statistical NER is a feature-
based linear model over sequences, trained using
the structured perceptron (Collins, 2002).8
In addition to lexical and morphological9 fea-
6Additional details appear in the supplement.
7We downloaded a snapshot of Arabic Wikipedia
(http://ar.wikipedia.org) on 8/29/2009 and pre-
processed the articles to extract main body text and metadata
using the mwlib package for Python (PediaPress, 2010).
8A more leisurely discussion of the structured percep-
tron and its connection to empirical risk minimization can
be found in the supplementary document.
9We obtain morphological analyses from the MADA tool
(Habash and Rambow, 2005; Roth et al 2008).
Training words NEs
ACE+ANER 212,839 15,796
Wikipedia (unlabeled, 397 docs) 1,110,546 ?
Development
ACE 7,776 638
Wikipedia (4 domains, 8 docs) 21,203 2,073
Test
ACE 7,789 621
Wikipedia (4 domains, 20 docs) 52,650 3,781
Table 4: Number of words (entity mentions) in data sets.
tures known to work well for Arabic NER (Be-
najiba et al 2008; Abdul-Hamid and Darwish,
2010), we incorporate some additional features
enabled by Wikipedia. We do not employ a
gazetteer, as the construction of a broad-domain
gazetteer is a significant undertaking orthogo-
nal to the challenges of a new text domain like
Wikipedia.10 A descriptive list of our features is
available in the supplementary document.
We use a first-order structured perceptron; none
of our features consider more than a pair of con-
secutive BIO labels at a time. The model enforces
the constraint that NE sequences must begin with
B (so the bigram ?O, I? is disallowed).
Training this model on ACE and ANER data
achieves performance comparable to the state of
the art (F1-measure11 above 69%), but fares much
worse on our Wikipedia test set (F1-measure
around 47%); details are given in ?5.
4.1 Recall-Oriented Perceptron
By augmenting the perceptron?s online update
with a cost function term, we can incorporate a
task-dependent notion of error into the objective,
as with structured SVMs (Taskar et al 2004;
Tsochantaridis et al 2005). Let c(y,y?) denote
a measure of error when y is the correct label se-
quence but y? is predicted. For observed sequence
x and feature weights (model parameters) w, the
structured hinge loss is `hinge(x,y,w) =
max
y?
(
w>g(x,y?) + c(y,y?)
)
?w>g(x,y)
(1)
The maximization problem inside the parentheses
is known as cost-augmented decoding. If c fac-
10A gazetteer ought to yield further improvements in line
with previous findings in NER (Ratinov and Roth, 2009).
11Though optimizing NER systems for F1 has been called
into question (Manning, 2006), no alternative metric has
achieved widespread acceptance in the community.
165
tors similarly to the feature function g(x,y), then
we can increase penalties for y that have more
local mistakes. This raises the learner?s aware-
ness about how it will be evaluated. Incorporat-
ing cost-augmented decoding into the perceptron
leads to this decoding step:
y? ? arg max
y?
(
w>g(x,y?) + c(y,y?)
)
, (2)
which amounts to performing stochastic subgradi-
ent ascent on an objective function with the Eq. 1
loss (Ratliff et al 2006).
In this framework, cost functions can be for-
mulated to distinguish between different types of
errors made during training. For a tag sequence
y = ?y1, y2, . . . , yM ?, Gimpel and Smith (2010b)
define word-local cost functions that differently
penalize precision errors (i.e., yi = O ? y?i 6= O
for the ith word), recall errors (yi 6= O? y?i = O),
and entity class/position errors (other cases where
yi 6= y?i). As will be shown below, a key problem
in cross-domain NER is poor recall, so we will
penalize recall errors more severely:
c(y,y?) =
M?
i=1
?
?
?
0 if yi = y?i
? if yi 6= O ? y?i = O
1 otherwise
(3)
for a penalty parameter ? > 1. We call our learner
the ?recall-oriented? perceptron (ROP).
We note that Minkov et al(2006) similarly ex-
plored the recall vs. precision tradeoff in NER.
Their technique was to directly tune the weight
of a single feature?the feature marking O (non-
entity tokens); a lower weight for this feature will
incur a greater penalty for predicting O. Below
we demonstrate that our method, which is less
coarse, is more successful in our setting.12
In our experiments we will show that injecting
?arrogance? into the learner via the recall-oriented
loss function substantially improves recall, espe-
cially for non-POL entities (?5.3).
4.2 Self-Training and Semisupervised
Learning
As we will show experimentally, the differences
between news text and Wikipedia text call for do-
main adaptation. In the case of Arabic Wikipedia,
12The distinction between the techniques is that our cost
function adjusts the whole model in order to perform better
at recall on the training data.
Input: labeled data ??x(n),y(n)??Nn=1; unlabeled
data ?x?(j)?Jj=1; supervised learner L;
number of iterations T ?
Output: w
w? L(??x(n),y(n)??Nn=1)
for t = 1 to T ? do
for j = 1 to J do
y?(j) ? arg maxy w
>g(x?(j),y)
w? L(??x(n),y(n)??Nn=1 ? ??x?
(j), y?(j)??Jj=1)
Algorithm 1: Self-training.
there is no available labeled training data. Yet
the available unlabeled data is vast, so we turn to
semisupervised learning.
Here we adapt self-training, a simple tech-
nique that leverages a supervised learner (like the
perceptron) to perform semisupervised learning
(Clark et al 2003; Mihalcea, 2004; McClosky
et al 2006). In our version, a model is trained
on the labeled data, then used to label the un-
labeled target data. We iterate between training
on the hypothetically-labeled target data plus the
original labeled set, and relabeling the target data;
see Algorithm 1. Before self-training, we remove
sentences hypothesized not to contain any named
entity mentions, which we found avoids further
encouragement of the model toward low recall.
5 Experiments
We investigate two questions in the context of
NER for Arabic Wikipedia:
? Loss function: Does integrating a cost func-
tion into our learning algorithm, as we have
done in the recall-oriented perceptron (?4.1),
improve recall and overall performance on
Wikipedia data?
? Semisupervised learning for domain adap-
tation: Can our models benefit from large
amounts of unlabeled Wikipedia data, in addi-
tion to the (out-of-domain) labeled data? We
experiment with a self-training phase following
the fully supervised learning phase.
We report experiments for the possible combi-
nations of the above ideas. These are summarized
in table 5. Note that the recall-oriented percep-
tron can be used for the supervised learning phase,
for the self-training phase, or both. This leaves us
with the following combinations:
? reg/none (baseline): regular supervised learner.
? ROP/none: recall-oriented supervised learner.
166
Figure 1: Tuning the recall-oriented cost parame-
ter for different learning settings. We optimized
for development set F1, choosing penalty ? = 200
for recall-oriented supervised learning (in the plot,
ROP/*?this is regardless of whether a stage of
self-training will follow); ? = 100 for recall-
oriented self-training following recall-oriented su-
pervised learning (ROP/ROP); and ? = 3200 for
recall-oriented self-training following regular super-
vised learning (reg/ROP).
? reg/reg: standard self-training setup.
? ROP/reg: recall-oriented supervised learner, fol-
lowed by standard self-training.
? reg/ROP: regular supervised model as the initial la-
beler for recall-oriented self-training.
? ROP/ROP (the ?double ROP? condition): recall-
oriented supervised model as the initial labeler for
recall-oriented self-training. Note that the two
ROPs can use different cost parameters.
For evaluating our models we consider the
named entity detection task, i.e., recognizing
which spans of words constitute entities. This
is measured by per-entity precision, recall, and
F1.13 To measure statistical significance of differ-
ences between models we use Gimpel and Smith?s
(2010) implementation of the paired bootstrap re-
sampler of (Koehn, 2004), taking 10,000 samples
for each comparison.
5.1 Baseline
Our baseline is the perceptron, trained on the
POL entity boundaries in the ACE+ANER cor-
pus (reg/none).14 Development data was used to
select the number of iterations (10). We per-
formed 3-fold cross-validation on the ACE data
and found wide variance in the in-domain entity
detection performance of this model:
P R F1
fold 1 70.43 63.08 66.55
fold 2 87.48 81.13 84.18
fold 3 65.09 51.13 57.27
average 74.33 65.11 69.33
(Fold 1 corresponds to the ACE test set described
in table 4.) We also trained the model to perform
POL detection and classification, achieving nearly
identical results in the 3-way cross-validation of
ACE data. From these data we conclude that our
13Only entity spans that exactly match the gold spans are
counted as correct. We calculated these scores with the
conlleval.pl script from the CoNLL 2003 shared task.
14In keeping with prior work, we ignore non-POL cate-
gories for the ACE evaluation.
baseline is on par with the state of the art for Ara-
bic NER on ACE news text (Abdul-Hamid and
Darwish, 2010).15
Here is the performance of the baseline entity
detection model on our 20-article test set:16
P R F1
technology 60.42 20.26 30.35
science 64.96 25.73 36.86
history 63.09 35.58 45.50
sports 71.66 59.94 65.28
overall 66.30 35.91 46.59
Unsurprisingly, performance on Wikipedia data
varies widely across article domains and is much
lower than in-domain performance. Precision
scores fall between 60% and 72% for all domains,
but recall in most cases is far worse. Miscella-
neous class recall, in particular, suffers badly (un-
der 10%)?which partially accounts for the poor
recall in science and technology articles (they
have by far the highest proportion of MIS entities).
5.2 Self-Training
Following Clark et al(2003), we applied self-
training as described in Algorithm 1, with the
perceptron as the supervised learner. Our unla-
beled data consists of 397 Arabic Wikipedia ar-
ticles (1 million words) selected at random from
all articles exceeding a simple length threshold
(1,000 words); see table 4. We used only one iter-
ation (T ? = 1), as experiments on development
data showed no benefit from additional rounds.
Several rounds of self-training hurt performance,
15Abdul-Hamid and Darwish report as their best result a
macroaveraged F1-score of 76. As they do not specify which
data they used for their held-out test set, we cannot perform
a direct comparison. However, our feature set is nearly a
superset of their best feature set, and their result lies well
within the range of results seen in our cross-validation folds.
16Our Wikipedia evaluations use models trained on
POLM entity boundaries in ACE. Per-domain and overall
scores are microaverages across articles.
167
SELF-TRAINING
SUPERVISED none reg ROP
reg 66.3 35.9 46.59 66.7 35.6 46.41 59.2 40.3 47.97
ROP 60.9 44.7 51.59 59.8 46.2 52.11 58.0 47.4 52.16
Table 5: Entity detection precision, recall, and F1 for each learning setting, microaveraged across the 24 articles
in our Wikipedia test set. Rows differ in the supervised learning condition on the ACE+ANER data (regular
vs. recall-oriented perceptron). Columns indicate whether this supervised learning phase was followed by self-
training on unlabeled Wikipedia data, and if so which version of the perceptron was used for self-training.
baseline
entities words recall
PER 1081 1743 49.95
ORG 286 637 23.92
LOC 1019 1413 61.43
MIS 1395 2176 9.30
overall 3781 5969 35.91
Figure 2: Recall improve-
ment over baseline in the test
set by gold NER category,
counts for those categories in
the data, and recall scores for
our baseline model. Markers
in the plot indicate different
experimental settings corre-
sponding to cells in table 5.
an effect attested in earlier research (Curran et al
2007) and sometimes known as ?semantic drift.?
Results are shown in table 5. We find that stan-
dard self-training (the middle column) has very
little impact on performance.17 Why is this the
case? We venture that poor baseline recall and the
domain variability within Wikipedia are to blame.
5.3 Recall-Oriented Learning
The recall-oriented bias can be introduced in ei-
ther or both of the stages of our semisupervised
learning framework: in the supervised learn-
ing phase, modifying the objective of our base-
line (?5.1); and within the self-training algorithm
(?5.2).18 As noted in ?4.1, the aim of this ap-
proach is to discourage recall errors (false nega-
tives), which are the chief difficulty for the news
text?trained model in the new domain. We se-
lected the value of the false positive penalty for
cost-augmented decoding, ?, using the develop-
ment data (figure 1).
The results in table 5 demonstrate improve-
ments due to the recall-oriented bias in both
stages of learning.19 When used in the super-
17In neither case does regular self-training produce a sig-
nificantly different F1 score than no self-training.
18Standard Viterbi decoding was used to label the data
within the self-training algorithm; note that cost-augmented
decoding only makes sense in learning, not as a prediction
technique, since it deliberately introduces errors relative to a
correct output that must be provided.
19In terms of F1, the worst of the 3 models with the ROP
supervised learner significantly outperforms the best model
with the regular supervised learner (p < 0.005). The im-
vised phase (bottom left cell), the recall gains
are substantial?nearly 9% over the baseline. In-
tegrating this bias within self-training (last col-
umn of the table) produces a more modest im-
provement (less than 3%) relative to the base-
line. In both cases, the improvements to recall
more than compensate for the amount of degra-
dation to precision. This trend is robust: wher-
ever the recall-oriented perceptron is added, we
observe improvements in both recall and F1. Per-
haps surprisingly, these gains are somewhat addi-
tive: using the ROP in both learning phases gives
a small (though not always significant) gain over
alternatives (standard supervised perceptron, no
self-training, or self-training with a standard per-
ceptron). In fact, when the standard supervised
learner is used, recall-oriented self-training suc-
ceeds despite the ineffectiveness of standard self-
training.
Performance breakdowns by (gold) class, fig-
ure 2, and domain, figure 3, further attest to the
robustness of the overall results. The most dra-
matic gains are in miscellaneous class recall?
each form of the recall bias produces an improve-
ment, and using this bias in both the supervised
and self-training phases is clearly most success-
ful for miscellaneous entities. Correspondingly,
the technology and science domains (in which this
class dominates?83% and 61% of mentions, ver-
provements due to self-training are marginal, however: ROP
self-training produces a significant gain only following reg-
ular supervised learning (p < 0.05).
168
Figure 3: Supervised
learner precision vs.
recall as evaluated
on Wikipedia test
data in different
topical domains. The
regular perceptron
(baseline model) is
contrasted with ROP.
No self-training is
applied.
sus 6% and 12% for history and sports, respec-
tively) receive the biggest boost. Still, the gaps
between domains are not entirely removed.
Most improvements relate to the reduction of
false negatives, which fall into three groups:
(a) entities occurring infrequently or partially
in the labeled training data (e.g. uranium);
(b) domain-specific entities sharing lexical or con-
textual features with the POL entities (e.g. Linux,
titanium); and (c) words with Latin characters,
common in the science and technology domains.
(a) and (b) are mostly transliterations into Arabic.
An alternative?and simpler?approach to
controlling the precision-recall tradeoff is the
Minkov et al(2006) strategy of tuning a single
feature weight subsequent to learning (see ?4.1
above). We performed an oracle experiment to
determine how this compares to recall-oriented
learning in our setting. An oracle trained with
the method of Minkov et aloutperforms the three
models in table 5 that use the regular perceptron
for the supervised phase of learning, but under-
performs the supervised ROP conditions.20
Overall, we find that incorporating the recall-
oriented bias in learning is fruitful for adapting to
Wikipedia because the gains in recall outpace the
damage to precision.
6 Discussion
To our knowledge, this work is the first sugges-
tion that substantively modifying the supervised
learning criterion in a resource-rich domain can
reap benefits in subsequent semisupervised appli-
cation in a new domain. Past work has looked
20Tuning the O feature weight to optimize for F1 on our
test set, we found that oracle precision would be 66.2, recall
would be 39.0, and F1 would be 49.1. The F1 score of our
best model is nearly 3 points higher than the Minkov et al
style oracle, and over 4 points higher than the non-oracle
version where the development set is used for tuning.
at regularization (Chelba and Acero, 2006) and
feature design (Daume? III, 2007); we alter the
loss function. Not surprisingly, the double-ROP
approach harms performance on the original do-
main (on ACE data, we achieve 55.41% F1, far
below the standard perceptron). Yet we observe
that models can be prepared for adaptation even
before a learner is exposed a new domain, sacri-
ficing performance in the original domain.
The recall-oriented bias is not merely encour-
aging the learner to identify entities already seen
in training. As recall increases, so does the num-
ber of new entity types recovered by the model:
of the 2,070 NE types in the test data that were
never seen in training, only 450 were ever found
by the baseline, versus 588 in the reg/ROP condi-
tion, 632 in the ROP/none condition, and 717 in
the double-ROP condition.
We note finally that our method is a simple
extension to the standard structured perceptron;
cost-augmented inference is often no more ex-
pensive than traditional inference, and the algo-
rithmic change is equivalent to adding one addi-
tional feature. Our recall-oriented cost function
is parameterized by a single value, ?; recall is
highly sensitive to the choice of this value (fig-
ure 1 shows how we tuned it on development
data), and thus we anticipate that, in general, such
tuning will be essential to leveraging the benefits
of arrogance.
7 Related Work
Our approach draws on insights from work in
the areas of NER, domain adaptation, NLP with
Wikipedia, and semisupervised learning. As all
are broad areas of research, we highlight only the
most relevant contributions here.
Research in Arabic NER has been focused on
compiling and optimizing the gazetteers and fea-
169
ture sets for standard sequential modeling algo-
rithms (Benajiba et al 2008; Farber et al 2008;
Shaalan and Raza, 2008; Abdul-Hamid and Dar-
wish, 2010). We make use of features identi-
fied in this prior work to construct a strong base-
line system. We are unaware of any Arabic NER
work that has addressed diverse text domains like
Wikipedia. Both the English and Arabic ver-
sions of Wikipedia have been used, however, as
resources in service of traditional NER (Kazama
and Torisawa, 2007; Benajiba et al 2008). Attia
et al(2010) heuristically induce a mapping be-
tween Arabic Wikipedia and Arabic WordNet to
construct Arabic NE gazetteers.
Balasuriya et al(2009) highlight the substan-
tial divergence between entities appearing in En-
glish Wikipedia versus traditional corpora, and
the effects of this divergence on NER perfor-
mance. There is evidence that models trained
on Wikipedia data generalize and perform well
on corpora with narrower domains. Nothman
et al(2009) and Balasuriya et al(2009) show
that NER models trained on both automatically
and manually annotated Wikipedia corpora per-
form reasonably well on news corpora. The re-
verse scenario does not hold for models trained
on news text, a result we also observe in Arabic
NER. Other work has gone beyond the entity de-
tection problem: Florian et al(2004) addition-
ally predict within-document entity coreference
for Arabic, Chinese, and English ACE text, while
Cucerzan (2007) aims to resolve every mention
detected in English Wikipedia pages to a canoni-
cal article devoted to the entity in question.
The domain and topic diversity of NEs has been
studied in the framework of domain adaptation
research. A group of these methods use self-
training and select the most informative features
and training instances to adapt a source domain
learner to the new target domain. Wu et al(2009)
bootstrap the NER leaner with a subset of unla-
beled instances that bridge the source and target
domains. Jiang and Zhai (2006) and Daume? III
(2007) make use of some labeled target-domain
data to tune or augment the features of the source
model towards the target domain. Here, in con-
trast, we use labeled target-domain data only for
tuning and evaluation. Another important dis-
tinction is that domain variation in this prior
work is restricted to topically-related corpora (e.g.
newswire vs. broadcast news), whereas in our
work, major topical differences distinguish the
training and test corpora?and consequently, their
salient NE classes. In these respects our NER
setting is closer to that of Florian et al(2010),
who recognize English entities in noisy text, (Sur-
deanu et al 2011), which concerns information
extraction in a topically distinct target domain,
and (Dalton et al 2011), which addresses English
NER in noisy and topically divergent text.
Self-training (Clark et al 2003; Mihalcea,
2004; McClosky et al 2006) is widely used
in NLP and has inspired related techniques that
learn from automatically labeled data (Liang et
al., 2008; Petrov et al 2010). Our self-training
procedure differs from some others in that we use
all of the automatically labeled examples, rather
than filtering them based on a confidence score.
Cost functions have been used in non-
structured classification settings to penalize cer-
tain types of errors more than others (Chan and
Stolfo, 1998; Domingos, 1999; Kiddon and Brun,
2011). The goal of optimizing our structured NER
model for recall is quite similar to the scenario ex-
plored by Minkov et al(2006), as noted above.
8 Conclusion
We explored the problem of learning an NER
model suited to domains for which no labeled
training data are available. A loss function to en-
courage recall over precision during supervised
discriminative learning substantially improves re-
call and overall entity detection performance, es-
pecially when combined with a semisupervised
learning regimen incorporating the same bias.
We have also developed a small corpus of Ara-
bic Wikipedia articles via a flexible entity an-
notation scheme spanning four topical domains
(publicly available at http://www.ark.cs.
cmu.edu/AQMAR).
Acknowledgments
We thank Mariem Fekih Zguir and Reham Al Tamime
for assistance with annotation, Michael Heilman for
his tagger implementation, and Nizar Habash and col-
leagues for the MADA toolkit. We thank members of
the ARK group at CMU, Hal Daume?, and anonymous
reviewers for their valuable suggestions. This publica-
tion was made possible by grant NPRP-08-485-1-083
from the Qatar National Research Fund (a member of
the Qatar Foundation). The statements made herein
are solely the responsibility of the authors.
170
References
Ahmed Abdul-Hamid and Kareem Darwish. 2010.
Simplified feature set for Arabic named entity
recognition. In Proceedings of the 2010 Named En-
tities Workshop, pages 110?115, Uppsala, Sweden,
July. Association for Computational Linguistics.
Mohammed Attia, Antonio Toral, Lamia Tounsi, Mon-
ica Monachini, and Josef van Genabith. 2010.
An automatically built named entity lexicon for
Arabic. In Nicoletta Calzolari, Khalid Choukri,
Bente Maegaard, Joseph Mariani, Jan Odijk, Ste-
lios Piperidis, Mike Rosner, and Daniel Tapias, ed-
itors, Proceedings of the Seventh Conference on
International Language Resources and Evaluation
(LREC?10), Valletta, Malta, May. European Lan-
guage Resources Association (ELRA).
Bogdan Babych and Anthony Hartley. 2003. Im-
proving machine translation quality with automatic
named entity recognition. In Proceedings of the 7th
International EAMT Workshop on MT and Other
Language Technology Tools, EAMT ?03.
Dominic Balasuriya, Nicky Ringland, Joel Nothman,
Tara Murphy, and James R. Curran. 2009. Named
entity recognition in Wikipedia. In Proceedings
of the 2009 Workshop on The People?s Web Meets
NLP: Collaboratively Constructed Semantic Re-
sources, pages 10?18, Suntec, Singapore, August.
Association for Computational Linguistics.
Yassine Benajiba, Paolo Rosso, and Jose? Miguel
Bened??Ruiz. 2007. ANERsys: an Arabic named
entity recognition system based on maximum en-
tropy. In Alexander Gelbukh, editor, Proceedings
of CICLing, pages 143?153, Mexico City, Mexio.
Springer.
Yassine Benajiba, Mona Diab, and Paolo Rosso. 2008.
Arabic named entity recognition using optimized
feature sets. In Proceedings of the 2008 Confer-
ence on Empirical Methods in Natural Language
Processing, pages 284?293, Honolulu, Hawaii, Oc-
tober. Association for Computational Linguistics.
Philip K. Chan and Salvatore J. Stolfo. 1998. To-
ward scalable learning with non-uniform class and
cost distributions: a case study in credit card fraud
detection. In Proceedings of the Fourth Interna-
tional Conference on Knowledge Discovery and
Data Mining, pages 164?168, New York City, New
York, USA, August. AAAI Press.
Ciprian Chelba and Alex Acero. 2006. Adaptation of
maximum entropy capitalizer: Little data can help
a lot. Computer Speech and Language, 20(4):382?
399.
Massimiliano Ciaramita and Mark Johnson. 2003. Su-
persense tagging of unknown nouns in WordNet. In
Proceedings of the 2003 Conference on Empirical
Methods in Natural Language Processing, pages
168?175.
Stephen Clark, James Curran, and Miles Osborne.
2003. Bootstrapping POS-taggers using unlabelled
data. In Walter Daelemans and Miles Osborne,
editors, Proceedings of the Seventh Conference on
Natural Language Learning at HLT-NAACL 2003,
pages 49?55.
Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models: theory and experi-
ments with perceptron algorithms. In Proceedings
of the ACL-02 Conference on Empirical Methods in
Natural Language Processing (EMNLP), pages 1?
8, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Silviu Cucerzan. 2007. Large-scale named entity
disambiguation based on Wikipedia data. In Pro-
ceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 708?716, Prague, Czech Republic,
June.
James R. Curran, Tara Murphy, and Bernhard Scholz.
2007. Minimising semantic drift with Mutual
Exclusion Bootstrapping. In Proceedings of PA-
CLING, 2007.
Jeffrey Dalton, James Allan, and David A. Smith.
2011. Passage retrieval for incorporating global
evidence in sequence labeling. In Proceedings of
the 20th ACM International Conference on Infor-
mation and Knowledge Management (CIKM ?11),
pages 355?364, Glasgow, Scotland, UK, October.
ACM.
Hal Daume? III. 2007. Frustratingly easy domain
adaptation. In Proceedings of the 45th Annual
Meeting of the Association of Computational Lin-
guistics, pages 256?263, Prague, Czech Republic,
June. Association for Computational Linguistics.
Pedro Domingos. 1999. MetaCost: a general method
for making classifiers cost-sensitive. Proceedings
of the Fifth ACM SIGKDD International Confer-
ence on Knowledge Discovery and Data Mining,
pages 155?164.
Benjamin Farber, Dayne Freitag, Nizar Habash, and
Owen Rambow. 2008. Improving NER in Arabic
using a morphological tagger. In Nicoletta Calzo-
lari, Khalid Choukri, Bente Maegaard, Joseph Mar-
iani, Jan Odjik, Stelios Piperidis, and Daniel Tapias,
editors, Proceedings of the Sixth International Lan-
guage Resources and Evaluation (LREC?08), pages
2509?2514, Marrakech, Morocco, May. European
Language Resources Association (ELRA).
Radu Florian, Hany Hassan, Abraham Ittycheriah,
Hongyan Jing, Nanda Kambhatla, Xiaoqiang Luo,
Nicolas Nicolov, and Salim Roukos. 2004. A
statistical model for multilingual entity detection
and tracking. In Susan Dumais, Daniel Marcu,
and Salim Roukos, editors, Proceedings of the Hu-
man Language Technology Conference of the North
171
American Chapter of the Association for Compu-
tational Linguistics: HLT-NAACL 2004, page 18,
Boston, Massachusetts, USA, May. Association for
Computational Linguistics.
Radu Florian, John Pitrelli, Salim Roukos, and Imed
Zitouni. 2010. Improving mention detection ro-
bustness to noisy input. In Proceedings of EMNLP
2010, pages 335?345, Cambridge, MA, October.
Association for Computational Linguistics.
Dayne Freitag. 2004. Trained named entity recog-
nition using distributional clusters. In Dekang Lin
and Dekai Wu, editors, Proceedings of EMNLP
2004, pages 262?269, Barcelona, Spain, July. As-
sociation for Computational Linguistics.
Kevin Gimpel and Noah A. Smith. 2010a. Softmax-
margin CRFs: Training log-linear models with loss
functions. In Proceedings of the Human Language
Technologies Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 733?736, Los Angeles, California,
USA, June.
Kevin Gimpel and Noah A. Smith. 2010b.
Softmax-margin training for structured log-
linear models. Technical Report CMU-LTI-
10-008, Carnegie Mellon University. http:
//www.lti.cs.cmu.edu/research/
reports/2010/cmulti10008.pdf.
Cyril Grouin, Sophie Rosset, Pierre Zweigenbaum,
Karn Fort, Olivier Galibert, and Ludovic Quin-
tard. 2011. Proposal for an extension of tradi-
tional named entities: from guidelines to evaluation,
an overview. In Proceedings of the 5th Linguis-
tic Annotation Workshop, pages 92?100, Portland,
Oregon, USA, June. Association for Computational
Linguistics.
Nizar Habash and Owen Rambow. 2005. Arabic to-
kenization, part-of-speech tagging and morpholog-
ical disambiguation in one fell swoop. In Proceed-
ings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics (ACL?05), pages
573?580, Ann Arbor, Michigan, June. Association
for Computational Linguistics.
Nizar Habash. 2010. Introduction to Arabic Natural
Language Processing. Morgan and Claypool Pub-
lishers.
Ahmed Hassan, Haytham Fahmy, and Hany Hassan.
2007. Improving named entity translation by ex-
ploiting comparable and parallel corpora. In Pro-
ceedings of the Conference on Recent Advances
in Natural Language Processing (RANLP ?07),
Borovets, Bulgaria.
Eduard Hovy, Mitchell Marcus, Martha Palmer,
Lance Ramshaw, and Ralph Weischedel. 2006.
OntoNotes: the 90% solution. In Proceedings of
the Human Language Technology Conference of
the NAACL (HLT-NAACL), pages 57?60, New York
City, USA, June. Association for Computational
Linguistics.
Dirk Hovy, Chunliang Zhang, Eduard Hovy, and
Anselmo Peas. 2011. Unsupervised discovery of
domain-specific knowledge from text. In Proceed-
ings of the 49th Annual Meeting of the Association
for Computational Linguistics: Human Language
Technologies, pages 1466?1475, Portland, Oregon,
USA, June. Association for Computational Linguis-
tics.
Jing Jiang and ChengXiang Zhai. 2006. Exploit-
ing domain structure for named entity recognition.
In Proceedings of the Human Language Technol-
ogy Conference of the NAACL (HLT-NAACL), pages
74?81, New York City, USA, June. Association for
Computational Linguistics.
Jun?ichi Kazama and Kentaro Torisawa. 2007.
Exploiting Wikipedia as external knowledge for
named entity recognition. In Proceedings of
the 2007 Joint Conference on Empirical Meth-
ods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 698?707, Prague, Czech Republic,
June. Association for Computational Linguistics.
Chloe Kiddon and Yuriy Brun. 2011. That?s what
she said: double entendre identification. In Pro-
ceedings of the 49th Annual Meeting of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies, pages 89?94, Portland, Ore-
gon, USA, June. Association for Computational
Linguistics.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Dekang Lin and
Dekai Wu, editors, Proceedings of EMNLP 2004,
pages 388?395, Barcelona, Spain, July. Association
for Computational Linguistics.
LDC. 2005. ACE (Automatic Content Extraction)
Arabic annotation guidelines for entities, version
5.3.3. Linguistic Data Consortium, Philadelphia.
Percy Liang, Hal Daume? III, and Dan Klein. 2008.
Structure compilation: trading structure for fea-
tures. In Proceedings of the 25th International Con-
ference on Machine Learning (ICML), pages 592?
599, Helsinki, Finland.
Chris Manning. 2006. Doing named entity recogni-
tion? Don?t optimize for F1. http://nlpers.
blogspot.com/2006/08/doing-named-
entity-recognition-dont.html.
David McClosky, Eugene Charniak, and Mark John-
son. 2006. Effective self-training for parsing. In
Proceedings of the Human Language Technology
Conference of the NAACL, Main Conference, pages
152?159, New York City, USA, June. Association
for Computational Linguistics.
Rada Mihalcea. 2004. Co-training and self-training
for word sense disambiguation. In HLT-NAACL
2004 Workshop: Eighth Conference on Computa-
tional Natural Language Learning (CoNLL-2004),
Boston, Massachusetts, USA.
172
Einat Minkov, Richard Wang, Anthony Tomasic, and
William Cohen. 2006. NER systems that suit user?s
preferences: adjusting the recall-precision trade-off
for entity extraction. In Proceedings of the Human
Language Technology Conference of the NAACL,
Companion Volume: Short Papers, pages 93?96,
New York City, USA, June. Association for Com-
putational Linguistics.
Luke Nezda, Andrew Hickl, John Lehmann, and Sar-
mad Fayyaz. 2006. What in the world is a Shahab?
Wide coverage named entity recognition for Arabic.
In Proccedings of LREC, pages 41?46.
Joel Nothman, Tara Murphy, and James R. Curran.
2009. Analysing Wikipedia and gold-standard cor-
pora for NER training. In Proceedings of the 12th
Conference of the European Chapter of the Associ-
ation for Computational Linguistics (EACL 2009),
pages 612?620, Athens, Greece, March. Associa-
tion for Computational Linguistics.
PediaPress. 2010. mwlib. http://code.
pediapress.com/wiki/wiki/mwlib.
Slav Petrov, Pi-Chuan Chang, Michael Ringgaard, and
Hiyan Alshawi. 2010. Uptraining for accurate de-
terministic question parsing. In Proceedings of the
2010 Conference on Empirical Methods in Natural
Language Processing, pages 705?713, Cambridge,
MA, October. Association for Computational Lin-
guistics.
Lev Ratinov and Dan Roth. 2009. Design chal-
lenges and misconceptions in named entity recog-
nition. In Proceedings of the Thirteenth Confer-
ence on Computational Natural Language Learning
(CoNLL-2009), pages 147?155, Boulder, Colorado,
June. Association for Computational Linguistics.
Nathan D. Ratliff, J. Andrew Bagnell, and Martin A.
Zinkevich. 2006. Subgradient methods for maxi-
mum margin structured learning. In ICML Work-
shop on Learning in Structured Output Spaces,
Pittsburgh, Pennsylvania, USA.
Ryan Roth, Owen Rambow, Nizar Habash, Mona
Diab, and Cynthia Rudin. 2008. Arabic morpho-
logical tagging, diacritization, and lemmatization
using lexeme models and feature ranking. In Pro-
ceedings of ACL-08: HLT, pages 117?120, Colum-
bus, Ohio, June. Association for Computational
Linguistics.
Satoshi Sekine, Kiyoshi Sudo, and Chikashi Nobata.
2002. Extended named entity hierarchy. In Pro-
ceedings of LREC.
Burr Settles. 2004. Biomedical named entity recogni-
tion using conditional random fields and rich feature
sets. In Nigel Collier, Patrick Ruch, and Adeline
Nazarenko, editors, COLING 2004 International
Joint workshop on Natural Language Processing in
Biomedicine and its Applications (NLPBA/BioNLP)
2004, pages 107?110, Geneva, Switzerland, Au-
gust. COLING.
Khaled Shaalan and Hafsa Raza. 2008. Arabic
named entity recognition from diverse text types. In
Advances in Natural Language Processing, pages
440?451. Springer.
Mihai Surdeanu, David McClosky, Mason R. Smith,
Andrey Gusev, and Christopher D. Manning. 2011.
Customizing an information extraction system to
a new domain. In Proceedings of the ACL 2011
Workshop on Relational Models of Semantics, Port-
land, Oregon, USA, June. Association for Compu-
tational Linguistics.
Ben Taskar, Carlos Guestrin, and Daphne Koller.
2004. Max-margin Markov networks. In Sebastian
Thrun, Lawrence Saul, and Bernhard Scho?lkopf,
editors, Advances in Neural Information Processing
Systems 16. MIT Press.
Antonio Toral, Elisa Noguera, Fernando Llopis, and
Rafael Mun?oz. 2005. Improving question an-
swering using named entity recognition. Natu-
ral Language Processing and Information Systems,
3513/2005:181?191.
Ioannis Tsochantaridis, Thorsten Joachims, Thomas
Hofmann, and Yasemin Altun. 2005. Large margin
methods for structured and interdependent output
variables. Journal of Machine Learning Research,
6:1453?1484, September.
Christopher Walker, Stephanie Strassel, Julie Medero,
and Kazuaki Maeda. 2006. ACE 2005 multi-
lingual training corpus. LDC2006T06, Linguistic
Data Consortium, Philadelphia.
Ralph Weischedel and Ada Brunstein. 2005.
BBN pronoun coreference and entity type cor-
pus. LDC2005T33, Linguistic Data Consortium,
Philadelphia.
Dan Wu, Wee Sun Lee, Nan Ye, and Hai Leong Chieu.
2009. Domain adaptive bootstrapping for named
entity recognition. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1523?1532, Singapore, August.
Association for Computational Linguistics.
Tianfang Yao, Wei Ding, and Gregor Erbach. 2003.
CHINERS: a Chinese named entity recognition sys-
tem for the sports domain. In Proceedings of the
Second SIGHAN Workshop on Chinese Language
Processing, pages 55?62, Sapporo, Japan, July. As-
sociation for Computational Linguistics.
173
Book Review
Design Patterns in Fluid Construction Grammar
Luc Steels (editor)
Universitat Pompeu Fabra and Sony Computer Science Laboratory, Paris
Amsterdam: John Benjamins Publishing Company (Constructional Approaches to
Language series, edited by Mirjam Fried and Jan-Ola O?stman, volume 11), 2012,
xi+332 pp; hardbound, ISBN 978-90-272-0433-2, e99.00, $149.00
Reviewed by
Nathan Schneider, Carnegie Mellon University
and Reut Tsarfaty, Uppsala University
In computational modeling of natural language phenomena, there are at least three
modes of research. The currently dominant statistical paradigm typically prioritizes
instance coverage: Data-driven methods seek to use as much information observed in
data as possible in order to generalize linguistic analyses to unseen instances. A second
approach prioritizes detailed description of grammatical phenomena, that is, forming
and defending theories with a focus on a small number of instances. A third approach
might be called integrative: Rather than addressing phenomena in isolation, different
approaches are brought together to address multiple challenges in a unified framework,
and the behavior of the system is demonstrated with a small number of instances. Design
Patterns in Fluid Construction Grammar (DPFCG) exemplifies the third approach, intro-
ducing a linguistic formalism called Fluid Construction Grammar (FCG) that addresses
parsing, production, and learning in a single computational framework.
The book emphasizes grammar-engineering, following broad-coverage descrip-
tive paradigms that can be traced back to Generalized Phrase Structure Grammar
(GPSG) (Gazdar et al 1985), Lexical Functional Grammar (LFG) (Bresnan 2000), Head-
Driven Phrase-Structure Grammar (HPSG) (Sag and Wasow 1999), and Combinatory
Categorial Grammar (CCG) (Steedman 1996). In all of these cases, a formal meta-
framework allows computational linguists to formalize their hypotheses and intuitions
about a language?s grammatical behavior and then explore how these representational
choices affect the processing of natural language utterances. Many of the aforemen-
tioned approaches have engendered large-scale platforms that can be used and reused
to provide formal description of grammars for different languages, such as Par-Gram
for LFG (Butt et al 2002) and the LinGO Grammar Matrix for HPSG (Bender, Flickinger,
and Oepen 2002).
? 2013 Association for Computational Linguistics
FCG offers a similar grammar engineering framework that follows the principles
of Construction Grammar (CxG) (Goldberg 2003; Hoffmann and Trousdale 2013). CxG
treats constructions as the basic units of grammatical organization in language. The con-
structions are viewed as learned associations between form (e.g., sounds, morphemes,
syntactic phrases) and function (semantics, pragmatics, discourse meaning, etc.). CxG
does not impose a strict separation between lexicon and grammar?indeed, it is per-
haps best known as treating semi-productive idioms like ?the X-er, the Y-er? and
?X let alne Y? on equal footing with lexemes and ?core? syntactic patterns (Fillmore,
doi:10.1162/COLI r 00154
Computational Linguistics Volume 39, Number 2
Kay, and O?Connor 1988; Kay and Fillmore 1999). FCG, like other CxG formalisms?
namely, Embodied Construction Grammar (Bergen and Chang 2005; Feldman, Dodge,
and Bryant 2009) and Sign-Based Construction Grammar (Boas and Sag 2012)?is
unification-based.1 The studies in this book describe constructions and how they can
be combined in order to model natural language interpretation or generation as feature
structure unification in a general search procedure.
The book has five parts, covering the groundwork, basic linguistic applications,
processing matters, advanced case studies, and, finally, features of FCG that make it
fluid and robust. Each chapter identifies general strategies (design patterns) that might
merit reuse in new FCG grammars, or perhaps in other computational frameworks.
Part I: Introduction lays the groundwork for the rest of the book. ?Introducing Fluid
Construction Grammar? (by Luc Steels) presents the aims of the FCG formalism. FCG
was designed as a framework for describing linguistic units (constructions?their form
and meaning), with an emphasis on language variation and evolution (?fluidity?). The
constructionist approach to language is described and the argument for applying it to
study language variation and change is defended. Psychological validity is explicitly
ruled out as a modeling goal (?The emphasis is on getting working systems, and this
is difficult enough?; page 4). The architects of FCG set out to include both sides of
the processing coin, however?parsing (interpretation) and production (generation).
The concept of search in processing is emphasized, though some of the explanations
of processing steps are too abstract for the reader to comprehend at this point. A
further desideratum?robustness to noisy input containing disfluencies, fragments, and
errors?is given as motivating a constructionist approach.
The next chapter, ?A First Encounter with Fluid Construction Grammar?
(by Steels), describes the mechanisms of FCG in detail. In FCG, a working analysis
hypothesized in processing is known as transient structure; the transduction of form to
meaning (and vice versa) selects a sequence of constructions that apply to the transient
structure to gradually expand it until reaching a final analysis. Identifying construc-
tions that may apply to a transient structure presents a non-trivial search problem,
also addressed by the architects of FCG. The sheer number of technical details make
this chapter somewhat overwhelming. Most of the chapter is devoted to the low-level
feature structures and the operations manipulating them. Templates?a practical means
of avoiding boilerplate code when defining constructions?are then introduced, and do
most of the heavy lifting in the rest of the book.
In Part II: Grammatical Structures, we begin to see how constructions are defined in
practice. ?A Design Pattern for Phrasal Constructions? (by Steels) illustrates how con-
structions are used to describe the combination of multiple units into higher-level, typed
phrases. Skeletal constructions compose with smaller units to form hierarchical struc-
tures (essentially similar to the Immediate Constituents Analysis of Bloomfield [1933]
and follow-up work in structuralist linguistics [Harris 1946]), and a range of additional
constructions impose form (e.g., ordering) constraints and add new meaning to the
newly created phrases. This chapter is of a tutorial nature, illustrating the step-by-step
application of four kinds of noun phrase constructions to expand transient structures
in processing. Over twenty templates are introduced in this chapter; they encapsulate
design patterns dealing with hierarchical structure, agreement, and feature percolation.
1 Less formal members of the Construction Grammar family include cognitive approaches in the
Berkeley tradition (Lakoff 1987; Goldberg 1995, 2006), Cognitive Grammar (Langacker 1987),
and Radical Construction Grammar (Croft 2001).
448
Book Review
An aspect of phrasal constructions that is not yet dealt with is the complex linking of se-
mantic arguments and the morphosyntactic categorizations of the composed elements.
?A Design Pattern for Argument Structure Constructions? (by Remi van Trijp)
then builds on the formal machinery presented in the previous chapter to explicitly
address the complex mappings between semantic arguments (agent, patient, etc.) and
syntactic arguments (subject, object, etc.). This mapping is a complex matter due to
language-specific conceptualization of semantic arguments and different means of
morphosyntactic realization used by different languages. In FCG, each lexical item
introduces its linking potential in terms of the different types of semantic and syntactic
arguments that it may take, with no particular mapping between them. Each argument
structure construction imposes a partial mapping between the syntactic and semantic
arguments to yield a particular argument structure instantiation, one of the multiple
alternatives that may be available for a single lexical item. This account stands in sharp
contrast to the lexicalist view of argument-structure (the view taken in LFG, HPSG, and
CCG) whereby each lexical entry dictates all the necessary linking information. The
construction-based approach is defended for its ability to deal with unknown words2
and constructional coercion3 (Goldberg 1995). The argument structure design pattern
allows FCG to crudely recover a partial specification of the form-meaning mapping of
these elements, which is important for robust processing (see subsequent discussion).
Part III: Managing Processing addresses how FCG transduces between a linguistic
string and a meaning representation, where the two directions (parsing and production)
share a common declarative representation of linguistic knowledge (the grammar). This
entails assembling an analysis incrementally on the basis of the grammar, the input, and
any partial analysis that has already been created. With FCG (and unification grammars
more broadly) this search is nontrivial, and streamlining search (i.e., minimizing non-
determinism and avoiding dead ends) is a key motivator of many of the grammar
design patterns suggested in the book.
?Search in Linguistic Processing? (by Joris Bleys, Kevin Stadler, and Joachim De
Beule) deals mainly with the problem of choosing which of multiple compatible con-
structions to apply next. Whereas the default heuristic search in FCG is a greedy, depth-
first search (which can backtrack if the user-defined end-goal has not yet been achieved)
the FCG framework allows for a guided search through scores that reflect the relative
tendency of a construction to apply next. The authors suggest that such scoring can be
informed by general principles, for instance: (i) specific constructions are preferred to
more general ones, and (ii) previously co-applied constructions are preferred. Choosing
appropriate constructions to apply early on dramatically reduces the time needed for
processing the utterance.
?Organizing Constructions in Networks? (by Pieter Wellens) takes this idea to
the next level, and proposes to organize the different constructions in networks of
conditional dependencies. A conditional dependency links two constructions where
one provides necessary information for the application of the other. These dependency
networks can be updated whenever an input is processed so that the system learns
to search more efficiently when the same constructions are encountered in the future.
Using these networks to guide the search thus significantly reduces the search for
compatible constructions. An empirical effort to quantify this effect indeed shows a
2 For example, ?He blicked the napkin off the table??a reasonable inference is that the subject caused the
napkin to leave the table.
3 ?He sneezed the napkin off the table??note that sneeze is not normally transitive.
449
Computational Linguistics Volume 39, Number 2
sharp reduction in search time; unlike the held-out experimental paradigm accepted in
statistical NLP, however, the parsed/produced sentence is assumed to have been seen
already by the system.
Part IV: Case Studies addresses three challenging linguistic phenomena in FCG.
?Feature Matrices and Agreement? (by van Trijp) on German case offers a new
unification-based solution to the problem of feature indeterminacy. For instance, in the
sentence Er findet und hilft Frauen ?He finds and helps women?, the first verb requires
an accusative object, whereas the second requires a dative object; the coordination
is allowed only because Frauen can be either accusative or dative. Kindern ?children?,
which can only be dative, is not licensed here. Encoding case in a single feature on
the Frauen construction wouldn?t work because the feature would have to unify with
contradictory values (from the verbs? inflectional features). Instead, case restrictions
specified lexically for a noun or verb can be expressed with a distinctive feature matrix,
with each matrix slot holding a variable or the value + or -. Unification then does
the right thing?allowing Frauen and forbidding Kindern?without resorting to type
hierarchies or disjunctive features.
?Construction Sets and Unmarked Forms? (by Katrien Beuls) on Hungarian verbal
agreement models a phenomenon whereby morphosyntactic, semantic, and phono-
logical factors affect the choice between poly- and mono-personal agreement?that is,
the decision whether a Hungarian transitive verb should agree with its object or just
with its subject. The case and definiteness of the object and the person hierarchy rela-
tionship between subject and object determine which kind of agreement obtains, and
phonological constraints determine its form. To make the different levels of structure
interact properly, constructions are grouped into sets (lexical, morphological, etc.) and
those sets are considered in a fixed order during processing. Construction sets also allow
for efficient handling of unmarked forms (null affixes)?they are considered only after
the overt affixes have had the opportunity to apply, thereby functioning as defaults.
?Syntactic Indeterminacy and Semantic Ambiguity? (by Michael Spranger and
Martin Loetzsch) on German spatial phrases models the German spatial terms for front,
back, left, and right. To model spatial language in situated interaction with robots, two
problems must be overcome. The first is syntactic indeterminacy: Any of these spatial
relations may be realized as an adjective, an adverb, or a preposition. The second is
semantic ambiguity, specifically when the perspective (e.g., whose ?left??) is implicit.
Both are forms of underspecification which could cause early splits in the search space
if handled na??vely. Much in the spirit of the argument structure constructions (see
above), the solutions (which are too technical to explain here) involve (a) disjunctive
representations of potential values of a feature, and (b) deferring decisions until a more
opportune stage.
Part V: Robustness and Fluidity (by Steels and van Trijp) surveys the different
features of the system that ensure robustness in the face of variation, disfluencies,
and noise. Natural language is fluid and open-ended. There is variation between
speakers, there are disfluencies and speech errors, and noise may corrupt the speech
signal. All of these may jeopardize the interpretability of the signal, but human
listeners are adept at processing such input. In the spirit of usage-based grammar
(Tomasello 2003), FCG emphasizes the attainment of a communicative goal, rather than
ensuring grammaticality of parsed/produced utterances. This is accomplished with
a diagnostic-repair process that runs in parallel to parsing/production. Diagnostics
can test for unknown words, unfamiliar meanings, missing constructions, and so on.
Diagnostic tests are implemented by reversing the direction of the transduction process:
a speaker may assume the hearer?s point of view to analyze what she has produced
450
Book Review
in order to see whether communicative success has been attained. Likewise, a hearer
may produce a phrase according to his own interpretation of the speaker?s form, and
check for a match. If a test fails, repair strategies such as proposing new constructions,
relaxing the matching process for construction application, and coercing constructions
to adapt to novel language use are considered.
Fluidity and robustness are the hallmarks of FCG, and the computational
framework has been used in experiments that assume embedded communication in
robotic agents. This research program is developed at length by Steels (2012b).
Discussion. Like the legacy of the GPSG book (Gazdar et al 1985), this book?s main merit
is not necessarily in its technical details or computational choices, but in demonstrating
the feasibility of implementing the constructional approach in a full-fledged computa-
tional framework. We suggest that the CxG perspective presents a formidable challenge
to the computational linguistics/natural language processing community. It posits a
different notion of modularity than is observed by most NLP systems: Rather than
treat different levels of linguistic structure independently, CxG recognizes that multiple
formal components (phonological, lexical, morphological, syntactic) may be tied by
convention to a specific meaning or function. Systematically describing these ?cross-
cutting? constructions and their processing, especially in a way that scales to large data
encompassing both form and meaning and accommodates both parsing and generation,
would in our view make for a more comprehensive account of language processing than
our field is able to offer today. Thus, we hope this book will be provocative even outside
of the grammar engineering community.
This book is not without its weaknesses. In parts the writing is quite technical and
terse, which can be daunting for readers new to FCG. Contextualization with respect to
other strands of computational linguistics and AI research is, for the most part, lacking,
though a second FCG book (Steels 2012a) picks up some of the slack on this front.4
DPFCG does not address the feasibility of learning constructions directly from data,5 nor
does it discuss the expressive power of the formalism in relation to learnability results
(such as that of Gold [1967]). As admitted by the authors, much more work would be
needed to build life-size grammars. Still, we hope that readers of DPFCG will appreciate
the authors? vision for a model of linguistic form and function that is at once formal,
computational, fluid, and robust.
References
Bender, Emily M., Dan Flickinger, and
Stephan Oepen. 2002. The grammar
matrix: An open-source starter-kit for the
rapid development of cross-linguistically
consistent broad-coverage precision
grammars. In Proceedings of the Workshop
on Grammar Engineering and Evaluation
at the 19th International Conference on
Computational Linguistics, pages 8?14,
Taipei.
Bergen, Benjamin K. and Nancy Chang.
2005. Embodied Construction Grammar in
simulation-based language understanding.
4 In particular, the Embodied Construction Grammar formalism noted earlier offers a similar
computational framework, though it reflects different research goals. The most important difference is
that FCG was designed to study language evolution, and ECG to study language acquisition and use
from a cognitive perspective. FCG is largely about processing (support for production as well as parsing
is considered essential); ECG places a greater premium on the meaning representation, formalizing a
number of theoretical constructs from cognitive semantics. Chang, De Beule, and Micelli (2012) compare
and contrast the two approaches in detail.
5 Ideas that may be of relevance here are discussed in the Data-Oriented Parsing literature (Scha 1990; Bod
2003) and the statistical learning of Stochastic Tree Substitution Grammars (O?Donnell 2011).
451
Computational Linguistics Volume 39, Number 2
In Jan-Ola O?stman and Mirjam Fried,
editors, Construction Grammars: Cognitive
Grounding and Theoretical Extensions. John
Benjamins, Amsterdam, pages 147?190.
Bloomfield, Leonard. 1933. Language. Holt,
Rinehart and Winston Inc.
Boas, Hans C. and Ivan A. Sag, editors.
2012. Sign-Based Construction Grammar.
Number 193 in CSLI Lecture Notes.
CSLI Publications, Stanford, CA.
Bod, Rens. 2003. An efficient implementation
of a new DOP model. In Proceedings of the
10th Conference of the European Chapter of the
Association for Computational Linguistics,
pages 19?26, Budapest.
Bresnan, Joan. 2000. Lexical-Functional Syntax.
Blackwell.
Butt, Miriam, Helge Dyvik, Tracy Holloway
King, Hiroshi Masuichi, and Christian
Rohrer. 2002. The parallel grammar
project. In Proceedings of COLING 2002,
Workshop on Grammar Engineering and
Evaluation, Taipei.
Chang, Nancy, Joachim De Beule, and
Vanessa Micelli. 2012. Computational
construction grammar: Comparing
ECG and FCG. In Luc Steels, editor,
Computational Issues in Fluid Construction
Grammar. Springer Verlag, Berlin,
pages 259?288.
Croft, William. 2001. Radical Construction
Grammar: Syntactic Theory in Typological
Perspective. Oxford University Press,
Oxford.
Feldman, Jerome A., Ellen Dodge,
and John Bryant. 2009. Embodied
Construction Grammar. In Bernd Heine
and Heiko Narrog, editors, The
Oxford Handbook of Linguistic Analysis.
Oxford University Press, Oxford,
pages 111?138.
Fillmore, Charles J., Paul Kay, and
Mary Catherine O?Connor. 1988.
Regularity and idiomaticity in
grammatical constructions:
The case of ?let alne.? Language,
64(3):501?538.
Gazdar, Gerald, Ewan Klein, Geoffrey K.
Pullum, and Ivan A. Sag. 1985. Generalised
Phrase Structure Grammar. Blackwell,
Oxford, England.
Gold, E. Mark. 1967. Language identication
in the limit. Information and Control,
10(5):447?474.
Goldberg, Adele E. 1995. Constructions:
A construction grammar approach to
argument structure. University of
Chicago Press, Chicago.
Goldberg, Adele E. 2003. Constructions:
A new theoretical approach to language.
Trends in Cognitive Sciences, 7(5):219?224.
Goldberg, Adele E. 2006. Constructions at
work: the nature of generalization in language.
Oxford University Press, Oxford.
Harris, Zellig S. 1946. From morpheme to
utterance. Language, 22(3):161?183.
Hoffmann, Thomas and Graeme Trousdale,
editors. 2013. The Oxford Handbook
of Construction Grammar. Oxford
University Press, Oxford.
Kay, Paul and Charles J. Fillmore. 1999.
Grammatical constructions and linguistic
generalizations: The What?s X doing Y?
construction. Language, 75(1):1?33.
Lakoff, George. 1987. Women, Fire, and
Dangerous Things: What Categories Reveal
About the Mind. University of Chicago
Press, Chicago.
Langacker, Ronald W. 1987. Foundations of
Cognitive Grammar, volume 1. Stanford
University Press, Stanford, CA.
O?Donnell, Timothy J. 2011. Productivity
and reuse in language. Ph.D. dissertation,
Harvard University, Cambridge, MA.
Sag, Ivan A. and Thomas Wasow. 1999.
Syntactic Theory: A Formal Introduction.
CSLI Publications, Stanford, CA.
Scha, Remko. 1990. Language theory and
language technology: Competence and
performance. In R. de Kort and G. L. J.
Leerdam, editors, Computertoepassingen
in de Neerlandistiek. Almere, pages 7?22.
Schneider, Nathan. 2010. Computational
cognitive morphosemantics: Modeling
morphological compositionality
in Hebrew verbs with Embodied
Construction Grammar. In Proceedings
of the 36th Annual Meeting of the Berkeley
Linguistics Society, Berkeley, CA.
Steedman, Mark. 1996. Surface Structures
and Interpretation. Number 30 in
Linguistic Inquiry Monographs.
The MIT Press, Cambridge, MA.
Steels, Luc, editor. 2012a. Computational
Issues in Fluid Construction Grammar.
Number 7249 in Lecture Notes in
Computer Science. Springer, Berlin.
Steels, Luc, editor. 2012b. Experiments in
Cultural Language Evolution. Number 3
in Advances in Interaction Studies.
John Benjamins Publishing, Amsterdam.
Tomasello, Michael. 2003. Constructing a
Language: A Usage-Based Theory of Language
Acquisition. Harvard University Press,
Cambridge, MA.
Tsarfaty, Reut. 2010. Relational-Realization
Parsing. PhD thesis, University of
Amsterdam.
452
Book Review
Nathan Schneider is a doctoral student at Carnegie Mellon University. His research has touched
several areas of computational semantics, including an account of Semitic morphology in
Embodied Construction Grammar (Schneider 2010). E-mail: nschneid@cs.cmu.edu. Reut Tsarfaty
is a postdoctoral researcher at Uppsala University. In her research she developed the Relational-
Realization framework for statistical modeling of flexible form-function correspondence patterns
(Tsarfaty 2010). Tsarfaty is a guest editor of the recent Computational Linguistics special issue on
Parsing Morphologically Rich Languages (PMRL) and she is the author of a book on the topic to
be published by Morgan & Claypool Publishers. E-mail: tsarfaty@stp.lingfil.uu.se.
453

Frame-Semantic Parsing
Dipanjan Das?
Google Inc.
Desai Chen??
Massachusetts Institute of Technology
Andre? F. T. Martins?
Priberam Labs
Instituto de Telecomunicac?o?es
Nathan Schneider?
Carnegie Mellon University
Noah A. Smith?
Carnegie Mellon University
Frame semantics is a linguistic theory that has been instantiated for English in the FrameNet
lexicon. We solve the problem of frame-semantic parsing using a two-stage statistical model
that takes lexical targets (i.e., content words and phrases) in their sentential contexts and
predicts frame-semantic structures. Given a target in context, the first stage disambiguates it to a
semantic frame. This model uses latent variables and semi-supervised learning to improve frame
disambiguation for targets unseen at training time. The second stage finds the target?s locally
expressed semantic arguments. At inference time, a fast exact dual decomposition algorithm
collectively predicts all the arguments of a frame at once in order to respect declaratively stated
linguistic constraints, resulting in qualitatively better structures than na??ve local predictors.
Both components are feature-based and discriminatively trained on a small set of annotated
frame-semantic parses. On the SemEval 2007 benchmark data set, the approach, along with a
heuristic identifier of frame-evoking targets, outperforms the prior state of the art by significant
margins. Additionally, we present experiments on the much larger FrameNet 1.5 data set. We
have released our frame-semantic parser as open-source software.
? Google Inc., New York, NY 10011. E-mail: dipanjand@google.com.
?? Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology,
Cambridge, MA 02139. E-mail: desaic@csail.mit.edu.
? Alameda D. Afonso Henriques, 41 - 2.? Andar, 1000-123, Lisboa, Portugal. E-mail: atm@priberam.pt.
? School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213.
E-mail: nschneid@cs.cmu.edu.
? School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213.
E-mail: nasmith@cs.cmu.edu.
Submission received: 4 May 2012; revised submission received: 10 November 2012; accepted for publication:
22 December 2012.
doi:10.1162/COLI a 00163
? 2014 Association for Computational Linguistics
Computational Linguistics Volume 40, Number 1
1. Introduction
FrameNet (Fillmore, Johnson, and Petruck 2003) is a linguistic resource storing consider-
able information about lexical and predicate-argument semantics in English. Grounded
in the theory of frame semantics (Fillmore 1982), it suggests?but does not formally
define?a semantic representation that blends representations familiar from word-sense
disambiguation (Ide and Ve?ronis 1998) and semantic role labeling (SRL; Gildea and
Jurafsky 2002). Given the limited size of available resources, accurately producing
richly structured frame-semantic structures with high coverage will require data-driven
techniques beyond simple supervised classification, such as latent variable modeling,
semi-supervised learning, and joint inference.
In this article, we present a computational and statistical model for frame-semantic
parsing, the problem of extracting from text semantic predicate-argument structures
such as those shown in Figure 1. We aim to predict a frame-semantic representation
with two statistical models rather than a collection of local classifiers, unlike earlier ap-
proaches (Baker, Ellsworth, and Erk 2007). We use a probabilistic framework that cleanly
integrates the FrameNet lexicon and limited available training data. The probabilistic
framework we adopt is highly amenable to future extension through new features, more
relaxed independence assumptions, and additional semi-supervised models.
Carefully constructed lexical resources and annotated data sets from FrameNet,
detailed in Section 3, form the basis of the frame structure prediction task. We de-
compose this task into three subproblems: target identification (Section 4), in which
frame-evoking predicates are marked in the sentence; frame identification (Section 5),
in which the evoked frame is selected for each predicate; and argument identification
(Section 6), in which arguments to each frame are identified and labeled with a role from
that frame. Experiments demonstrating favorable performance to the previous state of
the art on SemEval 2007 and FrameNet data sets are described in each section. Some
novel aspects of our approach include a latent-variable model (Section 5.2) and a semi-
supervised extension of the predicate lexicon (Section 5.5) to facilitate disambiguation of
words not in the FrameNet lexicon; a unified model for finding and labeling arguments
Figure 1
An example sentence from the annotations released as part of FrameNet 1.5 with three targets
marked in bold. Note that this annotation is partial because not all potential targets have been
annotated with predicate-argument structures. Each target has its evoked semantic frame
marked above it, enclosed in a distinct shape or border style. For each frame, its semantic roles
are shown enclosed within the same shape or border style, and the spans fulfilling the roles are
connected to the latter using dotted lines. For example, manner evokes the CONDUCT frame, and
has the AGENT and MANNER roles fulfilled by Austria and most un-Viennese, respectively.
10
Das et al. Frame-Semantic Parsing
(Section 6) that diverges from prior work in semantic role labeling; and an exact dual
decomposition algorithm (Section 7) that collectively predicts all the arguments of a
frame together, thereby incorporating linguistic constraints in a principled fashion.
Our open-source parser, named SEMAFOR (Semantic Analyzer of Frame Represen-
tations)1 achieves the best published results to date on the SemEval 2007 frame-semantic
structure extraction task (Baker, Ellsworth, and Erk 2007). Herein, we also present
results on newly released data with FrameNet 1.5, the latest edition of the lexicon.
Some of the material presented in this article has appeared in previously published
conference papers: Das et al. (2010) presented the basic model, Das and Smith (2011)
described semi-supervised lexicon expansion, Das and Smith (2012) demonstrated a
sparse variant of lexicon expansion, and Das, Martins, and Smith (2012) presented the
dual decomposition algorithm for constrained joint argument identification. We present
here a synthesis of those results and several additional details:
1. The set of features used in the two statistical models for frame identification and
argument identification.
2. Details of a greedy beam search algorithm for argument identification that avoids
illegal argument overlap.
3. Error analysis pertaining to the dual decomposition argument identification algo-
rithm, in contrast with the beam search algorithm.
4. Results on full frame-semantic parsing using graph-based semi-supervised learn-
ing with sparsity-inducing penalties; this expands the small FrameNet predicate
lexicon, enabling us to handle unknown predicates.
Our primary contributions are the use of efficient structured prediction tech-
niques suited to shallow semantic parsing problems, novel methods in semi-supervised
learning that improve the lexical coverage of our parser, and making frame-semantic
structures a viable computational semantic representation usable in other language
technologies. To set the stage, we next consider related work in the automatic prediction
of predicate-argument semantic structures.
2. Related Work
In this section, we will focus on previous scientific work relevant to the problem of
frame-semantic parsing. First, we will briefly discuss work done on PropBank-style
semantic role labeling, following which we will concentrate on the more relevant prob-
lem of frame-semantic structure extraction. Next, we review previous work that has
used semi-supervised learning for shallow semantic parsing. Finally, we discuss prior
work on joint structure prediction relevant to frame-semantic parsing.
2.1 Semantic Role Labeling
Since Gildea and Jurafsky (2002) pioneered statistical semantic role labeling, there
has been a great deal of computational work using predicate-argument structures
for semantics. The development of PropBank (Kingsbury and Palmer 2002), followed
by CoNLL shared tasks on semantic role labeling (Carreras and Ma`rquez 2004,
2005) boosted research in this area. Figure 2(a) shows an annotation from PropBank.
PropBank annotations are closely tied to syntax, because the data set consists of the
1 See http://www.ark.cs.cmu.edu/SEMAFOR.
11
Computational Linguistics Volume 40, Number 1
(a)
(b)
Figure 2
(a) A phrase-structure tree taken from the Penn Treebank and annotated with PropBank
predicate-argument structures. The verbs created and pushed serve as predicates in this
sentence. Dotted arrows connect each predicate to its semantic arguments (bracketed phrases).
(b) A partial depiction of frame-semantic structures for the same sentence. The words in bold
are targets, which instantiate a (lemmatized and part-of-speech?tagged) lexical unit and evoke
a semantic frame. Every frame annotation is shown enclosed in a distint shape or border style,
and its argument labels are shown together on the same vertical tier below the sentence.
See text for explanation of abbreviations.
phrase-structure syntax trees from the Wall Street Journal section of the Penn Treebank
(Marcus, Marcinkiewicz, and Santorini 1993) annotated with predicate-argument
structures for verbs. In Figure 2(a), the syntax tree for the sentence is marked with
various semantic roles. The two main verbs in the sentence, created and pushed, are
the predicates. For the former, the constituent more than 1.2 million jobs serves as the
semantic role ARG1 and the constituent In that time serves as the role ARGM-TMP. Similarly
for the latter verb, roles ARG1, ARG2, ARGM-DIR, and ARGM-TMP are shown in the figure.
PropBank defines core roles ARG0 through ARG5, which receive different interpretations
for different predicates. Additional modifier roles ARGM-* include ARGM-TMP (temporal)
and ARGM-DIR (directional), as shown in Figure 2(a). The PropBank representation
therefore has a small number of roles, and the training data set comprises some
40,000 sentences, thus making the semantic role labeling task an attractive one from the
perspective of machine learning.
There are many instances of influential work on semantic role labeling using
PropBank conventions. Pradhan et al. (2004) present a system that uses support vector
machines (SVMs) to identify the arguments in a syntax tree that can serve as semantic
roles, followed by classification of the identified arguments to role names via a collection
of binary SVMs. Punyakanok et al. (2004) describe a semantic role labeler that uses inte-
ger linear programming for inference and uses several global constraints to find the best
12
Das et al. Frame-Semantic Parsing
suited predicate-argument structures. Joint modeling for semantic role labeling with
discriminative log-linear models is presented by Toutanova, Haghighi, and Manning
(2005), where global features looking at all arguments of a particular verb together are
incorporated into a dynamic programming and reranking framework. The Computa-
tional Linguistics special issue on semantic role labeling (Ma`rquez et al. 2008) includes
other interesting papers on the topic, leveraging the PropBank conventions for labeling
shallow semantic structures. Recently, there have been initiatives to predict syntactic
dependencies as well as PropBank-style predicate-argument structures together using
one joint model (Surdeanu et al. 2008; Hajic? et al. 2009).
Here, we focus on the related problem of frame-semantic parsing. Note from the
annotated semantic roles for the two verbs in the sentence of Figure 2(a) that it is
unclear what the core roles ARG1 or ARG2 represent linguistically. To better understand
the roles? meaning for a given verb, one has to refer to a verb-specific file provided along
with the PropBank corpus. Although collapsing these verb-specific core roles into tags
ARG0-ARG5 leads to a small set of classes to be learned from a reasonable sized corpus,
analysis shows that the roles ARG2?ARG5 serve many different purposes for different
verbs. Yi, Loper, and Palmer (2007) point out that these four roles are highly overloaded
and inconsistent, and they mapped them to VerbNet (Schuler 2005) thematic roles to
get improvements on the SRL task. Recently, Bauer and Rambow (2011) presented
a method to improve the syntactic subcategorization patterns for FrameNet lexical
units using VerbNet. Instead of working with PropBank, we focus on shallow semantic
parsing of sentences in the paradigm of frame semantics (Fillmore 1982), to which we
turn next.
2.2 Frame-Semantic Parsing
The FrameNet lexicon (Fillmore, Johnson, and Petruck 2003) contains rich linguistic
information about lexical items and predicate-argument structures. A semantic frame
present in this lexicon includes a list of lexical units, which are associated words
and phrases that can potentially evoke it in a natural language utterance. Each frame
in the lexicon also enumerates several roles corresponding to facets of the scenario
represented by the frame. In a frame-analyzed sentence, predicates evoking frames
are known as targets, and a word or phrase filling a role is known as an argument.
Figure 2(b) shows frame-semantic annotations for the same sentence as in Figure 2(a).
(In the figure, for example, the CARDINAL NUMBERS frame, ?M? denotes the role Multiplier
and ?E? denotes the role Entity.) Note that the verbs created and pushed evoke the frames
INTENTIONALLY CREATE and CAUSE CHANGE POSITION ON A SCALE, respectively. The correspond-
ing lexical units2 from the FrameNet lexicon, create.V and push.V, are also shown.
The PropBank analysis in Figure 2(a) also has annotations for these two verbs. While
PropBank labels the roles of these verbs with its limited set of tags, the frame-
semantic parse labels each frame?s arguments with frame-specific roles shown in the
figure, making it immediately clear what those arguments mean. For example, for the
INTENTIONALLY CREATE frame, more than 1.2 million jobs is the Created entity, and In that time is
the Time when the jobs were created. FrameNet also allows non-verbal words and phrases
to evoke semantic frames: in this sentence, million evokes the frame CARDINAL NUMBERS
and doubles as its Number argument, with 1.2 as Multiplier, jobs as the Entity being quantified,
and more than as the Precision of the quantity expression.
2 See Section 5.1 for a detailed description of lexical units.
13
Computational Linguistics Volume 40, Number 1
EVENT
Place
Time
Event
TRANSITIVE_ACTION
Agent
Patient
Event
Cause
Place
TimeOBJECTIVE_INFLUENCE
Dependent_entity
uencing_situation
Place
Time
uencing_entity
CAUSE_TO_MAKE_NOISE
Agent
Sound_maker
Cause
Place
Time
MAKE_NOISE
Noisy_event
Sound
Sound_source
Place
Time
cough.v, gobble.v, 
hiss.v, ring.v, yodel.v, ...
blare.v, honk.v, play.v, 
ring.v, toot.v, ...?
affect.v, effect.n, 
impact.n, impact.v, ...
event.n, happen.v, 
occur.v, take place.v, ...
Inheritance relation Causative_of relation
Excludes relation
Purpose
Figure 3
Partial illustration of frames, roles, and lexical units related to the CAUSE TO MAKE NOISE frame,
from the FrameNet lexicon. Core roles are filled bars. Non-core roles (such as Place and Time) are
unfilled bars. No particular significance is ascribed to the ordering of a frame?s roles in its
lexicon entry (the selection and ordering of roles above is for illustrative convenience).
CAUSE TO MAKE NOISE defines a total of 14 roles, many of them not shown here.
Whereas PropBank contains verbal predicates and NomBank (Meyers et al. 2004) con-
tains nominal predicates, FrameNet counts these as well as allowing adjectives, adverbs,
and prepositions among its lexical units. Finally, FrameNet frames organize predicates
according to semantic principles, both by allowing related terms to evoke a common
frame (e.g., push.V, raise.V, and growth.N for CAUSE CHANGE POSITION ON A SCALE) and by
defining frames and their roles within a hierarchy (see Figure 3). PropBank does not
explicitly encode relationships among predicates.
Most early work on frame-semantic parsing has made use of the exemplar sentences
in the FrameNet corpus (see Section 3.1), each of which is annotated for a single frame
and its arguments. Gildea and Jurafsky (2002) presented a discriminative model for
arguments given the frame; Thompson, Levy, and Manning (2003) used a generative
model for both the frame and its arguments. Fleischman, Kwon, and Hovy (2003) first
used maximum entropy models to find and label arguments given the frame. Shi and
Mihalcea (2004) developed a rule-based system to predict frames and their arguments
in text, and Erk and Pado? (2006) introduced the Shalmaneser tool, which uses naive
Bayes classifiers to do the same. Other FrameNet SRL systems (Giuglea and Moschitti
2006, for instance) have used SVMs. Most of this work was done on an older, smaller
version of FrameNet, containing around 300 frames and fewer than 500 unique semantic
roles. Unlike this body of work, we experimented with the larger SemEval 2007 shared
task data set, and also the newer FrameNet 1.5,3 which lists 877 frames and 1,068 role
types?thus handling many more labels, and resulting in richer frame-semantic parses.
Recent work in frame-semantic parsing?in which sentences may contain multiple
frames which need to be recognized along with their arguments?was undertaken
as the SemEval 2007 task 19 of frame-semantic structure extraction (Baker, Ellsworth,
and Erk 2007). This task leveraged FrameNet 1.3, and also released a small corpus
3 Available at http://framenet.icsi.berkeley.edu as of 19 January 2013.
14
Das et al. Frame-Semantic Parsing
containing a little more than 2,000 sentences with full text annotations. The LTH system
of Johansson and Nugues (2007), which we use as our baseline (Section 3.4), had the
best performance in the SemEval 2007 task in terms of full frame-semantic parsing.
Johansson and Nugues broke down the task as identifying targets that could evoke
frames in a sentence, identifying the correct semantic frame for a target, and finally
determining the arguments that fill the semantic roles of a frame. They used a series
of SVMs to classify the frames for a given target, associating unseen lexical items to
frames and identifying and classifying token spans as various semantic roles. Both
the full text annotation corpus as well as the FrameNet exemplar sentences were
used to train their models. Unlike Johansson and Nugues, we use only the full text
annotated sentences as training data, model the whole problem with only two statis-
tical models, and obtain significantly better overall parsing scores. We also model the
argument identification problem using a joint structure prediction model and use semi-
supervised learning to improve predicate coverage. We also present experiments on
recently released FrameNet 1.5 data.
In other work based on FrameNet, Matsubayashi, Okazaki, and Tsujii (2009) in-
vestigated various uses of FrameNet?s taxonomic relations for learning generalizations
over roles; they trained a log-linear model on the SemEval 2007 data to evaluate features
for the subtask of argument identification. Another line of work has sought to extend
the coverage of FrameNet by exploiting VerbNet and WordNet (Shi and Mihalcea
2005; Giuglea and Moschitti 2006; Pennacchiotti et al. 2008) and by projecting entries
and annotations within and across languages (Boas 2002; Fung and Chen 2004; Pado
and Lapata 2005; Fu?rstenau and Lapata 2009b). Others have explored the application
of frame-semantic structures to tasks such as information extraction (Moschitti,
Morarescu, and Harabagiu 2003; Surdeanu et al. 2003), textual entailment (Burchardt
and Frank 2006; Burchardt et al. 2009), question answering (Narayanan and Harabagiu
2004; Shen and Lapata 2007), and paraphrase recognition (Pado? and Erk 2005).
2.3 Semi-Supervised Methods
Although there has been a significant amount of work in supervised shallow semantic
parsing using both PropBank- and FrameNet-style representations, a few improve-
ments over vanilla supervised methods using unlabeled data are notable. Fu?rstenau and
Lapata (2009b) present a method of projecting predicate-argument structures from some
seed examples to unlabeled sentences, and use a linear program formulation to find
the best alignment explaining the projection. Next, the projected information as well
as the seeds are used to train statistical model(s) for SRL. The authors ran experiments
using a set of randomly chosen verbs from the exemplar sentences of FrameNet and
found improvements over supervised methods. In an extension to this work, Fu?rstenau
and Lapata (2009a) present a method for finding examples for unseen verbs using a
graph alignment method; this method represents sentences and their syntactic analysis
as graphs and graph alignment is used to project annotations from seed examples to
unlabeled sentences. This alignment problem is again modeled as a linear program.
Fu?rstenau and Lapata (2012) present an detailed expansion of the aforementioned
papers. Although this line of work presents a novel direction in the area of SRL, the
published approach does not yet deal with non-verbal predicates and does not evaluate
the presented methods on the full text annotations of the FrameNet releases.
Deschacht and Moens (2009) present a technique of incorporating additional infor-
mation from unlabeled data by using a latent words language model. Latent variables
are used to model the underlying representation of words, and parameters of this model
15
Computational Linguistics Volume 40, Number 1
are estimated using standard unsupervised methods. Next, the latent information is
used as features for an SRL model. Improvements over supervised SRL techniques
are observed with the augmentation of these extra features. The authors also compare
their method with the aforementioned two methods of Fu?rstenau and Lapata (2009a,
2009b) and show relative improvements. Experiments are performed on the CoNLL
2008 shared task data set (Surdeanu et al. 2008), which follows the PropBank conven-
tions and only labels verbal and nominal predicates?in contrast to our work, which
includes most lexicosyntactic categories. A similar approach is presented by Weston,
Ratle, and Collobert (2008), who use neural embeddings of words, which are eventu-
ally used for SRL; improvements over state-of-the-art PropBank-style SRL systems are
observed.
Recently, there has been related work in unsupervised semantic role labeling (Lang
and Lapata 2010, 2011; Titov and Klementiev 2012) that attempts to induce semantic
roles automatically from unannotated data. This line of work may be useful in discov-
ering new semantic frames and roles, but here we stick to the concrete representation
provided in FrameNet, without seeking to expand its inventory of semantic types. We
present a new semi-supervised technique to expand the set of lexical items with the
potential semantic frames that they could evoke; we use a graph-based semi-supervised
learning framework to achieve this goal (Section 5.5).
2.4 Joint Inference and Shallow Semantic Parsing
Most high-performance SRL systems that use conventions from PropBank (Kingsbury
and Palmer 2002) and NomBank (Meyers et al. 2004) utilize joint inference for seman-
tic role labeling (Ma`rquez et al. 2008). To our knowledge, the separate line of work
investigating frame-semantic parsing has not previously dealt with joint inference. A
common trait in prior work, both in PropBank and FrameNet conventions, has been
the use of a two-stage model that identifies arguments first, then labels them, often
using dynamic programming or integer linear programs (ILPs); we treat both problems
together here.4
Recent work in natural language processing (NLP) problems has focused on ILP for-
mulations for complex structure prediction tasks like dependency parsing (Riedel and
Clarke 2006; Martins, Smith, and Xing 2009; Martins et al. 2010), sequence tagging (Roth
and Yih 2004), as well as PropBank SRL (Punyakanok et al. 2004). Whereas early work
in this area focused on declarative formulations tackled with off-the-shelf solvers, Rush
et al. (2010) proposed subgradient-based dual decomposition (also called Lagrangian
relaxation) as a way of exploiting the structure of the problem and existing combina-
torial algorithms. The method allows the combination of models that are individually
tractable, but not jointly tractable, by solving a relaxation of the original problem. Since
then, dual decomposition has been used to build more accurate models for dependency
parsing (Koo et al. 2010), combinatory categorical grammar supertagging and parsing
(Auli and Lopez 2011), and machine translation (Chang and Collins 2011; DeNero and
Macherey 2011; Rush and Collins 2011).
Recently, Martins et al. (2011b) showed that the success of subgradient-based dual
decomposition strongly relies on breaking down the original problem into a ?good?
4 In prior work, there are exceptions where identification and classification of arguments have been treated
in one step; for more details, please refer to the systems participating in the CoNLL-2004 shared task on
semantic role labeling (Carreras and Ma`rquez 2004).
16
Das et al. Frame-Semantic Parsing
decomposition, that is, one with few overlapping components. This leaves out many
declarative constrained problems, for which such a good decomposition is not readily
available. For those, Martins et al. proposed the Alternating Directions Dual Decom-
position (AD3) algorithm, which retains the modularity of previous methods, but can
handle thousands of small overlapping components. We adopt that algorithm as it
perfectly suits the problem of argument identification, as we observe in Section 7.5 We
also contribute an exact branch-and-bound technique wrapped around AD3.
Before delving into the details of our modeling framework, we describe in detail the
structure of the FrameNet lexicon and the data sets used to train our models.
3. Resources and Task
We consider frame-semantic parsing resources consisting of a lexicon and annotated
sentences with frame-semantic structures, evaluation strategies, and previous baselines.
3.1 FrameNet Lexicon
The FrameNet lexicon is a taxonomy of manually identified general-purpose semantic
frames for English.6 Listed in the lexicon with each frame are a set of lemmas (with
parts of speech) that can denote the frame or some aspect of it?these are called lexical
units (LUs). In a sentence, word or phrase tokens that evoke a frame are known as
targets. The set of LUs listed for a frame in FrameNet may not be exhaustive; we may
see a target in new data that does not correspond to an LU for the frame it evokes.
Each frame definition also includes a set of frame elements, or roles, corresponding
to different aspects of the concept represented by the frame, such as participants,
props, and attributes. We use the term argument to refer to a sequence of word tokens
annotated as filling a frame role. Figure 1 shows an example sentence from the training
data with annotated targets, LUs, frames, and role-argument pairs. The FrameNet
lexicon also provides information about relations between frames and between roles
(e.g., INHERITANCE). Figure 3 shows a subset of the relations between five frames and
their roles.
Accompanying most frame definitions in the FrameNet lexicon is a set of lexico-
graphic exemplar sentences (primarily from the British National Corpus) annotated
for that frame. Typically chosen to illustrate variation in argument realization pat-
terns for the frame in question, these sentences only contain annotations for a single
frame.
In preliminary experiments, we found that using exemplar sentences directly to
train our models hurt performance as evaluated on SemEval 2007 data, which formed
a benchmark for comparison with previous state of the art. This was a noteworthy
observation, given that the number of exemplar sentences is an order of magnitude
larger than the number of sentences in training data that we consider in our experiments
(Section 3.2). This is presumably because the exemplars are not representative as a
sample, do not have complete annotations, and are not from a domain similar to the
5 AD3 was previously referred to as ?DD-ADMM,? in reference to the use of dual decomposition with the
alternating directions method of multipliers.
6 Like the SemEval 2007 participants, we used FrameNet 1.3 and also the newer version of the lexicon,
FrameNet 1.5 (http://framenet.icsi.berkeley.edu).
17
Computational Linguistics Volume 40, Number 1
Table 1
Salient statistics of the data sets used in our experiments. There is a significant overlap between
the two data sets.
SemEval 2007 Data FrameNet 1.5 Release
count count
Exemplar sentences 139,439 154,607
Frame labels (types) 665 877
Role labels (types) 720 1,068
Sentences in training data 2,198 3,256
Targets in training data 11,195 19,582
Sentences in test data 120 2,420
Targets in test data 1,059 4,458
Unseen targets in test data 210 144
test data. Instead, we make use of these exemplars in the construction of features
(Section 5.2).
3.2 Data
In our experiments on frame-semantic parsing, we use two sets of data:
1. SemEval 2007 data: In benchmark experiments for comparison with previous
state of the art, we use a data set that was released as part of the SemEval 2007
shared task on frame-semantic structure extraction (Baker, Ellsworth, and Erk 2007).
Full text annotations in this data set consisted of a few thousand sentences con-
taining multiple targets, each annotated with a frame and its arguments. The then-
current version of the lexicon (FrameNet 1.3) was used for the shared task as the
inventory of frames, roles, and lexical units (Figure 3 illustrates a small portion
of the lexicon). In addition to the frame hierarchy, FrameNet 1.3 also contained
139,439 exemplar sentences containing one target each. Statistics of the data used
for the SemEval 2007 shared task are given in the first column of Table 1. A total
of 665 frame types and 720 role types appear in the exemplars and the training
portion of the data. We adopted the same training and test split as the SemEval
2007 shared task; however, we removed four documents from the training set7 for
development. Table 2 shows some additional information about the SemEval data
set; the variety of lexicosyntactic categories of targets stands in marked contrast
with the PropBank-style SRL data and task.
2. FrameNet 1.5 release: A more recent version of the FrameNet lexicon was released
in 2010.8 We also test our statistical models (only frame identification and argu-
ment identification) on this data set to get an estimate of how much improvement
additional data can provide. Details of this data set are shown in the second col-
umn of Table 1. Of the 78 documents in this release with full text annotations, we
selected 55 (19,582 targets) for training and held out the remaining 23 (4,458 tar-
gets) for testing. There are fewer target annotations per sentence in the test set than
7 These were: StephanopoulousCrimes, Iran Biological, NorthKorea Introduction, and WMDNews 042106.
8 Released on 15 September 2010, and downloadable from http://framenet.icsi.berkeley.edu as of
13 February 2013. In our experiments, we used a version downloaded on 22 September 2010.
18
Das et al. Frame-Semantic Parsing
Table 2
Breakdown of targets and arguments in the SemEval 2007 training set in terms of part of speech.
The target POS is based on the LU annotation for the frame instance. For arguments, this reflects
the part of speech of the head word (estimated from an automatic dependency parse); the
percentage is out of all overt arguments.
targets arguments
count % count %
Noun 5,155 52 Noun 9,439 55
Verb 2,785 28 Preposition or
complementizerAdjective 1,411 14 2,553 15
Preposition 296 3 Adjective 1,744 10
Adverb 103 1 Verb 1,156 7
Number 63 1 Pronoun 736 4
Conjunction 8 Adverb 373 2
Article 3 Other 1,047 6
9,824 17,048
the training set.9 Das and Smith (2011, supplementary material) give the names
of the test documents for fair replication of our work. We also randomly selected
4,462 targets from the training data for development of the argument identification
model (Section 6.1).
Preprocessing. We preprocessed sentences in our data set with a standard set of anno-
tations: POS tags from MXPOST (Ratnaparkhi 1996) and dependency parses from the
MST parser (McDonald, Crammer, and Pereira 2005); manual syntactic parses are not
available for most of the FrameNet-annotated documents. We used WordNet (Fellbaum
1998) for lemmatization. Our models treat these pieces of information as observations.
We also labeled each verb in the data as having ACTIVE or PASSIVE voice, using code
from the SRL system described by Johansson and Nugues (2008).
3.3 Task and Evaluation Methodology
Automatic annotations of frame-semantic structure can be broken into three parts:
(1) targets, the words or phrases that evoke frames; (2) the frame type, defined in the
lexicon, evoked by each target; and (3) the arguments, or spans of words that serve
to fill roles defined by each evoked frame. These correspond to the three subtasks
in our parser, each described and evaluated in turn: target identification (Section 4),
frame identification (Section 5, not unlike word-sense disambiguation), and argument
identification (Section 6, essentially the same as semantic role labeling).
The standard evaluation script from the SemEval 2007 shared task calculates pre-
cision, recall, and F1-measure for frames and arguments; it also provides a score that
gives partial credit for hypothesizing a frame related to the correct one. We present
9 For creating the splits, we first included the documents that had incomplete annotations as mentioned in
the initial FrameNet 1.5 data release in the test set; because we do not evaluate target identification for
this version of data, the small number of targets per sentence does not matter. After these documents
were put into the test set, we randomly selected 55 remaining documents for training, and picked the
rest for additional testing. The final test set contains a total of 23 documents. When these documents
are annotated in their entirety, the test set will become larger and the training set will be unaltered.
19
Computational Linguistics Volume 40, Number 1
precision, recall, and F1-measure microaveraged across the test documents, report labels-
only matching scores (spans must match exactly), and do not use named entity labels.10
More details can be found in the task description paper from SemEval 2007 (Baker,
Ellsworth, and Erk 2007). For our experiments, statistical significance is measured using
a reimplementation of Dan Bikel?s randomized parsing evaluation comparator, a strat-
ified shuffling test whose original implementation11 is accompanied by the following
description (quoted verbatim, with explanations of our use of the test given in square
brackets):
The null hypothesis is that the two models that produced the observed results are the
same, such that for each test instance [here, a set of predicate-argument structures for a
sentence], the two observed scores are equally likely. This null hypothesis is tested by
randomly shuffling individual sentences? scores between the two models and then
re-computing the evaluation metrics [precision, recall or F1 score in our case]. If the
difference in a particular metric after a shuffling is equal to or greater than the original
observed difference in that metric, then a counter for that metric is incremented. Ideally,
one would perform all 2n shuffles, where n is the number of test cases (sentences), but
given that this is often prohibitively expensive, the default number of iterations is
10,000 [we use independently sampled 10,000 shuffles]. After all iterations, the
likelihood of incorrectly rejecting the null [hypothesis, i.e., the p-value] is simply
(nc + 1)/(nt + 1), where nc is the number of random differences greater than the
original observed difference, and nt is the total number of iterations.
3.4 Baseline
A strong baseline for frame-semantic parsing is the system presented by Johansson and
Nugues (2007, hereafter J&N?07), the best system in the SemEval 2007 shared task. That
system is based on a collection of SVMs. They used a set of rules for target identification
which we describe in Appendix A. For frame identification, they used an SVM classifier
to disambiguate frames for known frame-evoking words. They used WordNet synsets
to extend the vocabulary of frame-evoking words to cover unknown words, and then
used a collection of separate SVM classifiers?one for each frame?to predict a single
evoked frame for each occurrence of a word in the extended set.
J&N?07 followed Xue and Palmer (2004) in dividing the argument identification
problem into two subtasks: First, they classified candidate spans as to whether they
were arguments or not; then they assigned roles to those that were identified as ar-
guments. Both phases used SVMs. Thus, their formulation of the problem involves
a multitude of independently trained classifiers that share no information?whereas
ours uses two log-linear models, each with a single set of parameters shared across all
contexts, to find a full frame-semantic parse.
We compare our models with J&N?07 using the benchmark data set from SemEval
2007. However, because we are not aware of any other work using the FrameNet 1.5 full
text annotations, we report our results on that data set without comparison to any other
system.
10 For microaveraging, we concatenated all sentences of the test documents and measured precision and
recall over the concatenation. Macroaveraging, on the other hand, would mean calculating these metrics
for each document, then averaging them. Microaveraging treats every frame or argument as a unit,
regardless of the length of the document in which it occurs.
11 See http://www.cis.upenn.edu/dbikel/software.html#comparator.
20
Das et al. Frame-Semantic Parsing
4. Target Identification
Target identification is the problem of deciding which word tokens (or word token
sequences) evoke frames in a given sentence. In other semantic role labeling schemes
(e.g., PropBank), simple part-of-speech criteria typically distinguish targets from non-
targets. But in frame semantics, verbs, nouns, adjectives, and even prepositions can
evoke frames under certain conditions. One complication is that semantically impov-
erished support predicates (such as make in make a request) do not evoke frames in the
context of a frame-evoking, syntactically dependent noun (request). Furthermore, only
temporal, locative, and directional senses of prepositions evoke frames.12
Preliminary experiments using a statistical method for target identification gave
unsatisfactory results; instead, we followed J&N?07 in using a small set of rules to
identify targets. First, we created a master list of all the morphological variants of
targets that appear in the exemplar sentences and a given training set. For a sentence in
new data, we considered as candidate targets only those substrings that appear in this
master list. We also did not attempt to capture discontinuous frame targets: for example,
we treat there would have been as a single span even though the corresponding LU is
there be.V.13
Next, we pruned the candidate target set by applying a series of rules identical
to the ones described by Johansson and Nugues (2007, see Appendix A), with two
exceptions. First, they identified locative, temporal, and directional prepositions using
a dependency parser so as to retain them as valid LUs. In contrast, we pruned all types
of prepositions because we found them to hurt our performance on the development
set due to errors in syntactic parsing. In a second departure from their target extraction
rules, we did not remove the candidate targets that had been tagged as support verbs
for some other target. Note that we used a conservative white list that filters out targets
whose morphological variants were not seen either in the lexicon or the training data.14
Therefore, when this conservative process of automatic target identification is used, our
system loses the capability to predict frames for completely unseen LUs, despite the fact
that our powerful frame identification model (Section 5) can accurately label frames for
new LUs.15
Results. Table 3 shows results on target identification tested on the SemEval 2007 test
set; our system gains 3 F1 points over the baseline. This is statistically significant with
p < 0.01. Our results are also significant in terms of precision (p < 0.05) and recall (p <
0.01). There are 85 distinct LUs for which the baseline fails to identify the correct target
while our system succeeds. A considerable proportion of these units have more than
12 Note that there have been dedicated shared tasks to determine relationships between nominals (Girju
et al. 2007) and word-sense disambiguation of prepositions (Litkowski and Hargraves 2007), but we do
not build specific models for predicates of these categories.
13 There are 629 multiword LUs in the lexicon, and they correspond to 4.8% of the targets in the training
set; among them are screw up.V, shoot the breeze.V, and weapon of mass destruction.N. In the SemEval 2007
training data, there are just 99 discontinuous multiword targets (1% of all targets).
14 This conservative approach violates theoretical linguistic assumptions about frame-evoking targets as
governed by frame semantics. It also goes against the spirit of using linguistic constraints to improve
the separate subtask of argument identification (see Section 7); however, due to varying distributions
of target annotations, high empirical error in identifying locative, temporal, and directional prepositions,
and support verbs, we resorted to this aggressive filtering heuristic to avoid making too many target
identification mistakes.
15 To predict frames and roles for new and unseen LUs, SEMAFOR provides the user with an option to
mark those LUs in the input.
21
Computational Linguistics Volume 40, Number 1
Table 3
Target identification results for our system and the baseline on the SemEval?07 data set. Scores in
bold denote significant improvements over the baseline (p < 0.05).
TARGET IDENTIFICATION P R F1
Our technique (?4) 89.92 70.79 79.21
Baseline: J&N?07 87.87 67.11 76.10
one token (e.g., chemical and biological weapon.N, ballistic missile.N), which J&N?07 do not
model. The baseline also does not label variants of there be.V (e.g., there are and there has
been), which we correctly label as targets. Some examples of other single token LUs that
the baseline fails to identify are names of months, LUs that belong to the ORIGIN frame
(e.g., iranian.A), and directions (e.g., north.A or north-south.A).16
5. Frame Identification
Given targets, our parser next identifies their frames, using a statistical model.
5.1 Lexical Units
FrameNet specifies a great deal of structural information both within and among
frames. For frame identification we make use of frame-evoking lexical units, the (lem-
matized and POS-tagged) words and phrases listed in the lexicon as referring to specific
frames. For example, listed with the BRAGGING frame are 10 LUs, including boast.N,
boast.V, boastful.A, brag.V, and braggart.N. Of course, due to polysemy and homonymy,
the same LU may be associated with multiple frames; for example, gobble.V is listed
under both the INGESTION and MAKE NOISE frames. We thus term gobble.V an ambiguous
LU. All targets in the exemplar sentences, our training data, and most in our test data,
correspond to known LUs. (See Section 5.4 for statistics of unknown LUs in the test sets.)
To incorporate frame-evoking expressions found in the training data but not the
lexicon?and to avoid the possibility of lemmatization errors?our frame identification
model will incorporate, via a latent variable, features based directly on exemplar and
training targets rather than LUs. Let L be the set of (unlemmatized and automati-
cally POS-tagged) targets found in the exemplar sentences of the lexicon and/or the
sentences in our training set. Let Lf ? L be the subset of these targets annotated as
evoking a particular frame f .17 Let Ll and Llf denote the lemmatized versions of L and
Lf , respectively. Then, we write boasted.VBD ? LBRAGGING and boast.VBD ? LlBRAGGINGto
indicate that this inflected verb boasted and its lemma boast have been seen to evoke the
BRAGGING frame. Significantly, however, another target, such as toot your own horn, might
be used elsewhere to evoke this frame. We thus face the additional hurdle of predicting
frames for unknown words.
16 We do not evaluate the target identification module on the FrameNet 1.5 data set; we instead ran
controlled experiments on those data to measure performance of the statistical frame identification and
argument identification subtasks, assuming that the correct targets were given. Moreover, as discussed
in Section 3.2, the target annotations on the FrameNet 1.5 test set were fewer in number in comparison
to the training set, resulting in a mismatch of target distributions between train and test settings.
17 For example, on average, there are 34 targets per frame in the SemEval 2007 data set; the average frame
ambiguity of each target in L is 1.17.
22
Das et al. Frame-Semantic Parsing
In producing full text annotations for the SemEval 2007 data set, annotators created
several domain-critical frames that were not already present in version 1.3 of the lexicon.
For our experiments we omit frames attested in neither the training data nor the exem-
plar sentences from the lexicon.18 This leaves a total of 665 frames for the SemEval 2007
data set and a total of 877 frames for the FrameNet 1.5 data set.
5.2 Model
For a given sentence x with frame-evoking targets t, let ti denote the ith target (a word
sequence).19 Let tli denote its lemma. We seek a list f = ?f1, . . . , fm? of frames, one per
target. In our model, the set of candidate frames for ti is defined to include every frame
f such that tli ? Llf ?or if tli ? Ll, then every known frame (the latter condition applies
for 4.7% of the annotated targets in the SemEval 2007 development set). In both cases,
we let Fi be the set of candidate frames for the ith target in x. We denote the entire set
of frames in the lexicon as F .
To allow frame identification for targets whose lemmas were seen in neither the
exemplars nor the training data, our model includes an additional variable, i. This
variable ranges over the seen targets in Lfi , which can be thought of as prototypes
for the expression of the frame. Importantly, frames are predicted, but prototypes are
summed over via the latent variable. The prediction rule requires a probabilistic model
over frames for a target:
fi ? argmax
f?Fi
?
?Lf
p?(f,  | ti, x) (1)
We model the probability of a frame f and the prototype unit , given the target and the
sentence x as:
p?(f,  | ti, x) =
exp?g(f, , ti, x)
?
f ??F
?
??Lf?
exp?g(f ?, ?, ti, x)
(2)
This is a conditional log-linear model: for f ? Fi and  ? Lf , where ? are the model
weights, and g is a vector-valued feature function. This discriminative formulation is
very flexible, allowing for a variety of (possibly overlapping) features; for example, a
feature might relate a frame type to a prototype, represent a lexical-semantic relation-
ship between a prototype and a target, or encode part of the syntax of the sentence.
Previous work has exploited WordNet for better coverage during frame identifica-
tion (Burchardt, Erk, and Frank 2005; Johansson and Nugues 2007, e.g., by expanding
the set of targets using synsets), and others have sought to extend the lexicon itself.
We differ in our use of a latent variable to incorporate lexical-semantic features in a
discriminative model, relating known lexical units to unknown words that may evoke
frames. Here we are able to take advantage of the large inventory of partially annotated
18 Automatically predicting new frames is a challenge not yet attempted to our knowledge (including here).
Note that the scoring metric (Section 3.3) gives partial credit for related frames (e.g., a more general frame
from the lexicon).
19 Here each ti is a word sequence ?wu, . . . , wv?, 1 ? u ? v ? n, though in principle targets can be
noncontiguous.
23
Computational Linguistics Volume 40, Number 1
Table 4
Features used for frame identification (Equation (2)). All also incorporate f , the frame being
scored.  = ?w,?? consists of the words and POS tags20 of a target seen in an exemplar or
training sentence as evoking f . The features with starred bullets were also used by Johansson
and Nugues (2007).
? the POS of the parent of the head word of ti
?? the set of syntactic dependencies of the head word21 of ti
?? if the head word of ti is a verb, then the set of dependency labels of its children
? the dependency label on the edge connecting the head of ti and its parent
? the sequence of words in the prototype, w
? the lemmatized sequence of words in the prototype
? the lemmatized sequence of words in the prototype and their part-of-speech tags ?
? WordNet relation22 ? holds between  and ti
? WordNet relation22 ? holds between  and ti, and the prototype is 
? WordNet relation22 ? holds between  and ti, the POS tag sequence of  is ?, and the POS
tag sequence of ti is ?t
exemplar sentences. Note that this model makes an independence assumption: Each
frame is predicted independently of all others in the document. In this way the model
is similar to J&N?07. However, ours is a single conditional model that shares features
and weights across all targets, frames, and prototypes, whereas the approach of J&N?07
consists of many separately trained models. Moreover, our model is unique in that it
uses a latent variable to smooth over frames for unknown or ambiguous LUs.
Frame identification features depend on the preprocessed sentence x, the prototype
 and its WordNet lexical-semantic relationship with the target ti, and of course the
frame f . Our model uses binary features, which are detailed in Table 4.
5.3 Parameter Estimation
Given a training data set (either SemEval 2007 data set or the FrameNet 1.5 full text
annotations), which is of the form ??x(j), t(j), f(j),A(j)??Nj=1, we discriminatively train the
frame identification model by maximizing the training data log-likelihood:23
max
?
N
?
j=1
mj
?
i=1
log
?
?L
f ( j)i
p?( f
(j)
i ,  | t
(j)
i , x
(j) ) (3)
In Equation (3), mj denotes the number of frames in a sentence indexed by j. Note
that the training problem is non-convex because of the summed-out prototype latent
20 POS tags are found automatically during preprocessing.
21 If the target is not a subtree in the parse, we consider the words that have parents outside the span,
and apply three heuristic rules to select the head: (1) choose the first word if it is a verb; (2) choose the
last word if the first word is an adjective; (3) if the target contains the word of, and the first word is a
noun, we choose it. If none of these hold, choose the last word with an external parent to be the head.
22 These are: IDENTICAL-WORD, SYNONYM, ANTONYM (including extended and indirect antonyms),
HYPERNYM, HYPONYM, DERIVED FORM, MORPHOLOGICAL VARIANT (e.g., plural form), VERB
GROUP, ENTAILMENT, ENTAILED-BY, SEE-ALSO, CAUSAL RELATION, and NO RELATION.
23 We found no benefit on either development data set from using an L2 regularizer (zero-mean
Gaussian prior).
24
Das et al. Frame-Semantic Parsing
Table 5
Frame identification results on both the SemEval 2007 data set and the FrameNet 1.5 release.
Precision, recall, and F1 were evaluated under exact and partial frame matching; see Section 3.3.
Bold indicates best results on the SemEval 2007 data, which are also statistically significant with
respect to the baseline (p < 0.05).
FRAME IDENTIFICATION (?5.2) exact matching partial matchingP R F1 P R F1
SemEval 2007 Data
gold targets 60.21 60.21 60.21 74.21 74.21 74.21
automatic targets (?4) 69.75 54.91 61.44 77.51 61.03 68.29
J&N?07 targets 65.34 49.91 56.59 74.30 56.74 64.34
Baseline: J&N?07 66.22 50.57 57.34 73.86 56.41 63.97
FrameNet 1.5 Release
gold targets 82.97 82.97 82.97 90.51 90.51 90.51
? unsupported features 80.30 80.30 80.30 88.91 88.91 88.91
& ? latent variable 75.54 75.54 75.54 85.92 85.92 85.92
variable  for each frame. To calculate the objective function, we need to cope with a
sum over frames and prototypes for each target (see Equation (2)), often an expensive
operation. We locally optimize the function using a distributed implementation of L-
BFGS.24 This is the most expensive model that we train: With 100 parallelized CPUs
using MapReduce (Dean and Ghemawat 2008), training takes several hours.25 Decoding
takes only a few minutes on one CPU for the test set.
5.4 Supervised Results
SemEval 2007 Data. On the SemEval 2007 data set, we evaluate the performance of
our frame identification model given gold-standard targets and automatically identified
targets (Section 4); see Table 5. Together, our target and frame identification outperform
the baseline by 4 F1 points. To compare the frame identification stage in isolation with
that of J&N?07, we ran our frame identification model with the targets identified by their
system as input. With partial matching, our model achieves a relative improvement of
0.6% F1 over J&N?07, as shown in the third row of Table 5 (though this is not significant).
Note that for exact matching, the F1 score of the automatic targets setting is better than
the gold target setting. This is due to the fact that there are many unseen predicates in
the test set on which the frame identification model performs poorly; however, for the
automatic targets that are mostly seen in the lexicon and training data, the model gets
high precision, resulting in better overall F1 score.
Our frame identification model thus performs on par with the previous state of the
art for this task, and offers several advantages over J&N?s formulation of the problem:
It requires only a single model, learns lexical-semantic features as part of that model
rather than requiring a preprocessing step to expand the vocabulary of frame-evoking
words, and is probabilistic, which can facilitate global reasoning.
24 We do not experiment with the initialization of model parameters during this non-convex optimization
process; all parameters are initialized to 0.0 before running the optimizer. However, in future work,
experiments can be conducted with different random initialization points to seek non-local optima.
25 In later experiments, we used another implementation with 128 parallel cores in a multi-core MPI setup
(Gropp, Lusk, and Skjellum 1994), where training took several hours.
25
Computational Linguistics Volume 40, Number 1
In the SemEval 2007 data set, for gold-standard targets, 210 out of 1,059 lemmas
were not present in the white list that we used for target identification (see Section 4).
Our model correctly identifies the frames for 4 of these 210 lemmas. For 44 of these
lemmas, the evaluation script assigns a score of 0.5 or more, suggesting that our model
predicts a closely related frame. Finally, for 190 of the 210 lemmas, a positive score is
assigned by the evaluation script. This suggests that the hidden variable model helps
in identifying related (but rarely exact) frames for unseen targets, and explains why
under exact?but not partial?frame matching, the F1 score using automatic targets is
commensurate with the score for oracle targets.26
For automatically identified targets, the F1 score falls because the model fails
to predict frames for unseen lemmas. However, our model outperforms J&N?07 by
4 F1 points. The partial frame matching F1 score of our model represents a significant
improvement over the baseline (p < 0.01). The precision and recall measures are
significant as well (p < 0.05 and p < 0.01, respectively). However, because targets
identified by J&N?07 and frames classified by our frame identification model resulted
in scores on par with the baseline, we note that the significant results follow due to
better target identification. Note from the results that the automatic target identification
model shows an increase in precision, at the expense of recall. This is because the white
list for target identification restricts the model to predict frames only for known LUs.
If we label the subset of test set with already seen LUs (seen only in the training set,
excluding the exemplars) with their corresponding most frequent frame, we achieve
an exact match accuracy between 52.9% and 91.2%, depending on the accuracy of the
unseen LUs (these bounds assume, respectively, that they are all incorrectly labeled or
all correctly labeled).
FrameNet 1.5 Release. The bottom three rows of Table 5 show results on the full text
annotation test set of the FrameNet 1.5 release. Because the number of annotations
nearly doubled, we see large improvements in frame identification accuracy. Note that
we only evaluate with gold targets as input to frame identification. (As mentioned in
Section 3.2, some documents in the test set have not been annotated for all targets, so
evaluating automatic target identification would not be informative.) We found that
50.1% of the targets in the test set were ambiguous (i.e., they associated with more than
one frame either in FrameNet or our training data). On these targets, the exact frame
identification accuracy is 73.10% and the partial accuracy is 85.77%, which indicates that
the frame identification model is robust to target ambiguity. On this data set, the most
frequent frame baseline achieves an exact match accuracy between 74.0% and 88.1%,
depending on the accuracy of the unseen LUs.
We conducted further experiments with ablation of the latent variable in our frame
identification model. Recall that the decoding objective used to choose a frame by
marginalizing over a latent variable , whose values range over targets known to
associate with the frame f being considered (see Equations (1) and (2)) in training. How
much do the prototypes, captured by the latent variable, contribute to performance?
Instead of treating  as a marginalized latent variable, we can fix its value to the observed
target.
26 J&N?07 did not report frame identification results for oracle targets; thus directly comparing the frame
identification models is difficult.
26
Das et al. Frame-Semantic Parsing
An immediate effect of this choice is a blow-up in the number of features; we
must instantiate features (see Table 4) for all 4,194 unique targets observed in training.
Because each of these features needs to be associated with all 877 frames in the partition
function of Equation (2), the result is an 80-fold blowup of the feature space (the latent
variable model had 465,317 features). Such a model is not computationally feasible in
our engineering framework, so we considered a model using only features observed to
fire at some point in the training data (called ?supported? features),27 resulting in only
72,058 supported features. In Table 5, we see a significant performance drop (on both
exact and partial matching accuracy) with this latent variable?free model, compared
both with our latent variable model with all features and with only supported features
(of which there are 165,200). This establishes that the latent variable in our frame
identification model helps in terms of accuracy, and lets us use a moderately sized
feature set incorporating helpful unsupported features.
Finally, in our test set, we found that 144 out of the 4,458 annotated targets were un-
seen, and our full frame identification model only labeled 23.1% of the frames correctly
for those unseen targets; in terms of partial match accuracy, the model achieved a score
of 46.6%. This, along with the results on the SemEval 2007 unseen targets, shows that
there is substantial opportunity for improvement when unseen targets are presented to
the system. We address this issue next.
5.5 Semi-Supervised Lexicon Expansion
We next address the poor performance of our frame identification model on targets that
were unseen as LUs in FrameNet or as instances in training data, and briefly describe
a technique for expanding the set of lexical units with potential semantic frames that
they can associate with. These experiments were carried out on the FrameNet 1.5 data
only. We use a semi-supervised learning (SSL) technique that uses a graph constructed
from labeled and unlabeled data. The widely used graph-based SSL framework?see
Bengio, Delalleau, and Le Roux (2006) and Zhu (2008) for introductory material on this
topic?has been shown to perform better than several other semi-supervised algorithms
on benchmark data sets (Chapelle, Scho?lkopf, and Zien 2006, chapter 21). The method
constructs a graph where a small portion of vertices correspond to labeled instances,
and the rest are unlabeled. Pairs of vertices are connected by weighted edges denoting
the similarity between the pair. Traditionally, Markov random walks (Szummer and
Jaakkola 2001; Baluja et al. 2008) or optimization of a loss function based on smoothness
properties of the graph (e.g., Corduneanu and Jaakkola 2003; Zhu, Ghahramani, and
Lafferty 2003; Subramanya and Bilmes 2008) are performed to propagate labels from
the labeled vertices to the unlabeled ones. In our work, we are interested in multi-class
generalizations of graph-propagation algorithms suitable for NLP applications, where
each graph vertex can assume one or more out of many possible labels (Subramanya and
Bilmes 2008, 2009; Talukdar and Crammer 2009). For us, graph vertices correspond to
natural language types (not tokens) and undirected edges between them are weighted
using a similarity metric. Recently, this set-up has been used to learn soft labels on
natural language types (say, word n-grams or in our case, syntactically disambiguated
27 The use of unsupported features (i.e., those that can fire for an analysis in the partition function but not
observed to fire in the training data) has been observed to give performance improvements in NLP
problems; see, for example, Sha and Pereira (2003) and Martins et al. (2010).
27
Computational Linguistics Volume 40, Number 1
predicates) from seed data, resulting in large but noisy lexicons, which are used to
constrain structured prediction models. Applications have ranged from domain adap-
tation of sequence models (Subramanya, Petrov, and Pereira 2010) to unsupervised
learning of POS taggers by using bilingual graph-based projections (Das and Petrov
2011).
We describe our approach to graph construction, propagation for lexicon expansion,
and the use of the result to impose constraints on frame identification.
5.5.1 Graph Construction. We construct a graph with lexical units as vertices. Thus, each
vertex corresponds to a lemmatized word or phrase appended with a coarse POS tag.
We use two resources for graph construction. First, we take all the words and phrases
present in a dependency-based thesaurus constructed using syntactic cooccurrence
statistics (Lin 1998), and aggregate words and phrases that share the same lemma and
coarse POS tag. To construct this resource, Lin used a corpus containing 64 million
words that was parsed with a fast dependency parser (Lin 1993, 1994), and syntactic
contexts were used to find similar lexical items for a given word or phrase. Lin sepa-
rately treated nouns, verbs, and adjectives/adverbs, so these form the three parts of the
thesaurus. This resource gave us a list of possible LUs, much larger in size than the LUs
present in FrameNet data.
The second component of graph construction comes from FrameNet itself. We
scanned the exemplar sentences in FrameNet 1.5 and the training section of the full
text annotations and gathered a distribution over frames for each LU appearing in
FrameNet data. For a pair of LUs, we measured the Euclidean distance between their
frame distributions. This distance was next converted to a similarity score and inter-
polated with the similarity score from Lin?s dependency thesaurus. We omit further
details about the interpolation and refer the reader to full details given in Das and Smith
(2011).
For each LU, we create a vertex and link it to the K nearest neighbor LUs under the
interpolated similarity metric. The resulting graph has 64,480 vertices, 9,263 of which
are labeled seeds from FrameNet 1.5 and 55,217 of which are unlabeled. Each vertex has
a possible set of labels corresponding to the 877 frames defined in the lexicon. Figure 4
shows an excerpt from the constructed graph.
Figure 4
Excerpt from our constructed graph over LUs. Green LUs are observed in the FrameNet 1.5 data.
Above/below them are shown the most frequently observed frame that these LUs associate
with. The black LUs are unobserved and graph propagation produces a distribution over most
likely frames that they could evoke as target instances.
28
Das et al. Frame-Semantic Parsing
5.5.2 Propagation by Optimization. Once the graph is constructed, the 9,263 seed ver-
tices with supervised frame distributions are used to propagate the semantic frame
information via their nearest neighbors to all vertices. Here we discuss two graph-
based SSL objective functions. Das and Smith (2012) compare several other graph-based
SSL algorithms for this problem; we refer the interested reader to that paper. Let V
denote the set of all vertices in our graph, V? ? V be the set of seed vertices, and F
denote the set of all frames. Let N (v) denote the set of neighbors of vertex v ? V. Let
q = {q1, q2, . . . , q|V|} be the set of frame distributions, one per vertex. For each seed
vertex v ? V?, we have a supervised frame distribution q?v. All edges in the graph are
weighted according to the aforementioned interpolated similarity score, denoted wuv
for the edge adjacent to vertices u and v. We find q by solving:
NGF-2 : arg min
q, s.t. q?0,
?v?V,?qv?1=1
?
v?V?
?q?v ? qv?22 + ?
?
v?V,u?N (v)
wuv?qv ? qu?22 + ?
?
v?V
?qv ? 1|F|?22
(4)
We call the objective in Equation (4) NGF-2 because it uses normalized probability dis-
tributions at each vertex and is a Gaussian field; it also utilizes a uniform 2 penalty?the
third term in the objective function. This is a multiclass generalization of the quadratic
cost criterion (Bengio, Delalleau, and Le Roux 2006), also used by Subramanya, Petrov,
and Pereira (2010) and Das and Petrov (2011). Our second graph objective function is as
follows:
UJSF-1,2 : arg min
q, s.t. q?0
?
v?V?
DJS(q?v?qv) + ?
?
v?V,u?N (v)
wuvDJS(qv?qu) + ?
?
v?V
?qv?21 (5)
We call it UJSF-1,2 because it uses unnormalized probability measures at each vertex
and is a Jensen-Shannon field, utilizing pairwise Jensen-Shannon divergences (Lin 1991;
Burbea and Rao 2006) and a sparse 1,2 penalty (Kowalski and Torre?sani 2009) as the
third term. Das and Smith (2012) proposed the objective function in Equation (5). It seeks
at each graph vertex a sparse measure, as we expect in a lexicon (i.e., few frames have
nonzero probability for a given target). These two graph objectives can be optimized
by iterative updates, whose details we omit in this article; more information about the
motivation behind using the 1,2 penalty in the UJSF-1,2 objective, the optimization
procedure, and an empirical comparison of these and other objectives on another NLP
task can be found in Das and Smith (2012).
5.5.3 Constraints for Frame Identification. Once a graph-based SSL objective function is
minimized, we arrive at the optimal set of frame distributions q?, which we use to
constrain our frame identification inference rule, expressed in Equation (1). In that
rule, ti is the ith target in a sentence x, and fi is the corresponding evoked frame. We
now add a constraint to that rule. Recall from Section 5.2 that for targets with known
lemmatized forms, Fi was defined to be the set of frames that associate with lemma tli
in the supervised data. For unknown lemmas, Fi was defined to be all the frames in the
lexicon. If the LU corresponding to ti is present in the graph, let it be the vertex vi. For
such targets ti covered by the graph, we redefine Fi as:
Fi = {f : f ? M-best frames under q?vi} (6)
29
Computational Linguistics Volume 40, Number 1
Table 6
Exact and partial frame identification accuracy on the FrameNet 1.5 data set with the size of
lexicon (in terms of non-zero frame components in the truncated frame distributions) used for
frame identification, given gold targets. The supervised model is compared with alternatives in
Table 5. Bold indicates best results. UJSF-1,2 produces statistically significant results (p < 0.001)
for all metrics with respect to the supervised baseline for both the unseen LUs as well as the
whole test set. Although the NGF-2 and UJSF-1,2 models are statistically indistinguishable,
it is noteworthy that the UJSF-1,2 objective produces a much smaller lexicon.
UNKNOWN TARGETS ALL TARGETS Graph
exact partial exact partial Lexicon
frame matching frame matching frame matching frame matching Size
Supervised 23.08 46.62 82.97 90.51 ?
Self-training 18.88 42.67 82.27 90.02 ?
NGF-2 39.86 62.35 83.51 91.02 128,960
UJSF-1,2 42.67 65.29 83.60 91.12 45,544
For targets ti in test data whose LUs are not present in the graph (and hence in
supervised data), Fi is the set of all frames. Note that in this semi-supervised extension
of our frame identification inference procedure, we introduced several hyperparam-
eters, namely, ?, ?, K (the number of nearest neighbors for each vertex included in
the graph) and M (the number of highest scoring frames per vertex according to the
induced frame distribution). We choose these hyperparameters using cross-validation
by tuning the frame identification accuracy on unseen targets. (Different values of the
first three hyperparameters were chosen for the different graph objectives and we omit
their values here for brevity; M turned out to be 2 for all models.)
Table 6 shows frame identification accuracy, both using exact match as well as
partial match. Performance is shown on the portion of the test set containing unknown
LUs, as well as the whole test set. The final column presents lexicon size in terms
of the set of truncated frame distributions (filtered according to the top M frames in
qv for a vertex v) for all the LUs in a graph. For comparison with a semi-supervised
baseline, we consider a self-trained system. For this system, we used the supervised
frame identification system to label 70,000 sentences from the English Gigaword corpus
with frame-semantic parses. For finding targets in a raw sentence, we used a relaxed
target identification scheme, where we marked as potential frame-evoking units all
targets seen in the lexicon and all other words which were not prepositions, particles,
proper nouns, foreign words, or WH-words. We appended these automatic annotations
to the training data, resulting in 711,401 frame annotations, more than 36 times the
annotated data. These data were next used to train a frame identification model.28 This
set-up is very similar to that of Bejan (2009) who used self-training to improve frame
identification. In our setting, however, self-training hurts relative to the fully supervised
approach (Table 6).
Note that for the unknown part of the test set the graph-based objectives outperform
both the supervised model as well as the self-training baseline by a margin of ?20%
28 We ran self-training with smaller amounts of data, but found no significant difference with the results
achieved with 711,401 frame annotations. As we observe in Table 6, in our case, self-training performs
worse than the supervised model, and we do not hope to improve with even more data.
30
Das et al. Frame-Semantic Parsing
absolute. The best model is UJSF-1,2, and its performance is significantly better than
the supervised model (p < 0.01). It also produces a smaller lexicon (using the sparsity-
inducing penalty) than NGF-2, requiring less memory during frame identification
inference. The small footprint can be attributed to the removal of LUs for which all
frame components were zero (qi = 0). The improvements of the graph-based objectives
over the supervised and the self-trained models are modest for the whole test set,
but the best model still has statistically significant improvements over the supervised
model (p < 0.01).
6. Argument Identification
Given a sentence x = ?x1, . . . , xn?, the set of targets t = ?t1, . . . , tm?, and a list of evoked
frames f = ?f1, . . . , fm? corresponding to each target, argument identification is the task
of choosing which of each fi?s roles are filled, and by which parts of x. This task is most
similar to the problem of semantic role labeling, but uses a richer set of frame-specific
labels than PropBank annotations.
6.1 Model
Let Rfi = {r1, . . . , r|Rfi |} denote frame fi?s roles (named frame element types) observed
in an exemplar sentence and/or our training set. A subset of each frame?s roles are
marked as core roles; these roles are conceptually and/or syntactically necessary for
any given use of the frame, though they need not be overt in every sentence involving
the frame. These are roughly analogous to the core arguments ARG0?ARG5 in PropBank.
Non-core roles?analogous to the various ARGM-* in PropBank?loosely correspond to
syntactic adjuncts, and carry broadly applicable information such as the time, place,
or purpose of an event. The lexicon imposes some additional structure on roles, in-
cluding relations to other roles in the same or related frames, and semantic types with
respect to a small ontology (marking, for instance, that the entity filling the protag-
onist role must be sentient for frames of cognition). Figure 3 illustrates some of the
structural elements comprising the frame lexicon by considering the CAUSE TO MAKE NOISE
frame.
We identify a set S of spans that are candidates for filling any role r ? Rfi . In
principle, S could contain any subsequence of x, but in this work we only consider
the set of contiguous spans that (a) contain a single word or (b) comprise a valid subtree
of a word and all its descendants in the dependency parse produced by the MST parser.
This covers approximately 80% of arguments in the development data for both data
sets.
The empty span, denoted ?, is also included in S , since some roles are not explicitly
filled; in the SemEval 2007 development data, the average number of roles an evoked
frame defines is 6.7, but the average number of overt arguments is only 1.7.29 In
29 In the annotated data, each core role is filled with one of three types of null instantiations indicating how
the role is conveyed implicitly. For instance, the imperative construction implicitly designates a role as
filled by the addressee, and the corresponding filler is thus CNI (constructional null instantiation). In this
work we do not distinguish different types of null instantiation. The interested reader may refer to Chen
et al. (2010), who handle the different types of null instantions during argument identification.
31
Computational Linguistics Volume 40, Number 1
training, if a labeled argument is not a subtree of the dependency parse, we add its
span to S .30
Let Ai denote the mapping of roles in Rfi to spans in S . Our model makes a
prediction for each Ai(rk) (for all roles rk ? Rfi ) using:
Ai(rk) ? argmax
s?S
p?(s | rk, fi, ti, x) (7)
We use a conditional log-linear model over spans for each role of each evoked frame:
p?(Ai(rk) = s | fi, ti, x) =
exp?h(s, rk, fi, ti, x)
?
s??S
exp?h(s?, rk, fi, ti, x)
(8)
Note that our model chooses the span for each role separately from the other roles
and ignores all frames except the frame the role belongs to. Our model departs
from the traditional SRL literature by modeling the argument identification problem
in a single stage, rather than first classifying token spans as arguments and then
labeling them. A constraint implicit in our formulation restricts each role to have at
most one overt argument, which is consistent with 96.5% of the role instances in the
SemEval 2007 training data and 96.4% of the role instances in the FrameNet 1.5 full text
annotations.
Out of the overt argument spans in the training data, 12% are duplicates, having
been used by some previous frame in the sentence (supposing some arbitrary ordering
of frames). Our role-filling model, unlike a sentence-global argument detection-and-
classification approach,31 permits this sort of argument sharing among frames. Word
tokens belong to an average of 1.6 argument spans, including the quarter of words that
do not belong to any argument.
Appending together the local inference decisions from Equation (7) gives us the best
mapping A?t for target t. Features for our log-linear model (Equation (8)) depend on the
preprocessed sentence x; the target t; a role r of frame f ; and a candidate argument span
s ? S .32 For features using the head word of the target t or a candidate argument span
s, we use the heuristic described in footnote 21 for selecting the head of non-subtree
spans.
Table 7 lists the feature templates used in our model. Every feature template has
a version that does not take into account the role being filled (so as to incorporate
overall biases). The  symbol indicates that the feature template also has a variant that
is conjoined with r, the name of the role being filled; and  indicates that the feature
30 Here is an example in the FrameNet 1.5 training data where this occurs. In the sentence: As capital of
Europe?s most explosive economy, Dublin seems to be changing before your very eyes, the word economy
evokes the ECONOMY frame with the phrase most explosive fulfilling the Descriptor role. However,
in the dependency parse for the sentence the phrase is not a subtree because both words in the frame
attach to the word economy. Future work may consider better heuristics to select potential arguments
from the dependency parses to recover more gold arguments than what the current work achieves.
31 J&N?07, like us, identify arguments for each target.
32 In this section we use t, f , and r without subscripts because the features only consider a single role of a
single target?s frame.
32
Das et al. Frame-Semantic Parsing
Table 7
Features used for argument identification. Section 6.1 describes the meanings of the different
circles attached to each feature.
Features with both null and non-null variants: These features come in two flavors:
if the argument is null, then one version fires; if it is overt (non-null), then another
version fires.
 some word in t has lemma ?  some word in t has POS ?
 some word in t has lemma ?, and the
sentence uses PASSIVE voice
 some word in t has lemma ?, and the
sentence uses ACTIVE voice
 the head of t has subcategorization
sequence ? = ??1, ?2, . . . ?
 some syntactic dependent of the head of t
has dependency type ?
 the head of t has c syntactic dependents  bias feature (always fires)
Span content features: apply to overt argument candidates.
 POS tag ? occurs for some word in s  the head word of s has POS ?
 the first word of s has POS ?  |s|, the number of words in the span
 the last word of s has POS ?  the first word of s has lemma ?
 the head word of s has syntactic
dependency type ?
 the first word of s: ws1 , and its POS tag ?s1 ,
if ?s1 is a closed-class POS
 ws2 and its closed-class POS tag ?s2 ,
provided that |s| ? 2
 the syntactic dependency type ?s1 of the
first word with respect to its head
 the head word of s has lemma ?  ?s2 , provided that |s| ? 2
 the last word of s: ws|s| has lemma ?  ?s|s| , provided that |s| ? 3
 ws|s| , and its closed-class POS tag ?s|s| ,
provided that |s| ? 3
 lemma ? is realized in some word in s
 lemma ? is realized in some word in s, the
voice denoted in the span, s?s position
with respect to t (BEFORE, AFTER, or
OVERLAPPING)
 lemma ? is realized in some word in s,
the voice denoted in the span (ACTIVE
or PASSIVE)
Syntactic features: apply to overt argument candidates.
 dependency path: sequence of labeled,
directed edges from the head word of s to
the head word of t
 length of the dependency path
Span context POS features: for overt candidates, up to 6 of these features will be active.
 a word with POS ? occurs up to 3 words
before the first word of s
 a word with POS ? occurs up to 3 words
after the last word of s
Ordering features: apply to overt argument candidates.
 the position of s with respect to the span
of t: BEFORE, AFTER, or OVERLAPPING (i.e.
there is at least one word shared by s and t)
 target-argument crossing: there is at least
one word shared by s and t, at least one
word in s that is not in t, and at least one
word in t that is not in s
 linear word distance between the nearest
word of s and the nearest word of t,
provided s and t do not overlap
 linear word distance between the middle
word of s and the middle word of t,
provided s and t do not overlap
template additionally has a variant that is conjoined with both r and f , the name of the
frame.33 The role-name-only variants provide for smoothing over frames for common
types of roles such as Time and Place; see Matsubayashi, Okazaki, and Tsujii (2009) for
a detailed analysis of the effects of using role features at varying levels of granularity.
Certain features in our model rely on closed-class POS tags, which are defined to be
all Penn Treebank tags except for CD and tags that start with V, N, J, or R. Finally, the
33 That is, the  symbol subsumes , which in turn subsumes .
33
Computational Linguistics Volume 40, Number 1
features that encode a count or a number are binned into groups: (??,?20], [?19,?10],
[?9,?5], ?4, ?3, ?2, ?1, 0, 1, 2, 3, 4, [5, 9], [10, 19], [20,?).
6.2 Parameter Estimation
We train the argument identification model by:
max
?
N
?
j=1
mj
?
i=1
|R
f (j)i
|
?
k=1
log p?(A
(j)
i (rk) | f
(j)
i , t
(j)
i , x
(j) ) ? C ???22 (9)
Here, N is the number of data points (sentences) in the training set, and m is the number
of frame annotations per sentence. This objective function is concave. For experiments
with the SemEval 2007 data, we trained the model using stochastic gradient ascent
(Bottou 2004) with no Gaussian regularization (C = 0).34 Early stopping was done by
tuning on the development set, and the best results were obtained with a batch size of 2
and 23 passes through the data.
On the FrameNet 1.5 release, we trained this model using L-BFGS (Liu and Nocedal
1989) and ran it for 1,000 iterations. C was tuned on the development data, and we
obtained best results for C = 1.0. We did not use stochastic gradient descent for this
data set as the number of training instances increased and parallelization of L-BFGS
on a multicore setup implementing MPI (Gropp, Lusk, and Skjellum 1994) gave faster
training speeds.
6.3 Decoding with Beam Search
Naive prediction of roles using Equation (7) may result in overlap among arguments
filling different roles of a frame, because the argument identification model fills each role
independently of the others. We want to enforce the constraint that two roles of a single
frame cannot be filled by overlapping spans.35 Toutanova, Haghighi, and Manning
(2005) presented a dynamic programming algorithm to prevent overlapping arguments
for SRL; however, their approach used an orthogonal view to the argument identi-
fication stage, wherein they labeled phrase-structure tree constituents with semantic
roles. That formulation admitted a dynamic programming approach; our formulation
of finding the best argument span for each role does not.
To eliminate illegal overlap, we adopt the beam search technique detailed in
Algorithm 1. The algorithm produces a set of k-best hypotheses for a frame instance?s
full set of role-span pairs, but uses an approximation in order to avoid scoring an
exponential number of hypotheses. After determining which roles are most likely not
explicitly filled, it considers each of the other roles in turn: In each iteration, hypotheses
incorporating a subset of roles are extended with high-scoring spans for the next role,
always maintaining k alternatives. We set k=10,000 as the beam width.36
34 This was the setting used by Das et al. (2010) and we kept it unchanged.
35 On rare occasions a frame annotation may include a secondary frame element layer, allowing arguments to
be shared among multiple roles in the frame; see Ruppenhofer et al. (2006) for details. The evaluation for
this task only considers the primary layer, which is guaranteed to have disjoint arguments.
36 We show the effect of varying beam widths in Table 9, where we present performance of an exact
algorithm for argument identification.
34
Das et al. Frame-Semantic Parsing
Algorithm 1 Joint decoding of frame fi?s arguments via beam search. topk(S , p?, rj)
extracts the k most probable spans from S , under p?, for role rj. extend(D0:(j?1),S ?)
extends each span vector in D0:(j?1) with the most probable non-overlapping span from
S ?, resulting in k best extensions overall.
Require: k > 0, Rfi , S , the distribution p? from Equation 8 for each role rj ? Rfi
Ensure: A?i, a high-scoring mapping of roles of fi to spans with no token overlap among
the spans
1: Calculate Ai according to Equation 7
2: ?r ? Rfi such that Ai(r) = ?, let A?i(r) ? ?
3: R+fi ? {r : r ? Rfi ,Ai(r) = ?}
4: n ? |R+fi |
5: Arbitrarily order R+fi as {r1, r2, . . . rn}
6: Let D0:j = ?D0:j1 , . . . , D
0:j
k ? refer to the k-best list of vectors of compatible filler spans
for roles r1 through rj
7: Initialize D0:0 to be empty
8: for j = 1 to n do
9: D0:j ? extend(D0:(j?1), topk(S , p?, rj))
10: end for
11: ?j ? {1, . . . , n}, A?i(rj) ? D0:n1 [j]
12: return A?i
6.4 Results
Performance of the argument identification model is presented in Table 8 for both data
sets in consideration. We analyze them here.
SemEval 2007 Data: For the SemEval data set, the table shows how performance
varies given different types of input: correct targets and correct frames, correct targets
but automatically identified frames, and ultimately, no oracle input (the full frame
parsing scenario). Rows 1?2 isolate the argument identification task from the frame
identification task. Given gold targets and frames, our argument identification model
(without beam search) gets an F1 score of 68.09%; when beam search is applied, this
increases to 68.46%, with a noticeable increase in precision. Note that an estimated 19%
of correct arguments are excluded because they are neither single words nor complete
subtrees (see Section 6.1) of the automatic dependency parses.37
Qualitatively, the problem of candidate span recall seems to be largely due to
syntactic parse errors.38 Although our performance is limited by errors when using
the syntactic parse to determine candidate spans, it could still improve; this suggests
37 We found that using all constituents from the 10-best syntactic parses would improve oracle recall of
spans in the development set by just a couple of percentage points, at the computational cost of a larger
pool of candidate arguments per role.
38 Note that, because of our labels-only evaluation scheme (Section 3.3), arguments missing a word or
containing an extra word receive no credit. In fact, of the frame roles correctly predicted as having an
overt span, the correct span was predicted 66% of the time, while 10% of the time the predicted starting
and ending boundaries of the span were off by a total of one or two words.
35
Computational Linguistics Volume 40, Number 1
Ta
b
le
8
A
rg
u
m
en
ti
d
en
ti
fi
ca
ti
on
re
su
lt
s
on
bo
th
th
e
Se
m
E
va
l?0
7
d
at
a
as
w
el
la
s
th
e
fu
ll
te
xt
an
no
ta
ti
on
s
of
Fr
am
eN
et
1.
5.
Fo
r
d
ec
od
in
g,
be
am
an
d
na
iv
e
in
d
ic
at
e
w
he
th
er
th
e
ap
p
ro
xi
m
at
e
jo
in
td
ec
od
in
g
al
go
ri
th
m
ha
s
be
en
u
se
d
or
lo
ca
li
nd
ep
en
d
en
td
ec
is
io
ns
ha
ve
be
en
m
ad
e
fo
r
ar
gu
m
en
ti
d
en
ti
fi
ca
ti
on
,
re
sp
ec
ti
ve
ly
.O
n
th
e
Se
m
E
va
l2
00
7
d
at
a,
fo
r
fu
ll
p
ar
si
ng
(a
u
to
m
at
ic
ta
rg
et
,f
ra
m
e,
an
d
ar
gu
m
en
ti
d
en
ti
fi
ca
ti
on
),
b
ol
d
sc
or
es
in
d
ic
at
e
be
st
re
su
lt
s,
w
hi
ch
ar
e
al
so
si
gn
ifi
ca
nt
im
p
ro
ve
m
en
ts
re
la
ti
ve
to
th
e
ba
se
lin
e
(p
<
0.
05
).
O
n
th
e
Fr
am
eN
et
1.
5
d
at
a
se
t,
b
ol
d
sc
or
es
in
d
ic
at
e
be
st
re
su
lt
s
on
au
to
m
at
ic
fr
am
e
an
d
ar
gu
m
en
ti
d
en
ti
fi
ca
ti
on
?
th
is
is
ac
hi
ev
ed
by
th
e
fr
am
e
id
en
ti
fi
ca
ti
on
m
od
el
th
at
u
se
s
th
e
U
JS
F-
 1
,2
gr
ap
h-
ob
je
ct
iv
e
an
d
au
to
m
at
ic
ar
gu
m
en
ti
d
en
ti
fi
ca
ti
on
u
si
ng
be
am
se
ar
ch
.T
hi
s
re
su
lt
is
st
at
is
ti
ca
lly
si
gn
ifi
ca
nt
ov
er
th
e
su
p
er
vi
se
d
re
su
lt
s
sh
ow
n
in
ro
w
9
(p
<
0.
00
1)
.I
n
te
rm
s
of
p
re
ci
si
on
an
d
F 1
sc
or
e
m
ea
su
re
d
w
it
h
p
ar
ti
al
fr
am
e
m
at
ch
in
g,
th
e
re
su
lt
s
w
it
h
th
e
U
JS
F-
 1
,2
m
od
el
is
st
at
is
ti
ca
lly
si
gn
ifi
ca
nt
ov
er
th
e
N
G
F-
 2
m
od
el
(p
<
0.
05
).
Fo
r
re
ca
ll
w
it
h
p
ar
ti
al
fr
am
e
m
at
ch
in
g,
an
d
fo
r
al
lt
he
th
re
e
m
et
ri
cs
w
it
h
ex
ac
tf
ra
m
e
m
at
ch
in
g,
th
e
re
su
lt
s
w
it
h
th
e
tw
o
gr
ap
h
ob
je
ct
iv
es
ar
e
st
at
is
ti
ca
lly
in
d
is
ti
ng
u
is
ha
bl
e.
N
ot
e
th
at
ce
rt
ai
n
p
ar
ti
al
m
at
ch
re
su
lt
s
ar
e
m
is
si
ng
be
ca
u
se
in
th
os
e
se
tt
in
gs
,g
ol
d
fr
am
es
ha
ve
be
en
u
se
d
fo
r
ar
gu
m
en
ti
d
en
ti
fi
ca
ti
on
.
A
R
G
U
M
E
N
T
ID
E
N
T
IF
IC
A
T
IO
N
ta
rg
et
s
fr
am
es
de
co
di
ng
ex
ac
tm
at
ch
in
g
p
ar
ti
al
m
at
ch
in
g
P
R
F 1
P
R
F 1
S
em
E
va
l?
07
D
at
a
A
rg
u
m
en
t
id
en
ti
fi
ca
ti
on
(f
u
ll)
go
ld
go
ld
na
iv
e
77
.4
3
60
.7
6
68
.0
9
1
go
ld
go
ld
be
am
78
.7
1
60
.5
7
68
.4
6
2
P
ar
si
ng
(o
ra
cl
e
ta
rg
et
s)
go
ld
su
p
er
vi
se
d
(?
5.
2)
be
am
49
.6
8
42
.8
2
46
.0
0
57
.8
5
49
.8
6
53
.5
6
3
P
ar
si
ng
(f
u
ll)
au
to
su
p
er
vi
se
d
(?
5.
2)
be
am
58
.0
8
38
.7
6
46
.4
9
62
.7
6
41
.8
9
50
.2
4
4
P
ar
si
ng
(J
&
N
?0
7
ta
rg
et
s
an
d
fr
am
es
)
au
to
su
p
er
vi
se
d
(?
3.
4)
be
am
56
.2
6
36
.6
3
44
.3
7
60
.9
8
39
.7
0
48
.0
9
5
B
as
el
in
e:
J&
N
?0
7
au
to
su
p
er
vi
se
d
(?
3.
4)
N
/
A
51
.5
9
35
.4
4
42
.0
1
56
.0
1
38
.4
8
45
.6
2
6
Fr
am
eN
et
1.
5
R
el
ea
se
A
rg
u
m
en
t
id
en
ti
fi
ca
ti
on
(f
u
ll)
go
ld
go
ld
na
iv
e
82
.0
0
76
.3
6
79
.0
8
7
go
ld
go
ld
be
am
83
.8
3
76
.2
8
79
.8
8
8
P
ar
si
ng
(o
ra
cl
e
ta
rg
et
s)
go
ld
su
p
er
vi
se
d
(?
5.
2)
be
am
67
.8
1
60
.6
8
64
.0
5
72
.4
7
64
.8
5
68
.4
5
9
go
ld
SS
L
(N
G
F-
 2
,?
5.
5)
be
am
68
.2
2
61
.0
4
64
.4
3
72
.8
7
65
.2
0
68
.8
2
10
go
ld
SS
L
(U
JS
F-
 1
,2
,?
5.
5)
be
am
68
.3
3
61
.1
4
64
.5
4
72
.9
8
65
.3
0
68
.9
3
11
36
Das et al. Frame-Semantic Parsing
that the model has trouble discriminating between good and bad arguments, and that
additional feature engineering or jointly decoding arguments of a sentence?s frames
may be beneficial.
Rows 3?4 show the effect of automatic supervised frame identification on overall
frame parsing performance. There is a 22% absolute decrease in F1 (18% when partial
credit is given for related frames), suggesting that improved frame identification
or joint prediction of frames and arguments is likely to have a sizeable impact on
overall performance. Rows 4?6 compare our full model (target, frame, and argument
identification) with the baseline, showing significant improvement of more than 4.4
F1 points for both exact and partial frame matching. As with frame identification, we
compared the argument identification stage with that of J&N?07 in isolation, using the
automatically identified targets and frames from the latter as input to our model. As
shown in row 5, with partial frame matching, this gave us an F1 score of 48.1% on the
test set?significantly better (p < 0.05) than 45.6%, the full parsing result from J&N?07
(row 6 in Table 8). This indicates that our argument identification model?which uses a
single discriminative model with a large number of features for role filling (rather than
argument labeling)?is more accurate than the previous state of the art.
FrameNet 1.5 Release: Rows 7?12 show results on the newer data set, which is part
of the FrameNet 1.5 release. As in the frame identification results of Table 5, we do not
show results using predicted targets, as we only test the performance of the statistical
models. First, we observe that for results with gold frames, the F1 score is 79.08%
with naive decoding, which is significantly higher than the SemEval counterpart. This
indicates that increased training data greatly improves performance on the task. We also
observe that beam search improves precision by nearly 2%, while getting rid of overlap-
ping arguments. When both model frames and model arguments are used, we get an
F1 score of 68.45%, which is encouraging in comparison to the best results we achieved
on the SemEval 2007 data set. Semi-supervised lexicon expansion for frame identifi-
cation further improves parsing performance. We observe the best results when the
UJSF-1,2 graph objective is used for frame identification, significantly outperforming
the fully supervised model on parsing (p < 0.001) for all evaluation metrics. The im-
provements with SSL can be explained by noting that frame identification performance
goes up when the graph objectives are used, which carries over to argument iden-
tification. Figure 5 shows an example where the graph-based model UJSF-1,2 corrects
an error made by the fully supervised model for the unseen LU discrepancy.N, both for
frame identification and full frame-semantic parsing.
7. Collective Argument Identification with Constraints
The argument identification strategy described in the previous section does not capture
some facets of semantic knowledge represented declaratively in FrameNet. In this
section, we present an approach that exploits such knowledge in a principled, unified,
and intuitive way. In prior NLP research using FrameNet, these interactions have been
largely ignored, though they have the potential to improve the quality and consistency
of semantic analysis. The beam search technique (Algorithm 1) handles one kind of
constraint: avoiding argument overlaps. It is, however, approximate and cannot handle
other forms of constraints.
Here, we present an algorithm that exactly identifies the best full collection of argu-
ments of a target given its semantic frame. Although we work within the conventions of
37
Computational Linguistics Volume 40, Number 1
Figure 5
(a) Output of the supervised frame-semantic parsing model, with beam search for argument
identification, given the target discrepancies. The output is incorrect. (b) Output using the
constrained frame identification model that takes into account the graph-based frame
distributions over unknown predicates. In this particular example, the UJSF-1,2 graph
objective is used. This output matches the gold annotation. The LU discrepancy.N is unseen
in supervised FrameNet data.
FrameNet, our approach is generalizable to other SRL frameworks. We model argument
identification as constrained optimization, where the constraints come from expert
knowledge encoded in FrameNet. Following prior work on PropBank-style SRL that
dealt with similar constrained problems (Punyakanok et al. 2004; Punyakanok, Roth,
and Yih 2008, inter alia), we incorporate this declarative knowledge in an integer linear
program.
Because general-purpose ILP solvers are proprietary and do not fully exploit the
structure of the problem, we turn to a class of optimization techniques called dual
decomposition (Komodakis, Paragios, and Tziritas 2007; Rush et al. 2010; Martins et al.
2011a). We derive a modular, extensible, parallelizable approach in which semantic con-
straints map not just to declarative components in the algorithm, but also to procedural
ones, in the form of ?workers.? Although dual decomposition algorithms only solve
a relaxation of the original problem, we make our approach exact by wrapping the
algorithm in a branch-and-bound search procedure. 39
We experimentally find that our algorithm achieves accuracy comparable to the
results presented in Table 8, while respecting all imposed linguistic constraints. In
comparison with beam search, which violates many of these constraints, the presented
exact decoder is slower, but it decodes nine times faster than CPLEX, a state-of-the-art,
proprietary, general-purpose exact ILP solver.40
39 Open-source code in C++ implementing the AD3 algorithm can be found at
http://www.ark.cs.cmu.edu/AD3.
40 See http://www-01.ibm.com/software/integration/optimization/cplex-optimizer.
38
Das et al. Frame-Semantic Parsing
7.1 Joint Inference
Here, we take a declarative approach to modeling argument identification using an ILP
and relate our formulation to prior work in shallow semantic parsing. We show how
knowledge specified in a linguistic resource (FrameNet in our case) can be used to
derive the constraints in our ILP. Finally, we draw connections of our specification to
graphical models, a popular formalism in artificial intelligence and machine learning,
and describe how the constraints can be treated as factors in a factor graph.
7.1.1 Declarative Specification. Let us simplify notation by considering a given target t and
not considering its index in a sentence x; let the semantic frame it evokes be f . To solely
evaluate argument identification, we assume that the semantic frame f is given, which is
traditionally the case in controlled experiments used to evaluate SRL systems (Ma`rquez
et al. 2008). Let the set of roles associated with the frame f be Rf . In sentence x, the set
of candidate spans of words that might fill each role is enumerated, usually following
an overgenerating heuristic, which is described in Section 6.1; as before, we call this set
of spans S . As before, this set also includes the null span ?; connecting it to a role r ? Rf
denotes that the role is not overt. Our approach assumes a scoring function that gives
a strength of association between roles and candidate spans. For each role r ? Rf and
span s ? S , this score is parameterized as:
c(r, s) = ?h(s, r, f, t, x), (10)
where ? are model weights and h is a feature function that looks at the target t,
the evoked frame f , sentence x, and its syntactic analysis, along with r and s. This
scoring function is identical in form to the numerator?s exponent in the log-linear model
described in Equation (8). The SRL literature provides many feature functions of this
form and many ways to use machine learning to acquire ?. Our presented method does
not make any assumptions about the score except that it has the form in Equation (10).
We define a vector z of binary variables zr,s ? {0, 1} for every role and span pair. We
have that: z ? {0, 1}d, where d = |Rf | ? |S|. zr,s = 1 means that role r is filled by span s.
Given the binary z vector, it is straightforward to recover the collection of arguments
by checking which components zr,s have an assignment of 1; we use this strategy to
find arguments, as described in Section 7.3 (strategies 4 and 6). The joint argument
identification task can be represented as a constrained optimization problem:
maximize
?
r?Rf
?
s?S
c(r, s) ? zr,s
with respect to z ? {0, 1}d
such that Az ? b (11)
In the last line, A is a k ? d matrix and b is a vector of length k. Thus, Az ? b is a set of
k inequalities representing constraints that are imposed on the mapping between roles
and spans; these are motivated on linguistic grounds and are described next. 41
41 Note that equality constraints a ? z = b can be transformed into double-side inequalities a ? z ? b and
?a ? z ? ?b.
39
Computational Linguistics Volume 40, Number 1
Uniqueness. Each role r is filled by at most one span in S . This constraint can be
expressed by:
?r ? Rf ,
?
s?S
zr,s = 1 (12)
There are O(|Rf |) such constraints. Note that because S contains the null span ?, non-
overt roles are also captured using the above constraints. Such a constraint is used
extensively in prior literature (Punyakanok, Roth, and Yih 2008, Section 3.4.1).
Overlap. SRL systems commonly constrain roles to be filled by non-overlapping spans.
For example, Toutanova, Haghighi, and Manning (2005) used dynamic programming
over a phrase structure tree to prevent overlaps between arguments, and Punyakanok,
Roth, and Yih (2008) used constraints in an ILP to respect this requirement. Inspired by
the latter, we require that each input sentence position of x be covered by at most one
argument of t. We define:
G(i) = {s | s ? S , s covers position i in x} (13)
We can define our overlap constraints in terms of G as follows, for every sentence
position i:
?i ? {1, . . . , |x|},
?
r?Rf
?
s?G(i)
zr,s ? 1 (14)
This gives us O(|x|) constraints. It is worth noting that this constraint aims to achieve the
same effect as beam search, as described in Section 6.3, which tries to avoid argument
overlap greedily.
Pairwise ?Exclusions.? For many target classes, there are pairs of roles forbidden to
appear together in the analysis of a single target token. Consider the following two
sentences:
(1) A blackberry
Entity 1
resembles a loganberry
Entity 2
.
(2) Most berries
Entities
resemble each other.
Consider the uninflected target resemble in both sentences, evoking the same
meaning. In Example (1), two roles?which we call Entity 1 and Entity 2?describe two
entities that are similar to each other. In the second sentence, a phrase fulfills a third
role, called Entities, that collectively denotes some objects that are similar. It is clear that
the roles Entity 1 and Entities cannot be overt for the same target at once, because the latter
already captures the function of the former; a similar argument holds for the Entity 2 and
Entities roles. We call this phenomenon the ?excludes? relationship. Let us define a set of
pairs from Rf that have this relationship:
Exclf = {(ri, rj) | ri and rj exclude each other}
40
Das et al. Frame-Semantic Parsing
Using the given set, we define the constraint:
?(ri, rj) ? Exclf , zri,? + zrj,? ? 1 (15)
If both roles are overt in a parse, this constraint will be violated, contravening the
?excludes? relationship specified between the pair of roles. If neither or only one of
the roles is overt, the constraint is satisfied. The total number of such constraints is
O(|Exclf |), which is the number of pairwise ?excludes? relationships of a given frame.
Pairwise ?Requirements.? The sentence in Example (1) illustrates another kind of con-
straint. The target resemble cannot have only one of Entity 1 and Entity 2 as roles in text.
For example,
(3) * A blackberry
Entity 1
resembles.
Enforcing the overtness of two roles sharing this ?requires? relationship is straight-
forward. We define the following set for a frame f :
Reqf = {(ri, rj) | ri and rj require each other}
This leads to constraints of the form
?(ri, rj) ? Reqf , zri,? ? zrj,? = 0 (16)
If one role is overt (or absent), the other must be as well. A related constraint has
been used previously in the SRL literature, enforcing joint overtness relationships be-
tween core arguments and referential arguments (Punyakanok, Roth, and Yih 2008,
Section 3.4.1), which are formally similar to our example.42
7.1.2 Integer Linear Program and Relaxation. Plugging the constraints in Equations 12,
14, 15, and 16 into the last line of Equation (11), we have the argument identification
problem expressed as an ILP, since the indicator variables z are binary. Here, apart from
the ILP formulation, we will consider the following relaxation of Equation (11), which
replaces the binary constraint z ? {0, 1}d by a unit interval constraint z ? [0, 1]d, yielding
a linear program:
maximize
?
r?Rf
?
s?S
c(r, s) ? zr,s
with respect to z ? [0, 1]d
such that Az ? b. (17)
42 We noticed that, in the annotated data, in some cases, the ?requires? constraint is violated by the
FrameNet annotators. This happens mostly when one of the required roles is absent in the sentence
containing the target, but is rather instantiated in an earlier sentence (Gerber and Chai 2010). We apply
the hard constraint in Equation (16), though extending our algorithm to seek arguments outside the
sentence is straightforward. For preliminary work extending SEMAFOR this way, see Chen et al. (2010).
41
Computational Linguistics Volume 40, Number 1
There are several LP and ILP solvers available, and a great deal of effort has been
spent by the optimization community to devise efficient generic solvers. An example
is CPLEX, a state-of-the-art solver for mixed integer programming that we use as a
baseline to solve the ILP in Equation (11) as well as its LP relaxation in Equation (17).
Like many of the best implementations, CPLEX is proprietary.
7.1.3 Linguistic Constraints from FrameNet. Although enforcing the four different sets of
constraints is intuitive from a general linguistic perspective, we ground their use in
definitive linguistic information present in the FrameNet lexicon. From the annotated
data in the FrameNet 1.5 release, we gathered that only 3.6% of the time is a role
instantiated multiple times by different spans in a sentence. This justifies the uniqueness
constraint enforced by Equation (12). Use of such a constraint is also consistent with
prior work in frame-semantic parsing (Johansson and Nugues 2007). Similarly, we
found that in the annotations, no arguments overlapped with each other for a given
target. Hence, the overlap constraints in Equation (14) are also justified.
Our third and fourth sets of constraints, presented in Equations (15) and (16), come
from FrameNet, too. Examples (1) and (2) are instances where the target resemble
evokes the SIMILARITY frame, which is defined in FrameNet as:
Two or more distinct entities, which may be concrete or abstract objects or types, are
characterized as being similar to each other. Depending on figure/ground relations, the
entities may be expressed in two distinct frame elements and constituents, Entity 1 and
Entity 2, or jointly as a single frame element and constituent, Entities.
For this frame, the lexicon lists several roles other than the three we have already
observed, such as Dimension (the dimension along which the entities are similar), Differ-
entiating fact (a fact that reveals how the concerned entities are similar or different), and
so forth. Along with the roles, FrameNet also declares the ?excludes? and ?requires?
relationships noted in our discussion in Section 7.1.1. The case of the SIMILARITY frame
is not unique; in Figure 1, the frame COLLABORATION, evoked by the target partners, also
has two roles Partner 1 and Partner 2 that share the ?requires? relationship. In fact, out
of 877 frames in FrameNet 1.5, 204 frames have at least a pair of roles for which the
?excludes? relationship holds, and 54 list at least a pair of roles that share the ?requires?
relationship.
7.1.4 Constraints as Factors in a Graphical Model. The LP in Equation (17) can be repre-
sented as a maximum a posteriori inference problem in an undirected graphical model.
In the factor graph, each component (zr,s) of the vector z corresponds to a binary
variable, and each instantiation of a constraint in Equations (12), (14), (15), and (16)
corresponds to a factor. Smith and Eisner (2008) and Martins et al. (2010) used such
a representation to impose constraints in a dependency parsing problem; the latter
discussed the equivalence of linear programs and factor graphs for representing dis-
crete optimization problems. All of our constraints take standard factor forms we can
describe using the terminology of Smith and Eisner and Martins et al. The uniqueness
constraint in Equation (12) corresponds to an XOR factor, while the overlap constraint
in Equation (14) corresponds to an ATMOSTONE factor. The constraints in Equation (15)
enforcing the ?excludes? relationship can be represented with an OR factor. Finally,
each ?requires? constraints in Equation (16) is equivalent to an XORWITHOUTPUT
factor.
42
Das et al. Frame-Semantic Parsing
In the following section, we describe how we arrive at solutions for the LP in
Equation (17) using dual decomposition, and how we adapt it to efficiently recover the
exact solution of the ILP (Equation (11)), without the need of an off-the-shelf ILP solver.
7.2 ?Augmented? Dual Decomposition
Dual decomposition methods address complex optimization problems in the dual, by
dividing them into simple worker problems (subproblems), which are repeatedly solved
until a consensus is reached. The simplest technique relies on the subgradient algorithm
(Komodakis, Paragios, and Tziritas 2007; Rush et al. 2010); as an alternative, Martins
et al. (2011a, 2011b) proposed an augmented Lagrangian technique, which is more
suitable when there are many small components ?commonly the case in declarative
constrained problems, like the one at hand. Here, we present a brief overview of the
latter, which is called AD3.
Let us start by establishing some notation. Let m ? {1, . . . , M} index a factor, and
denote by i(m) the vector of indices of variables linked to that factor. (Recall that each
factor represents the instantiation of a constraint.) We introduce a new set of variables,
u ? Rd, called the ?witness? vector. We split the vector z into M overlapping pieces
z1, . . . , zM, where each zm ? [0, 1]|i(m)|, and add M constraints zm = ui(m) to impose that
all the pieces must agree with the witness (and therefore with each other). Each of the M
constraints described in Section 7.1 can be encoded with its own matrix Am and vector
bm (which jointly define A and b in Equation (17)). For convenience, we denote by c ? Rd
the score vector, whose components are c(r, s), for each r ? Rf and s ? S (Equation (10)),
and define the following scores for the mth subproblem:
cm(r, s) = ?(r, s)?1c(r, s), ?(r, s) ? i(m)
where ?(r, s) is the number of constraints that involve role r and span s. Note that
according to this definition, c ? z =
?M
m=1 cm ? zm. We can rewrite the LP in Equation (17)
in the following equivalent form:
maximize
M
?
m=1
cm ? zm
with respect to u ? Rd, zm ? [0, 1]i(m), ?m
such that Amzm ? bm, ?m
zm = ui(m), ?m (18)
We introduce Lagrange multipliers ?m for the equality constraints in the last line.
The AD3 algorithm is depicted as Algorithm 2. Like dual decomposition approaches,
it repeatedly performs a broadcast operation (the zm-updates, which can be done in
parallel, one constraint per ?worker?) and a gather operation (the u- and ?-updates).
Each u-operation can be seen as an averaged voting which takes into consideration each
worker?s results.
Like in the subgradient method, the ?-updates can be regarded as price adjust-
ments, which will affect the next round of zm-updates. The only difference with respect
to the subgradient method (Rush et al. 2010) is that each subproblem involved in a
zm-update also has a quadratic penalty that penalizes deviations from the previous
43
Computational Linguistics Volume 40, Number 1
Algorithm 2 AD3 for Argument Identification
Require: role-span matching scores c := ?c(r, s)?r,s, structural constraints ?Am, bm?Mm=1,
penalty ? > 0
1: initialize t ? 1
2: initialize u1 uniformly (i.e., u(r, s) = 0.5, ?r, s)
3: initialize each ?1m = 0, ?m ? {1, . . . , M}
4: repeat
5: for each m = 1, . . . , M do
6: make a zm-update by finding the best scoring analysis for the mth constraint,
with penalties for deviating from the consensus u:
z(t+1)m ? argmax
Amztm?bm
(cm + ?tm) ? zm ?
?
2?zm ? u
t
i(m)?2 (19)
7: end for
8: make a u-update by updating the consensus solution, averaging z1, . . . , zm:
u(t+1)(r, s) ? 1
?(r, s)
?
m:(r,s)?i(m)
z(t+1)m (r, s)
9: make a ?-update:
?(t+1)m ? ?tm ? ?(z(t+1)m ? u(t+1)i(m) ), ?m
10: t ? t + 1
11: until convergence
Ensure: relaxed primal solution u? and dual solution ??. If u? is integer, it will encode
an assignment of spans to roles. Otherwise, it will provide an upper bound of the
true optimum.
average voting; it is this term that accelerates consensus and therefore convergence.
Martins et al. (2011b) also provide stopping criteria for the iterative updates using
primal and dual residuals that measure convergence; we refer the reader to that paper
for details.
A key attraction of this algorithm is that all the components of the declarative
specification remain intact in the procedural form. Each worker corresponds exactly
to one constraint in the ILP, which corresponds to one linguistic constraint. There is no
need to work out when, during the procedure, each constraint might have an effect, as
in beam search.
7.2.1 Solving the Subproblems. In a different application, Martins et al. (2011b, Section 4)
showed how to solve each zm-subproblem associated with the XOR, XORWITHOUTPUT
and OR factors in runtime O(|i(m)| log |i(m)|). The only subproblem that remains is that
of the ATMOSTONE factor; a solution with the same runtime is given in Appendix B.
7.2.2 Exact Decoding. It is worth recalling that AD3, like other dual decomposition
algorithms, solves a relaxation of the actual problem. Although we have observed that
the relaxation is often tight (cf. Section 7.3), this is not always the case. Specifically, a
fractional solution may be obtained, which is not interpretable as an argument, and
therefore it is desirable to have a strategy to recover the exact solution. Two observations
44
Das et al. Frame-Semantic Parsing
are noteworthy. First, the optimal value of the relaxed problem (Equation (17)) provides
an upper bound to the original problem (Equation (11)). This is because Equation (11)
has the additional integer constraint on the variables. In particular, any feasible dual
point provides an upper bound to the original problem?s optimal value. Second, dur-
ing execution of the AD3 algorithm, we always keep track of a sequence of feasible
dual points. Therefore, each iteration constructs tighter and tighter upper bounds.
With this machinery, we have all that is necessary for implementing a branch-and-
bound search that finds the exact solution of the ILP. The procedure works recursively
as follows:
1. Initialize L = ?? (our best value so far).
2. Run Algorithm 2. If the solution u? is integer, return u? and set L to the objec-
tive value. If along the execution we obtain an upper bound less than L, then
Algorithm 2 can be safely stopped and return ?infeasible??this is the bound part.
Otherwise (if u? is fractional) go to step 3.
3. Find the ?most fractional? component of u? (call it u?j ) and branch: constrain uj = 0
and go to step 2, eventually obtaining an integer solution u?0 or infeasibility; and
then constrain uj = 1 and do the same, obtaining u?1 . Return the u
? ? {u?0 , u?1} that
yields the largest objective value.
Although this procedure may have worst-case exponential runtime, we found it empir-
ically to rapidly obtain the exact solution in all test cases.
7.3 Results with Collective Argument Identification
We present experiments only on argument identification in this section, as our goal is
to exhibit the importance of incorporating the various linguistic constraints during our
inference procedure. We present results on the full text annotations of FrameNet 1.5, and
do not experiment on the SemEval 2007 benchmark, as we have already established our
constraint-agnostic models as state-of-the-art. The model weights ? used in the scoring
function c were learned as in Section 6.1 (i.e., by training a logistic regression model to
maximize conditional log-likelihood). The AD3 parameter ? was initialized to 0.1, and
we followed Martins et al. (2011b) in dynamically adjusting it to keep a balance between
the primal and dual residuals.
We compare the following algorithms to demonstrate the efficacy of our collective
argument identification approach:43
1. Naive: This strategy selects the best span for each role r according to the score
function c(r, s), independently of all other roles?the decoding rule formalized in
Equation (7) of Section 6.1. It ignores all constraints except ?uniqueness.?
2. Beam: This strategy employs greedy beam search to eliminate overlaps between
predicted arguments, as described in Algorithm 1. Note that it does not try to
respect the ?excludes? and ?requires? constraints between pairs of roles. The
default size of the beam in Section 1 was a safe 10,000; this resulted in extremely
slow decoding times. For time comparison, we tried beam sizes of 100 and 2 (the
latter being the smallest size that achieves the same F1 score on the FrameNet 1.5
dev set).
43 The first two strategies correspond to rows 7 and 9, respectively, of Table 8.
45
Computational Linguistics Volume 40, Number 1
Table 9
Comparison of decoding strategies in Section 7.3 on the data set released with the FrameNet 1.5
Release, given gold frames. We evaluate in terms of precision, recall, and F1 score on our test
set containing 4,458 targets. We also compute the number of constraint violations each model
makes: the three values are the numbers of overlapping arguments and violations of the
?requires? and ?excludes? constraints of Section 7.1. Finally, decoding time (without feature
computation steps) on the whole test set is shown in the last column averaged over five runs.
ARGUMENT IDENTIFICATION
Method P R F1 Violations Time (s)
naive 82.00 76.36 79.08 441 45 15 1.26 ? 0.01
beam = 2 83.68 76.22 79.78 0 49 0 2.74 ? 0.10
beam = 100 83.83 76.28 79.88 0 50 1 29.00 ? 0.25
beam = 10, 000 83.83 76.28 79.88 0 50 1 440.67 ? 5.53
CPLEX, LP 83.80 76.16 79.80 0 1 0 32.67 ? 1.29
CPLEX, exact 83.78 76.17 79.79 0 0 0 43.12 ? 1.26
AD3, LP 83.77 76.17 79.79 2 2 0 4.17 ? 0.01
AD3, exact 83.78 76.17 79.79 0 0 0 4.78 ? 0.04
3. CPLEX, LP: This uses CPLEX to solve the relaxed LP in Equation (17). To han-
dle fractional z, for each role r, we choose the best span s? such that s? =
argmaxs?Sr zr,s, breaking ties arbitrarily.
4. CPLEX, exact: This tackles the actual ILP (Equation (11)) with CPLEX.
5. AD3, LP: The relaxed problem is solved using AD3. We choose a span for each role
as in strategy 3.
6. AD3, exact: This couples AD3 with branch-and-bound search to get the exact
integer solution.
Table 9 shows performance of these decoding strategies on the test set. We report
precision, recall, and F1 scores. As with experiments in previous sections, we use the
evaluation script from SemEval 2007 shared task. Because these scores do not penalize
constraint violations, we also report the number of overlap, ?excludes,? and ?requires?
constraints that were violated in the test set. Finally, we tabulate each setting?s decoding
time in seconds on the whole test set averaged over five runs.44 The naive model
is very fast but suffers degradation in precision and violates one constraint roughly
per nine targets. The decoding strategy of Section 6.1 used a default beam size of
10,000, which is extremely slow; a faster version of beam size 100 results in the same
precision and recall values, but is 15 times faster on our test set. Beam size 2 results
in slightly worse precision and recall values, but is even faster. All of these, however,
result in many constraint violations. Strategies involving CPLEX and AD3 perform
similarly to each other and to beam search on precision and recall, but eliminate most
or all of the constraint violations. With respect to precision and recall, exact AD3 and
beam search with a width of 10,000 were found to be statistically indistinguishable
(p > 0.01). The decoding strategy with beam size 2 is 11?16 times faster than the
44 Experiments were conducted on a 64-bit machine with two 2.6-GHz dual-core CPUs (i.e., four processors
in all) and a total of 8 GB of RAM. The workers in AD3 were not parallelized, whereas CPLEX
automatically parallelized execution.
46
Das et al. Frame-Semantic Parsing
.
(a) Gold annotation.
.
(b) Beam search output.
Figure 6
An example from the test set where (a) exhibits the gold annotation for a target that evokes
the COLLABORATION frame, with the Partners role filled by the span international. (b) shows
the prediction made by the beam search decoding scheme (beam = 10,000), where it marks
international with the Partner 1 role, violating the ?requires? constraint; FrameNet notes that this
role should be present with the Partner 2 role. AD3 is conservative and predicts no role?it is
penalized by the evaluation script, but does not produce output that violates
linguistic constraints.
CPLEX strategies, but is only twice as fast as AD3, and results in significantly more
constraint violations. The exact algorithms are slower than the LP versions, but com-
pared with CPLEX, AD3 is significantly faster and has a narrower gap between its
exact and LP versions. We found that relaxation was tight 99.8% of the time on the test
examples.
The example in Figure 1 is taken from our test set, and shows an instance where two
roles, Partner 1 and Partner 2, share the ?requires? relationship; for this example, the beam
search decoder misses the Partner 2 role, which is a violation, while our AD3 decoder
identifies both arguments correctly. Note that beam search makes plenty of linguistic
violations. We found that beam search, when violating many ?requires? constraints,
often finds one role in the pair, which increases its recall. AD3 is sometimes more
conservative in such cases, predicting neither role. Figure 6 shows such an example
where beam search finds one role (Partner 1) while AD3 is more conservative and predicts
no roles. Figure 7 shows another example contrasting the output of beam search and
AD3 where the former predicts two roles sharing an ?excludes? relationship; AD3 does
not violate this constraint and tries to predict a more consistent argument set. Overall,
we found it interesting that imposing the constraints did not have much effect on
standard measures of accuracy.
Table 9 only shows results with gold frames. We ran the exact version of AD3 with
automatic frames as well. When the semi-supervised graph objective UJSF-1,2 is used
for frame identification, the performance with AD3 is only a bit worse in comparison
with beam search (row 11 in Table 8) when frame and argument identification are
evaluated together. We get a precision of 72.92, a recall of 65.22 and an F1 score of 68.86
(partial frame matching). Again, all linguistic constraints are respected, unlike beam
search.
47
Computational Linguistics Volume 40, Number 1
8. Conclusion
We have presented an approach to rich frame-semantic parsing, based on a combination
of knowledge from FrameNet, two probabilistic models trained on full text annota-
tions released along with the FrameNet lexicon, and expedient heuristics. The frame
identification model uses latent variables in order to generalize to predicates unseen
in either the FrameNet lexicon or training data, and our results show that, quite often,
this model chooses a frame closely related to the gold-standard annotation. We also
presented an extension of this model that uses graph-based semi-supervised learning
to better generalize to new predicates; this achieves significant improvements over the
fully supervised approach. Our argument identification model, trained using maximum
conditional log-likelihood, unifies the traditionally separate steps of detecting and
(a) Gold annotation.
(b) Beam search output.
(c) AD3 output.
Figure 7
An example from the test set where (a) exhibits the gold annotation for a target that evokes
the DISCUSSION frame, with the Interlocutor 1 role filled by the span neighbors. (b) shows
the prediction made by the beam search decoding scheme (beam = 10,000), where it marks
The next morning his households and neighbors with the Interlocutors role, which violates
the ?excludes? constraint with respect to the Interlocutor 2 role. In (c), AD3 marks the wrong
span as the Interlocutor 1 role, but it does not violate the constraint. Both beam and
AD3 inference miss the Topic role.
48
Das et al. Frame-Semantic Parsing
labeling arguments. Our system achieves improvements over the previous state of the
art on the SemEval 2007 benchmark data set at each stage of processing and collectively.
We also report stronger results on the more recent, larger FrameNet 1.5 release.
We applied the AD3 algorithm to collective prediction of a target?s arguments,
incorporating declarative linguistic knowledge as constraints. It outperforms the naive
local decoding scheme that is oblivious to the constraints. Furthermore, it is significantly
faster than a decoder employing a state-of-the-art proprietary solver; it is only twice as
slow as beam search (our chosen decoding method for comparison with the state of
the art), which is inexact and does not respect all linguistic constraints. This method is
easily amenable to the inclusion of additional constraints.
From our results, we observed that in comparison to the SemEval 2007 data
set, frame-semantic parsing performance significantly increases when we use the
FrameNet 1.5 release; this suggests that the increase in the number of full text anno-
tations and the size of the FrameNet lexicon is beneficial. We believe that with more
annotations in the future (say, in the range of the number of PropBank annotations), our
frame-semantic parser can reach even better accuracy, making it more useful for NLP
applications that require semantic analysis.
There are several open problems to be addressed. Firstly, we could further im-
prove the coverage of the frame-semantic parser by improving our semi-supervised
learning approach; two possibilities are custom metric learning approaches (Dhillon,
Talukdar, and Crammer 2010) that suit the frame identification problem in graph-based
SSL, and sparse word representations (Turian, Ratinov, and Bengio 2010) as features
in frame identification. The argument identification model might also benefit from
semi-supervised learning. Further feature engineering and improved preprocessing,
including tokenization into lexical units, improved syntactic parsing, and the use of
external knowledge bases, is expected to improve the system?s accuracy. Finally, the
FrameNet lexicon does not contain exhaustive semantic knowledge. Automatic frame
and role induction is an exciting direction of future research that could further enhance
our methods of automatic frame-semantic parsing. The parser described in this article
is available for download at http://www.ark.cs.cmu.edu/SEMAFOR.
Appendix
A. Target Identification Heuristics from J&N?07
We describe here the filtering rules that Johansson and Nugues (2007) used for identify-
ing frame evoking targets in their SemEval 2007 shared task paper. They built a filtering
component based on heuristics that removed words that appear in certain contexts, and
kept the remaining ones.45 These are:
 have was retained only if had an object,
 be was retained only if it was preceded by there,
 will was removed in its modal sense,
 of course and in particular were removed,
45 Although not explicitly mentioned in the paper, we believe that these rules were applied on a white list of
potential targets seen in FrameNet and the SemEval 2007 training data.
49
Computational Linguistics Volume 40, Number 1
 the prepositions above, against, at, below, beside, by, in, on, over, and under
were removed unless their head was marked as locative,
 after and before were removed unless their head was marked as temporal,
 into, to, and through were removed unless their head was marked as
direction,
 as, for, so, and with were always removed,
 because the only sense of the word of was the frame PARTITIVE, it was
removed unless it was preceded by only, member, one, most, many, some, few,
part, majority, minority, proportion, half, third, quarter, all, or none, or it was
followed by all, group, them, or us,
 all targets marked as support verbs for some other target were removed.
Note that J&N?07 used a syntactic parser that provided dependency labels correspond-
ing to locative, temporal, and directional arguments, which our syntactic parser of
choice (the MST parser) does not provide.
B. Solving ATMOSTONE subproblems in AD3
The ATMOSTONE subproblem can be transformed into that of projecting a point
(a1, . . . , ak) onto the set
Sm =
{
zm ? [0, 1]|i(m)| |
?|i(m)|
j=1 zm,j ? 1
}
This projection can be computed as follows:
1. Clip each aj into the interval [0, 1] (i.e., set a?j = min{max{aj, 0}, 1}). If the result
satisfies
?k
j=1 a
?
j ? 1, then return (a?1, . . . , a?k).
2. Otherwise project (a1, . . . , ak) onto the probability simplex:
{
zm ? [0, 1]|i(m)| |
?|i(m)|
j=1 zm,j = 1
}
This is precisely the XOR subproblem and can be solved in time O(|i(m)|
log |i(m)|).
The proof of this procedure?s correctness follows from the proof in Appendix B of
Martins et al. (2011b).
Acknowledgments
We thank Collin Baker, Katrin Erk, Richard
Johansson, and Nils Reiter for software, data,
evaluation scripts, and methodological
details. We thank the reviewers of this and
the earlier papers, Alan Black, Ric Crabbe,
Michael Ellsworth, Rebecca Hwa, Dan Klein,
Russell Lee-Goldman, Slav Petrov, Dan Roth,
Josef Ruppenhofer, Amarnag Subramanya,
Partha Talukdar, and members of the ARK
group for helpful comments. This work was
supported by DARPA grant NBCH-1080004,
NSF grants IIS-0836431 and IIS-0915187,
Qatar National Research Foundation grant
NPRP 08-485-1-083, Google?s support of the
Worldly Knowledge Project at CMU,
computational resources provided by Yahoo,
and TeraGrid resources provided by the
Pittsburgh Supercomputing Center under
grant TG-DBS110003.
50
Das et al. Frame-Semantic Parsing
References
Auli, Michael and Adam Lopez. 2011.
A comparison of loopy belief propagation
and dual decomposition for integrated
CCG supertagging and parsing.
In Proceedings of the 49th Annual Meeting
of the Association for Computational
Linguistics: Human Language Technologies,
pages 470?480, Portland, OR.
Baker, Collin, Michael Ellsworth, and
Katrin Erk. 2007. SemEval-2007 task 19:
Frame semantic structure extraction.
In Proceedings of the Fourth International
Workshop on Semantic Evaluations,
pages 99?104, Prague.
Baluja, Shumeet, Rohan Seth, D. Sivakumar,
Yushi Jing, Jay Yagnik, Shankar Kumar,
Deepak Ravichandran, and Mohamed
Aly. 2008. Video suggestion and discovery
for YouTube: taking random walks
through the view graph. In Proceedings
of the 17th International Conference on
the World Wide Web, pages 895?904,
Beijing.
Bauer, Daniel and Owen Rambow. 2011.
Increasing coverage of syntactic
subcategorization patterns in FrameNet
using VerbNet. In Proceedings of the 2011
IEEE Fifth International Conference on
Semantic Computing, pages 181?184,
Washington, DC.
Bejan, Cosmin A. 2009. Learning Event
Structures From Text. Ph.D. thesis, The
University of Texas at Dallas.
Bengio, Yoshua, Olivier Delalleau, and
Nicolas Le Roux. 2006. Label propagation
and quadratic criterion. In Olivier
Chapelle, Bernhard Scho?lkopf, and
Alexander Zien, editors, Semi-Supervised
Learning. MIT Press, Cambridge, MA,
pages 193?216.
Boas, Hans C. 2002. Bilingual FrameNet
dictionaries for machine translation.
In Proceedings of the Third International
Conference on Language Resources and
Evaluation, pages 1,364?1,371, Las Palmas.
Bottou, Le?on. 2004. Stochastic learning.
In Olivier Bousquet and Ulrike von
Luxburg, editors, Advanced Lectures on
Machine Learning, Lecture Notes in
Artificial Intelligence, LNAI 3176.
Springer Verlag, Berlin, pages 146?168.
Burbea, Jacob and Calyampudi R. Rao. 2006.
On the convexity of some divergence
measures based on entropy functions.
IEEE Transactions on Information Theory,
28(3):489?495.
Burchardt, Aljoscha, Katrin Erk, and
Anette Frank. 2005. A WordNet detour
to FrameNet. In Bernhard Fisseni,
Hans-Christian Schmitz, Bernhard
Schro?der, and Petra Wagner, editors,
Sprachtechnologie, mobile Kommunikation
und linguistische Resourcen, volume 8 of
Computer Studies in Language and Speech.
Peter Lang, Frankfurt am Main,
pages 408?421.
Burchardt, Aljoscha and Anette Frank. 2006.
Approaching textual entailment with LFG
and FrameNet frames. In Proceedings of the
Second PASCAL RTE Challenge Workshop,
pages 92?97, Venice.
Burchardt, Aljoscha, Marco Pennacchiotti,
Stefan Thater, and Manfred Pinkal. 2009.
Assessing the impact of frame semantics
on textual entailment. Natural Language
Engineering, 15(4):527?550.
Carreras, Xavier and Llu??s Ma`rquez. 2004.
Introduction to the CoNLL-2004 shared
task: Semantic role labeling. In Proceedings
of the Eighth Conference on Computational
Natural Language Learning, pages 89?97,
Boston, MA.
Carreras, Xavier and Llu??s Ma`rquez. 2005.
Introduction to the CoNLL-2005 shared
task: Semantic role labeling. In Proceedings
of the Ninth Conference on Computational
Natural Language Learning, pages 152?164,
Ann Arbor, MI.
Chang, Yin-Wen and Michael Collins.
2011. Exact decoding of phrase-based
translation models through Lagrangian
relaxation. In Proceedings of the 2011
Conference on Empirical Methods in Natural
Language Processing, pages 26?37,
Edinburgh.
Chapelle, Olivier, Bernhard Scho?lkopf,
and Alexander Zien, editors. 2006.
Semi-Supervised Learning. MIT Press,
Cambridge, MA.
Chen, Desai, Nathan Schneider, Dipanjan
Das, and Noah A. Smith. 2010. SEMAFOR:
Frame argument resolution with
log-linear models. In Proceedings of the
5th International Workshop on Semantic
Evaluation, pages 264?267, Upssala.
Corduneanu, Adrian and Tommi Jaakkola.
2003. On information regularization.
In Proceedings of the Nineteenth Conference
on Uncertainty in Artificial Intelligence,
pages 151?158, Acapulco.
Das, Dipanjan, Andre? F. T. Martins, and
Noah A. Smith. 2012. An exact dual
decomposition algorithm for shallow
semantic parsing with constraints.
In Proceedings of the First Joint Conference
on Lexical and Computational Semantics,
pages 209?217, Montre?al.
51
Computational Linguistics Volume 40, Number 1
Das, Dipanjan and Slav Petrov. 2011.
Unsupervised part-of-speech tagging
with bilingual graph-based projections.
In Proceedings of the 49th Annual Meeting
of the Association for Computational
Linguistics: Human Language Technologies,
pages 600?609, Portland, OR.
Das, Dipanjan, Nathan Schneider, Desai
Chen, and Noah A. Smith. 2010.
Probabilistic frame-semantic parsing.
In Proceedings of the Human Language
Technologies Conference of the North
American Chapter of the Association for
Computational Linguistics, pages 948?956,
Los Angeles, CA.
Das, Dipanjan and Noah A. Smith. 2011.
Semi-supervised frame-semantic parsing
for unknown predicates. In Proceedings of
the 49th Annual Meeting of the Association
for Computational Linguistics: Human
Language Technologies, pages 1,435?1,444,
Portland, OR.
Das, Dipanjan and Noah A. Smith. 2012.
Graph-based lexicon expansion with
sparsity-inducing penalties. In Proceedings
of the Human Language Technologies
Conference of the North American Chapter
of the Association for Computational
Linguistics, pages 677?687, Montre?al.
Dean, Jeffrey and Sanjay Ghemawat. 2008.
MapReduce: Simplified data processing
on large clusters. Communications of the
ACM, 51(1):107?113.
DeNero, John and Klaus Macherey. 2011.
Model-based aligner combination using
dual decomposition. In Proceedings of
the 49th Annual Meeting of the Association
for Computational Linguistics: Human
Language Technologies, pages 420?429,
Portland, OR.
Deschacht, Koen and Marie-Francine Moens.
2009. Semi-supervised semantic role
labeling using the Latent Words Language
Model. In Proceedings of the 2009 Conference
on Empirical Methods in Natural Language
Processing, pages 21?29, Singapore.
Dhillon, Paramveer S., Partha Pratim
Talukdar, and Koby Crammer. 2010.
Learning better data representation
using inference-driven metric
learning. In Proceedings of the ACL
2010 Conference Short Papers,
pages 377?381, Uppsala.
Erk, Katrin and Sebastian Pado?. 2006.
Shalmaneser?a toolchain for shallow
semantic parsing. In Proceedings of the
Fifth International Conference on Language
Resources and Evaluation, pages 527?532,
Genoa.
Fellbaum, Christiane, editor. 1998. WordNet:
an electronic lexical database. MIT Press,
Cambridge, MA.
Fillmore, Charles J. 1982. Frame semantics.
In Linguistics in the Morning Calm.
Hanshin Publishing Co., Seoul,
pages 111?137.
Fillmore, Charles J., Christopher R. Johnson,
and Miriam R. L. Petruck. 2003.
Background to FrameNet. International
Journal of Lexicography, 16.3:235?250.
Fleischman, Michael, Namhee Kwon, and
Eduard Hovy. 2003. Maximum entropy
models for FrameNet classification.
In Proceedings of the 2003 Conference on
Empirical Methods in Natural Language
Processing, pages 49?56, Sapporo.
Fung, Pascale and Benfeng Chen. 2004.
BiFrameNet: Bilingual frame semantics
resource construction by cross-lingual
induction. In Proceedings of the 20th
International Conference on Computational
Linguistics, pages 931?937, Geneva.
Fu?rstenau, Hagen and Mirella Lapata. 2009a.
Graph alignment for semi-supervised
semantic role labeling. In Proceedings of the
2009 Conference on Empirical Methods in
Natural Language Processing, pages 11?20,
Singapore.
Fu?rstenau, Hagen and Mirella Lapata. 2009b.
Semi-supervised semantic role labeling.
In Proceedings of the 12th Conference of the
European Chapter of the ACL, pages 220?228,
Athens.
Fu?rstenau, Hagen and Mirella Lapata. 2012.
Semi-supervised semantic role labeling
via structural alignment. Computational
Linguistics, 38(1):135?171.
Gerber, Matthew and Joyce Chai. 2010.
Beyond NomBank: A study of implicit
arguments for nominal predicates. In
Proceedings of ACL, pages 1,583?1,592,
Uppsala.
Gildea, Daniel and Daniel Jurafsky. 2002.
Automatic labeling of semantic roles.
Computational Linguistics, 28(3):245?288.
Girju, Roxana, Preslav Nakov, Vivi Nastase,
Stan Szpakowicz, Peter Turney, and Deniz
Yuret. 2007. SemEval-2007 task 04:
Classification of semantic relations
between nominals. In Proceedings of the
Fourth International Workshop on Semantic
Evaluations, pages 13?18, Prague.
Giuglea, Ana-Maria and Alessandro
Moschitti. 2006. Shallow semantic
parsing based on FrameNet, VerbNet
and PropBank. In Proceedings of the
17th European Conference on Artificial
Intelligence, pages 563?567, Amsterdam.
52
Das et al. Frame-Semantic Parsing
Gropp, W., E. Lusk, and A. Skjellum. 1994.
Using MPI: Portable Parallel Programming
with the Message-Passing Interface.
MIT Press, Cambridge, MA.
Hajic?, Jan, Massimiliano Ciaramita,
Richard Johansson, Daisuke Kawahara,
Maria Anto`nia Mart??, Llu??s Ma`rquez,
Adam Meyers, Joakim Nivre, Sebastian
Pado?, Jan S?te?pa?nek, Pavel Stran?a?k, Mihai
Surdeanu, Nianwen Xue, and Yi Zhang.
2009. The CoNLL-2009 shared task:
Syntactic and semantic dependencies in
multiple languages. In Proceedings of the
Thirteenth Conference on Computational
Natural Language Learning, pages 1?18,
Boulder, CO.
Ide, Nancy and Jean Ve?ronis. 1998.
Introduction to the special issue on word
sense disambiguation: The state of the art.
Computational Linguistics, 24(1):2?40.
Johansson, Richard and Pierre Nugues.
2007. LTH: Semantic structure extraction
using nonprojective dependency trees.
In Proceedings of the 4th International
Workshop on Semantic Evaluations,
pages 227?230, Prague.
Johansson, Richard and Pierre Nugues. 2008.
Dependency-based semantic role labeling
of PropBank. In Proceedings of the 2008
Conference on Empirical Methods in Natural
Language Processing, pages 69?78,
Honolulu, HI.
Kingsbury, Paul and Martha Palmer. 2002.
From TreeBank to PropBank. In Proceedings
of the 3rd International Conference on
Language Resources and Evaluation,
pages 1,989?1,993, Las Palmas.
Komodakis, Nikos, Nikos Paragios, and
Georgios Tziritas. 2007. MRF optimization
via dual decomposition: Message-passing
revisited. In Eleventh International
Conference on Computer Vision, pages 1?8,
Rio de Janeiro.
Koo, Terry, Alexander M. Rush, Michael
Collins, Tommi Jaakkola, and David
Sontag. 2010. Dual decomposition for
parsing with non-projective head
automata. In Proceedings of the 2010
Conference on Empirical Methods in Natural
Language Processing, pages 1,288?1,298,
Cambridge, MA.
Kowalski, Matthieu and Bruno Torre?sani.
2009. Sparsity and persistence: Mixed
norms provide simple signal models with
dependent coefficients. Signal, Image and
Video Processing, 3:251?264.
Lang, Joel and Mirella Lapata. 2010.
Unsupervised induction of semantic roles.
In Proceedings of the Human Language
Technologies Conference of the North
American Chapter of the Association for
Computational Linguistics, pages 939?947,
Los Angeles, CA.
Lang, Joel and Mirella Lapata. 2011.
Unsupervised semantic role induction
with graph partitioning. In Proceedings
of the 2011 Conference on Empirical
Methods in Natural Language Processing,
pages 1320?1331, Edinburgh.
Lin, Dekang. 1993. Principle-based parsing
without overgeneration. In Proceedings of
the 31st Annual Meeting of the Association for
Computational Linguistics, pages 112?120,
Columbus, OH.
Lin, Dekang. 1994. Principar?an efficient,
broad-coverage, principle-based parser.
In Proceedings of the 15th Conference on
Computational Linguistics, pages 482?488,
Kyoto.
Lin, Dekang. 1998. Automatic retrieval and
clustering of similar words. In Proceedings
of the 36th Annual Meeting of the Association
for Computational Linguistics and 17th
International Conference on Computational
Linguistics, pages 768?774, Montreal.
Lin, Jianhua. 1991. Divergence measures
based on the Shannon entropy. IEEE
Transactions on Information Theory,
37(1):145?151.
Litkowski, Kenneth C. and Orin Hargraves.
2007. SemEval-2007 task 06: Word-sense
disambiguation of prepositions. In
Proceedings of the Fourth International
Workshop on Semantic Evaluations
(SemEval-2007), pages 24?29, Prague.
Liu, Dong C. and Jorge Nocedal. 1989. On
the limited memory BFGS method for
large scale optimization. Mathematical
Programming, 45(3):503?528.
Marcus, Mitchell P., Mary Ann
Marcinkiewicz, and Beatrice Santorini.
1993. Building a large annotated corpus of
English: the Penn treebank. Computational
Linguistics, 19(2):313?330, June.
Ma`rquez, Llu??s, Xavier Carreras, Kenneth C.
Litkowski, and Suzanne Stevenson. 2008.
Semantic role labeling: an introduction to
the special issue. Computational Linguistics,
34(2):145?159, June.
Martins, Andre? F. T., Mario A. T. Figueiredo,
Pedro M. Q. Aguiar, Noah A. Smith, and
Eric P. Xing. 2011a. An augmented
Lagrangian approach to constrained MAP
inference. In Proceedings of the 28th
International Conference on Machine
Learning, pages 169?176, Bellevue, WA.
Martins, Andre? F. T., Noah A. Smith, Pedro
M. Q. Aguiar, and Mario A. T. Figueiredo.
53
Computational Linguistics Volume 40, Number 1
2011b. Dual decomposition with many
overlapping components. In Proceedings
of the 2011 Conference on Empirical
Methods in Natural Language Processing,
pages 238?249, Edinburgh.
Martins, Andre? F. T., Noah A. Smith, and
Eric P. Xing. 2009. Concise integer
linear programming formulations for
dependency parsing. In Proceedings of
the Joint Conference of the 47th Annual
Meeting of the Association for Computational
Linguistics and the 4th International
Joint Conference on Natural Language
Processing of the AFNLP, pages 342?350,
Suntec.
Martins, Andre? F. T., Noah A. Smith, Eric P.
Xing, Mario A. T. Figueiredo, and Pedro
M. Q. Aguiar. 2010. Turbo parsers:
Dependency parsing by approximate
variational inference. In Proceedings of the
2010 Conference on Empirical Methods in
Natural Language Processing, pages 34?44,
Cambridge, MA.
Matsubayashi, Yuichiroh, Naoaki Okazaki,
and Jun?ichi Tsujii. 2009. A comparative
study on generalization of semantic roles
in FrameNet. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the
Association for Computational Linguistics and
the 4th International Joint Conference on
Natural Language Processing of the AFNLP,
pages 19?27, Suntec.
McDonald, Ryan, Koby Crammer, and
Fernando Pereira. 2005. Online
large-margin training of dependency
parsers. In Proceedings of the 43rd Annual
Meeting of the Association for Computational
Linguistics, pages 91?98, Ann Arbor, MI.
Meyers, Adam, Ruth Reeves, Catherine
Macleod, Rachel Szekely, Veronika
Zielinska, Brian Young, and Ralph
Grishman. 2004. The NomBank project:
An interim report. In Proceedings of the
NAACL-HLT Workshop on Frontiers
in Corpus Annotation, pages 24?31,
Boston, MA.
Moschitti, Alessandro, Paul Morarescu, and
Sanda M. Harabagiu. 2003. Open-domain
information extraction via automatic
semantic labeling. In Ingrid Russell and
Susan M. Haller, editors, Proceedings of the
Sixteenth International Florida Artificial
Intelligence Research Society Conference,
pages 397?401, St. Augustine, FL.
Narayanan, Srini and Sanda Harabagiu.
2004. Question answering based on
semantic structures. In Proceedings of
the 20th International Conference on
Computational Linguistics, Geneva.
Pado?, Sebastian and Katrin Erk. 2005.
To cause or not to cause: cross-lingual
semantic matching for paraphrase
modelling. In Proceedings of the
Cross-Language Knowledge Induction
Workshop, Cluj-Napoca.
Pado, Sebastian and Mirella Lapata. 2005.
Cross-linguistic projection of role-semantic
information. In Proceedings of Human
Language Technology Conference and
Conference on Empirical Methods in Natural
Language Processing, pages 859?866,
Vancouver.
Pennacchiotti, Marco, Diego De Cao, Roberto
Basili, Danilo Croce, and Michael Roth.
2008. Automatic induction of FrameNet
lexical units. In Proceedings of the 2008
Conference on Empirical Methods in Natural
Language Processing, pages 457?465,
Honolulu, HI.
Pradhan, Sameer S., Wayne H. Ward,
Kadri Hacioglu, James H. Martin, and
Dan Jurafsky. 2004. Shallow semantic
parsing using support vector machines.
In Proceedings of the Human Language
Technologies Conference of the North
American Chapter of the Association for
Computational Linguistics, pages 233?240,
Boston, MA.
Punyakanok, Vasin, Dan Roth, and Wen-tau
Yih. 2008. The importance of syntactic
parsing and inference in semantic role
labeling. Computational Linguistics,
34(2):257?287.
Punyakanok, Vasin, Dan Roth, Wen-tau Yih,
and Dav Zimak. 2004. Semantic role
labeling via integer linear programming
inference. In Proceedings of the 20th
International Conference on Computational
Linguistics, pages 1,346?1,352, Geneva.
Ratnaparkhi, Adwait. 1996. A maximum
entropy model for part-of-speech tagging.
In Proceedings of the 1996 Empirical
Methods in Natural Language Processing,
pages 133?142, Copenhagen.
Riedel, Sebastian and James Clarke. 2006.
Incremental integer linear programming
for non-projective dependency parsing.
In Proceedings of the 2006 Conference on
Empirical Methods in Natural Language
Processing, pages 129?137, Sydney.
Roth, Dan and Wen-tau Yih. 2004. A linear
programming formulation for global
inference in natural language tasks.
In Proceedings of the Eighth Conference on
Computational Natural Language Learning,
pages 1?8, Boston, MA.
Ruppenhofer, Josef, Michael Ellsworth,
Miriam R. L. Petruck, Christopher R.
54
Das et al. Frame-Semantic Parsing
Johnson, and Jan Scheffczyk. 2006.
FrameNet II: extended theory and
practice. International Computer Science
Institute, Berkeley, CA.
Rush, Alexander M. and Michael Collins.
2011. Exact decoding of syntactic
translation models through Lagrangian
relaxation. In Proceedings of the 49th Annual
Meeting of the Association for Computational
Linguistics: Human Language Technologies,
pages 72?82, Portland, OR.
Rush, Alexander M, David Sontag, Michael
Collins, and Tommi Jaakkola. 2010.
On dual decomposition and linear
programming relaxations for natural
language processing. In Proceedings of the
2010 Conference on Empirical Methods in
Natural Language Processing, pages 1?11,
Cambridge, MA.
Schuler, Karin K. 2005. VerbNet: a
broad-coverage, comprehensive verb lexicon.
Ph.D. thesis, University of Pennsylvania.
Sha, Fei and Fernando Pereira. 2003. Shallow
parsing with conditional random fields.
In Proceedings of the Human Language
Technology Conference of the North American
Chapter of the Association for Computational
Linguistics, pages 134?141, Edmonton.
Shen, Dan and Mirella Lapata. 2007. Using
semantic roles to improve question
answering. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural
Language Processing and Computational
Natural Language Learning, pages 12?21,
Prague.
Shi, Lei and Rada Mihalcea. 2004. An
algorithm for open text semantic parsing.
In Proceedings of Workshop on Robust
Methods in Analysis of Natural Language
Data, pages 59?67, Geneva.
Shi, Lei and Rada Mihalcea. 2005. Putting
pieces together: Combining FrameNet,
VerbNet and WordNet for robust
semantic parsing. In Proceedings of the 6th
International Conference on Computational
Linguistics and Intelligent Text Processing,
pages 100?111, Mexico City.
Smith, David A. and Jason Eisner. 2008.
Dependency parsing by belief
propagation. In Proceedings of the 2008
Conference on Empirical Methods in Natural
Language Processing, pages 145?156,
Honolulu, HI.
Subramanya, Amarnag and Jeff Bilmes.
2008. Soft-supervised learning for text
classification. In Proceedings of the 2008
Conference on Empirical Methods in Natural
Language Processing, pages 1,090?1,099,
Honolulu, HI.
Subramanya, Amarnag, Slav Petrov, and
Fernando Pereira. 2010. Efficient
graph-based semi-supervised learning of
structured tagging models. In Proceedings
of the 2010 Conference on Empirical Methods
in Natural Language Processing,
pages 167?176, Cambridge, MA.
Surdeanu, Mihai, Sanda Harabagiu,
John Williams, and Paul Aarseth. 2003.
Using predicate-argument structures for
information extraction. In Proceedings of the
41st Annual Meeting on Association for
Computational Linguistics, pages 8?15,
Sapporo.
Surdeanu, Mihai, Richard Johansson,
Adam Meyers, Llu??s Ma`rquez, and Joakim
Nivre. 2008. The CoNLL 2008 shared task
on joint parsing of syntactic and semantic
dependencies. In Proceedings of the Twelfth
Conference on Computational Natural
Language Learning, pages 159?177,
Manchester.
Szummer, Martin and Tommi Jaakkola.
2001. Partially labeled classification with
Markov random walks. In Advances in
Neural Information Processing Systems 14,
pages 945?952, Vancouver.
Talukdar, Partha Pratim and Koby Crammer.
2009. New regularized algorithms for
transductive learning. In Proceedings of the
European Conference on Machine Learning
and Knowledge Discovery in Databases,
pages 442?457, Bled.
Thompson, Cynthia A., Roger Levy,
and Christopher D. Manning. 2003.
A generative model for semantic role
labeling. In Proceedings of the European
Conference on Machine Learning,
pages 397?408, Cavtat-Dubrovnik.
Titov, Ivan and Alexandre Klementiev. 2012.
A Bayesian approach to unsupervised
semantic role induction. In Proceedings of
the 13th Conference of the European Chapter of
the Association for Computational Linguistics,
pages 12?22, Avignon.
Toutanova, Kristina, Aria Haghighi,
and Christopher Manning. 2005.
Joint learning improves semantic
role labeling. In Proceedings of the
43rd Annual Meeting of the Association
for Computational Linguistics,
pages 589?596, Ann Arbor, MI.
Turian, Joseph, Lev-Arie Ratinov,
and Yoshua Bengio. 2010. Word
representations: A simple and general
method for semi-supervised learning.
In Proceedings of the 48th Annual Meeting
of the Association for Computational
Linguistics, pages 384?394, Uppsala.
55
Computational Linguistics Volume 40, Number 1
Weston, Jason, Fre?de?ric Ratle, and
Ronan Collobert. 2008. Deep learning
via semi-supervised embedding.
In Proceedings of the 25th International
Conference on Machine Learning,
pages 1,168?1,175, Helsinki.
Xue, Nianwen and Martha Palmer. 2004.
Calibrating features for semantic role
labeling. In Proceedings of the 2004
Conference on Empirical Methods in Natural
Language Processing, pages 88?94,
Barcelona.
Yi, Szu-ting, Edward Loper, and Martha
Palmer. 2007. Can semantic roles
generalize across genres? In Proceedings of
the Human Language Technologies Conference
of the North American Chapter of the
Association for Computational Linguistics,
pages 548?555, Rochester, NY.
Zhu, Xiaojin. 2008. Semi-supervised
learning literature survey. Available at
http://pages.cs.wisc.edu/?jerryzhu/
pub/ssl survey.pdf. Last Accessed
July 2013.
Zhu, Xiaojin, Zoubin Ghahramani, and
John Lafferty. 2003. Semi-supervised
learning using Gaussian fields and
harmonic functions. In Proceedings of the
20th International Conference on Machine
Learning, pages 912?919, Washington, DC.
56
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 948?956,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Probabilistic Frame-Semantic Parsing
Dipanjan Das Nathan Schneider Desai Chen Noah A. Smith
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{dipanjan@cs,nschneid@cs,desaic@andrew,nasmith@cs}.cmu.edu
Abstract
This paper contributes a formalization of
frame-semantic parsing as a structure predic-
tion problem and describes an implemented
parser that transforms an English sentence
into a frame-semantic representation. It finds
words that evoke FrameNet frames, selects
frames for them, and locates the arguments
for each frame. The system uses two feature-
based, discriminative probabilistic (log-linear)
models, one with latent variables to permit
disambiguation of new predicate words. The
parser is demonstrated to significantly outper-
form previously published results.
1 Introduction
FrameNet (Fillmore et al, 2003) is a rich linguistic
resource containing considerable information about
lexical and predicate-argument semantics in En-
glish. Grounded in the theory of frame semantics
(Fillmore, 1982), it suggests?but does not formally
define?a semantic representation that blends word-
sense disambiguation and semantic role labeling.
In this paper, we present a computational and
statistical model for frame-semantic parsing, the
problem of extracting from text semantic predicate-
argument structures such as those shown in Fig. 1.
We aim to predict a frame-semantic representation
as a structure, not as a pipeline of classifiers. We
use a probabilistic framework that cleanly integrates
the FrameNet lexicon and (currently very limited)
available training data. Although our models often
involve strong independence assumptions, the prob-
abilistic framework we adopt is highly amenable to
future extension through new features, relaxed in-
dependence assumptions, and semisupervised learn-
ing. Some novel aspects of our current approach
include a latent-variable model that permits disam-
biguation of words not in the FrameNet lexicon, a
unified model for finding and labeling arguments,
TRANSITIVE_
ACTION
Agent
Patient
Event
Cause
Place
Time
CAUSE_TO_
MAKE_NOISE
Agent
Sound_maker
Cause
Place
Time
MAKE_NOISE
Noisy_event
Sound
Sound_source
Place
Time
cough.v, gobble.v, 
ring.v, yodel.v, ...
blare.v, play.v, 
ring.v, toot.v, ...
?
Inheritance relation Causative_of relation
Excludes relation
Purpose
Figure 2. Partial illustration of frames, roles, and LUs
related to the CAUSE TO MAKE NOISE frame, from the
FrameNet lexicon. ?Core? roles are filled ovals. 8 addi-
tional roles of CAUSE TO MAKE NOISE are not shown.
and a precision-boosting constraint that forbids ar-
guments of the same predicate to overlap. Our parser
achieves the best published results to date on the
SemEval?07 FrameNet task (Baker et al, 2007).
2 Resources and Task
We consider frame-semantic parsing resources.
2.1 FrameNet Lexicon
The FrameNet lexicon is a taxonomy of manu-
ally identified general-purpose frames for English.1
Listed in the lexicon with each frame are several
lemmas (with part of speech) that can denote the
frame or some aspect of it?these are called lexi-
cal units (LUs). In a sentence, word or phrase to-
kens that evoke a frame are known as targets. The
set of LUs listed for a frame in FrameNet may not
be exhaustive; we may see a target in new data that
does not correspond to an LU for the frame it evokes.
Each frame definition also includes a set of frame el-
ements, or roles, corresponding to different aspects
of the concept represented by the frame, such as par-
ticipants, props, and attributes. We use the term ar-
1Like the SemEval?07 participants, we used FrameNet v. 1.3
(http://framenet.icsi.berkeley.edu).
948
bell.n
ring.v
there be.v
enough.a
LU
NOISE_MAKERS
SUFFICIENCY
Frame
EXISTENCE
CAUSE_TO_MAKE_NOISE
.bells
 
 
N_m
more than six of the eight
Sound_maker
Enabled_situation
ringtoringers
Item
enough
Entity
Agent
n'tarestillthereBut
Figure 1. A sentence from PropBank and the SemEval?07 training data, and a partial depiction of gold FrameNet
annotations. Each frame is a row below the sentence (ordered for readability). Thick lines indicate targets that evoke
frames; thin solid/dotted lines with labels indicate arguments. ?N m? under bells is short for the Noise maker role of
the NOISE MAKERS frame. The last row indicates that there. . . are is a discontinuous target. In PropBank, the verb
ring is the only annotated predicate for this sentence, and it is not related to other predicates with similar meanings.
FRAMENET LEXICON V. 1.3
lexical exemplars
entries counts coverage
8379 LUs 139K sentences, 3.1M words 70% LUs
795 frames 1 frame annotation / sentence 63% frames
7124 roles 285K overt arguments 56% roles
Table 1. Snapshot of lexicon entries and exemplar sen-
tences. Coverage indicates the fraction of types attested
in at least one exemplar.
gument to refer to a sequence of word tokens anno-
tated as filling a frame role. Fig. 1 shows an exam-
ple sentence from the training data with annotated
targets, LUs, frames, and role-argument pairs. The
FrameNet lexicon also provides information about
relations between frames and between roles (e.g.,
INHERITANCE). Fig. 2 shows a subset of the rela-
tions between three frames and their roles.
Accompanying most frame definitions in the
FrameNet lexicon is a set of lexicographic exemplar
sentences (primarily from the British National Cor-
pus) annotated for that frame. Typically chosen to il-
lustrate variation in argument realization patterns for
the frame in question, these sentences only contain
annotations for a single frame. We found that using
exemplar sentences directly to train our models hurt
performance as evaluated on SemEval?07 data, even
though the number of exemplar sentences is an order
of magnitude larger than the number of sentences in
our training set (?2.2). This is presumably because
the exemplars are neither representative as a sample
nor similar to the test data. Instead, we make use of
these exemplars in features (?4.2).
2.2 Data
Our training, development, and test sets consist
of documents annotated with frame-semantic struc-
tures for the SemEval?07 task, which we refer to col-
FULL-TEXT SemEval?07 data
ANNOTATIONS train dev test
Size (words sentences documents)
all 43.3K1.7K 22 6.3K 251 4 2.8K 120 3
ANC (travel) 3.9K 154 2 .8K 32 1 1.3K 67 1
NTI (bureaucratic) 32.2K1.2K 15 5.5K 219 3 1.5K 53 2
PropBank (news) 7.3K 325 5 0 0 0 0 0 0
Annotations (frames/word overt arguments/word)
all 0.23 0.39 0.22 0.37 0.37 0.65
Coverage of lexicon (% frames % roles % LUs)
all 64.1 27.4 21.0 34.0 10.2 7.3 29.3 7.7 4.9
Out-of-lexicon types (frames roles LUs)
all 14 69 71 2 4 2 39 99 189
Out-of-lexicon tokens (% frames % roles % LUs)
all 0.7 0.9 1.1 1.0 0.4 0.2 9.8 11.2 25.3
Table 2. Snapshot of the SemEval?07 annotated data.
lectively as the SemEval?07 data.2 For the most
part, the frames and roles used in annotating these
documents were defined in the FrameNet lexicon,
but there are some exceptions for which the annota-
tors defined supplementary frames and roles; these
are included in the possible output of our parser.
Table 2 provides a snapshot of the SemEval?07
data. We randomly selected three documents from
the original SemEval training data to create a devel-
opment set for tuning model hyperparameters. No-
tice that the test set contains more annotations per
word, both in terms of frames and arguments. More-
over, there are many more out-of-lexicon frame,
role, and LU types in the test set than in the training
set. This inconsistency in the data results in poor re-
call scores for all models trained on the given data
split, a problem we have not sought to address here.
2http://framenet.icsi.berkeley.edu/
semeval/FSSE.html
949
Preprocessing. We preprocess sentences in our
dataset with a standard set of annotations: POS
tags from MXPOST (Ratnaparkhi, 1996) and depen-
dency parses from the MST parser (McDonald et al,
2005) since manual syntactic parses are not available
for most of the FrameNet-annotated documents. We
used WordNet (Fellbaum, 1998) for lemmatization.
We also labeled each verb in the data as having AC-
TIVE or PASSIVE voice, using code from the SRL
system described by Johansson and Nugues (2008).
2.3 Task and Evaluation
Automatic annotations of frame-semantic structure
can be broken into three parts: (1) targets, the words
or phrases that evoke frames; (2) the frame type,
defined in the lexicon, evoked by each target; and
(3) the arguments, or spans of words that serve to
fill roles defined by each evoked frame. These cor-
respond to the three subtasks in our parser, each
described and evaluated in turn: target identifica-
tion (?3), frame identification (?4, not unlike word-
sense disambiguation), and argument identification
(?5, not unlike semantic role labeling).
The standard evaluation script from the
SemEval?07 shared task calculates precision,
recall, and F1-measure for frames and arguments;
it also provides a score that gives partial credit
for hypothesizing a frame related to the correct
one. We present precision, recall, and F1-measure
microaveraged across the test documents, report
labels-only matching scores (spans must match
exactly), and do not use named entity labels. More
details can be found in Baker et al (2007). For our
experiments, statistical significance is measured us-
ing a reimplementation of Dan Bikel?s randomized
parsing evaluation comparator.3
2.4 Baseline
A strong baseline for frame-semantic parsing is
the system presented by Johansson and Nugues
(2007, hereafter J&N?07), the best system in the
SemEval?07 shared task. For frame identifica-
tion, they used an SVM classifier to disambiguate
frames for known frame-evoking words. They used
WordNet synsets to extend the vocabulary of frame-
evoking words to cover unknown words, and then
3http://www.cis.upenn.edu/?dbikel/
software.html#comparator
TARGET IDENTIFICATION P R F1
Our technique (?3) 89.92 70.79 79.21
Baseline: J&N?07 87.87 67.11 76.10
Table 3. Target identification results for our system and
the baseline. Scores in bold denote significant improve-
ments over the baseline (p < 0.05).
used a collection of separate SVM classifiers?one
for each frame?to predict a single evoked frame for
each occurrence of a word in the extended set.
J&N?07 modeled the argument identification
problem by dividing it into two tasks: first, they
classified candidate spans as to whether they were
arguments or not; then they assigned roles to those
that were identified as arguments. Both phases used
SVMs. Thus, their formulation of the problem in-
volves a multitude of classifiers?whereas ours uses
two log-linear models, each with a single set of
weights, to find a full frame-semantic parse.
3 Target Identification
Target identification is the problem of deciding
which word tokens (or word token sequences) evoke
frames in a given sentence. In other semantic role
labeling schemes (e.g. PropBank), simple part-of-
speech criteria typically distinguish predicates from
non-predicates. But in frame semantics, verbs,
nouns, adjectives, and even prepositions can evoke
frames under certain conditions. One complication
is that semantically-impoverished support predi-
cates (such as make in make a request) do not
evoke frames in the context of a frame-evoking,
syntactially-dependent noun (request). Further-
more, only temporal, locative, and directional senses
of prepositions evoke frames.
We found that, because the test set is more com-
pletely annotated?that is, it boasts far more frames
per token than the training data (see Table 2)?
learned models did not generalize well and achieved
poor test recall. Instead, we followed J&N?07 in us-
ing a small set of rules to identify targets.
For a span to be a candidate target, it must ap-
pear (up to morphological variation) as a target in the
training data or the lexicon. We consider multiword
targets,4 unlike J&N?07 (though we do not consider
4There are 629 multiword LUs in the lexicon, and they cor-
respond to 4.8% of the targets in the training set; among them
are screw up.V, shoot the breeze.V, and weapon of mass de-
950
FRAME IDENTIFICATION exact frame matching partial frame matching
(?4) targets P R F1 P R F1
Frame identification (oracle targets) ? 60.21 60.21 60.21 74.21 74.21 74.21
Frame identification (predicted targets) auto ?3 69.75 54.91 61.44 77.51 61.03 68.29
Baseline: J&N?07 auto 66.22 50.57 57.34 73.86 56.41 63.97
Table 4. Frame identification results. Precision, recall, and F1 were evaluated under exact and partial frame matching;
see ?2.3. Bold indicates statistically significant results with respect to the baseline (p < 0.05).
discontinuous targets). Using rules from ?3.1.1 of
J&N?07, we further prune the list, with two modi-
fications: we prune all prepositions, including loca-
tive, temporal, and directional ones, but do not prune
support verbs. This is a conservative approach; our
automatic target identifier will never propose a target
that was not seen in the training data or FrameNet.
Results. Table 3 shows results on target identifica-
tion; our system gains 3 F1 points over the baseline.
4 Frame Identification
Given targets, the parser next identifies their frames.
4.1 Lexical units
FrameNet specifies a great deal of structural infor-
mation both within and among frames. For frame
identification we make use of frame-evoking lexical
units, the (lemmatized and POS-tagged) words and
phrases listed in the lexicon as referring to specific
frames. For example, listed with the BRAGGING
frame are 10 LUs, including boast.N, boast.V, boast-
ful.A, brag.V, and braggart.N. Of course, due to pol-
ysemy and homonymy, the same LU may be associ-
ated with multiple frames; for example, gobble.V is
listed under both the INGESTION and MAKE NOISE
frames. All targets in the exemplar sentences, and
most in our training and test data, correspond to
known LUs (see Table 2).
To incorporate frame-evoking expressions found
in the training data but not the lexicon?and to avoid
the possibility of lemmatization errors?our frame
identification model will incorporate, via a latent
variable, features based directly on exemplar and
training targets rather than LUs. Let L be the set of
(unlemmatized and automatically POS-tagged) tar-
gets found in the exemplar sentences of the lexi-
con and/or the sentences in our training set. Let
Lf ? L be the subset of these targets annotated as
struction.N. In the SemEval?07 training data, there are just 99
discontinuous multiword targets (1% of all targets).
evoking a particular frame f . Let Ll and Llf de-
note the lemmatized versions of L and Lf respec-
tively. Then, we write boasted.VBD ? LBRAGGING
and boast.VBD ? LlBRAGGING to indicate that this in-
flected verb boasted and its lemma boast have been
seen to evoke the BRAGGING frame. Significantly,
however, another target, such as toot your own horn,
might be used in other data to evoke this frame. We
thus face the additional hurdle of predicting frames
for unknown words.
The SemEval annotators created 47 new frames
not present in the lexicon, out of which 14 belonged
to our training set. We considered these with the 795
frames in the lexicon when parsing new data. Pre-
dicting new frames is a challenge not yet attempted
to our knowledge (including here). Note that the
scoring metric (?2.3) gives partial credit for related
frames (e.g., a more general frame from the lexicon).
4.2 Model
For a given sentence x with frame-evoking targets t,
let ti denote the ith target (a word sequence). Let tli
denote its lemma. We seek a list f = ?f1, . . . , fm?
of frames, one per target. In our model, the set of
candidate frames for ti is defined to include every
frame f such that tli ? L
l
f?or if t
l
i 6? L
l, then every
known frame (the latter condition applies for 4.7%
of the gold targets in the development set). In both
cases, we let Fi be the set of candidate frames for
the ith target in x.
To allow frame identification for targets whose
lemmas were seen in neither the exemplars nor the
training data, our model includes an additional vari-
able, `i. This variable ranges over the seen targets
in Lfi , which can be thought of as prototypes for
the expression of the frame. Importantly, frames are
predicted, but prototypes are summed over via the
latent variable. The prediction rule requires a prob-
abilistic model over frames for a target:
fi ? argmaxf?Fi
?
`?Lf
p(f, ` | ti,x) (1)
951
We adopt a conditional log-linear model: for f ? Fi
and ` ? Lf , p?(f, ` | ti,x) =
exp?>g(f, `, ti,x)
?
f ??Fi
?
`??Lf ?
exp?>g(f ?, `?, ti,x)
(2)
where ? are the model weights, and g is a vector-
valued feature function. This discriminative formu-
lation is very flexible, allowing for a variety of (pos-
sibly overlapping) features; e.g., a feature might re-
late a frame type to a prototype, represent a lexical-
semantic relationship between a prototype and a tar-
get, or encode part of the syntax of the sentence.
Previous work has exploited WordNet for better
coverage during frame identification (Johansson and
Nugues, 2007; Burchardt et al, 2005, e.g., by ex-
panding the set of targets using synsets), and others
have sought to extend the lexicon itself (see ?6). We
differ in our use of a latent variable to incorporate
lexical-semantic features in a discriminative model,
relating known lexical units to unknown words that
may evoke frames. Here we are able to take advan-
tage of the large inventory of partially-annotated ex-
emplar sentences.
Note that this model makes a strong independence
assumption: each frame is predicted independently
of all others in the document. In this way the model
is similar to J&N?07. However, ours is a single
conditional model that shares features and weights
across all targets, frames, and prototypes, whereas
the approach of J&N?07 consists of many separately
trained models. Moreover, our model is unique in
that it uses a latent variable to smooth over frames
for unknown or ambiguous LUs.
Frame identification features depend on the pre-
processed sentence x, the prototype ` and its
WordNet lexical-semantic relationship with the tar-
get ti, and of course the frame f . Our model instan-
tiates 662,020 binary features; see Das et al (2010).
4.3 Training
Given the training subset of the SemEval?07 data,
which is of the form
?
?x(j), t(j), f (j),A(j)?
?N
j=1
(N = 1663 is the number of sentences), we dis-
criminatively train the frame identification model by
maximizing the following log-likelihood:5
5We found no benefit on development data from using an L2
regularizer (zero-mean Gaussian prior).
max
?
N?
j=1
mj?
i=1
log
?
`?L
f
(j)
i
p?(f
(j)
i , ` | t
(j)
i ,x
(j)) (3)
Note that the training problem is non-convex be-
cause of the summed-out prototype latent variable
` for each frame. To calculate the objective func-
tion, we need to cope with a sum over frames and
prototypes for each target (see Eq. 2), often an ex-
pensive operation. We locally optimize the function
using a distributed implementation of L-BFGS. This
is the most expensive model that we train: with 100
CPUs, training takes several hours. (Decoding takes
only a few minutes on one CPU for the test set.)
4.4 Results
We evaluate the performance of our frame identifi-
cation model given gold-standard targets and auto-
matically identified targets (?3); see Table 4.
Given gold-standard targets, our model is able
to predict frames for lemmas not seen in training,
of which there are 210. The partial-match evalua-
tion gives our model some credit for 190 of these,
4 of which are exactly correct. The hidden vari-
able model, then, is finding related (but rarely exact)
frames for unknown target words. The net effect of
our conservative target identifier on F1 is actually
positive: the frame identifier is far more precise for
targets seen explicitly in training. Together, our tar-
get and frame identification outperform the baseline
by 4 F1 points. To compare the frame identification
stage in isolation with that of J&N?07, we ran our
frame identification model with the targets identified
by their system as input. With partial matching, our
model achieves a relative improvement of 0.6% F1
over J&N?07 (though this is not significant).
While our frame identification model thus per-
forms on par with the current state of the art for
this task, it improves upon J&N?s formulation of
the problem because it requires only a single model,
learns lexical-semantic features as part of that model
rather than requiring a preprocessing step to expand
the vocabulary of frame-evoking words, and is prob-
abilistic, which can facilitate global reasoning.
5 Argument Identification
Given a sentence x = ?x1, . . . , xn?, the set of tar-
gets t = ?t1, . . . , tm?, and a list of evoked frames
952
f = ?f1, . . . , fm? corresponding to each target, ar-
gument identification is the task of choosing which
of each fi?s roles are filled, and by which parts of x.
This task is most similar to the problem of semantic
role labeling, but uses frame-specific labels that are
richer than the PropBank annotations.
5.1 Model
Let Rfi = {r1, . . . , r|Rfi |} denote frame fi?s roles
(named frame element types) observed in an exem-
plar sentence and/or our training set. A subset of
each frame?s roles are marked as core roles; these
roles are conceptually and/or syntactically necessary
for any given use of the frame, though they need
not be overt in every sentence involving the frame.
These are roughly analogous to the core arguments
A0?A5 and AA in PropBank. Non-core roles?
analogous to the various AMs in PropBank?loosely
correspond to syntactic adjuncts, and carry broadly-
applicable information such as the time, place, or
purpose of an event. The lexicon imposes some
additional structure on roles, including relations to
other roles in the same or related frames, and se-
mantic types with respect to a small ontology (mark-
ing, for instance, that the entity filling the protag-
onist role must be sentient for frames of cogni-
tion). Fig. 2 illustrates some of the structural ele-
ments comprising the frame lexicon by considering
the CAUSE TO MAKE NOISE frame.
We identify a set S of spans that are candidates for
filling any role r ? Rfi . In principle, S could con-
tain any subsequence of x, but in this work we only
consider the set of contiguous spans that (a) contain
a single word or (b) comprise a valid subtree of a
word and all its descendants in the dependency parse
produced by the MST parser. This covers 81% of ar-
guments in the development data. The empty span
is also included in S, since some roles are not ex-
plicitly filled; in the development data, the average
number of roles an evoked frame defines is 6.7, but
the average number of overt arguments is only 1.7.6
In training, if a labeled argument is not a valid sub-
6In the annotated data, each core role is filled with one of
three types of null instantiations indicating how the role is con-
veyed implicitly. E.g., the imperative construction implicitly
designates a role as filled by the addressee, and the correspond-
ing filler is thus CNI (constructional null instantiation). In this
work we do not distinguish different types of null instantiations.
tree of the dependency parse, we add its span to S .
Let Ai denote the mapping of roles in Rfi to
spans in S. Our model makes a prediction for each
Ai(rk) (for all roles rk ? Rfi) using:
Ai(rk)? argmaxs?S p(s | rk, fi, ti,x) (4)
We use a conditional log-linear model over spans for
each role of each evoked frame:
p?(Ai(rk) = s | fi, ti,x) = (5)
exp?>h(s, rk, fi, ti,x)
?
s??S exp?
>h(s?, rk, fi, ti,x)
Note that our model chooses the span for each
role separately from the other roles and ignores all
frames except the frame the role belongs to. Our
model departs from the traditional SRL literature by
modeling the argument identification problem in a
single stage, rather than first classifying token spans
as arguments and then labeling them. A constraint
implicit in our formulation restricts each role to have
at most one overt argument, which is consistent with
96.5% of the role instances in the training data.
Out of the overt argument spans in the training
data, 12% are duplicates, having been used by some
previous frame in the sentence (supposing some ar-
bitrary ordering of frames). Our role-filling model,
unlike a sentence-global argument detection-and-
classification approach,7 permits this sort of argu-
ment sharing among frames. The incidence of span
overlap among frames is much higher; Fig. 1 illus-
trates a case with a high degree of overlap. Word
tokens belong to an average of 1.6 argument spans
each, including the quarter of words that do not be-
long to any argument.
Features for our log-linear model (Eq. 5) depend
on the preprocessed sentence x; the target t; a
role r of frame f ; and a candidate argument span
s ? S. Our model includes lexicalized and unlexi-
calized features considering aspects of the syntactic
parse (most notably the dependency path in the parse
from the target to the argument); voice; word order-
ing/overlap/distance of the argument with respect to
the target; and POS tags within and around the argu-
ment. Many features have a version specific to the
frame and role, plus a smoothed version incorporat-
ing the role name, but not the frame. These features
7J&N?07, like us, identify arguments for each target.
953
are fully enumerated in (Das et al, 2010); instanti-
ating them for our data yields 1,297,857 parameters.
5.2 Training
We train the argument identification model by:
max
?
N?
j=1
mj?
i=1
|R
f
(j)
i
|
?
k=1
log p?(A
(j)
i (rk) | f
(j)
i , t
(j)
i ,x
(j))
(6)
This objective function is concave, and we globally
optimize it using stochastic gradient ascent (Bottou,
2004). We train this model until the argument iden-
tification F1 score stops increasing on the develop-
ment data. Best results on this dataset were obtained
with a batch size of 2 and 23 passes through the data.
5.3 Approximate Joint Decoding
Na??ve prediction of roles using Eq. 4 may result
in overlap among arguments filling different roles
of a frame, since the argument identification model
fills each role independently of the others. We want
to enforce the constraint that two roles of a single
frame cannot be filled by overlapping spans. We dis-
allow illegal overlap using a 10000-hypothesis beam
search; the algorithm is given in (Das et al, 2010).
5.4 Results
Performance of the argument identification model
is presented in Table 5. The table shows how per-
formance varies given different types of perfect in-
put: correct targets, correct frames, and the set of
correct spans; correct targets and frames, with the
heuristically-constructed set of candidate spans; cor-
rect targets only, with model frames; and ultimately,
no oracle input (the full frame parsing scenario).
The first four rows of results isolate the argu-
ment identification task from the frame identifica-
tion task. Given gold targets and frames and an ora-
cle set of argument spans, our local model achieves
about 87% precision and 75% recall. Beam search
decoding to eliminate illegal argument assignments
within a frame (?5.3) further improves precision by
about 1.6%, with negligible harm to recall. Note
that 96.5% recall is possible under the constraint that
roles are not multiply-filled (?5.1); there is thus con-
siderable room for improvement with this constraint
in place. Joint prediction of each frame?s arguments
is worth exploring to capture correlations not en-
coded in our local models or joint decoding scheme.
The 15-point drop in recall when the heuristically-
built candidate argument set replaces the set of true
argument spans is unsurprising: an estimated 19% of
correct arguments are excluded because they are nei-
ther single words nor complete subtrees (see ?5.1).
Qualitatively, the problem of candidate span recall
seems to be largely due to syntactic parse errors.8
Still, the 10-point decrease in precision when using
the syntactic parse to determine candidate spans sug-
gests that the model has trouble discriminating be-
tween good and bad arguments, and that additional
feature engineering or jointly decoding arguments of
a sentence?s frames may be beneficial in this regard.
The fifth and sixth rows show the effect of auto-
matic frame identification on overall frame parsing
performance. There is a 22% decrease in F1 (18%
when partial credit is given for related frames), sug-
gesting that improved frame identification or joint
prediction of frames and arguments is likely to have
a sizeable impact on overall performance.
The final two rows of the table compare our full
model (target, frame, and argument identification)
with the baseline, showing significant improvement
of more than 4.4 F1 points for both exact and partial
frame matching. As with frame identification, we
compared the argument identification stage with that
of J&N?07 in isolation, using the automatically iden-
tified targets and frames from the latter as input to
our model. With partial frame matching, this gave us
an F1 score of 48.1% on the test set?significantly
better (p < 0.05) than 45.6%, the full parsing re-
sult from J&N?07. This indicates that our argument
identification model?which uses a single discrim-
inative model with a large number of features for
role filling (rather than argument labeling)?is more
powerful than the previous state of the art.
6 Related work
Since Gildea and Jurafsky (2002) pioneered statis-
tical semantic role labeling, a great deal of com-
8Note that, because of our labels-only evaluation scheme
(?2.3), arguments missing a word or containing an extra word
receive no credit. In fact, of the frame roles correctly predicted
as having an overt span, the correct span was predicted 66% of
the time, while 10% of the time the predicted starting and end-
ing boundaries of the span were off by a total of 1 or 2 words.
954
ARGUMENT IDENTIFICATION exact frame matching
targets frames spans decoding P R F1
Argument identifica-
tion (oracle spans)
? ? ? na??ve 86.61 75.11 80.45
? ? ? beam ?5.3 88.29 74.77 80.97
Argument identifica-
tion (full)
? ? model ?5 na??ve 77.43 60.76 68.09 partial frame matching
? ? model ?5 beam ?5.3 78.71 60.57 68.46 P R F1
Parsing (oracle targets) ? model ?4 model ?5 beam ?5.3 49.68 42.82 46.00 57.85 49.86 53.56
Parsing (full) auto ?3 model ?4 model ?5 beam ?5.3 58.08 38.76 46.49 62.76 41.89 50.24
Baseline: J&N?07 auto model model N/A 51.59 35.44 42.01 56.01 38.48 45.62
Table 5. Argument identification results. ? indicates that gold-standard labels were used for a given pipeline stage.
For full parsing, bolded scores indicate significant improvements relative to the baseline (p < 0.05).
putational work has investigated predicate-argument
structures for semantics. Briefly, we highlight some
relevant work, particularly research that has made
use of FrameNet. (Note that much related research
has focused on PropBank (Kingsbury and Palmer,
2002), a set of shallow predicate-argument annota-
tions for Wall Street Journal articles from the Penn
Treebank (Marcus et al, 1993); a recent issue of CL
(Ma`rquez et al, 2008) was devoted to the subject.)
Most work on frame-semantic role labeling has
made use of the exemplar sentences in the FrameNet
corpus (see ?2.1), each of which is annotated for a
single frame and its arguments. On the probabilis-
tic modeling front, Gildea and Jurafsky (2002) pre-
sented a discriminative model for arguments given
the frame; Thompson et al (2003) used a gener-
ative model for both the frame and its arguments;
and Fleischman et al (2003) first used maximum
entropy models to find and label arguments given
the frame. Shi and Mihalcea (2004) developed a
rule-based system to predict frames and their argu-
ments in text, and Erk and Pado? (2006) introduced
the Shalmaneser tool, which employs Na??ve Bayes
classifiers to do the same. Other FrameNet SRL
systems (Giuglea and Moschitti, 2006, for instance)
have used SVMs. Most of this work was done on an
older, smaller version of FrameNet.
Recent work on frame-semantic parsing?in
which sentences may contain multiple frames to be
recognized along with their arguments?has used
the SemEval?07 data (Baker et al, 2007). The LTH
system of Johansson and Nugues (2007), our base-
line (?2.4), performed the best in the SemEval?07
task. Matsubayashi et al (2009) trained a log-
linear model on the SemEval?07 data to evaluate
argument identification features exploiting various
types of taxonomic relations to generalize over roles.
A line of work has sought to extend the coverage
of FrameNet by exploiting VerbNet, WordNet, and
Wikipedia (Shi and Mihalcea, 2005; Giuglea and
Moschitti, 2006; Pennacchiotti et al, 2008; Tonelli
and Giuliano, 2009), and projecting entries and an-
notations within and across languages (Boas, 2002;
Fung and Chen, 2004; Pado? and Lapata, 2005;
Fu?rstenau and Lapata, 2009). Others have applied
frame-semantic structures to question answering,
paraphrase/entailment recognition, and information
extraction (Narayanan and Harabagiu, 2004; Shen
and Lapata, 2007; Pado? and Erk, 2005; Burchardt,
2006; Moschitti et al, 2003; Surdeanu et al, 2003).
7 Conclusion
We have provided a supervised model for rich
frame-semantic parsing, based on a combination
of knowledge from FrameNet, two probabilistic
models trained on SemEval?07 data, and expedi-
ent heuristics. Our system achieves improvements
over the state of the art at each stage of process-
ing and collectively, and is amenable to future ex-
tension. Our parser is available for download at
http://www.ark.cs.cmu.edu/SEMAFOR.
Acknowledgments
We thank Collin Baker, Katrin Erk, Richard Johansson,
and Nils Reiter for software, data, evaluation scripts, and
methodological details. We thank the reviewers, Alan
Black, Ric Crabbe, Michael Ellsworth, Rebecca Hwa,
Dan Klein, Russell Lee-Goldman, Dan Roth, Josef Rup-
penhofer, and members of the ARK group for helpful
comments. This work was supported by DARPA grant
NBCH-1080004, NSF grant IIS-0836431, and computa-
tional resources provided by Yahoo.
955
References
C. Baker, M. Ellsworth, and K. Erk. 2007. SemEval-
2007 Task 19: frame semantic structure extraction. In
Proc. of SemEval.
H. C. Boas. 2002. Bilingual FrameNet dictionaries for
machine translation. In Proc. of LREC.
L. Bottou. 2004. Stochastic learning. In Advanced Lec-
tures on Machine Learning. Springer-Verlag.
A. Burchardt, K. Erk, and A. Frank. 2005. A WordNet
detour to FrameNet. In B. Fisseni, H.-C. Schmitz,
B. Schro?der, and P. Wagner, editors, Sprachtech-
nologie, mobile Kommunikation und linguistische Re-
sourcen, volume 8. Peter Lang.
A. Burchardt. 2006. Approaching textual entailment
with LFG and FrameNet frames. In Proc. of the Sec-
ond PASCAL RTE Challenge Workshop.
D. Das, N. Schneider, D. Chen, and N. A. Smith.
2010. SEMAFOR 1.0: A probabilistic frame-semantic
parser. Technical Report CMU-LTI-10-001, Carnegie
Mellon University.
K. Erk and S. Pado?. 2006. Shalmaneser - a toolchain for
shallow semantic parsing. In Proc. of LREC.
C. Fellbaum, editor. 1998. WordNet: an electronic lexi-
cal database. MIT Press, Cambridge, MA.
C. J. Fillmore, C. R. Johnson, and M. R.L. Petruck. 2003.
Background to FrameNet. International Journal of
Lexicography, 16(3).
C. J. Fillmore. 1982. Frame semantics. In Linguistics in
the Morning Calm, pages 111?137. Hanshin Publish-
ing Co., Seoul, South Korea.
M. Fleischman, N. Kwon, and E. Hovy. 2003. Maximum
entropy models for FrameNet classification. In Proc.
of EMNLP.
P. Fung and B. Chen. 2004. BiFrameNet: bilin-
gual frame semantics resource construction by cross-
lingual induction. In Proc. of COLING.
H. Fu?rstenau and M. Lapata. 2009. Semi-supervised se-
mantic role labeling. In Proc. of EACL.
D. Gildea and D. Jurafsky. 2002. Automatic labeling of
semantic roles. Computational Linguistics, 28(3).
A.-M. Giuglea and A. Moschitti. 2006. Shallow
semantic parsing based on FrameNet, VerbNet and
PropBank. In Proc. of ECAI 2006.
R. Johansson and P. Nugues. 2007. LTH: semantic struc-
ture extraction using nonprojective dependency trees.
In Proc. of SemEval.
R. Johansson and P. Nugues. 2008. Dependency-based
semantic role labeling of PropBank. In Proc. of
EMNLP.
P. Kingsbury and M. Palmer. 2002. From TreeBank to
PropBank. In Proc. of LREC.
M. P. Marcus, M. A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of English:
the Penn Treebank. Computational Linguistics, 19(2).
L. Ma`rquez, X. Carreras, K. C. Litkowski, and S. Steven-
son. 2008. Semantic role labeling: an introduction to
the special issue. Computational Linguistics, 34(2).
Y. Matsubayashi, N. Okazaki, and J. Tsujii. 2009. A
comparative study on generalization of semantic roles
in FrameNet. In Proc. of ACL-IJCNLP.
R. McDonald, K. Crammer, and F. Pereira. 2005. Online
large-margin training of dependency parsers. In Proc.
of ACL.
A. Moschitti, P. Mora?rescu, and S. M. Harabagiu. 2003.
Open-domain information extraction via automatic se-
mantic labeling. In Proc. of FLAIRS.
S. Narayanan and S. Harabagiu. 2004. Question answer-
ing based on semantic structures. In Proc. of COLING.
S. Pado? and K. Erk. 2005. To cause or not to cause:
cross-lingual semantic matching for paraphrase mod-
elling. In Proc. of the Cross-Language Knowledge In-
duction Workshop.
S. Pado? and M. Lapata. 2005. Cross-linguistic projec-
tion of role-semantic information. In Proc. of HLT-
EMNLP.
M. Pennacchiotti, D. De Cao, R. Basili, D. Croce, and
M. Roth. 2008. Automatic induction of FrameNet
lexical units. In Proc. of EMNLP.
A. Ratnaparkhi. 1996. A maximum entropy model for
part-of-speech tagging. In Proc. of EMNLP.
D. Shen and M. Lapata. 2007. Using semantic roles
to improve question answering. In Proc. of EMNLP-
CoNLL.
L. Shi and R. Mihalcea. 2004. An algorithm for open
text semantic parsing. In Proc. of Workshop on Robust
Methods in Analysis of Natural Language Data.
L. Shi and R. Mihalcea. 2005. Putting pieces together:
combining FrameNet, VerbNet and WordNet for ro-
bust semantic parsing. In Computational Linguis-
tics and Intelligent Text Processing: Proc. of CICLing
2005. Springer-Verlag.
M. Surdeanu, S. Harabagiu, J. Williams, and P. Aarseth.
2003. Using predicate-argument structures for infor-
mation extraction. In Proc. of ACL.
C. A. Thompson, R. Levy, and C. D. Manning. 2003. A
generative model for semantic role labeling. In Proc.
of ECML.
S. Tonelli and C. Giuliano. 2009. Wikipedia as frame
information repository. In Proc. of EMNLP.
956
Proceedings of NAACL-HLT 2013, pages 380?390,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Improved Part-of-Speech Tagging for Online Conversational Text
with Word Clusters
Olutobi Owoputi? Brendan O?Connor? Chris Dyer?
Kevin Gimpel? Nathan Schneider? Noah A. Smith?
?School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, USA
?Toyota Technological Institute at Chicago, Chicago, IL 60637, USA
Corresponding author: brenocon@cs.cmu.edu
Abstract
We consider the problem of part-of-speech
tagging for informal, online conversational
text. We systematically evaluate the use of
large-scale unsupervised word clustering
and new lexical features to improve tagging
accuracy. With these features, our system
achieves state-of-the-art tagging results on
both Twitter and IRC POS tagging tasks;
Twitter tagging is improved from 90% to 93%
accuracy (more than 3% absolute). Quali-
tative analysis of these word clusters yields
insights about NLP and linguistic phenomena
in this genre. Additionally, we contribute the
first POS annotation guidelines for such text
and release a new dataset of English language
tweets annotated using these guidelines.
Tagging software, annotation guidelines, and
large-scale word clusters are available at:
http://www.ark.cs.cmu.edu/TweetNLP
This paper describes release 0.3 of the ?CMU
Twitter Part-of-Speech Tagger? and annotated
data.
1 Introduction
Online conversational text, typified by microblogs,
chat, and text messages,1 is a challenge for natu-
ral language processing. Unlike the highly edited
genres that conventional NLP tools have been de-
veloped for, conversational text contains many non-
standard lexical items and syntactic patterns. These
are the result of unintentional errors, dialectal varia-
tion, conversational ellipsis, topic diversity, and cre-
ative use of language and orthography (Eisenstein,
2013). An example is shown in Fig. 1. As a re-
sult of this widespread variation, standard model-
ing assumptions that depend on lexical, syntactic,
and orthographic regularity are inappropriate. There
1Also referred to as computer-mediated communication.
ikr
!
smh
G
he
O
asked
V
fir
P
yo
D
last
A
name
N
so
P
he
O
can
V
add
V
u
O
on
P
fb
?
lololol
!
Figure 1: Automatically tagged tweet showing nonstan-
dard orthography, capitalization, and abbreviation. Ignor-
ing the interjections and abbreviations, it glosses as He
asked for your last name so he can add you on Facebook.
The tagset is defined in Appendix A. Refer to Fig. 2 for
word clusters corresponding to some of these words.
is preliminary work on social media part-of-speech
(POS) tagging (Gimpel et al, 2011), named entity
recognition (Ritter et al, 2011; Liu et al, 2011), and
parsing (Foster et al, 2011), but accuracy rates are
still significantly lower than traditional well-edited
genres like newswire. Even web text parsing, which
is a comparatively easier genre than social media,
lags behind newspaper text (Petrov and McDonald,
2012), as does speech transcript parsing (McClosky
et al, 2010).
To tackle the challenge of novel words and con-
structions, we create a new Twitter part-of-speech
tagger?building on previous work by Gimpel et
al. (2011)?that includes new large-scale distribu-
tional features. This leads to state-of-the-art results
in POS tagging for both Twitter and Internet Relay
Chat (IRC) text. We also annotated a new dataset of
tweets with POS tags, improved the annotations in
the previous dataset from Gimpel et al, and devel-
oped annotation guidelines for manual POS tagging
of tweets. We release all of these resources to the
research community:
? an open-source part-of-speech tagger for online
conversational text (?2);
? unsupervised Twitter word clusters (?3);
? an improved emoticon detector for conversational
text (?4);
380
? POS annotation guidelines (?5.1); and
? a new dataset of 547 manually POS-annotated
tweets (?5).
2 MEMM Tagger
Our tagging model is a first-order maximum en-
tropy Markov model (MEMM), a discriminative se-
quence model for which training and decoding are
extremely efficient (Ratnaparkhi, 1996; McCallum
et al, 2000).2 The probability of a tag yt is condi-
tioned on the input sequence x and the tag to its left
yt?1, and is parameterized by a multiclass logistic
regression:
p(yt = k | yt?1,x, t;?) ?
exp
(
?(trans)yt?1,k +
?
j ?
(obs)
j,k fj(x, t)
)
We use transition features for every pair of labels,
and extract base observation features from token t
and neighboring tokens, and conjoin them against
all K = 25 possible outputs in our coarse tagset
(Appendix A). Our feature sets will be discussed
below in detail.
Decoding. For experiments reported in this paper,
we use the O(|x|K2) Viterbi algorithm for predic-
tion; K is the number of tags. This exactly max-
imizes p(y | x), but the MEMM also naturally al-
lows a fasterO(|x|K) left-to-right greedy decoding:
for t = 1 . . . |x|:
y?t ? argmaxk p(yt = k | y?t?1,x, t;?)
which we find is 3 times faster and yields similar ac-
curacy as Viterbi (an insignificant accuracy decrease
of less than 0.1% absolute on the DAILY547 test set
discussed below). Speed is paramount for social me-
dia analysis applications?which often require the
processing of millions to billions of messages?so
we make greedy decoding the default in the released
software.
2Although when compared to CRFs, MEMMs theoretically
suffer from the ?label bias? problem (Lafferty et al, 2001), our
system substantially outperforms the CRF-based taggers of pre-
vious work; and when comparing to Gimpel et al system with
similar feature sets, we observed little difference in accuracy.
This is consistent with conventional wisdom that the quality
of lexical features is much more important than the paramet-
ric form of the sequence model, at least in our setting: part-of-
speech tagging with a small labeled training set.
This greedy tagger runs at 800 tweets/sec. (10,000
tokens/sec.) on a single CPU core, about 40 times
faster than Gimpel et al?s system. The tokenizer by
itself (?4) runs at 3,500 tweets/sec.3
Training and regularization. During training,
the MEMM log-likelihood for a tagged tweet ?x,y?
is the sum over the observed token tags yt, each con-
ditional on the tweet being tagged and the observed
previous tag (with a start symbol before the first to-
ken in x),
`(x,y,?) =
?|x|
t=1 log p(yt | yt?1,x, t;?).
We optimize the parameters ? with OWL-QN, an
L1-capable variant of L-BFGS (Andrew and Gao,
2007; Liu and Nocedal, 1989) to minimize the regu-
larized objective
argmin
?
? 1N
?
?x,y? `(x,y,?) +R(?)
where N is the number of tokens in the corpus and
the sum ranges over all tagged tweets ?x,y? in the
training data. We use elastic net regularization (Zou
and Hastie, 2005), which is a linear combination of
L1 and L2 penalties; here j indexes over all features:
R(?) = ?1
?
j |?j |+
1
2?2
?
j ?
2
j
Using even a very small L1 penalty eliminates many
irrelevant or noisy features.
3 Unsupervised Word Clusters
Our POS tagger can make use of any number of pos-
sibly overlapping features. While we have only a
small amount of hand-labeled data for training, we
also have access to billions of tokens of unlabeled
conversational text from the web. Previous work has
shown that unlabeled text can be used to induce un-
supervised word clusters which can improve the per-
formance of many supervised NLP tasks (Koo et al,
2008; Turian et al, 2010; T?ckstr?m et al, 2012, in-
ter alia). We use a similar approach here to improve
tagging performance for online conversational text.
We also make our induced clusters publicly avail-
able in the hope that they will be useful for other
NLP tasks in this genre.
3Runtimes observed on an Intel Core i5 2.4 GHz laptop.
381
Binary path Top words (by frequency)
A1 111010100010 lmao lmfao lmaoo lmaooo hahahahaha lool ctfu rofl loool lmfaoo lmfaooo lmaoooo lmbo lololol
A2 111010100011 haha hahaha hehe hahahaha hahah aha hehehe ahaha hah hahahah kk hahaa ahah
A3 111010100100 yes yep yup nope yess yesss yessss ofcourse yeap likewise yepp yesh yw yuup yus
A4 111010100101 yeah yea nah naw yeahh nooo yeh noo noooo yeaa ikr nvm yeahhh nahh nooooo
A5 11101011011100 smh jk #fail #random #fact smfh #smh #winning #realtalk smdh #dead #justsaying
B 011101011 u yu yuh yhu uu yuu yew y0u yuhh youh yhuu iget yoy yooh yuo yue juu dya youz yyou
C 11100101111001 w fo fa fr fro ov fer fir whit abou aft serie fore fah fuh w/her w/that fron isn agains
D 111101011000 facebook fb itunes myspace skype ebay tumblr bbm flickr aim msn netflix pandora
E1 0011001 tryna gon finna bouta trynna boutta gne fina gonn tryina fenna qone trynaa qon
E2 0011000 gonna gunna gona gna guna gnna ganna qonna gonnna gana qunna gonne goona
F 0110110111 soo sooo soooo sooooo soooooo sooooooo soooooooo sooooooooo soooooooooo
G1 11101011001010 ;) :p :-) xd ;-) ;d (; :3 ;p =p :-p =)) ;] xdd #gno xddd >:) ;-p >:d 8-) ;-d
G2 11101011001011 :) (: =) :)) :] :?) =] ^_^ :))) ^.^ [: ;)) ((: ^__^ (= ^-^ :))))
G3 1110101100111 :( :/ -_- -.- :-( :?( d: :| :s -__- =( =/ >.< -___- :-/ </3 :\ -____- ;( /: :(( >_< =[ :[ #fml
G4 111010110001 <3 xoxo <33 xo <333 #love s2 <URL-twitition.com> #neversaynever <3333
Figure 2: Example word clusters (HMM classes): we list the most probable words, starting with the most probable, in
descending order. Boldfaced words appear in the example tweet (Figure 1). The binary strings are root-to-leaf paths
through the binary cluster tree. For example usage, see e.g. search.twitter.com, bing.com/social and
urbandictionary.com.
3.1 Clustering Method
We obtained hierarchical word clusters via Brown
clustering (Brown et al, 1992) on a large set of
unlabeled tweets.4 The algorithm partitions words
into a base set of 1,000 clusters, and induces a hi-
erarchy among those 1,000 clusters with a series of
greedy agglomerative merges that heuristically opti-
mize the likelihood of a hidden Markov model with a
one-class-per-lexical-type constraint. Not only does
Brown clustering produce effective features for dis-
criminative models, but its variants are better unsu-
pervised POS taggers than some models developed
nearly 20 years later; see comparisons in Blunsom
and Cohn (2011). The algorithm is attractive for our
purposes since it scales to large amounts of data.
When training on tweets drawn from a single
day, we observed time-specific biases (e.g., nu-
merical dates appearing in the same cluster as the
word tonight), so we assembled our unlabeled data
from a random sample of 100,000 tweets per day
from September 10, 2008 to August 14, 2012,
and filtered out non-English tweets (about 60% of
the sample) using langid.py (Lui and Baldwin,
2012).5 Each tweet was processed with our to-
4As implemented by Liang (2005), v. 1.3: https://
github.com/percyliang/brown-cluster
5https://github.com/saffsd/langid.py
kenizer and lowercased. We normalized all at-
mentions to ?@MENTION? and URLs/email ad-
dresses to their domains (e.g. http://bit.ly/
dP8rR8 ? ?URL-bit.ly?). In an effort to reduce
spam, we removed duplicated tweet texts (this also
removes retweets) before word clustering. This
normalization and cleaning resulted in 56 million
unique tweets (847 million tokens). We set the
clustering software?s count threshold to only cluster
words appearing 40 or more times, yielding 216,856
word types, which took 42 hours to cluster on a sin-
gle CPU.
3.2 Cluster Examples
Fig. 2 shows example clusters. Some of the chal-
lenging words in the example tweet (Fig. 1) are high-
lighted. The term lololol (an extension of lol for
?laughing out loud?) is grouped with a large number
of laughter acronyms (A1: ?laughing my (fucking)
ass off,? ?cracking the fuck up?). Since expressions
of laughter are so prevalent on Twitter, the algorithm
creates another laughter cluster (A1?s sibling A2),
that tends to have onomatopoeic, non-acronym vari-
ants (e.g., haha). The acronym ikr (?I know, right??)
is grouped with expressive variations of ?yes? and
?no? (A4). Note that A1?A4 are grouped in a fairly
specific subtree; and indeed, in this message ikr and
382
lololol are both tagged as interjections.
smh (?shaking my head,? indicating disapproval)
seems related, though is always tagged in the an-
notated data as a miscellaneous abbreviation (G);
the difference between acronyms that are interjec-
tions versus other acronyms may be complicated.
Here, smh is in a related but distinct subtree from the
above expressions (A5); its usage in this example
is slightly different from its more common usage,
which it shares with the other words in its cluster:
message-ending expressions of commentary or emo-
tional reaction, sometimes as a metacomment on the
author?s message; e.g., Maybe you could get a guy
to date you if you actually respected yourself #smh
or There is really NO reason why other girls should
send my boyfriend a goodmorning text #justsaying.
We observe many variants of categories tradition-
ally considered closed-class, including pronouns (B:
u = ?you?) and prepositions (C: fir = ?for?).
There is also evidence of grammatical categories
specific to conversational genres of English; clusters
E1?E2 demonstrate variations of single-word con-
tractions for ?going to? and ?trying to,? some of
which have more complicated semantics.6
Finally, the HMM learns about orthographic vari-
ants, even though it treats all words as opaque sym-
bols; cluster F consists almost entirely of variants
of ?so,? their frequencies monotonically decreasing
in the number of vowel repetitions?a phenomenon
called ?expressive lengthening? or ?affective length-
ening? (Brody and Diakopoulos, 2011; Schnoebe-
len, 2012). This suggests a future direction to jointly
model class sequence and orthographic informa-
tion (Clark, 2003; Smith and Eisner, 2005; Blunsom
and Cohn, 2011).
We have built an HTML viewer to browse these
and numerous other interesting examples.7
3.3 Emoticons and Emoji
We use the term emoticon to mean a face or icon
constructed with traditional alphabetic or punctua-
6One coauthor, a native speaker of the Texan English dialect,
notes ?finna? (short for ?fixing to?, cluster E1) may be an im-
mediate future auxiliary, indicating an immediate future tense
that is present in many languages (though not in standard En-
glish). To illustrate: ?She finna go? approximately means ?She
will go,? but sooner, in the sense of ?She is about to go.?
7http://www.ark.cs.cmu.edu/TweetNLP/
cluster_viewer.html
tion symbols, and emoji to mean symbols rendered
in software as small pictures, in line with the text.
Since our tokenizer is careful to preserve emoti-
cons and other symbols (see ?4), they are clustered
just like other words. Similar emoticons are clus-
tered together (G1?G4), including separate clusters
of happy [[ :) =) ?_? ]], sad/disappointed [[ :/ :(
-_- </3 ]], love [[ ?xoxo ?.? ]] and winking [[
;) (?_-) ]] emoticons. The clusters are not per-
fectly aligned with our POS annotation guidelines;
for example, the ?sad? emoticon cluster included
emotion-bearing terms that our guidelines define as
non-emoticons, such as #ugh, #tear, and #fml (?fuck
my life?), though these seem potentially useful for
sentiment analysis.
One difficult task is classifying different types
of symbols in tweets: our annotation guidelines
differentiate between emoticons, punctuation, and
garbage (apparently non-meaningful symbols or to-
kenization errors). Several Unicode character ranges
are reserved for emoji-style symbols (including the
three Unicode hearts in G4); however, depending
on the user?s software, characters in these ranges
might be rendered differently or not at all. We
have found instances where the clustering algo-
rithm groups proprietary iOS emoji symbols along
with normal emoticons; for example, the character
U+E056, which is interpreted on iOS as a smiling
face, is in the same G2 cluster as smiley face emoti-
cons. The symbol U+E12F, which represents a pic-
ture of a bag of money, is grouped with the words
cash and money.
3.4 Cluster-Based Features
Since Brown clusters are hierarchical in a binary
tree, each word is associated with a tree path rep-
resented as a bitstring with length ? 16; we use pre-
fixes of the bitstring as features (for all prefix lengths
? {2, 4, 6, . . . , 16}). This allows sharing of statisti-
cal strength between similar clusters. Using prefix
features of hierarchical clusters in this way was sim-
ilarly found to be effective for named-entity recog-
nition (Turian et al, 2010) and Twitter POS tag-
ging (Ritter et al, 2011).
When checking to see if a word is associated with
a cluster, the tagger first normalizes the word using
the same techniques as described in ?3.1, then cre-
ates a priority list of fuzzy match transformations
383
of the word by removing repeated punctuation and
repeated characters. If the normalized word is not
in a cluster, the tagger considers the fuzzy matches.
Although only about 3% of the tokens in the devel-
opment set (?6) did not appear in a clustering, this
method resulted in a relative error decrease of 18%
among such word tokens.
3.5 Other Lexical Features
Besides unsupervised word clusters, there are two
other sets of features that contain generalized lexi-
cal class information. We use the tag dictionary fea-
ture from Gimpel et al, which adds a feature for
a word?s most frequent part-of-speech tag.8 This
can be viewed as a feature-based domain adaptation
method, since it gives lexical type-level information
for standard English words, which the model learns
to map between PTB tags to the desired output tags.
Second, since the lack of consistent capitalization
conventions on Twitter makes it especially difficult
to recognize names?Gimpel et al and Foster et
al. (2011) found relatively low accuracy on proper
nouns?we added a token-level name list feature,
which fires on (non-function) words from names
from several sources: Freebase lists of celebrities
and video games (Google, 2012), the Moby Words
list of US Locations,9 and lists of male, female, fam-
ily, and proper names from Mark Kantrowitz?s name
corpus.10
4 Tokenization and Emoticon Detection
Word segmentation on Twitter is challenging due
to the lack of orthographic conventions; in partic-
ular, punctuation, emoticons, URLs, and other sym-
bols may have no whitespace separation from textual
8Frequencies came from the Wall Street Journal and Brown
corpus sections of the Penn Treebank. If a word has multiple
PTB tags, each tag is a feature with value for the frequency rank;
e.g. for three different tags in the PTB, this feature gives a value
of 1 for the most frequent tag, 2/3 for the second, etc. Coarse
versions of the PTB tags are used (Petrov et al, 2011). While
88% of words in the dictionary have only one tag, using rank
information seemed to give a small but consistent gain over only
using the most common tag, or using binary features conjoined
with rank as in Gimpel et al
9http://icon.shef.ac.uk/Moby/mwords.html
10http://www.cs.cmu.edu/afs/cs/project/
ai-repository/ai/areas/nlp/corpora/names/
0.html
words (e.g. no:-d,yes should parse as four tokens),
and internally may contain alphanumeric symbols
that could be mistaken for words: a naive split(/[^a-
zA-Z0-9]+/) tokenizer thinks the words ?p? and ?d?
are among the top 100 most common words on Twit-
ter, due to misanalysis of :p and :d. Traditional Penn
Treebank?style tokenizers are hardly better, often
breaking a string of punctuation characters into a
single token per character.
We rewrote twokenize (O?Connor et al,
2010), a rule-based tokenizer, emoticon, and URL
detector, for use in the tagger. Emoticons are es-
pecially challenging, since they are open-class and
productive. We revise O?Connor et al?s regular ex-
pression grammar that describes possible emoticons,
adding a grammar of horizontal emoticons (e.g. -_-),
known as ?Eastern-style,?11 though we observe high
usage in English-speaking Twitter (Fig. 2, G2?G3).
We also add a number of other improvements to the
patterns. Because this system was used as prepro-
cessing for the word clustering experiment in ?3, we
were able to infer the emoticon clusters in Fig. 2.
Furthermore, whether a token matches the emoticon
pattern is also used as a feature in the tagger (?2).
URL recognition is also difficult, since the http://
is often dropped, resulting in protocol-less URLs
like about.me. We add recognition patterns for these
by using a list of top-level and country domains.
5 Annotated Dataset
Gimpel et al (2011) provided a dataset of POS-
tagged tweets consisting almost entirely of tweets
sampled from one particular day (October 27,
2010). We were concerned about overfitting to time-
specific phenomena; for example, a substantial frac-
tion of the messages are about a basketball game
happening that day.
We created a new test set of 547 tweets for eval-
uation. The test set consists of one random English
tweet from every day between January 1, 2011 and
June 30, 2012. In order for a tweet to be considered
English, it had to contain at least one English word
other than a URL, emoticon, or at-mention. We no-
ticed biases in the outputs of langid.py, so we
instead selected these messages completely manu-
11http://en.wikipedia.org/wiki/List_of_
emoticons
384
ally (going through a random sample of one day?s
messages until an English message was found).
5.1 Annotation Methodology
Gimpel et al provided a tagset for Twitter (shown in
Appendix A), which we used unmodified. The orig-
inal annotation guidelines were not published, but in
this work we recorded the rules governing tagging
decisions and made further revisions while annotat-
ing the new data.12 Some of our guidelines reiter-
ate or modify rules made by Penn Treebank annota-
tors, while others treat specific phenomena found on
Twitter (refer to the next section).
Our tweets were annotated by two annotators who
attempted to match the choices made in Gimpel et
al.?s dataset. The annotators also consulted the POS
annotations in the Penn Treebank (Marcus et al,
1993) as an additional reference. Differences were
reconciled by a third annotator in discussion with all
annotators.13 During this process, an inconsistency
was found in Gimpel et al?s data, which we cor-
rected (concerning the tagging of this/that, a change
to 100 labels, 0.4%). The new version of Gimpel et
al.?s data (called OCT27), as well as the newer mes-
sages (called DAILY547), are both included in our
data release.
5.2 Compounds in Penn Treebank vs. Twitter
Ritter et al (2011) annotated tweets using an aug-
mented version of the PTB tagset and presumably
followed the PTB annotation guidelines. We wrote
new guidelines because the PTB conventions are in-
appropriate for Twitter in several ways, as shown in
the design of Gimpel et al?s tagset. Importantly,
?compound? tags (e.g., nominal+verbal and nomi-
nal+possessive) are used because tokenization is dif-
ficult or seemingly impossible for the nonstandard
word forms that are commonplace in conversational
text.
For example, the PTB tokenization splits contrac-
tions containing apostrophes: I?m? I/PRP ?m/VBP.
But conversational text often contains variants that
resist a single PTB tag (like im), or even chal-
lenge traditional English grammatical categories
12The annotation guidelines are available online at
http://www.ark.cs.cmu.edu/TweetNLP/
13Annotators are coauthors of this paper.
(like imma or umma, which both mean ?I am go-
ing to?). One strategy would be to analyze these
forms into a PTB-style tokenization, as discussed in
Forsyth (2007), who proposes to analyze doncha as
do/VBP ncha/PRP, but notes it would be difficult.
We think this is impossible to handle in the rule-
based framework used by English tokenizers, given
the huge (and possibly growing) number of large
compounds like imma, gonna, w/that, etc. These
are not rare: the word clustering algorithm discov-
ers hundreds of such words as statistically coherent
classes (e.g. clusters E1 and E2 in Fig. 2); and the
word imma is the 962nd most common word in our
unlabeled corpus, more frequent than cat or near.
We do not attempt to do Twitter ?normalization?
into traditional written English (Han and Baldwin,
2011), which we view as a lossy translation task. In
fact, many of Twitter?s unique linguistic phenomena
are due not only to its informal nature, but also a set
of authors that heavily skews towards younger ages
and minorities, with heavy usage of dialects that are
different than the standard American English most
often seen in NLP datasets (Eisenstein, 2013; Eisen-
stein et al, 2011). For example, we suspect that
imma may implicate tense and aspect markers from
African-American Vernacular English.14 Trying to
impose PTB-style tokenization on Twitter is linguis-
tically inappropriate: should the lexico-syntactic be-
havior of casual conversational chatter by young mi-
norities be straightjacketed into the stylistic conven-
tions of the 1980s Wall Street Journal? Instead, we
would like to directly analyze the syntax of online
conversational text on its own terms.
Thus, we choose to leave these word forms un-
tokenized and use compound tags, viewing com-
positional multiword analysis as challenging fu-
ture work.15 We believe that our strategy is suf-
ficient for many applications, such as chunking or
named entity recognition; many applications such
as sentiment analysis (Turney, 2002; Pang and Lee,
2008, ?4.2.3), open information extraction (Carl-
son et al, 2010; Fader et al, 2011), and informa-
tion retrieval (Allan and Raghavan, 2002) use POS
14See ?Tense and aspect? examples in http:
//en.wikipedia.org/wiki/African_American_
Vernacular_English
15For example, wtf has compositional behavior in ?Wtf just
happened???, but only debatably so in ?Huh wtf?.
385
#Msg. #Tok. Tagset Dates
OCT27 1,827 26,594 App. A Oct 27-28, 2010
DAILY547 547 7,707 App. A Jan 2011?Jun 2012
NPSCHAT 10,578 44,997 PTB-like Oct?Nov 2006
(w/o sys. msg.) 7,935 37,081
RITTERTW 789 15,185 PTB-like unknown
Table 1: Annotated datasets: number of messages, to-
kens, tagset, and date range. More information in ?5,
?6.3, and ?6.2.
patterns that seem quite compatible with our ap-
proach. More complex downstream processing like
parsing is an interesting challenge, since contraction
parsing on traditional text is probably a benefit to
current parsers. We believe that any PTB-trained
tool requires substantial retraining and adaptation
for Twitter due to the huge genre and stylistic differ-
ences (Foster et al, 2011); thus tokenization conven-
tions are a relatively minor concern. Our simple-to-
annotate conventions make it easier to produce new
training data.
6 Experiments
We are primarily concerned with performance on
our annotated datasets described in ?5 (OCT27,
DAILY547), though for comparison to previous
work we also test on other corpora (RITTERTW in
?6.2, NPSCHAT in ?6.3). The annotated datasets
are listed in Table 1.
6.1 Main Experiments
We use OCT27 to refer to the entire dataset de-
scribed in Gimpel et al; it is split into train-
ing, development, and test portions (OCT27TRAIN,
OCT27DEV, OCT27TEST). We use DAILY547 as
an additional test set. Neither OCT27TEST nor
DAILY547 were extensively evaluated against until
final ablation testing when writing this paper.
The total number of features is 3.7 million, all
of which are used under pure L2 regularization; but
only 60,000 are selected by elastic net regularization
with (?1, ?2) = (0.25, 2), which achieves nearly the
same (but no better) accuracy as pure L2,16 and we
use it for all experiments. We observed that it was
16We conducted a grid search for the regularizer values on
part of DAILY547, and many regularizer values give the best or
nearly the best results. We suspect a different setup would have
yielded similar results.
l
l
l
l l l l
1e+03 1e+05 1e+07
75
80
85
90
Number of Unlabeled Tweets
Ta
gg
ing
 Ac
cu
rac
y
l
l
l l
l l l
1e+03 1e+05 1e+07
0.6
0
0.6
5
0.7
0
Number of Unlabeled Tweets
To
ke
n 
Co
ve
ra
ge
Figure 3: OCT27 development set accuracy using only
clusters as features.
Model In dict. Out of dict.
Full 93.4 85.0
No clusters 92.0 (?1.4) 79.3 (?5.7)
Total tokens 4,808 1,394
Table 3: DAILY547 accuracies (%) for tokens in and out
of a traditional dictionary, for models reported in rows 1
and 3 of Table 2.
possible to get radically smaller models with only
a slight degradation in performance: (4, 0.06) has
0.5% worse accuracy but uses only 1,632 features, a
small enough number to browse through manually.
First, we evaluate on the new test set, training on
all of OCT27. Due to DAILY547?s statistical repre-
sentativeness, we believe this gives the best view of
the tagger?s accuracy on English Twitter text. The
full tagger attains 93.2% accuracy (final row of Ta-
ble 2).
To facilitate comparisons with previous work, we
ran a series of experiments training only on OCT27?s
training and development sets, then report test re-
sults on both OCT27TEST and all of DAILY547,
shown in Table 2. Our tagger achieves substantially
higher accuracy than Gimpel et al (2011).17
Feature ablation. A number of ablation tests in-
dicate the word clusters are a very strong source of
lexical knowledge. When dropping the tag dictio-
naries and name lists, the word clusters maintain
most of the accuracy (row 2). If we drop the clus-
ters and rely only on tag dictionaries and namelists,
accuracy decreases significantly (row 3). In fact,
we can remove all observation features except for
word clusters?no word features, orthographic fea-
17These numbers differ slightly from those reported by Gim-
pel et al, due to the corrections we made to the OCT27 data,
noted in Section 5.1. We retrained and evaluated their tagger
(version 0.2) on our corrected dataset.
386
Feature set OCT27TEST DAILY547 NPSCHATTEST
All features 91.60 92.80 91.19 1
with clusters; without tagdicts, namelists 91.15 92.38 90.66 2
without clusters; with tagdicts, namelists 89.81 90.81 90.00 3
only clusters (and transitions) 89.50 90.54 89.55 4
without clusters, tagdicts, namelists 86.86 88.30 88.26 5
Gimpel et al (2011) version 0.2 88.89 89.17 6
Inter-annotator agreement (Gimpel et al, 2011) 92.2 7
Model trained on all OCT27 93.2 8
Table 2: Tagging accuracies (%) in ablation experiments. OCT27TEST and DAILY547 95% confidence intervals are
roughly ?0.7%. Our final tagger uses all features and also trains on OCT27TEST, achieving 93.2% on DAILY547.
tures, affix n-grams, capitalization, emoticon pat-
terns, etc.?and the accuracy is in fact still better
than the previous work (row 4).18
We also wanted to know whether to keep the tag
dictionary and name list features, but the splits re-
ported in Fig. 2 did not show statistically signifi-
cant differences; so to better discriminate between
ablations, we created a lopsided train/test split of
all data with a much larger test portion (26,974 to-
kens), having greater statistical power (tighter con-
fidence intervals of ? 0.3%).19 The full system got
90.8% while the no?tag dictionary, no-namelists ab-
lation had 90.0%, a statistically significant differ-
ence. Therefore we retain these features.
Compared to the tagger in Gimpel et al, most of
our feature changes are in the new lexical features
described in ?3.5.20 We do not reuse the other lex-
ical features from the previous work, including a
phonetic normalizer (Metaphone), a name list con-
sisting of words that are frequently capitalized, and
distributional features trained on a much smaller un-
labeled corpus; they are all worse than our new
lexical features described here. (We did include,
however, a variant of the tag dictionary feature that
uses phonetic normalization for lookup; it seemed to
yield a small improvement.)
18Furthermore, when evaluating the clusters as unsupervised
(hard) POS tags, we obtain a many-to-one accuracy of 89.2%
on DAILY547. Before computing this, we lowercased the text
to match the clusters and removed tokens tagged as URLs and
at-mentions.
19Reported confidence intervals in this paper are 95% bino-
mial normal approximation intervals for the proportion of cor-
rectly tagged tokens: ?1.96
?
p(1? p)/ntokens . 1/
?
n.
20Details on the exact feature set are available in a technical
report (Owoputi et al, 2012), also available on the website.
Non-traditional words. The word clusters are es-
pecially helpful with words that do not appear in tra-
ditional dictionaries. We constructed a dictionary
by lowercasing the union of the ispell ?American?,
?British?, and ?English? dictionaries, plus the stan-
dard Unix words file from Webster?s Second Inter-
national dictionary, totalling 260,985 word types.
After excluding tokens defined by the gold stan-
dard as punctuation, URLs, at-mentions, or emoti-
cons,21 22% of DAILY547?s tokens do not appear in
this dictionary. Without clusters, they are very dif-
ficult to classify (only 79.2% accuracy), but adding
clusters generates a 5.7 point improvement?much
larger than the effect on in-dictionary tokens (Ta-
ble 3).
Varying the amount of unlabeled data. A tagger
that only uses word clusters achieves an accuracy of
88.6% on the OCT27 development set.22 We created
several clusterings with different numbers of unla-
beled tweets, keeping the number of clusters con-
stant at 800. As shown in Fig. 3, there was initially
a logarithmic relationship between number of tweets
and accuracy, but accuracy (and lexical coverage)
levels out after 750,000 tweets. We use the largest
clustering (56 million tweets and 1,000 clusters) as
the default for the released tagger.
6.2 Evaluation on RITTERTW
Ritter et al (2011) annotated a corpus of 787
tweets23 with a single annotator, using the PTB
21We retain hashtags since by our guidelines a #-prefixed to-
ken is ambiguous between a hashtag and a normal word, e.g. #1
or going #home.
22The only observation features are the word clusters of a
token and its immediate neighbors.
23https://github.com/aritter/twitter_nlp/
blob/master/data/annotated/pos.txt
387
Tagger Accuracy
This work 90.0 ? 0.5
Ritter et al (2011), basic CRF tagger 85.3
Ritter et al (2011), trained on more data 88.3
Table 4: Accuracy comparison on Ritter et al?s Twitter
POS corpus (?6.2).
Tagger Accuracy
This work 93.4 ? 0.3
Forsyth (2007) 90.8
Table 5: Accuracy comparison on Forsyth?s NPSCHAT
IRC POS corpus (?6.3).
tagset plus several Twitter-specific tags, referred
to in Table 1 as RITTERTW. Linguistic concerns
notwithstanding (?5.2), for a controlled comparison,
we train and test our system on this data with the
same 4-fold cross-validation setup they used, attain-
ing 90.0% (?0.5%) accuracy. Ritter et al?s CRF-
based tagger had 85.3% accuracy, and their best tag-
ger, trained on a concatenation of PTB, IRC, and
Twitter, achieved 88.3% (Table 4).
6.3 IRC: Evaluation on NPSCHAT
IRC is another medium of online conversational
text, with similar emoticons, misspellings, abbrevi-
ations and acronyms as Twitter data. We evaluate
our tagger on the NPS Chat Corpus (Forsyth and
Martell, 2007),24 a PTB-part-of-speech annotated
dataset of Internet Relay Chat (IRC) room messages
from 2006.
First, we compare to a tagger in the same setup as
experiments on this data in Forsyth (2007), training
on 90% of the data and testing on 10%; we average
results across 10-fold cross-validation.25 The full
tagger model achieved 93.4% (?0.3%) accuracy,
significantly improving over the best result they re-
port, 90.8% accuracy with a tagger trained on a mix
of several POS-annotated corpora.
We also perform the ablation experiments on this
corpus, with a slightly different experimental setup:
we first filter out system messages then split data
24Release 1.0: http://faculty.nps.edu/
cmartell/NPSChat.htm
25Forsyth actually used 30 different 90/10 random splits; we
prefer cross-validation because the same test data is never re-
peated, thus allowing straightforward confidence estimation of
accuracy from the number of tokens (via binomial sample vari-
ance, footnote 19). In all cases, the models are trained on the
same amount of data (90%).
into 5,067 training and 2,868 test messages. Results
show a similar pattern as the Twitter data (see final
column of Table 2). Thus the Twitter word clusters
are also useful for language in the medium of text
chat rooms; we suspect these clusters will be appli-
cable for deeper syntactic and semantic analysis in
other online conversational text mediums, such as
text messages and instant messages.
7 Conclusion
We have constructed a state-of-the-art part-of-
speech tagger for the online conversational text
genres of Twitter and IRC, and have publicly re-
leased our new evaluation data, annotation guide-
lines, open-source tagger, and word clusters at
http://www.ark.cs.cmu.edu/TweetNLP.
Acknowledgements
This research was supported in part by the National Sci-
ence Foundation (IIS-0915187 and IIS-1054319).
A Part-of-Speech Tagset
N common noun
O pronoun (personal/WH; not possessive)
^ proper noun
S nominal + possessive
Z proper noun + possessive
V verb including copula, auxiliaries
L nominal + verbal (e.g. i?m), verbal + nominal (let?s)
M proper noun + verbal
A adjective
R adverb
! interjection
D determiner
P pre- or postposition, or subordinating conjunction
& coordinating conjunction
T verb particle
X existential there, predeterminers
Y X + verbal
# hashtag (indicates topic/category for tweet)
@ at-mention (indicates a user as a recipient of a tweet)
~ discourse marker, indications of continuation across
multiple tweets
U URL or email address
E emoticon
$ numeral
, punctuation
G other abbreviations, foreign words, possessive endings,
symbols, garbage
Table 6: POS tagset from Gimpel et al (2011) used in this
paper, and described further in the released annotation
guidelines.
388
References
J. Allan and H. Raghavan. 2002. Using part-of-speech
patterns to reduce query ambiguity. In Proc. of SIGIR.
G. Andrew and J. Gao. 2007. Scalable training of L1-
regularized log-linear models. In Proc. of ICML.
P. Blunsom and T. Cohn. 2011. A hierarchical Pitman-
Yor process HMM for unsupervised part of speech in-
duction. In Proc. of ACL.
S. Brody and N. Diakopoulos. 2011.
Cooooooooooooooollllllllllllll!!!!!!!!!!!!!!: using
word lengthening to detect sentiment in microblogs.
In Proc. of EMNLP.
P. F. Brown, P. V. de Souza, R. L. Mercer, V. J.
Della Pietra, and J. C. Lai. 1992. Class-based n-gram
models of natural language. Computational Linguis-
tics, 18(4).
A. Carlson, J. Betteridge, B. Kisiel, B. Settles, E. R. Hr-
uschka Jr, and T. M. Mitchell. 2010. Toward an archi-
tecture for never-ending language learning. In Proc. of
AAAI.
A. Clark. 2003. Combining distributional and morpho-
logical information for part of speech induction. In
Proc. of EACL.
J. Eisenstein, N. A. Smith, and E. P. Xing. 2011. Discov-
ering sociolinguistic associations with structured spar-
sity. In Proc. of ACL.
J. Eisenstein. 2013. What to do about bad language on
the internet. In Proc. of NAACL.
A. Fader, S. Soderland, and O. Etzioni. 2011. Identifying
relations for open information extraction. In Proc. of
EMNLP.
E. N. Forsyth and C. H. Martell. 2007. Lexical and dis-
course analysis of online chat dialog. In Proc. of ICSC.
E. N. Forsyth. 2007. Improving automated lexical and
discourse analysis of online chat dialog. Master?s the-
sis, Naval Postgraduate School.
J. Foster, O. Cetinoglu, J. Wagner, J. L. Roux, S. Hogan,
J. Nivre, D. Hogan, and J. van Genabith. 2011. #hard-
toparse: POS tagging and parsing the Twitterverse. In
Proc. of AAAI-11 Workshop on Analysing Microtext.
K. Gimpel, N. Schneider, B. O?Connor, D. Das, D. Mills,
J. Eisenstein, M. Heilman, D. Yogatama, J. Flanigan,
and N. A. Smith. 2011. Part-of-speech tagging for
Twitter: Annotation, features, and experiments. In
Proc. of ACL.
Google. 2012. Freebase data dumps. http://
download.freebase.com/datadumps/.
B. Han and T. Baldwin. 2011. Lexical normalisation of
short text messages: Makn sens a #twitter. In Proc. of
ACL.
T. Koo, X. Carreras, and M. Collins. 2008. Simple semi-
supervised dependency parsing. In Proc. of ACL.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proc. of ICML.
P. Liang. 2005. Semi-supervised learning for natural
language. Master?s thesis, Massachusetts Institute of
Technology.
D. C. Liu and J. Nocedal. 1989. On the limited memory
BFGS method for large scale optimization. Mathemat-
ical programming, 45(1).
X. Liu, S. Zhang, F. Wei, and M. Zhou. 2011. Recogniz-
ing named entities in tweets. In Proc. of ACL.
M. Lui and T. Baldwin. 2012. langid.py: An off-the-
shelf language identification tool. In Proc. of ACL.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguistics,
19(2).
A. McCallum, D. Freitag, and F. Pereira. 2000. Maxi-
mum entropy Markov models for information extrac-
tion and segmentation. In Proc. of ICML.
D. McClosky, E. Charniak, and M. Johnson. 2010. Au-
tomatic domain adaptation for parsing. In Proc. of
NAACL.
B. O?Connor, M. Krieger, and D. Ahn. 2010.
TweetMotif: exploratory search and topic summariza-
tion for Twitter. In Proc. of AAAI Conference on We-
blogs and Social Media.
O. Owoputi, B. O?Connor, C. Dyer, K. Gimpel, and
N. Schneider. 2012. Part-of-speech tagging for Twit-
ter: Word clusters and other advances. Technical Re-
port CMU-ML-12-107, Carnegie Mellon University.
B. Pang and L. Lee. 2008. Opinion mining and sentiment
analysis. Now Publishers.
S. Petrov and R. McDonald. 2012. Overview of the 2012
shared task on parsing the web. Notes of the First
Workshop on Syntactic Analysis of Non-Canonical
Language (SANCL).
S. Petrov, D. Das, and R. McDonald. 2011. A
universal part-of-speech tagset. arXiv preprint
arXiv:1104.2086.
A. Ratnaparkhi. 1996. A maximum entropy model for
part-of-speech tagging. In Proc. of EMNLP.
A. Ritter, S. Clark, Mausam, and O. Etzioni. 2011.
Named entity recognition in tweets: An experimental
study. In Proc. of EMNLP.
T. Schnoebelen. 2012. Do you smile with your nose?
Stylistic variation in Twitter emoticons. University of
Pennsylvania Working Papers in Linguistics, 18(2):14.
N. A. Smith and J. Eisner. 2005. Contrastive estimation:
Training log-linear models on unlabeled data. In Proc.
of ACL.
O. T?ckstr?m, R. McDonald, and J. Uszkoreit. 2012.
Cross-lingual word clusters for direct transfer of lin-
guistic structure. In Proc. of NAACL.
389
J. Turian, L. Ratinov, and Y. Bengio. 2010. Word rep-
resentations: A simple and general method for semi-
supervised learning. In Proc. of ACL.
P. D. Turney. 2002. Thumbs up or thumbs down?: se-
mantic orientation applied to unsupervised classifica-
tion of reviews. In Proc. of ACL.
H. Zou and T. Hastie. 2005. Regularization and vari-
able selection via the elastic net. Journal of the Royal
Statistical Society: Series B (Statistical Methodology),
67(2):301?320.
390
Proceedings of NAACL-HLT 2013, pages 661?667,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Supersense Tagging for Arabic: the MT-in-the-Middle Attack
Nathan Schneider? Behrang Mohit? Chris Dyer? Kemal Oflazer? Noah A. Smith?
School of Computer Science
Carnegie Mellon University
?Pittsburgh, PA 15213, USA
?Doha, Qatar
{nschneid@cs.,behrang@,cdyer@cs.,ko@cs.,nasmith@cs.}cmu.edu
Abstract
We consider the task of tagging Arabic nouns
with WordNet supersenses. Three approaches
are evaluated. The first uses an expert-
crafted but limited-coverage lexicon, Arabic
WordNet, and heuristics. The second uses un-
supervised sequence modeling. The third and
most successful approach uses machine trans-
lation to translate the Arabic into English,
which is automatically tagged with English
supersenses, the results of which are then pro-
jected back into Arabic. Analysis shows gains
and remaining obstacles in four Wikipedia
topical domains.
1 Introduction
A taxonomic view of lexical semantics groups word
senses/usages into categories of varying granulari-
ties. WordNet supersense tags denote coarse seman-
tic classes, including person and artifact (for nouns)
and motion and weather (for verbs); these categories
can be taken as the top level of a taxonomy. Nominal
supersense tagging (Ciaramita and Johnson, 2003)
is the task of identifying lexical chunks in the sen-
tence for common as well as proper nouns, and la-
beling each with one of the 25 nominal supersense
categories. Figure 1 illustrates two such labelings of
an Arabic sentence. Like the narrower problem of
named entity recognition, supersense tagging of text
holds attraction as a way of inferring representations
that move toward language independence. Here we
consider the problem of nominal supersense tagging
for Arabic, a language with ca. 300 million speak-
ers and moderate linguistic resources, including a
WordNet (Elkateb et al, 2006), annotated datasets
(Maamouri et al, 2004; Hovy et al, 2006), monolin-
gual corpora, and large amounts of Arabic-English
parallel data.
The supervised learning approach that is used
in state-of-the-art English supersense taggers (Cia-
. HA

?J
J.?

J? @
	
Y
	
? @?
	
K ?? ?? ?
	
?? ?


	
?
	
Y
	
? @?
	
J? @ QK
Y? ??j

JK

Ann-A Gloss Ann-B
controls
communication
manager
communicationthe-windows
in
attribute configuration relation
shape and-layout shape
communication
windows
communicationthe-applications
?The window manager controls the configuration and
layout of application windows.?
Figure 1: A sentence from the ?X Window System? ar-
ticle with supersense taggings from two annotators and
post hoc English glosses and translation.
ramita and Altun, 2006) is problematic for Ara-
bic, since there are supersense annotations for only
a small amount of Arabic text (65,000 words by
Schneider et al, 2012, versus the 360,000 words that
are annotated for English). Here, we reserve that
corpus for development and evaluation, not training.
We explore several approaches in this paper, the
most effective of which is to (1) translate the Arabic
sentence into English, returning the alignment struc-
ture between the source and target, (2) apply En-
glish supersense tagging to the target sentence, and
(3) heuristically project the tags back to the Arabic
sentence across these alignments. This ?MT-in-the-
middle? approach has also been successfully used
for mention detection (Zitouni and Florian, 2008)
and coreference resolution (Rahman and Ng, 2012).
We first discuss the task and relevant resources
(?2), then the approaches we explored (?3), and fi-
nally present experimental results and analysis in ?4.
2 Task and Resources
A gold standard corpus of sentences annotated
with nominal supersenses (as in figure 1) fa-
cilitates automatic evaluation of supersense tag-
gers. For development and evaluation we use
661
the AQMAR Arabic Wikipedia Supersense Corpus1
(Schneider et al, 2012), which augmented a named
entity corpus (Mohit et al, 2012) with nominal
supersense tags. The corpus consists of 28 ar-
ticles selected from four topical areas: history
(e.g., ?Islamic Golden Age?), science (?Atom?),
sports (?Real Madrid?), and technology (?Linux?).
Schneider et al (2012) found the distributions of
supersense categories in these four topical domains
to be markedly different; e.g., most instances of
communication (which includes kinds of software)
occurred in the technology domain, whereas most
substances were found in the science domain.
The 18 test articles have 1,393 sentences (39,916
tokens) annotated at least once.2 As the corpus
was released with two annotators? (partially overlap-
ping) taggings, rather than a single gold standard,
we treat the output of each annotator as a separate
test set. Both annotated some of every article; the
first (Ann-A) annotated 759 sentences, the second
(Ann-B) 811 sentences.
Lexicon. What became known as ?supersense
tags? arose from a high-level partitioning of synsets
in the original English WordNet (Fellbaum, 1998)
into lexicographer files. Arabic WordNet (AWN)
(Elkateb et al, 2006) allows us to recover super-
sense categories for some 10,500 Arabic nominal
types, since many of the synsets in AWN are cross-
referenced to English WordNet, and can therefore
be associated with supersense categories. Further,
OntoNotes contains named entity annotations for
Arabic (Hovy et al, 2006).
From these, we construct an Arabic supersense
lexicon, mapping Arabic noun lemmas to supersense
tags. This lexicon contains 23,000 types, of which
11,000 are multiword units. Token coverage of the
test set is 18% (see table 1). Lexical units encoun-
tered in the test data were up to 9-ways supersense-
ambiguous; the average ambiguity of in-vocabulary
tokens was 2.0 supersenses.
Unlabeled Arabic text. For unsupervised learn-
ing we collected 100,000 words of Arabic Wikipedia
text, not constrained by topic. The articles in this
sample were subject to a minimum length threshold
1http://www.ark.cs.cmu.edu/ArabicSST
2Our development/test split of the data follows Mohit et al
(2012), but we exclude two test set documents??Light? and
?Ibn Tolun Mosque??due to preprocessing issues.
and are all cross-linked to corresponding articles in
English, Chinese, and German.
Arabic?English machine translation. We used
two independently developed Arabic-English MT
systems. One (QCRI) is a phrase-based system
(Koehn et al, 2003), similar to Moses (Koehn et
al., 2007); the other (cdec) is a hierarchical phrase-
based system (Chiang, 2007), as implemented in
cdec (Dyer et al, 2010). Both were trained on
about 370M tokens of parallel data provided by the
LDC (by volume, mostly newswire and UN data).
Each system includes preprocessing for Arabic mor-
phological segmentation and orthographic normal-
ization.3 The QCRI system used a 5-gram modi-
fied Kneser-Ney language model that generated full-
cased forms (Chen and Goodman, 1999). cdec
used a 4-gram KN language model over lowercase
forms and was recased in a post-processing step.
Both language models were trained using the Giga-
word v. 4 corpus. Both systems were tuned to opti-
mize BLEU on a held-out development set (Papineni
et al, 2002).
English supersense tagger. For English super-
sense tagging, an open-source reimplementation of
the approach of Ciaramita and Altun (2006) was
released by Michael Heilman.4 This tagger was
trained on the SemCor corpus (Miller et al, 1993)
and achieves 77% F1 in-domain.
3 Methods
We explored 3 approaches to the supersense tagging
of Arabic: heuristic tagging with a lexicon, unsuper-
vised sequence tagging, and MT-in-the-middle.
3.1 Heuristic Tagging with a Lexicon
Using the lexicon built from AWN and OntoNotes
(see ?2), our heuristic approach works as follows:
1. Stem and vocalize; we used MADA (Habash
and Rambow, 2005; Roth et al, 2008).
2. Greedily detect word sequences matching lexi-
con entries from left to right.
3. If a lexicon entry has more than one associated
supersense, Arabic WordNet synsets are
3QCRI accomplishes this using MADA (Habash and Ram-
bow, 2005; Roth et al, 2008). cdec includes a custom CRF-
based segmenter and standard normalization rules.
4http://www.ark.cs.cmu.edu/mheilman/questions
662
E? person location artifact substance Automatic English supersense tagging
e? 1 2 3 4 5 6 7 8 9 English sentence
a 1 2 3 4 5 6 Arabic sentence (e.g., token 6 aligns to English tokens 7?9)
N P N A N N Arabic POS tagging
A? person location artifact Projected supersense tagging
Figure 2: A hypothetical aligned sentence pair of 9 English words (with their supersense tags) and 6 Ara-
bic words (with their POS tags). Step 4 of the projection procedure constructs the Arabic-to-English mapping
{1?person11, 4?location
4
3, {5, 6}?artifact
7
6}, resulting in the tagging shown in the bottom row.
weighted to favor earlier senses (presumed
by lexicographers to be more frequent) and
then the supersense with the greatest aggregate
weight is selected. Formally: Let senses(w) be
the ordered list of AWN senses of lemma w.
Let senses(w, s) ? senses(w) be those senses
that map to a given supersense s. We choose
arg maxs(|senses(w, s)|/ mini:senses(w)i?senses(w,s) i).
3.2 Unsupervised Sequence Models
Unsupervised sequence labeling is our second ap-
proach (Merialdo, 1994). Although it was largely
developed for part-of-speech tagging, the hope is
to use in-domain Arabic data (the unannotated
Wikipedia corpus discussed in ?2) to infer clus-
ters that correlate well with supersense groupings.
We applied the generative, feature-based model of
Berg-Kirkpatrick et al (2010), replicating a feature-
set used previously for NER (Mohit et al, 2012)?
including context tokens, character n-grams, and
POS?and adding the vocalized stem and several
stem shape features: 1) ContainsDigit?; 2) dig-
its replaced by #; 3) digit sequences replaced by
# (for stems mixing digits with other characters);
4) YearLike??true for 4-digit numerals starting with
19 or 20; 5) LatinWord?, per the morphological an-
alysis; 6) the shape feature of Ciaramita and Al-
tun (2006) (Latin words only). We used 50 itera-
tions of learning (tuned on dev data). For evaluation,
a many-to-one mapping from unsupervised clusters
to supersense tags is greedily induced to maximize
their correspondence on evaluation data.
3.3 MT-in-the-Middle
A standard approach to using supervised linguistic
resources in a second language is cross-lingual pro-
jection (Yarowsky and Ngai, 2001; Yarowsky et al,
2001; Smith and Smith, 2004; Hwa et al, 2005; Mi-
halcea et al, 2007; Burkett and Klein, 2008; Burkett
et al, 2010; Das and Petrov, 2011; Kim et al, 2012,
who use parallel sentences extracted from Wikipedia
for NER). The simplest such approach starts with an
aligned parallel corpus, applies supersense tagging
to the English side, and projects the labels through
the word alignments. A supervised monolingual tag-
ger is then trained on the projected labels. Prelimi-
nary experiments, however, showed that this under-
performed even the simple heuristic baseline above
(likely due to domain mismatch), so it was aban-
doned in favor of a technique that we call MT-in-
the-middle projection.
This approach does not depend on having par-
allel data in the training domain, but rather on an
Arabic?English machine translation system that
can be applied to the sentences we wish to tag. The
approach is inspired by token-level pseudo-parallel
data methods of previous work (Zitouni and Flo-
rian, 2008; Rahman and Ng, 2012). MT output for
this language pair is far from perfect?especially for
Wikipedia text, which is distant from the domain
of the translation system?s training data?but, in the
spirit of Church and Hovy (1993), we conjecture that
it may still be useful. The method is as follows:
1. Preprocess the input Arabic sentence a to
match the decoder?s model of Arabic.
2. Translate the sentence, recovering not just
the English output e? but also the deriva-
tion/alignment structure z relating words and/or
phrases of the English output to words and/or
phrases of the Arabic input.
3. Apply the English supersense tagger to the En-
glish translation, discarding any verbal super-
sense tags. Call the tagger output E?.
4. Project the supersense tags back to the Ara-
bic sentence, yielding A?: Each Arabic token
a ? a that is (a) a noun, or (b) an adjec-
tive following 0 or more adjectives following a
noun is mapped to the first English supersense
mention in E? containing some word aligned
to a. Then, for each English supersense men-
663
Coverage Ann-A Ann-B
Nouns All Tokens Mentions P R F1 P R F1
Lexicon heuristics (?3.1) 8,058 33% 8,465 18% 8,407 32 55 16 29 21.6 37.9 29 53 15 27 19.4 35.6
Unsupervised (?3.2) 20 59 16 48 17.5 52.6 14 56 10 39 11.6 45.9
MT-in-the-middle
(?3.3)
QCRI 14,401 59% 16,461 35% 12,861 34 65 27 50 29.9 56.4 36 64 28 51 31.6 56.6
cdec 14,270 58% 15,542 33% 13,704 37 69 31 57 33.8 62.4 38 67 32 56 34.6 61.0
MTitM + Lex. cdec 16,798 68% 18,461 40% 16,623 35 64 36 65 35.5 64.6 36 63 36 63 36.0 63.2
Table 1: Supersense tagging results on the test set: coverage measures5 and gold-standard evaluation?exact la-
beled/unlabeled6 mention precision, recall, and F-score against each annotator. The last row is a hybrid: MT-in-the-
middle followed by lexicon heuristics to improve recall. Best single-technique and best hybrid results are bolded.
tion, all its mapped Arabic words are grouped
into a single mention and the supersense cat-
egory for that mention is projected. Figure 2
illustrates this procedure. The cdec system
provides word alignments for its translations
derived from the training data; whereas QCRI
only produces phrase-level alignments, so for
every aligned phrase pair ?a?, e?? ? z, we con-
sider every word in a? as aligned to every word
in e? (introducing noise when English super-
sense mention boundaries do not line up with
phrase boundaries).
4 Experiments and Analysis
Table 1 compares the techniques (?3) for full Arabic
supersense tagging.7 The number of nouns, tokens,
and mentions covered by the automatic tagging is
reported, as is the mention-level evaluation against
human annotations. The evaluation is reported sep-
arately for the two annotators in the dataset.
With heuristic lexicon lookup, 18% of the tokens
are marked as part of a nominal supersense mention.
Both labeled and unlabeled mention recall with this
method are below 30%; labeled precision is about
30%, and unlabeled mention precision is above
50%. From this we conclude that the biggest prob-
lems are (a) out-of-vocabulary items and (b) poor
semantic disambiguation of in-vocabulary items.
The unsupervised sequence tagger does even
worse on the labeled evaluation. It has some success
at detecting supersense mentions?unlabeled recall
is substantially improved, and unlabeled precision is
5The unsupervised evaluation greedily maps clusters to tags,
separately for each version of the test set; coverage numbers
thus differ and are not shown here.
6Unlabeled tagging refers to noun chunk detection only.
7It was produced in part using the chunkeval.py script: see
https://github.com/nschneid/pyutil
slightly improved. But it seems to be much worse
at assigning semantic categories; the number of la-
beled true positive mentions is actually lower than
with the lexicon-based approach.
MT-in-the-middle is by far the most success-
ful single approach: both systems outperform the
lexicon-only baseline by about 10 F1 points, de-
spite many errors in the automatic translation, En-
glish tagging, and projection, as well as underlying
linguistic differences between English and Arabic.
The baseline?s unlabeled recall is doubled, indicat-
ing substantially more nominal expressions are de-
tected, in addition to the improved labeled scores.
We further tested simple hybrids combining the
lexicon-based and MT-based approaches. Applying
MT-in-the-middle first, then expanding token cover-
age with the lexicon improves recall at a small cost
to precision (table 1, last row). Combining the tech-
niques in the reverse order is slightly worse than MT-
based projection without consulting the lexicon.
MT-in-the middle improves upon the lexicon-only
baseline, yet performance is still dwarfed by the su-
pervised English tagger (at least in the SemCor eval-
uation; see ?2), and also well below the 70% inter-
annotator F1 reported by Schneider et al (2012). We
therefore examine the weaknesses of our approach
for Arabic.
4.1 MT for Projection
In analyzing our projection framework, we per-
formed a small-scale MT evaluation with the
Wikipedia data. Reference English translations for
140 Arabic Wikipedia sentences?5 per article in
the corpus?were elicited from a bilingual linguist.
Table 2 compares the two systems under three stan-
dard metrics of overall sentence translation quality.8
8BLEU (Papineni et al, 2002); METEOR (Banerjee and
Lavie, 2005; Lavie and Denkowski, 2009), with default options;
664
. ????@ ?


	
? @Yg.

?Q

	??

?
	
Jj ??@

?J.k. ??

?@?
	
K ??k ??mProceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 42?47,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Part-of-Speech Tagging for Twitter: Annotation, Features, and Experiments
Kevin Gimpel, Nathan Schneider, Brendan O?Connor, Dipanjan Das, Daniel Mills,
Jacob Eisenstein, Michael Heilman, Dani Yogatama, Jeffrey Flanigan, and Noah A. Smith
School of Computer Science, Carnegie Mellon Univeristy, Pittsburgh, PA 15213, USA
{kgimpel,nschneid,brenocon,dipanjan,dpmills,
jacobeis,mheilman,dyogatama,jflanigan,nasmith}@cs.cmu.edu
Abstract
We address the problem of part-of-speech tag-
ging for English data from the popular micro-
blogging service Twitter. We develop a tagset,
annotate data, develop features, and report
tagging results nearing 90% accuracy. The
data and tools have been made available to the
research community with the goal of enabling
richer text analysis of Twitter and related so-
cial media data sets.
1 Introduction
The growing popularity of social media and user-
created web content is producing enormous quanti-
ties of text in electronic form. The popular micro-
blogging service Twitter (twitter.com) is one
particularly fruitful source of user-created content,
and a flurry of recent research has aimed to under-
stand and exploit these data (Ritter et al, 2010; Shar-
ifi et al, 2010; Barbosa and Feng, 2010; Asur and
Huberman, 2010; O?Connor et al, 2010a; Thelwall
et al, 2011). However, the bulk of this work eschews
the standard pipeline of tools which might enable
a richer linguistic analysis; such tools are typically
trained on newstext and have been shown to perform
poorly on Twitter (Finin et al, 2010).
One of the most fundamental parts of the linguis-
tic pipeline is part-of-speech (POS) tagging, a basic
form of syntactic analysis which has countless appli-
cations in NLP. Most POS taggers are trained from
treebanks in the newswire domain, such as the Wall
Street Journal corpus of the Penn Treebank (PTB;
Marcus et al, 1993). Tagging performance degrades
on out-of-domain data, and Twitter poses additional
challenges due to the conversational nature of the
text, the lack of conventional orthography, and 140-
character limit of each message (?tweet?). Figure 1
shows three tweets which illustrate these challenges.
(a) @Gunservatively@ obozo? willV goV nutsA
whenR PA? electsV aD RepublicanA GovernorN
nextP Tue? ., CanV youO sayV redistrictingV ?,
(b) SpendingV theD dayN withhhP mommmaN !,
(c) lmao! ..., s/oV toP theD coolA assN asianA
officerN 4P #1$ notR runninV myD licenseN and&
#2$ notR takinV druN booN toP jailN ., ThankV
uO God? ., #amen#
Figure 1: Example tweets with gold annotations. Under-
lined tokens show tagger improvements due to features
detailed in Section 3 (respectively: TAGDICT, METAPH,
and DISTSIM).
In this paper, we produce an English POS tagger
that is designed especially for Twitter data. Our con-
tributions are as follows:
? we developed a POS tagset for Twitter,
? we manually tagged 1,827 tweets,
? we developed features for Twitter POS tagging
and conducted experiments to evaluate them, and
? we provide our annotated corpus and trained POS
tagger to the research community.
Beyond these specific contributions, we see this
work as a case study in how to rapidly engi-
neer a core NLP system for a new and idiosyn-
cratic dataset. This project was accomplished in
200 person-hours spread across 17 people and two
months. This was made possible by two things:
(1) an annotation scheme that fits the unique char-
acteristics of our data and provides an appropriate
level of linguistic detail, and (2) a feature set that
captures Twitter-specific properties and exploits ex-
isting resources such as tag dictionaries and phonetic
normalization. The success of this approach demon-
strates that with careful design, supervised machine
learning can be applied to rapidly produce effective
language technology in new domains.
42
Tag Description Examples %
Nominal, Nominal + Verbal
N common noun (NN, NNS) books someone 13.7
O pronoun (personal/WH; not
possessive; PRP, WP)
it you u meeee 6.8
S nominal + possessive books? someone?s 0.1
? proper noun (NNP, NNPS) lebron usa iPad 6.4
Z proper noun + possessive America?s 0.2
L nominal + verbal he?s book?ll iono
(= I don?t know)
1.6
M proper noun + verbal Mark?ll 0.0
Other open-class words
V verb incl. copula,
auxiliaries (V*, MD)
might gonna
ought couldn?t is
eats
15.1
A adjective (J*) good fav lil 5.1
R adverb (R*, WRB) 2 (i.e., too) 4.6
! interjection (UH) lol haha FTW yea
right
2.6
Other closed-class words
D determiner (WDT, DT,
WP$, PRP$)
the teh its it?s 6.5
P pre- or postposition, or
subordinating conjunction
(IN, TO)
while to for 2 (i.e.,
to) 4 (i.e., for)
8.7
& coordinating conjunction
(CC)
and n & + BUT 1.7
T verb particle (RP) out off Up UP 0.6
X existential there,
predeterminers (EX, PDT)
both 0.1
Y X + verbal there?s all?s 0.0
Twitter/online-specific
# hashtag (indicates
topic/category for tweet)
#acl 1.0
@ at-mention (indicates
another user as a recipient
of a tweet)
@BarackObama 4.9
~ discourse marker,
indications of continuation
of a message across
multiple tweets
RT and : in retweet
construction RT
@user : hello
3.4
U URL or email address http://bit.ly/xyz 1.6
E emoticon :-) :b (: <3 o O 1.0
Miscellaneous
$ numeral (CD) 2010 four 9:30 1.5
, punctuation (#, $, '', (,
), ,, ., :, ``)
!!! .... ?!? 11.6
G other abbreviations, foreign
words, possessive endings,
symbols, garbage (FW,
POS, SYM, LS)
ily (I love you) wby
(what about you) ?s
 -->
awesome...I?m
1.1
Table 1: The set of tags used to annotate tweets. The
last column indicates each tag?s relative frequency in the
full annotated data (26,435 tokens). (The rates for M and
Y are both < 0.0005.)
2 Annotation
Annotation proceeded in three stages. For Stage 0,
we developed a set of 20 coarse-grained tags based
on several treebanks but with some additional cate-
gories specific to Twitter, including URLs and hash-
tags. Next, we obtained a random sample of mostly
American English1 tweets from October 27, 2010,
automatically tokenized them using a Twitter tok-
enizer (O?Connor et al, 2010b),2 and pre-tagged
them using the WSJ-trained Stanford POS Tagger
(Toutanova et al, 2003) in order to speed up man-
ual annotation. Heuristics were used to mark tokens
belonging to special Twitter categories, which took
precedence over the Stanford tags.
Stage 1 was a round of manual annotation: 17 re-
searchers corrected the automatic predictions from
Stage 0 via a custom Web interface. A total of
2,217 tweets were distributed to the annotators in
this stage; 390 were identified as non-English and
removed, leaving 1,827 annotated tweets (26,436 to-
kens).
The annotation process uncovered several situa-
tions for which our tagset, annotation guidelines,
and tokenization rules were deficient or ambiguous.
Based on these considerations we revised the tok-
enization and tagging guidelines, and for Stage 2,
two annotators reviewed and corrected all of the
English tweets tagged in Stage 1. A third anno-
tator read the annotation guidelines and annotated
72 tweets from scratch, for purposes of estimating
inter-annotator agreement. The 72 tweets comprised
1,021 tagged tokens, of which 80 differed from the
Stage 2 annotations, resulting in an agreement rate
of 92.2% and Cohen?s ? value of 0.914. A final
sweep was made by a single annotator to correct er-
rors and improve consistency of tagging decisions
across the corpus. The released data and tools use
the output of this final stage.
2.1 Tagset
We set out to develop a POS inventory for Twitter
that would be intuitive and informative?while at
the same time simple to learn and apply?so as to
maximize tagging consistency within and across an-
1We filtered to tweets sent via an English-localized user in-
terface set to a United States timezone.
2http://github.com/brendano/tweetmotif
43
notators. Thus, we sought to design a coarse tagset
that would capture standard parts of speech3 (noun,
verb, etc.) as well as categories for token varieties
seen mainly in social media: URLs and email ad-
dresses; emoticons; Twitter hashtags, of the form
#tagname, which the author may supply to catego-
rize a tweet; and Twitter at-mentions, of the form
@user, which link to other Twitter users from within
a tweet.
Hashtags and at-mentions can also serve as words
or phrases within a tweet; e.g. Is #qadaffi going down?.
When used in this way, we tag hashtags with their
appropriate part of speech, i.e., as if they did not start
with #. Of the 418 hashtags in our data, 148 (35%)
were given a tag other than #: 14% are proper nouns,
9% are common nouns, 5% are multi-word express-
sions (tagged as G), 3% are verbs, and 4% are some-
thing else. We do not apply this procedure to at-
mentions, as they are nearly always proper nouns.
Another tag, ~, is used for tokens marking spe-
cific Twitter discourse functions. The most popular
of these is the RT (?retweet?) construction to publish
a message with attribution. For example,
RT @USER1 : LMBO ! This man filed an
EMERGENCY Motion for Continuance on
account of the Rangers game tonight ! 
Wow lmao
indicates that the user @USER1 was originally the
source of the message following the colon. We ap-
ply ~ to the RT and : (which are standard), and
also, which separates the author?s comment from
the retweeted material.4 Another common discourse
marker is ellipsis dots (. . . ) at the end of a tweet,
indicating a message has been truncated to fit the
140-character limit, and will be continued in a sub-
sequent tweet or at a specified URL.
Our first round of annotation revealed that, due to
nonstandard spelling conventions, tokenizing under
a traditional scheme would be much more difficult
3Our starting point was the cross-lingual tagset presented by
Petrov et al (2011). Most of our tags are refinements of those
categories, which in turn are groupings of PTB WSJ tags (see
column 2 of Table 1). When faced with difficult tagging deci-
sions, we consulted the PTB and tried to emulate its conventions
as much as possible.
4These ?iconic deictics? have been studied in other online
communities as well (Collister, 2010).
than for Standard English text. For example, apos-
trophes are often omitted, and there are frequently
words like ima (short for I?m gonna) that cut across
traditional POS categories. Therefore, we opted not
to split contractions or possessives, as is common
in English corpus preprocessing; rather, we intro-
duced four new tags for combined forms: {nominal,
proper noun} ? {verb, possessive}.5
The final tagging scheme (Table 1) encompasses
25 tags. For simplicity, each tag is denoted with a
single ASCII character. The miscellaneous category
G includes multiword abbreviations that do not fit
in any of the other categories, like ily (I love you), as
well as partial words, artifacts of tokenization errors,
miscellaneous symbols, possessive endings,6 and ar-
rows that are not used as discourse markers.
Figure 2 shows where tags in our data tend to oc-
cur relative to the middle word of the tweet. We
see that Twitter-specific tags have strong positional
preferences: at-mentions (@) and Twitter discourse
markers (~) tend to occur towards the beginning of
messages, whereas URLs (U), emoticons (E), and
categorizing hashtags (#) tend to occur near the end.
3 System
Our tagger is a conditional random field (CRF; Laf-
ferty et al, 2001), enabling the incorporation of ar-
bitrary local features in a log-linear model. Our
base features include: a feature for each word type,
a set of features that check whether the word con-
tains digits or hyphens, suffix features up to length 3,
and features looking at capitalization patterns in the
word. We then added features that leverage domain-
specific properties of our data, unlabeled in-domain
data, and external linguistic resources.
TWORTH: Twitter orthography. We have features
for several regular expression-style rules that detect
at-mentions, hashtags, and URLs.
NAMES: Frequently-capitalized tokens. Micro-
bloggers are inconsistent in their use of capitaliza-
tion, so we compiled gazetteers of tokens which are
frequently capitalized. The likelihood of capital-
ization for a token is computed as Ncap+?CN+C , where
5The modified tokenizer is packaged with our tagger.
6Possessive endings only appear when a user or the tok-
enizer has separated the possessive ending from a possessor; the
tokenizer only does this when the possessor is an at-mention.
44
Figure 2: Average position, relative to the middle word in the tweet, of tokens labeled with each tag. Most tags fall
between ?1 and 1 on this scale; these are not shown.
N is the token count, Ncap is the capitalized to-
ken count, and ? and C are the prior probability
and its prior weight.7 We compute features for
membership in the top N items by this metric, for
N ? {1000, 2000, 3000, 5000, 10000, 20000}.
TAGDICT: Traditional tag dictionary. We add
features for all coarse-grained tags that each word
occurs with in the PTB8 (conjoined with their fre-
quency rank). Unlike previous work that uses tag
dictionaries as hard constraints, we use them as soft
constraints since we expect lexical coverage to be
poor and the Twitter dialect of English to vary sig-
nificantly from the PTB domains. This feature may
be seen as a form of type-level domain adaptation.
DISTSIM: Distributional similarity. When train-
ing data is limited, distributional features from un-
labeled text can improve performance (Schu?tze and
Pedersen, 1993). We used 1.9 million tokens from
134,000 unlabeled tweets to construct distributional
features from the successor and predecessor proba-
bilities for the 10,000 most common terms. The suc-
cessor and predecessor transition matrices are hori-
zontally concatenated into a sparse matrixM, which
we approximate using a truncated singular value de-
composition: M ? USVT, where U is limited to
50 columns. Each term?s feature vector is its row
in U; following Turian et al (2010), we standardize
and scale the standard deviation to 0.1.
METAPH: Phonetic normalization. Since Twitter
includes many alternate spellings of words, we used
the Metaphone algorithm (Philips, 1990)9 to create
a coarse phonetic normalization of words to simpler
keys. Metaphone consists of 19 rules that rewrite
consonants and delete vowels. For example, in our
7? = 1100 , C = 10; this score is equivalent to the posterior
probability of capitalization with a Beta(0.1, 9.9) prior.
8Both WSJ and Brown corpora, no case normalization. We
also tried adding the WordNet (Fellbaum, 1998) and Moby
(Ward, 1996) lexicons, which increased lexical coverage but did
not seem to help performance.
9Via the Apache Commons implementation: http://
commons.apache.org/codec/
data, {thangs thanks thanksss thanx thinks thnx}
are mapped to 0NKS, and {lmao lmaoo lmaooooo}
map to LM. But it is often too coarse; e.g. {war we?re
wear were where worry} map to WR.
We include two types of features. First, we use
the Metaphone key for the current token, comple-
menting the base model?s word features. Second,
we use a feature indicating whether a tag is the most
frequent tag for PTB words having the same Meta-
phone key as the current token. (The second feature
was disabled in both ?TAGDICT and ?METAPH ab-
lation experiments.)
4 Experiments
Our evaluation was designed to test the efficacy of
this feature set for part-of-speech tagging given lim-
ited training data. We randomly divided the set of
1,827 annotated tweets into a training set of 1,000
(14,542 tokens), a development set of 327 (4,770 to-
kens), and a test set of 500 (7,124 tokens). We com-
pare our system against the Stanford tagger. Due
to the different tagsets, we could not apply the pre-
trained Stanford tagger to our data. Instead, we re-
trained it on our labeled data, using a standard set
of features: words within a 5-word window, word
shapes in a 3-word window, and up to length-3
prefixes, length-3 suffixes, and prefix/suffix pairs.10
The Stanford system was regularized using a Gaus-
sian prior of ?2 = 0.5 and our system with a Gaus-
sian prior of ?2 = 5.0, tuned on development data.
The results are shown in Table 2. Our tagger with
the full feature set achieves a relative error reduction
of 25% compared to the Stanford tagger. We also
show feature ablation experiments, each of which
corresponds to removing one category of features
from the full set. In Figure 1, we show examples
that certain features help solve. Underlined tokens
10We used the following feature modules in the Stanford tag-
ger: bidirectional5words, naacl2003unknowns,
wordshapes(-3,3), prefix(3), suffix(3),
prefixsuffix(3).
45
Dev. Test
Our tagger, all features 88.67 89.37
independent ablations:
?DISTSIM 87.88 88.31 (?1.06)
?TAGDICT 88.28 88.31 (?1.06)
?TWORTH 87.51 88.37 (?1.00)
?METAPH 88.18 88.95 (?0.42)
?NAMES 88.66 89.39 (+0.02)
Our tagger, base features 82.72 83.38
Stanford tagger 85.56 85.85
Annotator agreement 92.2
Table 2: Tagging accuracies on development and test
data, including ablation experiments. Features are or-
dered by importance: test accuracy decrease due to ab-
lation (final column).
Tag Acc. Confused Tag Acc. Confused
V 91 N ! 82 N
N 85 ? L 93 V
, 98 ~ & 98 ?
P 95 R U 97 ,
? 71 N $ 89 P
D 95 ? # 89 ?
O 97 ? G 26 ,
A 79 N E 88 ,
R 83 A T 72 P
@ 99 V Z 45 ?
~ 91 ,
Table 3: Accuracy (recall) rates per class, in the test set
with the full model. (Omitting tags that occur less than
10 times in the test set.) For each gold category, the most
common confusion is shown.
are incorrect in a specific ablation, but are corrected
in the full system (i.e. when the feature is added).
The ?TAGDICT ablation gets elects, Governor,
and next wrong in tweet (a). These words appear
in the PTB tag dictionary with the correct tags, and
thus are fixed by that feature. In (b), withhh is ini-
tially misclassified an interjection (likely caused by
interjections with the same suffix, like ohhh), but is
corrected by METAPH, because it is normalized to the
same equivalence class as with. Finally, s/o in tweet
(c) means ?shoutout?, which appears only once in
the training data; adding DISTSIM causes it to be cor-
rectly identified as a verb.
Substantial challenges remain; for example, de-
spite the NAMES feature, the system struggles to
identify proper nouns with nonstandard capitaliza-
tion. This can be observed from Table 3, which
shows the recall of each tag type: the recall of proper
nouns (?) is only 71%. The system also struggles
with the miscellaneous category (G), which covers
many rare tokens, including obscure symbols and ar-
tifacts of tokenization errors. Nonetheless, we are
encouraged by the success of our system on the
whole, leveraging out-of-domain lexical resources
(TAGDICT), in-domain lexical resources (DISTSIM),
and sublexical analysis (METAPH).
Finally, we note that, even though 1,000 train-
ing examples may seem small, the test set accuracy
when training on only 500 tweets drops to 87.66%,
a decrease of only 1.7% absolute.
5 Conclusion
We have developed a part-of-speech tagger for Twit-
ter and have made our data and tools available to the
research community at http://www.ark.cs.
cmu.edu/TweetNLP. More generally, we be-
lieve that our approach can be applied to address
other linguistic analysis needs as they continue to
arise in the era of social media and its rapidly chang-
ing linguistic conventions. We also believe that the
annotated data can be useful for research into do-
main adaptation and semi-supervised learning.
Acknowledgments
We thank Desai Chen, Chris Dyer, Lori Levin, Behrang
Mohit, Bryan Routledge, Naomi Saphra, and Tae Yano
for assistance in annotating data. This research was sup-
ported in part by: the NSF through CAREER grant IIS-
1054319, the U. S. Army Research Laboratory and the
U. S. Army Research Office under contract/grant num-
ber W911NF-10-1-0533, Sandia National Laboratories
(fellowship to K. Gimpel), and the U. S. Department of
Education under IES grant R305B040063 (fellowship to
M. Heilman).
References
Sitaram Asur and Bernardo A. Huberman. 2010. Pre-
dicting the future with social media. In Proc. of WI-
IAT.
Luciano Barbosa and Junlan Feng. 2010. Robust senti-
ment detection on Twitter from biased and noisy data.
In Proc. of COLING.
Lauren Collister. 2010. Meaning variation of the iconic
deictics ? and <? in an online community. In New
Ways of Analyzing Variation.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. Bradford Books.
46
Tim Finin, Will Murnane, Anand Karandikar, Nicholas
Keller, Justin Martineau, and Mark Dredze. 2010. An-
notating named entities in Twitter data with crowd-
sourcing. In Proceedings of the NAACL HLT 2010
Workshop on Creating Speech and Language Data
with Amazon?s Mechanical Turk.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. In Proc. of
ICML.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19:313?330.
Brendan O?Connor, Ramnath Balasubramanyan,
Bryan R. Routledge, and Noah A. Smith. 2010a.
From tweets to polls: Linking text sentiment to public
opinion time series. In Proc. of ICWSM.
Brendan O?Connor, Michel Krieger, and David Ahn.
2010b. TweetMotif: Exploratory search and topic
summarization for Twitter. In Proc. of ICWSM (demo
track).
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2011.
A universal part-of-speech tagset. ArXiv:1104.2086.
Lawrence Philips. 1990. Hanging on the Metaphone.
Computer Language, 7(12).
Alan Ritter, Colin Cherry, and Bill Dolan. 2010. Unsu-
pervised modeling of Twitter conversations. In Proc.
of NAACL.
Hinrich Schu?tze and Jan Pedersen. 1993. A vector model
for syntagmatic and paradigmatic relatedness. In Pro-
ceedings of the 9th Annual Conference of the UW Cen-
tre for the New OED and Text Research.
Beaux Sharifi, Mark-Anthony Hutton, and Jugal Kalita.
2010. Summarizing microblogs automatically. In
Proc. of NAACL.
Mike Thelwall, Kevan Buckley, and Georgios Paltoglou.
2011. Sentiment in Twitter events. Journal of the
American Society for Information Science and Tech-
nology, 62(2):406?418.
Kristina Toutanova, Dan Klein, Christopher D. Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Proc. of
HLT-NAACL.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method for
semi-supervised learning. In Proc. of ACL.
Grady Ward. 1996. Moby lexicon. http://icon.
shef.ac.uk/Moby.
47
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 253?258,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Coarse Lexical Semantic Annotation with Supersenses:
An Arabic Case Study
Nathan Schneider? Behrang Mohit? Kemal Oflazer? Noah A. Smith?
School of Computer Science, Carnegie Mellon University
?Doha, Qatar ?Pittsburgh, PA 15213, USA
{nschneid@cs.,behrang@,ko@cs.,nasmith@cs.}cmu.edu
Abstract
?Lightweight? semantic annotation of text
calls for a simple representation, ideally with-
out requiring a semantic lexicon to achieve
good coverage in the language and domain.
In this paper, we repurpose WordNet?s super-
sense tags for annotation, developing specific
guidelines for nominal expressions and ap-
plying them to Arabic Wikipedia articles in
four topical domains. The resulting corpus
has high coverage and was completed quickly
with reasonable inter-annotator agreement.
1 Introduction
The goal of ?lightweight? semantic annotation of
text, particularly in scenarios with limited resources
and expertise, presents several requirements for a
representation: simplicity; adaptability to new lan-
guages, topics, and genres; and coverage. This
paper describes coarse lexical semantic annotation
of Arabic Wikipedia articles subject to these con-
straints. Traditional lexical semantic representations
are either narrow in scope, like named entities,1 or
make reference to a full-fledged lexicon/ontology,
which may insufficiently cover the language/domain
of interest or require prohibitive expertise and ef-
fort to apply.2 We therefore turn to supersense tags
(SSTs), 40 coarse lexical semantic classes (25 for
nouns, 15 for verbs) originating in WordNet. Previ-
ously these served as groupings of English lexicon
1Some ontologies like those in Sekine et al (2002) and BBN
Identifinder (Bikel et al, 1999) include a large selection of
classes, which tend to be especially relevant to proper names.
2E.g., a WordNet (Fellbaum, 1998) sense annotation effort
reported by Passonneau et al (2010) found considerable inter-
annotator variability for some lexemes; FrameNet (Baker et
al., 1998) is limited in coverage, even for English; and Prop-
Bank (Kingsbury and Palmer, 2002) does not capture semantic
relationships across lexemes. We note that the Omega ontol-
ogy (Philpot et al, 2003) has been used for fine-grained cross-
lingual annotation (Hovy et al, 2006; Dorr et al, 2010).
Q.

J?K

considers
H. A

J?
book
?

	
JJ
k.
Guinness
?A

P?

C?
for-records

?J
?AJ


?? @
the-standard
COMMUNICATION
	
?

@
that

???Ag.
university
	
?@?Q


?? @
Al-Karaouine
ARTIFACT
?


	
?
in
?A
	
?
Fez
H. Q
	??? @
Morocco
LOCATION
?Y

?

@
oldest

???Ag.
university
GROUP
?


	
?
in
??A?? @
the-world
LOCATION
IJ
k
where
??

'
was
A?D?J
?

A

K
established
ACT
?


	
?
in

?
	
J?
year
859 ?


XCJ
?
AD
TIME
.
?The Guinness Book of World Records considers the
University of Al-Karaouine in Fez, Morocco, established
in the year 859 AD, the oldest university in the world.?
Figure 1: A sentence from the article ?Islamic Golden
Age,? with the supersense tagging from one of two anno-
tators. The Arabic is shown left-to-right.
entries, but here we have repurposed them as target
labels for direct human annotation.
Part of the earliest versions of WordNet, the
supersense categories (originally, ?lexicographer
classes?) were intended to partition all English noun
and verb senses into broad groupings, or semantic
fields (Miller, 1990; Fellbaum, 1990). More re-
cently, the task of automatic supersense tagging has
emerged for English (Ciaramita and Johnson, 2003;
Curran, 2005; Ciaramita and Altun, 2006; Paa? and
Reichartz, 2009), as well as for Italian (Picca et al,
2008; Picca et al, 2009; Attardi et al, 2010) and
Chinese (Qiu et al, 2011), languages with WordNets
mapped to English WordNet.3 In principle, we be-
lieve supersenses ought to apply to nouns and verbs
in any language, and need not depend on the avail-
ability of a semantic lexicon.4 In this work we focus
on the noun SSTs, summarized in figure 2 and ap-
plied to an Arabic sentence in figure 1.
SSTs both refine and relate lexical items: they
capture lexical polysemy on the one hand?e.g.,
3Note that work in supersense tagging used text with fine-
grained sense annotations that were then coarsened to SSTs.
4The noun/verb distinction might prove problematic in some
languages.
253
Crusades ?Damascus ? Ibn Tolun Mosque ? Imam Hussein Shrine ? Islamic Golden Age ? Islamic History ?Ummayad Mosque 434s 16,185t 5,859m
Atom ? Enrico Fermi ? Light ? Nuclear power ? Periodic Table ? Physics ? Muhammad al-Razi 777s 18,559t 6,477m
2004 Summer Olympics ?Christiano Ronaldo ?Football ?FIFA World Cup ?Portugal football team ?Rau?l Gonza?les ?Real Madrid 390s 13,716t 5,149m
Computer ? Computer Software ? Internet ? Linux ? Richard Stallman ? Solaris ? X Window System 618s 16,992t 5,754m
Table 1: Snapshot of the supersense-annotated data. The 7 article titles (translated) in each domain, with total counts
of sentences, tokens, and supersense mentions. Overall, there are 2,219 sentences with 65,452 tokens and 23,239
mentions (1.3 tokens/mention on average). Counts exclude sentences marked as problematic and mentions marked ?.
disambiguating PERSON vs. POSSESSION for the
noun principal?and generalize across lexemes on
the other?e.g., principal, teacher, and student can
all be PERSONs. This lumping property might be
expected to give too much latitude to annotators; yet
we find that in practice, it is possible to elicit reason-
able inter-annotator agreement, even for a language
other than English. We encapsulate our interpreta-
tion of the tags in a set of brief guidelines that aims
to be usable by anyone who can read and understand
a text in the target language; our annotators had no
prior expertise in linguistics or linguistic annotation.
Finally, we note that ad hoc categorization
schemes not unlike SSTs have been developed for
purposes ranging from question answering (Li and
Roth, 2002) to animacy hierarchy representation for
corpus linguistics (Zaenen et al, 2004). We believe
the interpretation of the SSTs adopted here can serve
as a single starting point for diverse resource en-
gineering efforts and applications, especially when
fine-grained sense annotation is not feasible.
2 Tagging Conventions
WordNet?s definitions of the supersenses are terse,
and we could find little explicit discussion of the
specific rationales behind each category. Thus,
we have crafted more specific explanations, sum-
marized for nouns in figure 2. English examples
are given, but the guidelines are intended to be
language-neutral. A more systematic breakdown,
formulated as a 43-rule decision list, is included
with the corpus.5 In developing these guidelines
we consulted English WordNet (Fellbaum, 1998)
and SemCor (Miller et al, 1993) for examples and
synset definitions, occasionally making simplifying
decisions where we found distinctions that seemed
esoteric or internally inconsistent. Special cases
(e.g., multiword expressions, anaphora, figurative
5For example, one rule states that all man-made structures
(buildings, rooms, bridges, etc.) are to be tagged as ARTIFACTs.
language) are addressed with additional rules.
3 Arabic Wikipedia Annotation
The annotation in this work was on top of a small
corpus of Arabic Wikipedia articles that had al-
ready been annotated for named entities (Mohit et
al., 2012). Here we use two different annotators,
both native speakers of Arabic attending a university
with English as the language of instruction.
Data & procedure. The dataset (table 1) consists of
the main text of 28 articles selected from the topical
domains of history, sports, science, and technology.
The annotation task was to identify and categorize
mentions, i.e., occurrences of terms belonging to
noun supersenses. Working in a custom, browser-
based interface, annotators were to tag each relevant
token with a supersense category by selecting the to-
ken and typing a tag symbol. Any token could be
marked as continuing a multiword unit by typing <.
If the annotator was ambivalent about a token they
were to mark it with the ? symbol. Sentences were
pre-tagged with suggestions where possible.6 Anno-
tators noted obvious errors in sentence splitting and
grammar so ill-formed sentences could be excluded.
Training. Over several months, annotators alter-
nately annotated sentences from 2 designated arti-
cles of each domain, and reviewed the annotations
for consistency. All tagging conventions were deve-
loped collaboratively by the author(s) and annotators
during this period, informed by points of confusion
and disagreement. WordNet and SemCor were con-
sulted as part of developing the guidelines, but not
during annotation itself so as to avoid complicating
the annotation process or overfitting to WordNet?s
idiosyncracies. The training phase ended once inter-
annotator mention F1 had reached 75%.
6Suggestions came from the previous named entity annota-
tion of PERSONs, organizations (GROUP), and LOCATIONs, as
well as heuristic lookup in lexical resources?Arabic WordNet
entries (Elkateb et al, 2006) mapped to English WordNet, and
named entities in OntoNotes (Hovy et al, 2006).
254
O NATURAL OBJECT natural feature or nonliving object in
nature barrier reef nest neutron star
planet sky fishpond metamorphic rock Mediterranean cave
stepping stone boulder Orion ember universe
A ARTIFACT man-made structures and objects bridge
restaurant bedroom stage cabinet toaster antidote aspirin
L LOCATION any name of a geopolitical entity, as well as
other nouns functioning as locations or regions
Cote d?Ivoire New York City downtown stage left India
Newark interior airspace
P PERSON humans or personified beings; names of social
groups (ethnic, political, etc.) that can refer to an individ-
ual in the singular Persian deity glasscutter mother
kibbutznik firstborn worshiper Roosevelt Arab consumer
appellant guardsman Muslim American communist
G GROUP groupings of people or objects, including: orga-
nizations/institutions; followers of social movements
collection flock army meeting clergy Mennonite Church
trumpet section health profession peasantry People?s Party
U.S. State Department University of California population
consulting firm communism Islam (= set of Muslims)
$ SUBSTANCE a material or substance krypton mocha
atom hydrochloric acid aluminum sand cardboard DNA
H POSSESSION term for an entity involved in ownership or
payment birthday present tax shelter money loan
T TIME a temporal point, period, amount, or measurement
10 seconds day Eastern Time leap year 2nd millenium BC
2011 (= year) velocity frequency runtime latency/delay
middle age half life basketball season words per minute
curfew industrial revolution instant/moment August
= RELATION relations between entities or quantities
ratio scale reverse personal relation exponential function
angular position unconnectedness transitivity
Q QUANTITY quantities and units of measure, including
cardinal numbers and fractional amounts 7 cm 1.8 million
12 percent/12% volume (= spatial extent) volt real number
square root digit 90 degrees handful ounce half
F FEELING subjective emotions indifference wonder
murderousness grudge desperation astonishment suffering
M MOTIVE an abstract external force that causes someone
to intend to do something reason incentive
C COMMUNICATION information encoding and transmis-
sion, except in the sense of a physical object
grave accent Book of Common Prayer alphabet
Cree language onomatopoeia reference concert hotel bill
broadcast television program discussion contract proposal
equation denial sarcasm concerto software
? COGNITION aspects of mind/thought/knowledge/belief/
perception; techniques and abilities; fields of academic
study; social or philosophical movements referring to the
system of beliefs Platonism hypothesis
logic biomedical science necromancy hierarchical structure
democracy innovativeness vocational program woodcraft
reference visual image Islam (= Islamic belief system) dream
scientific method consciousness puzzlement skepticism
reasoning design intuition inspiration muscle memory skill
aptitude/talent method sense of touch awareness
S STATE stable states of affairs; diseases and their symp-
toms symptom reprieve potency
poverty altitude sickness tumor fever measles bankruptcy
infamy opulence hunger opportunity darkness (= lack of light)
@ ATTRIBUTE characteristics of people/objects that can be
judged resilience buxomness virtue immateriality
admissibility coincidence valence sophistication simplicity
temperature (= degree of hotness) darkness (= dark coloring)
! ACT things people do or cause to happen; learned pro-
fessions meddling malpractice faith healing dismount
carnival football game acquisition engineering (= profession)
E EVENT things that happens at a given place and time
bomb blast ordeal miracle upheaval accident tide
R PROCESS a sustained phenomenon or one marked by
gradual changes through a series of states
oscillation distillation overheating aging accretion/growth
extinction evaporation
X PHENOMENON a physical force or something that hap-
pens/occurs electricity suction tailwind tornado effect
+ SHAPE two and three dimensional shapes
D FOOD things used as food or drink
B BODY human body parts, excluding diseases and their
symptoms
Y PLANT a plant or fungus
N ANIMAL non-human, non-plant life
Science chemicals, molecules, atoms, and subatomic
particles are tagged as SUBSTANCE
Sports championships/tournaments are EVENTs
(Information) Technology Software names, kinds, and
components are tagged as COMMUNICATION (e.g. kernel,
version, distribution, environment). A connection is a RE-
LATION; project, support, and a configuration are tagged
as COGNITION; development and collaboration are ACTs.
Arabic conventions Masdar constructions (verbal
nouns) are treated as nouns. Anaphora are not tagged.
Figure 2: Above: The complete supersense tagset for nouns; each tag is briefly described by its symbol, NAME,
short description, and examples. Some examples and longer descriptions have been omitted due to space constraints.
Below: A few domain- and language-specific elaborations of the general guidelines.
255
Figure 3: Distribution of supersense mentions by
domain (left), and counts for tags occurring over
800 times (below). (Counts are of the union of the
annotators? choices, even when they disagree.)
tag num tag num
ACT (!) 3473 LOCATION (G) 1583
COMMUNICATION (C) 3007 GROUP (L) 1501
PERSON (P) 2650 TIME (T) 1407
ARTIFACT (A) 2164 SUBSTANCE ($) 1291
COGNITION (?) 1672 QUANTITY (Q) 1022
Main annotation. After training, the two annota-
tors proceeded on a per-document basis: first they
worked together to annotate several sentences from
the beginning of the article, then each was inde-
pendently assigned about half of the remaining sen-
tences (typically with 5?10 shared to measure agree-
ment). Throughout the process, annotators were en-
couraged to discuss points of confusion with each
other, but each sentence was annotated in its entirety
and never revisited. Annotation of 28 articles re-
quired approximately 100 annotator-hours. Articles
used in pilot rounds were re-annotated from scratch.
Analysis. Figure 3 shows the distribution of SSTs in
the corpus. Some of the most concrete tags?BODY,
ANIMAL, PLANT, NATURAL OBJECT, and FOOD?
were barely present, but would likely be frequent
in life sciences domains. Others, such as MOTIVE,
POSSESSION, and SHAPE, are limited in scope.
To measure inter-annotator agreement, 87 sen-
tences (2,774 tokens) distributed across 19 of the ar-
ticles (not including those used in pilot rounds) were
annotated independently by each annotator. Inter-
annotator mention F1 (counting agreement over en-
tire mentions and their labels) was 70%. Excluding
the 1,397 tokens left blank by both annotators, the
token-level agreement rate was 71%, with Cohen?s
? = 0.69, and token-level F1 was 83%.7
We also measured agreement on a tag-by-tag ba-
sis. For 8 of the 10 most frequent SSTs (fig-
ure 3), inter-annotator mention F1 ranged from 73%
to 80%. The two exceptions were QUANTITY at
63%, and COGNITION (probably the most heteroge-
neous category) at 49%. An examination of the con-
fusion matrix reveals four pairs of supersense cate-
gories that tended to provoke the most disagreement:
COMMUNICATION/COGNITION, ACT/COGNITION,
ACT/PROCESS, and ARTIFACT/COMMUNICATION.
7Token-level measures consider both the supersense label
and whether it begins or continues the mention.
The last is exhibited for the first mention in figure 1,
where one annotator chose ARTIFACT (referring to
the physical book) while the other chose COMMU-
NICATION (the content). Also in that sentence, an-
notators disagreed on the second use of university
(ARTIFACT vs. GROUP). As with any sense anno-
tation effort, some disagreements due to legitimate
ambiguity and different interpretations of the tags?
especially the broadest ones?are unavoidable.
A ?soft? agreement measure (counting as matches
any two mentions with the same label and at least
one token in common) gives an F1 of 79%, show-
ing that boundary decisions account for a major por-
tion of the disagreement. E.g., the city Fez, Mo-
rocco (figure 1) was tagged as a single LOCATION
by one annotator and as two by the other. Further
examples include the technical term ?thin client?,
for which one annotator omitted the adjective; and
?World Cup Football Championship?, where one an-
notator tagged the entire phrase as an EVENT while
the other tagged ?football? as a separate ACT.
4 Conclusion
We have codified supersense tags as a simple an-
notation scheme for coarse lexical semantics, and
have shown that supersense annotation of Ara-
bic Wikipedia can be rapid, reliable, and robust
(about half the tokens in our data are covered
by a nominal supersense). Our tagging guide-
lines and corpus are available for download at
http://www.ark.cs.cmu.edu/ArabicSST/.
Acknowledgments
We thank Nourhen Feki and Sarah Mustafa for assistance
with annotation, as well as Emad Mohamed, CMU ARK
members, and anonymous reviewers for their comments.
This publication was made possible by grant NPRP-08-
485-1-083 from the Qatar National Research Fund (a
member of the Qatar Foundation). The statements made
herein are solely the responsibility of the authors.
256
References
Giuseppe Attardi, Stefano Dei Rossi, Giulia Di Pietro,
Alessandro Lenci, Simonetta Montemagni, and Maria
Simi. 2010. A resource and tool for super-sense
tagging of Italian texts. In Nicoletta Calzolari,
Khalid Choukri, Bente Maegaard, Joseph Mariani,
Jan Odijk, Stelios Piperidis, Mike Rosner, and Daniel
Tapias, editors, Proceedings of the Seventh Interna-
tional Conference on Language Resources and Evalu-
ation (LREC?10), Valletta, Malta, May. European Lan-
guage Resources Association (ELRA).
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet project. In Proceed-
ings of the 36th Annual Meeting of the Association
for Computational Linguistics and 17th International
Conference on Computational Linguistics (COLING-
ACL ?98), pages 86?90, Montreal, Quebec, Canada,
August. Association for Computational Linguistics.
D. M. Bikel, R. Schwartz, and R. M. Weischedel. 1999.
An algorithm that learns what?s in a name. Machine
Learning, 34(1).
Massimiliano Ciaramita and Yasemin Altun. 2006.
Broad-coverage sense disambiguation and information
extraction with a supersense sequence tagger. In Pro-
ceedings of the 2006 Conference on Empirical Meth-
ods in Natural Language Processing, pages 594?602,
Sydney, Australia, July. Association for Computa-
tional Linguistics.
Massimiliano Ciaramita and Mark Johnson. 2003. Su-
persense tagging of unknown nouns in WordNet. In
Proceedings of the 2003 Conference on Empirical
Methods in Natural Language Processing, pages 168?
175, Sapporo, Japan, July.
James R. Curran. 2005. Supersense tagging of unknown
nouns using semantic similarity. In Proceedings of
the 43rd Annual Meeting on Association for Computa-
tional Linguistics (ACL?05), pages 26?33, Ann Arbor,
Michigan, June.
Bonnie J. Dorr, Rebecca J. Passonneau, David Farwell,
Rebecca Green, Nizar Habash, Stephen Helmreich,
Eduard Hovy, Lori Levin, Keith J. Miller, Teruko
Mitamura, Owen Rambow, and Advaith Siddharthan.
2010. Interlingual annotation of parallel text corpora:
a new framework for annotation and evaluation. Nat-
ural Language Engineering, 16(03):197?243.
Sabri Elkateb, William Black, Horacio Rodr??guez, Musa
Alkhalifa, Piek Vossen, Adam Pease, and Christiane
Fellbaum. 2006. Building a WordNet for Arabic.
In Proceedings of The Fifth International Conference
on Language Resources and Evaluation (LREC 2006),
pages 29?34, Genoa, Italy.
Christiane Fellbaum. 1990. English verbs as a semantic
net. International Journal of Lexicography, 3(4):278?
301, December.
Christiane Fellbaum, editor. 1998. WordNet: an elec-
tronic lexical database. MIT Press, Cambridge, MA.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. OntoNotes:
the 90% solution. In Proceedings of the Human Lan-
guage Technology Conference of the NAACL (HLT-
NAACL), pages 57?60, New York City, USA, June.
Association for Computational Linguistics.
Paul Kingsbury and Martha Palmer. 2002. From Tree-
Bank to PropBank. In Proceedings of the Third In-
ternational Conference on Language Resources and
Evaluation (LREC-02), Las Palmas, Canary Islands,
May.
Xin Li and Dan Roth. 2002. Learning question classi-
fiers. In Proceedings of the 19th International Con-
ference on Computational Linguistics (COLING?02),
pages 1?7, Taipei, Taiwan, August. Association for
Computational Linguistics.
George A. Miller, Claudia Leacock, Randee Tengi, and
Ross T. Bunker. 1993. A semantic concordance. In
Proceedings of the Workshop on Human Language
Technology (HLT ?93), HLT ?93, pages 303?308,
Plainsboro, NJ, USA, March. Association for Compu-
tational Linguistics.
George A. Miller. 1990. Nouns in WordNet: a lexical
inheritance system. International Journal of Lexicog-
raphy, 3(4):245?264, December.
Behrang Mohit, Nathan Schneider, Rishav Bhowmick,
Kemal Oflazer, and Noah A. Smith. 2012.
Recall-oriented learning of named entities in Arabic
Wikipedia. In Proceedings of the 13th Conference of
the European Chapter of the Association for Computa-
tional Linguistics (EACL 2012), pages 162?173, Avi-
gnon, France, April. Association for Computational
Linguistics.
Gerhard Paa? and Frank Reichartz. 2009. Exploiting
semantic constraints for estimating supersenses with
CRFs. In Proceedings of the Ninth SIAM International
Conference on Data Mining, pages 485?496, Sparks,
Nevada, USA, May. Society for Industrial and Applied
Mathematics.
Rebecca J. Passonneau, Ansaf Salleb-Aoussi, Vikas
Bhardwaj, and Nancy Ide. 2010. Word sense anno-
tation of polysemous words by multiple annotators.
In Nicoletta Calzolari, Khalid Choukri, Bente Mae-
gaard, Joseph Mariani, Jan Odijk, Stelios Piperidis,
Mike Rosner, and Daniel Tapias, editors, Proceed-
ings of the Seventh International Conference on Lan-
guage Resources and Evaluation (LREC?10), Valletta,
Malta, May. European Language Resources Associa-
tion (ELRA).
257
Andrew G. Philpot, Michael Fleischman, and Eduard H.
Hovy. 2003. Semi-automatic construction of a general
purpose ontology. In Proceedings of the International
Lisp Conference, New York, NY, USA, October.
Davide Picca, Alfio Massimiliano Gliozzo, and Mas-
similiano Ciaramita. 2008. Supersense Tagger
for Italian. In Nicoletta Calzolari, Khalid Choukri,
Bente Maegaard, Joseph Mariani, Jan Odjik, Stelios
Piperidis, and Daniel Tapias, editors, Proceedings of
the Sixth International Language Resources and Eval-
uation (LREC?08), pages 2386?2390, Marrakech, Mo-
rocco, May. European Language Resources Associa-
tion (ELRA).
Davide Picca, Alfio Massimiliano Gliozzo, and Simone
Campora. 2009. Bridging languages by SuperSense
entity tagging. In Proceedings of the 2009 Named
Entities Workshop: Shared Task on Transliteration
(NEWS 2009), pages 136?142, Suntec, Singapore, Au-
gust. Association for Computational Linguistics.
Likun Qiu, Yunfang Wu, Yanqiu Shao, and Alexander
Gelbukh. 2011. Combining contextual and struc-
tural information for supersense tagging of Chinese
unknown words. In Computational Linguistics and In-
telligent Text Processing: Proceedings of the 12th In-
ternational Conference on Computational Linguistics
and Intelligent Text Processing (CICLing?11), volume
6608 of Lecture Notes in Computer Science, pages 15?
28. Springer, Berlin.
Satoshi Sekine, Kiyoshi Sudo, and Chikashi Nobata.
2002. Extended named entity hierarchy. In Proceed-
ings of the Third International Conference on Lan-
guage Resources and Evaluation (LREC-02), Las Pal-
mas, Canary Islands, May.
Annie Zaenen, Jean Carletta, Gregory Garretson, Joan
Bresnan, Andrew Koontz-Garboden, Tatiana Nikitina,
M. Catherine O?Connor, and Tom Wasow. 2004. An-
imacy encoding in English: why and how. In Bon-
nie Webber and Donna K. Byron, editors, ACL 2004
Workshop on Discourse Annotation, pages 118?125,
Barcelona, Spain, July. Association for Computational
Linguistics.
258
Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 121?126,
Baltimore, Maryland USA, June 23-24, 2014.
c?2014 Association for Computational Linguistics
Simplified Dependency Annotations with GFL-Web
Michael T. Mordowanec Nathan Schneider Chris Dyer Noah A. Smith
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
michael.mordowanec@gmail.com, {nschneid,cdyer,nasmith}@cs.cmu.edu
Abstract
We present GFL-Web, a web-based in-
terface for syntactic dependency annota-
tion with the lightweight FUDG/GFL for-
malism. Syntactic attachments are spec-
ified in GFL notation and visualized as
a graph. A one-day pilot of this work-
flow with 26 annotators established that
even novices were, with a bit of training,
able to rapidly annotate the syntax of En-
glish Twitter messages. The open-source
tool is easily installed and configured; it
is available at: https://github.com/
Mordeaux/gfl_web
1 Introduction
High-quality syntactic annotation of natural lan-
guage is expensive to produce. Well-known large-
scale syntactic annotation projects, such as the
Penn Treebank (Marcus et al., 1993), the En-
glish Web Treebank (Bies et al., 2012), the Penn
Arabic Treebank (Maamouri et al., 2004), and
the Prague dependency treebanks (Haji
?
c, 1998;
?
Cmejrek et al., 2005), have relied on expert lin-
guists to produce carefully-controlled annotated
data. Because this process is costly, such anno-
tation projects have been undertaken for only a
handful of important languages. Therefore, devel-
oping syntactic resources for less-studied, lower-
resource, or politically less important languages
and genres will require alternative methods. To
address this, simplified annotation schemes that
trade cost for detail have been proposed (Habash
and Roth, 2009).
1
1
These can be especially effective when some details of
the syntax can be predicted automatically with high accuracy
(Alkuhlani et al., 2013).
The Fragmentary Unlabeled Dependency
Grammar (FUDG) formalism (Schneider et al.,
2013) was proposed as a simplified framework for
annotating dependency syntax. Annotation effort
is reduced by relaxing a number of constraints
placed on traditional annotators: partial fragments
can be specified where the annotator is uncertain
of part of the structure or wishes to focus only
on certain phenomena (such as verbal argument
structure). FUDG also offers mechanisms for
excluding extraneous tokens from the annotation,
for marking multiword units, and for describing
coordinate structures. FUDG is written in an
ASCII-based notation for annotations called
Graph Fragment Language (GFL), and text-based
tools for verifying, converting, and rendering GFL
annotations are provided.
Although GFL offers a number of conveniences
to annotators, the text-based UI is limiting: the
existing tools require constant switching between
a text editor and executing commands, and there
are no tools for managing a large-scale annotation
effort. Additionally, user interface research has
found marked preferences for and better perfor-
mance with graphical tools relative to text-based
interfaces?particularly for less computer-savvy
users (Staggers and Kobus, 2000). In this paper,
we present the GFL-Web tool, a web-based inter-
face for FUDG/GFL annotation. The simple inter-
face provides instantaneous feedback on the well-
formedness of a GFL annotation, and by wrapping
Schneider et al.?s notation parsing and rendering
software, gives a user-friendly visualization of the
annotated sentence. The tool itself is lightweight,
multi-user, and easily deployed with few software
dependencies. Sentences are assigned to anno-
tators via an administrative interface, which also
records progress and provides for a text dump of
121
(a) @Bryan_wright11 i lost all my contacts , smh .
(b) Texas Rangers are in the World Series ! Go
Rangers !!!!!!!!! http://fb.me/D2LsXBJx
Figure 1: FUDG annotation graphs for two tweets.
all annotations. The interface for annotators is de-
signed to be as simple as possible.
We provide an overview of the FUDG/GFL
framework (?2), detail how the tool is set up and
utilized (?3), and discuss a pilot exercise in which
26 users provided nearly 1,000 annotations of En-
glish Twitter messages (?4). Finally, we note some
of the technical aspects of the tool (?5) and related
syntactic annotation software (?6).
2 Background
GFL-Web is designed to simplify the creation
of dependency treebanks from noisy or under-
resourced data; to that end, it exploits the
lightweight FUDG/GFL framework of Schneider
et al. (2013). Here we outline how FUDG differs
from traditional Dependency Grammar (?2.1) and
detail major aspects of GFL (?2.2).
2.1 FUDG
Figure 1 displays two FUDG graphs of annota-
tions of Twitter messages (?tweets?, shown below
in tokenized form). Arrows point upwards from
dependents to their heads. These tweets illustrate
several characteristics of the formalism, including:
? The input may contain multiple independent
syntactic units, or ?utterances?; the annotation
indicates these by attaching their heads to a spe-
cial root node called **.
? Some input tokens are omitted if deemed ex-
trinsic to the syntax; by convention, these in-
clude most punctuation, hashtags, usernames,
and URLs.
? Multiword units may be joined to form com-
posite lexical nodes (e.g., World_Series in fig-
ure 1b). These nodes are not annotated with any
internal syntactic structure.
? Tokens that are used in the FUDG parse must be
unambiguous. If a word appears multiple times
in the input, it is disambiguated with ~ and an
index (e.g., Rangers~2 in figure 1b).
(Some of the other mechanisms in FUDG, such as
coordinate structures and underspecification, are
not shown here; they are not important for pur-
poses of this paper.)
2.2 GFL
The Graph Fragment Language is a simple ASCII-
based language for FUDG annotations. Its norms
are designed to be familiar to users with basic pro-
gramming language proficiency, and they are intu-
itive and easy to learn even for those without. The
annotation in figure 1a may be expressed in GFL
as:
2
i > lost** < ({all my} > contacts)
smh**
In GFL, angle brackets point from a dependent
(child) to its head (parent). Parentheses group
nodes together; the head of this group is then at-
tached to another node. The double asterisk (**)
marks a root node in an annotations containing
multiple utterances. Curly braces group nodes that
modify the same head.
GFL corresponding to Figure 1b is:
2
The abbreviation smh stands for shaking my head.
122
Sentence: Texas Rangers are in the World Series ! Go Rangers !!!!!!!!! http://fb.me/D2LsXBJx
Input format:
---
% ID data_set_name:417
% TEXT
Texas Rangers~1 are in the World Series ! Go Rangers~2 !!!!!!!!!
http://fb.me/D2LsXBJx
% ANNO
Texas Rangers~1 are in the World Series Go Rangers~2
http://fb.me/D2LsXBJx
Figure 2: Illustration of the GFL-Web input format for a tweet. The ANNO section will be shown to the user as the default
annotation; punctuation has been stripped out automatically to save time.
Figure 3: User home screen showing assigned batches for annotation, with links to the training set and blank annotation form.
[Texas Rangers~1] > are** < in
in < (the > [World Series])
Go** < Rangers~2
This uses square brackets for multiword expres-
sions. Similar to a programming language, there
are often many equivalent GFL annotation options
for a given sentence. The annotation can be split
across multiple lines so that annotators can ap-
proach smaller units first and then link them to-
gether.
3 Using GFL-Web
The GFL-Web tool uses the Python programming
language?s Flask microframework for server-side
scripting. This allows it to be deployed on a web
server, locally or via the Internet. This also en-
ables the interface to rely on scripts previously
created for analyzing GFL. Once installed, the re-
searcher need only configure a few settings and be-
gin entering data to be annotated.
3.1 Setup
There are a few simple configuration options. The
most useful of these options specify how many
sentences should be in each batch that is assigned
to an annotator, and how many sentences in each
batch should be doubly annotated, for the purpose
of assessing inter-annotator agreement. By de-
fault, the batch size is 10, and the first 2 sentences
of each batch overlap with the previous batch, so
4/10 of the sentences in the batch will be annotated
by someone else (assuming no two consecutive
batches are assigned to the same user). The pro-
gram requires tokenized input, with indices added
to distinguish between words that appear twice
(easily automated). The input format, figure 2, al-
lows for easy editing with a text editor if so de-
sired.
Once the input files have been placed in a des-
ignated directory, an admin interface can be used
to assign batches of data to specific users (annota-
tors).
3.2 Annotation
Annotators log in with their username and see a
home screen, figure 3. The home screen always
offers links to a training set to get them up to
speed, as well as a blank annotation form into
which they can enter and annotate any sentence.
Beneath these is a table of batches of sentences
which have been assigned to the user. Clicking
123
Figure 4: A well-formed GFL annotation is indicated by a
green background and visualization of the analysis graph.
any of these will take the annotator to an annota-
tion page, which displays the text to be annotated,
an input field, and a comments box.
The annotation interface is simple and intuitive
and provides instant feedback, preventing the an-
notator from submitting ill-formed annotations.
Annotators press the Analyze button and receive
feedback before submitting annotations (figure 4).
Common GFL errors such as unbalanced paren-
theses are caught by the program and brought to
the attention of the annotator with an informative
error message (figure 5). The annotator can then
fix the error, and will be able to submit once all
errors are resolved.
The training set consists of 15 sentences se-
lected from Rossman and Mills (1922), shown in
the same annotation interface. Examples become
increasingly more complicated in order to famil-
iarize the user with different syntactic phenomena
and the entry-analyze-review workflow. A button
displays the FUDG graph from an expert annota-
tion so the novice can compare it to her own and
consult the guidelines (or ask for help) where the
two graphs deviate.
4 Pilot User Study
We conducted a pilot annotation project in which
26 annotators were trained on GFL-Web and asked
to annotate English Twitter messages from the
daily547 and oct27 Twitter datasets of Gimpel
et al. (2011). The overwhelming majority were all
trained on the same day, having no prior knowl-
edge of GFL. Most, but not all, were native speak-
ers of English. Those who had no prior knowl-
edge of dependency grammar in general received
a short tutorial on the fundamentals before being
introduced to the annotation workflow. All par-
ticipants who were new to FUDG/GFL worked
through the training set before moving on to the
Twitter data. Annotators were furnished with the
English annotation guidelines of Schneider et al.
(2013).
3
4.1 Results
In the one-day event, 906 annotations were gen-
erated. Inter-annotator agreement was high?.9
according to the softComPrec measure (Schnei-
der et al., 2013)?and an expert?s examination of a
sample of the annotations found that 75% of con-
tained no major error.
Annotators used the analysis feature of the
interface?which displays either a visual represen-
tation of the tree or an error message?an aver-
age of 3.06 times per annotation. The interface re-
quires they analyze each annotation at least once.
Annotators have the ability to resubmit annota-
tions if they later realize they made an error, and
each annotation was submitted an average of 1.16
times. Disregarding instances that took over 1,000
seconds (under the assumption that these repre-
sent annotators taking breaks), the median time
between the first analysis and the first submission
of an annotation was 30 seconds. We take this
as evidence that annotators found the instant feed-
back features useful in refining their annotations.
4.2 Post-Pilot Improvements
Annotator feedback prompted some changes to the
interface. The annotation input box was changed
to incorporate bracket-matching. The graph visu-
alization for a correct annotation was added for
each example in the training set so new annota-
tors could compare their tree to an example. Pre-
sumably these changes would further reduce anno-
tators? training time and improve their efficiency.
Progress bars were added to the user home screen
to show per-batch completion information.
4.3 Other Languages
In addition to English, guidelines for Swahili,
Zulu, and Mandarin are currently in development.
3https://github.com/brendano/gfl_syntax/
blob/master/guidelines/guidelines.md
124
Figure 5: An example error notification. The red background indicates an error, and the cause of the error is displayed at the
bottom of the screen.
5 Technical Architecture
GFL-Web and its software dependencies for ana-
lyzing and visualizing GFL are largely written in
Python. The tool is built with Flask, a Python
framework for web applications. Data is stored
and transmitted to and from the browser in the
Javascript Object Notation (JSON) format, which
is supported by libraries in most programming lan-
guages. The browser interface uses AJAX tech-
niques to interact dynamically with the server.
GFL-Web is written for Python version 2.7.
It wraps scripts previously written for the analy-
sis and visualization of GFL (Schneider et al.,
2013). These in turn require Graphviz (Ellson
et al., 2002), which is freely available.
Flask provides a built-in server, but can also be
deployed in Apache, via WSGI or CGI, etc.
6 Other Tools
In treebanking, a good user interface is essen-
tial for annotator productivity and accuracy. Sev-
eral existing tools support dependency annota-
tion; GFL-Web is the first designed specifi-
cally for the FUDG/GFL framework. Some,
including WebAnno (Yimam et al., 2013) and
brat (Stenetorp et al., 2012), are browser-based,
while WordFreak (Morton and LaCivita, 2003),
Abar-Hitz (Ilarraza et al., 2004), and TrEd (Pa-
jas and Fabian, 2000?2011) are client-side appli-
cations. All offer tree visualizations; to the best
of our knowledge, ours is the only dependency
annotation interface that has text as the exclu-
sive mode of entry. Some, such as WebAnno
and brat, aim to be fairly general-purpose, sup-
porting a wide range of annotation schemes; by
contrast, GFL-Web supports a single annotation
scheme, which keeps the configuration (and code-
base) simple. In the future, GFL-Web might in-
corporate elements of monitoring progress, such
as display of evaluation measures computed with
existing FUDG/GFL scripts.
Certain elements of the FUDG/GFL framework
can be found in other annotation systems, such
as the PASSAGE syntactic representation (Vilnat
et al., 2010), which allows for grouping of words
into units, but still requires dependency relations
to be labeled.
Finally, we note that new approaches to corpus
annotation of semantic dependencies also come
with rich browser-based annotation interfaces (Ba-
narescu et al., 2013; Abend and Rappoport, 2013).
7 Conclusion
While the creation of high-quality, highly speci-
fied, syntactically annotated corpora is a goal that
is out of reach for most languages and genres,
GFL-Web facilitates a rapid annotation workflow
within a simple framework for dependency syn-
tax. More information on FUDG/GFL is avail-
able at http://www.ark.cs.cmu.edu/FUDG/,
and the source code for GFL-Web is available at
https://github.com/Mordeaux/gfl_web.
125
Acknowledgments
The authors thank Archna Bhatia, Lori Levin, Ja-
son Baldridge, Dan Garrette, Jason Mielens, Liang
Sun, Shay Cohen, Spencer Onuffer, Nora Ka-
zour, Manaal Faruqui, Wang Ling, Waleed Am-
mar, David Bamman, Dallas Card, Jeff Flani-
gan, Lingpeng Kong, Bill McDowell, Brendan
O?Connor, Tobi Owoputi, Yanchuan Sim, Swabha
Swayamdipta, and Dani Yogatama for annotating
data, and anonymous reviewers for helpful feed-
back. This research was supported by NSF grant
IIS-1352440.
References
Omri Abend and Ari Rappoport. 2013. Universal Con-
ceptual Cognitive Annotation (UCCA). In Proc. of
ACL, pages 228?238. Sofia, Bulgaria.
Sarah Alkuhlani, Nizar Habash, and Ryan Roth. 2013.
Automatic morphological enrichment of a mor-
phologically underspecified treebank. In Proc. of
NAACL-HLT, pages 460?470. Atlanta, Georgia.
Laura Banarescu, Claire Bonial, Shu Cai, Madalina
Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin
Knight, Philipp Koehn, Martha Palmer, and Nathan
Schneider. 2013. Abstract Meaning Representation
for sembanking. In Proc. of the 7th Linguistic An-
notation Workshop and Interoperability with Dis-
course, pages 178?186. Sofia, Bulgaria.
Ann Bies, Justin Mott, Colin Warner, and Seth Kulick.
2012. English Web Treebank. Technical Re-
port LDC2012T13, Linguistic Data Consortium,
Philadelphia, PA.
Martin
?
Cmejrek, Jan Cu
?
r?n, Jan Haji
?
c, and Ji
?
r? Havelka.
2005. Prague Czech-English Dependency Treebank:
resource for structure-based MT. In Proc. of EAMT,
pages 73?78. Budapest, Hungary.
John Ellson, Emden Gansner, Lefteris Koutsofios,
Stephen C. North, and Gordon Woodhull. 2002.
Graphviz?open source graph drawing tools. In Pe-
tra Mutzel, Michael J?nger, and Sebastian Leipert,
editors, Graph Drawing, pages 483?484. Springer,
Berlin.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein,
Michael Heilman, Dani Yogatama, Jeffrey Flanigan,
and Noah A. Smith. 2011. Part-of-speech tagging
for Twitter: annotation, features, and experiments.
In Proc. of ACL-HLT, pages 42?47. Portland, Ore-
gon.
Nizar Habash and Ryan Roth. 2009. CATiB: The
Columbia Arabic Treebank. In Proc. of ACL-
IJCNLP, pages 221?224. Suntec, Singapore.
Jan Haji
?
c. 1998. Building a syntactically annotated
corpus: the Prague Dependency Treebank. In Eva
Haji?cov?, editor, Issues of Valency and Meaning.
Studies in Honor of Jarmila Panevov?, pages 12?
19. Prague Karolinum, Charles University Press,
Prague.
Arantza D?az De Ilarraza, Aitzpea Garmendia, and
Maite Oronoz. 2004. Abar-Hitz: An annotation tool
for the Basque dependency treebank. In Proc. of
LREC, pages 251?254. Lisbon, Portugal.
Mohamed Maamouri, Ann Bies, Tim Buckwalter, and
Wigdan Mekki. 2004. The Penn Arabic Tree-
bank: building a large-scale annotated Arabic cor-
pus. In NEMLAR Conference on Arabic Language
Resources and Tools, pages 102?109. Cairo.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: the Penn Treebank. Computa-
tional Linguistics, 19(2):313?330.
Thomas Morton and Jeremy LaCivita. 2003.
WordFreak: An open tool for linguistic anno-
tation. In Proc. of HLT-NAACL: Demonstrations,
pages 17?18. Edmonton, Canada.
Petr Pajas and Peter Fabian. 2000?2011. Tree Editor
TrEd 2.0. http://ufal.mff.cuni.cz/tred/.
Mary Blanche Rossman and Mary Wilda Mills. 1922.
Graded Sentences for Analysis, Selected from the
Best Literature and Systematically Graded for Class
Use. L. A. Noble.
Nathan Schneider, Brendan O?Connor, Naomi Saphra,
David Bamman, Manaal Faruqui, Noah A. Smith,
Chris Dyer, and Jason Baldridge. 2013. A frame-
work for (under)specifying dependency syntax with-
out overloading annotators. In Proc. of the 7th Lin-
guistic Annotation Workshop and Interoperability
with Discourse, pages 51?60. Sofia, Bulgaria.
Nancy Staggers and David Kobus. 2000. Comparing
response time, errors, and satisfaction between text-
based and graphical user interfaces during nursing
order tasks. Journal of the American Medical Infor-
matics Association, 7(2):164?176.
Pontus Stenetorp, Sampo Pyysalo, Goran Topi
?
c,
Tomoko Ohta, Sophia Ananiadou, and Jun?ichi Tsu-
jii. 2012. brat: a web-based tool for NLP-assisted
text annotation. In Proc. of EACL: Demonstrations,
pages 102?107. Avignon, France.
Anne Vilnat, Patrick Paroubek, Eric Villemonte
de la Clergerie, Gil Francopoulo, and Marie-Laure
Gu?not. 2010. PASSAGE syntactic representation:
a minimal common ground for evaluation. In Proc.
of LREC, pages 2478?2485. Valletta, Malta.
Seid Muhie Yimam, Iryna Gurevych, Richard
Eckart de Castilho, and Chris Biemann. 2013.
WebAnno: A flexible, web-based and visually
supported system for distributed annotations. In
Proc. of ACL: Demonstrations, pages 1?6. Sofia,
Bulgaria.
126
Discriminative Lexical Semantic Segmentation with Gaps:
Running the MWE Gamut
Nathan Schneider Emily Danchik Chris Dyer Noah A. Smith
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{nschneid,emilydan,cdyer,nasmith}@cs.cmu.edu
Abstract
We present a novel representation, evaluation
measure, and supervised models for the task of
identifying the multiword expressions (MWEs)
in a sentence, resulting in a lexical seman-
tic segmentation. Our approach generalizes
a standard chunking representation to encode
MWEs containing gaps, thereby enabling effi-
cient sequence tagging algorithms for feature-
rich discriminative models. Experiments on a
new dataset of English web text offer the first
linguistically-driven evaluation of MWE iden-
tification with truly heterogeneous expression
types. Our statistical sequence model greatly
outperforms a lookup-based segmentation pro-
cedure, achieving nearly 60% F1 for MWE
identification.
1 Introduction
Language has a knack for defying expectations when
put under the microscope. For example, there is the
notion?sometimes referred to as compositionality?
that words will behave in predictable ways, with indi-
vidual meanings that combine to form complex mean-
ings according to general grammatical principles. Yet
language is awash with examples to the contrary:
in particular, idiomatic expressions such as awash
with NP, have a knack for VP-ing, to the contrary, and
defy expectations. Thanks to processes like metaphor
and grammaticalization, these are (to various degrees)
semantically opaque, structurally fossilized, and/or
statistically idiosyncratic. In other words, idiomatic
expressions may be exceptional in form, function,
or distribution. They are so diverse, so unruly, so
1. MW named entities: Prime Minister Tony Blair
2. MW compounds: hot air balloon, skinny dip
3. conventionally SW compounds: somewhere
4. verb-particle: pick up, dry out, take over, cut short
5. verb-preposition: refer to, depend on, look for
6. verb-noun(-preposition): pay attention (to)
7. support verb: make decisions, take pictures
8. other phrasal verb: put up with, get rid of
9. PP modifier: above board, at all, from time to time
10. coordinated phrase: cut and dry, more or less
11. connective: as well as, let alone, in spite of
12. semi-fixed VP: pick up where <one> left off
13. fixed phrase: scared to death, leave of absence
14. phatic: You?re welcome. Me neither!
15. proverb: Beggars can?t be choosers.
Figure 1: Some of the classes of idioms in English.
The examples included here contain multiple lexicalized
words?with the exception of those in (3), if the conven-
tional single-word (SW) spelling is used.
difficult to circumscribe, that entire theories of syn-
tax are predicated on the notion that constructions
with idiosyncratic form-meaning mappings (Fillmore
et al., 1988; Goldberg, 1995) or statistical properties
(Goldberg, 2006) offer crucial evidence about the
grammatical organization of language.
Here we focus on multiword expressions
(MWEs): lexicalized combinations of two or more
words that are exceptional enough to be considered
as single units in the lexicon. As figure 1 illus-
trates, MWEs occupy diverse syntactic and semantic
functions. Within MWEs, we distinguish (a) proper
names and (b) lexical idioms. The latter have proved
themselves a ?pain in the neck for NLP? (Sag et al.,
2002). Automatic and efficient detection of MWEs,
though far from solved, would have diverse appli-
193
Transactions of the Association for Computational Linguistics, 2 (2014) 193?206. Action Editor: Joakim Nivre.
Submitted 12/2013; Revised 1/2014; Published 4/2014. c?2014 Association for Computational Linguistics.
cations including machine translation (Carpuat and
Diab, 2010), information retrieval (Newman et al.,
2012), opinion mining (Berend, 2011), and second
language learning (Ellis et al., 2008).
It is difficult to establish any comprehensive tax-
onomy of multiword idioms, let alone develop lin-
guistic criteria and corpus resources that cut across
these types. Consequently, the voluminous litera-
ture on MWEs in computational linguistics?see ?7,
Baldwin and Kim (2010), and Ramisch (2012) for
surveys?has been fragmented, looking (for exam-
ple) at subclasses of phrasal verbs or nominal com-
pounds in isolation. To the extent that MWEs have
been annotated in existing corpora, it has usually
been as a secondary aspect of some other scheme.
Traditionally, such resources have prioritized certain
kinds of MWEs to the exclusion of others, so they
are not appropriate for evaluating general-purpose
identification systems.
In this article, we briefly review a shallow form
of analysis for MWEs that is neutral to expression
type, and that facilitates free text annotation with-
out requiring a prespecified MWE lexicon (?2). The
scheme applies to gappy (discontinuous) as well as
contiguous expressions, and allows for a qualitative
distinction of association strengths. In Schneider
et al. (2014) we have applied this scheme to fully an-
notate a 55,000-word corpus of English web reviews
(Bies et al., 2012a), a conversational genre in which
colloquial idioms are highly salient. This article?s
main contribution is to show that the representation?
constrained according to linguistically motivated as-
sumptions (?3)?can be transformed into a sequence
tagging scheme that resembles standard approaches
in named entity recognition and other text chunking
tasks (?4). Along these lines, we develop a discrim-
inative, structured model of MWEs in context (?5)
and train, evaluate, and examine it on the annotated
corpus (?6). Finally, in ?7 and ?8 we comment on
related work and future directions.
2 Annotated Corpus
To build and evaluate a multiword expression ana-
lyzer, we use the MWE-annotated corpus of Schnei-
der et al. (2014). It consists of informal English web
text that has been specifically and completely anno-
tated for MWEs, without reference to any particular
lexicon. To the best of our knowledge, this corpus
is the first to be freely annotated for many kinds of
MWEs (without reference to a lexicon), and is also
the first dataset of social media text with MWE an-
notations beyond named entities. This section gives
a synopsis of the annotation conventions used to de-
velop that resource, as they are important to under-
standing our models and evaluation.
Rationale. The multiword expressions community
has lacked a canonical corpus resource comparable
to benchmark datasets used for problems such as
NER and parsing. Consequently, the MWE litera-
ture has been driven by lexicography: typically, the
goal is to acquire an MWE lexicon with little or no
supervision, or to apply such a lexicon to corpus
data. Studies of MWEs in context have focused on
various subclasses of constructions in isolation, ne-
cessitating special-purpose datasets and evaluation
schemes. By contrast, Schneider et al.?s (2014) cor-
pus creates an opportunity to tackle general-purpose
MWE identification, such as would be desirable for
use by high-coverage downstream NLP systems. It is
used to train and evaluate our models below. The cor-
pus is publicly available as a benchmark for further
research.1
Data. The documents in the corpus are online user
reviews of restaurants, medical providers, retailers,
automotive services, pet care services, etc. Marked
by conversational and opinionated language, this
genre is fertile ground for colloquial idioms (Nunberg
et al., 1994; Moon, 1998). The 723 reviews (55,000
words, 3,800 sentences) in the English Web Tree-
bank (WTB; Bies et al., 2012b) were collected by
Google, tokenized, and annotated with phrase struc-
ture trees in the style of the Penn Treebank (Marcus
et al., 1993). MWE annotators used the sentence and
word tokenizations supplied by the treebank.2
Annotation scheme. The annotation scheme itself
was designed to be as simple as possible. It consists
of grouping together the tokens in each sentence that
belong to the same MWE instance. While annotation
guidelines provide examples of MWE groupings in
a wide range of constructions, the annotator is not
1http://www.ark.cs.cmu.edu/LexSem/
2Because we use treebank data, syntactic parses are available
to assist in post hoc analysis. Syntactic information was not
shown to annotators.
194
# of constituent tokens
2 3 ?4 total
strong 2257 595 172 3024
weak 269 121 69 459
2526 716 241 3483
# of gaps
0 1 2
2626 394 4
322 135 2
2948 529 6
Table 1: Counts in the MWE corpus.
tied to any particular taxonomy or syntactic structure.
This simplifies the number of decisions that have to
be made for each sentence, even if some are difficult.
Further instructions to annotators included:
? Groups should include only the lexically fixed parts
of an expression (modulo inflectional morphology);
this generally excludes determiners and pronouns:
made the mistake, pride themselves on.
? Multiword proper names count as MWEs.
? Misspelled or unconventionally spelled tokens are
interpreted according to the intended word if clear.
? Overtokenized words (spelled as two tokens, but
conventionally one word) are joined as multiwords.
Clitics separated by the tokenization in the corpus?
negative n?t, possessive ?s, etc.?are joined if func-
tioning as a fixed part of a multiword (e.g., T ?s
Cafe), but not if used productively.
Gaps. There are, broadly speaking, three reasons
to group together tokens that are not fully contigu-
ous. Most commonly, gaps contain internal modifiers,
such as good in make good decisions. Syntactic con-
structions such as the passive can result in gaps that
might not otherwise be present: in good decisions
were made, there is instead a gap filled by the pas-
sive auxiliary. Finally, some MWEs may take internal
arguments: they gave me a break. Figure 1 has addi-
tional examples. Multiple gaps can occur even within
the same expression, though it is rare: they agreed to
give Bob a well-deserved break.
Strength. The annotation scheme has two
?strength? levels for MWEs. Clearly idiomatic ex-
pressions are marked as strong MWEs, while mostly
compositional but especially frequent collocations/
phrases (e.g., abundantly clear and patently obvious)
are marked as weak MWEs. Weak multiword groups
are allowed to include strong MWEs as constituents
(but not vice versa). Strong groups are required to
cohere when used inside weak groups: that is, a weak
group cannot include only part of a strong group.
For purposes of annotation, there were no constraints
hinging on the ordering of tokens in the sentence.
Process. MWE annotation proceeded one sentence
at a time. The 6 annotators referred to and improved
the guidelines document on an ongoing basis. Every
sentence was seen independently by at least 2 an-
notators, and differences of opinion were discussed
and resolved (often by marking a weak MWE as a
compromise). See Schneider et al. (2014) for details.
Statistics. The annotated corpus consists of 723
documents (3,812 sentences). MWEs are frequent
in this domain: 57% of sentences (72% of sentences
over 10 words long) and 88% of documents contain
at least one MWE. 8,060/55,579=15% of tokens
belong to an MWE; in total, there are 3,483 MWE
instances. 544 (16%) are strong MWEs containing a
gold-tagged proper noun?most are proper names. A
breakdown appears in table 1.
3 Representation and Task Definition
We define a lexical segmentation of a sentence as a
partitioning of its tokens into segments such that each
segment represents a single unit of lexical meaning.
A multiword lexical expression may contain gaps,
i.e. interruptions by other segments. We impose two
restrictions on gaps that appear to be well-motivated
linguistically:
? Projectivity: Every expression filling a gap must
be completely contained within that gap; gappy
expressions may not interleave.
? No nested gaps: A gap in an expression may be
filled by other single- or multiword expressions, so
long as those do not themselves contain gaps.
Formal grammar. Our scheme corresponds to the
following extended CFG (Thatcher, 1967), where S
is the full sentence and terminals w are word tokens:
S ? X+
X ? w+ (Y+ w+)?
Y ? w+
Each expression X or Y is lexicalized by the words in
one or more underlined variables on the right-hand
side. An X constituent may optionally contain one or
more gaps filled by Y constituents, which must not
contain gaps themselves.3
3MWEs with multiple gaps are rare but attested in data: e.g.,
putting me at my ease. We encountered one violation of the gap
nesting constraint in the reviews data: I have21 nothing21 but21
fantastic things2 to21 say21 . Additionally, the interrupted phrase
195
Denoting multiword groupings with subscripts, My
wife had taken1 her ?072 Ford2 Fusion2 in1 for a
routine oil3 change3 contains 3 multiword groups?{taken, in}, {?07, Ford, Fusion}, {oil, change}?and
7 single-word groups. The first MWE is gappy (ac-
centuated by the box); a single word and a contiguous
multiword group fall within the gap. The projectivity
constraint forbids an analysis like taken1 her ?072
Ford1 Fusion2, while the gap nesting constraint for-
bids taken1 her2 ?07 Ford2 Fusion2 in1.
3.1 Two-level Scheme: Strong vs. Weak MWEs
Our annotated data distinguish two strengths of
MWEs as discussed in ?2. Augmenting the gram-
mar of the previous section, we therefore designate
nonterminals as strong (X , Y ) or weak (X? , Y? ):
S ? X?+
X? ? X+ (Y?+ X+)?
X ? w+ (Y?+ w+)?
Y? ? Y+
Y ? w+
A weak MWE may be lexicalized by single words
and/or strong multiwords. Strong multiwords cannot
contain weak multiwords except in gaps. Further, the
contents of a gap cannot be part of any multiword
that extends outside the gap.4
For example, consider the segmentation: he was
willing to budge1 a2 little2 on1 the price which
means4 a43 lot43 to4 me4. Subscripts denote strong
MW groups and superscripts weak MW groups; un-
marked tokens serve as single-word expressions. The
MW groups are thus {budge, on}, {a, little}, {a, lot},
and {means, {a, lot}, to, me}. As should be evident
from the grammar, the projectivity and gap-nesting
constraints apply here just as in the 1-level scheme.
3.2 Evaluation
Matching criteria. Given that most tokens do not
belong to an MWE, to evaluate MWE identification
we adopt a precision/recall-based measure from the
coreference resolution literature. The MUC criterion
(Vilain et al., 1995) measures precision and recall
great gateways never1 before1 , so23 far23 as23 Hudson knew2 ,
seen1 by Europeans was annotated in another corpus.
4This was violated 6 times in our annotated data: modifiers
within gaps are sometimes collocated with the gappy expression,
as in on12 a12 tight1 budget12 and have12 little1 doubt12.
of links in terms of groups (units) implied by the
transitive closure over those links.5 It can be defined
as follows:
Let a ? b denote a link between two elements
in the gold standard, and a??b denote a link in the
system prediction. Let the ? operator denote the tran-
sitive closure over all links, such that ?a??b? is 1 if
a and b belong to the same (gold) set, and 0 other-
wise. Assuming there are no redundant6 links within
any annotation (which in our case is guaranteed by
linking consecutive words in each MWE), we can
write the MUC precision and recall measures as:
P = ?a,b?a??b ?a??b??a,b?a??b 1 R =
?a,b?a?b ?a???b??a,b?a?b 1
This awards partial credit when predicted and gold
expressions overlap in part. Requiring full MWEs to
match exactly would arguably be too stringent, over-
penalizing larger MWEs for minor disagreements.
We combine precision and recall using the standard
F1 measure of their harmonic mean. This is the link-
based evaluation used for most of our experiments.
For comparison, we also report some results with
a more stringent exact match evaluation where the
span of the predicted MWE must be identical to the
span of the gold MWE for it to count as correct.
Strength averaging. Recall that the 2-level
scheme (?3.1) distinguishes strong vs. weak links/
groups, where the latter category applies to reason-
ably compositional collocations as well as ambigu-
ous or difficult cases. If where one annotation uses
a weak link the other has a strong link or no link at
all, we want to penalize the disagreement less than
if one had a strong link and the other had no link.
To accommodate the 2-level scheme, we therefore
average F?1 , in which all weak links have been con-
verted to strong links, and F?1 , in which they have
been removed: F1 = 12(F?1 +F?1 ).7 If neither annota-tion contains any weak links, this equals the MUC
5As a criterion for coreference resolution, the MUC measure
has perceived shortcomings which have prompted several other
measures (see Recasens and Hovy, 2011 for a review). It is not
clear, however, whether any of these criticisms are relevant to
MWE identification.
6A link between a and b is redundant if the other links already
imply that a and b belong to the same set. A set of N elements is
expressed non-redundantly with exactly N ?1 links.
7Overall precision and recall are likewise computed by aver-
aging ?strengthened? and ?weakened? measurements.
196
no gaps,
1-level
he
O
was
O
willing
O
to
O
budge
O
a
B
little
I
on
O
the
O
price
O
which
O
means
B
a
I
lot
I
to
I
me
I
.
O
(O?BI+)+
no gaps,
2-level
he
O
was
O
willing
O
to
O
budge
O
a
B
little
I?
on
O
the
O
price
O
which
O
means
B
a
I?
lot
I?
to
I?
me
I?
.
O
(O?B[I?I?]+)+
gappy,
1-level
he
O
was
O
willing
O
to
O
budge
B
a
b
little
i
on
I
the
O
price
O
which
O
means
B
a
I
lot
I
to
I
me
I
.
O
(O?B(o?bi+?I)?I+)+
gappy,
2-level
he
O
was
O
willing
O
to
O
budge
B
a
b
little
??
on
I?
the
O
price
O
which
O
means
B
a
I?
lot
I?
to
I?
me
I?
.
O
(O?B(o?b[????]+?[I?I?])?[I?I?]+)+
Figure 2: Examples and regular expressions for the 4 tagging schemes. Strong links are depicted with solid arcs, and
weak links with dotted arcs. The bottom analysis was provided by an annotator; the ones above are simplifications.
score because F1 = F?1 = F?1 . This method appliesto both the link-based and exact match evaluation
criteria.
4 Tagging Schemes
Following (Ramshaw and Marcus, 1995), shallow an-
alysis is often modeled as a sequence-chunking task,
with tags containing chunk-positional information.
The BIO scheme and variants (e.g., BILOU; Ratinov
and Roth, 2009) are standard for tasks like named
entity recognition, supersense tagging, and shallow
parsing.
The language of derivations licensed by the gram-
mars in ?3 allows for a tag-based encoding of MWE
analyses with only bigram constraints. We describe
4 tagging schemes for MWE identification, starting
with BIO and working up to more expressive variants.
They are depicted in figure 2.
No gaps, 1-level (3 tags). This is the standard con-
tiguous chunking representation from Ramshaw and
Marcus (1995) using the tags {O B I}. O is for to-
kens outside any chunk; B marks tokens beginning
a chunk; and I marks other tokens inside a chunk.
Multiword chunks will thus start with B and then I.
B must always be followed by I; I is not allowed at
the beginning of the sentence or following O.
No gaps, 2-level (4 tags). We can distinguish
strength levels by splitting I into two tags: I? for
strong expressions and I? for weak expressions. To
express strong and weak contiguous chunks requires
4 tags: {O B I? I?}. (Marking B with a strength as well
would be redundant because MWEs are never length-
one chunks.) The constraints on I? and I? are the same
as the constraints on I in previous schemes. If I? and
I? occur next to each other, the strong attachment will
receive higher precedence, resulting in analysis of
strong MWEs as nested within weak MWEs.
Gappy, 1-level (6 tags). Because gaps cannot
themselves contain gappy expressions (we do not
support full recursivity), a finite number of additional
tags are sufficient to encode gappy chunks. We there-
fore add lowercase tag variants representing tokens
within a gap: {O o B b I i}. In addition to the con-
straints stated above, no within-gap tag may occur at
the beginning or end of the sentence or immediately
following or preceding O. Within a gap, b, i, and o
behave like their out-of-gap counterparts.
Gappy, 2-level (8 tags). 8 tags are required to en-
code the 2-level scheme with gaps: {O o B b I? ?? I? ??}.
Variants of the inside tag are marked for strength of
the incoming link?this applies gap-externally (capi-
talized tags) and gap-internally (lowercase tags). If I?
or I? immediately follows a gap, its diacritic reflects
the strength of the gappy expression, not the gap?s
contents.
5 Model
With the above representations we model MWE iden-
tification as sequence tagging, one of the paradigms
that has been used previously for identifying con-
tiguous MWEs (Constant and Sigogne, 2011, see
?7).8 Constraints on legal tag bigrams are sufficient
to ensure the full tagging is well-formed subject to
the regular expressions in figure 2; we enforce these
8Hierarchical modeling based on our representations is left
to future work.
197
constraints in our experiments.9
In NLP, conditional random fields (Lafferty et al.,
2001) and the structured perceptron (Collins, 2002)
are popular techniques for discriminative sequence
modeling with a convex loss function. We choose
the second approach for its speed: learning and in-
ference depend mainly on the runtime of the Viterbi
algorithm, whose asymptotic complexity is linear in
the length of the input and (with a first-order Markov
assumption) quadratic in the number of tags. Below,
we review the structured perceptron and discuss our
cost function, features, and experimental setup.
5.1 Cost-Augmented Structured Perceptron
The structured perceptron?s (Collins, 2002) learn-
ing procedure, algorithm 1, generalizes the classic
perceptron algorithm (Freund and Schapire, 1999) to
incorporate a structured decoding step (for sequences,
the Viterbi algorithm) in the inner loop. Thus, train-
ing requires only max inference, which is fast with a
first-order Markov assumption. In training, features
are adjusted where a tagging error is made; the pro-
cedure can be viewed as optimizing the structured
hinge loss. The output of learning is a weight vector
that parametrizes a feature-rich scoring function over
candidate labelings of a sequence.
To better align the learning algorithm with our
F-score?based MWE evaluation (?3.2), we use a
cost-augmented version of the structured perceptron
that is sensitive to different kinds of errors during
training. When recall is the bigger obstacle, we can
adopt the following cost function: given a sentence
x, its gold labeling y?, and a candidate labeling y?,
cost(y?,y?,x) = ?y???
j=1c(y?j ,y?j) where
c(y?,y?) = ?y? ? y??+??y? ? {B,b}?y? ? {O,o}?
A single nonnegative hyperparameter, ? , controls
the tradeoff between recall and accuracy; higher ?
biases the model in favor of recall (possibly hurt-
ing accuracy and precision). This is a slight variant
of the recall-oriented cost function of Mohit et al.
(2012). The difference is that we only penalize
beginning-of-expression recall errors. Preliminary
9The 8-tag scheme licenses 42 tag bigrams: sequences such
as B O and o ?? are prohibited. There are also constraints on the
allowed tags at the beginning and end of the sequence.
Input: data ??x(n),y(n)??Nn=1; number of iterations Mw? 0
w? 0
t ? 1
for m = 1 to M do
for n = 1 to N do?x,y?? ?x(n),y(n)?
y?? argmaxy? (w?g(x,y?)+cost(y,y?,x))
if y? ? y then
w?w+g(x,y)?g(x, y?)
w?w+ tg(x,y)? tg(x, y?)
end
t ? t +1end
end
Output: w?(w/t)
Algorithm 1: Training with the averaged perceptron.
(Adapted from Daum?, 2006, p. 19.)
experiments showed that a cost function penalizing
all recall errors?i.e., with ??y? ? O?y? = O? as the
second term, as in Mohit et al.?tended to append
additional tokens to high-confidence MWEs (such
as proper names) rather than encourage new MWEs,
which would require positing at least two new non-
outside tags.
5.2 Features
Basic features. These are largely based on those
of Constant et al. (2012): they look at word unigrams
and bigrams, character prefixes and suffixes, and POS
tags, as well as lexicon entries that match lemmas10
of multiple words in the sentence. Appendix A lists
the basic features in detail.
Some of the basic features make use of lexicons.
We use or construct 10 lists of English MWEs: all
multiword entries in WordNet (Fellbaum, 1998); all
multiword chunks in SemCor (Miller et al., 1993);
all multiword entries in English Wiktionary;11 the
WikiMwe dataset mined from English Wikipedia
(Hartmann et al., 2012); the SAID database of
phrasal lexical idioms (Kuiper et al., 2003); the
named entities and other MWEs in the WSJ corpus
on the English side of the CEDT (Hajic? et al., 2012);
10The WordNet API in NLTK (Bird et al., 2009) was used for
lemmatization.
11http://en.wiktionary.org; data obtained from
https://toolserver.org/~enwikt/definitions/
enwikt-defs-20130814-en.tsv.gz
198
LOOKUP SUPERVISED MODEL
preexising lexicons entries max gap
length
P R F1 ? P R F1 ?
none 0 74.39 44.43 55.57 2.19
WordNet + SemCor 71k 0 46.15 28.41 35.10 2.44 74.51 45.79 56.64 1.90
6 lexicons 420k 0 35.05 46.76 40.00 2.88 76.08 52.39 61.95 1.67
10 lexicons 437k 0 33.98 47.29 39.48 2.88 75.95 51.39 61.17 2.30
best configuration with
in-domain lexicon
1 46.66 47.90 47.18 2.31 76.64 51.91 61.84 1.65
2 lexicons + MWtypes(train)?1 6 lexicons + MWtypes(train)?2
Table 2: Use of lexicons for lookup-based vs. statistical segmentation. Supervised learning used only basic features
and the structured perceptron, with the 8-tag scheme. Results are with the link-based matching criterion for evaluation.
Top: Comparison of preexisting lexicons. ?6 lexicons? refers to WordNet and SemCor plus SAID, WikiMwe, Phrases.net,
and English Wiktionary; ?10 lexicons? adds MWEs from CEDT, VNC, LVC, and Oyz. (In these lookup-based
configurations, allowing gappy MWEs never helps performance.)
Bottom: Combining preexisting lexicons with a lexicon derived from MWEs annotated in the training portion of each
cross-validation fold at least once (lookup) or twice (model).
All precision, recall, and F1 percentages are averaged across 8 folds of cross-validation on train; standard deviations
are shown for the F1 score. In each column, the highest value using only preexisting lexicons is underlined, and the
highest overall value is bolded. The boxed row indicates the configuration used as the basis for subsequent experiments.
the verb-particle constructions (VPCs) dataset of
(Baldwin, 2008); a list of light verb constructions
(LVCs) provided by Claire Bonial; and two idioms
websites.12 After preprocessing, each lexical entry
consists of an ordered sequence of word lemmas,
some of which may be variables like <something>.
Given a sentence and one or more of the lexicons,
lookup proceeds as follows: we enumerate entries
whose lemma sequences match a sequence of lemma-
tized tokens, and build a lattice of possible analyses
over the sentence. We find the shortest path (i.e.,
using as few expressions as possible) with dynamic
programming, allowing gaps of up to length 2.13
Unsupervised word clusters. Distributional clus-
tering on large (unlabeled) corpora can produce lexi-
cal generalizations that are useful for syntactic and
semantic analysis tasks (e.g.: Miller et al., 2004; Koo
et al., 2008; Turian et al., 2010; Owoputi et al., 2013;
Grave et al., 2013). We were interested to see whether
a similar pattern would hold for MWE identification,
given that MWEs are concerned with what is lexi-
cally idiosyncratic?i.e., backing off from specific
lexemes to word classes may lose the MWE-relevant
information. Brown clustering14 (Brown et al., 1992)
12http://www.phrases.net/ and http://home.
postech.ac.kr/~oyz/doc/idiom.html
13Each top-level lexical expression (single- or multiword)
incurs a cost of 1; each expression within a gap has cost 1.25.
14With Liang?s (2005) implementation: https://github.
com/percyliang/brown-cluster. We obtain 1,000 clusters
on the 21-million-word Yelp Academic Dataset15
(which is similar in genre to the annotated web re-
views data) gives us a hard clustering of word types.
To our tagger, we add features mapping the previ-
ous, current, and next token to Brown cluster IDs.
The feature for the current token conjoins the word
lemma with the cluster ID.
Part-of-speech tags. We compared three PTB-
style POS taggers on the full REVIEWS subcor-
pus (train+test). The Stanford CoreNLP tagger16
(Toutanova et al., 2003) yields an accuracy of 90.4%.
The ARK TweetNLP tagger v. 0.3.2 (Owoputi et al.,
2013) achieves 90.1% with the model17 trained on the
Twitter corpus of Ritter et al. (2011), and 94.9% when
trained on the ANSWERS, EMAIL, NEWSGROUP, and
WEBLOG subcorpora of WTB. We use this third con-
figuration to produce automatic POS tags for training
and testing our MWE tagger. (A comparison condi-
tion in ?6.3 uses oracle POS tags.)
5.3 Experimental Setup
The corpus of web reviews described in ?2 is used
for training and evaluation. 101 arbitrarily chosen
documents (500 sentences, 7,171 words) were held
from words appearing at least 25 times.
15https://www.yelp.com/academic_dataset
16v. 3.2.0, with english-bidirectional-distsim
17http://www.ark.cs.cmu.edu/TweetNLP/model.
ritter_ptb_alldata_fixed.20130723
199
LINK-BASED EXACT MATCH
configuration M ? ?w? P R F1 P R F1
base model 5 ? 1,765k 69.27 50.49 58.35 60.99 48.27 53.85+ recall cost 4 150 1,765k 61.09 57.94 59.41 53.09 55.38 54.17+ clusters 3 100 2,146k 63.98 55.51 59.39 56.34 53.24 54.70+ oracle POS 4 100 2,145k 66.19 59.35 62.53 58.51 57.00 57.71
Table 3: Comparison of supervised models on test (using the 8-tag scheme). The base model corresponds to the boxed
result in table table 2, but here evaluated on test. For each configuration, the number of training iterations M and (except
for the base model) the recall-oriented hyperparameter ? were tuned by cross-validation on train.
out as a final test set. This left 3,312 sentences/
48,408 words for training/development (train). Fea-
ture engineering and hyperparameter tuning were
conducted with 8-fold cross-validation on train. The
8-tag scheme is used except where otherwise noted.
In learning with the structured perceptron (algo-
rithm 1), we employ two well-known techniques that
can both be viewed as regularization. First, we use
the average of parameters over all timesteps of learn-
ing. Second, within each cross-validation fold, we de-
termine the number of training iterations (epochs) M
by early stopping?that is, after each iteration, we use
the model to decode the held-out data, and when that
accuracy ceases to improve, use the previous model.
The two hyperparameters are the number of iterations
and the value of the recall cost hyperparameter (?).
Both are tuned via cross-validation on train; we use
the multiple of 50 that maximizes average link-based
F1. The chosen values are shown in table 3. Experi-
ments were managed with the ducttape tool.18
6 Results
We experimentally address the following questions
to probe and justify our modeling approach.
6.1 Is supervised learning necessary?
Previous MWE identification studies have found
benefit to statistical learning over heuristic lexicon
lookup (Constant and Sigogne, 2011; Green et al.,
2012). Our first experiment tests whether this holds
for comprehensive MWE identification: it compares
our supervised tagging approach with baselines of
heuristic lookup on preexisting lexicons. The base-
lines construct a lattice for each sentence using the
same method as lexicon-based model features (?5.2).
If multiple lexicons are used, the union of their en-
18https://github.com/jhclark/ducttape/
tries is used to construct the lattice. The resulting
segmentation?which does not encode a strength
distinction?is evaluated against the gold standard.
Table 2 shows the results. Even with just the la-
beled training set as input, the supervised approach
beats the strongest heuristic baseline (that incorpo-
rates in-domain lexicon entries extracted from the
training data) by 30 precision points, while achieving
comparable recall. For example, the baseline (but not
the statistical model) incorrectly predicts an MWE in
places to eat in Baltimore (because eat in, meaning
?eat at home,? is listed in WordNet). The supervised
approach has learned not to trust WordNet too much
due to this sort of ambiguity. Downstream applica-
tions that currently use lexicon matching for MWE
identification (e.g., Ghoneim and Diab, 2013) likely
stand to benefit from our statistical approach.
6.2 How best to exploit MWE lexicons
(type-level information)?
For statistical tagging (right portion of table 2), using
more preexisting (out-of-domain) lexicons generally
improves recall; precision also improves a bit.
A lexicon of MWEs occurring in the non-held-out
training data at least twice19 (table 2, bottom right) is
marginally worse (better precision/worse recall) than
the best result using only preexisting lexicons.
6.3 Variations on the base model
We experiment with some of the modeling alterna-
tives discussed in ?5. Results appear in table 3 under
both the link-based and exact match evaluation cri-
teria. We note that the exact match scores are (as
expected) several points lower.
19If we train with access to the full lexicon of training
set MWEs, the learner credulously overfits to relying on that
lexicon?after all, it has perfect coverage of the training data!?
which proves fatal for the model at test time.
200
Recall-oriented cost. The recall-oriented cost
adds about 1 link-based F1 point, sacrificing precision
in favor of recall.
Unsupervised word clusters. When combined
with the recall-oriented cost, these produce a slight
improvement to precision/degradation to recall, im-
proving exact match F1 but not affecting link-based
F1. Only a few clusters receive high positive weight;
one of these consists of matter, joke, biggie, pun,
avail, clue, corkage, frills, worries, etc. These words
are diverse semantically, but all occur in collocations
with no, which is what makes the cluster coherent
and useful to the MWE model.
Oracle part-of-speech tags. Using human-
annotated rather than automatic POS tags improves
MWE identification by about 3 F1 points on test
(similar differences were observed in development).
6.4 What are the highest-weighted features?
An advantage of the linear modeling framework is
that we can examine learned feature weights to gain
some insight into the model?s behavior.
In general, the highest-weighted features are the
lexicon matching features and features indicative of
proper names (POS tag of proper noun, capitalized
word not at the beginning of the sentence, etc.).
Despite the occasional cluster capturing colloca-
tional or idiomatic groupings, as described in the
previous section, the clusters appear to be mostly
useful for identifying words that tend to belong (or
not) to proper names. For example, the cluster with
street, road, freeway, highway, airport, etc., as well
as words outside of the cluster vocabulary, weigh
in favor of an MWE. A cluster with everyday desti-
nations (neighborhood, doctor, hotel, bank, dentist)
prefers non-MWEs, presumably because these words
are not typically part of proper names in this corpus.
This was from the best model using non-oracle POS
tags, so the clusters are perhaps useful in correct-
ing for proper nouns that were mistakenly tagged
as common nouns. One caveat, though, is that it is
hard to discern the impact of these specific features
where others may be capturing essentially the same
information.
6.5 How heterogeneous are learned MWEs?
On test, the final model (with automatic POS tags)
predicts 365 MWE instances (31 are gappy; 23 are
POS pattern # examples (lowercased lemmas)
NOUN NOUN 53 customer service, oil change
VERB PREP 36 work with, deal with, yell at
PROPN PROPN 29 eagle transmission, comfort zone
ADJ NOUN 21 major award, top notch, mental health
VERB PART 20 move out, end up, pick up, pass up
VERB ADV 17 come back, come in, come by, stay away
PREP NOUN 12 on time, in fact, in cash, for instance
VERB NOUN 10 take care, make money, give crap
VERB PRON 10 thank you, get it
PREP PREP 8 out of, due to, out ta, in between
ADV ADV 6 no matter, up front, at all, early on
DET NOUN 6 a lot, a little, a bit, a deal
VERB DET NOUN 6 answer the phone, take a chance
NOUN PREP 5 kind of, care for, tip on, answer to
Table 4: Top predicted POS patterns and frequencies.
weak). There are 298 unique MWE types.
Organizing the predicted MWEs by their coarse
POS sequence reveals that the model is not too preju-
diced in the kinds of expressions it recognizes: the
298 types fall under 89 unique POS+strength patterns.
Table 4 shows the 14 POS sequences predicted 5 or
more times as strong MWEs. Some of the examples
(major award, a deal, tip on) are false positives, but
most are correct. Singleton patterns include PROPN
VERB (god forbid), PREP DET (at that), ADJ PRON
(worth it), and PREP VERB PREP (to die for).
True positive MWEs mostly consist of (a) named
entities, and (b) lexical idioms seen in training and/or
listed in one of the lexicons. Occasionally the sys-
tem correctly guesses an unseen and OOV idiom
based on features such as hyphenation (walk - in) and
capitalization/OOV words (Chili Relleno, BIG MIS-
TAKE). On test, 244 gold MWE types were unseen
in training; the system found 93 true positives (where
the type was predicted at least once), 109 false posi-
tives, and 151 false negatives?an unseen type recall
rate of 38%. Removing types that occurred in lexi-
cons leaves 35 true positives, 61 false positives, and
111 false negatives?a unseen and OOV type recall
rate of 24%.
6.6 What kinds of mismatches occur?
Inspection of the output turns up false positives due
to ambiguity (e.g., Spongy and sweet bread); false
negatives (top to bottom); and overlap (get high qual-
ity service, gold get high quality service; live up to,
gold live up to). A number of the mismatches turn
201
scheme ?Y ? ? M ?w? P R F1
no gaps, 1-level 3 100 2.1 733k 73.33 55.72 63.20
no gaps, 2-level 4 150 3.3 977k 72.60 59.11 65.09
gappy, 1-level 6 200 1.6 1,466k 66.48 61.26 63.65
gappy, 2-level 8 100 3.5 1,954k 73.27 60.44 66.15
Table 5: Training with different tagging schemes. Results
are cross-validation averages on train. All schemes are
evaluated against the full gold standard (8 tags).
out to be problems with the gold standard, like hav-
ing our water shut off (gold having our water shut
off ). This suggests that even noisy automatic taggers
might help identify annotation inconsistencies and
errors for manual correction.
6.7 Are gappiness and the strength distinction
learned in practice?
Three quarters of MWEs are strong and contain no
gaps. To see whether our model is actually sensi-
tive to the phenomena of gappiness and strength,
we train on data simplified to remove one or both
distinctions?as in the first 3 labelings in figure 2?
and evaluate against the full 8-tag scheme. For the
model with the recall cost, clusters, and oracle POS
tags, we evaluate each of these simplifications of
the training data in table 5. The gold standard for
evaluation remains the same across all conditions.
If the model was unable to recover gappy expres-
sions or the strong/weak distinction, we would expect
it to do no better when trained with the full tagset than
with the simplified tagset. However, there is some
loss in performance as the tagset for learning is sim-
plified, which suggests that gappiness and strength
are being learned to an extent.
7 Related Work
Our annotated corpus (Schneider et al., 2014) joins
several resources that indicate certain varieties of
MWEs: lexicons such as WordNet (Fellbaum, 1998),
SAID (Kuiper et al., 2003), and WikiMwe (Hartmann
et al., 2012); targeted lists (Baldwin, 2005, 2008;
Cook et al., 2008; Tu and Roth, 2011, 2012); web-
sites like Wiktionary and Phrases.net; and large-scale
corpora such as SemCor (Miller et al., 1993), the
French Treebank (Abeill? et al., 2003), the Szeged-
ParalellFX corpus (Vincze, 2012), and the Prague
Czech-English Dependency Treebank (C?mejrek et al.,
2005). The difference is that Schneider et al. (2014)
pursued a comprehensive annotation approach rather
than targeting specific varieties of MWEs or relying
on a preexisting lexical resource. The annotations
are shallow, not relying explicitly on syntax (though
in principle they could be mapped onto the parses in
the Web Treebank).
In terms of modeling, the use of machine learn-
ing classification (Hashimoto and Kawahara, 2008;
Shigeto et al., 2013) and specifically BIO sequence
tagging (Diab and Bhutada, 2009; Constant and Si-
gogne, 2011; Constant et al., 2012; Vincze et al.,
2013) for contextual recognition of MWEs is not
new. Lexical semantic classification tasks like named
entity recognition (e.g., Ratinov and Roth, 2009), su-
persense tagging (Ciaramita and Altun, 2006; Paa?
and Reichartz, 2009), and index term identification
(Newman et al., 2012) also involve chunking of cer-
tain MWEs. But our discriminative models, facili-
tated by the new corpus, broaden the scope of the
MWE identification task to include many varieties of
MWEs at once, including explicit marking of gaps
and a strength distinction. By contrast, the afore-
mentioned identification systems, as well as some
MWE-enhanced syntactic parsers (e.g., Green et al.,
2012), have been restricted to contiguous MWEs.
However, Green et al. (2011) allow gaps to be de-
scribed as constituents in a syntax tree. Gimpel and
Smith?s (2011) shallow, gappy language model al-
lows arbitrary token groupings within a sentence,
whereas our model imposes projectivity and nest-
ing constraints (?3). Blunsom and Baldwin (2006)
present a sequence model for HPSG supertagging,
and evaluate performance on discontinuous MWEs,
though the sequence model treats the non-adjacent
component supertags like other labels?it cannot en-
force that they mutually require one another, as we
do via the gappy tagging scheme (?3.1). The lexicon
lookup procedures of Bejc?ek et al. (2013) can match
gappy MWEs, but are nonstatistical and extremely
error-prone when tuned for high oracle recall.
Another major thread of research has pursued un-
supervised discovery of multiword types from raw
corpora, such as with statistical association measures
(Church et al., 1991; Pecina, 2010; Ramisch et al.,
2012, inter alia), parallel corpora (Melamed, 1997;
Moir?n and Tiedemann, 2006; Tsvetkov and Wint-
ner, 2010), or a combination thereof (Tsvetkov and
202
Wintner, 2011); this may be followed by a lookup-
and-classify approach to contextual identification
(Ramisch et al., 2010). Though preliminary experi-
ments with our models did not show benefit to incor-
porating such automatically constructed lexicons, we
hope these two perspectives can be brought together
in future work.
8 Conclusion
This article has presented the first supervised model
for identifying heterogeneous multiword expressions
in English text. Our feature-rich discriminative se-
quence tagger performs shallow chunking with a
novel scheme that allows for MWEs containing gaps,
and includes a strength distinction to separate highly
idiomatic expressions from collocations. It is trained
and evaluated on a corpus of English web reviews
that are comprehensively annotated for multiword
expressions. Beyond the training data, its features in-
corporate evidence from external resources?several
lexicons as well as unsupervised word clusters; we
show experimentally that this statistical approach is
far superior to identifying MWEs by heuristic lexicon
lookup alone. Future extensions might integrate addi-
tional features (e.g., exploiting statistical association
measures computed over large corpora), enhance the
lexical representation (e.g., by adding semantic tags),
improve the expressiveness of the model (e.g., with
higher-order features and inference), or integrate the
model with other tasks (such as parsing and transla-
tion).
Our data and open source software are released at
http://www.ark.cs.cmu.edu/LexSem/.
Acknowledgments
This research was supported in part by NSF CA-
REER grant IIS-1054319, Google through the Read-
ing is Believing project at CMU, and DARPA grant
FA8750-12-2-0342 funded under the DEFT program.
We are grateful to Kevin Knight, Martha Palmer,
Claire Bonial, Lori Levin, Ed Hovy, Tim Baldwin,
Omri Abend, members of JHU CLSP, the NLP group
at Berkeley, and the Noah?s ARK group at CMU, and
anonymous reviewers for valuable feedback.
A Basic Features
All are conjoined with the current label, yi.
Label Features
1. previous label (the only first-order feature)
Token Features
Original token
2. i = {1,2}
3. i = ?w??{0,1}
4. capitalized ? ?i = 0?
5. word shape
Lowercased token
6. prefix: [wi]k1 ?4k=1
7. suffix: [wi]?w?j ??w?j=?w??3
8. has digit
9. has non-alphanumeric c
10. context word: w j ?i+2j=i?2
11. context word bigram: w j+1j ?i+1j=i?2
Lemma Features
12. lemma + context lemma if one of them is a verb and the other
is a noun, verb, adjective, adverb, preposition, or particle: ?i ?
? j ?i+2j=i?2
Part-of-speech Features
13. context POS: pos j ?i+2j=i?2
14. context POS bigram: pos j+1j ?i+1j=i?2
15. word + context POS: wi?posi?1
16. context word + POS: wi?1?posi
Lexicon Features (unlexicalized)
WordNet only
17. OOV: ?i is not in WordNet as a unigram lemma ? posi
18. compound: non-punctuation lemma ?i and the {previous,
next} lemma in the sentence (if it is non-punctuation; an inter-
vening hyphen is allowed) form an entry in WordNet, possibly
separated by a hyphen or space
19. compound-hyphen: posi = HYPH ? previous and next tokens
form an entry in WordNet, possibly separated by a hyphen or
space
20. ambiguity class: if content word unigram ?i is in WordNet,
the set of POS categories it can belong to; else posi if not a
content POS ? the POS of the longest MW match to which ?i
belongs (if any) ? the position in that match (B or I)
For each multiword lexicon
21. lexicon name ? status of token i in the shortest path segmen-
tation (O, B, or I) ? subcategory of lexical entry whose match
includes token i, if matched ? whether the match is gappy
22. the above ? POS tags of the first and last matched tokens in
the expression
Over all multiword lexicons
23. at least k lexicons contain a match that includes this token (if
n ? 1 matches, n active features)
24. at least k lexicons contain a match that includes this token,
starts with a given POS, and ends with a given POS
203
References
Anne Abeill?, Lionel Cl?ment, and Fran?ois Toussenel.
2003. Building a treebank for French. In Anne Abeill?
and Nancy Ide, editors, Treebanks, volume 20 of Text,
Speech and Language Technology, pages 165?187.
Kluwer Academic Publishers, Dordrecht, The Nether-
lands.
Timothy Baldwin. 2005. Looking for prepositional verbs
in corpus data. In Proc. of the Second ACL-SIGSEM
Workshop on the Linguistic Dimensions of Prepositions
and their Use in Computational Linguistics Formalisms
and Applications, pages 115?126. Colchester, UK.
Timothy Baldwin. 2008. A resource for evaluating the
deep lexical acquisition of English verb-particle con-
structions. In Proc. of MWE, pages 1?2. Marrakech,
Morocco.
Timothy Baldwin and Su Nam Kim. 2010. Multiword
expressions. In Nitin Indurkhya and Fred J. Damerau,
editors, Handbook of Natural Language Processing,
Second Edition. CRC Press, Taylor and Francis Group,
Boca Raton, Florida, USA.
Eduard Bejc?ek, Pavel Stran??k, and Pavel Pecina. 2013.
Syntactic identification of occurrences of multiword
expressions in text using a lexicon with dependency
structures. In Proc. of the 9th Workshop on Multiword
Expressions, pages 106?115. Atlanta, Georgia, USA.
G?bor Berend. 2011. Opinion expression mining by ex-
ploiting keyphrase extraction. In Proc. of 5th Interna-
tional Joint Conference on Natural Language Process-
ing, pages 1162?1170. Chiang Mai, Thailand.
Ann Bies, Justin Mott, Colin Warner, and Seth Kulick.
2012a. English Web Treebank. Technical Report
LDC2012T13, Linguistic Data Consortium, Philadel-
phia, Pennsylvania, USA.
Ann Bies, Justin Mott, Colin Warner, and Seth Kulick.
2012b. English Web Treebank. Technical Report
LDC2012T13, Linguistic Data Consortium, Philadel-
phia, Pennsylvania, USA.
Steven Bird, Ewan Klein, and Edward Loper. 2009. Natu-
ral Language Processing with Python: Analyzing Text
with the Natural Language Toolkit. O?Reilly Media,
Inc., Sebastopol, California, USA.
Phil Blunsom and Timothy Baldwin. 2006. Multilingual
deep lexical acquisition for HPSGs via supertagging.
In Proc. of EMNLP, pages 164?171. Sydney, Australia.
Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vin-
cent J. Della Pietra, and Jenifer C. Lai. 1992. Class-
based n-gram models of natural language. Computa-
tional Linguistics, 18(4):467?479.
Marine Carpuat and Mona Diab. 2010. Task-based eval-
uation of multiword expressions: a pilot study in sta-
tistical machine translation. In Proc. of NAACL-HLT,
pages 242?245. Los Angeles, California, USA.
Kenneth Church, William Gale, Patrick Hanks, and Don-
ald Hindle. 1991. Using statistics in lexical analysis.
In Uri Zernik, editor, Lexical acquisition: exploiting
on-line resources to build a lexicon, pages 115?164.
Lawrence Erlbaum Associates, Hillsdale, New Jersey,
USA.
Massimiliano Ciaramita and Yasemin Altun. 2006. Broad-
coverage sense disambiguation and information extrac-
tion with a supersense sequence tagger. In Proc. of
EMNLP, pages 594?602. Sydney, Australia.
Michael Collins. 2002. Discriminative training methods
for Hidden Markov Models: theory and experiments
with perceptron algorithms. In Proc. of EMNLP, pages
1?8. Philadelphia, Pennsylvania, USA.
Matthieu Constant and Anthony Sigogne. 2011. MWU-
aware part-of-speech tagging with a CRF model and
lexical resources. In Proc. of the Workshop on Multi-
word Expressions: from Parsing and Generation to the
Real World, pages 49?56. Portland, Oregon, USA.
Matthieu Constant, Anthony Sigogne, and Patrick Watrin.
2012. Discriminative strategies to integrate multiword
expression recognition and parsing. In Proc. of ACL,
pages 204?212. Jeju Island, Korea.
Paul Cook, Afsaneh Fazly, and Suzanne Stevenson. 2008.
The VNC-Tokens dataset. In Proc. of MWE, pages
19?22. Marrakech, Morocco.
Hal Daum?, III. 2006. Practical structured learning tech-
niques for natural language processing. Ph.D. disserta-
tion, University of Southern California, Los Angeles,
California, USA. URL http://hal3.name/docs/
daume06thesis.pdf.
Mona Diab and Pravin Bhutada. 2009. Verb noun con-
struction MWE token classification. In Proc. of MWE,
pages 17?22. Suntec, Singapore.
Nick C. Ellis, Rita Simpson-Vlach, and Carson Maynard.
2008. Formulaic language in native and second lan-
guage speakers: psycholinguistics, corpus linguistics,
and TESOL. TESOL Quarterly, 42(3):375?396.
Christiane Fellbaum, editor. 1998. WordNet: an elec-
tronic lexical database. MIT Press, Cambridge, Mas-
sachusetts, USA.
Charles J. Fillmore, Paul Kay, and Mary Catherine
O?Connor. 1988. Regularity and idiomaticity in gram-
matical constructions: the case of ?let alone?. Language,
64(3):501?538.
Yoav Freund and Robert E. Schapire. 1999. Large margin
classification using the perceptron algorithm. Machine
Learning, 37(3):277?296.
Mahmoud Ghoneim and Mona Diab. 2013. Multiword
expressions in the context of statistical machine trans-
204
lation. In Proc. of IJCNLP, pages 1181?1187. Nagoya,
Japan.
Kevin Gimpel and Noah A. Smith. 2011. Generative
models of monolingual and bilingual gappy patterns.
In Proc. of WMT, pages 512?522. Edinburgh, Scotland,
UK.
Adele E. Goldberg. 1995. Constructions: a construction
grammar approach to argument structure. University
of Chicago Press, Chicago, Illinois, USA.
Adele E. Goldberg. 2006. Constructions at work: the
nature of generalization in language. Oxford University
Press, Oxford, UK.
Edouard Grave, Guillaume Obozinski, and Francis Bach.
2013. Hidden Markov tree models for semantic class
induction. In Proc. of CoNLL, pages 94?103. Sofia,
Bulgaria.
Spence Green, Marie-Catherine de Marneffe, John Bauer,
and Christopher D. Manning. 2011. Multiword expres-
sion identification with tree substitution grammars: a
parsing tour de force with French. In Proc. of EMNLP,
pages 725?735. Edinburgh, Scotland, UK.
Spence Green, Marie-Catherine de Marneffe, and Christo-
pher D. Manning. 2012. Parsing models for identify-
ing multiword expressions. Computational Linguistics,
39(1):195?227.
Jan Hajic?, Eva Hajic?ov?, Jarmila Panevov?, Petr Sgall,
Silvie Cinkov?, Eva Fuc??kov?, Marie Mikulov?, Petr
Pajas, Jan Popelka, Jir?? Semeck?, Jana ?indlerov?, Jan
?te?p?nek, Josef Toman, Zden?ka Ure?ov?, and Zdene?k
?abokrtsk?. 2012. Prague Czech-English Dependency
Treebank 2.0. Technical Report LDC2012T08, Linguis-
tic Data Consortium, Philadelphia, Pennsylvania, USA.
URL http://www.ldc.upenn.edu/Catalog/
catalogEntry.jsp?catalogId=LDC2003T10.
Silvana Hartmann, Gy?rgy Szarvas, and Iryna Gurevych.
2012. Mining multiword terms from Wikipedia. In
Maria Teresa Pazienza and Armando Stellato, editors,
Semi-Automatic Ontology Development. IGI Global,
Hershey, Pennsylvania, USA.
Chikara Hashimoto and Daisuke Kawahara. 2008. Con-
struction of an idiom corpus and its application to id-
iom identification based on WSD incorporating idiom-
specific features. In Proc. of EMNLP, pages 992?1001.
Honolulu, Hawaii, USA.
Terry Koo, Xavier Carreras, and Michael Collins. 2008.
Simple semi-supervised dependency parsing. In Proc.
of ACL-08: HLT, pages 595?603. Columbus, Ohio.
Koenraad Kuiper, Heather McCann, Heidi Quinn,
Therese Aitchison, and Kees van der Veer. 2003.
SAID. Technical Report LDC2003T10, Linguistic
Data Consortium, Philadelphia, Pennsylvania, USA.
URL http://www.ldc.upenn.edu/Catalog/
catalogEntry.jsp?catalogId=LDC2003T10.
John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: probabilistic
models for segmenting and labeling sequence data. In
Proc. of ICML, pages 282?289.
Percy Liang. 2005. Semi-supervised learning for nat-
ural language. Master?s thesis, Massachusetts In-
stitute of Technology, Cambridge, Massachusetts,
USA. URL http://people.csail.mit.edu/
pliang/papers/meng-thesis.pdf.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: the Penn Treebank. Computational
Linguistics, 19(2):313?330.
I. Dan Melamed. 1997. Automatic discovery of non-
compositional compounds in parallel data. In Proc.
of EMNLP, pages 97?108. Providence, Rhode Island,
USA.
George A. Miller, Claudia Leacock, Randee Tengi, and
Ross T. Bunker. 1993. A semantic concordance. In
Proc. of HLT, pages 303?308. Plainsboro, New Jersey,
USA.
Scott Miller, Jethran Guinness, and Alex Zamanian. 2004.
Name tagging with word clusters and discriminative
training. In Proc. of HLT-NAACL, pages 337?342.
Boston, Massachusetts, USA.
Behrang Mohit, Nathan Schneider, Rishav Bhowmick, Ke-
mal Oflazer, and Noah A. Smith. 2012. Recall-oriented
learning of named entities in Arabic Wikipedia. In
Proc. of EACL, pages 162?173. Avignon, France.
Begona Villada Moir?n and J?rg Tiedemann. 2006. Iden-
tifying idiomatic expressions using automatic word-
alignment. In Proc. of the EACL 2006 Workshop
on Multi-word Expressions in a Multilingual Context,
pages 33?40. Trento, Italy.
Rosamund Moon. 1998. Fixed expressions and idioms
in English: a corpus-based approach. Oxford Stud-
ies in Lexicography and Lexicology. Clarendon Press,
Oxford, UK.
David Newman, Nagendra Koilada, Jey Han Lau, and
Timothy Baldwin. 2012. Bayesian text segmentation
for index term identification and keyphrase extraction.
In Proc. of COLING 2012, pages 2077?2092. Mumbai,
India.
Geoffrey Nunberg, Ivan A. Sag, and Thomas Wasow. 1994.
Idioms. Language, 70(3):491?538.
Olutobi Owoputi, Brendan O?Connor, Chris Dyer, Kevin
Gimpel, Nathan Schneider, and Noah A. Smith. 2013.
Improved part-of-speech tagging for online conversa-
tional text with word clusters. In Proc. of NAACL-HLT,
pages 380?390. Atlanta, Georgia, USA.
Gerhard Paa? and Frank Reichartz. 2009. Exploiting
205
semantic constraints for estimating supersenses with
CRFs. In Proc. of the Ninth SIAM International Confer-
ence on Data Mining, pages 485?496. Sparks, Nevada,
USA.
Pavel Pecina. 2010. Lexical association measures and
collocation extraction. Language Resources and Evalu-
ation, 44(1):137?158.
Carlos Ramisch. 2012. A generic and open
framework for multiword expressions treatment:
from acquisition to applications. Ph.D. disser-
tation, University of Grenoble and Federal Uni-
versity of Rio Grande do Sul, Grenoble, France.
URL http://www.inf.ufrgs.br/~ceramisch/
download_files/thesis-getalp.pdf.
Carlos Ramisch, Vitor De Araujo, and Aline Villavicencio.
2012. A broad evaluation of techniques for automatic
acquisition of multiword expressions. In Proc. of ACL
2012 Student Research Workshop, pages 1?6. Jeju Is-
land, Korea.
Carlos Ramisch, Aline Villavicencio, and Christian Boitet.
2010. mwetoolkit: a framework for multiword expres-
sion identification. In Proc. of LREC, pages 662?669.
Valletta, Malta.
Lance A. Ramshaw and Mitchell P. Marcus. 1995. Text
chunking using transformation-based learning. In Proc.
of the Third ACL Workshop on Very Large Corpora,
pages 82?94. Cambridge, Massachusetts, USA.
Lev Ratinov and Dan Roth. 2009. Design challenges and
misconceptions in named entity recognition. In Proc.
of CoNLL, pages 147?155. Boulder, Colorado, USA.
Marta Recasens and Eduard Hovy. 2011. BLANC: Im-
plementing the Rand index for coreference evaluation.
Natural Language Engineering, 17(04):485?510.
Alan Ritter, Sam Clark, Mausam, and Oren Etzioni. 2011.
Named entity recognition in tweets: an experimental
study. In Proc. of EMNLP, pages 1524?1534. Edin-
burgh, Scotland, UK.
Ivan Sag, Timothy Baldwin, Francis Bond, Ann Copes-
take, and Dan Flickinger. 2002. Multiword expressions:
a pain in the neck for NLP. In Alexander Gelbukh,
editor, Computational Linguistics and Intelligent Text
Processing, volume 2276 of Lecture Notes in Computer
Science, pages 189?206. Springer, Berlin, Germany.
Nathan Schneider, Spencer Onuffer, Nora Kazour, Emily
Danchik, Michael T. Mordowanec, Henrietta Conrad,
and Noah A. Smith. 2014. Comprehensive annotation
of multiword expressions in a social web corpus. In
Proc. of LREC. Reykjav?k, Iceland.
Yutaro Shigeto, Ai Azuma, Sorami Hisamoto, Shuhei
Kondo, Tomoya Kouse, Keisuke Sakaguchi, Akifumi
Yoshimoto, Frances Yung, and Yuji Matsumoto. 2013.
Construction of English MWE dictionary and its appli-
cation to POS tagging. In Proc. of the 9th Workshop
on Multiword Expressions, pages 139?144. Atlanta,
Georgia, USA.
James W. Thatcher. 1967. Characterizing derivation trees
of context-free grammars through a generalization of
finite automata theory. Journal of Computer and System
Sciences, 1(4):317?322.
Kristina Toutanova, Dan Klein, Christopher D. Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Proc.
of HLT-NAACL, pages 173?180. Edmonton, Alberta,
Canada.
Yulia Tsvetkov and Shuly Wintner. 2010. Extraction of
multi-word expressions from small parallel corpora.
In Coling 2010: Posters, pages 1256?1264. Beijing,
China.
Yulia Tsvetkov and Shuly Wintner. 2011. Identification
of multi-word expressions by combining multiple lin-
guistic information sources. In Proc. of EMNLP, pages
836?845. Edinburgh, Scotland, UK.
Yuancheng Tu and Dan Roth. 2011. Learning English
light verb constructions: contextual or statistical. In
Proc. of the Workshop on Multiword Expressions: from
Parsing and Generation to the Real World, pages 31?39.
Portland, Oregon, USA.
Yuancheng Tu and Dan Roth. 2012. Sorting out the most
confusing English phrasal verbs. In Proc. of *SEM,
pages 65?69. Montr?al, Quebec, Canada.
Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio.
2010. Word representations: a simple and general
method for semi-supervised learning. In Proc. of ACL,
pages 384?394. Uppsala, Sweden.
Martin C?mejrek, Jan Cur??n, Jan Hajic?, and Jir?? Havelka.
2005. Prague Czech-English Dependency Treebank:
resource for structure-based MT. In Proc. of EAMT,
pages 73?78. Budapest, Hungary.
Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A model-theoretic
coreference scoring scheme. In Proc. of MUC-6, pages
45?52. Columbia, Maryland, USA.
Veronika Vincze. 2012. Light verb constructions in the
SzegedParalellFX English-Hungarian parallel corpus.
In Proc. of LREC. Istanbul, Turkey.
Veronika Vincze, Istv?n Nagy T., and J?nos Zsibrita. 2013.
Learning to detect English and Hungarian light verb
constructions. ACM Transactions on Speech and Lan-
guage Processing, 10(2):6:1?6:25.
206
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 264?267,
Uppsala, Sweden, 15-16 July 2010.
c
?2010 Association for Computational Linguistics
SEMAFOR: Frame Argument Resolution with Log-Linear Models
Desai Chen Nathan Schneider Dipanjan Das Noah A. Smith
School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, USA
{desaic@andrew,dipanjan@cs,nschneid@cs,nasmith@cs}.cmu.edu
Abstract
This paper describes the SEMAFOR sys-
tem?s performance in the SemEval 2010
task on linking events and their partici-
pants in discourse. Our entry is based
upon SEMAFOR 1.0 (Das et al, 2010a),
a frame-semantic probabilistic parser built
from log-linear models. The extended sys-
tem models null instantiations, including
non-local argument reference. Performance
is evaluated on the task data with and with-
out gold-standard overt arguments. In both
settings, it fares the best of the submitted
systems with respect to recall and F
1
.
1 Introduction
The theory of frame semantics (Fillmore, 1982)
holds that meaning is largely structured by holis-
tic units of knowledge, called frames. Each frame
encodes a conventionalized gestalt event or sce-
nario, often with conceptual dependents (partic-
ipants, props, or attributes) filling roles to elab-
orate the specific instance of the frame. In the
FrameNet lexicon (Fillmore et al, 2003), each
frame defines core roles tightly coupled with
the particular meaning of the frame, as well as
more generic non-core roles (Ruppenhofer et al,
2006). Frames can be evoked with linguistic pred-
icates, known as lexical units (LUs); role fillers
can be expressed overtly and linked to the frame
via (morpho)syntactic constructions. However, a
great deal of conceptually-relevant content is left
unexpressed or is not explicitly linked to the frame
via linguistic conventions; rather, it is expected
that the listener will be able to infer the appro-
priate relationships pragmatically. Certain types
of implicit content and implicit reference are for-
malized in the theory of null instantiations (NIs)
(Fillmore, 1986; Ruppenhofer, 2005). A complete
frame-semantic analysis of text thus incorporates
covert and overt predicate-argument information.
In this paper, we describe a system for frame-
semantic analysis, evaluated on a semantic role
labeling task for explicit and implicit arguments
(?2). Extending the SEMAFOR 1.0 frame-
semantic parser (Das et al, 2010a; outlined in ?3),
we detect null instantiations via a simple two-stage
pipeline: the first stage predicts whether a given
role is null-instantiated, and the second stage (?4)
predicts how it is null-instantiated, if it is not overt.
We report performance on the SemEval 2010 test
set under the full-SRL and NI-only conditions.
2 Data
The SemEval 2007 task on frame-semantic pars-
ing (Baker et al, 2007) provided a small (about
50,000 words and 2,000 sentences) dataset of
news text, travel guides, and bureaucratic accounts
of weapons stockpiles. Sentences in this dataset
were fully annotated with frames and their argu-
ments. The SemEval 2010 task (Ruppenhofer et
al., 2010) adds annotated data in the fiction do-
main: parts of two Sherlock Holmes stories by
Arthur Conan Doyle. The SemEval 2010 train-
ing set consists of the SemEval 2007 data plus
one document from the new domain. This doc-
ument has about 7800 words in 438 sentences;
it has 1492 annotated frame instances, including
3169 (overt and null-instantiated) argument anno-
tations. The test set consists of two chapters from
another story: Chapter 13 contains about 4000
words, 249 sentences, and 791 frames; Chapter 14
contains about 5000 words, 276 sentences, and
941 frames (see also Table 3). Figure 1 shows
two annotated test sentences. All data released for
the 2010 task include part-of-speech tags, lemmas,
and phrase-structure trees from a parser, with head
annotations for constituents.
3 Argument identification
Our starting point is SEMAFOR 1.0 (Das et
al., 2010a), a discriminative probabilistic frame-
semantic parsing model that operates in three
stages: (a) rule-based target selection, (b) proba-
bilistic disambiguation that resolves each target to
a FrameNet frame, and (c) joint selection of text
spans to fill the roles of each target through a sec-
ond probabilistic model.
1
1
Das et al (2010a) report the performance of this system
on the complete SemEval 2007 task at 46.49% F
1
.
264
`` I         THINK   that I shall be in a position to     MAKE   the situation rather   more   CLEAR to you before long .
It has been an exceedingly DIFFICULT and most complicated  business .
DIFFICULTY
difficult.a
Degree
Activity
OPINION
think.v
CAUSATION
make.v
OBVIOUSNESS
clear.n
Experiencer
Cognizer Opinion
Actor Effect
Experiencer
Degree AttributePhenomenon
Figure 1. Two consecutive sentences
in the test set, with frame-semantic an-
notations. Shaded regions represent
frames: they include the target word in
the sentence, the corresponding frame
name and lexical unit, and arguments.
Horizontal bars mark gold argument
spans?white bars are gold annotations
and black bars show mistakes of our
NI-only system.
Chapter 13 Chapter 14
Training Data Prec. Rec. F
1
Prec. Rec. F
1
SemEval 2010 data (includes SemEval 2007 data) 0.69 0.50 0.58 0.66 0.48 0.56
SemEval 2007 data + 50% new, in-domain data 0.68 0.47 0.55 0.66 0.45 0.54
SemEval 2007 data only 0.67 0.41 0.50 0.64 0.40 0.50
Table 1. Overt
argument labeling
performance.
Stage (c), known as argument identification or
SRL, is most relevant here. In this step, the system
takes the target (frame-evoking) phrase t and cor-
responding frame type f predicted by the previous
stages, and independently fills each role of f with
a word or phrase from the sentence, or the sym-
bol OTHER to indicate that the role has no (local)
overt argument. Features used to inform this de-
cision include aspects of the syntactic dependency
parse (e.g. the path in the parse from the target
to the argument); voice; word overlap of the argu-
ment with respect to the target; and part-of-speech
tags within and around the argument. SEMAFOR
as described in (Das et al, 2010a) does not dis-
tinguish between different types of null instantia-
tions or find non-local referents. Given perfect
input to stage (c), the system achieved 68.5% F
1
on the SemEval 2007 data (exact match, evaluat-
ing overt arguments only). The only difference
in our use of SEMAFOR?s argument identification
module is in preprocessing the training data: we
use dependency parses transformed from the head-
augmented phrase-structure parses in the task data.
Table 1 shows the performance of our argument
identification model on this task?s test data. The
SRL systems compared in (Ruppenhofer et al,
2010) all achieved precision in the mid 60% range,
but SEMAFOR achieved substantially higher re-
call, F
1
, and label accuracy on this subtask. (The
table also shows how performance of our model
degrades when half or all of the new data are not
used for training; the 9% difference in recall sug-
gests the importance of in-domain training data.)
4 Null instantiation detection
In this subtask, which follows the argument iden-
tification subtask (?3), our system seeks to char-
acterize non-overt core roles given gold standard
local frame-argument annotations. Consider the
following passage from the test data:
?That?s lucky for him?in fact, it?s lucky for all
of you, since you are all on the wrong side of the
law in this matter. I am not sure that as a consci-
entious detective [
Authorities
my] first duty is not to
arrest [
Suspect
the whole household]. [
DNI
Charges
?]
The frame we are interested in, ARREST, has four
core roles, two of which (Authorities and Sus-
pect) have overt (local) arguments. The third core
role, Charges, is annotated as having anaphoric
or definite null instantiation (DNI). ?Definite?
means that the discourse implies a specific referent
that should be recoverable from context, without
marking that referent linguistically. Some DNIs in
the data are linked to phrases in syntactically non-
local positions, such as in another sentence (see
Figure 1). This one is not (though our model in-
correctly labels this matter from the previous sen-
tence as a DNI referent for this role). The fourth
core role, Offense, is not annotated as a null in-
stantiation because it belongs to the same CoreSet
as Charges?which is to say they are relevant in
a similar way to the frame as a whole (both pertain
to the rationale for the arrest) and only one is typ-
ically expressed.
2
We will use the term masked
to refer to any non-overt core role which does not
need to be specified as null-instantiated due to a
structural connection to another role in its frame.
The typology of NIs given in Ruppenhofer
(2005) and employed in the annotation distin-
guishes anaphoric/definite NIs from existential or
indefinite null instantiations (INIs). Rather than
having a specific referent accessible in the dis-
course, INIs are left vague or deemphasized, as in
2
If the FrameNet lexicon marks a pair of roles within a
frame as being in a CoreSet or Excludes relationship, then
filling one of them satisfies the requirement that the other be
(expressly or implicitly) present in the use of the frame.
265
Chapter 13 Chapter 14
Training Data Prec. Rec. F
1
Prec. Rec. F
1
N
I
-
o
n
l
y
SemEval 2010 new: 100% 0.40 0.64 0.50 0.53 0.60 0.56
SemEval 2010 new: 75% 0.66 0.37 0.50 0.70 0.37 0.48
SemEval 2010 new: 50% 0.73 0.38 0.51 0.75 0.35 0.48
Full All 0.35 0.55 0.43 0.56 0.49 0.52
Table 2. Performance on the
full task and the NI-only task.
The NI model was trained on the
new SemEval 2010 document, ?The
Tiger of San Pedro? (data from the
2007 task was excluded because
none of the null instantiations in that
data had annotated referents).
Predicted
overt DNI INI masked inc. total
G
o
l
d
overt 2068 (1630) 5 362 327 0 2762
DNI 64 12 (3) 182 90 0 348
INI 41 2 214 96 0 353
masked 73 0 240 1394 0 1707
inc. 12 2 55 2 0 71
total 2258 21 1053 1909 0 3688 correct
Table 3. Instantiation type confusion ma-
trix for the full model (argument identifi-
cation plus NI detection). Parenthesized
numbers count the predictions of the cor-
rect type which also predicted the same
(argument or referent) span. On the NI-
only task, our system has a similar distri-
bution of NI detection errors.
the thing(s) eaten in the sentence We ate.
The problem can be decomposed into two steps:
(a) classifying each null instantiation as definite,
indefinite, or masked; and (b) resolving the DNIs,
which entails finding referents in the non-local
context. Instead, our model makes a single NI pre-
diction for any role that received no local argument
(OTHER) in the argument identification phase (?3),
thereby combining classification and resolution.
3
4.1 Model
Our model for this subtask is analogous to the ar-
gument identification model: it chooses one from
among many possible fillers for each role. How-
ever, whereas the argument identification model
considers parse constituents as potential local
fillers (which might constitute an overt argument
within the sentence) along with a special category,
OTHER, here the set of candidate fillers consists of
phrases from outside the sentence, along with spe-
cial categories INI or MASKED. When selected, a
non-local phrase will be interpreted as a non-local
argument and labeled as a DNI referent.
These non-local candidate fillers are handled
differently from candidates within the sentence
considered in the argument identification model:
they are selected using more restrictive criteria,
and are associated with a different set of features.
Restricted search space for DNI referents. We
consider nouns, pronouns, and noun phrases from
the previous three sentences as candidate DNI ref-
erents. This narrows the search space considerably
to make learning tractable, but at a cost: many
gold DNI referents will not even be considered.
In the training data, there are about 250 DNI in-
stances with explicit referents; their distribution is
3
Investigation of separate modeling is left to future work.
chaotic.
4
Judging by the training data, our heuris-
tics thus limit oracle recall to about 20% of DNIs.
5
Modified feature set. Since it is not obvious how
to calculate a syntactic path between two words
in different sentences, we replaced dependency
path features with simpler features derived from
FrameNet?s lexicographic exemplar annotations.
For each candidate span, we use two types of fea-
tures to model the affinity between the head word
and the role. The first indicates whether the head
word is used as a filler for this role in at least
one of the lexicographic exemplars. The second
encodes the maximum distributional similarity to
any word heading a filler of that role in the ex-
emplars.
6
In practice, we found that these fea-
tures received negligible weight and had virtually
no effect on performance, possibly due to data
sparseness. An additional change in the feature
set is that ordering/distance features (Das et al,
2010b, p. 13) were replaced with a feature indicat-
ing the number of sentences away the candidate
is from the target.
7
Otherwise, the null identifica-
4
91 DNI referents are found no more than three sentences
prior; another 90 are in the same sentence as the target. 20
DNIs have referents which are not noun phrases. Six appear
after the sentence containing its frame target; 28 appear at
least 25 sentences prior. 60 have no referent.
5
Our system ignores DNIs with no referent or with a ref-
erent in the same sentence as the target. Experiments with
variants on these assumptions show that the larger the search
space (i.e. the more candidate DNI referents are under con-
sideration), the worse the trained model performs at distin-
guishing NIs from non-NIs (though DNI vs. INI precision
improves). This suggests that data sparseness is hindering
our system?s ability to learn useful generalizations about NIs.
6
Distributional similarity scores are obtained
from D. Lin?s Proximity-based Thesaurus (http:
//webdocs.cs.ualberta.ca/~lindek/
Downloads/sims.lsp.gz) and quantized into bi-
nary features for intervals: [0, .03), [.03, .06), [.06, .08),
[.08,?).
7
All of the new features are instantiated in three forms:
266
tion model uses the same features as the argument
identification model.
The theory of null instantiations holds that the
grammaticality of lexically-licensed NI for a role
in a given frame depends on the LU: for exam-
ple, the verbs buy and sell share the same frame
but differ as to whether the Buyer or Seller role
may be lexically null-instantiated. Our model?s
feature set is rich enough to capture this in a soft
way, with lexicalized features that fire, e.g., when
the Seller role is null-instantiated and the target
is buy. Moreover, (Ruppenhofer, 2005) hypoth-
esizes that each role has a strong preference for
one interpretation (INI or DNI) when it is lexically
null-instantiated, regardless of LU. This, too, is
modeled in our feature set. In theory these trends
should be learnable given sufficient data, though it
is doubtful that there are enough examples of null
instantiations in the currently available dataset for
this learning to take place.
4.2 Evaluation
We trained the model on the non-overt arguments
in the new SemEval 2010 training document,
which has 580 null instantiations?303 DNIs and
277 INIs.
8,9
Then we used the task scoring proce-
dure to evaluate the NI detection subtask in isola-
tion (given gold-standard overt arguments) as well
as the full task (when this module is combined in a
pipeline with argument identification). Results are
shown in Table 2.
10
Table 3 provides a breakdown of our sys-
tem?s predictions on the test data by instantiation
type: overt local arguments, DNIs, INIs, and the
MASKED category (marking the role as redundant
or irrelevant for the particular use of the frame,
given the other arguments). It also shows counts
for incorporated (?inc.?) roles, which are filled by
the frame-evoking target, e.g. clear in Figure 1.
11
This table shows that the system is reasonably ef-
fective at discriminating NIs from masked roles,
one specific to the frame and the role, one specific to the role
name only, and one to learn the overall bias of the data.
8
For feature engineering we held out the last 25% of sen-
tences from the new training document as development data,
retraining on the full training set for final evaluation.
9
We used Nils Reiter?s FrameNet API, version 0.4
(http://www.cl.uni-heidelberg.de/trac/
FrameNetAPI) in processing the data.
10
The other system participating in the NI-only subtask
had much lower NI recall of 8% (Ruppenhofer et al, 2010).
11
We do not predict any DNIs without referents or in-
corporated roles, though the evaluation script gives us credit
when we predict INI for these cases.
but DNI identification suffers from low recall and
INI identification from low precision. Data sparse-
ness is likely the biggest obstacle here. To put this
in perspective, there are over 20,000 training ex-
amples of overt arguments, but fewer than 600 ex-
amples of null instantiations, two thirds of which
do not have referents. Without an order of mag-
nitude more NI data (at least), it is unlikely that
a supervised learner could generalize well enough
to recognize on new data null instantiations of the
over 7000 roles in the lexicon.
5 Conclusion
We have described a system that implements a
clean probabilistic model of frame-semantic struc-
ture, considering overt arguments as well as var-
ious forms of null instantion of roles. The sys-
tem was evaluated on SemEval 2010 data, with
mixed success at detecting null instantiations. We
believe in-domain data sparseness is the predom-
inant factor limiting the robustness of our super-
vised model.
Acknowledgments
This work was supported by DARPA grant
NBCH-1080004 and computational resources
provided by Yahoo. We thank the task organizers for
providing data and conducting the evaluation, and two
reviewers for their comments.
References
C. Baker, M. Ellsworth, and K. Erk. 2007. SemEval-2007
Task 19: Frame Semantic Structure Extraction. In Proc.
of SemEval.
D. Das, N. Schneider, D. Chen, and N. A. Smith. 2010a.
Probabilistic frame-semantic parsing. In Proc. of NAACL-
HLT.
D. Das, N. Schneider, D. Chen, and N. A. Smith. 2010b.
SEMAFOR 1.0: A probabilistic frame-semantic parser.
Technical Report CMU-LTI-10-001, Carnegie Mellon
University.
C. J. Fillmore, C. R. Johnson, and M. R.L. Petruck. 2003.
Background to FrameNet. International Journal of Lexi-
cography, 16(3).
C. J. Fillmore. 1982. Frame semantics. In Linguistics in the
Morning Calm, pages 111?137. Hanshin Publishing Co.,
Seoul, South Korea.
C. J. Fillmore. 1986. Pragmatically controlled zero
anaphora. In Proc. of Berkeley Linguistics Society, pages
95?107, Berkeley, CA.
J. Ruppenhofer, M. Ellsworth, M. R.L. Petruck, C. R. John-
son, and J. Scheffczyk. 2006. FrameNet II: extended the-
ory and practice.
J. Ruppenhofer, C. Sporleder, R. Morante, C. Baker, and
M. Palmer. 2010. SemEval-2010 Task 10: Linking
Events and Their Participants in Discourse. In Proc. of
SemEval.
J. Ruppenhofer. 2005. Regularities in null instantiation.
267
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 176?180,
Dublin, Ireland, August 23-24, 2014.
CMU: Arc-Factored, Discriminative Semantic Dependency Parsing
Sam Thomson Brendan O?Connor Jeffrey Flanigan David Bamman
Jesse Dodge Swabha Swayamdipta Nathan Schneider Chris Dyer Noah A. Smith
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{sthomson,brenocon,jflanigan,dbamman,jessed,
swabha,nschneid,cdyer,nasmith}@cs.cmu.edu
Abstract
We present an arc-factored statistical model
for semantic dependency parsing, as de-
fined by the SemEval 2014 Shared Task 8
on Broad-Coverage Semantic Dependency
Parsing. Our entry in the open track placed
second in the competition.
1 Introduction
The task of broad coverage semantic dependency
parsing aims to provide a shallow semantic analysis
of text not limited to a specific domain. As distinct
from deeper semantic analysis (e.g., parsing to a
full lambda-calculus logical form), shallow seman-
tic parsing captures relationships between pairs
of words or concepts in a sentence, and has wide
application for information extraction, knowledge
base population, and question answering (among
others).
We present here two systems that produce seman-
tic dependency parses in the three formalisms of the
SemEval 2014 Shared Task 8 on Broad-Coverage
Semantic Dependency Parsing (Oepen et al., 2014).
These systems generate parses by extracting fea-
tures for each potential dependency arc and learn-
ing a statistical model to discriminate between good
arcs and bad; the first treats each labeled edge de-
cision as an independent multiclass logistic regres-
sion (?3.2.1), while the second predicts arcs as part
of a graph-based structured support vector machine
(?3.2.2). Common to both models is a rich set of
features on arcs, described in ?3.2.3. We include a
discussion of features found to have no discernable
effect, or negative effect, during development (?4).
Our system placed second in the open track of
the Broad-Coverage Semantic Dependency Parsing
This work is licensed under a Creative Commons Attribution
4.0 International License. Page numbers and proceedings
footer are added by the organizers. License details: http:
//creativecommons.org/licenses/by/4.0/
Figure 1: Example annotations for DM (top), PAS (middle),
and PCEDT (bottom).
task (in which output from syntactic parsers and
other outside resources can be used). We present
our results in ?5.
2 Formalisms
The Shared Task 8 dataset consists of annota-
tions of the WSJ Corpus in three different se-
mantic dependency formalisms. DM is derived
from LinGO English Resource Grammar (ERG)
annotations in DeepBank (Flickinger et al., 2012).
PAS is derived from the Enju HPSG treebank us-
ing the conversion rules of Miyao et al. (2004).
PCEDT is derived from the tectogrammatical layer
of the Prague Czech-English Dependency Treebank
(Haji?c, 1998). See Figure 1 for an example.
The three formalisms come from very different
linguistic theories, but all are represented as labeled
directed graphs, with words as vertices, and all
have ?top? annotations, corresponding roughly to
the semantic focus of the sentence. (A ?top? need
not be a root of the graph.) This allows us to use
the same machinery (?3) for training and testing
statistical models for the three formalisms.
3 Models
We treat the problem as a three-stage pipeline. The
first stage prunes words by predicting whether they
have any incoming or outgoing edges at all (?3.1);
if a word does not, then it is not considered for
any attachments in later stages. The second stage
176
predicts where edges are present, and their labels
(?3.2). The third stage predicts whether a predicate
word is a top or not (?3.3). Formalisms sometimes
annotate more than one ?top? per sentence, but we
found that we achieve the best performance on all
formalisms by predicting only the one best-scoring
?top? under the model.
3.1 Singleton Classification
For each formalism, we train a classifier to rec-
ognize singletons, nodes that have no parents or
children. (For example, punctuation tokens are of-
ten singletons.) This makes the system faster with-
out affecting accuracy. For singleton prediction,
we use a token-level logistic regression classifier,
with features including the word, its lemma, and
its part-of-speech tag. If the classifier predicts a
probability of 99% or higher the token is pruned;
this removes around 10% of tokens. (The classi-
fier performs differently on different formalisms;
on PAS it has perfect accuracy, while on DM and
PCEDT accuracy is in the mid-90?s.)
3.2 Edge Prediction
In the second stage of the pipeline, we predict the
set of labeled directed edges in the graph. We use
the same set of edge-factored features (?3.2.3) in
two alternative models: an edge-independent mul-
ticlass logistic regression model (LOGISTICEDGE,
?3.2.1); and a structured SVM (Taskar et al., 2003;
Tsochantaridis et al., 2004) that enforces a deter-
minism constraint for certain labels, which allows
each word to have at most one outgoing edge with
that label (SVMEDGE, ?3.2.2). For each formalism,
we trained both models with varying features en-
abled and hyperparameter settings and submitted
the configuration that produced the best labeled F
1
on the development set. For DM and PCEDT, this
was LOGISTICEDGE; for PAS, this was SVMEDGE.
We report results only for the submitted configu-
rations, with different features enabled. Due to
time constraints, full hyperparameter sweeps and
comparable feature sweeps were not possible.
3.2.1 LOGISTICEDGE Parser
The LOGISTICEDGE model considers only token
index pairs (i, j) where |i ? j| ? 10, i 6= j,
and both t
i
and t
j
have been predicted to be non-
singletons by the first stage. Although this prunes
some gold edges, among the formalisms, 95%?97%
of all gold edges are between tokens of distance
10 or less. Both directions i ? j and j ? i are
considered between every pair.
Let L be the set of K + 1 possible output labels:
the formalism?s original K edge labels, plus the
additional label NOEDGE, which indicates that no
edge exists from i to j. The model treats every pair
of token indices (i, j) as an independent multiclass
logistic regression over output space L. Let x be
an input sentence. For candidate parent index i,
child index j, and edge label `, we extract a feature
vector f(x, i, j, `), where ` is conjoined with every
feature described in ?3.2.3. The multiclass logis-
tic regression model defines a distribution over L,
parametrized by weights ?:
P (` | ?, x, i, j) =
exp{? ? f(x, i, j, `)}
?
`
?
?L
exp{? ? f(x, i, j, `
?
)}
.
? is learned by minimizing total negative log-
likelihood of the above (with weighting; see be-
low), plus `
2
regularization. AdaGrad (Duchi et al.,
2011) is used for optimization. This seemed to opti-
mize faster than L-BFGS (Liu and Nocedal, 1989),
at least for earlier iterations, though we did no sys-
tematic comparison. Stochastic gradient steps are
applied one at a time from individual examples,
and a gradient step for the regularizer is applied
once per epoch.
The output labels have a class imbalance; in all
three formalisms, there are many more NOEDGE
examples than true edge examples. We improved
F
1
performance by downweighting NOEDGE
examples through a weighted log-likelihood
objective,
?
i,j
?
`
w
`
logP (` |?, x, i, j), with
w
NOEDGE
= 0.3 (selected on development set) and
w
`
= 1 otherwise.
Decoding: To predict a graph structure at test-time
for a new sentence, the most likely edge label is pre-
dicted for every candidate (i, j) pair of unpruned
tokens. If an edge is predicted for both directions
for a single (i, j) pair, only the edge with the higher
score is chosen. (There are no such bidirectional
edges in the training data.) This post-processing ac-
tually did not improve accuracy on DM or PCEDT;
it did improve PAS by ?0.2% absolute F
1
, but we
did not submit LOGISTICEDGE for PAS.
3.2.2 SVMEDGE Parser
In the SVMEDGE model, we use a structured SVM
with a determinism constraint. This constraint en-
sures that each word token has at most one outgoing
edge for each label in a set of deterministic labels
L
d
. For example, in DM a predicate never has more
177
than one child with edge label ?ARG1.? L
d
was
chosen to be the set of edges that were > 99.9%
deterministic in the training data.
1
Consider the fully dense graph of all edges be-
tween all words predicted as not singletons by the
singleton classifier ?3.1 (in all directions with all
possible labels). Unlike LOGISTICEDGE, the la-
bel set L does not include an explicit NOEDGE
label. If ? denotes the model weights, and f de-
notes the features, then an edge from i to j with
label ` in the dense graph has a weight c(i, j, `)
assigned to it using the linear scoring function
c(i, j, `) = ? ? f(x, i, j, `).
Decoding: For each node and each label `, if ` ?
L
d
, the decoder adds the highest scoring outgoing
edge, if its weight is positive. For ` 6? L
d
, every
outgoing edge with positive weight is added. This
procedure is guaranteed to find the highest scoring
subgraph (largest sum of edge weights) of the dense
graph subject to the determinism constraints. Its
runtime is O(n
2
).
The model weights are trained using the struc-
tured SVM loss. If x is a sentence and y is a
graph over that sentence, let the features be de-
noted f(x, y) =
?
(i,j,`)?y
f(x, i, j, `). The SVM
loss for each training example (x
i
, y
i
) is:
??
>
f(x
i
, y
i
)+max
y
?
>
f(x
i
, y)+cost(y, y
i
)
where cost(y, y
i
) = ?|y \ y
i
| + ?|y
i
\ y|. ? and
? trade off between precision and recall for the
edges (Gimpel and Smith, 2010). The loss is min-
imized with AdaGrad using early-stopping on a
development set.
3.2.3 Edge Features
Table 1 describes the features we used for predict-
ing edges. These features were computed over an
edge e with parent token s at index i and child
token t at index j. Unless otherwise stated, each
feature template listed has an indicator feature that
fires for each value it can take on. For the sub-
mitted results, LOGISTICEDGE uses all features
except Dependency Path v2, POS Path, and Dis-
tance Thresholds, and SVMEDGE uses all features
except Dependency Path v1. This was due to
SVMEDGE being faster to train than LOGISTIC-
EDGE when including POS Path features, and due
1
By this we mean that of the nodes that have at least
one outgoing ` edge, 99.9% of them have only one outgo-
ing ` edge. For DM, L
d
= L\{? and c,? ? or c,? ? then c,?
?loc,? ?mwe,? ?subord?}; for PAS, L
d
= L; and for PCEDT,
L
d
={?DPHR,? ?INTF,? ?VOCAT?}.
Tokens: The tokens s and t themselves.
Lemmas: Lemmas of s and t.
POS tags: Part of speech tags of s and t.
Linear Order: Fires if i < j.
Linear Distance: i? j.
Dependency Path v1 (LOGISTICEDGE only): The
concatenation of all POS tags, arc labels and up/down
directions on the path in the syntactic dependency tree
from s to t. Conjoined with s, with t, and without either.
Dependency Path v2 (SVMEDGE only): Same as De-
pendency Path v1, but with the lemma of s or t instead
of the word, and substituting the token for any ?IN? POS
tag.
Up/Down Dependency Path: The sequence of upward
and downward moves needed to get from s to t in the
syntactic dependency tree.
Up/Down/Left/Right Dependency Path: The unla-
beled path through the syntactic dependency tree from s
to t, annotated with whether each step through the tree
was up or down, and whether it was to the right or left in
the sentence.
Is Parent: Fires if s is the parent of t in the syntactic
dependency parse.
Dependency Path Length: Distance between s and t in
the syntactic dependency parse.
POS Context: Concatenated POS tags of tokens at i?1,
i, i+ 1, j ? 1, j, and j + 1. Concatenated POS tags of
tokens at i? 1, i, j ? 1, and j. Concatenated POS tags
of tokens at i, i+ 1, j, and j + 1.
Subcategorization Sequence: The sequence of depen-
dency arc labels out of s, ordered by the index of the
child. Distinguish left children from right children. If t
is a direct child of s, distinguish its arc label with a ?+?.
Conjoin this sequence with the POS tag of s.
Subcategorization Sequence with POS: As above, but
add the POS tag of each child to its arc label.
POS Path (SVMEDGE only): Concatenated POS tags
between and including i and j. Conjoined with head
lemma, with dependent lemma, and without either.
Distance Thresholds (SVMEDGE only): Fires for ev-
ery integer between 1 and blog(|i? j|+1)/ log(1.39)c
inclusive.
Table 1: Features used in edge prediction
to time constraints for the submission we were un-
able to retrain LOGISTICEDGE with these features.
3.2.4 Feature Hashing
The biggest memory usage was in the map from
feature names to integer indices during feature
extraction. For experimental expedience, we im-
plemented multitask feature hashing (Weinberger
et al., 2009), which hashes feature names to indices,
under the theory that errors due to collisions tend
to cancel. No drop in accuracy was observed.
3.3 Top Prediction
We trained a separate token-level binary logistic
regression model to classify whether a token?s node
had the ?top? attribute or not. At decoding time, all
predicted predicates (i.e., nodes where there is at
178
least one outbound edge) are possible candidates
to be ?top?; the classifier probabilities are evalu-
ated, and the highest-scoring node is chosen to be
?top.? This is suboptimal, since some graphs have
multiple tops (in PCEDT this is more common);
but selection rules based on probability thresholds
gave worse F
1
performance on the dev set. For a
given token t at index i, the top classifier?s features
included t?s POS tag, i, those two conjoined, and
the depth of t in the syntactic dependency tree.
4 Negative Results
We followed a forward-selection process during
feature engineering. For each potential feature,
we tested the current feature set versus the current
feature set plus the new potential feature. If the
new feature did not improve performance, we did
not add it. We list in table 2 some of the features
which we tested but did not improve performance.
In order to save time, we ran these feature se-
lection experiments on a subsample of the training
data, for a reduced number of iterations. These re-
sults thus have a strong caveat that the experiments
were not exhaustive. It may be that some of these
features could help under more careful study.
5 Experimental Setup
We participated in the Open Track, and used the
syntactic dependency parses supplied by the orga-
nizers. Feature engineering was performed on a
development set (?20), training on ??00?19. We
evaluate labeled precision (LP), labeled recall (LR),
labeled F
1
(LF), and labeled whole-sentence match
(LM) on the held-out test data using the evaluation
script provided by the organizers. LF was aver-
aged over the formalisms to determine the winning
system. Table 3 shows our scores.
6 Conclusion and Future Work
We found that feature-rich discriminative models
perform well at the task of mapping from sentences
to semantic dependency parses. While our final
approach is fairly standard for work in parsing,
we note here additional features and constraints
which did not appear to help (contrary to expecta-
tion). There are a number of clear extensions to
this work that could improve performance. While
an edge-factored model allows for efficient infer-
ence, there is much to be gained from higher-order
features (McDonald and Pereira, 2006; Martins
et al., 2013). The amount of information shared
Word vectors: Features derived from 64-dimensional
vectors from (Faruqui and Dyer, 2014), including the
concatenation, difference, inner product, and element-
wise multiplication of the two vectors associated with
a parent-child edge. We also trained a Random Forest
on the word vectors using Liaw and Wiener?s (2002) R
implementation. The predicted labels were then used as
features in LOGISTICEDGE.
Brown clusters Features derived from Brown clusters
(Brown et al., 1992) trained on a large corpus of web data.
Parent, child, and conjoined parent-child edge features
from cluster prefixes of length 2, 4, 6, 8, 10, and 12.
Conjunctions of those features with the POS tags of the
parent and child tokens.
Active/passive: Active/passive voice feature (as in Jo-
hansson and Nugues (2008)) conjoined with both the
Linear Distance features and the Subcategorization Se-
quence features. Voice information may already be cap-
tured by features from the Stanford dependency?style
parses, which include passivization information in arc
labels such as nsubjpass and auxpass (de Marneffe and
Manning, 2008).
Connectivity constraint: Enforcing that the graph is
connected (ignoring singletons), similar to Flanigan et al.
(2014). Almost all semantic dependency graphs in the
training data are connected (ignoring singletons), but
we found that enforcing this constraint significantly hurt
precision.
Tree constraint: Enforces that the graph is a tree. Un-
surprisingly, we found that enforcing a tree constraint
hurt performance.
Table 2: Features and constraints giving negative results.
LP LR LF LM
DM 0.8446 0.8348 0.8397 0.0875
PAS 0.9078 0.8851 0.8963 0.2604
PCEDT 0.7681 0.7072 0.7364 0.0712
Average 0.8402 0.8090 0.8241 0.1397
Table 3: Labeled precision (LP), recall (LR), F
1
(LF), and
whole-sentence match (LM) on the held-out test data.
between the three formalisms suggests that a multi-
task learning (Evgeniou and Pontil, 2004) frame-
work could lead to gains. And finally, there is
additional structure in the formalisms which could
be exploited (such as the deterministic processes
by which an original PCEDT tree annotation was
converted into a graph); formulating more subtle
graph constraints to capture this a priori knowl-
edge could lead to improved performance. We
leave such explorations to future work.
Acknowledgements
We are grateful to Manaal Faruqui for his help in word vector
experiments, and to reviewers for helpful comments. The re-
search reported in this paper was sponsored by the U.S. Army
Research Laboratory and the U. S. Army Research Office
under contract/grant number W911NF-10-1-0533, DARPA
grant FA8750-12-2-0342 funded under the DEFT program,
U.S. NSF grants IIS-1251131 and IIS-1054319, and Google?s
support of the Reading is Believing project at CMU.
179
References
Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vin-
cent J. Della Pietra, and Jenifer C. Lai. 1992. Class-based
n-gram models of natural language. Computational Lin-
guistics, 18(4):467?479.
Marie-Catherine de Marneffe and Christopher D. Manning.
2008. The Stanford typed dependencies representation. In
Coling 2008: Proc. of the Workshop on Cross-Framework
and Cross-Domain Parser Evaluation, pages 1?8. Manch-
ester, UK.
John Duchi, Elad Hazan, and Yoram Singer. 2011. Adap-
tive subgradient methods for online learning and stochas-
tic optimization. Journal of Machine Learning Research,
12:2121?2159.
Theodoros Evgeniou and Massimiliano Pontil. 2004. Regular-
ized multitask learning. In Proc. of KDD, pages 109?117.
Seattle, WA, USA.
Manaal Faruqui and Chris Dyer. 2014. Improving vector
space word representations using multilingual correlation.
In Proc. of EACL, pages 462?471. Gothenburg, Sweden.
Jeffrey Flanigan, Sam Thomson, Jaime Carbonell, Chris Dyer,
and Noah A. Smith. 2014. A discriminative graph-based
parser for the Abstract Meaning Representation. In Proc.
of ACL, pages 1426?1436. Baltimore, MD, USA.
Dan Flickinger, Yi Zhang, and Valia Kordoni. 2012. Deep-
Bank: a dynamically annotated treebank of the Wall Street
Journal. In Proc. of the Eleventh International Workshop on
Treebanks and Linguistic Theories, pages 85?96. Lisbon,
Portugal.
Kevin Gimpel and Noah A. Smith. 2010. Softmax-margin
training for structured log-linear models. Technical
Report CMU-LTI-10-008, Carnegie Mellon Univer-
sity. URL http://lti.cs.cmu.edu/sites/
default/files/research/reports/2010/
cmulti10008.pdf.
Jan Haji?c. 1998. Building a syntactically annotated corpus:
the Prague Dependency Treebank. In Eva Haji?cov?a, ed-
itor, Issues of Valency and Meaning. Studies in Honour
of Jarmila Panevov?a, pages 106?132. Prague Karolinum,
Charles University Press, Prague.
Richard Johansson and Pierre Nugues. 2008. Dependency-
based semantic role labeling of PropBank. In Proc. of
EMNLP, pages 69?78. Honolulu, HI, USA.
Andy Liaw and Matthew Wiener. 2002. Classification
and regression by randomForest. R News, 2(3):18?
22. URL http://cran.r-project.org/web/
packages/randomForest/.
Dong C. Liu and Jorge Nocedal. 1989. On the limited memory
BFGS method for large scale optimization. Mathematical
Programming, 45(3):503?528.
Andr?e F. T. Martins, Miguel Almeida, and Noah A. Smith.
2013. Turning on the turbo: Fast third-order non-projective
turbo parsers. In Proc. of ACL, pages 617?622. Sofia,
Bulgaria.
Ryan McDonald and Fernando Pereira. 2006. Online learning
of approximate dependency parsing algorithms. In Proc. of
EACL, pages 81?88. Trento, Italy.
Yusuke Miyao, Takashi Ninomiya, and Jun?ichi Tsujii. 2004.
Corpus-oriented grammar development for acquiring a
head-driven phrase structure grammar from the Penn Tree-
bank. In Proc. of IJCNLP, pages 684?693. Hainan Island,
China.
Stephan Oepen, Marco Kuhlmann, Yusuke Miyao, Daniel
Zeman, Dan Flickinger, Jan Haji?c, Angelina Ivanova, and
Yi Zhang. 2014. SemEval 2014 Task 8: Broad-coverage
semantic dependency parsing. In Proc. of SemEval. Dublin,
Ireland.
Ben Taskar, Carlos Guestrin, and Daphne Koller. 2003. Max-
margin Markov networks. In Proc. of NIPS, pages 25?32.
Vancouver, British Columbia, Canada.
Ioannis Tsochantaridis, Thomas Hofmann, Thorsten Joachims,
and Yasemin Altun. 2004. Support vector machine learning
for interdependent and structured output spaces. In Proc.
of ICML, pages 104?111. Banff, Alberta, Canada.
Kilian Weinberger, Anirban Dasgupta, John Langford, Alex
Smola, and Josh Attenberg. 2009. Feature hashing for
large scale multitask learning. In Proc. of ICML, pages
1113?1120. Montreal, Quebec, Canada.
180
Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 279?287,
Atlanta, Georgia, June 13 2013. c?2013 Association for Computational Linguistics
Identifying the L1 of non-native writers: the CMU-Haifa system
Yulia Tsvetkov? Naama Twitto? Nathan Schneider? Noam Ordan?
Manaal Faruqui? Victor Chahuneau? Shuly Wintner? Chris Dyer?
?Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA
cdyer@cs.cmu.edu
?Department of Computer Science
University of Haifa
Haifa, Israel
shuly@cs.haifa.ac.il
Abstract
We show that it is possible to learn to identify, with
high accuracy, the native language of English test
takers from the content of the essays they write.
Our method uses standard text classification tech-
niques based on multiclass logistic regression, com-
bining individually weak indicators to predict the
most probable native language from a set of 11 pos-
sibilities. We describe the various features used for
classification, as well as the settings of the classifier
that yielded the highest accuracy.
1 Introduction
The task we address in this work is identifying the
native language (L1) of non-native English (L2) au-
thors. More specifically, given a dataset of short
English essays (Blanchard et al, 2013), composed
as part of the Test of English as a Foreign Lan-
guage (TOEFL) by authors whose native language is
one out of 11 possible languages?Arabic, Chinese,
French, German, Hindi, Italian, Japanese, Korean,
Spanish, Telugu, or Turkish?our task is to identify
that language.
This task has a clear empirical motivation. Non-
native speakers make different errors when they
write English, depending on their native language
(Lado, 1957; Swan and Smith, 2001); understand-
ing the different types of errors is a prerequisite for
correcting them (Leacock et al, 2010), and systems
such as the one we describe here can shed interest-
ing light on such errors. Tutoring applications can
use our system to identify the native language of
students and offer better-targeted advice. Forensic
linguistic applications are sometimes required to de-
termine the L1 of authors (Estival et al, 2007b; Es-
tival et al, 2007a). Additionally, we believe that the
task is interesting in and of itself, providing a bet-
ter understanding of non-native language. We are
thus equally interested in defining meaningful fea-
tures whose contribution to the task can be linguis-
tically interpreted. Briefly, our features draw heav-
ily on prior work in general text classification and
authorship identification, those used in identifying
so-called translationese (Volansky et al, forthcom-
ing), and a class of features that involves determin-
ing what minimal changes would be necessary to
transform the essays into ?standard? English (as de-
termined by an n-gram language model).
We address the task as a multiway text-
classification task; we describe our data in ?3 and
classification model in ?4. As in other author attri-
bution tasks (Juola, 2006), the choice of features for
the classifier is crucial; we discuss the features we
define in ?5. We report our results in ?6 and con-
clude with suggestions for future research.
2 Related work
The task of L1 identification was introduced by Kop-
pel et al (2005a; 2005b), who work on the Inter-
national Corpus of Learner English (Granger et al,
2009), which includes texts written by students from
5 countries, Russia, the Czech Republic, Bulgaria,
France, and Spain. The texts range from 500 to
850 words in length. Their classification method
is a linear SVM, and features include 400 standard
function words, 200 letter n-grams, 185 error types
and 250 rare part-of-speech (POS) bigrams. Ten-
279
fold cross-validation results on this dataset are 80%
accuracy.
The same experimental setup is assumed by Tsur
and Rappoport (2007), who are mostly interested
in testing the hypothesis that an author?s choice of
words in a second language is influenced by the
phonology of his or her L1. They confirm this hy-
pothesis by carefully analyzing the features used by
Koppel et al, controlling for potential biases.
Wong and Dras (2009; 2011) are also motivated
by a linguistic hypothesis, namely that syntactic er-
rors in a text are influenced by the author?s L1.
Wong and Dras (2009) analyze three error types sta-
tistically, and then add them as features in the same
experimental setup as above (using LIBSVM with a
radial kernel for classification). The error types are
subject-verb disagreement, noun-number disagree-
ment and misuse of determiners. Addition of these
features does not improve on the results of Kop-
pel et al. Wong and Dras (2011) further extend
this work by adding as features horizontal slices of
parse trees, thereby capturing more syntactic struc-
ture. This improves the results significantly, yielding
78% accuracy compared with less than 65% using
only lexical features.
Kochmar (2011) uses a different corpus, the Cam-
bridge Learner Corpus, in which texts are 200-400
word long, and are authored by native speakers of
five Germanic languages (German, Swiss German,
Dutch, Swedish and Danish) and five Romance lan-
guages (French, Italian, Catalan, Spanish and Por-
tuguese). Again, SVMs are used as the classification
device. Features include POS n-grams, character n-
grams, phrase-structure rules (extracted from parse
trees), and two measures of error rate. The classi-
fier is evaluated on its ability to distinguish between
pairs of closely-related L1s, and the results are usu-
ally excellent.
A completely different approach is offered by
Brooke and Hirst (2011). Since training corpora for
this task are rare, they use mainly L1 (blog) cor-
pora. Given English word bigrams ?e1, e2?, they try
to assess, for each L1, how likely it is that an L1 bi-
gram was translated literally by the author, resulting
in ?e1, e2?. Working with four L1s (French, Span-
ish, Chinese, and Japanese), and evaluating on the
International Corpus of Learner English, accuracy is
below 50%.
3 Data
Our dataset in this work consists of TOEFL essays
written by speakers of eleven different L1s (Blan-
chard et al, 2013), distributed as part of the Na-
tive Language Identification Shared Task (Tetreault
et al, 2013). The training data consists of 1000
essays from each native language. The essays are
short, consisting of 10 to 20 sentences each. We
used the provided splits of 900 documents for train-
ing and 100 for development. Each document is an-
notated with the author?s English proficiency level
(low, medium, high) and an identification (1 to 8) of
the essay prompt. All essays are tokenized and split
into sentences. In table 1 we provide some statistics
on the training corpora, listed by the authors? profi-
ciency level. All essays were tagged with the Stan-
ford part-of-speech tagger (Toutanova et al, 2003).
We did not parse the dataset.
Low Medium High
# Documents 1,069 5,366 3,456
# Tokens 245,130 1,819,407 1,388,260
# Types 13,110 37,393 28,329
Table 1: Training set statistics.
4 Model
For our classification model we used the creg re-
gression modeling framework to train a 11-class lo-
gistic regression classifier.1 We parameterize the
classifier as a multiclass logistic regression:
p?(y | x) =
exp
?
j ? jh j(x, y)
Z?(x)
,
where x are documents, h j(?) are real-valued feature
functions of the document being classified, ? j are the
corresponding weights, and y is one of the eleven L1
class labels. To train the parameters of our model,
we minimized the following objective,
L = ?
`2 reg.
?????
j
?2j ?
?
{(xi,yi)}
|D|
i=1
(
log likelihood
?          ??          ?
log p?(yi | xi) +
?Ep?(y? |xi) log p?(y
? | xi)
?                      ??                      ?
?conditional entropy
)
,
1https://github.com/redpony/creg
280
which combines the negative log likelihood of the
training dataset D, an `2 (quadratic) penalty on the
magnitude of ? (weighted by ?), and the negative en-
tropy of the predictive model (weighted by ?). While
an `2 weight penalty is standard in regression prob-
lems like this, we found that the the additional en-
tropy term gave more reliable results. Intuitively,
the entropic regularizer encourages the model to re-
main maximally uncertain about its predictions. In
the metaphor of ?maximum entropy?, the entropic
prior finds a solution that has more entropy than the
?maximum? model that is compatible with the con-
straints.
The objective cannot be minimized in closed
form, but it does have a unique minimum and
is straightforwardly differentiable, so we used L-
BFGS to find the optimal weight settings (Liu et al,
1989).
5 Feature Overview
We define a large arsenal of features, our motivation
being both to improve the accuracy of classification
and to be able to interpret the characteristics of the
language produced by speakers of different L1s.
While some of the features were used in prior
work (?2), we focus on two broad novel categories
of features: those inspired by the features used
to identify translationese by Volansky et al (forth-
coming) and those extracted by automatic statisti-
cal ?correction? of the essays. Refer to figure 1 to
see the set of features and their values that were ex-
tracted from an example sentence.
POS n-grams Part-of-speech n-grams were used in
various text-classification tasks.
Prompt Since the prompt contributes information
on the domain, it is likely that some words (and,
hence, character sequences) will occur more fre-
quently with some prompts than with others. We
therefore use the prompt ID in conjunction with
other features.
Document length The number of tokens in the text
is highly correlated with the author?s level of flu-
ency, which in turn is correlated with the author?s
L1.
Pronouns The use of pronouns varies greatly
among different authors. We use the same list
of 25 English pronouns that Volansky et al (forth-
coming) use for identifying translationese.
Punctuation Similarly, different languages use
punctuation differently, and we expect this to taint
the use of punctuation in non-native texts. Of
course, character n-grams subsume this feature.
Passives English uses passive voice more fre-
quently than other languages. Again, the use of
passives in L2 can be correlated with the author?s
L1.
Positional token frequency The choice of the first
and last few words in a sentence is highly con-
strained, and may be significantly influenced by
the author?s L1.
Cohesive markers These are 40 function words
(and short phrases) that have a strong discourse
function in texts (however, because, in fact,
etc.). Translators tend to spell out implicit utter-
ances and render them explicitly in the target text
(Blum-Kulka, 1986). We use the list of Volansky
et al (forthcoming).
Cohesive verbs This is a list of manually compiled
verbs that are used, like cohesive markers, to spell
out implicit utterances (indicate, imply, contain,
etc.).
Function words Frequent tokens, which are mostly
function words, have been used successfully for
various text classification tasks. Koppel and Or-
dan (2011) define a list of 400 such words, of
which we only use 100 (using the entire list was
not significantly different). Note that pronouns
are included in this list.
Contextual function words To further capitalize
on the ability of function words to discriminate,
we define pairs consisting of a function word from
the list mentioned above, along with the POS tag
of its adjacent word. This feature captures pat-
terns such as verbs and the preposition or particle
immediately to their right, or nouns and the deter-
miner that precedes them. We also define 3-grams
consisting of one or two function words and the
POS tag of the third word in the 3-gram.
Lemmas The content of the text is not considered a
good indication of the author?s L1, but many text
categorization tasks use lemmas (more precisely,
the stems produced by the tagger) as features ap-
proximating the content.
Misspelling features Learning to perceive, pro-
duce, and encode non-native phonemic contrasts
281
Firstly the employers live more savely because they are going to have more money to spend for luxury .
Presence Considered alternatives/edits
Characters
"CHAR_l_y_ ": log 2 + 1
"CharPrompt_P5_g_o_i": log 1 + 1
"MFChar_e_ ": log 1 + 1
"Punc_period": log 1 + 1
"DeleteP_p_.": 1.0
"InsertP_p_,": 1.0
"MID:SUBST:v:f": log 1 + 1
"SUBST:v:f": log 1 + 1
Words
"DocLen_": log 19 + 1
"MeanWordRank": 422.6
"CohMarker_because": log 1 + 1
"MostFreq_have": log 1 + 1
"PosToken_last_luxury": log 1 + 1
"Pronouns_they": log 1 + 1
"MSP:safely": log 1 + 1
"Match_p_to": 0.5
"Delete_p_to": 0.5
"Delete_p_are": 1.0
"Delete_p_because": 1.0
"Delete_p_for": 1.0
POS "POS
_VBP_VBG_TO": log 1 + 1
"POS_p_VBP_VBG_TO": 0.059
Words + POS "VBP
_VBG_to": log 1 + 1
"FW__more RB": log 1 + 1
Figure 1: Some of the features extracted for an L1 German sentence.
is extremely difficult for L2 learners (Hayes-Harb
and Masuda, 2008). Since English?s orthogra-
phy is largely phonemic?even if it is irregular
in many places, we expect leaners whose na-
tive phoneme contrasts are different from those
of English to make characteristic spelling errors.
For example, since Japanese and Korean lack a
phonemic /l/-/r/ contrast, we expect native speak-
ers of those languages to be more likely to make
spelling errors that confuse l and r relative to
native speakers of languages such as Spanish in
which that pair is contrastive. To make this in-
formation available to our model, we use a noisy
channel spelling corrector (Kernighan, 1990) to
identify and correct misspelled words in the train-
ing and test data. From these corrections, we ex-
tract minimal edit features that show what inser-
tions, deletions, substitutions and joinings (where
two separate words are written merged into a sin-
gle orthographic token) were made by the author
of the essay.
Restored tags We focus on three important token
classes defined above: punctuation marks, func-
tion words and cohesive verbs. We first remove
words in these classes from the texts, and then
recover the most likely hidden tokens in a se-
quence of words, according to an n-gram lan-
guage model trained on all essays in the training
corpus corrected with a spell checker and con-
taining both words and hidden tokens. This fea-
ture should capture specific words or punctuation
marks that are consistently omitted (deletions),
or misused (insertions, substitutions). To restore
hidden tokens we use the hidden-ngram util-
ity provided in SRI?s language modeling toolkit
(Stolcke, 2002).
Brown clusters (Brown et al, 1992) describe an al-
gorithm that induces a hierarchical clustering of
a language?s vocabulary based on each vocabu-
lary item?s tendency to appear in similar left and
right contexts in a training corpus. While origi-
nally developed to reduce the number of parame-
ters required in n-gram language models, Brown
clusters have been found to be extremely effective
as lexical representations in a variety of regres-
sion problems that condition on text (Koo et al,
2008; Turian et al, 2010; Owoputi et al, 2013).
Using an open-source implementation of the al-
gorithm,2 we clustered 8 billion words of English
into 600 classes.3 We included log counts of all
4-grams of Brown clusters that occurred at least
100 times in the NLI training data.
5.1 Main Features
We use the following four feature types as the base-
line features in our model. For features that are sen-
sitive to frequency, we use the log of the (frequency-
plus-one) as the feature?s value. Table 2 reports the
accuracy of using each feature type in isolation (with
2https://github.com/percyliang/brown-cluster
3http://www.ark.cs.cmu.edu/cdyer/en-600/
cluster_viewer.html
282
Feature Accuracy (%)
POS 55.18
FreqChar 74.12
CharPrompt 65.09
Brown 72.26
DocLen 11.81
Punct 27.41
Pron 22.81
Position 53.03
PsvRatio 12.26
CxtFxn (bigram) 62.79
CxtFxn (trigram) 62.32
Misspell 37.29
Restore 47.67
CohMark 25.71
CohVerb 22.85
FxnWord 42.47
Table 2: Independent performance of feature types de-
tailed in ?5.1, ?5.2 and ?5.3. Accuracy is averaged over
10 folds of cross-validation on the training set.
10-fold cross-validation on the training set).
POS Part-of-speech n-grams. Features were ex-
tracted to count every POS 1-, 2-, 3- and 4-gram
in each document.
FreqChar Frequent character n-grams. We exper-
imented with character n-grams: To reduce the
number of parameters, we removed features only
those character n-grams that are observed more
than 5 times in the training corpus, and n ranges
from 1 to 4. High-weight features include:
TUR:<Turk>; ITA:<Ital>; JPN:<Japa>.
CharPrompt Conjunction of the character n-gram
features defined above with the prompt ID.
Brown Substitutions, deletions and insertions
counts of Brown cluster unigrams and bigrams in
each document.
The accuracy of the classifier on the development set
using these four feature types is reported in table 3.4
5.2 Additional Features
To the basic set of features we now add more spe-
cific, linguistically-motivated features, each adding
a small number of parameters to the model. As
above, we indicate the accuracy of each feature type
in isolation.
4For experiments in this paper combining multiple types of
features, we used Jonathan Clark?s workflow management tool,
ducttape (https://github.com/jhclark/ducttape).
Feature Group # Params Accuracy (%) `2
POS 540,947 55.18 1.0
+ FreqChar 1,036,871 79.55 1.0
+ CharPrompt 2,111,175 79.82 1.0
+ Brown 5,664,461 81.09 1.0
Table 3: Dev set accuracy with main feature groups,
added cumulatively. The number of parameters is always
a multiple of 11 (the number of classes). Only `2 regular-
ization was used for these experiments; the penalty was
tuned on the dev set as well.
DocLen Document length in tokens.
Punct Counts of each punctuation mark.
Pron Counts of each pronoun.
Position Positional token frequency. We use the
counts for the first two and last three words be-
fore the period in each sentence as features. High-
weight features for the second word include:
ARA:2<,>; CHI:2<is>; HIN:2<can>.
PsvRatio The proportion of passive verbs out of all
verbs.
CxtFxn Contextual function words. High-weight
features include: CHI:<some JJ>;
HIN:<as VBN>.
Misspell Spelling correction edits. Features
included substitutions, deletions, insertions,
doubling of letters and missing doublings of
letters, and splittings (alot?a lot), as well as the
word position where the error occurred.
High-weight features include: ARA:DEL<e>,
ARA:INS<e>, ARA:SUBST<e>/<i>;
GER:SUBST<z>/<y>; JPN:SUBST<l>/<r>,
JPN:SUBST<r>/<l>; SPA:DOUBLE<s>,
SPA:MID_INS<s>, SPA:INS<s>.
Restore Counts of substitutions, deletions and
insertions of predefined tokens that we restored
in the texts. High-weight features include:
CHI:DELWORD<do>; GER:DELWORD<on>;
ITA:DELWORD<be>
Table 4 reports the empirical improvement that each
of these brings independently when added to the
main features (?5.1).
5.3 Discarded Features
We also tried several other feature types that did not
improve the accuracy of the classifier on the devel-
opment set.
CohMark Counts of each cohesive marker.
283
Feature Group # Params Accuracy (%) `2
main + Position 6,153,015 81.00 1.0
main + PsvRatio 5,664,472 81.00 1.0
main 5,664,461 81.09 1.0
main + DocLen 5,664,472 81.09 1.0
main + Pron 5,664,736 81.09 1.0
main + Punct 5,664,604 81.09 1.0
main + Misspell 5,799,860 81.27 5.0
main + Restore 5,682,589 81.36 5.0
main + CxtFxn 7,669,684 81.73 1.0
Table 4: Dev set accuracy with main features plus addi-
tional feature groups, added independently. `2 regulariza-
tion was tuned as in table 3 (two values, 1.0 and 5.0, were
tried for each configuration; more careful tuning might
produce slightly better accuracy). Results are sorted by
accuracy; only three groups exhibited independent im-
provements over the main feature set.
CohVerb Counts of each cohesive verb.
FxnWord Counts of function words. These features
are subsumed by the highly discriminative CxtFxn
features.
6 Results
The full model that we used to classify the test set
combines all features listed in table 4. Using all
these features, the accuracy on the development set
is 84.55%, and on the test set it is 81.5%. The values
for ? and ? were tuned to optimize development set
performance, and found to be ? = 5, ? = 2.
Table 5 lists the confusion matrix on the test set,
as well as precision, recall and F1-score for each L1.
The largest error type involved predicting Telugu
when the true label was Hindi, which happened 18
times. This error is unsurprising since many Hindi
and Telugu speakers are arguably native speakers of
Indian English.
Production of L2 texts, not unlike translating from
L1 to L2, involves a tension between the impos-
ing models of L1 (and the source text), on the one
hand, and a set of cognitive constraints resulting
from the efforts to generate the target text, on the
other. The former is called interference in Trans-
lation Studies (Toury, 1995) and transfer in second
language acquisition (Selinker, 1972). Volansky et
al. (forthcoming) designed 32 classifiers to test the
validity of the forces acting on translated texts, and
found that features sensitive to interference consis-
tently yielded the best performing classifiers. And
indeed, in this work too, we find fingerprints of the
source language are dominant in the makeup of L2
texts. The main difference, however, between texts
translated by professionals and the texts we address
here, is that more often than not professional trans-
lators translate into their mother tongue, whereas L2
writers write out of their mother tongue by defini-
tion. So interference is ever more exaggerated in
this case, for example, also phonologically (Tsur and
Rappoport, 2007).
We explore the effects of interference by analyz-
ing several patterns we observe in the features. Our
classifier finds that the character sequence alot is
overrepresented in Arabic L2 texts. Arabic has no
indefinite article and we speculate that Arabic speak-
ers conceive a lot as a single word; the Arabic equiv-
alent for a lot is used adverbially like an -ly suffix
in English. For the same reason, another promi-
nent feature is a missing definite article before nouns
and adjectives. Additionally, Arabic, being an Ab-
jad language, rarely indicates vowels, and indeed we
find many missing e?s and i?s in the texts of Arabic
speakers. Phonologically, because Arabic conflates
/I/ and /@/ into /i/ (at least in Modern Standard Ara-
bic), we see that many e?s are indeed substituted for
i?s in these texts.
We find that essays that contain hyphens are more
likely to be from German authors. We again find
evidence of interference from the native language
here. First, relative clauses are widely used in Ger-
man, and we see this pattern in L2 English of L1
German speakers. For example, any given rational
being ? let us say Immanual Kant ? we find that.
Another source of extra hyphens stems from com-
pounding convention. So, for example, we find well-
known, community-help, spare-time, football-club,
etc. Many of these reflect an effort to both connect
and separate connected forms in the original (e.g.,
Fussballklub, which in English would be more natu-
rally rendered as football club). Another unexpected
feature of essays by native Germans is a frequent
substitution of the letter y for z and vice versa. We
suspect this owes to their switched positions on Ger-
man keyboards.
Lexical item frequency also provides clues to the
L1 of the essay writers. The word that occurs more
frequently in the texts of German L1 speakers. We
284
true? ARA CHI FRE GER HIN ITA JPN KOR SPA TEL TUR Precision (%) Recall (%) F1 (%)
ARA 80 0 2 1 3 4 1 0 4 2 3 80.8 80.0 80.4
CHI 3 80 0 1 1 0 6 7 1 0 1 88.9 80.0 84.2
FRE 2 2 81 5 1 2 1 0 3 0 3 86.2 81.0 83.5
GER 1 1 1 93 0 0 0 1 1 0 2 87.7 93.0 90.3
HIN 2 0 0 1 77 1 0 1 5 9 4 74.8 77.0 75.9
ITA 2 0 3 1 1 87 1 0 3 0 2 82.1 87.0 84.5
JPN 2 1 1 2 0 1 87 5 0 0 1 78.4 87.0 82.5
KOR 1 5 2 0 1 0 9 81 1 0 0 80.2 81.0 80.6
SPA 2 0 2 0 1 8 2 1 78 1 5 77.2 78.0 77.6
TEL 0 1 0 0 18 1 2 1 1 73 3 85.9 73.0 78.9
TUR 4 0 2 2 0 2 2 4 4 0 80 76.9 80.0 78.4
Table 5: Official test set confusion matrix with the full model. Accuracy is 81.5%.
hypothesize that in English it is optional in rela-
tive clauses whereas in German it is not, so Ger-
man speakers are less comfortable using the non-
obligatory form. Also, often is over represented. We
hypothesize that since it is cognate of German oft, it
is not cognitively expensive to retrieve it. We find
many times?a literal translation of muchas veces?
in Spanish essays.
Other informative features that reflect L1 features
include frequent misspellings involving confusions
of l and r in Japanese essays. More mysteriously,
the characters r and s are misused in Chinese and
Spanish, respectively. The word then is dominant
in the texts of Hindi speakers. Finally, it is clear
that authors refer to their native cultures (and, conse-
quently, native languages and countries); the strings
Turkish, Korea, and Ita were dominant in the texts of
Turkish, Korean and Italian native speakers, respec-
tively.
7 Discussion
We experimented with different classifiers and a
large set of features to solve an 11-way classifica-
tion problem. We hope that studying this problem
will improve to facilitate human assessment, grad-
ing, and teaching of English as a second language.
While the core features used are sparse and sensitive
to lexical and even orthographic features of the writ-
ing, many of them are linguistically informed and
provide insight into how L1 and L2 interact.
Our point of departure was the analogy between
translated texts as a genre in its own and L2 writ-
ers as pseudo translators, relying heavily on their
mother tongue and transferring their native models
to a second language. In formulating our features,
we assumed that like translators, L2 writers will
write in a simplified manner and overuse explicit
markers. Although this should be studied vis-?-vis
comparable outputs of mother tongue writers in En-
glish, we observe that the best features of our clas-
sifiers are of the ?interference? type, i.e. phonolog-
ical, morphological and syntactic in nature, mostly
in the form of misspelling features, restoration tags,
punctuation and lexical and syntactic modeling.
We would like to stress that certain features indi-
cating a particular L1 have no bearing on the quality
of the English produced. This has been discussed
extensively in Translation Studies (Toury, 1995),
where interference is observed by the overuse or un-
deruse of certain features reflecting the typological
differences between a specific pair of languages, but
which is still within grammatical limits. For exam-
ple, the fact that Italian native speakers favor the
syntactic sequence of determiner + adjective + noun
(e.g., a big risk or this new business) has little pre-
scriptive value for teachers.
A further example of how L2 quality and the
ability to predict L1 are uncorrelated, we noted
that certain L2 writers often repeat words appear-
ing in their essay prompts, and including informa-
tion about whether the writer was reusing prompt
words improved classification accuracy. We suggest
this reflects different educational backgrounds. This
feature says nothing about the quality of the text, just
as the tendency of Korean and Italian writers to men-
tion their home country more often does not.
285
Acknowledgments
This research was supported by a grant from the Is-
raeli Ministry of Science and Technology.
References
Daniel Blanchard, Joel Tetreault, Derrick Higgins, Aoife
Cahill, and Martin Chodorow. 2013. TOEFL11: A
corpus of non-native English. Technical report, Edu-
cational Testing Service.
Shoshana Blum-Kulka. 1986. Shifts of cohesion and co-
herence in translation. In Juliane House and Shoshana
Blum-Kulka, editors, Interlingual and intercultural
communication Discourse and cognition in translation
and second language acquisition studies, volume 35,
pages 17?35. Gunter Narr Verlag.
Julian Brooke and Graeme Hirst. 2011. Native language
detection with ?cheap? learner corpora. In Conference
of Learner Corpus Research (LCR2011), Louvain-la-
Neuve, Belgium. Presses universitaires de Louvain.
Peter F. Brown, Peter V. de Souza, Robert L. Mercer, Vin-
cent J. Della Pietra, and Jenifer C. Lai. 1992. Class-
based n-gram models of natural language. Computa-
tional Linguistics, 18(4).
Dominique Estival, Tanja Gaustad, Son Bao Pham, Will
Radford, and Ben Hutchinson. 2007a. Author profil-
ing for English emails. In Proc. of PACLING, pages
263?272, Melbourne, Australia.
Dominique Estival, Tanja Gaustad, Son Bao Pham, Will
Radford, and Ben Hutchinson. 2007b. TAT: An author
profiling tool with application to Arabic emails. In
Proc. of the Australasian Language Technology Work-
shop, pages 21?30, Melbourne, Australia, December.
Sylviane Granger, Estelle Dagneaux, Fanny Meunier,
and Magali Paquot. 2009. International Corpus of
Learner English. Presses universitaires de Louvain,
Louvain-la-Neuve.
Rachel Hayes-Harb and Kyoko Masuda. 2008. Devel-
opment of the ability to lexically encode novel second
language phonemic contrasts. Second Language Re-
search, 24(1):5?33.
Patrick Juola. 2006. Authorship attribution. Founda-
tions and Trends in Information Retrieval, 1(3):233?
334.
Mark D. Kernighan. 1990. A spelling correction pro-
gram based on a noisy channel model. In Proc. of
COLING.
Ekaterina Kochmar. 2011. Identification of a writer?s na-
tive language by error analysis. Master?s thesis, Uni-
versity of Cambridge.
Terry Koo, Xavier Carreras, and Michael Collins. 2008.
Simple semi-supervised dependency parsing. In Proc.
of ACL.
Moshe Koppel and Noam Ordan. 2011. Translationese
and its dialects. In Proc. of ACL-HLT, pages 1318?
1326, Portland, Oregon, USA, June. Association for
Computational Linguistics.
Moshe Koppel, Jonathan Schler, and Kfir Zigdon. 2005a.
Automatically determining an anonymous author?s na-
tive language. Intelligence and Security Informatics,
pages 41?76.
Moshe Koppel, Jonathan Schler, and Kfir Zigdon. 2005b.
Determining an author?s native language by mining
a text for errors. In Proc. of KDD, pages 624?628,
Chicago, IL. ACM.
Robert Lado. 1957. Linguistics across cultures: applied
linguistics for language teachers. University of Michi-
gan Press, Ann Arbor, Michigan, June.
Claudia Leacock, Martin Chodorow, Michael Gamon,
and Joel Tetreault. 2010. Automated Grammatical
Error Detection for Language Learners. Morgan and
Claypool.
Dong C. Liu, Jorge Nocedal, Dong C. Liu, and Jorge No-
cedal. 1989. On the limited memory BFGS method
for large scale optimization. Mathematical Program-
ming B, 45(3):503?528.
Olutobi Owoputi, Brendan O?Connor, Chris Dyer, Kevin
Gimpel, Nathan Schneider, and Noah A. Smith. 2013.
Improved part-of-speech tagging for online conversa-
tional text with word clusters. In Proc. of NAACL.
Larry Selinker. 1972. Interlanguage. International
Review of Applied Linguistics in Language Teaching,
10(1?4):209?232.
Andreas Stolcke. 2002. SRILM?an extensible lan-
guage modeling toolkit. In Procedings of Interna-
tional Conference on Spoken Language Processing,
pages 901?904.
Michael Swan and Bernard Smith. 2001. Learner En-
glish: A Teacher?s Guide to Interference and Other
Problems. Cambridge Handbooks for Language
Teachers. Cambridge University Press.
Joel Tetreault, Daniel Blanchard, and Aoife Cahill. 2013.
A report on the first native language identification
shared task. In Proc. of the Eighth Workshop on Inno-
vative Use of NLP for Building Educational Applica-
tions, Atlanta, GA, USA, June. Association for Com-
putational Linguistics.
Gideon Toury. 1995. Descriptive Translation Studies
and beyond. John Benjamins, Amsterdam / Philadel-
phia.
Kristina Toutanova, Dan Klein, Christopher D. Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Proc.
of HLT-NAACL, pages 173?180, Edmonton, Canada,
June. Association for Computational Linguistics.
286
Oren Tsur and Ari Rappoport. 2007. Using classifier fea-
tures for studying the effect of native language on the
choice of written second language words. In Proc. of
the Workshop on Cognitive Aspects of Computational
Language Acquisition, pages 9?16, Prague, Czech Re-
public, June. Association for Computational Linguis-
tics.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: A simple and general method
for semi-supervised learning. In Proc. of ACL.
Vered Volansky, Noam Ordan, and Shuly Wintner. forth-
coming. On the features of translationese. Literary
and Linguistic Computing.
Sze-Meng Jojo Wong and Mark Dras. 2009. Contrastive
analysis and native language identification. In Proc.
of the Australasian Language Technology Association
Workshop, pages 53?61, Sydney, Australia, December.
Sze-Meng Jojo Wong and Mark Dras. 2011. Exploiting
parse structures for native language identification. In
Proc. of EMNLP, pages 1600?1610, Edinburgh, Scot-
land, UK., July. Association for Computational Lin-
guistics.
287
Proceedings of the 7th Linguistic Annotation Workshop & Interoperability with Discourse, pages 51?60,
Sofia, Bulgaria, August 8-9, 2013. c?2013 Association for Computational Linguistics
A Framework for (Under)specifying Dependency Syntax
without Overloading Annotators
Nathan Schneider?? Brendan O?Connor? Naomi Saphra? David Bamman?
Manaal Faruqui? Noah A. Smith? Chris Dyer? Jason Baldridge?
?School of Computer Science, Carnegie Mellon University
?Department of Linguistics, The University of Texas at Austin
Abstract
We introduce a framework for lightweight
dependency syntax annotation. Our for-
malism builds upon the typical represen-
tation for unlabeled dependencies, per-
mitting a simple notation and annotation
workflow. Moreover, the formalism en-
courages annotators to underspecify parts
of the syntax if doing so would streamline
the annotation process. We demonstrate
the efficacy of this annotation on three lan-
guages and develop algorithms to evaluate
and compare underspecified annotations.
1 Introduction
Computational representations for natural lan-
guage syntax are borne of competing design con-
siderations. When designing such representations,
there may be a tradeoff between parsimony and
expressiveness. A range of linguistic theories at-
tract support due to differing purposes and aes-
thetic principles (Chomsky, 1957; Tesni?re, 1959;
Hudson, 1984; Sgall et al, 1986; Mel?c?uk, 1988,
inter alia). Formalisms concerned with tractable
computation may care chiefly about learnabil-
ity or parsing efficiency (Shieber, 1992; Sleator
and Temperly, 1993; Kuhlmann and Nivre, 2006).
Further considerations may include psychologi-
cal and evolutionary plausibility (Croft, 2001;
Tomasello, 2003; Steels et al, 2011; Fossum and
Levy, 2012), integration with other representa-
tions such as semantics (Steedman, 2000; Bergen
and Chang, 2005), or suitability for particular ap-
plications (e.g., translation).
Here we elevate ease of annotation as a pri-
mary design concern for a syntactic annotation
formalism. Currently, a lack of annotated data
is a huge bottleneck for robust NLP, standing in
the way of parsers for social media text (Foster
et al, 2011) and many low-resourced languages
(to name two examples). Traditional syntactic an-
notation projects like the Penn Treebank (Marcus
?Corresponding author: nschneid@cs.cmu.edu
et al, 1993) or Prague Dependency Treebank (Ha-
jic?, 1998) require highly trained annotators and
huge amounts of effort. Lowering the cost of an-
notation, by making it easier and more accessi-
ble, could greatly facilitate robust NLP in new lan-
guages and genres.
To that end, we design and test new, lightweight
methodologies for syntactic annotation. We pro-
pose a formalism, Fragmentary Unlabeled De-
pendency Grammar (FUDG) for unlabeled de-
pendency syntax that addresses some of the most
glaring deficiencies of basic unlabeled dependen-
cies (?2), with little added burden on annotators.
FUDG requires minimal theoretical commitments,
and can be supplemented with a project-specific
style guide (we provide a brief one for English).
We contribute a simple ASCII markup language?
Graph Fragment Language (GFL; ?3)?that al-
lows annotations to be authored using any text ed-
itor, along with tools for validating, normalizing,
and visualizing GFL annotations.1
An important characteristic of our framework is
annotator flexibility. The formalism supports this
by allowing underspecification of structural por-
tions that are unclear or unnecessary for the pur-
poses of a project. Fully leveraging this power re-
quires new algorithms for evaluation, e.g., of inter-
annotator agreement, where annotations are par-
tial; such algorithms are presented in ?4.2
Finally, small-scale case studies (?5) apply our
framework (formalism, notation, and evaluations)
to syntactically annotate web text in English, news
in Malagasy, and dialogues in Kinyarwanda.
2 A Dependency Grammar for
Annotation
Although dependency-based approaches to syntax
play a major role in computational linguistics, the
nature of dependency representations is far from
uniform. Exemplifying one end of the spectrum
is the Prague Dependency Treebank, which articu-
lates an elaborate dependency-based syntactic the-
1https://github.com/brendano/gfl_syntax/
2Parsing algorithms are left for future work.
51
Found the scarriest mystery door in my school . I?M SO CURIOUS D:
Found** < (the scarriest mystery door*)
Found < in < (my > school)
I?M** < (SO > CURIOUS)
D:**
my = I?M
thers still like 1 1/2 hours till Biebs bday here :P
thers** < still
thers < ((1 1/2) > hours < till < (Biebs > bday))
(thers like 1 1/2 hours)
thers < here
:P**
Figure 1: Two tweets with example GFL annotations. (The formalism and notation are described in ?3.)
ory in a rich, multi-tiered formalism (Hajic?, 1998;
B?hmov? et al, 2003). On the opposite end of
the spectrum are the structures used in dependency
parsing research which organize all the tokens of
a sentence into a tree, sometimes with category la-
bels on the edges (K?bler et al, 2009). Insofar as
they reflect a theory of syntax, these vanilla de-
pendency grammars provide a highly reduction-
ist view of structure?indeed, parses used to train
and evaluate dependency parses are often simpli-
fications of Prague-style parses, or else converted
from constituent treebanks.
In addition to the binary dependency links of
vanilla dependency representations, we offer three
devices to capture certain linguistic phenomena
more straightforwardly:3
1. We make explicit the meaningful lexical units
over which syntactic structure is represented. Our
approach (a) allows punctuation and other extrane-
ous tokens to be excluded so as not to distract from
the essential structure; and (b) permits tokens to be
grouped into shallow multiword lexical units.4
2. Coordination is problematic to represent with
unlabeled dependencies due to its non-binary na-
ture. A coordinating conjunction typically joins
multiple expressions (conjuncts) with equal sta-
tus, and other expressions may relate to the com-
pound structure as a unit. There are several differ-
ent conventions for forcing coordinate structures
into a head-modifier straightjacket (Nivre, 2005;
de Marneffe and Manning, 2008; Marec?ek et al,
2013). Conjuncts, coordinators, and shared de-
pendents can be distinguished with edge labels;
we equivalently use a special notation, permitting
the coordinate structure to be automatically trans-
formed with any of the existing conventions.5
3Some of this is inspired by the conventions of Reed-
Kellogg sentence diagramming, a graphical dependency an-
notation system for English pedagogy (Reed and Kellogg,
1877; Kolln and Funk, 1994; Florey, 2006).
4The Stanford representation supports a limited notion of
multiword expressions (de Marneffe and Manning, 2008).
For simplicity, our formalism treats multiwords as unana-
lyzed (syntactically opaque) wholes, though some multiword
expressions may have syntactic descriptions (Baldwin and
Kim, 2010).
5Tesni?re (1959) and Hudson (1984) similarly use
special structures for coordination (Schneider, 1998;
3. Following Tesni?re (1959), our formalism
offers a simple facility to express anaphora-
antecedent relations (a subset of semantic relation-
ships) that are salient in particular syntactic phe-
nomena such as relative clauses, appositives, and
wh-expressions.
Underspecification. Our desire to facilitate
lightweight annotation scenarios requires us to
abandon the expectation that syntactic informants
provide a complete parse for every sentence. On
one hand, an annotator may be uncertain about the
appropriate parse due to lack of expertise, insuf-
ficiently mature annotation conventions, or actual
ambiguity in the sentence. On the other hand, an-
notators may be indifferent to certain phenomena.
This can happen for a variety of reasons:
? Some projects may only need annotations of
specific constructions. For example, building a
semantic resource for events may require anno-
tation of syntactic verb-argument relations, but
not internal noun phrase structure.
? As a project matures, it may be more useful to
annotate only infrequent lexical items.
? Semisupervised learning from partial annota-
tions may be sufficient to learn complete parsers
(Hwa, 1999; Clark and Curran, 2006).
? Beginning annotators may wish to focus on eas-
ily understood syntactic phenomena.
? Different members of a project may wish to spe-
cialize in different syntactic phenomena, reduc-
ing training cost and cognitive load.
Rather than treating annotations as invalid unless
and until they are complete trees, we formally rep-
resent and reason about partial parse structures.
Annotators produce annotations, which encode
constraints on the (inferred) analysis, the parse
structure, of a sentence. We say that a valid anno-
tation supports (is compatible with) one or more
analyses. Both annotations and analyses are rep-
resented as graphs (the graph representation is de-
scribed below in ?3.2). We require that the di-
rected edges in an analysis graph must form a tree
over all the lexical items in the sentence.6 Less
Sangati and Mazza, 2009).
6While some linguistic phenomena (e.g., relative clauses,
control constructions) can be represented using non-tree
52
stringent well-formedness constraints on the an-
notation graph leave room for underspecification.
Briefly, an annotation can be underspecified in
two ways: (a) an expression may not be attached to
any parent, indicating it might depend on any non-
descendant in a full analysis?this is useful for an-
notating sentences piece by piece; and (b) multiple
expressions may be grouped together in a fudge
expression (?3.3), a constraint that the elements
form a connected subgraph in the full analysis
while leaving the precise nature of that subgraph
indeterminate?this is useful for marking relation-
ships between chunks (possibly constituents).
A formalism, not a theory. Our framework for
dependency grammar annotation is a syntactic
formalism, but it is not sufficiently comprehen-
sive to constitute a theory of syntax. Though
it standardizes the basic treatment of a few ba-
sic phenomena, simplicity of the formalism re-
quires us to be conservative about making such
extensions. Therefore, just as with simpler for-
malisms, language- and project-specific conven-
tions will have to be developed for specific linguis-
tic phenomena. By embracing underspecified an-
notation, however, our formalism aims to encour-
age efficient corpus coverage in a nascent anno-
tation project, without forcing annotators to make
premature decisions.
3 Syntactic Formalism and GFL
In our framework, a syntactic annotation of a sen-
tence follows an extended dependency formalism
based on the desiderata enumerated in the previ-
ous section. We call our formalism Fragmentary
Unlabeled Dependency Grammar (FUDG).
To make it simple to create FUDG annotations
with a text editor, we provide a plain-text de-
pendency notation called Graph Fragment Lan-
guage (GFL). Fragments of the FUDG graph?
nodes and dependencies linking them?are en-
coded in this language; taken together, these frag-
ments describe the annotation in its entirety. The
ordering of GFL fragments, and of tokens within
each fragment, is of no formal consequence. Since
the underlying FUDG representation is transpar-
ently related to GFL constructions, GFL notation
will be introduced alongside the discussion of each
kind of FUDG node.7
structures, we find that being able to alert annotators when
they inadvertently violate the tree constraint is more useful
than the expressive flexibility.
7In principle, FUDG annotations could be created with
3.1 Tokens
We expect a tokenized string, such as a sentence
or short message. The provided tokenization is re-
spected in the annotation. For human readability,
GFL fragments refer to tokens as strings (rather
than offsets), so all tokens that participate in an
annotation must be unambiguous in the input.8 A
token may be referenced multiple times in the an-
notation.
3.2 Graph Encoding
Directed arcs. As in other dependency
formalisms, dependency arcs are directed
links indicating the syntactic headedness
relationship between pairs of nodes. In
GFL, directed arcs are indicated with an-
gle brackets pointing from the dependent to
its head, as in black > cat or (equivalently)
cat < black. Multiple arcs can be chained to-
gether: the > cat < black < jet describes three
arcs. Parentheses help group portions of a chain:
(the > cat < black < jet) > likes < fish (the
structure black < jet > likes, in which jet
appears to have two heads, is disallowed). Note
that another encoding for this structure would be
to place the contents of the parentheses and the
chain cat > likes < fish on separate lines. Curly
braces can be used to list multiple dependents of
the same head: {cat fish} > likes.
Anaphoric links. These undirected links join
coreferent anaphora to each other and to their an-
tecedent(s). In English this includes personal pro-
nouns, relative pronouns (who, which, that), and
anaphoric do and so (Leo loves Ulla and so does
Max). This introduces a bit of semantics into our
annotation, though at present we do not attempt to
mark non-anaphoric coreference. It also allows a
more satisfying treatment of appositives and rel-
ative clauses than would be possible from just the
directed tree (the third example in figures 2 and 3).
Lexical nodes. Whereas in vanilla dependency
grammar syntactic links are between pairs of to-
ken nodes, FUDG abstracts away from the indi-
vidual tokens in the input. The lowest level of a
FUDG annotation consists of lexical nodes, i.e.,
an alternative mechanism such as a GUI, as in Hajic? et al
(2001).
8If a word is repeated within the sentence, it must be in-
dexed in the input string in order to be referred to from a
fragment. In our notation, successive instances of the same
word are suffixed with ~1, ~2, ~3, etc. Punctuation and other
tokens omitted from an annotation do not need to be indexed.
53
'll
If
's
I wake_up
restin' it~1
it~2
weapons
Our three
are
$a
fear surprise efficiency
ruthless
and~1 and~2
are
We knights
the
who
say
Ni
Figure 2: FUDG graphs corresponding to the examples in figure 3. The two special kinds of directed edges are for attaching
conjuncts (bolded) and their coordinators (dotted) in a coordinate structure. Anaphoric links are undirected. The root node of
each sentence is omitted.
If it~1 's restin' I 'll wake it~2 up .
If < (it~1 > 's < restin')
I > 'll < [wake up] < it~2
If > 'll**
it~1 = it~2
Our three weapons are fear and~1 surprise and~2
ruthless efficiency ...
{Our three} > weapons > are < $a
$a :: {fear surprise efficiency} :: {and~1 and~2}
ruthless > efficiency
We are the knights who say ... Ni !
We > are < knights < the
knights < (who > say < Ni)
who = knights
Figure 3: GFL for the FUDG graphs in figure 2.
lexical item occurrences. Every token node maps
to 0 or 1 lexical nodes (punctuation, for instance,
can be ignored).
A multiword is a lexical node incorporating
more than one input token and is atomic (does
not contain internal structure). A multiword node
may group any subset of input tokens; this allows
for multiword expressions which are not neces-
sarily contiguous in the sentence (e.g., the verb-
particle construction make up in make the story
up). GFL notates multiwords with square brack-
ets, e.g., [break a leg].
Coordination nodes. Coordinate structures re-
quire at least two kinds of dependents: co-
ordinators (i.e., lexical nodes for coordinat-
ing conjunctions?at least one per coordina-
tion node) and conjuncts (heads of the con-
joined subgraphs?at least one per coordination
node). The GFL annotation has three parts:
a variable representing the node, a set of con-
juncts, and a set of coordinator nodes. For in-
stance, $a :: {[peanut butter] honey} :: {and}
(peanut butter and honey) can be embedded
within a phrase via the coordination node
variable $a; a [fresh [[peanut butter] and
honey] sandwich] snack would be formed with
{fresh $a} > sandwich > snack < a. A graphical
example of coordination can be seen in figure 2?
note the bolded conjunct edges and the dotted co-
ordinator edges. If the conjoined phrase as a whole
takes modifiers, these are attached to the coordina-
tion node with regular directed arcs. For example,
in Sam really adores kittens and abhors puppies.,
the shared subject Sam and adverb really attach to
the entire conjoined phrase. In GFL:
$a :: {adores abhors} :: {and}
Sam > $a < really
adores < kittens abhors < puppies
Root node. This is a special top-level node used
to indicate that a graph fragment constitutes a stan-
dalone utterance or a discourse connective. For an
input with multiple utterances, the head of each
should be designated with ** to indicate that it at-
taches to the root.
3.3 Means of Underspecification
As discussed in ?2, our framework distinguishes
annotations from full syntactic analyses. With re-
spect to dependency structure (directed edges), the
former may underspecify the latter, allowing the
annotator to commit only to a partial analysis.
For an annotationA, we define support(A) to be
the set of full analyses compatible with that anno-
tation. A full analysis is required to be a directed
rooted tree over all lexical nodes in the annotation.
An annotation is valid if its support is non-empty.
The 2 mechanisms for dependency underspeci-
fication are unattached nodes and fudge nodes.
Unattached nodes. For any node in an annota-
tion, the annotator is free to simply leave it not
attached to any head. This is interpreted as al-
lowing its head to be any other node (including
the root node), subject to the tree constraint. We
call a node?s possible heads its supported par-
ents. Formally, for an unattached node v in an-
notation A, suppParentsA(v) = nodes(A) \ ({v} ?
descendants(v)).
Fudge nodes. Sometimes, however, it is desir-
able to represent a sort of skeletal structure with-
out filling in all the details. A fudge expres-
sion (FE) asserts that a group of nodes (the ex-
pression?s members) belong together in a con-
nected subgraph, while leaving the internal struc-
ture of that subgraph unspecified.9 The notation
9This underspecification semantics is, to the best of our
knowledge, novel, though it has been proposed that con-
nected dependency subgraphs (known as catenae) are of the-
oretical importance in syntax (Osborne et al, 2012).
54
FN2
a
b
f
FN1
c d e
f
b
f
b
b c
b
b a
a
d
a
c a
d
a
d
c db e fe c e fe
a
d d
cf
e e f
c
Figure 4: Left: An annotation graph with 2 fudge nodes and 6 lexical nodes; it can be encoded with GFL fragments
((a b)* c d) < e and b < f. Right: All of its supported analyses: prom(A) = 6. com(A) = 1 ?
log 6
log 75
= .816.
for this is a list of two or more nodes within
parentheses: an annotation for Few if any witches
are friends with Maria. might contain the FE
(Few if any) so as to be compatible with the
structures Few < if < any, Few > if > any, etc.?
but not, for instance, Few > witches < any. In
the FUDG graph, this is represented with a fudge
node to which members are attached by special
member arcs. Fudge nodes may be linked to other
nodes: the GFL fragment (Few if any) > witches
is compatible with (Few < if < any) > witches,
(Few < (if > any)) > witches, and so forth.
Properties. Let f be a fudge expression. From
the connected subgraph definition and the tree
constraint on analyses, it follows that:
? Exactly 1 member of f must, in any compatible
analysis, have a parent that is not a member of f.
Call this node the top of the fudge expression,
denoted f ?. f ? dominates all other members of
f; it can be considered f?s ?internal head.?
? f does not necessarily form a full subtree. Any
of its members may have dependents that are
not themselves members of the fudge expres-
sion. (Such dependencies can be specified in
additional GFL fragments.)
Top designation. A single member of a fudge
expression may optionally be designated as its top
(internal head). This is specified with an asterisk:
(Few* if any) > witches indicates that Few must
attach to witches and also dominate both if and
any. In the FUDG graph, this is represented with
a special top arc as depicted in bold in figure 4.
Nesting. One fudge expression may nest
within another, e.g. (Few (if any)) > witches;
the word analyzed as attaching to witches might
be Few or whichever of (if any) heads the other.
A nested fudge expression can be designated as
top: (Vanishingly few (if any)*).
Modifiers. An arc attaching a node to a
fudge expression as a whole asserts that the
external node should modify the top of the fudge
expression (whether or not that top is designated
in the annotation). For instance, two of the
interpretations of British left waffles on Falklands
would be preserved by specifying British > left
and (left waffles) < on < Falklands. Analyses
British > left < waffles < on < Falklands and
(British > left < on < Falklands) > waffles
would be excluded because the preposition does
not attach to the head of (left waffles).10
Multiple membership. A node may be a mem-
ber of multiple fudge expressions, or a member
of an FE while attached to some other node via
an explicit arc. Each connected component of
the FUDG graph is therefore a polytree (not nec-
essarily a tree). The annotation graph minus all
member edges of fudge nodes and all (undirected)
anaphoric links must be a directed tree or forest.
Enumerating supported parents. Fudge ex-
pressions complicate the procedure for listing a
node?s supported parents (see above). Consider an
FE f having some member v. v might be the top
of f (unless some other node is so designated), in
which case anything the fudge node can attach to
is a potential parent of v. If some node other than
v might be the top of f, then v?s head could be any
member of f. Below (?4.1) we develop an algo-
rithm for enumerating supported parents for any
annotation graph node.
4 Annotation Evaluation Measures
For an annotation task which allows for a great
deal of latitude?as in our case, where a syntac-
tic annotation may be full or partial?quantitative
evaluation of data quality becomes a challenge. In
the context of our formalism, we propose mea-
sures that address:
? Annotation efficiency, quantified in terms of
annotator productivity (tokens per hour).
? The amount of information in an underspeci-
fied annotation. Intuitively, an annotation that
flirts with many full analyses conveys less syn-
tactic information than one which supports few
analyses. We define an annotation?s promiscu-
ity to be the number of full analyses it supports,
and develop an algorithm to compute it (?4.1).
10Not all attachment ambiguities can be precisely encoded
in FUDG. For instance, there is no way to forbid an attach-
ment to a word that lies along the path between the pos-
sible heads. The best that can be done given a sentence
like They conspired to defenestrate themselves on Tuesday. is
They > conspired < to < defenestrate < themselves and
(conspired* to defenestrate (on < Tuesday)).
55
? Inter-annotator agreement between two par-
tial annotations. Our measures for dependency
structure agreement (?4.2) incorporate the no-
tion of promiscuity.
We test these evaluations on our pilot annotation
data in the case studies (?5).
4.1 Promiscuity vs. Commitment
Given a FUDG annotation of a sentence, we quan-
tify the extent to which it underspecifies the full
structure by counting the number of analyses that
are compatible with the constraints in the annota-
tion. We call this number the promiscuity of the
annotation. Each analysis tree is rooted with the
root node and must span all lexical nodes.11
A na?ve algorithm for computing promiscuity
would be to enumerate all directed spanning trees
over the lexical nodes, and then check each of
them for compatibility with the annotation. But
this quickly becomes intractable: for n nodes,
one of which is designated as the root, there are
nn?2 spanning trees. However, we can filter out
edges that are known to be incompatible with
the annotation before searching for spanning
trees. Our ?upward-downward? method for
constructing a graph of supported edges first
enumerates a set of candidate top nodes for every
fudge expression, then uses that information
to infer a set of supported parents for every
node.12 The supported edge graph then consists
of vertices lexnodes(A) ? {root} and edges
?
v?lexnodes(A) {(v? v?) ? v? ? suppParentsA(v)}.
From this graph we can count all directed span-
ning trees in cubic time using Kirchhoff?s matrix
tree theorem (Chaiken and Kleitman, 1978; Smith
and Smith, 2007; Margoliash, 2010).13 If some
lexical node has no supported parents, this reflects
conflicting constraints in the annotation, and no
spanning tree will be found.
Promiscuity will tend to be higher for longer
sentences. To control for this, we define a second
quantity, the annotation?s commitment quotient
(commitment being the opposite of promiscuity),
11This measure assumes a fixed lexical analysis (set of lex-
ical nodes) and does not consider anaphoric links. Coordinate
structures are simplified into ordinary dependencies, with co-
ordinate phrases headed by the coordinator?s lexical node. If
a coordination node has multiple coordinators, one is arbi-
trarily chosen as the head and the others as its dependents.
12Python code for these algorithms appears in Schneider
et al (2013) and the accompanying software release.
13Due to a technicality with non-member attachments to
fudge nodes, for some annotations this is only an upper bound
on promiscuity; see Schneider et al (2013).
which normalizes for the number of possible span-
ning trees given the sentence length. The commit-
ment quotient for an annotation of a sentence with
n?1 lexical nodes and one root node is given by:
com(A) = 1 ?
log prom(A)
log nn?2
(the logs are to attenuate the dominance of the ex-
ponential term). This will be 1 if only a single
tree is supported by the annotation, and 0 if the
annotation does not constrain the structure at all.
(If the constraints in the annotation are internally
inconsistent, then promiscuity will be 0 and com-
mitment undefined.) In practice, there is a trade-
off between efficiency and commitment: more de-
tailed annotations require more time. The value of
minimizing promiscuity will therefore depend on
the resources and goals of the annotation project.
4.2 Inter-Annotator Agreement
FUDG can encode flat groupings and coreference
at the lexical level, as well as syntactic structure
over lexical items. Inter-annotator agreement can
be measured separately for each of these facets.
Pilot annotator feedback indicated that our initial
lexical-level guidelines were inadequate, so we fo-
cus here on measuring structural agreement pend-
ing further clarification of the lexical conventions.
Attachment accuracy, a standard measure for
evaluating dependency parsers, cannot be com-
puted between two FUDG annotations if either of
them underspecifies any part of the dependency
structure. One solution is to consider the inter-
section of supported full trees, in the spirit of
our promiscuity measure. For annotations A1 and
A2 of sentence s, one annotation?s supported an-
alyses can be enumerated and then filtered sub-
ject to the constraints of the other annotation.
The tradeoff between inter-annotator compatibil-
ity and commitment can be accounted for by tak-
ing their product, i.e. comPrec(A1 | A2) =
com(A1)
|supp(A1)?supp(A2)|
|supp(A1)|
.
A limitation of this support-intersection ap-
proach is that if the two annotations are not
compatible, the intersection will be empty. A
more fine-grained approach is to decompose
the comparison by lexical node: we general-
ize attachment accuracy with softComPrec(A1 |
A2) = com(A1)
?
`?s
?
i?{1,2} suppParentsAi (`)?
`?s suppParentsA1 (`)
, comput-
ing com(?) and suppParents(?) as in the previous
section. As lexical nodes may differ between the
two annotations, a reconciliation step is required
56
Language Tokens Rate (tokens/hr)
English Tweets (partial) 667 430
English Tweets (full) 388 250
Malagasy 4,184 47
Kinyarwanda 8,036 80
Table 1: Productivity estimates from pilot annotation project.
All annotators were native speakers of English.
to compare the structures: multiwords proposed in
only one of the two annotations are converted to
fudge expressions. Tokens annotated by neither
annotator are ignored. Like with the promiscuity
measure, we simplify coordinate structures to or-
dinary dependencies (see footnote 11).
5 Case Studies
5.1 Annotation Time
To estimate annotation efficiency, we performed
a pilot annotation project consisting of annotating
several hundred English tweets, about 1,000 sen-
tences in Malagasy, and a further 1,000 sentences
in Kinyarwanda.14 Table 1 summarizes the num-
ber of tokens annotated and the effort required. For
the two Twitter cases, the same annotator was first
permitted to do partial annotation of 100 tweets,
and then spend the same amount of time doing a
complete annotation of all tokens. Although this is
a very small study, the results clearly suggest she
was able to make much more rapid progress when
partial annotation was an option.15
This pilot study helped us to identify linguistic
phenomena warranting specific conventions: these
include wh-expressions, comparatives, vocatives,
discourse connectives, null copula constructions,
and many others. We documented these cases in a
20-page style guide for English,16 which informed
the subsequent pilot studies discussed below.
5.2 Underspecification and Agreement
We annotated 2 small English data samples in
order to study annotators? use of underspecifica-
tion. The first is drawn from Owoputi et al?s 2013
Twitter part-of-speech corpus; the second is from
the Reviews portion of the English Web Treebank
14Malagasy is a VOS Austronesian language spoken by 15
million people, mostly in Madagascar. Kinyarwanda is an
SVO Bantu language spoken by 12 million people mostly in
Rwanda. All annotations were done by native speakers of En-
glish. The Kinyarwanda and Malagasy annotators had basic
proficiency in these languages.
15As a point of comparison, during the Penn Treebank
project, annotators corrected the syntactic bracketings pro-
duced by a high-quality hand-written parser (Fidditch) and
achieved a rate of only 375 tokens/hour using a specialized
GUI interface (Marcus et al, 1993).
16Included with the data and software release (footnote 1).
Omit. prom Hist. Mean
1Ws MWs Tkns FEs 1 >1 ?10 ?102 com
Tweets 60 messages, 957 tokens
A 597 56 304 23 43 17 11 5 .96
B 644 47 266 28 37 23 12 6 .95
Reviews 55 sentences, 778 tokens
A 609 33 136 2 53 2 2 1 1.00
C ? D 643 19 116 114 11 44 38 21 .82
T 704 ? 74 ? 55 0 0 0 1
Table 2: Measures of our annotation samples. Note that
annotator ?D? specialized in noun phrase?internal structure,
while annotator ?C? specialized in verb phrase/clausal phe-
nomena; C ? D denotes the combination of their annotation
fragments. ?T? denotes our dependency conversion of the
English Web Treebank parses. (The value 1.00 was rounded
up from .9994.)
(EWTB) (Bies et al, 2012). (Our annotators only
saw the tokenized text.) Both datasets are infor-
mal and conversational in nature, and are dom-
inated by short messages/sentences. In spite of
their brevity, many of the items were deemed to
contain multiple ?utterances,? which we define to
include discourse connectives and emoticons (at
best marginal parts of the syntax); utterance heads
are marked with ** in figure 1.
Table 2 indicates the sizes of the two data sam-
ples, and gives statistics over the output of each
annotator: total counts of single-word and mul-
tiword lexical nodes, tokens not represented by
any lexical node, and fudge nodes; as well as
a histogram of promiscuity counts and the aver-
age of commitment quotients (see ?4.1). For in-
stance, the two sets of annotations obtained for the
Tweets sample used underspecification in 17/60
and 23/60 tweets, respectively, though the promis-
cuity rarely exceeded 100 compatible trees per an-
notation. Examples can be seen in figure 1, where
annotator ?A? marked only the noun phrase head
for the scarriest mystery door, opted not to choose
a head within the quantity 1 1/2, and left ambigu-
ous the attachment of the hedge like. The strong
but not utter commitment to the dependency struc-
ture is reflected in the mean commitment quotients
for this dataset, both of which exceed 0.95.
Inter-annotator agreement (IAA) is quantified in
table 3. The row marked A ? B, for instance,
considers the agreement between annotator ?A?
and annotator ?B?. Measuring IAA on the depen-
dency structure requires a common set of lexical
nodes, so a lexical reconciliation step ensures that
(a) any token used by either annotation is present
in both, and (b) no multiword node is present
in only one annotation?solved by relaxing in-
compatible multiwords to FEs (which increases
promiscuity). For Tweets, lexical reconciliation
57
thus reduces the commitment averages for each
annotation?to a greater extent for annotator ?A?
(.96 in table 2 vs. .82 in table 3) because ?A?
marked more multiwords. An analysis fully com-
patible with both annotations exists for only 27/60
sentences; the finer-grained softComPrec measure
(?4.2), however, offers insight into the balance be-
tween commitment and agreement.
Qualitatively, we observe three leading causes
of incompatibilities (disagreements): obvious an-
notator mistakes (such as the marked as a head);
inconsistent handling of verbal auxiliaries; and un-
certainty whether to attach expressions to a verb
or the root node, as with here in figure 1.17 An-
notators noticed occasional ambiguous cases and
attempted to encode the ambiguity with fudge ex-
pressions: again in the tweet maybe put it off un-
til you feel like ~ talking again ? is one example.
More often, fudge expressions proved useful for
syntactically difficult constructions, such as those
shown in figure 1 as well as: 2 shy of breaking it,
asked what tribe I was from, a $ 13 / day charge,
you two, and the most awkward thing ever.
5.3 Annotator Specialization
As an experiment in using underspecification for
labor division, two of the annotators of Reviews
data were assigned specific linguistic phenomena
to focus on. Annotator ?D? was tasked with the in-
ternal structure of base noun phrases, including re-
solving the antecedents of personal pronouns. ?C?
was asked to mark the remaining phenomena?
i.e., utterance/clause/verb phrase structure?but to
mark base noun phrases as fudge expressions,
leaving their internal structure unspecified. Both
annotators provided a full lexical analysis. For
comparison, a third individual, ?A,? annotated the
same data in full. The three annotators worked
completely independently.
Of the results in tables 2 and 3, the most notable
difference between full and specialized annotation
is that the combination of independent specialized
annotations (C ? D) produces somewhat higher
promiscuity/lower commitment. This is unsurpris-
ing because annotators sometimes overlook rela-
tionships that fall under their specialty.18 Still, an-
notators reported that specialization made the task
17Another example: Some uses of conjunctions like and
and so can be interpreted as either phrasal coordinators or dis-
course connectives (cf. The PDTB Research Group, 2007).
18A more practical and less error-prone approach might be
for specialists to work sequentially or collaboratively (rather
than independently) on each sentence.
com softComPrec
IAA 1 2 N|?|>0 1|2 2|1 F1
Tweets (N=60)
A ? B .82 .91 27 .57 .72 .63
Reviews (N=55)
A ? (C ? D) .95 .76 30 .64 .40 .50
A ? T .92 1 26 .48 .91 .63
(C ? D) ? T .73 1.00 28 .33 .93 .49
Table 3: Measures of inter-annotator agreement. Annotator
labels are as in table 2. Per-annotator com (with lexical rec-
onciliation) and inter-annotator softComPrec are aggregated
over sentences by arithmetic mean.
less burdensome, and the specialized annotations
did prove complementary to each other.19
5.4 Treebank Comparison
Though the annotators in our study were native
speakers well acquainted with representations of
English syntax, we sought to quantify their agree-
ment with the expert treebankers who created the
EWTB (the source of the Reviews sentences). We
converted the EWTB?s constituent parses to de-
pendencies via the PennConverter tool (Johansson
and Nugues, 2007),20 then removed punctuation.
Agreement with the converted treebank parses
appears in the bottom two rows of table 3. Be-
cause the EWTB commits to a single analysis,
precision scores are quite lopsided. Most of its
attachments are consistent with our annotations
(softComPrec > 0.9), but these allow many ad-
ditional analyses (hence the scores below 0.5).
6 Conclusion
We have presented a framework for simple depen-
dency annotation that overcomes some of the rep-
resentational limitations of unlabeled dependency
grammar and embraces the practical realities of
resource-building efforts. Pilot studies (in multiple
languages and domains, supported by a human-
readable notation and a suite of open-source tools)
showed this approach lends itself to rapid annota-
tion with minimal training.
The next step will be to develop algorithms ex-
ploiting these representations for learning parsers.
Other future extensions might include additional
expressive mechanisms (e.g., multi-headedness,
labels), crowdsourcing of FUDG annotations
(Snow et al, 2008), or even a semantic counter-
part to the syntactic representation.
19In fact, for only 2 sentences did ?C? and ?D? have in-
compatible annotations, and both were due to simple mis-
takes that were then fixed in the combination.
20We ran PennConverter with options chosen to emulate
our annotation conventions; see Schneider et al (2013).
58
Acknowledgments
We thank Lukas Biewald, Yoav Goldberg, Kyle Jerro, Vi-
jay John, Lori Levin, Andr? Martins, and several anony-
mous reviewers for their insights. This research was sup-
ported in part by the U. S. Army Research Laboratory and
the U. S. Army Research Office under contract/grant number
W911NF-10-1-0533 and by NSF grant IIS-1054319.
References
Timothy Baldwin and Su Nam Kim. 2010. Multi-
word expressions. In Nitin Indurkhya and Fred J.
Damerau, editors, Handbook of Natural Language
Processing, Second Edition. CRC Press, Taylor and
Francis Group, Boca Raton, FL.
Benjamin K. Bergen and Nancy Chang. 2005. Embod-
ied Construction Grammar in simulation-based lan-
guage understanding. In Jan-Ola ?stman and Mir-
jam Fried, editors, Construction grammars: cog-
nitive grounding and theoretical extensions, pages
147?190. John Benjamins, Amsterdam.
Ann Bies, Justin Mott, Colin Warner, and Seth Kulick.
2012. English Web Treebank. Technical Re-
port LDC2012T13, Linguistic Data Consortium,
Philadelphia, PA.
Alena B?hmov?, Jan Hajic?, Eva Hajic?ov?, Barbora
Hladk?, and Anne Abeill?. 2003. The Prague De-
pendency Treebank: a three-level annotation sce-
nario. In Treebanks: building and using parsed cor-
pora, pages 103?127. Springer.
Seth Chaiken and Daniel J. Kleitman. 1978. Matrix
Tree Theorems. Journal of Combinatorial Theory,
Series A, 24(3):377?381.
Noam Chomsky. 1957. Syntactic Structures. Mouton,
La Haye.
Stephen Clark and James Curran. 2006. Partial training
for a lexicalized-grammar parser. In Proceedings of
the Human Language Technology Conference of the
NAACL (HLT-NAACL 2006), pages 144?151. As-
sociation for Computational Linguistics, New York
City, USA.
William Croft. 2001. Radical Construction Grammar:
Syntactic Theory in Typological Perspective. Oxford
University Press, Oxford.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. Stanford typed dependencies man-
ual. http://nlp.stanford.edu/downloads/
dependencies_manual.pdf.
Kitty Burns Florey. 2006. Sister Bernadette?s Barking
Dog: The quirky history and lost art of diagramming
sentences. Melville House, New York.
Victoria Fossum and Roger Levy. 2012. Sequential
vs. hierarchical syntactic models of human incre-
mental sentence processing. In Proceedings of the
3rd Workshop on Cognitive Modeling and Computa-
tional Linguistics (CMCL 2012), pages 61?69. As-
sociation for Computational Linguistics, Montr?al,
Canada.
Jennifer Foster, Ozlem Cetinoglu, Joachim Wagner,
Joseph Le Roux, Stephen Hogan, Joakim Nivre,
Deirdre Hogan, and Josef van Genabith. 2011.
#hardtoparse: POS Tagging and Parsing the Twitter-
verse. In Proceedings of the 2011 AAAI Workshop
on Analyzing Microtext, pages 20?25. AAAI Press,
San Francisco, CA.
The PDTB Research Group. 2007. The Penn Discourse
Treebank 2.0 annotation manual. Technical Report
IRCS-08-01, Institute for Research in Cognitive Sci-
ence, University of Pennsylvania, Philadelphia, PA.
Jan Hajic?. 1998. Building a syntactically annotated
corpus: the Prague Dependency Treebank. In Eva
Hajic?ov?, editor, Issues of Valency and Meaning.
Studies in Honor of Jarmila Panevov?, pages 12?
19. Prague Karolinum, Charles University Press,
Prague.
Jan Hajic?, Barbora Vidov? Hladk?, and Petr Pajas.
2001. The Prague Dependency Treebank: anno-
tation structure and support. In Proceedings of
the IRCS Workshop on Linguistic Databases, pages
105?114. University of Pennsylvania, Philadelphia,
USA.
Richard A. Hudson. 1984. Word Grammar. Blackwell,
Oxford.
Rebecca Hwa. 1999. Supervised grammar induction
using training data with limited constituent infor-
mation. In Proceedings of the 37th Annual Meet-
ing of the Association for Computational Linguistics
(ACL-99), pages 73?79. Association for Computa-
tional Linguistics, College Park, Maryland, USA.
Richard Johansson and Pierre Nugues. 2007. Extended
constituent-to-dependency conversion for English.
In Joakim Nivre, Heiki-Jaan Kaalep, Kadri Muis-
chnek, and Mare Koit, editors, Proceedings of the
16th Nordic Conference of Computational Linguis-
tics (NODALIDA-2007), pages 105?112. Tartu, Es-
tonia.
Martha Kolln and Robert Funk. 1994. Understanding
English Grammar. Macmillan, New York.
Sandra K?bler, Ryan McDonald, and Joakim Nivre.
2009. Dependency Parsing. Number 2 in Synthesis
Lectures on Human Language Technologies. Mor-
gan & Claypool, San Rafael, CA.
Marco Kuhlmann and Joakim Nivre. 2006. Mildly non-
projective dependency structures. In Proceedings
of the COLING/ACL 2006 Main Conference Poster
Sessions, pages 507?514. Association for Computa-
tional Linguistics, Sydney, Australia.
59
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: the Penn Treebank. Computa-
tional Linguistics, 19(2):313?330.
David Marec?ek, Martin Popel, Loganathan Ramasamy,
Jan ?te?p?nek, Daniel Zeman, Zdene?k ?abokrtsk?,
and Jan Hajic?. 2013. Cross-language study on in-
fluence of coordination style on dependency parsing
performance. Technial Report 49, ?FAL MFF UK.
Jonathan Margoliash. 2010. Matrix-Tree Theorem for
directed graphs. http://www.math.uchicago.
edu/~may/VIGRE/VIGRE2010/REUPapers/
Margoliash.pdf.
Igor Aleksandrovic? Mel?c?uk. 1988. Dependency Syn-
tax: Theory and Practice. SUNY Press, Albany,
NY.
Joakim Nivre. 2005. Dependency grammar and depen-
dency parsing. Technical Report MSI report 05133,
V?xj? University School of Mathematics and Sys-
tems Engineering, V?xj?, Sweden.
Timothy Osborne, Michael Putnam, and Thomas Gro?.
2012. Catenae: introducing a novel unit of syntactic
analysis. Syntax, 15(4):354?396.
Olutobi Owoputi, Brendan O?Connor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A.
Smith. 2013. Improved part-of-speech tagging for
online conversational text with word clusters. In
Proceedings of the 2013 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 380?390. Association for Computational Lin-
guistics, Atlanta, Georgia, USA.
Alonzo Reed and Brainerd Kellogg. 1877. Work on
English grammar & composition. Clark & Maynard.
Federico Sangati and Chiara Mazza. 2009. An English
dependency treebank ? la Tesni?re. In Marco Pas-
sarotti, Adam Przepi?rkowski, Savina Raynaud, and
Frank Van Eynde, editors, Proceedings of the Eigth
International Workshop on Treebanks and Linguistic
Theories, pages 173?184. EDUCatt, Milan, Italy.
Gerold Schneider. 1998. A linguistic comparison of
constituency, dependency and link grammar. Mas-
ter?s thesis, University of Zurich.
Nathan Schneider, Brendan O?Connor, Naomi Saphra,
David Bamman, Manaal Faruqui, Noah A. Smith,
Chris Dyer, and Jason Baldridge. 2013. A frame-
work for (under)specifying dependency syntax with-
out overloading annotators. arXiv:1306.2091
[cs.CL]. arxiv.org/pdf/1306.2091.
Petr Sgall, Eva Hajic?ov?, and Jarmila Panevov?.
1986. The Meaning of the Sentence in its Seman-
tic and Pragmatic Aspects. Reidel, Dordrecht and
Academia, Prague.
Stuart M. Shieber. 1992. Constraint-Based Grammar
Formalisms. MIT Press, Cambridge, MA.
Daniel Sleator and Davy Temperly. 1993. Parsing En-
glish with a link grammar. In Proceedings of the
Third International Workshop on Parsing Technol-
ogy (IWPT?93), pages 277?292. Tilburg, Nether-
lands.
David A. Smith and Noah A. Smith. 2007. Proba-
bilistic models of nonprojective dependency trees.
In Proceedings of the 2007 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP-CoNLL 2007), pages 132?140. Associa-
tion for Computational Linguistics, Prague, Czech
Republic.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and
Andrew Ng. 2008. Cheap and fast ? but is it good?
Evaluating non-expert annotations for natural lan-
guage tasks. In Proceedings of the 2008 Confer-
ence on Empirical Methods in Natural Language
Processing (EMNLP 2008), pages 254?263. As-
sociation for Computational Linguistics, Honolulu,
Hawaii.
Mark Steedman. 2000. The Syntatic Process. MIT
Press, Cambridge, MA.
Luc Steels, Jan-Ola ?stman, and Kyoko Ohara, editors.
2011. Design patterns in Fluid Construction Gram-
mar. Number 11 in Constructional Approaches to
Language. John Benjamins, Amsterdam.
Lucien Tesni?re. 1959. El?ments de Syntaxe Struc-
turale. Klincksieck, Paris.
Michael Tomasello. 2003. Constructing a Language: A
Usage-Based Theory of Language Acquisition. Har-
vard University Press, Cambridge, MA.
60
Proceedings of the 7th Linguistic Annotation Workshop & Interoperability with Discourse, pages 178?186,
Sofia, Bulgaria, August 8-9, 2013. c?2013 Association for Computational Linguistics
Abstract Meaning Representation for Sembanking
Laura Banarescu
SDL
lbanarescu@sdl.com
Claire Bonial
Linguistics Dept.
Univ. Colorado
claire.bonial@colorado.edu
Shu Cai
ISI
USC
shucai@isi.edu
Madalina Georgescu
SDL
mgeorgescu@sdl.com
Kira Griffitt
LDC
kiragrif@ldc.upenn.edu
Ulf Hermjakob
ISI
USC
ulf@isi.edu
Kevin Knight
ISI
USC
knight@isi.edu
Philipp Koehn
School of Informatics
Univ. Edinburgh
pkoehn@inf.ed.ac.uk
Martha Palmer
Linguistics Dept.
Univ. Colorado
martha.palmer@colorado.edu
Nathan Schneider
LTI
CMU
nschneid@cs.cmu.edu
Abstract
We describe Abstract Meaning Represen-
tation (AMR), a semantic representation
language in which we are writing down
the meanings of thousands of English sen-
tences. We hope that a sembank of simple,
whole-sentence semantic structures will
spur new work in statistical natural lan-
guage understanding and generation, like
the Penn Treebank encouraged work on
statistical parsing. This paper gives an
overview of AMR and tools associated
with it.
1 Introduction
Syntactic treebanks have had tremendous impact
on natural language processing. The Penn Tree-
bank is a classic example?a simple, readable file
of natural-language sentences paired with rooted,
labeled syntactic trees. Researchers have ex-
ploited manually-built treebanks to build statisti-
cal parsers that improve in accuracy every year.
This success is due in part to the fact that we have
a single, whole-sentence parsing task, rather than
separate tasks and evaluations for base noun iden-
tification, prepositional phrase attachment, trace
recovery, verb-argument dependencies, etc. Those
smaller tasks are naturally solved as a by-product
of whole-sentence parsing, and in fact, solved bet-
ter than when approached in isolation.
By contrast, semantic annotation today is balka-
nized. We have separate annotations for named en-
tities, co-reference, semantic relations, discourse
connectives, temporal entities, etc. Each annota-
tion has its own associated evaluation, and training
data is split across many resources. We lack a sim-
ple readable sembank of English sentences paired
with their whole-sentence, logical meanings. We
believe a sizable sembank will lead to new work in
statistical natural language understanding (NLU),
resulting in semantic parsers that are as ubiquitous
as syntactic ones, and support natural language
generation (NLG) by providing a logical seman-
tic input.
Of course, when it comes to whole-sentence se-
mantic representations, linguistic and philosophi-
cal work is extensive. We draw on this work to de-
sign an Abstract Meaning Representation (AMR)
appropriate for sembanking. Our basic principles
are:
? AMRs are rooted, labeled graphs that are
easy for people to read, and easy for pro-
grams to traverse.
? AMR aims to abstract away from syntac-
tic idiosyncrasies. We attempt to assign the
same AMR to sentences that have the same
basic meaning. For example, the sentences
?he described her as a genius?, ?his descrip-
tion of her: genius?, and ?she was a ge-
nius, according to his description? are all as-
signed the same AMR.
? AMR makes extensive use of PropBank
framesets (Kingsbury and Palmer, 2002;
Palmer et al, 2005). For example, we rep-
resent a phrase like ?bond investor? using
the frame ?invest-01?, even though no verbs
appear in the phrase.
? AMR is agnostic about how we might want
to derive meanings from strings, or vice-
versa. In translating sentences to AMR, we
do not dictate a particular sequence of rule
applications or provide alignments that re-
flect such rule sequences. This makes sem-
banking very fast, and it allows researchers
to explore their own ideas about how strings
178
are related to meanings.
? AMR is heavily biased towards English. It
is not an Interlingua.
AMR is described in a 50-page annotation guide-
line.1 In this paper, we give a high-level descrip-
tion of AMR, with examples, and we also provide
pointers to software tools for evaluation and sem-
banking.
2 AMR Format
We write down AMRs as rooted, directed, edge-
labeled, leaf-labeled graphs. This is a com-
pletely traditional format, equivalent to the sim-
plest forms of feature structures (Shieber et al,
1986), conjunctions of logical triples, directed
graphs, and PENMAN inputs (Matthiessen and
Bateman, 1991). Figure 1 shows some of these
views for the sentence ?The boy wants to go?. We
use the graph notation for computer processing,
and we adapt the PENMAN notation for human
reading and writing.
3 AMR Content
In neo-Davidsonian fashion (Davidson, 1969), we
introduce variables (or graph nodes) for entities,
events, properties, and states. Leaves are labeled
with concepts, so that ?(b / boy)? refers to an in-
stance (called b) of the concept boy. Relations link
entities, so that ?(d / die-01 :location (p / park))?
means there was a death (d) in the park (p). When
an entity plays multiple roles in a sentence, we
employ re-entrancy in graph notation (nodes with
multiple parents) or variable re-use in PENMAN
notation.
AMR concepts are either English words
(?boy?), PropBank framesets (?want-01?), or spe-
cial keywords. Keywords include special entity
types (?date-entity?, ?world-region?, etc.), quan-
tities (?monetary-quantity?, ?distance-quantity?,
etc.), and logical conjunctions (?and?, etc).
AMR uses approximately 100 relations:
? Frame arguments, following PropBank
conventions. :arg0, :arg1, :arg2, :arg3, :arg4,
:arg5.
? General semantic relations. :accompa-
nier, :age, :beneficiary, :cause, :compared-to,
:concession, :condition, :consist-of, :degree,
:destination, :direction, :domain, :duration,
1AMR guideline: amr.isi.edu/language.html
LOGIC format:
? w, b, g:
instance(w, want-01) ? instance(g, go-01) ?
instance(b, boy) ? arg0(w, b) ?
arg1(w, g) ? arg0(g, b)
AMR format (based on PENMAN):
(w / want-01
:arg0 (b / boy)
:arg1 (g / go-01
:arg0 b))
GRAPH format:
Figure 1: Equivalent formats for representating
the meaning of ?The boy wants to go?.
:employed-by, :example, :extent, :frequency,
:instrument, :li, :location, :manner, :medium,
:mod, :mode, :name, :part, :path, :polarity,
:poss, :purpose, :source, :subevent, :subset,
:time, :topic, :value.
? Relations for quantities. :quant, :unit,
:scale.
? Relations for date-entities. :day, :month,
:year, :weekday, :time, :timezone, :quarter,
:dayperiod, :season, :year2, :decade, :cen-
tury, :calendar, :era.
? Relations for lists. :op1, :op2, :op3, :op4,
:op5, :op6, :op7, :op8, :op9, :op10.
AMR also includes the inverses of all these rela-
tions, e.g., :arg0-of, :location-of, and :quant-of. In
addition, every relation has an associated reifica-
tion, which is what we use when we want to mod-
ify the relation itself. For example, the reification
of :location is the concept ?be-located-at-91?.
Our set of concepts and relations is designed to
allow us represent all sentences, taking all words
into account, in a reasonably consistent manner. In
the rest of this section, we give examples of how
AMR represents various kinds of words, phrases,
and sentences. For full documentation, the reader
is referred to the AMR guidelines.
179
Frame arguments. We make heavy use of
PropBank framesets to abstract away from English
syntax. For example, the frameset ?describe-01?
has three pre-defined slots (:arg0 is the describer,
:arg1 is the thing described, and :arg2 is what it is
being described as).
(d / describe-01
:arg0 (m / man)
:arg1 (m2 / mission)
:arg2 (d / disaster))
The man described the mission as a disaster.
The man?s description of the mission:
disaster.
As the man described it, the mission was a
disaster.
Here, we do not annotate words like ?as? or ?it?,
considering them to be syntactic sugar.
General semantic relations. AMR also in-
cludes many non-core relations, such as :benefi-
ciary, :time, and :destination.
(s / hum-02
:arg0 (s2 / soldier)
:beneficiary (g / girl)
:time (w / walk-01
:arg0 g
:destination (t / town)))
The soldier hummed to the girl as she
walked to town.
Co-reference. AMR abstracts away from co-
reference gadgets like pronouns, zero-pronouns,
reflexives, control structures, etc. Instead we re-
use AMR variables, as with ?g? above. AMR
annotates sentences independent of context, so if
a pronoun has no antecedent in the sentence, its
nominative form is used, e.g., ?(h / he)?.
Inverse relations. We obtain rooted structures
by using inverse relations like :arg0-of and :quant-
of.
(s / sing-01
:arg0 (b / boy
:source (c / college)))
The boy from the college sang.
(b / boy
:arg0-of (s / sing-01)
:source (c / college))
the college boy who sang ...
(i / increase-01
:arg1 (n / number
:quant-of (p / panda)))
The number of pandas increased.
The top-level root of an AMR represents the fo-
cus of the sentence or phrase. Once we have se-
lected the root concept for an entire AMR, there
are no more focus considerations?everything else
is driven strictly by semantic relations.
Modals and negation. AMR represents nega-
tion logically with :polarity, and it expresses
modals with concepts.
(g / go-01
:arg0 (b / boy)
:polarity -)
The boy did not go.
(p / possible
:domain (g / go-01
:arg0 (b / boy))
:polarity -))
The boy cannot go.
It?s not possible for the boy to go.
(p / possible
:domain (g / go-01
:arg0 (b / boy)
:polarity -))
It?s possible for the boy not to go.
(p / obligate-01
:arg2 (g / go-01
:arg0 (b / boy))
:polarity -)
The boy doesn?t have to go.
The boy isn?t obligated to go.
The boy need not go.
(p / obligate-01
:arg2 (g / go-01
:arg0 (b / boy)
:polarity -))
The boy must not go.
It?s obligatory that the boy not go.
(t / think-01
:arg0 (b / boy)
:arg1 (w / win-01
:arg0 (t / team)
:polarity -))
The boy doesn?t think the team will win.
The boy thinks the team won?t win.
Questions. AMR uses the concept ?amr-
unknown?, in place, to indicate wh-questions.
(f / find-01
:arg0 (g / girl)
:arg1 (a / amr-unknown))
What did the girl find?
(f / find-01
:arg0 (g / girl)
:arg1 (b / boy)
:location (a / amr-unknown))
Where did the girl find the boy?
180
(f / find-01
:arg0 (g / girl)
:arg1 (t / toy
:poss (a / amr-unknown)))
Whose toy did the girl find?
Yes-no questions, imperatives, and embedded wh-
clauses are treated separately with the AMR rela-
tion :mode.
Verbs. Nearly every English verb and verb-
particle construction we have encountered has a
corresponding PropBank frameset.
(l / look-05
:arg0 (b / boy)
:arg1 (a / answer))
The boy looked up the answer.
The boy looked the answer up.
AMR abstracts away from light-verb construc-
tions.
(a / adjust-01
:arg0 (g / girl)
:arg1 (m / machine))
The girl adjusted the machine.
The girl made adjustments to the machine.
Nouns.We use PropBank verb framesets to rep-
resent many nouns as well.
(d / destroy-01
:arg0 (b / boy)
:arg1 (r / room))
the destruction of the room by the boy ...
the boy?s destruction of the room ...
The boy destroyed the room.
We never say ?destruction-01? in AMR. Some
nominalizations refer to a whole event, while oth-
ers refer to a role player in an event.
(s / see-01
:arg0 (j / judge)
:arg1 (e / explode-01))
The judge saw the explosion.
(r / read-01
:arg0 (j / judge)
:arg1 (t / thing
:arg1-of (p / propose-01))
The judge read the proposal.
(t / thing
:arg1-of (o / opine-01
:arg0 (g / girl)))
the girl?s opinion
the opinion of the girl
what the girl opined
Many ?-er? nouns invoke PropBank framesets.
This enables us to make use of slots defined for
those framesets.
(p / person
:arg0-of (i / invest-01))
investor
(p / person
:arg0-of (i / invest-01
:arg1 (b / bond)))
bond investor
(p / person
:arg0-of (i / invest-01
:manner (s / small)))
small investor
(w / work-01
:arg0 (b / boy)
:manner (h / hard))
the boy is a hard worker
the boy works hard
However, a treasurer is not someone who trea-
sures, and a president is not (just) someone who
presides.
Adjectives. Various adjectives invoke Prop-
Bank framesets.
(s / spy
:arg0-of (a / attract-01))
the attractive spy
(s / spy
:arg0-of (a / attract-01
:arg1 (w / woman)))
the spy who is attractive to women
?-ed? adjectives frequently invoke verb framesets.
For example, ?acquainted with magic? maps to
?acquaint-01?. However, we are not restricted to
framesets that can be reached through morpholog-
ical simplification.
(f / fear-01
:arg0 (s / soldier)
:arg1 (b / battle-01))
The soldier was afraid of battle.
The soldier feared battle.
The soldier had a fear of battle.
For other adjectives, we have defined new frame-
sets.
(r / responsible-41
:arg1 (b / boy)
:arg2 (w / work))
The boy is responsible for the work.
The boy has responsibility for the work.
While ?the boy responsibles the work? is not good
English, it is perfectly good Chinese. Similarly,
we handle tough-constructions logically.
181
(t / tough
:domain (p / please-01
:arg1 (g / girl)))
Girls are tough to please.
It is tough to please girls.
Pleasing girls is tough.
?please-01? and ?girl? are adjacent in the AMR,
even if they are not adjacent in English. ?-able?
adjectives often invoke the AMR concept ?possi-
ble?, but not always (e.g., a ?taxable fund? is actu-
ally a ?taxed fund?).
(s / sandwich
:arg1-of (e / eat-01
:domain-of (p / possible)))
an edible sandwich
(f / fund
:arg1-of (t / tax-01))
a taxable fund
Pertainym adjectives are normalized to root form.
(b / bomb
:mod (a / atom))
atom bomb
atomic bomb
Prepositions. Most prepositions simply sig-
nal semantic frame elements, and are themselves
dropped from AMR.
(d / default-01
:arg1 (n / nation)
:time (d2 / date-entity
:month 6))
The nation defaulted in June.
Time and location prepositions are kept if they
carry additional information.
(d / default-01
:arg1 (n / nation)
:time (a / after
:op1 (w / war-01))
The nation defaulted after the war.
Occasionally, neither PropBank nor AMR has an
appropriate relation, in which case we hold our
nose and use a :prep-X relation.
(s / sue-01
:arg1 (m / man)
:prep-in (c / case))
The man was sued in the case.
Named entities. Any concept in AMR can be
modified with a :name relation. However, AMR
includes standardized forms for approximately 80
named-entity types, including person, country,
sports-facility, etc.
(p / person
:name (n / name
:op1 "Mollie"
:op2 "Brown"))
Mollie Brown
(p / person
:name (n / name
:op1 "Mollie"
:op2 "Brown")
:arg0-of (s / slay-01
:arg1 (o / orc)))
the orc-slaying Mollie Brown
Mollie Brown, who slew orcs
AMR does not normalize multiple ways of re-
ferring to the same concept (e.g., ?US? versus
?United States?). It also avoids analyzing seman-
tic relations inside a named entity?e.g., an orga-
nization named ?Stop Malaria Now? does not in-
voke the ?stop-01? frameset. AMR gives a clean,
uniform treatment to titles, appositives, and other
constructions.
(c / city
:name (n / name
:op1 "Zintan"))
Zintan
the city of Zintan
(p / president
:name (n / name
:op1 "Obama"))
President Obama
Obama, the president ...
(g / group
:name (n / name
:op1 "Elsevier"
:op2 "N.V.")
:mod (c / country
:name (n2 / name
:op1 "Netherlands"))
:arg0-of (p / publish-01))
Elsevier N.V., the Dutch publishing group...
Dutch publishing group Elsevier N.V. ...
Copula. Copulas use the :domain relation.
(w / white
:domain (m / marble))
The marble is white.
(l / lawyer
:domain (w / woman))
The woman is a lawyer.
(a / appropriate
:domain (c / comment)
:polarity -))
The comment is not appropriate.
182
The comment is inappropriate.
Reification. Sometimes we want to use an
AMR relation as a first-class concept?to be able
to modify it, for example. Every AMR relation has
a corresponding reification for this purpose.
(m / marble
:location (j / jar))
the marble in the jar ...
(b / be-located-at-91
:arg1 (m / marble)
:arg2 (j / jar)
:polarity -)
:time (y / yesterday))
The marble was not in the jar yesterday.
If we do not use the reification, we run into trou-
ble.
(m / marble
:location (j / jar
:polarity -)
:time (y / yesterday))
yesterday?s marble in the non-jar ...
Some reifications are standard PropBank frame-
sets (e.g., ?cause-01? for :cause, or ?age-01? for
:age).
This ends the summary of AMR content. For
lack of space, we omit descriptions of compara-
tives, superlatives, conjunction, possession, deter-
miners, date entities, numbers, approximate num-
bers, discourse connectives, and other phenomena
covered in the full AMR guidelines.
4 Limitations of AMR
AMR does not represent inflectional morphology
for tense and number, and it omits articles. This
speeds up the annotation process, and we do not
have a nice semantic target representation for these
phenomena. A lightweight syntactic-style repre-
sentation could be layered in, via an automatic
post-process.
AMR has no universal quantifier. Words like
?all? modify their head concepts. AMR does not
distinguish between real events and hypothetical,
future, or imagined ones. For example, in ?the boy
wants to go?, the instances of ?want-01? and ?go-
01? have the same status, even though the ?go-01?
may or may not happen.
We represent ?history teacher? nicely as ?(p /
person :arg0-of (t / teach-01 :arg1 (h / history)))?.
However, ?history professor? becomes ?(p / pro-
fessor :mod (h / history))?, because ?profess-01?
is not an appropriate frame. It would be reason-
able in such cases to use a NomBank (Meyers et
al., 2004) noun frame with appropriate slots.
5 Creating AMRs
We have developed a power editor for AMR, ac-
cessible by web interface.2 The AMR Editor al-
lows rapid, incremental AMR construction via text
commands and graphical buttons. It includes on-
line documentation of relations, quantities, reifi-
cations, etc., with full examples. Users log in,
and the editor records AMR activity. The ed-
itor also provides significant guidance aimed at
increasing annotator consistency. For example,
users are warned about incorrect relations, discon-
nected AMRs, words that have PropBank frames,
etc. Users can also search existing sembanks for
phrases to see how they were handled in the past.
The editor also allows side-by-side comparison of
AMRs from different users, for training purposes.
In order to assess inter-annotator agreement
(IAA), as well as automatic AMR parsing accu-
racy, we developed the smatch metric (Cai and
Knight, 2013) and associated script.3 Smatch re-
ports the semantic overlap between two AMRs by
viewing each AMR as a conjunction of logical
triples (see Figure 1). Smatch computes precision,
recall, and F-score of one AMR?s triples against
the other?s. To match up variables from two in-
put AMRs, smatch needs to execute a brief search,
looking for the variable mapping that yields the
highest F-score.
Smatch makes no reference to English strings
or word indices, as we do not enforce any par-
ticular string-to-meaning derivation. Instead, we
compare semantic representations directly, in the
same way that the MT metric Bleu (Papineni et
al., 2002) compares target strings without making
reference to the source.
For an initial IAA study, and prior to adjust-
ing the AMR Editor to encourage consistency, 4
expert AMR annotators annotated 100 newswire
sentences and 80 web text sentences. They then
created consensus AMRs through discussion. The
average annotator vs. consensus IAA (smatch) was
0.83 for newswire and 0.79 for web text. When
newly trained annotators doubly annotated 382
web text sentences, their annotator vs. annotator
IAA was 0.71.
2AMR Editor: amr.isi.edu/editor.html
3Smatch: amr.isi.edu/evaluation.html
183
6 Current AMR Bank
We currently have a manually-constructed AMR
bank of several thousand sentences, a subset of
which can be freely downloaded,4 the rest being
distributed via the LDC catalog.
In initially developing AMR, the authors built
consensus AMRs for:
? 225 short sentences for tutorial purposes
? 142 sentences of newswire (*)
? 100 sentences of web data (*)
Trained annotators at LDC then produced AMRs
for:
? 1546 sentences from the novel ?The Little
Prince?
? 1328 sentences of web data
? 1110 sentences of web data (*)
? 926 sentences from Xinhua news (*)
? 214 sentences from CCTV broadcast con-
versation (*)
Collections marked with a star (*) are also in
the OntoNotes corpus (Pradhan et al, 2007;
Weischedel et al, 2011).
Using the AMR Editor, annotators are able to
translate a full sentence into AMR in 7-10 minutes
and postedit an AMR in 1-3 minutes.
7 Related Work
Researchers working on whole-sentence semantic
parsing today typically use small, domain-specific
sembanks like GeoQuery (Wong and Mooney,
2006). The need for larger, broad-coverage sem-
banks has sparked several projects, including the
Groningen Meaning Bank (GMB) (Basile et al,
2012a), UCCA (Abend and Rappoport, 2013),
the Semantic Treebank (ST) (Butler and Yoshi-
moto, 2012), the Prague Dependency Treebank
(Bo?hmova? et al, 2003), and UNL (Uchida et al,
1999; Uchida et al, 1996; Martins, 2012).
Concepts. Most systems use English words
as concepts. AMR uses PropBank frames (e.g.,
?describe-01?), and UNL uses English WordNet
synsets (e.g., ?200752493?).
Relations. GMB uses VerbNet roles (Schuler,
2005), and AMR uses frame-specific PropBank
relations. UNL has a dedicated set of over 30 fre-
quently used relations.
Formalism. GMB meanings are written in
DRT (Kamp et al, 2011), exploiting full first-
4amr.isi.edu/download.html
order logic. GMB and ST both include universal
quantification.
Granularity. GMB and UCCA annotate short
texts, so that the same entity can participate in
events described in different sentences; other sys-
tems annotate individual sentences.
Entities. AMR uses 80 entity types, while
GMB uses 7.
Manual versus automatic. AMR, UNL, and
UCCA annotation is fully manual. GMB and ST
produce meaning representations automatically,
and these can be corrected by experts or crowds
(Venhuizen et al, 2013).
Derivations. AMR and UNL remain agnostic
about the relation between strings and their mean-
ings, considering this a topic of open research.
ST and GMB annotate words and phrases directly,
recording derivations as (for example) Montague-
style compositional semantic rules operating on
CCG parses.
Top-down verus bottom-up. AMR annota-
tors find it fast to construct meanings from the
top down, starting with the main idea of the sen-
tence (though the AMR Editor allows bottom-up
construction). GMB and UCCA annotators work
bottom-up.
Editors, guidelines, genres. These projects
have graphical sembanking tools (e.g., Basile et al
(2012b)), annotation guidelines,5 and sembanks
that cover a wide range of genres, from news to
fiction. UNL and AMR have both annotated many
of the same sentences, providing the potential for
direct comparison.
8 Future Work
Sembanking. Our main goal is to continue
sembanking. We would like to employ a large
sembank to create shared tasks for natural lan-
guage understanding and generation. These
tasks may additionally drive interest in theoreti-
cal frameworks for probabilistically mapping be-
tween graphs and strings (Quernheim and Knight,
2012b; Quernheim and Knight, 2012a; Chiang et
al., 2013).
Applications. Just as syntactic parsing has
found many unanticipated applications, we expect
sembanks and statistical semantic processors to be
used for many purposes. To get started, we are
exploring the use of statistical NLU and NLG in
5UNL guidelines: www.undl.org/unlsys/unl/unl2005
184
a semantics-based machine translation (MT) sys-
tem. In this system, we annotate bilingual Chi-
nese/English data with AMR, then train compo-
nents to map Chinese to AMR, and AMR to En-
glish. A prototype is described by Jones et al
(2012).
Disjunctive AMR. AMR aims to canonicalize
multiple ways of saying the same thing. We plan
to test how well we are doing by building AMRs
on top of large, manually-constructed paraphrase
networks from the HyTER project (Dreyer and
Marcu, 2012). Rather than build individual AMRs
for different paths through a network, we will con-
struct highly-packed disjunctive AMRs. With this
application in mind, we have developed a guide-
line6 for disjunctive AMR. Here is an example:
(o / *OR*
:op1 (t / talk-01)
:op2 (m / meet-03)
:OR (o2 / *OR*
:mod (o3 / official)
:arg1-of (s / sanction-01
:arg0 (s2 / state))))
official talks
state-sanctioned talks
meetings sanctioned by the state
AMR extensions. Finally, we would like
to deepen the AMR language to include more
relations (to replace :mod and :prep-X, for
example), entity normalization (perhaps wik-
ification), quantification, and temporal rela-
tions. Ultimately, we would like to also in-
clude a comprehensive set of more abstract
frames like ?Earthquake-01? (:magnitude, :epi-
center, :casualties), ?CriminalLawsuit-01? (:de-
fendant, :crime, :jurisdiction), and ?Pregnancy-
01? (:father, :mother, :due-date). Projects like
FrameNet (Baker et al, 1998) and CYC (Lenat,
1995) have long pursued such a set.
References
O. Abend and A. Rappoport. 2013. UCCA: A
semantics-based grammatical annotation scheme. In
Proc. IWCS.
C. Baker, C. Fillmore, and J. Lowe. 1998. The Berke-
ley FrameNet project. In Proc. COLING.
V. Basile, J. Bos, K. Evang, and N. Venhuizen. 2012a.
Developing a large semantically annotated corpus.
In Proc. LREC.
6Disjunctive AMR guideline: amr.isi.edu/damr.1.0.pdf
V. Basile, J. Bos, K. Evang, and N. Venhuizen. 2012b.
A platform for collaborative semantic annotation. In
Proc. EACL demonstrations.
A. Bo?hmova?, J. Hajic?, E. Hajic?ova?, and B. Hladka?.
2003. The Prague dependency treebank. In Tree-
banks. Springer.
A. Butler and K. Yoshimoto. 2012. Banking meaning
representations from treebanks. Linguistic Issues in
Language Technology, 7.
S. Cai and K. Knight. 2013. Smatch: An accu-
racy metric for abstract meaning representations. In
Proc. ACL.
D. Chiang, J. Andreas, D. Bauer, K. M. Hermann,
B. Jones, and K. Knight. 2013. Parsing graphs with
hyperedge replacement grammars. In Proc. ACL.
D. Davidson. 1969. The individuation of events.
In N. Rescher, editor, Essays in Honor of Carl G.
Hempel. D. Reidel, Dordrecht.
M. Dreyer and D. Marcu. 2012. Hyter: Meaning-
equivalent semantics for translation evaluation. In
Proc. NAACL.
B. Jones, J. Andreas, D. Bauer, K. M. Hermann, and
K. Knight. 2012. Semantics-based machine trans-
lation with hyperedge replacement grammars. In
Proc. COLING.
H. Kamp, J. Van Genabith, and U. Reyle. 2011. Dis-
course representation theory. In Handbook of philo-
sophical logic, pages 125?394. Springer.
P. Kingsbury and M. Palmer. 2002. From TreeBank to
PropBank. In Proc. LREC.
D. B. Lenat. 1995. Cyc: A large-scale investment in
knowledge infrastructure. Communications of the
ACM, 38(11).
R. Martins. 2012. Le Petit Prince in UNL. In Proc.
LREC.
C. M. I. M. Matthiessen and J. A. Bateman. 1991.
Text Generation and Systemic-Functional Linguis-
tics. Pinter, London.
A. Meyers, R. Reeves, C. Macleod, R. Szekely,
V. Zielinska, B. Young, and R. Grishman. 2004.
The NomBank project: An interim report. In HLT-
NAACL 2004 workshop: Frontiers in corpus anno-
tation.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
Proposition Bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1).
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In ACL, Philadelphia, PA.
185
S. Pradhan, E. Hovy, M. Marcus, M. Palmer,
L. Ramshaw, and R. Weischedel. 2007. Ontonotes:
A unified relational semantic representation. In-
ternational Journal of Semantic Computing (IJSC),
1(4).
D. Quernheim and K. Knight. 2012a. DAGGER: A
toolkit for automata on directed acyclic graphs. In
Proc. FSMNLP.
D. Quernheim and K. Knight. 2012b. Towards prob-
abilistic acceptors and transducers for feature struc-
tures. In Proc. SSST Workshop.
K. Schuler. 2005. VerbNet: A broad-coverage, com-
prehensive verb lexicon. Ph.D. thesis, University of
Pennsylvania.
S. Shieber, F. C. N. Pereira, L. Karttunen, and M. Kay.
1986. Compilation of papers on unification-based
grammar formalisms. Technical Report CSLI-86-
48, Center for the Study of Language and Informa-
tion, Stanford, California.
H. Uchida, M. Zhu, and T. Della Senta. 1996. UNL:
Universal Networking Language?an electronic lan-
guage for communication, understanding and col-
laboration. Technical report, IAS/UNU Tokyo.
H. Uchida, M. Zhu, and T. Della Senta. 1999. A
gift for a millennium. Technical report, IAS/UNU
Tokyo.
N. Venhuizen, V. Basile, K. Evang, and J. Bos. 2013.
Gamification for word sense labeling. In Proc.
IWCS.
R. Weischedel, E. Hovy, M. Marcus, M. Palmer,
R. Belvin, S. Pradhan, L. Ramshaw, and N. Xue.
2011. OntoNotes: A large training corpus for en-
hanced processing. In J. Olive, C. Christianson, and
J. McCary, editors, Handbook of Natural Language
Processing and Machine Translation. Springer.
Y. W. Wong and R. J. Mooney. 2006. Learning for se-
mantic parsing with statistical machine translation.
In Proc. HLT-NAACL.
186
